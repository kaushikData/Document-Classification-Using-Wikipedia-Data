{"id": "8522903", "url": "https://en.wikipedia.org/wiki?curid=8522903", "title": "Alexander Macklin", "text": "Alexander Macklin\n\nColonel Alexander Hepburne Macklin OBE MC TD (1889 – 21 March 1967) was a British physician who served as one of the two surgeons on Sir Ernest Shackleton's Imperial Trans-Antarctic Expedition of 1914–1917. In 1921–22 he joined Shackleton on his last expedition on the \"Quest\".\n\nAlexander Macklin was born in 1889 in India, where his father was a doctor. When the family returned to England Dr Macklin set up practice in the Scilly Isles, where young Macklin became an enthusiastic and proficient boat handler.\nHe went to Plymouth College and then to the University of London. After working for a short time as a deckhand, he continued his education at the Victoria University of Manchester, where he qualified as a doctor.\n\nSoon after qualifying he applied to join Shackleton's expedition and was accepted. As well as his surgeon's duties he was put in charge of the ship's dogs and was also assigned a team of sledge dogs to drive. The \"Endurance\" became trapped in the ice and was later crushed, forcing Shackleton to lead his men across the ice to open water where they travelled by boat to Elephant Island. After the ship became trapped in the ice, Macklin's dog team was put to work. He brought back several seals that had been shot for food, and once the men began the long trek across the ice, he and the other dog teams were sent back to Ocean Camp (the first camp established near the ship) to fetch supplies. Eventually all the dogs had to be shot, but Macklin's team was the last to be killed.\n\nAfter arriving at Elephant Island, Shackleton and five men took one of the boats, the \"James Caird\", and set out to fetch help from South Georgia. Macklin and McIlroy, the other surgeon, were left behind as Shackleton knew the skills would be required more on the island than on the boat: Rickinson had a heart condition, Hudson was suffering a nervous breakdown. Blackborow had gangrene in his toes and shortly after the boat left, Macklin and McIlroy were forced to amputate all the toes on his left foot; Macklin gave him a chloroform anaesthetic while McIlroy removed the toes. Like most of the other ship's officers, Macklin was awarded the Silver Polar Medal for his efforts during the expedition.\n\nOn his return to England, Macklin gained a commission as a temporary lieutenant in the Royal Army Medical Corps (RAMC), with effect from 22 November 1916. He was promoted to temporary captain on 22 November 1917. During World War I he served in France, Russia and Italy. He won the Military Cross (MC) for bravery in tending the wounded under fire while serving in Italy. \n\nAfter the war, Macklin continued to serve with the RAMC, seeing service with the Allied Expeditionary Force in Northern Russia along with his old Boss, Shackleton. He rose to the acting rank of major on 4 May 1919 and, for his service in Russia, was appointed an Officer of the Order of the British Empire on 11 November 1919 and awarded the Russian Order of Saint Stanislaus. He resigned his commission on 23 March 1920, retaining the rank of captain.\n\nTogether with former \"Endurance\" crew members Worsley, Hussey, Wild, McIlroy, Kerr, MacLeod and Charles Green, Shackleton invited Macklin to join him for the Shackleton-Rowett Expedition in 1922 on board the \"Quest\". Shackleton had no fixed plans for the expedition; the ship put in at Rio de Janeiro and then headed for South Georgia. Shackleton was troubled with heart pain throughout the voyage, but despite Macklin's orders refused to rest. In Rio, Shackleton suffered a heart attack but would not let Macklin examine him. The ship landed in South Georgia on 4 January 1922. Early in the morning of 5 January, Macklin was called to Shackleton's quarters to find him having another heart attack. He died shortly after Macklin arrived. As the ship's surgeon, it was Macklin's role to prepare the body for burial on South Georgia.\n\nIn 1926 Macklin established a practice in Dundee, where he would work for the next 21 years. At the start of World War II, he returned to active service as a major in the Medical Corps, serving in East Africa and rising to lieutenant-colonel. He received the Territorial Decoration (TD), and retired from the army in August 1948 with the honorary rank of colonel.\n\nHe married Jean in 1948 and moved to Aberdeen where he worked in various of the Aberdeen hospitals before retiring from full-time practice in 1960. He continued to work in the University Student Health Service for some years. He and Jean had two sons, Alexander and Richard. He died on 21 March 1967.\n\n"}
{"id": "383480", "url": "https://en.wikipedia.org/wiki?curid=383480", "title": "Algorithmic learning theory", "text": "Algorithmic learning theory\n\nAlgorithmic learning theory is a mathematical framework for analyzing \nmachine learning problems and algorithms. Synonyms include formal learning theory and algorithmic inductive inference. Algorithmic learning theory is different from statistical learning theory in that it does not make use of statistical assumptions and analysis. Both algorithmic and statistical learning theory are concerned with machine learning and can thus be viewed as branches of computational learning theory.\n\nUnlike statistical learning theory and most statistical theory in general, algorithmic learning theory does not assume that data are random samples, that is, that data points are independent of each other. This makes the theory suitable for domains where observations are (relatively) noise-free but not random, such as language learning and automated scientific discovery.\n\nThe fundamental concept of algorithmic learning theory is learning in the limit: as the number of data points increases, a learning algorithm should converge to a correct hypothesis on \"every\" possible data sequence consistent with the problem space. This is a non-probabilistic version of statistical consistency, \nwhich also requires convergence to a correct model in the limit, but allows a learner to fail on data sequences with probability measure 0.\n\nAlgorithmic learning theory investigates the learning power of Turing machines. Other frameworks consider a much more restricted class of learning algorithms than Turing machines, for example learners that compute hypotheses more quickly, for instance in polynomial time. An example of such a framework is probably approximately correct learning.\n\nThe concept was introduced in E. Mark Gold's seminal paper \"Language identification in the limit\". The objective of language identification is for a machine running one program to be capable of developing another program by which any given sentence can be tested to determine whether it is \"grammatical\" or \"ungrammatical\". The language being learned need not be English or any other natural language - in fact the definition of \"grammatical\" can be absolutely anything known to the tester.\n\nIn Gold's learning model, the tester gives the learner an example sentence at each step, and the learner responds with a hypothesis, which is a suggested program to determine grammatical correctness. It is required of the tester that every possible sentence (grammatical or not) appears in the list eventually, but no particular order is required. It is required of the learner that at each step the hypothesis must be correct for all the sentences so far.\n\nA particular learner is said to be able to \"learn a language in the limit\" if there is a certain number of steps beyond which its hypothesis no longer changes. At this point it has indeed learned the language, because every possible sentence appears somewhere in the sequence of inputs (past or future), and the hypothesis is correct for all inputs (past or future), so the hypothesis is correct for every sentence. The learner is not required to be able to tell when it has reached a correct hypothesis, all that is required is that it be true.\n\nGold showed that any language which is defined by a Turing machine program can be learned in the limit by another Turing-complete machine using enumeration. This is done by the learner testing all possible Turing machine programs in turn until one is found which is correct so far - this forms the hypothesis for the current step. Eventually, the correct program will be reached, after which the hypothesis will never change again (but note that the learner does not know that it won't need to change).\n\nGold also showed that if the learner is given only positive examples (that is, only grammatical sentences appear in the input, not ungrammatical sentences), then the language can only be guaranteed to be learned in the limit if there are only a finite number of possible sentences in the language (this is possible if, for example, sentences are known to be of limited length).\n\nLanguage identification in the limit is a highly abstract model. It does not allow for limits of runtime or computer memory which can occur in practice, and the enumeration method may fail if there are errors in the input. However the framework is very powerful, because if these strict conditions are maintained, it allows the learning of any program known to be computable. This is because a Turing machine program can be written to mimic any program in any conventional programming language. See Church-Turing thesis.\n\nLearning theorists have investigated other learning criteria, such as the following.\n\n\nMind change bounds are closely related to mistake bounds that are studied in statistical learning theory. Kevin Kelly has suggested that minimizing mind changes is closely related to choosing maximally simple hypotheses in the sense of Occam’s Razor.\n\n\n"}
{"id": "47116440", "url": "https://en.wikipedia.org/wiki?curid=47116440", "title": "Amin Faghiri", "text": "Amin Faghiri\n\nAmin Faghiri (30 Shiraz Persian date Azar 1322 AH, 22 December 1943 CE) is an Iranian researcher and writer.\n\nAmin Faghiri is from Shiraz and was born into a large family. He had a difficult childhood. \"Pour with the rain\" is a documentary about him by Ali Zare Ghanat Nowi which is going to be released soon.\n\nHis early difficulties led him to write short stories. He had a precise method and style of writing. His stories employed realism and an honest visual style.\n\nHis first collection of short stories took four years to write. At the age of 23, he published \"Primal\". The book was published five times before the Iranian revolution. The sixth edition was published in 1382 (2003 CE). In the book, he described the loneliness and boredom of rural teachers. He also told stories of peasant violence and the sociological persecution they faced. Faghiri, alongside Mahoud Dowlat Abadi, was one of the first Iranian rural story writers. The stories in the collection have been translated into more than a dozen languages, including English, German, Russian, Urdu, Italian, French, and Japanese. He has nineteen books in total.\n\nFaghiri won the Golden Tablet for best fiction in 1376 (1997 CE).\n"}
{"id": "24020907", "url": "https://en.wikipedia.org/wiki?curid=24020907", "title": "Andyrobertsite", "text": "Andyrobertsite\n\nAndyrobertsite is a rare, complex arsenate mineral with a blue color. It is found in the Tsumeb mine in Namibia and named after Andrew C. Roberts (b. 1950), mineralogist with the Geological Survey of Canada. A Ca-rich analogue (with Ca instead of Cd) is called calcioandyrobertsite and has a more greenish tint.\n"}
{"id": "779903", "url": "https://en.wikipedia.org/wiki?curid=779903", "title": "BP Structure", "text": "BP Structure\n\nThe BP Structure, also known as Gebel Dalma, is an exposed impact crater in Libya. It is so called because it was identified by a BP (then British Petroleum) geological survey team.\n\nThe crater is 2 km in diameter and its age is estimated to be less than 120 million years (Lower Cretaceous or later).\n\n\n"}
{"id": "42604296", "url": "https://en.wikipedia.org/wiki?curid=42604296", "title": "Belz (crater)", "text": "Belz (crater)\n\nBelz Crater is an impact crater in the Oxia Palus quadrangle of Mars, located at 23.93° N and 43.23° W. It is 10.21 km in diameter and was named after the city of Belz, Lviv Oblast, Ukraine.\n"}
{"id": "4628", "url": "https://en.wikipedia.org/wiki?curid=4628", "title": "Bilinear transform", "text": "Bilinear transform\n\nThe bilinear transform (also known as Tustin's method) is used in digital signal processing and discrete-time control theory to transform continuous-time system representations to discrete-time and vice versa.\n\nThe bilinear transform is a special case of a conformal mapping (namely, a Möbius transformation), often used to convert a transfer function formula_1 of a linear, time-invariant (LTI) filter in the continuous-time domain (often called an analog filter) to a transfer function formula_2 of a linear, shift-invariant filter in the discrete-time domain (often called a digital filter although there are analog filters constructed with switched capacitors that are discrete-time filters). It maps positions on the formula_3 axis, formula_4, in the s-plane to the unit circle, formula_5, in the z-plane. Other bilinear transforms can be used to warp the frequency response of any discrete-time linear system (for example to approximate the non-linear frequency resolution of the human auditory system) and are implementable in the discrete domain by replacing a system's unit delays formula_6 with first order all-pass filters.\n\nThe transform preserves stability and maps every point of the frequency response of the continuous-time filter, formula_7 to a corresponding point in the frequency response of the discrete-time filter, formula_8 although to a somewhat different frequency, as shown in the Frequency warping section below. This means that for every feature that one sees in the frequency response of the analog filter, there is a corresponding feature, with identical gain and phase shift, in the frequency response of the digital filter but, perhaps, at a somewhat different frequency. This is barely noticeable at low frequencies but is quite evident at frequencies close to the Nyquist frequency.\n\nThe bilinear transform is a first-order approximation of the natural logarithm function that is an exact mapping of the \"z\"-plane to the \"s\"-plane. When the Laplace transform is performed on a discrete-time signal (with each element of the discrete-time sequence attached to a correspondingly delayed unit impulse), the result is precisely the Z transform of the discrete-time sequence with the substitution of\n\nwhere formula_10 is the numerical integration step size of the trapezoidal rule used in the bilinear transform derivation; or, in other words, the sampling period. The above bilinear approximation can be solved for formula_11 or a similar approximation for formula_12 can be performed.\n\nThe inverse of this mapping (and its first-order bilinear approximation) is\n\nThe bilinear transform essentially uses this first order approximation and substitutes into the continuous-time transfer function, formula_1\n\nThat is\n\nA continuous-time causal filter is stable if the poles of its transfer function fall in the left half of the complex s-plane. A discrete-time causal filter is stable if the poles of its transfer function fall inside the unit circle in the complex z-plane. The bilinear transform maps the left half of the complex s-plane to the interior of the unit circle in the z-plane. Thus, filters designed in the continuous-time domain that are stable are converted to filters in the discrete-time domain that preserve that stability.\n\nLikewise, a continuous-time filter is minimum-phase if the zeros of its transfer function fall in the left half of the complex s-plane. A discrete-time filter is minimum-phase if the zeros of its transfer function fall inside the unit circle in the complex z-plane. Then the same mapping property assures that continuous-time filters that are minimum-phase are converted to discrete-time filters that preserve that property of being minimum-phase.\n\nAs an example take a simple low-pass RC filter. This continuous-time filter has a transfer function\n\nIf we wish to implement this filter as a digital filter, we can apply the bilinear transform by substituting for formula_18 the formula above; after some reworking, we get the following filter representation:\n\nThe coefficients of the denominator are the 'feed-backward' coefficients and the coefficients of the numerator are the 'feed-forward' coefficients used to implement a real-time digital filter.\n\nIt is possible to relate the coefficients of a continuous-time, analog filter with those of a similar discrete-time digital filter created through the bilinear transform process. Transforming a general, second-order continuous-time filter with the given transfer function\n\nusing the bilinear transform (without prewarping any frequency specification) requires the substitution of\n\nwhere\n\nHowever, if the frequency warping compensation as described below is used in the bilinear transform, so that both analog and digital filter gain and phase agree at frequency formula_22, then\n\nThis results in a discrete-time digital biquad filter with coefficients expressed in terms of the coefficients of the original continuous time filter:\n\nNormally the constant term in the denominator must be normalized to 1 before deriving the corresponding difference equation. This results in\n\nThe difference equation (using the Direct Form I) is\n\nTo determine the frequency response of a continuous-time filter, the transfer function formula_27 is evaluated at formula_28 which is on the formula_29 axis. Likewise, to determine the frequency response of a discrete-time filter, the transfer function formula_30 is evaluated at formula_31 which is on the unit circle, formula_32. The bilinear transform maps the formula_29 axis of the \"s\"-plane (of which is the domain of formula_27) to the unit circle of the \"z\"-plane, formula_32 (which is the domain of formula_30), but it is not the same mapping formula_37 which also maps the formula_29 axis to the unit circle. When the actual frequency of formula_39 is input to the discrete-time filter designed by use of the bilinear transform, then it is desired to know at what frequency, formula_40, for the continuous-time filter that this formula_39 is mapped to.\n\nThis shows that every point on the unit circle in the discrete-time filter z-plane, formula_43 is mapped to a point on the formula_44 axis on the continuous-time filter s-plane, formula_45. That is, the discrete-time to continuous-time frequency mapping of the bilinear transform is\n\nand the inverse mapping is\n\nThe discrete-time filter behaves at frequency formula_48 the same way that the continuous-time filter behaves at frequency formula_49. Specifically, the gain and phase shift that the discrete-time filter has at frequency formula_48 is the same gain and phase shift that the continuous-time filter has at frequency formula_51. This means that every feature, every \"bump\" that is visible in the frequency response of the continuous-time filter is also visible in the discrete-time filter, but at a different frequency. For low frequencies (that is, when formula_52 or formula_53), then the features are mapped to a \"slightly\" different frequency; formula_54.\n\nOne can see that the entire continuous frequency range\n\nis mapped onto the fundamental frequency interval\n\nThe continuous-time filter frequency formula_57 corresponds to the discrete-time filter frequency formula_58 and the continuous-time filter frequency formula_59 correspond to the discrete-time filter frequency formula_60\n\nOne can also see that there is a nonlinear relationship between formula_61 and formula_62 This effect of the bilinear transform is called frequency warping. The continuous-time filter can be designed to compensate for this frequency warping by setting formula_46 for every frequency specification that the designer has control over (such as corner frequency or center frequency). This is called pre-warping the filter design.\n\nIt is possible, however, to compensate for the frequency warping by pre-warping a frequency specification formula_64 (usually a resonant frequency or the frequency of the most significant feature of the frequency response) of the continuous-time system. These pre-warped specifications may then be used in the bilinear transform to obtain the desired discrete-time system. When designing a digital filter as an approximation of a continuous time filter, the frequency response (both amplitude and phase) of the digital filter can be made to match the frequency response of the continuous filter at a specified frequency formula_64, as well as matching at DC, if the following transform is substituted into the continuous filter transfer function. This is a modified version of Tustin's transform shown above.\n\nHowever, note that this transform becomes the original transform\n\nas formula_68.\n\nThe main advantage of the warping phenomenon is the absence of aliasing distortion of the frequency response characteristic, such as observed with Impulse invariance.\n\n\n"}
{"id": "17265186", "url": "https://en.wikipedia.org/wiki?curid=17265186", "title": "Binary liquid", "text": "Binary liquid\n\nBinary liquid is a type of chemical combination, which creates a special reaction or feature as a result of two liquid chemicals, normally inert or having no function by themselves, being mixed. A number of chemical products are produced as a result of mixing two chemicals as a binary liquid, such as plastic foams and some explosives.\n\n\n\n"}
{"id": "13624163", "url": "https://en.wikipedia.org/wiki?curid=13624163", "title": "Bridesman", "text": "Bridesman\n\nA bridesman is a close male relative and/or friend of the bride, one who walks down the aisle in the bridal ceremony in the traditional place of a bridesmaid.\n\nThe term, however, has an ancient and obscure, possibly confabulated origin. The term is first noted by the Encyclopaedia Judaica from the European Jewish Diaspora of the middle of the 13th century. In this context, a bridesman was not a friend of the bride but of the groom. He paid for and arranged the wedding from his own money and would be repaid someday by the groom. It was a position of the highest level of honor in male friendship. It was akin to the modern-day best man. \n\nIn Hungary, where the word for bridesman is \"vőfély\" or sometimes \"vőfény\" (depending on the region), the ancient tradition of the bridesman is still very popular. The vőfély is the \"spokesman\" of the bridegroom (\"vő\" means son-in-law).\n"}
{"id": "1417208", "url": "https://en.wikipedia.org/wiki?curid=1417208", "title": "Carnegie Science Center", "text": "Carnegie Science Center\n\nThe Carnegie Science Center is one of the four Carnegie Museums of Pittsburgh, Pennsylvania. It is located in the Chateau neighborhood. It is located across the street from Heinz Field.\n\nThe Carnegie Science Center is the most visited museum in Pittsburgh. It has four floors of interactive exhibits. Among its attractions are the Buhl Planetarium (which features the latest in digital projection technology), the Rangos Omnimax Theater (promoted as \"the biggest screen in Pittsburgh\"), SportsWorks, the Miniature Railroad & Village, the USS \"Requin\" (a World War II submarine) and Roboworld, touted as \"the world's largest permanent robotics exhibition.\" The Roboworld exhibition contains more than 30 interactive displays featuring \"all things robotic\", and is also the first physical home for Carnegie Mellon University’s Robot Hall of Fame. It is closed on Sundays when there is a Steelers home game.\n\nAccording to Nicholas Efran, \"The Carnegie Science Center has been a gathering place for kids and families for many years.\" However, currently there are many new exhibits that staff are \"Not able to include because of the smaller size of the building\" It is now undergoing construction to add a new wing to the building. (As of December 2017)\n\nIts predecessor was the Buhl Planetarium and Institute of Popular Science, which opened on October 24, 1939. The Buhl Planetarium was the fifth major planetarium in the United States, and was popular for several decades. However, by the 1980s it had begun to show signs of age. An expansion was ruled out, so the Institute was relocated to the Chateau neighborhood. However, it became apparent to the Buhl Institute that the relocation efforts would require more staffing than they were able to provide. At this point, the Carnegie Institute (under the leadership of Robert Wilburn) stepped in, showing interest in merging with the Buhl Institute. Both parties agreed to the merger in 1987. On October 5, 1989, construction began on the $40 million building, which was renamed the Carnegie Science Center as a result of the merger. The Henry Buhl, Jr. Planetarium and Observatory was reinvented in this new facility. The Center opened in October 1991.\n\nThe Roboworld area is the second-floor attraction at the Carnegie Science Center. It is touted as \"the world's largest permanent robotics exhibition\", with more than 30 interactive displays featuring \"all things robotic\".\n\nThe first robot encounter in Roboworld is Andy Roid, the Robothespian. He is an interactive, animatronic robot that introduces visitors to the concepts of robotic sensing, thinking, and acting. The area's other exhibits showcase different types of robots, and videos containing more information about them.\n\nThe Robot Hall of Fame features famous robots from science fiction films, television, and video games, such as R2-D2, C-3PO, Data, the T-800 Terminator, R.O.B., Maschinenmensch, Gort, Robby the Robot, Robot B-9, HAL 9000, and Huey, Dewey, and Louie from \"Silent Running\".\n\nHighmark SportsWorks (formerly UPMC SportsWorks) is one of the major, permanent exhibits of the Carnegie Science Center. It is one of larger science and sports exhibitions in the world, with over 30 interactive experiences in which visitors can participate. SportsWorks features three themed areas: Physics of Sports (exploring the science of balance, trajectory, center of gravity, momentum, etc.), LifeWorks (featuring information for keeping a healthy lifestyle), and Sports Challenge (demonstrating various physical activities present in many sports).\n\nThe previous sponsor, UPMC, ended its sponsorship of SportsWorks in 2006. On November 13, 2008, the Carnegie Science Center unveiled plans for a new SportsWorks, sponsored by Highmark. It reopened in the Fall of 2009. \n\nFrom October 8, 2007 until May 2008, SportsWorks housed the controversial exhibit BODIES... The Exhibition. At least one employee of the Carnegie Science Center left her job due to the implementation of this exhibit.\n\nA committee of the Pittsburgh Port Authority recommended in 2007 that the site be purchased and that SportsWorks be demolished to allow for construction of tracks for the North Shore Connector, an extension of Pittsburgh's light rail line to the North Side of Pittsburgh.\n\nThe \"E-motion cone\" is a white-colored, inverted cone which sits atop the Science Center building. It was installed in 2000. At night, it is lit with different colors, signalling the weather forecast for the coming day.\n\n"}
{"id": "8537444", "url": "https://en.wikipedia.org/wiki?curid=8537444", "title": "Cosmic Calendar", "text": "Cosmic Calendar\n\nThe Cosmic Calendar is a method to visualize the chronology of the universe, scaling its current age of 13.8 billion years to a single year in order to help intuit it for pedagogical purposes in science education or popular science.\n\nIn this visualization, the Big Bang took place at the beginning of January 1 at midnight, and the current moment maps onto the end of December 31 just before midnight. \nAt this scale, there are 437.5 years per second, 1.575 million years per hour, and 37.8 million years per day.\n\nThe concept was popularized by Carl Sagan in his book \"The Dragons of Eden\" (1977) and on his television series \"\". Sagan goes on to extend the comparison in terms of surface area, explaining that if the Cosmic Calendar is scaled to the size of a football field, then \"all of human history would occupy an area the size of [his] hand\".\n\nThe Cosmic Calendar shows the time-scale relationship of the universe and all events on Earth as plotted along a single 12-month, 365-day, year: \n\nDate in year calculated from formula\n\nT(days) = 365 days * 0.100/13.797 ( 1- T_Gya/13.797 )\n\n\n"}
{"id": "25227215", "url": "https://en.wikipedia.org/wiki?curid=25227215", "title": "Cumulonimbus velum", "text": "Cumulonimbus velum\n\nCumulonimbus velum (Cb vel) (from the Latin \"cumulonimbus\", \"column-rain\" + \"velum\", \"veil\") is a cumulonimbus cloud with a small layer of altostratus cloud wrapped around its mid area, representing an area of humid stable air created as a result of the growth of the parent cumulonimbus. The altostratus velum cloud appears dark in comparison to its parent cloud, and can persist even after the cumulonimbus has disintegrated. The velum is very rare, as conditions necessary in development are infrequent.\n"}
{"id": "58996620", "url": "https://en.wikipedia.org/wiki?curid=58996620", "title": "Data Economy", "text": "Data Economy\n\nData Economy is an international technology business news and opinion website, magazine and broadcaster founded in 2016. The media outlet is headquartered in London, UK and focuses on the business strategy and finance and investment within the IT infrastructure space, mainly data centres. \n\nBroadmedia Communications, trading as BroadGroup and established in 2002, and a member company of FTSE 250 firm Euromoney Institutional Investor PLC (LSE: ERM), is registered as Data Economy's publisher. The title is also a member of the Professional Publishers Association (PPA).\n\nThe Data Economy news website was created in September 2016 following a business dinner in Monaco between publisher and Broadmedia Communications CEO Philip Low and journalist João Marques Lima.\n\nThe publication acquired in November 2016, datacentres.com, created in 2005 and whose reporting focused on the development of data centres and cloud infrastructure across the globe. Before the end of the year, Data Economy also launched the Frontline video series, in which it interviews executive officers, technology entrepreneurs and government figures. \n\nData Economy became part of the Euromoney Institutional Investor PLC portfolio in March 2017, when the organisation acquired Broadmedia Communications. The Daily Mail and General Trust (LSE: DMGT) is the majority shareholder of Euromoney Institutional Investor PLC with 49% of the company's stake. \n\nThe news source released in June 2017 its first print magazine, with production amounting to six editions yearly. Data Economy also runs a special supplement for specific market regions on a monthly basis.\n\nIn late 2017, Data Economy launched its Live series of events made of forums and summits targeted at industry executives, especially investors, operational companies, government and regional investment authorities and law firms. Data Economy is also the official global media partner of all Broadmedia Communications events including the Datacloud and Edge Congress series. \n\nIn September 2018, the group revamped its slogan, changing from \"Defining data centres and cloud 24/7\" to \"Invest Wisely\", matching its finance and investment focused coverage of the IT infrastructure sector. \n\nData Economy is aimed at a worldwide elite of senior IT and operations professionals in the data centre, cloud and data spaces. The publication also targets executives in the finance and investment spectrum and legal ecosphere. \n\nThe publication's content is showcased across its website for and opinion, its magazine for in depth interviews and analysis, and its broadcasting services through video interviews and live streaming from Data Economy, Datacloud and Edge Congress editions worldwide. \n\nData Economy launched its first print edition in Monaco 2017 during Datacloud Europe, attended by 1,800 delegates that year.\nData Economy produces six editions a year with unveilings at events in Europe, APAC, North America and Africa along with regional Data Economy Brief magazines. The magazines are also distributed globally and hosted online. \n\nThe magazine is usually 92 pages long and features content with C-Level executive and analysis. Some of the personalities interviewed for the magazine include the Prime Minister of Luxembourg Xavier Bettel, Lenovo's chairman and CEO Yang Yuanqing, and NetApp's George Kurian.\n\nThe magazine is laid out into the following sections: Global Leadership Talks, Finance and Investment Watch, Technology Business Talks and a People & Lifestyle Watch. Each issue also contains a special focus such as the Data Economy EMEA 50 or APAC 50, which lists the industry’s top minds in data centre, cloud and edge in the given region.\n\nIn 2017, Data Economy launched the \"EMEA 50\", a regional ranking of the data centre and cloud sector's top 50 influencers. The list is composed biannually and realised in the publication's June magazine, during the Datacloud Global Congress in Monaco. In the first edition, English industry entrepreneur, businessman, author and philanthropist Michael Tobin OBE ranked as EMEA's most influential leader. \n\nFollowing the launch of the \"EMEA 50\", Data Economy also initiated other rankings including ‘The 30 Under 30’, showcasing the young people who are leading the data centre, cloud and data industries, the world’s top 25 finance and investment movers and shakers in data centres and cloud, and the world’s first top 50 edge computing influencers.\n\nLaunched at the same time as when the publication was founded, Data Economy Frontline is a series of interviews in which industry leaders are questioned about their business and market trends. \n\nIn November 2017, the first Data Economy Live event was held in London, focusing on edge computing. Other events include the Data Economy Summit due for April 2019, focusing on the global investment, finance and M&A activity across the global IT infrastructure space. The event, a spin-off of Broadmedia Communications' 11-year running Finance and Investment Forum, gathers the strategic and financial leadership, private equity specialists, financiers, industry investors, REITs, professional transactional and advisory firms, and government. \n\nThe Data Economy Awards are an accolade to celebrate the top talent within the finance and investment space in the data centre and cloud sectors. \n\nThe show is hosted together with the Data Economy Summit and consists of 12 industry awards, including: CFO Of The Year Award, Edge Investment Award, Edge-Tech Startup Award, Editor’s Choice Award, Emerging Markets Award, Global Financial Leader Award, Investor Of The Year Award, Law Firm of the Year Award, Lifetime Financial Achievement Award, M&A Of The Year Award, Private Equity Fund Award, TMT Global Award.\n\n\n"}
{"id": "11269762", "url": "https://en.wikipedia.org/wiki?curid=11269762", "title": "Decision-making models", "text": "Decision-making models\n\nAll people need to make decisions from time to time. Given limited time in formulating policies and addressing public problems, public administrators must enjoy a certain degree of discretion in planning, revising and implementing public policies. In other words, they must engage in decision-making (Gianakis, 2004). Over the years, many scholars tried to devise decision-making models to account for the policy making process.\n\nSince the development of public administration, scholars have assumed that people make decisions rationally. By rationality, (1976) means ”a style of behaviour that is appropriate to the achievement of given goals, within the limits imposed by given conditions and constraints” (P. 405).\nMax Weber, in the early part of the 20th century, suggested distinguishing two types of economic rationality: formal rationality and substantive rationality. The \"formal rationality of economic action\" referred to \"the extent of quantitative calculation or accounting which is technically possible and . . . actually applied.\" \"Substantive rationality\" referred to the degree to which economic action serves \"ultimate values no matter what they may be.\" (Weber, The Theory of Social and Economic Organization, Parsons, ed., 1947, pp. 184-186) Weber noted that \"the requirements of formal and of substantive rationality are always in principle in conflict.\" (Ibid., p. 212) Decades later, Simon used a similar terminology to distinct two meanings of \"rationality\", which have developed separately in economic and psychology. He defined substantive rationality, stemming from the concept of rationality within economics, as behavior that \"is appropriate to the achievement of given goals within the limits imposed by given conditions and constraints\". Procedural rationality, based in psychology, refers to behavior that \"is the outcome of appropriate deliberation\". \n\nAccording to Gortner (2001), facts are the information and knowledge that the public administrators possess in formulating policies. Facts are important in deciding the appropriate means to take to achieve higher ends. They may not be readily known by administrators but need to be acquired through extensive research and analysis.\nRationality is defined in terms of appropriateness for the accomplishment of specific goals.\n\nValues are internal perceptions on the desirability and priority of one’s actions and choices. (Van Wart, 2004) Besides setting goals for their plans, decision-makers make priorities, interpret facts and act upon objective situations according to their values. Besides balancing conflicting values within an individual, government has to weigh and balance values embodied in different departments (Van Wart, 1996, 1998).\n\nMeans are the instruments to satisfy a higher end (Simon, 1997). Although they are used to achieve a higher end, they are not neutral in value. When policy makers devise their strategies, they choose their means according to their internal values and consequences...\n\nEnds are the intermediate goals to a more final objective. In a means-end hierarchy, the concept of means and ends is relative. An action can be a mean relative to the higher levels in the hierarchy but an end relative to the lower levels. However, in this hierarchy, an action is more value-based when moving upwards in the hierarchy but more fact-based when moving downwards.\n\nThere are several models of decision-making: \nThis model comes from the classical economist models, in which the decision-maker is perfectly and completely rational in every way. In this, following conditions are assumed.\n\nAt the opposite extreme from the economic rationality model is the social model drawn from psychology. Sigmund Freud viewed humans as bundles of feelings, emotions and instincts, with their behavior guided by their unconscious desires. These processes have even an impact in the international arena as they provide some basic rules of protocol.\nTo present a more realistic alternative to the economic rationality model, Herbert Simon proposed an alternative model. He felt that management decision-making behaviour could be described as follows:\n\nIn cognitive neuroscience decision-making refers to the cognitive process of evaluating a number of possibilities, and selecting the most appropriate thereof in order to further a specific goal, or task. This faculty is a fundamental component of executive functions, although recent studies show that a complex brain network is involved including motor areas.\n\n"}
{"id": "7424474", "url": "https://en.wikipedia.org/wiki?curid=7424474", "title": "Dissociation (neuropsychology)", "text": "Dissociation (neuropsychology)\n\nIn neuropsychology, dissociation involves identifying the neural substrate of a particular brain function through identification of case studies, neuroimaging, or neuropsychological testing.\n\nWhen dissecting complex mental tasks into their subcomponents, a researcher can establish a \"single dissociation\" between functions. This is done by demonstrating that a lesion to brain structure A disrupts function X but not function Y. Such a demonstration allows one to infer that function X and function Y are independent of each other in some way.\n\nDr. Oliver Sacks has described many famous cases of dissociation in his books. An example of \"single dissociation\" is a patient who cannot name an object when he or she can only see it, but can when he or she uses other senses like touching or smelling. Patient D.F. was unable to place a card in a slot, but could do so when told to place it \"as if mailing a letter\". From this the conclusion was drawn that judging orientation is one ability (which D.F. had lost) and visual control of an action another (which D.F. could still do).\n\nTo strengthen a single dissociation, a researcher can establish a \"double dissociation\", a term that was introduced by Hans-Lukas Teuber in 1955. This is the demonstration that two experimental manipulations each have different effects on two dependent variables; if one manipulation affects the first variable and not the second, the other manipulation affects the second variable and not the first. If one can demonstrate that a lesion in brain structure A impairs function X but not Y, and further demonstrate that a lesion to brain structure B impairs function Y but spares function X, one can make more specific inferences about brain function and function localization.\n\nIn cognitive neuroscience, double dissociation is an experimental technique by which two areas of neocortex are functionally dissociated by two behavioral tests, each test being affected by a lesion in one zone and not the other. In a series of patients with traumatic brain injury, one might find two patients, A and B. Patient A has difficulty performing cognitive tests for, say auditory memory but has no problem with visual memory. Patient B has the opposite problem. By using neuroimaging (or neuropathology post-mortem) to identify the overlap and dissociation between lesioned areas of the brain, one can infer something about the localization of visual and auditory function in the normal brain.\n\nEstablishing a single dissociation between two functions provides limited and potentially misleading information, whereas a double dissociation can conclusively demonstrate that the two functions are localized in different areas of the brain.\n\nTo make the difference between single and double dissociations easier to understand, Parkin gives the following example:\nIf your TV set suddenly loses the color you can conclude that picture transmission and color information must be separate processes (single dissociation: they cannot be independent because you cannot lose the picture and still have the color). If on the other hand you have two TV sets, one without sound and one without a picture you can conclude that these must be two independent functions (double dissociation).\n\nPaul Broca and Carl Wernicke were two physicians of the 1800s whose patients were evidence of the double dissociation between generating language (speech) and understanding language. Broca's patients could no longer speak but could understand language (non-fluent aphasia) while Wernicke's patients could no longer understand language but could produce jumbled speech (fluent aphasia). Post-mortems revealed lesions in separate areas of the brain in each case (now referred to as Broca's area and Wernicke's area respectively). Although the neurophysiology of language is now known to be more complicated than described by Broca or Wernicke, this classic double dissociation acted to begin modern neuropsychological investigation of language.\n\nThe conditions Capgras delusion and prosopagnosia have also been argued to represent a double dissociation. In the former, a patient is able to recognise a person but does not get the feeling of knowing them. In the latter, a patient is unable to recognise a familiar person but sometimes has a feeling of knowing.\n"}
{"id": "22649696", "url": "https://en.wikipedia.org/wiki?curid=22649696", "title": "Electrocapillarity", "text": "Electrocapillarity\n\nElectrocapillarity or electrocapillary phenomena are the phenomena related to changes in the surface energy (or interfacial tension) of the dropping mercury electrode (DME) as the electrode potential changes or the electrolytic solution composition and concentration change. The term \"electro-capillary\" is used to describe the change in mercury (Hg) electrode potential as a function of the change in the surface or interfacial tension of the Hg determined by the capillary rise method. The phenomena are the historic main contributions for understanding and validating the models of the structure of the electrical double layer. The phenomena are related to the electrokinetic phenomena and consequently to the colloid chemistry.\n\nThe interfacial (surface) tension, St, (dyne cm), can be calculated by applying the equation of capillary rise method (when the contact angle Ө → 0):\n\nformula_1\n\nwhere:\n\nThe circuit contains Hg electrode as the ideally polarizable electrode and a reference electrode as the non-polarizable electrode. Thus, when an external voltage is applied, only EM/S of Hg/solution interface is changed.\n\n\n"}
{"id": "13756871", "url": "https://en.wikipedia.org/wiki?curid=13756871", "title": "Electrophoretic light scattering", "text": "Electrophoretic light scattering\n\nElectrophoretic light scattering (also known as laser Doppler electrophoresis or phase analysis light scattering ) is based on dynamic light scattering. The frequency shift or phase shift of an incident laser beam depends on the dispersed particles mobility. In the case of \"dynamic light scattering\", Brownian motion causes particle motion. In the case of \"electrophoretic light scattering\", oscillating electric field performs the same function.\n\nThis method is used for measuring electrophoretic mobility and then calculating zeta potential. Instruments to apply the method are commercially available from several manufacturers. The last set of calculations requires information on viscosity and dielectric permittivity of the dispersion medium. Appropriate electrophoresis theory is also required. Sample dilution is often necessary in order to eliminate multiple scattering of the incident laser beam and/or particle interactions.\n\nA laser beam passes through the electrophoresis cell, irradiates the particles dispersed in it, and is scattered by the particles. The scattered light is detected by a photo-multiplier after passing through two pinholes. There are two types of optical systems: heterodyne and fringe.\nWare and Flygare developed a heterodyne-type ELS instrument, that was the first instrument of this type. In a fringe optics ELS instrument, a laser beam is divided into two beams. Those cross inside the electrophresis cell at a fixed angle to produce a fringe pattern. The scattered light from the particles, which migrates inside the fringe, is intensity-modulated. The frequency shifts from both types of optics obey the same equations. The observed spectra resemble each other.\nOka et al. developed an ELS instrument of heterodyne-type optics that is now available commercially. Its optics is shown in Fig. 3.\n\nIf the frequencies of the intersecting laser beams are the same then it is not possible to resolve the direction of the motion of the migrating particles. Instead, only the magnitude of the velocity (i.e., the speed) can be determined. Hence, the sign of the zeta potential cannot be ascertained. This limitation can be overcome by shifting the frequency of one of the beams relative to the other. Such shifting may be referred to as frequency modulation or, more colloquially, just modulation. Modulators used in ELS may include piezo-actuated mirrors or acousto-optic modulators. This modulation scheme is employed by the heterodyne light scattering method, too.\n\nThe frequency of light scattered by particles undergoing electrophoresis is shifted by the amount of the Doppler effect, formula_1 from that of the incident light, :formula_2 .\nThe shift can be detected by means of heterodyne optics in which the scattering light is mixed with the reference light.\nThe autocorrelation function of intensity of the mixed light, formula_3, can be approximately described by the following damped cosine function [7].\n\nwhere formula_5 is a decay constant and A, B, and C are positive constants dependent on the optical system.\n\nDamping frequency formula_6 is an observed frequency, and is the frequency difference between scattered and reference light.\n\nwhere formula_8 is the frequency of scattered light, formula_9 the frequency of the reference light,\nformula_10 the frequency of incident light (laser light),\nand formula_11 the modulation frequency.\n\nThe power spectrum of mixed light, namely the Fourier transform of formula_3, gives a couple of Lorenz functions at formula_13 having a half-width of formula_14 at the half maximum.\n\nIn addition to these two, the last term in equation (1) gives another Lorenz function at\nformula_15\n\nThe Doppler shift of frequency and the decay constant are dependent on the geometry of the optical system and are expressed respectively by the equations.\n\nand\n\nwhere formula_18 is velocity of the particles, formula_19 is the amplitude of the scattering vector, and formula_20 is the translational diffusion constant of particles.\n\nThe amplitude of the scattering vector formula_19 is given by the equation\n\nSince velocity formula_18 is proportional to the applied electric field, formula_24, the apparent electrophoretic mobility formula_25 is define by the equation\n\nFinally, the relation between the Doppler shift frequency and mobility is given for the case of the optical configuration of Fig. 3 by the equation\n\nwhere formula_24 is the strength of the electric field, formula_29 the refractive index of the medium, formula_30, the wavelength of the incident light in vacuum, and formula_31 the scattering angle.\nThe sign of formula_32 is a result of vector calculation and depends on the geometry of the optics.\n\nThe spectral frequency can be obtained according to Eq. (2).\nWhen formula_33, Eq. (2) is modified and expressed as\n\nThe modulation frequency formula_35 can be obtained as the damping frequency without an electric field applied.\n\nThe particle diameter is obtained by assuming that the particle is spherical. This is called the hydrodynamic diameter, formula_36 .\n\nwhere formula_38 is Boltzmann coefficient, formula_39 is the absolute temperature, and formula_40 the dynamic viscosity of the surrounding fluid.\n\nFigure 4 shows two examples of heterodyne autocorrelation functions of scattered light from sodium polystyrene sulfate solution (NaPSS; MW 400,000; 4 mg/mL in 10 mM NaCl). The oscillating correlation function shown by Fig. 4a is a result of interference between the scattered light and the modulated reference light.\nThe beat of Fig. 4b includes additionally the contribution from the frequency changes of light scattered by PSS molecules under an electrical field of 40 V/cm.\n\nFigure 5 shows heterodyne power spectra obtained by Fourier transform of the autocorrelation functions shown in Fig. 4.\n\nFigure 6 shows plots of Doppler shift frequencies measured at various cell depth and electric field strengths, where a sample is the NaPSS solution.\nThese parabolic curves are called profiles of electro-osmotic flow and indicate that the velocity of the particles changed at different depth.\nThe surface potential of the cell wall produces electro-osmotic flow.\nSince the electrophoresis chamber is a closed system, backward flow is produced at the center of the cell.\nThen the observed mobility or velocity from Eq. (7) is a result of the combination of osmotic flow and electrophoretic movement.\n\nElectrophoretic mobility analysis has been studied by Mori and Okamoto [16], who have taken into account the effect of electro-osmotic flow at the side wall.\n\nThe profile of velocity or mobility at the center of the cell is given approximately by Eq. (11) for the case where k>5.\nwhere\n\nThe parabolic curve of frequency shift caused by electro-osmotic flow shown in Fig. 6 fits with Eq. (11) with application of the least squares method.\n\nSince the mobility is proportional to a frequency shift of the light scattered by a particle and the migrating velocity of a particle as indicated by Eq. (7), all the velocity, mobility, and frequency shifts are expressed by parabolic equations.\nThen the true electrophoretic mobility of a particle, the electro-osmotic mobility at the upper and lower cell walls, ware obtained.\nThe frequency shift caused only by the electrophoresis of particles is equal to the apparent mobility at the stationary layer.\n\nThe velocity of the electrophoretic migration thus obtained is proportional to the electric field as shown in Fig. 7.\nThe frequency shift increases with increase of the scattering angle as shown in Fig. 8.\nThis result is in agreement with the theoretical Eq. (7).\n\nElectrophoretic Light Scattering (ELS) is primarily used for characterizing the surface charges of colloidal particles like macromolecules or synthetic polymers (ex. polystyrene) in liquid media in an electric field. In addition to information about surface charges, ELS can also measure the particle size of proteins and determine the zeta potential distribution.\n\nELS is useful for characterizing information about the surface of proteins. Ware and Flygare (1971) demonstrated that electrophoretic techniques can be combined with laser beat spectroscopy in order to simultaneously determine the electrophoretic mobility and diffusion coefficient of bovine serum albumin. The width of a Doppler shifted spectrum of light that is scattered from a solution of macromolecules is proportional to the diffusion coefficient. The Doppler shift is proportional to the electrophoretic mobility of a macromolecule. From studies that have applied this method to poly (L-lysine), ELS is believed to monitor fluctuation mobilities in the presence of solvents with varying salt concentrations. It has also been shown that electrophoretic mobility data can be converted to zeta potential values, which enables the determination of the isoelectric point of proteins and the number of electrokinetic charges on the surface.\n\nOther biological macromolecules that can be analyzed with ELS include polysaccharides. pKa values of chitosans can be calculated from the dependency of electrophoretic mobility values on pH and charge density. Like proteins, the size and zeta potential of chitosans can be determined through ELS.\n\nELS has also been applied to nucleic acids and viruses. The technique can be extended to measure electrophoretic mobilities of large bacteria molecules at low ionic strengths.\n\nELS has been used to characterize the polydispersity, nanodispersity, and stability of single-walled carbon nanotubes in an aqueous environment with surfactants. The technique can be used in combination with dynamic light scattering to measure these properties of nanotubes in many different solvents.\n\n(1) Surfactant Science Series, Consulting Editor Martin J. Schick Consultant New York, Vol. 76 Electrical Phenomena at Interfaces Second Edition, Fundamentals, Measurements and Applications, Second Edition, Revised and Expanded. Ed by Hiroyuki Ohshima, Kunio Furusawa. 1998. K. Oka and K. Furusawa, Chapter 8　Electrophresis, p. 152 - 223. Marcel Dekker, Inc,\n\n(7) B.R. Ware and D.D. Haas, in Fast Method in Physical Biochemistry and Cell Biology. (R.I. Sha'afi and S.M. Fernandez, Eds), Elsevier, New York, 1983, Chap. 8.\n\n(9) R. Ware and W.H. Flygare, J.Colloid Interface Sci. 39: 670 (1972).\n\n(10) J. Josefwiicz and F.R. Hallett, Appl. Opt. 14: 740 ( (1975).\n\n(11) K. Oka, W. Otani, K. Kameyama, M. Kidai, and T. Takagi, Appl. Theor. Electrophor. 1: 273-278 (1990).\n\n(12) K. Oka, W. Otani, Y. Kubo, Y. Zasu, and M. Akagi, U.S. Patent Appl. 465, 186: Jpn. Patent H7-5227 (1995).\n\n(16) S. Mori and H. Okamoto, Flotation 28: 1 (1980). (in Japanese): Fusen 28(3): 117 (1980).\n\n(17) M. Smoluchowski, in Handbuch der Electrizitat und des Magnetismus. (L. Greatz. Ed). Barth, Leripzig, 1921, pp. 379.\n\n(18) P. White, Phil. Mag. S 7, 23, No. 155 (1937).\n\n(19) S. Komagat, Res. Electrotech. Lab. (Jpn) 348, March 1933.\n\n(20) Y. Fukui, S. Yuu and K. Ushiki, Power Technol. 54: 165 (1988).\n"}
{"id": "27428127", "url": "https://en.wikipedia.org/wiki?curid=27428127", "title": "European Association of Development Research and Training Institutes", "text": "European Association of Development Research and Training Institutes\n\nThe European Association of Development Research and Training Institutes (EADI) is the professional body for development studies and area studies in Europe. It has about 300 members in 27 countries. \n\nIt publishes the journal European Journal of Development Research.\n\nEvery three years, EADI holds a general conference, in cooperation with one of the member institutes.\n\nThe mission of EADI is to promote quality in research and education, scientific cooperation and the dissemination of development research to the public.\nEADI aims to further contacts between researchers, development agencies, politicians, individual stakeholders, and national and international organisations in Europe, Africa, Asia and Latin America. EADI communicates and disseminates research results and training opportunities to such constituencies as: government and non-government development agencies, international development organisations, and the wider media and policy-makers.\n\nThe association receives funding from various sources including membership fees and core funding from the Germany's Federal Ministry for Economic Cooperation and Development.\n\nAs part of the quality insurance work, an accreditation programme for Master courses was set up by EADI, named “International Accreditation Council” (IAC/EADI). The council is a member of INQAAHE (International Network for Quality Assurance Agencies in Higher Education).\n\nEADI was founded 1975 in Linz. It was originally based in Vienna, before moving to Tilburg (1982) and Geneva (1988). Since 2000, the secretariat is located in Bonn, hosted by the following three institutions:\n\n"}
{"id": "59861", "url": "https://en.wikipedia.org/wiki?curid=59861", "title": "Experiment", "text": "Experiment\n\nAn experiment is a procedure carried out to support, refute, or validate a hypothesis. Experiments provide insight into cause-and-effect by demonstrating what outcome occurs when a particular factor is manipulated. Experiments vary greatly in goal and scale, but always rely on repeatable procedure and logical analysis of the results. There also exists natural experimental studies.\n\nA child may carry out basic experiments to understand gravity, while teams of scientists may take years of systematic investigation to advance their understanding of a phenomenon. Experiments and other types of hands-on activities are very important to student learning in the science classroom. Experiments can raise test scores and help a student become more engaged and interested in the material they are learning, especially when used over time. Experiments can vary from personal and informal natural comparisons (e.g. tasting a range of chocolates to find a favorite), to highly controlled (e.g. tests requiring complex apparatus overseen by many scientists that hope to discover information about subatomic particles). Uses of experiments vary considerably between the natural and human sciences.\n\nExperiments typically include controls, which are designed to minimize the effects of variables other than the single independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. Ideally, all variables in an experiment are controlled (accounted for by the control measurements) and none are uncontrolled. In such an experiment, if all controls work as expected, it is possible to conclude that the experiment works as intended, and that results are due to the effect of the tested variable.\n\nIn the scientific method, an experiment is an empirical procedure that arbitrates competing models or hypotheses. Researchers also use experimentation to test existing theories or new hypotheses to support or disprove them.\n\nAn experiment usually tests a hypothesis, which is an expectation about how a particular process or phenomenon works. However, an experiment may also aim to answer a \"what-if\" question, without a specific expectation about what the experiment reveals, or to confirm prior results. If an experiment is carefully conducted, the results usually either support or disprove the hypothesis. According to some philosophies of science, an experiment can never \"prove\" a hypothesis, it can only add support. On the other hand, an experiment that provides a counterexample can disprove a theory or hypothesis, but a theory can always be salvaged by appropriate ad hoc modifications at the expense of simplicity. An experiment must also control the possible confounding factors—any factors that would mar the accuracy or repeatability of the experiment or the ability to interpret the results. Confounding is commonly eliminated through scientific controls and/or, in randomized experiments, through random assignment.\n\nIn engineering and the physical sciences, experiments are a primary component of the scientific method. They are used to test theories and hypotheses about how physical processes work under particular conditions (e.g., whether a particular engineering process can produce a desired chemical compound). Typically, experiments in these fields focus on replication of identical procedures in hopes of producing identical results in each replication. Random assignment is uncommon.\n\nIn medicine and the social sciences, the prevalence of experimental research varies widely across disciplines. When used, however, experiments typically follow the form of the clinical trial, where experimental units (usually individual human beings) are randomly assigned to a treatment or control condition where one or more outcomes are assessed. In contrast to norms in the physical sciences, the focus is typically on the average treatment effect (the difference in outcomes between the treatment and control groups) or another test statistic produced by the experiment. A single study typically does not involve replications of the experiment, but separate studies may be aggregated through systematic review and meta-analysis.\n\nThere are various differences in experimental practice in each of the branches of science. For example, agricultural research frequently uses randomized experiments (e.g., to test the comparative effectiveness of different fertilizers), while experimental economics often involves experimental tests of theorized human behaviors without relying on random assignment of individuals to treatment and control conditions.\n\nOne of the first methodical approaches to experiments in the modern sense is visible in the works of the Arab mathematician and scholar Ibn al-Haytham. He conducted his experiments in the field of optics - going back to optical and mathematical problems in the works of Ptolemy - by controlling his experiments due to factors such as self-criticality, reliance on visible results of the experiments as well as a criticality in terms of earlier results. He counts as one of the first scientists/ philosophers using an inductive-experimental method for achieving results. In his book \"Optics\" he describes the fundamentally new approach to knowledge and research in an experimental sense:\n\n\"“We should, that is, recommence the inquiry into its principles and premisses, beginning our investigation with an inspection of the things that exist and a survey of the conditions of visible objects. We should distinguish the properties of particulars, and gather by induction what pertains to the eye when vision takes place and what is found in the manner of sensation to be uniform, unchanging, manifest and not subject to doubt. After which we should ascend in our inquiry and reasonings, gradually and orderly, criticizing premisses and exercising caution in regard to conclusions – our aim in all that we make subject to inspection and review being to employ justice, not to follow prejudice, and to take care in all that we judge and criticize that we seek the truth and not to be swayed by opinion. We may in this way eventually come to the truth that gratifies the heart and gradually and carefully reach the end at which certainty appears; while through criticism and caution we may seize the truth that dispels disagreement and resolves doubtful matters. For all that, we are not free from that human turbidity which is in the nature of man; but we must do our best with what we possess of human power. From God we derive support in all things.“\"\n\nAccording to his explanation, a strictly controlled test execution with a sensibility for the subjectivity and susceptibility of outcomes due to the nature of man is necessary. Furthermore, a critical view on the results and outcomes of earlier scholars is necessary:\n\n\"“It is thus the duty of the man who studies the writings of scientists, if learning the truth is his goal, to make himself an enemy of all that he reads, and, applying his mind to the core and margins of its content, attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency.”\"\n\nThus, a comparison of earlier results with the experimental results is necessary for an objective experiment - the visible results being more important. In the end, this may mean that an experimental researcher must find enough courage to discard traditional opinions or results, especially if these results are not experimental but results from a logical/ mental derivation. In this process of critical consideration, the man himself should not forget that he tends to subjective opinions - through \"prejudices\" and \"leniency\" - and thus has to be critical about his own way of building hypotheses.\n\nFrancis Bacon (1561–1626), an English philosopher and scientist active in the 17th century, became an influential supporter of experimental science in the English renaissance. He disagreed with the method of answering scientific questions by deduction - similar to Ibn al-Haytham - and described it as follows: \"Having first determined the question according to his will, man then resorts to experience, and bending her to conformity with his placets, leads her about like a captive in a procession.\" Bacon wanted a method that relied on repeatable observations, or experiments. Notably, he first ordered the scientific method as we understand it today. \n\nIn the centuries that followed, people who applied the scientific method in different areas made important advances and discoveries. For example, Galileo Galilei (1564-1642) accurately measured time and experimented to make accurate measurements and conclusions about the speed of a falling body. Antoine Lavoisier (1743-1794), a French chemist, used experiment to describe new areas, such as combustion and biochemistry and to develop the theory of conservation of mass (matter). Louis Pasteur (1822-1895) used the scientific method to disprove the prevailing theory of spontaneous generation and to develop the germ theory of disease. Because of the importance of controlling potentially confounding variables, the use of well-designed laboratory experiments is preferred when possible.\n\nA considerable amount of progress on the design and analysis of experiments occurred in the early 20th century, with contributions from statisticians such as Ronald Fisher (1890-1962), Jerzy Neyman (1894-1981), Oscar Kempthorne (1919-2000), Gertrude Mary Cox (1900-1978), and William Gemmell Cochran (1909-1980), among others.\n\nExperiments might be categorized according to a number of dimensions, depending upon professional norms and standards in different fields of study. In some disciplines (e.g., psychology or political science), a 'true experiment' is a method of social research in which there are two kinds of variables. The independent variable is manipulated by the experimenter, and the dependent variable is measured. The signifying characteristic of a true experiment is that it randomly allocates the subjects to neutralize experimenter bias, and ensures, over a large number of iterations of the experiment, that it controls for all confounding factors.\n\nA controlled experiment often compares the results obtained from experimental samples against \"control\" samples, which are practically identical to the experimental sample except for the one aspect whose effect is being tested (the independent variable). A good example would be a drug trial. The sample or group receiving the drug would be the experimental group (treatment group); and the one receiving the placebo or regular treatment would be the control one. In many laboratory experiments it is good practice to have several replicate samples for the test being performed and have both a positive control and a negative control. The results from replicate samples can often be averaged, or if one of the replicates is obviously inconsistent with the results from the other samples, it can be discarded as being the result of an experimental error (some step of the test procedure may have been mistakenly omitted for that sample). Most often, tests are done in duplicate or triplicate. A positive control is a procedure similar to the actual experimental test but is known from previous experience to give a positive result. A negative control is known to give a negative result. The positive control confirms that the basic conditions of the experiment were able to produce a positive result, even if none of the actual experimental samples produce a positive result. The negative control demonstrates the base-line result obtained when a test does not produce a measurable positive result. Most often the value of the negative control is treated as a \"background\" value to subtract from the test sample results. Sometimes the positive control takes the quadrant of a standard curve.\n\nAn example that is often used in teaching laboratories is a controlled protein assay. Students might be given a fluid sample containing an unknown (to the student) amount of protein. It is their job to correctly perform a controlled experiment in which they determine the concentration of protein in the fluid sample (usually called the \"unknown sample\"). The teaching lab would be equipped with a protein standard solution with a known protein concentration. Students could make several positive control samples containing various dilutions of the protein standard. Negative control samples would contain all of the reagents for the protein assay but no protein. In this example, all samples are performed in duplicate. The assay is a colorimetric assay in which a spectrophotometer can measure the amount of protein in samples by detecting a colored complex formed by the interaction of protein molecules and molecules of an added dye. In the illustration, the results for the diluted test samples can be compared to the results of the standard curve (the blue line in the illustration) to estimate the amount of protein in the unknown sample.\n\nControlled experiments can be performed when it is difficult to exactly control all the conditions in an experiment. In this case, the experiment begins by creating two or more sample groups that are \"probabilistically equivalent,\" which means that measurements of traits should be similar among the groups and that the groups should respond in the same manner if given the same treatment. This equivalency is determined by statistical methods that take into account the amount of variation between individuals and the number of individuals in each group. In fields such as microbiology and chemistry, where there is very little variation between individuals and the group size is easily in the millions, these statistical methods are often bypassed and simply splitting a solution into equal parts is assumed to produce identical sample groups.\n\nOnce equivalent groups have been formed, the experimenter tries to treat them identically except for the one \"variable\" that he or she wishes to isolate. Human experimentation requires special safeguards against outside variables such as the \"placebo effect\". Such experiments are generally \"double blind\", meaning that neither the volunteer nor the researcher knows which individuals are in the control group or the experimental group until after all of the data have been collected. This ensures that any effects on the volunteer are due to the treatment itself and are not a response to the knowledge that he is being treated.\n\nIn human experiments, researchers may give a subject (person) a stimulus that the subject responds to. The goal of the experiment is to measure the response to the stimulus by a test method.\n\nIn the design of experiments, two or more \"treatments\" are applied to estimate the difference between the mean responses for the treatments. For example, an experiment on baking bread could estimate the difference in the responses associated with quantitative variables, such as the ratio of water to flour, and with qualitative variables, such as strains of yeast. Experimentation is the step in the scientific method that helps people decide between two or more competing explanations – or hypotheses. These hypotheses suggest reasons to explain a phenomenon, or predict the results of an action. An example might be the hypothesis that \"if I release this ball, it will fall to the floor\": this suggestion can then be tested by carrying out the experiment of letting go of the ball, and observing the results. Formally, a hypothesis is compared against its opposite or null hypothesis (\"if I release this ball, it will not fall to the floor\"). The null hypothesis is that there is no explanation or predictive power of the phenomenon through the reasoning that is being investigated. Once hypotheses are defined, an experiment can be carried out and the results analysed to confirm, refute, or define the accuracy of the hypotheses.\n\nThe term \"experiment\" usually implies a controlled experiment, but sometimes controlled experiments are prohibitively difficult or impossible. In this case researchers resort to \"natural experiments\" or \"quasi-experiments.\" Natural experiments rely solely on observations of the variables of the system under study, rather than manipulation of just one or a few variables as occurs in controlled experiments. To the degree possible, they attempt to collect data for the system in such a way that contribution from all variables can be determined, and where the effects of variation in certain variables remain approximately constant so that the effects of other variables can be discerned. The degree to which this is possible depends on the observed correlation between explanatory variables in the observed data. When these variables are \"not\" well correlated, natural experiments can approach the power of controlled experiments. Usually, however, there is some correlation between these variables, which reduces the reliability of natural experiments relative to what could be concluded if a controlled experiment were performed. Also, because natural experiments usually take place in uncontrolled environments, variables from undetected sources are neither measured nor held constant, and these may produce illusory correlations in variables under study.\n\nMuch research in several science disciplines, including economics, political science, geology, paleontology, ecology, meteorology, and astronomy, relies on quasi-experiments. For example, in astronomy it is clearly impossible, when testing the hypothesis \"Stars are collapsed clouds of hydrogen\", to start out with a giant cloud of hydrogen, and then perform the experiment of waiting a few billion years for it to form a star. However, by observing various clouds of hydrogen in various states of collapse, and other implications of the hypothesis (for example, the presence of various spectral emissions from the light of stars), we can collect data we require to support the hypothesis. An early example of this type of experiment was the first verification in the 17th century that light does not travel from place to place instantaneously, but instead has a measurable speed. Observation of the appearance of the moons of Jupiter were slightly delayed when Jupiter was farther from Earth, as opposed to when Jupiter was closer to Earth; and this phenomenon was used to demonstrate that the difference in the time of appearance of the moons was consistent with a measurable speed.\n\nField experiments are so named to distinguish them from laboratory experiments, which enforce scientific control by testing a hypothesis in the artificial and highly controlled setting of a laboratory. Often used in the social sciences, and especially in economic analyses of education and health interventions, field experiments have the advantage that outcomes are observed in a natural setting rather than in a contrived laboratory environment. For this reason, field experiments are sometimes seen as having higher external validity than laboratory experiments. However, like natural experiments, field experiments suffer from the possibility of contamination: experimental conditions can be controlled with more precision and certainty in the lab. Yet some phenomena (e.g., voter turnout in an election) cannot be easily studied in a laboratory.\n\nAn observational study is used when it is impractical, unethical, cost-prohibitive (or otherwise inefficient) to fit a physical or social system into a laboratory setting, to completely control confounding factors, or to apply random assignment. It can also be used when confounding factors are either limited or known well enough to analyze the data in light of them (though this may be rare when social phenomena are under examination). For an observational science to be valid, the experimenter must know and account for confounding factors. In these situations, observational studies have value because they often suggest hypotheses that can be tested with randomized experiments or by collecting fresh data.\n\nFundamentally, however, observational studies are not experiments. By definition, observational studies lack the manipulation required for Baconian experiments. In addition, observational studies (e.g., in biological or social systems) often involve variables that are difficult to quantify or control. Observational studies are limited because they lack the statistical properties of randomized experiments. In a randomized experiment, the method of randomization specified in the experimental protocol guides the statistical analysis, which is usually specified also by the experimental protocol. Without a statistical model that reflects an objective randomization, the statistical analysis relies on a subjective model. Inferences from subjective models are unreliable in theory and practice. In fact, there are several cases where carefully conducted observational studies consistently give wrong results, that is, where the results of the observational studies are inconsistent and also differ from the results of experiments. For example, epidemiological studies of colon cancer consistently show beneficial correlations with broccoli consumption, while experiments find no benefit.\n\nA particular problem with observational studies involving human subjects is the great difficulty attaining fair comparisons between treatments (or exposures), because such studies are prone to selection bias, and groups receiving different treatments (exposures) may differ greatly according to their covariates (age, height, weight, medications, exercise, nutritional status, ethnicity, family medical history, etc.). In contrast, randomization implies that for each covariate, the mean for each group is expected to be the same. For any randomized trial, some variation from the mean is expected, of course, but the randomization ensures that the experimental groups have mean values that are close, due to the central limit theorem and Markov's inequality. With inadequate randomization or low sample size, the systematic variation in covariates between the treatment groups (or exposure groups) makes it difficult to separate the effect of the treatment (exposure) from the effects of the other covariates, most of which have not been measured. The mathematical models used to analyze such data must consider each differing covariate (if measured), and results are not meaningful if a covariate is neither randomized nor included in the model.\n\nTo avoid conditions that render an experiment far less useful, physicians conducting medical trials – say for U.S. Food and Drug Administration approval – quantify and randomize the covariates that can be identified. Researchers attempt to reduce the biases of observational studies with complicated statistical methods such as propensity score matching methods, which require large populations of subjects and extensive information on covariates. Outcomes are also quantified when possible (bone density, the amount of some cell or substance in the blood, physical strength or endurance, etc.) and not based on a subject's or a professional observer's opinion. In this way, the design of an observational study can render the results more objective and therefore, more convincing.\n\nBy placing the distribution of the independent variable(s) under the control of the researcher, an experiment – particularly when it involves human subjects – introduces potential ethical considerations, such as balancing benefit and harm, fairly distributing interventions (e.g., treatments for a disease), and informed consent. For example, in psychology or health care, it is unethical to provide a substandard treatment to patients. Therefore, ethical review boards are supposed to stop clinical trials and other experiments unless a new treatment is believed to offer benefits as good as current best practice. It is also generally unethical (and often illegal) to conduct randomized experiments on the effects of substandard or harmful treatments, such as the effects of ingesting arsenic on human health. To understand the effects of such exposures, scientists sometimes use observational studies to understand the effects of those factors.\n\nEven when experimental research does not directly involve human subjects, it may still present ethical concerns. For example, the nuclear bomb experiments conducted by the Manhattan Project implied the use of nuclear reactions to harm human beings even though the experiments did not directly involve any human subjects.\n\nThe experimental method can be useful in solving juridical problems.\n\n\n\n"}
{"id": "12914558", "url": "https://en.wikipedia.org/wiki?curid=12914558", "title": "George Bass (optician)", "text": "George Bass (optician)\n\nGeorge Bass was an optician known to have made an achromatic doublet around 1733. \n\nThe instructions for the constructions were given by Chester Moore Hall. According to Hoyle, Hall wished to keep his work on the achromatic lenses a secret and contracted the manufacture of the crown and flint lenses to two different opticians, Edward Scarlett and James Mann. They in turn sub-contracted the work to the same person, George Bass. He realized the two components were for the same client and, after fitting the two parts together, noted the achromatic properties. Not being as reticent as Hall, Bass let others know of the lens's properties and the method of making an achromatic doublet spread.\n"}
{"id": "51220368", "url": "https://en.wikipedia.org/wiki?curid=51220368", "title": "Hans Gebien", "text": "Hans Gebien\n\nHans Gebien 4 October 1874, Horn, Hamburg- 9 October 1947, Großhansdorf) was a German entomologist who specialised in Tenebrionidae (Coleoptera) \nHis collections are in Biozentrum Grindel und Zoologisches Museum, Hamburg , the Natural History Museum of Basel and in Museo Civico di Storia Naturale di Milano (both ex Museum G. Frey Tutzing).\n\n\n\n"}
{"id": "14164788", "url": "https://en.wikipedia.org/wiki?curid=14164788", "title": "Izuo Hayashi", "text": "Izuo Hayashi\n\nHayashi was born in Tokyo in 1922 and graduated from the faculty of science, University of Tokyo in 1946. He worked as assistant professor at the Institute for Nuclear Research of the same university and defended his PhD in 1962. After the PhD defense, he stayed for a year at MIT, and between 1964 and 1971 worked at Bell Labs on semiconductor lasers. In 1971 he joined the Research Laboratories of NEC where he continued his studies of semiconductor lasers, aiming to improve their reliability and lifetime. Between 1982 and 1987 he was a head scientist at NEC and in 1987–1994 became director of the Optoelectronics Technology Research Laboratory in Tsukuba. From 1994 till retirement in 1996 he served as advisor in the same laboratory. Hayashi died of acute leukemia in 2005.\n\n"}
{"id": "5054615", "url": "https://en.wikipedia.org/wiki?curid=5054615", "title": "Journal of Men, Masculinities and Spirituality", "text": "Journal of Men, Masculinities and Spirituality\n\nJournal of Men, Masculinities and Spirituality (JMMS) is a free, online, scholarly, peer-reviewed, interdisciplinary, open access journal about men's studies. JMMS was established 2007, and is published twice a year with provision for other special editions. JMMS was founded by Joseph Gelfer who remains the executive editor.\n\nJMMS seeks to be as inclusive as possible in its area of enquiry. Papers address the full spectrum of masculinities and sexualities, particularly those which are seldom heard. Similarly, JMMS addresses not only monotheistic religions and spiritualities but also Eastern, indigenous, new religious movements and other spiritualities which resist categorization. JMMS papers address historical and contemporary phenomena as well as speculative essays about future spiritualities.\n\nThe first issue of JMMS featured, published in January 2007, an editorial by Joseph Gelfer; research notes of Yvonne Maria Werner and Anna Prestjan; articles by Roland Boer, Frank A. Salamone, David Shneer, Juan M. Marin and Rini Bhattacharya Mehta; and book reviews by Joseph Gelfer, James Bryant, Wisam Mansour, Sophie Smith, Katharina von Kellenbach and Nathan Abrams.\n\nIssues of JMMS are included in the Informit e-Library, an Australasian online scholarly research repository; as well as EBSCO, and Gale Cengage.\n\n\n"}
{"id": "17634320", "url": "https://en.wikipedia.org/wiki?curid=17634320", "title": "LI-900", "text": "LI-900\n\nLI-900 is a type of reusable surface insulation tile developed and manufactured by Lockheed Missiles and Space Company in Sunnyvale, California. It was designed for use on the Space Shuttle orbiter as part of its thermal protection system to minimize thermal conductivity while providing maximum thermal shock resistance.\n\nLI-900 has a bulk density of 144.2 kg/m³ (9 lb/ft³). It was for this reason that it was called the LI-900. It is made from 99.9% pure silica glass fibres, and is 94% air by volume. An LI-900 tile can be heated to 1204 °C (about 1478 K or 2200 Fahrenheit) and then immediately plunged into cold water and suffer no damage.\n\nBlack and white tiles were used on the Space Shuttle to control the temperature of the vehicle while in orbit.\n\n\nThere are typically 20,000 HRSI LI-900 tiles on a Space Shuttle, and 725 LRSI LI-900 tiles.\n\nAs a result of optimizing its thermal properties, overall strength was reduced. The tile was therefore not suitable to be used in high-stress areas such as around the landing gear doors and windows. To solve this, a higher strength version of the LI-900 material was produced, with a bulk density of 352.4 kg/m³ (22 lbs/ft³), which was called the LI-2200. This tile provided the strength and insulating properties, but with a considerable weight penalty.\n\nA research and development program was started to establish a means for substantially improving the damage resistance and micrometeorite orbital debris (MMOD) characteristics of the LI-900 shuttle baseline tile. This would result in a greatly reduced amount of damage and therefore less repair between flights.\n\nA short term solution was found by developing a new surface treatment for the tile, which took advantage of the Ames technology previously developed, and designated TUFI. A prototype of this material was produced successfully, its damage resistance was determined, and its microstructural stability was demonstrated.\n\nA longer term solution used AETB-8 as the insulation substrate. This substrate is significantly stronger than the LI-900 substrate, and is more compatible with the TUFI surface treatment.\n\n\n"}
{"id": "6697249", "url": "https://en.wikipedia.org/wiki?curid=6697249", "title": "List of Chinese astronauts", "text": "List of Chinese astronauts\n\nThis is a list of Chinese astronauts, people trained by the China National Space Administration (CNSA) to command, pilot, or serve as a crew member of a spacecraft.\n\nAs the Chinese space program developed during the sixties, various proposals for manned spacecraft were made. The first manned spacecraft proposed by the People's Republic of China during the late 1960s and early 1970s was the Shuguang One which was expected to bring the first Chinese astronaut in 1973 into space. For this programme 19 astronauts were selected in 1971. However, shortly after these plans were made, several leading scientists attached to the project were denounced during the Cultural Revolution, bringing progress to a standstill. Instead, NASA astronaut Taylor Wang, a naturalized U.S. citizen born in China, became the first ethnically Chinese person in space in 1985.\n\nThe People's Liberation Army Astronaut Corps was established in 1998 for the selection of Shenzhou program astronauts. In 2003, Yang Liwei was launched aboard Shenzhou 5, becoming the first person sent into space by the Chinese space program. This achievement made China the third country to independently send humans into space. During the Shenzhou 7 mission in 2008, Zhai Zhigang became the first Chinese citizen to carry out a spacewalk. In 2012, Liu Yang became the first Chinese woman to be launched into space when she was launched aboard Shenzhou 9.\n\nAs of 2017, eleven Chinese nationals have traveled in space.\n\n\n\n\n\n"}
{"id": "10328596", "url": "https://en.wikipedia.org/wiki?curid=10328596", "title": "List of Unicode characters", "text": "List of Unicode characters\n\nThis is a list of Unicode characters. As of version 11.0, Unicode contains a repertoire of over 137,000 characters covering 146 modern and historic scripts, as well as multiple symbol sets. As it is to list all of these characters in a single Wikipedia page, this list is limited to a subset of the most important characters for English-language readers, with links to other pages which list the supplementary characters. This page includes the 1062 characters in the Multilingual European Character Set 2 (MES-2) subset, and some additional related characters.\n\nAn HTML or XML \"numeric character reference\" refers to a character by its Universal Character Set/Unicode \"code point\", and uses the format\n\nor\n\nwhere \"nnnn\" is the code point in decimal form, and \"hhhh\" is the code point in hexadecimal form. The \"x\" must be lowercase in XML documents. The \"nnnn\" or \"hhhh\" may be any number of digits and may include leading zeros. The \"hhhh\" may mix uppercase and lowercase, though uppercase is the usual style.\n\nIn contrast, a \"character entity reference\" refers to a character by the name of an \"entity\" which has the desired character as its \"replacement text\". The entity must either be predefined (built into the markup language) or explicitly declared in a Document Type Definition (DTD). The format is the same as for any entity reference:\n\nwhere \"name\" is the case-sensitive name of the entity. The semicolon is required.\n\n65 characters, including DEL but not SP. All belong to the common script.\n\nThe Unicode Standard (version 7.0) classifies 1,338 characters as belonging to the Latin script.\n\n95 characters; the 52 alphabet characters belong to the Latin script. The remaining 43 belong to the common script.\nThe 33 characters classified as ASCII Punctuation & Symbols are also sometimes referred to as ASCII special characters. See § Latin-1 Supplement and § Unicode symbols for additional \"special characters\".\n\n96 characters; the 62 letters, and two ordinal indicators belong to the Latin script. The remaining 32 belong to the common script.\n\n128 characters; all belong to the Latin script.\n\n208 characters; all belong to the Latin script; 33 in the MES-2 subset.\n\n256 characters; all belong to the Latin script; 23 in the MES-2 subset. For the rest, see Latin Extended Additional (Unicode block).\n\n96 characters; all belong to the Latin script; three in the MES-2 subset. For the rest, see IPA Extensions (Unicode block).\n\n80 characters; 15 in the MES-2 subset.\n\n\n144 code points; 135 assigned characters; 85 in the MES-2 subset.\n\nFor polytonic orthography. 256 code points; 233 assigned characters, all in the MES-2 subset (#670 – 902).\n256 characters; 191 in the MES-2 subset.\n\n\n\n\nThe range from U+0900 to U+0DFF includes Devanagari, Bengali script, Gurmukhi, Gujarati script, Odia alphabet, Tamil script, Telugu script, Kannada script, Malayalam script, and the Sinhalese script.\n\n\nOther Brahmic and Indic scripts in Unicode include:\n\n\n\n\n\n\n112 code points; 111 assigned characters; 24 in the MES-2 subset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3343262", "url": "https://en.wikipedia.org/wiki?curid=3343262", "title": "List of accelerators in particle physics", "text": "List of accelerators in particle physics\n\nA list of particle accelerators used for particle physics experiments. Some early particle accelerators that more properly did nuclear physics, but existed prior to the separation of particle physics from that field, are also included. Although a modern accelerator complex usually has several stages of accelerators, only accelerators whose output has been used directly for experiments are listed.\n\nThese all used single beams with fixed targets. They tended to have very briefly run, inexpensive, and unnamed experiments.\n\n The magnetic pole pieces and return yoke from the 60-inch cyclotron were later moved to UC Davis and incorporated into a 76 inch isochronous cyclotron which is still in use today \n\nMore modern accelerators that were also run in fixed target mode; often, they will also have been run as colliders, or accelerated particles for use in subsequently built colliders.\n\nBesides the real accelerators listed above, there are hypothetical accelerators often used\nas hypothetical examples or optimistic projects by particle physicists.\n\n"}
{"id": "34393705", "url": "https://en.wikipedia.org/wiki?curid=34393705", "title": "List of animals of the Sawtooth National Recreation Area", "text": "List of animals of the Sawtooth National Recreation Area\n\nThis article is an incomplete list of the species of mammals, reptiles, amphibians, and fish found in the Sawtooth National Recreation Area in central Idaho. Gray wolves were reintroduced to central Idaho in the 1990s while grizzly bears have been extirpated from the area, and plans to reintroduce them have been abandoned. The Sawtooth National Recreation Area supports habitat for Canada lynx and wolverines, but there have been no recent sightings.\n\n\n\n\n\n\n\n"}
{"id": "18426", "url": "https://en.wikipedia.org/wiki?curid=18426", "title": "Lithography", "text": "Lithography\n\nLithography () is a method of printing originally based on the immiscibility of oil and water. The printing is from a stone (lithographic limestone) or a metal plate with a smooth surface. It was invented in 1796 by German author and actor Alois Senefelder as a cheap method of publishing theatrical works. Lithography can be used to print text or artwork onto paper or other suitable material.\n\nLithography originally used an image drawn with oil, fat, or wax onto the surface of a smooth, level lithographic limestone plate. The stone was treated with a mixture of acid and gum arabic, \"etching\" the portions of the stone that were not protected by the grease-based image. When the stone was subsequently moistened, these etched areas retained water; an oil-based ink could then be applied and would be repelled by the water, sticking only to the original drawing. The ink would finally be transferred to a blank paper sheet, producing a printed page. This traditional technique is still used in some fine art printmaking applications.\n\nIn modern lithography, the image is made of a polymer coating applied to a flexible plastic or metal plate. The image can be printed directly from the plate (the orientation of the image is reversed), or it can be offset, by transferring the image onto a flexible sheet (rubber) for printing and publication.\n\nAs a printing technology, lithography is different from intaglio printing (gravure), wherein a plate is either engraved, etched, or stippled to score cavities to contain the printing ink; and woodblock printing or letterpress printing, wherein ink is applied to the raised surfaces of letters or images. Today, most types of high-volume books and magazines, especially when illustrated in colour, are printed with offset lithography, which has become the most common form of printing technology since the 1960s.\n\nThe related term \"photolithography\" refers to when photographic images are used in lithographic printing, whether these images are printed directly from a stone or from a metal plate, as in offset printing. \"Photolithography\" is used synonymously with \"offset printing\". The technique as well as the term were introduced in Europe in the 1850s. Beginning in the 1960s, photolithography has played an important role in the fabrication and mass production of integrated circuits in the microelectronics industry.\n\nLithography uses simple chemical processes to create an image. For instance, the positive part of an image is a water-repelling (\"hydrophobic\") substance, while the negative image would be water-retaining (\"hydrophilic\"). Thus, when the plate is introduced to a compatible printing ink and water mixture, the ink will adhere to the positive image and the water will clean the negative image. This allows a flat print plate to be used, enabling much longer and more detailed print runs than the older physical methods of printing (e.g., intaglio printing, letterpress printing).\n\nLithography was invented by Alois Senefelder in the Kingdom of Bavaria in 1796. In the early days of lithography, a smooth piece of limestone was used (hence the name \"lithography\": \"lithos\" (λιθος) is the ancient Greek word for stone). After the oil-based image was put on the surface, a solution of gum arabic in water was applied, the gum sticking only to the non-oily surface. During printing, water adhered to the gum arabic surfaces and was repelled by the oily parts, while the oily ink used for printing did the opposite.\n\nLithography works because of the mutual repulsion of oil and water. The image is drawn on the surface of the print plate with a fat or oil-based medium (hydrophobic) such as a wax crayon, which may be pigmented to make the drawing visible. A wide range of oil-based media is available, but the durability of the image on the stone depends on the lipid content of the material being used, and its ability to withstand water and acid. After the drawing of the image, an aqueous solution of gum arabic, weakly acidified with nitric acid is applied to the stone. The function of this solution is to create a hydrophilic layer of calcium nitrate salt, , and gum arabic on all non-image surfaces. The gum solution penetrates into the pores of the stone, completely surrounding the original image with a hydrophilic layer that will not accept the printing ink. Using lithographic turpentine, the printer then removes any excess of the greasy drawing material, but a hydrophobic molecular film of it remains tightly bonded to the surface of the stone, rejecting the gum arabic and water, but ready to accept the oily ink.\n\nSenefelder had experimented during the early 19th century with multicolor lithography; in his 1819 book, he predicted that the process would eventually be perfected and used to reproduce paintings. Multi-color printing was introduced by a new process developed by Godefroy Engelmann (France) in 1837 known as chromolithography. A separate stone was used for each color, and a print went through the press separately for each stone. The main challenge was to keep the images aligned (\"in register\"). This method lent itself to images consisting of large areas of flat color, and resulted in the characteristic poster designs of this period.\n\n\"Lithography, or printing from soft stone, largely took the place of engraving in the production of English commercial maps after about 1852. It was a quick, cheap process and had been used to print British army maps during the Peninsula War. Most of the commercial maps of the second half of the 19th century were lithographed and unattractive, though accurate enough.\"\n\nHigh-volume lithography is used presently to produce posters, maps, books, newspapers, and packaging—just about any smooth, mass-produced item with print and graphics on it. Most books, indeed all types of high-volume text, are now printed using offset lithography.\n\nFor offset lithography, which depends on photographic processes, flexible aluminum, polyester, mylar or paper printing plates are used instead of stone tablets. Modern printing plates have a brushed or roughened texture and are covered with a photosensitive emulsion. A photographic negative of the desired image is placed in contact with the emulsion and the plate is exposed to ultraviolet light. After development, the emulsion shows a reverse of the negative image, which is thus a duplicate of the original (positive) image. The image on the plate emulsion can also be created by direct laser imaging in a CTP (Computer-To-Plate) device known as a platesetter. The positive image is the emulsion that remains after imaging. Non-image portions of the emulsion have traditionally been removed by a chemical process, though in recent times plates have come available that do not require such processing.\n\nThe plate is affixed to a cylinder on a printing press. Dampening rollers apply water, which covers the blank portions of the plate but is repelled by the emulsion of the image area. Hydrophobic ink, which is repelled by the water and only adheres to the emulsion of the image area, is then applied by the inking rollers.\n\nIf this image were transferred directly to paper, it would create a mirror-type image and the paper would become too wet. Instead, the plate rolls against a cylinder covered with a rubber \"blanket\", which squeezes away the water, picks up the ink and transfers it to the paper with uniform pressure. The paper passes between the blanket cylinder and a counter-pressure or impression cylinder and the image is transferred to the paper. Because the image is first transferred, or \"offset\" to the rubber blanket cylinder, this reproduction method is known as \"offset lithography\" or \"offset printing\".\n\nMany innovations and technical refinements have been made in printing processes and presses over the years, including the development of presses with multiple units (each containing one printing plate) that can print multi-color images in one pass on both sides of the sheet, and presses that accommodate continuous rolls (\"webs\") of paper, known as web presses. Another innovation was the continuous dampening system first introduced by Dahlgren, instead of the old method (conventional dampening) which is still used on older presses, using rollers covered with molleton (cloth) that absorbs the water. This increased control of the water flow to the plate and allowed for better ink and water balance. Current dampening systems include a \"delta effect or vario,\" which slows the roller in contact with the plate, thus creating a sweeping movement over the ink image to clean impurities known as \"hickies\".\n\nThe process of lithography printing is illustrated by this simplified diagram. This press is also called an ink pyramid because the ink is transferred through several layers of rollers with different purposes. Fast lithographic 'web' printing presses are commonly used in newspaper production.\n\nThe advent of desktop publishing made it possible for type and images to be modified easily on personal computers for eventual printing by desktop or commercial presses. The development of digital imagesetters enabled print shops to produce negatives for platemaking directly from digital input, skipping the intermediate step of photographing an actual page layout. The development of the digital platesetter during the late 20th century eliminated film negatives altogether by exposing printing plates directly from digital input, a process known as computer to plate printing.\n\nMicrolithography and nanolithography refer specifically to lithographic patterning methods capable of structuring material on a fine scale. Typically, features smaller than 10 micrometers are considered microlithographic, and features smaller than 100 nanometers are considered nanolithographic. Photolithography is one of these methods, often applied to semiconductor manufacturing of microchips. Photolithography is also commonly used for fabricating microelectromechanical systems (MEMS) devices. Photolithography generally uses a pre-fabricated photomask or reticle as a master from which the final pattern is derived.\n\nAlthough photolithographic technology is the most commercially advanced form of nanolithography, other techniques are also used. Some, for example electron beam lithography, are capable of much greater patterning resolution (sometimes as small as a few nanometers). Electron beam lithography is also important commercially, primarily for its use in the manufacture of photomasks. Electron beam lithography as it is usually practiced is a form of maskless lithography, in that a mask is not required to generate the final pattern. Instead, the final pattern is created directly from a digital representation on a computer, by controlling an electron beam as it scans across a resist-coated substrate. Electron beam lithography has the disadvantage of being much slower than photolithography.\n\nIn addition to these commercially well-established techniques, a large number of promising microlithographic and nanolithographic technologies exist or are being developed, including nanoimprint lithography, interference lithography, X-ray lithography, extreme ultraviolet lithography, magnetolithography and scanning probe lithography. Some of these new techniques have been used successfully for small-scale commercial and important research applications.\nSurface-charge lithography, in fact Plasma desorption mass spectrometry can be directly patterned on polar dielectric crystals via pyroelectric effect, \nDiffraction lithography.\n\nDuring the first years of the 19th century, lithography had only a limited effect on printmaking, mainly because technical difficulties remained to be overcome. Germany was the main center of production in this period. Godefroy Engelmann, who moved his press from Mulhouse to Paris in 1816, largely succeeded in resolving the technical problems, and during the 1820s lithography was adopted by artists such as Delacroix and Géricault. After early experiments such as \"Specimens of Polyautography\" (1803), which had experimental works by a number of British artists including Benjamin West, Henry Fuseli, James Barry, Thomas Barker of Bath, Thomas Stothard, Henry Richard Greville, Richard Cooper, Henry Singleton, and William Henry Pyne, London also became a center, and some of Géricault's prints were in fact produced there. Goya in Bordeaux produced his last series of prints by lithography—\"The Bulls of Bordeaux\" of 1828. By the mid-century the initial enthusiasm had somewhat diminished in both countries, although the use of lithography was increasingly favored for commercial applications, which included the prints of Daumier, published in newspapers. Rodolphe Bresdin and Jean-François Millet also continued to practice the medium in France, and Adolf Menzel in Germany. In 1862 the publisher Cadart tried to initiate a portfolio of lithographs by various artists, which was not successful but included several prints by Manet. The revival began during the 1870s, especially in France with artists such as Odilon Redon, Henri Fantin-Latour and Degas producing much of their work in this manner. The need for strictly limited editions to maintain the price had now been realized, and the medium became more accepted.\nIn the 1890s, color lithography gained success in part by the emergence of Jules Chéret, known as the \"father of the modern poster\", whose work went on to inspire a new generation of poster designers and painters, most notably Toulouse-Lautrec, and former student of Chéret, Georges de Feure. By 1900 the medium in both color and monotone was an accepted part of printmaking.\n\nDuring the 20th century, a group of artists, including Braque, Calder, Chagall, Dufy, Léger, Matisse, Miró, and Picasso, rediscovered the largely undeveloped artform of lithography thanks to the Mourlot Studios, also known as \"Atelier Mourlot\", a Parisian printshop founded in 1852 by the Mourlot family. The Atelier Mourlot originally specialized in the printing of wallpaper; but it was transformed when the founder's grandson, Fernand Mourlot, invited a number of 20th-century artists to explore the complexities of fine art printing. Mourlot encouraged the painters to work directly on lithographic stones in order to create original artworks that could then be executed under the direction of master printers in small editions. The combination of modern artist and master printer resulted in lithographs that were used as posters to promote the artists' work.\n\nGrant Wood, George Bellows, Alphonse Mucha, Max Kahn, Pablo Picasso, Eleanor Coen, Jasper Johns, David Hockney, Susan Dorothea White and Robert Rauschenberg are a few of the artists who have produced most of their prints in the medium. M. C. Escher is considered a master of lithography, and many of his prints were created using this process. More than other printmaking techniques, printmakers in lithography still largely depend on access to good printers, and the development of the medium has been greatly influenced by when and where these have been established.\n\nAs a special form of lithography, the serilith process is sometimes used. Seriliths are mixed media original prints created in a process in which an artist uses the lithograph and serigraph processes. The separations for both processes are hand-drawn by the artist. The serilith technique is used primarily to create fine art limited print editions.\n\n\n"}
{"id": "18395307", "url": "https://en.wikipedia.org/wiki?curid=18395307", "title": "MS Polarfront", "text": "MS Polarfront\n\nMS \"Polarfront\" was a Norwegian weather ship located in the North Atlantic. She was the last remaining weather ship in the world, maintained by the Norwegian Meteorological Institute.\n\nA weather ship is a ship stationed in mid-ocean to make meteorological observations for weather forecasting. Since the 1960s this role has been largely superseded by satellites, long range aircraft and weather buoys.\n\nMS \"Polarfront\" was known as the weather station M (\"Mike\"), and was located at 66°N, 02°E. Standard meteorological observations were performed on an hourly basis since the beginning of the 1960s.\n\nOn 27 February 2009, the cancellation of the station was announced. MS \"Polarfront\" was removed from service on 1 January 2010.\n\nThe International Civil Aviation Organization (ICAO) took the responsibility to operate an international network of Ocean Weather Stations in The North-Atlantic. The network was established in 1948 and consisted of 13 stations. Station ‘M’ (Mike) was one of these. The need for weather ships from civil aviation decreased gradually while the Meteorological society still needed the observations from the oceans. In 1974 the World Meteorological Organization (WMO) took the responsibility for the four remaining stations. The international agreement about weather ships was ended in 1990. Great Britain and Norway continued the operation of one station each, Lima west of Scotland and Mike in the Norwegian Sea. Lima was ended in the middle of the 1990s. Thus Mike was the only one still remaining.\n\nThe first two weather ships to man station “M” were \"Polarfront I\" and \"Polarfront II\". The Norwegian authorities were the ship owners. The ships were rebuilt Royal Navy corvettes ( and ). They were on duty until 1974 and 1976. In 1974 the Norwegian state made an agreement with the shipping company Misje Offshore Marine AS in Bergen to hire a new and modern ship, which was given the name \"Polarfront\".\n\nFor several years the ship alternated with the Dutch weather ship \"Cumulus\" to man station ‘M’. From 1986 and onward \"Polarfront\" manned station ‘M’ alone. Each month \"Polarfront\" left the station for 1–2 days to take on a new crew and new supplies. Once a year, usually in the beginning of October, the ship stayed in her home port for a week to carry out maintenance.\n\nSince the 28 June 2017, Polarfront is owned and operated by the French shipping company Latitude Blanche for expedition purposes in high latitudes.\n\n"}
{"id": "49641711", "url": "https://en.wikipedia.org/wiki?curid=49641711", "title": "Mesaite", "text": "Mesaite\n\nMesaite is a very rare mineral with formula CaMn(VO)•12HO. It is monoclinic (space group \"P\"2/\"n\"). It is related to fianelite, another manganese-rich divanadate. Examples of other divanadate minerals are volborthite, engelhauptite, karpenkoite, and martyite. \n"}
{"id": "12632957", "url": "https://en.wikipedia.org/wiki?curid=12632957", "title": "Michael E. Phelps", "text": "Michael E. Phelps\n\nMichael Epic Phelps (born August 24, 1939) is a professor and an American biophysicist. He is known for being one of the fathers of positron emission tomography (PET).\n\nPhelps was born in 1939 in Cleveland, Ohio. He spent his early life as a boxer. However, at age 19, he was severely injured in a car crash, leaving him in a coma for several days and effectively ending his boxing career. Phelps went on to earn his B.S. in Chemistry and Mathematics from Western Washington University in 1965, and his Ph.D. in Chemistry from Washington University in St. Louis, in 1970. He joined the faculty of Washington University School of Medicine in 1970. From 1975-1976, Phelps was a member of the faculty at the University of Pennsylvania. In 1976, he moved to the David Geffen School of Medicine at UCLA where he is the Norton Simon Professor & Chairman of the Department of Molecular & Medical Pharmacology and Director of two institutes, the Institute for Molecular Medicine and the Crump Institute for Molecular Imaging. He has been awarded some of science's highest honors: the Massry Prize from the Keck School of Medicine, University of Southern California in 2007; an Enrico Fermi Award and an appointment to the National Academy of Science. \n\nMichael Phelps currently resides in Los Angeles with wife, Dr. Patricia Phelps, who is a professor of Physiological Sciences at UCLA. They have two children: Patrick Phelps and Katy Phelps.\n\nhttp://www.ibp.ucla.edu/faculty.php\n"}
{"id": "2042349", "url": "https://en.wikipedia.org/wiki?curid=2042349", "title": "Outline of Microsoft", "text": "Outline of Microsoft\n\nMicrosoft Corporation is a multinational corporation based in Redmond, Washington, USA and founded by Bill Gates and Paul Allen that develops, manufactures, licenses, and supports a wide range of products and services predominantly related to computing. Due to the scope and size of the company, it encompasses a broad range of topics mostly revolving around critical analysis and the company's products and services.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4493", "url": "https://en.wikipedia.org/wiki?curid=4493", "title": "Outline of biology", "text": "Outline of biology\n\nBiology – The natural science that involves the study of life and living organisms, including their structure, function, growth, origin, evolution, distribution, and taxonomy.\n\nBranch of biology – subdiscipline of biology, also referred to as a biological science. Note that biology and all of its branches are also life sciences.\n\n\n\nOutline of ecology\n\nOutline of evolution\n\n\nOutline of cell biology\n\nOutline of biochemistry\n\nOutline of genetics\n\n\n\n\n"}
{"id": "33968044", "url": "https://en.wikipedia.org/wiki?curid=33968044", "title": "PRODIGAL (computer system)", "text": "PRODIGAL (computer system)\n\nPRODIGAL (Proactive discovery of insider threats using graph analysis and learning) is a computer system for predicting anomalous behavior among humans, by data mining network traffic such as emails, text messages and server log entries. It is part of DARPA's Anomaly Detection at Multiple Scales (ADAMS) project. The initial schedule is for two years and the budget $9 million.\n\nIt uses graph theory, machine learning, statistical anomaly detection, and high-performance computing to scan larger sets of data more quickly than in past systems. The amount of data analyzed is in the range of terabytes per day. The targets of the analysis are employees within the government or defense contracting organizations; specific examples of behavior the system is intended to detect include the actions of Nidal Malik Hasan and WikiLeaks source Chelsea Manning. Commercial applications may include finance. The results of the analysis, the five most serious threats per day, go to agents, analysts, and operators working in counterintelligence.\n\n\n"}
{"id": "6217255", "url": "https://en.wikipedia.org/wiki?curid=6217255", "title": "Pelvic fracture", "text": "Pelvic fracture\n\nA pelvic fracture is a break of the bony structure of the pelvis. This includes any break of the sacrum, hip bones (ischium, pubis, ilium), or tailbone. Symptoms include pain, particularly with movement. Complications may include internal bleeding, injury to the bladder, or vaginal trauma.\nCommon causes include falls, motor vehicle collisions, a pedestrian being hit by a vehicle, or a direct crush injury. In younger people significant trauma is typically required while in older people less significant trauma can result in a fracture. They are divided into two types: stable and unstable. Unstable fractures are further divided into anterior posterior compression, lateral compression, vertical shear, and combined mechanism fractures. Diagnosis is suspected based on symptoms and examination with confirmation by X-rays or CT scan. If a person is fully awake and has no pain of the pelvis medical imaging is not needed.\nEmergency treatment generally follows advanced trauma life support. This begins with efforts to stop bleeding and replace fluids. Bleeding control may be achieved by using a pelvic binder or bed-sheet to support the pelvis. Other efforts may include angiographic embolization or preperitoneal packing. After stabilization, the pelvis may require surgical reconstruction.\nPelvic fractures make up around 3% of adult fractures. Stable fractures generally have a good outcome. The risk of death with an unstable fracture is about 15%, while those who also have low blood pressure have a risk of death approaching 50%. Unstable fractures are often associated with injuries to other parts of the body.\nSymptoms include pain, particularly with movement.\nComplications are likely to result in cases of excess blood loss or punctures to certain organs, possibly leading to shock. Swelling and bruising may result, more so in high-impact injuries. Pain in the affected areas may differ where severity of impact increases its likelihood and may radiate if symptoms are aggravated when one moves around.\n\nCommon causes include falls, motor vehicle collisions, a pedestrian being hit by a vehicle, or a direct crush injury. In younger people significant trauma is typically required while in older people less significant trauma can result in a fracture.\n\nThe bony pelvis consists of the ilium (i.e., iliac wings), ischium, and pubis, which form an anatomic ring with the sacrum. Disruption of this ring requires significant energy. When it comes to the stability and the structure of the pelvis, or pelvic girdle, understanding its function as support for the trunk and legs helps to recognize the effect a pelvic fracture has on someone. The pubic bone, the ischium and the ilium make up the pelvic girdle, fused together as one unit. They attach to both sides of the spine and circle around to create a ring and sockets to place hip joints. Attachment to the spine is important to direct force into the trunk from the legs as movement occurs, extending to one’s back. This requires the pelvis to be strong enough to withstand pressure and energy. Various muscles play important roles in pelvic stability. Because of the forces involved, pelvic fractures frequently involve injury to organs contained within the bony pelvis. In addition, trauma to extra-pelvic organs is common. Pelvic fractures are often associated with severe hemorrhage due to the extensive blood supply to the region. The veins of the presacral pelvic plexus are particularly vulnerable. Greater than 85 percent of bleeding due to pelvic fractures is venous or from the open surfaces of the bone.\n\nPelvic fractures are most commonly described using one of two classification systems. The different forces on the pelvis result in different fractures. Sometimes they are determined based on stability or instability.\n\nThe Tile classification system is based on the integrity of the posterior sacroiliac complex.\n\nIn type A injuries, the sacroiliac complex is intact. The pelvic ring has a stable fracture that can be managed nonoperatively.\nType B injuries are caused by either external or internal rotational forces resulting in partial disruption of the posterior sacroiliac complex. These are often unstable.\nType C injuries are characterized by complete disruption of the posterior sacroiliac complex and are both rotationally and vertically unstable. These injuries are the result of great force, usually from a motor vehicle crash, fall from a height, or severe compression.\n\nThe Young-Burgess classification system is based on mechanism of injury: anteroposterior compression type I, II and III, lateral\ncompression types I, II and III, and vertical shear, or a combination of forces.\n\nLateral compression (LC) fractures involve transverse fractures of the pubic rami, either ipsilateral or contralateral to a posterior injury.\n\nThe most common force type, lateral compression (LC) forces, from side-impact automobile accidents and pedestrian injuries, can result in an internal rotation. The superior and inferior pubic rami may fracture anteriorly, for example. Injuries from shear forces, like falls from above, can result in disruption of ligaments or bones. When multiple forces occur, it is called combined mechanical injury (CMI).\n\nOne specific kind of pelvic fracture is known as an 'open book' fracture. This is often the result from a heavy impact to the groin (pubis), a common motorcycling accident injury. In this kind of injury, the left and right halves of the pelvis are separated at front and rear, the front opening more than the rear, i.e. like an open book that falls to the ground and splits in the middle. Depending on the severity, this may require surgical reconstruction before rehabilitation. Forces from an anterior or posterior direction, like head-on car accidents, usually cause external rotation of the hemipelvis, an “open-book” injury. Open fractures have increased risk of infection and hemorrhaging from vessel injury, leading to higher mortality.\n\nIf a person is fully awake and has no pain of the pelvis medical imaging of the pelvis is not needed.\n\nAs the human body ages, the bones become more weak and brittle and are therefore more susceptible to fractures. Certain precautions are crucial in order to lower the risk of getting pelvic fractures. The most damaging is one from a car accident, cycling accident, or falling from a high building which can result in a high energy injury. This can be very dangerous because the pelvis supports many internal organs and can damage these organs. Falling is one of the most common causes of a pelvic fracture. Therefore, proper precautions should be taken to prevent this from happening.\n\nA pelvic fracture is often complicated and treatment can be a long and painful process. Depending on the severity, pelvic fractures can be treated with or without surgery.\n\nA high index of suspicion should be held for pelvic injuries in any one with major trauma. The pelvis should be stabilized with a pelvic binder. This can be a purpose made device, but improvised pelvic binders have also been used around the world to good effect. Stabilisation of the pelvic ring reduces blood loss from the pelvic vessels and reduced the risk of death.\n\nSurgery is often required for pelvic fractures. Many methods of pelvic stabilization are used including external fixation or internal fixation and traction. There are often other injuries associated with a pelvic fracture so the type of surgery involved must be thoroughly planned.\n\nPelvic fractures that are treatable without surgery are treated with bed rest. Once the fracture has healed enough, rehabilitation can be started with first standing upright with the help of a physical therapist, followed by starting to walk using a walker and eventually progressing to a cane.\n\nMortality rates in people with pelvic fractures are between 10 and 16 percent. However, death is typically due to associated trauma affecting other organs, such as the brain. Death rates due to complications directly related to pelvic fractures, such as bleeding, are relatively low.\n\nAbout 10 percent of people that seek treatment at a level 1 trauma center after a blunt force injury have a pelvic fracture. Motorcycle injuries are the most common cause of pelvic fractures, followed by injuries to pedestrians caused by motor vehicles, large falls (over 15 feet), and motor vehicle crashes.\n\n\n"}
{"id": "25153936", "url": "https://en.wikipedia.org/wiki?curid=25153936", "title": "Performance-based building design", "text": "Performance-based building design\n\nPerformance-Based Building Design is an approach to the design of any complexity of building, from single-detached homes up to and including high-rise apartments and office buildings. A building constructed in this way is required to meet certain measurable or predictable performance requirements, such as energy efficiency or seismic load, without a specific prescribed method by which to attain those requirements. This is in contrast to traditional prescribed building codes, which mandate specific construction practises, such as stud size and distance between studs in wooden frame construction. Such an approach provides the freedom to develop tools and methods to evaluate the entire life cycle of the building process, from the business dealings, to procurement, through construction and the evaluation of results.\n\nOne of the first implementations of performance-based building design requirements was in Hammurabi's Code (c. 1795 to 1750 BC), where is stated that “a house should not collapse and kill anybody”. This concept is also described in Vitruvius’s \"“De architectura libri decem”\" (“The Ten Books of Architecture”) in first century BC.In modern times, the first definition of performance-based building design was introduced in 1965 in France by Blachère with the Agrément system \n\nDespite this, the building process remained relatively conventional for the next 50 years, based solely on experience and codes, regulations prescribed by law which stifled innovations and change. The prescription approach is a technical procedure based on past experience which consists of comparing the proposed design with standardized codes, so no simulation or verification tools are needed for the design and building process.\n\nA new approach began to emerge during the second half of the 20th century, when many local building markets began to show that they needed greater flexibility in the procurement procedures to facilitate the exchange of building goods between countries and to improve the speed of procedures and innovations in the building process. This innovative approach to the procurement, design, contracting, management and maintenance of buildings was performance-based building (PBBD).\n\nMost recently the clearest definition of performance based building approach was explained in 1982 by the CIB W60 Commission in the report n.64, where Gibson stated that \"“first and foremost, the performance approach is [...] the practice of thinking and working in terms of ends rather than means.[ …] It is concerned with what a building or building product is required to do, and not with prescribing how it is to be constructed”.\"\n\nMany research establishments have studied the implementation of PBBD during the last fifty years. A majority of areas of building design remain open to innovation.\n\nDuring 1998-2001, the CIB Board and Programme Committee initiated the Proactive Programme on Performance-Based Building in order to practically implement technical developments of performance-based building. This programme was followed by the establishment of the Performance-Based Building (PeBBu), running from October 2001 to October 2005, thanks to funds from the European Commission (EC) Fifth Framework Programme.\n\nThe PeBBu Network had a broad and varied programme, a set of activities and produced many papers to aid in the implementation of such vision.\n\nPeBBu Thematic Network was managed by the CIB General Secretariat (International Council for Research and Innovation in Building Construction), particularly by the CIB Development Foundation (CIBdf). The PeBBu Network started working in 2001 and completed in 2005. In the PeBBu Network 73 organisations, included CIBdf (coordinating contractor), BBRI (Belgium), VTT (Finland), CSTB (France), EGM (Netherlands), TNO (Netherlands), BRE (UK), cooperated to this project bringing people together to share their work, their information and knowledge. The objectives of the Network was to stimulate and facilitate international dissemination and implementation of Performance Based Building in building and construction sector, maximising the contribution to this by the international Research and Development community.\nThe PeBBu Thematic Network result is described and explained in 26 final reports which included three reports with an overall PBB scope, a multitude of research reports from the PeBBu Domains, User Platforms and Regional Platforms, a Final Management report and four practice reports for providing practical support to the actual application of PBB concept in building and construction sector.\n\nA conceptual framework for implementing a PBB market was identified while reviewing various viewpoints during the compilation of the 2nd International State of the Art Report for the PeBBu Thematic Network (Becker and Foliente 2005).\nThe building facility is a multi-component system with a generally very long life cycle. The system’s design agenda as a whole, and the more specific design objectives of its parts, originate from relevant user requirements. These requirements evolve into a comprehensive set of Performance Requirements that should be established by a large number of stakeholders (the users, entrepreneur/owner, regulatory framework, design team, and manufacturers).\nThe main steps in a Performance Based Building Design process are\n\nIn a Performance-based approach, the focus of all decisions, is on the required performance-in-use and on the evaluations and testing of building asset.\nPerformance Based Building (PBB) is focused on performance required in use for the business processes and the needs of the users, and then on the evaluations and verification of building assets result. The Performance approach can be used whether the process is about an existing or new assets. It is applicable to the procurement of constructed assets and to any phase of the whole life cycle Building Process, such as strategic planning, asset management, briefing/programming, design and construction, operation and maintenance, management and use, renovations and alterations, codes, regulations and standards.\nIt includes many topics and criteria, which can be categorized as physical, functional, environmental, financial, economical, psychological, social, facilities, and other more. These criteria are related to singular project, according to the context\nand the situation.\n\nPerformance concept is based on two key characteristics :\n\nThe Performance concept requires two languages: the language of demand requirements and the language of the required performance which should have a capability to fulfill the demand. It is important to recognize that these languages are different.\nSzigeti and Davis (Performance Based Building: Conceptual Framework, 2005) explain that \"“the dialog between client and supplier can be described as two halves of a “hamburger bun”, with the statement of the requirement in functional or performance language (FC - functional concept) matched to a solution (SC - solution concept) in more technical language, and the matching, verification / validation that needs to occur in between”.\" \nIn a recent paper Ang, Groosman, and Scholten (2005) explain that the functional concept represents the set of unquantified objectives and scopes to be satisfied by the supply solutions, related to performance requirements. The solution concept represents technical realization that satisfies at least the required performance. Design decision is a development of a solution concept.\n\nBuilding performance evaluation is the process of systematically comparing and matching the performance in use of building assets with explicitly documented or implicitly criteria for their expected performance. In the PBB approach is essential matching and comparing demand and supply. It can be done by using a validation method, by measurement, calculation, or testing. Tools and methods are used to permit some form of measurement of testing of the requirements, and the relating measurement of the capability of assets to perform.\n\nThere are many types of in-depth specialized technical evaluations and audits. These validations generally require time, a major effort by the customer group, and a high level of funding. Normally, the most valuable methods and tools are comprehensive scans which are performance based and include metrics that can easily be measured without lab-type instruments.\nEvaluations and reviews, are integral part of asset and portfolio management, design, construction, commissioning.\nEvaluations can be used for different purposes, depending on the requirements being considered, for example they could be used in support of funding decisions, they could include a condition assessment to ensure that the level of degradation or the obsolescence is known, they could include an assessment of the utilization or an assessment of the capability of the product result to perform functional expected requirements. Such evaluations can be used at any time during the life cycle of the asset.\nPBB evaluations should be done in a routine manner, really the evaluations are often done only as part of Commissioning or shortly thereafter, or when there is a problem.\n\nThere are two different kinds of performance verifications. Performance evaluations rate\nthe physical asset according to a set of existing criteria and indicators of capability, and match the results against the required levels of performance. The Occupant Satisfaction Surveys record the perceptions of the users, usually through a scale of satisfaction measurements. Both types of evaluations complement each other.\nInnovative decision-support methodologies are taking place in building sector. There are some tools explicitly based on the demand and supply concepts and other ones which employ standardized performance metrics that for the first time link facility\ncondition to the functional requirements of organizations and their customers. Projects can be planned, prioritize, and budgeted using a multi-criteria approach, that is transparent, comprehensive and auditable.\nOne of the methodologies that can be used is a gap analysis based on calibrated scales that measure both the levels of requirements and the capability of the asset that is either already used, or being designed, or offer to be bought, or leased. Such methodology is an ASTM and American National (ANSI) standard and is currently being considered as an ISO standard.\nIt is particularly useful when the information about the “gap”, if any, can be presented in support of funding decisions and actions.\n\nThere are a large number of verification methodologies, (e.g. POEs, CRE-FM), and all of these need to refer back to explicit statements of requirements to be able to compare with expected performance.\nTo evaluate the result of a building asset against the expected performance requirements it is necessary to fix some tools used during the process. These tools are the reference of whole life cycle building process, so organizations use ‘key performance indicators (KPI)’ to prove that they are meeting the targets that have been set by senior management. At the same time performance measurement (PM) becomes central to managing organizations, their operations and logistic support.\nThese methodologies include the feedback loop that links a facility in use to the requirements and capabilities that are compared and matched whenever decisions are needed.\n\nA prescriptive approach describes the way a building asset must be constructed, rather than the end result of the building process, and is related to the type and quality of materials used, the method of construction, and the workmanship, etc.\nThis type of approach is strictly mandated by a combination of law, codes, standards, and regulations, etc., and is based on past experience and consolidated know-how. The content of prescriptive codes and standards is usually a consequence of an accident causing injury or death which requires a remedy to avoid a repeat, as a consequence of some hazardous situation, or as a consequence of some recognized social need. In many countries, in both the public and private sector, research is taking place into a different set of codes, methods and tools based on performance criteria to complement the traditional prescriptive codes.\nIn the 1970s, this search produced the “Nordic Model” (NKB 1978), which constituted the reference model of next performance-based codes. This model links easily to one of the key characteristics of the Performance approach, the dialog between the WHY, the WHAT and the HOW.\n\nUsing a Performance Based approach does not preclude the use of prescriptive specifications. Although the benefits of the adopting of a PBBD approach are significant, it is recognized that employing a performance-based approach at any stage in the building process is more complex and expensive than using the simpler prescriptive route. So, the application of this approach should not be regarded as an end in itself. When simple building are concerned or well proven technologies are used, the use of prescriptive codes results more effective, efficient, faster, or less costly, so prescriptive specifications will continue to be useful in many situations.\n\nAt the same time for the complex projects use of the performance based route at every stage is indispensable, in particular during design and evaluation phases.\n\nIt is not likely that a facility will be planned, procured, delivered, maintained, used and renovated using solely Performance Based documents at each step of the way, down the supply chain, to the procurement of products and materials, because there is not yet enough experience with the Performance Based Building approach. At the same time the prescriptive approach can bring to stifle changes and innovations, so best way to set building process is blending both different approaches.\n\nThe Statements of Requirements represents a reference for the whole life cycle management of facilities, they are the core of the conceptual framework came up from the PeBBu Thematic Network. They constitute the key to implementation of the PBB in the construction sector.\n\nThe SoRs is a document prepared by clients, or in the verbal statements communicated to supplies, it is based on the user functional needs. These user requirements are converted into performance requirements, which can be explicit or implicit. Such document should include information about what is essential to the client. SoRs will take different forms depending on the kind of client and what is being procured, at what phase of the Life Cycle or where in the supply chain a document is being used.\nThe SoRs should be, dynamic, not static, and should include more and more details as projects proceed. This document should be prepared at different levels of granularity, how detailed the documentation is at each stage depends on the complexity of the project and on the procurement route chosen for the project.\n\nThe SoRs represent a very important part of a continuous process of communication between clients (demand) and their project team (supply), they will be updated and managed using computerized tools and will contain all requirements throughout the life of the facility. This process is called “briefing” in UK and Commonwealth English, and “programming” in American English. An SoR is normally prepared for any project, whether it is a PBB project or not. Assembling such a document usually leads to a more appropriate match between the needs of clients and users and the constructed assets.\nStatements of Requirements have to be very carefully stated so that it is easy to verify that a proposed solution can meet those requirements.\n\nHigh level statement of requirements need to be paired with indicators of capability so design solutions can be evaluated before they are built in order to avoid mistakes. In the SoRs it is important to take into account some design aspect like flexibility indicators because constructed assets need for change during their life cycle, uses and activities can change very rapidly, so it is essential to test different solutions way that the spaces might be used according to anticipate changes.\nSoRs, as understood in ISO 9000, include not only what the client requires and is prepared to pay for, but also the process and indicators that will provide the means to verify, and validate, that the product or service delivered meets those stated requirements.\n\nAs part of the worldwide movement to implement a PBB approach and to develop tools that will make it easier to shift to PBB, the International Alliance for Interoperability (IAI) set up projects to map the processes that are part of Whole Life Cycle Management as \"Portfolio and Asset Management: Performance (PAMPeR) and Early Design” (ED).\"\nThe IAI efforts are complemented by many other efforts to create standards for the information to be captured and analyzed to verify performance-in-use.\n\nPerformance requirements translate user requirements in more precise quantitative measurable and technical terms, usually for a specific purpose.\n\nSupply team prepares a document that includes, objectives and goals, performance requirements and criteria. It is important to include “indicators of performance” in the way that it can be measured the results against explicit requirements, whether qualitative or quantitative. Performance indicators need to be easily understood by the users and the evaluators. To validate the indicators and verify that required performance-in-use has been achieved it is necessary using appropriate methods and tools.\nLevels of performance requirements can be stated as part of the preparation of SoRs, as part of project programs, or as part of requests for proposals and procurement contracts.\nIt is preferable adopting a flexible approach to the expression and comparison of performance levels, so required and achieved performance can be expressed not as single values but as bands between upper and lower limits. In consequence, in performance terms the criteria can be expressed as graduated scales, divided into broad bands.\n\nIn the building and construction industry, until 25–30 years old, prescriptive codes, regulations and standards made innovation and change difficult and costly to implement, and created technical restrictions to trade. These concerns have been the major drivers towards the use of a Performance Based approach to codes, regulations and standards. Performance-based building regulations have been implemented or are being developed in many countries but they have not yet reached their full potential. In part, this can be attributed to the fact that the overall regulatory system has not yet been fully addressed, and gaps exist in several key areas.\nBringing the regulatory and non-regulatory models together is probably the best way to work. This is shown in the “Total Performance System Models” diagram (Meacham, et al. 2002), that maps the flow of decision making from society and business objectives to construction solutions.\n\nThe difference between the regulatory and non-regulatory parts of the Total Performance System Models is that the first one is mandated by codes and regulations based on the law, while those other functional requirements, included in Statements of Requirements, are an integral part of what the client requires and is willing to pay for.\nFor procurements in the public sector and for publicly traded corporations, it’s important that the decisions and choices are transparent and explicit, regardless of the specific procurement route. All procurement processes can be either Prescriptive or Performance Based. Design-Build, Public Private Partnerships (PPP), private finance initiative (PFI) and similar procurement procedures are particularly suited to the use of a strong Performance Based Building application. If the expected performance are not stated explicitly and verifiably then these procurement methods will likely be more subject to disappointments and legal problems.\nTo get the benefits from these procurement approaches, it is essential to organize the services of the supply chain in order to get innovative, less costly, or better solutions by shifting decisions about “how” to the integrated team.\n\nAlthough the effort of a large number of research establishments in order to implement and diffuse the PBBD approach, it is difficult to innovate the practice of working and thinking in building sector. The positive trend is that more and more interest in this approach is happening around the world. Many countries tried to reform the way that building and construction is regulated and managed.\n\nThe core of implementation of PBB approach is that the clients, the users really know and understand why and what they require and that they can state easily and clearly their SoRs document. It is also important that the client is active, involved, informed and able to understand and choose best way for its interest, whether he’s a private or a public client, a big or a little structure, whether the asset is for its own use or for use by others.\n\nProcurement will be the key to improve performance approach when clients learn to be more explicit in their demands and to participate more aggressively in the management and delivery process of their most significant investment and resource.\n\n\n\n\n\n\n"}
{"id": "19885183", "url": "https://en.wikipedia.org/wiki?curid=19885183", "title": "Petre Melikishvili", "text": "Petre Melikishvili\n\nPetre Melikishvili () (1850–1927) was a Georgian chemist. He was the co-founder of Tbilisi State University (TSU) and the first Rector of TSU. \nIn 1868 he finished Tbilisi First Gymnasium and in 1872 Novorossiya University, faculty of Physics and mathematics, department of Natural Sciences. In 1873 he is abroad were he acquainted \"L.Mayer's\" and \"Vilicenuce's\" laboratories. In 1876 he started working in the chemical laboratory of Novorossiya University. In 1881 he became Magister of the chemistry and in 1885 he became Doctor of the Chemistry. In 1884 he was a docent and from 1885 until 1917 he was Professor of Novorrossiya University.\n\nHis works in Organic Chemistry developed Stereochemical Theory. Melikishvili discovered one of the class of organic compounds Glycidacids (named by him). The second cycle of his works are about Inorganic Chemistry. Melikishvili with his pupil L. Pizarjevski synthesized superacids of some elements (\"U, Nb, Ta, W, Mo, B, Ti, V\") (1897–1913). they also showed that from many supposed formulas of Hydrogen Peroxide the correct is \"H-O-O-H\".\n\nMelikishvili has a big merit in creation of Georgian Chemical Terminology. He founded four chemical laboratories and departments of Inorganic, Organic and Agronomic Chemistry in TSU.\n\n"}
{"id": "51813885", "url": "https://en.wikipedia.org/wiki?curid=51813885", "title": "Primary rock", "text": "Primary rock\n\nPrimary rock is an early term in geology that refers to crystalline rock formed first in geologic time, containing no organic remains, such as granite, gneiss and schist as well as igneous and magmatic formations from all ages. Webster's Revised Unabridged Dictionary published in 1913 provides the following term as used in geology:\n\nNinety years later the McGraw-Hill Dictionary of Scientific & Technical Terms published in 2003 places the term in the geologic field of petrology:\n\nThe term dates from the late 18th century (see Giovanni Arduino and Abraham Gottlob Werner) when the first attempts to formulate a geologic time scale divided crustal rocks into four types: Primary, Secondary, Tertiary, and Quaternary. Darwin used the phrase \"primary rocks\" in 1838 in the Geological Introduction to Zoology of The Voyage of HMS Beagle Fossil Mammalia Described by Richard Owen The last two terms have survived on most geological time scales used in the 20th and 21st centuries. For an example of the extensive use and explication of this term as debated in the mid-18th century, see \"On the Origin of Eruptive and Primary Rocks\" by Thomas Macfarlane and published in three parts in The Canadian Naturalist and Geologist Journal of 1863. And for an example in Australian geology literature, where it is capitalized to stress the unique use of the word, see \"Report on Country in the Neighborhood of Lake Eyre\" by H.Y.L. Brown, Government Geologist, published in 1892.\nPrimary rock is also referred to as primitive rock, plutonic rock, and the crystalline basement rock of the Earth’s continental cratons. It is also loosely, and less precisely, referred to as bedrock, especially in civil engineering, geophysical surveys and drilling science.\n\nThe Austrian-American astrophysicist Thomas Gold utilized this term in his book \"The Deep Hot Biosphere\" in the chapter titled \"The Siljan Experiment\" regarding the deep drilling project in Sweden to prove the theory of abiotic/abiogenic oil and gas: \"...the ground of Sweden, composed almost entirely of primary rock and not of sediments...\"\n\nPrimary rocks are the source of primary minerals and primary water.\n\n"}
{"id": "19277655", "url": "https://en.wikipedia.org/wiki?curid=19277655", "title": "Samuel C. Lind", "text": "Samuel C. Lind\n\nSamuel Colville Lind (June 15, 1879, McMinnville, Tennessee – February 12, 1965) was a radiation chemist, referred to as \"the father of modern radiation chemistry\". He was elected a member of the United States National Academy of Sciences in 1930. He served as president of the American Electrochemical Society in 1927 and the American Chemical Society in 1940. Among his awards was the Ira Remsen Award in 1947, and the Priestley Medal in 1952.\n"}
{"id": "266885", "url": "https://en.wikipedia.org/wiki?curid=266885", "title": "Schmidt corrector plate", "text": "Schmidt corrector plate\n\nA Schmidt corrector plate is an aspheric lens which is designed to correct the spherical aberration in the spherical primary mirror it is combined with. It was invented by Bernhard Schmidt in 1931, although it may have been independently invented by Finnish astronomer Yrjö Väisälä in 1924 (sometimes called the Schmidt-Väisälä camera). Schmidt originally designed it as part of a wide-field photographic catadioptric telescope, the Schmidt camera, and is also used in other telescope designs, camera lenses and image projection systems.\n\nSchmidt corrector plates work because they are aspheric lenses with spherical aberration that is equal to but opposite of the spherical primary mirrors they are placed in front of. They are placed at the center of curvature \"C\" of the mirrors for a pure Schmidt camera and just behind the prime focus for a Schmidt-Cassegrain. The Schmidt corrector is thicker in the middle and the edge. This corrects the light paths so light reflected from the outer part of the mirror and light reflected from the inner portion of the mirror is brought to the same common focus \"F\". The Schmidt corrector only corrects for spherical aberration. It does not change the focal length of the system.\n\nSchmidt corrector plates can be manufactured in many ways. The most basic method, called the \"classical approach\", involves directly figuring the corrector by grinding and polishing the aspherical shape into a flat glass blank using specially shaped and sized tools. This method requires a high degree of skill and training on the part of the optical engineer creating the corrector.\n\nSchmidt himself worked out a second, more elegant, scheme for producing the complex figure needed for the correcting plate. A thin glass disk with a perfectly polished accurate flat form was placed on a heavy metal pan. The upper edge of the pan was ground at a precise angle or bevel based on the coefficient of elasticity of the particular type of glass plate that was being used. The glass plate was sealed to the ground edge of the pan, then a vacuum pump was used to exhaust the air until a particular negative pressure had been achieved. This caused the glass plate to warp slightly. The exposed side was then ground and polished to a perfect flat. When the vacuum was released, the plate sprang back until its bottom surface was again plane, while the upper surface had the correct figure. Schmidt's vacuum figuring method is rarely used today. The glass plate will usually break if bent enough to generate a curve for telescopes of focal ratio f/2.5 or faster. Also, for fast focal ratios, the curve obtained is not sufficiently exact and requires additional hand correction.\n\nA third method, invented in 1970 for Celestron by Tom Johnson and John O'rourke, uses a vacuum pan with the correct shape of the curve pre-shaped into the bottom of the pan, called a \"master block\". This removes the need to have to hold a shape by applying an exact vacuum and allows for the mass production of corrector plates of the same exact shape.\n\nThe technical difficulties associated with the production of Schmidt corrector plates led some designers, such as Dmitri Dmitrievich Maksutov and Albert Bouwers, to come up with alternative designs using more conventional Meniscus corrector lenses.\n\n"}
{"id": "5597919", "url": "https://en.wikipedia.org/wiki?curid=5597919", "title": "Science for the People", "text": "Science for the People\n\nScience for the People (SftP) is a left-wing socialist organization that emerged from the antiwar culture of the United States in the late 1960s. Since 2014 it has experienced a revival focusing primarily on the dual nature of science. The organization advocates for a scientific establishment that is not isolated from society, rather one that uses scientific discoveries to advocate for and advance social justice and critically approach science as a social endeavor.\n\nThe original group was composed of professors, students, workers, and other concerned citizens who sought to end potential oppression brought on by pseudoscience, or by what it considered the misuse of science. SftP generated much controversy in the 1970s for the radical tactics of some of its members. Over the initial few years there was an emergence of multiple differing opinions about the nature and mission of SftP should be. A faction wanted SftP to pay special attention to scientific issues that support class struggle. Another wanted to develop \"a science for the people.\" The majority, however, wanted to be the scientific community's critical conscience and expose, from within, the dangers of the misuse of science. After a bitter internal struggle and departure of many, the group that remained focused its efforts, primarily through its magazine, on criticism of scientific misuse. During this time it became identified with prominent academic scientists such as Stephen Jay Gould and Richard Lewontin.\n\nIn the first five years SftP became known in the US scientific community for its attempts at disrupting the American Association for the Advancement of Science (AAAS). SftP members considered the AAAS, the world’s largest association of scientists, aligned with the government and the ruling elite. SftP particularly decried what it considered AAAS' complicity in war, sexism, racism and capitalism. A specific focus of the activists was the scientific community's involvement in the Vietnam war. Some of the tactics use to disrupt the AAAS meetings were picketing, demonstrations, impromptu speeches and confrontational interruptions. These actions led to the arrest of several SftP activists in the early 1970s.\n\nPrior to the formation of SftP and its radical activism against the scientific establishment similar attempts had taken place with other organizations. One notable example is University of California, Berkeley nuclear physicist Charles Schwartz's 1967 attempt to amend the American Physical Society's (APS) constitution to allow 1% of members to call for a vote on any social or scientific issue. His motion was defeated because APS members did not think the society should take a stance on social issues. Another instance is the petition physicists began to the APS not to hold its 1970 meeting Chicago because of the police brutality at the Democratic National Convention in 1968. The APS Council polled members and upheld its decision to keep the meeting in Chicago.\n\nIn 1971 a proposed amendment to change the APS's mission statement to include the phrase \"The Society...shall shun those activities which are judged to contribute harmfully to the welfare of mankind.\" was defeated.\n\nIn following years, thanks to the actions of dedicated activists such as Schwartz and Martin Perl and others, APS took certain steps towards social responsibility. These included the 1972 creation of the Committee on the Status of Women in Physics the 1979 boycott of states that had not ratified the Equal Rights Amendment (ERA) and the 1983 Arms Control Resolution. The latter was strongly criticized by George Keyworth, science advisor to president Ronald Reagan.\n\nFrom its inception in January 1969 SftP opposed the involvement of scientists in the military. SftP also challenged the established notion that organizations such as the American Association for the Advancement of Science (AAAS) and the American Physical Society (APS) can stay neutral vis a vis the Vietnam war. Early on, a number of SftP scientists mobilized against US Congress' Anti-Ballistic Missile (ABM) Program, arguing that the ABM was not feasible and the funds would be better spent on basic scientific research. On March 4, 1969 MIT scientists staged a mass walkout in protest of the ABM.\n\nIn April 1969, Scientists and Engineers for Social and Political Action (SESPA), SftP's predecessor, held an orderly march of 250 physicists to the White House to protest the ABM.\n\nThis type of activism among scientists in the US led to the anti-ABM treaty of 1972 with the Soviet Union. In the 1980s SftP opposed president Reagan's attempt to revive the arms race with the Soviet Union as well as the US involvement in Nicaragua.\n\nIn the mid-70s SftP cautioned against the ways that nuclear power was being promoted as a safe and environmentally clean alternative to coal. In May of 1976 the organization published a pamphlet arguing that the push for nuclear energy in the US over solar and other cleaner, cheaper alternatives benefitted the Atomic-Industrial complex and not the general public. In the 1980s, especially in the wake of such disasters as Three Mile Island, SftP questioned the environmental safety of nuclear energy and the toxic waste it produces.\n\nThroughout the 1970s and 1980s SftP considered technology an important outcome of scientific advancement. The organization favored the more concrete nature of technological developments over purely intellectual exercise of theoretical science. Key to the group’s support for technology was the conviction that it should neither replace humans in the workplace nor harm the environment. SftP members advocated for research and development programs to be chosen based on equity and social need and not to meet the government's needs of economic and military security.\n\nOne of the core tenants of the SftP was that science and particularly biology and medicine cannot remain neutral. The organization not only believed that these disciplines should focus on correcting societal ills they also actively participated in educating people on work place hazards such as asbestos and other chemical and environmental exposures.\n\nIn the early 1970s a Boston several SftP members known as the Boston Science Teaching Group, published and distributed series of pamphlets on topics such as genetics and ecology. Other members who were professional educators volunteered to teach biology in Boston’s underserved school districts. In 1971 two university professors, Rita Arditti and Tom Strunk, in an attempt to reform college biology curriculum, created a socially conscious first year college course called “Objecting to Objectivity: A Course in Biology”. The course covered genetic engineering, physical and social limitations and implications of human gene maps, polygenic inheritance and prenatal diagnosis. It also discussed reproduction, birth control and abortion including the contemporary research and public policies about reproductive health. Other topics included population growth and Malthusian and Marxist theories and ethics of human research.\n\nAdvocating for racial and gender equality in science and medicine was one of the core tenets of SftP. The organization included multiple feminist members who were pioneering women in science. These included Arditti and other biologists such as, Anne Fausto-Sterling, Freda Friedman Salzman Ruth Hubbard, and author and activist Barbara Beckwith. Hubbard, for instance, was the first woman to attain tenure in biology at Harvard University. SftP also embraced the cause of gender equality in the society at large and advocated for reproductive rights, gender equality at the workplace and addressed issues surrounding sexuality. It also fought against domestic violence and traditional gender roles in family structure. While focusing on the world of science, feminist members of SftP faced an uphill battle in introducing gender parity for women in science at the universities. They also sought to change the discriminatory gender dynamics in academia and in laboratories.\n\nSftP's efforts at promoting gender equality were paralleled with its efforts to promote racial and ethnic equality. Although made up primarily of white Americans, some SftP members maintained relations with the Black Panther Party. The two organizations urged the scientific community to create a free science program for black communities to enhance their scientific knowledge. The organization also criticized attacks on affirmative action and featured pieces by black and other minority scientists in its publication. It also uncovered occupational health hazards among black and ethnic minority workers both in the US and abroad and fought to improve workplace conditions to eliminate these risks. SftP's antiracist ideology put it at odds with the concepts of sociobiology and genetic determinism.\n\nBiologists within SftP were highly critical of sociobiology, because of objectionable premises to the organization of the discipline and for the implications of using sociobiology to support racism, capitalism, and imperialism. E. O. Wilson, a biologist and entomology professor in the Department of Organismic and Evolutionary Biology at Harvard University, whose book \"\" had helped start the debate, wrote that \"the political objections forcefully made by the Sociobiology Study Group of Science for the People in particular took me by surprise.\"\n\nSftP condemned the 1969 arguments that genetic differences were the underlying reason for differences in educational achievements between blacks and whites. SftP also took issue with the Harvard XYY study in 1975. The goal of the XYY study was to assess the risk of criminality the extra Y chromosome supposedly conferred. The SftP scientists pointed out the ethical and methodological failures of the above study, including open ended consents, stigmatization of individuals with XYY, lack of controls and absence of double blinding.\n\nHealth care providers who were SftP members worked to strengthen healthcare infrastructure in underserved communities. They partnered with both the Black Panthers and Young Lords Organization to bring medical services to minorities, who often could not access the medical establishment both as practitioners and as patients. SftP joined with other New Left Health organizations such as Health Policy Advisory Center and Medical Committee for Human Rights, fought for a fair and just healthcare system and advocated for women’s reproductive rights.\n\nSftP members, such as cancer researcher John Valentine at Wayne State University, exposed the capitalist interests that drove biomedical research. He argued that the 1971 National Cancer Act, signed by president Richard Nixon, failed to fund research into cancer causes such as poor preventative healthcare, occupational hazards and environmental exposures. He also criticized the use of public funds only to develop new chemotherapeutic agents instead of using some of it to minimize cancer risk due to workplace exposures and cancer-causing consumer products.\n\nSftP biologists also opposed recombinant DNA (rDNA) research before its public health and environmental impact can be thoroughly elucidated. They also expressed concerns and, accurately, predicted that rDNA can commercialize biomedical research and make it a market commodity. They urged the scientific community and the general public to consider who decides what research gets done and who benefits from these decisions.\n\nSftP argued that the existing contemporary agricultural models were neither benefitting the consumer, as food prices were rising astronomically, nor the farmer because their increasing debt without a raise in income. The primary benefiters were input and output capital enterprises such has fertilizer companies, insecticide and herbicide manufacturers and farm machinery companies. Members of the SftP formed the New World Agriculture Group (NWAG) that attempted to discover and develop ecologically rational alternative agricultural methods. Methods that protected the environment and preserved long-term productive capacity. NWAG also proposed partnering with farm labor organizations to help bring an end to worker exploitation and the unequal wealth distribution.\n\nFrom its inception SftP condemned the use of technology and science to oppress and colonize other countries. The organization gave the examples of both Vietnam and Cuba where, it stated, the US technological and scientific superiority was being used to both militarily and economically repress the smaller nations. In response to the US policy, in 1971, a group of SftP members in Cambridge, Massachusetts collected and shipped large amounts of scientific books and journals to Vietnam and Cuba to aid in science education there. The same year, molecular biologist Dr. Mark Ptashne and zoologist Dr. Bert Pfeiffer went to Hanoi and lectured to Vietnamese scientists and physicians. There were also successful efforts of networking with scientists in China, and, in the 1980s, with the scientific and technological community in Nicaragua.\n\nSince the fall of 2014, an effort to revive and reorganize SftP has been underway. The SftP revitalization efforts emerged from the convention held April 11–13, 2014, at the University of Massachusetts Amherst. At the 2014 conference various topics including the history of the SftP, health care, climate change, social justice, science education, gender and racial bias and militarization of science were discussed. Since then, inspired by the original 1970s-1980s group, this new formation has dedicated itself to building a social movement around progressive and radical perspectives on science and society.\n\nSeveral local chapters of the SftP participated in the first annual March for Science on April 22, 2017. The revived SftP also published a statement titled \"Which Way for Science?\". The statement hailed the March for Science as \"an exciting first step,\" but it also criticized the \"apolitical\" nature of the event and for their lack of attention to the experiences of scientists from historically marginalized groups such as women, people of color and others. \"Which Way for Science\" called attention to science's historic ties to U.S. capitalism and militarism, and called for a radical shift in its practice.\n\nThe national convention, held at the University of Michigan's Ann Arbor campus in February 2018, brought together close to one hundred scientists and activists to formalize the group's bylaws and structure. During the three days the attendees discussed the history and future of SftP, heard from local chapters that included representatives from Atlanta, Mexico City, New York and seven other North American locations. The organizational structure of SftP was explored and these discussions served as a guide to developing an inclusive, radical and democratic political movement for scientists and STEM workers. There were a dozen presentations on variety of topics related to SftP's mission.\n\nIn addition to the call to organize more local chapters a number of working groups was also developed during the meeting. These included groups dealing with Climate Change, Reproductive Justice, Science education and others. Plans to participate in the second annual March for Science on April 14, 2018 were also initiated at the convention.\n\n\n\n\nFrom 1969 to 1989 the original SftP published a quarterly, then bimonthly, magazine, that has been digitized and available on the organization's website. On July 28, 2018, at Caveat in New York City the publication was relaunched online with a special issue dedicated to geo-engineering.. The event also featured the premier of a documentary on the organization.\n\n\n\n"}
{"id": "20747100", "url": "https://en.wikipedia.org/wiki?curid=20747100", "title": "Sine quadrant", "text": "Sine quadrant\n\nThe sine quadrant (Arabic: Rub‘ul mujayyab, الربع المجيب) was a type of quadrant used by medieval Arabic astronomers. It is also known as a \"sinecal quadrant\" in the English-speaking world. The instrument could be used to measure celestial angles, to tell time, to find directions, or to determine the apparent positions of any celestial object for any time. The name is derived from the Arabic \"rub‘‘‘\" meaning a quarter and \"mujayyab\" meaning marked with sine. It was described, according to King, by Muhammad ibn Mūsā al-Khwārizmī in 9th century Baghdad.\n\nThe instrument is a quarter of a circle made of wood or metal (usually brass) divided on its arc side into 90 equal parts or degrees. The 90 divisions are gathered in 18 groups of five degrees each and are generally numbered both ways from the ends of the arc. That is, one set of numbers begins at the left end of the arc and goes to 90 at the right end while the other set the zero is at the right and the 90 is at the left. This double numbering enables the instrument to measure either celestial altitude or zenith distance or both simultaneously. \n\nAt the apex where the two graduated straight sides of the grid pattern meet in a right angle there is a pin hole with a cord in it and a small weight on the free end with a small bead that slides along the cord. The cord is called “Khait” and is used as a plumb line when measuring celestial altitudes. It is also used as the indicator of angles when doing calculations with the instrument. The sliding bead facilitates trigonometric calculations with the instrument. \n\nTraditionally the line from the beginning of the arc to the apex is called “Jaibs” and the line from the end of the arc to the apex is called “Jaib tamams”. Both jaibs and jaib tamams are divided into 60 equal units and the sixty parallel lines to the jaibs are called sitheeniys or” sixtys “ and the sixty parallel lines to the jaib tamams are “juyoobul mabsootah”. \n\nThe reason for sixty divisions along the Jaibs and Jaib Tamams is that the instrument uses the Sexagesimal number system. That is it is graduated to the number base 60 and not to the base 10 or decimal system that we presently use. Time, angular measurement and geographical coordinate measurements are about the only hold overs from the Sumerian/Babylonian number system that are still in current use.\n\nLike the arc, the Jaibs and Jaib tamams have their sixty divisions gathered into groups of five that are numbered in both directions to and from the apex. The double numbering of the arc means that the “Jaibs” and “Jaib tamams” labels are relative to the measurement being taken or to the calculation being performed at the time and the terms are not attached to one or the other of the graduated scales on the instrument.\n\nOn one of the straight edges of the non maritime quadrant (solid sheet form) there are two sighting plates which are called “Hadafatani”. Each of the alignment plates having a small, centrally placed aperture or \"pinhole\", the two apertures (front and back) forming the optical axis through which one sights an incline object or Sun. The light rays from the Sun passing through both apertures, the spot image of the Sun being concentric with the center of the rear plate pinhole, imaging on to a finger (project screen) if desired but not necessary or less frequent the eye at night. The hanging plum line serving two functions the first being to provide a means (indicator) to reading the angular orientation of the instrument, the second function ensuring that the instrument when optically aligned with the object of interest is situated parallel with the vertical plane (perpendicular with the ground).\n\nIt has been stated by non astronomers and non navigators, that two people are required to use the small instrument successfully; one to take the sight and one to read the cord (plum line) position on the radius segment and incorrect presumption as the instrument is to be held flush (face on) and below eye level with a single user, the two holes used for projecting the image of the Sun not direct sighting. As one can see from the photograph, it is easy to optically aligned the instrument with the Sun using simple image projection method, the device held in a single hand. At the moment of alignment, one views the face of the instrument to read the angular position of the cord relative to the radii segment of the instrument. However, it does help to have another person write down the scale readings as they are taken if a single operator is not able to do such because of environmental conditions, the single operator not having the capability to sufficiently hold the device stable, retaining the optical alignment, with just one hand. \n\nMaking altitude (elevation) measurements of the Sun being simple and direct, requiring the user to aligning the image of the Sun through the front pinhole (aperture), centered onto the rear back plate which operates more like a mask and not a viewing pass hole for sighting the Sun with the naked eye. Much in the same manner as performing eyepiece projection with a telescope, one fashioned with a front and rear aperture (font lens and rear eyepiece) along with a projection screen behind the eyepiece, or more rudimentary, the projecting of the Sun’s image upon a small screen plate such as that done with a mariner’s back staff. The rear, or second aperture (pass hole) having the function of working as a blacken attenuator so that any reflecting, annulus shaped sunlight off the metal aperture is not too bright, the Sun’s image (light) passing through the second hole and similar in task though void of a fixed image plane, to that of an iris (diaphragm) in a camera lens to reduce the light intensity. For relative to human physiology, focusing on a bright spot of light such as a pin point image of the Sun for any extended period or repetitively over a short duration of time adversely effects momentarily, a person’s visual acuity, thus making it more challenging to focus one’s eyes to read the angular scale. It being typical to orientate the instrument such that the operator faces looking slightly down upon the scale, the Sun at the users left, with right hand placed in such a way that the rays of sunlight pass through the two, perforated sighting plates, forming a bright illuminated spot on the observer’s finger (see photo), the finger functioning as a projection screen. At the moment an optical alignment with the Sun is established, the angular value of the device is read by the operator at the point where the graduated scale is bisected by the hanging plumb line. \n\nThese instruments, with poor angular resolution, not principally intended to function with stars at night, as an astronomical measuring device, since it is impractical to sight a star through the front pin hole (aperture) less on a fixed, stabilized mount relative to the half degree width of the very intense Sun. The maritime (navigation) version of these devices being skeletal in design rather than solid sheet form, so as to limit buffeting or displacement of the instrument while in the operators hand from wind exposure.\n\nhttp://www.astrolabeproject.com/26/01/2014/deconstructing-the-sine-quadrant-part-1-introduction/"}
{"id": "2568333", "url": "https://en.wikipedia.org/wiki?curid=2568333", "title": "Sokol space suit", "text": "Sokol space suit\n\nThe Sokol space suit, also known as the Sokol IVA suit or simply the Sokol (, \"Falcon\"), is a type of Russian space suit, worn by all who fly on the Soyuz spacecraft. It was introduced in 1973 and is still used . The Sokol is described by its makers as a \"rescue suit\" and it is not capable of being used outside the spacecraft in a spacewalk or extra-vehicular activity. Instead, its purpose is to keep the wearer alive in the event of an accidental depressurisation of the spacecraft. In this respect, it is similar to the ACES suit that was worn aboard NASA's Space Shuttle during launch and landing.\n\nThe current version of the suit is the Sokol-KV2, manufactured by NPP Zvezda (НПП Звезда). It consists of an inner pressure layer of rubberised polycaprolactam and an outer layer of white nylon canvas. Boots are integrated with the suit but gloves are removable and attach by means of blue anodised aluminium wrist couplings. The polycarbonate visor can open on hinges mounted near the ears and seals with an anodised aluminium clavicle flange when closed; the hood or 'soft helmet' folds when the visor is raised. The suit has four pockets and adjustment straps on the arms, legs, chest, and abdomen.\n\nThere is a suit pressure gauge on the left wrist. A mirror on an elastic wrist band is worn on the right—this helps the wearer see things that would otherwise be outside his or her field of view. During re-entry, an altimeter on a wrist strap may also be worn this gives an immediate check on cabin pressure and warns when to brace for touchdown (during the last phase of landing the cabin opens to the outside air.) A wristwatch is often worn as well, with an elastic wrist band replacing the strap so it may fit over the bulky suit glove. The watches are often privately purchased and a wide variety of Swiss and Russian models have been used.\n\nElectrical cables are mounted on the right abdomen of the suit; on the left abdomen there are separate hoses for air and oxygen. Normally, an electric blower ventilates the suit with cabin air through the larger hose at the rate of per minute. If the cabin pressure drops below , the air supply is automatically replaced with oxygen from pressurised bottles. at a rate of per minute. Both air and oxygen exhaust through the blue pressure relief valve at the centre of the chest; this valve also regulates the pressure of the suit.\n\nEffectively, the suit uses an open-circuit life support system that somewhat resembles scuba equipment. This has the advantage of simplicity; the disadvantage of a high rate of oxygen consumption is considered acceptable given that it is only intended for emergency use.\n\nThe suits weigh around and are described by those who have used them as a considerable encumbrance when worn on the ground. Despite this, they are intended to be worn for up to 30 hours in a pressurised environment or two hours in a vacuum. They can also float and have a neck dam that allows the visor to be raised in water without the risk of flooding the suit. However, Soyuz crews are provided with buoyancy aids and cold-water survival suits which would preferably be used if the Soyuz accidentally landed in water.\n\n, a total of 309 flight suits had been made along with 135 training and testing suits.\n\nEach Soyuz crew member is provided with a made-to-measure suit for flight although, from the numbers made, it appears that the suits provided for ground training are re-used. It is considered vital that the flight suit fits correctly and the wearer will spend two hours sitting in a launch couch with the suit inflated to make sure of this. Straps on the arms, legs and chest allow the fit to be adjusted slightly. \n\nTo don the suit, the two zips that make a 'V' on the chest are opened. Underneath, there is a large tubular opening in the inner pressure layer known as the \"appendix\". Legs go in first, followed by the arms into the sleeves and head into the helmet. When the suit is on, an airtight seal is made by tightly rolling up the appendix and securing it with strong elastic bands. The large bulge of the rolled-up appendix is secured under the V shaped flap in the suit's outer layer. When worn on the ground, the suit is attached to a portable ventilation unit —a hand-held device that supplies air to the suit, cooling it first with an ice filled heat exchanger. Grey leather outer boots are also worn on the ground; they protect the feet of the suit from damage and are removed before entering the spacecraft to avoid carrying debris into the cabin. \n\nThe suit is worn during launch and re-entry of the Soyuz spacecraft—the gloves are attached and the visor is sealed at these times. In an emergency, the suit pressure is usually maintained at 400 hPa (0.39 atm, 5.8 psi) above the ambient by the pressure relief valve. However, the suits only have a rudimentary pressure relief layer so they tend to balloon when inflated. Movement of the wearer becomes restricted, although it is still possible to function inside the capsule.\n\nIf more than limited movement is required, the pressure relief valve may be adjusted to a lower setting of 270 hPa (0.26 atm, 3.9 psi). Pure oxygen at this pressure will support life, but the setting is only intended for use in extreme emergencies; the risk of decompression sickness becomes significant if the wearer spends more than 15 minutes at the lower pressure setting.\n\nThe maximum length of time the suits may be used in a vacuum is 125 minutes. The time is limited because the oxygen flow to the suit is enough for life support, but insufficient to carry away the cosmonaut's body heat and longer use of the suit risks Heat exhaustion. If the capsule becomes depressurised, either accidentally or deliberately to extinguish a fire, it must land within that time.\n\nPressure suits were worn on the Vostok space missions, but when the Soyuz spacecraft was being developed in the mid-1960s, the controversial decision was taken by its designers, OKB-1, not to use them on the new spacecraft. Some of the early Soyuz flights carried Yastreb space suits but these were only for space walks and were only worn in orbit.\n\nOn June 30, 1971, the crew of Soyuz 11 died when their spacecraft depressurised during re-entry. One of the recommendations of the investigating government commission was that pressure suits should be worn by future crews during critical phases of their mission - launch, docking and landing.\n\nNPP Zvezda was given the task of providing the suits. They rejected the use of existing Soviet space suits and chose to base a new suit on the existing Sokol aviation pressure suit. The main modification was the replacement of the Sokol suit's hard helmet. Other features of the aviation suit that were considered unnecessary were removed to save weight.\n\nAt the same time, a life support system was developed in co-operation with OKB-1. The new suit was named the Sokol-K, K (Kosmos) is the abbreviation of the Russian word for space.\n\nThe first version of the suit, it was used on Soyuz 12, launched on September 27, 1973.\n\nName: Sokol-K Rescue Spacesuit\n\nDerived from: Sokol aviation full pressure suit\n\nManufacturer: NPP Zvezda\n\nMissions: Soyuz 12 (1973) to Soyuz 40 (1981)\n\nFunction: Intra-vehicular activity (IVA)\n\nOperating Pressure: \n\nSuit Weight: \n\nA version intended for use with the TKS spacecraft which was to be used as part of the Almaz programme. The suit was never used as the TKS never flew with a crew. Its main difference was that it was designed to work with a regenerative life support system.\n\nWork on improving the Sokol-K began 1973, immediately after its introduction. The Sokol-KM and KV were intermediate models on which many of the features of the Sokol-KV2 were developed, neither was ever used in space.\n\nTo be donned, the Sokol-KM and KV split into upper and lower halves joined by zip fasteners. However, this feature was discarded in the Sokol-KV2 and the appendix was retained as a means of donning the suit it was thought to be more reliable than the airtight zippers the Russians were able to make. Other changes included alterations to the fabric around the joints, to improve mobility, and improvement of the gloves, to make it easier to operate the spacecraft controls. \n\nThe KM and KV also featured a liquid-cooled undergarment that would increase the comfort of the wearer by efficiently removing body-heat; other suits relied on the flow of air to do this.\n\nName: Sokol-KV Rescue Spacesuit\n\nManufacturer: NPP Zvezda\n\nMissions: None\n\nFunction: Intra-vehicular activity (IVA)\n\nOperating Pressure: \n\nSuit Weight: \n\nThe Sokol-KV2, the current version of the suit, was first used on the Soyuz T-2 mission, launched on June 5, 1980.\n\nThe main improvement was the replacement of the rubber pressure layer of the Sokol-K with rubberised polycaprolactam to save weight. The visor was modified and enlarged to give the wearer a better field of view. Laces in the outer canvas layer were replaced with zippers to make the suit quicker to don and the pressure relief valve was moved from the left abdomen to the centre of the chest so either hand could be used to alter the suit's pressure setting. The improved arms, legs, and gloves of the Sokol-KV were retained although the liquid cooled undergarment of the KM and KV was discarded.\n\nName: Sokol-KV-2 Rescue Spacesuit\n\nManufacturer: NPP Zvezda\n\nMissions: 1980 to present\n\nFunction: Intra-vehicular activity (IVA)\n\nOperating Pressure: \n\nSuit Weight: \n\nThe Chinese have bought a number of space suits from the Russians for use in their space programme. The suit worn by Yang Liwei on \"Shenzhou 5\", the first manned Chinese space flight, closely resembles a Sokol-KV2 suit, but it is believed to be a Chinese-made version rather than an actual Russian suit. Pictures show that the suits worn by Fei Junlong and Nie Haisheng on \"Shenzhou 6\" differ in detail from the earlier suit; they are also reported to be lighter.\n\nSokol suits have been bought for uses other than spaceflight. It was planned that the crew of the British QinetiQ 1 high-altitude balloon would wear modified Sokol suits purchased from Zvezda. As the balloonists would have occupied an open platform during their twelve-hour flight, the Sokol suits, together with heavily insulated outer garments, would have protected them from the cold and low pressure of the stratosphere as the balloon ascended to a height of around 40 km (25 miles, 132,000 feet).\n\nBulgaria developed its own version of the space suit in the mid-1970s.\n\nDuring the flight of Gemini 7, Frank Borman and Jim Lovell wore modified Gemini spacesuits that somewhat resemble the Sokol suits, but with significant differences. Their suits were known as the G5C by their manufacturer, the David Clark Company.\n\nSokol space suits, including ones flown in space, were first sold by Sotheby's at an auction devoted to Russian space history in 1993. Subsequently, components such as gloves, communications caps, and wrist mirrors have frequently come up for sale on eBay; even complete suits have occasionally come up for sale, such as the one that Heritage Auctions sold for US$31,070 in 2009. These are usually worn out items that have been discarded after use during ground training and were never intended for use in space. As these items are nominally the property of the Russian government, the legitimacy of their sale has been questioned.\n\n\n\n\n"}
{"id": "22434789", "url": "https://en.wikipedia.org/wiki?curid=22434789", "title": "Soyuz (rocket)", "text": "Soyuz (rocket)\n\nThe Soyuz (, meaning \"union\", GRAU index 11A511) was a Soviet expendable carrier rocket designed in the 1960s by OKB-1 and manufactured by State Aviation Plant No. 1 in Kuybyshev, Soviet Union. It was commissioned to launch Soyuz spacecraft as part of the Soviet human spaceflight program, first with 8 unmanned test flights, followed by the first 19 manned launches. The original Soyuz also propelled four test flights of the improved Soyuz 7K-T capsule between 1972 and 1974. In total it flew 30 successful missions over 10 years and suffered two failures.\n\nThe Soyuz 11A511 type, a member of the R-7 family of rockets. first flew in 1966. Derived from the Voskhod 11A57 type, It was a two-stage rocket, with four liquid-fuelled strap-on boosters clustered around the first stage, with a Block I second stage. The new, uprated core stage and strap-ons became standard for all R-7 derived launch vehicles to replace the numerous older variants in use on the 8A92, 11A57, and 8K78M types. While the original Blok I stage as developed in 1960 used RD-107 engines, the Soyuz boosters instead had RD-110s, which were more powerful due to the heavier weight of the Soyuz craft and also had several design improvements to increase reliability and safety on manned missions. The Molniya 8K78M booster also adopted the RD-110 during 1965, but Voskhod boosters continued using the older RD-107.\n\nStarting in 1973, the original Soyuz rocket was gradually superseded by the Soyuz-U derivative type, which became the world's most prolific launcher, flying hundreds of missions over 43 years until its retirement scheduled for 2016. Other direct variants were Soyuz-L for low Earth orbit tests of the LK lunar lander (3 flights) and Soyuz-M built for a quickly abandoned military spacecraft and used for reconnaissance satellites instead (8 flights).\n\nThe aborted Soyuz 18-1 launch in 1975 was the final manned flight of the 11A511 and as it occurred shortly before the ASTP mission, the United States requested that the Soviets provide details about this failure. They stated that Soyuz 19 would be using the newer 11A5511U booster model (i.e. Soyuz-U) so that the Soyuz 18-1 malfunction had no bearing on it.\n\nSoyuz rockets were assembled horizontally in the MIK Building at the launch site. The rocket was then rolled out, and erected on the launch pad.\n"}
{"id": "47860264", "url": "https://en.wikipedia.org/wiki?curid=47860264", "title": "SpARCS1049+56", "text": "SpARCS1049+56\n\nSpARCS1049+56 is a galaxy cluster whose heart is bursting with new stars and located at a distance of about 9.8 billion light-years away from Earth. It was discovered by NASA's Hubble and Spitzer space telescopes on 2015.\n\n"}
{"id": "9564390", "url": "https://en.wikipedia.org/wiki?curid=9564390", "title": "Swiss cheese model", "text": "Swiss cheese model\n\nThe Swiss cheese model of accident causation is a model used in risk analysis and risk management, including aviation safety, engineering, healthcare, emergency service organizations, and as the principle behind layered security, as used in computer security and defense in depth. It likens human systems to multiple slices of swiss cheese, stacked side by side, in which the risk of a threat becoming a reality is mitigated by the differing layers and types of defenses which are \"layered\" behind each other. Therefore, in theory, lapses and weaknesses in one defense do not allow a risk to materialize, since other defenses also exist, to prevent a single point of failure. The model was originally formally propounded by Dante Orlandella and James T. Reason of the University of Manchester, and has since gained widespread acceptance. It is sometimes called the cumulative act effect.\n\nAlthough the Swiss cheese model is respected and considered to be a useful method of relating concepts, it has been subject to criticism that it is used too broadly, and without enough other models or support.\n\nReason hypothesized that most accidents can be traced to one or more of four failure domains: organizational influences, supervision, preconditions, and specific acts.For example, in aviation, preconditions for unsafe acts include fatigued air crew or improper communications practices. Unsafe supervision encompasses for example, pairing inexperienced pilots on a night flight into known adverse weather. Organizational influences encompass such things as reduction in expenditure on pilot training in times of financial austerity.\n\nIn the Swiss cheese model, an organisation's defenses against failure are modeled as a series of barriers, represented as slices of cheese. The holes in the slices represent weaknesses in individual parts of the system and are continually varying in size and position across the slices. The system produces failures when a hole in each slice momentarily aligns, permitting (in Reason's words) \"a trajectory of accident opportunity\", so that a hazard passes through holes in all of the slices, leading to a failure.\n\nFrosch described Reason's model in mathematical terms as a model in percolation theory, which he analyses as a Bethe lattice.\n\nThe model includes both active and latent failures. Active failures encompass the unsafe acts that can be directly linked to an accident, such as (in the case of aircraft accidents) a navigation error. Latent failures include contributory factors that may lie dormant for days, weeks, or months until they contribute to the accident. Latent failures span the first three domains of failure in Reason's model.\n\nIn the early days of the Swiss Cheese model, late 1980 to about 1992, attempts were made to combine two theories: James Reason multi-layer defence model and Willem Albert Wagenaar’s Tripod theory of accident causation. This resulted in a period where the Swiss Cheese diagram was represented with the slices of cheese labels as Active Failures, Preconditions and latent failures. \n\nThese attempts to combine both theories still causes confusion today. A more correct version of the combined theories is shown with the Active Failures (now called immediate causes) Precondition and Latent Failure (now called underlying causes) shown as the reason each barrier (slice of cheese) has a hole in it and the slices of cheese as the barriers.\n\nThe same framework can be applicable in some areas of healthcare. For example, a latent failure could be the similar packaging of two drugs that are then stored close to each other in a pharmacy. Such a failure would be a contributory factor in the administration of the wrong drug to a patient. Such research led to the realization that medical error can be the result of \"system flaws, not character flaws\", and that greed, ignorance, malice or laziness are not the only causes of error.\n\nLubnau, Lubnau, and Okray apply the model to the engineering of firefighting systems, aiming to reduce human errors by \"inserting additional layers of cheese into the system\", namely the techniques of Crew Resource Management.\n\nThis is one of the many models listed, with references, in.\n\nKamoun and Nicho found the Swiss cheese model to be a useful theoretical model to explain the multifaceted (human, organizational and technological) aspects of healthcare data breaches.\n\n"}
{"id": "35216677", "url": "https://en.wikipedia.org/wiki?curid=35216677", "title": "TV Guide's 100 Greatest Episodes of All-Time", "text": "TV Guide's 100 Greatest Episodes of All-Time\n\n100 Greatest Episodes of All-Time (1997) and Top 100 Episodes of All Time (2009) are lists of the 100 best television show episodes in U.S. television history. \"TV Guide\" published both lists: the first, published in June 28, 1997, was produced in collaboration with Nick at Nite's TV Land. A revised list was published in June 15, 2009. The lists excluded game shows and variety shows, but included situation comedies and drama series.\n\nAbout 25 shows from the original list were featured during a special week on Nick at Night on Nickelodeon and TV Land. On the original list several shows, including \"I Love Lucy\", \"The Mary Tyler Moore Show\", \"The Dick Van Dyke Show\", \"Seinfeld\", \"Cheers\", and \"The Odd Couple\" had multiple entries, but none did on the 2009 list. The original list included 35 episodes from the 1950s and 1960s, while the 2009 list only included 10. Over one-third of the new list first aired in the twelve years since the original list, including 14 from the most recent two years. The June 2009 list included episodes as recent as the April 12, 2009, \"Peekaboo\" episode of \"Breaking Bad\" and the April 7, 2009, \"Baptism\" episode of \"Rescue Me\", while the 1997 list included a two-month-old episode of \"The X-Files\".\n\nCBS was the original broadcaster of 35 of the 1997 list members, including both of the top two: \"Chuckles Bites the Dust\" of \"The Mary Tyler Moore Show\" and \"Lucy Does a TV Commercial\" from \"I Love Lucy\". Several television series were represented by a different episode in 2009 than they were in 1997. The 2009 top rated show was \"The Contest\" from \"Seinfeld\" although in 1997 the show had been represented by \"The Boyfriend\" (number four) and \"The Parking Garage\" (number 33), instead of \"The Contest\". Three of the top 10 1997 episodes were removed although the series continued to be represented by other episodes: \"The $99,000 Answer\" (number 6) from \"The Honeymooners\", \"Thanksgiving Orphans\" (number 7) from \"Cheers\"; and \"Coast-to-Coast Big Mouth\" (number 8) from \"The Dick Van Dyke Show\".\n\nThe 1997 list included detailed show descriptions for all episodes, while only a select few were accompanied by more than a single sentence in the ascending order 2009 list. Only one of the thirteen contributors to the 2009 list was involved in the 1997 list.\n\n\nPaley Center for Media's James Sheridan regretted the recent bias of the newer list. Hal Boedeker of the \"Orlando Sentinel\" viewed the original list as a triumphant result despite its conflict of interest. The original list was derided for omitting \"Bonanza\" and \"The Golden Girls\" entirely. \"Daily News\" David Bianculli was critical of the original list, saying that many selections would not be on a more legitimate top-100 list. He claimed that episodes chosen from \"Murder One\", \"The Partridge Family\", \"The Love Boat\", and \"Speed Racer\" reveal a lack of either maturity or perspective, while others showed taste and bravery. \"Chicago Tribune\" critic Steve Johnson complained that it was premature to claim that any episode of \"ER\" had established sufficient cultural significance to rank third on the original list. Jaime Weinman of \"Maclean's\" complained that the 2009 list was composed of the most well-known episode of famous series, claiming that they were largely unexceptional episodes. He preferred the 1997 list, which he said was produced when \"TV Guide\" was a higher caliber publication.\n\n\n"}
{"id": "23002008", "url": "https://en.wikipedia.org/wiki?curid=23002008", "title": "Under det rosa täcket", "text": "Under det rosa täcket\n\nUnder det rosa täcket (English: \"Under the Pink Duvet\") is a Swedish feminist book written by Nina Björk, first published in 1996. The book made a great impact on the Swedish feminist field of the 1990s and is notable for introducing Butlerian queer theory to Sweden.\n"}
