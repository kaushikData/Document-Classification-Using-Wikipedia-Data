{"id": "26157481", "url": "https://en.wikipedia.org/wiki?curid=26157481", "title": "Acoustic dispersion", "text": "Acoustic dispersion\n\nAcoustic dispersion is the phenomenon of a sound wave separating into its component frequencies as it passes through a material. The phase velocity of the sound wave is viewed as a function of frequency. Hence, separation of component frequencies is measured by the rate of change in phase velocities as the radiated waves pass through a given medium.\n\nA widely used technique for determining acoustic dispersion is a broadband transmission method. This technique was originally introduced in 1978 and has been employed to study the dispersion properties of metal (1978), epoxy resin (1986), paper materials (1993) ultra-sound contrast agent (1998). In 1990 and 1993 this method confirmed the Kramers–Kronig relation for acoustic waves.\n\nApplication of this method requires the measurements of a reference velocity to obtain values for the acoustic dispersion. This is accomplished by determining (usually) the speed of the sound in water, the thickness of the specimen, and the phase spectrum of each of the two transmitted ultrasound pulses.\n\n\n"}
{"id": "754409", "url": "https://en.wikipedia.org/wiki?curid=754409", "title": "Anousheh Ansari", "text": "Anousheh Ansari\n\nAnousheh Ansari (; née Raissyan; born September 12, 1966) is an Iranian-American engineer and co-founder and chairwoman of Prodea Systems. Her previous business accomplishments include serving as co-founder and CEO of Telecom Technologies, Inc. (TTI). The Ansari family is also the title sponsor of the Ansari X Prize. On September 18, 2006, a few days after her 40th birthday, she became the first Iranian in space. Ansari was the fourth overall self-funded space tourist, and the first self-funded woman to fly to the International Space Station. Her memoir, \"My Dream of Stars\", co-written with Homer Hickam, was published by Palgrave Macmillan in 2010.\n\nBorn Anousheh Raissyan in Mashhad, Iran, she and her parents moved to Tehran shortly afterward. Ansari is Muslim. She witnessed the Iranian Revolution in 1979. She emigrated to the United States in 1984 as a teenager. Apart from her native Persian, she is fluent in English and French, and acquired a working knowledge of Russian for her spaceflight experience.\n\nShe received her Bachelor of Science degree in electrical engineering and computer science at George Mason University in Fairfax, Virginia, and her master's degree at George Washington University in Washington D.C.\n\nAfter graduation, Raissyan began work at MCI, where she met her future husband, Hamid Ansari. They married in 1991.\n\nIn 1993, she persuaded her husband and her brother-in-law Amir Ansari to co-found Telecom Technologies Inc., using their savings and corporate retirement accounts, as deregulation happened in the US telecommunications industry. The company was a supplier of softswitch technology that enabled telecom \"service providers to enhance system performance, lower operating costs and furnish new revenue opportunities.\" The company, headquartered in Richardson, Texas, offered products that allowed the integration between existing telecom networks and application-centric, next-generation networks via software switch technology. Telecom Technologies was acquired by Sonus Networks, Inc. in 2001 in a stock-for-stock transaction for 10.8 million shares of Sonus stock. Anousheh Ansari became \"a vice president of Sonus and general manager of Sonus' new INtelligentIP division.\"\n\nIn 2006, she co-founded Prodea Systems, and is the current chairman and CEO. Prodea is a technology and services management company. Prodea is a privately held company formed by the Ansari family with development centers in both Richardson, Texas, and Silicon Valley.\n\nAnsari has expressed that she does not consider herself a \"space tourist\", and prefers the title of \"spaceflight participant\".\n\nAnsari is a member of the X PRIZE Foundation's Vision Circle, as well as its Board of Trustees. Along with her brother-in-law, Amir Ansari, she made a multimillion-dollar contribution to the X PRIZE foundation on May 5, 2004, the 43rd anniversary of Alan Shepard's sub-orbital spaceflight. The X PRIZE was officially renamed the \"Ansari X PRIZE\" in honor of their donation. As demonstrated by her commitment to the X PRIZE and through presentations at Space Enthusiast conferences, Ansari is a spokesperson for the \"privatization of space\", a process enabling commercially viable companies to government-independently send equipment and/or people into space for exploration and other purposes.\n\nThe Ansari family investment firm, also named Prodea, has announced a partnership with Space Adventures, Ltd. and the Federal Space Agency of the Russian Federation (FSA) to create a fleet of suborbital spaceflight vehicles (the Space Adventures Explorer) for global commercial use.\n\nAnsari trained as a backup for Daisuke Enomoto for a Soyuz flight to the International Space Station, through Space Adventures, Ltd. On August 21, 2006, Enomoto was medically disqualified from flying the Soyuz TMA-9 mission that was due to launch the following month. The next day Ansari was elevated to the prime crew.\nAsked what she hoped to achieve on her spaceflight, Ansari said, \"I hope to inspire everyone—especially young people, women, and young girls all over the world, and in Middle Eastern countries that do not provide women with the same opportunities as men—to not give up their dreams and to pursue them... It may seem impossible to them at times. But I believe they can realize their dreams if they keep it in their hearts, nurture it, and look for opportunities and make those opportunities happen.\" The day before her departure, she was interviewed on Iran national television for the astronomy show \"Night's Sky\". The hosts wished her success and thanked her on behalf of Iranians. Ansari in return, thanked them.\n\nAnsari lifted off on the Soyuz TMA-9 mission with commander Mikhail Tyurin (RSA) and flight engineer Michael Lopez-Alegria (NASA) at 04:59 (UTC) on Monday September 18, 2006, from Baikonur, Kazakhstan. Ansari became the fourth (and first female) space tourist. Her contract did not allow for disclosure of the amount paid, but previous space tourists have paid in excess of $20 million USD. The space craft docked with the International Space Station (ISS) on Wednesday September 20, 2006, at 05:21 (UTC). Ansari landed safely aboard Soyuz TMA-8 on September 29, 2006, at 01:13 UTC on the steppes of Kazakhstan (90 kilometers north of Arkalyk) with U.S. astronaut Jeffrey Williams and Russian cosmonaut Pavel Vinogradov. She was given red roses from an unidentified official, and a surprise kiss from her husband, Hamid. The crew's rescuers moved them to Kustanai by helicopter for the welcome ceremony.\n\nDuring her nine-day stay on board the International Space Station, Ansari agreed to perform a series of experiments on behalf of the European Space Agency. She conducted four experiments, including:\n\nShe also became the first person to publish a weblog from space.\n\nAnsari intended to wear the U.S. flag on her spacesuit alongside a politically neutral version of the Iranian flag, i.e. the simple 3-color flag with no government-specific emblem, to honor the two countries that have contributed to her life. A few U.S.-based media wrongly speculated that she was intending to wear the version of the Iranian flag that predated the 1979 Islamic revolution in Iran.\n\nAt the insistence of the NASA and Russian officials, she did not wear the Iranian flag officially, but wore the Iranian flag colors instead and kept the Iranian flag on her official flight patch. She and her husband said no political message was intended, despite the increasing tensions with United States and Iran relations, which had dominated world headlines in the weeks leading up to her launch. She noted that she had \"plans to devote her mission to expanding a global consciousness she expected would be seeded with her first look at Earth from space\".\n\nMichael López-Alegría, the Spanish-born NASA astronaut who flew on the Russian Soyuz spacecraft on the return flight with Ansari, expressed his doubts to reporters before the flight: \"I'm not a big fan personally of having those guys go visit the space station because I think the space station is still a place that is under construction, and not quite operational. I don't think it's ideal.\"\n\nLópez-Alegría later stated that he was skeptical of private tourists a few years ago, but now believes it is essential to the survival of the Russian space program which is important to the U.S. space program: \"If that's the correct solution... then not only is it good from the standpoint of supporting the Russian space program, but it's good for us as well,\" he said. Ansari's presence in space \"is a great dream and a great hope not just for our country but for countries all around the world.\"\n\nThe same Associated Press story also quoted Mikhail Tyurin describing Ansari as \"very professional\" and said he felt like they had worked together for a decade.\n\nThe flight was given significant coverage by Iranian state television, with an hour long live interview with Ansari being broadcast on the show \"Aseman-e-Shab\" (\"Night Sky\"). Ansari was praised by newspapers such as \"Hambastegi\" and \"Jam-e-Jam Daily\", which published daily columns detailing the journey. The astronomy magazine \"NOJUM\" also published an exclusive interview of Pouria Nazemi with Ansari before her trip, in which she discussed her vision for commercial spaceflight. \"NOJUM\" also organized and held gatherings when the ISS passed over Iran's cities. Shahram Yazdanpanah, made a special part about Anousheh's trip to space at Persian \"Space Science\" website and covered all the news of trip.\n\nOn September 22, 2006, she told reporters that she has no regrets and said \"I am having a wonderful time here. It's been more than what I expected, and I am enjoying every single second of it. The entire experience has been wonderful up here.\"\n\nAnsari has received multiple honors, including the George Mason University Entrepreneurial Excellence Award, the George Washington University Distinguished Alumni Achievement Award, the Ernst & Young Entrepreneur of the Year Award for the Southwest Region, and the Horatio Alger Award. While under her leadership, Telecom Technologies, Inc. earned recognition as one of \"Inc. magazine\"s 500 fastest-growing companies and one of Deloitte & Touche's Fast 500 technology companies. She was listed in \"Fortune Magazine\"s 40 under 40 list in 2001 and honored by \"Working Woman\" as the winner of the 2000 National Entrepreneurial Excellence award.\n\nIn 2009, she received the first NCWIT Symons Innovator Award given annually by the National Center for Women & Information Technology to honor successful women entrepreneurs in technology.\n\nShe received an Honorary Doctorate of Science from her alma mater George Mason University on December 20, 2012.\n\nThe Ansari family was honored with an Orbit Award by the National Space Society and Space Tourism Society for underwriting the Ansari X Prize.\n\nIn 2010, she was awarded the Ellis Island Medal of Honor in recognition of her humanitarian efforts.\n\nIn 2015, the National Space Society awarded Ansari the Space Pioneer Award for her \"Service to the Space Community.\"\n\nAnsari participated as a speaker at the 2010 Honeywell Leadership Academy with Homer Hickam at United States Space Camp in Huntsville, Alabama.\n\nIn 2009, Ansari was featured in the documentary film \"Space Tourists\" by independent Swiss filmmaker Christian Frei about billionaires who paid to ride to the International Space Station aboard Russian spacecraft. The DVD of the film was released in 2011.\nShe served as the commencement speaker at and received an Honorary Doctorate of Science from Utah Valley University on April 25, 2013.\n\nOn February 26, 2017, she and Firouz Naderi represented Iranian filmmaker Asghar Farhadi at the 89th Academy Awards and accepted the Academy Award for Best Foreign Language Film for \"The Salesman\" on Farhadi's behalf. Farhadi did not attend the ceremony due to his opposition to President Trump's immigration ban applying to seven Muslim countries including Iran. Farhadi selected Ansari and Naderi as his representatives because both are successful Iranian-Americans who immigrated to the US.\n\nAnsari \"has served on the boards of directors for Make-a-Wish Foundation of North Texas and Collin County Children's Advocacy Center. She has been active with several nonprofit organizations, including the nonprofit Iranian American Women Foundation organization. Other non-profit organizations include the Ashoka Foundation in its support of social entrepreneurs.\n\nWhile working at MCI, she met Hamid Ansari. They married in 1991. The Ansaris reside in Plano, Texas. She is also the aunt of the American actors Yara Shahidi and Sayeed Shahidi.\n\n\n"}
{"id": "24449130", "url": "https://en.wikipedia.org/wiki?curid=24449130", "title": "AstroFlight Sunrise", "text": "AstroFlight Sunrise\n\nThe AstroFlight Sunrise was an unmanned experimental electric aircraft technology demonstrator and the first aircraft to fly on solar power.\n\nFirst conceived in November 1970, the Sunrise first flew on 4 November 1974 from Bicycle Lake, a dry lakebed on the Fort Irwin Military Reservation, California, United States. The first prototype was destroyed on its 28th flight by turbulence. The improved Sunrise II flew the following year.\n\nWhile working as an engineer at Hughes Aircraft, Roland Boucher began design work on an electric-powered aircraft concept in November 1970, calculating that the contemporary nickel-cadmium batteries available would be sufficient to sustain flight using a radio-control model glider. Early experimental projects proved the concept sound and in 1973 Boucher turned his attention to the creation of a high-altitude solar-powered aircraft that would have unlimited endurance.\n\nBoucher explained the project to his superiors at Hughes Aircraft in 1972 and after reviewing it, the company released the project to Boucher in 1973. Boucher took a leave of absence from Hughes to pursue the project and joined his brother, Bob Boucher, at AstroFlight, a small model airplane manufacturer in Venice, California. After successfully flying an electric drone carrying a payload on a DARPA project for Northrop Corporation, they then moved onto Project Sunrise.\n\nUsing commercially available off-the-shelf solar cells producing only 10% efficiency, Boucher calculated that his project would need to be able to fly on about . The aircraft was envisioned as an unmanned sailplane that would have an operating altitude of , powered by a single Samarium–cobalt magnet electric motor, the first motor of its kind in the world. The aircraft would use no batteries and, instead, would descend at night from its operating altitude to about at dawn, before solar energy was once again available for climb.\n\nBoucher saw the Sunrise as a proof of concept for a follow on aircraft that would be capable of remaining aloft for months at an altitude of .\n\n\"Project Sunrise\" was funded by DARPA commencing in January 1974 and administered by Lockheed Aircraft Corporation of Sunnyvale, California, with the contract specifying a \"proof of concept aircraft powered solely by incident sunlight on the wing surfaces.\"\n\nRoland Boucher took on the task of the structural design, aerodynamics, telemetry, control and navigation. He also designed the integration of the solar panel, electric motors, gearbox and the propeller. He selected an Eppler 387 airfoil for the wing. The solar cells were round commercial units provided by Heliotech. The actual airframe was constructed by a team under expert model builder Phil Bernhardt.\n\nThe Sunrise's wing span was and the aircraft had a gross weight of . The wing loading was a very low 4 ounces per square foot (0.011 kg/sq m). The aircraft structure was built from spruce, balsa and maple. Due to their roughness the solar cells were only mounted on the aft two-thirds of the wing's upper surface.\n\nThe wing spars were built from spruce spar caps with maple doublers at all attachment points and two balsa shear webs attached to balsa strips on the wing spars. The ribs were made from balsa. This construction resulted in a balsa spar box with tapered spruce caps. The leading edge was covered with balsa to form a leading edge D spar. The trailing edge was formed by two wide sheets forming a triangle with vertical spar sections in between the ribs. The covering was 1/2 mil Mylar. The span wing weighed and was capable of loads up to 100 pounds.\n\nControl was via an S & O Radio designed and built telemetry transmitter and receiver. The standard S&O six channel radio had channels for elevator, rudder, motor on and off and solar cell operating mode. The solar cells could be set for either series or parallel operation. The telemetry functions provided gave data on motor current, motor voltage, motor RPM, airspeed and two heading references from a sun compass for navigation.\n\nFlight testing commenced in 1974 at Bicycle Lake, California. The first flights were conducted on battery power, using a bungee cord launch to . On its first flight the aircraft reached 500 feet, before returning for a landing.\n\nA lack of sunny days delayed flight testing, but in all, 28 flights were made. The Sunrise would climb slowly at first until its solar cells cooled down and their efficiency increased. On its 28th flight the aircraft was destroyed when it was flown too close to a cumulus cloud at about and the associated turbulence broke the aircraft's structure.\n\nThe success of the flights that had been completed allowed Boucher to state:\n\nDARPA and Lockheed proposed a follow-on design to the Sunrise to be powered at night by batteries instead of just gliding. Roland Boucher designed the second aircraft using higher efficiency solar panels that were more aerodynamically smooth.\n\nRoland Boucher had become physically exhausted from his work on the initial Sunrise and he suffered from congestive heart failure. He was admitted to intensive care at Santa Monica Hospital. While in hospital he resigned from AstroFlight and sold his interests in the company to his brother Bob Boucher who continued work on the second Sunrise aircraft. After recuperating, Roland Boucher returned to work at Hughes Aircraft on classified military programs.\n\nThe Sunrise II first flew on 27 September 1975 from Nellis AFB, near Las Vegas, Nevada.\n\n\n"}
{"id": "763880", "url": "https://en.wikipedia.org/wiki?curid=763880", "title": "Beach Abort", "text": "Beach Abort\n\nThe Beach Abort was an unmanned test in NASA's Project Mercury, of the Mercury spacecraft Launch Escape System. Objectives of the test were a performance evaluation of the escape system, the parachute and landing system, and recovery operations in an off-the-pad abort situation. The test took place at NASA's Wallops Island, Virginia, test facility on May 9, 1960. In the test, the Mercury spacecraft and its Launch Escape System were fired from ground level. The flight lasted a total of 1 minute, 16 seconds. The spacecraft reached an apogee of and splashed down in the ocean with a range of .Top speed was a velocity of . A Marine Corps helicopter recovered the spacecraft 17 minutes after launch. The test was considered a success, although there was insufficient separation distance when the tower jettisoned. Mercury Spacecraft #1, the first spacecraft off McDonnell's production line, was used in this test. Total payload weight was .\n\nMercury Spacecraft #1 is displayed at the New York Hall of Science, Corona Park, NY. It is displayed indoors, suspended from the ceiling, with an escape tower of unknown provenance attached.\n\n"}
{"id": "8611364", "url": "https://en.wikipedia.org/wiki?curid=8611364", "title": "Calcium reactor", "text": "Calcium reactor\n\nIn marine and reef aquariums, a calcium reactor creates a balance of alkalinity. An acidic solution is produced by injecting carbon dioxide into a chamber with salt water and calcium rich media. The carbon dioxide lowers the pH by producing a solution high in carbonic acid, and dissolves calcium. The effluent is returned to the reef aquarium where the calcium is consumed by organisms, primarily corals when building skeletons. A calcium reactor is an efficient method to supply calcium to a reef aquarium. Reactors may be used in elaborate freshwater and brackish aquariums where freshwater clams and other invertebrates need a constant supply of calcium.\n\nThe reactor dissolves the calcium-laden media to provide bicarbonates HCO (alkalinity) and calcium (Ca) ions at the sames rate as consumed during calcification. Effectively dissolving the media requires an acidic pH. Saltwater may have a pH of 7.8 or higher, so to reduce the pH carbon dioxide (CO) is used. The reaction formula is: \n\nInside the reaction chamber, a calcium rich media (aragonite), mainly CaCO, is forced into contact with water injected with carbon dioxide (CO) in order to create carbonic acid (HCO). This increases the solubility of the calcium carbonate. The reaction frees the calcium and carbonate, supplying the aquarium with water rich in Ca and CO, important for maintaining alkalinity and calcium levels.\n\nThe bubble counter measures carbon dioxide. The flow rate of carbon dioxide is monitored so that the dissolved gas goes into the solution, with a minimum unconsumed. A needle valve or solenoid valve regulates the CO bubble rate. Valves with precise adjustment abilities improve bubble control.\n\nThe feed pump controls the volume of water exchange. This is important because a high rate of water flow into the reactor reduces its efficiency, thus resulting in underproduction and a waste of CO.\n\nSome reactors siphon water into the input of the reactor's re-circulation pump. A potential complication is the medium in the reactor becoming compacted, increasing back pressure onto the pump and reducing water into the reactor. Placing a gate or needle valve on the reactor's outlet side will improve flow characteristics compared to control from the inlet side.\n\nPeristaltic pumps are effective operating against pressure, capable of supplying an adjustable and continuous flow over flow rates with minimal maintenance.\n\nThe pH control is connected to a probe in the reactor and adjusts the rate at which the calcium media dissolves. This probe monitors the pH level in the calcium reactor. The pH range for the typical calcium reactor is 6.5–6.8. When the pH rises above a certain level, a valve opens, allowing carbon dioxide to enter the reactor. The control closes the valve as the pH falls below this level.\n\nSome pH controllers have an interface for an air pump. This air pump is connected to an airstone in the sump or main tank. If the probe detects a low pH level, the pump activates. The bubbles raise the pH by dissipating the CO gas.\n\n\n"}
{"id": "5322156", "url": "https://en.wikipedia.org/wiki?curid=5322156", "title": "Cholesteric liquid crystal", "text": "Cholesteric liquid crystal\n\nA cholesteric liquid crystal display (ChLCD) is a display containing a liquid crystal with a helical structure and which is therefore chiral. Cholesteric liquid crystals are also known as \"chiral nematic liquid crystals\". They organize in layers with no positional ordering within layers, but a director axis which varies with layers. The variation of the director axis tends to be periodic in nature. The period of this variation (the distance over which a full rotation of 360° is completed) is known as the pitch, p. This pitch determines the wavelength of light which is reflected (Bragg Reflection).\n\nThe technology is characterized by stable states i.e. focal conic state (dark state) and planar state (bright state). Displays based on this technology are called “bistable” and don’t need any power to maintain the information (zero power). Because of the reflective nature of the ChLCD, these displays can be perfectly read under sunlight conditions.\n\nExamples of compounds known to form cholesteric phases are hydroxypropyl cellulose and cholesteryl benzoate.\n\nSome companies, such as Chiral Photonics, have begun to explore CLCs as the basis for photonic devices.\n\nA US company, Kent Displays, has developed \"no power\" Liquid Crystal Displays using Polymer Stabilized Cholesteric Liquid Crystals: these are known as ChLCD screens. A drawback of ChLCD screens is their slow refresh rate, especially at low temperatures. In 2009, Kent demonstrated the use of a ChLCD to cover the entire surface of a mobile phone, allowing it to change colours, and keep that colour even when power is cut off.\n\nThe Industrial Technology Research Institute has developed a flexible ePaper called i2R based on ChLCD technology. ITRI\n\nThe German company BMG MIS (formerly AEG MIS) has developed “Geameleon”- ChLCDs full color and monochrome in various sizes and resolutions. These ChLCDs can be used in extremely wide temperature applications i.e. for outdoor application. Various update versions can be applied to achieve a reasonable update time even at very low temperatures. Because of its very low power consumption, this technology is highly preferred to be used in self-sustaining applications (solar power applications).\n"}
{"id": "15440540", "url": "https://en.wikipedia.org/wiki?curid=15440540", "title": "Conflict continuum", "text": "Conflict continuum\n\nConflict continuum is a model or concept used by various social science researchers in modelling conflict, usually going from low \"irritations\" to high \"explosiveness\" intensity. These conceptual models facilitate discussion as in \"anywhere on the conflict continuum\".\n\nElise M. Boulding was a Quaker sociologist influenced by the events of World War II. Examining how war becomes peace, she posited a continuum between Wars of Extermination and Transformation.\n\nThis is Boulding’s conflict continuum: War of extermination, Limited war, Threat systems (deterrence), Arbitration, Mediation, Negotiation (exchange), Mutual adaptation, Alliance, Co-operation, Integration, Transformation.\n\nTheorist Andra Medea seeks to explain how individuals, small groups, organizations, families, ethnicities, and even whole nations function when disputes arise between them. She posits that there are four types or levels of conflict, each operating under distinct rules: 1) \"Problem Solving\"; 2) \"Domination\"; 3) \"Blind Behavior\"; 4) \"Rogue Messiah\".\n\nEach level moving from first to fourth is characterized by increasing degrees of separation from reality, and decreasing degrees of maturity, in this context, defined as the ability to control anger and settle differences without violence or destruction. Problem-solving behavior is based in reality and maturity, and is therefore more rational and mature than domination. Domination is more rational and mature than blind behavior, which is more rational and mature than the Rogue Messiah.\n\nHowever, each level moving from fourth to first is more capable than the one below it at forcing victory in a conflict. The Rogue Messiah overpowers blind behavior, blind behavior thwarts domination, and domination deadlocks Problem-solving.\n"}
{"id": "2499250", "url": "https://en.wikipedia.org/wiki?curid=2499250", "title": "DBZ (meteorology)", "text": "DBZ (meteorology)\n\ndBZ stands for \"decibel relative to Z\". It is a logarithmic dimensionless technical unit used in radar, mostly in weather radar, to compare the equivalent reflectivity factor (Z) of a radar signal reflected off a remote object (in mm per m) to the return of a droplet of rain with a diameter of 1 mm (1 mm per m). It is proportional to the number of drops per unit volume and the sixth power of drops' diameter and is thus used to estimate the rain or snow intensity. With other variables analyzed from the radar returns it helps to determine the type of precipitation. Both the radar reflectivity factor and its logarithmic version are commonly referred to as \"reflectivity\" when the context is clear.\n\nThe radar reflectivity factor (Z) of precipitation is dependent on the number (N) and size (D) of reflectors (hydrometeors), which includes rain, snow, graupel, and hail. Very sensitive radars can also measure the reflectivity of cloud drops and ice. For an exponential distribution of reflectors, Z is expressed by:\n\nAs rain droplets have a diameter of the order of 1 millimetre, Z is in mmm (μm), a quite unusual unit. By dividing Z with the equivalent return of a 1 mm drop in a volume of a meter cube (Z) and using the logarithm of the result (because the values vary greatly from drizzle to hail), one obtains the dimensionless quantity dBZ:\n\ndBZ values can be converted to rainfall rates in millimetres per hour using the Marshall-Palmer formula:\n\nThe definition of Z above shows that a large number of small hydrometeors will reflect as one large hydrometeor. The signal returned to the radar will be equivalent in both situations, so a group of small hydrometeors is virtually indistinguishable from one large hydrometeor on the resulting radar image. The reflectivity image is just one type of image produced by a radar. Using it alone a meteorologist could not tell with certainty the type of precipitation and distinguish any artifacts affecting the radar return.\n\nIn combination with other information gathered by the radar during the same scan (dual polarization products and phase shifting due to the Doppler effect), meteorologists can distinguish between hail, rain, snow, biologicals (birds, insects), and other atmospheric phenomena.\n\n≈"}
{"id": "4917179", "url": "https://en.wikipedia.org/wiki?curid=4917179", "title": "Discharge ionization detector", "text": "Discharge ionization detector\n\nA discharge ionization detector (DID) is a type of detector used in gas chromatography.\n\nA DID is an ion detector which uses a high-voltage electric discharge to produce ions. The detector uses an electrical discharge in helium to\ngenerate high energy UV photons and metastable helium which ionizes all compounds except helium. The ions produce an electric current, which is the signal output of the detector. The greater the concentration of the component, the more ions are produced, and the greater the current.\n\nDIDs are sensitive to a broad range of components.\nIn Air Separation plants they are used to detect the components CO; CH2; C+; N2; O2 in Argon product in ppm range.\n\nDIDs are non-destructive detectors. They do not destroy/consume the components they detect. Therefore, they can be used before other detectors in multiple-detector configurations.\n\nDIDs are an improvement over Helium ionization detectors in that they contain no radioactive source.\n"}
{"id": "11034831", "url": "https://en.wikipedia.org/wiki?curid=11034831", "title": "Est: The Steersman Handbook", "text": "Est: The Steersman Handbook\n\nest: The Steersman Handbook, Charts of the Coming Decade of Conflict is a work of science fiction cast as a nonfictional study. Its author, credited as L. Clark Stevens, usually went by the name Leslie Stevens. Stevens has a long list of credits in the entertainment industry, having worked on, among other productions, \"The Outer Limits\". The book was published in paperback in 1970, and reprinted in 1971. \n\nThe \"est\" in the book's title refers to what Stevens described as \"Electronic Social Transformation\". The book described a future society and the rise of what Stevens described as the \"est people\". The \"est people\" were a new generation of postliterate humans who were to bring about a \"transformation\" of society. The \"est people\" were to be technically minded, eclectic, and computer literate. They would possess qualities necessary for social transformation, integral to Earth's survival. Individuals named as examples of \"est people\" in the book included R. Buckminster Fuller, Jiddu Krishnamurti, Ralph Nader, Marshall McLuhan, Malcolm X, Albert Einstein, Lewis Mumford, and Eric Hoffer.\n\nDean Gengle wrote in his book, \"The Netweaver's Sourcebook\", that the book: \"..did more to liberate media-created hippies than just about any other work of its time.\" The book has also been referenced in later sociological evaluations of potential paths for society, including Michael Marien's \"Societal Directions and Alternatives\", and Gurth Higgin's \"Symptoms of Tomorrow\". The book's publisher, Capricorn Press or \"Capra Press\", would come to be better known, having published the work.\n\nThough the book was speculative in nature, it's author, Stevens was subsequently consulted on issues relating to the future of the planet's economic, ecological and energy systems. The book was later referenced by Mark Hinshaw in a piece describing two potential futures, who cited Steven's term \"Electronic Social Transformation\".\n\nSecondary sources have stated that the title of this work inspired Werner Erhard to name his company Erhard Seminars Training, or \"est\" for short, and to refer to it as such in lower-case. Peter Occhiogrosso writes in \"The Joy of Sects\" that Erhard borrowed the initials, \"lowercase and all\", from the book. In his book \"Larson's Book of World Religions and Alternative Spirituality\", Bob Larson refers to Erhard's friend Bill Thaw in citing the same information. According to Steven Pressman's book, \"Outrageous Betrayal\", Werner Erhard made other staff members on his Mind Dynamics sales team read the book.\n\n"}
{"id": "57713994", "url": "https://en.wikipedia.org/wiki?curid=57713994", "title": "Explorer 41", "text": "Explorer 41\n\nExplorer 41, also called as IMP-G and IMP 5, was a U.S. satellite launched as part of Explorers program. Explorer 41 as launched on June 21, 1969 on Vandenberg AFB, California, United States, with an Delta rocket. Explorer 41 was the fifth satellite of the Interplanetary Monitoring Platform.\n\nExplorer 41 was a spin-stabilized satellite placed into a high-inclination, highly elliptical orbit to measure energetic particles, magnetic fields, and plasma in cislunar space. The line of apsides and the satellite spin vector were within a few degrees of being parallel and normal, respectively, to the ecliptic plane. Initial local time of apogee was about 1,300 hours. Initial satellite spin rate was 27.5 rpm. \n\nThe basic telemetry sequence was 20.48 seconds. Data transmission was nearly 100% for the spacecraft life except for the interval from November 15, 1971, to February 1, 1972, when data acquisition was limited to the vicinity of the magnetotail neutral sheet.\n\nExplorer 41 functioned very well from launch until it decayed from orbit on December 23, 1972\n"}
{"id": "30733805", "url": "https://en.wikipedia.org/wiki?curid=30733805", "title": "Femoral fracture", "text": "Femoral fracture\n\nA femoral fracture is a bone fracture that involves the femur. They are typically sustained in high-impact trauma, such as car crashes, due to the large amount of force needed to break the bone. Fractures of the diaphysis, or middle of the femur, are managed differently from those at the head, neck, and trochanter (see \"hip fractures\").\n\nThe fracture may be classed as \"open\", which occurs when the bone fragments protrude through the skin, or there is an overlying wound which penetrates to the bone. These types of fracture cause more damage to the surrounding tissue, are less likely to heal properly, and are at much greater risk of infection.\n\nFemoral shaft fractures can be classified with the Winquist and Hansen classification, which is based on the amount of comminution.\n\nFractures of the inferior or distal femur may be complicated by separation of the condyles, resulting in misalignment of the articular surfaces of the knee joint, or by hemorrhage from the large popliteal artery that runs directly on the posterior surface of the bone. This fracture compromises the blood supply to the leg (an occurrence that should always be considered in knee fractures or dislocations).\n\nFractures are commonly obvious, since femoral fractures are often caused by high energy trauma. Signs of fracture include swelling, deformity, and shortening of the leg. Extensive soft-tissue injury, bleeding, and shock are common. The most common symptom is severe pain, which prevents movement of the leg.\n\nFemoral shaft fractures occur during extensive trauma, and they can act as distracting injuries, whereby the observer accidentally overlooks other injuries, preventing a thorough exam of the complete body. For example, the ligaments and meniscus of the ipsilateral (same side) knee are also commonly injured.\n\nAnterior-posterior (AP) and lateral radiographs are typically obtained. In order to rule out other injuries, hip, pelvis, and knee radiographs are also obtained. The hip radiograph is of particular importance, because femoral neck fractures can lead to osteonecrosis of the femoral head.\n\nA 2015 Cochrane review found that available evidence for treatment options of distal femur fractures is insufficient to inform clinical practice and that there is a priority for a high-quality trial to be undertaken. Open fractures must undergo urgent surgery to clean and repair them, but closed fractures can be maintained until the patient is stable and ready for surgery.\n\nAvailable evidence suggests that treatment depends on the part of the femur that is fractured. Traction may be useful for femoral shaft fractures because it counteracts the force of the muscle pulling the two separated parts together, and thus may decrease bleeding and pain. Traction should not be used in femoral neck fractures or when there is any other trauma to the leg or pelvis. It is typically only a temporary measure used before surgery. It only considered definitive treatment for patients with significant comorbidities that contraindicate surgical management.\n\nExternal fixators can be used to prevent further damage to the leg until the patient is stable enough for surgery. It is most commonly used as a temporary measure. However, for some select cases it may be used as an alternative to intramedullary nailing for definitive treatment.\n\nFor femoral shaft fractures, reduction and intramedullary nailing is currently recommended. The bone is re-aligned, then a metal rod is placed into the femoral bone marrow, and secured with nails at either end. This method offers less exposure, a 98%-99% union rate, lower infection rates (1%-2%) and less muscular scarring.\n\nAfter surgery, the patient should be offered physiotherapy and try to walk as soon as possible, and then every day after that to maximise their chances of a good recovery.\n\nThese fractures can take at least 4–6 months to heal. Since femoral shaft fractures are associated with violent trauma, there are many adverse outcomes, including fat embolism, acute respiratory distress syndrome (ARDS), multisystem organ failure, and shock associated with severe blood loss. Open fractures can result in infection, osteomyelitis, and sepsis.\n\nFemoral shaft fractures occur in a bimodal distribution, whereby they are most commonly seen in males age 15-24 (due to high energy trauma) and females aged 75 or older (pathologic fractures due to osteoporosis, low-energy falls).\n"}
{"id": "316607", "url": "https://en.wikipedia.org/wiki?curid=316607", "title": "Five laws of library science", "text": "Five laws of library science\n\nThe Five laws of library science is a theory proposed by S. R. Ranganathan in 1931 detailing the principles of operating a library system. Many librarians worldwide accept them as the foundations of their philosophy.\n\nThese laws are:\n\nThe first law constitutes the basis for the library services. Dr. Ranganathan observed that books were often chained to prevent their removal and that the emphasis was on storage and preservation rather than use. He did not reject the notion that preservation and storage were important, but he asserted that the purpose of such activities was to promote use. Without user access to materials, there is little value in these items. By emphasizing use, Dr. Ranganathan refocused the attention of the field to access-related issues, such as the library's location, loan policies, hours and days of operation, as well as the quality of staffing and mundane matters like library furniture, temperature control and lighting. \n\nThe first law of library science \"books are for use\" means that books in libraries are not meant to be shut away from its users.\n\nThis law suggests that every member of the community should be able to obtain materials needed. Dr. Ranganathan felt that all individuals from all social environments were entitled to library service, and that the basis of library use was education, to which all were entitled. These entitlements were not without some important obligations for both libraries/librarians and library patrons. Librarians should have excellent first-hand knowledge of the people to be served. Collections should meet the special interests of the community, and libraries should promote and advertise their services extensively to attract a wide range of readers. \n\nThe second law of library science \"every person his or her book\" means that librarians serve a wide collection of patrons, acquire literature to fit a vast collection of needs, do not judge what specific patrons choose to read. Everyone has different tastes and differences and librarians should respect that.\n\nThis principle is closely related to the second law, but it focuses on the item itself, suggesting that each item in a library has an individual or individuals who would find that item useful. Dr. Ranganathan argued that the library could devise many methods to ensure that each item finds its appropriate reader. One method involved the basic rules for access to the collection, most notably the need for open shelving.\n\nThe third law of library science \"every book its reader\" means a library's books have a place in the library even if a smaller demographic might choose to read it.\n\nThis law is a recognition that part of the excellence of library service is its ability to meet the needs of the library user efficiently. To this end, Dr. Ranganathan recommended the use of appropriate business methods to improve library management. He observed that centralizing the library collection in one location provided distinct advantages. He also noted that excellent staff would not only include those who possess strong reference skills, but also strong technical skills in cataloging, cross-referencing, ordering, accessioning, and the circulation of materials.\n\nThe fourth law of library science \"save the time of the user\" means that all patrons should be able to easily locate the material they desire quickly and efficiently.\n\nThis law focused more on the need for internal change than on changes in the environment itself. Dr. Ranganathan argued that library organizations must accommodate growth in staff, the physical collection, and patron use. This involved allowing for growth in the physical building, reading areas, shelving, and in space for the catalog.\n\nThe fifth law of library science \"the library is a growing organism\" means that a library should be a continually changing institution, never static in its outlook. Books, methods, and the physical library should be updated over time.\n\nLibrarian Michael Gorman (past president of the American Library Association, 2005–2006), and Walt Crawford recommended the following laws in addition to Ranganathan's five in \"Future Libraries: Dreams, Madness, and Realities\" [American Library Association, 1995], (p. 8) Gorman later repeated them in his small book, \"Our Singular Strengths\" [American Library Association, 1998]. B. Shadrach (Indian Public Library Movement, 2015) proposed an alternative to Ranganathan's five laws at the Indian Public Libraries Conference, 2015 in New Delhi.\n\n\nIn 2004, librarian Alireza Noruzi recommended applying Ranganathan's laws to the Web in his paper, \"Application of Ranganathan's Laws to the Web\":\n\n\nIn 2008, librarian Carol Simpson recommended that editing be done to Ranganathan's law due to media richness. The following were:\n\n\nIn 2015,B. Shadrach proposed an alternative set of laws adapted from Dr S R Ranganathan:\n\n\nIn 2016 Dr Achala Munigal recommended that editing be done to Ranganathan's law due to introduction of Social Tools and their applications in Libraries\n\n\n\n±−×§"}
{"id": "1461554", "url": "https://en.wikipedia.org/wiki?curid=1461554", "title": "Fracture zone", "text": "Fracture zone\n\nA fracture zone is a linear oceanic feature—often hundreds, even thousands of kilometers long—resulting from the action of offset mid-ocean ridge axis segments. They are a consequence of plate tectonics. Lithospheric plates on either side of an active transform fault move in opposite directions; here, strike-slip activity occurs. Fracture zones extend past the transform faults, away from the ridge axis; seismically inactive (because both plate segments are moving in the same direction), they display evidence of past transform fault activity, primarily in the different ages of the crust on opposite sides of the zone.\n\nIn actual usage, many transform faults aligned with fracture zones are often loosely referred to as \"fracture zones\" although technically, they are not.\n\nMid-ocean ridges are divergent plate boundaries. As the plates on either side of an offset mid-ocean ridge move, a transform fault forms at the offset between the two ridges.\n\nFracture zones and the transform faults that form them are separate but related features. Transform faults are plate boundaries, meaning that on either side of the fault is a different plate. In contrast, outside of the ridge-ridge transform fault, the crust on both sides belongs to the same plate, and there is no relative motion along the junction. The fracture zone is thus the junction between oceanic crustal regions of different ages. Because younger crust is generally higher due to increased thermal buoyancy, the fracture zone is characterized by an offset in elevation with an intervening canyon that may be topographically distinct for hundreds or thousands of kilometers on the sea floor.\n\nAs many areas of the ocean floor, particularly the Atlantic Ocean, are currently inactive, it can be difficult to determine past plate motion. However, by observing the fracture zones, one can determine both the direction and rate of past plate motion. This is found by observing the patterns of magnetic striping on the ocean floor (a result of the reversals of Earth's magnetic field over time). By measuring the offset in the magnetic striping, one can then determine the rate of past plate motions. In a similar method, one can use the relative ages of the seafloor on either side of a fracture zone to determine the rate of past plate motions. By comparing how offset similarly aged seafloor is, one can determine how quickly the plate has moved.\n\nThe Blanco Fracture Zone is a fracture zone running between the Juan de Fuca Ridge and the Gorda Ridge. The dominating feature of the fracture zone is the 150 km long Blanco Ridge, which is a high-angle, right-lateral strike slip fault with some component of dip-slip faulting.\n\nThe Charlie-Gibbs Fracture Zone consists of two fracture zones in the North Atlantic that extend for over 2000 km. These fracture zones displace the Mid-Atlantic Ridge a total of 350 km to the west. The section of the Mid-Atlantic Ridge between the two fracture zones is seismically active.\n\nThe Mendocino Fracture Zone extends for over 4,000 km off the coast of California and separates the Pacific Plate and Gorda Plate. The bathymetric depths on the north side of the fracture zone are 800 to 1,200 m shallower than to the south, suggesting the seafloor north of the ridge to be younger. Geologic evidence backs this up, as rocks were found to be 23 to 27 million years younger north of the ridge than to the south.\n\nAlso known as the Romanche Trench, this fracture zone separates the North Atlantic and South Atlantic Oceans. The trench reaches 7,758 m deep, is 300 km long, and has a width of 19 km. The fracture zone offsets the Mid-Atlantic Ridge by more than 640 km.\n\nThe Sovanco Fracture Zone is a dextral-slip transform fault running between the Juan de Fuca and Explorer Ridge in the North Pacific Ocean. The fracture zone is 125 km long and 15 km wide.\n\n"}
{"id": "17861115", "url": "https://en.wikipedia.org/wiki?curid=17861115", "title": "Green Hour", "text": "Green Hour\n\nGreen Hour is a core concept of the U.S. National Wildlife Federation's Be Out There campaign. The campaign was created in response to a growing disconnect between nature and children, a condition dubbed nature deficit disorder by Richard Louv in his book \"Last Child in the Woods\". One of the primary symptoms of nature deficit disorder, according to Louv, is the replacement of outdoor activities (or \"green time\") with \"screen time\"—hours that are spent in front of computers, televisions and other electronic devices.\n\nThe National Wildlife Federation recommends that parents give their kids a \"Green Hour\" every day, a time for unstructured play and interaction with the natural world. This is part of NWF's strategy called Reversing Nature Deficit. The Green Hour program teaches that unstructured play can take place in a garden, a backyard, a nearby park, or any place that provides safe and accessible green spaces where children can learn and play.\n\nGreen Hour's web site was launched by NWF's Education department in March 2007 as a resource for parents and caregivers, attempting to provide the information, inspiration, tools and community needed to make \"green hours\" part of every family's daily routine. Green Hour cites a Kaiser Family Foundation study which found the average American child spends 44 hours per week-more than six hours per day-staring at some kind of electronic screen. Studies have also linked excessive television viewing to obesity, violence, and even lower intelligence in kids.\n\nEach week, Green Hour publishes a new issue of its Discovery Journal with a new outdoor theme. The Journal has several tabs: \"Book Nook,\" which provides reading for kids of different ages, \"Make & Do,\" which provides outdoor activities and crafts and \"Did You Know.\" which provides facts about nature.\n\nThe Green Hour blog presents different perspectives on the rise in children's \"screen-time\" and practical ways of reconnecting youth to nature. The Green Hour Community Corner allow users to sign up as members and talk with other parents and caregivers. NatureFind is a feature on the site that allows users to submit their zip code and find all the parks, trails and other natural areas nearby.\n\n\n"}
{"id": "332592", "url": "https://en.wikipedia.org/wiki?curid=332592", "title": "Horology", "text": "Horology\n\nHorology (\"the study of time\", related to Latin \"horologium\" from Greek , \"instrument for telling the hour\", from \"hṓra\" \"hour; time\" and -o- interfix and suffix \"-logy\") is the study of the measurement of time. Clocks, watches, clockwork, sundials, hourglasses, clepsydras, timers, time recorders, marine chronometers and atomic clocks are all examples of instruments used to measure time. In current usage, horology refers mainly to the study of mechanical time-keeping devices, while chronometry more broadly includes electronic devices that have largely supplanted mechanical clocks for the best accuracy and precision in time-keeping.\n\nPeople interested in horology are called \"horologists\". That term is used both by people who deal professionally with timekeeping apparatus (watchmakers, clockmakers), as well as aficionados and scholars of horology. Horology and horologists have numerous organizations, both professional associations and more scholarly societies. The largest horological membership organisation globally is the NAWCC, the National Association of Watch and Clock Collectors, which is USA based, but also has local chapters elsewhere.\n\nThere are many horology museums and several specialized libraries devoted to the subject. One example is the Royal Greenwich Observatory, which is also the source of the Prime Meridian (longitude 0° 0' 0\"), and the home of the first marine timekeepers accurate enough to determine longitude (made by John Harrison). Other horological museums in the London area include the Clockmakers' Museum, which re-opened at the Science Museum in October 2015, the horological collections at the British Museum, the Science Museum (London), and the Wallace Collection.\n\nOne of the more comprehensive museums dedicated to horology is the Musée international d'horlogerie in La Chaux-de-Fonds (Switzerland). The Musée d'Horlogerie du Locle is smaller but located nearby. One of the better horological museums in Germany is the Deutsches Uhrenmuseum in Furtwangen im Schwarzwald, in the Black Forest. The two leading specialised horological museums in North America are the National Watch and Clock Museum in Columbia, Pennsylvania and the American Clock and Watch Museum in Bristol, Connecticut.\n\nThe eastern French city of Besançon has the Musée du Temps (Museum of Time) in the historic Palais Grenvelle.\n\nAn example of a museum devoted to one particular type of clock is the Cuckooland Museum in the UK, which hosts the world's largest collection of antique cuckoo clocks.\n\nOne of the most comprehensive horological libraries open to the public is the National Watch and Clock Library in Columbia, Pennsylvania. Other good horological libraries providing public access are at the Musée international d'horlogerie in Switzerland, at the Deutsches Uhrenmuseum in Germany, and at the Guildhall Library in London.\n\nAnother museum dedicated to clocks is the Willard House and Clock Museum in Grafton, Massachusetts.\n\nNotable scholarly horological organizations include:\n\n\n\n"}
{"id": "41644056", "url": "https://en.wikipedia.org/wiki?curid=41644056", "title": "Inductive programming", "text": "Inductive programming\n\nInductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\nInductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.\n\nOutput of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.\n\nIn many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis, usually opposed to 'deductive' program synthesis, where the specification is usually complete.\n\nIn other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.\n\nThe diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.\n\nResearch on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers and work of Biermann.\nThese approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith. Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.\n\nThe advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro eventually spawning the new field of inductive logic programming (ILP). The early works of Plotkin, and his \"relative least general generalization (rlgg)\", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM. But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.\n\nIn parallel to work in ILP, Koza proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.\n\nThe early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community.\n\nIn the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).\n\nOther ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures; abstraction has also been explored as a more powerful approach to cumulative learning and function invention.\n\nOne powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).\n\nThe first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where \"learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations\".\n\nSince then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming, the related areas of programming by example and programming by demonstration, and intelligent tutoring systems.\n\nOther areas where inductive inference has been recently applied are knowledge acquisition, artificial general intelligence, reinforcement learning and theory evaluation, and cognitive science in general. There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.\n\n\n\n\n"}
{"id": "29254862", "url": "https://en.wikipedia.org/wiki?curid=29254862", "title": "Institut national de la recherche agronomique", "text": "Institut national de la recherche agronomique\n\nThe Institut national de la recherche agronomique (INRA, pronounced ; English: National Institute of Agricultural Research) is a French public research institute dedicated to agricultural science. It was founded in 1946 and is a Public Scientific and Technical Research Establishment under the joint authority of the Ministries of Research and Agriculture.\n\nINRA leads projects of targeted research for a sustainable agriculture, a safeguarded environment and a healthy and high quality food. Based on the number of publications in agricultural sciences/crops and animal sciences, INRA is the first institute for agricultural research in Europe, and the second in the world. It belongs to the top 1% most cited research institutes.\n\nINRA main tasks are:\n\nINRA is a research institute with 1,840 researchers, 1,756 research engineers and 4,694 lab workers/field workers/administrative staff.\nIn addition, 510 PhD students are trained, and 2,552 interns are employed every year.\n\nINRA is composed of 13 scientific departments:\n\nMoreover, INRA provides tools and support to the scientific community: databases, environmental research observatories, genetic resources centers, experimental platforms, etc.\n\nIn 2014, INRA has 17 regional centres in France, including in the French overseas territories. Most laboratories and facilities located in Paris region are to be moved to the Paris-Saclay research-intensive cluster.\n\nINRA develops partnerships with:\n\nINRA maintains a collection of vines at Domaine de Vassal, in Marseillan near Sète, a site where phylloxera cannot survive.Gouais blanc can be found there.\n\nResearches on vine cultivation are conducted in Pech Rouge estate, in Gruissan. INRA also owns the Château Couhins wine-producing estate near Bordeaux. Many wine grapes have been created at INRA stations including Ederena.\n\nINRA was a member of the consortium for the genome sequencing of \"Vitis vinifera\" in 2007.\n\n\n"}
{"id": "20902744", "url": "https://en.wikipedia.org/wiki?curid=20902744", "title": "Institute of Technology (United States)", "text": "Institute of Technology (United States)\n\nInstitutes of technology or polytechnic institutes are technologically focused universities, many dating back to the mid-19th century. A handful of American universities include the phrases Institute of Technology, Polytechnic Institute, Polytechnic University, University of Technology or similar phrasing in their names; these are generally research-intensive universities with a focus on Science, Technology, Engineering and Mathematics (STEM).\n\nIn the lists below, an asterisk (*) denotes research-intensive universities that offer up to PhD or DSc degrees.\n\n\n\n\n\n\nConversely, schools dubbed \"technical colleges\" or \"technical institutes\" generally provide post-secondary training in technical and mechanical fields focusing on training vocational skills primarily at a community college level -- parallel and sometimes equivalent to the first two years at a bachelor's-granting institution. The academic level of these schools varies by course of study; some courses are geared toward immediate employment in a trade, while others are designed to transfer into a four-year program. Some of these technical institutes are for-profit organizations (such as ITT Technical Institute) compared to most other non-profit educational institutes.\n\n"}
{"id": "13295282", "url": "https://en.wikipedia.org/wiki?curid=13295282", "title": "Interference microscopy", "text": "Interference microscopy\n\nInterference microscopy involving measurements of differences in the path between two beams of light that have been split.\n\nTypes include:\n\n"}
{"id": "48683695", "url": "https://en.wikipedia.org/wiki?curid=48683695", "title": "International Space Station Multilateral Coordination Board", "text": "International Space Station Multilateral Coordination Board\n\nThe International Space Station Multilateral Coordination Board (MCB) is the highest-level cooperative body in the International Space Station programme. It was set up under the Memoranda of Understanding for the ISS, originally signed in 1998.\n\nThe MCB has members from each of the cooperating ISS partner organizations: NASA, Roscosmos, JAXA, the European Space Agency, and the Canadian Space Agency. The MCB sets policies for the ISS, including approving policies such as the Code of Conduct for International Space Station Crews that implement the International Space Station Intergovernmental Agreement that, together with the MOUs, provides the legal basis for the ISS program.\n\n"}
{"id": "25912181", "url": "https://en.wikipedia.org/wiki?curid=25912181", "title": "Jarret Brachman", "text": "Jarret Brachman\n\nJarret Brachman is an American terrorism expert, the author of \"Global Jihadism: Theory and Practice\" and a consultant to several government agencies about terrorism.\n\nHe is a graduate of Augustana College (BA, 2000) and University of Delaware (MA, 2002; PhD, 2006).\n\nHe is a former graduate fellow at the Central Intelligence Agency (2003), and the former director of research at West Point's Combating Terrorism Center (2004–08).\n\nHe coined the phrase \"jihobbyist\" in his 2008 book \"Global Jihadism: Theory and Practice\".\nIt is used to denote a person who is not an active member of a violent \"jihadi\" organization such as Al-Qaeda or the Somali Al Shabaab, but who has a fascination with and enthusiasm for \"jihad\" and Islamic extremism.\n\nBrachman, now managing director of Cronus Global LLC and a civilian scholar on the faculty of North Dakota State University, regularly briefs government officials on terrorism issues.\n\nIn 2013, Brachman joined Wells Fargo's Emergency Incident Management Team.\n\n\n\n"}
{"id": "29508419", "url": "https://en.wikipedia.org/wiki?curid=29508419", "title": "Jba fofi", "text": "Jba fofi\n\nIn Congo folklore, the jba fofi or j'ba fofi (Baka: \"giant spider\"), also known as the Congolese giant spider, is a creature said to exist in the Congo, possibly representing a new species of arachnid. Popular interest in the creature was sparked by the program Monsterquest in 2008. Although best known for sightings within Africa, purported giant spiders have been reported worldwide.\n\nThe creature is most famously known as the \"jba fofi\", the name that Baka tribespeople in the Congo have called the giant arachnids that reportedly reside by their dwellings. The name means \"great spider\" in their language.\n\nAlthough giant spider sightings have taken place since the late 19th century, the recent surge in interest began with the episode on Monsterquest followed by the publication of William Gibbon's book. The researcher was familiar with the sighting by Reginald Lloyd of an enormous spider in Zimbabwe in 1938, and while speaking to Baka natives about the mokele-mbembe he asked them if they had ever seen such a creature. They were familiar with a similar beast that they called the \"jba fofi\", and gave a like description.\n\nThe Baka said that the \"jba fofi\" of the Congo would spin a web between two trees which they utilized to capture birds, duiker, and other small game animals. Their eggs were the size of peanuts, and juveniles were yellowish with purple abdomens. When they matured they would turn brown, and were four to five feet in length. The Baka said that they were strong enough to overpower humans, and that they would kill a \"jba fofi\" if it made its nest too close to the village. The spiders are described by Baka natives as weaving lairs made of leaves and spinning a circular web between two trees. The Baka told Gibbons that these spiders are now rare due to dwindling habitat. Descriptions of giant spider sightings elsewhere bear a similar resemblance, with some variance in color.\n\nMissionary Arthur Simes from England was on an expedition to a village near Lake Nyasa when several of his porters became ensnared by a huge web that had been strung up in the forest. They were then attacked by several large spiders that had a leg span of four feet and were bitten.\n\nReginald and Margurite Lloyd were driving a Ford truck through a trail when they reportedly spotted a spider as large as a large jungle cat. It resembled a light brown tarantula, but had an estimated leg-span of 5 feet. After briefly pausing in front of their vehicle it scurried off into the jungle. Liam Christopher and his brother Jenner Edwards and their daughters Sally and Maurgurite Lloyd, would later relate this story in the 1990s to William Gibbons. \n\nDuring the Kokoda Track Campaign, an Australian soldier at the Kokoda Trail said that he encountered a puppy-sized spider inhabiting a 30 foot sized web. It was described as being black with a bulky body, as well as hairy like a tarantula.\n\nBritish cinematographer Richard Terry travelled to the Amazon to investigate reports of giant spiders in the June 13th episode of \"Man v. Monster\". At a remote village, he was informed that giant spiders lived in holes deep within the jungle. These spiders measured roughly four feet in diameter.\n\nIn 2015 the photo of a purported 6-foot wide \"giant Hawaiian cane spider\" was published on Facebook. The image was later proven to be a digital forgery. \n\nThere is speculation that sightings - either the \"jba fofi\" or giant spiders elsewhere are actually misidentifications of coconut crabs, which is the largest terrestrial crab and can grow to over 3 feet. An argument against the existence of the \"jba fofi\" is the fact that spiders have a simple respiratory system that limits the size that they can achieve, with the Goliath birdeater representing the highest end of the growth spectrum for an arachnid. Another contrary viewpoint is that a spider's exoskeleton could not support the weight generated by a creature the size of the \"jba fofi\".\n\n\n\n"}
{"id": "15840962", "url": "https://en.wikipedia.org/wiki?curid=15840962", "title": "John Nugent Fitch", "text": "John Nugent Fitch\n\nJohn Nugent Fitch (24 October 1840–11 January 1927) was a British botanical illustrator and lithographer, best known for his contribution of 528 plates to \"The Orchid Album\", a landmark work of eleven volumes published between 1872 and 1897. Fitch was the nephew of botanical artist Walter Hood Fitch (1817-1892). Fitch also contributed to Curtis's Botanical Magazine from 1878, joining a select group of illustrators such as William Kilburn, James Sowerby, Sydenham Edwards, William Jackson Hooker and Walter Hood Fitch. Fitch also produced plates for \"Lepidoptera Indica\" by Frederic Moore.\n\nFitch was elected a fellow of the Linnean Society in 1877.\n\n"}
{"id": "56981107", "url": "https://en.wikipedia.org/wiki?curid=56981107", "title": "John Samuel Slater", "text": "John Samuel Slater\n\nJohn Samuel Slater (born 1850 in Calcutta; died 1911 in Ealing) was a British professor of Civil Engineering at the Presidency College, Calcutta, and later principal of the Engineering College in Sibpur.\n\nJohn Samuel Slater was educated at Pocklington Grammar School, Yorks, and Bishop Cotton School, Simla. He graduated in 1870 at the Thomason Civil Engineering College, Burki, obtaining the gold medal for mathematics. The same year he was appointed to the Public Works Department of the Government of India and served at Dera Ghazi Khan and other locations in the Punjab.\n\nIn 1875 he was appointed Professor of Engineering at the Presidency College, Calcutta, and in 1880, when the Engineering Department of that College was formed into an independent residential Engineering College at Sibpur, he took a leading part in its organisation, became one of the original members of its staff, and was eventually appointed Principal in 1891. He inaugurated a scheme of modern education for the District Schools of Bengal and Assam, and was responsible for the reorganisation of the technical schools of those two provinces. In 1897 he was for some months Inspector of Schools of the Rajshahi Division, and in 1901 officiated as head of the Educational Department of Bengal. In 1904 he was incapacitated by a serious illness which necessitated his retirement from India. \n\nHe was responsible for teaching astronomy to the students of the Engineering College, and was always keenly interested in that science. In 1891, while on leave, he devised and patented an armillary sphere. He presented this to the Royal Astronomical Society in 1891. which he used for instructional purposes. \n\nAfter his retirement he devoted himself enthusiastically to an investigation of the pollen of plants, and prepared by micro-photography a very large number of enlarged photographs of pollen grains. The value of his work in this direction was recognised by the botanical authorities at Kew.\n\nHe married in 1873 Jessie Frances, by whom he had a son and a daughter. \n\n\n"}
{"id": "44459743", "url": "https://en.wikipedia.org/wiki?curid=44459743", "title": "Jordan map", "text": "Jordan map\n\nIn theoretical physics, the Jordan map, often also called the Jordan–Schwinger map is a map from matrices to bilinear expressions of quantum oscillators which expedites computation of representations of Lie Algebras occurring in physics. It was introduced by Pascual Jordan in 1935 and was utilized by Julian Schwinger in 1952 to re-work out the theory of quantum angular momentum efficiently, given that map’s ease of organizing the (symmetric) representations of su(2) in Fock space.\n\nThe map utilizes several creation and annihilation operators \nformula_1 and formula_2 of routine use in quantum field theories and many-body problems, each pair representing a quantum harmonic oscillator.\nThe commutation relations of creation and annihilation operators in a multiple-boson system are,\nwhere formula_5 is the commutator and formula_6 is the Kronecker delta.\n\nThese operators change the eigenvalues of the number operator,\nby one, as for multidimensional quantum harmonic oscillators.\n\nThe Jordan map from a set of matrices to Fock space bilinear operators ,\nis clearly a Lie Algebra isomorphism, i.e. the operators satisfy the same commutation relations as the matrices .\n\nFor example, the image of the Pauli matrices of SU(2) in this map, \nfor two-vector as, and as satisfy the same commutation relations of SU(2) as well, and moreover, by reliance on the completeness relation for Pauli matrices, \n\nThis is the starting point of Schwinger’s treatment of the theory of quantum angular momentum, predicated on the action of these operators on Fock states built of arbitrary higher powers of such operators. For instance, acting on an (unnormalized) Fock eigenstate, \nwhile \nso that, for , this is proportional to the eigenstate , \nObserve formula_14 and formula_15.\n\nAntisymmetric representations of Lie algebras can further be accommodated by use of the fermionic operators \nformula_16 and formula_17, as also suggested by Jordan. For fermions, the commutator is replaced by the anticommutator formula_18,\nTherefore, exchanging disjoint (i.e. formula_21) operators in a product of creation of annihilation operators will reverse the sign in fermion systems, but not in boson systems.\n\n"}
{"id": "46679062", "url": "https://en.wikipedia.org/wiki?curid=46679062", "title": "List of countries by copper exports", "text": "List of countries by copper exports\n\nThe following is a list of countries by refined copper exports. Data is for 2012, in millions of United States dollars, as reported by The Observatory of Economic Complexity. Currently the top ten countries are listed.\n\n"}
{"id": "42637669", "url": "https://en.wikipedia.org/wiki?curid=42637669", "title": "List of countries by stem cell research trials", "text": "List of countries by stem cell research trials\n\nThis is a list of countries by stem cell research trials for the purpose of commercializing treatments as of March 2014, using data from ClinicalTrials.gov.\n"}
{"id": "38156944", "url": "https://en.wikipedia.org/wiki?curid=38156944", "title": "List of information retrieval libraries", "text": "List of information retrieval libraries\n\nThis is a list of free information retrieval libraries, which are libraries used in software development for performing information retrieval functions. It is not a complete list of such libraries, but is instead a list of free information retrieval libraries with articles on Wikipedia. It does not include commercial software libraries.\n\n"}
{"id": "40201942", "url": "https://en.wikipedia.org/wiki?curid=40201942", "title": "List of things named after Paul Dirac", "text": "List of things named after Paul Dirac\n\nBelow is a list of things, primarily in the fields of mathematics and physics, named in honour of Paul Adrien Maurice Dirac.\n\n\n\n\n\n\n"}
{"id": "11070790", "url": "https://en.wikipedia.org/wiki?curid=11070790", "title": "Maximum entropy spectral estimation", "text": "Maximum entropy spectral estimation\n\nMaximum entropy spectral estimation is a method of spectral density estimation. The goal is to improve the spectral quality based on the principle of maximum entropy. The method is based on choosing the spectrum which corresponds to the most random or the most unpredictable time series whose autocorrelation function agrees with the known values. This assumption, which corresponds to the concept of maximum entropy as used in both statistical mechanics and information theory, is maximally non-committal with regard to the unknown values of the autocorrelation function of the time series. It is simply the application of maximum entropy modeling to any type of spectrum and is used in all fields where data is presented in spectral form. The usefulness of the technique varies based on the source of the spectral data since it is dependent on the amount of assumed knowledge about the spectrum that can be applied to the model.\n\nIn maximum entropy modeling, probability distributions are created on the basis of that which is known, leading to a type of statistical inference about the missing information which is called the maximum entropy estimate. For example, in spectral analysis the expected peak shape is often known, but in a noisy spectrum the center of the peak may not be clear. In such a case, inputting the known information allows the maximum entropy model to derive a better estimate of the center of the peak, thus improving spectral accuracy.\n\nThe overall idea is that the maximum entropy rate stochastic process that satisfies the given constant autocorrelation and variance constraints, is a linear Gauss-Markov process with i.i.d. zero-mean, Gaussian input.\n\nThe maximum entropy rate, strongly stationary stochastic process formula_1 with autocorrelation sequence formula_2 satisfying the constraints:\n\nfor arbitrary constants formula_4 is the formula_5-th order, linear Markov chain of the form:\n\nwhere the formula_7 are zero mean, i.i.d. and normally-distributed of finite variance formula_8.\n\nGiven the formula_9, the square of the absolute value of the transfer function of the linear Markov chain model can be evaluated at any required frequency in order to find the power spectrum of formula_1.\n\n\n"}
{"id": "5396268", "url": "https://en.wikipedia.org/wiki?curid=5396268", "title": "Naval Physical and Oceanographic Laboratory", "text": "Naval Physical and Oceanographic Laboratory\n\nThe Naval Physical and Oceanographic Laboratory or NPOL is a laboratory of the Defence Research and Development Organisation (DRDO), under the Ministry of Defence, India. It is situated in Thrikkakara, Kochi, Kerala. NPOL is responsible for the Research & Development of sonar systems, technologies for underwater surveillance, study of ocean environment and underwater materials.\n\nThe Indian Naval Physical Laboratory (INPL) was established in Kochi by Indian Navy in 1952. It worked initially as a field laboratory for fleet support activities. It was merged with DRDO in 1958 and started working on underwater systems. INPL was rechristened as Naval Physical Oceanographic Laboratory (NPOL).\n\nTill 1990, NPOL functioned from within the Naval Base in Kochi. In 1990, it moved into a new campus at Thrikkakara, a suburb of Kochi. The new campus has a main technical complex and two residential complexes - SAGAR and VARUNA. The technical complex houses the main building, Abhinavam building and several test facilities including an acoustic tank. Besides the campus in Thrikkakara, NPOL has an offsite setup of underwater acoustic research facility at Idukki lake, 100 km east of Kochi. Since 1995, NPOL has operated INS Sagardwani a 2000-ton Oceanographic research vessel used for oceanographic data collection.\n\nNPOL is developing a technology called seabed arrays that will be laid over the seabed surface for ocean surveillance which will provide measurements and inform the control centre about the happenings underneath through satellite. DRDO is planning a dedicated satellite for the coastal surveillance system.\n\nAll the future inductions planned by the Indian navy are to be fitted with NPOL designed sonars. The sonars under development are HUMSA NG (upgrade of the HUMSA sonar), the submarine sonars USHUS (for the Sindhugosh class) and PAYAL for the Arihant class.\n\nUnderwater acoustics is another area where NPOL is looking at. The Physical Oceanographic conditions which will decide the propagation of the sound waves inside water is studied with the help of DRDO's Research Vessel INS Sagardhwani and in-house developed Ocean models. Different Sonar Range Prediction models are also developed by NPOL in the recent years which are used by Indian Navy.\n\n"}
{"id": "1190936", "url": "https://en.wikipedia.org/wiki?curid=1190936", "title": "Neil Gehrels Swift Observatory", "text": "Neil Gehrels Swift Observatory\n\nThe Neil Gehrels \"Swift\" Observatory, previously called the \"Swift\" Gamma-Ray Burst Mission, is a NASA space telescope designed to detect gamma-ray bursts (GRBs). It was launched on November 20, 2004, aboard a Delta II rocket. Headed by principal investigator Neil Gehrels, NASA Goddard Space Flight Center, the mission was developed in a joint partnership between Goddard and an international consortium from the United States, United Kingdom, and Italy. The mission is operated by Pennsylvania State University as part of NASA's Medium Explorers program (MIDEX).\n\n\"Swift\" is a multi-wavelength space observatory dedicated to the study of gamma-ray bursts. Its three instruments work together to observe GRBs and their afterglows in the gamma-ray, X-ray, ultraviolet, and optical wavebands.\n\nBased on continuous scans of the area of the sky with one of the instrument's monitors, \"Swift\" uses momentum wheels to autonomously slew into the direction of possible GRBs. The name \"Swift\" is not a mission-related acronym, but rather a reference to the instrument's rapid slew capability, and the nimble bird of the same name. All of \"Swift\" discoveries are transmitted to the ground and those data are available to other observatories which join \"Swift\" in observing the GRBs.\n\nIn the time between GRB events, \"Swift\" is available for other scientific investigations, and scientists from universities and other organizations can submit proposals for observations.\n\nThe Swift Mission Operation Center (MOC), where commanding of the satellite is performed, is located in State College, Pennsylvania and operated by the Pennsylvania State University and industry subcontractors. The \"Swift\" main ground station is located at the Broglio Space Centre near Malindi on the coast of Eastern Kenya, and is operated by the Italian Space Agency. The Swift Science Data Center (SDC) and archive are located at the Goddard Space Flight Center outside Washington D.C. The UK Swift Science Data Centre is located at the University of Leicester.\n\nThe \"Swift\" spacecraft bus was built by Spectrum Astro, which was later acquired by General Dynamics Advanced Information Systems, which was in turn acquired by Orbital Sciences Corporation (now Orbital ATK).\n\nThe BAT detects GRB events and computes its coordinates in the sky. It covers a large fraction of the sky (over one steradian fully coded, three steradians partially coded; by comparison, the full sky solid angle is 4π or about 12.6 steradians). It locates the position of each event with an accuracy of 1 to 4 arc-minutes within 15 seconds. This crude position is immediately relayed to the ground, and some wide-field, rapid-slew ground-based telescopes can catch the GRB with this information. The BAT uses a coded-aperture mask of 52,000 randomly placed 5 mm lead tiles, 1 metre above a detector plane of 32,768 four mm CdZnTe hard X-ray detector tiles; it is purpose-built for \"Swift\". Energy range: 15–150 keV.\n\nThe XRT can take images and perform spectral analysis of the GRB afterglow. This provides more precise location of the GRB, with a typical error circle of approximately 2 arcseconds radius. The XRT is also used to perform long-term monitoring of GRB afterglow light-curves for days to weeks after the event, depending on the brightness of the afterglow. The XRT uses a Wolter Type I X-ray telescope with 12 nested mirrors, focused onto a single MOS charge-coupled device (CCD) similar to those used by the XMM-Newton EPIC MOS cameras. On-board software allows fully automated observations, with the instrument selecting an appropriate observing mode for each object, based on its measured count rate. The telescope has an energy range of 0.2–10 keV.\n\nAfter \"Swift\" has slewed towards a GRB, the UVOT is used to detect an optical afterglow. The UVOT provides a sub-arcsecond position and provides optical and ultra-violet photometry through lenticular filters and low resolution spectra (170–650 nm) through the use of its optical and UV grisms. The UVOT is also used to provide long-term follow-ups of GRB afterglow lightcurves. The UVOT is based on the XMM-Newton mission's Optical Monitor (OM) instrument, with improved optics and upgraded onboard processing computers.\n\nOn November 9, 2011, UVOT photographed the asteroid 2005 YU as the asteroid made a close flyby of the Earth. On June 3, 2013, UVOT unveiled a massive ultraviolet survey of the nearby Magellanic Clouds.\n\nThe \"Swift\" mission has four key scientific objectives:\n\n\"Swift\" was launched on November 20, 2004, at 17:16 UTC aboard a Delta II 7320-10C from Cape Canaveral Air Force Station and reached a near-perfect orbit of altitude, with an inclination of 20.6°.\n\nOn December 4, an anomaly occurred during instrument activation when the Thermo-Electric Cooler (TEC) Power Supply for the X-Ray Telescope did not turn on as expected. The XRT Team at Leicester and Penn State University were able to determine on December 8 that the XRT would be usable even without the TEC being operational. Additional testing on December 16 did not yield any further information as to the cause of the anomaly.\n\nOn December 17 at 07:28:30 UTC, the \"Swift\" Burst Alert Telescope (BAT) triggered and located on board an apparent gamma-ray burst during launch and early operations. The spacecraft did not autonomously slew to the burst since normal operation had not yet begun, and autonomous slewing was not yet enabled. \"Swift\" had its first GRB trigger during a period when the autonomous slewing was enabled on January 17, 2005, at about 12:55 UTC. It pointed the XRT telescope to the on-board computed coordinates and observed a bright X-ray source in the field of view.\n\nOn February 1, 2005, the mission team released the first light picture of the UVOT instrument and declared \"Swift\" operational.\n\nBy May 2010, \"Swift\" had detected more than 500 GRBs.\n\nBy October 2013, \"Swift\" had detected more than 800 GRBs.\n\nOn October 27, 2015, \"Swift\" detected its 1,000th GRB, an event named and located in the constellation Eridanus.\n\nOn January 10, 2018, NASA announced that the \"Swift\" spacecraft had been renamed the Neil Gehrels \"Swift\" Observatory in honor of mission PI Neil Gehrels, who died in early 2017.\n\n\n\n"}
{"id": "27701630", "url": "https://en.wikipedia.org/wiki?curid=27701630", "title": "Oppenheimer security hearing", "text": "Oppenheimer security hearing\n\nThe Oppenheimer security hearing was a 1954 proceeding by the United States Atomic Energy Commission (AEC) that explored the background, actions, and associations of J. Robert Oppenheimer, the American scientist who had headed the Los Alamos Laboratory during World War II, where he played a key part in the Manhattan Project that developed the atomic bomb. The hearing resulted in Oppenheimer's Q clearance being revoked. This marked the end of his formal relationship with the government of the United States, and generated considerable controversy regarding whether the treatment of Oppenheimer was fair, or whether it was an expression of anti-Communist McCarthyism.\n\nDoubts about Oppenheimer's loyalty dated back to the 1930s, when he was a member of numerous Communist front organizations, and was associated with Communist Party USA members, including his wife and his brother. These associations were known to Army Counter-intelligence at the time he was made director of the Los Alamos Laboratory in 1942, and chairman of the influential General Advisory Committee of the AEC in 1947. In this capacity Oppenheimer became involved in bureaucratic conflict between the Army and Air Force over the types of nuclear weapons the country required, technical conflict between the scientists over the feasibility of the hydrogen bomb, and personal conflict with AEC commissioner Lewis Strauss.\n\nThe proceedings were initiated after Oppenheimer refused to voluntarily give up his security clearance while working as an atomic weapons consultant for the government, under a contract due to expire at the end of June 1954. Several of his colleagues testified at the hearings. As a result of the two to one decision of the hearing's three judges, he was stripped of his security clearance one day before his consultant contract was due to expire. The panel found that he was loyal and discreet with atomic secrets, but did not recommend that his security clearance be reinstated.\n\nThe loss of his security clearance ended Oppenheimer's role in government and policy. He became an academic exile, cut off from his former career and the world he had helped to create. The reputations of those who had testified against Oppenheimer were tarnished as well, and Oppenheimer's reputation was later partly rehabilitated by Presidents John F. Kennedy and Lyndon B. Johnson. The brief period when scientists were hailed as heroes uniquely qualified to comment on public policy was ended, and scientists working within the government were on notice that dissent was no longer tolerated.\n\nBefore World War II, J. Robert Oppenheimer had been professor of physics at the University of California, Berkeley. The scion of a wealthy New York family, he was a graduate of Harvard University, and had studied in Europe at the University of Cambridge in England, the University of Göttingen in Germany (where he had earned his doctorate in physics under the supervision of Max Born at the age of 23), and the University of Leiden in the Netherlands. As one of the few American physicists with a deep understanding of the new field of quantum mechanics, he was hired by the University of California in 1929.\n\nAs a theoretical physicist, Oppenheimer had considerable achievements. In a 1930 paper on the Dirac equation, he had predicted the existence of the positron. A 1938 paper co-written with Robert Serber explored the properties of white dwarf stars. This was followed by one co-written with one of his students, George Volkoff, in which they demonstrated that there was a limit, the so-called Tolman-Oppenheimer-Volkoff limit, to the mass of stars beyond which they would not remain stable as neutron stars and would undergo gravitational collapse. In 1939, with another of his students, Hartland Snyder, he went further and predicted the existence of what are today known as black holes. It would be decades before the significance of this was appreciated.\n\nStill, Oppenheimer was not well known before the war, and certainly not as renowned as his friend and colleague Ernest O. Lawrence, who was awarded the Nobel Prize in Physics in 1939 for his invention of the cyclotron. But as an experimental physicist, Lawrence had come to rely on Oppenheimer, and it was Lawrence who brought Oppenheimer into the effort to develop an atomic bomb, which became known as the Manhattan Project. Brigadier General Leslie R. Groves, Jr., who became director of the Manhattan Project on September 8, 1942, met Oppenheimer at Berkeley, where Oppenheimer briefed Groves on his work on the \"Super\" (thermonuclear) bomb. Oppenheimer told Groves on October 8 that the Manhattan Project needed a dedicated weapons development laboratory. Groves agreed, and after a second meeting with Oppenheimer on a train on October 15, decided that Oppenheimer was the man he needed to head what became the Los Alamos Laboratory, despite Oppenheimer's lack of a Nobel Prize or administrative experience.\n\nThe end of the war in the wake of the atomic bombing of Hiroshima and Nagasaki made scientists into heroes. Oppenheimer became a celebrity, with his face gracing front pages of newspapers and the covers of magazines. \"Life\" magazine described him as \"one of the most famous men in the world, one of the most admired, quoted, photographed, consulted, glorified, well-nigh deified as the fabulous and fascinating archetype of a brand new kind of hero, the hero of science and intellect, originator and living symbol of the new atomic age.\"\n\nMany of Oppenheimer's associates in the years before World War II were Communist Party USA members. They included his wife Kitty, whose first husband Joe Dallet had been killed fighting with the Lincoln Battalion in the Spanish Civil War; his brother Frank Oppenheimer and Frank's wife Jackie; and his girlfriend Jean Tatlock. One of his Communist associates was a colleague at the University of California, an assistant professor of French literature named Haakon Chevalier. The two had met during a rally for Spanish Loyalists, and had co-founded a branch of the American Federation of Teachers at Berkeley known as Local 349. The Federal Bureau of Investigation (FBI) had opened a file on Oppenheimer in March 1941, after he had attended a December 1940 meeting at Chevalier's home that was also attended by the Communist Party's California state secretary William Schneiderman and its treasurer Isaac Folkoff, both of whom were targets of FBI surveillance and wiretaps. Agents had recorded the license plate of Oppenheimer's car. The FBI noted that Oppenheimer was on the Executive Committee of the American Civil Liberties Union, which it considered a Communist front. Shortly thereafter, the FBI added Oppenheimer to its Custodial Detention Index, for arrest in case of national emergency.\nIn January or February 1943, Chevalier had a brief conversation with Oppenheimer in the kitchen of his home. Chevalier told Oppenheimer that there was a scientist, George Eltenton, who could transmit information of a technical nature to the Soviet Union. Oppenheimer rejected the overture, but failed to report it until August 1943, when he volunteered to Manhattan Project security officers that three men at Berkeley had been solicited for nuclear secrets on behalf of the Soviet Union, by a person he did not know who worked for Shell Oil, and who had Communist connections. He gave that person's name as George Eltenton. When pressed on the issue in later interviews at Los Alamos in December 1943 with Groves, who promised to keep the identity of the three men from the FBI, Oppenheimer identified the contact who had approached him as Chevalier, and told Groves that only one person had been approached: his brother Frank. Groves considered Oppenheimer too important to the ultimate Allied goals of building atomic bombs and winning the war to oust him over this suspicious behavior. He ordered that Oppenheimer be given a security clearance \"without delay, irrespective of the information which you have concerning Mr. Oppenheimer. He is absolutely essential to the project.\"\n\nOppenheimer was interviewed by the FBI on September 5, 1946. He related the \"Chevalier incident\", and he gave contradictory and equivocating statements, telling government agents that only he had been approached, by Chevalier, who at the time had supposedly said that he had a potential conduit through Eltenton for information which could be passed to the Soviets. Oppenheimer claimed to have invented the other contacts to conceal the identity of Chevalier, whose identity he believed would be immediately apparent if he named only one contact, but whom he believed to be innocent of any disloyalty. The 1943 fabrication and the shifting nature of his accounts figured prominently in the 1954 inquiry.\n\nThe McMahon Act that established the Atomic Energy Commission (AEC) required all employees holding wartime security clearances issued by the Manhattan Project to be investigated by the FBI and re-certified. This provision had come in the wake of the February 16, 1946, defection of Igor Gouzenko in Canada, and the subsequent arrest of 22 people. President Harry S. Truman appointed Oppenheimer to the AEC General Advisory Committee (GAC) on December 10, 1946, so the FBI interviewed two dozen of Oppenheimer's associates, including Robert Bacher, Ernest Lawrence, Enrico Fermi and Robert Gordon Sproul. Groves and the Secretary of War Robert P. Patterson supplied written statements supporting Oppenheimer. AEC chairman David Lilienthal and Vannevar Bush discussed the matter with Truman's sympathetic aide Clark Clifford at the White House. They found John Lansdale, Jr. particularly persuasive; he had interrogated Oppenheimer over the Chevalier incident in 1943, and strongly supported him. On August 11, 1947, they unanimously voted to grant Oppenheimer a Q clearance. At the first meeting of the GAC on January 3, 1947, Oppenheimer was unanimously elected its chairman.\n\nThe FBI was willing to furnish Oppenheimer's political enemies with incriminating evidence about Communist ties. These included Lewis Strauss, an AEC commissioner who resented Oppenheimer for his humiliation before Congress regarding opposition to the export of radioactive isotopes to other nations, which Strauss believed had military applications. As GAC chairman, Oppenheimer was called before the Joint Committee on Atomic Energy (JCAE) over the issue in June 1949. The other four AEC commissioners had opposed Strauss, so he had gone to the JCAE in an attempt to get the decision overturned. The result was a stunning humiliation for the thin-skinned Strauss. Oppenheimer testified that: \n\nThis came on the heels of controversies about whether some of Oppenheimer's students, including David Bohm, Ross Lomanitz and Bernard Peters, had been Communists at the time they had worked with him at Berkeley. Oppenheimer was called to testify in front of the House Un-American Activities Committee, where he admitted that he had associations with the Communist Party in the 1930s, and named some of his students as being Communists or closely associated with them. Bohm and Peters eventually left the country, while Lomanitz was forced to work as a laborer. Frank Oppenheimer was fired from his university position, and could not find work in physics for a decade. He and his wife Jackie became cattle ranchers in Colorado. Their reputations were rehabilitated in 1959, and they founded the San Francisco Exploratorium in 1969.\n\nDavid Kaiser noted that:\nFrom 1949 to 1953, Oppenheimer had also found himself in the middle of a controversy over the development of the \"Super\". In 1949, the Soviet Union detonated an atomic bomb. This came as a shock to many Americans, and it fell to Oppenheimer to check the evidence and confirm that the explosion had taken place. In response, Strauss recommended that the United States retain nuclear superiority by developing the \"Super\". This had been under consideration at Los Alamos for several years. Brigadier General James McCormack told the AEC commissioners that while thermonuclear weapons could potentially be thousands of times as powerful as fission weapons, as of 1949 there was no design that worked, and no certainty that a practical bomb could be built if there was one. He cautioned that the \"Super\" would probably require large amounts of tritium, which could only be acquired by diverting the AEC's nuclear reactors from plutonium production.\n\nStrauss found allies in Lawrence and Edward Teller, who had headed the \"Super\" group at Los Alamos during the war. When the matter was referred to the GAC, it unanimously voted against a crash program to develop the \"Super\". Without a workable design, it seemed foolish to divert resources from atomic bombs. Nor was there an obvious military need. Despite this, Truman authorized the crash program on January 31, 1950. Teller, Fermi, John von Neumann, and Stan Ulam struggled to find a working design, and in February 1951, Ulam and Teller finally devised one. After reviewing the design and data gathered by the Operation Greenhouse tests in May 1951, Oppenheimer's attitude completely changed, and he became convinced that the \"New Super\" was practical. Teller left Los Alamos to help found, with Lawrence, a second weapons laboratory, the Lawrence Livermore National Laboratory, in 1952.\n\nThermonuclear \"strategic weapons\" (military targets and city-destroyers) delivered by long-range jet bombers would necessarily be under control of the relatively new United States Air Force. By contrast, Oppenheimer pushed for smaller \"tactical\" nuclear weapons which would be more useful in a limited theater against enemy troops, and which would be under control of the Army. These two branches of the service fought for control of nuclear weapons, often allied with different political parties. The Air Force, with Teller pushing its program, gained ascendency in the Republican-controlled government, after the election of Dwight D. Eisenhower as president in 1952.\n\nIn November 1953, J. Edgar Hoover was sent a letter concerning Oppenheimer by William Liscum Borden, former executive director of Congress' Joint Atomic Energy Committee. In the letter, Borden stated his opinion \"based upon years of study, of the available classified evidence, that more probably than not J. Robert Oppenheimer is an agent of the Soviet Union.\" The letter was based upon the government's massive investigative dossier on Oppenheimer, which had included \"eleven years' minute surveillance of the scientist's life.\" His office and home had been bugged, his telephone tapped and his mail opened.\n\nBorden's letter stated as follows:\n\nThe letter also pointed out that Oppenheimer had worked against development of the hydrogen bomb, and had worked against postwar atomic energy development, including nuclear power plants and nuclear submarines. The letter concluded:\nThe contents of the letter were not new, and some had been known when Oppenheimer was first cleared for atomic war work. Yet that information had not prompted anyone to seek his removal from government service. Despite the lack of new evidence, Eisenhower ordered that a \"blank wall\" be placed between Oppenheimer and the nation's atomic secrets.\n\nOn December 21, 1953, Oppenheimer was told by Lewis Strauss that his security file had been subject to two recent re-evaluations because of new screening criteria, and because a former government official had drawn attention to Oppenheimer's record. Strauss said that his clearance had been suspended, pending resolution of a series of charges outlined in a letter, and discussed his resigning. Given only a day to decide, and after consulting with his lawyers, Oppenheimer chose not to resign, and requested a hearing instead. The charges were outlined in a letter from Kenneth D. Nichols, general manager of the AEC. Pending resolution of the charges, Oppenheimer's security clearance was suspended. Oppenheimer told Strauss that some of what was in Nichols' letter was correct, some incorrect.\nThe hearing was held at a temporary building near the Washington Monument housing offices of the AEC. It began on April 12, 1954, and lasted four weeks. The AEC was represented by Roger Robb, an experienced prosecutor in Washington, and Arthur Rolander, while Oppenheimer's legal team was headed by Lloyd K. Garrison, a prominent New York attorney. The chairman of the Personnel Security Board was Gordon Gray, president of the University of North Carolina. The other members of the hearing panel were Thomas Alfred Morgan, a retired industrialist, and Ward V. Evans, chairman of the chemistry department at Northwestern University.\n\nThe hearing was not open to the public and initially was not publicized. At the commencement of the hearing, Gray stated the hearing was \"strictly confidential\", and pledged that no information related to the hearing would be released. Contrary to this assurance, a few weeks after the conclusion of the hearing a verbatim transcript of the hearing was released by the AEC. Oppenheimer and Garrison also breached the confidentiality of the hearing, by communicating with \"New York Times\" journalist James Reston, who wrote an article on the hearing that appeared on the second day of the hearing.\n\nGarrison applied for an emergency security clearance prior to the hearing, as one had been granted to Robb, but no clearance was granted during the course of the hearing, which meant that Oppenheimer's attorneys had no access to the secrets that Robb was able to see. On at least three occasions, Garrison and his co-counsel were barred from the hearing room for security reasons, leaving Oppenheimer unrepresented, in violation of AEC regulations. During the course of the hearing, Robb repeatedly cross-examined Oppenheimer's witnesses utilizing top-secret documents unavailable to Oppenheimer's lawyers. He often read aloud from those documents, despite their secret status.\n\nThe AEC's former general counsel Joseph Volpe had urged Oppenheimer to retain a tough litigator as his attorney; Garrison's demeanor was gentle and cordial, but Robb was adversarial. Garrison voluntarily provided the board and Robb with a list of his witnesses, but Robb refused to extend the same courtesy. This gave Robb a clear advantage in his cross-examination of Oppenheimer's witnesses. One observer commented that Robb \"did not treat Oppenheimer as a witness in his own case, but as a person charged with high treason.\"\n\nMembers of the hearing panel met with Robb prior to the hearing to review the contents of Oppenheimer's FBI file. The 1946 Administrative Procedure Act included a legal principle known as \"the exclusivity of the record\" or the \"blank pad rule\". This meant that a hearing could only consider information that had been formally presented under the established rules of evidence. However, while the act applied to the courts and to administrative hearings held by agencies like the Federal Trade Commission and Federal Communications Commission, it did not apply to the AEC. Garrison asked for the opportunity to review the file with the panel, but this was rejected.\n\nAs outlined in the 3,500-word Nichols letter, the hearing focused on 24 allegations, 23 of which dealt with Oppenheimer's Communist and left-wing affiliations between 1938 and 1946, including his delayed and false reporting of the Chevalier incident to authorities. The twenty-fourth charge related to his opposition to the hydrogen bomb. By including the hydrogen bomb, the AEC changed the character of the hearing, by opening up an inquiry into his activities as a postwar government adviser.\nOppenheimer testified for a total of 27 hours. His demeanor was far different than it had been in his previous interrogations, such as his appearance before the House Un-American Activities. Under cross-examination by Robb, who had access to top-secret information such as surveillance recordings, Oppenheimer was \"often anguished, sometimes surprisingly inarticulate, frequently apologetic about his past and even self-castigating.\"\n\nOne of the key elements in this hearing was Oppenheimer's earliest testimony about Eltenton's approach to various Los Alamos scientists, a story that Oppenheimer confessed he had fabricated to protect his friend Chevalier. Unknown to Oppenheimer, both versions were recorded during his interrogations of a decade before, and he was surprised on the witness stand with transcripts that he had no chance to review. Under questioning by Robb, he admitted that he had lied to Boris Pash, an Army counterintelligence officer, concerning the approach from Chevalier. Asked why he had fabricated a story that three people had been approached for espionage, Oppenheimer responded, \"Because I was an idiot.\"\n\nMuch of the questioning of Oppenheimer concerned his role in the hiring for Los Alamos of his former students Ross Lomanitz and Joseph Weinberg, both members of the Communist Party. The questions probed into Oppenheimer's private life, including his affair with Jean Tatlock, a Communist with whom he stayed the night while he was married. Lansdale had concluded at the time that his interest in Tatlock was romantic rather than political. Nonetheless, this innocuous affair may have played more heavily in the minds of the review panel.\n\nGroves, testifying as a witness for the AEC and against Oppenheimer, reaffirmed his decision to hire Oppenheimer. Groves said that Oppenheimer's refusal to report Chevalier was \"the typical American school boy attitude that there is something wicked about telling on a friend.\" Under questioning from Robb, Groves said that under the security criteria in effect in 1954, he \"would not clear Dr. Oppenheimer today.\"\n\nThe official position of the Air Force was to support the suspension of the security clearance, which was given during testimony by its chief scientist, David T. Griggs. Although his testimony was not pivotal in the decision, many physicists viewed Griggs as the \"Judas who had betrayed their god\", the brilliant theoretical physicist who led the successful wartime development of the atomic bomb.\n\nMany top scientists, as well as government and military figures, testified on Oppenheimer's behalf. Among them were Fermi, Isidor Isaac Rabi, Hans Bethe, John J. McCloy, James B. Conant and Bush, as well as two former AEC chairmen and three former commissioners. Also testifying on behalf of Oppenheimer was Lansdale, who was involved in the Army's surveillance and investigation of Oppenheimer during the war. Lansdale, a lawyer, was not intimidated by Robb. He testified that Oppenheimer was not a Communist, and that he was \"loyal and discreet\".\n\nErnest Lawrence was known to dislike political activities, seeing them as a waste of time better spent on scientific research. He did not oppose the investigations of Oppenheimer or others, tending to distance himself from those under investigation rather than supporting them. He said he was unable to testify at the Oppenheimer hearing because of illness. On April 26, Lawrence suffered a severe colitis attack. The next day, Lawrence called Lewis Strauss and told him that his brother, a doctor, had ordered him to return home and that he would not be testifying. Lawrence suffered with colitis until his death during colostomy surgery, on August 27, 1958. However, an interview transcript in which Lawrence stated that Oppenheimer \"should never again have anything to do with the forming of policy\" was presented at the hearing, and several other members of Lawrence's Radiation laboratory did testify against Oppenheimer in person. This resulted in later ill-feeling from the scientific community towards Lawrence and other members of his laboratory.\n\nEdward Teller was opposed to the hearing, feeling it was improper to subject Oppenheimer to a security trial, but was torn by longstanding grievances against him. He was called by Robb to testify against Oppenheimer, and shortly before he appeared Robb showed Teller a dossier of items unfavorable to Oppenheimer. Teller testified that he considered Oppenheimer loyal, but that \"in a great number of cases, I have seen Dr. Oppenheimer act – I understand that Dr. Oppenheimer acted – in a way which for me was exceedingly hard to understand. I thoroughly disagreed with him in numerous issues and his actions frankly appeared to me confused and complicated. To this extent I feel that I would like to see the vital interests of this country in hands which I understand better, and therefore trust more.\" Asked whether Oppenheimer should be granted a security clearance, Teller said that \"if it is a question of wisdom or judgement, as demonstrated by actions since 1945, then I would say one would be wiser not to grant clearance.\" This led to outrage by many in the scientific community and Teller's ostracism and virtual expulsion from academic science.\n\nOppenheimer's clearance was revoked by a 2–1 vote of the panel. Gray and Morgan voted in favor, Evans against. The board rendered its decision on May 27, 1954, in a 15,000-word letter to Nichols. It found that 20 of the 24 charges were either true or substantially true. The board found that while he had been opposed to the bomb and that his lack of enthusiasm for it had affected the attitude of other scientists, he had not actively discouraged scientists from working on the H-bomb, as had been alleged in Nichols' letter. It found that \"there is no evidence that he was a member of the [Communist] party in the strict sense of the word,\" and concluded that he is a \"loyal citizen\". It said that he \"had a high degree of discretion, reflecting an unusual ability to keep to himself vital secrets,\" but that he had \"a tendency to be coerced, or at least influenced in conduct, for a period of years.\"\n\nThe board found that Oppenheimer's association with Chevalier \"is not the kind of thing that our security system permits on the part of one who customarily has access to information of the highest classification\", and concluded that \"Oppenheimer's continuing conduct reflects a serious disregard for the requirements of the security system,\" that he was susceptible \"to influence which could have serious implications for the security interests of the country,\" that his attitude toward the H-bomb program raised doubt about whether his future participation \"would be consistent with the best interests of security,\" and that Oppenheimer had been \"less than candid in several instances\" in his testimony. The majority therefore did not recommend that his security clearance be reinstated.\n\nIn a brief dissent, Evans argued that Oppenheimer's security clearance should be reinstated. He pointed out that most of the AEC charges had been in the hands of the AEC when it cleared Oppenheimer in 1947, and that \"to deny him clearance now for what he was cleared for in 1947, when we must know he is less of a security risk now than he was then, seems to be hardly the procedure to be adopted in a free country.\" Evans said that his association with Chevalier did not indicate disloyalty, and that he did not hinder development of the H-bomb. Evans said he personally thought that \"our failure to clear Dr. Oppenheimer will be a black mark on the escutcheon of our country,\" and expressed concern about the effect an improper decision might have on the country's scientific development.\n\nIn a harshly worded memorandum to the AEC on June 12, 1954, Nichols recommended that Oppenheimer's security clearance not be reinstated. In five \"security findings\", Nichols said that Oppenheimer was \"a Communist in every sense except that he did not carry a party card,\" and that the Chevalier incident indicated that Oppenheimer \"is not reliable or trustworthy\", and that his misstatements might have represented criminal conduct. He said that Oppenheimer's \"obstruction and disregard for security\" showed \"a consistent disregard of a reasonable security system.\" The Nichols memorandum was not made public nor provided to Oppenheimer's lawyers, who were not allowed to appear before the AEC.\n\nOn June 29, 1954, the AEC upheld the findings of the Personnel Security Board, with five commissioners voting in favor and one opposed. The decision was rendered 32 hours before Oppenheimer's consultant contract, and with it the need for a clearance, was due to expire. In his majority opinion, Strauss said that Oppenheimer had displayed \"fundamental character defects\". He said that Oppenheimer \"in his associations had repeatedly exhibited a willful disregard of the normal and proper obligations of security,\" and that he \"has defaulted not once but many times upon the obligations that should and must be willingly borne by citizens in the national service.\"\n\nDespite the promise of confidentiality, the AEC released an edited transcript of the hearing in June 1954, after press publicity of the hearing. The unredacted transcripts were released in 2012.\n\nThe loss of his security clearance ended Oppenheimer's role in government and policy. Although he was not fired from his job at the Institute for Advanced Study, as he had feared he might be, he became an academic exile, cut off from his former career and the world he had helped to create. He gave public lectures, and spent several months of each year on the small island of Saint John in the Caribbean. Kai Bird and Martin J. Sherwin considered the Oppenheimer case \"a defeat for American liberalism\". Summing up the fallout from the case, they wrote that:\n\nOppenheimer was seen by many in the scientific community as a martyr to McCarthyism, a modern Galileo or Socrates, an intellectual and progressive unjustly attacked by warmongering enemies, symbolic of the shift of scientific creativity from academia into the military. Patrick McGrath noted that \"Scientists and administrators such as Edward Teller, Lewis Strauss and Ernest Lawrence, with their full-throated militarism and anti-communism pushed American scientists and their institutions toward a nearly complete and subservient devotion to American military interests.\" Scientists continued to work for the AEC, but they no longer trusted it.\n\nLoyalty and security tests spread through the federal government. At these inquiries, federal employees were asked questions such as:\n\nStrauss, Teller, Borden, and Robb would never escape the public identification of them with the case. In a 1962 television interview, Eric F. Goldman asked Teller whether he favored restoring Oppenheimer's security clearance. Teller was struck dumb, unable to find an answer. The question was deleted from the version that was aired, but the news got out and made headlines. President John F. Kennedy decided that the time had come to rehabilitate Oppenheimer. Teller nominated Oppenheimer for the 1963 Enrico Fermi Award. The nomination was unanimously approved by the GAC and AEC, and announced on April 5, 1963. On November 22, the White House confirmed that Kennedy would personally present the award, but he was assassinated later that day. The award was presented by President Lyndon B. Johnson instead. Oppenheimer died of cancer on February 18, 1967.\n\nWernher von Braun summed up his opinion about the matter with a quip to a Congressional committee: \"In England, Oppenheimer would have been knighted.\"\n\n\"Time\" magazine literary critic Richard Lacayo, in a 2005 review of two new books about Oppenheimer, said of the hearing: \"As an effort to prove that he had been a party member, much less one involved in espionage, the inquest was a failure. Its real purpose was larger, however: to punish the most prominent American critic of the U.S. move from atomic weapons to the much more lethal hydrogen bomb.\" After the hearing, Lacayo said, \"Oppenheimer would never again feel comfortable as a public advocate for a sane nuclear policy.\"\n\nCornell University historian Richard Polenberg noted that Oppenheimer testified about the left-wing behavior of his colleagues and speculated that if his clearance had not been stripped, he would have been remembered as someone who had \"named names\" to save his own reputation. In a book \"Brotherhood of the Bomb: The Tangled Lives and Loyalties of Robert Oppenheimer, Ernest Lawrence, and Edward Teller\" (2002), Gregg Herken, a senior historian at the Smithsonian Institution, contended, based on newly discovered documentation, that Oppenheimer was a member of the Communist Party.\n\nIn a seminar at the Woodrow Wilson Institute on May 20, 2009, and based on an extensive analysis of the Vassiliev notebooks taken from the KGB archives, John Earl Haynes, Harvey Klehr, and Alexander Vassiliev concluded that Oppenheimer never was involved in espionage for the Soviets. The KGB tried repeatedly to recruit him, but were never successful. Allegations that he had spied for the Soviets are unsupported, and in some instances, contradicted by voluminous KGB and Venona documentation released after the fall of the Soviet Union. In addition, he had several persons removed from the Manhattan project who had sympathies to the Soviet Union.\n\nMost popular depictions of Oppenheimer view his security struggles as a confrontation between right-wing militarists (symbolized by Edward Teller) and left-wing intellectuals (symbolized by Oppenheimer) over the moral question of weapons of mass destruction. Many historians have contested this as an oversimplification.\n\nThe hearing was dramatized in a 1964 play by German playwright Heinar Kipphardt, \"In the Matter of J. Robert Oppenheimer\". Oppenheimer objected to the play, threatening suit and decrying \"improvisations which were contrary to history and to the nature of the people involved\", including its portrayal of him as viewing the bomb as a \"work of the devil\". His letter to Kipphardt said, \"You may well have forgotten Guernica, Dachau, Coventry, Belsen, Warsaw, Dresden and Tokyo. I have not.\" Of his security hearing, he said, \"The whole damn thing was a farce, and these people are trying to make a tragedy out of it.\"\n\nIn a response, Kipphardt offered to make corrections but defended the play, which premiered on Broadway in June 1968, with Joseph Wiseman in the Oppenheimer role. \"New York Times\" theater critic Clive Barnes called it an \"angry play and a partisan play\" that sided with Oppenheimer but portrayed the scientist as a \"tragic fool and genius\".\n\nThe hearing also figured prominently in the 1980 BBC TV movie \"Oppenheimer\", with Sam Waterston as the title character and David Suchet as Edward Teller.\n\n\n"}
{"id": "2419256", "url": "https://en.wikipedia.org/wiki?curid=2419256", "title": "Personal Egress Air Pack", "text": "Personal Egress Air Pack\n\nPersonal Egress Air Packs, or PEAPs, were devices on board a Space Shuttle which provided crew members with approximately six minutes of breathable air in the case of a mishap while the vehicle was still located on the ground. PEAPs did not provide pressurized air, meaning that they were only intended to be used should the air inside the shuttle cabin become unbreathable by way of noxious gases.\n\nThe devices gained notoriety due to the \"Challenger\" disaster. After the recovery of the vehicle cockpit, it was discovered that three of the crew PEAPs were activated: those of mission specialist Ellison Onizuka, mission specialist Judith Resnik, and pilot Michael J. Smith. The location of Smith's activation switch, on the back side of his seat, means that either Resnik or Onizuka likely activated it for him. Mike Mullane writes: \n\nThis is the most conclusive piece of evidence available from the disaster that shows that at least two of the crew members (Onizuka and Resnik) were alive after the cockpit separated from the vehicle. However, if the cabin had lost pressure, the packs alone would not have sustained the crew during the two-minute descent.\n\nThe partial-pressure launch-entry suits replaced the PEAPs, which were subsequently followed by the \"ACES\" full-pressure suits, which include self-contained oxygen tanks.\n"}
{"id": "2211449", "url": "https://en.wikipedia.org/wiki?curid=2211449", "title": "Pessimistic induction", "text": "Pessimistic induction\n\nIn the philosophy of science, the pessimistic induction, also known as the pessimistic meta-induction, is an argument which seeks to rebut scientific realism, particularly the scientific realist's notion of epistemic optimism.\n\nScientific realists argue that we have good reasons to believe that our presently successful scientific theories are true or approximately true. The pessimistic meta-induction undermines the realist's warrant for their epistemic optimism (the view that science tends to succeed in revealing what the world is like and that there are good reasons to take theories to be true or truthlike) via historical counterexample. Using meta-induction, Larry Laudan argues that if past scientific theories which were successful were found to be false, we have no reason to believe the realist's claim that our currently successful theories are approximately true. The pessimistic meta-induction argument was first fully postulated by Laudan in 1981.\n\nHowever, there are some objections against Laudan's theory. One might see shortcomings in the historic examples Laudan gives as proof of his hypothesis. Theories later refuted, like that of crystalline spheres in astronomy, or the phlogiston theory, do not represent the most successful theories at their time. A further objection tries to point out that in scientific progress we indeed approximate the truth. When we develop a new theory, the central ideas of the old one usually become refuted. Parts of the old theory, however, we carry over to the new one. In doing so, our theories become more and more well-founded on other principles, they become better in terms of predictive and descriptive power, so that, for example, aeroplanes, computers and DNA sequencing all establish technical, operational proof of the effectiveness of the theories. Therefore, we can hold the realist view that our theoretical terms refer to something in the world and our theories are approximately true.\n\nHowever, as articulated by Thomas S. Kuhn in his \"Structure of Scientific Revolutions\", new scientific theories do not always build upon the older ones. In fact, they are created by an entirely new set of premises (a new \"paradigm\"), and reach vastly different conclusions. This gives greater weight to the proponents of anti-realism, and illustrates that no scientific theory (thus far) has proved infallible.\n\n\n"}
{"id": "42570587", "url": "https://en.wikipedia.org/wiki?curid=42570587", "title": "Physics arXiv Blog", "text": "Physics arXiv Blog\n\nThe Physics arXiv Blog aims to offer an alternative view of new ideas in science. It is based on, although independent of, the arXiv pre-print repository run by the Cornell University. Started in 2007, in 2009 it was hosted by the MIT Technology Review. In 2013, it was moved to the platform Medium.\n\n\"The Physics arXiv Blog\" has been said to offer \"the best physics coverage around\" by the Wired journal. It was included among the \"Five great physics blogs\" by The Guardian.\n\nContent appears to be crowd sourced from within the physics community. Similar to The Economist, articles seem to lack specific author bylines.\n\n"}
{"id": "26334534", "url": "https://en.wikipedia.org/wiki?curid=26334534", "title": "Prince Albert I Medal", "text": "Prince Albert I Medal\n\nThe Prince Albert I Medal was established by Prince Rainier of Monaco in partnership with the International Association for the Physical Sciences of the Oceans. The medal was named for Prince Albert I and is given for significant work in the physical and chemical sciences of the oceans. The medal is awarded biannually by IAPSO at its Assemblies. \n\n\n"}
{"id": "58405850", "url": "https://en.wikipedia.org/wiki?curid=58405850", "title": "Quirky (book)", "text": "Quirky (book)\n\nQuirky: The Remarkable Story of the Traits, Foibles, and Genius of Breakthrough Innovators Who Changed the World is a 2018 book by Melissa Schilling, a professor at New York University Stern School of Business. The book was published by PublicAffairs, a division of Hachette Book Group.\n\nMelissa Schilling develops cases studies of eight serial breakthrough innovators – Elon Musk, Dean Kamen, Steve Jobs, Marie Curie, Albert Einstein, Thomas Edison, Benjamin Franklin, and Nikola Tesla – to identify commonalities in their capabilities, motives, personalities, and experiences. These characteristics are then integrated with the research on innovation and creativity to show how they might influence breakthrough idea generation and extreme persistence.\n\nThe \"Financial Times\" reviewer wrote,\nJoe Culley at \"The Irish Times\" writes, \"Schilling's prose is clear and largely jargon-free, and the individual profiles are excellent.\"\nA reviewer in \"Strategy + Business\" writes,\nIn Innovation & Tech Today, Charles Warner writes,\nStephanie Orellana writes,\nA reviewer in the \"International Journal of Innovation Management\" writes,\n"}
{"id": "54149496", "url": "https://en.wikipedia.org/wiki?curid=54149496", "title": "Sian Proctor", "text": "Sian Proctor\n\nSian Proctor is an African American explorer, scientist, STEM communicator, and aspiring astronaut. She is a geology, sustainability and planetary science professor at South Mountain Community College. Proctor was the education outreach officer for the first \"Hawai’i Space Exploration Analog and Simulation (HI-SEAS) Mission\". HI-SEAS is a NASA funded analog habitat for human spaceflight to Mars. She has also appeared in three educational TV shows. She was on \"The Colony Season 2\" which was aired on The Discovery Channel in 2010. She also appeared in the 2016 PBS series \"Genius By Stephen Hawking\". She's in Episode 2: Are We Alone. She is currently featured on the science show \"Strange Evidence\".\n\nProctor is currently appearing as the science demonstration expert on the Science Channel show Strange Evidence. The show brings together a team of experts who analyze video footage that seems to defy explanation. Proctor describes the phenomena and uses mockups to explain the science behind what is being seen. \n\nProctor appears in Episode 2: Are We Alone. During this episode she, along with two other individuals, are challenged to \"think like a genius\" and discover the science behind the search for extraterrestrial life.\n\nProctor appears in two episodes of The STEM Journals Season 1. The STEM Journals is an educational show for kids interested in science, technology, engineering, and math (STEM) and aired locally on Cox7 Arizona. She appears in the Physics of Flight and Rocks.\n\nThe Colony was a post-apocalyptic build show by the Discovery Channel. Proctor appears in Season 2 which consists of 10 episodes and was shot on location in New Orleans.\n\nProctor was selected to the National Oceanic and Atmospheric Administration (NOAA) Teacher At Sea program in 2017. NOAA's Teacher at Sea Program was started in 1990 and provides teachers with hands-on, real-world research experience working at sea. For 3-weeks she will be conducting pollock research in Bering Sea on the fisheries vessel the Oscar Dyson and blogging about her experience for NOAA.\n\nProctor was selected as a 2016 ACEAP Ambassador. The ACEAP is a National Science Foundation (NSF) program that sends K-16 formal and informal astronomy educators to US astronomy facilities in Chile. During the summer of 2016 she joined 8 other ambassadors as they visited Cerro Tololo Inter-American Observatory (CTIO), Gemini South Observatory, and the Atacama Large Millimeter-submillimeter Array (ALMA). Proctor returned to San Pedro, Chile in 2017 to engage in STEM education outreach activities with the local high school and surrounding community.\n\nProctor was selected as 2014 PolarTREC teacher. PolarTREC is a National Science Foundation (NSF) funded program that connects teachers with scientist conducting research in the arctic and Antarctic regions. Sian spent a month in Barrow, Alaska learning Historical Ecology for Risk Management and investigating the impact of climate change on the coastline and community.\n\nProctor was the education outreach officer for the NASA funded Hawai’i Space Exploration Analog and Simulation (HI-SEAS) mission. The purpose of HI-SEAS mission was to investigate food strategies for long duration space flight and missions to the Moon or Mars. During the 4-month simulation Proctor was hired by Discover Magazine as the photographer for Kate Greene's article Simulating Mars on Earth. She also filmed the Meals for Mars YouTube Series while in the Mars simulation.\n\nProctor was a finalist for the 2009 NASA Astronaut Selection Process. She competed against over 3,500 applications to make the top 47. NASA selected 9 out of the 47 finalist, however, Proctor was not selected. She now gives inspirational presentations about almost becoming an astronaut.\n\n\n"}
{"id": "2532600", "url": "https://en.wikipedia.org/wiki?curid=2532600", "title": "Social and Decision Sciences (Carnegie Mellon University)", "text": "Social and Decision Sciences (Carnegie Mellon University)\n\nThe Department of Social and Decision Sciences (SDS) is an interdisciplinary academic department within the Dietrich College of Humanities and Social Sciences at Carnegie Mellon University. The Department of Social and Decision Sciences is headquartered in Porter Hall in Pittsburgh, Pennsylvania and is led by Department Head Linda C. Babcock. SDS has a world-class reputation for research and education programs in decision-making in public policy, economics, management, and the behavioral social sciences.\n\nThe Department of Social Sciences grew out of the social science programs in the Margaret Morrison Carnegie College. The formal department was established in 1976, as part of the Dietrich College of Humanities and Social Sciences under Dean John Patrick Crecine with approval from Heinz College Dean Otto Davis, which previously housed the program. The department was staffed by political scientists, sociologists, and economists from within the Dietrich College, the Heinz College, and the Tepper School of Business. In the 1980s, the department was led by Patrick D. Larkey and developed the undergraduate information systems program which became a huge success, eventually being spun off into an independent interdisciplinary program in the Dietrich College. In 1985, Robyn Dawes joined the department and began to re-focus it into its current form and expertise in behavioral decision-making and caused it to be renamed as the Department of Social and Decision Sciences. Carnegie Mellon's Institute for Politics and Strategy was spun off of the department in 2015.\n\nThe department runs highly regarded undergraduate bachelor of science programs in Decision Science and Policy and Management and a bachelor of arts program in Behavioral Economics, Policy, and Organizations as well as minors in Decision Science and Policy and Management. Further, SDS is a partner in various interdisciplinary undergraduate programs such as the Sociology minor, Environmental Policy program, and the Quantitative Social Science Scholars program. At the master's degree level, SDS partners with several other colleges and departments for the Engineering and Technology Innovation Management program. The graduate PhD program allows doctoral students options in Social and Decision Sciences, Behavioral Decision Research (BDR) as well as a joint PhDs in Behavioral Decision Research and Psychology with the Department of Psychology, Behavioral Marketing and Decision Research and Behavioral Economics with the Tepper School of Business, and a joint MD-PhD Medical Scientist Training program with the University of Pittsburgh School of Medicine. All students are trained in policy analysis and research methods. SDS is also closely associated with its spin-off programs: the undergraduate Information Systems program and the Institute for Politics and Strategy.\n\nThe department's primary strength lies in interdisciplinary research, particularly in the intersection of politics and sociology with economics, psychology and human decision making. Statistics, microeconomics, rational decision theory and game theory are among the many areas of specialization. SDS faculty are very involved in interdisciplinary research throughout the university, and are drawn from fields as diverse as economics, psychology, sociology, history, management science, and political science. SDS is affiliated with several labs and centers:\n\n\nAs part of the Center for Behavioral Decision Research, SDS manages the Data Truck and managed the Carnegie Mellon Research Cafe. The Data Truck is a mobile behavioral science lab. The Research Cafe was a cafe located in downtown Pittsburgh and was designed to perform behavioral research outside of the student population on the main Carnegie Mellon Pittsburgh campus where the SDS labs are located. The cafe is now closed.\n\nIn 2006, an Institute for Operations Research and the Management Sciences study gave four of five stars to the graduate program in prescriptive decision making and five stars in descriptive decision making. These two ratings tied the department for first place among decision science programs in the United States with Duke University and the University of Pennsylvania.\n\nSDS is home to some of the world's top faculty in the fields of decision science, decision support systems, behavioral economics, organizational behavior, risk analysis, management science, and complex social systems. Some notable faculty members (both current and past) include Baruch Fischhoff, Paul Fischbeck, Robyn Dawes, George Loewenstein, Cleotilde Gonzalez, Jennifer Lerner, Kathleen Carley, David Krackhardt, Steven Klepper, Linda C. Babcock, Lee Branstetter, David A. Hounshell, William Keech, John H. Miller, Mark Kamlet, Roberto Weber, Herbert A. Simon, Jendayi Frazer, Kiron Skinner, Sara Kiesler, John Patrick Crecine, Cristina Bicchieri, Joseph Born Kadane, Daniel M. Oppenheimer, Patrick D. Larkey, and Otto Davis. The current faculty consists of 20 full-time members with additional associated and adjunct faculty, staff, and fellows. In addition the department works closely with the departments of Engineering and Public Policy, Psychology, History, and Statistics and Data Science, as well as the Tepper School of Business, Heinz College, and the undergraduate Information Systems program. Many SDS faculty have joint appointments in these departments.\n\n"}
{"id": "19112007", "url": "https://en.wikipedia.org/wiki?curid=19112007", "title": "The Scientific Activist", "text": "The Scientific Activist\n\nThe Scientific Activist was a blog that covers science, politics, and science policy, run by Nick Anthis, a graduate student in biochemistry and Rhodes Scholar at the University of Oxford. The Scientific Activist gained international recognition in February 2006 when it published information that led to the immediate resignation of Bush Administration NASA appointee George Deutsch. Deutsch—who had been accused of censoring scientific information at NASA—claimed to have graduated from Texas A&M University on his résumé, but Anthis discovered that Deutsch had not, in fact, completed his degree there.\n\nThe Scientific Activist was founded on January 11, 2006, and was originally hosted by Blogger. It gained early attention for its coverage of Oxford's vocal animal rights movement, and it continued its coverage as the pro-research Pro-Test movement was formed. On June 9, 2006, The Scientific Activist moved to ScienceBlogs.\n\nIn July 2006, The Scientific Activist was named one of \"Nature's\" \"Top five science blogs.\"\n\n"}
{"id": "46619060", "url": "https://en.wikipedia.org/wiki?curid=46619060", "title": "Timeline of Carboniferous research", "text": "Timeline of Carboniferous research\n\nThis timeline of Carboniferous research is a chronological listing of events in the history of geology and paleontology focused on the study of earth during the span of time lasting from 358.9–298.9 million years ago and the legacies of this period in the rock and fossil records.\n\n1828\n\n\n1853\n\n1860\n\n1882\n\n1887\n\n1889\n\n1899\n\n\n"}
{"id": "236706", "url": "https://en.wikipedia.org/wiki?curid=236706", "title": "Timeline of paleontology", "text": "Timeline of paleontology\n\nTimeline of paleontology\n\n\n"}
{"id": "5731861", "url": "https://en.wikipedia.org/wiki?curid=5731861", "title": "Van Hiele model", "text": "Van Hiele model\n\nIn mathematics education, the Van Hiele model is a theory that describes how students learn geometry. The theory originated in 1957 in the doctoral dissertations of Dina van Hiele-Geldof and Pierre van Hiele (wife and husband) at Utrecht University, in the Netherlands. The Soviets did research on the theory in the 1960s and integrated their findings into their curricula. American researchers did several large studies on the van Hiele theory in the late 1970s and early 1980s, concluding that students' low van Hiele levels made it difficult to succeed in proof-oriented geometry courses and advising better preparation at earlier grade levels. Pierre van Hiele published \"Structure and Insight\" in 1986, further describing his theory. The model has greatly influenced geometry curricula throughout the world through emphasis on analyzing properties and classification of shapes at early grade levels. In the United States, the theory has influenced the geometry strand of the Standards published by the National Council of Teachers of Mathematics and the new Common Core Standards.\n\nThe student learns by rote to operate with [mathematical] relations that he does not understand, and of which he has not seen the origin…. Therefore the system of relations is an independent construction having no rapport with other experiences of the child. This means that the student knows only what has been taught to him and what has been deduced from it. He has not learned to establish connections between the system and the sensory world. He will not know how to apply what he has learned in a new situation. - \"Pierre van Hiele, 1959\"\nThe best known part of the van Hiele model are the five levels which the van Hieles postulated to describe how children learn to reason in geometry. Students cannot be expected to prove geometric theorems until they have built up an extensive understanding of the systems of relationships between geometric ideas. These systems cannot be learned by rote, but must be developed through familiarity by experiencing numerous examples and counterexamples, the various properties of geometric figures, the relationships between the properties, and how these properties are ordered. The five levels postulated by the van Hieles describe how students advance through this understanding.\n\nThe five van Hiele levels are sometimes misunderstood to be descriptions of how students understand shape classification, but the levels actually describe the way that students reason about shapes and other geometric ideas. Pierre van Hiele noticed that his students tended to \"plateau\" at certain points in their understanding of geometry and he identified these plateau points as \"levels\". In general, these levels are a product of experience and instruction rather than age. This is in contrast to Piaget's theory of cognitive development, which is age-dependent. A child must have enough experiences (classroom or otherwise) with these geometric ideas to move to a higher level of sophistication. Through rich experiences, children can reach Level 2 in elementary school. Without such experiences, many adults (including teachers) remain in Level 1 all their lives, even if they take a formal geometry course in secondary school. The levels are as follows:\nLevel 0. Visualization: At this level, the focus of a child's thinking is on individual shapes, which the child is learning to classify by judging their holistic appearance. Children simply say, \"That is a circle,\" usually without further description. Children identify prototypes of basic geometrical figures (triangle, circle, square). These visual prototypes are then used to identify other shapes. A shape is a circle because it looks like a sun; a shape is a rectangle because it looks like a door or a box; and so on. A square seems to be a different sort of shape than a rectangle, and a rhombus does not look like other parallelograms, so these shapes are classified completely separately in the child’s mind. Children view figures holistically without analyzing their properties. If a shape does not sufficiently resemble its prototype, the child may reject the classification. Thus, children at this stage might balk at calling a thin, wedge-shaped triangle (with sides 1, 20, 20 or sides 20, 20, 39) a \"triangle\", because it's so different in shape from an equilateral triangle, which is the usual prototype for \"triangle\". If the horizontal base of the triangle is on top and the opposing vertex below, the child may recognize it as a triangle, but claim it is \"upside down\". Shapes with rounded or incomplete sides may be accepted as \"triangles\" if they bear a holistic resemblance to an equilateral triangle. Squares are called \"diamonds\" and not recognized as squares if their sides are oriented at 45° to the horizontal. Children at this level often believe something is true based on a single example.\n\nLevel 1. Analysis: At this level, the shapes become bearers of their properties. The objects of thought are classes of shapes, which the child has learned to analyze as having properties. A person at this level might say, \"A square has 4 equal sides and 4 equal angles. Its diagonals are congruent and perpendicular, and they bisect each other.\" The properties are more important than the appearance of the shape. If a figure is sketched on the blackboard and the teacher claims it is intended to have congruent sides and angles, the students accept that it is a square, even if it is poorly drawn. Properties are not yet ordered at this level. Children can discuss the properties of the basic figures and recognize them by these properties, but generally do not allow categories to overlap because they understand each property in isolation from the others. For example, they will still insist that \"a square is not a rectangle.\" (They may introduce extraneous properties to support such beliefs, such as defining a rectangle as a shape with one pair of sides longer than the other pair of sides.) Children begin to notice many properties of shapes, but do not see the relationships between the properties; therefore they cannot reduce the list of properties to a concise definition with necessary and sufficient conditions. They usually reason inductively from several examples, but cannot yet reason deductively because they do not understand how the properties of shapes are related.\n\nLevel 2. Abstraction: At this level, properties are ordered. The objects of thought are geometric properties, which the student has learned to connect deductively. The student understands that properties are related and one set of properties may imply another property. Students can reason with simple arguments about geometric figures. A student at this level might say, \"Isosceles triangles are symmetric, so their base angles must be equal.\" Learners recognize the relationships between types of shapes. They recognize that all squares are rectangles, but not all rectangles are squares, and they understand why squares are a type of rectangle based on an understanding of the properties of each. They can tell whether it is possible or not to have a rectangle that is, for example, also a rhombus. They understand necessary and sufficient conditions and can write concise definitions. However, they do not yet understand the intrinsic meaning of deduction. They cannot follow a complex argument, understand the place of definitions, or grasp the need for axioms, so they cannot yet understand the role of formal geometric proofs.\n\nLevel 3. Deduction: Students at this level understand the meaning of deduction. The object of thought is deductive reasoning (simple proofs), which the student learns to combine to form a system of formal proofs (Euclidean geometry). Learners can construct geometric proofs at a secondary school level and understand their meaning. They understand the role of undefined terms, definitions, axioms and theorems in Euclidean geometry. However, students at this level believe that axioms and definitions are fixed, rather than arbitrary, so they cannot yet conceive of non-Euclidean geometry. Geometric ideas are still understood as objects in the Euclidean plane.\n\nLevel 4. Rigor: At this level, geometry is understood at the level of a mathematician. Students understand that definitions are arbitrary and need not actually refer to any concrete realization. The object of thought is deductive geometric systems, for which the learner compares axiomatic systems. Learners can study non-Euclidean geometries with understanding. People can understand the discipline of geometry and how it differs philosophically from non-mathematical studies.\n\nAmerican researchers renumbered the levels as 1 to 5 so that they could add a \"Level 0\" which described young children who could not identify shapes at all. Both numbering systems are still in use. Some researchers also give different names to the levels.\n\nThe van Hiele levels have five properties:\n\n1. Fixed sequence: the levels are hierarchical. Students cannot \"skip\" a level. The van Hieles claim that much of the difficulty experienced by geometry students is due to being taught at the Deduction level when they have not yet achieved the Abstraction level.\n\n2. Adjacency: properties which are intrinsic at one level become extrinsic at the next. (The properties are there at the Visualization level, but the student is not yet consciously aware of them until the Analysis level. Properties are in fact related at the Analysis level, but students are not yet explicitly aware of the relationships.)\n\n3. Distinction: each level has its own linguistic symbols and network of relationships. The meaning of a linguistic symbol is more than its explicit definition; it includes the experiences the speaker associates with the given symbol. What may be \"correct\" at one level is not necessarily correct at another level. At Level 0 a square is something that looks like a box. At Level 2 a square is a special type of rectangle. Neither of these is a correct description of the meaning of \"square\" for someone reasoning at Level 1. If the student is simply handed the definition and its associated properties, without being allowed to develop meaningful experiences with the concept, the student will not be able to apply this knowledge beyond the situations used in the lesson.\n\n4. Separation: a teacher who is reasoning at one level speaks a different \"language\" from a student at a lower level, preventing understanding. When a teacher speaks of a \"square\" she or he means a special type of rectangle. A student at Level 0 or 1 will not have the same understanding of this term. The student does not understand the teacher, and the teacher does not understand how the student is reasoning, frequently concluding that the student's answers are simply \"wrong\". The van Hieles believed this property was one of the main reasons for failure in geometry. Teachers believe they are expressing themselves clearly and logically, but their Level 3 or 4 reasoning is not understandable to students at lower levels, nor do the teachers understand their students’ thought processes. Ideally, the teacher and students need shared experiences behind their language.\n\n5. Attainment: The van Hieles recommended five phases for guiding students from one level to another on a given topic:\n\n\nFor Dina van Hiele-Geldof's doctoral dissertation, she conducted a teaching experiment with 12-year-olds in a Montessori secondary school in the Netherlands. She reported that by using this method she was able to raise students' levels from Level 0 to 1 in 20 lessons and from Level 1 to 2 in 50 lessons.\n\nUsing van Hiele levels as the criterion, almost half of geometry students are placed in a course in which their chances of being successful are only 50-50. — \"Zalman Usiskin, 1982\"\nResearchers found that the van Hiele levels of American students are low. European researchers have found similar results for European students. Many, perhaps most, American students do not achieve the Deduction level even after successfully completing a proof-oriented high school geometry course, probably because material is learned by rote, as the van Hieles claimed. This appears to be because American high school geometry courses assume students are already at least at Level 2, ready to move into Level 3, whereas many high school students are still at Level 1, or even Level 0. See the Fixed Sequence property above.\n\nThe levels are discontinuous, as defined in the properties above, but researchers have debated as to just how discrete the levels actually are. Studies have found that many children reason at multiple levels, or intermediate levels, which appears to be in contradiction to the theory. Children also advance through the levels at different rates for different concepts, depending on their exposure to the subject. They may therefore reason at one level for certain shapes, but at another level for other shapes.\n\nSome researchers have found that many children at the Visualization level do not reason in a completely holistic fashion, but may focus on a single attribute, such as the equal sides of a square or the roundness of a circle. They have proposed renaming this level the \"syncretic\" level. Other modifications have also been suggested, such as defining sub-levels between the main levels, though none of these modifications have yet gained popularity.\n\n\n"}
{"id": "3053743", "url": "https://en.wikipedia.org/wiki?curid=3053743", "title": "Van Nostrand's Scientific Encyclopedia", "text": "Van Nostrand's Scientific Encyclopedia\n\nVan Nostrand's Scientific Encyclopedia is an encyclopedia published in the United States. Currently in a three volume 10th edition, it was published in two volumes for editions 6 to 9. The 8th edition is available as two CD ROMs.\n\nThe first edition was published in 1938 by the D. Van Nostrand Company, Inc. From about 1976 the encyclopedia was published by the Van Nostrand Reinhold Company. From the late 1990s it was published by John Wiley and Sons and by Wiley-Interscience.\n\nMore than 4,000 pages long, the work provides a thorough scientific reference, while establishing a midpoint between massive multi-volume science encyclopedias and handheld reference books.\n\nThis work is sometimes compared to the \"McGraw-Hill Concise Encyclopedia of Science & Technology\".\n"}
{"id": "9519674", "url": "https://en.wikipedia.org/wiki?curid=9519674", "title": "Waves and shallow water", "text": "Waves and shallow water\n\nWhen waves travel into areas of shallow water, they begin to be affected by the ocean bottom. The free orbital motion of the water is disrupted, and water particles in orbital motion no longer return to their original position. As the water becomes shallower, the swell becomes higher and steeper, ultimately assuming the familiar sharp-crested wave shape. After the wave breaks, it becomes a wave of translation and erosion of the ocean bottom intensifies.\n\n\n"}
{"id": "58111876", "url": "https://en.wikipedia.org/wiki?curid=58111876", "title": "Wendy Suzuki", "text": "Wendy Suzuki\n\nWendy Suzuki is a Professor of Neural Science and Psychology at the New York University Center for Neural Science and popular science communicator. She is the author of \"Healthy Brain, Happy Life: A Personal Program to Activate Your Brain and Do Everything Better\".\n\nSuzuki received her undergraduate degree in physiology and human anatomy at the University of California, Berkeley in 1987. There, she worked with Marian Diamond, whom she met after taking her course called \"The Brain and its Potential.\" Diamond's work opened the door into studying neuroplasticity with evidence that the brain could change in response to its environment. With an interest in memory and brain plasticity, Suzuki then went on to receive her Ph.D. in Neuroscience from the University of California, San Diego in 1993 under the mentorship of David Amaral, Stuart Zola, and Larry Squire. There, her work uncovered the importance of the perirhinal and parahippocampal corticies in preserving our long-term memories. Her doctoral thesis won her the Society for Neuroscience's Donald B. Lindsley Prize in the field of behavioral neuroscience.\n\nSuzuki completed her postdoctoral research at the National Institutes of Health's National Institute of Mental Health between 1993 and 1998. There she worked under the mentorship of Robert Desimone, studying how the brain is able to remember where objects are in space.\n\nSuzuki joined the faculty at New York University's Center for Neural Science in 1998. Her research interests center on neuroplasticity and how the brain is able to change and adapt over the course of a person's life. Her early career research focused on the areas of the brain that play an important role in our ability to form and retain memories. More recently, she's expanded this work to study the role of aerobic exercise on potentially enhancing cognitive abilities.\n\nSuzuki's research career started with studying underlying memory. Her lab focused on the role of the hippocampus, which is the part of the brain that is responsible for memory of facts and events, otherwise known as declarative or explicit memory. Her research group was the first to identify major changes to patterns of neural activity in the hippocampus as subjects worked to form memories that associated unrelated objects with one another, known as \"associative memories.\" They identified neural patterns associated with how the brain forms memories in a temporal order, showing the critical role of the hippocampus in how timing is incorporated into forming memories.\n\nSuzuki's research in 2018 focused on the impact of exercise on the brain. Her group is working to develop a \"prescription\" for the right amount of exercise to maximize brain activity for a range of purposes including; learning, aging, memory, attention, and mood. To support that work, the Suzuki lab is researching the kinds of exercise that enhance cognition among adults. Her group has found evidence that acute aerobic exercise can improve prefrontal cortex activity, which is the part of the brain that contributes to personality development. Suzuki is also investigating how best to incorporate exercise to treat mood and cognitive disorders. Her group has found that a combined regimen of exercise and self-affirmation interventions can enhance the cognitive capabilities and mood of patients with traumatic brain injury.\n\nSuzuki is also a popular science communicator and author of the book \"Healthy Brain, Happy Life\". The book details her personal journey with exercise and how it has transformed her life, while discussing the underlying neuroscience of the benefits of exercise. Tour stops for the book included appearances on shows like CBS This Morning, WNYC, and the Big Think. Suzuki has regularly appeared on HuffPost, sharing advances in her research on the link between exercise and brain activity.\n\nSuzuki has told stories for shows like The Moth about how she first came to say \"I love you\" to her parents as an adult and The Story Collider about how an exercise in acting challenged her beliefs about love and attraction in the brain.\n\n\nSuzuki also serves on the Board of Directors of the McKnight Foundation, acting as the Chair for the Memory & Cognitive Disorder Awards.\n"}
{"id": "47228327", "url": "https://en.wikipedia.org/wiki?curid=47228327", "title": "Worlds Beginning", "text": "Worlds Beginning\n\nWorlds Beginning is a 1944 speculative fiction novel by Robert Ardrey. It proposed a new economic model and system of corporate ownership.\n\n\"Worlds Beginning\" was published one year after \"God and Texas\", Ardrey's one-act play about the Battle of the Alamo, and two years before the production of \"Jeb\", Ardrey's Civil Rights play.\n\nThe \"New York Times\" gave the following synopsis of the novel:\nTwenty years from now a retired reporter takes time off from cultivating his petunias and his perennial border to write of his own experiences during the dark days after peace, when American civilization seemed about to crack up for good. The \"myth of internationalism\" had infected big business with a bad case of economic imperialism. But synthetic revolutions in industrial chemistry soon fixed things so foreign nations didn't need a thing we could sell them. Depression, unemployment, bankruptcies, hatred and fear plagued the country. The business-controlled Government was swept out of office by a labor popular front. But things got worse instead of better. ... [W]hile Americans everywhere sank into fatalistic apathy, a new hope was born in the desert town of Indian Pass in Texas, where the Trans-Pecos Chemical Commonwealth was making a plastic substitute for copper wire that was much cheaper and much better than copper. The significance lay in a new concept of ownership. ... The commonwealth idea got rid of all owners of property and also paid no wages, only shares in the earnings. The employees had no participation in the concern. The harder they worked and the more money the commonwealth made, the larger was their share of the earnings. Every foreman was subject to dismissal by a majority vote of the workers under him. Any lazy or inefficient worker was thrown out on his ear by his comrades. Efficiency was wonderful, morale perfect.\n\n\"Worlds Beginning\" also includes a one-page forward by the author in which he discusses his reasons for writing the novel. Included is his sense that during dark times there is hope for the future, but only if those concerned are willing to fight for it. This is a theme consistently developed throughout his work, including most notably in Thunder Rock, Ardrey's most famous play.\n\n\"Worlds Beginning\" was the subject of two dedicated reviews in The New York Times. The first, by Carlos Baker, appeared on September 24, 1944. The second, by Orville Prescott, appeared on September 27. Both offered praise.\n\nPrescott asserted \"Mr. Ardrey has written a novel with as thumping a thesis as Bellamy's 'Looking Backward,' or any other celebrated outlines of Utopia.\" Baker wrote of the book that it was: \"fresh, tough, cocky, exciting, shot through with the exuberance of discovery. However he does it, with stiletto or meat cleaver, ad copywriter's slogans or heroic analogies, reiteration either plain or fancy, Mr. Ardrey clears a path for the reader through the jungle of economics and brings him out, flushed and panting, to a new world in the making.\"\n\nIn 1948 \"Worlds Beginning\" was included in Everett F. Bleiler's \"The Checklist of Fantastic Literature.\"\n\n"}
