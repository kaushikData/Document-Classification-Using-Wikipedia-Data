{"id": "14873932", "url": "https://en.wikipedia.org/wiki?curid=14873932", "title": "3C 438", "text": "3C 438\n\n3C 438 is a Seyfert galaxy located in the constellation Cygnus.\n\n"}
{"id": "22206295", "url": "https://en.wikipedia.org/wiki?curid=22206295", "title": "Alisol", "text": "Alisol\n\nAn Alisol is a Reference Soil Group of the World Reference Base for Soil Resources (WRB).\n\nAlisols have an \"argic horizon\", which has a high cation exchange capacity. In the subsoil, the base saturation is low.\n\nThere exist mixed forms, for example 'Stagnic Alisol', that are mainly Alisol, but also contain components that are found in Stagnosols.\n\nAlisols occur in tropical and humid subtropical climates, but are also found in temperate regions. Compared to Lixisols, Acrisols and Ferralsols, Alisols have much higher-activity clays and are likely to be found on younger terrains or more geologically active regions such as Kyushu and Chugoku.\n\nAlisols are acidic (increased by limited drainage) and therefore need liming, contain few nutrients and therefore need fertilizer, and do not have much surface coherence so are easily eroded.\n\nAluminium and manganese toxicity is a very serious problem in Alisols, because at the low pH of these soils such generally insoluble metals become soluble and can poison plants which are not tolerant of them. Encyclopædia Britannica mentions oil palm, cotton, and maize (corn) as crops suitable to be grown on Alisols, though most crops require very intensive fertilisation for long-term success.\n\n\n"}
{"id": "2536187", "url": "https://en.wikipedia.org/wiki?curid=2536187", "title": "Alvarez hypothesis", "text": "Alvarez hypothesis\n\nThe Alvarez hypothesis posits that the mass extinction of the dinosaurs and many other living things during the Cretaceous–Paleogene extinction event was caused by the impact of a large asteroid on the Earth. Prior to 2013, it was commonly cited as having happened about 65 million years ago, but a 2013 paper by Renne et al. gave an updated value of 66 million years. Evidence indicates that the asteroid fell in the Yucatán Peninsula, at Chicxulub, Mexico. The hypothesis is named after the father-and-son team of scientists Luis and Walter Alvarez, who first suggested it in 1980. \n\nIn March 2010, an international panel of scientists endorsed the asteroid hypothesis, specifically the Chicxulub impact, as being the cause of the extinction. A team of 41 scientists reviewed 20 years of scientific literature and in so doing also ruled out other theories such as massive volcanism. They had determined that a space rock hurtled into earth at Chicxulub. For comparison, the Martian moon Phobos is and Mount Everest is just under . The collision would have released the same energy as , over a billion times the energy of the atomic bombs dropped on Hiroshima and Nagasaki.\n\nA 2016 drilling project into the peak ring of the crater strongly supported the hypothesis, and confirmed various matters that had been unclear until that point. These included the fact that the peak ring comprised granite (a rock found deep within the earth) rather than typical sea floor rock, which had been shocked, melted, and ejected to the surface in minutes, and evidence of colossal seawater movement directly afterwards from sand deposits. Crucially the cores also showed a near complete absence of gypsum, a sulfate-containing rock, which would have been vaporized and dispersed as an aerosol into the atmosphere, confirming the presence of a probable link between the impact and global longer-term effects on the climate and food chain.\n\nIn 1980, a team of researchers led by Nobel prize-winning physicist Luis Alvarez, his son, geologist Walter Alvarez, and chemists Frank Asaro and Helen Vaughn Michel discovered that sedimentary layers found all over the world at the Cretaceous–Paleogene boundary (K–Pg boundary, formerly called Cretaceous–Tertiary or K–T boundary) contain a concentration of iridium hundreds of times greater than normal. Iridium is extremely rare in the Earth's crust because it is very dense and has the affinity for iron that characterizes the siderophile elements (see Goldschmidt classification), and therefore most of it sank into the Earth's core while the earth was still molten. The Alvarez team suggested that an asteroid struck the earth at the time of the Cretaceous–Paleogene boundary.\n\nPreviously, in a 1953 publication, geologists Allan O. Kelly and Frank Dachille analyzed global geological evidence suggesting that one or more giant asteroids impacted the Earth, causing an angular shift in its axis, global floods, fire, atmospheric occlusion, and the extinction of the dinosaurs. There were other earlier speculations on the possibility of an impact event, but without strong confirming evidence.\n\nThe evidence for the Alvarez impact hypothesis is supported by chondritic meteorites and asteroids which contain a much higher iridium concentration than the Earth's crust. The isotopic ratio of iridium in meteorites is similar to that of the Cretaceous–Paleogene boundary layer but significantly different from the ratio in the Earth's crust. Chromium isotopic anomalies found in Cretaceous–Paleogene boundary sediments are similar to that of an asteroid or a comet composed of carbonaceous chondrites. Shocked quartz granules, glass spherules and tektites, indicative of an impact event, are common in the Cretaceous–Paleogene boundary, especially in deposits from around the Caribbean. All of these constituents are embedded in a layer of clay, which the Alvarez team interpreted as the debris spread all over the world by the impact. The location of the impact was unknown when the Alvarez team developed their hypothesis, but later scientists discovered the Chicxulub Crater in the Yucatán Peninsula, now considered the likely impact site.\n\nUsing estimates of the total amount of iridium in the K–Pg layer, and assuming that the asteroid contained the normal percentage of iridium found in chondrites, the Alvarez team went on to calculate the size of the asteroid. The answer was about 10 kilometers (6 mi) in diameter, about the size of Manhattan. Such a large impact would have had approximately the energy of 100 million megatons, i.e. about 2 million times as great as the most powerful thermonuclear bomb ever tested.\n\nPaul Renne of the Berkeley Geochronology Center has reported that the date of the asteroid event is 66,038,000 years ago, plus or minus 11,000 years, based on the radioactive decay of argon. He further posits that the mass extinction of dinosaurs occurred within 33,000 years of this date.\n\nThe most easily observable consequence of such an impact would be a vast dust cloud which would block sunlight and prevent photosynthesis for a few years, an event called an impact winter. This would account for the extinction of plants and phytoplankton and of all organisms dependent on them (including predatory animals as well as herbivores). But small creatures whose food chains were based on detritus would have a reasonable chance of survival. It is estimated that sulfuric acid aerosols were injected into the stratosphere, leading to a 10–20% reduction of solar transmission normal for that period. It would have taken at least ten years for those aerosols to dissipate.\n\nGlobal firestorms may have resulted as incendiary fragments from the blast fell back to Earth. Analyses of fluid inclusions in ancient amber suggest that the oxygen content of the atmosphere was very high (30–35%) during the late Cretaceous. This high O level would have supported intense combustion. The level of atmospheric O plummeted in the early Paleogene Period. If widespread fires occurred, they would have increased the CO content of the atmosphere and caused a temporary greenhouse effect once the dust cloud settled, and this would have exterminated the most vulnerable survivors of the \"long winter\".\n\nThe impact may also have produced acid rain, depending on what type of rock the asteroid struck. However, recent research suggests this effect was relatively minor. Chemical buffers would have limited the changes, and the survival of animals vulnerable to acid rain effects (such as frogs) indicate this was not a major contributor to extinction.\n\nImpact hypotheses can only explain very rapid extinctions, since the dust clouds and possible sulphuric aerosols would wash out of the atmosphere in a fairly short time — possibly under ten years.\n\nAlthough further studies of the K–Pg layer consistently show the excess of iridium, the idea that the dinosaurs were exterminated by an asteroid remained a matter of controversy among geologists and paleontologists for more than a decade.\nAmongst others, Charles B. Officer and Gerta Keller have been critical of the theory.\n\nOfficer and Jake Page describe in their \"The Great Dinosaur Extinction Controversy\" how\n\nKeller has focussed on Deccan Traps volcanism as a likely cause of a more gradual extinction. \n\nIn 2016, a scientific drilling project drilled deep into the peak ring of the Chicxulub impact crater, to obtain rock core samples from the impact itself. The discoveries were widely seen as confirming current theories related to both the crater impact, and its effects. They confirmed that the rock composing the peak ring had been subjected to immense pressures and forces and had been melted by immense heat and shocked by immense pressure from its usual state into its present form in just minutes; the fact that the peak ring was made of granite was also significant, since granite is not a rock found in sea-floor deposits, it originates much deeper in the earth and had been ejected to the surface by the immense pressures of impact; that gypsum, a sulfate-containing rock that \"is\" usually present in the shallow seabed of the region, had been almost entirely removed and must therefore have been almost entirely vaporized and entered the atmosphere, and that the event was immediately followed by a huge megatsunami (a massive movement of sea waters) sufficient to lay down the largest known layer of sand separated by grain size directly above the peak ring. \n\nThese strongly support the hypothesis that the impactor was large enough to create a 120 mile peak ring, to melt, shock and eject basement granite from the midcrust deep within the earth, to create colossal water movements, and to eject an immense quantity of vaporized rock and sulfates into the atmosphere, where they would have persisted for a long time. This global dispersal of dust and sulfates would have led to a sudden and catastrophic effect on the climate worldwide, large temperature drops, and devastated the food chain.\n"}
{"id": "1849608", "url": "https://en.wikipedia.org/wiki?curid=1849608", "title": "AstroGrid", "text": "AstroGrid\n\nAstroGrid was a £7.7M project which built a data-grid for UK astronomy, forming part of the UK contribution to the International Virtual Observatory. AstroGrid announced its first full production release on 1 April 2008.\n\nThe project ran, in three phases, from 2001 to 2009. Accounts of its end-days suggest that many in the community regretted its early closing.\n\n"}
{"id": "46995734", "url": "https://en.wikipedia.org/wiki?curid=46995734", "title": "British Mass Spectrometry Society", "text": "British Mass Spectrometry Society\n\nThe \"British Mass Spectrometry Society\" is a registered charity encouraging participation in every aspect of mass spectrometry. The society was founded in 1964.\n\nThe society awards the Aston Medal.\n\nIn 2015 the British Mass Spectrometry Society announced they will be funding 6-10 summer studentship projects.\n"}
{"id": "27155713", "url": "https://en.wikipedia.org/wiki?curid=27155713", "title": "Carl Linnaeus bibliography", "text": "Carl Linnaeus bibliography\n\nThe bibliography of Carl Linnaeus includes academic works about botany, zoology, nomenclature and taxonomy written by the Swedish botanist Carl Linnaeus (1707–1778). Linnaeus laid the foundations for the modern scheme of binomial nomenclature and is known as the father of modern taxonomy. His most famous works is \"Systema Naturae\" which is considered as the starting point for zoological nomenclature together with \"Species Plantarum\" which is internationally accepted as the beginning of modern botanical nomenclature.\n\n\n"}
{"id": "5129726", "url": "https://en.wikipedia.org/wiki?curid=5129726", "title": "Casimir pressure", "text": "Casimir pressure\n\nCasimir pressure is created by the Casimir force of virtual particles.\n\nAccording to experiments, the Casimir force formula_1 between two closely spaced neutral parallel plate conductors is directly proportional to their surface area formula_2:\n\nformula_3\n\nTherefore, dividing the magnitude of Casimir force by the area of each conductor, Casimir pressure formula_4 can be found. Because the Casimir force between conductors is attractive, the Casimir pressure in space between the conductors is negative.\n\nBecause virtual particles are physical representations of the zero point energy of physical vacuum, the Casimir pressure is the difference in the density of the zero point energy of empty space inside and outside of cavity made by conductive plates.\n\nSome scientists believe that zero point energy is the dominant energy of the Universe and that the Casimir pressure of this energy is the main cause of the observed accelerated expansion of the Universe. In other words, virtual particles drive the accelerated expansion of the Universe.\n\n"}
{"id": "54177704", "url": "https://en.wikipedia.org/wiki?curid=54177704", "title": "Cathy Olkin", "text": "Cathy Olkin\n\nCathy Olkin is a planetary scientist at the Southwest Research Institute, focusing on the outer solar system. She is deputy principal investigator for NASA's Lucy mission examining the Trojan asteroids around Jupiter, to launch in 2021 and fly past its targets between 2025 and 2033. \n\nOlkin was born and raised in Michigan. As a child, Olkin considered a variety of careers in science and academia, including geologist, paleontologist, archaeologist, and doctor. \n\nIn college, she was pre-med before switching to engineering, earning a B.S. in Aeronautics and Astronautics from Massachusetts Institute of Technology (MIT) in 1988, then an M.S., also in Aeronautics and Astronautics, from Stanford University in 1989. Olkin then returned to MIT where she earned a Ph.D. in Earth, Atmospheric and Planetary Science in 1996. Her dissertation advisor was James L. Elliot.\n\nOlkin was a deputy project scientist on NASA's New Horizons team responsible for the July 2015 flyby of Pluto and became co-principal investigator for New Horizons' Ralph instrument, a color camera and near-infrared imaging spectrometer. \n\nShe is deputy principal investigator for NASA's Lucy mission examining the Trojan asteroids around Jupiter, to launch in 2021 and fly past its targets between 2025 and 2033. \n\nOlkin's scientific research has earned an h-index of 24. She has published over 400 papers, with more than 2,000 citations.\n\nShe is also the author of the title story \"All These Wonders\" in \"The Moth\" radio show's 20th anniversary collection, \"The Moth Presents: All These Wonders. True Stories About Facing the Unknown\"; reviewing the collection in \"The New York Times\". Michiko Kakutani described Olkin's contribution as \"a thrilling account...of last-minute emergency repairs made to the New Horizons spacecraft as it traveled three billion miles to get a close-up of Pluto.\" \n\nOlkin also engages in public outreach. In 2015, Olkin shared discoveries from her work with NASA's New Horizons mission at a TEDxDetroit talk. \n"}
{"id": "55000503", "url": "https://en.wikipedia.org/wiki?curid=55000503", "title": "Character dictionary", "text": "Character dictionary\n\nCharacter dictionary (; ), known as zìdiǎn in Mandarin Chinese, is a dictionary which lists individual Chinese characters (or kanji) and defines the characters' meanings, usages, and pronunciations. Character dictionaries are often arranged according to the shape of characters and usually include some rare characters.\n\n"}
{"id": "42242866", "url": "https://en.wikipedia.org/wiki?curid=42242866", "title": "Chibaite", "text": "Chibaite\n\nChibaite is a rare silicate mineral. It is a silica clathrate with formula SiO·n(CH,CH,CH,i-CH) (n = 3/17 (max)). The mineral is cubic (diploidal class, m) and the silica hosts or traps various hydrocarbon molecules.\n\nChibaite was first described for specimens collected from Arakawa, Minamibōsō, Chiba Prefecture, Honshu Island, Japan. The mineral was approved by the IMA in 2009.\n\n"}
{"id": "4673091", "url": "https://en.wikipedia.org/wiki?curid=4673091", "title": "De Lalande (crater)", "text": "De Lalande (crater)\n\nDe Lalande is a multiring impact crater on Venus. It has a diameter of and wall width of . The crater has an outer rim but no peak and is in close proximity to the volcano Gula Mons.\n\nThe de Lalande crater is named after the French astronomer Marie-Jeanne de Lalande (1768-1832), illegitimate daughter of astronomer Joseph Jerome de Lalande (1732-1807).\n"}
{"id": "4545466", "url": "https://en.wikipedia.org/wiki?curid=4545466", "title": "Double salt", "text": "Double salt\n\nDouble salts are salts containing more than one cation or anion, and are obtained by combination of two different salts which were crystallized in the same regular ionic lattice. Examples of double salts include alums (with the general formula MM[SO]·12HO) or Tutton's salts (with the general formula [M]M[SO]·6HO). Other examples include potassium sodium tartrate, ammonium iron(II) sulfate (Mohr's salt), and bromlite. Aluminium sulfacetate, , is an example of a double salt with two distinct anions; another is the salt where hydroxyl-centred triangles of thallium, , are a recurring structural motif.\n\nDouble salts should not be confused with complexes. When dissolved in water, a double salt completely dissociates into simple ions while a complex does not; the complex ion remains unchanged. For example, KCeF is a double salt and gives K, Ce and F ions when dissolved in water, whereas K[YbI] is a complex salt and contains the discrete [YbI] ion which remains intact in aqueous solutions. It is therefore important to indicate the complex ion by adding square brackets \"[ ]\" around it. Double salts are distinct from mixed-crystal systems where two salts co-crystallise; according to an Ida Freund publication from 1904 reprinted in 2014, the former involves a chemical combination with fixed composition, whereas the latter is a mixture.\n\nIn general, the properties of the double salt formed will not be the same as the properties of its component single salts.\n"}
{"id": "5196720", "url": "https://en.wikipedia.org/wiki?curid=5196720", "title": "Eureka! (TV series)", "text": "Eureka! (TV series)\n\nEureka! is a Canadian educational television series which was produced and broadcast by TVOntario in 1980 and 1981. The series was narrated by Billy Van, and featured a series of animated vignettes which taught physics lessons to children. It is currently available online.\n\nEureka! is a series of animated shorts that illustrate concepts in physics. Each program takes a simple and direct approach to the subject matter; while the basic concepts are explained in a voice-over, cartoon characters and a variety of animated objects demonstrate the principles on the screen. Constant review and reinforcement make the message clear; as a result, the study of physics becomes easy and accessible - even to viewers without a solid background in the subject. Basic formulae and concepts are introduced with a recap of what was learnt in the previous episode to build knowledge on a topic and create connections.\n\nAnimation - Grafilm Productions Inc.\n\nDesign - Joe Meluck\n\nEducational Consultants - John Kuropatwa, Paul Henshall, Bryan Kaufman, Ernie McFarland, Michael Broschart\n\nUnit Manager - Vickie Gilchrist\n\nProduction Assistant - George Pyron\n\n30 episodes were produced. All of the episodes are five minutes in length.\n\n\n\n\n\n\n\n"}
{"id": "22990622", "url": "https://en.wikipedia.org/wiki?curid=22990622", "title": "Exploratorium", "text": "Exploratorium\n\nThe Exploratorium is a museum in San Francisco that allows visitors to explore the world through science, art, and human perception. Its mission is to create inquiry-based experiences that transform learning worldwide. It has been described by the \"New York Times\" as the most important science museum to have opened since the mid-20th century, an achievement attributed to \"the nature of its exhibits, its wide-ranging influence and its sophisticated teacher training program\". Characterized as \"a mad scientist's penny arcade, a scientific funhouse, and an experimental laboratory all rolled into one\", the participatory nature of its exhibits and its self-identification as a center for informal learning has led to it being cited as the prototype for participatory museums around the world.\n\nThe Exploratorium was founded by physicist and educator Frank Oppenheimer and opened in 1969 at the Palace of Fine Arts, its home until January 2, 2013. On April 17, 2013, the Exploratorium reopened at Piers 15 and 17 on San Francisco's Embarcadero. The historic interior and exterior of Pier 15 was renovated extensively prior to the move, and is divided into several galleries mainly separated by content, including the physics of seeing and listening (Light and Sound), Human Behavior, Living Systems, Tinkering (including electricity and magnetism), the Outdoor Gallery, and the Bay Observatory Gallery, which focuses on local environment, weather, and landscape.\n\nSince the museum's founding, over 1,000 participatory exhibits have been created, approximately 600 of which are on the floor at any given time. The exhibit-building workshop space is contained within the museum and is open to view. In addition to the public exhibition space, the Exploratorium has been engaged in the professional development of teachers, science education reform, and the promotion of museums as informal education centers since its founding. Since Oppenheimer's death in 1985, the Exploratorium has expanded into other domains, including its 50,000-page website and two iPad apps on sound and color. It has also inspired an international network of participatory museums working to engage the public with general science education. The new Exploratorium building is also working to showcase environmental sustainability efforts as part of its goal to become the largest net-zero museum in the country.\n\nThe Exploratorium offers visitors a variety of ways—including exhibits, webcasts, websites and events—to explore and understand the world around them. In 2011, the Exploratorium received the National Science Board 2011 Public Service Science Award for its contributions to public understanding of science and engineering.\n\nThe Exploratorium is the brainchild of Frank Oppenheimer, an experimental physicist and university professor. Oppenheimer, who worked on the Manhattan Project with his brother J. Robert Oppenheimer, was diverted from an academic career when he was forced to resign from his position at the University of Minnesota in 1949 as a result of an inquiry by the House Un-American Activities Committee. He was blacklisted from academic positions across the country, and withdrew with his family to run a Colorado cattle ranch for almost a decade.\n\nOppenheimer also began lending a hand with science projects of local high school students, eventually becoming the sole science teacher at the high school in Pagosa Springs, Colorado. The field trips and experiments he did with his high school students would become a blueprint for the hands-on methods of teaching and learning he would later bring to the Exploratorium.\nWhen Oppenheimer was invited to join the University of Colorado's physics department in 1959, he found himself less interested in traditional laboratory research and much more interested in exploring methods of provoking curiosity and inquiry. He received a grant from the National Science Foundation, which he used to build models of nearly a hundred science experiments. This \"Library of Experiments\" would become the core of the Exploratorium exhibit collection, and was the forerunner of the \"Exploratorium Cookbook\", a manual explaining how to build these basic science exhibits.\n\nConvinced of the need for public museums to supplement science curricula at all levels, he toured Europe and studied museums on a Guggenheim Fellowship in 1965. Three European museums, encountered during that year, served as important influences on the founding of the Exploratorium: the Palais de la Découverte, which displayed models to teach scientific concepts and employed students as demonstrators, a practice that directly inspired the Exploratorium's much-lauded High School Explainer Program; the South Kensington Museum of Science and Art, which Oppenheimer and his wife visited frequently; and the Deutsches Museum in Munich, the world's largest science museum, which had a number of interactive displays that impressed the Oppenheimers.\n\nBack in the United States, Oppenheimer was invited to do the initial planning for a new branch of the Smithsonian, but he turned it down to work on what he called his \"San Francisco project\". In 1967, the Oppenheimers came to San Francisco with a view towards opening an independent museum of their own. Oppenheimer sought funding and support for the endeavor using a grassroots approach, bringing a written proposal and some handmade exhibits with him as he visited scientists, businesses, city and school officials, relatives, and friends. Many prominent scientists and cultural figures endorsed the project, and the offers of support in conjunction with a $50,000 grant from the San Francisco Foundation made the museum realizable.\n\nIn late August 1969, the Exploratorium opened with little fanfare at the Palace of Fine Arts. Oppenheimer \"simply opened the doors\". Although the building needed many improvements, Oppenheimer couldn't afford to make the changes, and decided to allow the public to come and watch exhibits being built and changes being made as part of the participatory ethos of the institution. An early proposal would have erected a wall between the workshop where exhibits were being developed and the main visitor areas. Instead, Oppenheimer insisted that the workshop be placed without a wall, right next to the main entrance so that visitors could experience \"the way a shop smells when you burn the wood in a saw, or smell the oil from a lathe\". Above the workshop was a sign, made by the wife of George Gamow, a physicist, educator, and friend of Oppenheimer who had died just a year before the opening of the Exploratorium. The inscription said \"Here Is Being Created the Exploratorium a Community Museum Dedicated to Awareness\". (Today, a copy of this motto is inscribed above the main entrance to the new Exploratorium at Pier 15.)\n\nOppenheimer served as the museum’s director until his death in 1985. Dr. Robert L. White served as director from 1987 to 1990. Dr. Goéry Delacôte served as executive director from 1991 until 2005. Dr. Dennis Bartels served as executive director of the Exploratorium from 2006 to 2016. In June 2016, the Exploratorium welcomed its newest director, Chris Flink. The museum has expanded greatly since the 1980s, increasing outreach, expanding programs for educators, creating an expanded Web presence, and forming museum partnerships around the world.\n\nThe Exploratorium relocated from the Palace of Fine Arts to Piers 15 and 17, located between the San Francisco Ferry Building and Pier 39 along the San Francisco Embarcadero, in April 2013. The Piers location was identified by Goéry Delacôte and then-board chairman Van Kasper as a potential space for relocation in 2004. In 2005, the San Francisco Board of Supervisors passed a resolution exempting the museum’s 66-year lease of the piers from San Francisco’s competitive bidding process due to its unique nature as a cultural and educational institution. Groundbreaking for the project, which required substantial construction and renovation, occurred on October 19, 2010. The Exploratorium holds a 66-year lease on the piers with the Port of San Francisco. Exhibits are currently only viewable at the Pier 15 campus; Pier 17 houses some staff, with the option for future expansion.\n\nPiers 15 and 17 are historic piers, built in 1931 and 1912 respectively. In 1954, the area between the piers was infilled and paved over. This infill was removed as part of the construction phase, restoring the space between the piers to public plazas, a pedestrian bridge, and open water.\n\nThe Exploratorium campus comprises of indoor and outdoor exhibit space, and includes of freely accessible public space. The exhibits are housed in and around Pier 15, which extends over over the Bay.\n\nThe Exploratorium at Pier 15 was designed by architecture firm EHDD. Nibbi Brothers served as the General Contractor, and AGA (Architectural Glass and Aluminum) as the Glazing Contractor. The piers had been neglected for decades leading up to the Exploratorium’s move, and extensive renovation and repair was required. Nearly two thirds of the pilings under Pier 15 were repaired, including almost every piling needed to provide structural integrity, and new pilings were sunk. The removal of the parking lot between the piers was done slowly over the two years of construction, and the debris from the removal was captured and recycled. Several pilings were left in the water between the piers, both for aesthetic reasons and to support future exhibits.\n\nAn effort was made in the construction of the new location to preserve the historic elements of Pier 15. The Bay Observatory was the only new structure added to the site. The east end of the pier was cleaned of lead paint, revealing historic lettering underneath; designers chose to preserve the lettering rather than paint it over. As a result, the traces of the shipping lines that originally frequented the pier can still be seen. Some of the preservation efforts presented challenges in design, however; historic windows created energy losses that had to be offset elsewhere, and the historic interior trusswork was mainly restored rather than removed, meaning that the upper-level staff offices had to be built around them.\n\nOther challenges to the design of the facilities were presented by the museum’s sustainability initiatives. The use of natural light whenever possible challenged exhibit designers relying on carefully controlled light levels; this was solved by using curtains and glare-reducing paint colors. Other conflicts between construction and energy use included the glass in the Observatory, which would have presented a problem in cooling the building on warm days. This was overcome by adding fritted glass to the windows in thin horizontal lines through the panes to decrease the transparency without affecting the views. The fritting also makes the reflective surfaces of the Bay Observatory safe for birds.\n\nPier 15 incorporated two seismic joints as part of its seismic retrofit, one separating the Bay Observatory from the Pier 15 shed and the other separating the entire pier from the land. This second joint ensures that the entire pier will move independently from the land mass in the event of an earthquake, significantly reducing the potential torsional stress. The café at the west end of the Exploratorium is named the Seismic Joint in honor of the joint, which cuts through the area of the building where the café is situated.\n\nThe aesthetic of the project was defined as \"industrial naval chic\" in keeping with the pier’s history.\n\nThe design aesthetic for both the Seismic Joint and Seaglass Restaurant was created by designer Olle Lundberg and based on the exhibit \"Color of Water\". The bar at Seaglass features a specially designed version of Exploratorium artist Shawn Lani’s exhibit \"Icy Bodies\".\n\nThe Exploratorium at Pier 15 has a net-zero energy goal as part of its overall sustainability efforts. Setting this net-zero goal means that, while in operation, The Exploratorium will produce more energy on-site than it will consume on an annual basis. The museum highlights its sustainability efforts in visible ways throughout the museum as part of a stated intention to lead by example.\n\nThe Exploratorium, in order to reach its net-zero energy goal, produces energy with a roof-mounted array of photovoltaics. There are 5,874 PV modules on the roof, totaling , with a projected year-1 yield of 1.3 MW-AC/square foot (13.9 MW-AC/m), or a total year-1 yield of 2,113,715 kWh. Any surplus energy generated is intended to be fed back into the utility grid, as the projected annual energy use for the building totals at 1,275,936 kWh.\nIn addition to solar power, the museum makes use of an HVAC system that takes advantage of the relatively constant, moderate temperature of the bay water under the piers, which is , to heat and cool the building. The bay water is filtered and sterilized before it is brought into a cistern below the pier, where it is held for use. When needed, the bay water is moved to a titanium heat exchanger, of which the building has two, where it is either used to heat or cool water that is cycled through a system of thermally activated radiant slabs. There are of plastic tubing in the radiant heating system in the floor, creating 82 different heating and cooling zones with distinct control systems. After the bay water passes through the heat exchangers, it is returned to the San Francisco Bay as allowed by a permit issued by the California Regional Water Quality Control Board.\n\nMost of the energy savings are expected to come from using the bay water as a heat sink for the building’s cooling needs. When the temperature of the bay water is below that of the chilled water return from the radiant pipes, which is the case for most of the year in the Bay Area’s temperate climate, the system works in waterside economizer mode. In this mode, the cooling loads are met either entirely or partially through passive heat exchange between the colder bay water and the warmer return water, greatly reducing the building’s energy needs.\n\nThe Exploratorium at Pier 15 has a separate system for its ventilation needs, pairing a dedicated outdoor air system (DOAS) with displacement ventilation distribution to bring outdoor air into the building. By integrating radiant heating and cooling and displacement ventilation, the Exploratorium has greatly reduced the portion of its HVAC system that relies on forced air. Reducing the size of a building’s forced air system has the associated benefits of both lower energy loads as well as reduced ductwork, both of which are cost-saving.\n\nThe Exploratorium has multiple features designed to reduce its water consumption. Two large cisterns under the structural beams connecting the southeast pilings capture up to of rainwater and fog runoff for reuse in the facility. The plumbing is designed for water conservation, with waterless urinals and dual-flush toilets projected to save an annual million gallons of water. Additionally, the bay water heating and cooling system is estimated to save two million gallons of potable water a year by eliminating the need for traditional evaporative cooling towers.\n\nThe Exploratorium at Pier 15 also makes use of natural light in the effort to reduce energy loads. The existing building had many clerestory windows and an overhead skylight that runs the length of the interior space. In compliance with historic preservation requirements, the building’s façade was left mostly unchanged, allowing for much of the interior space to benefit from the existing architecture’s ample daylight. The retrofit did include the addition of high performance glazing to the existing windows.\n\nIn January 2014, the Exploratorium was awarded LEED Platinum certification. According to the \"New York Times\", \"After a two-year post-opening shakedown period of monitoring and adjusting the systems, the Exploratorium hopes to become the largest net-zero-energy-use museum in the United States and possibly the world.\" The Exploratorium is also turning its commitment to sustainability into a learning experience by showcasing elements of the building’s green design on site.\n\nThe new site contains over 600 exhibits, 25% of which were developed specifically for the Pier 15 site. With the exception of some art installations, all exhibits are developed and made onsite. The indoor and outdoor spaces are divided into six galleries, each highlighting a specific content group. Many exhibits are mobile, however, and move among different galleries; similarly, not all exhibits fall into distinct categories.\n\nExhibits cover a range of subject areas, including human perception (such as vision, hearing, learning and cognition), the life sciences, physical phenomena (such as light, motion, electricity, waves and resonance, and magnetism), local environment (water, wind, fog, rain, sun, and other elements, as well as cityscape, landscape, and the flora and fauna of the Bay) and human behavior (such as cooperation, competition, and sharing).\n\nThe West Gallery focuses on human behavior. Its signage and exhibits encourage visitors to play with perception; investigate memory, emotion, and judgment; and experiment with how people cooperate, compete, and share. It holds exhibits such as \"Poker Face\" (partners try to assess when someone is bluffing), \"Trust Fountain\" (an experimental exhibit from the museum’s National Science Foundation-funded Science of Sharing project, this two-person drinking fountain is based on the Prisoner’s dilemma, a classic scenario centering on negotiation and trust), and the \"Tactile Dome\", a pitch-black environment visitors explore by touch, which was originally designed by August Coppola. The West Gallery also included the temporary exhibition \"The Changing Face of What is Normal: Mental Health\", which showcased the personal artifacts of patients from the now-decommissioned Willard Psychiatric Center, which was on view through April 2014.\n\nThe West Gallery also houses the Kanbar Forum, a cabaret-style theater that will host music events, science lectures, and other programs when it opens in summer 2013.\n\nThe South Gallery is a workshop area where visitors can engage in learning through hands-on making, located directly across from the Exploratorium's own internal exhibit workshop, which is also open to their view. Oppenheimer wanted visitors to be able to “smell the oil”, and insisted that the usually hidden exhibit-building activities be on display as an exhibit in its own right. Exhibits in the South Gallery highlight a DIY aesthetic, and include \"Animation Stations\" where visitors can make their own stop-motion films.\n\nArtworks on display include the \"Tinkerer’s Clock\" (a 22-foot-high clock constructed by artist Tim Hunkin, with figurines in his noted cartoon style that can be manipulated by visitors and unfold into a clockface on the hour); and \"Rolling Through The Bay\" (a sculpture made by artist Scott Weaver over the course of 37 years, utilizing over 100,000 toothpicks and depicting many of the Bay Area’s iconic landmarks, through which a ping-pong ball can roll on one of several different \"tours\").\n\nThe Bechtel Central gallery houses many of the \"classic\" Exploratorium exhibits, including many of those that have been on display since the very earliest years of the museum. It includes a mix of new and old exhibits that investigate physics and the perception of light, color, and sound, such as \"Sound Bite\" (a demonstration of hearing with the jawbone instead of the ears) and \"Bright Black\" (a trick of perception convinces viewers that an object is white when it is almost entirely black).\n\nThe East Gallery houses a much-expanded selection of life sciences exhibits. Many exhibits relate directly to the immediate local environment, such as the \"Glass Settling Plate\" (barnacles and other creatures are grown on a plate in the Bay, then put live under a mobile microscope to be observed from both above and below) and the \"Algae Chandelier\" (visitors can pump air to nourish overhead tanks of colorful phytoplankton). Other exhibits explore different biological systems and processes, such as the imaging station with mouse stem cells, the live cow’s eye dissections, and the \"Live Chicken Embryo\" (one of the oldest of the Living Systems exhibits, showing live chicken embryos at different stages of development).\n\nThe Bay Observatory building is the only new building constructed on the Exploratorium’s campus. It holds the Seaglass restaurant on its lower level and exhibits on the upper level relating to the waterfront and the cityscape. The gallery focuses on what visitors can see in real time, including the movement of clouds and tides, the changing waterfront, the movement of ships, and interpretation of oceanographic data. The Observatory has glass walls on all four sides to facilitate observation. Many of the exhibits were developed specifically for the location, such as \"Oculus\" (a circular opening in the ceiling that allows the entire gallery to be used as a timepiece, tracking seasons, solstices, and the sun’s movement), \"Visualizing the Bay\" (a 3-D topographic map of the Bay Area that allows visitors to see real data mapped over the landscape, such as the movement of fog and the salinity of the Bay over the course of days or years), and the \"Map Table\" (an assortment of historic and contemporary maps and atlases displaying different views and perspectives on the landscape).\n\nThe Bay Observatory also houses the Wired Pier project, which consists of more than a dozen sensors on and around the Bay Observatory that stream real-time data about the surrounding environment, such as quality of air and bay water, weather, tides and pollution, and compile it into interactive visualizations.\n\nThe Outdoor Gallery comprises the north, south, and east aprons of Pier 15, and extends through both ticketed and unticketed space. Focus is on direct interaction with the Bay environment, which can be seen in exhibits such as \"Color of Water\" (an installation of 32 distinct color swatches suspended below the rail surrounding the pier so that visitors can investigate the changing colors of the Bay’s water). Another notable exhibit is \"Remote Rains\", which allows visitors to choose a past rainstorm as profiled by the Hydrometeorology Testbed, which is then recreated by a rain machine that duplicates the frequency, size, and velocity of the raindrops, giving a tangible experience of NOAA research data on storms.\n\nAlong the publicly accessible bridge connecting Piers 15 and 17, artist Fujiko Nakaya created an installation called \"Fog Bridge #72494\" that creates bursts of fog for six minutes every half-hour as the first in a series of large-scale temporary installations called \"Over the Water\". The \"Fog Bridge\" is long and makes use of 800 nozzles to create the fog, which Nakaya hopes will inspire visitors to pay attention to the nature of one of San Francisco’s best-known weather patterns. Although originally slated to be temporary, it is now on permanent display. A desalination system, located in Pier 17, conditions bay water for use in the artwork. \n\nThe Exploratorium campus includes of publicly accessible open space. This includes the plaza facing on the Embarcadero, the connector bridge between Piers 15 and 17 where \"Fog Bridge # 72494\" is installed, the south apron of Pier 17, and the east and south aprons of Pier 15. This public space overlaps with the Outdoor Gallery, and includes some notable exhibits, such as the \"Aeolian Harp\" (an expanded version of the original installation by Doug Hollis on the roof of the Exploratorium at the Palace of Fine Arts, first created in collaboration with Frank Oppenheimer in 1976) and the \"Bay Windows\" (visitors spin disks filled with samples of Bay mud, sand, and gravel gathered from five distinct regions of the Bay itself).\n\nThe lower level of the Bay Observatory Building houses the Seaglass Restaurant, which, like the Seismic Joint Cafe, is open to unticketed members of the public. Both the Seismic Joint and Seaglass are run by Loretta Keller, chef-owner at Coco500, in partnership with Bon Appetit Management Company.\n\nThe Exploratorium seeks to bring hands-on inquiry to education, including training teachers in the teaching of science. Between 1995 and 2012, an estimated 6,400 educators from 48 states and 11 countries directly participated in Exploratorium workshops.\n\nThe Teacher Institute, founded in 1984, is an Exploratorium-based professional development program geared towards middle and high school science teachers. In addition to providing workshops at the museum that teach hands-on and inquiry-based teaching methods, it provides coaches and support for novice teachers. Studies have shown that while 30 to 50 percent of new teachers leave the profession within five years, the retention rate for teachers who go through the Teacher Institute is 85 to 90 percent.\n\nThe Teacher Institute is also home to the Iron Science Teacher, a national competition that celebrates innovation and creativity in science teaching, which originated at the Exploratorium in San Francisco. Parodying the cult Japanese TV program, \"Iron Chef\", this competition showcases science teachers as they devise classroom activities using a particular ingredient — an everyday item such as a plastic bag, milk carton, or nail. Contestants are currently or formerly part of the Exploratorium's Teacher Institute and compete before a live audience for the title of \"Iron Science Teacher\". Shows are also archived on the Exploratorium's website.\n\nTwo out of three teachers applying were being turned away due to space limitations, by the time the Exploratorium closed at its former location; following the move to the Piers, the Exploratorium has been expanding its professional development for teachers through the Teacher Institute. , two MOOC courses were also being made available through MOOC provider Coursera. One course integrates engineering into middle and high school STEM classrooms, and the other integrates making and tinkering activities into elementary and middle school classrooms.\n\nThe Exploratorium operates several programs centering on informal learning. The Institute for Inquiry (IFI) is a professional development program of the Exploratorium geared towards educators, scientists, administrators, and policymakers. The Institute is a recipient of National Science Foundation funding and designs programs, materials and tools to help leaders in the science education community further the role of inquiry in elementary science education and strengthen reform efforts. It consists of workshops and an online library of resources available to participants in the Institute.\n\nThe Institute for Inquiry partnered with the Sonoma Valley Unified School District on a program combining science education with English Language Development (ELD). Data from the two-year pilot study showed that a professional development program designed to help teachers integrate ELD strategies into science lessons had a significant, measurable impact on the achievement of students in both ELD and in science.\n\nThe Center for Informal Learning and Schools (CILS) is a collaboration between the Exploratorium, the University of California Santa Cruz, and King's College London. CILS studies the intersection between museums and schools as centers of informal learning with the intention of understanding how informal science learning occurs and how informal educational centers such as the Exploratorium can contribute to science education reform.\n\nThe Tinkering Studio began in 2008 as an in-house program geared towards maker culture and a “think with your hands” approach. It is housed within the museum in a dedicated space in the South Gallery, where it runs free do-it-yourself activities for museum visitors; it also shares its work with a larger audience of educators in afterschool programs, schools, museums and other learning environments. It is being cited as a prototype for similar programs across the globe, including South Korea, Canada, India, and Saudi Arabia. The Exploratorium also operates as afterschool tinkering program in partnership with San Francisco chapters of the Boys and Girls Club. In 2012 the Exploratorium was awarded a grant to create the California Tinkering Network, in collaboration with the Community Science Workshops, Techbridge, the Discovery Science Center, the California Afterschool Network, and the California STEM Learning Network. These organizations partner with over 20 local afterschool or summer programs to provide STEM-enriched activities for children in underserved communities. The initiative was designed to test an adaptable model for providing tinkering activities to promote learning and development in an afterschool setting.\n\nThe Exploratorium also houses a number of other educational resources. These include the Learning Commons, a library and media resource center that houses a collection of print and digital science teaching resources for use by regional educators; a webcast studio, located in the Central Gallery, which produces 75 educational Webcasts from the museum and locations around the world annually, including a live webcast of the Mars Curiosity Rover launch and landing; and Lifelong Learning, which creates educational programming for children, teens, family groups, and adults. Lifelong Learning programs further the Exploratorium’s stated dedication to informal learning and the museum as teaching tool, and include day camps, workshops for families, the Homeschool Science series (in-house classes geared specifically towards homeschooled students), the Girl’s Science Institute (multi-day workshops geared towards girls 9-11), and excursions for adults. The Exploratorium has also published a number of books, and many of the 50,000 pages on its website are hands-on activity ideas or science experiments in the museums’ signature open-ended style.\n\nThe Exploratorium operates several educational outreach programs. The Community Outreach Program works with community organizations to provide exhibit-based educational activities for underserved children and families in the local community. The Exploratorium is also home to XTech, a science education program for underserved middle school students. Begun in 2006, XTech was primarily funded by a National Science Foundation grant and provided afterschool activities in science, engineering, and technology in partnership with two community-based organizations in the Bay Area. XTech serves over 100 students a year in addition to 10-15 youth facilitators.\n\nThe Exploratorium Explainer program, which has been operating since the museum opened, hires and trains high school students and young educators each year. The program tripled its capacity, hiring 300 Explainers, following the relocation to Pier 15 in 2013. The Explainers function essentially as docents. There are two types of Explainers: High School Explainers, who are teenagers, and Field Trip Explainers, who are college students and young educators. The program was conceived by Frank Oppenheimer in the early days of the museum. He wanted to provide a visitor experience that was a learning experience in a museum context and allowed for guesswork and the absence of \"correct\" answers. He felt young people would be more capable than adults at conveying the open-ended experience he was looking for. His plan with the Explainers was to \"loosen up the whole feeling of learning.\" Oppenheimer also intended the program to allow students to experience learning outside the framework of their school systems. The Explainers come from a highly diverse array of socioeconomic backgrounds, and he hoped they would bring families and friends who would not otherwise be likely to visit a museum. Both the High School and Field Trip Explainers are paid positions.\n\nThe Explainer Program was inspired by the staff demonstrations Frank observed at the Palais de la Decouverte, although the facilitators at the Palais when Oppenheimer visited were either graduate students or practicing scientists. The success of the Exploratorium’s Explainer program led the Palais to eventually hire teenage explainers of their own. Former Explainers often cite their experiences at the Exploratorium as defining elements of their success, including several notable tech CEOs.\n\nDespite being generally thought of as a science museum, the Exploratorium has always incorporated both science and art. As early as 1966, Frank Oppenheimer presented a paper discussing the connections between art and science, and the role of a museum in appealing to both casual visitors and serious students of all ages.\n\nThe formal artist in residence program was started in 1974, but artworks such as Bob Miller’s \"Sun Painting\" were commissioned shortly after the museum was opened in 1969. Since the founding of the artist in residence program, over 250 artworks in various disciplines have been created.\n\nEach year, the museum invites ten to twenty artists to participate in residencies ranging from two weeks to two years. Artists-in-residence work with staff and the visiting public to create original installations, exhibits, or performances. Artists are given a stipend, housing, travel expenses, and technical support, and they have at their disposal the Exploratorium's full array of metal and woodworking shops and materials. Two artists-in-residence who went on to become staff members have been awarded MacArthur Fellowship \"genius\" grants: Walter Kitundu and Ned Kahn.\n\nThe new Embarcadero campus opened with more than 40 pieces by prominent artists, including Douglas Hollis, Golan Levin, Lucky Dragons, Amy Balkin, and Fujiko Nakaya. The Center for Art and Inquiry, a new project at the new location, is an initiative to catalyze and orchestrate art across the museum.\n\nThe Exploratorium has an equally long history with musical, film and other performances. Participating artists and performers included Laurie Anderson, John Cage, Philip Glass, Steve Reich, Brian Eno, Ali Akbar Khan, Trimpin, and The Mermen.\n\nIn addition to the artists in residence, the museum’s Osher Fellows Program hosts 4-8 resident scholars, scientists, educators, and artists every year. Notable Osher Fellows have included Walter Murch, James Crutchfield, Christian de Duve, Arthur Ganson, Tim Hunkin, Lewis Hyde, Evelyn Fox Keller, Guillermo Gómez-Peña, Rosamond Wolff Purcell, Oliver Sacks, Mierle Laderman Ukeles, and Juan Felipe Herrera.\n\nThere are hands-on activities for younger visitors (under 14 - needs confirmation). The pendulum art activity has a youngster picking four different color markers. The youngster then pushes a large board suspended by 4 wires to develop a pattern. At the youngster's say so, the attendant places a selected color into an arm and lowers it onto a white paper weighted to the board. The marker patterns the movement of the board. After 4 markers are done, the project is complete, signed by the 'young artist', and taken home.\n\nIn 2007, the Exploratorium was highlighted in the book \"Forces For Good\" as one of the 12 most effective non-profits in the United States, and was the only West Coast institution and only museum to make the list. It has inspired science museums worldwide, from the Reuben H. Fleet Science Center in San Diego to the Garden of Archimedes in Florence, Italy. In 2003, \"The Oxford Companion to the History of Modern Science\" noted that about 400 science centers in 43 countries were established after the example of the Exploratorium.\n\nIn 2012, 570,000 people visited the Exploratorium; 55% were adults and 45% were children. Geographically, 52% were from the Bay Area, 24% from the rest of California, 14% from other states, and 10% outside the US. Some 36% received free or discounted admission, and 44,000 attended on free admission days. Prior to the relocation, 97,000 students and chaperons visited the museum each year; of these, 67,000 participated in the Field Trip program. It was estimated that 180 million people visited Exploratorium exhibits at science centers and other locations worldwide.\n\nAnnual attendance at the new Pier 15 location was anticipated to exceed one million. However, in August 2013, the \"New York Times\" reported that although attendance had soared, revenues still fell short of goals, forcing the Exploratorium to lay off around 1/5 of its staff. Museum officials had decided not to make a big publicity push upon reopening, fearing the overcrowding experienced by the California Academy of Sciences in 2008, but this was later viewed as \"an opportunity lost\". A staffer (expecting himself to be laid off) observed that the Exploratorium seemed to be \"moving from visionary, internally developed work to work-for-hire for other museums around the world”, as the management scrambled to earn more money from consulting for other organizations to replace unrealized admissions revenue. On April 17, 2014 the Exploratorium announced that attendance in the first year since re-opening was 1.1 million visitors, and that other key statistics had doubled or tripled.\n\nOnline since 1993, the Exploratorium was one of the first museums to build a site on the World Wide Web. The site serves 13 million visitors each year. It has received six Webby Awards since 1997, including four for Best Science Website and one for Best Education website, and has been an honoree an additional ten times.\n\nThe Exploratorium's website is an extension of the experiences on the museum's floor and provides a large number of hands-on activities and exhibits for online-only visitors. The Exploratorium also broadcasts live video and/or audio directly from the museum floor (or from satellite feeds in the field, at such locations as Antarctica or the Belize rainforest) onto the Internet from its Webcast Studio. Webcasts provide access to special events, scientists, and other museum resources for audiences on the Web. Visitors to the website can hear or view interviews with scientists, \"meet\" interesting people, or tour unique locations from factories to particle accelerators.\n\nThe Exploratorium has additionally released two free iPad apps, Sound Uncovered (2013) and Color Uncovered (2011), which assist in making its unique educational model as accessible as possible. Color Uncovered has been downloaded more than a million times.\n\nThe Exploratorium has a number of partnerships with other organizations. One notable current partnership is with the National Oceanographic and Atmospheric Administration (NOAA). NOAA research vessels periodically berth at the end of Pier 15 and use the working biological labs featured in the museum to bring their research and data to the public. A number of exhibits at the Exploratorium, such as \"Remote Rains\", are based on NOAA data. NOAA scientists additionally provided training for the Explainer program to assist Explainers in explaining science to visitors based on the NOAA’s areas of research.\n\nThe Exploratorium Global Studios initiative is an entrepreneurial endeavor that shares resources, exhibits, and research with foreign governments, universities, partner museums, libraries, hospitals, and other public and private entities around the world. One area of significant current activity for the Global Studios initiative is the Middle East, where it hopes to assist countries that are making long-term investments in education and transitioning to more information-based economies. For example, the Tinkering Studio visited a science festival in Al Khobar, Saudi Arabia in the summer of 2012, where they trained a group of teachers to help thousands of festival participants to experience the hands-on learning style favored by the Exploratorium.\n\nThe Exploratorium is open Tuesday through Sunday from 10am-5pm, with adults-only evening hours from 6pm-10pm on Thursdays (for visitors age 18+). The entire museum is wheelchair-accessible.\n\nThe Exploratorium is located on the Embarcadero at Green Street, between Fisherman’s Wharf and the Ferry Building. The museum is accessible by multiple modes of public transit, including BART, Muni streetcar, bus, taxi, and pedicab. It is located from the Embarcadero BART and MUNI stations, and has its own stop on the F streetcar line. The Exploratorium does not own or operate any parking lots, but is located near several publicly accessible lots and on-street parking, including a lot directly across the street, operated by the Port of San Francisco.\n\nCommunity π Day started at the Exploratorium by Larry Shaw and is celebrated annually on 3/14 (March 14). Iron Science Teacher competition (like Iron Chef) showcases science teachers as they devise classroom activities using a particular ingredient. Monthly events include \"Full-Spectrum Science with Ron Hipschman\". Every Thursday is \"After Dark\" for adults.\n\nThe Exploratorium maintains exhibits in public Bay Area spaces. The Outdoor Exploratorium consists of 14 different exhibits relating to the local environment, all placed outside in the Fort Mason area and accessible to the general public.\n\nThe \"Wave Organ\" is another notable public artwork of the Exploratorium. Created by former staff artist Peter Richards, this acoustic sculpture is situated on a point of land jutting into the San Francisco Bay not far from the Exploratorium’s original Palace of Fine Arts location.\n\n"}
{"id": "9025199", "url": "https://en.wikipedia.org/wiki?curid=9025199", "title": "Golden-crowned sifaka", "text": "Golden-crowned sifaka\n\nThe golden-crowned sifaka or Tattersall's sifaka (\"Propithecus tattersalli\") is a medium-sized lemur characterized by mostly white fur, prominent furry ears, and a golden-orange crown. It is one of the smallest sifakas (genus \"Propithecus\"), weighing around and measuring approximately from head to tail. Like all sifakas, it is a vertical clinger and leaper, and its diet includes mostly seeds and leaves. The golden-crowned sifaka is named after its discoverer, Ian Tattersall, who first spotted the species in 1974. However, it was not formally described until 1988, after a research team led by Elwyn L. Simons observed and captured some specimens for captive breeding. The golden-crowned sifaka most closely resembles the western forest sifakas of the \"P. verreauxi\" group, yet its karyotype suggests a closer relationship with the \"P. diadema\" group of eastern forest sifakas. Despite the similarities with both groups, more recent studies of its karyotype support its classification as a distinct species.\n\nFound in gallery, deciduous, and semi-evergreen forest, its restricted range includes 44 forest fragments, totaling an area of , centered on the town of Daraina in northeast Madagascar. Its estimated population is 18,000 individuals. It is primarily active during the day, although it also tends to be active at dawn and dusk during the rainy season. It sleeps in tall emergent trees and is preyed upon by the fossa. The golden-crowned sifaka lives in groups of around five to six individuals, containing a balanced number of adult males and females. Scent is used to mark territories, which are defended by growling, chasing, and ritualistic leaping displays. Reproduction is seasonal, with gestation lasting six months and lactation lasting five months. Infants are weaned during the wet season to ensure the best chances of survival.\n\nThe small range and fragmented populations of this species weigh heavily on its survival. Forest fragmentation, habitat destruction, poaching, slash-and-burn agriculture, and other human factors threaten its existence. The golden-crowned sifaka is listed by the IUCN Red List as \"Critically Endangered\". Its range was originally not covered by any national parks or protected areas in Madagascar, but a new protected area was established in 2005 to include a portion. Attempts have been made to keep the golden-crowned sifaka in captivity at the Duke Lemur Center in Durham, North Carolina. The small colony was maintained from 1988 to 2008. In Madagascar, lawlessness resulting from the 2009 political coup led to increased poaching of this species, and many were sold to local restaurants as a delicacy.\n\nThe golden-crowned or Tattersall's sifaka (\"Propithecus tattersalli\"), known locally as \"ankomba malandy\" (or \"akomba malandy\", meaning \"white lemur\"), was discovered in 1974 north of Vohemar in northeast Madagascar by Ian Tattersall, who observed but did not capture the animal. Unsure of its classification, Tattersall provisionally considered it a variant of the silky sifaka in his 1982 book, \"The Primates of Madagascar\", citing its mostly off-white to yellowish fur, but also noting its uncharacteristic orange crown patch and tufted ears. Driven by a report in 1986 that the forest where Tattersall had observed this unique sifaka was contracted to be clear-cut for charcoal production, a research team from the Duke Lemur Center, led by Elwyn L. Simons, obtained permits to capture specimens for a captive breeding program. Simons and his team were the first to capture and observe the golden-crowned sifaka, formally describing it as a new species in 1988 and naming it in honor of Tattersall. The specimens were found northeast of Daraina, a village in the northeast corner of Madagascar.\n\nThere have been conflicting studies regarding the taxonomic status of the golden-crowned sifaka. When described by Simons in 1988, size, vocalizations, and karyotypes (the number and appearance of chromosomes) were compared with the other sifakas. In terms of size, general morphology, and vocalizations, the golden-crowned sifaka is more comparable to the western forest sifakas (known as the \"P. verreauxi\" group) in that it is smaller in length and weight. Its karyotype, however, is more similar to that of the eastern forest sifakas (known as the \"P. diadema\" group). The golden-crowned sifaka has 42 chromosomes (2n=42), 16 of which are autosomal pairs (not sex chromosomes) that are meta- or submetacentric (where chromosome arms are equal or unequal in length, respectively). The remaining autosomal pairs are smaller and acrocentric (with the shorter chromosome arm difficult to observe). Its X chromosome is metacentric, which is comparable to that of the \"P. diadema\" group, not the \"P. verreauxi\" group. Given the conflicting information, its geographic isolation, as well as the unique long fur tufts on the ears—a trait not shared by any other sifaka—the golden-crowned sifaka was recognized as a distinct species.\n\nIn 1997, comparisons of repeated DNA sequences within the family Indriidae supported Simon's classification, placing the golden-crowned sifaka as a sister group to the other sifakas. In 2001, a study involving mitochondrial DNA suggested a very recent divergence between it and the Coquerel's sifaka, then considered a subspecies of the \"P. verreauxi\" group. If this were true, the golden-crowned sifaka would not merit species status and would form a subclade with the Coquerel's sifaka within the \"P. verreauxi\" group. In 2004, a comparative study of the karyotypes of the three traditional species of sifakas provided insight into the chromosomal arrangements of all three groups. This study found that the golden-crowned sifaka differs from \"P. verreauxi\" group and \"P. diadema\" group by 9 and 17 chromosomal rearrangements respectively, and conversely argued that the golden-crowned sifaka is indeed a separate species and is more closely related to the \"P. verreauxi\" group. More recently, in 2007 a craniodental (skull and tooth) study provided evidence for 9 or 10 distinct sifaka species, including the golden-crowned sifaka. It also placed the golden-crowned sifaka within the \"P. verreauxi\" group.\n\nThe golden-crowned sifaka is one of the smallest sifaka species with a weight of , a head-body length of , a tail length of , and total length of . It is comparable in size to the sifakas inhabiting the southern and western dry forests, such as Coquerel's sifaka, the crowned sifaka, Von der Decken's sifaka, and Verreaux's sifaka. It has a coat of moderately long, creamy-white fur with a golden tint, dark black or chocolate-brown fur on its neck and throat, pale orange fur on the tops of its legs and forelimbs, a white tail and hindlimbs, and a characteristic bright orange-gold crown. It is the only sifaka with prominent tufts of white fur protruding from its ears, making its head appear somewhat triangular and distinctive in appearance. Its eyes are orange, and its face is black and mostly hairless, with dark gray-black fur with white hairs stretching from beneath the eyes to the cheeks. Its snout is blunt and rounded, and its broad nose helps to distinguish it from other sifakas. Occasionally the bridge of the nose will have a patch of white fur. Similar to other sifakas, this arboreal animal has long, strong legs that enable it to cling and leap between tree trunks and branches.\n\nThe golden-crowned sifaka lives in dry deciduous, gallery, and semi-evergreen forests and is found at altitudes up to , though it seems to prefer lower elevations. Surveys have shown it to be limited to highly fragmented forests surrounding the town of Daraina in an area encircled by the Loky and Manambato rivers in northeastern Madagascar. The golden-crowned sifaka has one of the smallest geographic ranges of all indriid lemur species. Out of 75 forest fragments studied by researchers, its presence could be definitively reported in only 44, totaling . This study, published in 2002, also estimated the total species population and observed population densities. Home range size varied between per group. With an average group size of five individuals, the population density ranged between 17 and 28 individuals per km. Another home range size estimate of has also been suggested with a population density range of 10 and 23 individuals per km. The forested area available to the species within its desired elevation range was estimated at , yielding an estimated population of 6,120–10,080 and a breeding population between 2,520 and 3,960 individuals. However, a study published in 2010 using line transect data from 2006 and 2008 in five major forest fragments yielded an estimated population of 18,000 individuals.\n\nThe species is sympatric (coexists) with two other medium-sized lemurs: the Sanford's brown lemur (\"Eulemur sanfordii\") and the crowned lemur (\"Eulemur coronatus\").\n\nThe golden-crowned sifaka is primarily active during the day (diurnal), but researchers have witnessed activity in the early morning and evening (crepuscular) during the rainy season (November through April). In captivity, it has been observed feeding at night, unlike captive Verreaux's sifakas. It travels between per day, an intermediate range compared to other sifakas of the eastern forests. The golden-crowned sifaka can be observed feeding and resting higher in the canopy during the dry season (May through October). It sleeps in the taller trees (the emergent layer) of the forest at night.\n\nWhen stressed, the golden-crowned sifaka emits grunting vocalizations as well as repeated \"\"churrs\" that escalate into a high-amplitude \"whinney\".\" Its ground predator alarm call, which sounds like \"shē-fäk\", closely resembles that of Verreaux's sifaka. It also emits mobbing alarm calls in response to birds of prey.\n\nThe diet of the golden-crowned sifaka consists of a wide variety of plants—as many as 80 species—whose availability varies based on the season. It is a seed predator, making seeds a year-round staple in its diet when available. The golden-crowned sifaka also eats unripe fruits, flowers, and leaves. One study showed a diet composition of 37% unripe fruit and seeds, 22% immature leaves, 17% mature leaves, 13% flowers, and 9% fruit pulp. Individuals have also been observed consuming tree bark during the dry season. In general, approximately 60% of its diet consists of unripe fruits and seed, mainly from leguminous pods, and less than 50% consists of leaves. At Daraina, it has been observed feeding on the sakoa tree (\"Poupartia caffra\") and on mango trees. Immature leaves and flowers are eaten when available, in the early wet season. Daily traveling distance tends to increase when immature leaves are available. Studies have also shown that when food distribution is patchy, feeding times are shorter and more time is spent traveling. Dietary diversity has been shown to be consistent between populations, suggesting that it is important for the lemur to get a varied mix of nutrients and to protect itself from high levels of specific plant toxins.\n\nA study in 1993 showed variability and flexibility in feeding preferences between three research sites around Daraina. Plant species preferences (measured in feeding time) changed between wetter, intermediate, and drier forests:\n\nThe social structure of the golden-crowned sifaka is very similar to that of Verreaux's sifaka, both averaging between five and six individuals per group, with a range between three and ten. Unlike the Verreaux's sifaka, group sex ratios are more evenly balanced, consisting of two or more members of both sexes. Females are dominant within the group, and only one female breeds successfully each season. Males will roam between groups during the mating season.\n\nBecause of their smaller home ranges relative to other sifakas, group encounters are slightly more common, occurring a few times a month. It has been noted that the temperament of the golden-crowned sifaka is more volatile than that of other sifaka species and, in the case of a dispute, this animal frequently emits a grunt-like vocalization that seems to signal annoyance. Aggressive interactions between groups are generally non-physical but include loud growling, territorial marking, chasing, and ritualistic leaping displays. Same-sexed individuals act most aggressively towards each other during such encounters. Scent marking is the most common form of territorial defense, with scent marks acting as \"signposts\" to demarcate territorial boundaries. Females use glands in the genital regions (\"anogenital\") while males use both anogenital and chest glands.\n\nThe golden-crowned sifaka is a seasonal breeder, often mating during the last week of January. Its gestation period is a little less than six months, and its lactation period is five months. Research has indicated that reproduction is strategically linked with forest seasonality. Gestation starts in the later part of the wet season (late January), and continues for approximately 170 days. Parturition occurs in the middle of the dry season (late June or July). Weaning occurs during the middle of the wet season, in December, when an abundance of immature leaves is available. It is thought that such reproductive timing exists to ensure adequate protein intake from the immature leaves for both mother and child at the end of the lactation period.\n\nFemales reproduce once every two years. Infants are born with little hair and initially cling to their mother's belly. As they mature, they begin to ride on her back. Following weaning, riding on the back is only tolerated for short durations, particularly when the group is alerted to the presence of a predator. By one year of age, the juveniles are 70% of their full adult body weight. Infant mortality is high in this species. Upon reaching sexual maturity, males leave their natal group and transfer to neighboring social groups. Observations by researchers and reports from local people indicate that this species will jump to the ground and cross more than of grassland to reach nearby forest patches. This suggests that forest fragmentation may not completely isolate separated populations.\n\nThe only predator known to target this species is the fossa, although the golden-crowned sifaka reacts to the presence of birds of prey with alarm calls. A hematology and serum chemistry study published in 1995 revealed that 59% of the wild golden-crowned sifakas sampled were infected with a microfilarial parasite, a potentially unknown species of nematode in the genus \"Mansonella\". Healthy, infected individuals did not appear to be adversely affected by the infestation, but the overall effect on the dwindling population is unknown. Also, no malarial or intestinal parasites were found, although 48% of the golden-crowned sifakas examined had external ear mites.\n\nWhile the golden-crowned sifaka faces few biological threats, such as predation, it faces many significant human-caused (anthropogenic) threats. Its habitat has been highly fragmented, with forest patches isolated by severely degraded grasslands. By 1985 it was estimated that 34% of the entire eastern rainforest of the island had disappeared, and by extrapolation it is predicted that at this rate of deforestation there will be no eastern rainforest left by 2020. Illegal logging practices, slash-and-burn agriculture (known as \"tavy\"), uncontrolled grass fires, gold mining, poaching, and clearing land for agricultural use have all significantly contributed to the significant deforestation witnessed in Madagascar and the ongoing decline of suitable habitat for this species.\n\nMalagasy farmers continue to use fire to clear out agricultural land and pasture for livestock, promoting grass growth while inhibiting forest regeneration. The fires sometimes burn out of control and destroy forest edges along with the natural flora, increasing the damage even further than intended. Due to the nature of Madagascar's geology and soil, \"tavy\" also depletes the fertility of the soil, accelerating the crop rotation rate and necessitating expansion into primary forests.\n\nAlthough coal is the preferred cooking fuel of the Malagasy people, the most affordable and prominent source of energy is timber, known as \"kitay\". Wood is also used as a primary building material, only adding further incentive to remove trees from the forest. With the depletion of dead wood from the forest patches, the people have begun to remove young, healthy trees. This is seen most commonly in areas closest to villages. Although the shapes and sizes of forest fragments around the Daraina region have been mostly stable for 50 years prior to a study in 2002, the six years preceding the study had seen 5% of the small- to medium-sized forest fragments disappear due to increased human encroachment.\n\nA newly emergent threat facing the golden-crowned sifaka is hunting by the gold miners moving into the region's forests. Although mining operations are small scale, the practice of gold mining takes a toll on the forested regions because deep mining pits are often dug near or underneath large trees, disturbing the extensive root systems and ultimately killing the trees in the area. The influx of gold miners has also increased poaching pressure. Although the species is protected from hunting by local \"fady\" (taboo) around Daraina, due to their likeness to humans, and by Malagasy law, the gold miners who have immigrated to the area have begun to hunt the golden-crowned sifaka as a source of bushmeat. In 1993, David M. Meyers, a researcher who has studied the golden-crowned sifaka, speculated that if bushmeat hunting were to escalate, the species would go extinct in less than ten years since it is easy to find and not fearful of humans. Indeed, bushmeat hunting by people from nearby Ambilobe has already extirpated at least one isolated population.\n\nBecause studies have shown that the golden-crowned sifaka are most likely to be found in large forest fragments (greater than ), the species is thought to be sensitive to forest fragmentation and degradation. However, since it has been found around gold mining camps and degraded forests, it is not restricted to undisturbed forests and appears to tolerate human activity. Regardless, with its low population, highly restricted range, and badly fragmented habitat, the prospect for survival for the golden-crowned sifaka is considered bleak. For these reasons, the International Union for Conservation of Nature (IUCN) added it to its list of the 25 most endangered primates in 2000. Previously, in 1992, the IUCN's Species Survival Commission (IUCN/SSC) Primate Specialist Group also assigned the species its highest priority rating. As of its 2014 assessment, the golden-crowned sifaka is once again \"Critically Endangered\" on the IUCN Red List. This was an upgrade from 2008 when it was listed as \"Endangered\". In previous assessments it was listed as \"Critically Endangered\" (1996 and 2000) and \"Endangered\" (1990 and 1994).\n\nThe area inhabited by the golden-crowned sifaka is also an important agricultural and economical resource for the human population. Suggested conservation action aimed at protecting this species and its habitat has focused on offering varying degrees of protection to forest fragments in the region, allowing human activity and resource extraction in areas that have less conservation potential while strictly protecting areas critical to the species' survival. In 2002, none of the forested areas that the golden-crowned sifaka inhabits were part of a formally protected national park or reserve. A conservation study from 1989 called for the creation of a national park that includes the forest of Binara as well as the dry forests to the north of Daraina. A more recent study from 2002 proposed a network of protected forest areas including areas outside of the village of Daraina, forests north of the Monambato River, and the northern forests that constitute the species' northern reservoir. In 2005, Fanamby, a Malagasy non-governmental organization (NGO), teamed up with Conservation International to create a protected area that both Association Fanamby and the Ministry of Water and Forests manage. As of 2008, only ten forest patches that could support viable populations remained, according to the IUCN.\nOnly one captive population of golden-crowned sifakas has been represented in a zoological collection. Building on a successful record of maintaining a viable captive Verreaux's sifaka population, the Duke Lemur Center (DLC) in Durham, North Carolina, requested and obtained permission from the government of Madagascar to capture and export this (then) unknown species for captive breeding. Plans were also made to establish a captive breeding program at the Ivoloina Forestry Station, now known as Parc Ivoloina. In November 1987, during the same expedition that resulted in the formal description of the species, two males and two females were caught and measured. Five others were also caught, but were released because they were juvenile males. In July 1988, a golden-crowned sifaka was born in captivity at the DLC. However, the captive population was small and not viable for long-term breeding, and captive sifakas have proven difficult to maintain due to their specialized dietary needs. The last captive individual died in 2008. Despite the loss of its small colony after 20 years, DLC believes that establishment of a captive population for conservation-oriented captive breeding purposes could provide an important second level of protection, particularly if habitat protection measures are unsuccessful.\n\nAs a result of the political crisis that began in 2009 and the resulting breakdown of law and order in Madagascar, poachers have hunted lemurs in the Daraina area and sold them to local restaurants as a delicacy. Pictures of dead lemurs that had been smoked for transport were taken by Fanamby and released by Conservation International in August 2009. The lemurs in the photographs included the endangered golden-crowned sifaka, as well as crowned lemurs. Around the time the photographs were released, 15 people were arrested for selling smoked lemurs, which were bought from hunters for 1,000 ariary, or around US$0.53, and then sold in restaurants for 8,000 ariary (US$4.20). Russell Mittermeier, president of Conservation International, said that the arrests would not end the poaching since the poachers would \"just get slaps on the wrist\".\n\n"}
{"id": "949900", "url": "https://en.wikipedia.org/wiki?curid=949900", "title": "Group 70", "text": "Group 70\n\nGroup 70 is a non-profit educational organization of people from many countries all around the world and many walks of life seeking to make astronomy available to the peoples of the world.\nThey began in 1988 with the goal of building a large amateur telescope, a astronomical instrument. Upon completion it would be the largest telescope in the world built by and for amateur astronomers. Beyond building a large aperture telescope, they offer related instrumentation and services to those in amateur, professional and educational fields of astronomy.\n\nAs of mid-2008, the project has moved from San Jose, CA to Fremont CA and work has moved from the mirror to the steel and other optics.\n\nUse of a comparable telescope in California is very expensive: a rental must be scheduled well in advance and the weather may not cooperate.\n\n\n"}
{"id": "49635454", "url": "https://en.wikipedia.org/wiki?curid=49635454", "title": "Gurimite", "text": "Gurimite\n\nGurimite is a rare mineral with formula Ba(VO). It is a simple barium vanadate, one of the most simple barium minerals known. It is named after its type locality - Gurim anticline in Israel. It has formed in the rocks of the Hatrurim Formation. Gurimite's stoichiometry is similar to that of copper vanadates mcbirneyite and pseudolyonsite. An example of other barium vanadate mineral is tokyoite.\n"}
{"id": "8772915", "url": "https://en.wikipedia.org/wiki?curid=8772915", "title": "Hell and High Water (book)", "text": "Hell and High Water (book)\n\nHell and High Water: Global Warming – the Solution and the Politics – and What We Should Do is a book by author, scientist, and former U.S. Department of Energy official Joseph J. Romm, published December 26, 2006. The author is \"one of the world's leading experts on clean energy, advanced vehicles, energy security, and greenhouse gas mitigation.\"\n\nThe book warned of dire consequences to the U.S. and the world if wide-scale environmental changes are not enacted by the U.S. government. It reviewed the evidence that the initial global warming changes would lead to feedbacks and accelerated warming. According to Romm, the oceans, soils, Arctic permafrost, and rainforests could become sources of greenhouse gas emissions. The book claimed that, without serious government action, sea levels would rise high enough to submerge numerous coastal communities and inland areas on both U.S. coasts and around the world by the year 2100.\n\nIn 2008, TIME magazine wrote that \"On [Romm's] blog and in his most recent book, \"Hell and High Water\", you can find some of the most cogent, memorable, and deployable arguments for immediate and overwhelming action to confront global warming.\" Romm was interviewed on Fox News on January 31, 2007 about the book and the IPCC Fourth Assessment Report climate report.\n\nPart I, comprising the first four chapters of the book, reviews the science of climate change, setting forth the evidence that humans are causing an unprecedented increase in carbon emissions that is, in turn causing global warming. The book describes the consequences of unchecked climate change, such as destruction of coastal cities due to rising sea levels and mega-hurricanes; increasing droughts and deadly water shortages; infestation of insects into new ranges; and increased famines, heat waves, forest fires and desertification. The book sets forth the research on \"feedback loops\" that would contribute to accelerating climate change, including: \n\nRomm proposes an eight-point program, based on existing technologies, to counter and then reverse the trend toward catastrophic global warming: performance-based efficiency programs; energy efficiency gains from industry and power generation through cogeneration of heat and power; building wind farms; capturing carbon dioxide from proposed coal plants; building nuclear plants; greatly improving the fuel economy of our vehicles using PHEVs; increasing production of high-yield energy crops; and stopping tropical deforestation while planting more trees.(pp. 22–23)\n\nPart I then offers extrapolations, based on various models and analyses, of what will happen to the U.S. and the world by 2025, 2050 and 2100 if decisive action is not taken quickly. Treehugger.com called this \"an explanation that is both comprehensive and comprehensible.\" The book claimed that without serious government action within the following ten years, sea levels will eventually rise high enough to submerge numerous coastal communities and inland areas on both U.S. coasts and around the world causing over 100 million \"environmental refugees\" to flee the coasts by the year 2100.\n\nPart II, comprising the next six chapters, discusses the politics and media issues that the author says are delaying such decisive action (and how this has a negative influence on the behavior of other countries, particularly China) and also discusses the currently available technological solutions to global warming. The book asserts that there has been a disingenuous, concerted and effective campaign to convince Americans that the science is not proven, or that global warming is the result of natural cycles, and that there needs to be more research. The book claims that, to delay action, industry and government spokesmen suggest falsely that \"technology breakthroughs\" will eventually save us with hydrogen cars and other fixes. It asserts that the reason for this denial and delay is that \"ideology trumps rationality... Most conservatives cannot abide the solution to global warming – strong government regulations and a government-led effort to accelerate clean-energy technologies into the market.\"(p. 107) Romm says that the media have acted as enablers of this program of denial in the misguided belief that the pursuit of \"balance\" is superior to the pursuit of truth - even in science journalism. The book describes how this has led to skewed public opinion and to congress cutting funds for programmes aimed at accelerating the deployment into the American market of cost-effective technologies already available.\n\nThe book spends many pages refuting the \"hydrogen myth\" (see also Romm's previous book, \"The Hype about Hydrogen\") and \"the geo-engineering fantasy.\" In Chapter 7, the book describes technology strategies that it claims would permit the U.S., over the next two decades, to cut its carbon dioxide emissions by two-thirds without increasing the energy costs of either consumers or businesses. These include launching \"a massive performance-based efficiency program for homes, commercial buildings and new construction... [and] a massive effort to boost the efficiency of heavy industry and expand the use of cogeneration... [c]apture the from 800 new large coal plants and store it underground, [b]uild 1 million large wind turbines... [and] [b]uild 700 new large nuclear power plants\".\n\nThe book's conclusion calls on voters to demand immediate action. The conclusion is followed by over 50 pages of extensive endnotes and an index.\n\nTyler Hamilton, in his review of the book for The Toronto Star, summarizes the book's contents as follows: \"Whereas the first third of Romm's book presents overwhelming and disturbing evidence that human-caused greenhouse gases are the primary ingredients behind global warming, the pages that follow offer alarming detail on how the U.S. public is being misled by a federal government (backed by conservative political forces) that is intent on inaction, and that's also on a mission to derail international efforts to curb emissions.\"\n\nIn his book \"Hell and High Water,\" Romm discusses the urgency to act and the sad fact that America is refusing to do so... Romm gives a name to those such as ExxonMobil who deny that global warming is occurring and are working to persuade others of this money-making myth: they are the Denyers and Delayers. They are better rhetoriticians than scientists are... Global warming is happening now, and Romm... gives us 10 years to change the way we live before it’s too late to use existing technology to save the world. \"...humanity already possesses the fundamental scientific, technical, and industrial know-how to solve the carbon and climate problem for the next half-century. The tragedy, then, as historians of the future will most certainly recount, is that we ruined their world not because we lacked the knowledge or the technology to save it but simply because we chose not to make the effort”(Romm, 25).\n\nThe book claims that U.S. politicians who deny the science and have failed to take genuine action on conservation and alternative energy initiatives are following a disastrous course by delaying serious changes that he says are imminently needed. Romm also criticizes the media for what he says is sloppy reporting and an unwillingness to probe behind political rhetoric, which he says are lulling Americans into accepting continuing delays on implementing emission-cutting technologies. The book argues that there is a window of opportunity of only about a decade to head off the most catastrophic effects of global warming and it calls upon Americans to demand government action to require the use of emission-cutting technologies.\n\nRomm writes that strategies to combat climate change with current technologies can significantly slow global warming and buy more time for the world to develop new technologies and take even stronger action. The book lays out a number of proposed solutions to avoiding a climate catastrophe, including:\n\nThe book states, \"The IPCC's Fourth Assessment Report this year (2007) will present a much stronger consensus and a much clearer and darker picture of our likely future than the Third Assessment—but it will almost certainly still underestimate the likely impacts. The Fifth Assessment, due around 2013, should include many of the omitted feedbacks, like that of the [carbon emissions caused by] defrosting tundra, and validate the scenarios described on these pages...\" (p. 94)\n\nThe Toronto Star's January 1, 2007 review of the book says that Romm \"convincingly shoots down the arguments of those who claim global warming is a hoax or some kind of natural cycle not associated with human activities.\" The review laments that the \"'Denyers and Delayers' are winning the political battle in the United States, the world's highest emitter of greenhouse gases and a saboteur of Kyoto talks\" and that the media's policy of \"giving 'equal time' to Denyers gives the public the wrong impression about our understanding and level of certainty around global warming science.\" The review concludes, \"The book itself is a short and easy read, not as intimidating as some other works, and it hits all the main points on the science and politics behind global warming, and the policy and technological solutions to minimize damage to the planet, economy and humanity.\"\n\nA review in the Detroit Free Press's Freep.com stated, \"Joseph Romm's \"Hell and High Water\" is a great book for people who want to understand the complexities of global warming and, perhaps more important, what we could be doing about it other than wringing our hands or sticking our collective head in the sand.\" \"Technology Review\" concluded, \"His book provides an accurate summary of what is known about global warming and climate change, a sensible agenda for technology and policy, and a primer on how political disinformation has undermined climate science.\" \"BooksPath Reviews\" commented, \"Hell and High Water\" is nothing less than a wake-up call to the country. It is a searing critique of American environmental and energy policy and a passionate call to action by a writer with a unique command of the science and politics of climate change. \"Hell and High Water\" goes beyond ideological rhetoric to offer pragmatic solutions to avert the threat of global warming — solutions that must be taken seriously by every American. On February 21, 2007, Bill Moore at EV World.com wrote: \"...it seemed every paragraph, every page revealed some new outrage that just got my dander up. If it doesn't do the same to you, I'll really be surprised.\"\n\n\"Grist\"'s blog \"Gristmill\" noted, \"Joseph Romm's \"Hell and High Water\" may be the most depressing book on global warming I've ever read. ... My hope is that a lifetime spent in insider elite politics causes him to underestimate what a bottom-up grassroots movement can accomplish. ... A coalition that supported real action on global warming, as part of movement that supported real solutions on these other issues too, would have a much better chance of winning than a single-issue group. It would have a broader base and could offer more immediate relief from problems; because global warming wouldn't be its only or even main issue, it would produce quicker results in the lives of ordinary people. ... Technically, Romm is sound.\" The writer amended his statement as follows: \"I referred to the book as 'depressing', but the tone is frank, not truly gloomy... Romm... is known as a level-headed, optimistic analyst. His book is no exception – he documents the problem and the (quite mainstream) solutions he endorses thoroughly and meticulously.\" The \"Foreign Policy in Focus\" article \"An Inconvenient Truth II\" cites the book with approval and references its analysis twice.\n\nThe blog \"Political Cortex\" wrote: \"\"Hell and High Water\" might be the Global Warming work of most interest to the politically engaged (Democratic and/or Republican). Romm lays a strong case as to how Global Warming could be the death sentence for the Republican Party as reality becomes ever blatantly at odds with Republican Party rhetoric... Romm also highlights how, in an ever more difficult world in the years to come, either the United States figures out how to lead in dealing with mitigating/muting Global Warming and its impacts or risks becoming a pariah nation, with dire implications for the Republic and its citizens.\" \"Booklist\"'s reviewer wrote that the book \"presents a clear and effective primer on climate science. But the most salient aspects of this provocative expose involve Romm's documentation of what he calls the Bush administration's irresponsible and backward energy policies, the censorship of legitimate and urgent information pertaining to global warming, and the threats rising temperatures pose to \"the health and well-being of this nation and the world. Romm explains that we already possess the technologies and know-how we need to reduce greenhouse gas emissions.\"\n\nIn 2008, the Greenpeace staff blog noted, \"If you’re concerned about global warming and want to do something about it, Joseph Romm’s \"Hell and High Water\"... is a fantastic primer. ... Romm clearly and concisely details the technologies and policies we need to adopt to avoid the worst consequences of global warming\".\n\n"}
{"id": "377957", "url": "https://en.wikipedia.org/wiki?curid=377957", "title": "Hermia (Finland)", "text": "Hermia (Finland)\n\nHermia is a science park near Tampere University of Technology (TUT). Hermia is located in Hervanta, a suburb of Tampere, Finland. Hermia is also acting as a technology centre for its region.\n\nHermia offers office space and facilities for both small technology startups and larger companies. It consists of 100,000 m of office space and is home to 150 companies and research organizations, for example to many research and development units of the cell phone manufacturer Nokia.\n\nIn the offices of Hermia was developed the first Nokia Communicator mobile phone, as well as Nokia's first camera-phone. The site is also home to one of the offices of Nokia Research Center, where among other activities the EFR and WB-AMR codecs were developed. \n"}
{"id": "6338482", "url": "https://en.wikipedia.org/wiki?curid=6338482", "title": "High-altitude research", "text": "High-altitude research\n\nThere are a wide range of potential applications for research at high altitude, including medical, physiological, and cosmic physics research.\n\nThe most obvious and direct application of high-altitude research is to understand altitude illnesses such as acute mountain sickness, and the rare but rapidly fatal conditions, high-altitude pulmonary edema (HAPE) and high-altitude cerebral edema (HACE). Research at high altitude is also an important way to learn about sea level conditions that are caused or complicated by hypoxia such as chronic lung disease and sepsis. Patients with these conditions are very complex and usually suffer from several other diseases at the same time, so it is virtually impossible to work out which of their problems is caused by lack of oxygen. Altitude research gets round this by studying the effects of oxygen deprivation on otherwise healthy people.\n\nTravelling to high altitude is often used as a way of studying the way the body responds to a shortage of oxygen. It is difficult and prohibitively expensive to conduct some of this research at sea level\n\nAlthough the shortage of air contributes to the effects on the human body, research has found that most altitude sicknesses can be linked to the lack of atmospheric pressure. At low elevation, the pressure is higher because the molecules of air are compressed from the weight of the air above them. However, at higher elevations, the pressure is lower and the molecules are more dispersed. The percentage of oxygen in the air at sea level is the same at high altitudes. But because the air molecules are more spread out at higher altitudes, each breath takes in less oxygen to the body. With this in mind, the lungs take in as much air as possible, but because the atmospheric pressure is lower the molecules are more dispersed, resulting in a lower amount of oxygen per breath.\n\nAt 26,000 feet the body reaches a maximum and can no longer adjust to the altitude, often referred to as the \"Death Zone\".\n\n 3. http://www.summitpost.org/high-altitude-what-happens-to-the-human-body-in-the-death-zone/371306\n\n"}
{"id": "29517067", "url": "https://en.wikipedia.org/wiki?curid=29517067", "title": "Hostuviroid", "text": "Hostuviroid\n\nHostuviroid is a genus of viroids that includes the hop stunt viroids, a group of viroids that infects many different types of plants, including the common hop plant.\n\nSome hostuviroids:\n"}
{"id": "40323", "url": "https://en.wikipedia.org/wiki?curid=40323", "title": "Inertial confinement fusion", "text": "Inertial confinement fusion\n\nInertial confinement fusion (ICF) is a type of fusion energy research that attempts to initiate nuclear fusion reactions by heating and compressing a fuel target, typically in the form of a pellet that most often contains a mixture of deuterium and tritium.\n\nTo compress and heat the fuel, energy is delivered to the outer layer of the target using high-energy beams of laser light, electrons or ions, although for a variety of reasons, almost all ICF devices have used lasers. The heated outer layer explodes outward, producing a reaction force against the remainder of the target, accelerating it inwards, compressing the target. This process is designed to create shock waves that travel inward through the target. A sufficiently powerful set of shock waves can compress and heat the fuel at the center so much that fusion reactions occur.\n\nThe energy released by these reactions will then heat the surrounding fuel, and if the heating is strong enough this could also begin to undergo fusion. The aim of ICF is to produce a condition known as \"ignition\", where this heating process causes a chain reaction that burns a significant portion of the fuel. Typical fuel pellets are about the size of a pinhead and contain around 10 milligrams of fuel: in practice, only a small proportion of this fuel will undergo fusion, but if all this fuel were consumed it would release the energy equivalent to burning a barrel of oil (BOE).\nICF is one of two major branches of fusion energy research, the other being magnetic confinement fusion. When it was first proposed in the early 1970s, ICF appeared to be a practical approach to fusion power production and the field flourished. Experiments during the 1970s and '80s demonstrated that the efficiency of these devices was much lower than expected, and reaching ignition would not be easy. Throughout the 1980s and '90s, many experiments were conducted in order to understand the complex interaction of high-intensity laser light and plasma. These led to the design of newer machines, much larger, that would finally reach ignition energies.\n\nThe largest operational ICF experiment is the National Ignition Facility (NIF) in the US, designed using all of the decades-long experience of earlier experiments. Like those earlier experiments, however, NIF has failed to reach ignition and is, as of 2015, generating about of the required energy levels. As of October 7, 2013, this facility is understood to have achieved an important milestone towards commercialization of fusion, namely, for the first time a fuel capsule gave off more energy than was applied to it. This is a major step forward. A similar large-scale device in France, Laser Mégajoule, was officially inaugurated in October 2014. Experiments have started since then, albeit with low laser energies involved.\n\nFusion reactions combine lighter atoms, such as hydrogen, together to form larger ones. Generally the reactions take place at such high temperatures that the atoms have been ionized, their electrons stripped off by the heat; thus, fusion is typically described in terms of \"nuclei\" instead of \"atoms\".\n\nNuclei are positively charged, and thus repel each other due to the electrostatic force. Overcoming this repulsion costs a considerable amount of energy, which is known as the \"Coulomb barrier\" or \"fusion barrier energy\". Generally, less energy will be needed to cause lighter nuclei to fuse, as they have less charge and thus a lower barrier energy, and when they do fuse, more energy will be released. As the mass of the nuclei increase, there is a point where the reaction no longer gives off net energy—the energy needed to overcome the energy barrier is greater than the energy released in the resulting fusion reaction.\n\nThe best fuel from an energy perspective is a one-to-one mix of deuterium and tritium; both are heavy isotopes of hydrogen. The D-T (deuterium & tritium) mix has a low barrier because of its high ratio of neutrons to protons. The presence of neutral neutrons in the nuclei helps pull them together via the nuclear force, while the presence of positively charged protons pushes the nuclei apart via electrostatic force. Tritium has one of the highest ratios of neutrons to protons of any stable or moderately unstable nuclide—two neutrons and one proton. Adding protons or removing neutrons increases the energy barrier.\n\nA mix of D-T at standard conditions does not undergo fusion; the nuclei must be forced together before the nuclear force can pull them together into stable collections. Even in the hot, dense center of the sun, the average proton will exist for billions of years before it fuses. For practical fusion power systems, the rate must be dramatically increased by heating the fuel to tens of millions of degrees, and/or compressing it to immense pressures. The temperature and pressure required for any particular fuel to fuse is known as the Lawson criterion. These conditions have been known since the 1950s when the first H-bombs were built. To meet the Lawson Criterion is extremely difficult on Earth, which explains why fusion research has taken many years to reach the current high state of technical prowess.\n\nIn a hydrogen bomb, the fusion fuel is compressed and heated with a separate fission bomb (see Teller-Ulam design). A variety of mechanisms transfers the energy of the fission \"trigger\"'s explosion into the fusion fuel. The requirement of a fission bomb makes the method impractical for power generation. Not only would the triggers be prohibitively expensive to produce, but there is a minimum size that such a bomb can be built, defined roughly by the critical mass of the plutonium fuel used. Generally it seems difficult to build nuclear devices smaller than about 1 kiloton in yield, which would make it a difficult engineering problem to extract power from the resulting explosions.\n\nAs the explosion size is scaled down, so too is the amount of energy needed to start the reaction off. Studies from the late 1950s and early 1960s suggested that scaling down into the megajoule energy range would require energy levels that could be delivered by any number of means. This led to the idea of using a device that would \"beam\" the energy at the fusion fuel, ensuring mechanical separation. By the mid-1960s, it appeared that the laser would develop to the point where the required energy levels would be available.\n\nGenerally ICF systems use a single laser, the \"driver\", whose beam is split up into a number of beams which are subsequently individually amplified by a trillion times or more. These are sent into the reaction chamber (called a target chamber) by a number of mirrors, positioned in order to illuminate the target evenly over its whole surface. The heat applied by the driver causes the outer layer of the target to explode, just as the outer layers of an H-bomb's fuel cylinder do when illuminated by the X-rays of the fission device.\n\nThe material exploding off the surface causes the remaining material on the inside to be driven inwards with great force, eventually collapsing into a tiny near-spherical ball. In modern ICF devices the density of the resulting fuel mixture is as much as one-hundred times the density of lead, around 1000 g/cm. This density is not high enough to create any useful rate of fusion on its own. However, during the collapse of the fuel, shock waves also form and travel into the center of the fuel at high speed. When they meet their counterparts moving in from the other sides of the fuel in the center, the density of that spot is raised much further.\n\nGiven the correct conditions, the fusion rate in the region highly compressed by the shock wave can give off significant amounts of highly energetic alpha particles. Due to the high density of the surrounding fuel, they move only a short distance before being \"thermalised\", losing their energy to the fuel as heat. This additional energy will cause additional fusion reactions in the heated fuel, giving off more high-energy particles. This process spreads outward from the centre, leading to a kind of self-sustaining burn known as \"ignition\".\n\nThe primary problems with increasing ICF performance since the early experiments in the 1970s have been of energy delivery to the target, controlling symmetry of the imploding fuel, preventing premature heating of the fuel (before maximum density is achieved), preventing premature mixing of hot and cool fuel by hydrodynamic instabilities and the formation of a 'tight' shockwave convergence at the compressed fuel center.\n\nIn order to focus the shock wave on the center of the target, the target must be made with extremely high precision and sphericity with aberrations of no more than a few micrometres over its surface (inner and outer). Likewise the aiming of the laser beams must be extremely precise and the beams must arrive at the same time at all points on the target. Beam timing is a relatively simple issue though and is solved by using delay lines in the beams' optical path to achieve picosecond levels of timing accuracy. The other major problem plaguing the achievement of high symmetry and high temperatures/densities of the imploding target are so called \"beam-beam\" imbalance and beam anisotropy. These problems are, respectively, where the energy delivered by one beam may be higher or lower than other beams impinging on the target and of \"hot spots\" within a beam diameter hitting a target which induces uneven compression on the target surface, thereby forming Rayleigh–Taylor instabilities in the fuel, prematurely mixing it and reducing heating efficacy at the time of maximum compression. The Richtmyer-Meshkov instability is also formed during the process due to shock waves being formed.\n\nAll of these problems have been substantially mitigated to varying degrees in the past two decades of research by using various beam smoothing techniques and beam energy diagnostics to balance beam to beam energy; however, RT instability remains a major issue. Target design has also improved tremendously over the years. Modern cryogenic hydrogen ice targets tend to freeze a thin layer of deuterium just on the inside of a plastic sphere while irradiating it with a low power IR laser to smooth its inner surface while monitoring it with a microscope equipped camera, thereby allowing the layer to be closely monitored ensuring its \"smoothness\". Cryogenic targets filled with a deuterium tritium (D-T) mixture are \"self-smoothing\" due to the small amount of heat created by the decay of the radioactive tritium isotope. This is often referred to as \"beta-layering\".\n\nCertain targets are surrounded by a small metal cylinder which is irradiated by the laser beams instead of the target itself, an approach known as \"indirect drive\". In this approach the lasers are focused on the inner side of the cylinder, heating it to a superhot plasma which radiates mostly in X-rays. The X-rays from this plasma are then absorbed by the target surface, imploding it in the same way as if it had been hit with the lasers directly. The absorption of thermal x-rays by the target is more efficient than the direct absorption of laser light, however these \"hohlraums\" or \"burning chambers\" also take up considerable energy to heat on their own thus significantly reducing the overall efficiency of laser-to-target energy transfer. They are thus a debated feature even today; the equally numerous \"direct-drive\" design does not use them. Most often, indirect drive hohlraum targets are used to simulate thermonuclear weapons tests due to the fact that the fusion fuel in them is also imploded mainly by X-ray radiation.\n\nA variety of ICF drivers are being explored. Lasers have improved dramatically since the 1970s, scaling up in energy and power from a few joules and kilowatts to megajoules (see NIF laser) and hundreds of terawatts, using mostly frequency doubled or tripled light from neodymium glass amplifiers.\n\nHeavy ion beams are particularly interesting for commercial generation, as they are easy to create, control, and focus. On the downside, it is very difficult to achieve the very high energy densities required to implode a target efficiently, and most ion-beam systems require the use of a hohlraum surrounding the target to smooth out the irradiation, reducing the overall efficiency of the coupling of the ion beam's energy to that of the imploding target further.\n\nInertial confinement fusion history can be traced back to the \"Atoms For Peace\" conference held in 1957 in Geneva. This was a large, international UN sponsored conference between the superpowers of the US and Russia. Among the many topics covered during the event, some thought was given to using a hydrogen bomb to heat a water-filled underground cavern. The resulting steam would then be used to power conventional generators, and thereby provide electrical power.\n\nThis meeting led to the Operation Plowshare efforts, given this name in 1961. Three primary concepts were studied as part of Plowshare; energy generation under Project PACER, the use of large nuclear explosions for excavation, and as a sort of nuclear fracking for the natural gas industry. PACER was directly tested in December 1961 when the 3 kt Project Gnome device was emplaced in bedded salt in New Mexico. In spite of all theorizing and attempts to stop it, radioactive steam was released from the drill shaft, some distance from the test site. Further studies as part of Project PACER led to a number of engineered cavities replacing natural ones, but through this period the entire Plowshare efforts turned from bad to worse, especially after the failure of 1962's Sedan which released huge quantities of fallout. PACER nevertheless continued to receive some funding until 1975, when a 3rd party study demonstrated that the cost of electricity from PACER would be the equivalent to conventional nuclear plants with fuel costs over ten times as great as they were.\n\nAnother outcome of the \"Atoms For Peace\" conference was to prompt John Nuckolls to start considering what happens when the fusion side of the bomb. When A fission bomb explodes, it releases X-Rays, which implodes the fusion side. This \"secondary,\" was scaled down to very small size. His earliest work concerned the study of how small a fusion bomb could be made while still having a large \"gain\" to provide net energy output. This work suggested that at very small sizes, on the order of milligrams, very little energy would be needed to ignite it, much less than a fission \"primary\". He proposed building, in effect, tiny all-fusion explosives using a tiny drop of D-T fuel suspended in the center of a metal shell, today known as a hohlraum. The shell provided the same effect as the bomb casing in an H-bomb, trapping x-rays inside so they irradiated the fuel. The main difference is that the x-rays would not be supplied by a primary within the shell, but some sort of external device that heated the shell from the outside until it was glowing in the x-ray region (see thermal radiation). The power would be delivered by a then-unidentified pulsed power source he referred to using bomb terminology, the \"primary\".\n\nThe main advantage to this scheme is the efficiency of the fusion process at high densities. According to the Lawson criterion, the amount of energy needed to heat the D-T fuel to break-even conditions at ambient pressure is perhaps 100 times greater than the energy needed to compress it to a pressure that would deliver the same rate of fusion. So, in theory, the ICF approach would be dramatically more efficient in terms of gain. This can be understood by considering the energy losses in a conventional scenario where the fuel is slowly heated, as in the case of magnetic fusion energy; the rate of energy loss to the environment is based on the temperature difference between the fuel and its surroundings, which continues to increase as the fuel is heated. In the ICF case, the entire hohlraum is filled with high-temperature radiation, limiting losses.\n\nAround the same time (in 1956) a meeting was organized at the Max Planck Institute in Germany by the fusion pioneer Carl Friedrich von Weizsäcker. At this meeting Friedwardt Winterberg proposed the non-fission ignition of a thermonuclear micro-explosion by a convergent shock wave driven with high explosives. Further reference to Winterberg's work in Germany on nuclear micro explosions (mininukes) is contained in a declassified report of the former East German Stasi (Staatsicherheitsdienst).\n\nIn 1964 Winterberg proposed that ignition could be achieved by an intense beam of microparticles accelerated to a velocity of 1000 km/s. And in 1968, he proposed to use intense electron and ion beams, generated by Marx generators, for the same purpose. The advantage of this proposal is that the generation of charged particle beams is not only less expensive than the generation of laser beams but also can entrap the charged fusion reaction products due to the strong self-magnetic beam field, drastically reducing the compression requirements for beam ignited cylindrical targets.\n\nIn 1967 research fellow Gurgen Askaryan published article with proposition to use focused laser beam in fusion lithium deuteride or deuterium.\n\nThrough the late 1950s, Nuckolls and collaborators at the Lawrence Livermore National Laboratory (LLNL) ran a number of computer simulations of the ICF concept. In early 1960 this produced a full simulation of the implosion of 1 mg of D-T fuel inside a dense shell. The simulation suggested that a 5 MJ power input to the hohlraum would produce 50 MJ of fusion output, a gain of 10. At the time the laser had not yet been invented, and a wide variety of possible drivers were considered, including pulsed power machines, charged particle accelerators, plasma guns, and hypervelocity pellet guns.\n\nThrough the year two key theoretical advances were made. New simulations considered the timing of the energy delivered in the pulse, known as \"pulse shaping\", leading to better implosion. Additionally, the shell was made much larger and thinner, forming a thin shell as opposed to an almost solid ball. These two changes dramatically increased the efficiency of the implosion, and thereby greatly lowered the energy required to compress it. Using these improvements, it was calculated that a driver of about 1 MJ would be needed, a five-fold improvement. Over the next two years several other theoretical advancements were proposed, notably Ray Kidder's development of an implosion system without a hohlraum, the so-called \"direct drive\" approach, and Stirling Colgate and Ron Zabawski's work on very small systems with as little as 1 μg of D-T fuel.\n\nThe introduction of the laser in 1960 at Hughes Research Laboratories in California appeared to present a perfect driver mechanism. Starting in 1962, Livermore's director John S. Foster, Jr. and Edward Teller began a small-scale laser study effort directed toward the ICF approach. Even at this early stage the suitability of the ICF system for weapons research was well understood, and the primary reason for its ability to gain funding. Over the next decade, LLNL made several small experimental devices for basic laser-plasma interaction studies.\n\nIn 1967 Kip Siegel started KMS Industries using the proceeds of the sale of his share of an earlier company, Conductron, a pioneer in holography. In the early 1970s he formed KMS Fusion to begin development of a laser-based ICF system. This development led to considerable opposition from the weapons labs, including LLNL, who put forth a variety of reasons that KMS should not be allowed to develop ICF in public. This opposition was funnelled through the Atomic Energy Commission, who demanded funding for their own efforts. Adding to the background noise were rumours of an aggressive Soviet ICF program, new higher-powered CO and glass lasers, the electron beam driver concept, and the 1970s energy crisis which added impetus to many energy projects.\n\nIn 1972 Nuckolls wrote an influential public paper in \"Nature\" introducing ICF and suggesting that testbed systems could be made to generate fusion with drivers in the kJ range, and high-gain systems with MJ drivers.\n\nIn spite of limited resources and numerous business problems, KMS Fusion successfully demonstrated fusion from the ICF process on 1 May 1974. However, this success was followed not long after by Siegel's death, and the end of KMS fusion about a year later, having run the company on Siegel's life insurance policy. By this point several weapons labs and universities had started their own programs, notably the solid-state lasers (Nd:glass lasers) at LLNL and the University of Rochester, and krypton fluoride excimer lasers systems at Los Alamos and the Naval Research Laboratory.\n\nAlthough KMS's success led to a major development effort, the advances that followed were, and still are, hampered by the seemingly intractable problems that characterize fusion research in general.\n\nHigh-energy ICF experiments (multi-hundred joules per shot and greater experiments) began in earnest in the early-1970s, when lasers of the required energy and power were first designed. This was some time after the successful design of magnetic confinement fusion systems, and around the time of the particularly successful tokamak design that was introduced in the early '70s. Nevertheless, high funding for fusion research stimulated by the multiple energy crises during the mid to late 1970s produced rapid gains in performance, and inertial designs were soon reaching the same sort of \"below break-even\" conditions of the best magnetic systems.\n\nLLNL was, in particular, very well funded and started a major laser fusion development program. Their Janus laser started operation in 1974, and validated the approach of using Nd:glass lasers to generate very high power devices. Focusing problems were explored in the Long path laser and Cyclops laser, which led to the larger Argus laser. None of these were intended to be practical ICF devices, but each one advanced the state of the art to the point where there was some confidence the basic approach was valid. At the time it was believed that making a much larger device of the Cyclops type could both compress and heat the ICF targets, leading to ignition in the \"short term\". This was a misconception based on extrapolation of the fusion yields seen from experiments utilizing the so-called \"exploding pusher\" type of fuel capsules. During the period spanning the years of the late '70s and early '80s the estimates for laser energy on target needed to achieve ignition doubled almost yearly as the various plasma instabilities and laser-plasma energy coupling loss modes were gradually understood. The realization that the simple exploding pusher target designs and mere few kilojoule (kJ) laser irradiation intensities would never scale to high gain fusion yields led to the effort to increase laser energies to the 100 kJ level in the UV and to the production of advanced ablator and cryogenic DT ice target designs.\n\nOne of the earliest serious and large scale attempts at an ICF driver design was the Shiva laser, a 20-beam neodymium doped glass laser system built at the Lawrence Livermore National Laboratory (LLNL) that started operation in 1978. Shiva was a \"proof of concept\" design intended to demonstrate compression of fusion fuel capsules to many times the liquid density of hydrogen. In this, Shiva succeeded and compressed its pellets to 100 times the liquid density of deuterium. However, due to the laser's strong coupling with hot electrons, premature heating of the dense plasma (ions) was problematic and fusion yields were low. This failure by Shiva to efficiently heat the compressed plasma pointed to the use of optical frequency multipliers as a solution which would frequency triple the infrared light from the laser into the ultraviolet at 351 nm. Newly discovered schemes to efficiently frequency triple high intensity laser light discovered at the Laboratory for Laser Energetics in 1980 enabled this method of target irradiation to be experimented with in the 24 beam OMEGA laser and the NOVETTE laser, which was followed by the Nova laser design with 10 times the energy of Shiva, the first design with the specific goal of reaching ignition conditions.\n\nNova also failed in its goal of achieving ignition, this time due to severe variation in laser intensity in its beams (and differences in intensity between beams) caused by filamentation which resulted in large non-uniformity in irradiation smoothness at the target and asymmetric implosion. The techniques pioneered earlier could not address these new issues. But again this failure led to a much greater understanding of the process of implosion, and the way forward again seemed clear, namely the increase in uniformity of irradiation, the reduction of hot-spots in the laser beams through beam smoothing techniques to reduce Rayleigh–Taylor instability imprinting on the target and increased laser energy on target by at least an order of magnitude. Funding for fusion research was severely constrained in the 80's, but Nova nevertheless successfully gathered enough information for a next generation machine.\nThe resulting design, now known as the National Ignition Facility, started construction at LLNL in 1997. NIF's main objective will be to operate as the flagship experimental device of the so-called nuclear stewardship program, supporting LLNLs traditional bomb-making role. Completed in March 2009, NIF has now conducted experiments using all 192 beams, including experiments that set new records for power delivery by a laser.\nThe first credible attempts at ignition were initially scheduled for 2010, but ignition was not achieved as of September 30, 2012. As of October 7, 2013, the facility is understood to have achieved an important milestone towards commercialization of fusion, namely, for the first time a fuel capsule gave off more energy than was applied to it. This is still a long way from satisfying the Lawson criterion, but is a major step forward.\n\nA more recent development is the concept of \"fast ignition,\" which may offer a way to directly heat the high density fuel after compression, thus decoupling the heating and compression phases of the implosion. In this approach the target is first compressed \"normally\" using a driver laser system, and then when the implosion reaches maximum density (at the stagnation point or \"bang time\"), a second ultra-short pulse ultra-high power petawatt (PW) laser delivers a single pulse focused on one side of the core, dramatically heating it and hopefully starting fusion ignition. The two types of fast ignition are the \"plasma bore-through\" method and the \"cone-in-shell\" method. In the first method the petawatt laser is simply expected to bore straight through the outer plasma of an imploding capsule and to impinge on and heat the dense core, whereas in the cone-in-shell method, the capsule is mounted on the end of a small high-z (high atomic number) cone such that the tip of the cone projects into the core of the capsule. In this second method, when the capsule is imploded, the petawatt has a clear view straight to the high density core and does not have to waste energy boring through a 'corona' plasma; however, the presence of the cone affects the implosion process in significant ways that are not fully understood. Several projects are currently underway to explore the fast ignition approach, including upgrades to the OMEGA laser at the University of Rochester, the GEKKO XII device in Japan, and an entirely new £500 million facility, known as HiPER, proposed for construction in the European Union. If successful, the fast ignition approach could dramatically lower the total amount of energy needed to be delivered to the target; whereas NIF uses UV beams of 2 MJ, HiPER's driver is 200 kJ and heater 70 kJ, yet the predicted fusion gains are nevertheless even higher than on NIF.\n\nLaser Mégajoule, the French project, has seen its first experimental line achieved in 2002, and its first target shots were finally conducted in 2014. The machine was roughly 75% complete as of 2016.\n\nUsing a different approach entirely is the \"z\"-pinch device. \"Z\"-pinch uses massive amounts of electric current which is switched into a cylinder comprising extremely fine wires. The wires vaporize to form an electrically conductive, high current plasma; the resulting circumferential magnetic field squeezes the plasma cylinder, imploding it and thereby generating a high-power x-ray pulse that can be used to drive the implosion of a fuel capsule. Challenges to this approach include relatively low drive temperatures, resulting in slow implosion velocities and potentially large instability growth, and preheat caused by high-energy x-rays.\n\nMost recently, Winterberg has proposed the ignition of a deuterium microexplosion, with a gigavolt super-Marx generator, which is a Marx generator driven by up to 100 ordinary Marx generators.\n\nPractical power plants built using ICF have been studied since the late 1970s when ICF experiments were beginning to ramp up to higher powers; they are known as inertial fusion energy, or IFE plants. These devices would deliver a successive stream of targets to the reaction chamber, several a second typically, and capture the resulting heat and neutron radiation from their implosion and fusion to drive a conventional steam turbine.\n\nIFE faces continued technical challenges in reaching the conditions needed for ignition. But even if these were all to be solved, there are a significant number of practical problems that seem just as difficult to overcome. Laser-driven systems were initially believed to be able to generate commercially useful amounts of energy. However, as estimates of the energy required to reach ignition grew dramatically during the 1970s and '80s, these hopes were abandoned. Given the low efficiency of the laser amplification process (about 1 to 1.5%), and the losses in generation (steam-driven turbine systems are typically about 35% efficient), fusion gains would have to be on the order of 350 just to energetically break even. These sorts of gains appeared to be impossible to generate, and ICF work turned primarily to weapons research.\n\nWith the recent introduction of fast ignition and similar approaches, things have changed dramatically. In this approach gains of 100 are predicted in the first experimental device, HiPER. Given a gain of about 100 and a laser efficiency of about 1%, HiPER produces about the same amount of \"fusion\" energy as electrical energy was needed to create it. It also appears that an order of magnitude improvement in laser efficiency may be possible through the use of newer designs that replace the flash lamps with laser diodes that are tuned to produce most of their energy in a frequency range that is strongly absorbed. Initial experimental devices offer efficiencies of about 10%, and it is suggested that 20% is a real possibility with some additional development.\n\nWith \"classical\" devices like NIF about 330 MJ of electrical power are used to produce the driver beams, producing an expected yield of about 20 MJ, with the maximum credible yield of 45 MJ. Using the same sorts of numbers in a reactor combining fast ignition with newer lasers would offer dramatically improved performance. HiPER requires about 270 kJ of laser energy, so assuming a first-generation diode laser driver at 10% the reactor would require about 3 MJ of electrical power. This is expected to produce about 30 MJ of fusion power. Even a very poor conversion to electrical energy appears to offer real-world power output, and incremental improvements in yield and laser efficiency appear to be able to offer a commercially useful output.\n\nICF systems face some of the same secondary power extraction problems as magnetic systems in generating useful power from their reactions. One of the primary concerns is how to successfully remove heat from the reaction chamber without interfering with the targets and driver beams. Another serious concern is that the huge number of neutrons released in the fusion reactions react with the plant, causing them to become intensely radioactive themselves, as well as mechanically weakening metals. Fusion plants built of conventional metals like steel would have a fairly short lifetime and the core containment vessels will have to be replaced frequently.\n\nOne current concept in dealing with both of these problems, as shown in the HYLIFE-II baseline design, is to use a \"waterfall\" of FLiBe, a molten mix of fluoride salts of lithium and beryllium, which both protect the chamber from neutrons and carry away heat. The FLiBe is then passed into a heat exchanger where it heats water for use in the turbines. The tritium produced by fissioning lithium nuclei can also be extracted in order to close the power plant's thermonuclear fuel cycle, a necessity for perpetual operation because tritium does not exist in quantity naturally and must be manufactured. Another concept, Sombrero, uses a reaction chamber built of Carbon-fiber-reinforced polymer which has a very low neutron cross section. Cooling is provided by a molten ceramic, chosen because of its ability to stop the neutrons from traveling any further, while at the same time being an efficient heat transfer agent.\n\nEven if these technical advances solve the considerable problems in IFE, another factor working against IFE is the cost of the fuel. Even as Nuckolls was developing his earliest detailed calculations on the idea, co-workers pointed this out: if an IFE machine produces 50 MJ of fusion energy, one might expect that a shot could produce perhaps 10 MJ of power for export. Converted to better known units, this is the equivalent of 2.8 kWh of electrical power. Wholesale rates for electrical power on the grid were about 0.3 cents/kWh at the time, which meant the monetary value of the shot was perhaps one cent. In the intervening 50 years the price of power has remained about even with the rate of inflation, and the rate in 2012 in Ontario, Canada was about 2.8 cents/kWh\n\nThus, in order for an IFE plant to be economically viable, fuel shots would have to cost considerably less than ten cents in year 2012 dollars. At the time this objection was first noted, Nuckolls suggested using liquid droplets sprayed into the hohlraum from an eye-dropper-like apparatus. Given the ever-increasing demands for higher uniformity of the targets, this approach does not appear practical, as even the inner ablator and fuel itself currently costs several orders of magnitude more than this. Moreover, Nuckolls' solution had the fuel dropped into a fixed hohlraum that would be re-used in a continual cycle, but at current energy levels the hohlraum is destroyed with every shot.\n\nDirect-drive systems avoid the use of a hohlraum and thereby may be less expensive in fuel terms. However, these systems still require an ablator, and the accuracy and geometrical considerations are even more important. They are also far less developed than the indirect-drive systems, and face considerably more technical problems in terms of implosion physics. Currently there is no strong consensus whether a direct-drive system would actually be less expensive to operate.\n\nThe various phases of such a project are the following, the sequence of inertial confinement fusion development follows much the same outline:\n\nAt the moment, according to the available data, inertial confinement fusion experiments have not gone beyond the first phase, although Nova and others have repeatedly demonstrated operation within this realm. In the short term a number of new systems are expected to reach the second stage.\n\nFor a true industrial demonstration, further work is required. In particular, the laser systems need to be able to run at high operating frequencies, perhaps one to ten times a second. Most of the laser systems mentioned in this article have trouble operating even as much as once a day. Parts of the HiPER budget are dedicated to research in this direction as well. Because they convert electricity into laser light with much higher efficiency, diode lasers also run cooler, which in turn allows them to be operated at much higher frequencies. HiPER is currently studying devices that operate at 1 MJ at 1 Hz, or alternately 100 kJ at 10 Hz.\n\nR&D continued on inertial fusion energy in the European Union and in Japan. The High Power laser Energy Research facility (HiPER) is a proposed experimental fusion device undergoing preliminary design for possible construction in the European Union to continue the development of laser-driven inertial confinement approach. HiPER is the first experiment designed specifically to study the \"fast ignition\" approach to generating nuclear fusion. Using much smaller lasers than conventional designs, yet produces fusion power outputs of about the same magnitude would offer a much higher \"Q\" with a reduction in construction costs of about ten times. Theoretical research since the design of HiPER in the early 2000s has cast doubt on fast ignition but a new approach known as \"shock ignition\" has been proposed to address some of these problems. Japan developed the KOYO-F fusion reactor design and laser inertial fusion test (LIFT) experimental reactor. In April 2017, Bloomberg News reported that Mike Cassidy, former Google vice-president and director of Project Loon with Google[x], started a clean energy startup, Apollo Fusion, to develop a hybrid fusion-fission reactor technology.\n\nThe very hot and dense conditions encountered during an Inertial Confinement Fusion experiment are similar to those created in a thermonuclear weapon, and have applications to the nuclear weapons program. ICF experiments might be used, for example, to help determine how warhead performance will degrade as it ages, or as part of a program of designing new weapons. Retaining knowledge and corporate expertise in the nuclear weapons program is another motivation for pursuing ICF. Funding for the NIF in the United States is sourced from the 'Nuclear Weapons Stockpile Stewardship' program, and the goals of the program are oriented accordingly. It has been argued that some aspects of ICF research may violate the Comprehensive Test Ban Treaty or the Nuclear Non-Proliferation Treaty. In the long term, despite the formidable technical hurdles, ICF research might potentially lead to the creation of a \"pure fusion weapon\".\n\nInertial confinement fusion has the potential to produce orders of magnitude more neutrons than spallation. Neutrons are capable of locating hydrogen atoms in molecules, resolving atomic thermal motion and studying collective excitations of photons more effectively than X-rays. Neutron scattering studies of molecular structures could resolve problems associated with protein folding, diffusion through membranes, proton transfer mechanisms, dynamics of molecular motors, etc. by modulating thermal neutrons into beams of slow neutrons. In combination with fissionable materials, neutrons produced by ICF can potentially be used in Hybrid Nuclear Fusion designs to produce electric power.\n\n\n"}
{"id": "41214485", "url": "https://en.wikipedia.org/wiki?curid=41214485", "title": "Interspecies family", "text": "Interspecies family\n\nThe term \"interspecies family\" refers to a group consisting of at least two members of different species who deeply care for each other. Examples include a human and their dog, a couple and their cats, a dog and a cat, or even a mule and a sheep. The emphasis is on love and that the members of the group care for and treat each other like a family. For instance, \"interspecies family\" may describe a group composed of a dog and a person who refers to their dog as their child, best friend, or other phrase that connotes a stronger bond than just a \"pet\", a term which implies a sense of property and ownership.\n\nMost often this is used to discuss non-human interspecies families, typically where a mother of one species will foster a youngling from a different species. With the more recent growth of the fields of anthrozoology and animal studies, this is being used more frequently to refer especially to bonds between human and non-human animals. \n\nSome of the earliest instances of the interspecies family involve the use of anthropomorphism, such as in the children's book \"Stuart Little\" in which a mouse is a member of a human family. This is a fairly common idea which led to some of the first uses of \"interspecies family\" in media targeted towards children.\n\nThere is also a popular trend in books and internet sites that involves capturing photographs and stories of interspecies non-human animal families. These heartwarming cases promote the idea that animals can look past such differences. However, in these cases they are more often referred to as \"interspecies adoptions\", \"interspecies pairings\", and \"interspecies friendships\".\n\nJust recently, the term has been used to describe non-fiction situations involving human and non-human animal relationships. In 2011 a dissertation was written by Avigdor Edminster entitled \"'This Dog Means Life': Making Interspecies Relations at an Assistance Dog Agency\" in which \"Interspecies families\" is used frequently. There have been other instances since then, but they have been strictly within academic works.\n\nIn the fall of 2013, The National Museum of Animals & Society created the exhibit entitled \"My Dog Is My Home: The Experience of Human-Animal Homelessness\". The exhibit explores the experience of being homeless with an animal, and specifically the needs that are unmet because of homeless services' failure to recognize the legitimacy of the human-animal bond and their status as an \"interspecies family\". The exhibit helped popularize the phrase \"interspecies family\" among its animal protection audience as well as among social service providers. Part of the museum's mission is to promote this idea of inter-species families and make the general public more aware of the strong bonds that can be shared between human and non-human animals.\n"}
{"id": "53842346", "url": "https://en.wikipedia.org/wiki?curid=53842346", "title": "James Douglas Beason", "text": "James Douglas Beason\n\nJames Douglas Beason from the Air Force Research Laboratory, was awarded the status of Fellow in the American Physical Society, after they were nominated by their American Physical Society in 2000, for \"his leadership advancing, advocating and formulating national science policy, in particular, for his impact throughout the government in basic research, and for his fundamental contributions solving the relativistic Compton scattering kernel, and inventing innovative techniques for simulating lasers and plasmas.\"\n"}
{"id": "50978057", "url": "https://en.wikipedia.org/wiki?curid=50978057", "title": "Jean Louis Florent Polydore Roux", "text": "Jean Louis Florent Polydore Roux\n\nJean Louis Florent Polydore Roux (27 July 1792, Marseille -12 April 1833 , Bombay) was a French painter and naturalist.\n\nJean-Louis-Florent-Polydore Roux was, from his childhood, interested in natural history and had a large insect collection.He was taught by Pierre André Latreille and Georges Cuvier at Académie des sciences de l'Institut de France in Paris and was in 1819 appointed curator of the Muséum d'histoire naturelle de Marseille.He published \"Catalogue d’insectes de Provence\", 1820 a 2 volume work on birds \"Ornithologie provençale\", 1833 and \"Crustacés de la Méditerranée et de son littoral\",1828–30, which including 45 coloured plates, which he himself had made. He also published on molluscs \"Iconographie conchyliologique\", 1828.Marine painting was another of his occupations and a Roux family concern. Roux was a correspondent of Risso who in 1826 named the copepod \"Pandarus rouxi\" after him. In 1831 he joined Charles von Hügel, who was travelling for the Austrian government, on an excursion to Egypt and from there in 1832 to Bombay, where he later died of plague.\n\nA species of Indian lizard, \"Calotes rouxii\", is named in his honor.\n\n\n"}
{"id": "18576196", "url": "https://en.wikipedia.org/wiki?curid=18576196", "title": "Lariat chain", "text": "Lariat chain\n\nA Lariat chain is a loop of chain that hangs off, and is spun by a wheel. It is often used as a science exhibit or a toy. \n\nThe original Lariat Chain was created in 1986 by Norman Tuck, as an Artist-in-Residence project at the Exploratorium in San Francisco. \n\nLariat Chain was developed from an earlier Tuck piece entitled Chain Reaction (1984). Chain Reaction was hand cranked, and utilized a heavy chain attached by magnets onto an iron flywheel. As in Lariat Chain, Chain Reaction used a brush to disrupt the motion of the traveling chain. \n\nThe speed of the chain is arranged to equal the wave speed of transverse waves, so that waves moving against the motion of the chain appear to be standing still.\n\n\n"}
{"id": "38364055", "url": "https://en.wikipedia.org/wiki?curid=38364055", "title": "Linear parameter-varying control", "text": "Linear parameter-varying control\n\nLinear parameter-varying control (LPV control) deals with the control of linear parameter-varying systems, a class of nonlinear systems which can be modelled as parametrized linear systems whose parameters change with their state.\n\nIn designing feedback controllers for dynamical systems a variety of modern, multivariable controllers are used. In general, these controllers are often designed at various operating points using linearized models of the system dynamics and are scheduled as a function of a parameter or parameters for operation at intermediate conditions. It is an approach for the control of non-linear systems that uses a family of linear controllers, each of which provides satisfactory control for a different operating point of the system. One or more observable variables, called the scheduling variables, are used to determine the current operating region of the system and to enable the appropriate linear controller. For example, in case of aircraft control, a set of controllers are designed at different gridded locations of corresponding parameters such as AoA, Mach, dynamic pressure, CG etc. In brief, gain scheduling is a control design approach that constructs a nonlinear controller for a nonlinear plant by patching together a collection of linear controllers. These linear controllers are blended in real-time via switching or interpolation.\n\nScheduling multivariable controllers can be very tedious and time-consuming task. A new paradigm is the linear parameter-varying (LPV) techniques which synthesize of automatically scheduled multivariable controller.\n\n\nThough the approach is simple and the computational burden of linearization scheduling approaches is often much less than for other nonlinear design approaches, its inherent drawbacks outweigh its advantages and necessitates a new paradigm for the control of dynamical systems. New methodologies such as Adaptive control based on Artificial Neural Networks (ANN), Fuzzy logic etc. try to address such problems, the lack of proof of stability and performance of such approaches over entire operating parameter regime requires design of a parameter dependent controller with guaranteed properties for which, a Linear Parameter Varying controller could be an ideal candidate.\n\nLPV systems are a very special class of nonlinear systems which appears to be well suited for control of dynamical systems with parameter variations. In general, LPV techniques provide a systematic design procedure for gain-scheduled multivariable controllers. This methodology allows performance, robustness and bandwidth limitations to be incorporated into a unified framework. A brief introduction on the LPV systems and the explanation of terminologies are given below.\n\nIn control engineering, a state space representation is a mathematical model of a physical system as a set of input, formula_1 output, formula_2 and state variables, formula_3 related by first-order differential equations. The dynamic evolution of a nonlinear, non-autonomous system is represented by\n\nIf the system is time variant\n\nThe state variables describe the mathematical \"state\" of a dynamical system and in modeling large complex nonlinear systems if such state variables are chosen to be compact for the sake of practicality and simplicity, then parts of dynamic evolution of system are missing. The state space description will involve other variables called exogenous variables whose evolution is not understood or is too complicated to be modeled but affect the state variables evolution in a known manner and are measurable in real-time using sensors.\nWhen a large number of sensors are used, some of these sensors measure outputs in the system theoretic sense as known, explicit nonlinear functions of the modeled states and time, while other sensors are accurate estimates of the exogenous variables. Hence, the model will be a time varying, nonlinear system, with the future time variation unknown, but measured by the sensors in real-time.\nIn this case, if formula_7 denotes the exogenous variable vector, and formula_8 denotes the modeled state, then the state equations are written as\n\nThe parameter formula_10 is not known but its evolution is measured in real time and used for control. If the above equation of parameter dependent system is linear in time then it is called Linear Parameter Dependent systems. They are written similar to Linear Time Invariant form albeit the inclusion in time variant parameter.\n\nParameter-dependent systems are linear systems, whose state-space descriptions are known functions of time-varying parameters. The time variation of each of the parameters is not known in advance, but is assumed to be measurable in real time. The controller is restricted to be a linear system, whose state-space entries depend causally on the parameter’s history. There exist three different methodologies to design a LPV controller namely,\n\nThese problems are solved by reformulating the control design into finite-dimensional, convex feasibility problems which can be solved exactly, and infinite-dimensional convex feasibility problems which can be solved approximately .\nThis formulation constitutes a type of gain scheduling problem and contrast to classical gain scheduling, this approach address the effect of parameter variations with assured stability and performance.\n\n"}
{"id": "37924849", "url": "https://en.wikipedia.org/wiki?curid=37924849", "title": "List of Gaviiformes by population", "text": "List of Gaviiformes by population\n\nThis is a list of Gaviiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nAll Gaviiformes have been quantified.\n"}
{"id": "47816925", "url": "https://en.wikipedia.org/wiki?curid=47816925", "title": "List of U.S. state cacti", "text": "List of U.S. state cacti\n\nTwo states of the United States have designated an official state indigenous species of cactus.\n\n"}
{"id": "7280414", "url": "https://en.wikipedia.org/wiki?curid=7280414", "title": "List of commonly available chemicals", "text": "List of commonly available chemicals\n\nMany chemicals are commonly available in pure form. Others are available as reagents - inexpensive, convenient sources of chemicals with a bit of processing. This is convenient for both amateur and professional chemistry work. Common reagents include:\n\nDetails on the constituent chemicals of various household products can be found on the U.S. Department of Health and Human Services Household Products Index.\n\n"}
{"id": "731879", "url": "https://en.wikipedia.org/wiki?curid=731879", "title": "List of eponymously named medical signs", "text": "List of eponymously named medical signs\n\nEponymous medical signs are those that are named after a person or persons, usually the physicians who first described them, but occasionally named after a famous patient. This list includes other eponymous entities of diagnostic significance; i.e. tests, reflexes, etc.\n\nNumerous additional signs can be found for Graves disease under Graves' ophthalmopathy.\n\n"}
{"id": "23260", "url": "https://en.wikipedia.org/wiki?curid=23260", "title": "List of peninsulas", "text": "List of peninsulas\n\nA peninsula ( from \"paene\" \"almost\" and \"insula\" \"island\") is a piece of land that is bordered by water on three sides but connected to mainland. The surrounding water is usually understood to be continuous, though not necessarily named as such. A peninsula can also be a headland, cape, island promontory, bill, point,\nor spit. A point is generally considered a tapering piece of land projecting into a body of water that is less prominent than a cape. In English, the plural of peninsula is \"peninsulas\" or, less commonly, \"peninsulae\". A river which courses through a very tight meander is also sometimes said to form a \"peninsula\" within the (almost closed) loop of water.\n\nPresented below is a list of peninsulas.\n\nThe Horn of Africa is a peninsula in Northeast Africa that juts into the Guardafui Channel, and is the easternmost projection of the African continent. It denotes the region containing the countries of Somalia, Eritrea, Djibouti and Ethiopia.\n\nAntarctica \n\n\n\n\n\nThe Indian subcontinent is a peninsula, the only land feature in the world that is widely recognized as a subcontinent in the English language.\n\n\n\n\n\nThe whole land mass encompassing North and South Korea is a peninsula, surrounded by the Sea of Japan on the east, the East China Sea to the south, and the Yellow Sea to the west, with the Korea Strait connecting the first two bodies of water.\n\n\n\n\n\nEurope is sometimes considered to be a large peninsula extending off Eurasia. As such, it is one of the largest peninsulas in the world and the only one to have the status as a full continent, largely as a matter of convention rather than science. It is composed of many smaller peninsulas, the four main component peninsulas being the Iberian, Scandinavian, Italian, and Balkan peninsulas.\n\nThe Balkans is a peninsula including Albania, Bosnia and Herzegovina, Bulgaria, Croatia, Greece, Kosovo, Macedonia, Montenegro, Romania, Serbia, Slovenia and the European part of Turkey.\n\n\nEncompassing continental Spain and Portugal, Andorra, British overseas territory of Gibraltar and a small amount of southern France.\n\n\nThe Kamchatka Peninsula in Russia is one of the most well-known peninsulas in the world, surrounded by the Pacific Ocean to the east and the Sea of Okhotsk to the east. It is one of the most volcanically active places in the world, part of the Ring of Fire system, with over a hundred volcanoes. The Kamchatka Krai occupies the peninsula but the northern part is in the mainland.\n\nScandinavia is a peninsula and along with other islands encompassing present-day Sweden, Norway, Denmark, and part of Finland.\n\n\n\n\n\n\n\n\n\n\n\n\nFlorida is a well-known example of a large peninsula, with its land area divided between the larger Florida peninsula and the smaller Florida panhandle on the north and west. It has several smaller peninsulas within it:\n\n\n\n\nMichigan – the only bi-peninsular state – is very distinguishable for its mitten-shaped Lower Peninsula of Michigan which includes:\n\nThe rifle-shaped Upper Peninsula of Michigan contains: \n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4354406", "url": "https://en.wikipedia.org/wiki?curid=4354406", "title": "List of pharmacists", "text": "List of pharmacists\n\nThis is a list of notable pharmacists, sorted by particular fields in which they distinguished themselves:\n\n\n\n\n\n\n\n\n"}
{"id": "20178324", "url": "https://en.wikipedia.org/wiki?curid=20178324", "title": "Lists of statistics topics", "text": "Lists of statistics topics\n\nThis article itemizes the various lists of statistics topics. Some of these lists link to hundreds of articles; some link only to a few.\n\n\n\n\n\n\n\n"}
{"id": "428860", "url": "https://en.wikipedia.org/wiki?curid=428860", "title": "Maceral", "text": "Maceral\n\nA maceral is a component, organic in origin, of coal or oil shale. The term 'maceral' in reference to coal is analogous to the use of the term 'mineral' in reference to igneous or metamorphic rocks. Examples of macerals are inertinite, vitrinite, and liptinite.\n\nInertinite is considered to be the equivalent of charcoal and degraded plant material. It is highly oxidised in nature and may be said to be burnt. A large portion of South Africa's coal reserves consist of inertinite.\n\nVitrinite is shiny, glass-like material that is considered to be composed of cellular plant material such as roots, bark, plant stems and tree trunks. Vitrinite macerals when observed under the microscope show a boxlike, cellular structure, often with oblong voids and cavities which are likely the remains of plant stems. This has a high calorific value (24 - 28 MJ/kg) and a large proportion of volatile matter (24 - 30%). It often occurs interbanded or interlaminated with inertinite and can be recognised as bright bands.\n\nLiptinite macerals are considered to be produced from decayed leaf matter, spores, pollen and algal matter. Resins and plant waxes can also be part of liptinite macerals. Liptinite macerals tend to retain their original plant form, i.e., they resemble plant fossils. These are hydrogen rich and have the highest calorific values of all coal macerals.\n\nMacerals of liptinite are sporinite, cutinite, resinite, alginite (telalginite and lamalginite), liptodetrinite, fluorinite, and bituminite.\n\nMacerals are considered to be dehydrogenated plant fragments. Evidence for this includes remnant pollen spores, fossilised leaves, remnant cellular structure and similar. In rare cases, maceral and fossilised pollen can be found in terrestrial sedimentary rocks.\n\nMaceral maturity can be estimated by \"vitrinite reflectance\". This gives information on the carbon, hydrogen and nitrogen composition of the coal, and determines the type of coal: lignite, brown coal, bituminous coal, anthracite or graphite.\n\nMacerals found in kerogen source rocks are often observed under the microscope to determine the kerogen maturity of the sedimentary formations. This is a vital component of oil and gas exploration.\n\nMacerals are observed under the petrographic microscope under reflected light. Coal fragments must be extremely highly polished down to less than half a micrometre before they can be observed under the microscope.\n\n\n"}
{"id": "36842564", "url": "https://en.wikipedia.org/wiki?curid=36842564", "title": "Microdensitometer", "text": "Microdensitometer\n\nA microdensitometer is an optical instrument used to measure optical densities in the microscopic domain. A well-known microdensitometer, used in the photographic industry, is a granularity instrument or granularity machine. The granularity measurement involves the use of an optical aperture, 10-50 micrometers in diameter, and in the recording of thousands of optical density readings. The standard deviation of this series of measurements is known as the \"granularity\" of the measured transmission surface, optical film, or photographic film, in particular .\n\nAn alternative version to the traditional point-by-point microdensitometer is the beam expanded laser microdensitometer. This instrument can illuminate simultaneously an area a few centimeters wide with an ultra thin height, in the micrometer regime. Advantages include increased depth of focus, significant increases in data collection speed, and superior signal to noise ratios. In microscopy applications, this type of ultra thin beam-expanded illumination can also be known as light sheet illumination or selective plane illumination.\n\nThis measurement technique, using ultra-thin expanded laser beams, is particularly useful to detect microscopic imperfections in optical coatings or transmission optical surfaces.\n\n"}
{"id": "47707028", "url": "https://en.wikipedia.org/wiki?curid=47707028", "title": "Mycoplankton", "text": "Mycoplankton\n\nMycoplankton are saprotropic members of the plankton communities of marine and freshwater ecosystems. They are composed of filamentous free-living fungi and yeasts that are associated with planktonic particles or phytoplankton. Similar to bacterioplankton, these aquatic fungi play a significant role in heterotrophic mineralization and nutrient cycling. Mycoplankton can be up to 20 mm in diameter and over 50 mm in length.\n\nIn a typical milliliter of seawater, there are approximately 10 to 10 fungal cells. This number is greater in coastal ecosystems and estuaries due to nutritional runoff from terrestrial communities. The greatest diversity and number of species of mycoplankton is found in surface waters (< 1000 m), and the vertical profile depends on the abundance of phytoplankton. Furthermore, this difference in distribution may vary between seasons due to nutrient availability. Aquatic fungi survive in a constant oxygen deficient environment, and therefore depend on oxygen diffusion by turbulence and oxygen generated by photosynthetic organisms.\n\nAquatic fungi can be classified using three groups:\nThe majority of mycoplankton species are higher fungi, found in the Ascomycota and Basidiomycota phylums.\n\nAccording to fossil records, fungi date back to the late Proterozoic era, 900-570 million years ago. It is hypothesized that mycoplankton evolved from terrestrial fungi, likely in the Paleozoic era (390 million years ago). The methods and pathways of terrestrial fungi's adaption to the marine environment are still under study.\n\nThe primary role of all fungi is to degrade detrital organic matter from plants, and mycoplankton is no exception. By working with microbial communities, mycoplankton efficiently converts particulate organic matter to dissolved organic matter as part of the biogeochemical cycle. Mycoplankton and heterotrophic bacteria mediate carbon, nitrogen, oxygen, and other nutrient fluxes in marine ecosystems. It has been shown that there are higher concentrations of mycoplankton near the surface and in shallow waters, which indicates their connection with the upwelling of organic matter. This further correlates with abundant phytoplankton communities at the surface, implying that mycoplankton is intimately involved in organic matter consumption in the euphotic zone.\n\n"}
{"id": "28462966", "url": "https://en.wikipedia.org/wiki?curid=28462966", "title": "Open-access mandate", "text": "Open-access mandate\n\nAn open-access mandate is a policy adopted by a research institution, research funder, or government which requires researchers—usually university faculty or research staff and/or research grant recipients—to make their published, peer-reviewed journal articles and conference papers open access (1) by self-archiving their final, peer-reviewed drafts in a freely accessible institutional repository or disciplinary repository (\"Green OA\") or (2) by publishing them in an open-access journal (\"Gold OA\") or both.\n\nAmong the universities that have adopted open-access mandates for faculty are Harvard University, Massachusetts Institute of Technology, University College London, Queensland University of Technology, University of Minho (Portugal), University of Liege and ETH Zürich. Among the funding organizations that have adopted open-access mandates for grant recipients are National Institutes of Health (with the NIH Public Access Policy), Research Councils UK, National Fund for Scientific Research, Wellcome Trust and European Research Council. For a full index of institutional and funder open-access mandates adopted to date, see the Registry of Open Access Mandatory Archiving Policies (ROARMAP).\n\nOpen-access mandates can be classified in many ways: by the type of mandating organization (employing institution or research funder), by the locus (institutional or institution-external) and timing of deposit itself (immediate, delayed), by the time (immediate, delayed) at which the deposit is made open access, and by whether or not there is a default copyright-retention contract (and whether it can be waived). Mandate types can also be compared for strength and effectiveness (in terms of the annual volume, proportion and timing of deposits, relative to total annual article output, as well as the time that access to the deposit is set as open access. Mandates are classified and ranked by some of these properties in MELIBEA.\n\nUniversities can adopt open-access mandates for their faculty. All such mandates make allowances for special cases. Tenured faculty cannot be required to publish; nor can they be required to make their publications open access. However, mandates can take the form of administrative procedures, such as designating repository deposit as the official means of submitting publications for institutional research performance review, or for research grant applications or renewal. Many European university mandates have taken the form of administrative requirements, whereas many U.S. university mandates have taken the form of a unanimous or near-unanimous self-imposed faculty consensus consisting of a default rights-retention contract (together with a waiver option for individual special cases).\n\nResearch funders such as government funding agencies or private foundations can adopt open-access mandates as contractual conditions for receiving funding.\n\n\"Mandate\" can mean either \"authorize\" or \"oblige\". Both senses are important in inducing researchers to provide OA. Open-access advocate Peter Suber has remarked that \"'mandate' is not a good word...\" for open-access policies, \"...but neither is any other English word.\" Other ways to describe a mandate include \"shifting the default publishing practice to open access\" in the case of university faculty or \"putting an open-access condition\" on grant recipients. Mandates are stronger than policies which either request or encourage open access, because they require that authors provide open access. Some mandates allow the author to opt out if they give reasons for doing so.\n\n\nMandates may include the following clauses:\n\n\nMost institutional open-access mandates require that authors self archive their papers in their own institutional repository. Some funder mandates specify institutional deposit, some specify institution-external deposit, and some allow either.\n\nMandates may require deposit immediately upon publication (or acceptance for publication) or after an allowable embargo.\n\nMandates may require opening access to the deposit immediately upon publication (or acceptance for publication) or after an allowable embargo.\n\nThe Canadian Institutes of Health Research (CIHR) proposed a mandate in 2006 and adopted it in September 2007, becoming the first North American public research funder to do so. The CIHR Policy on Access to Research Outputs provides two options to researchers: publication in open access journals, and making their manuscripts available in an online central (PubMed Central Canada is recommended) or institutional repository.\n\nIn October 2013, the two other Canadian federal funding agencies, the National Science and Engineering Council (NSERC) and the Social Science and Humanities Research Council (SSHRC) jointly proposed the same mandate as CIHR's, and launched a two-month consultation on what will become the Tri-Agency Open Access Policy.\n\nOn February 27, 2015 a Tri-Agency Open Access Policy on Publications was announced. Peer-reviewed journal publications arising from Agency-supported research must be made freely available within 12 months of publication, whether by depositing in an online repository or by publishing in a journal that offers immediate or delayed open access. The policy is effective for grants awarded from May 1, 2015 onward.\n\nOn May 1, 2015 the International Development Research Centre adopted a new open access policy. Books and journal articles must be made freely available within 12 months of publication, whether by publishing open access and using open access journals, or by uploading to an open access repository. The policy is effective for proposals received on or after July 20, 2015.\n\nIn May 2006, the US Federal Research Public Access Act (FRPAA) was proposed toward improving the NIH Public Access Policy. Besides points about making open access mandatory, to which the NIH complied in 2008, it argues to extend self-archiving to the full spectrum of major US-funded research. In addition, the FRPAA would no longer stipulate that the self-archiving must be central; the deposit can now be in the author's own institutional repository (IR). The new U.S. National Institutes of Health's Public Access Policy took effect in April 2008 and states that \"all articles arising from NIH funds must be submitted to PubMed Central upon acceptance for publication\". It stipulates self-archiving in PubMed Central rather than in the author's own institutional repository. In 2012, the NIH announced it would enforce its Public Access Policy by blocking the renewal of grant funds to authors who don't follow the policy.\n\nIn February 2013, the Fair Access to Science and Technology Research bill was introduced into both houses of Congress. It was described as a \"strengthened version of FRPAA\" In the same month, the White House issued a directive requiring federal agencies \"with over $100 million in annual conduct of research and development expenditures\" to develop, within the next 6 months, a plan to make the peer-reviewed publications directly arising from Federal funding \"publicly accessible to search, retrieve, and analyze\".\n\nIn April 2006, the European Commission recommended: \"EC Recommendation A1: \"Research funding agencies... should [e]stablish a European policy mandating published articles arising from EC-funded research to be available after a given time period in open access archives...\" This recommendation has since been updated and strengthened by the European Research Advisory Board (EURAB). The project OpenAIRE (Open Access Infrastructure for Research in Europe) has since been launched.\n\nThe global shift towards open access to the results of publicly funded research (publications and data) has been a core strategy in the European Commission to improve knowledge circulation and thus innovation. It is illustrated in particular by the general principle for open access to scientific publications in Horizon 2020 and the pilot for research data. In 2012, via a Recommendation, the European Commission encouraged all EU Member States to put publicly funded research results in the public sphere in order to strengthen science and the knowledge-based economy. In 2017 it emerged that the European Commission are looking to create its own open access publishing platform for papers that emerge from the Horizon 2020 programme. The platform is likely to be similar to the one used by Wellcome Trust for Wellcome Open Research and Gates Foundation's Gates Open Research.\n\nTo somewhat improve on the European Commission's (and FRPAA's) allowable embargo of up to six months, EURAB has revised the mandate: all articles must be deposited \"immediately upon acceptance for publication\"; the allowable delay for complying with publisher embargoes applies only to the time when access to the deposit must be made open access rather than to the time when it must be deposited. Immediate deposit is required so that individual users can then request an immediate individual copy of any deposited eprint during the embargo period by clicking on a \"RequestCopy\" Button provided by the Institutional Repository software (e.g., DSPACE, EPrints). The Button automatically sends an email message to the author requesting an individual eprint; the author can comply with one click and the software immediately emails the eprint to the requestor. This is not open access, but may cover some immediate research needs during any embargo. A related idea was later put forth as the Open Access Button for papers that have not been deposited in an Institutional Repository .\n\nFor the four institutions with the oldest self-archiving mandates, the averaged percentage of green open-access self-archiving has been compared to the percentage for control articles from other institutions published in the same journals (for years 2002–2009, measured in 2011). Open-access mandates triple the percent Green OA (see figure below). Respective totals are derived from the Thomson Reuters Web of Science.\n\nAs of May 2015, open-access mandates have been adopted by over 550 universities and research institutions, and over 140 research funders worldwide.\nExamples of universities which have open-access mandates are Harvard University and MIT in the United States and University College London and ETH Zürich in the European Union. Funders which require open access when their funding recipients publish include the NIH in the US and RCUK and ERC in the EU. Mandate policy models and guidance have been provided by the Open Society Institute's EPrints Handbook, EOS, OASIS and Open Access Archivangelism.\n\nROARMAP, the searchable Registry of Open Access Repository Mandates and Policies at the University of Southampton indexes the world's institutional, funder and governmental OA mandates (and the Open Access Scholarly Information Sourcebook (OASIS) as well as EnablingOpenScholarship (EOS) graph the quarterly outcome). SHERPA/JULIET is a SHERPA service which lists funder mandates only.\n\nIn international cross-disciplinary surveys conducted by Swan (2005), the vast majority of researchers respond that they would self archive willingly if their institutions or funders mandated it. Outcome studies by Sale (2006) have confirmed these survey results. Both mandated and unmandated institutional and disciplinary repositories worldwide are indexed by SHERPA's \"OpenDOAR\" and their rate of growth is monitored and displayed by the University of Southampton's Registry of Open Access Repositories (\"ROAR\").\n\nRecent studies have tested which mandate conditions are most effective in generating deposit. The three most important conditions identified were: (1) immediate deposit required, (2) deposit required for performance evaluation, and (3) unconditional opt-out allowed for the OA requirement but no opt-out allowed for the deposit requirement.\n\n\n\n"}
{"id": "18682025", "url": "https://en.wikipedia.org/wiki?curid=18682025", "title": "Paul Breslin", "text": "Paul Breslin\n\nDr. Paul Breslin is a geneticist and biologist.\n\nHe is most notable for his work in taste perception and oral irritation, in humans as well as in \"Drosophila melanogaster\", the common fruit fly.\n\nHe is a Member at the Monell Chemical Senses Center and acts as Director of the Science Apprenticeship Program. He is a Professor in the Department of Nutritional Sciences at Rutgers, the State University of New Jersey.\n"}
{"id": "2137509", "url": "https://en.wikipedia.org/wiki?curid=2137509", "title": "Perfect fluid", "text": "Perfect fluid\n\nIn physics, a perfect fluid is a fluid that can be completely characterized by its rest frame mass density formula_1 and \"isotropic\" pressure \"p\".\n\nReal fluids are \"sticky\" and contain (and conduct) heat. Perfect fluids are idealized models in which these possibilities are neglected. Specifically, perfect fluids have no shear stresses, viscosity, or heat conduction.\n\nIn space-positive metric signature tensor notation, the stress–energy tensor of a perfect fluid can be written in the form\nwhere \"U\" is the 4-velocity vector field of the fluid and where formula_3 is the metric tensor of Minkowski spacetime.\n\nIn time-positive metric signature tensor notation, the stress–energy tensor of a perfect fluid can be written in the form\nwhere \"U\" is the 4-velocity of the fluid and where formula_5 is the metric tensor of Minkowski spacetime\n\nThis takes on a particularly simple form in the rest frame\nwhere formula_7 is the \"energy density\" and formula_8 is the \"pressure\" of the fluid.\nPerfect fluids admit a Lagrangian formulation, which allows the techniques used in field theory, in particular, quantization, to be applied to fluids. This formulation can be generalized, but unfortunately, heat conduction and anisotropic stresses cannot be treated in these generalized formulations.\n\nPerfect fluids are used in general relativity to model idealized distributions of matter, such as the interior of a star or an isotropic universe. In the latter case, the equation of state of the perfect fluid may be used in Friedmann–Lemaître–Robertson–Walker equations to describe the evolution of the universe.\n\nIn general relativity, the expression for the stress–energy tensor of a perfect fluid is written as\nwhere \"U\" is the 4-velocity vector field of the fluid and where formula_10 is the metric, \nwritten with a space-positive signature.\n\n\n\n"}
{"id": "28243573", "url": "https://en.wikipedia.org/wiki?curid=28243573", "title": "Photosynthetic capacity", "text": "Photosynthetic capacity\n\nPhotosynthetic capacity (A) is a measure of the maximum rate at which leaves are able to fix carbon during photosynthesis. It is typically measured as the amount of carbon dioxide that is fixed per metre squared per second, for example as μmol m sec.\n\nPhotosynthetic capacity is limited by carboxylation capacity and electron transport capacity. For example, in high carbon dioxide concentrations or in low light, the plant is not able to regenerate ribulose-1,5-bisphosphate fast enough (also known RUBP, the acceptor molecule in photosynthetic carbon reduction). So in this case, photosynthetic capacity is limited by electron transport of the light reaction, which generates the NADPH and ATP required for the PCR (Calvin) Cycle, and regeneration of RUBP. On the other hand, in low carbon dioxide concentrations, the capacity of the plant to perform carboxylation (adding carbon dioxide to Rubisco) is limited by the amount of available carbon dioxide, with plenty of Rubsico left over.¹ Light response, or photosynthesis-irradiance, curves display these relationships.\n\nRecent studies have shown that photosynthetic capacity in leaves can be increased with an increase in the number of stomata per leaf. This could be important in further crop development engineering to increase the photosynthetic efficiency through increasing diffusion of carbon dioxide into the plant.²\n\n1. Hopkins, WIlliam G. and N. P. A. Hüner. 2009. Introduction to Plant Physiology, 4th ed. John Wiley & Sons, Inc.\n\n2. Tanaka, Y., S. S. Sugano, T. Shimada, and I. Hara-Nishimura. 2013. Enhancement of leaf photosynthetic capacity through increased stomatal density in \"Arabidopsis\". New Phytologist 198: 757-764.\n"}
{"id": "11501863", "url": "https://en.wikipedia.org/wiki?curid=11501863", "title": "Plate trick", "text": "Plate trick\n\nIn mathematics and physics, the plate trick, also known as Dirac's string trick, the belt trick, or the Balinese cup trick, is any of several demonstrations of the idea that rotating an object with strings attached to it by 360 degrees does not return the system to its original state, while a second rotation of 360 degrees, a total rotation of 720 degrees, does. Mathematically, it is a demonstration of the theorem that SU(2) (which double-covers SO(3)) is simply connected. To say that SU(2) double-covers SO(3) essentially means that the unit quaternions represent the group of rotations twice over.\n\nOne way of doing the trick is to rest a small plate flat on the palm, then perform two rotations of one's hand while keeping the plate upright, ending in the original position. The hand makes one rotation passing over its shoulder, twisting the arm, and then another rotation passing under, untwisting it.\n\nThere is a Balinese candle dance, where an open cup of liquid is held instead of a plate. Since the feet can remain fixed during the manoeuvre, but the hand rotates twice, and all the arm and shoulder and other body segments smoothly connect the feet to the hand and undergo the intermediate rotations, then the rotation loops that each segment undergoes are progressively collapsed as one progresses from the hand along the arm to the shoulder, torso, legs and finally the feet, which represent the collapse of the loop to a point, since they did not rotate. Similarly, for the belt version of the trick, the buckle rotates twice, but the opposite end of the belt remains fixed, and all along the rest of the belt are the progressively collapsing rotation loops from buckle to fixed end.\n\nFigure-eight twirling used in baton twirling, staff twirling in martial arts, and swordsmanship, provides a similar demonstration. Here it is also fairly easy and natural to collapse the motion of the hand progressively down through a wiggle to a stationary position, providing an additional, and perhaps more intuitive demonstration that the double rotation loop can be collapsed to a point.\n\nIn mathematical physics, the trick illustrates the quaternionic mathematics behind the spin of spinors. As with the plate trick, these particles' spins return to their original state only after two full rotations, not after one.\n\nThe same phenomenon can be demonstrated using a leather belt with an ordinary frame buckle, whose prong serves as a pointer. The end opposite the buckle is clamped so it cannot move. The belt is extended without a twist and the buckle is kept horizontal while being turned clockwise one complete turn (360°), as evidenced by watching the prong. The belt will then appear twisted, and no maneuvering of the buckle that keeps it horizontal and pointed in the same direction can undo the twist. Obviously a 360° turn counterclockwise would undo the twist. The surprise element of the trick is that a second 360° turn in the clockwise direction, while apparently making the belt even more twisted, does allow the belt to be returned to its untwisted state by maneuvering the buckle under the clamped end while always keeping the buckle horizontal and pointed in the same direction. \n\nMathematically, the belt serves as a record, as one moves along it, of how the buckle was transformed from its original position, with the belt untwisted, to its final rotated position. The clamped end always represents the null rotation. The trick demonstrates that a path in rotation space (SO3) that produces a 360 degree rotation is not homotopy equivalent to a null rotation, but a path that produces a double rotation (720°) is null equivalent.\n\nA fictional extension of the belt trick appears in Ian McEwan's novel \"Solar\" as a plot device to explain the protagonist's Nobel prize work.\n\n\n\n"}
{"id": "382993", "url": "https://en.wikipedia.org/wiki?curid=382993", "title": "Project Alpha", "text": "Project Alpha\n\nProject Alpha was an elaborate hoax that began in 1979 and ended with its deliberate disclosure in 1983. It was orchestrated by the stage magician and skeptic James Randi. It involved planting two fake psychics, Steve Shaw (now better known as Banachek) and Michael Edwards, into a parapsychology (PSI) research project at Washington University. Introduced to the researchers during the initial stages of the program, the young men convinced the researchers that their psychic powers were real. With spoon bending demonstrated, the lab ran a long series of experiments to test the range of their abilities, involving everything from moving objects in sealed globes, to changing electronic clocks, to making images appear on unexposed film.\n\nAfter over a year of such tests, the lab began to prepare papers for presentation at a major parapsychology meeting in Syracuse in August 1981. In July 1981, Randi leaked statements about the project at a magician's meeting in Pittsburgh. The August meeting was dominated by discussions of the experiments and Randi's role; opinions were sharply divided about what was actually happening. Randi presented a critique of the lab's videotapes, pointing out obvious fakery. When the team returned to the lab and ran a number of the experiments with tighter controls, all indications of PSI powers disappeared. At this point the lab largely ended their involvement with the two, quoting \"meager results.\" Other researchers were happy to continue working with them, and for the next year they travelled about and were engaged in a wide variety of experiments. Many glowing reports were published in various journals and magazines.\n\nIn early 1983, Randi called a press conference at the offices of \"Discover\" magazine, ostensibly to announce the first example of true psychic abilities. When introducing the two, Randi casually asked how it all worked. Edwards replied \"To be quite honest, we cheat\", resulting in gasps from the assembled reporters. The fallout was immediate; the Washington lab was shut down shortly thereafter. One PSI researcher claimed that \"Randi has set back the field 100 years!\" To which Randi responded that they were the ones who tried to set back the study of parapsychology, but he \"brought it into the 20th century.\" Others came to believe Randi and the lead researcher, Philips, were conspiring to discredit the field, which is widely considered a pseudoscience.\n\nFollowing Project Alpha, Randi went on to use variations of the technique on several other occasions. Perhaps the most famous example led to the downfall of TV evangelist and faith healer Peter Popoff, when Randi had a man pose as a woman with uterine cancer, which Popoff happily \"cured\". In another example, Randi worked with performance artist José Alvarez, who posed as a channeller known as \"Carlos\", who was presented on Australian TV and soon had a wide following. After this hoax was exposed, the artist was constantly approached by people who believed him to be genuine, even if he told them directly that he was an actor. Recently, Project Alpha has become the subject of a movie development.\n\nDuring the 1970s, James S. McDonnell, board chairman of McDonnell Douglas and believer in the paranormal, began to give grants to a number of researchers who were working in the PSI field. Looking for a more substantial effort, he approached his home town's Washington University in St. Louis, Missouri with plans to set up a permanent PSI research facility. At first his overtures were rebuffed, but eventually physicist Peter Phillips, who was also interested in the field, agreed to lead up a parapsychology lab at the school. In 1979, McDonnell arranged a $500,000 USD grant for the establishment and five years operation of the McDonnell Laboratory for Psychical Research.\n\nThe formation of the lab was relatively well reported, and Philips was often on-camera explaining their efforts. He was most interested in spoon bending by children, also known as \"psychokinetic metal bending\", or PKMB. In response to these stories, James Randi wrote to the lab with a list of 11 \"caveats\" they should be wary of, and his suggestions on how to avoid them.\nThese included a rigid adherence to the protocol of the test, so that the subjects would not be allowed to change it in the midst of the run. This had been the modus operandi of Uri Geller while being tested at Stanford Research Institute; whenever something did not work, he simply did something else instead. The researchers then reported this as a success, when in fact the original test had failed. Other suggestions included having only one object of study at any time, permanently marking the object or objects used so they could not be switched, and having as few people in the room as possible to avoid distractions. Randi also offered his services to watch the experiments as a control, noting that a conjurer would be an excellent person to look for fakery. Phillips did not take Randi up on the offer because of the skeptic's reputation of being \"a showman rather than an unprejudiced critic\" and his perceived hostility towards psychic claimants. In his letters, Randi even told the researchers that the subjects were fake, but the researchers did not check out their backgrounds.\n\nThroughout the early phases of the project, many people claiming to have psychic powers presented themselves to the lab. The vast majority quickly proved to have no such ability, or, just as commonly, used sleight of hand to make their \"abilities\" work. Many of these were convinced what they were doing was \"real\". However, after a short while it became apparent that two young men, Steve Shaw and Michael Edwards, were much more successful, and the lab started to focus their energies on them. In fact, the two young men were \"plants\", friends of Randi whom he had met some time before as part of his magician's trade. Part of Randi's instructions to these men was to tell the truth if they were ever asked whether they were faking the results; they were never asked this question directly. The researchers assumed that the participants would have no qualms about lying in their answer to a straightforward question if they were also lying about their abilities.\n\nShaw and Edwards (aged 18 and 17, respectively, at the time Alpha started) were skilled amateur magicians who managed to fool the researchers with fairly simple tricks during the first stages of investigations. The project had originally started with spoon bending, so the two quickly developed a way to accomplish this. Contrary to one of the caveats Randi noted in his letter, the test setup included not one, but many and all sorts of spoons on the table, labeled with paper on a loop of string instead of some permanent marking. When starting to bend a spoon, they would actually pick up two or more and remove the labels, which they were allowed to do, because they claimed they were in the way. They would then simply switch the labels when putting them back and wait. The spoons were measured before and after the experiment, and since all sorts of spoons were used, simply switching the labels would produce different measurements, causing the scientist to believe that something paranormal had occurred. In other cases, they would drop one of the spoons in their lap and bend it below the table with one hand, while pretending to bend a spoon in their other hand, distracting the scientists.\n\nBecause the studio was set up to allow people in front of the camera to see themselves on monitors, and the videotapes were available to be watched by anyone, the two used the video to critique their own performance. They would deliberately fail on their first attempt at any given demonstration, and then use the video to find out what was visible to the researchers and what was not. They would then develop a technique that would not appear on video. Edwards found that one particular camera operator was on guard to capture any attempts at sleight of hand, so he picked the man to assist him in one experiment, and he was replaced by a less competent cameraman. This was also a clear violation of one of Randi's caveats; the test run should have been stopped at this point and recorded as a failure.\n\nThe two were so successful at spoon bending that several other tests were invented. In one they were given pictures in sealed envelopes and then asked to try to identify them from a list shown to them later. The two were left alone in a room with the envelopes, and although there was a possibility that they would peek, this was supposed to be controlled by examining the envelopes later. The envelopes were held closed with four staples, which they simply pried open with their fingernails, looked at the picture, and then resealed by inserting the staples back into the same holes and forcing them closed by pressing them against the table.\n\nAnother test was electronic; they were asked to influence the burnout point of a common fuse. After they were given a chance to work it with their mind, an increasing amount of current was run through the fuse until it blew. The two proved to have amazing abilities in this test after a few trials, eventually causing the fuses to blow immediately once they got used to it. In fact, they were simply palming the already blown fuses and then handing them back to the experimenters. They also found that pressing down on one end of the fuse in its holder, or just touching it briefly, caused the instruments to record unusual results that were interpreted by the experimenters as psi effects.\n\nIn one instance, Shaw and Edwards were asked to move small objects in a sealed transparent globe, normally small bits of paper balanced on an edge. At first they were unable to get anything to happen, but later noticed that the container was being removed to replace the object within. During one such event they took the opportunity to roll up a small ball of metal foil and drop it into the circular ring cut into the surface of the table that held the globe. This introduced a small gap under one edge, which they could blow into to cause the paper to move.\n\nOther examples included their ability to make digital clocks stop working properly (Edwards put it in a microwave oven for a few seconds), or make images appear on film just by staring at the camera (Shaw spat on the lens). In an example of a sort of Rorschach test, the experimenters later described the images as being various bits of the female anatomy, which, \"of course\", young men (such as Shaw and Edwards) would be interested in.\n\nIn one particular experiment, Shaw tried to get them to say on film that he was not allowed to touch the object he was supposed to bend in the experiment (he had already secretly done that, and with them affirming that he could not and had not touched it, the \"miracle\" would look even greater). When Shaw asked: \"Can I touch it?\", the reply was, to his surprise: \"You do what you have to do.\"\n\nThe researchers explain these apparently inexcusable inadequacies in experimental protocols by drawing a clear distinction between two different stages of an investigation: the exploratory, informal experiments and the formal experiments. During the exploratory phase, the researchers would simply be trying to determine whether there is a phenomenon that could be worth further investigation, which often implies the use of much more complicated protocols and expensive equipment. In doing so, they are also trying to set up a comfortable, relaxed atmosphere that is believed to be conducive to psychic phenomena. It is during this stage that Shaw and Edwards were able to convince the researchers of psychic abilities\n\nIn mid-1981, the two were fairly famous in the psi world, and even outside it, and Phillips decided to release a research brief at a workshop of the Parapsychological Association Convention (August 1981). According to the researchers' official version, Phillips also wrote to Randi to ask for a tape of fake metal-bending, which was to be shown alongside the recording of Shaw and Edwards. The researchers were looking for opinions and critical input from the parapsychology community and finally released a revised abstract that reflected the received criticism in its conservative and skeptical language. After the announcements in the press, Randi wrote to the lab again and stated that it was entirely possible the two were magicians, using common sleight of hand to fool the researchers.\n\nRandi started to leak stories that the two were a plant of his. These stories reached the lab, but they were not taken seriously on the grounds that (1) it seemed hard to believe that a plot could have been carried out so consistently by Randi and the two young magicians for 21 months, and (2) there was not precedent known of such a plot ever being carried out by a skeptic. The story had been widely circulated by the time the meeting was held the next month. Reactions were varied; some thought it was simply a lie, others that Randi was pulling off a hoax, and still others concluded the entire experiment was dreamed up as a conspiracy by Randi and Phillips to discredit the field.\n\nUpon returning from the meeting, Phillips immediately changed the test protocols. The two found that they were no longer able to fool the experimenters so easily, and in most cases, not at all. During this time the lab started releasing additional reports that seriously toned down the success rate. In their own words, \"We did not conclude that they must be frauds, but only that after extensive testing, they were not behaving nearly as psychically as they had led us to expect.\" This improvement of protocols later led James Randi to list a \"straight spoon\" award to Phillips in a press release of his \"bent spoon\" awards, but that award was omitted from the list of awards published by \"Omni\" magazine and the \"Skeptical Inquirer\"; it was reported in the latter in a subsequent letter to the editor.\n\nAt this point Shaw and Edwards were so famous that they were asked to travel widely and present their powers. Many other psi investigators interviewed the two and gave glowing reviews, thus tainting themselves in the eventual aftermath.\n\nRandi decided to end the project and announced the entire affair in \"Discover\" magazine. Many of the researchers who endorsed Shaw and Edwards after the August meeting were now burned in the process. One went so far as to claim that the young men really did have psychic powers, and that they were now lying about being magicians. The bad press was so widespread that the McDonnell Lab was shut down.\n\nThe \"Skeptical Inquirer\" revealed that Shaw was a fake psychic in their fall 1980 issue. At the time neither the authors (McBurney and Greenberg) nor the editor (Kendrick Frazier) knew that Shaw was part of Project Alpha. Shaw had, in fact, posed as a fake psychic prior to Project Alpha, and his high-school paper ran a story about his powers. In addition, rumors that the psychics were fake reached the researchers, but they didn't believe them. According to the researchers,\n\nThe rumor seemed unlikely to be true for several reasons: the two young men came from different states and had never met before being brought to the MacLab; if they were both conspiring with Randi, then the plot had gone on for 21 months. What critic would be so persistent in engaging in fraud and conspiracy on such a time-scale? There seemed to be no precedent. Nor was it possible to track down how reliable the rumor might be.\n\nThe complaint of psi investigators for years had been that they did not have enough funding for their experiments. However, in Randi's opinion it was not funding but the experimenters that were the problem. With $500,000 from McDonnell, Randi felt that lack of funding could no longer be blamed for any failure. Randi's purpose was to show that no matter how much money was spent, there would still be no reliable results.\n\nSome within the parapsychology community were outraged, with Berthold Schwarz declaring: \"Randi has set parapsychology back 100 years!\" Randi's approach also raised outcries concerning ethical considerations and doubts about positive effects on methodology awareness, both within the parapsychology and the skeptic communities. But Randi reports that other parapsychology researchers have contacted him with praise, describing the project as \"splendid and deserved\", \"an important sanitary service\", \"commendable\", and \"long-needed\".\n\nEven decades later, Project Alpha remains one of the talking points in the PSI field. A 2011 biography of Michael Thalbourne, one of the Mac-Lab experimenters, dismisses the entire affair thus:\n\nJames Randi, who sought to discredit Mac-Lab staff just to prove some vague point. He had two of his poorly trained magicians infiltrate the lab so they could indulge in deceptive practices that included cheating and all manner of fraud. Needless to say, this reprehensible behaviour was detected, and no published works ever emerged from Randi’s farcical escapade.\n\n\n\n\n\n\n"}
{"id": "47633979", "url": "https://en.wikipedia.org/wiki?curid=47633979", "title": "Proposed SLS and Orion missions", "text": "Proposed SLS and Orion missions\n\nNASA's new Space Launch System (SLS) and Orion programs will allow for beyond low-Earth orbit spaceflight. There are a number of notional, proposed missions for the programs, none of which are confirmed. Some of these mission concepts include:\n\nThe Exploration Mission 1 will be the first mission of NASA's Orion on the Space Launch System, planned for June 2020. It will spend approximately 3 weeks in space, including 6 days in a retrograde orbit around the Moon. It is planned to be followed by Exploration Mission 2 in 2023.\n\nThe Exploration Mission 2 is scheduled to be the first crewed mission of NASA's Orion on the SLS, which will transport the power and propulsion module for the Lunar Orbital Platform-Gateway in lunar orbit. Exploration Missions 2 through 8 would complete the assembly of the Gateway Space Station.\n\nThis is a crewed flexible path mission to one of the Martian moons, Phobos or Deimos. It would include 40 days in the vicinity of Mars and a return Venus flyby.\n\nThis would be a crewed mission, with four to six astronauts, to a semi-permanent habitat for at least 540 days on the surface of Mars in 2033 or 2045. The mission would include in-orbit assembly, with the launch of seven SLS Block 2 heavy-lift vehicles (HLVs). The seven HLV payloads, three of which would contain nuclear propulsion modules, would be assembled in LEO into three separate vehicles for the journey to Mars; one cargo In-Situ Resource Utilization Mars Lander Vehicle (MLV) created from two HLV payloads, one Habitat MLV created from two HLV payloads and a crewed Mars Transfer Vehicle (MTV), known as \"Copernicus\", assembled from three HLV payloads launched a number of months later. Nuclear Thermal Rocket engines such as the Pewee of Project Rover were selected in the Mars Design Reference Architecture (DRA) study as they met mission requirements being the preferred propulsion option because it uses proven technology, has higher performance, lower launch mass, creates a versatile vehicle design, offers simple assembly, and has growth potential. A nuclear reactor serving as the power source for Ion propulsion was also an alternative option, in the case NTRs were not available.\n\nThis is a crewed flight with a telerobotic Mars sample return mission proposed by NASA's Mars Program Planning Group. The time frame suggests SLS-5, a 105 t rocket to deliver an Orion capsule, a solar electric propulsion (SEP) robotic rover, and a Mars Ascent Vehicle (MAV). \"Sample canister could be captured, inspected, encased and retrieved tele-robotically. Robot brings sample back and rendezvous with a crew vehicle.\" The mission may also include a Mars orbiter with solar electric propulsion.\n\nConcepts for sample return missions to Europa and Enceladus are also being studied.\n\nSkylab II, proposal by Brand Griffin, an engineer with Gray Research Inc working with NASA's Marshall Space Flight Center in Huntsville, Alabama, to use the upper stage hydrogen tank from SLS to build a 21st-century version of Skylab for future NASA missions to asteroids, Earth-Moon Lagrangian point-2 (EML2) and Mars.\n\nFive near Earth object (NEO) mission concepts have been suggested, ranging from minimum to full capabilities. Among these are two near Earth object missions suggested for in 2026. A 155-day mission to NEO 1999 AO10, a 304-day mission to NEO 2001 GP2, a 490-day mission to a potentially hazardous asteroid such as 2000 SG344, utilizing two Block 1B SLS vehicles, and a Boeing-proposed NEO mission to NEA 2008 EV5 in 2024. The latter would start from the proposed Earth-Moon L2 based Exploration Gateway Platform. Utilising an SLS third stage the trip would take about 100 days to arrive at the asteroid, 30 days for exploration, and a 235-day return trip to Earth.\n\nThis is a crewed Venus mission concept using two SLS Block 1B launches to send a crew of 2 to explore the atmosphere of Venus for 1 month, with retrieval in Earth orbit by a commercial crew vehicle or an Orion.\n\nThe SLS has been proposed as the launch vehicle for the future Large UV Optical Infrared Surveyor (LUVOIR) space telescope, which will have a main segmented mirror between 8 and 16 meters in diameter, making it 300 times more powerful than Hubble Space Telescope. The two competing concepts to become LUVOIR are ATLAST and HDST. It would be deployed at the Earth-Sun L2 point in 2035.\n\nSLS has been proposed as the launch vehicle for the planned \"Europa Clipper\", an orbiter to Europa to study its atmosphere, magnetic and thermal characteristics, gravitational harmonics, sample any water plumes, etc. Its launch is proposed for the mid-2020s.\n\nIf funded, the launch of a Europa Lander would require the SLS system, and it would be launched after \"Europa Clipper\" in the mid-2020s to complement its studies.\n\nSLS has been proposed by Boeing as a launch vehicle for a Uranian probe. The rocket would \"Deliver a small payload into orbit around Uranus and a shallow probe into the planet’s atmosphere.\" The mission would study the Uranian atmosphere, magnetic and thermal characteristics, gravitational harmonics, as well as do flybys of Uranian moons.\n\nSLS has been proposed as a launch vehicle for a probe to Saturn and its moons.\n\nThe 2017 Pre-Decadal Survey on missions to the ice giants, states that a single SLS launch vehicle could launch two spacecraft, one to each ice giant. Launch dates are suggested from 2024 to 2037, and would take about 4 years cruising time for a SLS-1B.\n\nDeep Space Habitat are different commissioned concepts developed for NASA that were submitted by Bigelow Aerospace , Boeing, Lockheed Martin, Orbital ATK, Sierra Nevada Corporation, Space Systems Loral, and NanoRacks to build a large enough living space for humans to travel to destinations such as Mars, near Earth asteroids, or cislunar space. The living space would need to be large, so the SLS is a proposed vehicle to launch it.\n"}
{"id": "47995848", "url": "https://en.wikipedia.org/wiki?curid=47995848", "title": "Psyche (spacecraft)", "text": "Psyche (spacecraft)\n\nPsyche is a planned orbiter mission that will explore the origin of planetary cores by studying the metallic asteroid . Lindy Elkins-Tanton of Arizona State University is the Principal Investigator who proposed this mission for NASA's Discovery Program. NASA's Jet Propulsion Laboratory (JPL) will manage the project.\n\n16 Psyche is the heaviest known M-type asteroid, and is thought to be the exposed iron core of a protoplanet. This asteroid may be the remnant of a violent collision with another object that stripped off the outer crust. Radar observations of the asteroid from Earth indicate an iron–nickel composition. On January 4, 2017, the \"Psyche\" mission was chosen along with the \"Lucy\" mission as NASA's next Discovery-class missions.\n\n\"Psyche\" was submitted as part of a call for proposals for NASA's Discovery Program that closed in February 2015. It was shortlisted on September 30, 2015, as one of five finalists and awarded $3 million for further concept development. One aspect of selection was enduring the \"site visit\" in which about 30 NASA personnel come and interview, inspect, and question the proposers and their plan.\n\nOn January 4, 2017, it was selected along with \"Lucy\" as one of two winners of this round of Discovery mission selection, with launch set for 2023 as the 14th Discovery mission. In May 2017 the launch date was moved up to target a more efficient trajectory, launching in 2022 and arriving in 2026 with a Mars gravity assist in 2023.\n\nThe \"Psyche\" spacecraft will use solar electric propulsion, and the scientific payload will be an imager, a magnetometer, and a gamma-ray spectrometer.\n\nData shows asteroid to have a diameter of about . Scientists think that could be the exposed core of an early planet that could have been as large as Mars and lost its surface in a series of violent collisions.\n\nThe mission will launch in 2022 and arrive in four years to perform 21 months of science. The spacecraft will be built by NASA JPL in collaboration with SSL (formerly Space Systems/Loral) and Arizona State University.\n\nDifferentiation was a fundamental process in shaping many asteroids and all terrestrial planets, and direct exploration of a core could greatly enhance understanding of this process. The \"Psyche\" mission will characterize geology, shape, elemental composition, magnetic field, and mass distribution. It is expected that this mission will increase the understanding of planetary formation and interiors.\n\nSpecifically, the science goals for the mission are:\n\nThe science objectives are:\n\nThe science questions this mission will address are:\n\nThe \"Psyche\" spacecraft would orbit at decreasing altitudes. Its first regime, , will see the spacecraft enter a orbit for magnetic field characterization and preliminary mapping for a duration of 56 days. It will then descend to , set at altitude for 76 days, for topography and magnetic field characterization. It will then descend to , at altitude for 100 days to perform gravity investigations and continue magnetic field observations. Finally, the orbiter will enter , set at to determine the chemical composition of the surface using its gamma-ray and neutron spectrometers. It will also acquire continued imaging, gravity, and magnetic field mapping. The mission is planned to orbit the asteroid for at least 21 months.\n\n\"Psyche\" will fly a payload of , consisting on four scientific instruments.\n\nThe spacecraft will also test an experimental laser communication technology called Deep Space Optical Communications. It is hoped that the device will be able to increase spacecraft communications performance and efficiency by 10 to 100 times over conventional means. The laser beams from the spacecraft will be received by a ground telescope at Palomar Observatory in California.\n\nThis mission will use the model SPT-140 engine, a Hall-effect thruster utilizing solar electric propulsion, where electricity generated from solar panels is transmitted to an electric, rather than chemically powered, rocket engine. The thruster is nominally rated at 4.5 kW operating power, but it will also operate for long durations at about 900 watts.\n\nThe SPT-140 (SPT stands for \"Stationary Plasma Thruster\") is a production line commercial propulsion system that was invented in Russia by OKB Fakel and developed by NASA's Glenn Research Center, Space Systems Loral, and Pratt & Whitney since the late 1980s. The SPT-140 thruster was first tested in US as a 3.5 kW unit in 2002 as part of the Air Force Integrated High Payoff Rocket Propulsion Technology program. Using solar electric thrusters will allow the spacecraft to arrive at (at 3.3 astronomical units) much faster while consuming only 10% of the propellant it would need using conventional chemical propulsion.\n\nElectricity will be generated by bilateral solar panels in an x-shaped configuration, with 5 panels on each side. Prior to the mission being moved forward with a new trajectory, the panels were to be arranged in straight lines, with only 4 panels on each side of the spacecraft. \n\n"}
{"id": "5882940", "url": "https://en.wikipedia.org/wiki?curid=5882940", "title": "Saturn INT-20", "text": "Saturn INT-20\n\nThe Saturn INT-20 was a proposed intermediate-payload follow-on from the Apollo Saturn V launch vehicle. A conical-form interstage would be fitted on top of the S-IC stage to support the S-IVB stage, so it could be considered either a retrofitted Saturn IB with a more powerful first stage, or a stubby, cut-down Saturn V without the S-II second stage.\n\nThree variants were studied, one with three F-1 engines in the first stage, one with four, and one with five.\n\nWithout the S-II stage, which made up a large fraction of the mass of the Saturn V, a version of the INT-20 using an unmodified five-engine version of the S-IC booster would be greatly overpowered and accelerate substantially faster than the Saturn V. This would create excessive aerodynamic stress in the low atmosphere. Several solutions to this problem were considered.\n\nUsing the original five-engine S-IC would require three engines to be shut down 88 seconds after launch, with the remainder of the first-stage flight flown on only two engines. This meant that a considerable amount of the firing time would be carrying three engines of \"dead weight\". As a consequence the extra payload over a four-engine variant would only have been about one thousand pounds, and the extra cost and complexity of the fifth engine was unjustified.\n\nA four-engine variant would launch with four engines firing and shut down two engines 146 seconds after launch. The remaining two engines would burn until first-stage shutdown 212 seconds after launch. This variant could put approximately 132,000 pounds into a 100 nautical mile (185 km or 115 statute mile) orbit, versus around 250,000 pounds for the three-stage Saturn V.\n\nThe three-engine variant would burn all three engines up to first-stage shutdown at 146 seconds after launch. This variant could put approximately 78,000 pounds of payload into a 100 nautical mile (185 km) orbit, around twice the useful payload of the Saturn IB.\n\nBoth three- and four-engine variants would therefore have provided useful payload capacities (Saturn C-3) intermediate between the Saturn IB and Saturn V, and re-using Saturn V components would reduce costs and simplify ground operations compared to building an entirely new launcher in that payload range.\n\n\n"}
{"id": "53942957", "url": "https://en.wikipedia.org/wiki?curid=53942957", "title": "Smash and Grab (biology)", "text": "Smash and Grab (biology)\n\nSmash and Grab is the name given to a technique developed by Charles S. Hoffman and Fred Winston used in molecular biology to rescue plasmids from yeast transformants into \"Escherichia coli\", also known as E. coli, in order to amplify and purify them. In addition, it can be used to prepare yeast genomic DNA (and DNA from tissue samples) for Southern blot analyses or polymerase chain reaction (PCR).\n"}
{"id": "57517295", "url": "https://en.wikipedia.org/wiki?curid=57517295", "title": "Society for the Scientific Study of Psychopathy", "text": "Society for the Scientific Study of Psychopathy\n\nThe Society for the Scientific Study of Psychopathy (abbreviated SSSP) is an international learned society dedicated to promoting and advancing scientific research on the personality disorder psychopathy. It was established in 2005 and held its first meeting that year in Vancouver, British Columbia, Canada. Since then, the society has held biennial conferences to provide a venue for psychopathy researchers to present their most recent findings. As of 2009, the society had over 160 members, most of whom were from the United States.\nThe current president of the SSSP is Adelle Forth. Previous presidents include:\n"}
{"id": "5848040", "url": "https://en.wikipedia.org/wiki?curid=5848040", "title": "Stone &amp; Webster", "text": "Stone &amp; Webster\n\nStone & Webster was an American engineering services company based in Stoughton, Massachusetts. It was founded as an electrical testing lab and consulting firm by electrical engineers Charles Stone and Edwin S. Webster in 1889. In the early 20th century, Stone & Webster was known for operating streetcar systems in many cities across the United States; examples include Dallas, Houston and Seattle. \nThe company grew to provide engineering, construction, environmental, and plant operation and maintenance services, and it has long been involved in power generation projects, starting with hydroelectric plants of the late 19th-century; and with most American nuclear power plants.\n\nStone & Webster was acquired and integrated as a division of The Shaw Group in 2000, and in 2012, the French engineering conglomerate Technip acquired Stone & Webster's energy and chemical business, and process technologies and associated oil and gas engineering capabilities from The Shaw Group. The CB&I acquisition of other assets of The Shaw Group, also in 2012, resulted in the formation of a nuclear power subsidiary, CB&I Stone Webster, which operated for about 4 years, being sold in January 2016 to Westinghouse Electric Company.\n\nCharles A. Stone and Edwin S. Webster first met in 1884 and became close friends while studying electrical engineering at the Massachusetts Institute of Technology. In 1890, only two years after graduating, they formed the Massachusetts Electrical Engineering Company. The name was changed to Stone & Webster in 1893. Their company was one of the earliest electrical engineering consulting firms in the United States.\n\nStone & Webster's first major project was the construction of a hydroelectric plant for the New England paper company in 1890. Stone & Webster not only had valuable insight into developing and managing utilities but they also had keen intuition for businesses to invest in. Through the panic of 1893, Stone & Webster were able to acquire the Nashville Electric Light and Power Co. for a few thousand dollars and later sold it for $500,000.\n\nThroughout the next ten years, Stone & Webster acquired interest in large number of utilities while offering managerial, engineering and financial consulting to a number of independent utility firms. Even though Stone & Webster were not a holding company, their financial and managerial presence meant that they had considerable influence in policy decisions. They would often be paid in utility stock.\n\nStone & Webster became involved in Washington State engineering projects—Washington's natural resources, and hydroelectric power, and resulting development opportunities brought companies like Stone & Webster to the state—beginning with Puget Sound area street railways. By 1900, they had controlled and merged eight small rail lines in Seattle; soon after, they also took over the street railway systems of Tacoma and Everett. By 1908, Stone & Webster listed thirty-one railway and lighting companies under its management including five located in Washington State: the Puget Sound Electric Railway, Puget Sound International Railway and Power Co., Puget Sound Power Co., The Seattle Electric Co., and Whatcom County Railway and Light Co. Stone & Webster leadership was sensitive to the concerns of large utility holding companies and were careful to emphasize the complete independence of these utilities, but Edwin Webster believed that outside capital was crucial to develop the resources of Washington, and chided those who thought otherwise. In 1905, Stone & Webster bought out the power and lighting properties that were once owned by the Bellingham Bay Improvement Co., including the York Street Steam plant and the partially built Nooksack Falls Hydroelectric Power Plant. Stone & Webster took over construction operations and on September 21, 1906, Bellingham received power from the plant via a transmission line. Despite the independence allowed its subsidiaries, J.D. Ross, superintendent of Seattle City Light issued a report critical of Stone & Webster's presence in Seattle. listing 49 companies under Stone & Webster's management at the time.\n\nBy 1912, the company, nationally, had divided itself into three specialized subsidiaries:\n\nIn 1927, Stone & Webster expanded the investments business, merging its securities subsidiaries with the investment banking firm of Blodgett & Co. founded in 1886, to form Stone & Webster and Blodgett Inc. In January, 1946, the name of the business, was changed to Stone and Webster Securities Corporation. Stone and Webster Securities was one of the 17 U.S. investment banking and securities firms named in the United States Department of Justice's antitrust investigation of Wall Street commonly known as the Investment Bankers Case. The Stone & Webster investment banking operations were eventually acquired by Kidder Peabody which already had overlapping ownership.\n\nStone & Webster, Inc., has since offered its customers in the United States and the world engineering, design, construction, consulting, and environmental services to build electric power plants, petrochemical plants and refineries, factories, infrastructure, and civil works projects. Stone & Webster helped build substantial portions of the nation's power production infrastructure, including coal, oil, natural gas, nuclear, and hydroelectric plants constituting around 20 percent of U.S. generating capacity. The company played a significant role in the nation's defense efforts during World War I and II and afterwards, helping develop the A-Bomb, constructing large shipyards, and creating alternate means of production of strategic materials such as synthetic rubber. Much of the world's capacity in petrochemical and plastics development was also developed as a result of Stone & Webster efforts.\n\nAmerica's entry into World War II brought a dramatic increase in demand for all types of engineering and construction, and Stone & Webster became intensely involved in the war effort. According to former Stone & Webster president Allen, \"Few elements of war production were not impacted in a significant way by Stone & Webster.\" Typical Stone & Webster wartime assignments included the design and construction of cartridge case plants, a complete steel foundry, a plant to produce bombsights and other equipment, a plant furnishing fire-control instruments, a facility producing aircraft superchargers, and three TNT-production plants, in addition to meeting demands for infrastructure and power facilities. The company was also called upon to engage in more creative projects.\n\nPerhaps the most creative Stone & Webster wartime effort was its involvement in the Manhattan Project, which designed and built the atomic bomb. Stone & Webster was involved in creating the facilities and laboratories for the Manhattan Project. Prior to its acquisition it was also part of the Maine Yankee decommissioning project. The company was selected in June 1942 by the first MED District Engineer, Colonel James C. Marshall, as the main subcontractor for the project. Eventually the company established a completely separate engineering organization employing 800 engineers and draftsmen to study ways to separate large quantities of fissionable uranium-235. At Oak Ridge, Tennessee, Stone & Webster constructed a new city that ultimately housed 75,000 workers and the Y-12 electromagnetic separation plant.\n\nImmediately after the end of the war, demand for Stone & Webster services rose rapidly among U.S. public utilities. Under the leadership of Texan George Clifford, the company began to build interstate gas pipelines and compressor stations and also became the largest single stockholder in the Tennessee Gas Transmission Company (Tenneco). The company was also retained on tasks that helped shift the nation's economy from a defense to a civilian basis, such as estimating the costs of deactivation and stand-by maintenance of defense plants and shipyards, providing technical advice and services on Japanese reparations, evaluating the mobile equipment that remained in overseas theaters, and continuing work at Oak Ridge.\n\nStone & Webster was perhaps the most significant engineering company to be involved in the nation's developing nuclear power industry. Chosen after a competition with 90 other companies to build the nation's first nuclear power plant in Shippingport, Pennsylvania, Stone & Webster was subsequently selected to design and supervise the construction of a large accelerator at the Brookhaven National Laboratory, design the neutron shield tank for the nuclear-powered merchant ship N.S. Savannah, and engineer and construct a prototype atomic energy power plant for the U.S. Army.\n\nThe steady demand for electric power generation also meant an increase in construction contracts for more conventional power plants. By the early 1950s, Stone & Webster had built 27 separate hydroelectric plants constituting 5 percent of U.S. capacity, steam power plants aggregating six million kilowatts in capacity, and some 6,000 miles of power transmission lines.\nDuring this time, the company also obtained a variety of chemical process contracts in the United States, Canada, Japan and other countries to meet the worldwide demand for plastics.\n\nAs the 1960s drew on, however, Stone & Webster's petrochemical and plastics activity began to slow as U.S. refinery capacity caught up with customer demand and declined accordingly. To smooth the impact of these fluctuations, the company diversified its process interests, developing, for example, a more extensive relationship with the paper industry. During the decade, the company designed the first commercial mill to make pulp from hardwood trees.\nSlowing business activity also resulted in some conceptual restructuring within the company, including an effort to standardize designs in areas of proven success and placing a greater emphasis on the use of project work teams that combined staff with differing specialized skills. The increased emphasis on teaming fit well with Stone & Webster's need to address problems that developed in the energy supply sector in the mid- to late 1960s and was used in the design of synthetic natural gas plants, a liquified natural gas distribution center, and demonstration projects in coal and oil gasification.\n\nDuring the 1970s, major world events—including the OPEC (Organization of Petroleum Exporting Countries) oil embargoes of 1973 and 1979, uncertainty in the chemical process industry with respect to feedstock supplies, increasing opposition to the use of nuclear power, and a growing public awareness of environmental issues—brought difficulties as well as new business opportunities for Stone & Webster.\n\nThe high prices that followed the embargoes, for example, constrained energy demand and thus reduced the need for new electric generating capacity. Utilities looked into every possible alternative to meet demand, short of constructing major new baseload stations, resulting in \"one of the severest drop-offs in building in the history of the engineering-construction industry,\" according to former Stone & Webster president William Allen in \"Public Utilities Fortnightly\" (July 20, 1989). An equally severe, simultaneous downturn in international construction compounded the problem.\n\nThe investment banking affiliate, Stone & Webster Securities, had attempted to grow by acquiring two smaller, regional brokerage houses in 1968: Hayden, Miller & Co., based in Cleveland, and Atlanta-based Wyatt, Neal & Waggoner. That increased the number of offices of the firm from nine to 28, but cultural and style differences between the parent company's traditional engineering management and retail brokerage management led to an exodus of key employees, and the Securities firm closed its doors in 1974.\n\nStone & Webster's difficulties with constructing conventional power plants were matched by its problems in nuclear construction. By the late 1970s, the company had attained a central role in the nuclear power industry—a significant portion of all nuclear energy in the United States was being generated at plants designed and generated by Stone & Webster. In 1975, the company had been selected to construct the Clinch River Breeder Reactor. However, increasing public opposition to the construction of nuclear plants, lengthy delays brought by challenges before public utility commissions, and corresponding increases in plant construction costs, capped by the incident at Three Mile Island in 1979, brought about a moratorium on the construction of large nuclear plants and the cancellation of many existing orders.\n\nThe company began to respond to these challenges during the remainder of the 1970s and into the early 1980s. Stone & Webster met its clients' reluctance to build by improving engineering and construction efficiencies through the use of computer-assisted design and innovative working agreements with contractors and the building trades unions, as well as by providing services that kept plants operating safely, efficiently, and for a longer time than originally intended.\nTo further survive in this complex business environment, Stone & Webster began to more intensely solicit government and international business, increase its activity in the area of environmental protection and alternative energy production, continue its activity in extending the lives of existing power plants, and develop other areas of diversification as long as they did not distract from the company's core business—engineering. Stone & Webster also began to phase out those parts of the company that were unrelated to its core activities and no longer considered financially viable, such as its securities subsidiary.\n\nIn the 1990s, Stone & Webster faced a business environment in which its core activities of power plant and petrochemical plant construction were lagging, and new areas targeted for growth had not yet fulfilled their potential. As a result, company stock performance was sluggish, and in 1992 a stockholder group headed by corporate gadfly Bob Monks attacked Stone & Webster management, asserting that the company had not exploited its assets to keep its stock price high and inquiring as to growth plans the company intended to institute in order to raise stock value.\n\nIn 1994, the company registered a net loss of $7.8 million despite revenues of over $818 million. Recognizing that a need existed to improve its financial picture, Stone & Webster opted for a further change in its traditional marketing strategy. The company centered its hopes for future growth on a broader expansion of its core businesses into global markets, a cutback in its dependence on power generation, and the expansion of its environmental and transportation efforts.\n\nStone & Webster's infrastructure and transportation activities during this time included the engineering and design of railway and other large transit systems, including part of the Washington, D.C. metro; major airport improvements in Denver and Miami; bridge construction, such as the eight mile-long bridge linking Prince Edward Island to the Canadian mainland; and roadway upgrading, including work on the New Jersey Turnpike. Moreover, the company's advanced computer applications efforts included the use of three-dimensional models; expert systems which monitored, diagnosed, and recommended solutions in areas from equipment vibration to chemical plant processing; and advanced controls that continuously monitored all plant operations.\n\nDespite its challenges, Stone & Webster appeared to retain considerable strengths on which to draw. Once the impact of strategies responsive to the business environment of the 1990s had been put in place, company officials and outside observers seemed reasonably optimistic about the company's prospects as it moved on into its second century.\n\nThere was little sign that anything was wrong at the company in the late 1990s. Stone & Webster announced it was selling some buildings in Boston in 1997 but claimed that this was only because the company had an excess of real estate. Stone & Webster also moved its corporate headquarters out of New York City and back to Boston, and sold or subleased office space in New York and New Jersey. Whatever was actually the case in 1997, by 1999 Stone & Webster had serious cash flow problems. Company executives later charged that chairman Smith had consistently underbid on projects in order to win business, putting Stone & Webster on shaky ground.\n\nThe company collapsed in 2000 after a major bribery scandal. It had attempted to pay $147 million to a relative of Indonesian President Suharto to secure the largest contract in Stone & Webster's history. But the plan went bad, and the company fell along with it.\n\nSubsequently, Stone & Webster filed for Chapter 11 bankruptcy protection in 2000 because of cash flow problems. It was bought at auction by the Shaw Group for US$150 million.\n\nThe Shaw Energy and Chemicals division integrated Stone & Webster branded technology. Shaw's E&C division attempted to compete with other more successful engineering contractors such as Bechtel, Foster Wheeler, Jacobs and Technip. Since the Shaw buyout, the Power group performed record business in engineering and construction of coal-fired power plants and power plant environmental control retrofits including FGD and SCR technology. Shaw's alliance with Westinghouse led to substantial Stone & Webster technology and engineering applications in the nuclear power industry. In 2002, it won a contract for managing a construction project for gas works for the Abu Dhabi Marine Operating Co. in the United Arab Emirates, and likewise in the early 2000s began power and other projects in the United States, Turkey, countries of the Middle East, and China.\n\nThe company finally emerged from bankruptcy in late 2003, and under the Shaw Group, Stone & Webster was a subsidiary of a global leader with revenue of over $3.3 billion. The Stone & Webster subsidiary retained 5,000 employees, with significant global operations associated with construction and engineering projects, hazardous waste management, and environmental services. In 2008, \"ENR\" ranked the Stone & Webster subsidiary of The Shaw Group subsidiary as first in revenue for Power EPC, and fifth by Revenue in Process & Petrochemical EPC.\n\nIn 2012, Technip, a French engineering conglomerate, agreed to purchase most of the Energy and Chemical Division of Shaw Group Technip announced on August 31, 2012, the completion of the acquisition of Stone & Webster process technologies and associated oil and gas engineering capabilities, as a new business unit of Technip. Technip paid cash consideration of about €225 million from existing cash resources, which was subject to customary price adjustments. This transaction was originally announced on May 21, 2012. It allowed Technip to:\n\nThe remainder of The Shaw Group assets were ultimately purchased by Chicago Bridge & Iron Company, for about US$3 billion, completing the acquisition in February 2013. A subsidiary that was formed as a result, CB&I Stone Webster—a result of Shaw Groups earlier acquisition of Stone & Webster during its bankruptcy—was again sold, in January 2016, to Westinghouse Electric Co., for US$ 229M.\n\n\n"}
{"id": "24084168", "url": "https://en.wikipedia.org/wiki?curid=24084168", "title": "Timeline of physical chemistry", "text": "Timeline of physical chemistry\n\nThe timeline of physical chemistry lists the sequence of physical chemistry theories and discoveries in chronological order.\n\n\n"}
