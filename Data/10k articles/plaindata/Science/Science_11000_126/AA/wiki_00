{"id": "11540757", "url": "https://en.wikipedia.org/wiki?curid=11540757", "title": "ACCRA Cost of Living Index", "text": "ACCRA Cost of Living Index\n\nThe Cost of Living Index (COLI), formerly the \"ACCRA Cost of Living Index\" is a measure of living cost differences among urban areas in the United States compiled by the Council for Community and Economic Research. First published in 1968, the index compares the price of goods and services among metro areas across the US. The index is widely used by economists, researchers and corporations to measure relative cost of living.\n\n"}
{"id": "37076302", "url": "https://en.wikipedia.org/wiki?curid=37076302", "title": "Against Sadomasochism", "text": "Against Sadomasochism\n\nAgainst Sadomasochism: A Radical Feminist Analysis is a 1982 radical feminist anthology edited by Robin Ruth Linden, Darlene R. Pagano, Diana E. H. Russell, and Susan Leigh Star. The authors critique sadomasochism and BDSM from a feminist perspective, with most identifying sadomasochism as rooted in \"patriarchal sexual ideology\".\n\nThe compilation includes essays by a variety of radical feminists such as Alice Walker, Robin Morgan, Kathleen Barry, Diana E. H. Russell, Susan Leigh Star, Ti-Grace Atkinson, John Stoltenberg, Sarah Lucia Hoagland, Darlene Pagano, Susan Griffin, Cheri Lesh, and Judith Butler. Butler, credited as \"Judy Butler,\" criticized sadomasochism and the Samois collective in her essay \"Lesbian S&M: The Politics of Dis-Illusion.\" The anthology also includes an interview between Audre Lorde and Susan Leigh Star. The essays express opposition to sadomasochism from a number of different viewpoints. Three pieces, a letter by Alice Walker, the interview with Audre Lorde, and a conversation between Karen Sims, Darlene Pagano, and Rose Mason, criticize the movement as insensitive to the experiences of Black women, particularly criticizing \"master/slave\" relationships. Susan Leigh Star criticizes the use of swastikas and other Nazi imagery by some BDSM practitioners as anti-Semitic and racist. Marissa Jonel and Elizabeth Harris's articles are accounts of personal experiences with sadomasochism, and Paula Tiklicorect and Melissa Bay Mathis use satire in their pieces. Several essays criticize Samois, a BDSM organization founded by and for lesbians. Susan Griffin's article, reprinted from her book \"Pornography and Silence\" with an introduction, criticizes \"Story of O\", the book from which Samois took their name. Griffin argues that \"Story of O\" shows \"how a pornographic society turns a woman's heart against herself.\"\n\nIn a review for lesbian feminist magazine \"off our backs\", Carol Anne Douglas highly recommended the book, praising its arguments as convincing and calling parts of the book \"moving.\" Charles Moser wrote a negative review for \"The Journal of Sex Research\", admitting that the essays are \"well-written\" but nonetheless calling the book \"infuriating.\" Moser compares the feminist arguments against sadomasochism in the book to religious arguments against homosexuality, saying both of these cause unnecessary guilt.\n\n"}
{"id": "46573763", "url": "https://en.wikipedia.org/wiki?curid=46573763", "title": "Algorithms Unlocked", "text": "Algorithms Unlocked\n\nAlgorithms Unlocked is a book by Thomas H. Cormen about the basic principles and applications of computer algorithms. The book consists of ten chapters, and deals with the topics of searching, sorting, basic graph algorithms, string processing, the fundamentals of cryptography and data compression, and an introduction to the theory of computation.\n"}
{"id": "41491826", "url": "https://en.wikipedia.org/wiki?curid=41491826", "title": "Anatomical terms of muscle", "text": "Anatomical terms of muscle\n\nMuscles are described using unique anatomical terminology according to their actions and structure.\n\nThere are three types of muscle tissue in the human body: skeletal, smooth, and cardiac.\n\nSkeletal striated muscle, or \"voluntary muscle\", primarily joins to bone with tendons. Skeletal muscle enables movement of the bones of the human skeleton and maintains posture.\n\nSmooth muscle tissue is found in parts of the body where it conveys action without conscious intent. The majority of this type of muscle tissue is found in the digestive and urinary systems where it acts by propelling forward food, chyme, and feces in the former and urine in the latter. Other places smooth muscle can be found are within the uterus, where it helps facilitate birth, and the eye, where the pupillary sphincter controls pupil size.\n\nCardiac muscle is specific to the heart. It is also involuntary in its movement, and is additionally self-excitatory, contracting without outside stimuli.\n\nAs well as anatomical terms of motion, which describe the motion made by a muscle, unique terminology is used to describe the action of a set of muscles.\n\nAgonist muscles and antagonist muscles refer to muscles that cause or inhibit a movement.\n\n\"Agonist\" muscles cause a movement to occur through their own activation. For example, the triceps brachii contracts, producing a shortening contraction, during the up phase of a push-up (elbow extension). During the down phase of a push-up, the same triceps brachii actively controls elbow flexion while producing a lengthening contraction. It is still the agonist, because while resisting gravity during relaxing, the triceps brachii continues to be the prime mover, or controller, of the joint action. Agonists are also interchangeably referred to as \"prime movers,\" since they are the muscles considered primarily responsible for generating or controlling a specific movement.\n\nAnother example is the dumbbell curl at the elbow. The \"elbow flexor\" group is the agonist, shortening during the lifting phase (elbow flexion). During the lowering phase the \"elbow flexor\" muscles lengthen, remaining the agonists because they are controlling the load and the movement (elbow extension). For both the lifting and lowering phase, the \"elbow extensor\" muscles are the antagonists (see below). They lengthen during the dumbbell lifting phase and shorten during the dumbbell lowering phase. Here it is important to understand that it is common practice to give a name to a muscle group (e.g. elbow flexors) based on the joint action they produce during a shortening (concentric) contraction. However, this naming convention does not mean they are only agonists during shortening. This term typically describes the function of skeletal muscles.\n\n\"Antagonist\" muscles are simply the muscles that produce an opposing joint torque to the agonist muscles. This torque can aid in controlling a motion. The opposing torque can slow movement down - especially in the case of a ballistic movement. For example, during a very rapid (ballistic) discrete movement of the elbow, such as throwing a dart, the triceps muscles will be activated very briefly and strongly (in a \"burst\") to rapidly accelerate the extension movement at the elbow, followed almost immediately by a \"burst\" of activation to the elbow flexor muscles that decelerates the elbow movement to arrive at a quick stop. To use an automotive analogy, this would be similar to pressing your gas pedal rapidly and then immediately pressing the brake. Antagonism is not an intrinsic property of a particular muscle or muscle group; it is a role that a muscle plays depending on which muscle is currently the agonist. During slower joint actions that involve gravity, just as with the agonist muscle (mentioned above), the antagonist muscle can shorten and lengthen. Using the example above of the triceps brachii during a push-up, the elbow flexor muscles are the antagonists at the elbow during both the up phase and down phase of the movement. During the dumbbell curl, the elbow extensors are the antagonists for both the lifting and lowering phases. \n\nAntagonist and agonist muscles often occur in pairs, called \"antagonistic pairs\". As one muscle contracts, the other relaxes. An example of an antagonistic pair is the biceps and triceps; to contract - the triceps relaxes while the biceps contracts to lift the arm. \"Reverse motions\" need antagonistic pairs located in opposite sides of a joint or bone, including abductor-adductor pairs and flexor-extensor pairs. These consist of an extensor muscle, which \"opens\" the joint (by increasing the angle between the two bones) and a flexor muscle, which does the opposite by decreasing the angle between two bones.\n\nHowever muscles don't always work this way - sometimes agonists and antagonists contract at the same time to produce force, as per Lombard's paradox. Also, sometimes during a joint action controlled by an agonist muscle (see above definition of agonist), the antagonist will be slightly activated, naturally. This occurs normally and is not considered to be a problem unless it is excessive or uncontrolled and disturbs the control of the joint action. This is called agonist/antagonist co-activation and serves to mechanically stiffen the joint.\n\nNot all muscles are paired in this way. An example of exception is the deltoid.\n\n\"Synergist\" muscles perform, or help perform, the same set of joint motion as the agonists. Synergists muscles act on movable joints. Synergists are sometimes referred to as \"neutralizers\" because they help cancel out, or neutralize, extra motion from the agonists to make sure that the force generated works within the desired plane of motion.\n\nMuscle fibers can only contract up to 40% of their fully stretched length. Thus the short fibers of pennate muscles are more suitable where power rather than range of contraction is required. This limitation in the range of contraction affects all muscles, and those that act over several joints may be unable to shorten sufficiently to produce the full range of movement at all of them simultaneously (active insufficiency, e.g., the fingers cannot be fully flexed when the wrist is also flexed). Likewise, the opposing muscles may be unable to stretch sufficiently to allow such movement to take place (passive insufficiency). For both these reasons, it is often essential to use other muscles, called fixators or synergists, in this type of action to fix certain of the joints so that others can be moved effectively, e.g., fixation of the wrist during full flexion of the fingers in clenching the fist. Synergists are muscles that facilitate the fixation action.\n\nThere is an important difference between a helping synergist muscle and a true synergist muscle. A true synergist muscle is one that only neutralizes an undesired joint action, whereas a helping synergist is one that neutralizes an undesired action but also assists with the desired action. \n\nA muscle that fixes or holds a bone so that the agonist can carry out the intended movement is said to have a neutralising action. A good famous example of this are the hamstrings; the semitendinosus and semimembranosus muscles perform knee flexion and knee internal rotation whereas the biceps femoris carries out knee flexion and knee external rotation. For the knee to flex while not rotating in either direction, all three muscles contract to stabilize the knee while it moves in the desired way.\n\n\"Composite\" or \"hybrid\" muscles have more than one set of fibers that perform the same function, and are usually supplied by different nerves for different set of fibers. For example, the tongue itself is a composite muscle made up of various components like longitudinal, transverse, horizontal muscles with different parts innervated having different nerve supply.\n\nThe \"insertion\" and \"origin\" of a muscle are the two places where it is anchored, one at each end. The tissue of the attachment is called an \"enthesis\".\n\nThe \"origin\" of a muscle is the bone, typically proximal, which has greater mass and is more stable during a contraction than a muscle's insertion. For example, with the latissimus dorsi muscle, the origin site is the torso, and the insertion is the arm. When this muscle contracts, normally the arm moves due to having less mass than the torso. This is the case when grabbing objects lighter than the body, as in the typical use of a lat pull down machine. This can be reversed however, such as in a chin up where the torso moves up to meet the arm.\n\nThe \"insertion\" of a muscle is the structure that it attaches to and tends to be moved by the contraction of the muscle. This may be a bone, a tendon or the subcutaneous dermal connective tissue. Insertions are usually connections of muscle via tendon to bone. The insertion is a bone that tends to be distal, have less mass, and greater motion than the origin during a contraction.\n\nMuscles may also be described by the direction that the muscle fibres run in.\n\nHypertrophy is increase in muscle size from an increase in size of individual muscle cells. This usually occurs as a result of exercise.\n\n\n"}
{"id": "22584328", "url": "https://en.wikipedia.org/wiki?curid=22584328", "title": "Aotearoa New Zealand Development Studies Network", "text": "Aotearoa New Zealand Development Studies Network\n\nThe Aotearoa New Zealand Development Studies network (DevNet) came about to link together New Zealand's 'development community' to foster dialogue and information sharing. In 1993, Massey University organised the 'Development that Works: Lessons from the Asia Pacific' conference. It brought together for the first time a diverse array of development representatives to discuss successful (and not so successful) initiatives and the lessons learned from these.\n\nBetween 1995 and 1997, three Development Studies programmes were running in Aotearoa New Zealand universities (Auckland, Massey and Victoria). To maintain cooperation and information sharing, together with the New Zealand Ministry of Foreign Affairs and Trade's development division (the earlier manifestation of NZAID), and the Council for International Development (CID), the development departments sought closer links with the wider development community. After consultation, in 1997, the Aotearoa New Zealand International Development Studies Network (DevNet) was officially established to link development studies programmes, students of development, development practitioners, non-governmental organisations and donor agencies together as a way to share information and cooperate on activities of mutual interest.\n\nFollowing DevNet's establishment, the network was located at the University of Auckland. In November 1998, DevNet's secretariat was moved to Dev-Zone, a programme of the Development Resource Centre. DevNet has one representative from each New Zealand university, as well as one at CID, Global Focus and NZAID.\n\nWith over 2,000 members, DevNet's members have access to the Aotearoa New Zealand development research database, a calendar of development events and notices, discussion forums, information on Development Studies in Aotearoa New Zealand and other useful links. Monthly Updates are sent our by the secretariat. DevNet Conferences run every two years.\n\nTo date, DevNet has held five Development Studies conferences DevNet Conference Page\n\nPoverty, Prosperity and Progress, Development Resource Centre, Wellington, December 2000\n\nContesting Development: Pathways to Better Practice, Massey University, Palmerston North, December 2002\n\nDevelopment on the Edge, Auckland University, December 2004\n\nSouthern Perspectives on Development: Dialogue or Division? University of Otago, Dunedin, December 2006\n\nPeripheral Vision, Victoria University of Wellington, December 2008\n\n"}
{"id": "14383330", "url": "https://en.wikipedia.org/wiki?curid=14383330", "title": "Aquacultural engineering", "text": "Aquacultural engineering\n\nAquacultural engineering is a multidisciplinary field of engineering that aims to solve technical problems associated with farming aquatic vertebrates, invertebrates, and algae. Common aquaculture systems requiring optimization and engineering include sea cages, ponds, and recirculating systems. The design and management of these systems is based on their production goals and the economics of the farming operation.\n\nAquaculture technology is varied with design and development requiring knowledge of mechanical, biological and environmental systems along with material engineering and instrumentation. Furthermore, engineering techniques often involve solutions borrowed from wastewater treatment, fisheries, and traditional agriculture.\n\nAquacultural engineering has played a role in the expansion of the aquaculture industry, which now accounts for half of all seafood products consumed in the world. To identify effective solutions the discipline is combined with both fish physiology and business economics knowledge.\n\nRecirculating aquaculture systems often involve intensive, high-density culture of a species with limited water usage and extensive filtration. In a typical recirculating aquaculture system, a series of filtration steps maintains a high level of water quality that promotes rapid fish growth. Steps include solids removal, biofiltration, oxygenation, and pumping, with each one requiring different equipment and engineering considerations. Comprehensive instrumentation and sensor controls are required to monitor this equipment and the underlying water conditions such as temperature, dissolved oxygen, and pH. Development of recirculating aquaculture systems is still underway in 2017, and engineering advances are needed to make the systems economically viable for culturing most species.\n\nThe \"Journal of Aquacultural Engineering\" publishes engineers' studies related to the design and development of aquacultural systems. Worldwide, universities provide aquacultural engineering education often under the umbrella of agricultural or biological engineering.\n\n"}
{"id": "33255509", "url": "https://en.wikipedia.org/wiki?curid=33255509", "title": "Atmosphere-Space Transition Region Explorer", "text": "Atmosphere-Space Transition Region Explorer\n\nAtmosphere-Space Transition Region Explorer (ASTRE) is a mission concept proposed in 2011 to NASA's Medium-Class Explorers program (MIDEX) to study the interaction between the Earth's atmosphere and the ionized gases in outer space in an effort to understand how space-induced currents in electric power grids originate, as well as improve satellite drag models. The spacecraft would measure ionized gases within the transition region/boundary layer between 150-250 km altitude. The concept was not selected for development at that time.\n\nThe Principal Investigator is Robert F. Pfaff from the Goddard Space Flight Center in Greenbelt, Maryland.\n"}
{"id": "5761692", "url": "https://en.wikipedia.org/wiki?curid=5761692", "title": "Attachment measures", "text": "Attachment measures\n\nAttachment measures refer to the various procedures used to assess attachment in children and adults.\n\nResearchers have developed various ways of assessing patterns of attachment in children. A variety of methods allow children to be classified into four attachment pattern groups: secure, anxious-ambivalent, anxious-avoidant, and disorganized/disoriented, or assess disorders of attachment. These patterns are also referred to as Secure (Group B); Anxious/Resistant (Group C); Avoidant (Group A) and Disorganized/Controlling (Group D). The disorganized/controlling attachment classification is thought to represent a breakdown in the attachment-caregiving partnership such that the child does not have an organized behavioral or representational strategy to achieve protection and care from the attachment figure. Each pattern group is further broken down into several sub-categories. A child classified with the disorganized/controlling attachment will be given a \"next best fit\" organized classification.\n\nAttachment in adults is commonly measured using the Adult Attachment Interview, the Adult Attachment Projective Picture System, and self-report questionnaires. Self-report questionnaires assess attachment style, a personality dimension that describes attitudes about relationships with romantic partners. Adult attachment style is thought to be similar to childhood attachment patterns. There is some research that shows a link between childhood attachment patterns and attachment personality dimensions with romantic partners, but correlations are mild to moderate. The most common approach to defining attachment style is a two-dimension approach in defining attachment style. One dimension deals with anxiety about the relationship, and the other dimension dealing with avoidance in the relationship. Another approach defines four adult attachment style categories: secure, preoccupied, dismissive-avoidant, and fearful-avoidant.\n\nSome methods are based on observation of infants and toddlers either in natural or 'arranged' situations. Other methods, suitable for older children, are based on asking children to complete \"attachment story stems,\" draw a picture of their family, or describe their relationships.\n\nThe Strange Situation procedure was formulated to observe attachment relationships between a caregiver and children between the age of nine and 18 months. It was developed by Mary Ainsworth, a developmental psychologist Originally it was devised to enable children to be classified into the attachment styles known as \"secure\", \"anxious-avoidant\" and \"anxious-ambivalent\". As research accumulated and atypical patterns of attachment became more apparent it was further developed by Main and Solomon in 1986 and 1990 to include the new category of disorganized/disoriented attachment.\n\nIn this procedure the child is observed playing for 20 minutes while caregivers and strangers enter and leave the room, recreating the flow of the familiar and unfamiliar presence in most children's lives. The situation varies in stressfulness and the child's responses are observed. The child experiences the following situations:\n\n\nTwo aspects of the child's behaviour are observed:\n\nAs the SSP is not suitable beyond 18 months of age, other measures have been developed for older ages groups, which include observational measures (in a controlled or naturalistic environment), representational methods and interview methods. Some are developed for research purposes whereas others have been developed for clinical use. Effective training of evaluators is essential, as some items to be assessed require interpretation reliability (e.g., child is \"suddenly aggressive toward mother for no reason\").\n\nAlthough originally designed for 1-year-old children, Ainsworth’s strange situation has been adapted to measure the attachment and exploratory behavior of children between the ages of 2-4½ years-old. A fundamental feature of the strange situation is that the situation the child is placed in must elicit stress. If the strange situation fails to stress the child, it cannot serve as an adequate environment for the measurement of attachment. The preschool strange situation features several alterations to facilitate the creation of stress in older children. These modifications include a slightly longer separation, changes in the role and/or gender of the stranger, and changes in the instructions to the caregiver. Some versions of the preschool strange situations omit the stranger altogether, thus leaving the child alone in the room throughout both separations. The coding system used to interpret the attachment style expressed by the child has also been modified. Rather than focusing entirely on the expression of specific behaviors and emotions, the revised coding system assesses ways in which a variety of behaviors, such as talking, are organized to maintain and negotiate proximity and contact. Cassidy, Marvin and the MacArthur Working group published a version of the Strange Situation procedure designed for children within the age group of 3- to 4-years-old. In addition to categorizing a child’s attachment as secure, insecure/avoidant, insecure/ambivalent, and insecure/disorganized, the measure includes a seven-point avoidance scale and nine-point security scale.\n\nThis system, devised in 1988, analyses the reunion of child and parent after a 1-hour separation. It is aimed at 6-year-olds and classifies their attachment status.\n\nThe PAA was devised by P.Crittenden for the purpose of assessing patterns of attachment in 18-month to 5-year-old children. Like the SSP it involves an observation which is then coded. The classifications include all the SSP categories plus patterns that develop during the second year of life. The three basic strategies for negotiating interpersonal relationships are modified to fit preschoolers and the patterns are renamed \"secure/balanced\", or Type B, \"defended\", or Type A and \"coercive\" or Type C. It is also intended to be able to distinguish the unendangered from the endangered compulsive and obsessive subpatterns that may have implications for emotional and behavioral development.\n\nThe MIM is a structured observation of the interaction between parent and child. The MIM was created by Marianne Marschak in the 1960s at the Yale Child Study Center. Salo & Makela (2006) of Finland have standardized and published a rating scale for the MIM for research purposes. Anne Stewart has developed the MIM Behavior Rating Scale (MIMBRS).\n\nThis method, devised by Waters and Deane in 1985, utilizes Q-Sort methodology. It is based on a set period of observation of children aged 1 – 5 in a number of environments. It consists of nearly 100 items intended to cover the spectrum of attachment related behaviors including secure base and exploratory behaviors, affective response and social cognition. The observer sorts the cards corresponding to the degree to which the child exhibits the item, which is then scored. The overall score for each child will result in a variable ranging from +1.0 (i.e., very secure) to -1.0 (i.e., very insecure). Despite its ability to classify secure attachment, the score derived from the Q-set measure does not classify the type of insecure attachment.\n\nThis approach uses dolls and narrative to enact a story. The dolls represent family members. The interviewer enacts the beginning of the story and then hands the dolls over for the child to complete it with varying degrees of prompting and encouragement. These techniques are designed to access the child's internal working models of their attachment relationships. Methods include the MacArthur Story Stem Battery (MSSB) and the Attachment Story Completion Test, developed in 1990 for children between the age of 3 to 8 years; the Story Stem Assessment Profile (SSAP) developed in 1990 for children aged 4 – 8; the Attachment Doll Play Assessment developed in 1995 for children age 4.5-11; the Manchester Child Attachment Story Task (MCAST) developed in 2000 for children aged 4.5 - 8.5.\n\nThe Attachment Story Completion Task (ASCT) is a semi-projective attachment measure designed by Inge Bretherton and colleagues to assess the internal working model of children between the age of 3 to 9 years old (though it requires modification when used with older children). The measure evaluates a child's attachment style by analyzing how a child resolves a stress inducing story. In a 30-minute recorded interview, five story stems are presented through the use of props, such as small family figures. The stories are designed to access how that child interacts with their primary caregiver in five situations: separation, confrontation, fear, reunion, and pain. The Interviewer prompts the child to complete each story by saying \"show me, or tell me what happens next.\" The information derived from the interview is later coded according to the organization and content of the story completion. Avoidant attachment, for example, can be disclosed by a child refusing to acknowledge the attachment issue presented in the story stem (through claiming that the event did not take place). A child may also avoid addressing attachment by focusing solely on minor details, such as how the protagonist is dressed. Secure attachment, alternately, is indicated when a child provides coherent and constructive resolutions to the stories.\n\nThe MCAST is a semi-structured doll play 'story stem' methodology, developed by Jonathan Green, Charlie Stanley, Ruth Goldwyn and Vicky Smith, to evaluate and understand the internal (mental) representations of their attachment relationship with a specific primary caregiver in children of 4 to 8.5 years. The concepts and procedures used have a basis in the Strange Situation Procedure and Adult Attachment Interview, and involves 4 story stem vignettes involving two dolls representing the caregiver-child dyad of interest and a dolls house, presented with affective arousal to mobilise attachment representations in a way that children of this age range find accessible and engaging. Responses are usually videotaped in order to reliably rate aspects of the child's represented narrative content and behaviour, and the child's own behaviour, to ascertain an attachment classification, with a particular focus on disorganised attachment, as well as providing other supporting ratings. Clinical development of the MCAST started in 1992, validation was published in 2000, and it has been since used in a range of cultural contexts and clinical and at-risk groups. Training is required for its use.\n\nLike the stem stories, these techniques are designed to access the child's internal working models of attachment relationships. The child is shown attachment related pictures and asked to respond. Methods include the Separation Anxiety Test (SAT) developed in 1972 for children aged between 11 and 17. Revised versions have been produced for 4 - 7-year-olds. The SAT was doctored.\n\nThis is a semi-structured interview designed by Target et al. (2003) for children aged 7 to 11. It is based on the Adult Attachment Interview, adapted for children by focussing on representations of relationships with parents and attachment related events. Scores are based on both verbal and non-verbal communications.\n\nDisturbances of Attachment Interview developed, by Smyke and Zeanah, (1999), is a semi-structured interview designed to be administered by clinicians to caregivers. This method is designed to pick up not only reactive attachment disorder but also Zeannah et al.'s (1993) suggested new alternative categories of disorders of attachment. It covers 12 items, namely having a discriminated, preferred adult, seeking comfort when distressed, responding to comfort when offered, social and emotional reciprocity, emotional regulation, checking back after venturing away from the care giver, reticence with unfamiliar adults, willingness to go off with relative strangers, self endangering behavior, excessive clinging, vigilance/hypercompliance and role reversal.\n\nThis is a version of the Adult Attachment Interview (AAI) rendered age appropriate for adolescents. The classifications of \"dismissing, secure, preoccupied\" and \"unresolved\" are the same as under the AAI described below.\n\nExisting measures have not necessarily been developed to a useful level. \"Behavioral observation is a natural starting point for assessing attachment disorders because behavioral descriptions... have been central to the development of the concept... despite the fact that observations have figured prominently... no established observational protocol has been established\" \n\nAlso, questionable measures of attachment in school-age children have been presented. For example, a protocol for establishing attachment status was described by Sheperis and his colleagues. Unfortunately, this protocol was validated against another technique, the Randolph Attachment Disorder Questionnaire, that was itself poorly validated and that is based on a nonconventional view of attachment.\n\nPsychiatrist Michael Rutter describe the limitations of the procedure in the following terms;\n\n\"It is by no means free of limitations (see Lamb, Thompson, Gardener, Charnov & Estes, 1984). To begin with, it is very dependent on brief separations and reunions having the same meaning for all children. This maybe a major constraint when applying the procedure in cultures, such as that in Japan (see Miyake et al., 1985), where infants are rarely separated from their mothers in ordinary circumstances. Also, because older children have a cognitive capacity to maintain relationships when the older person is not present, separation may not provide the same stress for them. Modified procedures based on the Strange Situation have been developed for older preschool children (see Belsky et al., 1994; Greenberg et al., 1990) but it is much more dubious whether the same approach can be used in middle childhood. Also, despite its manifest strengths, the procedure is based on just 20 minutes of behaviour. It can be scarcely expected to tap all the relevant qualities of a child's attachment relationships. Q-sort procedures based on much longer naturalistic observations in the home, and interviews with the mothers have developed in order to extend the data base (see Vaughn & Waters, 1990). A further constraint is that the coding procedure results in discrete categories rather than continuously distributed dimensions. Not only is this likely to provide boundary problems, but also it is not at all obvious that discrete categories best represent the concepts that are inherent in attachment security. It seems much more likely that infants vary in their degree of security and there is need for a measurement systems that can quantify individual variation\".\nWith respect to the ecological validity of the Strange Situation, a meta-analysis of 2,000 infant-parent dyads, including several from studies with non-Western language and/or cultural bases found the global distribution of attachment categorizations to be A (21%), B (65%), and C (14%) This global distribution was generally consistent with Ainsworth et al.'s (1978) original attachment classification distributions.\n\nHowever, controversy has been raised over a few cultural differences in these rates of 'global' attachment classification distributions. In particular, two studies diverged from the global distributions of attachment classifications noted above. One study was conducted in North Germany in which more avoidant (A) infants were found than global norms would suggest, and the other in Sapporo, Japan where more resistant (C) infants were found. Of these two studies, the Japanese findings have sparked the most controversy as to the meaning of individual differences in attachment behavior as originally identified by Ainsworth et al. (1978).\n\nIn a recent study conducted in Sapporo, Behrens, et al., 2007. found attachment distributions consistent with global norms using the six-year Main & Cassidy scoring system for attachment classification. In addition to these findings supporting the global distributions of attachment classifications in Sapporo, Behrens et al. also discuss the Japanese concept of amae and its relevance to questions concerning whether the insecure-resistant (C) style of interaction may be engendered in Japanese infants as a result of the cultural practice of amae.\n\nRegarding the issue of whether the breadth of infant attachment functioning can be captured by a categorical classification scheme, it should be noted that continuous measures of attachment security have been developed which have demonstrated adequate psychometric properties. These have been used either individually or in conjunction with discrete attachment classifications in many published reports [see Richters et al., 1998; Van IJzendoorn et al., 1990).] The original Richter’s et al. (1998) scale is strongly related to secure versus insecure classifications, correctly predicting about 90% of cases. Readers further interested in the categorical versus continuous nature of attachment classifications (and the debate surrounding this issue) should consult the paper by Fraley and Spieker and the rejoinders in the same issue by many prominent attachment researchers including J. Cassidy, A. Sroufe, E. Waters & T. Beauchaine, and M. Cummings.\n\nThe three main ways of measuring attachment in adults include the Adult Attachment Interview (AAI), the Adult Attachment Projective Picture System (AAP), and self-report questionnaires. The AAI, AAP, and the self-report questionnaires were created with somewhat different aims in mind. Shaver and Fraley note:\n\n\"If you are a novice in this research area, what is most important for you to know is that self-report measures of romantic attachment and the AAI were initially developed completely independently and for quite different purposes. One asks about a person's feelings and behaviors in the context of romantic or other close relationships; the other is used to make inferences about the defenses associated with an adult's current state of mind regarding childhood relationships with parents. In principle, these might have been substantially associated, but in fact they seem to be only moderately related--at least as currently assessed. One kind of measure receives its construct validity mostly from studies of romantic relationships, the other from prediction of a person's child's behavior in Ainsworth's Strange Situation. Correlations of the two kinds of measures with other variables are likely to differ, although a few studies have found the AAI to be related to marital relationship quality and a few have found self-report romantic attachment measures to be related to parenting.\" (Shaver & Fraley, 2004) \nThe AAI, the AAP, and the self-report questionnaires offer distinct, but equally valid, perspectives on adult attachment. It's therefore worthwhile to become familiar with each approach.\n\nDeveloped by Carol George, Nancy Kaplan, and Mary Main in 1984, this is a quasi-clinical semi-structured interview that takes about one hour to administer. It involves about twenty questions and has extensive research validation to support it. A good description can be found in Chapter 25 of Attachment Theory, Research and Clinical Applications (2nd ed.), edited by J. Cassidy and P. R. Shaver, Guilford Press, NY, 2008. The chapter title is \"The Adult Attachment Interview: Historical and Current Perspectives,\" and is written by E. Hesse. The interview taps into adult representation of attachment (i.e. internal working models) by assessing general and specific recollections from their childhood. The interview is coded based on quality of discourse (especially coherence) and content. Categories are designed to predict parental stances on Berkeley infant data. \n\nParental AAI Attachment status includes:\n\nSome of the strongest external validation of the measures involves its demonstrated ability to predict interviewees' children's classifications in the Strange Situation. The measure also has been shown to have some overlap with attachment constructs measured by the less time-intensive measures of the peer/romantic attachment tradition (Hazan & Shaver, Bartholomew), as reported by Shaver, P. R., Belsky, J., & Brennan, K. A. (2000). However, there are important differences in what is measured by the AAI—rather than being a measure of romantic attachment, it taps primarily into a person's state of mind regarding their attachment in their family of origin (nuclear family).\n\nDeveloped by Carol George and Malcolm West in 1999, this is a free response task that involved telling stories in response to eight picture stimuli (1 warm-up & 7 attachment scenes). A good description can be found in George and West's 1999 paper in the journal Attachment and Human Development. A book describing the measure is forthcoming from Guilford Press in spring 2011.\n\nThe AAP identifies the same adult attachment groups as the AAI, as described above. In addition to providing adult group classifications, the AAP is also used to code attachment defensive processing patterns, attachment synchrony, and personal agency.\n\nThe strongest concurrent validation of the measure is the correspondence between AAP and AAI classification agreement. The AAP is demonstrated to be increasingly useful in clinical and neurobiological settings. The AAP is being used to assess attachment in adults and adolescents.\n\nHazan and Shaver created the first questionnaire to measure attachment in adults. \n\nTheir questionnaire was designed to classify adults into the three attachment styles identified by Ainsworth. The questionnaire consisted of three sets of statements, each set of statements describing an attachment style:\n\n\nPeople participating in their study were asked to choose which set of statements best described their feelings. The chosen set of statements indicated their attachment style. Later versions of this questionnaire presented scales so people could rate how well each set of statements described their feelings.\n\nOne important advance in the development of attachment questionnaires was the addition of a fourth style of attachment. Bartholomew and Horowitz presented a model that identified four categories or styles of adult attachment. \nTheir model was based on the idea attachment styles reflected people's thoughts about their partners and thought about themselves. Specifically, attachment styles depended on whether or not people judge their partners to be generally accessible and responsive to requests for support, and whether or not people judge themselves to be the kind of individuals towards which others want to respond and lend help. They proposed four categories based on positive or negative thoughts about partners and on positive or negative thoughts about self.\n\nBartholomew and Horowitz used this model to create the Relationship Questionnaire (RQ-CV). The RQ-CV consisted of four sets of statements, each describing a category or style of attachment:\n\n\nTests demonstrated the four attachment styles were distinct in how they related to other kinds of psychological variables. Adults indeed appeared to have four styles of attachment instead of three attachment styles.\n\nDavid Schmitt, together with a large number of colleagues, validated the attachment questionnaire created by Bartholomew and Horowitz in 62 cultures. \nThe distinction of thoughts about self and thoughts about partners proved valid in nearly all cultures. However, the way these two kinds of thoughts interacted to form attachment styles varied somewhat across cultures. The four attachment styles had somewhat different meanings across cultures.\n\nA second important advance in attachment questionnaires was the use of independent items to assess attachment. Instead of asking people to choose between three or four sets of statements, people rated how strongly they agreed with dozens of individual statements. The ratings for the individual statements were combined to provide an attachment score. Investigators have created several questionnaires using this strategy to measure adult attachment.\n\nTwo popular questionnaires of this type are the Experiences in Close Relationships (ECR) questionnaire and the Experiences in Close Relationships - Revised (ECR-R) questionnaire. The ECR was created by Brennan, Clark, and Shaver in 1998. \nThe ECR-R was created by Fraley, Waller, and Brennan in 2000. \nAnalysis of the ECR and ECR-R reveal that the questionnaire items can be grouped into two dimensions of attachment. One group of questionnaire items deal with how anxious a person is about their relationship. These items serve as a scale for anxiety. The remaining items deal with how avoidant a person is in their relationship. These items serve as a scale for avoidance. Many researchers now use scores from the anxiety and avoidance scales to perform statistical analyses and test hypotheses.\n\nScores on the anxiety and avoidance scales can still be used to classify people into the four adult attachment styles. \nThe four styles of attachment defined in Bartholomew and Horowitz's model were based on thoughts about self and thoughts about partners. The anxiety scale in the ECR and ECR-R reflect thoughts about self. Attachment anxiety relates to beliefs about self-worth and whether or not one will be accepted or rejected by others. The avoidance scale in the ECR and ECR-R relates to thoughts about partners. Attachment avoidance relates to beliefs about taking risks in approaching or avoiding other people. Combinations of anxiety and avoidance can thus be used to define the four attachment styles. The secure style of attachment is characterized by low anxiety and low avoidance; the preoccupied style of attachment is characterized by high anxiety and low avoidance; the dismissive avoidant style of attachment is characterized by low anxiety and high avoidance; and the fearful avoidant style of attachment is characterized by high anxiety and high avoidance.\n\n\n"}
{"id": "37908412", "url": "https://en.wikipedia.org/wiki?curid=37908412", "title": "Baldy Center for Law and Social Policy", "text": "Baldy Center for Law and Social Policy\n\nThe Baldy Center for Law & Social Policy is a research center at the University at Buffalo that advances interdisciplinary research on law, legal institutions and social policy. Founded in 1978, the Baldy Center is housed within UB’s School of Law but serves faculty with law and policy interests throughout the university. The Center was endowed by a bequest from Christopher Baldy, a prominent Buffalo attorney and UB School of Law graduate who died in 1959.\n\nThe Baldy Center sponsors interdisciplinary research focusing on the intersection of law and social policy, awarding research grants annually to scholars across the university, especially in law, the humanities and social sciences. It also hosts and sponsors conferences that bring together researchers from across the world to discuss their Law and Society-related research. A list of grants awarded for research and conferences and their recipients is available at the Center's website.\n\nThe Center’s intensive book manuscript workshops are organized around drafts of books about law, legal institutions or social policy by UB-affiliated scholars. The interdisciplinary discussions are designed to provide feedback to the authors from interested faculty and outside specialists.\n\nThe Baldy Center maintains cooperative ties to other interdisciplinary research centers and cosponsors a regional network of sociolegal scholars in New York and Canada. The Center additionally hosts distinguished scholars from around the world as visitors, speakers, consultants and conference participants.\n\nThe Center offers postdoctoral fellowships to budding scholars hoping to join the academy at American and global universities, along with research fellowships and senior fellowships. Research fellowships link law-and-policy scholars into the Baldy community and its resources, while senior fellowships are geared toward accomplished academics from other universities whose projects are closely related to the Baldy Center’s mission.\n\nThrough the Buffalo Legal Studies Research Paper Series, hosted and distributed by the Social Science Research Network, the Center provides an international audience for faculty and visiting scholars.\n\nChristopher Baldy graduated from the Buffalo School of Law in 1910, in the days when UB was a private school. He died a bachelor and left most of his estate, initially valued at $1.4 million, to his three brothers, stipulating that when they died, the remainder would be a gift to UB.\n\nA surrogate judge in 1973 approved using the Baldy funding to start a new program in law and social policy, a subject area favored by UB’s law dean, Red Schwartz, a sociologist and the first-ever non-interim dean of an American law school who was not a lawyer. Schwartz was a key figure in the Law and Society movement that emerged in the 1960s and sought to explore how the law actually interacted with society, as opposed to the more formalistic, theoretical and academic approach that permeated legal scholarship for much of the 20th century.\n\nFive years later, under then-Dean Thomas Headrick, the program in law and social policy was formalized into the Baldy Center.\n\nFrom 1981 to 2009, the Center hosted the peer-reviewed academic journal Law & Policy.\n\nFor much of its history, the Baldy Center’s activities were organized around working groups focused on specific research areas. The number of groups fluctuated over time and during the 1990s were concentrated into more formal Baldy Programs such as Environmental Law and Policy; Children, Families and the Law; Community and Difference; Law, Gender and Social Policy; Human Rights Law and Policy; and Regulation and Public Policy.\n\nAfter 2010, the Center retired the working groups model and instituted its fellowship program while continuing to focus on conferences, book manuscript workshops and supporting research.\n\nDirectors of the Baldy Center have included Barry B. Boyer (co-director 1978-81; director 1981-92), Jim Brady (co-director 1978-81), David M. Engel (1992-2001), Lynn Mather (2002-08), Rebecca French (2008-10) and Errol Meidinger (interim director 2001-02; director 2010-present).\n\nAssistant directors have included Wendy Katkin (1978-85), Venice Feeley Cadwallader (1986-90), Laura Mangan (1990-2006; 2008-09) and Laura Wirth (2009-present).\n"}
{"id": "11628729", "url": "https://en.wikipedia.org/wiki?curid=11628729", "title": "Berendsen thermostat", "text": "Berendsen thermostat\n\nThe Berendsen thermostat is an algorithm to re-scale the velocities of particles in molecular dynamics simulations to control the simulation temperature.\n\nIn this scheme, the system is weakly coupled to a heat bath with some temperature. The thermostat suppresses fluctuations of the kinetic energy of the system and therefore cannot produce trajectories consistent with the canonical ensemble. The temperature of the system is corrected such that the deviation exponentially decays with some time constant formula_1.\n\nThough the thermostat does not generate a correct canonical ensemble (especially for small systems), for large systems on the order of hundreds or thousands of atoms/molecules, the approximation yields roughly correct results for most calculated properties. The scheme is widely used due to the efficiency with which it relaxes a system to some target (bath) temperature. In many instances, systems are initially equilibrated using the Berendsen scheme, while properties are calculated using the widely known Nosé-Hoover thermostat, which correctly generates trajectories consistent with a canonical ensemble. However, the Berendsen thermostat can result in the flying ice cube effect, an artifact which can be eliminated by using the more rigorous Bussi-Donadio-Parrinello thermostat; for this reason, it has been recommended that usage of the Berendsen thermostat be discontinued in almost all cases except for replication of prior studies.\n\n"}
{"id": "14054604", "url": "https://en.wikipedia.org/wiki?curid=14054604", "title": "Computers and Intractability", "text": "Computers and Intractability\n\nIn computer science, more specifically computational complexity theory, Computers and Intractability: A Guide to the Theory of NP-Completeness is an influential textbook by Michael Garey and David S. Johnson.\nIt was the first book exclusively on the theory of NP-completeness and computational intractability. The book features an appendix providing a thorough compendium of NP-complete problems (which was updated in later printings of the book). The book is now outdated in some respects as it does not cover more recent development such as the PCP theorem. It is nevertheless still in print and is regarded as a classic: in a 2006 study, the CiteSeer search engine listed the book as the most cited reference in computer science literature.\n\nAnother appendix of the book featured problems for which it was not known whether they were NP-complete or in P (or neither). The problems (with their original names) are:\n\nSoon after it appeared, the book received positive reviews by reputed researchers in the area of theoretical computer science.\n\nIn his review, Ronald V. Book recommends the book to \"anyone who wishes to learn about the subject of NP-completeness\", and he explicitly mentions the \"extremely useful\" appendix with over 300 NP-hard computational problems. He concludes: \"Computer science needs more books like this one.\"\n\nHarry R. Lewis praises the mathematical prose of the authors: \"Garey and Johnson's book is a thorough, clear, and practical exposition of NP-completeness. In many respects it is hard to imagine a better treatment of the subject.\" Also, he considers the appendix as \"unique\" and \"as a starting point in attempts to show new problems to be NP-complete\".\n\nTwenty-three years after the book appeared, Lance Fortnow, editor-in-chief of the scientific journal \"Transactions on Computational Theory\", states: \"I consider Garey and Johnson the single most important book on my office bookshelf. Every computer scientist should have this book on their shelves as well. [...] Garey and Johnson has the best introduction to computational complexity I have ever seen.\" \n\n"}
{"id": "33890874", "url": "https://en.wikipedia.org/wiki?curid=33890874", "title": "De novo transcriptome assembly", "text": "De novo transcriptome assembly\n\n\"De novo\" transcriptome assembly is the de novo sequence assembly method of creating a transcriptome without the aid of a reference genome.\n\nAs a result of the development of novel sequencing technologies, the years between 2008 and 2012 saw a large drop in the cost of sequencing. Per megabase and genome, the cost dropped to 1/100,000th and 1/10,000th of the price, respectively. Prior to this, only transcriptomes of organisms that were of broad interest and utility to scientific research were sequenced; however, these developed in 2010s high-throughput sequencing (also called next-generation sequencing) technologies are both cost- and labor- effective, and the range of organisms studied via these methods is expanding. Transcriptomes have subsequently been created for chickpea, planarians, \"Parhyale hawaiensis\", as well as the brains of the Nile crocodile, the corn snake, the bearded dragon, and the red-eared slider, to name just a few.\n\nExamining non-model organisms can provide novel insights into the mechanisms underlying the \"diversity of fascinating morphological innovations\" that have enabled the abundance of life on planet Earth. In animals and plants, the \"innovations\" that cannot be examined in common model organisms include mimicry, mutualism, parasitism, and asexual reproduction. \"De novo\" transcriptome assembly is often the preferred method to studying non-model organisms, since it is cheaper and easier than building a genome, and reference-based methods are not possible without an existing genome. The transcriptomes of these organisms can thus reveal novel proteins and their isoforms that are implicated in such unique biological phenomena.\n\nA set of assembled transcripts allows for initial gene expression studies. Prior to the development of transcriptome assembly computer programs, transcriptome data were analyzed primarily by mapping on to a reference genome. Though genome alignment is a robust way of characterizing transcript sequences, this method is disadvantaged by its inability to account for incidents of structural alterations of mRNA transcripts, such as alternative splicing. Since a genome contains the sum of all introns and exons that may be present in a transcript, spliced variants that do not align continuously along the genome may be discounted as actual protein isoforms. Even if a reference genome is available, de novo assembly should be performed, as it can recover transcripts that are transcribed from segments of the genome that are missing from the genome assembly.\n\nUnlike genome sequence coverage levels – which can vary randomly as a result of repeat content in non-coding intron regions of DNA – transcriptome sequence coverage levels can be directly indicative of gene expression levels. These repeated sequences also create ambiguities in the formation of contigs in genome assembly, while ambiguities in transcriptome assembly contigs usually correspond to spliced isoforms, or minor variation among members of a gene family. Genome assembler can't be directly used in transcriptome assembly for several reasons. First, genome sequencing depth is usually the same across a genome, but the depth of transcripts can vary. Second, both strands are always sequenced in genome sequencing, but RNA-seq can be strand-specific. Third, transcriptome assembly is more challenging because transcript variants from the same gene can share exons and are difficult to resolve unambiguously.\n\nOnce RNA is extracted and purified from cells, it is sent to a high-throughput sequencing facility, where it is first reverse transcribed to create a cDNA library. This cDNA can then be fragmented into various lengths depending on the platform used for sequencing. Each of the following platforms utilizes a different type of technology to sequence millions of short reads: 454 Sequencing, Illumina, and SOLiD.\n\nSee also List_of_RNA-Seq_bioinformatics_tools.\n\nThe cDNA sequence reads are assembled into transcripts via a short read transcript assembly program. Most likely, some amino acid variations among transcripts that are otherwise similar reflect different protein isoforms. It is also possible that they represent different genes within the same gene family, or even genes that share only a conserved domain, depending on the degree of variation.\n\nA number of assembly programs are available (see Assemblers). Although these programs have been generally successful in assembling genomes, transcriptome assembly presents some unique challenges. Whereas high sequence coverage for a genome may indicate the presence of repetitive sequences (and thus be masked), for a transcriptome, they may indicate abundance. In addition, unlike genome sequencing, transcriptome sequencing can be strand-specific, due to the possibility of both sense and antisense transcripts. Finally, it can be difficult to reconstruct and tease apart all splicing isoforms.\n\nShort read assemblers generally use one of two basic algorithms: overlap graphs and de Bruijn graphs. Overlap graphs are utilized for most assemblers designed for Sanger sequenced reads. The overlaps between each pair of reads is computed and compiled into a graph, in which each node represents a single sequence read. This algorithm is more computationally intensive than de Bruijn graphs, and most effective in assembling fewer reads with a high degree of overlap.\nDe Bruijn graphs align k-mers (usually 25-50 bp) based on k-1 sequence conservation to create contigs. The k-mers are shorter than the read lengths allowing fast hashing so the operations in de Bruijn graphs are generally less computationally intensive.\n\nFunctional annotation of the assembled transcripts allows for insight into the particular molecular functions, cellular components, and biological processes in which the putative proteins are involved. Blast2GO (B2G) enables Gene Ontology based data mining to annotate sequence data for which no GO annotation is available yet. It is a research tool often employed in functional genomics research on non-model species. It works by blasting assembled contigs against a non-redundant protein database (at NCBI), then annotating them based on sequence similarity. GOanna is another GO annotation program specific for animal and agricultural plant gene products that works in a similar fashion. It is part of the AgBase database of curated, publicly accessible suite of computational tools for GO annotation and analysis. Following annotation, KEGG (Kyoto Encyclopedia of Genes and Genomes) enables visualization of metabolic pathways and molecular interaction networks captured in the transcriptome.\n\nIn addition to being annotated for GO terms, contigs can also be screened for open reading frames (ORFs) in order to predict the amino acid sequence of proteins derived from these transcripts. Another approach is to annotate protein domains and determine the presence of gene families, rather than specific genes.\n\nSince a reference genome is not available, the quality of computer-assembled contigs may be verified either by comparing the assembled sequences to the reads used to generate them (reference-free), or by aligning the sequences of conserved gene domains found in mRNA transcripts to transcriptomes or genomes of closely related species (reference-based). Tools such as Transrate and DETONATE allow statistical analysis of assembly quality by these methods. Another method is to design PCR primers for predicted transcripts, then attempt to amplify them from the cDNA library. Often, exceptionally short reads are filtered out. Short sequences (< 40 amino acids) are unlikely to represent functional proteins, as they are unable to fold independently and form hydrophobic cores.\n\nThe following is a partial compendium of assembly software that has been used to generate transcriptomes, and has also been cited in scientific literature.\n\nSeqMan NGen, part of DNASTAR's software pipeline, includes a de novo transcriptome assembler for small or large transcriptome data sets. SeqMan NGen uses a patented algorithm which utilizes RefSeq to identify and merge transcripts, and automatically annotates assembled transcripts using DNASTAR's proprietary transcript annotation tool to identify and highlight known and novel genes.\n\nSOAPdenovo-Trans is a de novo transcriptome assembler inherited from the SOAPdenovo2 framework, designed for assembling transcriptome with alternative splicing and different expression level. The assembler provides a more comprehensive way to construct the full-length transcript sets compare to SOAPdenovo2.\n\nThe Velvet algorithm uses de Bruijn graphs to assemble transcripts. In simulations, Velvet can produce contigs up to 50-kb N50 length using prokaryotic data and 3-kb N50 in mammalian bacterial artificial chromosomes (BACs). These preliminary transcripts are transferred to Oases, which uses paired end read and long read information to build transcript isoforms.\n\nABySS is a parallel, paired-end sequence assembler. Trans-ABySS (Assembly By Short Sequences) is a software pipeline written in Python and Perl for analyzing ABySS-assembled transcriptome contigs. This pipeline can be applied to assemblies generated across a wide range of k values. It first reduces the dataset into smaller sets of non-redundant contigs, and identifies splicing events including exon-skipping, novel exons, retained introns, novel introns, and alternative splicing. The Trans-ABySS algorithms are also able to estimate gene expression levels, identify potential polyadenylation sites, as well as candidate gene-fusion events.\n\nTrinity first divides the sequence data into a number of de Bruijn graphs, each representing transcriptional variations at a single gene or locus. It then extracts full-length splicing isoforms and distinguishes transcripts derived from paralogous genes from each graph separately. Trinity consists of three independent software modules, which are used sequentially to produce transcripts:\n\n"}
{"id": "40328965", "url": "https://en.wikipedia.org/wiki?curid=40328965", "title": "Deferribacter autotrophicus", "text": "Deferribacter autotrophicus\n\nDeferribacter autotrophicus is an iron-reducing bacteria. It is thermophilic, anaerobic, chemolithoautotrophic, motile, straight to bent rod-shaped with one polar flagellum, 0.5–0.6 µm in width and 3.0–3.5 µm in length. The type strain is SL50 (=DSM 21529 =VKPM B-10097).\n\n"}
{"id": "8409476", "url": "https://en.wikipedia.org/wiki?curid=8409476", "title": "Dirk Inzé", "text": "Dirk Inzé\n\nDirk Inzé (born 19 October 1957) is a Belgian molecular biologist and professor at Ghent University (Ghent, Belgium). In 2002, he succeeded Marc Zabeau as scientific director of the VIB Department of Plant Systems Biology. His research interest is on the molecular networks underpinning yield and organ growth both under standard as well as mild drought stress conditions in Arabidopsis and the C4 crop maize. He is a member of the European Molecular Biology Organization (EMBO). He was recipient of the 1994 Körber European Science Prize. In 2005, he was awarded the Francqui Prize on Biological and Medical Sciences for his research on plant systems biology.\n\n\n"}
{"id": "7885932", "url": "https://en.wikipedia.org/wiki?curid=7885932", "title": "Dorothy Nelkin", "text": "Dorothy Nelkin\n\nDorothy Wolfers Nelkin (– ) was an American sociologist of science most noted for her work researching and chronicling the unsettled relationship between science and society at large. Her work often drew attention to the ramifications of unchecked scientific advances and the unwariness of the public towards scientific authority. She was the author or co-author of 26 books, including \"Selling Science: How the Press Covers Science and Technology,\" \"The Molecular Gaze: Art in the Genetic Age,\" and \"Body Bazaar: The Market for Human Tissue in the Biotechnology Age.\" She was a supporter (and listed as a member of the Advisory Council, since 2013) of the National Center for Science Education, and in 1981 testified for the plaintiffs in McLean v. Arkansas. She had a broad impact in science studies, the history of science, bioethics and in the public assessment of science and technology. She was one of the founding members of the Society for the Social Studies of Science and served on governmental and other advisory boards. She often addressed the legal community, political leaders and the general public.\n\nNelkin was born on July 30, 1933, in Boston. She grew up in Brookline, Massachusetts, the daughter of Henry L. Wolfers, who founded and ran the Wolfers Lighting Company in Boston. She was the first member of her family to attend college.\n\nComing of age as a scholar in the 1960s, Nelkin was part of that generation of female scholars who saw dramatic changes in the prevailing practices of American academe. She was a faculty wife, married to the physicist Mark S Nelkin. Like many other female scholars then and now the course of her career was shaped by motherhood. She stayed home for almost a decade. She never earned any graduate degree. She rose through the academic hierarchy with a 1954 BA from the Department of Philosophy at Cornell University, and no other formal credentials. Her early book jacket covers identify her as “Mrs. Nelkin.” By the early 1970s she was a senior research associate at Cornell. She rose to the rank of University Professor at the New York University (NYU) despite holding no advanced degrees.\n\nHer work was widely cited and she received many honors, including a Guggenheim Fellowship in 1984, the John Desmond Bernal Prize of the Society for the Social Studies of Science in 1988, the John McGovern Award of the American Medical Writers Association in 1999, and election to the Institute of Medicine of the National Academy of Sciences in 1993. She was on editorial boards for journals in sociology, science studies, law, history and public health. She participated as an advisor or consultant on projects in the United States, France, Canada, Israel and Britain, on questions raised by risk assessment, privacy, science and the media, Huntington's disease, gene enhancement and data ownership.\n\nHer earliest work in science studies, on a proposed nuclear power plant, exemplified her method. In \"Nuclear Power and its Critics: The Cayuga Lake Controversy\", 1971, Nelkin analyzed the roles of technical experts and technical assessments in a dispute about a proposed nuclear plant. She tracked the perspectives of environmentalists, scientists at Cornell, utility spokesmen, electric power industry executives, New York State Department of Health officials and Ithaca residents. Nelkin suggested that by watching how technical experts engaged in public debate, one could gain insight into the values and practices of the scientific community. The strains expressed in this particular controversy reflected “not so much substantive disagreement as concern with the mode of presentation of scientific data, the appropriate behavior of scientists with respect to public issues, and the effect of publicity on the scientific dimensions of the problem,” Nelkin wrote (p. 43). Some scientists involved felt that taking a position threatened the credibility of scientists; others thought that taking a firm position was necessary. Her development of controversy studies became a productive long-term research program. She later looked at the controversy over Logan Airport, which engaged with technical interpretations of sound pollution, at the creationism controversy, at interpretations of atomic power in France and Germany, at the controversy at MIT over the Instrumentation Lab, and at many other public issues relating to technical knowledge, its application and its management.\n\nIn the course of her work on creation science, she became an expert in a highly contentious legal setting. She testified in the Arkansas creationism trial, and her work provoked a wide public response. In her 1982 book, \"The Creation Controversy: Science or Scripture in the Schools\", she noted that the creationists represented themselves as scientists (it was “creation science” at the time; it is now “intelligent design”) yet their ideas challenged the norms of science directly. In the creationism debate, science and religion proved to be intertwined in paradoxical ways, and their arguments shared some important qualities. Indeed, the general tone and structure of the debates would be recognizable to people across a broad political spectrum: The claim of technical neutrality, the discomfort of those most directly affected, the conflicting invocation of facts and details to support either side, the appeal to the press and the public, and to the legal system and legislature. Perhaps unique in its overt fusion of religion and science, the creationism controversy was nonetheless typical in its expression of tensions about local control, public participation in the assessment of science and technology, and the increasingly disputed role of expertise in public policy. “Biologists and creationists alike claim the other bases its beliefs on faith; each group argues with passion for its own dispassionate objectivity; and each bemoans the moral, political and legal implications of the alternative ideology,” she wrote.\n\nNelkin’s work on the press and public culture followed naturally from this interest in controversy. Journalists are active players in controversies surrounding science, and Nelkin became interested in how press coverage was shaped not only by scientific pronouncements but also by the internal culture of journalism, which has its own norms and practices. These practices encouraged certain kinds of reporting, for example an emphasis on breakthroughs, the lionization of great personalities, and even a resentful reaction to technological failure, particularly when press coverage of the technology involved had been so enthusiastic. In \"Selling Science: How the Press Covers Science and Technology\", she examined the selection pressures shaping coverage showing that journalists use images that express value judgments, for example about AIDS or toxic dumps, thus placing facts in an environment that implies possible solutions. The culturally loaded stories that journalists used to explain science, she suggested, reflected the norms of science journalism, a field that developed after 1945, partly in response to the importance of science and technology during the war. The notion of “facts over values” that shaped science also shaped journalism, and journalists were as invested in the sanctity of “objectivity” as any neophyte physics graduate student. Science, with its assumed reverence for facts, was therefore the model for proper journalism, and the reverence for facts “served the same purpose for journalists as it did for scientists, helping both professions maintain autonomy and independence from public control.” (87) Objectivity, of course, in journalism, generally meant adoring coverage of science and technology in terms of “cosmic breakthroughs” and “revolutionary developments.”\n\nIn her later work, over the last 14 years of her life, Nelkin focused on the cultures of biomedicine. Her 1989 book with the law and medicine scholar Lawrence Tancredi, \"Dangerous Diagnostics: The Social Power of Biological Information\" was a turning point in her intellectual trajectory. She was still interested in controversies, but she was now involved in generating them, rather than just analyzing them. Dangerous Diagnostics was intended to provoke debate about issues that had not attracted much public attention. She was becoming a much more activist scholar, someone who would take on a technological innovation widely viewed in glowing terms and suggest some of the issues that it raised. She still wrote about controversial subjects at times—her work on AIDS, for example —but her intent seems to have shifted. She was moving “inside,” not inside the science but inside the power networks, and she was becoming an independent voice in the critique of science and a major player in science policy and public assessments of technology. This was increasingly reflected in her service on policy boards and assessment panels in the United States and elsewhere.\n\nDangerous Diagnostics was a breakthrough book in other ways as well. It explored the contemporary social control purposes of medical testing, showing how new imaging and diagnostic tests extended institutional power from the arenas of work or education, into the personal lives of clients, patients, students and employees. Looking at predictive tests across a wide range of medical specialties, Nelkin and Tancredi suggested that the increasing preoccupation with testing in American society reflected intersecting tendencies of actuarial thinking and biological reductionism. Actuarial thinking encourages organizations and institutions to seek information about those they manage or hire; biological reductionism moves problems into the neutral realm of science in ways that can make values and assumptions disappear. By looking at how disciplinary norms and values intersected with diagnostic test results and institutional priorities, Nelkin and Tancredi provided a brilliant analysis that remains extremely relevant and important.\n\nHer work on DNA in American popular culture was a nod to her early work on science and the press, but again, like the book with Tancredi, it identified an issue that was not generally on the map, and elucidated problems with public imagery that had not been noticed or emphasized by others. \"The DNA Mystique: The Gene as a Cultural Icon\", co-written with the historian of science Susan Lindee, became a widely used teaching text. It brought together approaches from media studies, science studies, and sociology to consider how popular images of the gene affected legal decisions, educational practices, and social experiences of identity and relationships. Like her last two book collaborations, with Lori Andrews (\"The Body Bazaar\") and Suzanne Anker (\"The Molecular Gaze\"), it was a book about the social impact of biology with a strong policy orientation.\n\n\n\n"}
{"id": "17576886", "url": "https://en.wikipedia.org/wiki?curid=17576886", "title": "Emily Rayfield", "text": "Emily Rayfield\n\nEmily Rayfield is a British palaeontologist, who is a Professor in Palaeobiology in the School of Earth Sciences at the University of Bristol.\n\nHer research primarily focuses on the functional anatomy of extinct vertebrates, especially dinosaurs, using computational methods such as finite element analysis (FEA). In the landmark paper Rayfield \"et al.\" (2001), the skull of the theropod dinosaur \"Allosaurus\" was analysed using FEA in order to quantitatively assess different feeding hypotheses. This paper was the first use of FEA on a three-dimensional structure in palaeontology (in collaboration with CT scanning), and spurred the current trend of CT-scanned skull FEA on feeding biomechanics in zoology and palaeontology.\n\nIn addition, she helped elucidate the cranial biomechanics of the noted carnivorous dinosaur \"Tyrannosaurus\" using two-dimensional FEA. This study was expanded upon in a comparative finite element analysis of 2D theropod skulls (namely \"Allosaurus\" \"Coelophysis\" and \"Tyrannosaurus\"), in order to quantitately compare cranial biomechanics.\n"}
{"id": "57722295", "url": "https://en.wikipedia.org/wiki?curid=57722295", "title": "Explorer 45", "text": "Explorer 45\n\nExplorer 45 (also called as S-Cubed A and SSS-A) was a NASA satellite launched as part of Explorers program. Explorer 45 as launched on 15 November 1971 from the San Marco platform of the Broglio Space Center, with an Scout rocket. Explorer 45 was the only one to be released from the program Small Scientific Satellite.\n\nExplorer 45 was designed to perform a wide variety of investigations within the magnetosphere with regards to particle fluxes, electric fields, and magnetic fields. Its primary scientific objectives were:\n\n\nExplorer 45 had the capability for complete inflight control of the data format through the use of an onboard set of stored program instructions. These instructions governed the collection of data and were reprogrammable via ground command. The antenna system consisted of four dipole antennas spaced 90° apart on the surface of the spacecraft cover. The satellite contained two transmitters, one for digital (PCM) data at 446 bps, and the other for either the digital data or wideband analog data from 30 Hz to 10 kHz from the ac electric field probes and from one search coil sensor. \n\nThe satellite power system consisted of a rechargeable battery and an array of solar cells. The spin rate was about 7 rpm, and the spin axis lay in the spacecraft orbital plane which was approximately the same as the earth's equatorial plane. The initial local time of apogee was about 21.8 hours and the line of apsides moved around toward the sun at an initial rate of 12° per month. The satellite was operationally turned off on 30 September 1974, after approximately 3 years of successful and productive operation.\n\n"}
{"id": "42665086", "url": "https://en.wikipedia.org/wiki?curid=42665086", "title": "Falk Herwig", "text": "Falk Herwig\n\nFalk Herwig (born 1969) is a Canadian astrophysicist who is known for his researches at the University of Victoria. He has over 200 peer-reviewed articles which brought him an h-index of 37.\n\nIn 1998 he and another astrophysicist, Thomas Driebe, described the evolution of helium white dwarfs and two years later published his finding on evolution of convective overshooting of asymptotic giant branch stars. In 1999 he and his colleagues described what happens after the star explodes. He used the PG 1159 star as an example and proved the existence of convective overshooting.\n\n"}
{"id": "36687166", "url": "https://en.wikipedia.org/wiki?curid=36687166", "title": "Flick (physics)", "text": "Flick (physics)\n\nIn optical engineering and telecommunications engineering, the flick is a unit of spectral radiance. One flick corresponds to a spectral radiance of 1 watt per steradian per square centimeter of surface per micrometer of span in wavelength (W·sr·cm·μm). This is equivalent to 10 watts per steradian per cubic meter (W·sr·m). In practice, spectral radiance is typically measured in microflicks (10 flicks). One microflick is equivalent to 10 kilowatts per steradian per cubic meter (kW·sr·m).\n\nIn radio astronomy, the unit flik was coined by a group at Lockheed in Palo Alto, California as a substitute for the SI derived unit W cm sr µm, or watts divided by centimeters squared, steradians, and micrometers. While it started out used only in Lockheed, many in the radio astronomy field adopted its use.\n"}
{"id": "56979757", "url": "https://en.wikipedia.org/wiki?curid=56979757", "title": "Food and biological process engineering", "text": "Food and biological process engineering\n\nFood and biological process engineering is a discipline concerned with applying principles of engineering to the fields of food production and distribution and biology. It is a broad field, with workers fulfilling a variety of roles ranging from design of food processing equipment to genetic modification of organisms. In some respects it is a combined field, drawing from the disciplines of food science and biological engineering to improve the earth's food supply.\n\nCreating, processing, and storing food to support the world's population requires extensive interdisciplinary knowledge. Notably, there are many biological engineering processes within food engineering to manipulate the multitude of organisms involved in our complex food chain. Food safety in particular requires biological study to understand the microorganisms involved and how they affect humans. However, other aspects of food engineering, such as food storage and processing, also require extensive biological knowledge of both the food and the microorganisms that inhabit it. This food microbiology and biology knowledge becomes biological engineering when systems and processes are created to maintain desirable food properties and microorganisms while providing mechanisms for eliminating the unfavorable or dangerous ones.\n\nMany different concepts are involved in the field of food and biological process engineering. Below are listed several major ones.\n\nThe science behind food and food production involves studying how food behaves and how it can be improved. Researchers analyze longevity and composition (i.e., ingredients, vitamins, minerals, etc.) of foods, as well as how to ensure food safety.\n\nModern food and biological process engineering relies heavily on applications of genetic manipulation. By understanding plants and animals on the molecular level, scientists are able to engineer them with specific goals in mind.\n\nAmong the most notable applications of such genetic engineering is the creation of disease or insect resistant plants, such as those modified to produce Bacillus thuringiensis, a bacterium that kills strain-specific varieties of insect upon consumption. However, insects are able to adapt to Bacillus thuringiensis strains, necessitating continued research to maintain disease-resistance.\n\nAn important task within the realm of food safety is the elimination of microorganisms responsible for food-borne illness. Food and waterborne diseases still pose a serious health concern, with hundreds of outbreaks reported per year since 1971 in the United States alone. The risk of these diseases has risen throughout the years, mainly due to the mishandling of raw food, poor sanitation, and poor socioeconomic conditions. In addition to diseases caused by direct infection by pathogens, some food borne diseases are caused by the presence of toxins produced by microorganisms in food. There are five main types of microbial pathogens which contaminate food and water: viruses, bacteria, fungi, pathogenic protozoa and helminths.\n\nSeveral bacteria, such as E. coli, Clostridium botulinum, and Salmonella enterica, are well-known and are targeted for elimination via various industrial processes. Though bacteria are often the focus of food safety processes, viruses, protozoa, and molds are also known to cause food-borne illness and are of concern when designing processes to ensure food safety. Although the goal of food safety is to eliminate harmful organisms from food and prevent food-borne illness, detecting said organisms is another important function of food safety mechanisms.\n\nThe goal of most monitoring and detection processes is the rapid detection of harmful microorganisms with minimal interruption to the processing of food products. An example of a detection mechanism that relies heavily on biological processes is usage of chromogenic microbiological media.\n\nChromogenic microbiological media use colored enzymes to detect the presence of certain bacteria. In conventional bacteria culturing, bacteria are allowed to grow on a medium that supports many strains. Since it is hard to isolate bacteria, many cultures of different bacteria are able to form. To identify a particular bacteria culture, scientists must identify it using only its physical characteristics. Then further tests can be performed to confirm the presence of the bacteria, such as serology tests that find antibodies formed in organisms as a response to infection. In contrast, chromogenic microbiological media use particular color-producing enzymes that are targeted for metabolism by a certain strain of bacteria. Thus, if the given cultures are present, the media will become colored accordingly as the bacteria metabolize the color-producing enzyme. This greatly facilitates the identification of certain bacteria cultures and can eliminate need for further testing. To guard against misidentification of bacteria, the chromogenic plates typically incorporate additional enzymes that will be processed by other bacteria. Now, as the non-target bacteria interact with the additional enzymes, they will produce colors that distinguish them from the target bacteria.\n\nFood safety has been practiced for thousands of years, but with the rise of heavily industrial agriculture, the demand for food safety has steadily increased, prompting more research into the ways to achieve greater food safety. A primary mechanism that will be discussed in this article is heating of food products to kill microorganisms, as this has a millennia-long history and is still extensively used. However, more recent mechanisms have been created such as application of ultraviolet light, usage of ozone, and irradiation of food.\n\nA report given to the Food and Drug Administration by the Institute of Food Technologists thoroughly discusses the thermal processing of food. A notable step in development of heat application to food processing is pasteurization, developed by Louis Pasteur in the nineteenth century. Pasteurization is used to kill microorganisms that could pose risks to consumers or shorten the shelf life of food products. Primarily applied to liquid food products, pasteurization is regularly applied to fruit juice, beer, milk, and ice cream. Heat applied during pasteurization varies from around 60 °C to kill bacteria to around 80 °C to kill yeasts. Most pasteurization processes have been optimized recently to involve several steps of heating at various temperatures and minimize the time needed for the process. A more severe food heating mechanism is thermal sterilization. While pasteurization destroys most bacteria and yeast growing in food products, the goal of sterilization is to kill almost all viable organisms found in food products including yeast, mold, bacteria, and spore forming organisms. Done properly, this process will greatly extend the shelf life of food products and can allow them to be stored at room temperature. As detailed in The Handbook of Food Preservation, thermal sterilization typically involves four steps. First, food products are heated to between 110-125 °C, and the products are given time for the heat to travel through the material completely. After this, the temperature must be maintained long enough to kill microorganisms before the food product is cooled to prevent cooking. In practice, though complete sterility of food products could be achieved, the intense and extended heating needed to accomplish this could reduce the nutritive value of the food products, thus, only a partial sterilization is performed.\n\nLow-temperature processing also plays an essential role in food processing and storage. During this process, microorganisms and enzymes are subjected to low temperatures. Unlike heating, chilling does not destroy the enzymes and microorganisms but simply reduces their activity, which is effective as long as the temperature is maintained. As the temperature is raised, activity will rise again accordingly. It follows that, unlike heating, the effect of preservation by cold is not permanent; hence the importance of maintaining the \"cold chain\" throughout the shelf life of the food product. (Chapter 16 pg, 396) \n\nIt is important to note that there are two distinct low temperature processes: chilling and freezing. Chilling is the application of temperatures within the range of 0-8 °C, while freezing is usually below 18 °C. Refrigeration does slow spoilage in food and reduce the risk of bacterial growth, however, it does not improve the quality of the product.\n\nFood irradiation is another notable biological engineering process to achieve food safety. Research into the potential utilization of ionizing irradiation for food preservation started in the 1940s as an extension of studies on the effect of radiation on living cells. The FDA approved usage of ionizing radiation on food products in 1990. This radiation removes electrons from atoms, and these electrons go on to damage the DNA of microorganisms living in the food, killing the microorganisms. Irradiation can be used to pasteurize food products, such as seafood, poultry, and red meat, thus making these food products safer for consumers. Some irradiation is also used to delay fruit ripening processes, which can kill microorganisms that accelerate the ripening and spoilage of produce. Low dosages of radiation can also be used to kill insects living in harvested crops, as the radiation will stunt the insects' development at various stages and damage their ability to reproduce.\n\nFood storage and preservation is a key component of food engineering processes and relies heavily on biological engineering to understand and manipulate the organisms involved. Note that the above food safety processes such as pasteurization and sterilization destroy the microorganisms that also contribute to deterioration of food products while not necessarily posing a risk to people. Understanding of these processes, their effects, and the microorganisms at play in various food processing techniques is a very important biological engineering task within food engineering. Factories and processes must be created to ensure that food products can be processed in an efficient and effective manner, which again relies heavily on biological engineering expertise.\n\nPreservation and processing of fresh produce poses many biological engineering challenges. Understanding of biology is particularly important to processing produce because most fruits and vegetables are living organisms from the time of harvest to the time of consumption. Before harvesting, understanding of plant ontogeny, or origin and development, and the manipulation of these developmental processes are key components of the industrial agriculture process. Understanding of plant developmental cycles governs how and when plants are harvested, impacts storage environments, and contributes to creating intervention processes. Even after harvesting, fruits and vegetables undergo the biological processes of respiration, transpiration, and ripening. Control over these natural plant processes should be achieved to prevent food spoilage, sprouting or growth of produce during storage, and reduction in quality or desirability, such as through wilting or loss of desirable texture.\n\nWhen considering food storage and preservation, the technologies of modified atmosphere and controlled atmosphere are widely used for the storage and packing of several types of foods. They offer several advantages such as delay of ripening and senescence of horticultural commodities, control of some biological processes such as rancidity, insects, bacteria and decay, among others. Controlled atmosphere (CA) storage refers to atmospheres that are different than normal air and strictly controlled at all times. This type of storage manipulates the CO and O levels within airtight stores of containers. Modified atmosphere (MA) storage refers to any atmosphere different from normal air, typically made by mixing CO, O, and N\n\nAnother biological engineering process within food engineering involves the processing of agricultural waste. Though it may fall more within the realm of environmental engineering, understanding how organisms in the environment will respond to the waste products is important for assessing the impact of the processes and comparing waste processing strategies. It is also important to understand which organisms are involved in the decomposition of the waste products, and the byproducts that will be produced as a result of their activity.\n\nTo discuss direct application of biological engineering, biological waste processing techniques are used to process organic waste and sometimes create useful byproducts. There are two main processes by which organic matter is processed via microbes: aerobic processes and anaerobic processes. These processes convert organic matter to cell mass through synthesis processes of microorganisms. Aerobic processes occur in the presence of oxygen, take organic matter as input, and produce water, carbon dioxide, nitrate, and new cell mass. Anaerobic processes occur in the absence of oxygen and produce less cell mass than aerobic processes. An additional benefit of anaerobic processes is that they also generate methane, which can be burned as a fuel source. Design of both aerobic and anaerobic biological waste processing plants requires careful control of temperature, humidity, oxygen concentration, and the waste products involved. Understanding of all aspects of the system and how they interact with one another is important for developing efficient waste management plants and falls within the realm of biological engineering.\n\n\n"}
{"id": "32002131", "url": "https://en.wikipedia.org/wiki?curid=32002131", "title": "Gait Analysis: Normal and Pathological Function", "text": "Gait Analysis: Normal and Pathological Function\n\nGait Analysis: Normal and Pathological Function is a textbook that focuses on human gait analysis and is written by Drs. Jacquelin Perry and Judith M. Burnfield. It is an updated and revised version of \"Gait Analysis: Normal and Pathological Function\" (1992), a text many consider to be a staple for the curriculum of education of gait analysis. It is frequently cited in academic publications as well as journals for orthopedics, physical therapy and athletic training.\n\n\n"}
{"id": "5204415", "url": "https://en.wikipedia.org/wiki?curid=5204415", "title": "Hubertus Strughold", "text": "Hubertus Strughold\n\nHubertus Strughold (June 15, 1898 – September 25, 1986) was a German-born physiologist and prominent medical researcher. Beginning in 1935 he served as chief of aeromedical research for the Luftwaffe, holding this position throughout World War II. In 1947 he was brought to the United States as part of Operation Paperclip and held a series of high-ranking medical positions with both the US Air Force and NASA.\n\nFor his role in pioneering the study of the physical and psychological effects of manned spaceflight he became known as \"The Father of Space Medicine\". Following his death, Strughold's activities in Germany during World War II came under greater scrutiny and allegations surrounding his involvement in Nazi-era human experimentation greatly diminished his reputation.\n\nStrughold was born in the town of Westtünnen-im-Hamm in the Prussian province of Westphalia on 15 June 1898. As a young man he studied medicine and the natural sciences at the Ludwig Maximilian University of Munich and the Georg August University of Göttingen, where he received his doctorate (Dr. med. et phil.) in 1922. He later went on to obtain his medical degree (Dr. med.) from the University of Münster and completed his habilitation (Dr. habil.) at the Julius Maximilian University of Würzburg in 1927. Strughold also worked as a research assistant to the renowned German-Austrian physiologist Dr. Maximilian von Frey. He remained at Würzburg and pursued a career as a professor of physiology.\n\nDuring this time Strughold's attention was increasingly drawn to the emerging science of aviation medicine and he collaborated with the famed World War I pilot Robert Ritter von Greim to study the effects of high-altitude flight on human biology. In 1928 Strughold traveled to the United States on a year-long research fellowship from the Rockefeller Foundation. He conducted specialized studies into aviation medicine and human physiology at the University of Chicago and Case Western Reserve University in Cleveland, Ohio. He would also visit the medical laboratories at Harvard, Columbia and the Mayo Clinic. Strughold returned to Germany the following year and accepted a teaching position at the Würzburg Physiological Institute, eventually becoming an adjunct professor there in 1933. He would later serve as a professor of physiology at Friedrich Wilhelm University in Berlin.\n\nThrough his association with von Greim (now Adolf Hitler's personal pilot) Strughold became acquainted socially with various high-ranking members of the Nazi regime and in April, 1935 he was appointed Director of the Berlin-based \"Research Institute for Aviation Medicine\", a medical think tank that operated under the auspices of Hermann Göring's Ministry of Aviation. Under Strughold's leadership the Institute grew to become Germany's foremost aeromedical research establishment, pioneering the study of the medical effects of high-altitude and supersonic speed flight along with establishing the altitude chamber concept of \"time of useful consciousness\". Though Strughold was ostensibly a civilian researcher, the majority of the studies and projects his Institute undertook were commissioned and financed by the German armed forces (principally the Luftwaffe) as part of the ongoing German re-armament. With the outbreak of World War II in 1939, Strughold's organization was absorbed into the Luftwaffe itself and was attached its medical service. It was renamed the \"Air Force Institute for Aviation Medicine\", and placed under the command of Luftwaffe Surgeon-General (\"Generaloberstabsarzt\") Erich Hippke. Strughold himself was also commissioned as an officer in the German air force, eventually rising to the rank of Colonel (\"Oberst\").\n\nIn October 1942, Strughold and Hippke attended a medical conference in Nuremberg at which SS physician Sigmund Rascher delivered a presentation outlining various medical experiments he had conducted, in conjunction with the Luftwaffe, in which prisoners from the Dachau concentration camp were used as human test subjects. These experiments included physiological tests during which camp inmates were immersed in freezing water, placed in air pressure chambers and made to endure invasive surgical procedures without anesthetic. Many of the inmates forced to participate died as a result. Various Luftwaffe physicians had participated in the experiments and several of them had close ties to Strughold, both through the Institute for Aviation Medicine and the Luftwaffe Medical Corps.\n\nFollowing the German defeat in May, 1945, Strughold claimed to Allied authorities that, despite his influential position within the Luftwaffe Medical Service and his attendance at the October 1942 medical conference, he had no knowledge of the atrocities committed at Dachau. He was never subsequently charged with any wrongdoing by the Allies. However, a 1946 memorandum produced by the staff of the Nuremberg Trials listed Strughold as one of thirteen \"persons, firms or individuals implicated\" in the war crimes committed at Dachau. Also, several of the former Luftwaffe physicians associated with Strughold and the Institute for Aviation Medicine (among them Strughold's former research assistant Hermann Becker-Freyseng) were convicted of crimes against humanity in connection with the Dachau experiments at the 1947 Nuremberg Doctor's Trial. During these proceedings, Strughold contributed several affidavits for the defense on behalf of his accused colleagues.\n\nIn October, 1945 Strughold returned to academia, becoming director of the Physiological Institute at Heidelberg University. He also began working on behalf of the US Army Air Force, becoming Chief Scientist of its Aeromedical Center, located on the campus of the former Kaiser Wilhelm Institute for Medical Research. In this capacity Strughold edited \"German Aviation Medicine in World War II\", a book-length summary of the knowledge gained by German aviation researchers during the war.\n\nIn 1947 Strughold was brought to the United States, along with many other highly valuable German scientists, as part of Operation Paperclip. With another former Luftwaffe physician, Richard Lindenberg, Strughold was assigned to the US Air Force School of Aviation Medicine at Randolph Field near San Antonio, Texas. It was while at Randolph Field that Strughold began conducting some of the first research into the potential medical challenges posed by space travel, in conjunction with fellow \"Paperclip Scientist\" Dr. Heinz Haber. Strughold coined the terms \"space medicine\" and \"astrobiology\" to describe this area of study in 1948. The following year he was appointed as the first and only Professor of Space Medicine at the US Air Force's newly established School of Aviation Medicine (SAM), one of the first institutions dedicated to conducting research on \"astrobiology\" and the so-called \"human factors\" associated with manned spaceflight.\n\nUnder Strughold, the School of Aviation Medicine conducted pioneering studies on issues such as atmospheric control, the physical effects of weightlessness and the disruption of normal time cycles. In 1951 Strughold revolutionized existing notions concerning spaceflight when he co-authored the influential research paper \"Where Does Space Begin?\" in which he proposed that space was present in small gradations that grew as altitude levels increased, rather than existing in remote regions of the atmosphere. Between 1952 and 1954 he would oversee the building of the space cabin simulator, a sealed chamber in which human test subjects were placed for extended periods of time in order to view the potential physical, astrobiological, and psychological effects of extra-atmospheric flight.\n\nStrughold obtained US citizenship in 1956 and was appointed Chief Scientist of the National Aeronautics and Space Administration's (NASA) Aerospace Medical Division in 1962. While at NASA, Strughold played a central role in designing the pressure suit and onboard life support systems used by both the Gemini and Apollo astronauts. He also directed the specialized training of the flight surgeons and medical staff of the Apollo program in advance of the planned mission to the Moon. Strughold retired from his position at NASA in 1968.\n\nDuring his work on behalf of the Air Force and NASA, Strughold was the subject of three separate US government investigations into his suspected involvement in war crimes committed under the Nazis. A 1958 investigation by the Justice Department fully exonerated Strughold, while a second inquiry launched by the Immigration and Naturalization Service in 1974 was later abandoned due to lack of evidence. In 1983 the Justice Department's Office of Special Investigations reopened his case but withdrew from the effort when Strughold died in September, 1986.\n\nFollowing his death, Strughold's alleged connection to the Dachau experiments became more widely known following the release of US Army Intelligence documents from 1945 that listed him among those being sought as war criminals by US authorities. These revelations did significant damage to Strughold's reputation and resulted in the revocation of various honors that had been bestowed upon him over the course of his career. In 1993, at the request of the World Jewish Congress, his portrait was removed from a mural of prominent physicians displayed at Ohio State University. Following similar protests by the Anti-Defamation League (ADL), the Air Force decided in 1995 to rename the Hubertus Strughold Aeromedical Library at Brooks Air Force Base, which had been named in Strughold's honor in 1977. His portrait, however, still hangs there. Further action by the ADL also led to Strughold's removal from the International Space Hall of Fame in Alamogordo, New Mexico in May 2006.\n\nFurther questions about Strughold's activities during World War II emerged in 2004 following an investigation conducted by the Historical Committee of the German Society of Air and Space Medicine. The inquiry uncovered evidence of oxygen deprivation experiments carried out by Strughold's Institute for Aviation Medicine in 1943. According to these findings six epileptic children, between the ages of 11 and 13, were taken from the Nazi's Brandenburg Euthanasia Centre to Strughold's Berlin laboratory where they were placed in vacuum chambers to induce epileptic seizures in an effort to simulate the effects of high-altitude sicknesses, such as hypoxia. While, unlike the Dachau experiments, all the test subjects survived the research process, this revelation led the Society of Air and Space Medicine to abolish a major award bearing Strughold's name. A similar campaign by American scholars prompted the US branch of the Aerospace Medical Association to announce in 2012 that it would also consider rechristening a similar award, also named in Strughold's honor, which it had been bestowing since 1963. The move was met with opposition from defenders of Strughold, citing his massive contributions to the American space program and the lack of any formal proof of his direct involvement in war crimes.\n\nKnown as \"The Father of Space Medicine\"\n\nThe Hubertus Strughold Award was established by the Space Medicine Branch, known today as the Space Medicine Association, a member organization of the Aerospace Medical Association. In 1962 the Award was established in honor of Dr. Hubertus Strughold, also known as \"The Father of Space Medicine\". The award was presented every year from 1963 through 2012 to a Space Medicine Branch member for outstanding contributions in applications and research in the field of space-related medical research.\n\n\n\n\n\n\n\n\n\n"}
{"id": "545825", "url": "https://en.wikipedia.org/wiki?curid=545825", "title": "Impulse response", "text": "Impulse response\n\nIn signal processing, the impulse response, or impulse response function (IRF), of a dynamic system is its output when presented with a brief input signal, called an impulse. More generally, an impulse response is the reaction of any dynamic system in response to some external change. In both cases, the impulse response describes the reaction of the system as a function of time (or possibly as a function of some other independent variable that parameterizes the dynamic behavior of the system).\n\nIn all these cases, the dynamic system and its impulse response may be actual physical objects, or may be mathematical systems of equations describing such objects.\n\nSince the impulse function contains all frequencies, the impulse response defines the response of a linear time-invariant system for all frequencies.\n\nMathematically, how the impulse is described depends on whether the system is modeled in discrete or continuous time. The impulse can be modeled as a Dirac delta function for continuous-time systems, or as the Kronecker delta for discrete-time systems. The Dirac delta represents the limiting case of a pulse made very short in time while maintaining its area or integral (thus giving an infinitely high peak). While this is impossible in any real system, it is a useful idealisation. In Fourier analysis theory, such an impulse comprises equal portions of all possible excitation frequencies, which makes it a convenient test probe.\n\nAny system in a large class known as \"linear, time-invariant\" (LTI) is completely characterized by its impulse response. That is, for any input, the output can be calculated in terms of the input and the impulse response. (See LTI system theory.) The impulse response of a linear transformation is the image of Dirac's delta function under the transformation, analogous to the fundamental solution of a partial differential operator.\n\nIt is usually easier to analyze systems using transfer functions as opposed to impulse responses. The transfer function is the Laplace transform of the impulse response. The Laplace transform of a system's output may be determined by the multiplication of the transfer function with the input's Laplace transform in the complex plane, also known as the frequency domain. An inverse Laplace transform of this result will yield the output in the time domain.\n\nTo determine an output directly in the time domain requires the convolution of the input with the impulse response. When the transfer function and the Laplace transform of the input are known, this convolution may be more complicated than the alternative of multiplying two functions in the frequency domain.\n\nThe impulse response, considered as a Green's function, can be thought of as an \"influence function\": how a point of input influences output.\n\nIn practical systems, it is not possible to produce a perfect impulse to serve as input for testing; therefore, a brief pulse is sometimes used as an approximation of an impulse. Provided that the pulse is short enough compared to the impulse response, the result will be close to the true, theoretical, impulse response. In many systems, however, driving with a very short strong pulse may drive the system into a nonlinear regime, so instead the system is driven with a pseudo-random sequence, and the impulse response is computed from the input and output signals.\n\nAn application that demonstrates this idea was the development of impulse response loudspeaker testing in the 1970s. Loudspeakers suffer from phase inaccuracy, a defect unlike other measured properties such as frequency response. Phase inaccuracy is caused by (slightly) delayed frequencies/octaves that are mainly the result of passive cross overs (especially higher order filters) but are also caused by resonance, energy storage in the cone, the internal volume, or the enclosure panels vibrating. Measuring the impulse response, which is a direct plot of this \"time-smearing,\" provided a tool for use in reducing resonances by the use of improved materials for cones and enclosures, as well as changes to the speaker crossover. The need to limit input amplitude to maintain the linearity of the system led to the use of inputs such as pseudo-random maximum length sequences, and to the use of computer processing to derive the impulse response.\n\nImpulse response analysis is a major facet of radar, ultrasound imaging, and many areas of digital signal processing. An interesting example would be broadband internet connections. DSL/Broadband services use adaptive equalisation techniques to help compensate for signal distortion and interference introduced by the copper phone lines used to deliver the service.\n\nIn control theory the impulse response is the response of a system to a Dirac delta input. This proves useful in the analysis of dynamic systems; the Laplace transform of the delta function is 1, so the impulse response is equivalent to the inverse Laplace transform of the system's transfer function.\n\nIn acoustic and audio applications, impulse responses enable the acoustic characteristics of a location, such as a concert hall, to be captured. Various packages are available containing impulse responses from specific locations, ranging from small rooms to large concert halls. These impulse responses can then be utilized in convolution reverb applications to enable the acoustic characteristics of a particular location to be applied to target audio.\n\nIn economics, and especially in contemporary macroeconomic modeling, impulse response functions are used to describe how the economy reacts over time to exogenous impulses, which economists usually call shocks, and are often modeled in the context of a vector autoregression. Impulses that are often treated as exogenous from a macroeconomic point of view include changes in government spending, tax rates, and other fiscal policy parameters; changes in the monetary base or other monetary policy parameters; changes in productivity or other technological parameters; and changes in preferences, such as the degree of impatience. Impulse response functions describe the reaction of endogenous macroeconomic variables such as output, consumption, investment, and employment at the time of the shock and over subsequent points in time. Recently, asymmetric impulse response functions have been suggested in the literature that separate the impact of a positive shock from a negative one. \n\n"}
{"id": "813041", "url": "https://en.wikipedia.org/wiki?curid=813041", "title": "International HapMap Project", "text": "International HapMap Project\n\nThe International HapMap Project was an organization that aimed to develop a haplotype map (HapMap) of the human genome, to describe the common patterns of human genetic variation. HapMap is used to find genetic variants affecting health, disease and responses to drugs and environmental factors. The information produced by the project is made freely available for research.\n\nThe International HapMap Project is a collaboration among researchers at academic centers, non-profit biomedical research groups and private companies in Canada, China, Japan, Nigeria, the United Kingdom, and the United States. It officially started with a meeting on October 27 to 29, 2002, and was expected to take about three years. It comprises two phases; the complete data obtained in Phase I were published on 27 October 2005. The analysis of the Phase II dataset was published in October 2007. The Phase III dataset was released in spring 2009.\n\nUnlike with the rarer Mendelian diseases, combinations of different genes and the environment play a role in the development and progression of common diseases (such as diabetes, cancer, heart disease, stroke, depression, and asthma), or in the individual response to pharmacological agents. To find the genetic factors involved in these diseases, one could in principle do a genome-wide association study: obtain the complete genetic sequence of several individuals, some with the disease and some without, and then search for differences between the two sets of genomes. At the time, this approach was not feasible because of the cost of full genome sequencing. The HapMap project proposed a shortcut.\n\nAlthough any two unrelated people share about 99.5% of their DNA sequence, their genomes differ at specific nucleotide locations. Such sites are known as single nucleotide polymorphisms (SNPs), and each of the possible resulting gene forms is called an allele. The HapMap project focuses only on common SNPs, those where each allele occurs in at least 1% of the population.\n\nEach person has two copies of all chromosomes, except the sex chromosomes in males. For each SNP, the combination of alleles a person has is called a genotype. Genotyping refers to uncovering what genotype a person has at a particular site. The HapMap project chose a sample of 269 individuals and selected several million well-defined SNPs, genotyped the individuals for these SNPs, and published the results.\n\nThe alleles of nearby SNPs on a single chromosome are correlated. Specifically, if the allele of one SNP for a given individual is known, the alleles of nearby SNPs can often be predicted. This is because each SNP arose in evolutionary history as a single point mutation, and was then passed down on the chromosome surrounded by other, earlier, point mutations. SNPs that are separated by a large distance on the chromosome are typically not very well correlated, because recombination occurs in each generation and mixes the allele sequences of the two chromosomes. A sequence of consecutive alleles on a particular chromosome is known as a haplotype.\n\nTo find the genetic factors involved in a particular disease, one can proceed as follows. First a certain region of interest in the genome is identified, possibly from earlier inheritance studies. In this region one locates a set of tag SNPs from the HapMap data; these are SNPs that are very well correlated with all the other SNPs in the region. Thus, learning the alleles of the tag SNPs in an individual will determine the individual's haplotype with high probability. Next, one determines the genotype for these tag SNPs in several individuals, some with the disease and some without. By comparing the two groups, one determines the likely locations and haplotypes that are involved in the disease.\n\nHaplotypes are generally shared between populations, but their frequency can differ widely. Four populations were selected for inclusion in the HapMap: 30 adult-and-both-parents Yoruba trios from Ibadan, Nigeria (YRI), 30 trios of Utah residents of northern and western European ancestry (CEU), 44 unrelated Japanese individuals from Tokyo, Japan (JPT) and 45 unrelated Han Chinese individuals from Beijing, China (CHB). Although the haplotypes revealed from these populations should be useful for studying many other populations, parallel studies are currently examining the usefulness of including additional populations in the project.\n\nAll samples were collected through a community engagement process with appropriate informed consent. The community engagement process was designed to identify and attempt to respond to culturally specific concerns and give participating communities input into the informed consent and sample collection processes.\n\nIn phase III, 11 global ancestry groups have been assembled: ASW (African ancestry in Southwest USA); CEU (Utah residents with Northern and Western European ancestry from the CEPH collection); CHB (Han Chinese in Beijing, China); CHD (Chinese in Metropolitan Denver, Colorado); GIH (Gujarati Indians in Houston, Texas); JPT (Japanese in Tokyo, Japan); LWK (Luhya in Webuye, Kenya); MEX (Mexican ancestry in Los Angeles, California); MKK (Maasai in Kinyawa, Kenya); TSI (Tuscans in Italy); YRI (Yoruba in Ibadan, Nigeria).\n\nThree combined panels have also been created, which allow better identification of SNPs in groups outside the nine homogenous samples: CEU+TSI (Combined panel of Utah residents with Northern and Western European ancestry from the CEPH collection and Tuscans in Italy); JPT+CHB (Combined panel of Japanese in Tokyo, Japan and Han Chinese in Beijing, China) and JPT+CHB+CHD (Combined panel of Japanese in Tokyo, Japan, Han Chinese in Beijing, China and Chinese in Metropolitan Denver, Colorado). CEU+TSI, for instance, is a better model of UK British individuals than is CEU alone.\n\nFor the Phase I, one common SNP was genotyped every 5,000 bases. Overall, more than one million SNPs were genotyped. The genotyping was carried out by 10 centres using five different genotyping technologies. Genotyping quality was assessed by using duplicate or related samples and by having periodic quality checks where centres had to genotype common sets of SNPs.\n\nThe Canadian team was led by Thomas J. Hudson at McGill University in Montreal and focused on chromosomes 2 and 4p. The Chinese team was led by Huanming Yang with centres in Beijing, Shanghai and Hong Kong and focused on chromosomes 3, 8p and 21. The Japanese team was led by Yusuke Nakamura at the University of Tokyo and focused on chromosomes 5, 11, 14, 15, 16, 17 and 19. The British team was led by David R. Bentley at the Sanger Institute and focused on chromosomes 1, 6, 10, 13 and 20. There were four United States' genotyping centres: a team led by Mark Chee and Arnold Oliphant at Illumina Inc. in San Diego (studying chromosomes 8q, 9, 18q, 22 and X), a team led by David Altshuler and Mark Daly at the Broad Institute in Cambridge, USA (chromosomes 4q, 7q, 18p, Y and mitochondrion), a team led by Richard Gibbs at the Baylor College of Medicine in Houston (chromosome 12), and a team led by Pui-Yan Kwok at the University of California, San Francisco (chromosome 7p).\n\nTo obtain enough SNPs to create the Map, the Consortium funded a large re-sequencing project to discover millions of additional SNPs. These were submitted to the public dbSNP database. As a result, by August 2006, the database included more than ten million SNPs, and more than 40% of them were known to be polymorphic. By comparison, at the start of the project, fewer than 3 million SNPs were identified, and no more than 10% of them were known to be polymorphic.\n\nDuring Phase II, more than two million additional SNPs have been genotyped throughout the genome by the company Perlegen Sciences and 500,000 by the company Affymetrix.\n\nAll of the data generated by the project, including SNP frequencies, genotypes and haplotypes, were placed in the public domain and are available for download. This website also contains a genome browser which allows to find SNPs in any region of interest, their allele frequencies and their association to nearby SNPs. A tool that can determine tag SNPs for a given region of interest is also provided. These data can also be directly accessed from the widely used Haploview program.\n\n\n\n"}
{"id": "33298861", "url": "https://en.wikipedia.org/wiki?curid=33298861", "title": "Iron Science Teacher", "text": "Iron Science Teacher\n\nThe Iron Science Teacher is a national competition that celebrates innovation and creativity in science teaching. The competition originated at the Exploratorium in San Francisco. Parodying the cult Japanese TV program, “Iron Chef,” this competition showcases science teachers as they devise classroom activities using a particular ingredient — an everyday item such as a plastic bag, milk carton, or nail. Contestants are currently or formally part of the Exploratorium's Teacher Institute and compete before a live audience for the title of \"Iron Science Teacher.\" Shows are also archived on the Exploratorium's site.\n\nAstrophysicist Dr. Linda Shore, Director of the Exploratorium Teacher Institute and host of the competition, says one goal of the Iron Science Teacher is to \"provide teachers with ideas about how to teach multimillion dollar state and national science teaching standards using, trash, recyclables, and inexpensive materials\" as well as \"to allow teachers to receive applause for great teaching.\"\n\nBack in 1997, the Exploratorium's Phyllis C. Wattis Webcast Studio was looking for new shows. During a staff brainstorming session, a fan of the popular Food Network television show, The Iron Chef, suggested naming a secret ingredient for science teachers to use in an experiment to present to the audience. \"It was honestly and truly a joke,\" Shore says. \"We thought we'd do one show.\"\n\nNow 10 to 12 shows are produced annually for the Exploratorium's website. \"Secret\" ingredients, which are revealed in advance to participants so they can practice, have included everything from ordinary baking soda and food coloring to Marshmallow Peeps and pantyhose.\n\nThe Canadian Iron Science Teacher also parodies the popular TV series Iron Chef and is hosted by Jay Ingram of Daily Planet on Discovery Channel. Unlike the Exploratorium version, where championship comes with no tangible prize, in the Canadian version, five \"finalist\" teachers, with their support teams, are selected to compete in the Iron Science Teacher finals at the University of Calgary in order to win a variety of cash prizes. \n\nColorado Springs, CO initiated their own CoOL Iron Science Teacher Competition as part of their What If: A Festival of Creativity & Innovation on September 11, 2010.\n\n\"(plus links to the webcasts)\"\n"}
{"id": "53843899", "url": "https://en.wikipedia.org/wiki?curid=53843899", "title": "Jerome Lewis Duggan", "text": "Jerome Lewis Duggan\n\nJerome Lewis Duggan from the University of North Texas, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Forum on Industrial and Applied Physics in 2000, for \"outstanding contributions in the application of low energy nuclear technology for analysis in the semiconductor, metals, and geophysics industries, and for initiating an international conference as a forum for the interaction of industrial and academic physicists.\"\n"}
{"id": "21756219", "url": "https://en.wikipedia.org/wiki?curid=21756219", "title": "List of HTTP status codes", "text": "List of HTTP status codes\n\nThis is a list of Hypertext Transfer Protocol (HTTP) response status codes. Status codes are issued by a server in response to a client's request made to the server. It includes codes from IETF Request for Comments (RFCs), other specifications, and some additional codes used in some common applications of the Hypertext Transfer Protocol (HTTP). The first digit of the status code specifies one of five standard classes of responses. The message phrases shown are typical, but any human-readable alternative may be provided. Unless otherwise stated, the status code is part of the HTTP/1.1 standard (RFC 7231).\n\nThe Internet Assigned Numbers Authority (IANA) maintains the official registry of HTTP status codes.\n\nMicrosoft Internet Information Services (IIS) sometimes uses additional decimal sub-codes for more specific information, however these sub-codes only appear in the response payload and in documentation, not in the place of an actual HTTP status code.\n\nAll HTTP response status codes are separated into five classes (or categories). The first digit of the status code defines the class of response. The last two digits do not have any class or categorization role. There are five values for the first digit:\n\nAn informational response indicates that the request was received and understood. It is issued on a provisional basis while request processing continues. It alerts the client to wait for a final response. The message consists only of the status line and optional header fields, and is terminated by an empty line. As the HTTP/1.0 standard did not define any 1xx status codes, servers \"must not\" send a 1xx response to an HTTP/1.0 compliant client except under experimental conditions.\n\n\nThis class of status codes indicates the action requested by the client was received, understood and accepted.\n\n\nThis class of status code indicates the client must take additional action to complete the request. Many of these status codes are used in URL redirection.\n\nA user agent may carry out the additional action with no user interaction only if the method used in the second request is GET or HEAD. A user agent may automatically redirect a request. A user agent should detect and intervene to prevent cyclical redirects.\n\n\nThis class of status code is intended for situations in which the error seems to have been caused by the client. Except when responding to a HEAD request, the server \"should\" include an entity containing an explanation of the error situation, and whether it is a temporary or permanent condition. These status codes are applicable to any request method. User agents \"should\" display any included entity to the user.\n\n\n\nThe server failed to fulfil a request.\n\nResponse status codes beginning with the digit \"5\" indicate cases in which the server is aware that it has encountered an error or is otherwise incapable of performing the request. Except when responding to a HEAD request, the server \"should\" include an entity containing an explanation of the error situation, and indicate whether it is a temporary or permanent condition. Likewise, user agents \"should\" display any included entity to the user. These response codes are applicable to any request method.\n\n\nThe following codes are not specified by any standard.\n\nMicrosoft's Internet Information Services web server expands the 4xx error space to signal errors with the client's request.\n\nThe nginx web server software expands the 4xx error space to signal issues with the client's request. \n\nCloudflare's reverse proxy service expands the 5xx series of errors space to signal issues with the origin server.\n\n\n\n"}
{"id": "32127622", "url": "https://en.wikipedia.org/wiki?curid=32127622", "title": "List of Samsung System on Chips", "text": "List of Samsung System on Chips\n\nSamsung has a long history of designing and producing systems on chip (SoCs) and has been manufacturing SoCs for its own devices as well as for sale to other manufacturers. The first Samsung SoC, the \"S3C44B0\", was built around an ARM7 CPU which operated at 66 MHz clock frequency. Later, several SoCs (S3C2xxx) containing an ARM9 CPU were produced. For more information on Samsung's current SoCs see Exynos.\n"}
{"id": "2851112", "url": "https://en.wikipedia.org/wiki?curid=2851112", "title": "List of U.S. state foods", "text": "List of U.S. state foods\n\nThis is a list of official U.S. state foods:\n\n\n"}
{"id": "14485401", "url": "https://en.wikipedia.org/wiki?curid=14485401", "title": "List of members of the National Academy of Sciences (Social and political sciences)", "text": "List of members of the National Academy of Sciences (Social and political sciences)\n"}
{"id": "37315646", "url": "https://en.wikipedia.org/wiki?curid=37315646", "title": "List of virus families and subfamilies", "text": "List of virus families and subfamilies\n\nThis is an alphabetical list of biological virus families and subfamilies; it includes those families and subfamilies listed by the ICTV 2018 report.\n\n\n\n"}
{"id": "19822072", "url": "https://en.wikipedia.org/wiki?curid=19822072", "title": "Macrocognition", "text": "Macrocognition\n\nMacrocognition indicates a descriptive level of cognition performed in natural instead of artificial (laboratory) environments. This term is reported to have been coined by Pietro Cacciabue and Erik Hollnagel in 1995. However, it is also reported that the term was used in the 1980s in European Cognitive Systems Engineering research. Possibly the earliest reference is the following, although it does not use the exact term \"macrocognition\":\n\nA macro-theory is a theory which is concerned with the obvious regularities of human experience, rather than with some theoretically defined unit. To refer to another psychological school, it would correspond to a theory at the level of Gestalten. It resembles Newell’s suggestion for a solution that would analyse more complex tasks, although the idea of a macro-theory does not entail an analysis of the mechanistic materialistic kind which is predominant in cognitive psychology. Thus we should have a macro-theory of remembering rather than of memory, to say nothing of short-term memory, proactive inhibition release, or memory scanning. To take another example, we should have a macro-theory of attending, rather than a mini-theory of attention, or micro-theories of limited channel capacities or logarithmic dependencies in disjunctive reaction times. This would ease the dependence on the information processing analogy, but not necessarily lead to an abandonment of the information processing terminology, the Flowchart, or the concept of control structures. The meta-technical sciences can contribute to a psychology of cognition as well as to cognitive psychology. What should be abandoned is rather the tendency to think in elementaristic terms and to increase the plethora of mini-and micro-theories.\nTo conclude, if the psychological study of cognition shall have a future that is not a continued description of human information processing, its theories must be at what we have called the macro-level. This means that they must correspond to the natural units of experience and consider these in relation to the regularities of human experience, rather than as manifestations of hypothetical information processing mechanisms in the brain. A psychology should start at the level of natural units in human experience and try to work upwards towards the level of functions and human action, rather than downwards towards the level of elementary information processes and the structure of the IPS.\nThe use of the term suggests that there is strong evidence in which naturalistic decision-making and the environments in which they occur are navigated in cognitively different ways than artificial or controlled environments.\n\nMacrocognition is distinguished from microcognition by elements of time-pressure and risk, performance by experts (as opposed to college students or novices), ambiguity of goals and outcomes, and complex and unclear conditions.\n\n"}
{"id": "24849570", "url": "https://en.wikipedia.org/wiki?curid=24849570", "title": "Molecular Biology of the Cell (textbook)", "text": "Molecular Biology of the Cell (textbook)\n\nMolecular Biology of the Cell is a cellular and molecular biology textbook published by Garland Science and currently authored by Bruce Alberts, Alexander D. Johnson, Julian Lewis (deceased), David Morgan, Martin Raff, Keith Roberts and Peter Walter. The book was first published in 1983 and is now in its sixth edition. The molecular biologist James Watson contributed to the first three editions.\n\n\"Molecular Biology of the Cell\" is widely used in introductory courses at the university level, being considered as a reference in many libraries and laboratories around the world. It describes the current understanding of cell biology and includes basic biochemistry, experimental methods for investigating cells, the properties common to most eukaryotic cells, the expression and transmission of genetic information, the internal organization of cells, and the behaviour of cells in multicellular organisms. \"Molecular Biology of the Cell\" has been described as “the most influential cell biology textbook of its time”. The sixth edition is dedicated to the memory of co-author Julian Lewis who died in early 2014.\n\n"}
{"id": "24210650", "url": "https://en.wikipedia.org/wiki?curid=24210650", "title": "NASA 360", "text": "NASA 360\n\nNASA 360 is a half-hour vodcast developed by NASA in partnership with the National Institute of Aerospace. The show premiered in August 2008. It has aired on more than 450 TV stations across the country, is available on air and cruise lines, and is consistently one of the top-downloaded programs on the NASA.gov website. It is currently in its tenth season.\n\n\"NASA 360\" is one of four programs in NASA's award-winning eClips suite of web-based shows designed to encourage careers in science, technology, engineering, and mathematics. \"NASA 360\" is written, produced, and edited by Timothy J. Allen, Tom Shortridge, and Scott Bednar; Rebecca Jaramillo is the Senior Educator and Project Coordinator for \"NASA 360\", and Harla Sherwood the Principal Investigator - all of the National Institute of Aerospace.\n\n\"NASA 360\" shows how NASA has changed and continues to change life on Earth by examining how technologies developed by or for NASA are being used in everything from space exploration to everyday consumer products. These include lithium ion batteries, medical innovations, sporting equipment, and automotive and aircraft safety and efficiency, among many more.\n\n\"NASA 360\" is shot on-location at NASA centers across the country, as well as at other relevant sites across the globe. The fifth season marked the revitalization of \"NASA 360\" and features new hosts Caleb Kinchlow and Molly McKinney, B-roll, animations, and interviews conducted with NASA researchers, engineers, and astronauts, as well as with outside sources with expertise relevant to the topics being discussed.\n\nThe show is produced for a young adult audience, and stylistically this is accomplished through the use of hand-held cameras, quick edits, and numerous transitions, effects, and filters used in post-production.\n\n\"NASA 360\" was originally created in 2006 by producers Kevin Krigsvold and Michael Bibbo. It was hosted by Johnny Alonso and Jennifer Pulley through 2012. 23 episodes were produced during this period.\n\nIn addition to reaching millions of viewers of traditional broadcast on over 450 stations in the U.S., including every major market, \"NASA 360\" also has a strong presence on the internet, including more than 5.4 million Facebook fans. It is also available from numerous outlets, including Hulu.com, iTunes, YouTube, Miro Guide, AOL Video, Red Orbit, and Truveo.\n\n\"NASA 360\" is also available on Hulu.com. Hulu along with Hulu Plus has 29 million unique viewers every month.\n\n\"NASA 360\" is an active member of the social networking communities at Twitter and Facebook with more than 103,000 social media followers and fans.\n\n\"NASA 360\"s \"Backstage\" photo gallery averages about 5,000 views per week.\n\n\"NASA 360\" won a Capital Chesapeake Bay Emmy Award on June 15, 2013 for the program, \"\"NASA 360\": Robots, Rocks & Rovers.\" The same episode, which highlights NASA's Sample Return Robot Centennial Challenge also garnered two 2013 Telly awards in the Government and Instructional/Public Outreach categories.\n\nIn June 2011, former Director/Editor Michael Bibbo was nominated in the single camera editing category for the National Daytime Emmy Awards.,\n\n\"NASA 360\" has won numerous other awards, including four (4) Communicator Awards for overall program and editing, two (2) Omni Awards for overall program and editing, two (2) Davey Awards for overall program and editing, two Marcom Awards, and two Ava Awards, as well as two (2) Videographer awards, four additional (4) Telly awards (including the 30th Anniversary Telly Award for Overall Program and Editing), and two (2) EMPIXX awards.\n\nIn 2010, former Director and first camera operator Michael Bibbo and 2nd camera operator, now Producer, Tom Shortridge won the 2nd place award for NASA Videographer of the Year in the production category.\n\nOn June 6, 2009, \"NASA 360\" won the Emmy for non-news program editing from the National Capital Chesapeake Bay Chapter of the National Academy of Television Arts and Sciences, which includes 29 media outlets in Washington D.C., Virginia and Maryland.\n\n\"NASA 360\" has partnered with AMP International to air programs on airlines and cruise lines around the globe. Singapore Airlines, US Air, and Philippines Airlines air \"NASA 360\" as an on-board entertainment option.\n\nAs of May 2013, the program had been downloaded nearly 14 million times from the NASA portal.\n\n"}
{"id": "100286", "url": "https://en.wikipedia.org/wiki?curid=100286", "title": "National Medal of Science", "text": "National Medal of Science\n\nThe National Medal of Science is an honor bestowed by the President of the United States to individuals in science and engineering who have made important contributions to the advancement of knowledge in the fields of behavioral and social sciences, biology, chemistry, engineering, mathematics and physics. The twelve member presidential Committee on the National Medal of Science is responsible for selecting award recipients and is administered by the National Science Foundation (NSF).\n\nThe National Medal of Science was established on August 25, 1959, by an act of the Congress of the United States under . The medal was originally to honor scientists in the fields of the \"physical, biological, mathematical, or engineering sciences\". The Committee on the National Medal of Science was established on August 23, 1961, by executive order 10961 of President John F. Kennedy.\n\nOn January 7, 1979, the American Association for the Advancement of Science (AAAS) passed a resolution proposing that the medal be expanded to include the social and behavioral sciences. In response, Senator Ted Kennedy introduced the \"Science and Technology Equal Opportunities Act\" into the Senate on March 7, 1979, expanding the medal to include these scientific disciplines as well. President Jimmy Carter's signature enacted this change as Public Law 96-516 on December 12, 1980.\n\nIn 1992, the National Science Foundation signed a letter of agreement with the National Science and Technology Medals Foundation that made the National Science and Technology Medals Foundation the metaorganization over both the National Medal of Science and the very similar National Medal of Technology.\n\nThe first National Medal of Science was awarded on February 18, 1963, for the year 1962 by President John F. Kennedy to Theodore von Kármán for his work at the Caltech Jet Propulsion Laboratory. The citation accompanying von Kármán's award reads:\n\nFor his leadership in the science and engineering basic to aeronautics; for his effective teaching and related contributions in many fields of mechanics, for his distinguished counsel to the Armed Services, and for his promoting international cooperation in science and engineering.\n\nThe first woman to receive a National Medal of Science was Barbara McClintock, who was awarded for her work on plant genetics in 1970.\n\nAlthough Public Law 86-209 provides for 20 recipients of the medal per year, it is typical for approximately 8–15 accomplished scientists and engineers to receive this distinction each year. There have been a number of years where no National Medals of Science were awarded. Those years include: 1985, 1984, 1980, 1978, 1977, 1972 and 1971.\n\nThe awards ceremony is organized by the Office of Science and Technology Policy. It takes place at the White House and is presided by the sitting United States president.\n\nEach year the National Science Foundation sends out a call to the scientific community for the nomination of new candidates for the National Medal of Science. Individuals are nominated by their peers with each nomination requiring three letters of support from individuals in science and technology. Nominations are then sent to the Committee of the National Medal of Science which is a board composed of fourteen presidential appointees comprising twelve scientists, and two \"ex officio\" members—the director of the Office of Science and Technology Policy (OSTP) and the president of the National Academy of Sciences (NAS).\n\nAccording to the Committee, successful candidates must be U.S. citizens or permanent residents who are applying for U.S. citizenship, who have done work of significantly outstanding merit or that has had a major impact on scientific thought in their field. The Committee also values those who promote the general advancement of science and individuals who have influenced science education, although these traits are less important than groundbreaking or thought-provoking research. The nomination of a candidate is effective for three years; at the end of three years, the candidates peers are allowed to renominate the candidate. The Committee makes their recommendations to the President for the final awarding decision.\n\nThe National Medal of Science depicts Man, surrounded by earth, sea, and sky, contemplating and struggling to understand Nature. The crystal in his hand represents the universal order and also suggests the basic unit of living things. The formula being outlined in the sand symbolizes scientific abstraction.\n\n\n"}
{"id": "54080922", "url": "https://en.wikipedia.org/wiki?curid=54080922", "title": "Nordic Microscopy Society", "text": "Nordic Microscopy Society\n\nThe Nordic Microscopy Society is a learned society for the promotion of microscopy in the Nordic countries. It was founded on 16 October, 1948 at the Research Institute of Experimental Physics in Stockholm, Sweden and was originally called the \"Scandinavian Society for Electron Microscopy\" (SCANDEM), reflecting the region of Europe the founding members of Denmark, Norway and Sweden are located. This name was in use until 2002 when it was changed to its current name to reflect the shift towards encompassing the broader field of microscopy. The society is a member of the European Microscopy Society committee of the International Federation of Societies for Microscopy.\n\nListed below are the presidents of the scoiety from 1973 until present.\n"}
{"id": "24562", "url": "https://en.wikipedia.org/wiki?curid=24562", "title": "Pareto principle", "text": "Pareto principle\n\nThe Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes. Management consultant Joseph M. Juran suggested the principle and named it after Italian economist Vilfredo Pareto, who noted the 80/20 connection while at the University of Lausanne in 1896, as published in his first work, \"Cours d'économie politique\". Essentially, Pareto showed that approximately 80% of the land in Italy was owned by 20% of the population.\n\nIt is an axiom of business management that \"80% of sales come from 20% of clients\". Richard Koch authored the book, \"The 80/20 Principle,\" which illustrated some practical applications of the Pareto principle in business management and life.\n\nMathematically, the 80/20 rule is roughly followed by a power law distribution (also known as a Pareto distribution) for a particular set of parameters, and many natural phenomena have been shown empirically to exhibit such a distribution.\n\nThe Pareto principle is only tangentially related to Pareto efficiency. Pareto developed both concepts in the context of the distribution of income and wealth among the population.\n\nThe original observation was in connection with population and wealth. Pareto noticed that approximately 80% of Italy's land was owned by 20% of the population. He then carried out surveys on a variety of other countries and found to his surprise that a similar distribution applied.\n\nA chart that gave the inequality a very visible and comprehensible form, the so-called \"champagne glass\" effect, was contained in the 1992 United Nations Development Program Report, which showed that distribution of global income is very uneven, with the richest 20% of the world's population controlling 82.7% of the world's income.\nThe Pareto principle also could be seen as applying to taxation. In the US, the top 20% of earners have paid roughly 80-90% of Federal income taxes in 2000 and 2006, and again in 2018.\n\nIn computer science the Pareto principle can be applied to optimization efforts. For example, Microsoft noted that by fixing the top 20% of the most-reported bugs, 80% of the related errors and crashes in a given system would be eliminated. Lowell Arthur expressed that \"20 percent of the code has 80 percent of the errors. Find them, fix them!\" It was also discovered that in general the 80% of a certain piece of software can be written in 20% of the total allocated time. Conversely, the hardest 20% of the code takes 80% of the time. This factor is usually a part of COCOMO estimating for software coding.\n\nIt has been inferred the Pareto principle applies to athletic training, where roughly 20% of the exercises and habits have 80% of the impact and the trainee should not focus so much on a varied training. This does not necessarily mean that having a healthy diet or going to the gym are not important, but they are not as significant as the key activities. It is also important to note this 80/20 rule has yet to be scientifically tested in controlled studies with regards to athletic training.\n\nIn baseball, the Pareto principle has been perceived in Wins Above Replacement (an attempt to combine multiple statistics to determine a player's overall importance to a team). \"15% of the all the players last year produced 85% of the total wins with the other 85% of the players creating 15% of the wins. The Pareto Principle holds up pretty soundly when it is applied to baseball...\" Unfortunately a brief look at this article's methods shows that the choice of alternate and probably better indicators of performance yields drastically different results that fail to support the Pareto Principle for this context. Thus this probably entails an example of cherry-picking.\n\nOccupational health and safety professionals use the Pareto principle to underline the importance of hazard prioritization. Assuming 20% of the hazards account for 80% of the injuries, and by categorizing hazards, safety professionals can target those 20% of the hazards that cause 80% of the injuries or accidents. Alternatively, if hazards are addressed in random order, a safety professional is more likely to fix one of the 80% of hazards that account only for some fraction of the remaining 20% of injuries.\n\nAside from ensuring efficient accident prevention practices, the Pareto principle also ensures hazards are addressed in an economical order as the technique ensures the resources used are best used to prevent the most accidents.\n\nIn engineering control theory, such as for electromechanical energy converters, the 80/20 principle applies to optimization efforts.\n\nThe law of the few can be also seen in betting, where it is said that with 20% effort you can match the accuracy of 80% of the bettors.\n\nIn the systems science discipline, Joshua M. Epstein and Robert Axtell created an agent-based simulation model called Sugarscape, from a decentralized modeling approach, based on individual behavior rules defined for each agent in the economy. Wealth distribution and Pareto's 80/20 principle became emergent in their results, which suggests the principle is a collective consequence of these individual rules.\n\nThe Pareto principle has many applications in quality control. It is the basis for the Pareto chart, one of the key tools used in total quality control and Six Sigma techniques. The Pareto principle serves as a baseline for ABC-analysis and XYZ-analysis, widely used in logistics and procurement for the purpose of optimizing stock of goods, as well as costs of keeping and replenishing that stock.\n\nIn health care in the United States, in one instance 20% of patients have been found to use 80% of health care resources., \n\nSome cases of super-spreading conform to the 20/80 rule, where approximately 20% of infected individuals are responsible for 80% of transmissions, although super-spreading can still be said to occur when super-spreaders account for a higher or lower percentage of transmissions. In epidemics with super-spreading, the majority of individuals infect relatively few secondary contacts.\n\nThe Dunedin Study has found 80% of crimes are committed by 20% of criminals. This statistic has been used to support both stop-and-frisk policies and broken windows policing, as catching those criminals committing minor crimes will supposedly net many criminals wanted for (or who would normally commit) larger ones. Once again, however, data from the same Dunedin study provide other results that nowhere near approximate the 80-20 principle; choose a different indicator and the 80% may become 36%. Thus support for the applicability of the Pareto Principle again seems to require cherry-picking.\n\nThe idea has a rule of thumb application in many places, but it is commonly misused. For example, it is a misuse to state a solution to a problem \"fits the 80/20 rule\" just because it fits 80% of the cases; it must also be that the solution requires only 20% of the resources that would be needed to solve all cases. Additionally, it is a misuse of the 80/20 rule to interpret a small number of categories or observations.\n\nThis is a special case of the wider phenomenon of Pareto distributions. If the Pareto index α, which is one of the parameters characterizing a Pareto distribution, is chosen as α = log5 ≈ 1.16, then one has 80% of effects coming from 20% of causes.\n\nIt follows that one also has 80% of that top 80% of effects coming from 20% of that top 20% of causes, and so on. Eighty percent of 80% is 64%; 20% of 20% is 4%, so this implies a \"64/4\" law; and similarly implies a \"51.2/0.8\" law. Similarly for the bottom 80% of causes and bottom 20% of effects, the bottom 80% of the bottom 80% only cause 20% of the remaining 20%. This is broadly in line with the world population/wealth table above, where the bottom 60% of the people own 5.5% of the wealth, approximating to a 64/4 connection.\n\nThe 64/4 correlation also implies a 32% 'fair' area between the 4% and 64%, where the lower 80% of the top 20% (16%) and upper 20% of the bottom 80% (also 16%) relates to the corresponding lower top and upper bottom of effects (32%). This is also broadly in line with the world population table above, where the second 20% control 12% of the wealth, and the bottom of the top 20% (presumably) control 16% of the wealth.\n\nThe term 80/20 is only a shorthand for the general principle at work. In individual cases, the distribution could just as well be, say, nearer to 80/20 or 70/30. There is no need for the two numbers to add up to the number 100, as they are measures of different things, (e.g., 'number of customers' vs 'amount spent'). However, each case in which they do not add up to 100%, is equivalent to one in which they do. For example, as noted above, the \"64/4 law\" (in which the two numbers do not add up to 100%) is equivalent to the \"80/20 law\" (in which they do add up to 100%). Thus, specifying two percentages independently does not lead to a broader class of distributions than what one gets by specifying the larger one and letting the smaller one be its complement relative to 100%. Thus, there is only one degree of freedom in the choice of that parameter.\n\nAdding up to 100 leads to a nice symmetry. For example, if 80% of effects come from the top 20% of sources, then the remaining 20% of effects come from the lower 80% of sources. This is called the \"joint ratio\", and can be used to measure the degree of imbalance: a joint ratio of 96:4 is very imbalanced, 80:20 is significantly imbalanced (Gini index: 76%), 70:30 is moderately imbalanced (Gini index: 28%), and 55:45 is just slightly imbalanced (Gini index 14%).\n\nThe Pareto principle is an illustration of a \"power law\" relationship, which also occurs in phenomena such as brush fires and earthquakes.\nBecause it is self-similar over a wide range of magnitudes, it produces outcomes completely different from Normal or Gaussian distribution phenomena. This fact explains the frequent breakdowns of sophisticated financial instruments, which are modeled on the assumption that a Gaussian relationship is appropriate to, for example, stock price movements.\n\nUsing the \"\"A\" : \"B\"\" notation (for example, 0.8:0.2) and with \"A\" + \"B\" = 1, inequality measures like the Gini index (G) \"and\" the Hoover index (H) can be computed. In this case both are the same.\n\nThe Theil index is an entropy measure used to quantify inequalities. The measure is 0 for 50:50 distributions and reaches 1 at a Pareto distribution of 82:18. Higher inequalities yield Theil indices above 1.\n\n\nParetoRule.cf : The Pareto Rule\n\n"}
{"id": "56412019", "url": "https://en.wikipedia.org/wiki?curid=56412019", "title": "Plants of the World Online", "text": "Plants of the World Online\n\nPlants of the World Online is an online database published by the Royal Botanic Gardens, Kew. It was launched in March 2017 with the ultimate aim being \"to enable users to access information on all the world's known seed-bearing plants by 2020\". The initial focus was on tropical African Floras, particularly Flora Zambesiaca, Flora of West Tropical Africa and Flora of Tropical East Africa.\n\n"}
{"id": "5060840", "url": "https://en.wikipedia.org/wiki?curid=5060840", "title": "Recursive grammar", "text": "Recursive grammar\n\nIn computer science, a grammar is informally called a recursive grammar if it contains production rules that are recursive, meaning that expanding a non-terminal according to these rules can eventually lead to a string that includes the same non-terminal again. Otherwise it is called a non-recursive grammar.\n\nFor example, a grammar for a context-free language is left recursive if there exists a non-terminal symbol \"A\" that can be put through the production rules to produce a string with \"A\" (as the leftmost symbol).\nAll types of grammars in the Chomsky hierarchy can be recursive and it is recursion that allows the production of infinite sets of words.\n\nA non-recursive grammar can produce only a finite language; and each finite language can be produced by a non-recursive grammar.\nFor example, a straight-line grammar produces just a single word.\n\nA recursive context-free grammar that contains no useless rules necessarily produces an infinite language. This property forms the basis for an algorithm that can test efficiently whether a context-free grammar produces a finite or infinite language.\n"}
{"id": "4802556", "url": "https://en.wikipedia.org/wiki?curid=4802556", "title": "Rheoscopic fluid", "text": "Rheoscopic fluid\n\nRheoscopic fluid literally means \"current showing\" fluid. Such liquids are effective in visualizing dynamic currents in fluids, such as convection and laminar flow. They are microscopic crystalline platelets such as mica, metallic flakes, or fish scales in suspension in a fluid such as water or glycol stearate.\n\nWhen the fluid is put in motion, the suspended particles orient themselves in localized, preferential alignment, larger parts of the fluid moving sheer parallel to other parts of the fluid. With appropriate illumination, the particle-filled fluid will reflect differing intensities of light, making the movement of the currents visible.\n\n\n"}
{"id": "55831906", "url": "https://en.wikipedia.org/wiki?curid=55831906", "title": "SeNSS", "text": "SeNSS\n\nThe South East Network for Social Sciences (SeNSS) is a consortium of ten universities in the UK. All pioneers and world leaders in social-science research, knowledge production and training, the universities cooperate under ESRC to provide funding, expertise and an arena for Social Science and Economics researchers; their ESRC funding was announced in August 2016 after SeNSS's 2015 foundation. In 2016, SAGE Publishing revealed that it would begin a partnership with SeNSS.\n\nSeNSS members are:\n\nThe SeNSS is funded by the ESRC and provides funding for PhD students in the social sciences, training and workshops as well as a yearly conference. SeNSS also provides post-doctoral fellowships, placements and researcher support; SeNSS started accepting applicants in 2017.\n\nSeNSS focuses on providing inter-disciplinary research training through engaging its scholars with expertise drawn from different scholarly fields.\n"}
{"id": "7233280", "url": "https://en.wikipedia.org/wiki?curid=7233280", "title": "Semantic interoperability", "text": "Semantic interoperability\n\nSemantic interoperability is the ability of computer systems to exchange data with unambiguous, shared meaning. Semantic interoperability is a requirement to enable machine computable logic, inferencing, knowledge discovery, and data federation between information systems.\n\nSemantic interoperability is therefore concerned not just with the packaging of data (syntax), but the simultaneous transmission of the meaning with the data (semantics). This is accomplished by adding data about the data (metadata), linking each data element to a controlled, shared vocabulary. The meaning of the data is transmitted with the data itself, in one self-describing \"information package\" that is independent of any information system. It is this shared vocabulary, and its associated links to an ontology, which provides the foundation and capability of machine interpretation, inference, and logic.\n\nSyntactic interoperability is a prerequisite for semantic interoperability. Syntactic interoperability refers to the packaging and transmission mechanisms for data. In healthcare, HL7 has been in use for over thirty years (which predates the internet and web technology), and uses the pipe character (|) as a data delimiter. The current internet standard for document markup is XML, which uses \"< >\" as a data delimiter. The data delimiters convey no meaning to the data other than to structure the data. Without a data dictionary to translate the contents of the delimiters, the data remains meaningless. While there are many attempts at creating data dictionaries and information models to associate with these data packaging mechanisms, none have been practical to implement. This has only perpetuated the ongoing \"babelization\" of data and inability to exchange data with meaning.\n\nSince the introduction of the Semantic Web concept by Tim Berners-Lee in 1999, there has been growing interest and application of the W3C (World Wide Web Consortium, W3C) standards to provide web-scale semantic data exchange, federation, and inferencing capabilities.\n\nSyntactic interoperability, provided by for instance XML or the SQL standards, is a pre-requisite to semantic. It involves a common data format and common protocol to structure any data so that the manner of processing the information will be interpretable from the structure. It also allows detection of syntactic errors, thus allowing receiving systems to request resending of any message that appears to be garbled or incomplete. No semantic communication is possible if the syntax is garbled or unable to represent the data. However, information represented in one syntax may in some cases be accurately translated into a different syntax. Where accurate translation of syntaxes is possible, systems using different syntaxes may also interoperate accurately. In some cases the ability to accurately translate information among systems using different syntaxes may be limited to one direction, when the formalisms used have different levels of \"expressivity\" (ability to express information).\n\nA single ontology containing representations of every term used in every application is generally considered impossible, because of the rapid creation of new terms or assignments of new meanings to old terms. However, though it is impossible to anticipate \"every\" concept that a user may wish to represent in a computer, there is the possibility of finding some finite set of \"primitive\" concept representations that can be combined to create any of the more specific concepts that users may need for any given set of applications or ontologies. Having a foundation ontology (also called \"upper ontology\") that contains all those primitive elements would provide a sound basis for general semantic interoperability, and allow users to define any new terms they need by using the basic inventory of ontology elements, and still have those newly defined terms properly interpreted by any other computer system that can interpret the basic foundation ontology. Whether the number of such primitive concept representations is in fact finite, or will expand indefinitely, is a question under active investigation. If it is finite, then a stable foundation ontology suitable to support accurate and general semantic interoperability can evolve after some initial foundation ontology has been tested and used by a wide variety of users. At the present time, no foundation ontology has been adopted by a wide community, so such a stable foundation ontology is still in the future.\n\nOne persistent misunderstanding recurs in discussion of semantics - the confusion of words and meanings. The meanings of words change, sometimes rapidly. But a formal language such as used in an ontology can encode the meanings (semantics) of concepts in a form that does not change. In order to determine what is the meaning of a particular word (or term in a database, for example) it is necessary to label each fixed concept representation in an ontology with the word(s) or term(s) that may refer to that concept. When multiple words refer to the same (fixed) concept in language this is called synonymy; when one word is used to refer to more than one concept, that is called ambiguity. Ambiguity and synonymy are among the factors that make computer understanding of language very difficult. The use of words to refer to concepts (the meanings of the words used)is very sensitive to the context and the purpose of any use for many human-readable terms. The use of ontologies in supporting semantic interoperability is to provide a fixed set of concepts whose meanings and relations are stable and can be agreed to by users. The task of determining which terms in which contexts (each database is a different context) is then separated from the task of creating the ontology, and must be taken up by the designer of a database, or the designer of a form for data entry, or the developer of a program for language understanding. When a word used in some interoperability context changes its meaning, then to preserve interoperability it is necessary to change the pointer to the ontology element(s) that specifies the meaning of that word.\n\nA knowledge representation language may be sufficiently expressive to describe nuances of meaning in well understood fields. There are at least five levels of complexity of these.\n\nFor general semi-structured data one may use a general purpose language such as XML.\n\nLanguages with the full power of first-order predicate logic may be required for many tasks.\n\nHuman languages are highly expressive, but are considered too ambiguous to allow the accurate interpretation desired, given the current level of human language technology. \n\nSemantic interoperability may be distinguished from other forms of interoperability by considering whether the information transferred has, in its communicated form, all of the meaning required for the receiving system to interpret it correctly, even when the algorithms used by the receiving system are unknown to the sending system. Consider sending one number:\n\nIf that number is intended to be the sum of money owed by one company to another, it implies some action or lack of action on the part of both those who send it and those who receive it.\n\nIt may be correctly interpreted if sent in response to a specific request, and received at the time and in the form expected. This correct interpretation does not depend only on the number itself, which could represent almost any of millions of types of quantitative measurement, rather it depends strictly on the circumstances of transmission. That is, the interpretation depends on both systems expecting that the algorithms in the other system use the number in exactly the same sense, and it depends further on the entire envelope of transmissions that preceded the actual transmission of the bare number. By contrast, if the transmitting system does not know how the information will be used by other systems, it is necessary to have a shared agreement on how information with some specific meaning (out of many possible meanings) will appear in a communication. For a particular task, one solution is to standardize a form, such as a request for payment; that request would have to encode, in standardized fashion, all of the information needed to evaluate it, such as: the agent owing the money, the agent owed the money, the nature of the action giving rise to the debt, the agents, goods, services, and other participants in that action; the time of the action; the amount owed and currency in which the debt is reckoned; the time allowed for payment; the form of payment demanded; and other information. When two or more systems have agreed on how to interpret the information in such a request, they can achieve semantic interoperability \"for that specific type of transaction\". For semantic interoperability generally, it is necessary to provide standardized ways to describe the meanings of many more things than just commercial transactions, and the number of concepts whose representation needs to be agreed upon are at a minimum several thousand.\n\nHow to achieve semantic interoperability for more than a few restricted scenarios is currently a matter of research and discussion. For the problem of General Semantic Interoperability, some form of foundation ontology ('upper ontology') is required that is sufficiently comprehensive to provide the defining concepts for more specialized ontologies in multiple domains. Over the past decade more than ten foundation ontologies have been developed, but none have as yet been adopted by a wide user base.\n\nThe need for a single comprehensive all-inclusive ontology to support Semantic Interoperability can be avoided by designing the common foundation ontology as a set of basic (\"primitive\") concepts that can be combined to create the logical descriptions of the meanings of terms used in local domain ontologies or local databases. This tactic is based on the principle that:\n\nIf:\n\nThen:\n\nTherefore:\nThis tactic then limits the need for prior agreement on meanings to only those ontology elements in the common Foundation Ontology (FO). Based on several considerations, this is likely to be fewer than 10,000 elements (types and relations).\n\nIn practice, together with the FO focused on representations of the primitive concepts, a set of domain extension ontologies to the FO with elements specified using the FO elements will likely also be used. Such pre-existing extensions will ease the cost of creating domain ontologies by providing existing elements with the intended meaning, and will reduce the chance of error by using elements that have already been tested. Domain extension ontologies may be logically inconsistent with each other, and that needs to be determined if different domain extensions are used in any communication.\n\nWhether use of such a single foundation ontology can itself be avoided by sophisticated mapping techniques among independently developed ontologies is also under investigation.\n\nThe practical significance of semantic interoperability has been measured by several studies that estimate the cost (in lost efficiency) due to lack of semantic interoperability. One study, focusing on the lost efficiency in the communication of healthcare information, estimated that US$77.8 billion per year could be saved by implementing an effective interoperability standard in that area. Other studies, of the construction industry and of the automobile manufacturing supply chain, estimate costs of over US$10 billion per year due to lack of semantic interoperability in those industries. In total these numbers can be extrapolated to indicate that well over US$100 billion per year is lost because of the lack of a widely used semantic interoperability standard in the US alone.\n\nThere has not yet been a study about each policy field that might offer big cost savings applying semantic interoperability standards. But to see which policy fields are capable of profiting from semantic interoperability see 'Interoperability' in general. Such policy fields are eGovernment, health, security and many more. The EU also set up the Semantic Interoperability Centre Europe in June 2007.\n\n\n"}
{"id": "9538066", "url": "https://en.wikipedia.org/wiki?curid=9538066", "title": "Sir Peter Blake Marine Education and Recreation Centre", "text": "Sir Peter Blake Marine Education and Recreation Centre\n\nSir Peter Blake Marine Education & Recreation Centre (MERC) is a not for profit charity (Registration Number CC29903) based in Long Bay, Auckland, New Zealand. MERC offers marine-based environmental education and outdoor recreational experiences to schools, not-for-profit organisations, and business and corporate groups. \n\nMERC can accommodate up to 85 people at any time for overnight stays and activities, and up to a total of 120 people per day. A day programme consists of 3 activities on the day and instruction is given between 9am and 4pm. There is scope to organise short duration, single day and multi-day courses, and these are specifically developed to meet the needs of each group. MERCs facilities have separate teacher/leader accommodation, multi-purpose hall, wheelchair access and large kitchen & dining facilities.\n\nThe Centre's mission statement is \"to provide life-changing marine environmental education and outdoor experiences for young New Zealanders.\"\n\nThe Centre has been in operation since 1990. A group of local yachtsmen and teachers (Don St.Clair Brown, Don Burfoot, Laurie Baxter, Ian Sage, Dr. Ross Garrett and John Orams, responded to the vision of fellow yachtsman Dr. David Gray, that a centre be built on the present site which was designated “Conditional Use Reserve” land belonging to Takapuna City Council. In 1980 this site was littered with building debris, scraps of machinery and foundations of early buildings. These basic amenities had serviced the large Long Bay camping population as well as local holiday residents in their numerous beaches. There was a store with petrol pumps – started by Tom Vaughan about 1920 but taken over by Mr and Mrs Aston and demolished about 1950 (?), a butchery owned by Mr Lopes which later became a private beach, a holiday house built above the sea wall (1956) still tenanted in 1989.\n\n"}
{"id": "18877204", "url": "https://en.wikipedia.org/wiki?curid=18877204", "title": "Stephan Swanson", "text": "Stephan Swanson\n\nStephan Swanson came to prominence as a marine researcher when he successfully placed the satellite transmitter on the famous Great white shark Nicole, the first great white shark ever to be tracked on a 20,000 kilometer migration from South Africa to Australia and back. Due to his ability to handle large marine predators, such as the great white shark, he was contracted as an expedition biologist to travel to Guadeloupe and place satellite transmitters on the dorsal fins of Great Whites. His historical capture and release of a 5m long, 1800 kilogram great white shark is documented in the National Geographic Marine Special \"Ultimate Shark\".\n\n\n\n"}
{"id": "7188290", "url": "https://en.wikipedia.org/wiki?curid=7188290", "title": "Stretch receptor", "text": "Stretch receptor\n\nStretch receptors are mechanoreceptors responsive to distention of various organs and muscles, and are neurologically linked to the medulla in the brain stem via afferent nerve fibers. Examples include stretch receptors in the arm and leg muscles and tendons, in the heart, in the colon wall, and in the lungs.\n\nStretch receptors are also found around the carotid artery, where they monitor blood pressure and stimulate the release of antidiuretic hormone (ADH) from the posterior pituitary gland.\n\nTypes include:\n"}
{"id": "56226932", "url": "https://en.wikipedia.org/wiki?curid=56226932", "title": "Timeline of the gunpowder age in Japan", "text": "Timeline of the gunpowder age in Japan\n\nThis is a timeline of the history of gunpowder and related topics such as weapons, warfare, and industrial applications in Japan.\n\n\n"}
{"id": "43224338", "url": "https://en.wikipedia.org/wiki?curid=43224338", "title": "Winquist and Hansen classification", "text": "Winquist and Hansen classification\n\nThe Winquist and Hansen classification is a system of categorizing femoral shaft fractures based upon the degree of comminution.\n"}
{"id": "48614053", "url": "https://en.wikipedia.org/wiki?curid=48614053", "title": "Wonders of the Universe (book)", "text": "Wonders of the Universe (book)\n\nWonders of the Universe is a 2011 book by the theoretical physicists Brian Cox and Andrew Cohen. The book is about cosmology and the universe, and is explained in a way that is accessible to a general reader. The book is based on a series with the same name \"Wonders of the Universe\".\n"}
{"id": "31173628", "url": "https://en.wikipedia.org/wiki?curid=31173628", "title": "Yoichirō Hirase", "text": "Yoichirō Hirase\n\n"}
