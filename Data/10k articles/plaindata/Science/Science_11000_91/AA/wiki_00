{"id": "41943372", "url": "https://en.wikipedia.org/wiki?curid=41943372", "title": "Accretion (meteorology)", "text": "Accretion (meteorology)\n\nAccretion is defined as the gradual collection of something over time. In meteorology or atmospheric science it is the process of accumulation of frozen water as precipitation over time as it descends through the atmosphere, in particular when an ice crystal or snowflake hits a supercooled liquid droplet, which then freeze together, increasing the size of the water particle. The collection of these particles eventually forms snow or hail in clouds and depending on lower atmosphere temperatures may become rain, sleet, or graupel. Accretion is the basis for cloud formation and can also be seen as water accumulates on the particulate matter and form jet contrails. This is because water vapor in the air requires condensation nuclei to form large droplets of solid or liquid water.\n"}
{"id": "2175893", "url": "https://en.wikipedia.org/wiki?curid=2175893", "title": "American Institute of Physics", "text": "American Institute of Physics\n\nThe American Institute of Physics (AIP) promotes science, the profession of physics, publishes physics journals, and produces publications for scientific and engineering societies. The AIP is made up of various member societies. Its corporate headquarters are at the American Center for Physics in College Park, Maryland, but the institute also has offices in Melville, New York and Beijing.\n\nThe focus of the AIP appears to be organized around a set of core activities. The first delineated activity is to support member societies regarding essential society functions. This is accomplished by annually convening the various society officers to discuss common areas of concern. A range of topics is discussed which includes scientific publishing, public policy issues, membership-base issues, philanthropic giving, science education, science careers for a diverse population, and a forum for sharing ideas. \n\nAnother core activity is publishing the science of physics in research journals, magazines, and conference proceedings. Member societies continue nevertheless to publish their own journals.\n\nOther core activities are tracking employment and education trends with six decades of coverage, being a liaison between research science and industry, historical collections and physics outreach programs, and supporting science education initiatives and supporting undergraduate physics. One other core activity is as an advocate for science policy to the U.S. Congress and the general public.\n\nThe AIP was founded in 1931 as a response to lack of funding for the sciences during the Great Depression. It formally incorporated in 1932 consisting of five original \"member societies\", and a total of four thousand members. A new set of member societies was added beginning in the mid-1960s. As soon as the AIP was established it began publishing scientific journals.\n\nThe AIP has a subsidiary called \"AIP Publishing\" (wholly owned non-profit) dedicated to scholarly publishing by the AIP and its member societies, as well on behalf of other partners.\n\n\n"}
{"id": "997021", "url": "https://en.wikipedia.org/wiki?curid=997021", "title": "Asymptotic gain model", "text": "Asymptotic gain model\n\nThe asymptotic gain model (also known as the Rosenstark method) is a representation of the gain of negative feedback amplifiers given by the asymptotic gain relation:\nwhere formula_2 is the return ratio with the input source disabled (equal to the negative of the loop gain in the case of a single-loop system composed of unilateral blocks), \"G\" is the asymptotic gain and \"G\" is the direct transmission term. This form for the gain can provide intuitive insight into the circuit and often is easier to derive than a direct attack on the gain.\n\nFigure 1 shows a block diagram that leads to the asymptotic gain expression. The asymptotic gain relation also can be expressed as a signal flow graph. See Figure 2. The asymptotic gain model is a special case of the extra element theorem.\nAs follows directly from limiting cases of the gain expression, the asymptotic gain \"G\" is simply the gain of the system when the return ratio approaches infinity:\n\nwhile the direct transmission term \"G\" is the gain of the system when the return ratio is zero:\n\n\nDirect application of the model involves these steps:\n\nThese steps can be implemented directly in SPICE using the small-signal circuit of hand analysis. In this approach the dependent sources of the devices are readily accessed. In contrast, for experimental measurements using real devices or SPICE simulations using numerically generated device models with inaccessible dependent sources, evaluating the return ratio requires special methods.\n\nClassical feedback theory neglects feedforward (\"G\"). If feedforward is dropped, the gain from the asymptotic gain model becomes\n\nwhile in classical feedback theory, in terms of the open loop gain \"A\", the gain with feedback (closed loop gain) is:\n\nComparison of the two expressions indicates the feedback factor \"β\" is:\n\nwhile the open-loop gain is:\n\nIf the accuracy is adequate (usually it is), these formulas suggest an alternative evaluation of \"T\": evaluate the open-loop gain and \"G\" and use these expressions to find \"T\". Often these two evaluations are easier than evaluation of \"T\" directly.\n\nThe steps in deriving the gain using the asymptotic gain formula are outlined below for two negative feedback amplifiers. The single transistor example shows how the method works in principle for a transconductance amplifier, while the second two-transistor example shows the approach to more complex cases using a current amplifier.\n\nConsider the simple FET feedback amplifier in Figure 3. The aim is to find the low-frequency, open-circuit, transresistance gain of this circuit \"G\" = \"v\" / \"i\" using the asymptotic gain model.\n\nThe small-signal equivalent circuit is shown in Figure 4, where the transistor is replaced by its hybrid-pi model.\n\nIt is most straightforward to begin by finding the return ratio \"T\", because \"G\" and \"G\" are defined as limiting forms of the gain as \"T\" tends to either zero or infinity. To take these limits, it is necessary to know what parameters \"T\" depends upon. There is only one dependent source in this circuit, so as a starting point the return ratio related to this source is determined as outlined in the article on return ratio.\n\nThe return ratio is found using Figure 5. In Figure 5, the input current source is set to zero, By cutting the dependent source out of the output side of the circuit, and short-circuiting its terminals, the output side of the circuit is isolated from the input and the feedback loop is broken. A test current \"i\" replaces the dependent source. Then the return current generated in the dependent source by the test current is found. The return ratio is then \"T\" = −\"i / i\". Using this method, and noticing that \"R\" is in parallel with \"r\", \"T\" is determined as:\nwhere the approximation is accurate in the common case where \"r\" » \"R\". With this relationship it is clear that the limits \"T\" → 0, or ∞ are realized if we let transconductance \"g\" → 0, or ∞.\n\nFinding the asymptotic gain \"G\" provides insight, and usually can be done by inspection. To find \"G\" we let \"g\" → ∞ and find the resulting gain. The drain current, \"i\" = \"g\" \"v\", must be finite. Hence, as \"g\" approaches infinity, \"v\" also must approach zero. As the source is grounded, \"v\" = 0 implies \"v\" = 0 as well. With \"v\" = 0 and the fact that all the input current flows through \"R\" (as the FET has an infinite input impedance), the output voltage is simply −\"i\" \"R\". Hence\n\nAlternatively \"G\" is the gain found by replacing the transistor by an ideal amplifier with infinite gain - a nullor.\n\nTo find the direct feedthrough formula_11 we simply let \"g\" → 0 and compute the resulting gain. The currents through \"R\" and the parallel combination of \"R\" || \"r\" must therefore be the same and equal to \"i\". The output voltage is therefore \"i\" \"(R\" \"|| r\"\")\".\n\nHence\n\nwhere the approximation is accurate in the common case where \"r\" » \"R\".\n\nThe overall transresistance gain of this amplifier is therefore:\n\nExamining this equation, it appears to be advantageous to make \"R\" large in order make the overall gain approach the asymptotic gain, which makes the gain insensitive to amplifier parameters (\"g\" and \"R\"). In addition, a large first term reduces the importance of the direct feedthrough factor, which degrades the amplifier. One way to increase \"R\" is to replace this resistor by an active load, for example, a current mirror.\n\nFigure 6 shows a two-transistor amplifier with a feedback resistor \"R\". This amplifier is often referred to as a \"shunt-series feedback\" amplifier, and analyzed on the basis that resistor \"R\" is in series with the output and samples output current, while \"R\" is in shunt (parallel) with the input and subtracts from the input current. See the article on negative feedback amplifier and references by Meyer or Sedra. That is, the amplifier uses current feedback. It frequently is ambiguous just what type of feedback is involved in an amplifier, and the asymptotic gain approach has the advantage/disadvantage that it works whether or not you understand the circuit.\n\nFigure 6 indicates the output node, but does not indicate the choice of output variable. In what follows, the output variable is selected as the short-circuit current of the amplifier, that is, the collector current of the output transistor. Other choices for output are discussed later.\n\nTo implement the asymptotic gain model, the dependent source associated with either transistor can be used. Here the first transistor is chosen.\n\nThe circuit to determine the return ratio is shown in the top panel of Figure 7. Labels show the currents in the various branches as found using a combination of Ohm's law and Kirchhoff's laws. Resistor \"R\" \"= R\" \"// r\" and \"R\" \"= R\" \"// R\". KVL from the ground of \"R\" to the ground of \"R\" provides:\n\nKVL provides the collector voltage at the top of \"R\" as\n\nFinally, KCL at this collector provides\n\nSubstituting the first equation into the second and the second into the third, the return ratio is found as\n\nThe circuit to determine \"G\" is shown in the center panel of Figure 7. In Figure 7, the output variable is the output current β\"i\" (the short-circuit load current), which leads to the short-circuit current gain of the amplifier, namely β\"i\" / \"i\":\n\nUsing Ohm's law, the voltage at the top of \"R\" is found as\n\nor, rearranging terms,\n\nUsing KCL at the top of \"R\":\n\nEmitter voltage \"v\" already is known in terms of \"i\" from the diagram of Figure 7. Substituting the second equation in the first, \"i\" is determined in terms of \"i\" alone, and \"G\" becomes:\n\nGain \"G\" represents feedforward through the feedback network, and commonly is negligible.\n\nThe circuit to determine \"G\" is shown in the bottom panel of Figure 7. The introduction of the ideal op amp (a nullor) in this circuit is explained as follows. When \"T \"→ ∞, the gain of the amplifier goes to infinity as well, and in such a case the differential voltage driving the amplifier (the voltage across the input transistor \"r\") is driven to zero and (according to Ohm's law when there is no voltage) it draws no input current. On the other hand, the output current and output voltage are whatever the circuit demands. This behavior is like a nullor, so a nullor can be introduced to represent the infinite gain transistor.\n\nThe current gain is read directly off the schematic:\n\nUsing the classical model, the feed-forward is neglected and the feedback factor β is (assuming transistor β » 1):\n\nand the open-loop gain \"A\" is:\n\nThe above expressions can be substituted into the asymptotic gain model equation to find the overall gain G. The resulting gain is the \"current\" gain of the amplifier with a short-circuit load.\n\nIn the amplifier of Figure 6, \"R\" and \"R\" are in parallel.\nTo obtain the transresistance gain, say \"A\", that is, the gain using voltage as output variable, the short-circuit current gain \"G\" is multiplied by \"R // R\" in accordance with Ohm's law:\n\nThe \"open-circuit\" voltage gain is found from \"A\" by setting \"R\" → ∞.\n\nTo obtain the current gain when load current \"i\" in load resistor \"R\" is the output variable, say \"A\", the formula for current division is used: \"i = i × R / ( R + R )\" and the short-circuit current gain \"G\" is multiplied by this loading factor:\n\nOf course, the short-circuit current gain is recovered by setting \"R\" = 0 Ω.\n\n\n"}
{"id": "1786983", "url": "https://en.wikipedia.org/wiki?curid=1786983", "title": "Canada/USA Mathcamp", "text": "Canada/USA Mathcamp\n\nCanada/USA Mathcamp is a five-week academic summer program for middle and high school students with talent and passion for mathematics. Rather than training students for contests or providing courses for credit, Mathcamp introduces students to the various branches of advanced mathematics they are otherwise unlikely to discover until well into their college years.\n\nMathcamp was founded in 1993 by Dr. George Thomas, who recognized that students interested in mathematics frequently lacked the resources and camaraderie to pursue their interest. Mira Bernstein became the director when Thomas left in 2002 to found MathPath, a program for younger students.\n\nMathcamp is held each year on a different college campus in the United States or Canada. Past locations have included the University of Toronto, the University of Washington, Colorado College, Reed College, University of Puget Sound, Colby College, the University of British Columbia and Mount Holyoke College. Mathcamp enrolls about 120 students yearly, 45-55 returning and 65-75 new.\n\nThe application process for new students includes an entrance exam (the \"Qualifying Quiz\"), personal essay, and two letters of recommendation, but no grade reports. The process helps ensure that the students who are most passionate about math come to camp. Admission is selective: in 2016, the acceptance rate was 15%.\n\nClasses at Mathcamp come in four designations of pace and difficulty. The milder classes often include basic proof techniques, number theory, graph theory, and combinatorial game theory, while the spicier classes cover advanced topics in abstract algebra, topology, theoretical computer science, category theory, and mathematical analysis. There are generally four class periods each day and five classes offered during each period, allowing students to create a custom schedule of classes that match their interests and background. Graduate student mentors teach most of the classes, while undergraduate junior counselors, all of them Mathcamp alumni, do most of the behind-the-scenes work. Each year there are a number of renowned guest speakers, who have included John Conway, Avi Wigderson, and Serge Lang.\n\nSince Mathcamp is as much designed to let students interested in mathematics meet each other as for them to meet math, staff and campers organize a number of non-mathematical events such as field trips, hikes, a puzzle hunt, and game tournaments. The campers also enjoy a great deal of freedom and are expected to act responsibly. In this way, Mathcamp often eases the transition to the full freedom and responsibility of college.\n\nMathcamp culture is best described as quirky and fun-loving. In 2004, some campers created Foodtongue, a constructed language in which every word is a word that means a food in the English language. One of the cardinal rules of the language is an agreed ban of direct translation. Foodtongue remains popular among campers, and there now exists an active wiki, updated and referenced by speakers of the language, many of them campers from later years.\n\n"}
{"id": "20302266", "url": "https://en.wikipedia.org/wiki?curid=20302266", "title": "Carl Adolf Otth", "text": "Carl Adolf Otth\n\nCarl Adolf (Adolphe) Otth (April 2, 1803, Bern - May 16, 1839) was a Swiss physician and naturalist. He was the brother of mycologist Gustav Heinrich Otth (1806-1874).\n\nIn 1822 he studied medicine in Bern, and afterwards attended classes on natural history in Geneva, where he had as instructors, Augustin Pyramus de Candolle (1778-1841) and Nicolas Charles Seringe (1776-1858). He later studied medicine at the Universities of Kiel and Berlin, where in 1828 he received his doctorate. After six months in Paris, he returned to Bern.\n\nIn 1836 as a naturalist, he journeyed to Dauphiné and Provence in France, to the Balearic Islands and also to Algeria. From these travels he collected a large number of insect, reptile and amphibian species. In 1837 he was the first to describe the frog genus \"Discoglossus\" based on studies of the Mediterranean painted frog (\"Discoglossus pictus\").\n\nIn 1838 he published a book with thirty lithographs based on a trip to Algiers, titled \"Esquisses africaines, dessinées pendant un voyage a Alger et lithographiées par Adolphe Otth\". In 1839 during a journey to the Middle East, he died in Jerusalem at the age of 36.\n"}
{"id": "42711872", "url": "https://en.wikipedia.org/wiki?curid=42711872", "title": "Collective Behavior and Social Movements Section of the ASA", "text": "Collective Behavior and Social Movements Section of the ASA\n\nCollective Behavior and Social Movements (CBSM) is a section of the American Sociological Association (ASA) composed of sociologists who focus on the study of emerging and extra-institutional group phenomena. These include the behaviors associated with crowds, disasters, fads, revolutionary movements, riots, and social movements. The purpose of the section is to foster the study of these topics, which is done so by communicating through its newsletter \"Critical Mass\", organizing research-related participation, and sponsoring workshops.\n\nWithin the larger ASA, there are constituent parts known as sections. In the 1970s, there was a desire among some members of the ASA to establish a group that would study collective behavior and social movements as a fused topic. Since the ASA section on social psychology had, at the time, just been reorganized, one proposal was to establish a collective behavior-social movement group as a subsection of the newly reconstituted social psychology section. In response to this idea, sociologists Enrico Quarantelli and Jack Weller conducted a survey, whose results indicated that some social movement academics felt that a collective behavior-social movement group would be misplaced as a subsection within social psychology. In order to communicate about this controversy, a newsletter was created: \"Critical Mass\".\n\nThe first issue of \"Critical Mass\", published in October 1973, was written by sociologist Thelma McCormack. McCormack suggested that the name “Social Organization” would be appropriate for a new section interested in collective behavior and social movements. However, John Lofland, who was central in the effort to establish an entirely separate section within the ASA for this cause, responded adamantly that the title should pay homage to the traditional link between collective behavior and social movements. The bond between collective behavior and social movements had formed earlier in the twentieth century through the work of Robert Ezra Park and Herbert Blumer. Since then, American sociological tradition had maintained that link. Thus, in 1978, a formal petition was circulated to create a new section, which would be called Collective Behavior and Social Movements (CBSM). CBSM was officially made a section of the ASA in 1980, and is now one of the largest and most active sections of the ASA.\n\nIn recent years, the relevance of \"CB\" (Collective Behavior) in \"CBSM\" has been questioned. For some, the CB in CBSM has been replaced by CA (Collective Action). This has created the potential for a chasm between the two orientations. Additionally, as the section gains more international attendance, the link between collective behavior and social movements has become more obscure, given that the traditional American sociological link between the two areas is tenuous for non-US academics.\n\nThe newsletter of the CBSM section is \"Critical Mass.\" In the areas of physics and chemistry, critical mass refers to the amount of fissile material needed for nuclear fission. Drawing upon this meaning, social movement scholars and activists use the term critical mass in reference to the idea that some threshold of participants or action must be crossed in order for a social movement to burst into existence. Fittingly, the newsletter and its title pre-dated the formation of the CBSM section, which was itself borne out of action and movement.\n\nThe CBSM section of the ASA gives the following awards:\n\n"}
{"id": "838846", "url": "https://en.wikipedia.org/wiki?curid=838846", "title": "Concept inventory", "text": "Concept inventory\n\nA concept inventory is a criterion-referenced test designed to help determine whether a student has an accurate working knowledge of a specific set of concepts. Historically, concept inventories have been in the form of multiple-choice tests in order to aid interpretability and facilitate administration in large classes. Unlike a typical, teacher-authored multiple-choice test, questions and response choices on concept inventories are the subject of extensive research. The aims of the research include ascertaining (a) the range of what individuals think a particular question is asking and (b) the most common responses to the questions. Concept inventories are evaluated to ensure test reliability and validity. In its final form, each question includes one correct answer and several distractors.\n\nIdeally, a score on a criterion-referenced test reflects the amount of content knowledge a student has mastered. Criterion-referenced tests differ from norm-referenced tests in that (in theory) the former is not used to compare an individual's score to the scores of the group. Ordinarily, the purpose of a criterion-referenced test is to ascertain whether a student mastered a predetermined amount of content knowledge; upon obtaining a test score that is at or above a cutoff score, the student can move on to study a body of content knowledge that follows next in a learning sequence. In general, item difficulty values ranging between 30% and 70% are best able to provide information about student understanding.\n\nThe distractors are incorrect or irrelevant answers that are usually (but not always) based on students' commonly held misconceptions. Test developers often research student misconceptions by examining students' responses to open-ended essay questions and conducting \"think-aloud\" interviews with students. The distractors chosen by students help researchers understand student thinking and give instructors insights into students' prior knowledge (and, sometimes, firmly held beliefs). This foundation in research underlies instrument construction and design, and plays a role in helping educators obtain clues about students' ideas, scientific misconceptions, and didaskalogenic (\"teacher-induced\" or \"teaching-induced\") confusions and conceptual lacunae that interfere with learning.\n\nConcept inventories are education-related diagnostic tests. In 1985 Halloun and Hestenes introduced a \"multiple-choice mechanics diagnostic test\" to examine students' concepts about motion. It evaluates student understanding of basic concepts in classical (macroscopic) mechanics. A little later, the Force Concept Inventory (FCI), another concept inventory, was developed. The FCI was designed to assess student understanding of the Newtonian concepts of force. Hestenes (1998) found that while \"nearly 80% of the [students completing introductory college physics courses] could state Newton's Third Law at the beginning of the course. FCI data showed that less than 15% of them fully understood it at the end\".These results have been replicated in a number of studies involving students at a range of institutions (see sources section below). That said, there remains questions as what exactly the FCI measures. Results from Hake (1998) using the FCI have led to greater recognition in the science education community of the importance of students' \"interactive engagement\" with the materials to be mastered. .\n\nSince the development of the FCI, other physics instruments have been developed. These include the Force and Motion Conceptual Evaluation developed by Thornton and Sokoloff and the Brief Electricity and Magnetism Assessment developed by Ding et al. For a discussion of how a number of concept inventories were developed see Beichner. Information about physics concept tests can be found at the NC State Physics Education Research Group website (see the external links below).\n\nIn addition to physics, concept inventories have been developed in statistics, chemistry, astronomy, basic biology, natural selection, genetics, engineering, geoscience. and computer science.\n\nIn many areas, foundational scientific concepts transcend disciplinary boundaries. An example of an inventory that assesses knowledge of such concepts is an instrument developed by Odom and Barrow (1995) to evaluate understanding of diffusion and osmosis. In addition, there are non-multiple choice conceptual instruments, such as the essay-based approach suggested by Wright et al. (1998) and the essay and oral exams used by Nehm and Schonfeld (2008). and Cooper et al to measure student understanding of Lewis structures in chemistry.\n\nSome concept inventories are problematic. The concepts tested may not be fundamental or important in a particular discipline, the concepts involved may not be explicitly taught in a class or curriculum, or answering a question correctly may require only a superficial understanding of a topic. It is therefore possible to either over-estimate or under-estimate student content mastery. While concept inventories designed to identify trends in student thinking may not be useful in monitoring learning gains as a result of pedagogical interventions, disciplinary mastery may not be the variable measured by a particular instrument. Users should be careful to ensure that concept inventories are actually testing conceptual understanding, rather than test-taking ability, language skills, or other abilities that can influence test performance.\n\nThe use of multiple-choice exams as concept inventories is not without controversy. The very structure of multiple-choice type concept inventories raises questions involving the extent to which complex, and often nuanced situations and ideas must be simplified or clarified to produce unambiguous responses. For example, a multiple-choice exam designed to assess knowledge of key concepts in natural selection does not meet a number of standards of quality control. One problem with the exam is that the two members of each of several pairs of parallel items, with each pair designed to measure exactly one key concept in natural selection, sometimes have very different levels of difficulty. Another problem is that the multiple-choice exam overestimates knowledge of natural selection as reflected in student performance on a diagnostic essay exam and a diagnostic oral exam, two instruments with reasonably good construct validity. Although scoring concept inventories in the form of essay or oral exams is labor-intensive, costly, and difficult to implement with large numbers of students, such exams can offer a more realistic appraisal of the actual levels of students' conceptual mastery as well as their misconceptions. Recently, however, computer technology has been developed that can score essay responses on concept inventories in biology and other domains (Nehm, Ha, & Mayfield, 2011), promising to facilitate the scoring of concept inventories organized as (transcribed) oral exams as well as essays.\n\n"}
{"id": "26244741", "url": "https://en.wikipedia.org/wiki?curid=26244741", "title": "Conrad Schlumberger Award", "text": "Conrad Schlumberger Award\n\nThe Conrad Schlumberger Award is an award given to one of the members of European Association of Geoscientists and Engineers. The award is given each year to one that has made an outstanding contribution over a period of time to the scientific and technical advancement of the geosciences, particularly geophysics. The award is made annually by the EAGE Board.\n\nThe \"Conrad Schlumberger Award\" was created in 1955, as a recognition of Conrad Schlumberger's outstanding contribution to exploration geophysics, by the European Association of Geoscientists and Engineers (then named The European Association of Exploration Geophysicists.)\n\nSource:EAGE\n\n\n\n"}
{"id": "59863", "url": "https://en.wikipedia.org/wiki?curid=59863", "title": "Correspondence principle", "text": "Correspondence principle\n\nIn physics, the correspondence principle states that the behavior of systems described by the theory of quantum mechanics (or by the old quantum theory) reproduces classical physics in the limit of large quantum numbers. In other words, it says that for large orbits and for large energies, quantum calculations must agree with classical calculations.\n\nThe principle was formulated by Niels Bohr in 1920, though he had previously made use of it as early as 1913 in developing his model of the atom.\n\nThe term codifies the idea that a new theory should reproduce under some conditions the results of older well-established theories in those domains where the old theories work. This concept is somewhat different from the requirement of a formal limit under which the new theory reduces to the older, thanks to the existence of a deformation parameter. \n\nClassical quantities appear in quantum mechanics in the form of expected values of observables, and as such the Ehrenfest theorem (which predicts the time evolution of the expected values) lends support to the correspondence principle.\n\nThe rules of quantum mechanics are highly successful in describing microscopic objects, atoms and elementary particles. But \"macroscopic systems,\" like springs and capacitors, are accurately described by classical theories like classical mechanics and classical electrodynamics. If quantum mechanics were to be applicable to macroscopic objects, there must be some limit in which quantum mechanics reduces to classical mechanics. \"Bohr's correspondence principle demands that classical physics and quantum physics give the same answer when the systems become large\". A. Sommerfeld (1924) referred to the principle as \"Bohrs Zauberstab\" (Bohr's magic wand).\n\nThe conditions under which quantum and classical physics agree are referred to as the correspondence limit, or the classical limit. Bohr provided a rough prescription for the correspondence limit: it occurs \"when the quantum numbers describing the system are large\". A more elaborated analysis of quantum-classical correspondence (QCC) in wavepacket spreading leads to the distinction between robust \"restricted QCC\" and fragile \"detailed QCC\". \"Restricted QCC\" refers to the first two moments of the probability distribution and is true even when the wave packets diffract, while \"detailed QCC\" requires smooth potentials which vary over scales much larger than the wavelength, which is what Bohr considered.\n\nThe post-1925 new quantum theory came in two different formulations. In matrix mechanics, the correspondence principle was built in and was used to construct the theory. In the Schrödinger approach classical behavior is not clear because the waves spread out as they move. Once the Schrödinger equation was given a probabilistic interpretation, Ehrenfest showed that Newton's laws hold on average: the quantum statistical expectation value of the position and momentum obey Newton's laws.\n\nThe correspondence principle is one of the tools available to physicists for selecting quantum theories corresponding to reality. The principles of quantum mechanics are broad: states of a physical system form a complex vector space and physical observables are identified with Hermitian operators that act on this Hilbert space. The correspondence principle limits the choices to those that reproduce classical mechanics in the correspondence limit.\n\nBecause quantum mechanics only reproduces classical mechanics in a statistical interpretation, and because the statistical interpretation only gives the probabilities of different classical outcomes, Bohr has argued that quantum physics does not reduce to classical mechanics similarly as classical mechanics emerges as an approximation of special relativity at small velocities. He argued that classical physics exists independently of quantum theory and cannot be derived from it. His position is that it is inappropriate to understand the experiences of observers using purely quantum mechanical notions such as wavefunctions because the different states of experience of an observer are defined classically, and do not have a quantum mechanical analog. The relative state interpretation of quantum mechanics is an attempt to understand the experience of observers using only quantum mechanical notions. Niels Bohr was an early opponent of such interpretations.\n\nMany of these conceptual problems, however, resolve in the phase-space formulation of quantum mechanics, where the \"same variables with the same interpretation\" are utilized to describe both quantum and classical mechanics.\n\nThe term \"correspondence principle\" is used in a more general sense to mean the reduction of a new scientific theory to an earlier scientific theory in appropriate circumstances. This requires that the new theory explain all the phenomena under circumstances for which the preceding theory was known to be valid, the \"correspondence limit\".\n\nFor example, \n\nIn order for there to be a correspondence, the earlier theory has to have a domain of validity—it must work under \"some\" conditions. Not all theories have a domain of validity. For example, there is no limit where Newton's mechanics reduces to Aristotle's mechanics because Aristotle's mechanics, although academically dominant for 18 centuries, does not have any domain of validity.\n\nIf an electron in an atom is moving on an orbit with period , classically the electromagnetic radiation will repeat itself every orbital period. If the coupling to the electromagnetic field is weak, so that the orbit doesn't decay very much in one cycle, the radiation will be emitted in a pattern which repeats every period, so that the Fourier transform will have frequencies which are only multiples of . This is the classical radiation law: the frequencies emitted are integer multiples of .\n\nIn quantum mechanics, this emission must be in quanta of light, of frequencies consisting of integer multiples of , so that classical mechanics is an approximate description at large quantum numbers. This means that the energy level corresponding to a classical orbit of period must have nearby energy levels which differ in energy by , and they should be equally spaced near that level,\n\nBohr worried whether the energy spacing 1/ should be best calculated with the period of the energy state formula_2, or formula_3, or some average—in hindsight, this model is only the leading semiclassical approximation.\n\nBohr considered circular orbits. Classically, these orbits must decay to smaller circles when photons are emitted. The level spacing between circular orbits can be calculated with the correspondence formula. For a Hydrogen atom, the classical orbits have a period determined by Kepler's third law to scale as . The energy scales as , so the level spacing formula amounts to\nIt is possible to determine the energy levels by recursively stepping down orbit by orbit, but there is a shortcut.\n\nThe angular momentum of the circular orbit scales as . The energy in terms of the angular momentum is then\n\nAssuming, with Bohr, that quantized values of are equally spaced, the spacing between neighboring energies is\nThis is as desired for equally spaced angular momenta. If one kept track of the constants, the spacing would be , so the angular momentum should be an integer multiple of ,\n\nThis is how Bohr arrived at his model. Since only the level \"spacing\" is determined heuristically by the correspondence principle, one could always add a small fixed offset to the quantum number— could just as well have been . \n\nBohr used his physical intuition to decide which quantities were best to quantize. It is a testimony to his skill that he was able to get so much from what is only the leading order approximation. A less heuristic treatment accounts for needed offsets in the ground state L, cf. Wigner–Weyl transform.\n\nBohr's correspondence condition can be solved for the level energies in a general one-dimensional potential. Define a quantity which is a function only of the energy, and has the property that\n\nThis is the analog of the angular momentum in the case of the circular orbits. The orbits selected by the correspondence principle are the ones that obey for integer, since\n\nThis quantity is canonically conjugate to a variable which, by the Hamilton equations of motion changes with time as the gradient of energy with . Since this is equal to the inverse period at all times, the variable increases steadily from 0 to 1 over one period.\n\nThe angle variable comes back to itself after 1 unit of increase, so the geometry of phase space in coordinates is that of a half-cylinder, capped off at = 0, which is the motionless orbit at the lowest value of the energy. These coordinates are just as canonical as , but the orbits are now lines of constant instead of nested ovoids in space.\n\nThe area enclosed by an orbit is invariant under canonical transformations, so it is the same in space as in . But in the coordinates, this area is the area of a cylinder of unit circumference between 0 and , or just . So is equal to the area enclosed by the orbit in coordinates too,\n\nThe quantization rule is that the action variable is an integer multiple of .\n\nBohr's correspondence principle provided a way to find the semiclassical quantization rule for a one degree of freedom system. It was an argument for the old quantum condition mostly independent from the one developed by Wien and Einstein, which focused on adiabatic invariance. But both pointed to the same quantity, the action.\n\nBohr was reluctant to generalize the rule to systems with many degrees of freedom. This step was taken by Sommerfeld, who proposed the general quantization rule for an integrable system,\n\nEach action variable is a separate integer, a separate quantum number.\n\nThis condition reproduces the circular orbit condition for two dimensional motion: let be polar coordinates for a central potential. Then is already an angle variable, and the canonical momentum conjugate is , the angular momentum. So the quantum condition for reproduces Bohr's rule:\n\nThis allowed Sommerfeld to generalize Bohr's theory of circular orbits to elliptical orbits, showing that the energy levels are the same. He also found some general properties of quantum angular momentum which seemed paradoxical at the time. One of these results was that the z-component of the angular momentum, the classical inclination of an orbit relative to the z-axis, could only take on discrete values, a result which seemed to contradict rotational invariance. This was called \"space quantization\" for a while, but this term fell out of favor with the new quantum mechanics since no quantization of space is involved.\n\nIn modern quantum mechanics, the principle of superposition makes it clear that rotational invariance is not lost. It is possible to rotate objects with discrete orientations to produce superpositions of other discrete orientations, and this resolves the intuitive paradoxes of the Sommerfeld model.\n\nHere is a demonstration\nof how large quantum numbers can give rise to classical (continuous) behavior.\n\nConsider the one-dimensional quantum harmonic oscillator. Quantum mechanics tells us that the total (kinetic and potential) energy of the oscillator, , has a set of discrete values,\nwhere is the angular frequency of the oscillator.\n\nHowever, in a classical harmonic oscillator such as a lead ball attached to the end of a spring, we do not perceive any discreteness. Instead, the energy of such a macroscopic system appears to vary over a continuum of values. We can verify that our idea of macroscopic systems fall within the correspondence limit. The energy of the classical harmonic oscillator with amplitude , is\n\nThus, the quantum number has the value\n\nIf we apply typical \"human-scale\" values = 1kg, = 1 rad/s, and = 1 m, then ≈ 4.74×10. This is a very large number, so the system is indeed in the correspondence limit.\n\nIt is simple to see why we perceive a continuum of energy in this limit. With = 1 rad/s, the difference between each energy level is ≈ 1.05 × 10J, well below what we normally resolve for macroscopic systems. One then describes this system through an emergent classical limit.\n\nHere we show that the expression of kinetic energy from special relativity becomes arbitrarily close to the classical expression, for speeds that are much slower than the speed of light, .\n\nEinstein's mass-energy equation\nwhere the velocity, is the velocity of the body relative to the observer, formula_17 is the \"rest\" mass (the observed mass of the body at zero velocity relative to the observer), and is the speed of light.\n\nWhen the velocity vanishes, the energy expressed above is not zero, and represents the \"rest\" energy,\n\nWhen the body is in motion relative to the observer, the total energy exceeds the rest energy by an amount that is, by definition, the \"kinetic\" energy,\n\nUsing the approximation\nwe get, when speeds are much slower than that of light, or , \n\nwhich is the Newtonian expression for kinetic energy.\n"}
{"id": "18005152", "url": "https://en.wikipedia.org/wiki?curid=18005152", "title": "Coupon leverage", "text": "Coupon leverage\n\nCoupon leverage, or leverage factor, is the amount by which a reference rate is multiplied to determine the floating interest rate payable by an inverse floater. Some debt instruments leverage the particular effects of interest rate changes, most commonly in inverse floaters.\n\nAs an example, an inverse floater with a multiple may pay interest at the rate, or coupon, of 22 percent minus the product of 2 times the 1-month London Interbank Offered Rate (LIBOR). The coupon leverage is 2, in this example, and the reference rate is the 1-month LIBOR.\n\n"}
{"id": "37858202", "url": "https://en.wikipedia.org/wiki?curid=37858202", "title": "Development Policy Review", "text": "Development Policy Review\n\nDevelopment Policy Review is a peer-reviewed academic journal published by Wiley-Blackwell 6 times a year on behalf of the Overseas Development Institute (ODI). The journal was established in 1966 and focuses on the links between research and policy in international development, addressing contemporary questions from a range of disciplines across the social sciences.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2011 impact factor of 1.522, ranking it 12th out of 54 journals in the category \"Planning & Development\".\n"}
{"id": "15775369", "url": "https://en.wikipedia.org/wiki?curid=15775369", "title": "Drug fraud", "text": "Drug fraud\n\nDrug fraud is a type of fraud in which drugs, legal or illegal, are cut or altered in such a way that diminishes their value below that which they are sold for.\n\nThis type of drug fraud occurs then the dealer cuts or commingles the pure drug with a similar substance such as baby powder or powdered milk. When this is sold to the user, the user receives less of a high and so must buy more to get the previous high. There may also be adverse health consequences as a result of the cutting substance, such as a bad trip or overdose. This can also be seen in prescription drugs.\n\nLegal drug fraud occurs when a physician prescribes medication for a patient under false pretenses. This may be because the manufacturers of the drug have paid the doctor a fee to dispense their drug. Another cause may be drug pricing fraud, in which a physician prescribes a patient expensive drugs, that they may or may not need, in order to profit from the receipts.\n\nPatients too, may participate in this. A common method is the forging of doctor prescriptions to gain access to prescription medications. A somewhat rarer type is a citizen posing as a doctor to, among other things, gain access to the free samples of drugs that some drug manufacturers give out. The samples may also be sold to desperate patients at an exorbitant rate. Actual physicians may do this also. Others may prescribe drugs without sufficient cause.\n\nIllegal drug fraud is rarely addressed in court, as victims rarely involve police, but legal drug fraud is on the rise. Its upswing has increased calls for accountability as well as a bill being passed in the United States, 2007 Senate Bill 88.\n\n"}
{"id": "2784213", "url": "https://en.wikipedia.org/wiki?curid=2784213", "title": "Escuela Superior Latinoamericana de Informática", "text": "Escuela Superior Latinoamericana de Informática\n\nThe Escuela Superior Latinoamericana de Informática (Spanish for \"Latin American Superior School of Informatics\", ESLAI) was an Argentine undergraduate school of computer science established in 1986. Classes were held in a former countryhouse at the Pereyra Iraola Park in Buenos Aires Province, at approximately 40 km from Buenos Aires.\n\nThe school had Argentine mathematician Manuel Sadosky among its main founders. In spite of its short life, it had a considerable impact on informatics teaching and research in Argentina and South America. ESLAI courses were attended by students from several Spanish-speaking countries in South America such as Argentina, Uruguay, Paraguay, Bolivia, Peru, Ecuador, Colombia and Venezuela. All students had a full scholarship and the admission process was passed by about 15% of applicants. \n\nThe school established cooperation programs with a number of foreign universities in the Americas as well as in Europe. Those agreements sponsored important visitors to the school, such as Alberto O. Mendelzon, Jean-Raymond Abrial, Ugo Montanari, Carlo Ghezzi and Giorgio Ausiello, and enabled its students to attend graduate school at foreign universities.\n\nThe school had a remarkable European influence and was oriented towards theoretical aspects of computer science, such as typed lambda calculus, formal verification, and Martin-Löf's intuitionistic type theory.\n\nUnfortunately, the school was never able to develop a relationship with local companies, which in an emergent economy like Argentina's is essential to be involved with more practical problems. Without financial support, ESLAI had to close down in September 1990 during the presidency of Carlos Menem.\n\n"}
{"id": "6058086", "url": "https://en.wikipedia.org/wiki?curid=6058086", "title": "European Chemist", "text": "European Chemist\n\nEuropean Chemist (EurChem) is an international professional qualification awarded by the European Chemist Registration Board (ECRB) of the European Association for Chemical and Molecular Sciences for chemists and is for use in many European countries.\n\nAs Europe increasingly develops common standards, it is important that the professional competence of those who oversee the maintenance of standards is recognised. Academic qualifications alone have limited value. In providing an acceptable common professional standard, the European Chemist requires experience in the application of knowledge, level of skill, safety and environmental consciousness, sense of responsibility, ability to communicate and level of supervision received. Through the European Chemist designation the chemical societies in the EU have ensured that there is an easily understood title to indicate a high level of competence in the practice of chemistry. The award of EurChem will assist individual chemists who are moving from one employer to another in different member states, receiving equal treatment across the EU. \n\nEurChem Candidates must meet the following requirements:\n\n\nThe title EurChem is \"post-nominal\", i.e. it is placed after the name such as academic degrees. The ECRB maintains also a Register of European Chemists. The title is equivalent to the national chartered status, e.g. the title Chartered Chemist in the United Kingdom.\n\nRecognition of the qualification and title are generally not specifically incorporated into national law, however in the United Kingdom the Privy Council has approved the use of the title; however in all cases approval is only after peer review by the appropriate national , in addition the EU Directive 89/48/EEC generally exempts a bearer from additional examination in the Union.\n\n\n"}
{"id": "22935957", "url": "https://en.wikipedia.org/wiki?curid=22935957", "title": "Expression problem", "text": "Expression problem\n\nThe expression problem is a term used in discussing strengths and weaknesses of various programming paradigms and programming languages.\n\nPhilip Wadler coined the term in response to a discussion with Rice University's \"Programming Languages Team\" (PLT):\n\nThe expression problem is a new name for an old problem. The goal is to define a datatype by cases, where one can add new cases to the datatype and new functions over the datatype, without recompiling existing code, and while retaining static type safety (e.g., no casts).\n\nAt ECOOP '98, Krishnamurthi et al. presented a design pattern solution to the problem of simultaneously extending an expression-oriented programming language and its tool-set. They dubbed it the \"expressivity problem\" because they thought programming language designers could use the problem to demonstrate the expressive power of their creations. For PLT, the problem had shown up in the construction of DrScheme, now DrRacket, and they solved it via a rediscovery of mixins. To avoid using a programming language problem in a paper about programming languages, Krishnamurthi et al. used an old geometry programming problem to explain their pattern-oriented solution. In conversations with Felleisen and Krishnamurthi after the ECOOP presentation, Wadler understood the PL-centric nature of the problem and he pointed out that Krishnamurthi's solution used a cast to circumvent Java's type system. The discussion continued on the types mailing list, where Corky Cartwright (Rice) and Kim Bruce (Williams) showed how type systems for OO languages might eliminate this cast. In response Wadler formulated his essay and stated the challenge, \"whether a language can solve the expression problem is a salient indicator of its capacity for expression.\" The label \"expression problem\" puns on expression = \"how much can your language express\" and expression =\n\"the terms you are trying to represent are language expressions\".\n\nOthers co-discovered variants of the expression problem around the same time as Rice University's PLT, in particular Thomas Kühne in his dissertation, and Smaragdakis and Batory in a parallel ECOOP 98 article.\n\nSome follow-up work used the expression problem to showcase the power of programming language designs.\n\nThe expression problem is also a fundamental problem in multi-dimensional Software Product Line design and in particular as an application or special case of FOSD Program Cubes.\n\nThere are various solutions to the expression problem. Each solution varies in the amount of code a user must write to implement them, and the language features they require.\n\n\n\n\n"}
{"id": "9023795", "url": "https://en.wikipedia.org/wiki?curid=9023795", "title": "General Relativity (book)", "text": "General Relativity (book)\n\nGeneral Relativity is a popular textbook on Einstein's theory of general relativity written by Robert Wald.\n\nIt was published by the University of Chicago in 1984. The book, a tome of almost 500 pages, covers many aspects of the General Theory of Relativity. It is divided into two parts, the second of which covers more advanced topics such as causal structure, spinors and quantum effects. The book uses the Abstract index notation for tensors.\n"}
{"id": "7154152", "url": "https://en.wikipedia.org/wiki?curid=7154152", "title": "Information audit", "text": "Information audit\n\nThe information audit (IA) extends the concept of auditing holistically from a traditional scope of accounting and finance to the organisational information management system. Information is representative of a resource which requires effective management and this led to the development of interest in the use of an IA.\n\nPrior the 1990s and the methodologies of Orna, Henczel, Wood, Buchanan and Gibb, IA approaches and methodologies focused mainly upon an identification of formal information resources (IR). Later approaches included an organisational analysis and the mapping of the information flow. This gave context to analysis within an organisation's information systems and a holistic view of their IR and as such could contribute to the development of the information systems architecture (ISA). In recent years the IA has been overlooked in favour of the systems development process which can be less expensive than the IA, yet more heavily technically focused, project specific (not holistic) and does not favour the top-down analysis of the IA.\n\nA definition for the Information Audit cannot be universally agreed-upon amongst scholars, however the definition offered by ASLIB received positive support from a few notable scholars including Henczel, Orna and Wood; “(the IA is a) systematic examination of information use, resources and flows, with a verification by reference to both people and existing documents, in order to establish the extent to which they are contributing to an organisation’s objectives” In summary, the term audit itself implies a counting, the IA being much the same yet it counts IR and analyses how they are used and how critical they are to the success of a given task.\n\nIn much the same way as the IA is difficult to define, it can be utilised in a range of contexts by the information professional, from complying with freedom of information legislation to identifying any existing gaps, duplications, bottlenecks or other inefficiencies in information flows and to understand how existing channels can be used for knowledge transfer \n\nIn 2007 Buchanan and Gibb developed upon their 1998 examination of the IA process by outlining a summary of its main objectives:\n\n\nFurthermore, Buchanan and Gibb went on to state that the IA also had to meet the following additional objectives:\n\n\nIn 1976 Riley first published a definition of IA as a way of analysing IR based on a cost-benefit model. Since Riley, scholars have outlined further developed methodologies. Henderson took a cost-benefit approach hoping to draw focus from manpower-costing to information storage and acquisition which he felt was being overlooked. In 1985 Gillman focused upon identifying the relationships which existed between various components in order to map them to one another. Neither Henderson nor Gillman’s methods offered alternative approaches beyond the existing organisational frameworks. Quinn took a hybrid-approach combining Gillman and Henderson’s methods to identify the purpose of existing IR and to position them within the organisation, as did Worlock. The differentiator between Quinn and Worlock lay in Worlock’s consideration of solutions outside of the current organisational structure. These approaches had thus far had paid little attention to the needs of the user or in making structured recommendations for the development of a corporate information strategy. Therefore, here follows a brief outline and overall comparison of four published strategic approaches in order that one might understand the development of the IA methodology.\n\nIn 1988 Burk and Horton developed InfoMap, the first IA methodology developed for widespread use. It aimed to discover, map and evaluate the IR within an organisation using a 4-stage process:\nAlthough the method inventoried all IR (and therefore met standard ISO 1779) this bottom-up approach revealed limited analysis of the organisation holistically and the steps were not explicit enough.\n\nOrna produced a top-down methodology in contrast to Burk and Horton, placing emphasis upon the importance of organisational analysis and aimed to assist in the production of a corporate information policy. Initially the method had just 4-stages, this later revised to a 10-stage process which included pre and post-audit stages as below:\nOrna’s method introduced the need for a cyclical IA to be put in place in order for the IR to be continually tracked and improvements made regularly. Again this method was criticised for lacking some practical application and in 2004 Orna revised the methodology once more to try to rectify this problem \n\nIn 1998, similarly to Orna's earlier publication, Buchanan and Gibb took a top-down approach, drawing techniques from established management disciplines to provide a framework and a level of familiarity for information professionals. This set of techniques was a notable contribution to IA methodologies and understood the need to be flexible for each organisation. Theirs was a 5-stage process:\nThis was the introduction of a new approach to costing the IR and had an integrated strategic direction, yet the scholars admitted that this method may be impractical for smaller organisations.\n\nHenczel’s methodology drew upon the strengths of Orna and Buchanan and Gibb to produce a 7-stage process:\nFocus was made once more on the strategic direction of the organisation conducting the IA. Furthermore, Henczel made examination into the use of the IA as a first-step in the development of a knowledge audit or knowledge management strategy as discussed in the later section.\n\nScholars and information professionals have since tested the above methodologies with varied results. An early case study produced by Soy and Bustelo in a Spanish financial institution in 1999 aimed to identify the use of information resources for qualitative and quantitative data analysis due to the rapid expansion of the organisation within a six-year period. Although the methodology was not explicitly credited to any of the above-mentioned scholars, it did follow a strategic (post 1990's) IA process including gaining support from management, the use of questionnaires for data collection, analysis and evaluation of the data, identification and mapping of the IR, cost-analysis and outlining recommendations to assist with the establishment of an Information policy. In addition the IA report suggested that the process would need to be continual (cyclical as Orna, Henczel and Buchanan and Gibb suggest). Conclusions of this case-study stated that the IA allowed a greater understanding of the basis of the organisation's information policy and personnel and identified technical problems within the organisation. In addition this method was believed to be cost and time-effective to the two auditors involved and the organisation used the results of the IA as a marketing tool to promote services users may not have been previously aware of.\n\nIn 2006 a paper testing the 'viability' of Henczel's methodology was published in the \"South African Journal of Library and Information Science.\" The purpose of the study was to determine the financial position of an organisation (Statistics SA) through a report of financial statements made over a period of time. The IA was used as part of the holistic audit process and was limited to just the methodology of Henczel after consideration and dismissal of others. The IA followed the seven-stage process as outlined above (planning, data collection, analysis, evaluation, communicating and implementing recommendations and the IA as a continuum) and the objectives were outlined as identifying the needs of respondents, identifying information sources and their significance, mapping information flow and identifying gaps in available sources. Upon conducting the IA several recommendations were made including making print the preferred format for information and that the library should be responsible for missing information and address this in a development plan. Finally, advantages and disadvantages to this methodology were identified. It was reported that the methodology was flexible in its application as it provided a framework which could be adjusted to suit the needs of an organisation, that the scope could be easily expanded to cover more objectives and that guidelines for data collection and analysis were varied. Secondly it was felt that the methodology was cost-effective as it made use of existing resources (email, workspace etc.). The greatest disadvantage reported was that the process was 'cumbersome' and that the researcher became weary with the repetitive nature of the planning process. In conclusion, the study reported that Henczel's methodology had allowed the information professionals to effectively manage the information management activities of the public sector organisation in question. Positively it had been viewed as cost-effective and depicted a snapshot of the use of information in the organisation, yet it had been a 'cumbersome' process with some repetition within the planning phases.\n\nIn 2007 Buchanan and Gibb published another examination of the IA to include case studies of their own methodology, an element they had been previously criticised for not including. In the first case study conducted in a university research department, the objectives were to identify how effective information flow within the department was and what improvement would be required. The methodology was tailored to the study by removing the costing stage and establishing a workgroup to assist at various stages of the process, whilst all other stages remained as per the above (promote, identify, analyse and synthesise). Recommendations were made by the auditor towards greater synergy and systems analysis and it was found that staff immediately recognised the value of this output. \nThe second case study dealt with a public body within the arts sector with the objectives to streamline evaluation and approval processes and to improve communications between stakeholders. The sponsor also specified that a list of recommendations were required to be incorporated into a change management programme and this aligns well to Buchanan and Gibb's strategic directional method. The IA methodology was scoped to be primarily resource-orientated and once again the costing stage was removed. All else remained as per the original methodology. The IA output recommended that for all organisational processes a shift was needed from process-modelling to capturing process descriptions.\nThe main strengths identified in Buchanan and Gibb's methodology were its logical structuring of stages, provision of tools, inclusive process for stakeholders, adequate illustration of the role of effective Information Management within an organisation and much like Henczel, the flexibility to tailor the methodology. Their scope matrix was also proven to be useful. A weakness lay still in the limited instructional depth which affected the qualitative data analysis. As regards the applicability of this method, it should be noted that the costing stage was not included in either study and that this might suggest it is not required in an information audit. Further study is required to produce a more conclusive analysis of the methodology.\n\nThe conclusions of the case studies suggested that further development into the methodologies for IA was needed to include further explanation of tools, techniques, templates, interview preparation, process modelling and analysis. In 2008 Buchanan and Gibb drew comparisons from the published IA methodologies of the following scholars: Burk and Horton, Orna, Buchanan and Gibb and Henczel, with the purpose of understanding if a hybrid-methodology could be produced as a ‘baseline’ from all four.\n\nThe hybrid-methodology was formed of seven stages and is as below:\nHowever, Buchanan and Gibb themselves declared that this should not be considered a conclusive comparison as it is high-level \"and does not assess how well each methodology addresses each stage.\"\n\nIn more recent years, since the development of the top-down methodologies, IA have been used as a basis for the development of a knowledge audit, which itself in-turn contributes to an organisation's knowledge management strategy. Once complete, the IA allows examination into where knowledge is produced, where there may be need for further input and where knowledge transfer is required. Furthermore, this analysis develops strategy for knowledge capture, access, storage, dissemination and validation. Dissimilarly to the IA, the objectives of the knowledge audit are to identify any people-related issues which impact the ways in which knowledge is created, transferred and shared and to identify where knowledge could be captured, where it is required and then determine how best to undertake a knowledge transfer as \"unlike information, knowledge is bound to a person, organisation or community.\" Similarities between the knowledge and information audit methodologies can be noted however, as questionnaires, the development of an inventory, analysis of flow and a data map are here again used. The importance of this audit therefore is to understand the strategic significance of an organisation's knowledge assets to ensure management is focused to those areas it is specifically required.\n\n\nBUCHANAN, S. and GIBB, F., 1998. The information audit: an integrated strategic approach. \"International Journal of Information Management,\" 18(1), pp. 29–47.\n\nBUCHANAN, S. and GIBB, F., 2007. The information audit: role and scope. \"International Journal of Information Management,\" 27(3), pp. 159–172.\n\nBUCHANAN, S. and GIBB, F., 2008. The information audit: methodology selection. \"International Journal of Information Management,\" 28(1), pp. 3–11.\n\nBUCHANAN, S. and GIBB, F., 2008. The information audit: Theory versus practice. \"International Journal of Information Management,\" 28(3), pp. 150–160\n\nBURNETT, S. et al., 2004. Knowledge Auditing and Mapping: a Pragmatic Approach. \"Knowledge and Process Management\", 11(0), pp. 1–13.\n\nELLIS, D. et al., 1993. Information audits, communication audits and information mapping: a review and survey. \"International Journal of Information Management,\" 13(2), pp. 29–47.\n\nHENCZEL, S., 2001. \"The Information Audit: a practical guide.\" Wiltshire: Anthony Rowe Ltd.\n\nHENCZEL, S., 2000. The information audit as a first step towards effective knowledge management. \"IFLA Publications,\" 108(1), pp. 91–106.\n\nMEARNS, M.A. and DU TOIT, A.S.A., 2008. Knowledge audit: Tools of the trade transmitted to tools for tradition. \"International Journal of Information Management\", 28(1), pp. 161–167.\n\nRALIPHADA, L. and BOTHA, D., 2006. Testing the viability of Henczel’s information audit methodology in practice. \"South African Journal of Library and Information Science\", 72(3), pp. 242–250.\n\nSOY, C. and BUSTELO, C., 1999. A practical approach to information audit: case study. \"Managing Information,\" 6(9), pp. 30–36.\n\nSOY, C. and BUSTELO, C., 1999. A practical approach to information audit: case study La Caixa Bank, Barcelona Part 2. \"Managing Information,\" 6(10), pp. 60–61.\n\nWOOD, S., 2004. \"Information auditing: a guide for Information Managers.\" Middlesex: Freepint.\n\nBRYSON, J., 2006. \"Managing Information Services : A Transformational Approach\". Hampshire: Ashgate Publishing Ltd.\n\nGILLMAN, P.L., 1985. An analytical approach to the information audit. \"Electronic library,\" 3, pp. 56–60.\n\nHENDERSON, H.L., 1980. Cost effective information provision and the role for the information audit. \"Information Management,\" 1(4), pp. 7–9.\n\nHIGSON, C. and NOKES, S., 2004. \"The information audit asset register: a practitioner’s guide for the Freedom of Information Act\". London: Rivington Publishing Ltd.\n\nQUINN, A.V., 1979. The Information Audit: a new tool for the Information Manager. \"Information Manager\", 1(4), pp. 18–19.\n\nRILEY, R.H., 1976. The Information Audit. \"Bulletin of the American Society for Information Science\", 2(5), pp. 24–25.\n\nTHEAKSTON, C., 1998. An information audit of National Westminster Bank UK’s Learning and Resource Centres. \"International Journal of Information Management\", 18(5), pp. 371–375.\n\nWEBER, R., 1999. \"Information systems, control and audit\". New Jersey: Prentice-Hall Inc.\n\n"}
{"id": "54039873", "url": "https://en.wikipedia.org/wiki?curid=54039873", "title": "John Boland (chemist)", "text": "John Boland (chemist)\n\nJohn Boland is an Irish chemist specialising in nanoscale materials and systems who is Dean of Research at Trinity College Dublin.\n\nBoland earned a bachelor's degree in chemistry from University College Dublin and a PhD in chemical physics from the California Institute of Technology.\n\nIn the US, Boland was a researcher at the IBM T.J. Watson Research Center and at the University of North Carolina at Chapel Hill was J. J. Hermans Professor of Chemistry and Applied and Materials Sciences and head of physical, computational and materials chemistry in the School of Chemistry. At Trinity College Dublin he was a professor and in 2004 became director of the Centre for Research on Adaptive Nanostructures and Nanodevices. he is Dean of Research and a researcher at the university's AMBER material science research centre.\n\nBoland is a Fellow of Trinity College (elected 2008), of the American Vacuum Society (2009) and the American Association for the Advancement of Science (2010). In 2011 he was awarded the ACSIN Nanoscience Prize. In 2013 he was the recipient of the second European Research Council Advanced Award for the physical sciences in Ireland.\n\n"}
{"id": "14372314", "url": "https://en.wikipedia.org/wiki?curid=14372314", "title": "Lateral shoot", "text": "Lateral shoot\n\nCommonly called a branch. It develops from axillary buds on the stem's surface, extending laterally from the plant's stem.\n\nA lateral shoot is a part of a plant's shoot system.\nAs a plant grows it requires more energy, it also is required to out-compete nearby plants for this energy. One of the ways a plant can compete for this energy is to increase its height, another is to increase its overall surface area. That is to say, the more lateral shoots a plant develops, the more foliage the plant can support increases how much photosynthesis the plant can perform as it allows for more area for the plant to uptake carbon dioxide as well as sunlight. \n\nThrough testing with Arabidopsis thaliana (A plant considered a model organism for plant genetic studies) genes including MAX1 and MAX2 have been found to affect growth of lateral shoots. Gene knockouts of these genes cause abnormal proliferation of the plants affected, implying they are used for repressing said growth in wild type plants. Another set of experiments with Arabidopsis thaliana testing genes in the plant hormone florigen, two genes FT and TSF (which are abbreviations for Flowering Locus T, and Twin Sister of FT) when knocked out, appear to affect lateral shoot in a negative fashion. These mutants cause slower growth and improper formation of lateral shoots, which could also mean that lateral shoots are important to florigen's function. Along with general growth there are also transcription factors that directly effect the production of additional lateral shoots like the TCP family (also known as Teosinte branched 1/cycloidea/proliferating cell factor) which are plant specific proteins that suppress lateral shoot branching. Additionally the TCP family has been found to be partially responsible for inhibiting the cell's Growth hormone–releasing hormone (GHRF) which means it also inhibits cell proliferation.\nApical dominance\n\nShoot apical meristem\n\nAuxin, another plant growth hormone\n\nCell growth\n"}
{"id": "34035812", "url": "https://en.wikipedia.org/wiki?curid=34035812", "title": "Leopold Kober", "text": "Leopold Kober\n\nLeopold Kober (21 September 1883 – 6 September 1970), an influential Austrian geologist, proposed a number of (subsequently largely discredited) theories of orogeny and coined the term \"kratogen\" to describe stable continental crust, later this term was shortened to \"kraton\" by Hans Stille. Kober, developing geosyncline theory, posited that stable blocks known as forelands move toward each other, forcing the sediments of the intervening geosynclinal region to move over the forelands, forming marginal mountain ranges known as \"Randketten\", while leaving an intervening median mass known as the \"Zwischengebirge\".\n"}
{"id": "19455403", "url": "https://en.wikipedia.org/wiki?curid=19455403", "title": "Lindau Nobel Laureate Meetings", "text": "Lindau Nobel Laureate Meetings\n\nThe Lindau Nobel Laureate Meetings are annual, scientific conferences held in Lindau, Bavaria, Germany since 1951. Their aim is to bring together Nobel laureates and young scientists to foster scientific exchange between different generations and cultures.\n\nEvery Lindau Meeting consists of a multitude of scientific sessions like plenary lectures and panel discussions as well as a variety of networking and social events. The meetings assume a unique position amongst international scientific conferences: With 30-40 Nobel laureates attending they are the largest congregation of Nobel laureates (apart from the Nobel Prize Award Ceremonies in Stockholm) in the world. The meetings are not centered on the presentation of research results, but instead, their main goals are the exchange of ideas and the discussion of topics globally relevant to all scientists. The Nobel laureates do not receive any kind of payment for their participation and are free to choose the topics of their presentations. More than 300 are members of the meetings’ \"Founders Assembly.\"\n\nBilled by the organising Council for the Lindau Nobel Laureate Meetings as their ‘Mission Education’, the aim of the meetings is to facilitate the transfer of knowledge between Nobel laureates and young scientists but also among the international scientific community and the general public. The opportunity for participants to form international networks of scientists is also regarded as a prime objective by the organisers. The meetings’ leitmotif is \"‘Educate, Inspire, Connect.’\"\n\nAfter World War II, Germany was disconnected from the international scientific community due to the ramifications of the Nazi regime. During this time, hardly any scientific conferences of high value took place in Germany.\n\nThe two physicians Franz Karl Hein and Gustav Parade from Lindau, a small town located on the Bavarian shore of Lake Constance, conceived the idea of organising a scientific meeting to bring together German researchers and physicians with Nobel laureates. They convinced Count Lennart Bernadotte af Wisborg, a member of the Swedish Royal Family and proprietor of nearby Mainau Island, to call upon his good connections to Sweden’s Nobel Committee to support the undertaking. The first meeting, subsequently held in 1951, was dedicated to the fields of medicine and physiology and was attended by seven Nobel laureates, among them Adolf Butenandt and Hans von Euler-Chelpin. After the success of the initial meeting, the scientific scope was broadened to include the other two natural science Nobel Prize disciplines chemistry and physics. Thus, a mode of annually alternating disciplines for the meetings was established.\nIn 1954, the Council for the Lindau Nobel Laureate Meetings was founded and henceforth established as the organising committee of the meetings. Count Lennart was appointed as first president of the Council. Also in 1954, the concept of inviting students and young scientists to the meetings was introduced. This step was seen as a measure to add additional value for society to the meetings. Among the young scientists participating that year were also students from Eastern Germany.\n\nWhile originally conceived by Hein and Parade as a European meeting of scientists, the Lindau Nobel Laureate Meetings slowly but steadily became more international. In the beginnings, only students from Lake Constance’s bordering countries Germany, Switzerland, and Austria attended but year after year new nations began to send representatives. Since 2000, each Lindau Meeting is attended by young scientists from between 80 and 90 countries.\n\nIn 1987, Count Bernadotte resigned from his position as president of the Council due to health reasons and his wife, Countess Sonja Bernadotte af Wisborg, took over.\n\nShortly before the turn of the millennium, the future of the Lindau Nobel Laureate Meetings was endangered due to financial uncertainties. In order to counter this negative development, Countess Sonja Bernadotte expanded the Council and added experts from charitable foundations and public affairs as well as representatives of Stockholm’s Nobel Foundation to the committee.\n\nTwo main goals of Countess Sonja Bernadotte’s aegis were the further internationalisation of the meetings and to improve its public image, both domestically and internationally.\n\nOn the occasion of the 50th Lindau Nobel Laureate Meeting in the year 2000, the establishment of the \"Foundation Lindau Nobel Laureate Meetings\" was officially announced. Its main goal since then has been to secure the funding of the Lindau Nobel Laureate Meetings. Upon its creation, over 40 Nobel Laureates joined the Founders Assembly of the foundation and support the continuance of the conference both morally and financially.\n\nThe 50th Lindau Meeting in 2000 was also the first interdisciplinary meeting that united Nobel laureates and students from all three natural science disciplines (physics, chemistry and physiology or medicine) of the Nobel Prize.\n\nWhen Countess Sonja Bernadotte died in October 2008, her daughter, Countess Bettina Bernadotte, was elected President of the Council. She continued her mother’s course and worked on establishing further cooperation with research institutions around the world but also introduced educational aspects for society in general to the Lindau Meetings.\n\nEver since their inception, the Lindau Nobel Laureate Meetings have taken place in the small Bavarian town of Lindau on the shores of Lake Constance. The city’s center is located on an island in the lake that is connected to the mainland via bridges.\n\nThe meetings focus alternately on physiology and medicine, physics, and chemistry – the three natural science Nobel Prize disciplines. An interdisciplinary meeting revolving around all three natural sciences is held every five years. In addition, the Lindau Meeting on Economic Sciences is held every three years with recipients of the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel.\nThe following session types are currently part of the scientific programme of the Lindau Nobel Laureate Meetings:\nThe Lindau Nobel Laureate Meetings also host several events with social functions like dinners, BBQs and cultural events. Since 2010 one evening of each meeting is dedicated to an alternating partner country. The partner countries so far have been India (2009), the European Union (2010), the United States of America (2011), Singapore (2012), South Korea (2013), Australia (2014), France (2015) and Austria (2016).\n\nThe last day of each meeting includes both scientific and social programme features with a boat trip to Mainau Island. The visit to the island owned by the Bernadotte family is designed both to let the meetings culminate in a final panel discussion on a topic combining science and society and to give the young scientists the opportunity to visit Mainau – the ‘flower island’.\n\nMeetings dedicated to a single discipline are usually attended by about 30-40 Nobel laureates and 500-600 young scientists representing around 80 different countries. In addition, several special guests of honour from politics, industry and academia as well as international journalists attend the meetings.\n\nYoung scientists who want to participate need to pass a multi-stage application process. The application is open to undergraduates, PhD students and post-doc researchers under the age of 35 who are at the top of their class and do not hold a permanent position yet. Young scientists can only participate once in a Lindau meeting.\n\nOver the years many noteworthy guests of honour have visited the Lindau Nobel Laureate Meetings: Among them former German Presidents Roman Herzog, Johannes Rau, Horst Köhler and Christian Wulff. Current German President Joachim Gauck gave the opening address at the interdisciplinary meeting in 2015 and German Chancellor Angela Merkel held the opening speech at the 2014 meeting on economic sciences.\nFurther notable guests include philanthropist and software entrepreneur Bill Gates, former President of the European Commission José Manuel Barroso and the President of Singapore Tony Tan.\n\nThe Lindau Nobel Laureate Meetings are made up of two legal bodies: The \"Council\" and the \"Foundation\". While the council’s responsibility is to put together the scientific programme and to organise the meetings via its executive secretariat, the foundation’s task is to maintain the financial stability of the meetings and to ensure ongoing financial support.\n\nThe Council sustains a global network of academic partner institutions that range from national science academies, to universities, foundations and government ministries. Applications for meeting participation need to be addressed to the academic partners, who in turn nominate the best young scientists for the meetings.\n\nThe Lindau meetings are funded by both public resources and private donations. The costs and funding of each meeting are made public in the annual report of the respective year. Private supporters are listed on the organisation’s website and various other publications.\n\nEver since their beginnings, the Lindau Nobel Laureate Meetings have aimed to facilitate an atmosphere in which scientists could assume more responsibility towards society. Therefore, several political appeals have been issued in Lindau over the years.\n\nThe impact these meetings have had on the careers of the roughly 30,000 young scientists, who have participated, is hard to quantify but profound: As one of the supporters put it: \"You see the deep inspiration and motivation for the years to come in the eyes of the young researchers when they leave Lindau.”\n\nThe meetings provide many networking opportunities for young scientists who in turn form global networks that often yield research collaborations or knowledge transfer not limited by borders or differing cultures.\n\"See article: Mainau Declaration\"\n\nIn 1955 at the 5th Lindau Nobel Laureate Meeting German physics Nobel laureates Max Born and Otto Hahn initiated the ‘Mainau Declaration against the Use of Nuclear Weapons’ that was meant to urge world leaders to abstain from using nuclear weapons. It was initially signed by 18 Nobel laureates attending the meeting, but the number of signatories grew to 52 within a year.\n\nFifty years later, at the 65th Lindau Nobel Laureate Meeting in the summer of 2015, a second Mainau Declaration was issued, this time making a statement on the need to combat climate change. The declaration was initially signed by 36 attending Nobel laureates who were later joined by 35 additional colleagues.\n\nDue to the fact that the Lindau Meetings have such a long tradition and history, a digital and open to the public archive was established. It currently contains about 400 hours of video footage of lectures held by Nobel laureates during the Lindau meetings. In addition, photos, animated educational films and interactive content like virtual tours of Nobel laureates’ laboratories and an interactive map showing the career paths of the laureates are also available.\nThe mediatheque is also used as an educational tool providing topic dossiers and introductions to certain scientific fields that can be used by teachers and professors.\n\nThe Lindau Nobel Laureate Meetings are also actively engaging in outreach projects and science communication based on their \"‘Mission Education’\" leitmotif. Among these projects are the photo exhibition ‘Sketches of Science’ by German photographer Volker Steger being exhibited world-wide and a permanent exhibition on the history of the meetings in Lindau’s city museum. Additional educational content is provided through close collaborations with media partners such as the Nature Publishing Group.\n\n\n"}
{"id": "30943833", "url": "https://en.wikipedia.org/wiki?curid=30943833", "title": "List of Copper Country smelters", "text": "List of Copper Country smelters\n\nThere were seven copper smelters built in the Copper Country in the Upper Peninsula of Michigan:\n\n\n"}
{"id": "23077263", "url": "https://en.wikipedia.org/wiki?curid=23077263", "title": "List of Northern Cordilleran volcanoes", "text": "List of Northern Cordilleran volcanoes\n\nThe geography of northwestern British Columbia and Yukon, Canada is dominated by volcanoes of the Northern Cordilleran Volcanic Province formed due to continental rifting of the North American Plate. It is the most active volcanic region in Canada. Some of the volcanoes are notable for their eruptions, for instance, Tseax Cone for its catastrophic eruption estimated to have occurred in the 18th century which was responsible for the death of at least 2,000 Nisga'a people from poisonous volcanic gases, the Mount Edziza volcanic complex for at least 20 eruptions throughout the past 10,000 years, and The Volcano (also known as Lava Fork volcano) for the most recent eruption in Canada during 1904. The majority of volcanoes in the Northern Cordilleran Volcanic Province lie in Canada while a very small portion of the volcanic province lies in the U.S. state of Alaska.\n\nVolcanoes of the Northern Cordilleran Volcanic Province are a part of the Pacific Ring of Fire. The largest and most persistent volcanoes are the Mount Edziza volcanic complex and Level Mountain in northwestern British Columbia which have had volcanic activity for millions of years. In the past 7.5 million years, the Mount Edziza volcanic complex has had five phases of volcanic activity while Level Mountain north of Edziza has had three phases of volcanic activity in the past 14.9 million years. The Mount Edziza volcanic complex has been made into a provincial park since 1972 to protect its volcanic landscape. The 102 Northern Cordilleran volcanoes in the list below are grouped into their political regions in north-south order.\n\nThere is no single standard definition for a volcano. It can be defined from individual vents, volcanic edifices or volcanic fields. Interior of ancient volcanoes may have been eroded, creating a new subsurface magma chamber as a separate volcano. Many contemporary volcanoes rise as young parasitic cones from flank vents or at a central crater. Some volcanoes are grouped into one volcano name, for instance, the Mount Edziza volcanic complex, although individual vents are named by local people. The status of a volcano, either active, dormant or extinct, cannot be defined precisely. An indication of a volcano is determined by either its historical records, potassium-argon dating, radiocarbon dating, or geothermal activities.\n\nThe primary source of the list below is taken from the Geological Survey of Canada website, compiled by the Earth Sciences Sector of Natural Resources Canada, in which Northern Cordilleran volcanoes in the past 66.4 million years are listed. The Geological Survey of Canada use a catalogue of volcanoes grouped by volcano fields, lava fields and mountain ranges. The Geological Survey of Canada list is the most complete list of volcanoes in the Northern Cordilleran Volcanic Province, but work of understanding the frequency and eruption characteristics at volcanoes in Canada is a slow process. This is because most of Canada's dormant and potentially active volcanoes are located in isolated jagged regions, very few scientists study Canadian volcanoes and the provision of money in the Canadian government is limited. Because of these issues, scientists that study Canada's volcanoes have a basic understanding of Canada's volcanic heritage and how it might impact people in the future. Therefore, instead of using the dates of recorded eruptions, the Geological Survey of Canada mostly uses geological epochs for estimating when a volcano last erupted. Geological epoches include the Cenozoic (66.4 million years ago to present) and its subdivisions Miocene (23.7 to 5.3 million years ago), Pliocene (5.3 to 1.6 million years ago), Quaternary (1.6 million years ago to present), Pleistocene (1.6 to 0.01 million years ago) and Holocene (0.01 million years ago to present).\n\nThe northernmost portion of the Northern Cordilleran Volcanic Province extends just across the Alaska-Yukon border into the Southeast Fairbanks Census Area of eastcentral Alaska. Here, a single cinder cone, dated at 177,000 years old occurs within the metamorphic and granitic composed upland of the Yukon-Tanana Terrane. Prindle Volcano is approximately west of the Alaska-Yukon border.\n\nThe central portion of the Northern Cordilleran Volcanic Province extends through Yukon where very few Northern Cordilleran volcanoes exist. Near the junction of the Yukon and Pelly rivers in central Yukon lies the Fort Selkirk Volcanic Field. It is the northernmost Holocene age volcanic field in Canada, consisting of a sequence of valley-filling basalt and basanite lava flows. Further south near the capital city of Whitehorse, a group of volcanoes and lava flows were constructed near Alligator Lake possibly in the past 10,000 years.\n\nOver half of the Northern Cordilleran volcanoes are located in northwestern British Columbia. This portion is where the most recent eruptions in Canada and of the Northern Cordilleran Volcanic Province have occurred, including the catastrophic 18th century eruption of Tseax Cone and the 1904 eruption of The Volcano.\n\nThe Northern Cordilleran volcanoes of British Columbia comprises shield volcanoes, stratovolcanoes, subglacial volcanoes, lava domes and a large number of small cinder cones and associated lava plains. The Northern Cordilleran volcanoes of northwestern British Columbia are disposed along short, northerly trending segments which are unmistakably involved with north-trending rift structures including synvolcanic grabens and grabens with one major fault line along only one of the boundaries (half-grabens) similar to those associated with the East African Rift, which extends from the Afar Triple Junction southward across eastern Africa.\n\n\n"}
{"id": "12038022", "url": "https://en.wikipedia.org/wiki?curid=12038022", "title": "List of Six Sigma software packages", "text": "List of Six Sigma software packages\n\nThere are generally four classes of software used to support the Six Sigma process improvement protocol:\n"}
{"id": "19444957", "url": "https://en.wikipedia.org/wiki?curid=19444957", "title": "List of cultural icons of Wales", "text": "List of cultural icons of Wales\n\nThe List of cultural icons of Wales is a list of links to known cultural icons of Wales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6153690", "url": "https://en.wikipedia.org/wiki?curid=6153690", "title": "List of genetic genealogy topics", "text": "List of genetic genealogy topics\n\nThis is a list of genetic genealogy topics.\n\n\n\n\n\n\n\n\n"}
{"id": "47107855", "url": "https://en.wikipedia.org/wiki?curid=47107855", "title": "List of species described in 2015", "text": "List of species described in 2015\n\nList of species formally described and other new taxa of organism in 2015 classified by time of publication.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12066660", "url": "https://en.wikipedia.org/wiki?curid=12066660", "title": "List of steroid abbreviations", "text": "List of steroid abbreviations\n\nThe steroid hormones are referred to by various abbreviations in the biological literature. The purpose of this list is to give commonly used abbreviations for steroid hormones, with supporting references to the literature.\n"}
{"id": "7120123", "url": "https://en.wikipedia.org/wiki?curid=7120123", "title": "List of volcanoes in Mongolia", "text": "List of volcanoes in Mongolia\n\nThis is a list of active and extinct volcanoes in Mongolia. \n"}
{"id": "1539960", "url": "https://en.wikipedia.org/wiki?curid=1539960", "title": "Léon Fairmaire", "text": "Léon Fairmaire\n\nLéon Marc Herminie Fairmaire (29 June 1820 – 1 April 1906) was a French entomologist.\n\nA specialist in Coleoptera he assembled an immense collection comparable with that of Pierre François Marie Auguste Dejean (1780-1845). This is in the Muséum national d'histoire naturelle. Fairmaire wrote 450 scientific papers and other publications relating to Coleoptera (). He also worked on Hemiptera.\n\n"}
{"id": "32542796", "url": "https://en.wikipedia.org/wiki?curid=32542796", "title": "Men, Microscopes, and Living Things", "text": "Men, Microscopes, and Living Things\n\nMen, Microscopes, and Living Things is a children's book written by the American author Katherine Shippen and illustrated by Anthony Ravielli. The book was first published in 1955 and is a 1956 Newbery Honor recipient.\n\nShippen traces the history of biological thought beginning with Aristotle and followed by Pliny, Linnaeus, Cuvier, Lamarck, Darwin, and several others. The book is 190 pages including a 7-page index.\n"}
{"id": "26530571", "url": "https://en.wikipedia.org/wiki?curid=26530571", "title": "Mental operations", "text": "Mental operations\n\nMental operations are operations that affect mental contents. Initially, operations of reasoning have been the object of logic alone. Pierre Janet was one of the first to use the concept in psychology. Mental operations have been investigated at a developmental level by Jean Piaget, and from a psychometric perspective by J. P. Guilford. There is also a cognitive approach to the subject, as well as a systems view of it.\n\nSince Antiquity, mental operations, more precisely, formal operations of reasoning have been the object of logic.\n\nIn 1903, Pierre Janet described two types of mental operations:\nJean Piaget differentiated a preoperational stage, and operational stages of cognitive development, on the basis of presence of mental operations as an adaptation tool.\n\nJ. P. Guilford's Structure of Intellect model described up to 180 different intellectual abilities organized along three dimensions—Operations, Content, and Products.\n\nAccording to \nmost logicians, the three primary mental operations are apprehension (understanding), judgement, and inference.\n\nApprehension is the mental operation by which an idea is formed in the mind. If you were to think of a sunset or a baseball, the action of forming that picture in your mind is apprehension. The verbal expression of apprehension is called a term.\n\nJudgment is the mental operation by which we predicate something of a subject. Were you to think, \"That sunset is beautiful\" or \"Baseball is the all-American sport\" is to make a judgment. The verbal expression of judgment is the statement (or proposition).\n\nInference (or reasoning) is the mental operation by which we draw conclusions from other information. If you were to think, \"I like to look at that sunset, because I enjoy beautiful things, and that sunset is beautiful\" you would be reasoning. The verbal expression of reasoning is the logical argument.\n\nJean Piaget identifies several mental operations of the concrete operational stage of cognitive development:\n\n\nPiaget also describes a formal operational stage, with formal operations of abstract thinking: hypothesizing, hypothesis testing, and deduction.\n\nAccording to J. P. Guilford's Structure of Intellect (SI) theory, an individual's performance on intelligence tests can be traced back to the underlying mental abilities or factors of intelligence. SI theory comprises multiple intellectual abilities organized along three dimensions—Operations, Content, and Products.\n\n\nSI includes six operations or general intellectual processes:\nCognition—The ability to understand, comprehend, discover, and become aware of information.\nMemory recording—The ability to encode information.\nMemory retention—The ability to recall information.\nDivergent production—The ability to generate multiple solutions to a problem; creativity.\nConvergent production—The ability to deduce a single solution to a problem; rule-following or problem-solving.\nEvaluation—The ability to judge whether or not information is accurate, consistent, or valid.\n\n\nSI includes five broad areas of information to which the human intellect applies the six operations:\nVisual—Information perceived through seeing.\nAuditory—Information perceived through hearing.\nKinesthetic -through actions\nSymbolic—Information perceived as symbols or signs that have no meaning by themselves; e.g., Arabic numerals or the letters of an alphabet.\nSemantic—Information perceived in words or sentences, whether oral, written, or silently in one's mind.\nBehavioral—Information perceived as acts of people.\n\n\nAs the name suggests, this dimension contains results of applying particular operations to specific contents. The SI model includes six products, in increasing complexity:\nUnits—Single items of knowledge.\nClasses—Sets of units sharing common attributes.\nRelations—Units linked as opposites or in associations, sequences, or analogies.\nSystems—Multiple relations interrelated to comprise structures or networks.\nTransformations—Changes, perspectives, conversions, or mutations to knowledge.\nImplications—Predictions, inferences, consequences, or anticipations of knowledge.\n\nTherefore, according to Guilford there are 6 x 5 x 6 = 180 intellectual abilities or factors. Each ability stands for a particular operation in a particular content area and results in a specific product, such as Comprehension of Figural Units or Evaluation of Semantic Implications.\n\nFollowing on the footsteps of Silvio Ceccato, Giulio Benedetti describes several types of mental operations:\n\nTaking into account all mental processes, the following types of mental operations have been described:\n\n"}
{"id": "598143", "url": "https://en.wikipedia.org/wiki?curid=598143", "title": "Millennium Technology Prize", "text": "Millennium Technology Prize\n\nThe Millennium Technology Prize () is one of the world's largest technology prizes. It is awarded once every two years by Technology Academy Finland, an independent fund established by Finnish industry and the Finnish state in partnership. The prize is presented by the President of Finland. The Millennium Technology Prize is Finland's tribute to innovations for a better life. The aims of the prize are to promote technological research and Finland as a high-tech Nordic welfare state. The prize was inaugurated in 2004.\n\nThe idea of the prize came originally from the Finnish academician Pekka Jauho, with American real estate investor and philanthropist Arthur J Collingsworth encouraging its establishment. The Prize celebrates innovations that have a favorable and sustainable impact on quality of life and well-being of people. The innovations also must have been applied in practice and stimulate further research and development. Compared to the Nobel Prize the Millennium Technology Prize is a technology award, whereas the Nobel Prize is a science award. Furthermore, the Nobel Prize is awarded for basic research, but the Millennium Technology Prize may be given to a recently conceived innovation which is still being developed. The Millennium Technology Prize is not intended as a reward for lifetime achievement.\n\nThe Millennium Technology Prize is awarded by Technology Academy Finland (formerly Millennium Prize Foundation and Finnish Technology Award Foundation), established in 2002 by eight Finnish organisations supporting technological development and innovation. The prize sum is 1 million euros (~US$1.3 million). The Millennium Technology Prize is awarded every second year and is presented by the president of Finland. The Millennium Technology Prize is the world's largest technology award. The predecessor to the Millennium Prize was the Walter Ahlström prize.\n\nUniversities, research institutes, national scientific and engineering academies and high-tech companies around the world are eligible to nominate individuals or groups for the award, excluding military technology. In accordance with the rules of the Technology Academy Finland, a proposal concerning the winner of the Millennium Technology Prize is made to the board of the foundation by the eight-member international selection committee, and the final decision on the prize winner is made by the board.\n\nCurrent members of the selection committee:\n\n\n"}
{"id": "52926775", "url": "https://en.wikipedia.org/wiki?curid=52926775", "title": "Minimum bias event", "text": "Minimum bias event\n\nMinimum bias (MB) events are inelastic events selected by a high-energy experiment's loose (minimum bias) trigger with as little bias as possible. MB events can include both non-diffractive and diffractive processes although the precise definition and relative contributions vary among experiments and analyses. Quite often the beam hadrons ooze through each other and fall apart without any hard collisions occurring in the event. MB event is not the same as the underlying event (UE), which consists of particles accompanying a hard scattering. The density of particles in the UE in jet events is found to be roughly a factor of two greater than that in MB in proton-proton collisions at the Tevatron and the LHC.\n"}
{"id": "28072231", "url": "https://en.wikipedia.org/wiki?curid=28072231", "title": "Mohammed Sultan Khan Ghauri", "text": "Mohammed Sultan Khan Ghauri\n\nMohammed Sultan Khan Ghauri is a biologist specialist of Hemiptera.\n\n"}
{"id": "202286", "url": "https://en.wikipedia.org/wiki?curid=202286", "title": "Preadolescence", "text": "Preadolescence\n\nPreadolescence, also known as pre-teen or tween, is a stage of human development following early childhood and preceding adolescence. It commonly ends with the beginning of puberty, but may also be defined as ending with the start of the teenage years. For example, dictionary definitions generally designate it as 10–13 years. Preadolescence can bring its own challenges and anxieties.\n\nBeing prepubescent is not the same thing as being preadolescent. Instead, \"prepubescent\" (and sometimes \"child\") is a term for boys and girls who have not developed secondary sex characteristics, while \"preadolescent\" is generally defined as those ranging from age 10 to 13 years. Preadolescence may also be defined as the period from 9 to 14 years. \n\nThe point at which a child becomes an adolescent is defined by the onset of puberty or the beginning of the teenage stage. Adolescence is also viewed as ending with the teenage stage. However, in some individuals (particularly females), puberty begins in the preadolescence years. Studies indicate that the onset of puberty has been one year earlier with each generation since the 1950s.\n\nOne can also distinguish middle childhood and preadolescence – middle childhood from approximately 5–8 years, as opposed to the time children are generally considered to reach preadolescence. There is no exact agreement as to when preadolescence starts and ends, and research by Gesell et al. suggests that \"chronological time...is by no means identical with developmental time\" – the duration of the \"inner\" stages of growth' – or with physiological time.\n\nWhile known as \"preadolescent\" in psychology, the terms \"preteen\", \"preteenager\" or \"tween \"are common in everyday use. A preteen or preteenager is a person 12 and under. Generally, the term is restricted to those close to reaching age 12, especially age 11. \"Tween\" is an American neologism and marketing term for \"preteen\", which is a blend of \"between\" and \"teen\". People within this age range are variously described as tweens, preadolescents, tweenies, preteens, pubescents, junior highers or tweenagers.\n\nThe term \"tween\" was previously used in J. R. R. Tolkien's 1954 novel \"The Lord of the Rings\" to refer to hobbits in their twenties: \"\"tweens\" as hobbits called the irresponsible twenties between childhood and the coming of age at thirty-three.\" In this context, the word is really either a shortened version of \"between\" or a portmanteau of \"teen\" and \"twenty\", and in either case has no connection to teens, preteens or the American marketing niche.\n\nOf the 'two major socializing agents in children's lives: the family environment...and formal educational institutions,' it is 'the family in its function a primary socializer of the child' that predominates in the first five years of life: middle childhood by contrast is characterized by 'a child's readiness for school...being self-assured and interested; knowing what kind of behavior is expected...being able to wait, to follow directions, and getting along with other children.'\n\nPreadolescent children in fact have a different view of the world from younger children in many significant ways. Typically, theirs is a more realistic view of life than the intense, fantasy-oriented world of earliest childhood. Preadolescents have more mature, sensible, realistic thoughts and actions: 'the most \"sensible\" stage of development...the child is a much \"less emotional being\" now.' They will often have developed a sense of ' \"intentionality\". The wish and capacity to have an impact, and to act upon that with persistence'; and will have a more developed sense of looking into the future and seeing effects of their actions (as opposed to early childhood where children often do not worry about their future). This can include more realistic job expectations (\"I want to be an engineer when I grow up\", as opposed to \"I want to be a wizard\"). Middle children generally show more investment 'in \"control over external reality\" through the acquisition of knowledge and competence': where they do have worries, these may be more a fear of kidnappings, rapes, and scary media events, as opposed to fantasy things (e.g., witches, monsters, ghosts).\n\nPreadolescents may well view human relationships differently (e.g. they may notice the flawed, human side of authority figures). Alongside that, they may begin to develop a sense of self-identity, and to have increased feelings of independence: 'may feel an individual, no longer \"just one of the family.\"' A different view on morality can emerge; and the middle child will also show more \"cooperativeness\". The ability to balance one's own needs with those of others in group activities'. Many preadolescents will often start to question their home life and surroundings around this time and they may also start to form opinions that may differ from their upbringing in regards to issues such as politics, religion, sexuality, and gender roles.\n\nGreater responsibility within the family can also appear, as middle children become responsible for younger siblings and relatives, as with babysitting; while preadolescents may start caring about what they look like and what they are wearing.\n\nMiddle children often begin to experience infatuation, limerence, puppy love, or love itself, though arguably at least with 'girls carrying out all the romantic interest...preadolescent girls' romantic pursuits often seem to be more aggressive than affectionate.'\n\nPreadolescents may still suffer tantrums at the age of 13, sometimes leading to rash decisions regarding risky actions. Such decisions may in rare cases result in grave situations such as accidental death.\n\nWhere development has been optimal, preadolescents 'come to school for something to be added to their lives; they want to learn lessons...which can lead to their eventually working in a job like their parents.' When earlier developmental stages have gone astray, however, then, on the principle that 'if you miss a stage, you can always go through it later,' some middle children 'come to school for another purpose...[not] to learn but to find a home from home...a stable emotional situation in which they can exercise their own emotional liability, a group of which they can gradually become a part.'\n\nChildren at the threshold of adolescence in the nine-to-twelve-year-old group would seem to have particular vulnerabilities to parental separation. Among such problems were the very 'eagerness of these youngsters to be co-opted into the parental battling; their willingness to take sides...and the intense, compassionate, caretaking relations which led these youngsters to attempt to rescue a distressed parent often to their own detriment.\n\nPreadolescents may well be more exposed to popular culture than younger children and have interests based on internet trends, television shows and movies (no longer just cartoons), fashion, technology, music and social media. Preadolescents generally prefer certain brands, and are a heavily targeted market of many advertisers. Their tendency to buy brand-name items may be due to a desire to fit in, although the desire is not as strong as it is with teenagers.\n\nSome scholars suggest that 'pre-adolescents ... reported frequent encounters with sexual material in the media, valued the information received from it, and used it as a learning resource ... and evaluated such content through what they perceived to be sexual morality.' However, other research has suggested that sexual media influences on preadolescent and adolescent sexual behavior is minimal.\n\nFreud called this stage the \"latency\" period to indicate that sexual feelings and interest went underground ... the \"feelings\" that create that first \"eternal triangle\" with the parents fade, and free energy for other interests and activities.' Erik H. Erikson confirmed that 'violent drives are normally dormant ... a lull before the storm of puberty, when all the earlier drives re-emerge in a new combination, to be brought under the dominance of genitality.'\n\nLatency period children can then direct more of their energy into asexual pursuits such as school, athletics, and same-sex friendships: middle childhood especially is marked by 'the importance of school, teams, classes, friends, gangs and organised activities ... and the adults who run those.' Nevertheless, recent research suggests that \"most children do not cease sexual development, interest and behavior\" at this time: rather, they \"cease to share their interest with adults and are less frequently observed.\" Because \"they've learned the rules ... [they] fit in with the grown-up's \"belief\" that they're not interested. But the curiosity about it all continues, and there's quite a lot of experimenting going on between them.' alongside other pursuits\n\nBut while the eight-year-old still has \"years to wait until puberty, adolescence and finally sexual maturity ... a sort of lull before puberty arrives,' with preadolescence proper (9–12), and the move forward from middle childhood, what have been called 'the introspective and social concerns of the prepubescent' tend to come more to the fore. Clearly \"few experiences are more prominent in the lives of preadolescents than the onset of puberty\"; so that \"at eleven or twelve you're just reaching the end of a long period during which change was steady and incremental\": Freud's latency years.\n\n\n"}
{"id": "39181419", "url": "https://en.wikipedia.org/wiki?curid=39181419", "title": "Project Hindsight", "text": "Project Hindsight\n\nProject Hindsight was a retrospective study conducted to determine the effectiveness of several post-World War II weapons research projects. The project was conducted by the Office of the Director of Defense Research and Engineering, a sub-agency of the United States Department of Defense. \n\nThe study ran from 1963 to 1967 and the final report was published in October 1969 and released to the public in September 1970. The project had two goals: the first was to identify R&D management productivity and the second was to measure the overall cost-effectiveness of using recently developed weapon systems compared to their predecessors that were in use 10 to 20 years earlier.\n\nOf all 'events' studied by Project Hindsight, 91% were technological, and only 9% were classed as science. Within the latter category 8.7% were applied science, whereas only 0.3%, or two 'events', were due to basic or undirected science.\n\nAs science and technology studies scholar Edwin Layton observed in 1971, 'the publication of these results produced a spate of indignant letters to the editors of \"Science\".' He noted that many of these letters missed the point, and instead should focus on the interaction between science and technology rather than attempting to demonstrate that fundamental scientific research influences technology development more than Project Hindsight had suggested. \n\nHe also called attention to a subsequent study, \"Technology in Retrospect and Critical Events in Science\" (TRACES), which 'revealed cases in which mission-oriented research or development effort elicited later nonmission research, which often was found to be crucial to the ultimate innovation'.\n\n"}
{"id": "1567800", "url": "https://en.wikipedia.org/wiki?curid=1567800", "title": "Publius Enigma", "text": "Publius Enigma\n\nThe Publius Enigma is an Internet phenomenon and an unsolved problem that began with cryptic messages posted by a user identifying only as \"Publius\" to the unmoderated Usenet newsgroup alt.music.pink-floyd through the Penet remailer, a now defunct anonymous information exchange service. The messenger proposed a riddle in connection with the 1994 Pink Floyd album \"The Division Bell\", promising that the answer would lead to a reward. Pink Floyd's lead singer, David Gilmour, denied any involvement while album artist Storm Thorgerson was bemused by the ordeal. According to drummer Nick Mason, EMI Records were ultimately responsible. It remains unclear if the enigma involves a genuinely solvable puzzle as part of an early Internet-based contest or was a convoluted hoax engineered in part by the band's management. Regardless, the mystery continues to attract a small but loyal cult following.\n\nDuring the 1994 Division Bell World Tour, Columbia Records flew a airship named \"The Division Belle\" between Pink Floyd concert locations. The Columbia Electronic Press Kit was released to the media, along with the Promo Spots Video consisting of interviews with band members, footage of the airship in action, and a segment which contained the following:\n\nOn 11 June 1994, a user of the anonymous Penet remailer service posted the following message to the Usenet newsgroup alt.music.pink-floyd:\n\nA follow-up clarified the challenge:\n\nIn order to refute the ensuing scepticism, Publius agreed to provide proof of his authenticity. On 16 July 1994 he delivered a prediction:\n\nOn the night of 18 July 1994, patterns in the lights on the front of the stage at the Pink Floyd concert in East Rutherford momentarily spelled out the words ENIGMA PUBLIUS.\n\nIn September 1996, the Penet remailer service was shut down by its creator over legal threats posed to the guaranteed anonymity of its users. As a consequence, contact to the newsgroup through the associated Publius account ceased. Subsequent Publius-style posts from other addresses have led to differing opinions over the status of the enigma and whether or not it has ever been solved. It is unclear if the original poster remained or remains active, or if the enigma has been abandoned by those responsible for starting it.\n\nDuring a 2002 webchat, David Gilmour responded to a question about the subject.\n\nIn April 2005, during a book signing of his biographical work \"\", Nick Mason also asserted that the Publius Enigma had been instigated by the record company rather than the band, and that the prize for solving the riddle would have been a \"crop of trees planted in a clear cut area of forest or something to that effect\".\n\nThe comments made by Mason corroborate parts of a previous interview by Sean Heisler with Marc Brickman, Pink Floyd's lighting and production designer and the man apparently responsible for putting the \"ENIGMA PUBLIUS\" message in the lights at the New Jersey concert.\n\nBrickman later expressed regret regarding his comments:\n\nAuthor Douglas Adams, who was responsible for giving \"The Division Bell\" its title, was also questioned during an Internet chat session in 2001.\n\nThe Pink Floyd magazine \"Brain Damage \"had a Q&A section reserved for a correspondent known only as \"Uncle Custard\". The name (phonetically similar to \"Uncool Car Stud\") was created by Glen Povey, apparently an allusion to Nick Mason's passion for auto racing.\n\nIssue No.34 of the magazine contains the following:\n\nAlthough the answers given by Uncle Custard over the years have all been written by several different people affiliated with the magazine, this particular response has been attributed to former editor and final publisher of the printed version of \"Brain Damage\", Jeff Jensen. The accuracy of the content of this answer and under what authority (if any) Jensen had to produce it remains unclear.\n\nPossible references to the Publius Enigma can be found in various Pink Floyd releases:\n\n\n"}
{"id": "7455322", "url": "https://en.wikipedia.org/wiki?curid=7455322", "title": "Ricardo Cirera", "text": "Ricardo Cirera\n\nRicardo Cirera Salse (Os de Balaguer, 1864-Barcelona, August 1932) was a geomagnetist who conducted the first geomagnetic survey of the Philippines and who founded the El Ebro Observatory in Roquetes (1904), Catalonia, Spain. He was also involved in the founding of the scientific journal Ibérica.\n\n\n"}
{"id": "35760811", "url": "https://en.wikipedia.org/wiki?curid=35760811", "title": "Richard S. Boardman", "text": "Richard S. Boardman\n\nRichard S. Boardman was an American paleontologist and curator of the Department of Paleobiology at the United States National Museum (now the National Museum of Natural History). Boardman worked for the museum from 1957 to 1985 and subsequently became a founding member of the International Bryozoology Association (IBA). Boardman is best known for the hard/soft thin-sectioning technique that he developed in order to compare the internal morphology of living and fossilized bryozoans which have revealed new information about bryozoan life history.\n"}
{"id": "1027166", "url": "https://en.wikipedia.org/wiki?curid=1027166", "title": "Roman ring", "text": "Roman ring\n\nIn general relativity, a Roman ring (proposed by Matt Visser in 1997 and named after the Roman arch, a concept proposed by Mike Morris and Kip Thorne in 1988 and named after physicist Tom Roman) is a configuration of wormholes where no subset of wormholes is near to chronology violation, though the combined system can be arbitrarily close to chronology violation.\n\nFor example, an Earth–Moon wormhole whose far end is 0.5 seconds in the \"past\" will not violate causality, since information sent to the far end via the wormhole and back through normal space will still arrive back on Earth (-0.5 + 1) = 0.5 seconds after it was transmitted; but an additional wormhole in the other direction will allow information to arrive back on Earth 1 second \"before\" it was transmitted (time travel), although it is believed that relative time between the transmission of the information in one wormhole throat and out the other end in a ring structure will remain the same, because light wouldn't have violated local proper time, because the distance traveled by the information would take time, either by going the long way or through the wormhole.\n\nSemiclassical approaches to incorporating quantum effects into general relativity seem to show that the chronology protection conjecture postulated by physicist Stephen Hawking fails to prevent the formation of such rings, although Matt Visser feels that there are reasons to think the semiclassical approach is unreliable here, and that a full theory of quantum gravity will likely uphold chronology protection.\n"}
{"id": "57271629", "url": "https://en.wikipedia.org/wiki?curid=57271629", "title": "Sidney Hugh Reynolds", "text": "Sidney Hugh Reynolds\n\nSidney Hugh Reynolds DSc, FGS (18 December 1867 – 20 August 1949) was an English geologist, palaeontologist, and zoologist.\n\nReynolds was born in Brighton. He was educated at Marlborough College and Trinity College, Cambridge, where he received B.A. (Nat.\nSci. Tripos, Pt I, 1st Class) 1889; (Pt II, 1st Class, 1890); M.A. 1894; Sc.D. 1913. He was acting professor of zoology at Madras Christian College in 1891–1892 and in 1897–1898. At the University of Bristol he was a lecturer from 1894 to 1899, an assistant professor of zoology and geology from 1899 to 1900, a professor of zoology and geology from 1900 to 1910, and a professor of geology from 1910 to 1933, when he retired as professor emeritus. He then became the curator of the Stroud District Cowle Museum.\n\nHe was the president of Section \"G\" of the British Association in 1926. He was awarded the Lyell Medal in 1928. (In the same year William Dickson Lang was also awarded the Lyell Medal for work done independently.) He died in Clifton, Bristol, aged 81.\n\n"}
{"id": "26302996", "url": "https://en.wikipedia.org/wiki?curid=26302996", "title": "Social Service Review", "text": "Social Service Review\n\nSocial Service Review is an academic journal published by the University of Chicago Press which covers social welfare policy and practice and its effects. It was established in 1927 and the editor-in-chief is Mark E. Courtney (University of Chicago).\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.738.\n"}
{"id": "17507803", "url": "https://en.wikipedia.org/wiki?curid=17507803", "title": "Social orphan", "text": "Social orphan\n\nSocial orphan is a term in various foreign languages, such as Ukrainian, denoting that a child has no adults looking after it, even though one or more parents are still alive. Usually the parents are alcoholics, drug abusers, or simply are not interested in the child. It is therefore not the same as an orphan, who has no living parents.\n\nThe Convention on the Rights of the Child has brought many countries to reassess their mandate to care for children inside their borders thus bringing to light various new ways of thinking about international child care.\n\nIn a study of Honduras it was found that 54.3% of children commonly identified as \"orphans\" were actually social orphans.\n\n"}
{"id": "19816553", "url": "https://en.wikipedia.org/wiki?curid=19816553", "title": "Sylva, or A Discourse of Forest-Trees and the Propagation of Timber", "text": "Sylva, or A Discourse of Forest-Trees and the Propagation of Timber\n\nSylva, or A Discourse of Forest-Trees and the Propagation of Timber in His Majesty's Dominions by the English writer John Evelyn was first presented in 1662 as a paper to the Royal Society. It was published as a book two years later in 1664, and is recognised as one of the most influential texts on forestry ever published.\n\n\n\nFive editions were edited by Alexander Hunter (1729-1809):\n\n\n\n"}
{"id": "38847195", "url": "https://en.wikipedia.org/wiki?curid=38847195", "title": "TP model transformation in control theory", "text": "TP model transformation in control theory\n\nBaranyi and Yam proposed the TP model transformation as a new concept in quasi-LPV (qLPV) based control, which plays a central role in the highly desirable bridging between identification and polytopic systems theories. It is uniquely effective in manipulating the convex hull of polytopic forms, and, hence, has revealed and proved the fact that convex hull manipulation is a necessary and crucial step in achieving optimal solutions and decreasing conservativeness in modern linear matrix inequality based control theory. Thus, although it is a transformation in a mathematical sense, it has established a conceptually new direction in control theory and has laid the ground for further new approaches towards optimality.\n\nFor details please visit: TP model transformation.\n\n\nA free MATLAB implementation of the TP model transformation can be downloaded at or an old version of the toolbox is available at MATLAB Central . Be careful, in the MATLAB toolbox the assignments of the dimensions of the core tensor is in the opposite way in contrast to the notation used in the related literature. In the ToolBox, the first two dimension of the core tensor is assigned to the vertex systems. In the TP model literature the last two. A simple example is given below.\n\n\n\nwith input formula_2, output formula_3 and state\nvector formula_4. The system matrix formula_5 is a parameter-varying object, where formula_6 is a time varying formula_7-dimensional parameter vector which is an element of\nclosed hypercube formula_8. As a matter of fact, further parameter dependent channels can be inserted to formula_9 that represent various control performance requirements.\n\n\nformula_10 in the above LPV model can also include some elements of the state vector\nformula_4, and, hence this model belongs to the class of non-linear systems, and is also referred to as a quasi LPV (qLPV) model.\n\n\nwith input formula_2, output formula_3 and state\nvector formula_4. The system matrix formula_16 is a parameter-varying object, where formula_6 is a time varying formula_7-dimensional parameter vector which is an element of\nclosed hypercube formula_8, and the weighting functions formula_20 are the elements of vector formula_21. Core tensor contains elements formula_22 which are the vertexes of the system.\nAs a matter of fact, further parameter dependent channels can be inserted to formula_9 that represent various control performance requirements.\nHere\n\nThis means that formula_26 is within the vertexes formula_22 of the system (within the convex hull defined by the vertexes) for all formula_28. \nNote that the TP type polytopic model can always be given in the form\n\nwhere the vertexes are the same as in the TP type polytopic form and the multi variable weighting functions are the product of the one variable weighting functions according to the TP type polytopic form, and r is the linear index equivalent of the multi-linear indexing formula_30.\n\nAssume a given qLPV model formula_31, where formula_32, whose TP polytopic structure may be unknown (e.g. it is given by neural networks). The TP model transformation determines its TP polytopic structure as\n\nnamely it generates core tensor formula_34 and weighting functions of formula_35 for all formula_36. Its free MATLAB implementation is downloadable at or at MATLAB Central .\n\nIf the given model does not have (finite element) TP polytopic structure, then the TP model transformation determines its approximation:\n\nwhere trade-off is offered by the TP model transformation between complexity (number of vertexes stored in the core tensor or the number of weighting functions) and the approximation accuracy. The TP model can be generated according to various constrains. Typical TP models generated by the TP model transformation are:\n\n\nSince the TP type polytopic model is a subset of the polytopic model representations, the analysis and design methodologies developed for polytopic representations are applicable for the TP type polytopic models as well. \nOne typical way is to search the nonlinear controller in the form:\n\nwhere the vertexes formula_39 of the controller is calculated from formula_40. Typically, the vertexes formula_40 are substituted into Linear Matrix Inequalities in order to determine formula_39.\n\nIn TP type polytopic form the controller is:\n\nwhere the vertexes formula_44 stored in the core tensor formula_45 are determined from the vertexes formula_46 stored in formula_34. Note that the polytopic observer or other components can be generated in similar way, such as these vertexes are also generated from formula_48.\n\n\nThe polytopic representation of a given qLPV model is not invariant. I.e. a given formula_49 has formula_50 number of different representation as:\n\nwhere formula_52. In order to generate an optimal control of the given model formula_49 we apply, for instance LMIs. Thus, if we apply the selected LMIs to the above polytopic model we arrive at:\n\nSince the LMIs realize a non-linear mapping between the vertexes in formula_48 and formula_56 we may find very different controllers for each formula_52. This means that we have formula_58 different number of \"optimal\" controllers to the same system formula_49. Thus, the question is: which one of the \"optimal\" controllers is really the optimal one. The TP model transformation let us to manipulate the weighting functions systematically that is equivalent to the manipulation of the vertexes. The geometrical meaning of this manipulation is the manipulation of the convex hull defined by the vertexes. We can easily demonstrate the following facts:\n\n\nof a given model formula_49, then we can generate a controller as\n\nthen we solved the control problem of all systems formula_63 that can be given by the same vertexes, but with different weighting functions as:\n\nwhere\n\nIf one of these systems are very hardly controllable (or even uncontrollable) then we arrive at a very conservative solution (or unfeasible LMIs). Therefore, we expect that during tightening the convex hull we exclude such problematic systems.\n\n\n\n"}
{"id": "1291342", "url": "https://en.wikipedia.org/wiki?curid=1291342", "title": "Time-variant system", "text": "Time-variant system\n\nA time-variant system is a system that is not time invariant (TIV). Roughly speaking, its output characteristics depend explicitly upon time. In other words, a system in which certain quantities governing the system's behavior change with time, so that the system will respond differently to the same input at different times.\n\nThere are many well developed techniques for dealing with the response of linear time invariant systems, such as Laplace and Fourier transforms. However, these techniques are not strictly valid for time-varying systems. A system undergoing slow time variation in comparison to its time constants can usually be considered to be time invariant: they are close to time invariant on a small scale. An example of this is the aging and wear of electronic components, which happens on a scale of years, and thus does not result in any behaviour qualitatively different from that observed in a time invariant system: day-to-day, they are effectively time invariant, though year to year, the parameters may change. Other linear time variant systems may behave more like nonlinear systems, if the system changes quickly – significantly differently between measurements.\n\nThe following things can be said about a time-variant system:\n\nThe following time varying systems cannot be modelled by assuming that they are time invariant:\n\n"}
{"id": "17740009", "url": "https://en.wikipedia.org/wiki?curid=17740009", "title": "Topological data analysis", "text": "Topological data analysis\n\nIn applied mathematics, topological data analysis (TDA) is an approach to the analysis of datasets using techniques from topology. Extraction of information from datasets that are high-dimensional, incomplete and noisy is generally challenging. TDA provides a general framework to analyze such data in a manner that is insensitive to the particular metric chosen and provides dimensionality reduction and robustness to noise. Beyond this, it inherits functoriality, a fundamental concept of modern mathematics, from its topological nature, which allows it to adapt to new mathematical tools.\n\nThe initial motivation is to study the shape of data. TDA has combined algebraic topology and other tools from pure mathematics to allow mathematically rigorous study of \"shape\". The main tool is persistent homology, an adaptation of homology to point cloud data. Persistent homology has been applied to many types of data across many fields. Moreover, its mathematical foundation is also of theoretical importance. The unique features of TDA make it a promising bridge between topology and geometry.\n\nThe premise underlying TDA is that shape matters. Real data in high dimensions is nearly always sparse, and tends to have relevant low dimensional features. One task of TDA is to provide a precise characterization of this fact. An illustrative example is a simple predator-prey system governed by the Lotka–Volterra equations. One can easily observe that the trajectory of the system forms a closed circle in state space. TDA provides tools to detect and quantify such recurrent motion.\n\nMany algorithms for data analysis, including those used in TDA, require the choice of various parameters. Without prior domain knowledge, the correct collection of parameters for a data set is difficult to choose. The main insight of persistent homology is that we can use the information obtained from all values of a parameter. Of course this insight alone is easy to make; the hard part is encoding this huge amount of information into an understandable and easy-to-represent form. With TDA, there is a mathematical interpretation when the information is a homology group. In general, the assumption is that features that persist for a wide range of parameters are \"true\" features. Features persisting for only a narrow range of parameters are presumed to be noise, although the theoretical justification for this is unclear.\n\nPrecursors to the full concept of persistent homology appeared gradually over time. In 1990, Patrizio Frosini introduced the size function, which is equivalent to the 0th persistent homology. Nearly a decade later, Vanessa Robins studied the images of homomorphisms induced by inclusion. Finally, shortly thereafter, Edelsbrunner et al. introduced the concept of persistent homology together with an efficient algorithm and its visualization as a persistence diagram. Carlsson et al. reformulated the initial definition and gave an equivalent visualization method called persistence barcodes, interpreting persistence in the language of commutative algebra. \n\nIn algebraic topology, the set of critical values of smooth Morse function was canonically partitioned into pairs \"birth-death\", filtered complexes were classified and the visualization of their invariants, equivalent to persistence diagram and persistence barcodes, was given in 1994 by Barannikov's canonical form.\n\nSome widely used concepts are introduced below. Note that some definitions may vary from author to author.\n\nA point cloud is often defined as a finite set of points in some Euclidean space, but may be taken to be any finite metric space.\n\nThe Čech complex of a point cloud is the \"nerve\" of the \"cover\" of balls of a fixed radius around each point in the cloud.\n\nA persistence module formula_1 indexed by formula_2 is a vector space formula_3 for each formula_4, and a linear map formula_5 whenever formula_6, such that formula_7 for all formula_8 and formula_9 whenever formula_10 An equivalent definition is a functor from formula_2 considered as a partially ordered set to the category of vector spaces.\n\nThe persistent homology group formula_12 of a point cloud is the persistence module defined as formula_13, where formula_14 is the Čech complex of radius formula_15 of the point cloud formula_16 and formula_17 is the homology group.\n\nA persistence barcode is a multiset of intervals in formula_18, and a persistence diagram is a multiset of points in formula_19(formula_20).\n\nThe Wasserstein distance between two persistence diagrams formula_16 and formula_22 is defined as formula_23where formula_24 and formula_25 ranges over bijections between formula_16 and formula_22. Please refer to figure 3.1 in Munch for illustration.\n\nThe bottleneck distance between formula_16 and formula_22 is formula_30 This is a special case of Wasserstein distance, letting formula_31.\n\nThe first classification theorem for persistent homology appeared in 2005: for a finitely generated persistence module formula_32 with field formula_33 coefficients, \nformula_34\nIntuitively, the free parts correspond to the homology generators that appear at filtration level formula_35 and never disappear, while the torsion parts correspond to those that appear at filtration level formula_36 and last for formula_37 steps of the filtration (or equivalently, disappear at filtration level formula_38).\n\nPersistent homology is visualized through a barcode or persistence diagram. The barcode has its root in abstract mathematics, though not at first sight; essentially, the derived category of chain complexes over a field is equivalent to the graded category of vector spaces.\n\nStability is desirable because it provides robustness against noise. If formula_16 is any space which is homeomorphic to a simplicial complex, and formula_40 are continuous tame functions, then the persistence vector spaces formula_41 and formula_42 are finitely presented, and formula_43, where formula_44 refers to the bottleneck distance and formula_45 is the map taking a continuous tame function to the persistence diagram of its formula_46-th homology.\n\nThe basic workflow in TDA is:\n\nGraphically speaking, \n\nThe first algorithm for persistent homology over formula_51 was given by Edelsbrunner et al. Zomorodian and Carlsson gave the first practical algorithm to compute persistent homology over all fields. Edelsbrunner and Harer's book gives general guidance on computational topology.\n\nOne issue that arises in computation is the choice of complex. The Čech complex and Vietoris-Rips complex are most natural at first glance; however, their size grows rapidly with the number of data points. The Vietoris-Rips complex is preferred over Čech complex because its definition is simpler and the Čech complex requires extra effort to define in a general finite metric space. Efficient ways to lower the computational cost of homology have been studied. For example, the α-complex and witness complex are used to reduce the dimension and size of complexes.\n\nRecently, Discrete Morse theory has shown promise for computational homology because it can reduce a given simplicial complex to a much smaller cellular complex which is homotopic to the original one. This reduction can in fact be performed as the complex is constructed by using matroid theory, leading to further performance increases. Another recent algorithm saves time by ignoring the homology classes with low persistence.\n\nVarious software packages are available, such as javaPlex, Dionysus, Perseus, PHAT, DIPHA, Gudhi, Ripser, and TDAstats. A comparison between these tools is done by Otter et al. An R package TDA is capable of calculating recently invented concepts like landscape and the kernel distance estimator. The Topology ToolKit is specialized for continuous data defined on manifolds of low dimension (1, 2 or 3), as typically found in scientific visualization. Also, a recently released R package, TDAstats, implements the fast C++ Ripser library to calculate persistent homology. It also uses the ubiquitous ggplot2 package to generate reproducible, customizable, publication-quality visualizations of persistent homology, specifically topological barcodes and persistence diagrams. The sample code below gives an example of how the R programming language can be used to compute persistent homology.\n\nHigh-dimensional data is impossible to visualize directly. Many methods have been invented to extract a low-dimensional structure from the data set, such as principal component analysis and multidimensional scaling. However, it is important to note that the problem itself is ill-posed, since many different topological features can be found in the same data set. Thus, the study of visualization of high-dimensional spaces is of central importance to TDA, although it does not necessarily involve the use of persistent homology. However, recent attempts have been made to use persistent homology in data visualization.\n\nCarlsson et al. have proposed a general method called MAPPER. It inherits the idea of Serre that a covering preserves homotopy. A generalized formulation of MAPPER is as follows:\n\nLet formula_16 and formula_53 be topological spaces and let formula_54 be a continuous map. Let formula_55 be a finite open covering of formula_53. The output of MAPPER is the nerve of the pullback cover formula_57, where each preimage is split into its connected components. This is a very general concept, of which the Reeb graph and merge trees are special cases.\n\nThis is not quite the original definition. Carlsson et al. choose formula_53 to be formula_18 or formula_60, and cover it with open sets such that at most two intersect. This restriction means that the output is in the form of a complex network. Because the topology of a finite point cloud is trivial, clustering methods (such as single linkage) are used to produce the analogue of connected sets in the preimage formula_61 when MAPPER is applied to actual data.\n\nMathematically speaking, MAPPER is a variation of the Reeb graph. If the formula_62 is at most one dimensional, then for each formula_63, formula_64 The added flexibility also has disadvantages. One problem is instability, in that some change of the choice of the cover can lead to major change of the output of the algorithm. Work has been done to overcome this problem.\n\nThree successful applications of MAPPER can be found in Carlsson et al. A comment on the applications in this paper by J. Curry is that \"a common feature of interest in applications is the presence of flares or tendrils.\"\n\nA free implementation of MAPPER is available online written by Daniel Müllner and Aravindakshan Babu. MAPPER also forms the basis of Ayasdi's AI platform.\n\nMultidimensional persistence is important to TDA. The concept arises in both theory and practice. The first investigation of multidimensional persistence was early in the development of TDA, and is one of the founding papers of TDA. The first application to appear in the literature is a method for shape comparison, similar to the invention of TDA.\n\nThe definition of an \"n\"-dimensional persistence module in formula_65 is \nIt might be worth noting that there are controversies on the definition of multidimensional persistence.\n\nOne of the advantages of one-dimensional persistence is its representability by a diagram or barcode. However, discrete complete invariants of multidimensional persistence modules do not exist. The main reason for this is that the structure of the collection of indecomposables is extremely complicated by Gabriel's theorem in the theory of quiver representations, although a finitely n-dim persistence module can be uniquely decomposed into a direct sum of indecomposables due to the Krull-Schmidt theorem.\n\nNonetheless, many results have been established. Carlsson and Zomorodian introduced the rank invariant formula_73, defined as the formula_74, in which formula_75 is a finitely generated n-graded module. In one dimension, it is equivalent to the barcode. In the literature, the rank invariant is often referred as the persistent Betti numbers (PBNs). In many theoretical works, authors have used a more restricted definition, an analogue from sublevel set persistence. Specifically, the persistence Betti numbers of a function formula_76 are given by the function formula_77, taking each formula_78 to formula_79, where formula_80 and formula_81.\n\nSome basic properties include monotonicity and diagonal jump. Persistent Betti numbers will be finite if formula_16 is a compact and locally contractible subspace of formula_65.\n\nUsing a foliation method, the k-dim PBNs can be decomposed into a family of 1-dim PBNs by dimensionality deduction. This method has also led to a proof that multi-dim PBNs are stable. The discontinuities of PBNs only occur at points formula_84 where either formula_85 is a discontinuous point of formula_86 or formula_87 is a discontinuous point of formula_88 under the assumption that formula_89 and formula_16 is a compact, triangulable topological space.\n\nPersistent space, a generalization of persistent diagram, is defined as the multiset of all points with multiplicity larger than 0 and the diagonal. It provides a stable and complete representation of PBNs. An ongoing work by Carlsson et al. is trying to give geometric interpretation of persistent homology, which might provide insights on how to combine machine learning theory with topological data analysis.\n\nThe first practical algorithm to compute multidimensional persistence was invented very early. After then, many other algorithms have been proposed, based on such concepts as discrete morse theory and finite sample estimating.\n\nThe standard paradigm in TDA is often referred as sublevel persistence. Apart from multidimensional persistence, many works have been done to extend this special case.\n\nThe nonzero maps in persistence module are restricted by the preorder relationship in the category. However, mathematicians have found that the unanimity of direction is not essential to many results. \"The philosophical point is that the decomposition theory of graph representations is somewhat independent of the orientation of the graph edges\". Zigzag persistence is important to the theoretical side. The examples given in Carlsson's review paper to illustrate the importance of functorality all share some of its features.\n\nSome attempts is to lose the stricter restriction of the function. Please refer to the Categorization and cosheaf and Impact on mathematics sections for more information.\n\nIt's natural to extend persistence homology to other basic concepts in algebraic topology, such as cohomology and relative homology/cohomology. An interesting application is the computation of circular coordinates for a data set via the first persistent cohomology group.\n\nNormal persistence homology studies real-valued functions. The circle-valued map might be useful, \"persistence theory for circle-valued maps promises to play the role for some vector fields as does the standard persistence theory for scalar fields\", as commented in D. Burghelea et al. The main difference is that Jordan cells(very similar in format to the ones in linear algebra) are nontrivial in circle-valued functions, which would be zero in real-valued case, and combing with barcodes give the invariants of a tame map, under moderate conditions.\n\nTwo techniques they use are More-Novikov theory and graph representation theory. More recent results can be found in D. Burghelea et al. For example, the tameness requirement can be replaced by the much weaker condition, continuous.\n\nThe proof of the structure theorem relies on the base domain being field, so not many attempts have been made on persistence homology with torsion. Frosini defined a pseudometric on this specific module and proved its stability. One of its novelty is that it doesn't depend on some classification theory to define the metric.\n\nOne advantage of category theory is its ability to lift concrete results to a higher level, showing relationships between seemingly unconnected objects. Bubenik et al. offers a short introduction of category theory fitted for TDA.\n\nCategory theory is the language of modern algebra, and has been widely used in the study of algebraic geometry and topology. It has been noted that \"the key observation of is that the persistence diagram produced by depends only on the algebraic structure carried by this diagram.\" The use of category theory in TDA has proved to be fruitful.\n\nFollowing the notations made in Bubenik et al., the indexing category formula_91 is any preordered set (not necessarily formula_92 or formula_18), the target category formula_45 is any category (instead of the commonly used formula_95), and functors formula_96 are called generalized persistence modules in formula_45, over formula_91.\n\nOne advantage of using category theory in TDA is a clearer understanding of concepts and the discovery of new relationships between proofs. Take two examples for illustration. The understanding of the correspondence between interleaving and matching is of huge importance, since matching has been the method used in the beginning (modified from Morse theory). A summary of works can be found in Vin de Silva et al. Many theorems can be proved much more easily in a more intuitive setting. Another example is the relationship between the construction of different complexes from point clouds. It has long been noticed that Čech and Vietoris-Rips complexes are related. Specifically, formula_99. The essential relationship between Cech and Rips complexes can be seen much more clearly in categorical language.\n\nThe language of category theory also helps cast results in terms recognizable to the broader mathematical community. Bottleneck distance is widely used in TDA because of the results on stability with respect to the bottleneck distance. In fact, the interleaving distance is the terminal object in a poset category of stable metrics on multidimensional persistence modules in a prime field.\n\nSheaves, a central concept in modern algebraic geometry, are intrinsically related to category theory. Roughly speaking, sheaves are the mathematical tool for understanding how local information determines global information. Justin Curry regards level set persistence as the study of fibers of continuous functions. The objects that he studies are very similar to those by MAPPER, but with sheaf theory as the theoretical foundation. Although no breakthrough in the theory of TDA has yet used sheaf theory, it is promising since there are many beautiful theorems in algebraic geometry relating to sheaf theory. For example, a natural theoretical question is whether different filtration methods result in the same output.\n\nStability is of central importance to data analysis, since real data carry noises. By usage of category theory, Bubenik et al. have distinguished between soft and hard stability theorems, and proved that soft cases are formal. Specifically, general workflow of TDA is \n\nThe soft stability theorem asserts that formula_100 is Lipschitz continuous, and the hard stability theorem asserts that formula_101 is Lipschitz continuous.\n\nBottleneck distance is widely used in TDA. The isometry theorem asserts that the interleaving distance formula_102 is equal to the bottleneck distance. Bubenik et al. have abstracted the definition to that between functors formula_103 when formula_91 is equipped with a sublinear projection or superlinear family, in which still remains a pseudometric. Considering the magnificent characters of interleaving distance, here we introduce the general definition of interleaving distance(instead of the first introduced one): Let formula_105 (a function from formula_91 to formula_91 which is monotone and satisfies formula_108 for all formula_109). A formula_110-interleaving between F and G consists of natural transformations formula_111 and formula_112, such that formula_113 and formula_114.\n\nThe two main results are \nThese two results summarize many results on stability of different models of persistence.\n\nFor the stability theorem of multidimensional persistence, please refer to the subsection of persistence.\n\nThe structure theorem is of central importance to TDA; as commented by G. Carlsson, \"what makes homology useful as a discriminator between topological spaces is the fact that there is a classification theorem for finitely generated abelian groups.\" (see the fundamental theorem of finitely generated abelian groups).\n\nThe main argument used in the proof of the original structure theorem is the standard structure theorem for finitely generated modules over a principal ideal domain. However, this argument fails if the indexing set is formula_126.\n\nIn general, not every persistence module can be decomposed into intervals. Many attempts have been made at relaxing the restrictions of the original structure theorem. The case for pointwise finite-dimensional persistence modules indexed by a locally finite subset of formula_18 is solved based on the work of Webb. The most notable result is done by Crawley-Boevey, which solved the case of formula_18. Crawley-Boevey's theorem states that any pointwise finite-dimensional persistence module is a direct sum of interval modules.\n\nTo understand the definition of his theorem, some concepts need introducing. An interval in formula_126 is defined as a subset formula_130 having the property that if formula_131 and if there is an formula_132 such that formula_72, then formula_134 as well. An interval module formula_135 assigns to each element formula_134 the vector space formula_46 and assigns the zero vector space to elements in formula_138. All maps formula_139 are the zero map, unless formula_140 and formula_69, in which case formula_139 is the identity map. Interval modules are indecomposable.\n\nAlthough the result of Crawley-Boevey is a very powerful theorem, it still doesn't extend to the q-tame case. A persistence module is q-tame if the rank of formula_139 is finite for all formula_69. There are examples of q-tame persistence modules that fail to be pointwise finite. However, it turns out that a similar structure theorem still holds if the features that exist only at one index value are removed. This holds because the infinite dimensional parts at each index value do not persist, due to the finite-rank condition. Formally, the observable category formula_145 is defined as formula_146, in which formula_147 denotes the full subcategory of formula_148 whose objects are the ephemeral modules (formula_149 whenever formula_150).\n\nNote that the extended results listed here do not apply to zigzag persistence, since the analogue of a zigzag persistence module over formula_151 is not immediately obvious.\n\nReal data is always finite, and so its study requires us to take stochasticity into account. Statistical analysis gives us the ability to separate true features of the data from artifacts introduced by random noise. Persistent homology has no inherent mechanism to distinguish between low-probability features and high-probability features.\n\nOne way to apply statistics to topological data analysis is to study the statistical properties of topological features of point clouds. The study of random simplicial complexes offers some insight into statistical topology. K. Turner et al. offers a summary of work in this vein.\n\nAnother way is to study probability distributions on the persistence space. The persistence space formula_152 is formula_153, where formula_154 is the space of all barcodes containing exactly formula_155 intervals and the equivalences are formula_156 if formula_157. This space is fairly complicated; for example, it is not complete under the bottleneck metric. The first attempt made to study it is by Y. Mileyko et al. The space of persistence diagrams formula_158 in their paper is defined as formula_159where formula_19 is the diagonal line in formula_60. A nice property is that formula_158 is complete and separable in the Wasserstein metric formula_163. Expectation, variance, and conditional probability can be defined in the Fréchet sense. This allows many statistical tools to be ported to TDA. Works on null hypothesis significance test, confidence intervals, and robust estimates are notable steps.\n\nPersistence landscapes, introduced by Peter Bubenik, are a different way to represent of barcodes, more amenable to statistical analysis. The persistence landscape of a persistent module formula_75 is defined as a function formula_165, formula_166, where formula_167 denotes the extended real line and formula_168. The space of persistence landscapes is very nice: it inherits all good properties of barcode representation (stability, easy representation, etc.), but statistical quantities can be readily defined, and some problems in Y. Mileyko et al.'s work, such as the non-uniqueness of expectations, can be overcome. Effective algorithms for computation with persistence landscapes are available. Another approach is to use revised persistence, which is image, kernel and cokernel persistence.\n\nMore than one way exists to classify the applications of TDA. Perhaps the most natural way is by field. A very incomplete list of successful applications includes data skeletonization, shape study, graph reconstruction, \n\nimage analysis,\n, bacteria classification using molecular spectroscopy , hyperspectral imaging in physical-chemistry and remote sensing .\n\nAnother way is by distinguishing the techniques by G. Carlsson,\n\nAyasdi is a data analysis company relying heavily on TDA, cofounded by a number of leading researchers in the field. There are several notable interesting features of the recent applications of TDA:\nOne of the main fields of data analysis today is machine learning. Some examples of machine learning in TDA can be found in Adcock et al. A conference is dedicated to the link between TDA and machine learning. In order to apply tools from machine leaning, the information obtained from TDA should be represented in vector form. An ongoing and promising attempt is the persistence landscape discussed above. Another attempt uses the concept of persistence images. However, one problem of this method is the loss of stability, since the hard stability theorem depends on the barcode representation.\n\nTopological data analysis and persistent homology have had impacts on Morse theory. Morse theory has played a very important role in the theory of TDA, including on computation. Some work in persistent homology has extended results about Morse functions to tame functions or, even to continuous functions. A forgotten result of R. Deheuvels long before the invention of persistent homology extends Morse theory to all continuous functions.\n\nOne recent result is that the category of Reeb graphs is equivalent to a particular class of cosheaf. This is motivated by theoretical work in TDA, since the Reeb graph is related to Morse theory and MAPPER is derived from it. The proof of this theorem relies on the interleaving distance.\n\nIt is evident to mathematicians that persistent homology is closely related to spectral sequences. Zigzag persistence may turn out to be of theoretical importance to spectral sequences.\n\n\nBrief Introduction \nMonograph\nVideo Lecture\nTextbook on Topology\nOther Resources of TDA\n"}
