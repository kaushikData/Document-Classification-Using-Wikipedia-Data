{"id": "49256", "url": "https://en.wikipedia.org/wiki?curid=49256", "title": "Age of the Earth", "text": "Age of the Earth\n\nThe age of the Earth is 4.54 ± 0.05 billion years This age may represent the age of the Earth's accretion, of core formation, or of the material from which the Earth formed. This dating is based on evidence from radiometric age-dating of meteorite material and is consistent with the radiometric ages of the oldest-known terrestrial and lunar samples.\n\nFollowing the development of radiometric age-dating in the early 20th century, measurements of lead in uranium-rich minerals showed that some were in excess of a billion years old.\nThe oldest such minerals analyzed to date—small crystals of zircon from the Jack Hills of Western Australia—are at least 4.404 billion years old. Calcium–aluminium-rich inclusions—the oldest known solid constituents within meteorites that are formed within the Solar System—are 4.567 billion years old, giving a lower limit for the age of the solar system.\n\nIt is hypothesised that the accretion of Earth began soon after the formation of the calcium-aluminium-rich inclusions and the meteorites. Because the exact amount of time this accretion process took is not yet known, and the predictions from different accretion models range from a few million up to about 100 million years, the exact age of Earth is difficult to determine. It is also difficult to determine the exact age of the oldest rocks on Earth, exposed at the surface, as they are aggregates of minerals of possibly different ages.\n\nStudies of strata, the layering of rocks and earth, gave naturalists an appreciation that Earth may have been through many changes during its existence. These layers often contained fossilized remains of unknown creatures, leading some to interpret a progression of organisms from layer to layer.\n\nNicolas Steno in the 17th century was one of the first naturalists to appreciate the connection between fossil remains and strata. His observations led him to formulate important stratigraphic concepts (i.e., the \"law of superposition\" and the \"principle of original horizontality\"). In the 1790s, William Smith hypothesized that if two layers of rock at widely differing locations contained similar fossils, then it was very plausible that the layers were the same age. William Smith's nephew and student, John Phillips, later calculated by such means that Earth was about 96 million years old.\n\nIn the mid-18th century, the naturalist Mikhail Lomonosov suggested that Earth had been created separately from, and several hundred thousand years before, the rest of the universe. Lomonosov's ideas were mostly speculative. In 1779 the Comte du Buffon tried to obtain a value for the age of Earth using an experiment: He created a small globe that resembled Earth in composition and then measured its rate of cooling. This led him to estimate that Earth was about 75,000 years old.\n\nOther naturalists used these hypotheses to construct a history of Earth, though their timelines were inexact as they did not know how long it took to lay down stratigraphic layers. In 1830, geologist Charles Lyell, developing ideas found in James Hutton's works, popularized the concept that the features of Earth were in perpetual change, eroding and reforming continuously, and the rate of this change was roughly constant. This was a challenge to the traditional view, which saw the history of Earth as static, with changes brought about by intermittent catastrophes. Many naturalists were influenced by Lyell to become \"uniformitarians\" who believed that changes were constant and uniform.\n\nIn 1862, the physicist William Thomson, 1st Baron Kelvin published calculations that fixed the age of Earth at between 20 million and 400 million years.\nHe assumed that Earth had formed as a completely molten object, and determined the amount of time it would take for the near-surface to cool to its present temperature. His calculations did not account for heat produced via radioactive decay (a process then unknown to science) or, more significantly, convection inside the Earth, which allows more heat to escape from the interior to warm rocks near the surface. Even more constraining were Kelvin's estimates of the age of the Sun, which were based on estimates of its thermal output and a theory that the Sun obtains its energy from gravitational collapse; Kelvin estimated that the Sun is about 20 million years old.\n\nGeologists such as Charles Lyell had trouble accepting such a short age for Earth. For biologists, even 100 million years seemed much too short to be plausible. In Darwin's theory of evolution, the process of random heritable variation with cumulative selection requires great durations of time. (According to modern biology, the total evolutionary history from the beginning of life to today has taken place since 3.5 to 3.8 billion years ago, the amount of time which passed since the last universal ancestor of all living organisms as shown by geological dating.)\n\nIn a lecture in 1869, Darwin's great advocate, Thomas H. Huxley, attacked Thomson's calculations, suggesting they appeared precise in themselves but were based on faulty assumptions. The physicist Hermann von Helmholtz (in 1856) and astronomer Simon Newcomb (in 1892) contributed their own calculations of 22 and 18 million years respectively to the debate: they independently calculated the amount of time it would take for the Sun to condense down to its current diameter and brightness from the nebula of gas and dust from which it was born. Their values were consistent with Thomson's calculations. However, they assumed that the Sun was only glowing from the heat of its gravitational contraction. The process of solar nuclear fusion was not yet known to science.\n\nIn 1895 John Perry challenged Kelvin's figure on the basis of his assumptions on conductivity, and Oliver Heaviside entered the dialogue, considering it \"a vehicle to display the ability of his operator method to solve problems of astonishing complexity.\"\n\nOther scientists backed up Thomson's figures. Charles Darwin's son, the astronomer George H. Darwin, proposed that Earth and Moon had broken apart in their early days when they were both molten. He calculated the amount of time it would have taken for tidal friction to give Earth its current 24-hour day. His value of 56 million years added additional evidence that Thomson was on the right track.\n\nThe last estimate Thomson gave, in 1897, was: \"that it was more than 20 and less than 40 million year old, and probably much nearer 20 than 40\". In 1899 and 1900, John Joly calculated the rate at which the oceans should have accumulated salt from erosion processes, and determined that the oceans were about 80 to 100 million years old.\n\nBy their chemical nature, rock minerals contain certain elements and not others; but in rocks containing radioactive isotopes, the process of radioactive decay generates exotic elements over time. By measuring the concentration of the stable end product of the decay, coupled with knowledge of the half life and initial concentration of the decaying element, the age of the rock can be calculated. Typical radioactive end products are argon from decay of potassium-40, and lead from decay of uranium and thorium. If the rock becomes molten, as happens in Earth's mantle, such nonradioactive end products typically escape or are redistributed. Thus the age of the oldest terrestrial rock gives a minimum for the age of Earth, assuming that no rock has been intact for longer than the Earth itself.\n\nIn 1892, Thomson had been made Lord Kelvin in appreciation of his many scientific accomplishments. Kelvin calculated the age of the Earth by using thermal gradients, and he arrived at an estimate of about 100 million years. He did not realize that the Earth mantle was convecting, and this invalidated his estimate. In 1895, John Perry produced an age-of-Earth estimate of 2 to 3 billion years using a model of a convective mantle and thin crust. Kelvin stuck by his estimate of 100 million years, and later reduced it to about 20 million years.\n\nThe discovery of radioactivity introduced another factor in the calculation. After Henri Becquerel's initial discovery in 1896, Marie and Pierre Curie discovered the radioactive elements polonium and radium in 1898; and in 1903, Pierre Curie and Albert Laborde announced that radium produces enough heat to melt its own weight in ice in less than an hour. Geologists quickly realized that this upset the assumptions underlying most calculations of the age of Earth.\nThese had assumed that the original heat of the Earth and Sun had dissipated steadily into space, but radioactive decay meant that this heat had been continually replenished. George Darwin and John Joly were the first to point this out, in 1903.\n\nRadioactivity, which had overthrown the old calculations, yielded a bonus by providing a basis for new calculations, in the form of radiometric dating.\n\nErnest Rutherford and Frederick Soddy jointly had continued their work on radioactive materials and concluded that radioactivity was due to a spontaneous transmutation of atomic elements. In radioactive decay, an element breaks down into another, lighter element, releasing alpha, beta, or gamma radiation in the process. They also determined that a particular isotope of a radioactive element decays into another element at a distinctive rate. This rate is given in terms of a \"half-life\", or the amount of time it takes half of a mass of that radioactive material to break down into its \"decay product\".\n\nSome radioactive materials have short half-lives; some have long half-lives. Uranium and thorium have long half-lives, and so persist in Earth's crust, but radioactive elements with short half-lives have generally disappeared. This suggested that it might be possible to measure the age of Earth by determining the relative proportions of radioactive materials in geological samples. In reality, radioactive elements do not always decay into nonradioactive (\"stable\") elements directly, instead, decaying into other radioactive elements that have their own half-lives and so on, until they reach a stable element. These \"decay chains\", such as the uranium-radium and thorium series, were known within a few years of the discovery of radioactivity and provided a basis for constructing techniques of radiometric dating.\n\nThe pioneers of radioactivity were chemist Bertram B. Boltwood and the energetic Rutherford. Boltwood had conducted studies of radioactive materials as a consultant, and when Rutherford lectured at Yale in 1904, Boltwood was inspired to describe the relationships between elements in various decay series. Late in 1904, Rutherford took the first step toward radiometric dating by suggesting that the alpha particles released by radioactive decay could be trapped in a rocky material as helium atoms. At the time, Rutherford was only guessing at the relationship between alpha particles and helium atoms, but he would prove the connection four years later.\n\nSoddy and Sir William Ramsay had just determined the rate at which radium produces alpha particles, and Rutherford proposed that he could determine the age of a rock sample by measuring its concentration of helium. He dated a rock in his possession to an age of 40 million years by this technique. Rutherford wrote,\n\nRutherford assumed that the rate of decay of radium as determined by Ramsay and Soddy was accurate, and that helium did not escape from the sample over time. Rutherford's scheme was inaccurate, but it was a useful first step.\n\nBoltwood focused on the end products of decay series. In 1905, he suggested that lead was the final stable product of the decay of radium. It was already known that radium was an intermediate product of the decay of uranium. Rutherford joined in, outlining a decay process in which radium emitted five alpha particles through various intermediate products to end up with lead, and speculated that the radium-lead decay chain could be used to date rock samples. Boltwood did the legwork, and by the end of 1905 had provided dates for 26 separate rock samples, ranging from 92 to 570 million years. He did not publish these results, which was fortunate because they were flawed by measurement errors and poor estimates of the half-life of radium. Boltwood refined his work and finally published the results in 1907.\n\nBoltwood's paper pointed out that samples taken from comparable layers of strata had similar lead-to-uranium ratios, and that samples from older layers had a higher proportion of lead, except where there was evidence that lead had leached out of the sample. His studies were flawed by the fact that the decay series of thorium was not understood, which led to incorrect results for samples that contained both uranium and thorium. However, his calculations were far more accurate than any that had been performed to that time. Refinements in the technique would later give ages for Boltwood's 26 samples of 410 million to 2.2 billion years.\n\nAlthough Boltwood published his paper in a prominent geological journal, the geological community had little interest in radioactivity. Boltwood gave up work on radiometric dating and went on to investigate other decay series. Rutherford remained mildly curious about the issue of the age of Earth but did little work on it.\n\nRobert Strutt tinkered with Rutherford's helium method until 1910 and then ceased. However, Strutt's student Arthur Holmes became interested in radiometric dating and continued to work on it after everyone else had given up. Holmes focused on lead dating, because he regarded the helium method as unpromising. He performed measurements on rock samples and concluded in 1911 that the oldest (a sample from Ceylon) was about 1.6 billion years old. These calculations were not particularly trustworthy. For example, he assumed that the samples had contained only uranium and no lead when they were formed.\n\nMore important research was published in 1913. It showed that elements generally exist in multiple variants with different masses, or \"isotopes\". In the 1930s, isotopes would be shown to have nuclei with differing numbers of the neutral particles known as \"neutrons\". In that same year, other research was published establishing the rules for radioactive decay, allowing more precise identification of decay series.\n\nMany geologists felt these new discoveries made radiometric dating so complicated as to be worthless. Holmes felt that they gave him tools to improve his techniques, and he plodded ahead with his research, publishing before and after the First World War. His work was generally ignored until the 1920s, though in 1917 Joseph Barrell, a professor of geology at Yale, redrew geological history as it was understood at the time to conform to Holmes's findings in radiometric dating. Barrell's research determined that the layers of strata had not all been laid down at the same rate, and so current rates of geological change could not be used to provide accurate timelines of the history of Earth.\n\nHolmes' persistence finally began to pay off in 1921, when the speakers at the yearly meeting of the British Association for the Advancement of Science came to a rough consensus that Earth was a few billion years old, and that radiometric dating was credible. Holmes published \"The Age of the Earth, an Introduction to Geological Ideas\" in 1927 in which he presented a range of 1.6 to 3.0 billion years. No great push to embrace radiometric dating followed, however, and the die-hards in the geological community stubbornly resisted. They had never cared for attempts by physicists to intrude in their domain, and had successfully ignored them so far. The growing weight of evidence finally tilted the balance in 1931, when the National Research Council of the US National Academy of Sciences decided to resolve the question of the age of Earth by appointing a committee to investigate. Holmes, being one of the few people on Earth who was trained in radiometric dating techniques, was a committee member, and in fact wrote most of the final report.\n\nThus, Arthur Holmes' report concluded that radioactive dating was the only reliable means of pinning down geological time scales. Questions of bias were deflected by the great and exacting detail of the report. It described the methods used, the care with which measurements were made, and their error bars and limitations.\n\nRadiometric dating continues to be the predominant way scientists date geologic timescales. Techniques for radioactive dating have been tested and fine-tuned on an ongoing basis since the 1960s. Forty or so different dating techniques have been utilized to date, working on a wide variety of materials. Dates for the same sample using these different techniques are in very close agreement on the age of the material.\n\nPossible contamination problems do exist, but they have been studied and dealt with by careful investigation, leading to sample preparation procedures being minimized to limit the chance of contamination.\n\nAn age of 4.55 ± 0.07 billion years, very close to today's accepted age, was determined by Clair Cameron Patterson using uranium-lead isotope dating (specifically lead-lead dating) on several meteorites including the Canyon Diablo meteorite and published in 1956.\n\nThe quoted age of Earth is derived, in part, from the Canyon Diablo meteorite for several important reasons and is built upon a modern understanding of cosmochemistry built up over decades of research.\n\nMost geological samples from Earth are unable to give a direct date of the formation of Earth from the solar nebula because Earth has undergone differentiation into the core, mantle, and crust, and this has then undergone a long history of mixing and unmixing of these sample reservoirs by plate tectonics, weathering and hydrothermal circulation.\n\nAll of these processes may adversely affect isotopic dating mechanisms because the sample cannot always be assumed to have remained as a closed system, by which it is meant that either the parent or daughter nuclide (a species of atom characterised by the number of neutrons and protons an atom contains) or an intermediate daughter nuclide may have been partially removed from the sample, which will skew the resulting isotopic date. To mitigate this effect it is usual to date several minerals in the same sample, to provide an isochron. Alternatively, more than one dating system may be used on a sample to check the date.\n\nSome meteorites are furthermore considered to represent the primitive material from which the accreting solar disk was formed. Some have behaved as closed systems (for some isotopic systems) soon after the solar disk and the planets formed. To date, these assumptions are supported by much scientific observation and repeated isotopic dates, and it is certainly a more robust hypothesis than that which assumes a terrestrial rock has retained its original composition.\n\nNevertheless, ancient Archaean lead ores of galena have been used to date the formation of Earth as these represent the earliest formed lead-only minerals on the planet and record the earliest homogeneous lead-lead isotope systems on the planet. These have returned age dates of 4.54 billion years with a precision of as little as 1% margin for error.\n\nStatistics for several meteorites that have undergone isochron dating are as follows:\n\nThe Canyon Diablo meteorite was used because it is both large and representative of a particularly rare type of meteorite that contains sulfide minerals (particularly troilite, FeS), metallic nickel-iron alloys, plus silicate minerals.\nThis is important because the presence of the three mineral phases allows investigation of isotopic dates using samples that provide a great separation in concentrations between parent and daughter nuclides. This is particularly true of uranium and lead. Lead is strongly chalcophilic and is found in the sulfide at a much greater concentration than in the silicate, versus uranium. Because of this segregation in the parent and daughter nuclides during the formation of the meteorite, this allowed a much more precise date of the formation of the solar disk and hence the planets than ever before.\n\nThe age determined from the Canyon Diablo meteorite has been confirmed by hundreds of other age determinations, from both terrestrial samples and other meteorites. The meteorite samples, however, show a spread from 4.53 to 4.58 billion years ago. This is interpreted as the duration of formation of the solar nebula and its collapse into the solar disk to form the Sun and the planets. This 50 million year time span allows for accretion of the planets from the original solar dust and meteorites.\n\nThe moon, as another extraterrestrial body that has not undergone plate tectonics and that has no atmosphere, provides quite precise age dates from the samples returned from the Apollo missions. Rocks returned from the Moon have been dated at a maximum of 4.51 billion years old. Martian meteorites that have landed upon Earth have also been dated to around 4.5 billion years old by lead-lead dating. Lunar samples, since they have not been disturbed by weathering, plate tectonics or material moved by organisms, can also provide dating by direct electron microscope examination of cosmic ray tracks. The accumulation of dislocations generated by high energy cosmic ray particle impacts provides another confirmation of the isotopic dates. Cosmic ray dating is only useful on material that has not been melted, since melting erases the crystalline structure of the material, and wipes away the tracks left by the particles.\n\nAltogether, the concordance of age dates of both the earliest terrestrial lead reservoirs and all other reservoirs within the Solar System found to date are used to support the fact that Earth and the rest of the Solar System formed at around 4.53 to 4.58 billion years ago.\n\n\n"}
{"id": "17076143", "url": "https://en.wikipedia.org/wiki?curid=17076143", "title": "American Automatic Control Council", "text": "American Automatic Control Council\n\nThe American Automatic Control Council (AACC) is an organization founded in 1957 for research in control theory. AACC is a member of the International Federation of Automatic Control (IFAC) and is an association of the control systems divisions of nine member societies:\n\nThe American Control Conference (ACC) is an annual research conference sponsored by the AACC and is one of the most prestigious conferences in the field of control theory. Dating back to 1960, the attendees of the ACC are about 50% from the Americas and about 50% from other countries, consisting mostly of researchers with a large portion being students.\n\nThe AACC issues five awards for achievements in control theory:\n\n\n"}
{"id": "1905175", "url": "https://en.wikipedia.org/wiki?curid=1905175", "title": "Aviator Glacier", "text": "Aviator Glacier\n\nAviator Glacier is major valley glacier in Antarctica that is over long and wide, descending generally southward from the plateau of Victoria Land along the west side of Mountaineer Range, and entering Lady Newnes Bay between Cape Sibbald and Hayes Head where it forms the Aviator Glacier Tongue.\n\nA glacier is a mass of ice with sufficient thickness to flow away from the source area in lobes, tongues or masses. Glaciers are usually found at high altitudes or high elevations.\n\nThe glacier was photographed from the air by Captain W.M. Hawkes, US Navy, on the historic first flight from New Zealand to McMurdo Sound on December 17, 1955. An attempt to reconnoiter it by helicopter and to land a party of the NZGSAE on it had to be abandoned when USS \"Glacier\" was damaged in pressure ice in December 1958. Named by NZGSAE, 1958–59, as a tribute to the hazardous work of pilots and other airmen in Antarctic exploratory and scientific operations.\n\nThe Aviator Glacier Tongue in Antarctica is a seaward extension of Aviator Glacier into the Ross Sea, between Wood Bay and Lady Newness Bay along the coast of Victoria Land. \nThis floating ice tongue extends into the water for about .\nThe name was recommended by the Advisory Committee on Antarctic Names (US-ACAN) in association with Aviator Glacier.\n\n\n"}
{"id": "13415486", "url": "https://en.wikipedia.org/wiki?curid=13415486", "title": "Bolaamphiphile", "text": "Bolaamphiphile\n\nBolaamphiphiles (also known as \"bolaform surfactants\",\n\"bolaphiles\", or \"alpha-omega-type surfactants\") are amphiphilic molecules that have hydrophilic groups at both ends of a sufficiently long hydrophobic hydrocarbon chain. Compared to single-headed amphiphiles, the introduction of a second head-group generally induces a higher solubility in water, an increase in the critical micelle concentration (cmc), and a decrease in aggregation number. The aggregate morphologies of bolaamphiphiles include spheres, cylinders, disks, and vesicles. Bolaamphiphiles are also known to form helical structures that can form monolayer microtubular self-assemblies.\n\nFuhrhop, J-H; Wang, T. Bolaamphiphile, Chem. Rev. (2004), 104(6), 2901-2937.\n\nChen, Yuxia; Liu, Yan; Guo, Rong. Aggregation behavior of an amino acid-derived bolaamphiphile and a conventional surfactant mixed system. Journal of Colloid and Interface Science (2009), 336(2), 766-772. CODEN: JCISA5 . AN 2009:776584\n\nYin, Shouchun; Wang, Chao; Song, Bo; Chen, Senlin; Wang, Zhiqiang. Self-Organization of a Polymerizable Bolaamphiphile Bearing a Diacetylene Group and L-Aspartic Acid Group. Langmuir (2009), 25(16), 8968-8973. CODEN: LANGD5 . CAN 151:173915 AN 2009:383258\n\nWang, H.; Li, M.; Xu, Z.; Qiao, W.; Li, Z. Interfacial tension of unsymmetrical bolaamphiphile surfactant in surfactant/alkali/crude oil systems. Energy Sources, Part A: Recovery, Utilization, and Environmental Effects (2008), 30(16), 1442-1450. CODEN: ESPACB . CAN 150:475745 AN 2008:763292\n\nChen, Senlin; Song, Bo; Wang, Zhiqiang; Zhang, Xi. Self-Organization of Bolaamphiphile Bearing Biphenyl Mesogen and Aspartic-Acid Headgroups. Journal of Physical Chemistry C (2008), 112(9), 3308-3313. CODEN: JPCCCK . CAN 148:372219 AN 2008:176360\n\nFeng Qiu, Chengkang Tang, Yongzhu Chen Amyloid-like aggregation of designer bolaamphiphilic peptides: Effect of hydrophobic section and hydrophilic heads. Journal of peptide science. (2017) DOI: 10.1002/psc.3062\n"}
{"id": "36266089", "url": "https://en.wikipedia.org/wiki?curid=36266089", "title": "Carbon Shift", "text": "Carbon Shift\n\n\"Carbon Shift: How Peak Oil and the Climate Crisis Will Change Canada (and Our Lives)\" is a 2009 non-fiction book edited by Thomas Homer-Dixon and Nick Garrison that collects six essays that discusses the issues of peak oil and climate change. The book was first published in hardcover by Random House of Canada in 2009 under the title \"Carbon Shift: How the Twin Crises of Oil Depletion and Climate Change Will Define the Future\", and became a national bestseller. In 2010, the paperback was published by Vintage Canada, a division of Random House Canada, the sub-title then changing to \"How Peak Oil and the Climate Crisis Will Change Canada (and Our Lives)\".\n\n\"Carbon Shift\" encompasses six essays by experts in the fields of economics, geology, politics, and science. The essays argue points such as humanity's potential for exhausting the supply of non-renewable fuels and what could be done to prevent this.\n\n\nQuill and Quire stated that the differing outlook of the essays showed that it was \"hard to imagine working together on solutions when there is so little consensus about the exact nature of the problems\". Peter Robinson echoed this statement, saying that the book's essays \"reinforce the conclusion that it will take all of our ingenuity, will and perseverance to prevent catastrophe.\" Andrew Nikiforuk praised \"Carbon Shift\", saying that it does \"a fine job of exposing Canada's big oily gamble\".\n\n"}
{"id": "2081987", "url": "https://en.wikipedia.org/wiki?curid=2081987", "title": "Chapman function", "text": "Chapman function\n\nA Chapman function describes the integration of atmospheric absorption along a slant path on a spherical earth, relative to the vertical case. It applies for any quantity with a concentration decreasing exponentially with increasing altitude. To a first approximation, valid at small zenith angles, the Chapman function for optical absorption is equal to \n\nwhere \"z\" is the zenith angle and sec denotes the secant function. \n\nThe Chapman function is named after Sydney Chapman.\n\n\n\n"}
{"id": "7039", "url": "https://en.wikipedia.org/wiki?curid=7039", "title": "Control theory", "text": "Control theory\n\nControl theory in control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without \"delay or overshoot\" and ensuring control stability.\n\nTo do this, a \"controller\" with the requisite corrective behaviour is required. This controller monitors the controlled process variable (PV), and compares it with the reference or set point (SP). The difference between actual and desired value of the process variable, called the \"error\" signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are controllability and observability. On this is based the advanced type of automation that revolutionized manufacturing, aircraft, communications and other industries. This is \"feedback control\", which is usually \"continuous\" and involves taking measurements using a sensor and making calculated adjustments to keep the measured variable within a set range by means of a \"final control element\", such as a control valve.\n\nExtensive use is usually made of a diagrammatic style known as the block diagram. In it the transfer function, also known as the system function or network function, is a mathematical model of the relation between the input and output based on the differential equations describing the system.\n\nControl theory dates from the 19th century, when the theoretical basis for the operation of governors was first described by James Clerk Maxwell. Control theory was further advanced by Edward Routh in 1874, Charles Sturm and in 1895, Adolf Hurwitz, who all contributed to the establishment of control stability criteria; and from 1922 onwards, the development of PID control theory by Nicolas Minorsky.\nAlthough a major application of control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs.\n\nAlthough control systems of various types date back to antiquity, a more formal analysis of the field began with a dynamics analysis of the centrifugal governor, conducted by the physicist James Clerk Maxwell in 1868, entitled \"On Governors\". This described and analyzed the phenomenon of self-oscillation, in which lags in the system may lead to overcompensation and unstable behavior. This generated a flurry of interest in the topic, during which Maxwell's classmate, Edward John Routh, abstracted Maxwell's results for the general class of linear systems. Independently, Adolf Hurwitz analyzed system stability using differential equations in 1877, resulting in what is now known as the Routh–Hurwitz theorem.\n\nA notable application of dynamic control was in the area of manned flight. The Wright brothers made their first successful test flights on December 17, 1903 and were distinguished by their ability to control their flights for substantial periods (more so than the ability to produce lift from an airfoil, which was known). Continuous, reliable control of the airplane was necessary for flights lasting longer than a few seconds.\n\nBy World War II, control theory was becoming an important area of research. Irmgard Flügge-Lotz developed the theory of discontinuous automatic control systems, and applied the bang-bang principle to the development of automatic flight control equipment for aircraft. Other areas of application for discontinuous controls included fire-control systems, guidance systems and electronics.\n\nA Centrifugal governor is used to regulate the windmill velocity.\n\nSometimes, mechanical methods are used to improve the stability of systems. For example, ship stabilizers are fins mounted beneath the waterline and emerging laterally. In contemporary vessels, they may be gyroscopically controlled active fins, which have the capacity to change their angle of attack to counteract roll caused by wind or waves acting on the ship.\n\nThe Space Race also depended on accurate spacecraft control, and control theory has also seen an increasing use in fields such as economics.\n\nFundamentally, there are two types of control loops: open loop control and closed loop (feedback) control.\n\nIn open loop control, the control action from the controller is independent of the \"process output\" (or \"controlled process variable\" - PV). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. The control action is the timed switching on/off of the boiler, the process variable is the building temperature, but neither is linked.\n\nIn closed loop control, the control action from the controller is dependent on feedback from the process in the form of the value of the process variable (PV). In the case of the boiler analogy, a closed loop would include a thermostat to compare the building temperature (PV) with the temperature set on the thermostat (the set point - SP). This generates a controller output to maintain the building at the desired temperature by switching the boiler on and off. A closed loop controller, therefore, has a feedback loop which ensures the controller exerts a control action to manipulate the process variable to be the same as the \"Reference input\" or \"set point\". For this reason, closed loop controllers are also called feedback controllers.\n\nThe definition of a closed loop control system according to the British Standard Institution is \"a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to zero.\" \n\nLikewise; \"A \"Feedback Control System\" is a system which tends to maintain a prescribed relationship of one system variable to another by comparing functions of these variables and using the difference as a means of control.\"\n\nAn example of a control system is a car's cruise control, which is a device designed to maintain vehicle speed at a constant \"desired\" or \"reference\" speed provided by the driver. The \"controller\" is the cruise control, the \"plant\" is the car, and the \"system\" is the car and the cruise control. The system output is the car's speed, and the control itself is the engine's throttle position which determines how much power the engine delivers.\n\nA primitive way to implement cruise control is simply to lock the throttle position when the driver engages cruise control. However, if the cruise control is engaged on a stretch of flat road, then the car will travel slower going uphill and faster when going downhill. This type of controller is called an \"open-loop controller\" because there is no feedback; no measurement of the system output (the car's speed) is used to alter the control (the throttle position.) As a result, the controller cannot compensate for changes acting on the car, like a change in the slope of the road.\n\nIn a \"closed-loop control system\", data from a sensor monitoring the car's speed (the system output) enters a controller which continuously compares the quantity representing the speed with the reference quantity representing the desired speed. The difference, called the error, determines the throttle position (the control). The result is to match the car's speed to the reference speed (maintain the desired system output). Now, when the car goes uphill, the difference between the input (the sensed speed) and the reference continuously determines the throttle position. As the sensed speed drops below the reference, the difference increases, the throttle opens, and engine power increases, speeding up the vehicle. In this way, the controller dynamically counteracts changes to the car's speed. The central idea of these control systems is the \"feedback loop\", the controller affects the system output, which in turn is measured and fed back to the controller.\n\nTo overcome the limitations of the open-loop controller, control theory introduces feedback.\nA closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is \"fed back\" as input to the process, closing the loop.\n\nClosed-loop controllers have the following advantages over open-loop controllers:\n\nIn some systems, closed-loop and open-loop control are used simultaneously. In such systems, the open-loop control is termed feedforward and serves to further improve reference tracking performance.\n\nA common closed-loop controller architecture is the PID controller.\n\nThe output of the system \"y(t)\" is fed back through a sensor measurement \"F\" to a comparison with the reference value \"r(t)\". The controller \"C\" then takes the error \"e\" (difference) between the reference and the output to change the inputs \"u\" to the system under control \"P\". This is shown in the figure. This kind of controller is a closed-loop controller or feedback controller.\n\nThis is called a single-input-single-output (\"SISO\") control system; \"MIMO\" (i.e., Multi-Input-Multi-Output) systems, with more than one input/output, are common. In such cases variables are represented through vectors instead of simple scalar values. For some distributed parameter systems the vectors may be infinite-dimensional (typically functions).\n\nIf we assume the controller \"C\", the plant \"P\", and the sensor \"F\" are linear and time-invariant (i.e., elements of their transfer function \"C(s)\", \"P(s)\", and \"F(s)\" do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations:\n\nSolving for \"Y\"(\"s\") in terms of \"R\"(\"s\") gives\n\nThe expression formula_5 is referred to as the \"closed-loop transfer function\" of the system. The numerator is the forward (open-loop) gain from \"r\" to \"y\", and the denominator is one plus the gain in going around the feedback loop, the so-called loop gain. If formula_6, i.e., it has a large norm with each value of \"s\", and if formula_7, then \"Y(s)\" is approximately equal to \"R(s)\" and the output closely tracks the reference input.\n\nA proportional–integral–derivative controller (PID controller) is a control loop feedback mechanism control technique widely used in control systems.\n\nA PID controller continuously calculates an \"error value\" formula_8 as the difference between a desired setpoint and a measured process variable and applies a correction based on proportional, integral, and derivative terms. \"PID\" is an initialism for \"Proportional-Integral-Derivative\", referring to the three terms operating on the error signal to produce a control signal.\n\nThe theoretical understanding and application dates from the 1920s, and they are implemented in nearly all analogue control systems; originally in mechanical controllers, and then using discrete electronics and latterly in industrial process computers.\nThe PID controller is probably the most-used feedback control design.\n\nIf \"u(t)\" is the control signal sent to the system, \"y(t)\" is the measured output and \"r(t)\" is the desired output, and formula_9 is the tracking error, a PID controller has the general form\n\nThe desired closed loop dynamics is obtained by adjusting the three parameters formula_11, formula_12 and formula_13, often iteratively by \"tuning\" and without specific knowledge of a plant model. Stability can often be ensured using only the proportional term. The integral term permits the rejection of a step disturbance (often a striking specification in process control). The derivative term is used to provide damping or shaping of the response. PID controllers are the most well-established class of control systems: however, they cannot be used in several more complicated cases, especially if MIMO systems are considered.\n\nApplying Laplace transformation results in the transformed PID controller equation\n\nwith the PID controller transfer function\n\nAs an example of tuning a PID controller in the closed-loop system formula_17, consider a 1st order plant given by\n\nwhere formula_19 and formula_20 are some constants. The plant output is fed back through\n\nwhere formula_22 is also a constant. Now if we set formula_23, formula_24, and formula_25, we can express the PID controller transfer function in series form as\n\nPlugging formula_27, formula_28, and formula_29 into the closed-loop transfer function formula_17, we find that by setting\n\nformula_32. With this tuning in this example, the system output follows the reference input exactly.\n\nHowever, in practice, a pure differentiator is neither physically realizable nor desirable due to amplification of noise and resonant modes in the system. Therefore, a phase-lead compensator type approach or a differentiator with low-pass roll-off are used instead.\n\nThe field of control theory can be divided into two branches:\n\nMathematical techniques for analyzing and designing control systems fall into two different categories:\nIn contrast to the frequency domain analysis of the classical control theory, modern control theory utilizes the time-domain state space representation, a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations. To abstract from the number of inputs, outputs, and states, the variables are expressed as vectors and the differential and algebraic equations are written in matrix form (the latter only being possible when the dynamical system is linear). The state space representation (also known as the \"time-domain approach\") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With inputs and outputs, we would otherwise have to write down Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions. \"State space\" refers to the space whose axes are the state variables. The state of the system can be represented as a point within that space.\n\nControl systems can be divided into different categories depending on the number of inputs and outputs.\n\nThe \"stability\" of a general dynamical system with no input can be described with Lyapunov stability criteria.\n\nFor simplicity, the following descriptions focus on continuous-time and discrete-time linear systems.\n\nMathematically, this means that for a causal linear system to be stable all of the poles of its transfer function must have negative-real values, i.e. the real part of each pole must be less than zero. Practically speaking, stability requires that the transfer function complex poles reside\nThe difference between the two cases is simply due to the traditional method of plotting continuous time versus discrete time transfer functions. The continuous Laplace transform is in Cartesian coordinates where the formula_33 axis is the real axis and the discrete Z-transform is in circular coordinates where the formula_34 axis is the real axis.\n\nWhen the appropriate conditions above are satisfied a system is said to be asymptotically stable; the variables of an asymptotically stable control system always decrease from their initial value and do not show permanent oscillations. Permanent oscillations occur when a pole has a real part exactly equal to zero (in the continuous time case) or a modulus equal to one (in the discrete time case). If a simply stable system response neither decays nor grows over time, and has no oscillations, it is marginally stable; in this case the system transfer function has non-repeated poles at the complex plane origin (i.e. their real and complex component is zero in the continuous time case). Oscillations are present when poles with real part equal to zero have an imaginary part not equal to zero.\n\nIf a system in question has an impulse response of\n\nthen the Z-transform (see this example), is given by\n\nwhich has a pole in formula_37 (zero imaginary part). This system is BIBO (asymptotically) stable since the pole is \"inside\" the unit circle.\n\nHowever, if the impulse response was\n\nthen the Z-transform is\n\nwhich has a pole at formula_40 and is not BIBO stable since the pole has a modulus strictly greater than one.\n\nNumerous tools exist for the analysis of the poles of a system. These include graphical systems like the root locus, Bode plots or the Nyquist plots.\n\nMechanical changes can make equipment (and control systems) more stable. Sailors add ballast to improve the stability of ships. Cruise ships use antiroll fins that extend transversely from the side of the ship for perhaps 30 feet (10 m) and are continuously rotated about their axes to develop forces that oppose the roll.\n\nControllability and observability are main issues in the analysis of a system before deciding the best control strategy to be applied, or whether it is even possible to control or stabilize the system. Controllability is related to the possibility of forcing the system into a particular state by using an appropriate control signal. If a state is not controllable, then no signal will ever be able to control the state. If a state is not controllable, but its dynamics are stable, then the state is termed \"stabilizable\". Observability instead is related to the possibility of \"observing\", through output measurements, the state of a system. If a state is not observable, the controller will never be able to determine the behavior of an unobservable state and hence cannot use it to stabilize the system. However, similar to the stabilizability condition above, if a state cannot be observed it might still be detectable.\n\nFrom a geometrical point of view, looking at the states of each variable of the system to be controlled, every \"bad\" state of these variables must be controllable and observable to ensure a good behavior in the closed-loop system. That is, if one of the eigenvalues of the system is not both controllable and observable, this part of the dynamics will remain untouched in the closed-loop system. If such an eigenvalue is not stable, the dynamics of this eigenvalue will be present in the closed-loop system which therefore will be unstable. Unobservable poles are not present in the transfer function realization of a state-space representation, which is why sometimes the latter is preferred in dynamical systems analysis.\n\nSolutions to problems of an uncontrollable or unobservable system include adding actuators and sensors.\n\nSeveral different control strategies have been devised in the past years. These vary from extremely general ones (PID controller), to others devoted to very particular classes of systems (especially robotics or aircraft cruise control).\n\nA control problem can have several specifications. Stability, of course, is always present. The controller must ensure that the closed-loop system is stable, regardless of the open-loop stability. A poor choice of controller can even worsen the stability of the open-loop system, which must normally be avoided. Sometimes it would be desired to obtain particular dynamics in the closed loop: i.e. that the poles have formula_41, where formula_42 is a fixed value strictly greater than zero, instead of simply asking that formula_43.\n\nAnother typical specification is the rejection of a step disturbance; including an integrator in the open-loop chain (i.e. directly before the system under control) easily achieves this. Other classes of disturbances need different types of sub-systems to be included.\n\nOther \"classical\" control theory specifications regard the time-response of the closed-loop system. These include the rise time (the time needed by the control system to reach the desired value after a perturbation), peak overshoot (the highest value reached by the response before reaching the desired value) and others (settling time, quarter-decay). Frequency domain specifications are usually related to robustness (see after).\n\nModern performance assessments use some variation of integrated tracking error (IAE,ISA,CQI).\n\nA control system must always have some robustness property. A robust controller is such that its properties do not change much if applied to a system slightly different from the mathematical one used for its synthesis. This requirement is important, as no real physical system truly behaves like the series of differential equations used to represent it mathematically. Typically a simpler mathematical model is chosen in order to simplify calculations, otherwise, the true system dynamics can be so complicated that a complete model is impossible.\n\n\nThe process of determining the equations that govern the model's dynamics is called system identification. This can be done off-line: for example, executing a series of measures from which to calculate an approximated mathematical model, typically its transfer function or matrix. Such identification from the output, however, cannot take account of unobservable dynamics. Sometimes the model is built directly starting from known physical equations, for example, in the case of a system we know that formula_44. Even assuming that a \"complete\" model is used in designing the controller, all the parameters included in these equations (called \"nominal parameters\") are never known with absolute precision; the control system will have to behave correctly even when connected to a physical system with true parameter values away from nominal.\n\nSome advanced control techniques include an \"on-line\" identification process (see later). The parameters of the model are calculated (\"identified\") while the controller itself is running. In this way, if a drastic variation of the parameters ensues, for example, if the robot's arm releases a weight, the controller will adjust itself consequently in order to ensure the correct performance.\n\nAnalysis of the robustness of a SISO (single input single output) control system can be performed in the frequency domain, considering the system's transfer function and using Nyquist and Bode diagrams. Topics include gain and phase margin and amplitude margin. For MIMO (multi-input multi output) and, in general, more complicated control systems, one must consider the theoretical results devised for each control technique (see next section). I.e., if particular robustness qualities are needed, the engineer must shift his attention to a control technique by including them in its properties.\n\nA particular robustness issue is the requirement for a control system to perform properly in the presence of input and state constraints. In the physical world every signal is limited. It could happen that a controller will send control signals that cannot be followed by the physical system, for example, trying to rotate a valve at excessive speed. This can produce undesired behavior of the closed-loop system, or even damage or break actuators or other subsystems. Specific control techniques are available to solve the problem: model predictive control (see later), and anti-wind up systems. The latter consists of an additional control block that ensures that the control signal never exceeds a given threshold.\n\nFor MIMO systems, pole placement can be performed mathematically using a state space representation of the open-loop system and calculating a feedback matrix assigning poles in the desired positions. In complicated systems this can require computer-assisted calculation capabilities, and cannot always ensure robustness. Furthermore, all system states are not in general measured and so observers must be included and incorporated in pole placement design.\n\nProcesses in industries like robotics and the aerospace industry typically have strong nonlinear dynamics. In control theory it is sometimes possible to linearize such classes of systems and apply linear techniques, but in many cases it can be necessary to devise from scratch theories permitting control of nonlinear systems. These, e.g., feedback linearization, backstepping, sliding mode control, trajectory linearization control normally take advantage of results based on Lyapunov's theory. Differential geometry has been widely used as a tool for generalizing well-known linear control concepts to the non-linear case, as well as showing the subtleties that make it a more challenging problem. Control theory has also been used to decipher the neural mechanism that directs cognitive states.\n\nWhen the system is controlled by multiple controllers, the problem is one of decentralized control. Decentralization is helpful in many ways, for instance, it helps control systems to operate over a larger geographical area. The agents in decentralized control systems can interact using communication channels and coordinate their actions.\n\nA stochastic control problem is one in which the evolution of the state variables is subjected to random shocks from outside the system. A deterministic control problem is not subject to external random shocks.\n\nEvery control system must guarantee first the stability of the closed-loop behavior. For linear systems, this can be obtained by directly placing the poles. Non-linear control systems use specific theories (normally based on Aleksandr Lyapunov's Theory) to ensure stability without regard to the inner dynamics of the system. The possibility to fulfill different specifications varies from the model considered and the control strategy chosen.\n\n\n\nMany active and historical figures made significant contribution to control theory including\n\n\n\n\n\n\n\n"}
{"id": "47825086", "url": "https://en.wikipedia.org/wiki?curid=47825086", "title": "Dark data", "text": "Dark data\n\nDark data is data which is acquired through various computer network operations but not used in any manner to derive insights or for decision making. The ability of an organisation to collect data can exceed the throughput at which it can analyse the data. In some cases the organisation may not even be aware that the data is being collected. IBM estimate that roughly 90 percent of data generated by sensors and analog-to-digital conversions never get used.\n\nIn an industrial context, dark data can include information gathered by sensors and telematics.\n\nOrganizations retain dark data for a multitude of reasons, and it is estimated that most companies are only analyzing 1% of their data. Often it is stored for regulatory compliance and record keeping. Some organizations believe that dark data could be useful to them in the future, once they have acquired better analytic and business intelligence technology to process the information. Because storage is inexpensive, storing data is easy. However, storing and securing the data usually entails greater expenses (or even risk) than the potential return profit.\n\nA lot of dark data is unstructured, which means that the information is in formats that may be difficult to categorise, be read by the computer and thus analysed. Often the reason that business do not analyse their dark data is because of the amount of resources it would take and the difficulty of having that data analysed. According to Computer Weekly, 60% of organisations believe that their own business intelligence reporting capability is \"inadequate\" and 65% say that they have \"somewhat disorganised content management approaches\".\n\nUseful data may become dark data after it becomes irrelevant, as it is not processed fast enough. This is called \"perishable insights\" in \"live flowing data\". For example, if the geolocation of a customer is known to a business, the business can make offer based on the location, however if this data is not processed immediately, it may be irrelevant in the future. According to IBM, about 60 percent of data loses its value immediately. Not analysing data immediately and letting it go 'dark' can lead to significant losses for an organisation in terms of not identifying fraud, for example, fast enough and then only addressing the issue when it is too late.\n\nAccording to the New York Times, 90% of energy used by data centres is wasted. If data was not stored, energy costs could be saved. Furthermore, there are costs associated with the underutilisation of information and thus missed opportunities. According to Datamation, \"the storage environments of EMEA organizations consist of 54 percent dark data, 32 percent Redundant, Obsolete and Trivial data and 14 percent business-critical data. By 2020, this can add up to $891 billion in storage and management costs that can otherwise be avoided.\"\n\nThe continuous storage of dark data can put an organisation at risk, especially if this data is sensitive. In the case of a breach, this can result in serious repercussions. These can be financial, legal and can seriously hurt an organisation's reputation. For example, a breach of private records of customers could result in the stealing of sensitive information, which could result in identity theft. Another example could be the breach of the company's own sensitive information, for example relating to research and development. These risks can be mitigated by assessing and auditing whether this data is useful to the organisation, employing strong encryption and security and finally, if it is determined to be discarded, then it should be discarded in a way that it becomes unretrievable.\n\nIt is generally considered that as more advanced computing systems for analysis of data are built, the higher the value of dark data will be. It has been noted that \"data and analytics will be the foundation of the modern industrial revolution\". Of course, this includes data that is currently considered \"dark data\" since there are not enough resources to process it. All this data that is being collected can be used in the future to bring maximum productivity and an ability for organisations to meet consumers' demand. Furthermore, many organisations do not realise the value of dark data right now, for example in healthcare and education organisations deal with large amounts of data that could create a significant \"potential to service students and patients in the manner in which the consumer and financial services pursue their target population\".\n"}
{"id": "9730", "url": "https://en.wikipedia.org/wiki?curid=9730", "title": "Electron microscope", "text": "Electron microscope\n\nAn electron microscope is a microscope that uses a beam of accelerated electrons as a source of illumination. As the wavelength of an electron can be up to 100,000 times shorter than that of visible light photons, electron microscopes have a higher resolving power than light microscopes and can reveal the structure of smaller objects. A scanning transmission electron microscope has achieved better than 50 pm resolution in annular dark-field imaging mode and magnifications of up to about 10,000,000x whereas most light microscopes are limited by diffraction to about 200 nm resolution and useful magnifications below 2000x.\n\nElectron microscopes have electron optical lens systems that are analogous to the glass lenses of an optical light microscope.\n\nElectron microscopes are used to investigate the ultrastructure of a wide range of biological and inorganic specimens including microorganisms, cells, large molecules, biopsy samples, metals, and crystals. Industrially, electron microscopes are often used for quality control and failure analysis. Modern electron microscopes produce electron micrographs using specialized digital cameras and frame grabbers to capture the images.\n\nIn 1926 Hans Busch developed the electromagnetic lens.\n\nAccording to Dennis Gabor, the physicist Leó Szilárd tried in 1928 to convince him to build an electron microscope, for which he had filed a patent. The first prototype electron microscope, capable of four-hundred-power magnification, was developed in 1931 by the physicist Ernst Ruska and the electrical engineer Max Knoll. The apparatus was the first practical demonstration of the principles of electron microscopy. In May of the same year, Reinhold Rudenberg, the scientific director of Siemens-Schuckertwerke, obtained a patent for an electron microscope. In 1932, Ernst Lubcke of Siemens & Halske built and obtained images from a prototype electron microscope, applying the concepts described in Rudenberg's patent. \n\nIn the following year, 1933, Ruska built the first electron microscope that exceeded the resolution attainable with an optical (light) microscope. Four years later, in 1937, Siemens financed the work of Ernst Ruska and Bodo von Borries, and employed Helmut Ruska, Ernst's brother, to develop applications for the microscope, especially with biological specimens. Also in 1937, Manfred von Ardenne pioneered the scanning electron microscope. Siemens produced the first commercial electron microscope in 1938. The first North American electron microscope was constructed in 1938, at the University of Toronto, by Eli Franklin Burton and students Cecil Hall, James Hillier, and Albert Prebus. Siemens produced a transmission electron microscope (TEM) in 1939. Although current transmission electron microscopes are capable of two million-power magnification, as scientific instruments, they remain based upon Ruska’s prototype.\n\nThe original form of the electron microscope, the transmission electron microscope (TEM), uses a high voltage electron beam to illuminate the specimen and create an image. The electron beam is produced by an electron gun, commonly fitted with a tungsten filament cathode as the electron source. The electron beam is accelerated by an anode typically at +100 keV (40 to 400 keV) with respect to the cathode, focused by electrostatic and electromagnetic lenses, and transmitted through the specimen that is in part transparent to electrons and in part scatters them out of the beam. When it emerges from the specimen, the electron beam carries information about the structure of the specimen that is magnified by the objective lens system of the microscope. The spatial variation in this information (the \"image\") may be viewed by projecting the magnified electron image onto a fluorescent viewing screen coated with a phosphor or scintillator material such as zinc sulfide. Alternatively, the image can be photographically recorded by exposing a photographic film or plate directly to the electron beam, or a high-resolution phosphor may be coupled by means of a lens optical system or a fibre optic light-guide to the sensor of a digital camera. The image detected by the digital camera may be displayed on a monitor or computer.\n\nThe resolution of TEMs is limited primarily by spherical aberration, but a new generation of hardware correctors can reduce spherical aberration to increase the resolution in high-resolution transmission electron microscopy (HRTEM) to below 0.5 angstrom (50 picometres), enabling magnifications above 50 million times. The ability of HRTEM to determine the positions of atoms within materials is useful for nano-technologies research and development.\n\nTransmission electron microscopes are often used in electron diffraction mode. The advantages of electron diffraction over X-ray crystallography are that the specimen need not be a single crystal or even a polycrystalline powder, and also that the Fourier transform reconstruction of the object's magnified structure occurs physically and thus avoids the need for solving the phase problem faced by the X-ray crystallographers after obtaining their X-ray diffraction patterns of a single crystal or polycrystalline powder.\n\nOne major disadvantage of the transmission electron microscope is the need for extremely thin sections of the specimens, typically about 100 nanometers. Creating these thin sections for biological and materials specimens is technically very challenging. Semiconductor thin sections can be made using a focused ion beam. Biological tissue specimens are chemically fixed, dehydrated and embedded in a polymer resin to stabilize them sufficiently to allow ultrathin sectioning. Sections of biological specimens, organic polymers, and similar materials may require staining with heavy atom labels in order to achieve the required image contrast.\n\nOne application of TEM is serial-section electron microscopy (ssEM), for example in analyzing the connectivity in volumetric samples of brain tissue by imaging many thin sections in sequence.\n\nThe SEM produces images by probing the specimen with a focused electron beam that is scanned across a rectangular area of the specimen (raster scanning). When the electron beam interacts with the specimen, it loses energy by a variety of mechanisms. The lost energy is converted into alternative forms such as heat, emission of low-energy secondary electrons and high-energy backscattered electrons, light emission (cathodoluminescence) or X-ray emission, all of which provide signals carrying information about the properties of the specimen surface, such as its topography and composition. The image displayed by an SEM maps the varying intensity of any of these signals into the image in a position corresponding to the position of the beam on the specimen when the signal was generated. In the SEM image of an ant shown below and to the right, the image was constructed from signals produced by a secondary electron detector, the normal or conventional imaging mode in most SEMs.\n\nGenerally, the image resolution of an SEM is lower than that of a TEM. However, because the SEM images the surface of a sample rather than its interior, the electrons do not have to travel through the sample. This reduces the need for extensive sample preparation to thin the specimen to electron transparency. The SEM is able to image bulk samples that can fit on its stage and still be maneuvered, including a height less than the working distance being used, often 4 millimeters for high-resolution images. The SEM also has a great depth of field, and so can produce images that are good representations of the three-dimensional surface shape of the sample. Another advantage of SEMs comes with environmental scanning electron microscopes (ESEM) that can produce images of good quality and resolution with hydrated samples or in low, rather than high, vacuum or under chamber gases. This facilitates imaging unfixed biological samples that are unstable in the high vacuum of conventional electron microscopes.\n\nIn their most common configurations, electron microscopes produce images with a single brightness value per pixel, with the results usually rendered in grayscale. However, often these images are then colorized through the use of feature-detection software, or simply by hand-editing using a graphics editor. This may be done to clarify structure or for aesthetic effect and generally does not add new information about the specimen.\n\nIn some configurations information about several specimen properties is gathered per pixel, usually by the use of multiple detectors. In SEM, the attributes of topography and material contrast can be obtained by a pair of backscattered electron detectors and such attributes can be superimposed in a single color image by assigning a different primary color to each attribute. Similarly, a combination of backscattered and secondary electron signals can be assigned to different colors and superimposed on a single color micrograph displaying simultaneously the properties of the specimen.\n\nSome types of detectors used in SEM have analytical capabilities, and can provide several items of data at each pixel. Examples are the Energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and Cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors, it is common to color code the signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal, which is not modified in any way.\n\nIn the reflection electron microscope (REM) as in the TEM, an electron beam is incident on a surface but instead of using the transmission (TEM) or secondary electrons (SEM), the reflected beam of elastically scattered electrons is detected. This technique is typically coupled with reflection high energy electron diffraction (RHEED) and \"reflection high-energy loss spectroscopy (RHELS)\". Another variation is spin-polarized low-energy electron microscopy (SPLEEM), which is used for looking at the microstructure of magnetic domains.\n\nThe STEM rasters a focused incident probe across a specimen that (as with the TEM) has been thinned to facilitate detection of electrons scattered \"through\" the specimen. The high resolution of the TEM is thus possible in STEM. The focusing action (and aberrations) occur before the electrons hit the specimen in the STEM, but afterward in the TEM. The STEMs use of SEM-like beam rastering simplifies annular dark-field imaging, and other analytical techniques, but also means that image data is acquired in serial rather than in parallel fashion. Often TEM can be equipped with the scanning option and then it can function both as TEM and STEM.\n\nMaterials to be viewed under an electron microscope may require processing to produce a suitable sample. The technique required varies depending on the specimen and the analysis required:\n\nElectron microscopes are expensive to build and maintain, on the order of other complex machines such as airplanes. Microscopes designed to achieve high resolutions must be housed in stable buildings (sometimes underground) with special services such as magnetic field canceling systems. Operating the electron microscope requires specialized training and continuing practice and education.\n\nThe samples largely have to be viewed in vacuum, as the molecules that make up air would scatter the electrons. An exception is liquid-phase electron microscopy using either a closed liquid cell or an environmental chamber, for example, in the environmental scanning electron microscope, which allows hydrated samples to be viewed in a low-pressure (up to ) wet environment. Various techniques for in situ electron microscopy of gaseous samples have been developed as well.\n\nScanning electron microscopes operating in conventional high-vacuum mode usually image conductive specimens; therefore non-conductive materials require conductive coating (gold/palladium alloy, carbon, osmium, etc.). The low-voltage mode of modern microscopes makes possible the observation of non-conductive specimens without coating. Non-conductive materials can be imaged also by a variable pressure (or environmental) scanning electron microscope.\n\nSmall, stable specimens such as carbon nanotubes, diatom frustules and small mineral crystals (asbestos fibres, for example) require no special treatment before being examined in the electron microscope. Samples of hydrated materials, including almost all biological specimens have to be prepared in various ways to stabilize them, reduce their thickness (ultrathin sectioning) and increase their electron optical contrast (staining). These processes may result in \"artifacts\", but these can usually be identified by comparing the results obtained by using radically different specimen preparation methods. Since the 1980s, analysis of cryofixed, vitrified specimens has also become increasingly used by scientists, further confirming the validity of this technique.\n\nBiology and life sciences\n\n\n\n\n\n"}
{"id": "10180670", "url": "https://en.wikipedia.org/wiki?curid=10180670", "title": "Felix Karl Ludwig Machatschki", "text": "Felix Karl Ludwig Machatschki\n\nKarl Ludwig Felix Machatschki (22 September 1895 – 17 February 1970) was an Austrian mineralogist. \n\nHe was born in Arnfels (near Leibnitz) in Styria, Austria. He studied at the University of Graz, obtaining his habilitation in 1925; in 1927 he joined the group of Victor Goldschmidt in Oslo for one year. In 1930 he was appointed as a professor at the University of Tübingen. He changed university twice, first in 1941 to the University of Munich and finally in 1944 to the University of Vienna. \n\nIn 1928 he published \"Zur Frage der Struktur und Konstitution der Feldspäte\", a paper in which he develops the concept of the atomic structure of silicates and formulates the construction principle of feldspars. In 1946 he published \"Grundlagen der allgemeinen Mineralogie und Kristallchemie\" (\"Fundamentals of general mineralogy and crystal chemistry\").\n\nIn 1961, Machatschki was awarded the Austrian Medal for Science and Art. The \"Felix-Machatschki-Preis\" is an award given by the Österreichische Mineralogische Gesellschaft in recognition of outstanding international scientific work in the field of mineralogy. The mineral machatschkiite commemorates his name.\nHe was also the author of 140 individual articles in scientific journals.\n"}
{"id": "6073980", "url": "https://en.wikipedia.org/wiki?curid=6073980", "title": "Fermentation in food processing", "text": "Fermentation in food processing\n\n'Fermentation in food processing' is the process of converting carbohydrates to alcohol or organic acids using microorganisms—yeasts or bacteria—under anaerobic conditions. Fermentation usually implies that the action of microorganisms is desired. The science of fermentation is known as zymology or zymurgy.\n\nThe term fermentation sometimes refers specifically to the chemical conversion of sugars into ethanol, producing alcoholic drinks such as wine, beer, and cider. However, similar processes take place in the leavening of bread (CO produced by yeast activity), and in the preservation of sour foods with the production of lactic acid, such as in sauerkraut and yogurt.\n\nOther widely consumed fermented foods include vinegar, olives, and cheese. More localised foods prepared by fermentation may also be based on beans, grain, vegetables, fruit, honey, dairy products, fish, meat, or tea.\n\nNatural fermentation precedes human history. Since ancient times, humans have exploited the fermentation process. The earliest archaeological evidence of fermentation was 13,000 year old residues of a beer with the consistency of gruel, in a cave near Haifa in Israel. Another early alcoholic drink, made from fruit, rice, and honey, dates from 7000 to 6600 BC, in the Neolithic Chinese village of Jiahu, and winemaking dates from 6000 BC, in Georgia, in the Caucasus area. Seven-thousand-year-old jars containing the remains of wine, now on display at the University of Pennsylvania, were excavated in the Zagros Mountains in Iran. There is strong evidence that people were fermenting alcoholic drinks in Babylon c. 3000 BC, ancient Egypt c. 3150 BC, pre-Hispanic Mexico c. 2000 BC, and Sudan c. 1500 BC.\n\nThe French chemist Louis Pasteur founded zymology, when in 1856 he connected yeast to fermentation.\nWhen studying the fermentation of sugar to alcohol by yeast, Pasteur concluded that the fermentation was catalyzed by a vital force, called \"ferments\", within the yeast cells. The \"ferments\" were thought to function only within living organisms. \"Alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells\", he wrote.\n\nNevertheless, it was known that yeast extracts can ferment sugar even in the absence of living yeast cells. While studying this process in 1897, the German chemist and zymologist Eduard Buchner of Humboldt University of Berlin, Germany, found that sugar was fermented even when there were no living yeast cells in the mixture, by an enzyme complex secreted by yeast that he termed \"zymase\". In 1907 he received the Nobel Prize in Chemistry for his research and discovery of \"cell-free fermentation\".\n\nOne year earlier, in 1906, ethanol fermentation studies led to the early discovery of NAD.\n\nFood fermentation is the conversion of sugars and other carbohydrates into alcohol or preservative organic acids and carbon dioxide. All three products have found human uses. The production of alcohol is made use of when fruit juices are converted to wine, when grains are made into beer, and when foods rich in starch, such as potatoes, are fermented and then distilled to make spirits such as gin and vodka. The production of carbon dioxide is used to leaven bread. The production of organic acids is exploited to preserve and flavor vegetables and dairy products.\n\nFood fermentation serves five main purposes: to enrich the diet through development of a diversity of flavors, aromas, and textures in food substrates; to preserve substantial amounts of food through lactic acid, alcohol, acetic acid, and alkaline fermentations; to enrich food substrates with protein, essential amino acids, and vitamins; to eliminate antinutrients; and to reduce cooking time and the associated use of fuel.\n\n\nCheonggukjang, doenjang, fermented bean curd, miso, natto, soy sauce, stinky tofu, tempeh, oncom, soybean paste, Beijing mung bean milk, kinama, iru\n\nAmazake, beer, bread, choujiu, gamju, injera, kvass, makgeolli, murri, ogi, rejuvelac, sake, sikhye, sourdough, sowans, rice wine, malt whisky, grain whisky, idli, dosa, vodka, boza\n\nKimchi, mixed pickle, sauerkraut, Indian pickle, gundruk, tursu\n\nWine, vinegar, cider, perry, brandy, atchara, nata de coco, burong mangga, asinan, pickling, vişinată, chocolate, rakı\n\nMead, metheglin\n\nSome kinds of cheese also, kefir, kumis (mare milk), shubat (camel milk), cultured milk products such as quark, filmjölk, crème fraîche, smetana, skyr, and yogurt\n\nBagoong, faseekh, fish sauce, Garum, Hákarl, jeotgal, rakfisk, shrimp paste, surströmming, shidal\n\nChorizo, salami, sucuk, pepperoni, nem chua, som moo, saucisson\n\nPu-erh tea, Kombucha\n\nAlaska has witnessed a steady increase of cases of botulism since 1985. It has more cases of botulism than any other state in the United States of America. This is caused by the traditional Eskimo practice of allowing animal products such as whole fish, fish heads, walrus, sea lion, and whale flippers, beaver tails, seal oil, and birds, to ferment for an extended period of time before being consumed. The risk is exacerbated when a plastic container is used for this purpose instead of the old-fashioned, traditional method, a grass-lined hole, as the \"Clostridium botulinum\" bacteria thrive in the anaerobic conditions created by the air-tight enclosure in plastic.\n\nThe World Health Organization has classified pickled foods as possibly carcinogenic, based on epidemiological studies. Other research found that fermented food contains a carcinogenic by-product, ethyl carbamate (urethane). \"A 2009 review of the existing studies conducted across Asia concluded that regularly eating pickled vegetables roughly doubles a person's risk for esophageal squamous cell carcinoma.\"\n\n"}
{"id": "1028755", "url": "https://en.wikipedia.org/wiki?curid=1028755", "title": "Fuzzy associative matrix", "text": "Fuzzy associative matrix\n\nA fuzzy associative matrix expresses fuzzy logic rules in tabular form. These rules usually take two variables as input, mapping cleanly to a two-dimensional matrix, although theoretically a matrix of any number of dimensions is possible.\n\nSuppose a professional is tasked with writing fuzzy logic rules for a video game monster. In the game being built, entities have two variables: hit points (HP) and firepower (FP):\n\nThis translates to:\n\nMultiple rules can fire at once, and often will, because the distinction between \"very low\" and \"low\" is fuzzy. If it is more \"very low\" than it is low, then the \"very low\" rule will generate a stronger response. The program will evaluate all the rules that fire and use an appropriate defuzzification method to generate its actual response.\n\nAn implementation of this system might use either the matrix or the explicit IF/THEN form. The matrix makes it easy to visualize the system, but it also makes it impossible to add a third variable just for one rule, so it is less flexible.\n\nThere is no inherent pattern in the matrix. It appears as if the rules were just made up, and indeed they were. This is both a strength and a weakness of fuzzy logic in general. It is often impractical or impossible to find an exact set of rules or formulae for dealing with a specific situation. For a sufficiently complex game, a mathematician would not be able to study the system and figure out a mathematically accurate set of rules. However, this weakness is intrinsic to the realities of the situation, not of fuzzy logic itself. The strength of the system is that even if one of the rules is wrong, even greatly wrong, other rules that are correct are likely to fire as well and they may compensate for the error.\n\nThis does not mean a fuzzy system should be sloppy. Depending on the system, it might get away with being sloppy, but it will underperform. While the rules are fairly arbitrary, they should be chosen carefully. If possible, an expert should decide on the rules, and the sets and rules should be tested vigorously and refined as needed. In this way, a fuzzy system is like an expert system. (Fuzzy logic is used in many true expert systems, as well.)\n"}
{"id": "19181256", "url": "https://en.wikipedia.org/wiki?curid=19181256", "title": "Gerber method", "text": "Gerber method\n\nThe Gerber Method is a primary and historic chemical test to determine the fat content of substances, most commonly milk and cream. The Gerber Method is the primary testing method in Europe and much of the world. The fairly similar Babcock test is used primarily in the United States, although the Gerber Method also enjoys significant use in the U.S. as well.\n\nThe Gerber Method was developed and patented by Dr. Niklaus Gerber of Switzerland in 1891.\n\nMilk fat is separated from proteins by adding sulfuric acid. The separation is facilitated by using amyl alcohol and centrifugation. The fat content is read directly via a special calibrated butyrometer. Gerber developed specialized butyrometers (tubes), pipettes, and centrifuges. Water baths built specifically for the Gerber tubes are often used.\n\nThe test is still in widespread use today and is the basis for numerous national and international standards such as ISO 2446, International Dairy Federation (FIL) Regulation 105, BS 696 (United Kingdom), and IS 1223 (India). Larger facilities may prefer to use faster analysis techniques such as infrared spectroscopy as these greatly reduce the potential for user error and reduce the time and COSHH requirements.\n\nThe test continues to be improved and standardized.\n\nThe two major defects associated with the Gerber method include:\n\n"}
{"id": "34557168", "url": "https://en.wikipedia.org/wiki?curid=34557168", "title": "Gladstone Professor of Government", "text": "Gladstone Professor of Government\n\nThe Gladstone Professorship of Government is located at All Souls College at the University of Oxford. It was instituted in memory of William Ewart Gladstone. The professorship has never been held by a woman. Its past holders have been:\n\n"}
{"id": "22761309", "url": "https://en.wikipedia.org/wiki?curid=22761309", "title": "Gliding motility", "text": "Gliding motility\n\nGliding motility is a type of flagella-independent translocation that allows the microorganism to glide smoothly along a surface without the aid of propulsive organelles on the outer membrane. Gliding motility and twitching motility both allow microorganisms to travel along the surface of low aqueous films, but while gliding motility is smooth, twitching motility is jerky and uses the pili as its mechanism of transport. Motor proteins found within the inner membrane of the bacteria utilize a proton-conducting channel to transduce a mechanical force to the cell surface. The movement of the cytoskeletal filaments causes a mechanical force which travels to the adhesion complexes on the substrate to move the cell forward. Motor and regulatory proteins that convert intracellular motion into mechanical forces like traction force have been discovered to be a conserved class of intracellular motors in bacteria that have been adapted to produce cell motility. The speed of the gliding varies from organism to organism and the reversal of direction is seemingly regulated by an internal clock of some sort. Gliding motility known as bacterial gliding in bacteria, is not unique to bacteria since it can also be seen being utilized by the Apicomplexa, a Eukaryota parasite, travelling at fast rates between 1–10 μm a second when the \"Myxococcus xanthus\" glide at a rate of 5 μm a minute. Cell invasion and gliding motility have TRAP(thrombospondin-related anonymous protein), a surface protein, as a common molecular basis that is both essential for infection and locomotion of the invasive Apicomplexa parasite.\n\n"}
{"id": "30493028", "url": "https://en.wikipedia.org/wiki?curid=30493028", "title": "Harald Bjørlykke", "text": "Harald Bjørlykke\n\nHarald Bjørlykke (14 September 1901 – 28 February 1968) was a Norwegian geologist.\n\nHe was born in Ås, a son of Knut Olai Bjørlykke. He was the father of Arne Bjørlykke.\n\nHe took the dr.philos. degree in 1935, and mainly worked within mineralogy and the geology of ore. He was the director of Norsk Bergverk from 1958, director of the Norwegian Geological Survey from 1958 to 1966, and professor of geology at the Norwegian Institute of Technology from 1966 to his death.\n"}
{"id": "38448495", "url": "https://en.wikipedia.org/wiki?curid=38448495", "title": "Index of physics articles (X)", "text": "Index of physics articles (X)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "24658682", "url": "https://en.wikipedia.org/wiki?curid=24658682", "title": "International Society for Knowledge Organization", "text": "International Society for Knowledge Organization\n\nThe International Society for Knowledge Organization, or ISKO, is the principal professional association for scholars of knowledge organization, knowledge structures, classification studies, and information organization and structure. Founded in 1989, ISKO's mission is \"to advance conceptual work in knowledge organization in all kinds of forms, and for all kinds of purposes, such as databases, libraries, dictionaries and the Internet.\" An interdisciplinary association, ISKO's worldwide membership draws from fields such as information science, philosophy, linguistics, library science, archive studies, science studies, and computer science. ISKO \"promotes research, development and applications of knowledge organization systems that advance the philosophical, psychological and semantic approaches for ordering knowledge; provides the means of communication and networking on knowledge organization for its members; and functions as a connecting link between all institutions and national societies, working with problems related to the conceptual organization and processing of knowledge.\" \n\nThe Society publishes the academic journal \"Knowledge Organization\" as the official bi-monthly journal of ISKO. Founded in 1973 by Dr. Ingetraut Dahlberg, the first President of ISKO, it began publication the following year under the banner International Classification. In 1993 the title was changed to its present form. The journal publishes original research articles relating to general ordering theory, philosophical foundations of knowledge and its artifacts, theoretical bases of classification, data analysis and reduction. It also describes practical operations associated with indexing and classification. In addition to being a technical resource, the journal traces the history of knowledge organization and discusses questions of education and training in classification.\n\nThe organization also holds biennial international conferences. \n\nISKO officially recognizes national chapters in Brazil, Canada, China, France, Germany, India, Italy, Poland, Spain, the United Kingdom, and the United States. ISKO cooperates with international and national organizations such as UNESCO, the European Commission, the International Organization for Standardization, the International Federation of Library Associations and Institutions, the Association for Information Science and Technology, the Networked Knowledge Organization Systems/Services, and the International Information Centre for Terminology.\n\nDahlberg, Ingetraut (2010) International Society for Knowledge Organization (ISKO), \"Encyclopedia of Library and Information Sciences, Third Edition\", 1: 1, 2941 — 2949.\n\n"}
{"id": "29925661", "url": "https://en.wikipedia.org/wiki?curid=29925661", "title": "Isolation index", "text": "Isolation index\n\nAn isolation index is a measure of the segregation of the activities of multiple populations. They have been used in studies of racial segregation and ideological segregation.\n\nExamples of isolation indices include Lieberson's isolation index and Bell's isolation index.\n\n"}
{"id": "18864768", "url": "https://en.wikipedia.org/wiki?curid=18864768", "title": "JASON Project", "text": "JASON Project\n\nThe JASON Project is a US middle school science curriculum program that is designed to motivate and inspire students to pursue interests and careers in science, technology, engineering and mathematics.\n\nThe JASON Project's approach to science education immerses students in real-world situations where they are mentored by scientists from organizations like NASA, NOAA, the U.S. Department of Energy, and parent company National Geographic Society. JASON creates these connections using educational games, videos, live interactivity and social networking to embed its partners' research in the curriculum.\n\nThe JASON Project was started in 1989 by Dr. Robert Ballard, the oceanographer who discovered the wreck of the RMS \"Titanic\". The JASON Foundation for Education was founded in 1990 as a 501(c)(3) non-profit organization to administer the project. The Foundation became a subsidiary of the National Geographic Society in 2005.\n\nThe project won a scientific public engagement award from the American Association for the Advancement of Science, and \"Computerworld's\" Smithsonian Award for its use of technology. The JASON curricula are available in print and free online, aligned to national and state standards. The JASON Mission Center contains all student and teacher content, communications systems, digital experiences, and tools to manage, assess and track student performance and online usage. \n\nJASON curricula are available free online, free print-on-demand and in print editions for purchase.\n"}
{"id": "47596400", "url": "https://en.wikipedia.org/wiki?curid=47596400", "title": "Janet Abbate", "text": "Janet Abbate\n\nJanet Abbate is an associate professor of science, technology, and society at Virginia Tech. Her research focuses on the history of computer science and the Internet, particularly on the participation of women.\n\nAbbate received her Ph.D. from the University of Pennsylvania in 1994. From 1996 to 1998, she was a postdoctoral fellow with the IEEE History Center, where she conducted research on women in computing. She joined the faculty of Virginia Tech's Northern Capital Region campus in 2004 and is now associate professor and co-director of the graduate program in Science, Technology, and Society.\n\nPrior to her academic work, Abbate was herself a computer programmer, and her background has been cited in reviews of her work as relevant to her research approach.\n\nAbbate is the author of two books: \"Inventing the Internet\" (2000) and \"Recoding Gender: Women’s Changing Participation in Computing\" (2012). \"Inventing the Internet\" was widely reviewed as an important work in the history of computing and networking, particularly in highlighting the role of social dynamics and of non-American participation in early networking development, although others pointed to Abbate's background in computing as causing difficulty in presenting a non-technical narrative. \"Recoding Gender\" also received positive reviews, especially for its incorporation of interviews with women in the field. The book received the 2014 Computer History Museum prize.\n"}
{"id": "2894219", "url": "https://en.wikipedia.org/wiki?curid=2894219", "title": "Kat-5 (vehicle)", "text": "Kat-5 (vehicle)\n\nKat-5 is an autonomous vehicle created by Team Gray, an organization comprising employees from The Gray Insurance Company and students from Tulane University's School of Engineering, for the 2005 DARPA Grand Challenge. \n\nKat-5 is a 2005 Ford Escape Hybrid modified with the sensors and actuators needed for autonomous operation. It has an INS/GPS (Inertial Navigation System/Global Positioning System) from Oxford Technical Solutions and LIDAR units from SICK and Riegl. Kat-5 finished the 2005 DARPA Grand Challenge and placed fourth with a time of 7 hours, 30 minutes, only 37 minutes behind Stanley, the winner of the competition. Kat-5 is powered by almost 100% pure Java and runs both the Mac OS X and Linux operating systems.\n\nKat-5 uses oscillating LIDARs and information from the INS/GPS unit to create a picture of the surrounding environment. This information is then used to build a path for the vehicle to follow. Kat-5's primary electrical system, used to run the computers and drive-by-wire system, is powered by the standard electrical system of the vehicle while the 24-volt system, used to power the LIDAR sensors, is powered by six solar panels on the roof platform of the vehicle.\n\nDuring the National Qualification Event of the Grand Challenge, Kat-5 suffered 2 major crashes and one stall (due to a faulty circuit breaker) before finally completing three runs. The last run completed was done flawlessly with Kat-5 avoiding every obstacle and passing through all 50 required gates. The vehicle was given a pole position of 16 going into the Grand Challenge Event in Primm, NV.\n\nOn the morning of October 8, 2005, Kat-5 left the starting line without a hitch and began the 132-mile journey through the surrounding desert. Although given the opportunity to stop at sunset and continue the race the following morning, the team chose to allow their vehicle to finish that night since darkness would have no effect on the vehicle's performance.\n\nAfter crossing the finish line, it was found that Kat-5 had completed the entire race using only 7 gallons of fuel, averaging a little less than 19 miles per gallon. The vehicle averaged 17.5 miles per hour throughout the course of the race. It was later discovered that a small software bug had caused the vehicle to slow to a crawl in the wider segments of the race.\n\nTeam Gray was made up of IT professionals from within the company and students from Tulane University’s Computer Science, Biomedical Engineering, and Mechanical Engineering departments. Eric Gray, President and Director of The Gray Exploration Company and The Gray Oil & Gas Company, was the leader of the team. The primary development team consisted of six people:\n\n\nThe team developed Kat-5 in a period of six actual months (three weeks were lost to Hurricane Katrina) at a total cost of approximately $650,000. The physical value of the vehicle when it ran on October 8 was estimated to be $150,000.\n\n"}
{"id": "51876625", "url": "https://en.wikipedia.org/wiki?curid=51876625", "title": "Let Them Eat Precaution", "text": "Let Them Eat Precaution\n\nLet Them Eat Precaution: How Politics is Undermining the Genetic Revolution in Agriculture is a 2005 book edited by American author and journalist Jon Entine, about how politics is affecting the use of genetically modified food. The 10 contributing authors from the United States and the United Kingdom provide insights into the benefits of agricultural biotechnology, offer an international perspective on why some groups are opposed to it, and suggest potential solutions to the controversy.\n\nResistance to GMO food production and use comes from the European Union. The EU evokes the Precautionary Principle in much of their opposition to GMO's.\"Communication from the Commission on the Precautionary Principle\", noted that, \"The precautionary principle is not defined in the Treaty, which prescribes it [the Precautionary Principle] only once – to protect the environment. But in practice, its scope is much wider, and specifically where preliminary-objective-scientific-evaluation indicates that there are reasonable grounds for concern that potentially dangerous effects on the environment, human, animal or [and] plant health may be inconsistent with the high level of protection [for what] chosen for the Community.\". There is also opposition from environmental groups such as Greenpeace, Friends of the Earth, organic advocates and Christian Aid. In this book much of the arguments in favor of GMO's center around the advantages they provide and the general scientific consensus of the safety of GMO foods.\n\nThis book grew out of a 2003 conference, \"Food Biotechnology, the Media, and Public Policy\", held at the American Enterprise Institute. Jon Entine, the editor, is an adjunct fellow at the institute and a scholar in residence at Miami University in Ohio.\n\n\"Let Them Eat Precaution does a superb job of educating the reading public on the basic issues of genetically modified foods. The distinguished authors provide a devastating point-by-point refutation of the anti-GMO activists' false claims, providing a reasoned, scientifically grounded perspective on this critical issue. As the Marie Antoinette title implies, though the affluent may be leading the charge against GMO foods, it is the poor who are most likely to suffer the effects of activists that falsely claim to speak for the world's poor.\" —Thomas DeGregori, professor of economics, University of Houston, and author of Origins of the Organic Agriculture Debate.\n\n\"A well-funded global antibiotech activist campaign, abetted by European Union regulators more interested in political pandering than good science, threatens to starve millions of the world's poorest people by denying them access to environmentally safer and higher yielding biotech crops. The distinguished experts assembled in Let Them Eat Precaution make it abundantly clear that humanity's health and well-being depend on innovation, not a technological freeze in the name of the \"precautionary principle,\" which demands perfect safety from all new technologies. The contributors carefully document not only the policy challenges facing agricultural biotechnology but the real benefits—from a massive reduction in pesticide use to a slew of new pharmaceuticals and vitamin-enriched foods—that may never come to fruition if antiscience advocacy groups prevail in this battle of ideas.\" —Ronald Bailey, author of Liberation Biology: The Scientific and Moral Case for the Biotech Revolution and science correspondent for Reason magazine\n\n\"This fine volume fills a very useful role in the ongoing debate over the use of biotechnology in foods and pharmaceuticals. Let Them Eat Precaution covers every aspect of the issue, catalogs what is known about GM crops, and helps us understand the ideological basis for opposition to the use of this life-saving technology. The antibiotechnology campaigns are denying food to starving millions—a high price to pay for ideology. \" —Peter Raven, director of the Missouri Botanical Garden, St. Louis, Mo.\n\n\"The book would be appropriate for a college-level course in science communication or in agricultural or science policy. Scientists involved in molecular biology and related research might find the book helps them better understand how something that they may think is a safe and exciting scientific discovery is not readily accepted by others in society.\" - Martin Marshall Senior Associate Director ARP and Assistant Dean, Professor Agricultural Economics, Purdue University.\n\n"}
{"id": "8330095", "url": "https://en.wikipedia.org/wiki?curid=8330095", "title": "List of Bohol provincial symbols", "text": "List of Bohol provincial symbols\n\nThe following is a list of symbols of Bohol province, Philippines.\n\n"}
{"id": "11785930", "url": "https://en.wikipedia.org/wiki?curid=11785930", "title": "List of North Dakota state symbols", "text": "List of North Dakota state symbols\n\nThe following is a list of officially designated symbols of the U.S. state of North Dakota.\n\nUnofficial symbols of North Dakota\n\n"}
{"id": "346362", "url": "https://en.wikipedia.org/wiki?curid=346362", "title": "List of U.S. state fossils", "text": "List of U.S. state fossils\n\nMost American states have made a state fossil designation, in many cases during the 1980s. It is common to designate one species in which fossilization has occurred, rather than a single specimen, or a category of fossils not limited to a single species.\n\nSome states that lack an explicit state fossil have nevertheless singled out a fossil for formal designation as a state dinosaur, rock, gem or stone.\n\n"}
{"id": "80155", "url": "https://en.wikipedia.org/wiki?curid=80155", "title": "List of astronomers", "text": "List of astronomers\n\nThe following are list of astronomers, astrophysicists and other notable people who have made contributions to the field of astronomy. They may have won major prizes or awards, developed or invented widely used techniques or technologies within astronomy, or are directors of major observatories or heads of space-based telescope projects.\n\nThe following is a list of notable astronomers.\n\nIn alphabetical order:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is a list of people who are not astronomers but made a contribution to the field of astronomy and astrophysics.\n\n\n\n"}
{"id": "50777971", "url": "https://en.wikipedia.org/wiki?curid=50777971", "title": "List of colors by shade", "text": "List of colors by shade\n\nThis is a list of colors and a brief description of them.\n\nBlack is the darkest color, the result of the absence or complete absorption of light. Like white and grey, it is an achromatic color, literally a color without hue.\n\nBlue is a color, the perception of which is evoked by light having a spectrum dominated by energy with a wavelength of roughly 440–490 nm. It is considered one of the additive primary colors.\n\nBrown colors are dark or muted shades of reds, oranges, and yellows on the RGB and CMYK color schemes. In practice, browns are created by mixing two complementary colors from the RYB color scheme (combining all three primary colors). In theory, such combinations should produce black, but produce brown because most commercially available blue pigments tend to be comparatively weaker; the stronger red and yellow colors prevail, thus creating the following tones. Brown can also be made if multiple paint colours are added to each other.\n\nCyan is any of the colors in the blue-green range of the visible spectrum, i.e., between approximately 490 and 520 nm. It is considered one of the mainsubtractive primary colors. Cyan is sometimes considered green or blue because of the way we see it and is not found in the rainbow even though blue and green are together.\n\nGreen is a color, the perception of which is evoked by light having a spectrum dominated by energy with a wavelength of roughly 520–570 nm. It is considered one of the additive primary colors.\n\nAchromatic grays are colors between black and white with no hue. Chromatic grays are achromatic grays mixed with \"warm hues\" such as yellow (warm grays) \"or cool hues\" such as azure \"(\"cool grays\"). This gray color template includes both achromatic and chromatic grays.\n\nMagenta is variously defined as a purplish-red, reddish-purple, or a mauvish–crimson color. On color wheels of the RGB and CMY color models, it is located midway between red and blue, opposite green. Complements of magenta are evoked by light having a spectrum dominated by energy with a wavelength of roughly 500–530 nm. It is considered one of the subtractive primary colors.\n\nOrange is the color in the visible spectrum between red and yellow with a wavelength around 585 – 620 nm. In the HSV color space, it has a hue of around 30°.\n\nPink is any of a number of similar colors evoked by light, consisting predominantly of a combination of both the longest, and shortest wavelengths discernible by the human eye, in the wavelength ranges of roughly 625–750 nm and 380-490 nm.\n\nViolet refers to any colour perceptually evoked by light with a predominant wavelength of roughly 380–450 nm. Tones of violet tending towards the blue are called indigo. Purple colors are colors that are various blends of violet or blue light with red light.\n\nRed is any of a number of similar colors evoked by light, consisting predominantly of the longest wavelengths discernible by the human eye, in the wavelength range of roughly 625–750 nm. It is considered one of the additive primary colors.\n\nWhite is a balanced combination of all the colors of the visible light spectrum, or of a pair of complementary colors, or of three or more colors, such as additive primary colors. It is a neutral or achromatic (\"without color\") color, like black and gray.\n\nYellow is the color of light with wavelengths predominately in the range of roughly 570–580 nm. In the HSV color space, it has a hue of around 60°. It is considered one of the subtractive primary colors.\n"}
{"id": "13954426", "url": "https://en.wikipedia.org/wiki?curid=13954426", "title": "List of kidney stone formers", "text": "List of kidney stone formers\n\nThere are a number of documented cases of historical figures and distinguished members of society who were kidney stone formers. This condition is caused by nephrolithiasis, which are more commonly known as kidney stones, or urolithiasis, where the stone forms in the urinary system. These are crystal deposits that can accrete in the urinary system when certain chemical substances become concentrated in the urine. Among the symptoms associated with nephrolithiasis are intense colicky pain, nausea, fever, chills, and the reduction or blockage of urine flow. Historically, the condition of having a kidney or bladder stone was referred to as \"the stone\" or \"the gravel\".\n\nIn certain cases, kidney stone formation played a pivotal role in history. Most notably, some members of the royalty and military leaders became debilitated at important moments, such as Napoleon III of France during the Franco-Prussian War of 1870 and Athenian commander Nicias in the disastrous Sicilian Expedition of 415-413 BC. Despite this condition, artists such as Arthur Sullivan and Michel de Montaigne managed to produce historically distinguished works; providing an example of perseverance in the face of severe and chronic pain. The medical advances of the twentieth century have allowed patients to survive the condition, whereas in the past it may have proven debilitating or fatal (as shown by the examples below).\n\nKidney stones can reach exceptional size. In December 2003, a kidney stone weighing 356 g (12.5 oz) was removed from the right kidney of Peter Baulman of Australia. At its widest point, the stone measured 11.86 cm (4.66 in). In 2017, a 2 kg (4.4 lb) stone spanning 20 cm was surgically removed from Abdul Abu Al Hajjar in Kensington, England. As of August 2006, the most kidney stones ever passed naturally was 5,704 by Canadian Donald Winfield. The largest number removed through surgery was 728, during a three-hour operation upon Mangilal Jain of India, on January 27, 2004.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFictional incidents of kidney stones have been portrayed in the media on several occasions.\n\n\n\n\n\n"}
{"id": "5365746", "url": "https://en.wikipedia.org/wiki?curid=5365746", "title": "Marc Hauser", "text": "Marc Hauser\n\nMarc D. Hauser (born October 25, 1959) is an American evolutionary biologist and a researcher in primate behavior, animal cognition and human behavior. Hauser was a Harvard University professor from 1998 to 2011, regarded as \"a leader in the field of animal and human cognition\".\n\nIn 2010, Harvard found him guilty of scientific misconduct, and a year later he resigned. Because his research was financed by government grants, the Office of Research Integrity of the Health and Human Services Department also investigated, finding in 2012 that Hauser had fabricated data, manipulated experimental results, and published falsified findings.\n\nHauser's research lies at the intersection of evolutionary biology and cognitive neuroscience. It was aimed at understanding the processes and consequences of cognitive evolution. His observations and experiments focused on nonhuman animals and humans of different ages and mental competence, incorporating methodology and theory from ethology, infant cognitive development, evolutionary theory, cognitive neuroscience and neurobiology. Research interests included: studies of language evolution, the nature of moral judgments, the development and evolution of mathematical representations, comparative studies of economic-like choice, the precursors to musical competence, and the nature of event perception.\n\nHis most widely known work involved long-term studies of monkeys and other primates. His lab at Harvard was called the \"monkey lab\". \nAnother of his research projects was the internet-based 'The Moral Sense Test' in which the participant is presented with a series of hypothetical moral dilemmas and is asked to offer a judgment regarding each one.\n\n\n\n\nIn 2007, Harvard University announced an internal investigation of alleged scientific misconduct by Hauser. \nIn August 2010, the investigators found him solely responsible for eight counts of misconduct, and he took a year's leave of absence.\nIn July 2011, Hauser resigned his faculty position at Harvard, effective August 1, 2011.\n\nIn his resignation, Hauser stated that he had \"some exciting opportunities in the private sector\" involving education for high-risk teenagers, but that he might go back to academia \"in the years to come.\" As of May 2013, Hauser's LinkedIn profile listed him as a co-founder of the website Gamience, claiming \"Scientifically-based games that address global health problems of self-control.\"\n\nIn September 2012, after conducting a separate investigation, the Office of Research Integrity (ORI) found Hauser guilty of scientific misconduct. \nThey concluded that Hauser had fabricated data in one study, manipulated results in multiple experiments, and incorrectly described how studies were conducted. The ORI barred Hauser from certain types of research and required that other research be conducted under supervision.\nThey published a notice stating:\nNotice is hereby given that the Office of Research Integrity (ORI) has taken final action in the following case: \n\"Marc Hauser, Ph.D., Harvard University\": \nORI found that Dr. Marc Hauser … engaged in research misconduct in research supported by\n\nThe details of this investigation were not publicly released, and the lack of transparency evoked substantial speculation. Writing in the \"New York Times\" in August 2010, Nicholas Wade summarized:\n\nOn August 20, 2010, Michael Smith, Dean of Harvard's Faculty of Arts and Sciences, released a statement confirming that an internal investigation had found Hauser guilty of eight counts of scientific misconduct. Three counts involved published papers, and five involved unpublished studies. The statement said that Harvard was cooperating with further investigations by the US Office of Research Integrity, the National Science Foundation Office of Inspector General, and the U.S. Attorney for the District of Massachusetts. They stated that they would conduct their own review and make their conclusions available to the public.\n\nA 2002 paper published in the journal \"Cognition\" was retracted. \nIn this paper, Hauser and his collaborators concluded that cotton-top tamarin monkeys could learn simple rule-like patterns.\n\nIn two additional published papers, some field notes or video recordings were \"incomplete\", although Hauser and his co-author replicated the experiments. The \"Proceedings of the Royal Society\" published the replication of the missing data in an addendum to one of the papers. \nIn April 2011 Hauser and Justin Wood (coauthor of the original paper) replicated the results of the 2007 \"Science\" study and published them—as an addendum—in the journal.\n\nCharles Gross published an article in The Nation that detailed the Hauser case (Disgrace: On Marc Hauser. A case of scientific misconduct at Harvard).\n\nIn August 2010, after the initial allegations came out, various publications published other accusations and speculations about Hauser's research, often citing reports by his former students and research assistants.\n\nMichael Tomasello, another well-known animal cognition researcher, claimed that some of Hauser's previous students personally told him that there \"was a pattern and they had specific evidence\". Tomasello also stated, prior to the official announcement, that he had information from \"a Harvard faculty member and from former students of Dr. Hauser\" that the investigation found evidence for eight counts of scientific misconduct; this statement was later confirmed by Harvard's dean (see previous section).\n\n\"The Chronicle of Higher Education\" reported the contents of allegations made by a former research assistant of Hauser. The former research assistant stated that Hauser falsely coded videotapes of monkey behavior, resisted research assistants and students' requests to have them re-coded by another observer and pressured his students to accept his data analysis. When they re-coded the data without Hauser's permission, they allegedly found Hauser's coding bore little relation to what was on the tapes. According to the document, several other lab members had similar run-ins with Hauser.\n\nAn article in \"New Scientist\" claimed that Harvard opened its investigation of Hauser's lab after students who had worked there made allegations of data falsification.\n\nGerry Altmann, the editor of \"Cognition\", subsequently posted his personal conclusion that Hauser fabricated data as part of a deception, after being given a summary of the relevant portions of Harvard's inquiry. \nAltmann noted that the conclusion of fabrication was his own conjecture, and not that of the Harvard investigation, which offered no explanation for discrepancies between the video record and the published paper.\n\nWhile Harvard confirmed that misconduct was committed by Hauser, scientists continued to criticize Harvard over a lack of transparency in the investigation. However, Harvard stated that \"in cases where the government concludes scientific misconduct occurred, the federal agency makes those findings publicly available.\"\n\nAlthough Hauser took a year-long leave of absence from Harvard in 2010, he was at first still planning to teach at the Harvard Extension School, which generated further controversy. On September 1, 2010, his classes at the Extension School were canceled. \nIn April 2011, he was barred from teaching in the Psychology department or any other Arts and Sciences department.\n\nIn 1995, Hauser reported that cotton-top tamarins can recognize themselves in a mirror. Gordon G. Gallup questioned Hauser's findings, and reviewed some video recordings of Hauser's experiment, saying that \"when I played the videotapes [for Hauser's experiments], there was not a thread of compelling evidence — scientific or otherwise — that any of the tamarins had learned to correctly decipher mirrored information about themselves.\" \nUpon requesting the remaining videotapes, Gallup was informed that the other tapes had been stolen. Together with Anderson, Gallup published a critical response to Hauser's article. \nTheir criticism of Hauser's paper stated that the coding criteria were described in insufficient detail to code the monkeys' behavior and that, according to their assessment, the cotton-top tamarins did not show the behavior that they considered as evidence for mirror recognition in chimpanzees or other great apes.\n\nHauser and a co-author published a reply to these criticisms, clarifying their coding criteria. \nHowever, in 2001 Hauser reported that his subsequent attempts to replicate the experiments were unsuccessful, observing no evidence for the previously claimed result.\n\n\n\n"}
{"id": "6436309", "url": "https://en.wikipedia.org/wiki?curid=6436309", "title": "Master of Physics", "text": "Master of Physics\n\nA Master of Physics (or MPhys (Hons) ) honours degree is a specific master's degree for courses in the field of physics.\n\nIn England and Wales, the MPhys is an undergraduate award available after pursuing a four-year course of study at a university. In Scotland the course has a five-year duration. In some universities, the degree has the variant abbreviation MSci. These are taught courses, with a research element in the final year — this can vary from a small component to an entire year working with a research group — and are not available as postgraduate qualifications in most cases, although depending on institution the final year can be considered as approximately equivalent to an MSc.\n\nIn terms of course structure, MPhys degrees usually follow the pattern familiar from bachelor's degrees with lectures, laboratory work, coursework and exams each year. Usually one, or more commonly two, substantial projects are to be completed in the fourth year which may well have research elements. At the end of the second or third years, there is usually a threshold of academic performance in examinations to be reached to allow progression into the final year. Final results are, in most cases, awarded on the standard British undergraduate degree classification scale, although some universities award something structurally similar to 'Distinction', 'Merit', 'Pass' or 'Fail', as this is often the way that taught postgraduate master's degrees are classified.\n\nIt is usual for there to be some variation in the MPhys schemes, to allow for students to study the area of physics which most interests them. For example, Lancaster University's physics department offer the following schemes:\n\nThese schemes will usually incorporate the same core modules with additional scheme specific modules. Students tend to take all the same core modules during their first year and start to specialise in their second year. In some cases, optional modules can be taken from other schemes.\n\n"}
{"id": "1799997", "url": "https://en.wikipedia.org/wiki?curid=1799997", "title": "Mutualism (economic theory)", "text": "Mutualism (economic theory)\n\nMutualism is an economic theory and anarchist school of thought that advocates a society with free markets and \"occupation and use\", or \"usufruct\" property norms. One implementation of this scheme involves the establishment of a mutual-credit bank that would lend to producers at a minimal interest rate, just high enough to cover administration. Mutualism is based on a version of the labor theory of value holding that when labor or its product is sold, in exchange it ought to receive goods or services embodying \"the amount of labor necessary to produce an article of exactly similar and equal utility\". Mutualism originated from the writings of philosopher Pierre-Joseph Proudhon.\n\nMutualists disagree with the idea of individuals receiving an income through loans, investments and rent as they believe these individuals are not laboring. Though Proudhon opposed this type of income, he expressed that he had never intended \"to forbid or suppress, by sovereign decree, ground rent and interest on capital. I think that all these manifestations of human activity should remain free and voluntary for all: I ask for them no modifications, restrictions or suppressions, other than those which result naturally and of necessity from the universalization of the principle of reciprocity which I propose\". Insofar as they ensure the worker's right to the full product of their labor, mutualists support markets and property in the product of labor. However, they argue for conditional titles to land, whose ownership is legitimate only so long as it remains in use or occupation (which Proudhon called \"possession\")—thus advocating personal property, but not private property.\n\nAlthough mutualism is similar to the economic doctrines of the 19th-century American individualist anarchists, unlike them mutualism is in favor of large industries. Therefore, mutualism has been retrospectively characterized sometimes as being a form of individualist anarchism and as ideologically situated between individualist and collectivist forms of anarchism as well. Proudhon himself described the \"liberty\" he pursued as \"the synthesis of communism and property\". Some consider Proudhon to be an individualist anarchist while others regard him to be a social anarchist.\n\nMutualists have distinguished mutualism from state socialism and do not advocate state control over the means of production. Benjamin Tucker said of Proudhon that \"though opposed to socializing the ownership of capital, [Proudhon] aimed nevertheless to socialize its effects by making its use beneficial to all instead of a means of impoverishing the many to enrich the few...by subjecting capital to the natural law of competition, thus bringing the price of its own use down to cost\".\n\nAs a term, \"mutualism\" has seen a variety of related uses. Charles Fourier first used the French term \"mutualisme\" in 1822, although the reference was not to an economic system. The first use of the noun \"mutualist\" was in the \"New-Harmony Gazette\" by an American Owenite in 1826. In the early 1830s, a labor organization in Lyons, France called themselves the \"Mutuellists\".\n\nPierre-Joseph Proudhon was involved with the Lyons mutualists and later adopted the name to describe his own teachings. In \"What Is Mutualism?\", Clarence Lee Swartz gives his own account of the origin of the term, claiming that \"[t]he word \"mutualism\" seems to have been first used by John Gray, an English writer, in 1832\". When John Gray's 1825 \"Lecture on Human Happiness\" was first published in the United States in 1826, the publishers appended the \"Preamble and constitution of the Friendly Association for Mutual Interests, located at Valley Forge\". 1826 also saw the publication of the \"Constitution of the Friendly Association for Mutual Interests at Kendal, Ohio\".\n\nBy 1846, Proudhon was speaking of \"mutualité\" in his writings and he used the term \"mutuellisme\", at least as early as 1848, in his \"Programme Révolutionnaire\". William B. Greene, in 1850, used the term \"mutualism\" to describe a mutual credit system similar to that of Proudhon. In 1850, the American newspaper \"The Spirit of the Age\", edited by William Henry Channing, published proposals for a \"mutualist township\" by Joshua King Ingalls and Albert Brisbane, together with works by Proudhon, William B. Greene, Pierre Leroux, and others.\n\nDuring the Second French Republic (1848–1852), Proudhon had his biggest public effect through his involvement with four newspapers: \"Le Représentant du Peuple\" (February 1848 – August 1848); \"Le Peuple\" (September 1848 – June 1849); \"La Voix du Peuple\" (September 1849 – May 1850); and \"Le Peuple de 1850\" (June 1850 – October 1850). His polemical writing style, combined with his perception of himself as a political outsider, produced a cynical, combative journalism that appealed to many French workers but alienated others. He repeatedly criticised the government's policies and promoted reformation of credit and exchange. He tried to establish a popular bank (Banque du peuple) early in 1849, but despite over 13,000 people signing up (mostly workers), receipts were limited falling short of 18,000FF and the whole enterprise was a failure from the beginning. \n\nProudhon ran for the constituent assembly in April 1848, but was not elected, although his name appeared on the ballots in Paris, Lyon, Besançon, and Lille, France. He was successful in the complementary elections of June 4 and served as a deputy during the debates over the National Workshops, created by the February 25, 1848 decree passed by Republican Louis Blanc. The workshops were to give work to the unemployed. Proudhon was never enthusiastic about such workshops, perceiving them to be essentially charitable institutions that did not resolve the problems of the economic system. He was against their elimination unless an alternative could be found for the workers who relied on the workshops for subsistence.\n\nProudhon was surprised by the Revolutions of 1848 in France. He participated in the February uprising and the composition of what he termed \"the first republican proclamation\" of the new republic. But he had misgivings about the new provisional government, headed by Dupont de l'Eure (1767–1855), who since the French Revolution in 1789 had been a longstanding politician, although often in the opposition. Proudhon published his own perspective for reform which was completed in 1849, \"Solution du problème social\" (\"Solution of the Social Problem\"), in which he laid out a program of mutual financial cooperation among workers. He believed this would transfer control of economic relations from capitalists and financiers to workers. The central part of his plan was the establishment of a bank to provide credit at a very low rate of interest and the issuing of exchange notes that would circulate instead of money based on gold.\n\nMutualism has been associated with two types of currency reform. Labor notes were first discussed in Owenite circles and received their first practical test in 1827 in the Time Store of former New Harmony member and individualist anarchist Josiah Warren. Mutual banking aimed at the monetization of all forms of wealth and the extension of free credit. It is most closely associated with William B. Greene, but Greene drew from the work of Proudhon, Edward Kellogg, and William Beck, as well as from the land bank tradition.\n\nMutualism can in many ways be considered \"the original anarchy\", since Proudhon was the first to identify himself as an anarchist. Though mutualism is generally associated with anarchism, it is not \"necessarily\" anarchist. Historian Wendy McElroy reports that American individualist anarchism received an important influence of 3 European thinkers. \"One of the most important of these influences was the french political philosopher Pierre-Joseph Proudhon whose words \"Liberty is not the Daughter But the Mother of Order\" appeared as a motto on \"Liberty\"s masthead\" (influential individualist anarchist publication of Benjamin Tucker). For American anarchist historian Eunice Minette Schuster: \"It is apparent...that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews ... William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form\".\n\nAfter 1850, Greene became active in labor reform. He was elected vice-president of the New England Labor Reform League, the majority of the members holding to Proudhon's scheme of mutual banking; and in 1869 president of the Massachusetts Labor Union. He then publishes \"Socialistic, Mutualistic, and Financial Fragments\" (1875). He saw mutualism as the synthesis of \"liberty and order\". His \"associationism...is checked by individualism...\"Mind your own business\", \"Judge not that ye be not judged\". Over matters which are purely personal, as for example, moral conduct, the individual is sovereign as well as over that which he himself produces. For this reason, he demands \"mutuality\" in marriage—the equal right of a woman to her own personal freedom and property\".\n\nBenjamin Tucker, editor of the anarchist publication \"Liberty\", later connected his economic views with those of Pierre-Joseph Proudhon, Josiah Warren and Karl Marx, taking sides with Proudhon and Josiah Warren: \n\nMutualist ideas found a fertile ground in the nineteenth century in Spain. In Spain, Ramón de la Sagra established the anarchist journal \"El Porvenir\" in A Coruña in 1845 which was inspired by Proudhon´s ideas. The Catalan politician Francesc Pi i Margall became the principal translator of Proudhon's works into Spanish and later briefly became president of Spain in 1873 while being the leader of the Democratic Republican Federal Party. According to George Woodcock: \"These translations were to have a profound and lasting effect on the development of Spanish anarchism after 1870, but before that time Proudhonian ideas, as interpreted by Pi, already provided much of the inspiration for the federalist movement which sprang up in the early 1860's\". According to the \"Encyclopædia Britannica\": \"During the Spanish revolution of 1873, Pi y Margall attempted to establish a decentralized, or “cantonalist”, political system on Proudhonian lines\". Pi i Margall was a dedicated theorist in his own right, especially through book-length works such as \"La reacción y la revolución\" (\"Reaction and Revolution\" from 1855), \"Las nacionalidades\" (\"Nationalities\" from 1877) and \"La Federación\" (\"The Federation\" from 1880). For prominent anarcho-syndicalist Rudolf Rocker: \"The first movement of the Spanish workers was strongly influenced by the ideas of Pi y Margall, leader of the Spanish Federalists and disciple of Proudhon. Pi y Margall was one of the outstanding theorists of his time and had a powerful influence on the development of libertarian ideas in Spain. His political ideas had much in common with those of Richard Price, Joseph Priestly [\"sic\"], Thomas Paine, Jefferson, and other representatives of the Anglo-American liberalism of the first period. He wanted to limit the power of the state to a minimum and gradually replace it by a Socialist economic order\".\n\nFor historian of the First International G. M. Stekloff: \"In April, 1856, there arrived from Paris a deputation of Proudhonist workers whose aim it was to bring about the foundation of a Universal League of Workers. The object of the League was the social emancipation of the working class, which, it was held, could only be achieved by a union of the workers of all lands against international capital. Since the deputation was one of Proudhonists, of course this emancipation was to be secured, not by political methods, but purely by economic means, through the foundation of productive and distributive co-operatives\".\n\n\"It was in the 1863 elections that for the first time workers' candidates were run in opposition to bourgeois republicans, but they secured very few votes...agroup of working-class Proudhonists (among whom were Murat and Tolain, who were subsequently to participate in the founding of the (First) International issued the famous Manifesto of the Sixty, which, though extremely moderate in tone, marked a turning point in the history of the French movement. For years and years the bourgeois liberals had been insisting that the revolution of 1789 had abolished class distinctions. The Manifesto of the Sixty loudly proclaimed that classes still existed. These classes were the bourgeoisie and the proletariat. The latter had its specific class interests, which none but workers could be trusted to defend. The inference drawn by the Manifesto was that there must be independent working-class candidates\". For Stekloff, \"the Proudhonists, who were at that date the leaders of the French section of the International. They looked upon the International Workingmen's Association as a sort of academy or synagogue, where Talmudists or similar experts could \"investigate\" the workers' problem; where in the spirit of Proudhon they could excogitate means for an accurate solution of the problem, without being disturbed by the stresses of a political campaign. Thus Fribourg, voicing the opinions of the Parisian group of the Proudhonists (Tolain and Co.) assured his readers that “the International was the greatest attempt ever made in modern times to aid the proletariat towards the conquest, by peaceful, constitutional, and moral methods, of the place which rightly belongs to the workers in the sunshine of civilisation\".\n\n\"The Belgian Federation threw in its lot with the anarchist International at its Brussels Congress, held in December, 1872...those taking part in the socialist movement of the Belgian intelligentsia were inspired by Proudhonist ideas which naturally led them to oppose the Marxist outlook\".\n\nNineteenth-century mutualists considered themselves libertarian socialists and are still considered libertarian socialists to this day. While oriented towards cooperation, mutualists favor free market solutions, believing that most inequalities are the result of preferential conditions created by government intervention. Mutualism is something of a middle way between classical economics and socialism of the collectivist variety, with some characteristics of both. Modern-day mutualist Kevin Carson considers anarchist mutualism to be \"free market socialism\".\n\nProudhon supported labor-owned cooperative firms and associations for \"we need not hesitate, for we have no choice ... it is necessary to form an ASSOCIATION among workers ... because without that, they would remain related as subordinates and superiors, and there would ensue two ... castes of masters and wage-workers, which is repugnant to a free and democratic society\" and so \"it becomes necessary for the workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism\". As for capital goods (man-made, non-land, \"means of production\"), mutualist opinions differs on whether these should be commonly managed public assets or private property.\n\nMutualism also had a considerable influence in the Paris Commune. George Woodcock manifests that \"a notable contribution to the activities of the Commune and particularly to the organization of public services was made by members of various anarchist factions, including the mutualists Courbet, Longuet, and Vermorel, the libertarian collectivists Varlin, Malon, and Lefrangais, and the bakuninists Elie and Elisée Reclus and Louise Michel\".\n\nThe primary aspects of mutualism are free association, mutualist credit, contract (or federation/confederation) and gradualism (or dual-power). Mutualism is often described by its proponents as advocating an \"anti-capitalist free market\". Mutualists argue that most of the economic problems associated with capitalism each amount to a violation of the \"cost principle\", or as Josiah Warren interchangeably said: \"Cost the limit of price\". It was inspired by the labor theory of value, which was popularized, though not invented, by Adam Smith in 1776 (Proudhon mentioned Smith as an inspiration). The labor theory of value holds that the actual price of a thing (or the \"true cost\") is the amount of labor that was undertaken to produce it. In Warren's terms, cost should be the \"limit of price\", with \"cost\" referring to the amount of labor required to produce a good or service. Anyone who sells goods should charge no more than the cost to himself of acquiring these goods.\n\nMutualists argue that association is only necessary where there is an organic combination of forces. For instance, an operation that requires specialization and many different workers performing their individual tasks to complete a unified product, i.e. a factory. In this situation, workers are inherently dependent on each other—and without association they are related as subordinate and superior, master and wage-slave.\n\nAn operation that can be performed by an individual without the help of specialized workers does not require association. Proudhon argued that peasants do not require societal form and only feigned association for the purposes of solidarity in abolishing rents, buying clubs, \"et cetera\". He recognized that their work is inherently sovereign and free. In commenting on the degree of association that is preferable, Proudhon said:\n\nIn cases in which production requires great division of labour, it is necessary to form an ASSOCIATION among the workers... because without that they would remain isolated as subordinates and superiors, and there would ensue two industrial castes of masters and wage workers, which is repugnant in a free and democratic society. But where the product can be obtained by the action of an individual or a family... there is no opportunity for association.\n\nFor Proudhon, mutualism involved creating \"industrial democracy\", a system where workplaces would be \"handed over to democratically organised workers' associations ... We want these associations to be models for agriculture, industry and trade, the pioneering core of that vast federation of companies and societies woven into the common cloth of the democratic social Republic\". He urged \"workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism\". This would result in \"Capitalistic and proprietary exploitation, stopped everywhere, the wage system abolished, equal and just exchange guaranteed\". Workers would no longer sell their labour to a capitalist but rather work for themselves in co-operatives.\n\nAs Robert Graham notes: \"Proudhon's market socialism is indissolubly linked to his notions of industry democracy and workers' self-management\". K. Steven Vincent notes in his in-depth analysis of this aspect of Proudhon's ideas that \"Proudhon consistently advanced a program of industrial democracy which would return control and direction of the economy to the workers\". For Proudhon, \"strong workers' associations ... would enable the workers to determine jointly by election how the enterprise was to be directed and operated on a day-to-day basis\".\n\nMutualists argue that free banking should be taken back by the people to establish systems of free credit. They contend that banks have a monopoly on credit, just as capitalists have a monopoly on the means of production, and landlords have a monopoly on land. Banks are essentially creating money by lending out deposits that do not actually belong to them, then charging interest on the difference. Mutualists argue that by establishing a democratically run mutual bank or credit union, it would be possible to issue free credit so that money could be created for the benefit of the participants rather than for the benefit of the bankers. Individualist anarchists noted for their detailed views on mutualist banking include Proudhon, William B. Greene and Lysander Spooner. Some modern forms of mutual credit are LETS and the Ripple monetary system project.\n\nIn a session of the French legislature, Proudhon proposed a government-imposed income tax to fund his mutual banking scheme, with some tax brackets reaching as high as 33 percent and 50 percent, which was turned down by the legislature. This income tax Proudhon proposed to fund his bank was to be levied on rents, interest, debts and salaries. Specifically, Proudhon's proposed law would have required all capitalists and stockholders to disburse one sixth of their income to their tenants and debtors, and another sixth to the national treasury to fund the bank.\n\nThis scheme was vehemently objected to by others in the legislature, including Frédéric Bastiat; the reason given for the income tax's rejection was that it would result in economic ruin and that it violated \"the right of property\". In his debates with Bastiat, Proudhon did once propose funding a national bank with a voluntary tax of 1%. Proudhon also argued for the abolition of all taxes.\n\nMutualism holds that producers should exchange their goods at cost-value using systems of \"contract\". While Proudhon's early definitions of cost-value were based on fixed assumptions about the value of labor-hours, he later redefined cost-value to include other factors such as the intensity of labor, the nature of the work involved, \"et cetera\". He also expanded his notions of \"contract\" into expanded notions of \"federation\". As Proudhon argued,\n\nDual power is the process of building alternatives institutions to the ones that already exist in modern society. Originally theorized by Proudhon, it has become adopted by many anti-state movements like Autonomism and Agorism. Proudhon described it as: \n\nBeneath the governmental machinery, in the shadow of political institutions, out of the sight of statemen and priests, society is producing its own organism, slowly and silently; and constructing a new order, the expression of its vitality and autonomy...\n\nDual power should not be confused with the Dual Power popularized by Vladimir Lenin, which was also theorized by Proudhon, but is referring to a more specific scenario where a revolutionary entity intentionally maintains the structure of the previous political institutions until the power of the previous institution is weakened enough such that the revolutionary entity can overtake it entirely. Dual-power as implemented by Mutualists and Agorists is the development of the alternative institution itself, which can create the Leninist scenario.\n\nIn Europe, a contemporary critic of Proudhon was the early anarcho-communist Joseph Déjacque. Unlike and against Proudhon, he argued that \"it is not the product of his or her labor that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature\". Returning to New York, he was able to serialise his book in his periodical \"Le Libertaire, Journal du Mouvement social\". Published in 27 issues from June 9, 1858 to February 4, 1861, \"Le Libertaire\" was the first anarcho-communist journal published in the United States.\n\nOne area of disagreement between mutualists and anarcho-communists stems from Proudhon's advocacy of money and later labour vouchers to compensate individuals for their labor as well as markets or artificial markets for goods and services. Peter Kropotkin, like other anarcho-communists, advocated the abolition of labor remuneration and questioned \"how can this new form of wages, the labor note, be sanctioned by those who admit that houses, fields, mills are no longer private property, that they belong to the commune or the nation?\". According to George Woodcock, Kropotkin believed that a wage system in any form, whether \"administered by Banks of the People or by workers' associations through labor cheques\" is a form of compulsion.\n\nCollectivist anarchist Mikhail Bakunin was an adamant critic of Proudhonian mutualism as well, stating: \"How ridiculous are the ideas of the individualists of the Jean Jacques Rousseau school and of the Proudhonian mutualists who conceive society as the result of the free contract of individuals absolutely independent of one another and entering into mutual relations only because of the convention drawn up among men. As if these men had dropped from the skies, bringing with them speech, will, original thought, and as if they were alien to anything of the earth, that is, anything having social origin\".\n\nCriticism from pro-market sectors has been common as well. Economist George Reisman charges that mutualism supports exploitation when it does not recognize a right of an individual to protect land that he has mixed his labor with if he happens to not be using it. Reisman sees the seizure of such land as the theft of the product of labor and has said: \"Mutualism claims to oppose the exploitation of labor, i.e. the theft of any part of its product. But when it comes to labor that has been mixed with land, it turns a blind eye out foursquare on the side of the exploiter\".\n\nKevin Carson is a contemporary mutualist and author of \"Studies in Mutualist Political Economy\". In its preface, Carson describes this work as \"an attempt to revive individualist anarchist political economy, to incorporate the useful developments of the last hundred years, and to make it relevant to the problems of the twenty-first century\". Contemporary mutualists are among those involved in the Alliance of the Libertarian Left and in the Voluntary Cooperation Movement.\n\nCarson holds that capitalism has been founded on \"an act of robbery as massive as feudalism\" and argues that capitalism could not exist in the absence of a state. He says \"[i]t is state intervention that distinguishes capitalism from the free market\". He does not define capitalism in the idealized sense, but says that when he talks about \"capitalism\" he is referring to what he calls \"actually existing capitalism\". He believes the term \"\"laissez-faire\" capitalism\" is an oxymoron because capitalism, he argues, is \"organization of society, incorporating elements of tax, usury, landlordism, and tariff, which thus denies the Free Market while pretending to exemplify it\". However, he says he has no quarrel with anarcho-capitalists who use the term \"\"laissez-faire\" capitalism\" and distinguish it from \"actually existing capitalism\". He says he has deliberately chosen to resurrect an old definition of the term. However many Anarchists including Mutualists continue to use the term and do not consider it an old definition of the term. \n\nCarson argues that the centralization of wealth into a class hierarchy is due to state intervention to protect the ruling class by using a money monopoly, granting patents and subsidies to corporations, imposing discriminatory taxation and intervening militarily to gain access to international markets. Carson's thesis is that an authentic free market economy would not be capitalism as the separation of labor from ownership and the subordination of labor to capital would be impossible, bringing a class-less society where people could easily choose between working as a freelancer, working for a fair wage, taking part of a cooperative, or being an entrepreneur. As did Tucker before him, he notes that a mutualist free market system would involve significantly different property rights than capitalism is based on, particularly in terms of land and intellectual property.\n\n\n"}
{"id": "6258404", "url": "https://en.wikipedia.org/wiki?curid=6258404", "title": "OK-GLI", "text": "OK-GLI\n\nThe OK-GLI (), also known as Buran Analog BST-02 (БТС-02), was a test vehicle (\"Buran aerodynamic analogue\") in the Buran programme. It was constructed in 1984, and was used for 25 test flights between 1985 and 1988 before being retired. It is now an exhibit at the Technik Museum Speyer in Germany.\n\nThe development of the Buran began in the late 1970s as a response to the U.S. Space Shuttle program. The construction of the orbiters began in 1980, and by 1984 the first full-scale Buran was rolled out. The first suborbital test flight of a scale-model took place as early as July 1983. As the project progressed, five additional scale-model flights were performed.\n\nThe OK-GLI (Buran Analog BST-02) test vehicle (\"Buran aerodynamic analogue\") was constructed in 1984. It was fitted with four AL-31 jet engines mounted at the rear (the fuel tank for the engines occupied a quarter of the cargo bay). This Buran could take off under its own power for flight tests, in contrast to the American \"Enterprise\" test vehicle, which was entirely unpowered and relied on an air launch.\n\nThe jets were used to take off from a normal landing strip, and once it reached a designated point, the engines were cut and the OK-GLI glided back to land. This provided invaluable information about the handling characteristics of the Buran design, and significantly differed from the carrier plane/air drop method used by the US and the \"Enterprise\" test craft.\n\nNine taxi tests and twenty-five test flights of the OK-GLI were performed, after which the vehicle was \"worn out\". All tests and flights were carried out at Baikonur.\n\nAfter the program was cancelled, the OK-GLI was stored at Gromov Flight Research Institute, near Moscow, where it was displayed during the annual MAKS air show.\n\nIn 2000, the OK-GLI was sold to an Australian company called the Buran Space Corporation, owned by Australian astronaut Paul Scully-Power. It was disassembled and transported by ship to Sydney, Australia, via Gothenburg, Sweden; arriving on 9 February 2000 and appeared as a static tourist attraction under a large temporary structure in Darling Harbour for a few years.\n\nUpon reassembly, the OK-GLI was put on display in a temporary enclosure for the 2000 Summer Olympics in Sydney. Visitors could walk around and inside the vehicle (a walkway was built along the cargo bay), and plans were in place for a tour of various cities in Australia and Asia. The owners went into bankruptcy after the Olympics, and the vehicle was moved into the open air and stored for a year, in a fenced-in parking lot and protected by nothing more than a large tarpaulin, where it suffered deterioration and repeated vandalism.\n\nThe OK-GLI was then offered for sale, including by a radio auction on the American News 980 KFWB-AM with a starting price of , however it did not receive any genuine bids.\n\nIn September 2004 a team of German journalists found the OK-GLI in Bahrain, having been abandoned after it was on display as an attraction of the 2002 \"Bahrain Summer\" festival.\n\nIt was then bought by the Sinsheim Auto & Technik Museum, to be transported to Germany in 2005. Due to legal issues, it remained in Bahrain for several years, pending settlement of an international court case over fees.\n\nOn 4 March 2008 the OK-GLI began its journey by sea to the Technik Museum Speyer where it was refurbished and serves as a walk-in exhibit.\n\nThe journey got off to an inauspicious start when, during the transfer from the storage barge to the ship, there was a failure of the aft spreader (part of the lifting mechanism) and the tail of the vehicle dropped from just above deck height to the bottom of the hold. No one was hurt and both the ship and vehicle seemed to suffer only minor damage.\n\n\n"}
{"id": "2267234", "url": "https://en.wikipedia.org/wiki?curid=2267234", "title": "Objective approach", "text": "Objective approach\n\nTaking an objective approach to an issue means having due regard for the known valid evidence (relevant facts, logical implications and viewpoints and human purposes) pertaining to that issue. If relevant valid evidence is denied, an objective approach is impossible. An objective approach is particularly important in science, and in decision-making processes which affect large numbers of people (e.g. politics).\n\nScientific progress can be regarded as a three-way contest between rival theories and a stock of evidence held in common. If rival interpretations are denied or if evidence is denied, then this impairs the possibility for rational debate and criticism, and consequently the growth of knowledge. On that ground, many scientists have proclaimed themselves in favour of freedom of thought and expression. If evidence is falsified as for example in conducting a control experiment knowledge is gained leading to the progress of an objective argument as the falsification resembles proof.\n\nIn decisions affecting large numbers of people (such as in politics) ignoring relevant evidence or alternative interpretations could lead to policies which, although perhaps well-intentioned, have the opposite effect of what was really intended.\n\nIn this context, it is often argued that although democracy might hamper swift, decisive action, it is nevertheless the best guarantee that all relevant facts and interpretations are included in the decision-making process, resulting in policies with greater long-term benefits.\n549749\n\nTaking an \"objective approach\" may not always be relevant, particularly in cases where it is impossible to be objective either because the relevant facts and viewpoints necessary are lacking, or because it is the subjective opinion or response that happens to be important. Thus it is possible to take an \"objective approach\" inappropriately in situations which call for an expression of subjective thought or feeling.\n\nSometimes it is argued that an objective approach is impossible because people will naturally take a partisan, self-interested approach.\nThat is, they will select out those views and facts which agree with their own (cf. confirmation bias). However this view fails to explain why, for example, people will do things which are not in their self-interest, based on what they believe to be an objective approach.\n\nA scientist or politician may never be \"neutral\" (they may have a vested interest in particular theories or policies) but they might also take an objective approach in the sense of remaining open to alternative viewpoints and new evidence.\n\nIn a rational discourse, such an \"open-minded\" stance is important, especially because it may not be known in advance which facts and arguments are truly relevant to resolving an issue. A \"closed\" stance would foreclose discussion and debate, usually on the assumption that the relevant facts and arguments are already known and judged.\n\nTaking an objective approach often contrasts with arguments from authority, where it is argued that \"X\" is true because an authority \"Y\" says so. The presumption is that \"Y\" is an authority capable of taking the most objective approach. But it may be necessary to evaluate the view of \"Y\" against other authorities likewise claiming to take an objective approach. This is an important aspect of academic scholarly method in the modern sense. Also, note that Wikipedia is not an uncontroversial (and some would argue not objective) source, and one should consider this when using it for research (which you should not do uncritically or unreservedly).\n\n"}
{"id": "24734755", "url": "https://en.wikipedia.org/wiki?curid=24734755", "title": "Orfeu Bertolami", "text": "Orfeu Bertolami\n\nOrfeu Bertolami (São Paulo, Brazil, 1959) is a theoretical physicist who works in problems of astrophysics, cosmology, general relativity and quantum gravity. He worked at Instituto Superior Técnico in Lisbon, Portugal from 1991 to 2010. He is currently professor at Departamento de Física e Astronomia of the Faculdade de Ciências da Universidade do Porto. \nHe is the author of a book to raise the awareness of the public on science about the history of ideas in astronomy, cosmology and theories of gravity (in Portuguese) and of a technical text on the aspects of gravity and propulsion in space published by the European Space Agency:\n\n\nIn particular, Bertolami studied baryogengesis, the cosmological constant problem and its connection with the equivalence principle, the generalized Chaplygin gas model of unification of Dark Energy (to explain the cosmic acceleration) and Dark Matter (to explain the flattening of the rotation curves of galaxies) and some Modified models of gravity with non-minimal coupling between curvature and matter. He has also worked on the Pioneer Anomaly, which was shown to be most likely due to \non-board thermal effects and reflection of the generated radiation. Most recent work concerns extensions of quantum mechanics based on phase-space noncommutativity.\n\n"}
{"id": "12668563", "url": "https://en.wikipedia.org/wiki?curid=12668563", "title": "Out of the Everywhere", "text": "Out of the Everywhere\n\nOut of the Everywhere is a collection of seventeen scientific essays by Isaac Asimov. It is the twenty-first of a series of books collecting essays from \"The Magazine of Fantasy and Science Fiction\". It was first published by Doubleday & Company in May 1990. The title may be found in George Macdonald's poem \"Baby\" which begins with the lines: \n\n\n"}
{"id": "424723", "url": "https://en.wikipedia.org/wiki?curid=424723", "title": "Palindromic prime", "text": "Palindromic prime\n\nA palindromic prime (sometimes called a palprime) is a prime number that is also a palindromic number. Palindromicity depends on the base of the numbering system and its writing conventions, while primality is independent of such concerns. The first few decimal palindromic primes are:\n\nExcept for 11, all palindromic primes have an odd number of digits, because the divisibility test for 11 tells us that every palindromic number with an even number of digits is a multiple of 11. It is not known if there are infinitely many palindromic primes in base 10. The largest known is (320,237 digits):\nIt was found in 2014 by David Broadhurst. The previous record was \n10 − 8×10 − 1, found by Darren Bedwell in 2013. On the other hand, it is known that, for any base, almost all palindromic numbers are composite, i.e. the ratio between palindromic composites and all palindromes below \"n\" tends to 1.\n\nIn binary, the palindromic primes include the Mersenne primes and the Fermat primes. All binary palindromic primes except binary 11 (decimal 3) have an odd number of digits; those palindromes with an even number of digits are divisible by 3. The sequence of binary palindromic primes begins (in binary):\n\nThe palindromic primes in base 12 are: (using reversed two and three for ten and eleven, respectively)\n\nDue to the superstitious significance of the numbers it contains, the palindromic prime 1000000000000066600000000000001 is known as Belphegor's Prime, named after Belphegor, one of the seven princes of Hell. Belphegor's Prime consists of the number 666, on either side enclosed by thirteen zeroes and a one. Belphegor's Prime is an example of a beastly palindromic prime in which a prime \"p\" is palindromic with 666 in the center. Another beastly palindromic prime is 700666007.\n\nRibenboim defines a triply palindromic prime as a prime \"p\" for which: \"p\" is a palindromic prime with \"q\" digits, where \"q\" is a palindromic prime with \"r\" digits, where \"r\" is also a palindromic prime. For example, \"p\" = 10 + 4661664 + 1, which has \"q\" = 11311 digits, and 11311 has \"r\" = 5 digits. The first (base-10) triply palindromic prime is the 11-digit 10000500001. It's possible that a triply palindromic prime in base 10 may also be palindromic in another base, such as base 2, but it would be highly remarkable if it were also a triply palindromic prime in that base as well.\n"}
{"id": "5228813", "url": "https://en.wikipedia.org/wiki?curid=5228813", "title": "Park system", "text": "Park system\n\nOne of the earliest park systems, in London, came into existence by chance. As London expanded around former royal parks in the nineteenth century, St. James's Park, Green Park and Hyde Park became part of the urban area. This arrangement was admired in France and adopted for the nineteenth century re-planning of Paris by Baron Haussmann. It was also admired by Frederick Law Olmsted and used to create the famous Emerald Necklace in Boston. Another example is Ebenezer Howard's Adirondack Park concept. These green networks were part of the nineteenth century Garden City Movement. \n\nIn 1927, the Maryland-National Capital Park and Planning Commission was formed to plan and acquire parklands along stream valley corridors in the then-rural northern and eastern suburbs of Washington, D.C. Over 33,000 acres (130 km²) are now protected in the Montgomery County, Maryland portion and provide welcome green space in this urbanized region. A major proposal for a park system was included in Patrick Abercrombie's 1943-4 County of London Plan.\n\nThe largest continuous urban parks system in North America is the North Saskatchewan River valley parks system in Edmonton, Alberta, Canada, which is in size and in length, and also includes 22 ravines, which have a combined total length of . The largest urban parks system in Australia is the Western Sydney Parklands, which is in size and in length.\n\n"}
{"id": "723859", "url": "https://en.wikipedia.org/wiki?curid=723859", "title": "Pioneer Venus Multiprobe", "text": "Pioneer Venus Multiprobe\n\nThe Pioneer Venus Multiprobe, also known as Pioneer Venus 2 or Pioneer 13 was a spacecraft launched in 1978 to explore Venus as part of NASA's Pioneer program. This part of the mission included a spacecraft Bus which was launched from Earth carrying one large and three smaller probes, which after separating penetrated the Venusian atmosphere at a different location, returning data as they descended into the planets thick atmosphere. The entry occurred on December 9, 1978.\nThere was also an orbiter launched this year also, part of the overall Pioneer Venus project along with this entry probe mission. Whereas the probes entered the atmosphere in 1978, the orbiter would stay in orbit throughout the 1980s and the early 1990s. (see Pioneer Venus Orbiter) The next major mission was the Magellan spacecraft, which was an orbiter capable mapping Venus by seeing through its opaque clouds with radar.\n\nAnother NASA entry probe mission was the Galileo probe, which was for planet Jupiter.\n\nThe Pioneer Venus Multiprobe bus was constructed by the Hughes Aircraft Company, built around the HS-507 bus. It was cylindrical in shape, with a diameter of and a mass of . Unlike the probes, which did not begin making direct measurements until they had decelerated lower in the atmosphere, the bus returned data on Venus' upper atmosphere.\n\nThe bus was targeted to enter the Venusian atmosphere at a shallow entry angle and transmit data until destruction by the heat of atmospheric friction. The objective was to study the structure and composition of the atmosphere down to the surface, the nature and composition of the clouds, the radiation field and energy exchange in the lower atmosphere, and local information on atmospheric circulation patterns. With no heat shield or parachute, the bus made upper atmospheric measurements with two instruments:\n\n\nThe spacecraft operated down to an altitude of about 110 km before disintegrating.\n\nThe spacecraft carried one large and three small atmospheric probes, designed to collect data as they descended into the atmosphere of Venus. The probes did not carry photographic instruments, and were not designed to survive landing – the smaller probes were not equipped with parachutes, and the larger probe's parachute was expected to detach as it neared the ground. All four probes continued transmitting data until impact; however, one survived and continued to transmit data from the surface.\n\nThe Large probe carried seven experiments, contained within a sealed spherical pressure vessel. The science experiments were:\n\nThis pressure vessel was encased in a nose cone and aft protective cover. After deceleration from initial atmospheric entry at about near the equator on the night side of Venus, a parachute was deployed at 67 km altitude. The large probe was about in diameter and the pressure vessel itself was in diameter.\n\nThree identical small probes, around in diameter, were deployed. These probes consisted of spherical pressure vessels surrounded by an aeroshell, but unlike the large probe, they had no parachutes and the aeroshells did not separate from the probes.\n\nThe science experiments were:\n\nThe radio signals from all four probes were also used to characterize the winds, turbulence, and propagation in the atmosphere. The small probes were each targeted at different parts of the planet and were named accordingly.\n\nThe Pioneer Venus Multiprobe was launched by an Atlas SLV-3D Centaur-D1AR rocket, which flew from Launch Complex 36A at the Cape Canaveral Air Force Station. The launch occurred at 07:33 on August 8, 1978, and deployed the Multiprobe into heliocentric orbit for its coast to Venus.\n\nPrior to the Multiprobe reaching Venus, the four probes were deployed from the main bus. The large probe was released on November 16, 1978, and the three small probes on November 20.\n\nAll four probes and the bus reached Venus on December 9, 1978. The large probe was the first to enter the atmosphere, at 18:45:32 UTC, followed over the next 11 minutes by the other three probes. The bus entered the atmosphere at 20:21:52 UTC, and returned its last signal at 20:22:55 from an altitude of .\n\nThe four probes transmitted data until they impacted the surface of Venus. The Day Probe survived impact, returning data from the surface for over an hour.\n\nBelow the altitude of 50 km the temperatures measured by the four probes are identical to within a few degrees. They are between 448 °C and 459 °C (838 °F and 858 °F) on the surface. The ground pressure is between 86.2 bar and 94.5 bar. Nephelometers identified three cloud layers with different characteristics. The most remarkable discovery was that the ratio of argon / argon isotopes much higher than in the atmosphere which seems to indicate that the genesis of the Venusian atmosphere is very different from that of Earth. The reconstituted trajectory of atmospheric probes was determined that the wind averaged a speed of 200 m/s in the middle cloud layer at 50 m/s at the base of these clouds and just 1 m/s at the ground. Overall data from airborne sensors confirmed, while specifying the data obtained by the Soviet space probe Venera program that preceded this mission.\n\nDiagram of the PVM's path to planet Venus from Earth in 1978, and this also notes the launch of the Pioneer Venus Orbiter which took place that year also.\n\n\n"}
{"id": "31549809", "url": "https://en.wikipedia.org/wiki?curid=31549809", "title": "Rocket City Space Pioneers", "text": "Rocket City Space Pioneers\n\nThe Rocket City Space Pioneers (RCSP) was one of 29 teams from 17 different countries officially registered and in the competition for the Google Lunar X PRIZE (GLXP) during 2010–2012.\n\nThe RCSP was drawn from Alabama businesses, educational institutions, and non-profit organizations. It announced entry into the competition on 7 September 2010, and departed from the competition after the team was acquired by Moon Express in December 2012.\n\nIn December 2012, the Rocket City Space Pioneers team was acquired by one of the other Google Lunar X-Prize teams, Moon Express, from Dynetics for an undisclosed sum. The new agreement makes Tim Pickens, the former lead of the RCSP team, the Chief Propulsion Engineer for MoonEx.\n\nThe Rocket City Space Pioneers' stated primary mission was to capture the GLXP $20 million Grand Prize, as well as the $4 million Apollo Heritage Bonus Prize.\n\nThe RCSP was developing a low-cost lunar lander/rover system for conducting commercial and scientific missions on the Moon. This system was intended to be capable of making a soft landing on the Moon and deploying a robotic rover. It was to be launched from the Earth by a medium-lift launch vehicle as part of a larger payload, then taken to a lunar orbit by a “tug” rocket-propulsion unit, and finally dropped to the Moon’s surface.\n\nA Falcon 9 two-stage rocket, built by SpaceX, was notionally planned to be launched from Cape Canaveral Air Force Station to place several metric tons of payload into a Geosynchronous Transfer Orbit (GTO). This RCSP payload was planned to consist of a primary payload, multiple rideshare payloads, and a tug (an ESPA ring with an added propulsion module). Once in Geosynchronous Transfer Orbit, the primary payload was to be deployed, along with three secondary payloads. The tug, containing the remaining three payloads, was to have separated from the Falcon 9 second stage and perform a Trans Lunar Injection burn. As the tug neared the Moon, it was planned to make a final braking burn, inserting it with its payloads in a Low Lunar Orbit (LLO)\n\nOnce in LLO, the tug was to have deployed the remaining three payloads, including the RCSP lander. The lander/rover will have its own small retrorocket, and, after being jettisoned from the tug, was projected to conduct a braking maneuver to land softly on the lunar surface. Once on the surface, the lander, would deploy a small rover for the required exploration.\n\nAs initially planned by the RCSP team, the rover was to have been capable of driving at least when it reached the lunar surface; this is twice the required GXLP roving distance. The rover was projected to be approximately in size and less than in mass. The early design included a plan to leave the rover tethered to the lander, with all external control and communications through this multi-circuit tether. The rover was to have carried two or more high-definition video cameras and possibly other sensors. The lander was to contain the system for communicating with the control center on the Earth, possibly being relayed through the orbiting tug.\n\nThe specific landing site and exploration traverse was never announced.\n\nThe RCSP is based in and named after Huntsville, Alabama (widely known as \"The Rocket City\"). Huntsville has been a leader in rocket development and space exploration since the arrival of Wernher von Braun and his team in 1950.\n\nMembers of the RSCP come from businesses, educational institutions, and non-profit organizations having operations in the Huntsville community. The following is a list of those organizations.\n\nThe RCSP is led by Tim Pickens, chief propulsion engineer and commercial space advisor at Dynetics. An inventor, innovator, and educator, Pickens was previously the founder and CEO of Orion Propulsion and, earlier, was the lead propulsion engineer on SpaceShipOne, the winner of the $10 million Ansari X Prize.\n"}
{"id": "2650611", "url": "https://en.wikipedia.org/wiki?curid=2650611", "title": "School of Natural Philosophy", "text": "School of Natural Philosophy\n\nSchool of Natural Philosophy is a scientific textbook by Richard Green Parker. It is credited with inspiring the inventor Thomas Edison.\n\n"}
{"id": "12282720", "url": "https://en.wikipedia.org/wiki?curid=12282720", "title": "Shape-memory coupling", "text": "Shape-memory coupling\n\nShape-memory coupling is a system for connecting pipes using shape-memory alloys. In its typical form the technique uses an internally ribbed sleeve of alloy such as Tinel that is slightly smaller in diameter than the pipes it is to connect. The sleeve is cooled in liquid nitrogen then, in this low-temperature state, mechanically expanded with a mandrel to fit easily over the two pipe ends to be joined. After fitting, it is allowed to rewarm, when the memory effect causes the sleeve to shrink back to its original smaller size, creating a tight joint.\n\nIt was first produced in the late 1960s or early 1970s by the Raychem Corporation under the trade name CryoFit. Manufacture of these couplings for aerospace hydraulic connections was later transferred to AMCI (Advanced Metal Components Inc.) and then later to Aerofit Products Inc. Additional products using the same shape-memory alloy technology are produced under Cryolive and CryoFlare trade names.\n\n"}
{"id": "13207515", "url": "https://en.wikipedia.org/wiki?curid=13207515", "title": "Smith Breeden Prize", "text": "Smith Breeden Prize\n\nThe Smith Breeden Prize is an annual prize given to authors with the best finance research papers published in the \"Journal of Finance\" in any area other than corporate finance.\n\nEach year the associate editors of the Journal of Finance award five papers for excellence. The two best finance papers in the subfield of corporate finance and the three best other papers from among all those papers that appeared in the first five issues of that year and in the December issue from the previous year are awarded prizes at the annual American Finance Association in January of the following year. Currently the Smith Breeden prizes are $10,000 for first place and $5,000 for second, but these amounts may change from time to time. Although the prize is funded by Smith Breeden Associates Inc., the administration of the Smith Breeden Prize is the responsibility of the Editor of The Journal of Finance and is carried out in conjunction with the selection of the Brattle Prizes. Associate Editors vote for the best two corporate finance papers (for the Brattle Prizes) and the best three other papers (for the Smith-Breeden Prizes). The papers receiving the most votes in their categories receive the prizes; however, a paper may not win in both categories.\n\nThe Journal of Finance is one of the most prestigious and highly cited journals in finance and economics. Each year hundreds of papers are submitted for publication. In 2006, there were 1239 submissions (1037 new manuscripts), 86 articles published, and 8 Smith Breeden Prize nominees from which 1 first place winning paper and two second place distinguished papers were chosen.\n"}
{"id": "58120041", "url": "https://en.wikipedia.org/wiki?curid=58120041", "title": "Studies in the Social History of Medicine", "text": "Studies in the Social History of Medicine\n\nThis is a list of books in the series Studies in the Social History of Medicine. The series was produced by the Society for the Social History of Medicine and Tavistock, later Routledge, between 1989 and 2009. It totalled 37 volumes.\n\nTitles in the series were:\n\n"}
{"id": "5059719", "url": "https://en.wikipedia.org/wiki?curid=5059719", "title": "Taufik Akbar", "text": "Taufik Akbar\n\nTaufik Akbar (born January 8, 1951 in Medan) is an Indonesian engineer and former astronaut candidate.\n\nAfter graduating at the Bandung Institute of Technology with a Bachelor of Science in Electrical Engineering in 1975, he worked as a telecommunication engineer. While working for Telkom in the development of the Palapa telecommunication satellite system, he was selected to take part in the Space Shuttle mission STS-61-H as a Payload Specialist in October 1985. While Pratiwi Sudarmono was chosen to be in the flight crew, he was supposed to be her backup on the mission. However, after the Challenger disaster the deployment of commercial satellites like the Indonesian Palapa B-3 planned for that mission was canceled, thus the mission never took place. The satellite was later launched with a Delta rocket.\n\nAfter his astronaut career, he continued to work for Telkom. Within 1990-92, he was General Manager Telecommunication Planning, Executive General Manager for Palapa Satellites Operation (1992–1993), President Director of Aplikanusa Lintasarta (1994–2000). In 2000, he became Director of Human Resources.\n\n"}
{"id": "7510900", "url": "https://en.wikipedia.org/wiki?curid=7510900", "title": "The Man Who Knew Infinity", "text": "The Man Who Knew Infinity\n\nThe Man Who Knew Infinity: A Life of the Genius Ramanujan is a biography of the Indian mathematician Srinivasa Ramanujan, written in 1991 by Robert Kanigel and published by Washington Square Press. The book gives a detailed account of his upbringing in India, his mathematical achievements, and his mathematical collaboration with English mathematician G. H. Hardy. The book also reviews the life of Hardy and the academic culture of Cambridge University during the early twentieth century.\n\nA feature film of the same title and based on the book was directed by Matt Brown using his own script. Srinivasa Ramanujan is played by Dev Patel, G. H. Hardy by Jeremy Irons, and Devika Bhise plays Ramanujan’s wife. Filming began in August 2014 at Trinity College, Cambridge.\n\n"}
{"id": "31492", "url": "https://en.wikipedia.org/wiki?curid=31492", "title": "The Skeptical Environmentalist", "text": "The Skeptical Environmentalist\n\nThe Skeptical Environmentalist: Measuring the Real State of the World (, literal translation: \"The True State of the World\") is a book by Danish environmentalist author Bjørn Lomborg, controversial for its claims that overpopulation, declining energy resources, deforestation, species loss, water shortages, certain aspects of global warming, and an assortment of other global environmental issues are unsupported by statistical analysis of the relevant data. It was first published in Danish in 1998, while the English edition was published as a work in environmental economics by Cambridge University Press in 2001.\n\nDue to the scope of the project, comprising the range of topics addressed, the diversity of data and sources employed, and the many types of conclusions and comments advanced, \"The Skeptical Environmentalist\" does not fit easily into a particular scientific discipline or methodology. Although published by the social sciences division of Cambridge University Press, the findings and conclusions were widely challenged on the basis of natural science. This interpretation of \"The Skeptical Environmentalist\" as a work of environmental science generated much of the controversy and debate that surrounded the book.\n\nPrior to becoming the Director of the Copenhagen Consensus Center and Adjunct Professor at the Copenhagen Business School, Bjørn Lomborg was an Associate Professor of Political Science at the University of Aarhus.\n\nSome critics focus on his lack of training or professional experience in the environmental sciences or economics. Supporters argue his research is an appropriate application of his expertise in cost-benefit analysis, a standard analytical tool in policy assessment. His advocates further note that many of the scientists and environmentalists who criticized the book are not themselves environmental policy experts or experienced in cost-benefit research.\n\nIn numerous interviews, Lomborg ascribed his motivation for writing \"The Skeptical Environmentalist\" to his personal convictions, making clear that he was a pro-environmentalist and Greenpeace supporter. He has stated that he began his research as an attempt to counter what he saw as anti-ecological arguments by Julian Lincoln Simon in an article in \"Wired\", but changed his mind after starting to analyze data. Lomborg describes the views he attributes to environmental campaigners as the \"Litany\", which he at one time claims to have affirmed, but purports to correct in his work.\n\nThe general analytical approach employed by Lomborg is based on cost-benefit analyses as employed in economics, social science, and the formulation and assessment of government policy. Much of Lomborg's examination of his Litany is based on statistical data analysis, therefore his work may be considered a work of that nature. Since it examines the costs and benefits of its many topics, it could be considered a work in economics, as categorized by its publisher. However, \"The Skeptical Environmentalist\" is methodologically eclectic and cross-disciplinary, combining interpretation of data with assessments of the media and human behavior, evaluations of scientific theories, and other approaches, to arrive at its various conclusions.\n\nIn arriving at the final work, Lomborg has used a similar approach in each of his work's main areas and subtopics. He progresses from the general to the specific, starting with a broad concern, such as pollution or energy, dividing it into subtopics (e.g. air pollution; fossil fuel depletion), and then identifying one or more widely held fears and their source (e.g. our air is growing increasingly toxic, by X measure, according to Y). From there, Lomborg chooses data that he considers to be the most reliable and reasonable available. He then analyzes that data to prove or disprove his selected proposition. In every case, his calculations find that the claim is not substantiated, and is either an exaggeration, or a completely reversed portrayal of an improving situation, rather than a deteriorating one. Having established what he calls \"the true state of the world\", for each topic and subtopic, Lomborg examines a variety of theories, technologies, implementation strategies and costs, and suggests alternative ways to improve not-so-dire situations, or advance in other areas not currently considered as pressing.\n\n\"The Skeptical Environmentalist\"'s subtitle refers to the \"State of the World\" report, published annually since 1984 by the Worldwatch Institute. Lomborg designated the report \"one of the best-researched and academically most ambitious environmental policy publications,\" but criticized it for using short-term trends to predict disastrous consequences, in cases where long-term trends would not support the same conclusions.\n\nIn establishing its arguments, \"The Skeptical Environmentalist\" examined a wide range of issues in the general area of environmental studies, including environmental economics and science, and came to an equally broad set of conclusions and recommendations. Lomborg's work directly challenged popular examples of green concerns by interpreting data from some 3,000 assembled sources. The author suggested that environmentalists diverted potentially beneficial resources to less deserving environmental issues in ways that were economically damaging. Much of the book's methodology and integrity have been subject to criticism which argue that Lomborg distorted the fields of research he covers. Support for the book was staunch as well.\n\n\"The Litany\" comprises very diverse areas where, Lomborg claims, overly pessimistic claims are made and bad policies are implemented as a result. He cites accepted mainstream sources, like the United States government, United Nations agencies and others, preferring global long-term data over regional and short-term statistics.\n\n\"The Skeptical Environmentalist\" is arranged around four major themes:\n\nLomborg's main argument is that the vast majority of environmental problems—such as pollution, water shortages, deforestation, and species loss, as well as population growth, hunger, and AIDS—are area-specific and highly correlated with poverty. Therefore, challenges to human prosperity are essentially logistical matters, and can be solved largely through economic and social development. Concerning problems that are more pressing at the global level, such as the depletion of fossil fuels and global warming, Lomborg argues that these issues are often overstated and that recommended policies are often inappropriate if assessed against alternatives.\n\nLomborg analyzes three major themes: life expectancy, food and hunger, and prosperity, finding that life expectancy and health levels have \"dramatically\" improved over the past centuries, even though several regions of the world remain threatened, in particular by AIDS. He dismisses Thomas Malthus' theory that increases in the world's population lead to widespread hunger. On the contrary, Lomborg claims that food is widespread, and humanity's daily intake of calories is increasing, and will continue to rise until hunger's eradication, thanks to technological improvements in agriculture. However, Lomborg notes that Africa in particular still produces too little sustenance, an effect he attributes to the continent's dismal economic and political systems. Concerning prosperity, Lomborg argues that wealth, as measured by per capita GDP, should not be the only judging criterion. He points to improvements in education, safety, leisure, and ever more widespread access to consumer goods as signs that prosperity is increasing in most parts of the world.\n\nIn this section, Lomborg looks at the world's natural resources and draws a conclusion that contrasts starkly to that of the well known report \"The Limits to Growth\". First, he analyzes food once more, this time from an ecological perspective, and again claims that most food products are not threatened by human growth. An exception, however, is fish, which continues to be depleted. As a partial solution, Lomborg presents fish farms, which cause a less disruptive impact on the world's oceans. Next, Lomborg looks at forests. He finds no indication of widespread deforestation, and notes that even the Amazon still retains more than 80% of its 1978 tree cover. Lomborg points out that in developing countries, deforestation is linked to poverty and poor economic conditions, so he proposes that economic growth is the best means to tackle the loss of forests.\nConcerning energy, Lomborg asserts that oil is not being depleted as fast as is claimed, and that improvements of technology will provide people with fossil fuels for years to come. The author further asserts that many alternatives already exist, and that with time they will replace fossil fuels as an energy source. Concerning other resources, such as metals, Lomborg suggests that based on their price history they are not in short supply. Examining the challenge of collecting sufficient amounts of water, Lomborg says that wars will probably not erupt over water because fighting such wars is not cost-effective (one week of war with the Palestinians, for instance, would cost Israel more than five desalination plants, according to an Israeli officer). Lomborg emphasizes the need for better water management, as water is distributed unequally around the world.\n\nLomborg considers pollution from different angles. He notes that air pollution in wealthy nations has steadily decreased in recent decades. He finds that air pollution levels are highly linked to economic development, with moderately developed countries polluting most. Again, Lomborg argues that faster growth in emerging countries would help them reduce their air pollution levels. Lomborg suggests that devoting resources to reduce the levels of specific air pollutants would provide the greatest health benefits and save the largest number of lives (per amount of money spent), continuing an already decades-long improvement in air quality in most developed countries. Concerning water pollution, Lomborg notes again that it is connected with economic progress. He also notes that water pollution in major Western rivers decreased rapidly after the use of sewage systems became widespread. Concerning waste, Lomborg notes once again that fears are overblown, as the entire waste produced by the United States in the 21st century could fit into a square 100 feet thick and 28 km along each side, or 0.009% of the total surface of the United States.\n\nIn this last section, Lomborg puts forward his main assertion: based on a cost-benefit analysis, the environmental threats to human prosperity are overstated and much of policy response is misguided. As an example, Lomborg cites worries about pesticides and their link to cancer. He argues that such concerns are vastly exaggerated in the public perception, as alcohol and coffee are the foods that create by far the greatest risk of cancer, as opposed to vegetables that have been sprayed with pesticides. Furthermore, if pesticides were not used on fruit and vegetables, their cost would rise, and consequently their consumption would go down, which would cause cancer rates to increase. He goes on to criticize the fear of a vertiginous decline in biodiversity, proposing that 0.7% of species have gone extinct in the last 50 years (as compared to a maximum of 50%, as claimed by some biologists). While Lomborg admits that extinctions are a problem, he asserts that they are not the catastrophe claimed by some, and have little effect on human prosperity.\n\nLomborg's most contentious assertion, however, involves global warming. From the outset, Lomborg \"accepts the reality of man-made global warming\" though he refers to a number of uncertainties in the computer simulations of climate change and some aspects of data collection. His main contention involves not the science of global warming but the politics and the policy response to scientific findings. Lomborg points out that, given the amount of greenhouse gas reduction required to combat global warming, the current Kyoto protocol is grossly insufficient. He argues that the economic costs of legislative restrictions that aim to slow or reverse global warming are far higher than the alternative of international coordination. Moreover, he asserts that the cost of combating global warming would be disproportionately shouldered by developing countries. Lomborg proposes that since the Kyoto agreement limits economic activities, developing countries that suffer from pollution and poverty most, will be perpetually handicapped economically.\n\nLomborg proposes that the importance of global warming in terms of policy priority is low compared to other policy issues such as fighting poverty, disease and aiding poor countries, which has direct and more immediate impact both in terms of welfare and the environment. He therefore suggests that a global cost-benefit analysis be undertaken before deciding on future measures. The Copenhagen Consensus that Lomborg later organized concluded that combating global warming does have a benefit but its priority compared to other issues is \"poor\" (ranked 13th) and three projects addressing climate change (optimal carbon tax, the Kyoto protocol and value-at-risk carbon tax), are the least cost-efficient of its proposals.\n\nLomborg concludes his book by once again reviewing the Litany, and noting that the real state of the world is much better than the Litany claims. According to Lomborg, this discrepancy poses a problem, as it focuses public attention on relatively unimportant issues, while ignoring those that are paramount. In the worst case, \"The Skeptical Environmentalist\" argues, the global community is pressured to adopt inappropriate policies which have adverse effects on humanity, wasting resources that could be put to better use in aiding poor countries or fighting diseases such as AIDS. Lomborg thus urges us to look at what he calls the true problems of the world, since solving those will also solve the Litany.\n\n\"The Skeptical Environmentalist\" was controversial even before its English-language release, with anti-publication efforts launched against Cambridge University Press. Once in the public arena, the book elicited strong reactions in scientific circles and in the mainstream media. Opinion was largely polarized. Environmental groups were generally critical.\n\nDr. Chris Harrison (Publishing Director of social science publishing for Cambridge University Press), anticipating the level of controversy a book like \"The Skeptical Environmentalist\" would likely provoke, took extra care with the book's peer-review process. Instead of choosing candidates from the usual list of social science referees, Cambridge University Press chose from a list provided by their environmental science publishing program. Four were chosen: a climate scientist, an expert in biodiversity and sustainable development, a specialist on the economics of climate change (whose credentials include reviewing publications for the UN's Intergovernmental Panel on Climate Change (IPCC)) and a \"pure\" economist. All four members of Cambridge's initial review panel agreed that the book should be published.\n\nWhile criticism of the book was to be expected, the publisher was apparently surprised by the pressure brought against it to not publish \"The Skeptical Environmentalist\". The complaints of some critics included demands that Cambridge convene a special panel to review the book in order to identify errors (despite existing pre-publication peer review), that Cambridge transfer their publishing rights to a \"non-scholarly publishing house\" and that they review their own policies to prevent publication of any book described as \"essentially a political tract\" in the future.\n\nIn the article, entitled \"Peer review, politics and pluralism\", Dr. Harrison noted that \"many of the critical reviews of \"The Skeptical Environmentalist\" went beyond the usual unpicking of a thesis and concentrated instead on the role of the publisher in publishing the book at all. The post tray and e-mail inbox of editors and senior managers at the press bore witness to a concerted campaign to persuade Cambridge to renounce the book.\" He went on to describe complaints from environmentalists who feared the book would be \"abused by corporate interests\". Cambridge University Press felt it necessary to issue a formal, written statement, in order to \"explain the editorial decisions that led not just to publishing the book but also to Cambridge's resistance to concerted pressure to withdraw it from the market.\" With these complaints and the publication of a \"Scientific American\" issue regarding the book (described below), Cambridge stated, in response to those who claimed the book lacked \"peer-review credentials\", \"it would be quite wrong to abandon an author who had satisfied the requirements of our peer-review system.\"\n\nCambridge took the additional step of inviting submissions of publishing proposals for books which offered an opposing argument to Lomborg's but noted that they had, to the best of Chris Harrison's knowledge, seen no attempt by any of the critics to submit such a proposal. This is seen by some to suggest that criticism of the book was political rather than academic. Subsequent to Cambridge's unequivocal assertion that \"The Skeptical Environmentalist\" had been subject to peer-review, Harrison noted that\nCambridge University Press maintained their position and the book was published.\n\nThe January 2002 issue of \"Scientific American\" contained, under the heading \"Misleading Math about the Earth\", a set of essays by several scientists, which claim that Lomborg and \"The Skeptical Environmentalist\" misrepresent both scientific evidence and scientific opinion. The magazine then refused Lomborg's request to print a lengthy point-by-point rebuttal in his own defence, on the grounds that the 32 pages would have taken a disproportionate share of the month's installment. \"Scientific American\" allowed Lomborg a one-page defense in the May 2002 edition, and then attempted to remove Lomborg's publication of his complete response online, citing a copyright violation. After receiving much criticism, the magazine published his complete rebuttal on its website, along with the counter rebuttals of John Rennie and John P. Holdren.\n\n\"Nature\" also published a harsh review of Lomborg's book, in which Stuart Pimm of the Center for Environmental Research and Conservation at Columbia University and Jeff Harvey of the Netherlands Institute of Ecology wrote: \"the text employs the strategy of those who, for example, argue that gay men aren't dying of AIDS, that Jews weren't singled out by the Nazis for extermination, and so on.\" Lomborg has also been criticized for using straw man arguments, with charges that his Litany of environmental doom-mongering does not accurately represent the mainstream views of the contemporary green movement.\n\nThe \"separately written expert reviews\" further detail the various expert opinions. Peter Gleick's assessment, for example, states:\n\nJerry Mahlman's appraisal of the chapter he was asked to evaluate, states:\n\nDavid Pimentel, who was repeatedly criticized in the book, also wrote a critical review.\n\nOne critical article, \"The Skeptical Environmentalist: A Case Study in the Manufacture of News\", attributes this media success to its initial, influential supporters:\n\nThe media was criticized for the biased selection of reviewers and not informing readers of reviewers' background. Richard C. Bell, writing for Worldwatch noted that the Wall Street Journal, \"instead of seeking scientists with a critical perspective,\" like many publications \"put out reviews by people who were closely associated with Lomborg\", with the Journal soliciting a review from the Competitive Enterprise Institute's Ronald Bailey, someone \"who had earlier written a book called The True State of the World, from which much of Lomborg's claims were taken.\" Bell also criticized the Washington Post, whose Sunday Book World assigned the book review to Denis Dutton, identified as \"a professor of philosophy who lectures on the dangers of pseudoscience at the science faculties of the University of Canterbury in New Zealand\", and as the editor of the web site Arts and Letters Daily. Bell noted that:\n\n\"The Post did not tell its readers that Dutton's web site features links to the Global Climate Coalition, an anti-Kyoto consortium of oil and coal businesses, and to the messages of Julian Simon --the man whose denial that global warming was occurring apparently gave Lomborg the idea for his book in the first place. It was hardly surprising that Dutton anointed Lomborg's book as 'the most significant work on the environment since the appearance of its polar opposite, Rachel Carson's Silent Spring, in 1962. It's a magnificent achievement.'\"\n\nSome critics of \"The Skeptical Environmentalist\" took issue not with the statistical investigation of Lomborg's Litany, but with the suggestions and conclusions for which they were the foundation. This line of criticism considered the book as a contribution to the policy debate over environment rather than the work of natural science. In a BBC column from August 23, 2001, veteran BBC environmental correspondent Alex Kirby wrote:\n\nKirby's first concern was not with the extensive research and statistical analysis, but the conclusions drawn from them:\n\nOn September 5, 2001, at a Lomborg book reading in England, British environmentalist author Mark Lynas threw a cream pie in Lomborg's face. In a September 9, 2001, article, \"Why I pied Lomborg\", Lynas stated:\n\nThe December 12, 2001 issue of \"Grist\" devoted an issue to \"The Skeptical Environmentalist\", with a series of essays from various scientists challenging individual sections. A separate article examining the book's overall approach took issue with the framing of Lomborg's conclusions:\n\nAddressing the apparent difficulty of scientists opposing \"The Skeptical Environmentalist\" in criticizing the book strictly on the basis of statistics and challenging the conclusions about areas of environmental sciences that were drawn from them, Lynas contends:\n\nInfluential UK newsweekly \"The Economist\" weighed in at the start with heavy support, publishing an advance essay by Lomborg in which he detailed his Litany, and following up with a highly favorable review and supportive coverage. It stated that \"This is one of the most valuable books on public policy—not merely environmental policy— to have been written for the intelligent general reader in the past ten years...\"The Skeptical Environmentalist\" is a triumph.\"\n\nAmong the general media, \"The New York Times\" stated that \"The primary target of the book, a substantial work of analysis with almost 3,000 footnotes, are statements made by environmental organizations like the Worldwatch Institute, the World Wildlife Fund and Greenpeace.\" The \"Wall Street Journal\" deemed Lomborg's work \"a superbly documented and readable book.\". A review in \"The Washington Post\" claimed that \"Bjørn Lomborg's good news about the environment is bad news for Green ideologues. His richly informative, lucid book is now the place from which environmental policy decisions must be argued. In fact,\" The Skeptical Environmentalist \"is the most significant work on the environment since the appearance of its polar opposite, Rachel Carson's \"Silent Spring\", in 1962. It's a magnificent achievement.\" \"Rolling Stone\" wrote that \"Lomborg pulls off the remarkable feat of welding the techno-optimism of the Internet age with a lefty's concern for the fate of the planet.\"\n\nIn March 2003 the \"New York Law School Law Review\" published an examination of the critical reviews of \"Skeptical Environmentalist\" from the \"Scientific American\", \"Nature\" and \"Science\" magazines by Professor of Law David Shoenbrod and then Senior Law Student Christi Wilson of New York Law School. The authors take the perspective of a court faced with an argument against hearing an expert witness in order to evaluate whether Lomborg was credible as an expert, and whether his testimony is valid to his expertise. They classify the types of criticisms leveled at Lomborg and his arguments, and proceed to evaluate each of the reasons given for disqualifying Lomborg. They conclude that a court should accept Lomborg as a credible expert in the field of statistics, and that his testimony was appropriately restricted to his area of expertise. Of course, Professor Shoenbrod and Wilson note, Mr. Lomborg's factual conclusions may not be correct, nor his policy proposals effective, but his criticisms should be addressed, not merely dismissed out of hand.\n\nThe Union of Concerned Scientists and the Danish Committees on Scientific Dishonesty raised concern about the responses of certain sections of the scientific community to a peer reviewed book published under the category of environmental economics. The groups worried that the receptions to Lomborg were a politicization of science by scientists. This unease was reflected in the involvement of the Union of Concerned Scientists and Danish Committees on Scientific Dishonesty in \"When scientists politicize science: making sense of controversy over The Skeptical Environmentalist\", where Roger A. Pielke argued:\n\nIn \"Green with Ideology - The hidden agenda behind the \"scientific\" attacks on Bjørn Lomborg’s controversial book, The Skeptical Environmentalist\", Ronald Bailey stated that \"The bitter anti-Lomborg campaign reveals the hidden crisis of what we might call ideological environmentalism.\" He further wrote:\n\nAfter the publication of \"The Skeptical Environmentalist\", Lomborg was accused of scientific dishonesty. Several environmental scientists brought a total of three complaints against Lomborg to the Danish Committees on Scientific Dishonesty (DCSD), a body under Denmark's Ministry of Science, Technology and Innovation. Lomborg was asked whether he regarded the book as a \"debate\" publication, and thereby not under the purview of the DCSD, or as a scientific work; he chose the latter, clearing the way for the inquiry that followed. The charges claimed that \"The Skeptical Environmentalist\" contained deliberately misleading data and flawed conclusions. Due to the similarity of the complaints, the DCSD decided to proceed on the three cases under one investigation.\n\nOn January 6, 2003, a mixed DCSD ruling was released, in which the Committees decided that \"The Skeptical Environmentalist\" was scientifically dishonest, but Lomborg was innocent of wrongdoing due to a lack of expertise in the relevant fields:\n\nThe DCSD cited \"The Skeptical Environmentalist\" for:\n\n\nOn February 13, 2003, Lomborg filed a complaint against the DCSD's decision with the Ministry of Science, Technology and Innovation (MSTI), which oversees the group.\n\nOn December 17, 2003, the Ministry found that the DCSD had made a number of procedural errors, including:\n\nThe Ministry remitted the case to the DCSD. In doing so the Ministry indicated that it regarded the DCSD's previous findings of scientific dishonesty in regard to the book as invalid. The Ministry also instructed the DCSD to decide whether to reinvestigate. On March 12, 2004, the Committee formally decided not to act further on the complaints, reasoning that renewed scrutiny would, in all likelihood, result in the same conclusion.\n\nThe original DCSD decision about Lomborg provoked a petition among Danish academics from 308 scientists, many from the social sciences, who criticised the DCSD's investigative methods.\n\nAnother group of Danish scientists collected signatures in support of the DCSD. The 640 signatures in this second petition came almost exclusively from the medical and natural sciences, and included Nobel laureate in Chemistry Jens Christian Skou, former university rector Kjeld Møllgård, and professor Poul Harremoës from the Technical University of Denmark.\n\nA group of scientists published an article in 2005 in the \"Journal of Information Ethics\", in which they concluded that most criticism against Lomborg was unjustified, and that the scientific community had misused their authority to suppress the author.\n\nThe claim that allegations against Lomborg were unsubstantiated was challenged in the next issue of \"Journal of Information Ethics\" by Kåre Fog, one of the original DCSD petitioners. Fog reasserted his contention that, despite the ministry's decision, most of the accusations against Lomborg were valid, and rejected what he called \"the Galileo hypothesis\", which portrays Lomborg as a brave young man confronting an entrenched opposition.\n\nFog has established a curated catalogue of criticisms against Lomborg, which includes a section for each page of every \"Skeptical Environmentalist\" chapter. Fog enumerates and details what he believes to be flaws and errors in Lomborg's work. He explicitly indicates if particular mistakes may have been made deliberately by Lomborg, in order to mislead. According to Fog, since none of his denunciations of Lomborg's work have been proven false, the suspicion that Lomborg has misled deliberately is maintained. Lomborg has written a full text published online as Godehetens Pris (Danish) that goes through the main allegations put forward by Fog and others.\n\n\n\n\n\n"}
{"id": "89008", "url": "https://en.wikipedia.org/wiki?curid=89008", "title": "Theory of multiple intelligences", "text": "Theory of multiple intelligences\n\nThe theory of multiple intelligences differentiates human intelligence into specific 'modalities', rather than seeing intelligence as dominated by a single general ability. Howard Gardner proposed this model in his 1983 book \"Frames of Mind: The Theory of Multiple Intelligences\". According to the theory, an intelligence 'modality' must fulfill eight criteria:\n\nGardner proposed seven abilities that he held to meet these criteria: \nHe later suggested that existential and moral intelligences may also be worthy of inclusion.\n\nAlthough the distinction between intelligences has been set out in great detail, Gardner opposes the idea of labeling learners to a specific intelligence. Gardner maintains that his theory should \"empower learners\", not restrict them to one modality of learning. According to Gardner, an intelligence is \"a biopsychological potential to process information that can be activated in a cultural setting to solve problems or create products that are of value in a culture.\" According to a 2006 study, each of the domains proposed by Gardner involves a blend of the general \"g\" factor, cognitive abilities other than \"g\", and, in some cases, non-cognitive abilities or personality characteristics.\n\nThis area has to do with sensitivity to sounds, rhythms, tones, and music. People with a high musical intelligence normally have good pitch and may even have absolute pitch, and are able to sing, play musical instruments, and compose music. They have sensitivity to rhythm, pitch, meter, tone, melody or timbre.\n\nThis area deals with spatial judgment and the ability to visualize with the mind's eye. Spatial ability is one of the three factors beneath \"g\" in the hierarchical model of intelligence.\n\nPeople with high verbal-linguistic intelligence display a facility with words and languages. They are typically good at reading, writing, telling stories and memorizing words along with dates. Verbal ability is one of the most \"g\"-loaded abilities.\nThis type of intelligence is measured with the Verbal IQ in WAIS-IV.\n\nThis area has to do with logic, abstractions, reasoning, numbers and critical thinking. This also has to do with having the capacity to understand the underlying principles of some kind of causal system. Logical reasoning is closely linked to fluid intelligence and to general intelligence (\"g\" factor).\n\nThe core elements of the bodily-kinesthetic intelligence are control of one's bodily motions and the capacity to handle objects skillfully. Gardner elaborates to say that this also includes a sense of timing, a clear sense of the goal of a physical action, along with the ability to train responses.\n\nPeople who have high bodily-kinesthetic intelligence should be generally good at physical activities such as sports, dance, acting, and making things.\n\nGardner believes that careers that suit those with high bodily-kinesthetic intelligence include: athletes, dancers, musicians, actors, builders, police officers, and soldiers. Although these careers can be duplicated through virtual simulation, they will not produce the actual physical learning that is needed in this intelligence.\n\nIn theory, individuals who have high interpersonal intelligence are characterized by their sensitivity to others' moods, feelings, temperaments, motivations, and their ability to cooperate in order to work as part of a group. According to Gardner in \"How Are Kids Smart: Multiple Intelligences in the Classroom\", \"Inter- and Intra- personal intelligence is often misunderstood with being extroverted or liking other people...\" Those with high interpersonal intelligence communicate effectively and empathize easily with others, and may be either leaders or followers. They often enjoy discussion and debate.\" Gardner has equated this with emotional intelligence of Goleman.\n\nGardner believes that careers that suit those with high interpersonal intelligence include sales persons, politicians, managers, teachers, lecturers, counselors and social workers.\n\nThis area has to do with introspective and self-reflective capacities. This refers to having a deep understanding of the self; what one's strengths or weaknesses are, what makes one unique, being able to predict one's own reactions or emotions.\n\nNot part of Gardner's original seven, naturalistic intelligence was proposed by him in 1995. \"If I were to rewrite Frames of Mind today, I would probably add an eighth intelligence – the intelligence of the naturalist. It seems to me that the individual who is readily able to recognize flora and fauna, to make other consequential distinctions in the natural world, and to use this ability productively (in hunting, in farming, in biological science) is exercising an important intelligence and one that is not adequately encompassed in the current list.\" This area has to do with nurturing and relating information to one's natural surroundings. Examples include classifying natural forms such as animal and plant species and rocks and mountain types. This ability was clearly of value in our evolutionary past as hunters, gatherers, and farmers; it continues to be central in such roles as botanist or chef.\n\nThis sort of ecological receptiveness is deeply rooted in a \"sensitive, ethical, and holistic understanding\" of the world and its complexities – including the role of humanity within the greater ecosphere.\n\nGardner did not want to commit to a spiritual intelligence, but suggested that an \"existential\" intelligence may be a useful construct, also proposed after the original 7 in his 1999 book. The hypothesis of an existential intelligence has been further explored by educational researchers.\n\nOn January 13, 2016, Gardner mentioned in an interview with BigThink that he is considering adding the teaching-pedagogical intelligence \"which allows us to be able to teach successfully to other people\". In the same interview, he explicitly refused some other suggested intelligences like humour, cooking and sexual intelligence.\n\nGardner argues that there is a wide range of cognitive abilities, but that there are only very weak correlations among them. For example, the theory postulates that a child who learns to multiply easily is not necessarily more intelligent than a child who has more difficulty on this task. The child who takes more time to master multiplication may best learn to multiply through a different approach, may excel in a field outside mathematics, or may be looking at and understanding the multiplication process at a fundamentally deeper level.\n\nIntelligence tests and psychometrics have generally found high correlations between different aspects of intelligence, rather than the low correlations which Gardner's theory predicts, supporting the prevailing theory of general intelligence rather than multiple intelligences (MI). The theory has been criticized by mainstream psychology for its lack of empirical evidence, and its dependence on subjective judgement. However research by Dweck (2006), referred to as Growth Mindset Theory, shows that individuals with low correlations can attain also high correlations through intelligence growth. This challenges the notion of fixed or static intelligence levels that general intelligence tests measure. More importantly, it challenges the notion that intelligence test scores are an accurate predictor for future ability.\n\nOne major criticism of the theory is that it is \"ad hoc\": that Gardner is not expanding the definition of the word \"intelligence\", but rather denies the existence of intelligence as traditionally understood, and instead uses the word \"intelligence\" where other people have traditionally used words like \"ability\" and \"aptitude\". This practice has been criticized by Robert J. Sternberg, Eysenck, and Scarr. White (2006) points out that Gardner's selection and application of criteria for his \"intelligences\" is subjective and arbitrary, and that a different researcher would likely have come up with different criteria.\n\nDefenders of MI theory argue that the traditional definition of intelligence is too narrow, and thus a broader definition more accurately reflects the differing ways in which humans think and learn.\n\nSome criticisms arise from the fact that Gardner has not provided a test of his multiple intelligences. He originally defined it as the ability to solve problems that have value in at least one culture, or as something that a student is interested in. He then added a disclaimer that he has no fixed definition, and his classification is more of an artistic judgment than fact:\n\nGenerally, linguistic and logical-mathematical abilities are called intelligences, but artistic, musical, athletic, etc. abilities are not. Gardner argues this causes the former to be needlessly aggrandized. Certain critics are wary of this widening of the definition, saying that it ignores \"the connotation of intelligence ... [which] has always connoted the kind of thinking skills that makes one successful in school.\"\n\nGardner writes \"I balk at the unwarranted assumption that certain human abilities can be arbitrarily singled out as intelligence while others cannot.\" Critics hold that given this statement, any interest or ability can be redefined as \"intelligence\". Thus, studying intelligence becomes difficult, because it diffuses into the broader concept of ability or talent. Gardner's addition of the naturalistic intelligence and conceptions of the existential and moral intelligences are seen as the fruits of this diffusion. Defenders of the MI theory would argue that this is simply a recognition of the broad scope of inherent mental abilities, and that such an exhaustive scope by nature defies a one-dimensional classification such as an IQ value.\n\nThe theory and definitions have been critiqued by Perry D. Klein as being so unclear as to be tautologous and thus unfalsifiable. Having a high musical ability means being good at music while at the same time being good at music is explained by having a high musical ability.\n\nHenri Wallon shows that « We can not distinguish intelligence from its operations ». Yves Richez discovers 10 Natural Operating Modes (Modes Opératoire Naturel - MoON). The Richez's studies show a gap between Chinese thought and Western thought. In China, notion of « Being » (Self) and notion of « intelligence » don't exist. Those are greek-Latin inventions (Platon). Instead of intelligence, Chinese speaks of « operating modes ». Thus, Yves Richez doesn't talk of « intelligence » but of « natural operating modes » (Mo.O.N.).\n\nAndreas Demetriou suggests that theories which overemphasize the autonomy of the domains are as simplistic as the theories that overemphasize the role of general intelligence and ignore the domains. He agrees with Gardner that there are indeed domains of intelligence that are relevantly autonomous of each other. Some of the domains, such as verbal, spatial, mathematical, and social intelligence are identified by most lines of research in psychology. In Demetriou's theory, one of the neo-Piagetian theories of cognitive development, Gardner is criticized for underestimating the effects exerted on the various domains of intelligences by the various subprocesses that define overall processing efficiency, such as speed of processing, executive functions, working memory, and meta-cognitive processes underlying self-awareness and self-regulation. All of these processes are integral components of general intelligence that regulate the functioning and development of different domains of intelligence.\n\nThe domains are to a large extent expressions of the condition of the general processes, and may vary because of their constitutional differences but also differences in individual preferences and inclinations. Their functioning both channels and influences the operation of the general processes. Thus, one cannot satisfactorily specify the intelligence of an individual or design effective intervention programs unless both the general processes and the domains of interest are evaluated.\n\nThe premise of the multiple intelligences hypothesis, that human intelligence is a collection of specialist abilities, have been criticized for not being able to explain human adaptation to most if not all environments in the world. In this context, humans are contrasted to social insects that indeed have a distributed \"intelligence\" of specialists, and such insects may spread to climates resembling that of their origin but the same species never adapt to a wide range of climates from tropical to temperate by building different types of nests and learning what is edible and what is poisonous. While some such as the leafcutter ant grow fungi on leaves, they do not cultivate different species in different environments with different farming techniques as human agriculture does. It is therefore argued that human adaptability stems from a general ability to falsify hypotheses and make more generally accurate predictions and adapt behavior thereafter, and not a set of specialized abilities which would only work under specific environmental conditions.\n\nGardner argues that IQ tests only measure linguistic and logical-mathematical abilities. He argues the importance of assessing in an \"intelligence-fair\" manner. While traditional paper-and-pen examinations favor linguistic and logical skills, there is a need for intelligence-fair measures that value the distinct modalities of thinking and learning that uniquely define each intelligence.\n\nPsychologist Alan S. Kaufman points out that IQ tests have measured spatial abilities for 70 years. Modern IQ tests are greatly influenced by the Cattell-Horn-Carroll theory which incorporates a general intelligence but also many more narrow abilities. While IQ tests do give an overall IQ score, they now also give scores for many more narrow abilities.\n\nAccording to a 2006 study many of Gardner's \"intelligences\" correlate with the \"g\" factor, supporting the idea of a single dominant type of intelligence. According to the study, each of the domains proposed by Gardner involved a blend of \"g\", of cognitive abilities other than \"g\", and, in some cases, of non-cognitive abilities or of personality characteristics.\n\nLinda Gottfredson (2006) has argued that thousands of studies support the importance of intelligence quotient (IQ) in predicting school and job performance, and numerous other life outcomes. In contrast, empirical support for non-\"g\" intelligences is either lacking or very poor. She argued that despite this the ideas of multiple non-\"g\" intelligences are very attractive to many due to the suggestion that everyone can be smart in some way.\n\nA critical review of MI theory argues that there is little empirical evidence to support it:\nThe same review presents evidence to demonstrate that cognitive neuroscience research does not support the theory of multiple intelligences:\nThe theory of multiple intelligences is sometimes cited as an example of pseudoscience because it lacks empirical evidence or falsifiability, though Gardner has argued otherwise.\n\nGardner defines an intelligence as \"bio-psychological potential to process information that can be activated in a cultural setting to solve problems or create products that are of value in a culture.\" According to Gardner, there are more ways to do this than just through logical and linguistic intelligence. Gardner believes that the purpose of schooling \"should be to develop intelligences and to help people reach vocational and avocational goals that are appropriate to their particular spectrum of intelligences. People who are helped to do so, [he] believe[s], feel more engaged and competent and therefore more inclined to serve society in a constructive way.\"\n\nGardner contends that IQ tests focus mostly on logical and linguistic intelligence. Upon doing well on these tests, the chances of attending a prestigious college or university increase, which in turn creates contributing members of society. While many students function well in this environment, there are those who do not. Gardner's theory argues that students will be better served by a broader vision of education, wherein teachers use different methodologies, exercises and activities to reach all students, not just those who excel at linguistic and logical intelligence. It challenges educators to find \"ways that will work for this student learning this topic\".\n\nJames Traub's article in \"The New Republic\" notes that Gardner's system has not been accepted by most academics in intelligence or teaching. Gardner states that \"while Multiple Intelligences theory is consistent with much\nempirical evidence, it has not been subjected to strong experimental tests ... Within the area of education, the applications of the theory are currently being examined in many projects. Our hunches will have to be revised many times in light of actual classroom experience.\"\n\nJerome Bruner agreed with Gardner that the intelligences were \"useful fictions,\" and went on to state that \"his approach is so far beyond the data-crunching of mental testers that it deserves to be cheered.\"\n\nGeorge Miller, a prominent cognitive psychologist, wrote in \"The New York Times Book Review\" that Gardner's argument consisted of \"hunch and opinion\" and Charles Murray and Richard J. Herrnstein in \"The Bell Curve\" (1994) called Gardner's theory \"uniquely devoid of psychometric or other quantitative evidence.\"\n\nIn spite of its lack of general acceptance in the psychological community, Gardner's theory has been adopted by many schools, where it is often conflated with learning styles, and hundreds of books have been written about its applications in education. Some of the applications of Gardner's theory have been described as \"simplistic\" and Gardner himself has said he is \"uneasy\" with the way his theory has been used in schools. Gardner has denied that multiple intelligences are learning styles and agrees that the idea of learning styles is incoherent and lacking in empirical evidence. Gardner summarizes his approach with three recommendations for educators: individualize the teaching style (to suit the most effective method for each student), pluralize the teaching (teach important materials in multiple ways), and avoid the term \"styles\" as being confusing.\n\n\n\n\n"}
{"id": "21193032", "url": "https://en.wikipedia.org/wiki?curid=21193032", "title": "Trick or Treatment?", "text": "Trick or Treatment?\n\nTrick or Treatment? Alternative Medicine on Trial (North American title: Trick or Treatment: The Undeniable Facts about Alternative Medicine) is a 2008 book about alternative medicine by Simon Singh and Edzard Ernst. Singh is a physicist and the writer of several popular science books. Ernst is a professor of complementary medicine.\n\nThe book evaluates the scientific evidence for acupuncture, homeopathy, herbal medicine, and chiropractic, and briefly covers 36 other treatments. It finds that the scientific evidence for these alternative treatments is generally lacking. Homeopathy is concluded to be completely ineffective: \"It's nothing but a placebo, despite what homeopaths say\".\n\nAlthough \"Trick or Treatment\" presents evidence that acupuncture, chiropractic and herbal remedies have limited efficacy for certain ailments, the authors conclude that the dangers of these treatments outweigh any potential benefits. Such potential risks outlined by the authors are contamination or unexpected interactions between components in the case of herbal medicine, risk of infection in the case of acupuncture and the potential for chiropractic manipulation of the neck to cause delayed stroke.\n\nThe book is very critical of Prince Charles' advocacy of alternative medicine and the actions of his now-defunct The Prince's Foundation for Integrated Health. \"Trick or Treatment\" is dedicated, in an ironic fashion, to the Prince.\n\nThe book contains six chapters:\n\nHow do you determine the truth?\n\nThe truth about acupuncture\n\nThe truth about homeopathy\n\nThe truth about chiropractic therapy\n\nThe truth about herbal medicine\n\nDoes the truth matter?\n\nThe book received generally good reviews. \"The New England Journal of Medicine\"'s review said this about the authors: \"Simon Singh is a physicist and science journalist, and his coauthor, Edzard Ernst, is a physician and professor of complementary medicine. Ernst is one of the best qualified people to summarize the evidence on this topic.\" \"The Daily Telegraph\" found the book to be \"a clearly written, scrupulously scientific examination of the health claims of key areas of alternative medicine: acupuncture, homeopathy, chiropractic therapy and herbal medicine. The results are stark. In no case, apart from in some limited ways in herbal medicine, do any of these 'therapies’ work. On the contrary, they can be life-threatening.\" The journal \"Nature\" tempered a generally positive review with a concern that the authors' sense of certainty \"mirrors that of the proponents of alternative therapies, leaving each position as entrenched as ever.\"\n\n\"Trick or Treatment\" drew criticism from consumers and practitioners of alternative therapies. The \"British Journal of General Practice\" published a review by Jeremy Swayne (former dean of the Faculty of Homeopathy) that was critical of the book and its argument.\n\nA review by Harriet A. Hall on Quackwatch stated that some negative reviews of \"Trick or Treatment\" demonstrated \"an appalling poverty of thought\"; articulating that since the reasoning behind the author's conclusions is solid, critics instead deny the methods of science, misrepresent the book's contents and use ad hominem attacks against the authors.\n\nSingh was sued for libel by the British Chiropractic Association for comments he wrote in a column in \"The Guardian\" about the book. In 2010, after 2 years, the BCA dropped the case after the court of the appeal found that Singh was expressing opinion, rather than stating facts. The presiding judges commented that \"this litigation has almost certainly had a chilling effect on public debate which might otherwise have assisted potential patients to make informed choices about the possible use of chiropractic\".\n\n\n"}
{"id": "24351081", "url": "https://en.wikipedia.org/wiki?curid=24351081", "title": "Zamani Project", "text": "Zamani Project\n\nThe Zamani Project is part of the African Cultural Heritage Sites and Landscapes Database. Zamani is a research group at the University of Cape Town, they acquire, model, present and manage spatial and other data of cultural heritage sites. The present focus of the Zamani project is Africa, with the principal objective of developing “The African Cultural Heritage Sites and Landscapes Database”. Zamani comes from the Swahili phrase “Hapo zamani za kale” which means “Once upon a time”, and can be used to mean 'the past'. The word is derived from Arabic root for temporal vocabulary, ‘Zaman,’ and appears in several languages around the world.\n\nThe Zamani initiative was conceptualised in the Geomatics Division of the University of Cape Town by Professor Heinz Rüther in 2001 in collaboration with ITHAKA and Aluka [now an initiative of JSTOR] as the “African Cultural Heritage Sites and Landscapes Database” in 2004 with a number of sequential grants from the Andrew W. Mellon Foundation. The project developed out of a long history of heritage documentation in the Geomatics division reaching from conventional mapping of archaeological sites in the early stages of the project to advanced digital modeling of complex sites in its present phase.\n\nThe documentation project aims to capture spatial information to create a permanent record of important heritage sites for restoration and conservation purposes and as a record for future generations. The project seeks to provide material for education, research and site management and increase international awareness of African heritage on a not-for-profit basis.\n\nSpatial data of architectural structures and historical landscapes are acquired by means of laser scanning, conventional surveys, GPS surveys and photogrammetric imaging with calibrated cameras. Satellite images, aerial photography and full-dome panorama photography are also employed as are contextual photography and videos. The data are captured by the project team during field campaigns. The acquired data are processed to produce Geographic Information Systems (GIS), 3D computer models, maps, architectural sections and building plans and interactive panorama tours of the heritage sites. Sites are seen in the context of their physical environment and therefore landscapes surrounding sites are modelled in 3D using satellite and aerial imagery wherever possible.\n\nThe following is a list of sites which have been documented:\n\nCameroon\n\nEthiopia\n\nGhana\n\nKenya\n\nMali\n\nMozambique\n\nSouth Africa\nSudan\nTanzania\nUganda\nZimbabwe\n\nThese sites have been documented by the project team but not included in the database \n1. Algeria, Tassili: Engravings (TARA)\n2. Egypt, Luxor: Valley of the Queens (Getty Conservation Institute, L.A.)\n3. Jordan, Petra: SIQ canyon and tombs in co-operation with UNESCO\n4. Tanzania, Laetoli: Hominid Trackway (Getty Conservation Institute, L.A.)\n5. United Arab Emirates, Al Ain: Archaeological site (ADACH, Abu Dhabi)\n\nThe database is conceptualised as a holistic system and therefore spatial data acquired by the Zamani group are augmented by contextual non-spatial data contributed to JSTOR by various additional partners and organizations. This entire set of materials is made available to the scholarly community via JSTOR's Aluka initiative. Access to this collection is free to all institutions and organizations in Africa and portions of the developing world. Libraries and academic institutions outside of Africa can license access to these collections via JSTOR. The Zamani Project website provides a showcase for the data it produces. Subsets of the 3D data, such as panorama tours and flythrough animations can be viewed on the site as well as examples of plans, sections and photographs.\n\nThe African Cultural Heritage Sites and Landscapes Database exclusively funded by the Andrew W. Mellon Foundation, New York. Administrative support, office space and other academic services are provided by the University of Cape Town and the Geomatics Division at UCT. Additional funds are generated through documentation work for the Getty Conservation Institute, Los Angeles and the World Monuments Fund, New York. In the early stages of the project UNESCO provided financial support for documentation in Lalibela.\n\nThe Zamani team members comprises three Chief Scientific Officers (Ralph Schroeder, Roshan Bhurtha and Stephen Wessels) under the leadership of the Principal Investigator, Prof. Heinz Rüther.\n\n\n"}
