{"id": "14862139", "url": "https://en.wikipedia.org/wiki?curid=14862139", "title": "3C 319", "text": "3C 319\n\n3C 319 is a radio galaxy with a quasar-like appearance located in the constellation Draco.\n\n"}
{"id": "16711712", "url": "https://en.wikipedia.org/wiki?curid=16711712", "title": "A Contract with the Earth", "text": "A Contract with the Earth\n\nA Contract with the Earth is a book by Newt Gingrich and Terry L. Maple, with a foreword by E. O. Wilson. Its title is derived from a 10-point \"contract\" the authors put forward in the book.\n\n\"A Contract with the Earth\" is, broadly, a manifesto that challenges those on the right to provide a strategy for repairing the planet and calls on government to embrace the concept that a healthy environment is required for a healthy democracy and economy. This approach, alternately branded mainstream and entrepreneurial environmentalism by the authors,\n\nWith its 10 \"commandments\", \"A Contract with the Earth\" calls for politicians to abandon adversarial politics and for business and conservationists to form compatible partnerships. In one of the book's themes, Gingrich and Maple argue that environmental efforts shouldn't be exclusive to one political philosophy and reject the idea that free enterprise and a cleaner world are opposing forces.\n\nThe book generated a storm of media attention in late 2007 and early 2008 as the U.S. presidential campaign began to heat up. Gingrich in particular made numerous media appearances arguing that the Republican Party was losing popular support because their response to environmental policy was simply, as he put it, \"NO!\" Maple toured the country as Gingrich's stand-in, most notably before the Republicans for Environmental Protection (REP, www.repamerica.org) during their annual meeting (at which John McCain was endorsed as the most \"green\" of the Republican presidential candidates). In 2008 Gingrich published another book that advocated oil drilling, \"Drill Here, Drill Now, Pay Less\", and many pundits called his environmental commitment into question. However, this book's fifth chapter provided an argument for environmental protection. Like many aspects of Gingrich's career, his interest in environmental issues has generated controversy.\n\nThe book, whose title is similar to Gingrich's co-authored book \"Contract with America\", criticizes the Democratic Party's legislation and litigation on environmental protection issues.\n\nGingrich has been described by Katharine Mieszkowski as a \"green conservative.\" He is the former Speaker of House of Representatives, and Maple is president and CEO of the Palm Beach Zoo and professor of conservation and behavior at the Georgia Institute of Technology. Wilson is a prize-winning conservation biologist and author.\n\n"}
{"id": "28913468", "url": "https://en.wikipedia.org/wiki?curid=28913468", "title": "Archer Glacier", "text": "Archer Glacier\n\nArcher Glacier () is a glacier flowing northwest into the head of Bolson Cove, Flandres Bay, on the west coast of Graham Land. It was first charted by the Belgian Antarctic Expedition under Adrien de Gerlache, 1897–99, and named by the United Kingdom Antarctic Place-Names Committee in 1960 for Frederick Scott Archer, an English architect who in 1849 invented the wet collodion process of photography, the first practical process on glass.\n\n"}
{"id": "7279337", "url": "https://en.wikipedia.org/wiki?curid=7279337", "title": "Center for Meteorite Studies", "text": "Center for Meteorite Studies\n\nThe Center for Meteorite Studies was founded in 1960, on the Tempe Campus of Arizona State University, and houses the world's largest university-based meteorite collection. The collection contains specimens from over 1,600 separate meteorite falls and finds, and is actively used internationally for planetary, geological and space science research. The Center also operates a meteorite museum which is open to the public.\n\n\n"}
{"id": "9866953", "url": "https://en.wikipedia.org/wiki?curid=9866953", "title": "Comet Galaxy", "text": "Comet Galaxy\n\nThe Comet Galaxy, a spiral galaxy located 3.2 billion light-years from Earth, in the galaxy cluster Abell 2667, was found with the Hubble Space Telescope. This galaxy has slightly more mass than our Milky Way. It was detected on 2 March 2007.\n\nThis unique spiral galaxy, which is situated 3.2 billion light-years from the Earth, has an extended stream of bright blue knots and diffuse wisps of young stars. It rushes at 3.5 million km/h through the cluster Abell 2667 and therefore, like a comet, shows a tail, with a length of 600,000 light-years.\n\nThe Сomet Galaxy is currently being ripped to pieces, moving through a cluster at speeds of greater than 2 million mph. As the galaxy speeds through, its gas and stars are being stripped away by the tidal forces exerted by the cluster - just as the tidal forces exerted by the Moon and Sun push and pull the Earth's oceans. Also contributing to this destructive process is the pressure of the cluster's hot gas plasma reaching temperatures as high as 100 million degrees. Scientists estimate that the total duration of the transformation process is close to one billion years. What is seen now in the Hubble's image is roughly 200 million years into the process. Even though the Comet Galaxy’s mass is slightly greater than the Milky Way, it will lose all its gas and dust, and so not be able to generate stars later in life. It will become a gas-poor galaxy with an old population of red stars.\n\nDuring the ram pressure stripping process, the charged particles strip and push away the infalling galaxy’s gas, just as the solar wind of charged particles pushes ionized gas away from a comet to create a gas tail. For this reason the scientists have nicknamed the stretched spiral the \"comet galaxy.\"\n\n\"This unique galaxy, situated 3.2 billion light-years from Earth, has an extended stream of bright blue knots and diffuse wisps of young stars driven away by the tidal forces and the ram pressure stripping' of the hot dense gas,\" said Jean-Paul Kneib, a study collaborator from the Laboratoire d'Astrophysique de Marseille.\n\nEven though its mass is slightly larger than that of the Milky Way, the spiral will inevitably lose all its gas and dust as well as its chance of generating new stars later, and become a gas-poor galaxy with an old population of red stars. The finding sheds light on the process by which gas-rich galaxies might evolve into gas-poor galaxies over billions of years. The new observations also reveal one mechanism for forming of “homeless” stars seen scattered throughout galaxy clusters.\n\nThe strong gravitational pull exerted by the galaxy cluster's collective mass has bent the light of other, more distant galaxies and distorted their shapes - an effect called gravitational lensing. The giant bright banana-shaped arc seen just to the left of the cluster center corresponds to the magnified and distorted image of a distant galaxy located behind the cluster's core.\n\n\n"}
{"id": "5597859", "url": "https://en.wikipedia.org/wiki?curid=5597859", "title": "Commensurability (astronomy)", "text": "Commensurability (astronomy)\n\nCommensurability is the property of two orbiting objects, such as planets, satellites, or asteroids, whose orbital periods are in a rational proportion.\n\nExamples include the 2:3 commensurability between the orbital periods of Neptune and Pluto, the 3:4 commensurability between the orbital periods of the Saturnian satellites Titan and Hyperion, the orbital periods associated with the Kirkwood gaps in the asteroid belt relative to that of Jupiter, and the 2:1 commensurability between Gliese 876 b and Gliese 876 c.\n\nCommensurabilities are normally the result of an orbital resonance, rather than being due to coincidence.\n\n"}
{"id": "14386680", "url": "https://en.wikipedia.org/wiki?curid=14386680", "title": "Committee on the Public Understanding of Science", "text": "Committee on the Public Understanding of Science\n\nThe Committee on the Public Understanding of Science or Copus was founded in 1985 by the British Association for the Advancement of Science (BAAS), the Royal Institution and the Royal Society. Its aim was to interpret scientific advances and make them more accessible to non-scientists.\n\nIt played a part in developing the public understanding of science it establishing standards for communicating science and technology\n\nThe Copus Grant Schemes was set up in 1987 and the last round of grants was for 2003/4. The scheme was funded by the Office of Science and Technology and the Royal Society. 25 grants worth a total of over £750,000 were awarded in 2003/2004.\n\nIn 2000 The new Copus Council was formed to be a more inclusive partnership for science communication in the UK. In 2002 following a report commissioned by the Office of Science and Technology the Copus Council was discontinued.\n"}
{"id": "22491755", "url": "https://en.wikipedia.org/wiki?curid=22491755", "title": "Coupled-wave method", "text": "Coupled-wave method\n\nIn physics, the coupled-wave method (CWM) is a method for analysing the interaction between two electromagnetic waves in a crystal or a grating.\n"}
{"id": "22226078", "url": "https://en.wikipedia.org/wiki?curid=22226078", "title": "Duke University Center for International Studies", "text": "Duke University Center for International Studies\n\nThe Duke University Center for International Studies (DUCIS) is an international studies national resource center housed within the John Hope Franklin Center for Interdisciplinary and International Studies on Duke University's west campus.\n\nThe current director is Gilbert W. Merkx. The executive director is Rob Sikorski.\n\nThe Duke University Center for International Studies provides salary support for instruction in Persian, Polish, Romanian, Turkish and Wolof. It provides additional academic year and summer funding for students to study a wider range of critical languages including Arabic, Czech, Hungarian and Russian.\n\nDUCIS' public programs include the University Seminar on Global Governance and Democracy, a popular evening seminar series which draws speakers from across the globe, to present in-progress research on a variety of subjects, ranging from transnational banking trends, to regional election reform, to international concepts of justice.\n\nThe Duke University Center for International Studies is home to two national organizations: the Association of International Education Administrators and the Council of National Resource Centers.\n\n"}
{"id": "4312726", "url": "https://en.wikipedia.org/wiki?curid=4312726", "title": "Empressite", "text": "Empressite\n\nEmpressite is a mineral form of silver telluride, AgTe.\nIt is a rare, grey, orthorhombic mineral with which can form compact masses, rarely as bipyrimidal crystals.\n\nRecent crystallographic analysis has confirmed that empressite is a distinct mineral with orthorhombic crystal structure, different from the hexagonal AgTe with which empressite has been commonly confused in mineralogy literature. \nAt the same time, empressite does not appear on the equilibrium Ag-Te phase diagram, and therefore it is only metastable at ambient conditions. Given infinite time, it would phase separate into pure AgTe and pure Te.\n\nThe name empressite comes from the location of its discovery – the Empress Josephine mine, Saguache County, Colorado, US. It was first described in 1914.\n"}
{"id": "4245410", "url": "https://en.wikipedia.org/wiki?curid=4245410", "title": "Exformation", "text": "Exformation\n\nExformation (originally spelled \"eksformation\" in Danish) is a term coined by Danish science writer Tor Nørretranders in his book \"The User Illusion\" published in English 1998. It is meant to mean \"explicitly discarded information\". However, the term also has other meanings related to information, for instance \"useful and relevant information\" or a specific kind of information explosion.\n\nConsider the following phrase: \"the best horse at the race is number 7\". The information carried is very small, if considered from the point of view of information theory: just a few words. However let's assume that this phrase was spoken by a knowledgeable person, after a complex study of all the horses in the race, to someone interested in betting. The details are discarded, but the receiver of the information might get the same practical value of a complete analysis.\n\nEffective communication depends on a shared body of knowledge between the persons communicating. In using words, sounds, and gestures, the speaker has deliberately thrown away a huge body of information, though it remains implied. This shared context is called exformation.\n\nExformation is everything we do not actually say but have in our heads when, or before, we say anything at all - whereas information is the measurable, demonstrable utterance we actually come out with.\n\nIf someone is talking about computers, what is said will have more meaning if the person listening has some prior idea what a computer is, what it is good for, and in what contexts one might encounter one. From the information content of a message alone, there is no way of measuring how much exformation it contains.\n\nIn 1862 the author Victor Hugo wrote to his publisher asking how his most recent book, \"Les Misérables\", was getting on. Hugo just wrote \"?\" in his message, to which his publisher replied \"!\", to indicate it was selling well. This exchange of messages would have no meaning to a third party because the shared context is unique to those taking part in it. The amount of information (a single character) was extremely small, and yet because of exformation a meaning is clearly conveyed.\n\n\n"}
{"id": "3273564", "url": "https://en.wikipedia.org/wiki?curid=3273564", "title": "First-degree relatives", "text": "First-degree relatives\n\nA first-degree relative is one's offspring, sibling or parent. It constitutes a category of family members that largely overlaps with the term nuclear family, but without spouses. First-degree relatives are a common measure used to diagnose risks for common diseases by analyzing family history.\n\n"}
{"id": "1596342", "url": "https://en.wikipedia.org/wiki?curid=1596342", "title": "Floodgate effect", "text": "Floodgate effect\n\nA floodgate effect is situation in which a small action can result in a far greater effect with no easily discernible limit. The original analogy is that of a floodgate, which once opened, no matter how minutely, will allow water to flow from either side through the gate until both sides are balanced up. It may also be used to refer to the effect where, once a floodgate has been opened, water will gush out in a torrent through the gate, making it easier to continue to open the gate, but harder to close it.\n\nThe term is commonly used to illustrate a situation where a precedent will set the stage for repeated performances, the number of which is hard to control. Such an example can be illustrated as follows:\n\n\"The setting up of a redlight district here will create a floodgate effect, causing redlight districts to be set up elsewhere.\"\n"}
{"id": "9502303", "url": "https://en.wikipedia.org/wiki?curid=9502303", "title": "Flux-corrected transport", "text": "Flux-corrected transport\n\nFlux-corrected transport (FCT) is a conservative shock-capturing scheme for solving Euler equations and other hyperbolic equations which occur in gas dynamics, aerodynamics, and magnetohydrodynamics. It is especially useful for solving problems involving shock or contact discontinuities. An FCT algorithm consists of two stages, a transport stage and a flux-corrected anti-diffusion stage. The numerical errors introduced in the first stage (i.e., the transport stage) are corrected in the anti-diffusion stage. \n\n\nFully multidimensional flux-corrected transport algorithms for fluids\n\n"}
{"id": "39388728", "url": "https://en.wikipedia.org/wiki?curid=39388728", "title": "Global-scale Observations of the Limb and Disk", "text": "Global-scale Observations of the Limb and Disk\n\nThe Global-scale Observations of the Limb and Disk (GOLD) mission is a heliophysics Mission of Opportunity for NASA’s Explorers program. Led by Richard Eastes at the Laboratory for Atmospheric and Space Physics, which is located at the University of Colorado, Boulder, \"GOLD\" mission is to image the boundary between Earth and space in order to answer questions about the effects of solar and atmospheric variability of Earth's space weather. \"GOLD\" was one of 11 proposals selected, of the 42 submitted, for further study in September 2011. On 12 April 2013, NASA announced that \"GOLD\", along with the Ionospheric Connection Explorer (\"ICON\"), had been selected for flight in 2017. GOLD, along with its commercial host satellite SES-14, launched on 25 January 2018.\n\n\"GOLD\" is intended to perform a two-year mission imaging Earth's thermosphere and ionosphere from geostationary orbit. \"GOLD\" is a two-channel far-ultraviolet (FUV) imaging spectrograph built by the Laboratory for Atmospheric and Space Physics at the University of Colorado Boulder and flown as a hosted payload on the commercial communications satellite SES-14. Additional organizations participating in the \"GOLD\" mission include the National Center for Atmospheric Research, Virginia Tech, the University of California Berkeley, the University of Central Florida, Computational Physics Inc., the National Oceanic and Atmospheric Administration, the U.S. Naval Research Laboratory, Boston University, and Clemson University.\n\nIn June 2017, SES announced the successful integration of \"GOLD\" with the SES-14 satellite under construction at Airbus Defence and Space in Toulouse, France. \"GOLD\" was launched on 25 January 2018 at 22:20 UTC aboard Ariane 5 flight VA241 from the Guiana Space Centre.\n\nThe scientific objectives of the \"GOLD\" mission are to determine how geomagnetic storms alter the temperature and composition of Earth’s atmosphere, to analyze the global-scale response of the thermosphere to solar extreme-ultraviolet variability, to investigate the significance of atmospheric waves and tides propagating from below the temperature structure of the thermosphere and to resolve how the structure of the equatorial ionosphere influences the formation and evolution of equatorial plasma density irregularities.\nThe viewpoint provided by \"GOLD\"’s geostationary orbit – from which the same hemisphere is always observable – is a new perspective on the Earth’s upper atmosphere. This viewpoint allows local time, universal time and longitudinal variations of the thermosphere and ionosphere's response to the various forcing mechanisms to be uniquely determined.\n\n"}
{"id": "18813809", "url": "https://en.wikipedia.org/wiki?curid=18813809", "title": "Histcite", "text": "Histcite\n\nHistCite is a software package used for bibliometric analysis and information visualization. It was developed by Eugene Garfield, the founder of the Institute for Scientific Information and the inventor of important information retrieval tools such as \"Current Contents\" and the \"Science Citation Index\". \n\nThe main purpose of the software is to make it easier for individuals to perform bibliometric analysis and visualization tasks. Bibliometric analysis is the use of the bibliographic information (titles, authors, dates, author addresses, references, etc.) that describe published items to measure and otherwise study various aspects of a specific field of scholarly endeavor. \n\nSome typical questions asked by bibliometricians that can be answered by HistCite analysis are:\n\n\nThe answers to such questions are valuable to researchers, librarians, and administrators.\n\nInformation visualization is the transformation of non-numerical data into a graphic format. Visualization helps various researchers and scholars understand large collections of information. Although there are numerous uses for information visualization, HistCite performs one specific application: it converts bibliographies into diagrams called historiographs. \n\nA historiograph is a time-based network diagram of the papers in a bibliography and their citation relationships to each other. Historiographs are based on the citation relationships between the papers in a bibliography. In a historiograph, each paper in the bibliography is represented by a symbol selected by the user. The symbols are arranged over a timeline of the publication dates of the papers. By changing the time frame of the analysis, the resulting historiograph can form a snapshot of a specific period or an in-depth look at the total history of a subject. Once a historiograph is created for a bibliography, it is easier to see and understand the subject’s key publication events, their chronology, and their relative influence.\n\nA wide variety of professionals who need to analyze the published literature use HistCite for analysis, for example: researchers, historians, journal editors, librarians and patent lawyers.\n\nIn order to perform its functions, HistCite must import a bibliography from another source. Once the bibliography is imported, basic point-and-click commands initiate the various analyses and visualizations. \n\nHistCite is currently set up to import bibliographies created by searches of the Web of Science database offered by Thomson-Reuters Scientific. Bibliographies from other sources can be manually entered into HistCite.\n\nThe bibliography the user feeds to HistCite represents the literature of the subject area as it is defined by that user’s unique perspective. Thus, the analyses and visualizations produced by HistCite from that bibliography are one-of-a-kind. \nHistCite operates on Windows computers with Internet Explorer.\n\n"}
{"id": "24207795", "url": "https://en.wikipedia.org/wiki?curid=24207795", "title": "Hong Kong Applied Science and Technology Research Institute", "text": "Hong Kong Applied Science and Technology Research Institute\n\nThe Hong Kong Applied Science and Technology Research Institute is a public research institute in Hong Kong. It was established in 2000 by the Government of Hong Kong, as part of the development of Hong Kong Science Park. In 2018, Its purpose is to foster the development of high-technology industry in Hong Kong and in southern China, in five key areas, including financial technology, smart cities, next-generation communications networks, smart manufacturing and health technology. In its first ten years of activity, it facilitated more than 360 transfers of technology, and registered about 130 patents.\n\nASTRI is currently the largest government-funded scientific research institute in Hong Kong. Its primary objective is to conduct forward-looking and applied scientific research, transfer technology to different industries, and nurture skilled research personnel. ASTRI plays an important role in the value chain of science and technology, and contacts various R&D resources in public organizations, private companies, and academic institutions. ASTRI registers as a private limited company in Hong Kong and is supervised by the Innovation and Technology Commission under the Innovation and Technology Bureau of the Government of Hong Kong. \n\nASTRI is headed by the Chief Executive Officer and is governed by a board of directors from the business and academic worlds of Hong Kong and Hong Kong SAR government representatives. The Board of Directors has established three functional committees, namely the Finance and Administration Committee, the Science and Technology Committee and the Audit Committee to assist the Board of Directors in governing the Academy.\n\nThe current board members include:\n\nChairman: Mr. Wang Mingxin, Bronze Bauhinia Star, Justice of the Peace\nDirectors: Ms. Chen Shanshan, Mr. Zheng Zhuorong, Professor Qian Dakang, Bronze Bauhinia Star, Professor of Peace, Professor Cheng Bozhong, Bronze Bauhinia Star, Mr. Qiu Dagen, Mr. Cai Chuqing, Mr. Tai Ping, Mr. Cai Shaozhou, Mr. Xia Yongquan, Bronze Bauhinia Star, Zhai Zhiqiang Mr., JP, Dr. Lin Xiaofeng, Engineer, Mr. Li Huiguang, Mr. Taiping, Mr. Liu Anting, Dr. Luo Guowei, Ms. Xiao Jieyun, Mr. Xie Diyang and Mr. Huang Pingda\nOfficer: Mr. Chung Wing Hing, Permanent Secretary for Innovation and Technology, Ms. Choi Siu-yiu, Director of the Justice and Innovation and Technology Commission, JP .\n\nChief Executive Officer : Mr. Zhou Xianben\nChief Technology Officer: Dr. Yang Meiji\nChief Executive Officer: Ms. Xu Yiting\nChief Financial Officer: Ms. Rong Huiqi\n\nASTRI has 7 major scientific research areas, which are: Mixed signal system chip, financial technology, blockchain and big data analytics, intelligent manufacturing, next-generation communication networks, health technology, and smart city. \n\nASTRI has established multiple R&D laboratories in response to collaboration with other agencies or for specific objectives, including:\n\nNational ASIC System Engineering Technology Research Center Hong Kong Branch: Focusing on the research and development of microelectronics and integrated circuits and systems, conducting scientific research, engineering transformation and personnel training in the three directions of hybrid signal system chip, advanced digital system and package design jobs.\n\nASTRI Cyber Security Research and Training Centre: The first research and training centre in Hong Kong that can monitor and simulate cyber attacks everywhere. It was established by the ASTRI and the Hong Kong Police Force, with the goal of training for law enforcement and financial security personnel in financial institutions.\n\nASTRI Network Security Institute: Research and Development and Knowledge Sharing Institute for Network Security. ASTRI's R&D team conducts in-depth investigations of the latest cyber attacks and exchanges information through ASTRI's cybersecurity information sharing platform \"ASLintel\".\n\nASTRI-Hong Kong International Airport Intelligent Airport Joint R&D Center\n\nHKMA - ASTRI Financial Technology Innovation Centre\n\nBank of China (Hong Kong) - ASTRI Joint Innovation Center for Finance and Technology\n\nHSBC - ASTRI Joint Innovation Lab\n"}
{"id": "49753691", "url": "https://en.wikipedia.org/wiki?curid=49753691", "title": "Itelmenite", "text": "Itelmenite\n\nItelmenite is a rare sulfate mineral with the formula NaMgCu(SO). It is one of many fumarolic minerals discovered on the Tolbachik volcano.\n\nSaranchinaite and dravertite are examples of other anhydrous complex copper-bearing sulfates, also coming from the Tolbachik volcano.\n\n"}
{"id": "51638205", "url": "https://en.wikipedia.org/wiki?curid=51638205", "title": "Jan Drenth", "text": "Jan Drenth\n\nJan Drenth (born 20 February 1925) is a Dutch chemist. He was a professor of structural chemistry at the University of Groningen from 1969 to 1990.\n\nDrenth was born in Groningen. He obtained his PhD in mathematics and physics under Eelco Wiebenga at the University of Groningen in 1957, with a dissertation titled: \"Een röntgenografisch onderzoek van excelsine, edestine en tabakszaadglobuline\". Drenth subsequently moved to New York, United States, where he became a post-doc and studied protein crystallography under Barbara Low. Drenth then returned to the Netherlands and in 1967 was appointed as lector. In 1969 he was appointed as professor of structural chemistry, which he remained until his retirement in 1990.\n\nHe was elected a member of the Royal Netherlands Academy of Arts and Sciences in 1973.\n\n"}
{"id": "21784990", "url": "https://en.wikipedia.org/wiki?curid=21784990", "title": "Jonathan Spira", "text": "Jonathan Spira\n\nJonathan B. Spira (born 1961) is a researcher and industry analyst known for his work in the area of collaboration and knowledge sharing and the problem of information overload.\n\nSpira was born in New York and grew up in New York City and Vienna, the son of photographic pioneer and his wife Marilyn (née Hacker). He studied Central European History at the University of Pennsylvania where he was a member of the Pi Lambda Phi fraternity and conducted his graduate studies at the University of Munich (Ludwig-Maximilians Universität)\n\nSpira began his career in business and technology while still in high school when he became involved in the management of office systems at Spiratone, a company founded and run by . After completing his studies in 1983, he founded a research and IT advisory firm, Basex (originally called The Basex Group) in 1983, that focused on helping organizations understand how knowledge workers work and what they can do to manage them more effectively. He has been associated with the firm ever since. Though some may consider that significant, Spira's career has not had sufficient enough impact to meet Wikipedia's ,and this page, which is arguably the most tangible evidence of his impact, will likely be deleted or merged.\n\nSpira's research focuses on the problems organizations are having as they migrate from the industrial age to the knowledge economy and what managers can do to remain competitive. According to Knowledge Management in the Public Sector, the main thrust of Spira's arguments is that software companies should develop new systems that are designed from the beginning for knowledge and information work but collaboration and knowledge sharing are \"less a question of technology than of systems that facilitate people working together.\"\n\nSpira started researching the problem of information overload in the early 1990s and was interviewed in March 1993 by CNBC on the topic. In 2003, he published research that assigned specific costs to components of the problem of information overload and in 2008 published an estimate of the cost of information overload to the U.S. economy ($900 billion per annum). In 2008, he helped found the Information Overload Research Group, a business and academic consortium that is working to bring attention to the problem and potential solutions.\n\nSpira is the author of Overload! How Too Much Information Is Bad For Your Organization, published by Wiley in 2011. He is also the co-author of The History of Photography (Aperture, 2001), named a \"best book of the year\" by the New York Times when it came out. He is the author of Managing the Knowledge Workforce: Understanding the Information Revolution That's Changing the Business World (Mercury Business Press, 2005). He has also written several major reports on information overload including Information Overload: We Have Met the Enemy and He Is Us. He is regularly quoted by publications including the New York Times, Wall Street Journal, the Financial Times, Business Week, among others.\n\n"}
{"id": "47908879", "url": "https://en.wikipedia.org/wiki?curid=47908879", "title": "Jonny Holmstrom", "text": "Jonny Holmstrom\n\nJonny Holmstrom is a Swedish professor of Informatics at Umeå University and director and co-founder of Swedish Center for Digital Innovation.\n\nHolmstrom was born in Arvidsjaur in 1968. He received his Ph.D. from Umeå University in 2000, and has been a visiting scholar at Georgia State University and Florida International University. Holmstrom is a senior editor at Information and Organization and serves at the editorial boards for European Journal of Information Systems and Communications of the AIS. Holmstrom has written 2 books and has published over 100 scholarly articles in professional journals and edited volumes. \n\nHolmstrom is a professor of Informatics at Umeå University and director and co-founder of the Swedish Center for Digital Innovation. His work has appeared in journals including Communications of the AIS, Convergence, Design Issues, European Journal of Information Systems, Industrial Management and Data Systems, Information and Organization, Information Resources Management Journal, Information Systems Journal, Information Technology and People, Journal of the AIS, Journal of Strategic Information Systems, Research Policy, and The Information Society.\n\nHolmstrom’s early work examined the interaction between information technology and organizations. Prior research often assumed information technology to be either an objective, external force with deterministic impacts on organizations, or the outcome of social action. Holmstrom’s research suggested that either view is incomplete, and he proposed to take both perspectives into account. Drawing from actor-network theory as analytical lens, Holmstrom has explored the role of IT in contexts such as municipal organizations, airports, and digital cash projects. In these studies, Holmstrom stressed the notion of non-human agency in which processes, technological tools and other similar concepts can be viewed as non-human actors that acquire an identity of their own.\n\nIn more recent research, Holmstrom has addressed digital innovation and digital transformation, specifically how organizations are dealing with “the digitization of everything” as a challenge and an opportunity. To deal with digital transformation, Holmstrom argues that organizations need to develop a comprehensive digital strategy. Holmstrom’s research addresses how digital capabilities increasingly determines which companies create or lose value. Among these studies we find studies of firms in the mining industry, the paper and pulp industry, and the publishing industry. He also published a Harvard Business School case focusing on the ways in which digitization brings challenges as well as opportunities to firms in the publishing industry.\n\nHolmstrom was awarded a post-doctoral fellowship by the STINT Foundation (The Swedish Foundation for International Co-operation in Research and Higher Education) for post-doctoral research at Georgia State University, Atlanta, GA in 2000/2001. He was also awarded a fellowship by the STINT Foundation to enable a visiting research position at Decision Sciences and Information Systems Department, College of Business Administration, Florida International University, Miami, FL, USA, in 2004. Holmstrom received Royal Skytteanska Samfundets award for young researchers at the Social Sciences faculty, Umeå University, in 2005, and Nordea’s Scientific Award 2007. In 2009 he received Umeå University’s Young Researcher Award and the AIS senior scholars award for best IS journal paper of the year in 2010. Holmstrom was also awarded with the Pedagogical Award of the Year in the IS discipline in Sweden 2011 by the Swedish Association of Information Systems.\n\n"}
{"id": "2204442", "url": "https://en.wikipedia.org/wiki?curid=2204442", "title": "Liroconite", "text": "Liroconite\n\nLiroconite is a complex mineral: Hydrated copper aluminium arsenate hydroxide, with the formula CuAl[(OH)|AsO]·4(HO). It is a vitreous monoclinic mineral, colored bright blue to green, often associated with malachite, azurite, olivenite, and clinoclase. It is quite soft, with a Mohs hardness of 2 - 2.5, and has a specific gravity of 2.9 - 3.0.\n\nIt was first identified in 1825 in the tin and copper mines of Devon and Cornwall, England. Although it remains quite rare it has subsequently been identified in a variety of locations including France, Germany, Australia, New Jersey and California.\n\nThe type locality for Liroconite is Wheal Gorland in St Day, Cornwall in the United Kingdom.\n\nIt occurs as a secondary mineral in copper deposits in association with olivenite, chalcophyllite, clinoclase, cornwallite, strashimirite, malachite, cuprite and limonite.\n"}
{"id": "29235275", "url": "https://en.wikipedia.org/wiki?curid=29235275", "title": "List of Russian biologists", "text": "List of Russian biologists\n\nThis list of Russian biologists includes the famous biologists from the Russian Federation, the Soviet Union, the Russian Empire and other predecessor states of Russia. Biologists of all specialities may be listed here, including ecologists, botanists, zoologists, paleontologists, biochemists, physiologists and others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14560779", "url": "https://en.wikipedia.org/wiki?curid=14560779", "title": "List of countries by national capital, largest and second-largest cities", "text": "List of countries by national capital, largest and second-largest cities\n\nThis is a list of the largest and second-largest cities by population in each country. If a territory or region of a certain country is listed, the name of the country is specified in parentheses immediately after the name of the territory.\n\nThis is a list of sovereign states whose capital is not their largest city. \n</ref>\n"}
{"id": "30993486", "url": "https://en.wikipedia.org/wiki?curid=30993486", "title": "List of historic monuments in Romania", "text": "List of historic monuments in Romania\n\nThis list of historic monuments in Romania includes major sites from the National Register of Historic Monuments in Romania which was created between 2004 and 2005. The National Register contains 29,540 Heritage sites are entered in the National Cultural Heritage of Romania and it is maintained by the Romanian National Institute of Historical Monuments, part of the Ministry of Culture and National Patrimony Romania.\n\n\n\n\n"}
{"id": "47474831", "url": "https://en.wikipedia.org/wiki?curid=47474831", "title": "List of mechanical keyboards", "text": "List of mechanical keyboards\n"}
{"id": "41434426", "url": "https://en.wikipedia.org/wiki?curid=41434426", "title": "List of social thinkers", "text": "List of social thinkers\n\nThis article provides a list of social thinkers.\n\nThe title social thinker denotes a person who is acknowledged as a visionary for social advancement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13536941", "url": "https://en.wikipedia.org/wiki?curid=13536941", "title": "List of vacuum tube computers", "text": "List of vacuum tube computers\n\nVacuum tube computers, now termed first generation computers, are programmable digital computers using vacuum tube logic circuitry.\nThey were preceded by systems using electromechanical relays and followed by systems built from discrete transistors. Later entries in this list may have been built using transistors in addition to vacuum tubes.\n\nThis is a list of vacuum tube computers, arranged by date put into service:\n"}
{"id": "558750", "url": "https://en.wikipedia.org/wiki?curid=558750", "title": "Lumpers and splitters", "text": "Lumpers and splitters\n\nLumpers and splitters are opposing factions in any discipline that has to place individual examples into rigorously defined categories. The lumper-splitter problem occurs when there is the need to create classifications and assign examples to them, for example schools of literature, biological taxa and so on. A \"lumper\" is an individual who takes a gestalt view of a definition, and assigns examples broadly, assuming that differences are not as important as signature similarities. A \"splitter\" is an individual who takes precise definitions, and creates new categories to classify samples that differ in key ways.\n\nThe earliest known use of these terms was by Charles Darwin, in a letter to J. D. Hooker in 1857 (\"It is good to have hair-splitters & lumpers.\") They were introduced more widely by George G. Simpson in his 1945 work \"The Principles of Classification and a Classification of Mammals\". As he put it, \"splitters make very small units – their critics say that if they can tell two animals apart, they place them in different genera ... and if they cannot tell them apart, they place them in different species. ... Lumpers make large units – their critics say that if a carnivore is neither a dog nor a bear, they call it a cat.\"\n\nAnother early use can be found in the title of a 1969 paper by the medical geneticist Victor McKusick: \"On lumpers and splitters, or the nosology of genetic disease\".\n\nReference to lumpers and splitters also appeared in a debate in 1975 between J. H. Hexter and Christopher Hill, in the \"Times Literary Supplement\". It followed from Hexter's detailed review of Hill's book \"Change and Continuity in Seventeenth Century England\", in which Hill developed Max Weber's argument that the rise of capitalism was facilitated by Calvinist Puritanism. Hexter objected to Hill's \"mining\" of sources to find evidence that supported his theories. Hexter argued that Hill plucked quotations from sources in a way that distorted their meaning. Hexter explained this as a mental habit that he called \"lumping\". According to him, lumpers rejected differences and chose to emphasize similarities. Any evidence that did not fit their arguments was ignored as aberrant. Splitters, by contrast, emphasised differences, and resisted simple schemes. While lumpers consistently tried to create coherent patterns, splitters preferred incoherent complexity.\n\nThe categorization and naming of a particular species should be regarded as a \"hypothesis\" about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two named species are agreed to be of the same species, the older species name is almost always retained dropping the newer species name honoring a convention known as \"priority of nomenclature\". This form of lumping is technically called synonymization. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as \"lumpers\" or \"splitters\" by their colleagues, depending on their personal approach to recognizing differences or commonalities between organisms.\n\nIn history, lumpers are those who tend to create broad definitions that cover large periods of time and many disciplines, whereas splitters want to assign names to tight groups of inter-relationships. Lumping tends to create a more and more unwieldy definition, with members having less and less mutually in common. This can lead to definitions which are little more than conventionalities, or groups which join fundamentally different examples. Splitting often leads to \"distinctions without difference\", ornate and fussy categories, and failure to see underlying similarities.\n\nFor example, in the arts, \"Romantic\" can refer specifically to a period of German poetry roughly from 1780–1810, but would exclude the later work of Goethe, among other writers. In music it can mean every composer from Hummel through Rachmaninoff, plus many that came after.\n\nSoftware engineering often proceeds by building models (sometimes known as model-driven architecture). A lumper is keen to generalize, and produces models with a small number of broadly defined objects. A splitter is reluctant to generalize, and produces models with a large number of narrowly defined objects. Conversion between the two styles is not necessarily symmetrical. For example, if error messages in two narrowly defined classes behave in the same way, the classes can be easily combined. But if some messages in a broad class behave differently, every object in the class must be examined before the class can be split. This illustrates the principle that \"splits can be lumped more easily than lumps can be split\".\n\nThere is no agreement among historical linguists about what amount of evidence is needed for two languages to be safely classified in the same language family. For this reason, many language families have had lumper–splitter controversies, including Altaic, Pama–Nyungan, Nilo-Saharan, and most of the larger families of the Americas. At a completely different level, the splitting of a mutually intelligible dialect continuum into different languages, or lumping them into one, is also an issue that continually comes up, though the consensus in contemporary linguistics is that there is no completely objective way to settle the question.\n\nSplitters regard the comparative method (meaning not comparison in general, but only reconstruction of a common ancestor or protolanguage) as the only valid proof of kinship, and consider genetic relatedness to be the question of interest. American linguists of recent decades tend to be splitters.\n\nLumpers are more willing to admit techniques like mass lexical comparison or lexicostatistics, and mass typological comparison, and to tolerate the uncertainty of whether relationships found by these methods are the result of linguistic divergence (descent from common ancestor) or language convergence (borrowing). Much long-range comparison work has been from Russian linguists like Vladislav Illich-Svitych and Sergei Starostin. In the US, Greenberg's and Ruhlen's work has been met with little acceptance from linguists. Earlier American linguists like Morris Swadesh and Edward Sapir also pursued large-scale classifications like , accompanied by controversy similar to that today.\n\nPaul F. Bradshaw suggests that the same principles of lumping and splitting apply to the study of early Christian liturgy. Lumpers, who tend to predominate, try to find a single line of texts from the apostolic age to the fourth century (and later). Splitters see many parallel and overlapping strands which intermingle and flow apart so that there is not a single coherent path in development of liturgical texts. Liturgical texts must not be taken solely at face value; often there are hidden agendas in texts.\n\nThe Hindu religion is essentially a lumper's concept, sometimes also known as Smartism. Hindu splitters, and individual adherents, often identify themselves as adherents of a religion such as Shaivism, Vaishnavism, or Shaktism according to which deity they believe to be the supreme creator of the universe.\n\nFreeman Dyson has suggested that \"observers of the philosophical scene\" can be broadly, if over-simplistically, divided into splitters and lumpers, roughly corresponding to materialists, who imagine the world as divided into atoms, and Platonists, who regard the world as made up of ideas.\n\n\n"}
{"id": "3148927", "url": "https://en.wikipedia.org/wiki?curid=3148927", "title": "Major actinide", "text": "Major actinide\n\nMajor actinides is a term used in the nuclear power industry that refers to the plutonium and uranium present in used nuclear fuel, as opposed to the minor actinides neptunium, americium, curium, berkelium, and californium.\n"}
{"id": "4298660", "url": "https://en.wikipedia.org/wiki?curid=4298660", "title": "Mausumi Dikpati", "text": "Mausumi Dikpati\n\nMausumi Dikpati is a scientist at the High Altitude Observatory operated by the National Center for Atmospheric Research.\n\nIn March 2006, she was the first person to predict the strength and timing of the next solar cycle based on simulations of the physics of the solar interior. During 2006-2007 Mausumi Dikpati issued three predictions for solar cycle 24 -- (i) a delayed onset of solar cycle 24 which would start in late 2008 instead of 2006, (ii) a strong solar cycle 24 whose peak would be 30%-50% stronger than the previous cycle ('Cycle 23'), and (iii) the solar cycle in southern hemisphere would be stronger than that in the northern hemisphere of the Sun. Two of these three predictions, (i) and (iii) have been validated. Her research paper explaining the cause of delayed onset of solar cycle 24 was one of the top 100 discoveries in the Discover Magazine Currently she is intensely involved in improving her solar dynamo model for building a more accurate dynamo-based solar cycle prediction tool which can assimilate solar magnetic fields and flow data in a sophisticated way as used in oceanic and atmospheric predictions, such as El Nino.\n\n"}
{"id": "1476307", "url": "https://en.wikipedia.org/wiki?curid=1476307", "title": "Memory foam", "text": "Memory foam\n\nMemory foam consists mainly of polyurethane as well as additional chemicals increasing its viscosity and density. It is often referred to as \"viscoelastic\" polyurethane foam, or low-resilience polyurethane foam (LRPu). The foam bubbles or ‘cells’ are open, effectively creating a matrix through which air can move. Higher-density memory foam softens in reaction to body heat, allowing it to mold to a warm body in a few minutes. Newer foams may recover more quickly to their original shape.\n\nMemory foam was developed in 1966 under a contract by NASA's Ames Research Center to improve the safety of aircraft cushions. The temperature-sensitive memory foam was initially referred to as \"slow spring back foam\"; Yost called it \"temper foam\". Created by feeding gas into a polymer matrix, the foam has an open-cell solid structure that matches pressure against it, yet slowly springs back to its original shape.\n\nLater commercialisation of the foam included use in both medical equipment such as X-ray table pads and sports equipment such as American / Canadian football helmet liners.\n\nWhen NASA released memory foam to the public domain in the early 1980s, Fagerdala World Foams was one of the few companies willing to work with the foam, as the manufacturing process remained difficult and unreliable. Their 1991 product, the \"Tempur-Pedic Swedish Mattress\" eventually led to the mattress and cushion company, Tempur World.\n\nMemory foam was subsequently used in medical settings. For example, it was commonly used in cases where the patient was required to lie immobile in their bed on a firm mattress for an unhealthy period of time. The pressure on some of their body regions impaired the blood flow to the region, causing pressure sores or gangrene. Memory foam mattresses significantly decreased such events. Claims have also been made that memory foam reduces the severity of pain associated with fibromyalgia.\n\nMemory foam was initially too expensive for widespread use, but became cheaper. Its most common domestic uses are mattresses, pillows, shoes and blankets. It has medical uses, such as wheelchair seat cushions, hospital bed pillows and padding for people suffering long-term pain or postural problems; for example, a memory foam cervical pillow may alleviate chronic neck pain. Its heat-retaining properties may help some pain sufferers who find the added warmth helps to decrease the pain.\n\nThe heat-retaining properties can also be a disadvantage when used in mattresses and pillows so in the second generation memory foam, companies began using open cell structure to improve breathability. In 2006, the third generation of memory foam was introduced. Gel visco or gel memory foam consists of gel particles fused with visco foam to reduce trapped body heat, speed up spring back time and help the mattress feel softer. This technology was originally developed and patented by Peterson Chemical Technology, and gel mattresses became popular with the release of Serta's iComfort line and Simmons' Beautyrest line in 2011. Gel-infused memory foam was next developed with what were described as \"beads\" containing the gel which, as a phase-change material, would achieve the desired temperature stabilization or cooling effect by changing from a solid to a liquid \"state\" within the capsule. Changing physical states can significantly alter the heat absorption properties of an element, which is why the technology was applied to memory foam.\n\nSince the development of gel memory foam, other materials have been added. Aloe vera, green tea extract and activated charcoal have been combined with the foam to reduce odors and even provide aromatherapy while sleeping. Rayon has been used in woven mattress covers over memory foam beds to wick moisture away from the body to increase comfort. Phase-change materials (PCMs) have also been used in the covers that are used on memory foam pillows, beds, and mattress pads.\n\nA memory foam mattress is usually denser than other foam mattresses, making it both more supportive and heavier. Memory foam mattresses are often sold for higher prices than traditional mattresses. Memory foam used in mattresses is commonly manufactured in densities ranging from less than 1.5 lb/ft to 8 lb/ft density.\n\nThe property of firmness (hard to soft) of memory foam is used in determining comfort. Firmness is measured by a foam's indentation force deflection (IFD) rating. However, it is not a complete measurement of a \"soft\" or \"firm\" \"feel\". A foam of higher IFD but lower density can feel soft when compressed.\n\nIFD measures the force (in pounds-force) required to make a dent 1 inch into a foam sample 15\" x 15\" x 4\" by an 8-inch-diameter (50 sq in) disc—known as IFD @ 25% compression. IFD ratings for memory foams range between super soft (IFD 10) and semi-rigid (IFD 12). Most memory foam mattresses are firm (IFD 12 to IFD 16).\n\nSome report that IFD is a poor way to measure softness of memory foam, and that foam density as a measure of quality is more important, but not always true. Foam density of 5 pounds per cubic foot (80 kg/m) or greater is considered high quality, although most standard memory foam has a density of 1 to 5 lb/ft (16–80 kg/m). Most bedding, such as topper pads and comfort layers in mattresses is 3 to 4.5 lb/ft. Very high densities such as 5.3 lb/ft (85 kg/m) are used infrequently in mattresses.\n\nThe new second and third generation memory foams have an open-cell structure that reacts to body heat and weight by 'molding' to the sleeper's body, helping relieve pressure points, preventing pressure sores, etc. Most memory foam has the same basic chemical composition, however the density and layer thickness of the foam makes different mattresses feel very different. A high-density mattress will have better compression ratings over the life of the bedding. A lower-density one will have slightly shorter life due to the compression that takes place after repeated use.\n\nEmissions from memory foam mattresses may directly cause more respiratory irritation than other mattresses. Memory foam, like other polyurethane products, can be combustible. Laws in several jurisdictions have been enacted to require that all bedding, including memory foam items, be resistant to ignition from an open flame such as a candle or cigarette lighter. US bedding laws that went into effect in 2010 change the Cal-117 Bulletin for FR testing. There is concern that high levels of the fire retardant PBDE, commonly used in memory foam, could cause health problems for users. PBDEs are no longer used in most bedding foams, especially in the European Union.\n\nManufacturers caution about leaving babies and small children unattended on memory foam mattresses, as they may find it difficult to turn over, and may suffocate.\n\nThe United States Environmental Protection Agency published two documents proposing National Emissions Standards for Hazardous Air Pollutants (HAP) concerning hazardous emissions produced during the making of flexible polyurethane foam products. The HAP emissions associated with polyurethane foam production include methylene chloride, toluene diisocyanate, methyl chloroform, methylene diphenyl diisocyanate, propylene oxide, diethanolamine, methyl ethyl ketone, methanol, and toluene however not all chemical emissions associated with the production of these material have been classified. Methylene chloride makes up over 98 percent of the total HAP emissions from this industry. Short-term exposure to high concentrations of methylene chloride also irritates the nose and throat. The effects of chronic (long-term) exposure to methylene chloride in humans involve the central nervous system, and include headaches, dizziness, nausea, and memory loss. Animal studies indicate that inhalation of methylene chloride affects the liver, kidney, and cardiovascular system. Developmental or reproductive effects of methylene chloride have not been reported in humans, but limited animal studies have reported lowered fetal body weights in rats exposed.\n\n"}
{"id": "12028435", "url": "https://en.wikipedia.org/wiki?curid=12028435", "title": "Mitsuru Hotta", "text": "Mitsuru Hotta\n\nHotta was born in Osaka, Japan in 1935. He graduated from the Agricultural Department of Osaka Prefecture University in 1960. The same year, he took part in the Tonga and Fiji Expedition organised by Kyoto University. Between 1963 and 1964, Hotta made numerous plant collections in Borneo together with Professor Minoru Hirano of Osaka City University.\n"}
{"id": "2415632", "url": "https://en.wikipedia.org/wiki?curid=2415632", "title": "Mobile Launcher Platform", "text": "Mobile Launcher Platform\n\nThe Mobile Launcher Platform (MLP) is one of three two-story steel structures used by NASA at the Kennedy Space Center to support the Space Shuttle stack throughout the build-up and launch process: during assembly at the Vehicle Assembly Building (VAB), while being transported to Launch Pads 39A and B, and as the vehicle's launch platform. NASA's three MLPs were originally constructed for the Apollo program to launch the Saturn V rockets in the 1960s and 1970s, and remained in service through the end of the shuttle program in 2011 with alterations. The Space Launch System rocket will be mounted atop a renovated platform.\n\nThe MLP was constructed for transporting and launching the Saturn V rocket for the Apollo program lunar landing missions of the 1960s and 1970s. Each MLP originally had a single exhaust vent for the Saturn V's engines. The MLPs also featured a Launch Umbilical Tower (LUT) with nine arms that permitted servicing of the vehicle on the launch pad, and swung away from it at launch. After the Apollo test program, Cape Canaveral Launch Complex 37 was decommissioned and launches of the Saturn IB rocket were moved to Kennedy Space Center Launch Complex 39B for the Skylab and Apollo-Soyuz programs. MLP No. 1 was therefore modified to add a four-legged pedestal (nicknamed \"the milkstool\" because of its resemblance to the stool used by farmers when milking cows by hand) for the shorter Saturn IB to stand on. This allowed use of the Saturn V tower and service arms for the Saturn IB, and Saturn V Ground Support Equipment (GSE) was removed or de-activated and Saturn IB GSE equipment was installed.\n\nAfter the Apollo program, the launcher platforms were modified for the Space Shuttle (Space Transport System). The umbilical towers from Mobile Launchers 2 and 3 were removed. Portions of these tower structures were erected at the two launch pads, 39A and 39B. These permanent structures are now known as the Fixed Service Structures (FSS). The umbilical tower from Mobile Launcher 1 (which was the platform used for the most significant Apollo missions) was taken apart and stored in the Kennedy Space Center's industrial area. Efforts to preserve it in the 1990s failed, however, for lack of funding and it was scrapped.\n\nIn addition to removal of the umbilical towers, each Shuttle-era MLP was extensively reconfigured with the addition of two Tail Service Masts, one on either side of the Main Engine exhaust vent. These masts contained the feed lines through which liquid hydrogen (LH) and liquid oxygen (LOX) were loaded into the shuttle's external fuel tank, as well as electrical hookups and flares that were used to burn off any ambient hydrogen vapors at the launch site immediately prior to Main Engine start.\n\nThe Space Shuttle main engines (SSMEs) vented their exhaust through the original opening used for the Saturn rocket exhaust. Two additional exhaust ports were added to vent exhaust from the solid rocket boosters that flanked the external fuel tank.\n\nThe Space Shuttle assembly was held to the MLP at eight holddown points using large studs, four on the aft skirt of each Solid Rocket Booster. Immediately before SRB ignition, frangible nuts attached to the top of these studs were detonated, releasing the Shuttle assembly from the platform.\n\nWhen NASA began launching Shuttle missions, it became clear that the MLP might inadvertently pose a danger to the crew or the vehicle, due to the possibility of massive acoustic shock waves bouncing off the platform and hitting the Shuttle as it lifted off. This was true for the Saturn V launches as well, but there was less risk because the Apollo spacecraft, atop the stack, was much farther away from the engines. Because the Shuttle was about half the height of the Saturn, the crew cabin and payload bay were much closer to the platform, and much more vulnerable to the tremendous forces bouncing back off the MLP - on the first mission, STS-1, the shock waves damaged many of the protective thermal tiles.\n\nNASA's solution to this danger was to cushion the MLP at every launch with a flood of flowing water. Starting 6.6 seconds before engine ignition, a water tower at the launch site began dumping water down a pipeline and into the exhaust vents of the MLP. Next, six -high towers known as \"rainbirds\" began to spray water over the MLP and into the flame deflector trenches below it. The water absorbed some of the bruising forces of the acoustic waves, and discouraged fires that might be caused by the rocket exhaust. This water-dumping mechanism, known as the Sound Suppression System, emptied the launch pad tank in around 41 seconds. The giant white clouds that billowed around the shuttle at each launch were not smoke, but water vapor generated as the rocket exhaust boiled away huge quantities of water. The suppression system reduced the acoustic sound level to approximately 142 dB.\n\nEach MLP weighs unloaded and roughly with an unfueled Shuttle aboard, measures , and is high. It was carried by a crawler-transporter, which measures , and is high. Each crawler weighs about unloaded, has a maximum speed of about per hour loaded, and has a leveling system designed to keep the launch vehicle vertical while negotiating the 5 percent grade leading to the top of the launch pad. Two diesel engines power each crawler.\n\nOriginally designated the \"Mobile Launcher\", the MLP was designed as part of NASA's strategy for vertical assembly and transport of space vehicles. Vertical assembly allows the preparation of the spacecraft in a ready-for-launch position, and avoids the additional step of lifting or craning a horizontally-assembled vehicle onto the launchpad (as the engineers of the Soviet space program chose to do).\n\nThe Mobile Launcher Platform was set atop six legs, each tall, when stationary. The Solid Rocket Boosters were mounted on top of the MLP. The External Tank was then lowered between the two boosters and attached to them. After that, the orbiter was lowered into position and attached to the External Tank. The crawler-transporter then carried the combined platform and vehicle to the launch site, and deposited them there together. Once the launch was completed, the crawler-transporter retrieved the empty MLP from the pad to be readied for its next use.\n\nWith the retirement of the Shuttle in 2011 and its planned replacement Orion spacecraft and launcher in the design and test phase, NASA converted LC-39B from Shuttle operations to support Orion launches. The Ares I-X suborbital mission utilized MLP-1, to support the stacking and launch operations. The cancelled Ares I-Y would have used the same MLP.\n\nThe MLP that was built to launch the Ares I rocket will be modified to support the Space Launch System. The mobile launcher will have to be altered in order to support the heavier weight and additional thrust of the heavy lift rocket. The biggest modifications to the MLP will be on the platform's base, where engineers will increase the size of a exhaust duct to a rectangle stretching and strengthen the surrounding structure. The SLS will weigh more than twice as much as the planned Ares I rocket. The Ares 1 rocket would have featured a single solid-fueled first stage, while the Space Launch System will include two large solid rocket boosters and a powerful core with four Space Shuttle main engines.\n\nEarly in 2016, NASA finished upgrading CT-2 to a \"Super Crawler\" for use in the Space Launch System program. CT-1 is now in the process of being modified to serve a variety of commercial spacecraft. In April 2016, Orbital ATK and NASA entered negotiations for the lease of CT-1 and one of the four Vehicle Assembly Building bays.\n\n"}
{"id": "33907840", "url": "https://en.wikipedia.org/wiki?curid=33907840", "title": "Nascent state (chemistry)", "text": "Nascent state (chemistry)\n\nNascent state or in statu nascendi (Lat. newly formed moiety: \"in the state of being born\" or \"just emerging\"), is an obsolete theory in chemistry. It refers to the form of a chemical element (or sometimes compound) in the instance of their liberation or formation. Often encountered are atomic oxygen (O), nascent hydrogen (H), and similar forms of chlorine (Cl) or bromine (Br).\n\nThe concept of a \"nascent state\" was developed to explain the observation that gases generated in situ are frequently more reactive than identical chemicals that have been stored for an extended period of time. First usage of the term was in work by Joseph Priestley around 1790. Auguste Laurent expanded on the theory in the mid 19th century.\n\nConstantine Zenghelis hypothesized in 1920 that the increased reactivity of the \"nascent\" state was due to the fine dispersion of the molecules, not their status as free atoms. Still popular in the early 20th century, the nascent state theory was recognized as declining by 1942.\n\nA 1990 review noted that the term was still found as a passing mention in contemporary textbooks. The review summarized that the increased activity observed is actually caused by multiple kinetic effects, and that grouping all these effects into a single term could cause chemists to view the effect too simplistically.\n\n"}
{"id": "54663499", "url": "https://en.wikipedia.org/wiki?curid=54663499", "title": "Online discussion platform", "text": "Online discussion platform\n\nAn online discussion platform is an online platform that allows for, or is built specifically for, online discussion. \n\nIn 1979 students from Duke University created the first online discussion platform with Usenet.\n\nOnline discussion platforms can engage people in collective reflection and exchanging perspectives and cross-cultural understanding.\n\nPublic display of ideas can encourage intersubjective meaning making.\n\nOnline discussion platforms may be an important structural means for effective large-scale participation.\n\nOnline discussion platforms can play a role in education. In recent years, online discussion platform have become a significant part of not only distance education but also in campus-based settings.\n\nThe proposed interactive e-learning community (iELC) is a platform that engages physics students in online and classroom learning tasks. In brief classroom discussions fundamental physics formulas, definitions and concepts are disclosed, after which students participate in the iELC form discussion and utilize chat and dialogue tools to improve their understanding of the subject. The teacher then discusses selected forum posts in the subsequent classroom session.\n\nClassroom online discussion platforms are one type of such platforms.\n\nRose argues that the basic motivation for the development of e–learning platforms is efficiency of scale — teaching more students for less money.\n\nA study found that learners will enhance the frequencies of course discussion and actively interact with e-learning platform when e-learning platform integrates the curriculum reward mechanism into learning activities.\n\n\"City townhall\" includes a participation platform for policy-making in Rotterdam.\n\nOnline discussion platforms may be designed and improved to streamline discussions for efficiency, usefulness and quality. For instance voting, targeted notifications, user levels, gamification, subscriptions, bots, discussion requirements, structurization, layout, sorting, linking, feedback-mechanisms, reputation-features, demand-signaling features, requesting-features, visual highlighting, separation, curation, tools for real-time collaboration, tools for mobilization of humans and resources, standardization, data-processing, segmentation, summarization, moderation, time-intervals, categorization/tagging, rules and indexing can be leveraged in synergy to improve the platform.\n\nIn 2013 Sarah Perez claimed that the best platform for online discussion doesn't yet exist, noting that comment sections could be more useful if they showed \"which comments or shares have resonated and why\" and which \"understands who deserves to be heard\".\n\nOnline platforms don't intrinsically guarantee informed citizen input. Research demonstrates that such spaces can even undermine deliberative participation when they allow hostile, superficial and misinformed content to dominate the conversation (see also: Internet troll, shitposting). A necessary mechanism that enables these platforms to yield informed citizen debate and contribution to policy is deliberation. It is argued that the challenge lies in creating an online context that does not merely aggregate public input but promotes informed public discussion that may benefit the policy-making process.\n\nOnline citizen communication has been studied for an evaluations of how deliberative their content is and how selective perception and ideological fragmentation play a role in them (see also: filter bubble).\nOne sub-branch of online deliberation research is dedicated to the development of new platforms that \"facilitate deliberative experiences that surpass currently available options\".\n\n"}
{"id": "33792090", "url": "https://en.wikipedia.org/wiki?curid=33792090", "title": "Overcategorization", "text": "Overcategorization\n\nOvercategorization, overcategorisation or category clutter is the process of assigning too many categories, classes or index terms to a given document. It is related to the Library and Information Science (LIS) concepts of document classification and subject indexing.\n\nIn LIS, the ideal number of terms that should be assigned to classify an item are measured by the variables precision and recall. Assigning few category labels that are most closely related to the content of the item being classified will result in searches that have high precision, I.e., where a high proportion of the results are closely related to the query. Assigning more category labels to each item will reduce the precision of each search, but increase the recall, retrieving more relevant results. Related LIS concepts include exhaustivity of indexing and information overload.\n\nIf too many categories are assigned to a given document, the implications for users depend on how informative the links are. If the user is able to distinguish between useful and not useful links, the damage is limited: The user only wastes time selecting links. In many cases, however, the user cannot judge whether or not a given link will turn out to be fruitful. In that case he or she has to follow the link and to read or skim another document. The worst case scenario is, of course, that even after reading the new document the user is unable to decide whether or not it might be useful if its subject matter is not thoroughly investigated. \n\nOvercategorization also has another unpleasant implication: It makes the system (for example ) difficult to maintain in a consistent way. If the system is inconsistent, it means that when the user considers the links in a given category, he or she will not find all documents relevant to that category.\n\nBasically, the problem of overcategorization should be understood from the perspective of relevance and the traditional measures of recall and precision. If too few \"relevant\" categories are assigned to a document, recall may decrease. If too many non-relevant categories are assigned, precision becomes lower. The hard job is to say which categories are fruitful or relevant for future use of the document.\n\n"}
{"id": "14503014", "url": "https://en.wikipedia.org/wiki?curid=14503014", "title": "Pamphile", "text": "Pamphile\n\nPamphile (), \"Plateae filia\" or \"Latoi filia\", was the daughter of Platea, or of Apollo (Latous), a woman of the Greek island of Kos. \nIt is said that silk was first spun by her. She also invented the technique of preparing a thread from cotton wool for spinning on a distaff. She developed the technique of weaving from cotton thread. \n\nPliny the Elder described in 70 BC: \"Silk was obtained by removing the down from the leaves with the help of water\". He also recounted the legend of Pamphile, who invented silk weaving on the Greek island of Kos. He said that Pamphile discovered the technique of weaving like a spider's web and that \"she ought not to be cheated of the glory of making a silk dress that covers a woman but reveals her charms\". Aristotle also associated Pamphile with inventing the concept of weaving silk.\n\n"}
{"id": "1793234", "url": "https://en.wikipedia.org/wiki?curid=1793234", "title": "Paul Niggli", "text": "Paul Niggli\n\nNiggli was born in Zofingen and studied at the Swiss Federal Institute of Technology (ETH) in Zurich and the University of Zurich, where he obtained a doctorate. His 1919 book, \"Geometrische Kristallographie des Diskontinuums\", played a seminal role in the refinement of space group theory. In this book, Niggli demonstrated that although X-ray reflection conditions do not always uniquely determine the space group to which a crystal belongs, they do reveal a small number of possible space groups to which it could belong. Niggli used morphological methods to account for internal structure and, in his 1928 \"Kristallographische und Strukturtheoretische Grundbegriffe,\" he took up what is essentially the reverse process, the task of establishing the connection between space lattices and external crystal morphology. The great aim of his life was to integrate the whole field of Earth sciences.\n\nIn 1920, Niggli became the lead scientist at the ETH's \"Institut für Mineralogie und Petrographie\", where he brought his systematic approach to the study of crystal morphologies using X-ray diffraction. In 1935, Niggli and his doctoral student Werner Nowacki (1909–1988) determined the 73 three-dimensional arithmetic crystal classes (symmorphic space groups). Niggli retired from the Institute in 1949. He was also professor of mineralogy at the Eidgenössische Technische Hochschule and at the University of Zurich.\n\nNiggli succeeded Paul Heinrich von Groth (1843 - 1927) as editor of \"Zeitschrift für Kristallographie\".\n\nIn 1948, Niggli was awarded the Roebling Medal of the Mineralogical Society of America.\n\nThe Paul Niggli Foundation awards medals to outstanding Swiss mineral scientists below the age of 35 with a strong perspective for an academic career.\n\nDorsum Niggli on the Moon was named after him.\n"}
{"id": "47325951", "url": "https://en.wikipedia.org/wiki?curid=47325951", "title": "Quintessence: The Search for Missing Mass in the Universe", "text": "Quintessence: The Search for Missing Mass in the Universe\n\nQuintessence: The Search for Missing Mass in the Universe is the fifth non-fiction book by the American theoretical physicist Lawrence M. Krauss. The book was published by Basic Books on December 21, 2000. This text is an update of his 1989 book \"The Fifth Essence\". It was retitled \"Quintessence\" after the now widely accepted term for dark energy.\n\nKrauss focuses on theoretical physics and has published researches on a number of topics within that field. His primary contribution is to cosmology as one of the first physicists to suggest that most of the mass and energy of the universe resides in empty space, an idea now widely known as \"dark energy\". Furthermore, Krauss has formulated a model in which the universe could have potentially come from \"nothing,\" as outlined in his later book \"A Universe from Nothing\".\n\nWhether our universe is ever-expanding depends on the amount and properties of matter, but there is too little visible matter around us to explain the behavior we can see—over 90% of the universe consists of the missing mass or dark matter, which Krauss termed \"the fifth essence.\" In this book Krauss demonstrates how the dark matter problem is now connected with two widely discussed areas in the modern cosmology: the ultimate fate of the universe and the cosmological constant. He also discusses an antigravity force that may explain recent observations of a permanently expanding universe.\n\n\n"}
{"id": "14728882", "url": "https://en.wikipedia.org/wiki?curid=14728882", "title": "Raymond Heacock", "text": "Raymond Heacock\n\nRaymond L. Heacock (January 9, 1928 – December 20, 2016) was an American engineer who spent his career at NASA's Jet Propulsion Laboratory where he worked on the Ranger program in the 1960s and on the Voyager program in the 1970s and 1980s. A Caltech engineering graduate, he was the winner of the James Watt International Medal for 1979.\n\nHeacock joined the Jet Propulsion Laboratory in 1953, after receiving his Master of Science Degree in Engineering from the California Institute of Technology. Prior to joining the Voyager Project in 1972 as Spacecraft Systems Manager he had advanced through various positions of responsibility at the Laboratory. In October 1977, he was appointed Deputy Manager of the Voyager Project and became Manager in 1979. He was a member of the American Institute of Aeronautics and Astronautics and has served as Secretary, Treasurer, Vice-President and President of the Board of Directors of the Caltech Alumni Association. Heacock is a native of Santa Ana, California and now lives in La Crescenta.\n\nSince the inception of the Voyager Project in 1972, Heacock was deeply involved in guiding and shaping the successful development and operation of the sophisticated craft. The scientific data from the flight experiments carried aboard them have yielded startling new information on Jupiter, Saturn, and Uranus. Heacock was a leader in the design, development and flight operations of these craft as well as of their scientific instruments complement. As Spacecraft System Manager, Deputy Project Manager, and Project Manager he contributed personally to the development of various advanced design features leading to the Project's outstanding success.\n\nNASA's two robot spacecraft, \"Voyager 1\" and \"Voyager 2\", were launched in the Summer of 1977 on their journeys to Jupiter of more than 625 million miles. A fitting tribute to the efforts and ingenuity of many engineers and scientists, the spacecraft have now completed the exploration of the outer solar system. \"Voyager 1\" reached Saturn in November 1981, and then left the solar system. Nearly 10 years later \"Voyager 1\" turned around to point its cameras towards Earth and took the famous \"Pale Blue Dot\" image. \"Voyager 2\" reached Saturn in August 1981, then went on to Uranus in 1986, and Neptune in 1989.\n\nThe presentation of the 1979 James Watt International Medal was made on Wednesday, June 25, 1980 at the Institute of Mechanical Engineers in London. The Medal was presented to Heacock, Project Manager—Voyager, Jet Propulsion Laboratory, California Institute of Technology in Pasadena, California, by the President of the Institution of Mechanical Engineers, Bryan Hildrew, C.B.E., M.Sc, D.I.C., F.Eng., F.I.Mech.E., F.I.Mar.E., who read a citation.\n\nThe James Watt International Medal is awarded biennially to an engineer of any nationality who is deemed worthy of this, the highest award which the Institution of Mechanical Engineers can bestow. The Council awarded the 1979 Medal to Heacock for his outstanding achievements as leader of the team responsible for shaping the development and execution of the technically advanced spacecraft used by the United States of America in the exploration of the outer planets of our solar system.\n"}
{"id": "26404", "url": "https://en.wikipedia.org/wiki?curid=26404", "title": "Risk management", "text": "Risk management\n\nRisk management is the identification, evaluation, and prioritization of risks (defined in ISO 31000 as \"the effect of uncertainty on objectives\") followed by coordinated and economical application of resources to minimize, monitor, and control the probability or impact of unfortunate events or to maximize the realization of opportunities.\n\nRisks can come from various sources including uncertainty in financial markets, threats from project failures (at any phase in design, development, production, or sustainment life-cycles), legal liabilities, credit risk, accidents, natural causes and disasters, deliberate attack from an adversary, or events of uncertain or unpredictable root-cause. There are two types of events i.e. negative events can be classified as risks while positive events are classified as opportunities. Several risk management standards have been developed including the Project Management Institute, the National Institute of Standards and Technology, actuarial societies, and ISO standards. Methods, definitions and goals vary widely according to whether the risk management method is in the context of project management, security, engineering, industrial processes, financial portfolios, actuarial assessments, or public health and safety.\n\nStrategies to manage threats (uncertainties with negative consequences) typically include avoiding the threat, reducing the negative effect or probability of the threat, transferring all or part of the threat to another party, and even retaining some or all of the potential or actual consequences of a particular threat, and the opposites for opportunities (uncertain future states with benefits).\n\nCertain aspects of many of the risk management standards have come under criticism for having no measurable improvement on risk; whereas the confidence in estimates and decisions seem to increase. For example, one study found that one in six IT projects were \"black swans\" with gigantic overruns (cost overruns averaged 200%, and schedule overruns 70%).\n\nA widely used vocabulary for risk management is defined by \"ISO Guide 73:2009\", \"Risk management. Vocabulary.\"\n\nIn ideal risk management, a prioritization process is followed whereby the risks with the greatest loss (or impact) and the greatest probability of occurring are handled first, and risks with lower probability of occurrence and lower loss are handled in descending order. In practice the process of assessing overall risk can be difficult, and balancing resources used to mitigate between risks with a high probability of occurrence but lower loss versus a risk with high loss but lower probability of occurrence can often be mishandled.\n\nIntangible risk management identifies a new type of a risk that has a 100% probability of occurring but is ignored by the organization due to a lack of identification ability. For example, when deficient knowledge is applied to a situation, a knowledge risk materializes. Relationship risk appears when ineffective collaboration occurs. Process-engagement risk may be an issue when ineffective operational procedures are applied. These risks directly reduce the productivity of knowledge workers, decrease cost-effectiveness, profitability, service, quality, reputation, brand value, and earnings quality. Intangible risk management allows risk management to create immediate value from the identification and reduction of risks that reduce productivity.\n\nRisk management also faces difficulties in allocating resources. This is the idea of opportunity cost. Resources spent on risk management could have been spent on more profitable activities. Again, ideal risk management minimizes spending (or manpower or other resources) and also minimizes the negative effects of risks.\n\nAccording to the definition to the risk, the risk is the possibility that an event will occur and adversely affect the achievement of an objective. Therefore, risk itself has the uncertainty. Risk management such as COSO ERM, can help managers have a good control for their risk. Each company may have different internal control components, which leads to different outcomes. For example, the framework for ERM components includes Internal Environment, Objective Setting, Event Identification, Risk Assessment, Risk Response, Control Activities, Information and Communication, and Monitoring.\n\nFor the most part, these methods consist of the following elements, performed, more or less, in the following order.\n\n\nThe International Organization for Standardization (ISO) identifies the following principles of risk management:\n\nRisk management should:\n\nAccording to the standard ISO 31000 \"Risk management – Principles and guidelines on implementation,\" the process of risk management consists of several steps as follows:\n\nThis involves:\n\nAfter establishing the context, the next step in the process of managing risk is to identify potential risks. Risks are about events that, when triggered, cause problems or benefits. Hence, risk identification can start with the source of our problems and those of our competitors (benefit), or with the problem itself.\nExamples of risk sources are: stakeholders of a project, employees of a company or the weather over an airport.\nWhen either source or problem is known, the events that a source may trigger or the events that can lead to a problem can be investigated. For example: stakeholders withdrawing during a project may endanger funding of the project; confidential information may be stolen by employees even within a closed network; lightning striking an aircraft during takeoff may make all people on board immediate casualties.\n\nThe chosen method of identifying risks may depend on culture, industry practice and compliance. The identification methods are formed by templates or the development of templates for identifying source, problem or event. Common risk identification methods are:\n\nOnce risks have been identified, they must then be assessed as to their potential severity of impact (generally a negative impact, such as damage or loss) and to the probability of occurrence. These quantities can be either simple to measure, in the case of the value of a lost building, or impossible to know for sure in the case of an unlikely event, the probability of occurrence of which is unknown. Therefore, in the assessment process it is critical to make the best educated decisions in order to properly prioritize the implementation of the risk management plan.\n\nEven a short-term positive improvement can have long-term negative impacts. Take the \"turnpike\" example. A highway is widened to allow more traffic. More traffic capacity leads to greater development in the areas surrounding the improved traffic capacity. Over time, traffic thereby increases to fill available capacity. Turnpikes thereby need to be expanded in a seemingly endless cycles. There are many other engineering examples where expanded capacity (to do any function) is soon filled by increased demand. Since expansion comes at a cost, the resulting growth could become unsustainable without forecasting and management.\n\nThe fundamental difficulty in risk assessment is determining the rate of occurrence since statistical information is not available on all kinds of past incidents and is particularly scanty in the case of catastrophic events, simply because of their infrequency. Furthermore, evaluating the severity of the consequences (impact) is often quite difficult for intangible assets. Asset valuation is another question that needs to be addressed. Thus, best educated opinions and available statistics are the primary sources of information. Nevertheless, risk assessment should produce such information for senior executives of the organization that the primary risks are easy to understand and that the risk management decisions may be prioritized within overall company goals. Thus, there have been several theories and attempts to quantify risks. Numerous different risk formulae exist, but perhaps the most widely accepted formula for risk quantification is: \"Rate (or probability) of occurrence multiplied by the impact of the event equals risk magnitude.\"\n\nRisk mitigation measures are usually formulated according to one or more of the following major risk options, which are:\n\n\nLater research has shown that the financial benefits of risk management are less dependent on the formula used but are more dependent on the frequency and how risk assessment is performed.\n\nIn business it is imperative to be able to present the findings of risk assessments in financial, market, or schedule terms. Robert Courtney Jr. (IBM, 1970) proposed a formula for presenting risks in financial terms. The Courtney formula was accepted as the official risk analysis method for the US governmental agencies. The formula proposes calculation of ALE (annualized loss expectancy) and compares the expected loss value to the security control implementation costs (cost-benefit analysis).\n\nOnce risks have been identified and assessed, all techniques to manage the risk fall into one or more of these four major categories:\n\n\nIdeal use of these risk control strategies may not be possible. Some of them may involve trade-offs that are not acceptable to the organization or person making the risk management decisions. Another source, from the US Department of Defense (see link), Defense Acquisition University, calls these categories ACAT, for Avoid, Control, Accept, or Transfer. This use of the ACAT acronym is reminiscent of another ACAT (for Acquisition Category) used in US Defense industry procurements, in which Risk Management figures prominently in decision making and planning.\n\nThis includes not performing an activity that could carry risk. An example would be not buying a property or business in order to not take on the legal liability that comes with it. Another would be not flying in order not to take the risk that the airplane were to be hijacked. Avoidance may seem the answer to all risks, but avoiding risks also means losing out on the potential gain that accepting (retaining) the risk may have allowed. Not entering a business to avoid the risk of loss also avoids the possibility of earning profits. Increasing risk regulation in hospitals has led to avoidance of treating higher risk conditions, in favor of patients presenting with lower risk.\n\nRisk reduction or \"optimization\" involves reducing the severity of the loss or the likelihood of the loss from occurring. For example, sprinklers are designed to put out a fire to reduce the risk of loss by fire. This method may cause a greater loss by water damage and therefore may not be suitable. Halon fire suppression systems may mitigate that risk, but the cost may be prohibitive as a strategy.\n\nAcknowledging that risks can be positive or negative, optimizing risks means finding a balance between negative risk and the benefit of the operation or activity; and between risk reduction and effort applied. By an offshore drilling contractor effectively applying Health, Safety and Environment (HSE) management in its organization, it can optimize risk to achieve levels of residual risk that are tolerable.\n\nModern software development methodologies reduce risk by developing and delivering software incrementally. Early methodologies suffered from the fact that they only delivered software in the final phase of development; any problems encountered in earlier phases meant costly rework and often jeopardized the whole project. By developing in iterations, software projects can limit effort wasted to a single iteration.\n\nOutsourcing could be an example of risk sharing strategy if the outsourcer can demonstrate higher capability at managing or reducing risks. For example, a company may outsource only its software development, the manufacturing of hard goods, or customer support needs to another company, while handling the business management itself. This way, the company can concentrate more on business development without having to worry as much about the manufacturing process, managing the development team, or finding a physical location for a center.\n\nBriefly defined as \"sharing with another party the burden of loss or the benefit of gain, from a risk, and the measures to reduce a risk.\"\n\nThe term of 'risk transfer' is often used in place of risk sharing in the mistaken belief that you can transfer a risk to a third party through insurance or outsourcing. In practice if the insurance company or contractor go bankrupt or end up in court, the original risk is likely to still revert to the first party. As such in the terminology of practitioners and scholars alike, the purchase of an insurance contract is often described as a \"transfer of risk.\" However, technically speaking, the buyer of the contract generally retains legal responsibility for the losses \"transferred\", meaning that insurance may be described more accurately as a post-event compensatory mechanism. For example, a personal injuries insurance policy does not transfer the risk of a car accident to the insurance company. The risk still lies with the policy holder namely the person who has been in the accident. The insurance policy simply provides that if an accident (the event) occurs involving the policy holder then some compensation may be payable to the policy holder that is commensurate with the suffering/damage.\n\nSome ways of managing risk fall into multiple categories. Risk retention pools are technically retaining the risk for the group, but spreading it over the whole group involves transfer among individual members of the group. This is different from traditional insurance, in that no premium is exchanged between members of the group up front, but instead losses are assessed to all members of the group.\n\nRisk retention involves accepting the loss, or benefit of gain, from a risk when the incident occurs. True self-insurance falls in this category. Risk retention is a viable strategy for small risks where the cost of insuring against the risk would be greater over time than the total losses sustained. All risks that are not avoided or transferred are retained by default. This includes risks that are so large or catastrophic that either they cannot be insured against or the premiums would be infeasible. War is an example since most property and risks are not insured against war, so the loss attributed to war is retained by the insured. Also any amounts of potential loss (risk) over the amount insured is retained risk. This may also be acceptable if the chance of a very large loss is small or if the cost to insure for greater coverage amounts is so great that it would hinder the goals of the organization too much.\n\nSelect appropriate controls or countermeasures to mitigate each risk. Risk mitigation needs to be approved by the appropriate level of management. For instance, a risk concerning the image of the organization should have top management decision behind it whereas IT management would have the authority to decide on computer virus risks.\n\nThe risk management plan should propose applicable and effective security controls for managing the risks. For example, an observed high risk of computer viruses could be mitigated by acquiring and implementing antivirus software. A good risk management plan should contain a schedule for control implementation and responsible persons for those actions.\n\nAccording to ISO/IEC 27001, the stage immediately after completion of the risk assessment phase consists of preparing a Risk Treatment Plan, which should document the decisions about how each of the identified risks should be handled. Mitigation of risks often means selection of security controls, which should be documented in a Statement of Applicability, which identifies which particular control objectives and controls from the \nstandard have been selected, and why.\n\nImplementation follows all of the planned methods for mitigating the effect of the risks. Purchase insurance policies for the risks that it has been decided to transferred to an insurer, avoid all risks that can be avoided without sacrificing the entity's goals, reduce others, and retain the rest.\n\nInitial risk management plans will never be perfect. Practice, experience, and actual loss results will necessitate changes in the plan and contribute information to allow possible different decisions to be made in dealing with the risks being faced.\n\nRisk analysis results and management plans should be updated periodically. There are two primary reasons for this: \n\nPrioritizing the \"risk management processes\" too highly could keep an organization from ever completing a project or even getting started. This is especially true if other work is suspended until the risk management process is considered complete.\n\nIt is also important to keep in mind the distinction between risk and uncertainty. Risk can be measured by impacts × probability.\n\nIf risks are improperly assessed and prioritized, time can be wasted in dealing with risk of losses that are not likely to occur. Spending too much time assessing and managing unlikely risks can divert resources that could be used more profitably. Unlikely events do occur but if the risk is unlikely enough to occur it may be better to simply retain the risk and deal with the result if the loss does in fact occur. Qualitative risk assessment is subjective and lacks consistency. The primary justification for a formal risk assessment process is legal and bureaucratic.\n\nAs applied to corporate finance, \"risk management\" is the technique for measuring, monitoring and controlling the financial or operational risk on a firm's balance sheet, a traditional measure is the value at risk (VaR), but there also other measures like profit at risk (PaR) or margin at risk. The Basel II framework breaks risks into market risk (price risk), credit risk and operational risk and also specifies methods for calculating capital requirements for each of these components.\n\nIn Information Technology, Risk management includes \"Incident Handling\", an action plan for dealing with intrusions, cyber-theft, denial of service, fire, floods, and other security-related events. According to the SANS Institute, it is a six step process: Preparation, Identification, Containment, Eradication, Recovery, and Lessons Learned.\n\nIn enterprise risk management, a risk is defined as a possible event or circumstance that can have negative influences on the enterprise in question. Its impact can be on the very existence, the resources (human and capital), the products and services, or the customers of the enterprise, as well as external impacts on society, markets, or the environment. In a financial institution, enterprise risk management is normally thought of as the combination of credit risk, interest rate risk or asset liability management, liquidity risk, market risk, and operational risk.\n\nIn the more general case, every probable risk can have a pre-formulated plan to deal with its possible consequences (to ensure \"contingency\" if the risk becomes a \"liability\").\n\nFrom the information above and the average cost per employee over time, or cost accrual ratio, a project manager can estimate:\n\n\nRisk in a project or process can be due either to Special Cause Variation or Common Cause Variation and requires appropriate treatment. That is to re-iterate the concern about extremal cases not being equivalent in the list immediately above.\n\nESRM is a security program management approach that links security activities to an enterprise's mission and business goals through risk management methods. The security leader's role in ESRM is to manage risks of harm to enterprise assets in partnership with the business leaders whose assets are exposed to those risks. ESRM involves educating business leaders on the realistic impacts of identified risks, presenting potential strategies to mitigate those impacts, then enacting the option chosen by the business in line with accepted levels of business risk tolerance\n\nFor medical devices, risk management is a process for identifying, evaluating and mitigating risks associated with harm to people and damage to property or the environment. Risk management is an integral part of medical device design and development, production processes and evaluation of field experience, and is applicable to all types of medical devices. The evidence of its application is required by most regulatory bodies such as the US FDA. The management of risks for medical devices is described by the International Organization for Standardization (ISO) in ISO 14971:2007, Medical Devices—The application of risk management to medical devices, a product safety standard. The standard provides a process framework and associated requirements for management responsibilities, risk analysis and evaluation, risk controls and lifecycle risk management.\n\nThe European version of the risk management standard was updated in 2009 and again in 2012 to refer to the Medical Devices Directive (MDD) and Active Implantable Medical Device Directive (AIMDD) revision in 2007, as well as the In Vitro Medical Device Directive (IVDD). The requirements of EN 14971:2012 are nearly identical to ISO 14971:2007. The differences include three \"(informative)\" Z Annexes that refer to the new MDD, AIMDD, and IVDD. These annexes indicate content deviations that include the requirement for risks to be reduced \"as far as possible\", and the requirement that risks be mitigated by design and not by labeling on the medical device (i.e., labeling can no longer be used to mitigate risk).\n\nTypical risk analysis and evaluation techniques adopted by the medical device industry include hazard analysis, fault tree analysis (FTA), failure mode and effects analysis (FMEA), hazard and operability study (HAZOP), and risk traceability analysis for ensuring risk controls are implemented and effective (i.e. tracking risks identified to product requirements, design specifications, verification and validation results etc.). FTA analysis requires diagramming software. FMEA analysis can be done using a spreadsheet program. There are also integrated medical device risk management solutions.\n\nThrough a draft guidance, the FDA has introduced another method named \"Safety Assurance Case\" for medical device safety assurance analysis. The safety assurance case is structured argument reasoning about systems appropriate for scientists and engineers, supported by a body of evidence, that provides a compelling, comprehensible and valid case that a system is safe for a given application in a given environment. With the guidance, a safety assurance case is expected for safety critical devices (e.g. infusion devices) as part of the pre-market clearance submission, e.g. 510(k). In 2013, the FDA introduced another draft guidance expecting medical device manufacturers to submit cybersecurity risk analysis information.\n\nProject risk management must be considered at the different phases of acquisition. In the beginning of a project, the advancement of technical developments, or threats presented by a competitor's projects, may cause a risk or threat assessment and subsequent evaluation of alternatives (see Analysis of Alternatives). Once a decision is made, and the project begun, more familiar project management applications can be used:\n\n\nMegaprojects (sometimes also called \"major programs\") are large-scale investment projects, typically costing more than $1 billion per project. Megaprojects include major bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, coastal flood protection schemes, oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, and defense systems. Megaprojects have been shown to be particularly risky in terms of finance, safety, and social and environmental impacts. Risk management is therefore particularly pertinent for megaprojects and special methods and special education have been developed for such risk management.\n\nIt is important to assess risk in regard to natural disasters like floods, earthquakes, and so on. Outcomes of natural disaster risk assessment are valuable when considering future repair costs, business interruption losses and other downtime, effects on the environment, insurance costs, and the proposed costs of reducing the risk. The Sendai Framework for Disaster Risk Reduction is a 2015 international accord that has set goals and targets for disaster risk reduction in response to natural disasters. There are regular International Disaster and Risk Conferences in Davos to deal with integral risk management.\n\nThe management of risks to persons and property in wilderness and remote natural areas has developed with increases in outdoor recreation participation and decreased social tolerance for loss. Organizations providing commercial wilderness experiences can now align with national and international consensus standards for training and equipment such as ANSI/NASBLA 101-2017 (boating), UIAA 152 (ice climbing tools), and European Norm 13089:2015 + A1:2015 (mountaineering equipment). The Association for Experiential Education offers accreditation for wilderness adventure programs. The Wilderness Risk Management Conference provides access to best practices, and specialist organizations provide wilderness risk management consulting and training.\n\nIT risk is a risk related to information technology. This is a relatively new term due to an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports.\n\nISACA's \"Risk IT\" framework ties IT risk to enterprise risk management.\n\nDuty of Care Risk Analysis (DoCRA) evaluates risks and their safeguards and considers the interests of all parties potentially affected by those risks.\n\nCIS RAM provides a method to design and evaluate the implementation of the CIS Controls™.\n\nFor the offshore oil and gas industry, operational risk management is regulated by the safety case regime in many countries. Hazard identification and risk assessment tools and techniques are described in the international standard ISO 17776:2000, and organisations such as the IADC (International Association of Drilling Contractors) publish guidelines for Health, Safety and Environment (HSE) Case development which are based on the ISO standard. Further, diagrammatic representations of hazardous events are often expected by governmental regulators as part of risk management in safety case submissions; these are known as bow-tie diagrams (see Network theory in risk assessment). The technique is also used by organisations and regulators in mining, aviation, health, defence, industrial and finance.\n\nThe principles and tools for quality risk management are increasingly being applied to different aspects of pharmaceutical quality systems. These aspects include development, manufacturing, distribution, inspection, and submission/review processes throughout the lifecycle of drug substances, drug products, biological and biotechnological products (including the use of raw materials, solvents, excipients, packaging and labeling materials in drug products, biological and biotechnological products). Risk management is also applied to the assessment of microbiological contamination in relation to pharmaceutical products and cleanroom manufacturing environments.\n\nRisk communication is a complex cross-disciplinary academic field related to core values of the targeted audiences. Problems for risk communicators involve how to reach the intended audience, how to make the risk comprehensible and relatable to other risks, how to pay appropriate respect to the audience's values related to the risk, how to predict the audience's response to the communication, etc. A main goal of risk communication is to improve collective and individual decision making. Risk communication is somewhat related to crisis communication. Some experts coincide that risk is not only enrooted in the communication process but also it cannot be dissociated from the use of language. Though each culture develops its own fears and risks, these construes apply only by the hosting culture.\n\n\n"}
{"id": "855950", "url": "https://en.wikipedia.org/wiki?curid=855950", "title": "Sleeper effect", "text": "Sleeper effect\n\nThe sleeper effect is a psychological phenomenon that relates to persuasion. It is a delayed increase of the effect of a message that is accompanied by a discounting cue.\nWhen people are exposed normally to a persuasive message (such as an engaging or persuasive television advertisement), their attitudes toward the advocacy of the message display a significant increase.\n\nOver time, however, their newly formed attitudes seem to gravitate back toward the opinion held prior to receiving the message, almost as if they were never exposed to the communication. This pattern of normal decay in attitudes has been documented as the most frequently observed longitudinal pattern of persuasion research (Eagly & Chaiken, 1993).\n\nIn contrast, some messages are often accompanied with a discounting cue (e.g., a message disclaimer, a low-credibility source) that would arouse a recipient’s suspicion of the validity of the message and suppress any attitude change that might occur by exposure to the message alone. Furthermore, when people are exposed to a persuasive message followed by a discounting cue, people tend to be more persuaded over time; this is referred to as the sleeper effect (Hovland & Weiss, 1951; Cook & Flay, 1978).\n\nFor example, in political campaigns during important elections, undecided voters often see negative advertisements about a party or candidate for office. At the end of the advertisement, they also might notice that the opposing candidate paid for the advertisement. Presumably, this would make voters question the truthfulness of the advertisement, and consequently, they may not be persuaded initially. However, even though the source of the advertisement lacked credibility, voters will be more likely to be persuaded later (and ultimately, vote against the candidate disfavored by the advertisement).\n\nThis pattern of attitude change has puzzled social psychologists for nearly half a century, primarily due to its counter-intuitive nature and for its potential to aid in understanding attitude processes (Eagly & Chaiken, 1993). It has been a very widely studied phenomenon of persuasion research (Kumkale & Albarracín, 2004; see also Cook & Flay, 1978).\nDespite a long history, the sleeper effect has been notoriously difficult to obtain \nor to replicate, with the exception of a pair of studies by Gruder et al. (1978).\n\nOne of the more challenging aspects that the sleeper effect posed to some researchers in early studies was the sheer difficulty of obtaining the effect.\n\nThe sleeper effect is controversial because the influence of a persuasive communication is greater when one measures the effect closer to the presentation instead of farther from the time of the reception.\n\nAfter attempting to replicate the effect and failing, some researchers suggested that it might be better to accept the null hypothesis and conclude that the sleeper effect does not exist.\n\nThe sleeper effect is involved with initial message impression so the phenomenon has implications for models of persuasion, including teaching methods, as well as more recent conceptualizations, such as the heuristic-systematic model and the elaboration likelihood model.\n\nHowever, Cook and his associates responded by suggesting that previous studies failed to obtain the sleeper effect because the requirements for a strong test were not met. Specifically, they argued that the sleeper effect will occur only if:\n\nExperimental studies conducted did, in fact, provide evidence for the sleeper effect occurring under such theoretically relevant conditions. Furthermore, the sleeper effect did not occur when any of the four requirements were not met.\n\nAccording to the dissociation interpretation, a sleeper effect appears to happen when a convincing message is conferred with a discounting cue (such as a low-credible source or counterargument). A sleeper effect occurs because of an impulsive dissociation of a message and a discounting cue over time (contrasting to a simple forgetting of a source).\n\nThe sleeper effect was first identified in U.S. soldiers during World War II, after attempts to change their opinions and morals. Hovland et al. measured the soldier’s opinions five days or nine weeks after they were shown a movie presentation of army propaganda. It was found that the difference in opinions of those who had observed the army propaganda movie and those who did not watch the movie were greater nine weeks after viewing it than five days. The difference in delayed persuasion is (which Hovland et al. termed) the sleeper effect, where there was a significant increase of persuasion in the experimental group.\n\nThe first efforts to justify the effect were consistent with the understanding of persuasion processes at that time. Hovland and his colleagues introduced a program of research to study how recall of the message and the source persuaded the sleeper effect. They first hypothesized that message receivers forget the noncredible communicator as time goes by, and therefore the initial message rejection diminishes. Nevertheless, they later propositioned that message receivers may not entirely forget the cue, yet the association between the representations of the discounting cue and the message content may fade over time and produce a sleeper effect. These two formulations vary in that (a) forgetting suggests that the traces of the cue disappear or become unavailable in memory over time, while (b) dissociation suggests that cue remains available in memory but is simply less easily retrieved (less accessible) in relation to the topic of communication.\n\nBecause the sleeper effect has been considered to be counter-intuitive, researchers since the early 1950s have attempted to explain how and why it occurs.\nAccording to the forgetting hypothesis, a discounting cue associated with a message initially decreases acceptance of the message. As time goes by, one may observe a delayed increase of persuasion if the recipient forgets the cue but recalls the merits of the message (Hovland et al., 1949). To test this hypothesis, Hovland and his colleagues (Hovland & Weiss, 1951; Kelman & Hovland, 1953; Weiss, 1953) initiated a series of experiments in which participants received messages attributed to either trustworthy or untrustworthy sources and then completed measures of opinions as well as of recall of the message content and the source. Overall, messages with credible sources produced greater initial persuasion than messages delivered by non credible sources.\n\nHovland, Lumsdaine, and Sheffield (1949) first discovered the effect by a well-known study that demonstrated the delayed impact of a World War II propaganda movie on American soldiers.\n\nWith a subset of conditions that caused participants to question the credibility of the source in the movie, participants later reported a slight increase of persuasion (much to the researchers’ surprise). After examining the results, they initially hypothesized that forgetting of the discounting cue (in this case, the non-credible source) was causing the effect. Over time, however, the effect of the messages presented by credible sources decayed, whereas the effect of the messages presented by non-credible sources either remained the same or increased slightly. Despite evidence for the sleeper effect from this series of studies, the recall measures indicated that recipients could still remember the non-credible sources of the messages at the time of the delayed follow-up.\n\nThis is when the forgetting hypothesis was replaced by the dissociation hypothesis. Now according to the dissociation hypothesis the sleeper effect does not need to imply that the discounting cue becomes permanently unavailable in memory. A weakened association between the cue and the message may be sufficient for the sleeper effect to occur. As the association weakens over time, rendering the cue less accessible in relation to the communication topic, there may be a delayed increase in persuasion as long as the message arguments are still memorable. To this extent, factors that facilitate retention of the message content should create settings conducive to the sleeper effect.\n\nAccording to this reasoning, the sleeper effect occurs because the association between the discounting cue and the message in one’s memory becomes weakened over time; hence, when the message is recalled for purposes of producing an attitude, the source is not readily associated.\n\nSomething that Hovland and his team ignored that is important is why over time, the discounting cue becomes less accessible than the message even when both pieces are similarly effective at the onset. To answer this question Greenwald, Pratkanis, and their team (Greenwald et al., 1986; Pratkanis et al., 1988) implemented a study to identify the conditions by which the sleeper effect does and does not occur.\nPratkanis directed a series of seventeen experiments in which he presented the discounting cue either before or after the message and found that the sleeper effect occurred mostly when the cue followed the message but not when the cue was first. \nIn order to explain his findings, Pratkanis and his team proposed a modified forgetting hypothesis, which suggested that the sleeper effect occurs because the effect of the message and the cues decay at different rates. Based on this suggestion the message and the cue act like two communications operating in opposite directions. The sleeper effect emerges when the effect of these communications is about equal, promptly following message exposure, but the effect of the cue later decays more rapidly than that of the message. However, the timing of the discounting cue is essential to produce the effect because information presented first lasts longer, whereas more recent information dissipates more rapidly (Miller & Campbell, 1959). Thus, the sleeper effect should occur when the discounting cue occurs at the end of a persuasive communication and stimulates a primacy effect of the message content.\nYears later, Pratkanis, Greenwald, Leippe, and Baumgardner (1988) offered an alternative hypothesis that differed from Hovland and his colleagues.\n\nThey argued that the conditions under which the sleeper effect is more likely to occur were not emphasized by the dissociation hypothesis. Additionally, the requirements for a sleeper effect specified by Gruder et al. (1978) did not detail the empirical conditions necessary to observe the sleeper effect.\n\n\n\n"}
{"id": "9519992", "url": "https://en.wikipedia.org/wiki?curid=9519992", "title": "Surf zone", "text": "Surf zone\n\nAs ocean surface waves come closer to shore they break, forming the foamy, bubbly surface called surf. The region of breaking waves defines the surf zone. After breaking in the surf zone, the waves (now reduced in height) continue to move in, and they run up onto the sloping front of the beach, forming an uprush of water called swash. The water then runs back again as backswash. The nearshore zone where wave water comes onto the beach is the surf zone. The water in the surf zone, or breaker zone, is shallow, usually between 5 and 10 m (16 and 33 ft) deep; this causes the waves to be unstable.\n\nThe animals that often are found living in the surf zone are crabs, clams, and snails. Surf clams and mole crabs are two species that stand out as inhabitants of the surf zone. Both of these animals are very fast burrowers. The surf clam, also known as the variable coquina, is a filter feeder that uses its gills to filter microalgae, tiny zooplankton, and small particulates out of seawater. The mole crab is a suspension feeder that eats by capturing zooplankton with its antennae. All of these creatures burrow down into the sand to escape from being pulled into the ocean from the tides and waves. They also burrow themselves in the sand to protect themselves from predators. The surf zone is full of nutrients, oxygen, and sunlight which leaves the zone very productive with animal life.\n\nThe surf zone can contain dangerous rip currents: strong local currents which flow offshore and pose a threat to swimmers. Rip-current outlooks use the following set of qualifications:\n\n\n\n"}
{"id": "3911092", "url": "https://en.wikipedia.org/wiki?curid=3911092", "title": "Sydney Observatory", "text": "Sydney Observatory\n\nThe Sydney Observatory is an heritage-listed meteorological station, astronomical observatory, function venue, science museum, and education facility located on Observatory Hill at Upper Fort Street, in the inner city Sydney suburb of Millers Point in the City of Sydney local government area of New South Wales, Australia. It was designed by William Weaver (plans) and Alexander Dawson (supervision) and built from 1857 to 1859 by Charles Bingemann & Ebenezer Dewar. It is also known as The Sydney Observatory; Observatory; Fort Phillip; Windmill Hill; and Flagstaff Hill. It was added to the New South Wales State Heritage Register on 22 December 2000.\n\nThe site was formerly a defence fort, semaphore station, time ball station, meteorological station, observatory and windmills. The site evolved from a fort built on 'Windmill Hill' in the early 19th century to an observatory during the nineteenth century. It is now a working museum where evening visitors can observe the stars and planets through a modern Schmidt-Cassegrain telescope and an historic refractor telescope built in 1874, the oldest telescope in Australia in regular use.\n\nThe site of the Sydney Observatory has been a significant place in Sydney and has undergone a number of name changes. It was known as Windmill Hill in the 1790s when it was the site of the first windmill. After 1804 references are made to it as Fort Phillip or Citadel Hill, referring to the construction , but never completion, of a citadel on the site at Governor King's instruction for use in the case of an insurrection in Sydney. This was prompted by an influx of \"Death or Liberty\" Boys after the abortive 1798 uprising in Ireland, some of whom he believed to be of the most desperate character and cause for constant suspicion. Construction began but the citadel was not completed until Bligh had been installed in office. There were further discussions about a citadel during the Macquarie period but nothing eventuated beyond a half built powder magazine, Francis Greenway's first work after his appointment as civil architect in 1815.\n\nIn 1797, early on during the European settlement of New South Wales, Australia, a windmill was built on the hill above the first settlement. Within ten years the windmill had deteriorated to the point of being useless; the canvas sails were stolen, a storm damaged the machinery, and already by 1800 the foundations were giving way. The name of Millers Point remembers this early land use.\n\nIn 1803, Fort Philip was built on the site under the direction of Governor Hunter to defend the new settlement against a possible attack by the French and also from rebellious convicts. The fort was never required to be used for any such purposes. In 1825 the eastern wall of the fort was converted to a signal station. Flags were used to send messages to ships in the harbour and to the signal station on the South Head of the harbour.\n\nThe site was known as Flagstaff Hill during and after the Macquarie era. A flagstaff had been erected on the site by 1811. Flag signalling was a cumbersome process and Commissioner Bigge advised Macquarie that it was expedient to erect a semaphore at South Head and Fort Phillip. The flag and semaphore were used for signalling in a variety of combinations.\n\nAn early observatory was established in 1788 on Dawes Point, at the foot of Observatory Hill, in an ultimately unsuccessful attempt to observe in 1790 the return of a comet suggested by Edmond Halley of Halley's Comet fame. \n\nIn 1848, a new signal station was built by the Colonial Architect, Mortimer Lewis, on top of the fort wall on Windmill Hill. At the instigation of the Governor, Sir William Denison, it was agreed seven years later to build a full observatory next to the signal station. The first Government Astronomer, William Scott, was appointed in 1856, and work on the new observatory was completed in 1858.\n\nThe most important role of the observatory was to provide time through the time-ball tower. Every day at exactly 1.00 pm, the time-ball on top of the tower would drop to signal the correct time to the city and harbour below. At the same time a cannon on Dawes Point was fired, later the cannon was moved to Fort Denison. The first time-ball was dropped at noon on 5 June 1858. Soon after the drop was rescheduled to one o'clock. The time-ball is still dropped daily at 1pm using the original mechanism, but with the aid of an electric motor, not as in the early days when the ball was raised manually.\n\nAfter the federation of Australia in 1901, meteorology became a function for the Commonwealth Government from 1908, while the observatory continued its astronomical role. The observatory continued to contribute observations to \"The astrographic catalogue\", kept time and provided information to the public. For example, each day the Observatory supplied Sydney newspapers with the rising and setting times of the sun, moon and planets. A proposal to close the observatory in 1926 was narrowly avoided, but, by the mid-1970s, the increasing problems of air pollution and city light made work at the observatory more and more difficult. In 1982, the NSW Government decided that Sydney Observatory was to be converted into a museum of astronomy and related fields as part of what is now the Powerhouse Museum.\n\nIn November 1821 Governor Brisbane arrived with a set of astronomical instruments, a plan for an observatory and two personal employees with astronomical expertise - Charles Rumker and James Dunlop. Brisbane set up an observatory at the Governor's residence in Parramatta. Problems developed between Brisbane and Rumker. Rumker lost his position and it was not until Brisbane had been recalled that Rumker was reinstated by the Colonial Secretary. The following year Governor Darling, the new Governor, appointed Rumker as Government Astronomer, the first to hold the title in Australia. In 1831 Dunlop was appointed Superintendent at the observatory, Rumker again losing his position while on a visit to London.\n\nBrisbane's instruments remained at Parramatta when he left and they were used in that observatory until it was closed in 1847. The recommendation for the closure came from a commission appointed by Governor Fitzroy at the prompting of London. Dunlop had become increasingly frail and negligent and the Parramatta observatory had fallen into decay. The instruments were placed in ordnance storage at the urge of Phillip Parker King, a leading astronomer in Australia.\n\nKing argued that a government observatory should be set up, and not just the suggested time ball. King's preference for Fort Phillip to be the site was eventually accepted. In the eight years from Edmund Blacket's modest 1850 plan for the time ball observatory until its completion, the plans underwent progressive enlargement. The 1850 plan was a room for a transit telescope and timekeeping apparatus with a small ante-room. In 1851 an enlarged version was presented to the Colonial Secretary but it had no time ball tower, because neither King or Blacket, the Colonial Architect, knew how it worked. The need for an Observer's dwelling was noted.\n\nPlans were redrawn in the next couple of years. When Blacket resigned in 1854 to take on the design and supervision of construction of The University of Sydney, plans were underway for an observatory that would be both functional and of architectural quality. Blacket's successor, William Weaver, replaced him on the observatory project. Weaver was appointed Colonial Architect in October 1854. Correspondence from him to Blacket in the early years indicates that Weaver was much happier in direct supervision of works than performing the duties of his desk-bound role. As head of an over-loaded department, he complained:\n\nA Select Committee on the Colonial Architect's Department in August 1855 questioned an overpayment to the stonemasonry contractor of the Dead House at Circular Quay and accused him of defrauding the Government. Weaver, as head of the Department, was accused of negligence for paying him and subsequently submitted his resignation in apparent disgust. Weaver was only 18 months as Colonial Architect and of the two major architectural works to come from his Department during his term in office, the Government Printing Office at the corner of Phillip and Bent Streets no longer stands and the Sydney Observatory has been generally attributed to his successor. In fact, Sir William Denison approved Weaver's plans \"for an Observatory and Astronomical resicence\" in August 1855 after some specifications supplied by Denison had been incorporated. When building commenced a year later the new Colonial Architect Alexander Dawson adopted those plans.\n\nLittle more was done until the arrival of Sir William Denison as Governor General in January 1855. Denison saw an observatory as an important addition to the colony. As a result the allocated to the time ball and building was augmented by an additional vote of for a complete observatory and Denison wrote to the Astronomer Royal asking him to find a competent astronomer. Plans and estimates were submitted in August 1855 but Denison decided to defer the final decision on the site and design until the arrival of the astronomer.\n\nAlexander Dawson replaced Weaver as Colonial Architect in April 1856 and the new Government Astronomer, Reverend William Scott, M.A., arrived with his family in October that year. Tenders for the construction were advertised in February 1857. The successful tenderers were Charles Bingemenn and Ebenezer Dewar. The plans used appear to have been the work of Dawson rather than those of his predecessors, there being numerous references by Scott to consultations with the Colonial Architect on the design of the building. Extra work was approved after Bingemann and Dewar won their tender. This included the addition of a telescope dome and an increase in the height of the time ball tower. This increased height caused some dismay for Scott as it blocked out an increased area of the eastern sky.\n\nThe completed building combined, for the first time in a major Sydney building, two architectural streams - Italian High Renaissance Palazzo and the Italian Villa forms. These contributed the symmetry of the townhouse facade for the residence and an asymmetry for the observatory born of the peculiar needs of transit room, equatorial dome and time ball tower. The building was thus elevated from basic necessity to fashionable stylishness. Dawson's budget had enabled him to emphasise the distinction between the private and the public, the domestic and the official. The style and form was overlaid with early Victorian theories of fitness and association, that style should be chosen to indicate the nature and status of the building and in some cases, the site.\n\nScott occupied the residence in 1858 and commenced a trial operation of the time ball in June. His initial equipment was modest, mostly the instruments from Parramatta. He did, however, obtain the money for an equatorial telescope. In 1862 Scott resigned, recommending prominent amateur astronomer John Tebbutt as his replacement. Tebbutt declined the offer and the search for a replacement was commenced. In the meantime, his assistant Henry Chamberlain Russell was left in charge of the observatory. In January 1864 the new appointee George Robarts Smalley arrived and Russell was his second in command.\n\nIn 1870 Smalley died and was replaced by Russell. Russell's talent, entrepreneurial flair, intimate knowledge of how to work the political and bureaucratic system of NSW and longevity gave him a 35-year tenure as Government Astronomer and made him the Grand Old Man of physical science in the colonies. It was during Russell's period that Sydney Observatory was popularly believed to have been at its professional zenith, particularly from the 1870s through to the 1890s. Russell wasted no time in pressing the government for the necessary physical and instrumental resources to carry out his astronomical programs at the Observatory. The addition of a west wing designed by colonial architect James Barnett was the main work resulting from this. It provided for a major ground floor room for Russell, a library, a second equatorial dome on a tower at its northern extremity which removed the blind spot imposed by the time ball tower. An enlarged Muntz metal dome was also placed on the old equatorial tower to accommodate a new Schroeder telescope. The telescope remains a prized and functional possession today. Russell also turned his attention to improving the residence, claiming it was not large enough to accommodate his family. In 1875 Russell succeeded in securing an extension of the Observatory enclosure. Like his predecessors, he had been concerned with the restrictive nature of the Observatory grounds which made siting of meteorological and auxiliary astronomical instruments difficult, if not impossible. This extension, together with the adjacent signal station give the site its present symmetrical perimeter. The Astrographic Catalogue was Russell's greatest commitment and would affect programs at the observatory for 80 years. His interest in the application of photography to astronomy and a visit to Paris in 1887 prompted Russell to take part in a \"great star catalogue\". The Sydney Zone of the catalogue was a massive logistical enterprise and was not practically completed until 1964. Russell died in 1907 after taking leave for an extended period of time due to ill health. His assistant Alfred Lenehan was appointed acting Government Astronomer during this period and later Government Astronomer in 1907. However, in 1906 a premier's conference resolved that the Commonwealth Government would take over meteorological work, leaving astronomy to the states. Thus, the meteorological section of the Observatory became a Commonwealth agency under the direction of a former officer of the Observatory, Henry Hunt. Lenehan and Hunt continuously quarrelled and did not develop a good working relationship.\n\nIn January 1908 Lenehan had a stroke and never returned to work. At the same time the Commonwealth agency was installed in the Observatory residence. William Edward Raymond, the officer responsible for transit work, became officer in charge for four years, until the appointment of William Ernest Cooke in 1912. Cooke was lured to Sydney from Perth Observatory with promises of a new site located in Wahroongah, then free of city lights and traffic, the purchase of modern instruments and a world trip to investigate the latest developments. None of these eventuated during Cooke's fourteen years at the observatory. In 1916 the board of visitors to the Observatory was reconstituted. Russell had allowed it to lapse during his term of office and in 1917 the residence was again inhabited by the Astronomer.\n\nAll government astronomers from Scott to Cooke were worried about increasing levels of city light, vibration from traffic and magnetic disturbance which rendered the Flagstaff Hill site increasingly unsuitable. Recommendations had been made by Smalley in 1864 and others in the first quarter of the twentieth century. While Russell had managed to have the astrographic telescope relocated to Pennant Hills, there was general worry over the reaction to the cost of relocation of the whole observatory. In July 1925 Cooke wrote to his minister pointing out the problems at the site and with the equipment. The State Cabinet took him at his word and in October decided to close the Observatory rather than face the cost of removal and re-equipment. However, protests from the Board of Visitors, the Royal Society of NSW, the NSW Branch of the British Astronomical Association, the University of Sydney and interested members of the public caused the Government to change its mind and allow the observatory to continue - but with a heavily reduced staff and program. Most of the staff were transferred to other departments and Cooke was retired the following year. Only the time ball and completion of the astrographic program survived. This experience inhibited later Government Astronomers in their arguments for a new site.\n\nTwo World Wars, a great depression and a commitment to a logistically exacting astrographic program helped reduce the vitality of the establishment in the twentieth century. The deployment of major resources to the astrographic program became something of an incubus as the twentieth century progressed. The Government Astronomers could not suspend or abort the program even if they had thought it desirable. At the same time the fulfilment of international obligations under the program was largely instrumental in the survival of the Observatory.\n\nThe completion of the program in 1964 and publication of the final volume in 1971 meant the Observatory's days were numbered. Other fundamental reasons also contributed to the notion that the Observatory was no longer a viable proposition. The transfer of meteorology to the Commonwealth in 1908 removed the Observatory's most high profile public service, electric telegraphy and radio had reduced and in time eliminated the need for local navigational and time services. Ambient city light was starting to restrict astronomical observation though the place was still suitable for the time consuming analysis of the observations and other astronomical work together with functions such as a public observatory and a centre for public and media enquiries.\n\nPost World War II was an exciting time for Australian astronomical development, particularly in radio astronomy. These developments bypassed Sydney though the Government Astronomer Harley Wood kept a close involvement as the first president of the Astronomical Society of Australia (ASA) in 1966 and as the co-ordinator of the first International Astronomical Union (IAU) General Assembly to be held in the southern hemisphere in Sydney, 1973. Without major capital funds to develop its own specialisations in the west, Sydney remained tied to its traditional role. Despite this there was some positive activity at the Observatory. During the 1950s and 1960s under Wood, the Observatory enjoyed a modest renaissance. Staff numbers were built up and new equipment acquired. Both the Sydney and Melbourne sections of the Astrographic catalogue were completed and published. A new domed building was constructed in the south east corner of the Observatory to house the Melbourne star camera that replaced the original Sydney one. A new survey of the southern sky was commenced and by 1982 Wood's successor William Robertson had completed the photography and measurement was underway. Education was another aspect of the observatory's work that Wood developed. Always one of its aims, increasing numbers of visitors, including teaching students, attended the Observatory.\n\nThese activities commanded respect for Sydney Observatory in astronomical circles, but its image in the NSW Parliament and associated Public Service remained forgettable. Wood's annual reports failed to help this. They did not communicate any sense of excitement and worth in the Observatory.\n\nThe disestablishment of the Observatory echoed that of fifty years earlier when Cooke stressed the need for a new location. The Chairman of the Board of Visitors wrote a letter to the Premier in 1979 urging the establishment of a remote observing site for the Observatory and stressing the difficulty of the conditions at the existing site. This co-incided with a nation-wide review of astronomy facilities commissioned by the ASA and led by Monash University Professor of Astronomy Kevin Westfold (1980) This concluded that astronomy was a federal responsibility and that resources should be allocated to research operations, highlighting radio astronomy. It should also be noted that the State of NSW was in financial difficulties. This, and likely other pressures, resulted in a letter from the Premier in June 1982 announcing his decision to transfer the Observatory to the Museum of Applied Arts and Sciences and discontinue scientific work. Despite letters from international astronomers, and a concerted offort from now-retired Harley Wood, the Government did not rescind its decision.\n\nIn July 1984 the Minister for Public Works, Ports and Roads announced an $800,000 project to restore Sydney Observatory for astronomy education, public observatory and a Museum of Astronomy. While the importance of the exterior was recognised, the interior was less fortunate. Work inside the building in the creation of the museum involved the staged removal of almost all instruments, equipment, and furniture and furnishings to the Museum's store. The astrographic building was demolished and the dome, instruments and most of the glass plate and paper collection was removed to Macquarie University for future research use.\n\nIn 1997 the observatory was refurbished, this time instruments were returned to their original locations or showcased. 'The \"By the light of the Southern Stars\" exhibition theme also included the Parramatta Observatory instruments and Indigenous Astronomy. In 1999 a major stonemasonry repair project on the observatory building commenced. This continued through to 2008. In 2002 the conservation plan was updated by Kerr, this time complimentary on the relocation and interpretation of the instruments.\n\nA number of key astronomical events haveoccurred in recent years, most notable are Halleys Comet (1986), The impact of Shoemaker Levy on Jupiter (1994), Mars at its closest encounter (2003), transits of Venus (2004, 2012), Comet McNaught (2007), planetary alignments and eclipses. Thousands of people came to the Observatory to view these through telescopes and to see relevant exhibitions. Further the Observatory provided information about these events to many more people either directly or through the media.\n\nIn 2008, for the 150th anniversary, the Signal Station building was stabilised, one of the original two flagstaffs re-constructed and an archaeological investigation commenced around the base of the fort led by NSW Government Architects, building design and Heritage office and Casey and Lowe. Original fort footings were uncovered and the base of a room which was once a bombproof inside the fort wall foundations.\n\nIn 2009 permission was granted for a temporary marquee to be erected for a restricted period of time in order to raise funds. Furthermore the Astrographic dome and instruments have been returned by Macquarie University to the Museum store where they are awaiting conservation and a Heritage NSW approved structure on the Observatory site. The most significant change to Sydney Observatory in 50 years, the new Eastern Dome was opened on 27 January 2015, by the Deputy Premier Troy Grant and Minister for Disability Services, John Ajaka.\n\nLocated at the Sydney Observatory is a vintage 7.25-inch refracting telescope on an Equatorial mount that was manufactured by the German company Georg Merz and Sons between 1860 and 1861. The 7.25-inch Merz refracting telescope arrived at Sydney Observatory, Sydney, Australia, in 1861.\n\nThe observatory is a sandstone two-storey building in the Italianate style. There are two telescope domes on octagonal bases and a four-storey tower for the time-ball. The 1858 building designed by the Colonial Architect, Alexander Dawson, comprised a dome to house the equatorial telescope, a room with long, narrow windows for the transit telescope, an office for calculations, and a residence for the astronomer. A western wing was added in 1877 with office and library space and a second dome for another telescope. Some of the first astronomical photographs of the southern sky were taken at the observatory, under the direction of Henry Chamberlain Russell. The observatory also took part in the compilation of the first atlas of the whole sky, \"The astrographic catalogue\". The part completed at Sydney took over 70 years, from 1899 to 1971, and filled 53 volumes. The observatory once contained offices, instruments, a library and an astronomer's residence. It is now a public observatory and a museum of astronomy and meteorology.\n\nThe building is of Florentine Renaissance style and the storeys are divided by string courses while articulated quoins at corners, stone bracketed eaves and entablatures to openings of the residence contribute to the fine stone masonry work. A single storey wing to the north has had a timber balcony verandah with a stone balustrade built above. Windows are of twelve pane type and the doors are six panels.\n\nThe physical condition is good.\n\n\nAs at 20 October 2005, the Observatory is of exceptional significance in terms of European culture. Its dominant location beside and above the port town and, later, City of Sydney made it the site for a range of changing uses, all of which were important to, and reflected, stages in the development of the colony. These uses included: milling (the first windmill); defence (the first, and still extant, fort fabric); communications (the flagstaffs, first semaphore and first electric telegraph connection); astronomy, meteorology and time keeping. The surviving structures, both above and below ground, are themselves physical documentary evidence of 195 years changes of use, technical development and ways of living. As such they are a continuing resource for investigation and public interpretation.\n\nThe place has an association with an extensive array of historical figures most of whom have helped shape its fabric. These include: colonial Governors Hunter, Bligh, Macquarie & Denison; military officers and engineers Macarthur; Barrallier; Bellasis and Minchin; convicts: the as yet unnamed constructors of the mill and fort; architects: Greenway (also a convict), Lewis, Blacket, Weaver, Dawson and Barnet; signallers and telegraphists such as Jones and the family Moffitt; astronomers: particularly PP King, Scott, Smalley, Russell, Cooke and Wood. The elevation of the site, with its harbour and city views and vistas framed by mature Moreton Bay fig (\"Ficus macrophylla\") trees of the surrounding park, make it one of the most pleasant and spectacular locations in Sydney.\n\nThe picturesque Italianate character and stylistic interest of the Observatory and residence building, together with the high level of competence of the masonry (brick and stone) of all major structures on the site, combine to create a precinct of unusual quality; Finally, the continued use of the observatory for astronomical observations and the survival of astronomical instruments, equipment (Appendix 4) and some early furniture (Appendix 3), although temporarily dispersed, and the retention of most interior spaces, joinery, plasterwork, fireplaces, and supports ensure that the observatory can remain the most intact and longest serving early scientific building in the State. Also of significance for relationship of Commonwealth and State powers. Site of the first intercolonial conference on meterology and astronomy. An excellent example of a Colonial building erected for scientific purposes and continuing to perform its function at the present time. The structure makes an imposing composition atop the historic hill originally known as Flagstaff Hill and occupies the historic Fort Phillip site (1804-45). Designed by the colonial architect Alexander Dawson and built in 1858.\n\nSydney Observatory was listed on the New South Wales State Heritage Register on 22 December 2000 having satisfied the following criteria.\n\nThe place is important in demonstrating the course, or pattern, of cultural or natural history in New South Wales.\n\nThe Observatory's dominant location beside and above the port town, and later, city of Sydney, made it the site for a range of changing uses. All of these were important to, and reflected changes in the development of the colony. The place has an association with an extensive array of historical figures, most of whom have helped shape its fabric. These include colonial governors, military officers and enginees, convicts, architects and astronomers\n\nThe place is important in demonstrating aesthetic characteristics and/or a high degree of creative or technical achievement in New South Wales.\n\nThe elevation of the site with its harbour and city views and vistas framed by the mature fig trees of the surrounding park, make it one of the most pleasant and spectacular locations. The picturesque Italianate character and stylistic interest of the observatory and residence building, together with the high level of competence of the masonry (both stone and brick) of all major structures on the site, combine to create a precinct of unusual quality.\n\nThe place has potential to yield information that will contribute to an understanding of the cultural or natural history of New South Wales.\n\nThe surviving structures, both above and below ground, are themselves physical documentary evidence of 195 years of changes of use, technical development and ways of living. As such they are a continuing resource for investigation and public interpretation.\n\n\n\n"}
{"id": "26730591", "url": "https://en.wikipedia.org/wiki?curid=26730591", "title": "Terminal sliding mode", "text": "Terminal sliding mode\n\nIn the early 1990s, a new type of sliding mode control, named terminal sliding modes (TSM) was invented at the Jet Propulsion Laboratory (JPL) by Venkataraman and Gulati. TSM is robust non-linear control approach.\n\nThe main idea of terminal sliding mode control evolved out of seminal work on terminal attractors done by Zak in the JPL, and is evoked by the concept of terminal attractors which guarantee finite time convergence of the states. While, in normal sliding mode, asymptotic stability is promised which leads to the convergence of the states to the origin.\nBut this convergence may only be guaranteed within infinite time. In TSM, a nonlinear term is introduced in the sliding surface design so that the manifold is formulated as an attractor. After the sliding surface is intercepted, the trajectory is attracted within the manifold and converges to the origin following a power rule.\n\nThere are some variations of the TSM including: Non-singular TSM, Fast TSM,\n\nTerminal sliding mode also has been widely applied to nonlinear process control, for example, rigid robot control etc.. Several open questions still remain on the mathematical treatment of the system's behavior at the origin since it is non-Lipschitz.\n\nConsider a continuous nonlinear system in canonical form\n\nformula_1 ...\n\nformula_2\n\nformula_3\n\nwhere formula_4 is the state vector, formula_5 is the control\ninput, formula_6 and formula_7 are nonlinear functions in formula_8.\nThen a sequence of terminal sliding surfaces can be designed as follows:\n\nformula_9\n\nformula_10 ...\n\nformula_11 where formula_12 and formula_13 . formula_14 are positive odd numbers and formula_15.\n\nVenkataraman, S., Gulati, S., Control of Nonlinear Systems Using Terminal Sliding Modes\nJ. Dyn. Sys., Meas., Control, Sept 1993, Volume 115, Issue 3.\n"}
{"id": "10559845", "url": "https://en.wikipedia.org/wiki?curid=10559845", "title": "Test method", "text": "Test method\n\nA test method is a method for a test in science or engineering, such as a physical test, chemical test, or statistical test. It is a definitive procedure that produces a test result. In order to ensure accurate and relevant test results, a test method should be \"explicit, unambiguous, and experimentally feasible.\", as well as effective and reproducible.\n\nA test can be considered an observation or experiment that determines one or more characteristics of a given sample, product, process, or service. The purpose of testing involves a prior determination of expected observation and a comparison of that expectation to what one actually observes. The results of testing can be qualitative (yes/no), quantitative (a measured value), or categorical and can be derived from personal observation or the output of a precision measuring instrument.\n\nUsually the test result is the dependent variable, the measured response based on the particular conditions of the test or the level of the independent variable. Some tests, however, may involve changing the independent variable to determine the level at which a certain response occurs: in this case, the test result is the independent variable.\n\nIn software development, engineering, science, manufacturing, and business, its developers, researchers, manufacturers, and related personnel must understand and agree upon methods of obtaining data and making measurements. It is common for a physical property to be strongly affected by the precise method of testing or measuring that property. As such, fully documenting experiments and measurements while providing needed documentation and descriptions of specifications, contracts, and test methods is vital.\n\nUsing a standardized test method, perhaps published by a respected standards organization, is a good place to start. Sometimes it is more useful to modify an existing test method or to develop a new one, though such home-grown test methods should be validated and, in certain cases, demonstrate technical equivalency to primary, standardized methods. Again, documentation and full disclosure are necessary.\n\nA well-written test method is important. However, even more important is choosing a method of measuring the correct property or characteristic. Not all tests and measurements are equally useful: usually a test result is used to predict or imply suitability for a certain purpose. For example, if a manufactured item has several components, test methods may have several levels of connections:\n\n\nThese connections or correlations may be based on published literature, engineering studies, or formal programs such as quality function deployment. Validation of the suitability of the test method is often required.\n\nQuality management systems usually require full documentation of the procedures used in a test. The document for a test method might include:\n\n\nTest methods are often scrutinized for their validity, applicability, and accuracy. It is very important that the scope of the test method be clearly defined, and any aspect included in the scope is shown to be accurate and repeatable through validation.\n\nTest method validations often encompass the following considerations:\n\n\n\n"}
{"id": "19934039", "url": "https://en.wikipedia.org/wiki?curid=19934039", "title": "The Whetstone of Witte", "text": "The Whetstone of Witte\n\nThe Whetstone of Witte is the shortened title of Robert Recorde's mathematics book published in 1557, the full title being \"The whetstone of witte, whiche is the seconde parte of Arithmetike: containyng thextraction of Rootes: The \"Coßike\" practise, with the rule of \"Equation\": and the woorkes of \"Surde Nombers. The book covers topics including whole numbers, the extraction of roots and irrational numbers. The work is notable for containing the first recorded use of the equals sign and also for being the first book in English to use the plus and minus signs.\n\nRecordian notation for exponentiation, however, differed from the later Cartesian notation formula_1. Recorde expressed indices and surds larger than 3 in a systematic form based on the prime factorization of the exponent: a factor of two he termed a \"zenzic\", and a factor of three, a \"cubic\". Recorde termed the larger prime numbers appearing in this factorization \"sursolids\", distinguishing between them by use of ordinal numbers: that is, he defined 5 as the \"first sursolid\", written as ʃz and 7 as the \"second sursolid\", written as Bʃz.\nHe also devised symbols for these factors: a zenzic was denoted by z, and a cubic by &. For instance, he referred to \"p=p\" as zzz (the zenzizenzizenzic), and \"q=q\" as zz& (the zenzizenzicubic).\n\nLater in the book he includes a chart of exponents all the way up to \"p=p\" written as zzzzʃz. There is an error in the chart, however, writing \"p\" as Sʃz, despite it not being a prime. It should be \"p\" or &Gʃz.\n\n"}
{"id": "48323528", "url": "https://en.wikipedia.org/wiki?curid=48323528", "title": "Zenodo", "text": "Zenodo\n\nZenodo is a general-purpose open-access repository by OpenAIRE and CERN.\n\nIt was created in 2013 as the OpenAire orphan records repository, to let researchers in any subject area to comply with any open science deposit requirement absent an institutional repository.\nIt was re-launched as Zenodo in 2015 to provide a place for researchers to deposit datasets and allows upload files up to 50 GB.\n\nIt's recommended by Peter Suber as green open access repository in his instructions for researchers.\n\nIt provides a DOI to datasets and other submitted data which lacks one, to make the work easier to cite and supports various data and license types. One supported source are GitHub repositories.\n\nZenodo took part in Google Summer of Code 2017.\n\n\n\n"}
