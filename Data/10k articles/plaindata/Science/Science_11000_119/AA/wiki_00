{"id": "28913242", "url": "https://en.wikipedia.org/wiki?curid=28913242", "title": "Amos Glacier", "text": "Amos Glacier\n\nAmos Glacier () is a long glacier that flows southeast from Bettle Peak to a juncture with the Blue Glacier (southeast of Hannon Hill), in Victoria Land, Antarctica. It was named in 1992 by the Advisory Committee on Antarctic Names after Larry Leon Amos, a civil engineer with the United States Geological Survey (USGS), and a member of the USGS two man astronomic surveying team to South Pole Station and Byrd Station in the 1969–70 field season. Among other work, the team established the position of the Geographic South Pole (previously done 1956) and established a tie to the Byrd Ice Strain net which had been under study for several years.\n\n"}
{"id": "25412300", "url": "https://en.wikipedia.org/wiki?curid=25412300", "title": "Arrow security", "text": "Arrow security\n\nAn Arrow security is an instrument with a fixed payout of one unit in a specified state and no payout in other states. It is a type of hypothetical asset used in the Arrow market structure model. In contrast to the Arrow-Debreu market structure model, an Arrow market is a market in which the individual agents engage in trading assets at every time period t. In an Arrow-Debreu model, trading occurs only once at the beginning of time. An Arrow Security is an asset traded in an Arrow market structure model which encompasses a complete market.\nArrow Security is also called State Contingent Claim, Supershare Option.\n\n"}
{"id": "10969446", "url": "https://en.wikipedia.org/wiki?curid=10969446", "title": "Austin Glacier", "text": "Austin Glacier\n\nAustin Glacier () is a glacier flowing north to Beckmann Fjord, Bay of Isles, on the north coast of South Georgia. The name appears to be first used on a 1931 British Admiralty chart.\n\n"}
{"id": "7208706", "url": "https://en.wikipedia.org/wiki?curid=7208706", "title": "Bruneau jasper", "text": "Bruneau jasper\n\nBruneau jasper is a variety of the mineral jasper. It is a \"picture jasper\" – a jasper that exhibits particular patterns and colors – and is used as an opaque gemstone.\n\nThe stone exhibits layered patterns of brown, reddish brown and cream color, or sometimes of red and green color. It was discovered in the Bruneau River canyon in Idaho, near the bottom of the canyon walls, where the rhyolite in which the jasper occurs is exposed for a length of . \n\nAs of 2009 Bruneau jasper was no longer in production, and was difficult to locate.\n"}
{"id": "54097356", "url": "https://en.wikipedia.org/wiki?curid=54097356", "title": "Cashel Lookout Formation", "text": "Cashel Lookout Formation\n\nThe Cashel Lookout Formation is a formation cropping out in Newfoundland.\n"}
{"id": "24044102", "url": "https://en.wikipedia.org/wiki?curid=24044102", "title": "Cellular model", "text": "Cellular model\n\nCreating a cellular model has been a particularly challenging task of systems biology and mathematical biology.\nIt involves developing efficient algorithms, data structures, visualization and communication tools to orchestrate the integration of large quantities of biological data with the goal of computer modeling.\n\nIt is also directly associated with bioinformatics, computational biology and Artificial life.\n\nIt involves the use of computer simulations of the many cellular subsystems such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks to both analyze and visualize the complex connections of these cellular processes.\n\nThe complex network of biochemical reaction/transport processes and their spatial organization make the development of a predictive model of a living cell a grand challenge for the 21st century.\n\nThe eukaryotic cell cycle is very complex and is one of the most studied topics, since its misregulation leads to cancers.\nIt is possibly a good example of a mathematical model as it deals with simple calculus but gives valid results. Two research groups have produced several models of the cell cycle simulating several organisms. They have recently produced a generic eukaryotic cell cycle model which can represent a particular eukaryote depending on the values of the parameters, demonstrating that the idiosyncrasies of the individual cell cycles are due to different protein concentrations and affinities, while the underlying mechanisms are conserved (Csikasz-Nagy et al., 2006).\nBy means of a system of ordinary differential equations these models show the change in time (dynamical system) of the protein inside a single typical cell; this type of model is called a deterministic process (whereas a model describing a statistical distribution of protein concentrations in a population of cells is called a stochastic process).\nTo obtain these equations an iterative series of steps must be done: first the several models and observations are combined to form a consensus diagram and the appropriate kinetic laws are chosen to write the differential equations, such as rate kinetics for stoichiometric reactions, Michaelis-Menten kinetics for enzyme substrate reactions and Goldbeter–Koshland kinetics for ultrasensitive transcription factors, afterwards the parameters of the equations (rate constants, enzyme efficiency coefficients and Michaelis constants) must be fitted to match observations; when they cannot be fitted the kinetic equation is revised and when that is not possible the wiring diagram is modified. The parameters are fitted and validated using observations of both wild type and mutants, such as protein half-life and cell size.\nIn order to fit the parameters the differential equations need to be studied. This can be done either by simulation or by analysis. \nIn a simulation, given a starting vector (list of the values of the variables), the progression of the system is calculated by solving the equations at each time-frame in small increments.\nIn analysis, the properties of the equations are used to investigate the behavior of the system depending of the values of the parameters and variables. A system of differential equations can be represented as a vector field, where each vector described the change (in concentration of two or more protein) determining where and how fast the trajectory (simulation) is heading. Vector fields can have several special points: a stable point, called a sink, that attracts in all directions (forcing the concentrations to be at a certain value), an unstable point, either a source or a saddle point which repels (forcing the concentrations to change away from a certain value), and a limit cycle, a closed trajectory towards which several trajectories spiral towards (making the concentrations oscillate). A better representation which can handle the large number of variables and parameters is called a bifurcation diagram (bifurcation theory): the presence of these special steady-state points at certain values of a parameter (e.g. mass) is represented by a point and once the parameter passes a certain value, a qualitative change occurs, called a bifurcation, in which the nature of the space changes, with profound consequences for the protein concentrations: the cell cycle has phases (partially corresponding to G1 and G2) in which mass, via a stable point, controls cyclin levels, and phases (S and M phases) in which the concentrations change independently, but once the phase has changed at a bifurcation event (cell cycle checkpoint), the system cannot go back to the previous levels since at the current mass the vector field is profoundly different and the mass cannot be reversed back through the bifurcation event, making a checkpoint irreversible. In particular the S and M checkpoints are regulated by means of special bifurcations called a Hopf bifurcation and an infinite period bifurcation.\nCell Collective is a modeling software that enables one to house dynamical biological data, build computational models, stimulate, break and recreate models. The development is led by Tomas Helikar, a researcher within the field of computational biology. It is designed for biologists, students learning about computational biology, teachers focused on teaching life sciences, and researchers within the field of life science. The complexities of math and computer science are built into the backend and one can learn about the methods used for modeling biological species, but complex math equations, algorithms, programming are not required and hence won't impede model building.\n\nThe mathematical framework behind Cell Collective is based on a common qualitative (discrete) modeling technique where the regulatory mechanism of each node is described with a logical function [for more comprehensive information on logical modeling, see ].\n\nModel validation\nThe model was constructed using local (e.g., protein–protein interaction) information from the primary literature. In other words, during the construction phase of the model, there was no attempt to determine the local interactions based on any other larger phenotypes or phenomena. However, after the model was completed, verification of the accuracy of the model involved testing it for the ability to reproduce complex input–output phenomena that have been observed in the laboratory. To do this, the T-cell model was simulated under a multitude of cellular conditions and analyzed in terms of input–output dose–response curves to determine whether the model behaves as expected, including various downstream effects as a result of activation of the TCR, G-protein-coupled receptor, cytokine, and integrin pathways.\n\nThe E-Cell Project aims \"to make precise whole cell simulation at the molecular level possible\".\n\nCytoSolve - developed by V. A. Shiva Ayyadurai and C. Forbes Dewey Jr. of Department of Biological Engineering at the Massachusetts Institute of Technology - provided a method to model the whole cell by dynamically integrating multiple molecular pathway models. .\"\n\nIn the July 2012 issue of \"Cell\", a team led by Markus Covert at Stanford published the most complete computational model of a cell to date. The model of the roughly 500-gene \"Mycoplasma genitalium\" contains 28 algorithmically-independent components incorporating work from over 900 sources. It accounts for interactions of the complete genome, transcriptome, proteome, and metabolome of the organism, marking a significant advancement for the field.\n\nMost attempts at modeling cell cycle processes have focused on the broad, complicated molecular interactions of many different chemicals, including several cyclin and cyclin-dependent kinase molecules as they correspond to the S, M, G1 and G2 phases of the cell cycle. In a 2014 published article in PLOS computational biology, collaborators at University of Oxford, Virginia Tech and Institut de Génétique et Développement de Rennes produced a simplified model of the cell cycle using only one cyclin/CDK interaction. This model showed the ability to control totally functional cell division through regulation and manipulation only the one interaction, and even allowed researchers to skip phases through varying the concentration of CDK. This model could help understand how the relatively simple interactions of one chemical translate to a cellular level model of cell division.\n\nMultiple projects are in progress.\n\n"}
{"id": "41651205", "url": "https://en.wikipedia.org/wiki?curid=41651205", "title": "Chain fountain", "text": "Chain fountain\n\nThe chain fountain phenomenon, also known as the self-siphoning beads or the Mould effect, is a counterintuitive physical phenomenon observed with a chain of beads placed inside a jar, when one end of the chain is yanked from the jar and is allowed to fall to the floor beneath. This establishes a self-sustaining flow of the chain of beads which rises up from the jar into an arch ascending into the air over and above the edge of the jar with a noticeable gap, and down to the floor or ground beneath it, as if being sucked out of the jar by an invisible siphon.\n\nThe effect is most pronounced when using a long ball chain with rigid links. The higher the jar is placed above the ground, the higher the chain will rise above the jar during the \"siphoning\" phase. As demonstrated in an experiment, when the jar is placed 30 meters above the ground and the chain is sufficiently long, the arch of the chain fountain can reach the height about above the jar.\n\nThe phenomenon was first brought to widespread public attention in a video made by science presenter Steve Mould.\nMould's YouTube video, in which he demonstrated the phenomenon of self-siphoning beads and proposed an explanation, brought the problem to the attention of academics John Biggins and Mark Warner of Cambridge University, who published their findings about what has now been called the \"chain fountain\" in \"Proceedings of the Royal Society A\".\n\nA variety of explanations have been proposed as to how the phenomenon can best be explained in terms of kinematic physics concepts such as energy and momentum. The scientific consensus has shown that the chain fountain effect is driven by upward forces which originate inside the jar. The origin of the upward force is related to the stiffness of the chain links, and the bending restrictions of each chain joint. When a link of chain is pulled upward from the jar, it rotates like a stiff rod being picked up from one end. This rotation produces a downward force on the opposite end of the link, which in turn generates an upward reactive force. It is this upward reactive force that has been shown to drive the chain fountain phenomenon. Furthermore, because the beads of chain can drag laterally within the jar across other stationary links, the moving beads of the chain can bounce or jump vertically when they strike the immobile links. This effect contributes to the effect, but is not the primary driver.\n\n\n"}
{"id": "429171", "url": "https://en.wikipedia.org/wiki?curid=429171", "title": "Characterology", "text": "Characterology\n\nCharacterology (from Greek \"character\" and , \"-logia\") is a method of character reading that attempted to combine revised physiognomy, reconstructed phrenology and amplified pathognomy, with ethnology, sociology and anthropology. Developed by L. Hamilton McCormick in the 1920s, characterology was an attempt to produce a scientific, objective system to assess an individual's character.\n\nCharacterology attempted to resolve flaws in the phrenological systems of Franz Joseph Gall \nand Johann Spurzheim. McCormick tried to distance himself from those earlier systems, and wrote extensively about how his ideas improved upon them.\n\nMcCormick suggested possible applications for characterology, e.g., advice for parents and educators, guidance in military officer promotions, evaluating thinking patterns (i.e., reason-oriented or memory-oriented ), assessing business associates and competitors, career counseling, and selecting marital partners.\n\n"}
{"id": "28136455", "url": "https://en.wikipedia.org/wiki?curid=28136455", "title": "Circulation of elite", "text": "Circulation of elite\n\nThe circulation of elite is a theory of regime change described by Italian social scientist Vilfredo Pareto (1848–1923).\n\nChanges of regime, revolutions, and so on occur not when rulers are overthrown from below, but when one elite replaces another. The role of ordinary people in such transformation is not that of initiators or principal actors, but as followers and supporters of one elite or another.\n\nIt is a basic axiom for Pareto that people are unequal physically, as well as intellectually and morally. In society as a whole, and in any of its particular strata and groupings, some people are more gifted than others. Those who are most capable in any particular grouping are the elite.\n\nThe term elite has no moral or honorific connotations in Pareto's usage. It denotes simply \"a class of the people who have the highest indices in their branch of activity.\" Pareto argues that \"It will help if we further divide that [elite] class into two classes: a governing elite, comprising individuals who directly or indirectly play some considerable part in government, and a non-governing elite, comprising the rest.\" His main discussion focuses on the governing elite.\n\nThere is a basic ambiguity in Pareto's treatment of the notion of the elite. In some passages, as in the one quoted above, it would appear that those occupying elite positions are, by definition, the most qualified. But there are many other passages where Pareto asserts that people are assigned elite positions by virtue of being so labeled. That is, men assigned elite positions may not have the requisite capabilities, while others not so labeled may have them.\n\nIt would seem that Pareto believed that only in perfectly open societies, those with perfect social mobility, would elite position correlate fully with superior capacity. Only under such conditions would the governing elite, for example, consist of the people most capable of governing. The actual social fact is that obstacles such as inherited wealth, family connections, and the like prevent the free circulation of individuals through the ranks of society, so that those wearing an elite label and those possessing highest capacity tend to diverge to greater or lesser degrees.\n\nGiven the likelihood of divergencies between ascribed elite position and actual achievement and capacity, Pareto is a passionate advocate of maximum social mobility and of careers open to all. He saw the danger that elite positions that were once occupied by men of real talent would in the course of time be preempted by men devoid of such talent.\n\nWhen governing or nongoverning elites attempt to close themselves to the influx of newer and more capable elements from the underlying population, when the circulation of elites is impeded, social equilibrium is upset and the social order will decay. Pareto argued that if the governing elite does not \"find ways to assimilate the exceptional individuals who come to the front in the subject classes,\" an imbalance is created in the body politic and the body social until this condition is rectified, either through a new opening of channels of mobility or through violent overthrow of an old ineffectual governing elite by a new one that is capable of governing.\n\nPareto introduced a social taxonomy that included six classes, Class I through Class VI. Class I corresponds to the adventurous \"foxes\" in Machiavelli, and Class II to the conservative \"lions,\" particularly in the governing elite.\n\nNot only are intelligence and aptitudes unequally distributed among the members of society, but the residues as well. Under ordinary circumstances, the \"conservative\" residues of Class II preponderate in the masses and thus make them submissive. The governing elite, however, if it is to be effective, must consist of individuals who have a strong mixture of both Class I and Class II elements.\n\nThe ideal governing class contains a judicious mixture of lions and foxes, of men capable of decisive and forceful action and of others who are imaginative, innovative, and unscrupulous. When imperfections in the circulation of governing elites prevent the attainment of such judicious mixtures among the governing, regimes either degenerate into hidebound and ossified bureaucracies incapable of renewal and adaptation, or into weak regimes of squabbling lawyers and rhetoricians incapable of decisive and forceful action. When this happens, the governed will succeed in overthrowing their rulers and new elites will institute a more effective regime.\n\nWhat applies to political regimes applies to the economic realm as well. In this field, \"speculators\" are akin to the foxes and \"rentiers\" to the lions. Speculators and rentiers do not only have different interests but they reflect different temperaments and different residues. Neither is very good at using force, but they both otherwise fall roughly into the same dichotomous classes that explain political fluctuations.\n\nIn the speculator group Class I residues predominate, in the rentier group, Class II residues. . . . The two groups perform functions of differing utility in society. The [speculator] group is primarily responsible for change, for economic and social progress. The [rentier] group, instead, is a powerful element in stability, and in many cases counteracts the dangers attending the adventurous capers of the [speculators]. A society in which the [rentiers] almost exclusively predominate remains stationary and, as it were, crystallized. A society in which [the speculators] predominate lacks stability, lives in a state of shaky equilibrium that may be upset by a slight accident from within or from without.\n\nLike in the governing elite where things work best when both residues of Class I and Class II are represented, so in the economic order maximum effectiveness is attained when both rentiers and speculators are present, each providing a balance by checking the excesses of the other. Pareto implies throughout that a judicious mixture in top elites of men with Class I and Class II residues makes for the most stable economic structure, as well as for the most enduring political structure.\n"}
{"id": "227956", "url": "https://en.wikipedia.org/wiki?curid=227956", "title": "Columbus (ISS module)", "text": "Columbus (ISS module)\n\nColumbus is a science laboratory that is part of the International Space Station (ISS) and is the largest single contribution to the ISS made by the European Space Agency (ESA).\n\nLike the \"Harmony\" and \"Tranquility\" modules, the \"Columbus\" laboratory was constructed in Turin, Italy by Thales Alenia Space. The functional equipment and software of the lab was designed by EADS in Bremen, Germany. It was also integrated in Bremen before being flown to the Kennedy Space Center (KSC) in Florida in an Airbus Beluga. It was launched aboard Space Shuttle \"Atlantis\" on February 7, 2008 on flight STS-122. It is designed for ten years of operation. The module is controlled by the Columbus Control Centre, located at the German Space Operations Centre, part of the German Aerospace Center in Oberpfaffenhofen near Munich, Germany.\n\nThe European Space Agency has spent €1.4 billion (about US$2 billion) on building \"Columbus\", including the experiments that will fly in it and the ground control infrastructure necessary to operate them.\n\nThe laboratory is a cylindrical module with two end cones. It is in external diameter and in overall length, excluding the projecting external experiment racks. Its shape is very similar to that of the Multi-Purpose Logistics Modules (MPLMs),\nsince both were designed to fit in the cargo bay of a Space Shuttle orbiter. The starboard end cone contains most of the laboratory's on-board computers. The port end cone contains the Common Berthing Mechanism.\n\nESA chose EADS Astrium Space Transportation as prime contractor for \"Columbus\". The \"Columbus\" flight structure, the micro-meteorite protection system, the active and passive thermal control, the environmental control, the harness and all the related ground support equipment were designed and qualified by Alcatel Alenia Space in Turin, Italy as defined by the PICA – Principle (for definition see History below); the related hardware was pre-integrated and sent as PICA in September 2001 to Bremen. The lab was then fully integrated and qualified on system level at the EADS Astrium Space Transportation facilities in Bremen, Germany.\n\nIn November 2007, \"Columbus\" was moved out of the KSC Space Station Processing Facility, and installed into the payload bay of the \"Atlantis\" orbiter for launch on ISS assembly flight 1E (STS-122).\n\nDuring cryo-filling of the space shuttle External Tank (ET) with liquid hydrogen and liquid oxygen prior to the first launch attempt on December 6, 2007, two of four LH2 ECO sensors failed a test. Mission rules called for at least three of the four sensors to be in working order for a launch attempt to proceed. As a result of the failure, the Launch Director Doug Lyons postponed the launch, initially for 24 hours. This was later revised into a 72-hour delay, resulting in a next launch attempt set for Sunday December 9, 2007. This launch attempt was scrubbed when one of the ECO sensors again failed during fuelling.\n\nThe ECO sensors external connector was changed on the space shuttle external tank, causing a two-month delay in the launch. \"Columbus\" was finally launched successfully on the third attempt at 2:45pm EST, February 7, 2008.\nOnce at the station, Canadarm2 removed \"Columbus\" from the docked shuttle's cargo bay\nand attached it to the starboard hatch of \"Harmony\" (also known as Node 2),\nwith the cylinder pointing outwards on February 11, 2008.\n\nActivities in the lab are controlled on the ground by the Columbus Control Center (at DLR Oberpfaffenhofen in Germany) and by the associated User Support Operations Centres throughout Europe.\n\nThe laboratory can accommodate ten active International Standard Payload Racks (ISPRs) for science payloads.\nAgreements with NASA allocate to ESA 51% usage of the Columbus Laboratory.\nESA is thus allocated five active rack locations, with the other five being allocated to NASA.\nFour active rack locations are on the forward side of the deck, four on the aft side, and two are in overhead locations.\nThree of the deck racks are filled with life support and cooling systems.\nThe remaining deck rack and the two remaining overhead racks are storage racks.\n\nIn addition, four un-pressurized payload platforms can be attached outside the starboard cone, on the Columbus External Payload Facility (CEPF). Each external payload is mounted on an adaptor able to accommodate small instruments and experiments totalling up to .\n\nThe following European ISPRs have been initially installed inside \"Columbus\":\n\nThe first external payloads were mounted on \"Columbus\" by crew members of the mission STS-122 mission. The three payloads mounted are:\n\nPlanned additional external payloads:\n\nSee also:\n\nIn 2014 the ISS-RapidScat instrument was installed, which was operated until late 2016. ISS-RapidScat was transported to ISS by the SpaceX CRS-4 spaceflight.\n\n\"Columbus\" was originally planned as part of the Columbus program, an ESA program to develop an autonomous manned space station that could be used for a variety of microgravity experiments. The program ran from 1986 to 1991 and included three components: a \"Man-Tended Free Flyer\" (MTFF) element serviced by the Hermes shuttle, an \"Attached Pressurized Module\" (APM), and a Polar Platform (\"PPF\"). After several budget cuts and cancellation of Hermes shuttle, all that remained in the Columbus program was the APM, finally renamed to \"Columbus\".\n\nWhen only the APM was left in the program there were not enough tasks for the two main contributors Germany and Italy represented by MBB-ERNO and Alenia respectively. As compromise the PICA (Pre Integrated Columbus APM) – Principle was invented meaning a split responsibility where Alenia as a co-prime is responsible for the overall Columbus configuration, the mechanical and thermal/life support systems, HFE and harness design/manufacturing whereas EADS Astrium Space Transportation is responsible for the overall Columbus design and all Avionics systems including electrical harness design and software. Splitting off systems engineering responsibility and harness design under separate fixed-price contracts was found not to be advantageous with respect to efficiency and fast decision making as financial reasonings were pre-dominant in the last phase of development and verification.\n\nThe structure used for \"Columbus\" is based on the MPLM module built for NASA by Thales Alenia Space. In 2000 the pre-integrated module (structure including harness and tubing) was delivered to Bremen in Germany by the Co-prime contractor Alenia. The final integration and system testing was performed by the overall prime contractor EADS Astrium Space Transportation, after that the initial Payload was integrated and the overall complement checked-out.\n\nOn May 27, 2006 \"Columbus\" was flown from Bremen, Germany to Kennedy Space Center on board an Airbus Beluga.\n\nThe final schedule was much longer than originally planned due to development problems (several caused by the complex responsibility splitting between the Co-prime and the Overall prime contractor) and design changes introduced by ESA but being affordable due to the Shuttle problems delaying the \"Columbus\" launch for several years. The main design change was the addition of the \"External Payload Facility\" (EPF), which was driven by the different European Payload organizations being more interested in outer space than internal experiments. Also the addition of a terminal for direct communications to/from ground, which could have been used also as back-up for the \"ISS\" system, was studied but not implemented for cost reasons.\n\n\n"}
{"id": "889722", "url": "https://en.wikipedia.org/wiki?curid=889722", "title": "Coronagraph", "text": "Coronagraph\n\nA coronagraph is a telescopic attachment designed to block out the direct light from a star so that nearby objects – which otherwise would be hidden in the star's bright glare – can be resolved. Most coronagraphs are intended to view the corona of the Sun, but a new class of conceptually similar instruments (called \"stellar coronagraphs\" to distinguish them from \"solar coronagraphs\") are being used to find extrasolar planets and circumstellar disks around nearby stars.\n\nThe coronagraph was introduced in 1931 by the French astronomer Bernard Lyot; since then, coronagraphs have been used at many solar observatories. Coronagraphs operating within Earth's atmosphere suffer from scattered light in the sky itself, due primarily to Rayleigh scattering of sunlight in the upper atmosphere. At view angles close to the Sun, the sky is much brighter than the background corona even at high altitude sites on clear, dry days. Ground-based coronagraphs, such as the High Altitude Observatory's Mark IV Coronagraph on top of Mauna Loa, use polarization to distinguish sky brightness from the image of the corona: both coronal light and sky brightness are scattered sunlight and have similar spectral properties, but the coronal light is Thomson-scattered at nearly a right angle and therefore undergoes scattering polarization, while the superimposed light from the sky near the Sun is scattered at only a glancing angle and hence remains nearly unpolarized.\n\nCoronagraph instruments are extreme examples of stray light rejection and precise photometry because the total brightness from the solar corona is less than one millionth (10) the brightness of the Sun. The apparent surface brightness is even fainter because, in addition to delivering less total light, the corona has a much greater apparent size than the Sun itself.\n\nDuring a total solar eclipse, the Moon acts as an occluding disk and any camera in the eclipse path may be operated as a coronagraph until the eclipse is over. More common is an arrangement where the sky is imaged onto an intermediate focal plane containing an opaque spot; this focal plane is reimaged onto a detector. Another arrangement is to image the sky onto a mirror with a small hole: the desired light is reflected and eventually reimaged, but the unwanted light from the star goes through the hole and does not reach the detector. Either way, the instrument design must take into account scattering and diffraction to make sure that as little unwanted light as possible reaches the final detector. Lyot's key invention was an arrangement of lenses with stops, known as Lyot stops, and baffles such that light scattered by diffraction was focused on the stops and baffles, where it could be absorbed, while light needed for a useful image missed them.\n\nAs an example, imaging instruments on the Hubble Space Telescope offer coronagraphic capability.\n\nA \"band-limited coronagraph\" uses a special kind of mask called a \"band-limited mask\". This mask is designed to block light and also manage diffraction effects caused by removal of the light. The band-limited coronagraph has served as the baseline design for the canceled Terrestrial Planet Finder coronagraph. Band-limited masks will also be available on the James Webb Space Telescope.\n\nSee also: \n\nA phase-mask coronagraph (such as the so-called four-quadrant phase-mask coronagraph) uses a transparent mask to shift the phase of the stellar light in order to create a self-destructive interference, rather than a simple opaque disc to block it. See also: \n\nAn optical vortex coronagraph uses a phase-mask in which the phase-shift varies azimuthally around the center. Several varieties of optical vortex coronagraphs exist:\n\n\nThis works with stars other than the sun because they are so far away their light is, for this purpose, a spatially coherent plane wave. The coronagraph using interference masks out the light along the center axis of the telescope, but allows the light from off axis objects through.\n\nCoronagraphs in outer space are much more effective than the same instruments would be if located on the ground. This is because the complete absence of atmospheric scattering eliminates the largest source of glare present in a terrestrial coronagraph. Several space missions such as NASA-ESA's SOHO, and NASA's SPARTAN, Solar Maximum Mission, and Skylab have used coronagraphs to study the outer reaches of the solar corona. The Hubble Space Telescope (HST) is able to perform coronagraphy using the Near Infrared Camera and Multi-Object Spectrometer (NICMOS), and there are plans to have this capability on the James Webb Space Telescope (JWST) using its Near Infrared Camera (NIRCam) and Mid Infrared Instrument (MIRI).\n\nWhile space-based coronagraphs such as LASCO avoid the sky brightness problem, they face design challenges in stray light management under the stringent size and weight requirements of space flight. Any sharp edge (such as the edge of an occulting disk or optical aperture) causes Fresnel diffraction of incoming light around the edge, which means that the smaller instruments that one would want on a satellite unavoidably leak more light than larger ones would. The LASCO C-3 coronagraph uses both an external occulter (which casts shadow on the instrument) and an internal occulter (which blocks stray light that is Fresnel-diffracted around the external occulter) to reduce this \"leakage\", and a complicated system of baffles to eliminate stray light scattering off the internal surfaces of the instrument itself.\n\nThe coronagraph has recently been adapted to the challenging task of finding planets around nearby stars. While stellar and solar coronagraphs are similar in concept, they are quite different in practice because the object to be occulted differs by a factor of a million in linear apparent size. (The Sun has an apparent size of about 1900 arcseconds, while a typical nearby star might have an apparent size of 0.0005 and 0.002 arcseconds.) Earth-like exoplanet detection requires 10 contrast . To achieve such contrast requires extreme optothermal stability.\n\nA stellar coronagraph concept was studied for flight on the canceled Terrestrial Planet Finder mission. On ground-based telescopes, a stellar coronagraph can be combined with adaptive optics to search for planets around nearby stars .\n\nThis link shows an HST image of a dust disk surrounding a bright star with the star hidden by the coronagraph. \n\nIn November 2008, NASA announced that a planet was directly observed orbiting the nearby star Fomalhaut. The planet could be seen clearly on images taken by Hubble's Advanced Camera for Surveys' coronagraph in 2004 and 2006 . The dark area hidden by the coronagraph mask can be seen on the images, though a bright dot has been added to show where the star would have been.\nUp until the year 2010, telescopes could only directly image exoplanets under exceptional circumstances. Specifically, it is easier to obtain images when the planet is especially large (considerably larger than Jupiter), widely separated from its parent star, and hot so that it emits intense infrared radiation. However, in 2010 a team from NASAs Jet Propulsion Laboratory demonstrated that a vector vortex coronagraph could enable small telescopes to directly image planets. They did this by imaging the previously imaged HR 8799 planets using just a 1.5 m portion of the Hale Telescope.\n\n\n"}
{"id": "11825732", "url": "https://en.wikipedia.org/wiki?curid=11825732", "title": "DEN 0255-4700", "text": "DEN 0255-4700\n\nDENIS 0255-4700 is an extremely faint brown dwarf approximately 16 light years from the Solar System in the southern constellation of Eridanus. It is the closest isolated L brown dwarf (no undiscovered L dwarves are expected to be closer), and only after the binary Luhman 16. It is also the faintest brown dwarf (with the absolute magnitude of M=24.44) having measured visible magnitude. A number of nearer T and Y-type dwarfs are known, specifically WISE 0855−0714, Epsilon Indi B and C, SCR 1845-6357 B, DEN 1048-3956, and UPGS 0722-05.\n\nDENIS 0255-4700 was identified for the first time as a probable nearby object in 1999. Its proximity to the Solar System was established by the RECONS group in 2006 when its trigonometric parallax was measured. DENIS 0255-4700 has a relatively small tangential velocity of .\n\nThe photospheric temperature of DENIS 0255-4700 is estimated at about 1300 K. Its atmosphere in addition to hydrogen and helium contains water vapor, methane and possibly ammonia. The mass of DENIS 0255-4700 lies in the range from 25 to 65 Jupiter masses corresponding to the age range from 0.3 to 10 billion years.\n\n\n"}
{"id": "9074164", "url": "https://en.wikipedia.org/wiki?curid=9074164", "title": "EIA standards", "text": "EIA standards\n\nHere is a list of Electronic Industries Alliance (EIA) Standards. The EIA ceased operations on February 11, 2011, but the former sectors continue to serve the constituencies of EIA. EIA designated ECA to continue to develop standards for interconnect, passive and electro-mechanical (IP&E) electronic components under the ANSI-designation of EIA standards. All other electronic components standards are managed by their respective sectors. ECA is expected to merge with the National Electronic Distributors Association (NEDA) to form the Electronic Components Industry Association (ECIA). However, the EIA standards brand will continue for IP&E standards within ECIA. As currently authorized, any ANSI standard designated at ANSI EIA-xxx is developed and/or managed by ECA (and, in the future, ECIA).\n\nFormer EIA Sectors:\n\n Note: EIA-170 only appears on the cover page of the document, internal pages are labeled RS-170 in the upper right corner. \n EIA-170/RS-170 applies only to monochrome (black & white) video. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3131342", "url": "https://en.wikipedia.org/wiki?curid=3131342", "title": "Electrogravitics", "text": "Electrogravitics\n\nElectrogravitics is claimed to be an unconventional type of effect or anti-gravity force created by an electric field's effect on a mass. The name was coined in the 1920s by the discoverer of the effect, Thomas Townsend Brown, who spent most of his life trying to develop it and sell it as a propulsion system. Through Brown's promotion of the idea it was researched for a short while by aerospace companies in the 1950s. Electrogravitics is popular with conspiracy theorists with claims that it is powering flying saucers and the B-2 Stealth Bomber.\n\nSince apparatus based on Browns' ideas have often yielded varying and highly controversial results when tested within controlled vacuum conditions, the effect observed has often been attributed to the ion drift or ion wind effect instead of anti-gravity.\n\nElectrogravitics had its origins in experiments started in 1921 by Thomas Townsend Brown (who coined the name) while he was still in high school. He discovered an unusual effect while experimenting with a Coolidge tube, a type of X-ray vacuum tube where, if he placed on a balance scale with the tube’s positive electrode facing up, the tube's mass seemed to decrease, when facing down the tube's mass seemed to increase. Brown showed this effect to his college professors and even newspaper reporters and told them he was convinced that he had managed to influence gravity electronically. Brown developed this into large high-voltage capacitors that would produce a tiny propulsive force causing the capacitor to jump in one direction when the power was turned on. In 1929 Brown published \"How I Control Gravitation,\" in \"Science and Invention\" where he claimed the capacitors were producing a mysterious force that interacted with the pull of gravity. He envisions a future where, if his device could be scaled up, \"Multi-impulse gravitators weighing hundreds of tons may propel the ocean liners of the future\" or even \"fantastic 'space cars'\" to Mars. Somewhere along the way Brown came up with the name Biefeld–Brown effect, named after his former teacher, professor of astronomy Paul Alfred Biefeld at Denison University in Ohio. Brown claimed Biefeld as his mentor and co-experimenter. After World War II Brown sought to develop the effect as a means of propulsion for aircraft and spacecraft, demonstrating a working apparatus to an audience of scientists and military officials in 1952. A Cal-Tech physicist invited to observe Brown's disk device in the early 50s noted during the demonstration that its motivation force was the well known phenomenon of \"electric wind\", and not anti-gravity, saying “I’m afraid these gentlemen played hooky from their high school physics classes….” Research in the phenomenon was popular in the mid-1950s, at one point the Glenn L. Martin Company placed advertisements looking for scientists who were \"interested in gravity\", but rapidly declined in popularity thereafter.\n\nSince this effect could not be explained by known physics at the time, the effect has been believed to be caused by ionized particles that produces a type of ion drift or ionic wind that transfers its momentum to surrounding neutral particles, an electrokinetic phenomena or more widely referred to as \"electrohydrodynamics\" (EHD).\n\nElectrogravitics has become popular with UFO, anti-gravity, and government conspiracy theorists where it is seen as an example of something much more exotic than electrokinetics, i.e. that electrogravitics is a true anti-gravity technology that can \"create a force that depends upon an object’s mass, even as gravity does\". There are claims that all major aerospace companies in the 1950s including Martin, Convair, Lear, Sperry, Raytheon were working on it, that the technology became highly classified in the early 1960s, that it is used to power the B-2 bomber, and that it can be used to generate \"free energy\". Charles Berlitz devoted an entire chapter of his book on The Philadelphia Experiment (\"The Philadelphia Experiment: Project Invisibility\") to a retelling of Brown's early work with the effect, implying the electrogravitics effect was being used by UFOs. The researcher and author Paul LaViolette has produced many self-published books on electrogravitics, making many claims over the years including his view that the technology could have helped to avoid another Space Shuttle Columbia disaster.\n\nMany claims as to the validity of electrogravitics as an anti-gravity force revolve around research and videos on the internet purported to show lifter-style capacitor devices working in a vacuum, therefore not receiving propulsion from ion drift or ion wind being generated in air. Followups on the claims (R. L. Talley in a 1990 U.S. Air Force study, NASA scientist Jonathan Campbell in a 2003 experiment, and Martin Tajmar in a 2004 paper) have found that no thrust could be observed in a vacuum, consistent with the phenomenon of ion wind. Campbell pointed out to a Wired magazine reporter that creating a true vacuum similar to space for the test requires tens of thousands of dollars in equipment.\n\nByron Preiss in his 1985 book on the current science and future of the Solar System titled \"The Planets\" commented that electrogravitics development seemed to be \"much ado about nothing, started by a bunch of engineers who didn't know enough physics\". Preiss stated that electrogravitics, like exobiology, is \"a science without a single specimen for study\".\n\n\n"}
{"id": "4505203", "url": "https://en.wikipedia.org/wiki?curid=4505203", "title": "Flags of Europe", "text": "Flags of Europe\n\nThis is a list of international, national and subnational flags used in Europe.\n\nMany states have separate civil and state versions of their flags; the state flags (listed) include the state arms, while the civil versions don't. See Flags of German states.\n\n"}
{"id": "49955849", "url": "https://en.wikipedia.org/wiki?curid=49955849", "title": "Hidden Figures", "text": "Hidden Figures\n\nHidden Figures is a 2016 American biographical drama film directed by Theodore Melfi and written by Melfi and Allison Schroeder. It is loosely based on the non-fiction book of the same name by Margot Lee Shetterly about black female mathematicians who worked at the National Aeronautics and Space Administration (NASA) during the Space Race. The film stars Taraji P. Henson as Katherine Johnson, a mathematician who calculated flight trajectories for Project Mercury and other missions. The film also features Octavia Spencer as NASA supervisor and mathematician Dorothy Vaughan and Janelle Monáe as NASA engineer Mary Jackson, with Kevin Costner, Kirsten Dunst, Jim Parsons, Glen Powell, and Mahershala Ali in supporting roles.\n\nPrincipal photography began in March 2016 in Atlanta and was wrapped up in May 2016. \"Hidden Figures\" had a limited release on December 25, 2016, by 20th Century Fox, before going wide in the United States on\nJanuary 6, 2017. The film received positive reviews from critics and grossed $236 million worldwide. It was chosen by National Board of Review as one of the top ten films of 2016 and was nominated for numerous awards, including three Oscar nominations (Best Picture, Best Adapted Screenplay and Best Supporting Actress for Spencer), and two Golden Globes (Best Supporting Actress for Spencer and Best Original Score). It also won the Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture.\n\nIn 1961, mathematician Katherine Goble works as a human computer in the gender segregated and racially segregated division West Area Computers of the Langley Research Center in Hampton, Virginia, alongside her colleagues, aspiring engineer Mary Jackson and their unofficial acting-supervisor Dorothy Vaughan.\n\nFollowing the successful Soviet launch of Yuri Gagarin, pressure to send American astronauts into space increases. Supervisor Vivian Mitchell assigns Katherine to assist Al Harrison's Space Task Group, given her skills in analytic geometry. She becomes the first black woman on the team.\n\nKatherine's new colleagues are initially dismissive and demeaning, especially head engineer Paul Stafford. Meanwhile, Mitchell informs Dorothy that she will not be promoted, as there are no plans to assign a \"permanent supervisor for the colored group\". Mary is assigned to the space capsule heat shield team, and immediately identifies a flaw. With encouragement from the team leader, a Polish-Jewish Holocaust survivor, she submits an application for an official NASA engineer position and begins to pursue additional engineering coursework, as she already has a mathematics and physical science degree, but needs additional certification courses offered only through the all-white nearby Hampton high school. First, Mary successfully petitions a local judge to grant her legal authorization to attend the segregated all white school.\n\nKatherine meets National Guard Lt. Col. Jim Johnson at a barbecue, but she is disappointed when he voices skepticism about women's mathematical abilities. He later apologizes, and begins spending time with Katherine and her three daughters.\n\nWhen Harrison invites his subordinates to solve a complex mathematical equation, Katherine develops the solution, leaving him impressed. The Mercury 7 astronauts visit Langley and astronaut John Glenn is cordial to the West Area Computers.\n\nHarrison is enraged when he finds out that Katherine is forced to walk a half-mile (800 meters) to another building to use the colored people's bathroom. Harrison abolishes bathroom segregation, knocking down the \"Colored Bathroom\" sign. Harrison allows Katherine to be included in their meetings, in which she creates an equation within that meeting to guide the space capsule during re-entry. Despite this, Katherine is forced to remove her name from the reports, which are credited solely to Stafford. Stafford said that computers cannot author such things. Meanwhile, Mary goes to court and convinces the judge to grant her permission to attend night classes in an all-white school to obtain her engineering degree.\n\nDorothy learns of the impending installation of an IBM 7090 electronic computer that could replace human computers. She visits the computer room to learn about it, and successfully starts the machine. Later, she visits a public library, where the librarian scolds her for visiting the whites-only section, to borrow a book about Fortran. She stole that book and began studying on her own. After teaching herself programming and training her West Area co-workers, she is officially promoted to supervise the Programming Department, bringing 30 of her co-workers with her. Mitchell eventually addresses Dorothy as \"Mrs. Vaughan,\" indicating her new-found respect.\n\nAs the final arrangements for John Glenn's launch are made, Katherine is reassigned back to West Area Computers. Harrison told her that they no longer needs computer in their department and it's beyond his decision. As a wedding and farewell gift from her colleagues (Katherine is now married to Jim Johnson), Katherine is given a pearl necklace, the only jewelry allowed under the dress code.\n\nThe day of the launch, discrepancies arise in the IBM 7090 calculations for the capsule's landing coordinates, and Astronaut Glenn requests that Katherine be called in to check them. She quickly does so, only to have the door slammed in her face after delivering the results to the control room. However, Harrison gives her a security pass so they can relay the results to Glenn together. \nAfter a successful launch and orbit, the space capsule has a heat shield problem. Mission control decides to land it after three orbits instead of seven. Katherine suggests that they leave the retro-rocket attached to the heat shield for reentry. The instructions prove correct, and Friendship 7 successfully lands.\n\nFollowing the mission, the mathematicians are laid off and ultimately replaced by electronic computers. Katherine is reassigned to the Analysis and Computation Division, Dorothy continues to supervise the Programming Department, and Mary obtains her engineering degree and gains employment at NASA as an engineer. At the end of the film, we see Stafford, showing a change of heart, bringing Katherine a cup of coffee and accepting that her name is included on the report.\n\nAn epilogue reveals that Katherine calculated the trajectories for the Apollo 11 and Space Shuttle missions. In 2015 she was awarded the Presidential Medal of Freedom. The following year, NASA dedicated the Langley Research Center's Katherine G. Johnson Computational Building in her honor.\n\nOn July 9, 2015, it was announced that producer Donna Gigliotti had acquired Margot Lee Shetterly's nonfiction book \"Hidden Figures\", about a group of black female mathematicians that helped NASA win the Space Race. Allison Schroeder wrote the script, which was developed by Gigliotti through Levantine Films. Schroeder grew up by Cape Canaveral and her grandparents worked at NASA, where she also interned as a teenager, and as a result saw the project as a perfect fit for herself. Levantine Films produced the film with Peter Chernin's Chernin Entertainment. Fox 2000 Pictures acquired the film rights, and Theodore Melfi signed on to direct. After coming aboard, Melfi revised Schroeder's script, and in particular focused on balancing the home lives of the three protagonists with their careers at NASA. After the film's development was announced, actresses considered to play the black female roles included Oprah Winfrey, Viola Davis, Octavia Spencer, and Taraji P. Henson.\n\nChernin and Jenno Topping produced, along with Gigliotti and Melfi. On February 10, 2016, Fox cast Henson to play the lead role of mathematician Katherine Goble Johnson. On February 17, Spencer was selected to play Dorothy Vaughan, one of the three lead mathematicians at NASA. On March 1, 2016, Kevin Costner was cast in the film to play the fictional head of the space program. Singer Janelle Monáe signed on to play the third lead mathematician, Mary Jackson. Later the same month, Kirsten Dunst, Glen Powell, and Mahershala Ali were cast in the film: Powell to play astronaut John Glenn, and Ali as Johnson's love interest.\n\nPrincipal photography began in March 2016 on the campus of Morehouse College in Atlanta, Georgia. Filming also took place at Lockheed Martin Aeronautics at Dobbins Air Reserve Base. On April 1, 2016, Jim Parsons was cast in the film to play the head engineer of the Space Task Group at NASA, Paul Stafford. In April 2016, Pharrell Williams came on board as a producer on the film. He also wrote original songs and handled the music department and soundtrack of the film, with Hans Zimmer and Benjamin Wallfisch. Morehouse College mathematics professor Rudy L. Horne was brought in to be the on-set mathematician.\n\nThe film, set at NASA Langley Research Center in 1961, depicts segregated facilities such as the West Area Computing unit, where an all-black group of female mathematicians were originally required to use separate dining and bathroom facilities. However, in reality, Dorothy Vaughan was promoted to supervisor of West Computing in 1949, becoming the first black supervisor at the National Advisory Committee for Aeronautics (NACA) and one of the few female supervisors. In 1958, when NACA became NASA, segregated facilities, including the West Computing office, were abolished. Dorothy Vaughan and many of the former West Computers transferred to the new Analysis and Computation Division (ACD), a racially and gender-integrated group.\n\nMary Jackson was the one who had to find her own way to a colored bathroom, which did exist on the East Side. Katherine (then Goble) was originally unaware that the East Side bathrooms were segregated, and used the unlabeled \"whites-only\" bathrooms for years before anyone complained. She ignored the complaint, and the issue was dropped. In an interview with WHRO-TV, Katherine Johnson denied the feeling of segregation. \"I didn't feel the segregation at NASA, because everybody there was doing research. You had a mission and you worked on it, and it was important to you to do your job ... and play bridge at lunch. I didn't feel any segregation. I knew it was there, but I didn't feel it.\"\n\nMary Jackson did not have to get a court order to attend night classes at the whites-only high school. She asked the city of Hampton for an exception, and it was granted. The school turned out to be run down and dilapidated, a hidden cost of running two parallel school systems. She completed her engineering courses and earned a promotion to engineer in 1958.\n\nKatherine Goble/Johnson carpooled with Eunice Smith, a nine-year West End computer veteran at the time Katherine joined NACA. Smith was her neighbor and friend from sorority and church choir. The three Goble children were teenagers at the time of Katherine's marriage to Jim Johnson.\n\nKatherine Goble/Johnson was assigned to the Flight Research Division in 1953, a move that soon became permanent. When the Space Task Group was created in 1958, engineers from the Flight Research Division formed the core of the Group, and Katherine moved along with them. She coauthored a research report in 1960, the first time a woman in the Flight Research Division had received credit as an author of a research report.\n\nKatherine gained access to editorial meetings as of 1958 simply through persistence, not because one particular meeting was critical.\n\nThe Space Task Group was led by Robert Gilruth, not the fictional character Al Harrison, who was created to simplify a more complex management structure.\n\nThe scene where Harrison smashes the Colored Ladies Room sign never happened, as in real life Katherine refused to walk the extra distance to use the colored bathroom and, in her words, \"just went to the White one\". Harrison also lets her into Mission Control to witness the launch. Neither scene happened in real life, and screenwriter Theodore Melfi said he saw no problem with adding the scenes, \"There needs to be white people who do the right thing, there needs to be black people who do the right thing, and someone does the right thing. And so who cares who does the right thing, as long as the right thing is achieved?\" Dexter Thomas of \"Vice News\" criticized Melfi's additions as creating the white savior trope, \"In this case, it means that a white person doesn't have to think about the possibility that, were they around back in the 1960s South, they might have been one of the bad ones.\" \"The Atlantic\"s Megan Garber said that the film's \"narrative trajectory\" involved \"thematic elements of the white savior\". Melfi said he found \"hurtful\" the \"accusations of a 'white savior' storyline\", saying, \"It was very upsetting to me because I am at a place where I've lived my life colorless and I grew up in Brooklyn. I walked to school with people of all shapes, sizes, and colors, and that's how I've lived my life. So it's very upsetting that we still have to have this conversation. I get upset when I hear 'black film,' and so does Taraji P. Henson ... It's just a film. And if we keep labeling something 'a black film,' or 'a white film'— basically it's modern day segregation. We're all humans. Any human can tell any human's story. I don't want to have this conversation about black film or white film anymore. I wanna have conversations about film.\"\n\n\"The Huffington Post\"s Zeba Blay said of Melfi's frustration, \"His frustration is also a perfect example of how, when it comes to open dialogue about depictions of people of color on screen, it behooves white people (especially those who position themselves as 'allies') to listen ... the inclusion of the bathroom scene doesn't make Melfi a bad filmmaker, or a bad person, or a racist. But his suggestion that a feel-good scene like that was needed for the marketability and overall appeal of the film speaks to the fact that Hollywood at large still has a long way to go in telling black stories, no matter how many strides have been made.\"\n\nThe fictional characters Vivian Mitchell and Paul Stafford are composites of several team members, and reflect common social views and attitudes of the time. Karl Zielinski is based on Mary Jackson's mentor, Kazimierz \"Kaz\" Czarnecki.\n\nJohn Glenn, who was about a decade older than depicted at the time of launch, did ask specifically for Johnson to verify the IBM calculations, although she had several days before the launch date to complete the process.\n\nThe author Margot Lee Shetterly has agreed that there are differences between her book and the movie, but found that to be understandable. \nFor better or for worse, there is history, there is the book and then there's the movie. Timelines had to be conflated and [there were] composite characters, and for most people [who have seen the movie] have already taken that as the literal fact. ... You might get the indication in the movie that these were the only people doing those jobs, when in reality we know they worked in teams, and those teams had other teams. There were sections, branches, divisions, and they all went up to a director. There were so many people required to make this happen. ... It would be great for people to understand that there were so many more people. Even though Katherine Johnson, in this role, was a hero, there were so many others that were required to do other kinds of tests and checks to make [Glenn's] mission come to fruition. But I understand you can't make a movie with 300 characters. It is simply not possible.\n\nJohn Glenn's flight was not terminated early as incorrectly stated in the movie's closing subtitles. The MA-6 mission was planned for three orbits and landed at the expected time. The press kit published before launch states that \"The Mercury Operations Director may elect a one, two or three orbit mission.\" The post mission report also shows that retrofire was scheduled to occur on the third orbit. \n\nThe Mercury Control Center was located at Cape Canaveral, Florida, not at the Langley Research Center in Virginia. The orbit plots displayed in the front of the room incorrectly show a six-orbit mission, which did not happen until Walter Schirra's MA-8 mission in October 1962. The movie also incorrectly shows NASA flight controllers monitoring live telemetry from the Soviet Vostok launch, which the Soviet Union would not have been sharing with NASA in 1961.\n\nKatherine Johnson's Technical Note D-233, co-written with T.H. Skopinski, can be found on the NASA Technical Reports Server.\n\nThe film began a limited release on December 25, 2016, before a wide release on January 6, 2017.\n\nAfter \"Hidden Figures\" was released on December 25, 2016, certain charities, institutions and independent businesses who regard the film as relevant to the cause of improving youth awareness in education and careers in the science, technology, engineering, and mathematics (STEM) fields, organized free screenings of the film in order to spread the message of the film's subject matter. A collaborative effort between Western New York STEM Hub, AT&T and the Girl Scouts of the USA allowed more than 200 Buffalo Public School students, Girl Scouts and teachers to see the film. WBFO's Senior Reporter Eileen Buckley stated the event was designed to help encourage a new generation of women to consider STEM careers. Research indicates that by the year 2020, there will be 2.4 million unfilled STEM jobs.\n\nAlso, the film's principal actors (Henson, Spencer, Monáe and Parsons), director (Melfi), producer/musical creator (Williams), and other non-profit outside groups have offered free screenings to \"Hidden Figures\" at several cinema locations around the world. Some of the screenings were open to all-comers, while others were arranged to benefit girls, women and the underprivileged. The campaign began as individual activism by Spencer, and made a total of more than 1,500 seats for \"Hidden Figures\" available, free of charge, to poor individuals and families. The end result was seven more screenings for people who otherwise might not have been able to afford to see the 20th Century Fox film - in Atlanta (sponsored by Monáe), in Washington, D.C. (sponsored by Henson), in Chicago (also Henson), in Houston (by Parsons), in Hazelwood, Missouri (by Melfi and actress/co-producer Kimberly Quinn), and in Norfolk and Virginia Beach, Virginia (both sponsored by Williams).\n\nIn February 2017, AMC Theatres and 21st Century Fox announced that free screenings of \"Hidden Figures\" would take place in celebration of Black History Month in up to 14 select U.S. cities (including Atlanta, Chicago, Dallas, Los Angeles and Miami). The statement described the February charity screenings as building broader awareness of the film's true story of black women mathematicians who worked at NASA during the Space Race. 21st Century Fox and AMC Theatres also invited schools, community groups and non-profit organizations to apply for additional special screenings to be held in their towns. \"As we celebrate Black History Month and look ahead to Women's History Month in March, this story of empowerment and perseverance is more relevant than ever,\" said Liba Rubenstein, 21st Century Fox's Senior Vice President of Social Impact, \"We at 21CF were inspired by the grassroots movement to bring this film to audiences that wouldn't otherwise be able to see it - audiences that might include future innovators and barrier-breakers - and we wanted to support and extend that movement\".\n\nPhilanthropic non-profit outside groups and other local efforts by individuals have offered free screenings of \"Hidden Figures\" by using crowdfunding platforms on the Internet, that allow people to raise money for free film screening events. Dozens of other GoFundMe free screening campaigns have appeared since the film's general release, all by people wanting to raise money to pay for students to see the film.\n\nFollowing the 2017 \"Lego Ideas Contest\", Denmark-based toy maker The Lego Group announced that will manufacture a fan-designed \"Women of NASA\" figurine set of five female scientists, engineers and astronauts, as based on real women who have worked for the NASA Space Agency. The minifigures set did not end up including any of the figures from the film. It had computer scientist Margaret Hamilton; astronaut, physicist and educator Sally Ride; astronomer Nancy Grace Roman; and astronaut and physician Mae Jemison (who is also African American). The \"Women of NASA\" set was released November 1, 2017.\n\n\"Hidden Figures\" was released on Digital HD on March 28, 2017 and Blu-ray, 4K Ultra HD, and DVD on April 11, 2017. The film debuted at No. 3 on the home video sales chart.\n\n\"Hidden Figures\" grossed $169.6 million in the United States and Canada, and $66.3 million in other territories, for a worldwide gross of $235.9 million, against a production budget of $25 million. Domestically, \"Hidden Figures\" was the highest-grossing Best Picture nominee at the 89th Academy Awards. \"Deadline Hollywood\" calculated the net profit of the film to be $95.55 million, when factoring together all expenses and revenues for the film, making it one of the top twenty most profitable release of 2016.\n\nDuring its limited release in 25 theaters from December 25, 2016 to January 5, 2017, the film grossed $3 million. In North America, \"Hidden Figures\" had its expansion alongside the opening of \"\" and the wide expansions of \"Lion\" and \"A Monster Calls.\" It was expected to gross around $20 million from 2,471 theaters in its opening weekend, with the studio projecting a more conservative $15–17 million debut. It made $1.2 million from Thursday night previews and $7.6 million on its first day. Initially, projections had the film grossing $21.8 million in its opening weekend, finishing second behind \"Rogue One: A Star Wars Story\" ($22 million). Final figures revealed the film tallied a weekend total of $22.8 million, beating \"Rogue One\"s $21.9 million. In its second weekend, the film grossed $20.5 million (for a four-day MLK Weekend total of $27.5 million), again topping the box office.\n\nOn review aggregator website Rotten Tomatoes, the film has an approval rating of 93% based on 262 reviews, with an average score of 7.6/10. The website's critical consensus reads, \"In heartwarming, crowd-pleasing fashion, \"Hidden Figures\" celebrates overlooked—and crucial—contributions from a pivotal moment in American history.\" On Metacritic, the film has a weighted average score of 74 out of 100, based on 47 critics, indicating \"generally favorable reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A+\" on an A+ to F scale, one of fewer than 60 films in the history of the service to receive such a score.\n\nSimon Thompson of IGN gave the film a rating of 9/10, writing, \"\"Hidden Figures\" fills in an all too forgotten, or simply too widely unknown, blank in US history in a classy, engaging, entertaining and hugely fulfilling way. Superb performances across the board and a fascinating story alone make \"Hidden Figures\" a solid, an accomplished and deftly executed movie that entertains, engages and earns your time, money and attention.\" Ty Burr of \"The Boston Globe\" wrote, \"the film's made with more heart than art and more skill than subtlety, and it works primarily because of the women that it portrays and the actresses who portray them. Best of all, you come out of the movie knowing who Katherine Johnson and Dorothy Vaughn and Mary Jackson are, and so do your daughters and sons.\"\n\nClayton Davis of Awards Circuit gave the film 3.5 stars, saying \"Precisely marketed as terrific adult entertainment for the Christmas season, \"Hidden Figures\" is a faithful and truly beautiful portrait of our country's consistent gloss over the racial tensions that have divided and continue to plague the fabric our existence. Lavishly engaging from start to finish, \"Hidden Figures\" may be able to catch the most inopportune movie-goer off guard and cause them to fall for its undeniable and classic storytelling. The film is not to be missed.\"\n\nOther reviews criticized the film for its fictional embellishments and conventional, feel-good style. Tim Grierson, writing for Screen International, states that \"Hidden Figures is almost patronisingly earnest in its depiction of sexism and racism. An air of do-gooder self-satisfaction hovers over the proceedings\", while Jesse Hassenger at The A.V. Club comments that \"lack of surprise is in this movie's bones.\" Eric John of IndieWire argues that the film \"trivializes history; as a hagiographic tribute to its brilliant protagonists, it doesn't dig into the essence of their struggles\" and similarly, Paul Byrnes concludes that \"When a film purports to be selling history, we're entitled to ask where the history went, even if it offers a good time instead.\"\n\n\n"}
{"id": "41081248", "url": "https://en.wikipedia.org/wiki?curid=41081248", "title": "Influenza A virus subtype H6N1", "text": "Influenza A virus subtype H6N1\n\nInfluenza A virus subtype H6N1, also known as H6N1, is a subtype of the influenza A virus. It has only infected one person, a woman in Taiwan, who recovered. Known to infect Eurasian teal, it is closely related to subtype H5N1.\n"}
{"id": "7869341", "url": "https://en.wikipedia.org/wiki?curid=7869341", "title": "Institutional model theory", "text": "Institutional model theory\n\n\"This page is about the concept in mathematical logic. For the concept in sociology, see Institutional logic\".\n\nInstitutional model theory generalizes a large portion of first-order model theory to an arbitrary logical system.\n\nThe notion of \"logical system\" here is formalized as an institution. Institutions constitute a model-oriented meta-theory on logical systems similar to how the theory of rings and modules constitute a meta-theory for classical linear algebra. Another analogy can be made with universal algebra versus groups, rings, modules etc. By abstracting away from the realities of the actual conventional logics, it can be noticed that institution theory comes in fact closer to the realities of non-conventional logics.\n\nInstitutional model theory analyzes and generalizes classical model-theoretic notions and results, like\n\n\nFor each concept and theorem, the infrastructure and properties required are analyzed and formulated as conditions on institutions, thus providing a detailed insight on which properties of first-order logic they rely and how much they can be generalized to other logics.\n\n\n"}
{"id": "5595761", "url": "https://en.wikipedia.org/wiki?curid=5595761", "title": "Jepson School of Leadership Studies", "text": "Jepson School of Leadership Studies\n\nThe Jepson School of Leadership Studies, founded in 1992 at the University of Richmond, is dedicated to the academic study of leadership. The school is named for benefactors Robert S. Jepson, Jr., and his wife, Alice Andrews Jepson. The school opened as the first undergraduate school of leadership studies in the United States with a full-time multidisciplinary faculty.\n\nIn 1987, Jepson School of Leadership Studies founders, Robert S. Jepson, Jr., and his wife, Alice, announced they would provide a $20 million challenge gift to develop a leadership studies program at the University of Richmond. E. Bruce Heilman, Richard L. Morrill, and Zeddie Brown were among the University leaders who helped form the school.\n\nThe first dean of the school was Howard T. Prince, who developed leadership programs for the U.S. Army and the U.S. Military Academy in West Point. James McGregor Burns, a preeminent scholar in leadership studies, was named a senior fellow. The school's inaugural faculty included Joanne Ciulla, Richard Couto, Karin Klenke, William Howe,Gill Hickman, and J. Thomas Wren.\n\nThe first class entered the school in 1992. General Norman Schwarzkopf, Jr. spoke at the dedication of Jepson Hall on September 9, 1992.\n\nAs of fall 2017, the Jepson School of Leadership Studies has 15 full-time faculty members.\n\nVirginia State Council of Higher Education teaching awards: Joanne Ciulla (retired; 2003), Don Forsyth (2002)\n\nStudents are elected annually to the Jepson Student Government Association, which acts a liaison between students and faculty and between Jepson School students and University of Richmond as well as organizing programs and speakers. Students are appointed to the Jepson Corps, an emissary group that promotes the student experience, the Jepson School curriculum, and the School’s mission.\n\nThe Science Leadership Scholars Program is a co-curricular program offered through the Jepson School of Leadership Studies and the University of Richmond School of Arts and Sciences for students pursuing leadership studies and science. The program met for the first time in January 2016 at a dinner with journalist James Hamblin.\n\nA collaboration with the University of Richmond School of Law, the Jepson at Cambridge summer program allows students to spend five weeks studying at Emmanuel College, Cambridge University. Students participate in classes, lectures, and excursions to examine law and leadership in an international context.\n\nStudents work with faculty members to research leadership as a part of the human condition. They may apply for grants to conduct full-time or part-time research with a faculty member during the summer. Students who have demonstrated scholarly drive and initiative may complete and defend a senior honors thesis in order to earn honors in leadership studies.\n\nStudents present research projects at an annual symposium. In recent years, students have presented research on the relationship effect of ADHD on leadership, the leadership impact of satirical news, and the impact of socioeconomic status on perceptions of leadership.\n\nThe school’s annual Fredric M. Jablin Award for Undergraduate Research—given in memory of Dr. Fredric Jablin, a beloved professor and scholar at the school—is an annual award of $2,000 that is presented to a rising Jepson School senior conducting research in a wide variety of fields. The recipient is recognized at Finale, the Jepson School’s senior recognition and appreciation ceremony held each spring.\n\nFour seniors are selected from the Leadership & Ethics capstone course annually to represent the Jepson School in the APPE InterCollegiate Ethics Bowl competition. The 2014–15 team finished in the top ten nationally.\n\nJepson School students attend conferences held throughout the country, including the Hatton W. Sumners Student Leadership Conference and the US Naval Academy Conference.\n\nOrganized and led by Jepson School alumni, the Jepson EDGE Institute prepares students for success in life after college. Students network with alumni while developing strategies for securing and excelling in internships, jobs, graduate school, and post-collegiate life.\n\nEvery year, the Jepson Leadership Forum presents a series of programs that explore leadership topics around a theme. Forum speakers have included Elie Wiesel, Bill Moyers, Nina Totenberg, Maria Konnikova, Peter Singer, Annette Gordon-Reed, and Isabel Wilkerson.\n\nThe John Marshall International Center for the Study of Statesmanship hosts a variety of seminars, conferences, and speakers to explore topics surrounding constitutionalism, political economy, politics, and ethical reasoning. Speakers include Václav Klaus, Gen. Michael V. Hayden, Jennifer Rubin, and Harvey C. Mansfield. The late Margaret Thatcher served as an adviser to the center. In 2017, the Jepson School center hosted a conference surrounding Thatcher’s leadership with guests such as Robin Harris, John Bolton, and Sir David Cannadine.\n\nEach academic year, notable leaders at the local, state, and national level are invited to serve as Leaders-in-Residence within the Jepson school. Particularly notable holders of this position have included: Leo K. Thorsness, Tim Kaine, Mary Sue Terry, Leland Melvin, and Vivian Pinn.\n\n"}
{"id": "39952681", "url": "https://en.wikipedia.org/wiki?curid=39952681", "title": "John Desmond Bernal Prize", "text": "John Desmond Bernal Prize\n\nThe John Desmond Bernal Prize is an award given annually by the Society for Social Studies of Science (4S) to scholars judged to have made a distinguished contribution to the field of Science and Technology Studies (STS). The award was launched in 1981, with the support of Eugene Garfield.\n\nThe award is named after the scientist John Desmond Bernal.\n\nSource: Society for Social Studies of Science\n\n"}
{"id": "35784828", "url": "https://en.wikipedia.org/wiki?curid=35784828", "title": "KAT-7", "text": "KAT-7\n\nKAT-7 is a radio telescope constructed in the Northern Cape of South Africa. Part of the Karoo Array Telescope project, it is the precursor engineering test bed to the larger MeerKAT telescope, but it has become a science instrument in its own right. The construction was completed in 2011 and commissioned in 2012. It also served as a technology demonstrator for South Africa's bid to host the Square Kilometre Array. KAT-7 is the first Radio telescope to be built with a composite reflector and uses a stirling pump for 75 K cryogenic cooling. The telescope was built to test various system for the MeerKAT array, from the ROACH correlators designed and manufactured in Cape Town, now used by various telescopes internationally, to composite construction techniques. With the short baselines the telescope is suited to observing diffuse sources, but will begin VLBI observation in 2013.\n\nKAT-7 consist of 7 dishes of 12 metres in diameter each a Prime Focus Reflecting telescope.\n\nMeerKAT supports a wide range of observing modes, including deep continuum, polarisation and spectral line imaging, pulsar timing and transient searches. A range of standard data products are provided, including an imaging pipeline. A number of \"data spigots\" are also available to support user-provided instrumentation. Significant design and qualification efforts are planned to ensure high reliability in order to achieve low operational cost and high availability.\n\nIn April 2010 four of the seven dishes were linked together as an integrated system to produce its first interferometric image of an astronomical object. In Dec 2010, there was a successful detection of very long baseline interferometry (VLBI) fringes between the Hartebeesthoek Radio Astronomy Observatory's 26 m dish and one of the KAT-7 dishes.\n\n\n"}
{"id": "40596532", "url": "https://en.wikipedia.org/wiki?curid=40596532", "title": "Kate Charlton-Robb", "text": "Kate Charlton-Robb\n\nDr Kate Charlton-Robb, grew up on the Mornington Peninsula is an Australian zoologist, molecular genetist, and founding director & principal researcher of the Australian Marine Mammal Conservation Foundation, who, along with colleagues, declared in 2011 a new species of the genus \"Tursiops\", and formally named it \"Tursiops australis\".\n\n\n"}
{"id": "173351", "url": "https://en.wikipedia.org/wiki?curid=173351", "title": "Laboratory", "text": "Laboratory\n\nA laboratory (, ; colloquially lab) is a facility that provides controlled conditions in which scientific or technological research, experiments, and measurement may be performed.\n\nLaboratories used for scientific research take many forms because of the differing requirements of specialists in the various fields of science and engineering. A physics laboratory might contain a particle accelerator or vacuum chamber, while a metallurgy laboratory could have apparatus for casting or refining metals or for testing their strength. A chemist or biologist might use a wet laboratory, while a psychologist's laboratory might be a room with one-way mirrors and hidden cameras in which to observe behavior. In some laboratories, such as those commonly used by computer scientists, computers (sometimes supercomputers) are used for either simulations or the analysis of data. Scientists in other fields will use still other types of laboratories. Engineers use laboratories as well to design, build, and test technological devices.\n\nScientific laboratories can be found as research room and learning spaces in schools and universities, industry, government, or military facilities, and even aboard ships and spacecraft.\n\nDespite the underlying notion of the lab as a confined space for experts, the term \"laboratory\" is also increasingly applied to workshop spaces such as Living Labs, Fab Labs, or Hackerspaces, in which people meet to work on societal problems or make prototypes, working collaboratively or sharing resources. This development is inspired by new, participatory approaches to science and innovation and relies on user-centred design methods and concepts like Open innovation or User innovation. One distinctive feature of work in Open Labs is phenomena of translation, driven by the different backgrounds and levels of expertise of the people involved.\n\nEarly instances of \"laboratories\" recorded in English involved alchemy and the preparation of medicines.\n\nThe emergence of Big Science during World War II increased the size of laboratories and scientific equipment, introducing particle accelerators and similar devices.\n\nThe earliest laboratory according to the present evidence is a home laboratory of Pythagoras of Samos, the well-known Greek philosopher and scientist. This laboratory was created when Pythagoras conducted an experiment about tones of sound and vibration of string.\n\nIn the painting of Louis Pasteur by Albert Edelfelt in 1885, Louis Pasteur is shown comparing a note in his left hand with a bottle filled with a solid in his right hand, and not wearing any personal protective equipment. \n\nResearching in teams started in the 19th century, and many new kinds of equipment were developed in the 20th century.\n\nA 16th century underground alchemical laboratory was accidentally discovered in the year 2002. Rudolf II, Holy Roman Emperor was believed to be the owner. The laboratory is called Speculum Alchemiae and is preserved as a museum in Prague.\n\nLaboratory techniques are the set of procedures used on natural sciences such as chemistry, biology, physics to conduct an experiment, all of them follow the scientific method; while some of them involve the use of complex laboratory equipment from laboratory glassware to electrical devices, and others require more specific or expensive supplies.\n\nLaboratory equipment refers to the various tools and equipment used by scientists working in a laboratory:\n\nThe classical equipment includes tools such as Bunsen burners and microscopes as well as specialty equipment such as operant conditioning chambers, spectrophotometers and calorimeters.\n\n\n\nLaboratory equipment is generally used to either perform an experiment or to take measurements and gather data. Larger or more sophisticated equipment is generally called a scientific instrument.\n\nThe title of \"laboratory\" is also used for certain other facilities where the processes or equipment used are similar to those in scientific laboratories. These notably include:\n\n\nIn many laboratories, hazards are present. Laboratory hazards might include poisons; infectious agents; flammable, explosive, or radioactive materials; moving machinery; extreme temperatures; lasers, strong magnetic fields or high voltage. Therefore, safety precautions are vitally important. Rules exist to minimize the individual's risk, and safety equipment is used to protect the lab users from injury or to assist in responding to an emergency.\n\nThe Occupational Safety and Health Administration (OSHA) in the United States, recognizing the unique characteristics of the laboratory workplace, has tailored a standard for occupational exposure to hazardous chemicals in laboratories. This standard is often referred to as the \"Laboratory Standard\". Under this standard, a laboratory is required to produce a Chemical Hygiene Plan (CHP) which addresses the specific hazards found in its location, and its approach to them.\n\nIn determining the proper Chemical Hygiene Plan for a particular business or laboratory, it is necessary to understand the requirements of the standard, evaluation of the current safety, health and environmental practices and assessment of the hazards. The CHP must be reviewed annually. Many schools and businesses employ safety, health, and environmental specialists, such as a Chemical Hygiene Officer (CHO) to develop, manage, and evaluate their CHP. Additionally, third party review is also used to provide an objective \"outside view\" which provides a fresh look at areas and problems that may be taken for granted or overlooked due to habit.\n\nInspections and audits like also be conducted on a regular basis to assess hazards due to chemical handling and storage, electrical equipment, biohazards, hazardous waste management, chemical waste, housekeeping and emergency preparedness, radiation safety, ventilation as well as respiratory testing and indoor air quality. An important element of such audits is the review of regulatory compliance and the training of individuals who have access to and/or work in the laboratory. Training is critical to the ongoing safe operation of the laboratory facility. Educators, staff and management must be engaged in working to reduce the likelihood of accidents, injuries and potential litigation. Efforts are made to ensure laboratory safety videos are both relevant and engaging.\n\n"}
{"id": "3059514", "url": "https://en.wikipedia.org/wiki?curid=3059514", "title": "Law of truly large numbers", "text": "Law of truly large numbers\n\nThe law of truly large numbers (a statistical adage), attributed to Persi Diaconis and Frederick Mosteller, states that with a sample size large enough, any outrageous thing is likely to happen. Because we never find it notable when likely events occur, we highlight unlikely events and notice them more. The law seeks to debunk one element of supposed supernatural phenomenology. It is meant to make a statement about probabilities.\n\nFor a simplified example of the law, assume that a given event happens with a probability for its occurrence of 0.1%, within a single trial. Then, the probability that this so-called unlikely event does \"not\" happen (improbability) in a single trial is 99.9% (0.999).\n\nIn a sample of 1000 independent trials, however, the probability that the event \"does not\" happen in any of them, even once (improbability), is 0.999, or approximately 36.8%. Then, the probability that the event does happen, at least once, in 1000 trials is or 63.2%. This means that this \"unlikely event\" has a probability of 63.2% of happening if 1000 independent trials are conducted, or over 99.9% for 10,000 trials.\n\nThe probability that it happens at least once in 10,000 trials is In other words, a highly unlikely event, given enough trials with some fixed number of draws per trial, is even more likely to occur.\n\nThe law comes up in criticism of pseudoscience and is sometimes called the Jeane Dixon effect (see also Postdiction). It holds that the more predictions a psychic makes, the better the odds that one of them will \"hit\". Thus, if one comes true, the psychic expects us to forget the vast majority that did not happen (confirmation bias). Humans can be susceptible to this fallacy.\n\nAnother similar (to a small degree, see Psychologism and Anti-psychologism) manifestation of the law can be found in gambling, where gamblers tend to remember their wins and forget their losses, even if the latter far outnumbers the former (though depending on a particular person's environment, behaviors, customs or habits, so the opposite may also be local truth – \"statistical prevalence not featured\"). Mikal Aasved links it with \"selective memory bias\", allowing gamblers to mentally distance themselves from the consequences of their gambling by holding an inflated view of their real winnings (or losses in the opposite case).\n\n\n\n"}
{"id": "36469718", "url": "https://en.wikipedia.org/wiki?curid=36469718", "title": "List of archaeological excavations in Jerusalem", "text": "List of archaeological excavations in Jerusalem\n\nList of archaeological excavations in Jerusalem is an incomplete list of archaeological excavations in Jerusalem.\n\nThe nineteenth century saw much interest in Jerusalem develop. British Protestants, eager to find hard evidence for their Christian convictions, set out to dig the Holy City. Among them were Flinders Petrie, Charles Warren, Charles William Wilson and Montague Parker.\n\nDuring the Mandate, efforts to excavate Jerusalem continued with digs by R. A. Stewart Macalister in the City of David.\n\nUnder Jordanian rule, Kathleen Kenyon excavated in the City of David, discovering numerous important finds including the proto-Ionic capital.\n"}
{"id": "43028270", "url": "https://en.wikipedia.org/wiki?curid=43028270", "title": "List of botanists by author abbreviation (Q–R)", "text": "List of botanists by author abbreviation (Q–R)\n\nTo find entries for A–P, use the table of contents above.\n\n\n\nTo find entries for S–Z, use the table of contents above.\n"}
{"id": "18987057", "url": "https://en.wikipedia.org/wiki?curid=18987057", "title": "List of chemical elements naming controversies", "text": "List of chemical elements naming controversies\n\nThe currently accepted names and symbols of the chemical elements are determined by the International Union of Pure and Applied Chemistry (IUPAC), usually following recommendations by the recognized discoverers of each element. However the names of several elements have been the subject of controversies until IUPAC established an official name. In most cases the controversy was due to a priority dispute as to who first found conclusive evidence for the existence of an element, or as to what evidence was in fact conclusive.\n\nVanadium (named after Vanadis, another name for Freyja, the Scandinavian goddess of fertility) was originally discovered by Andrés Manuel del Río (a Spanish-born Mexican mineralogist) in Mexico City in 1801. He discovered the element after being sent a sample of \"brown lead\" ore (\"plomo pardo de Zimapán\", now named vanadinite). Through experimentation, he found it to form salts with a wide variety of colors, so he named the element panchromium (Greek: all colors). He later renamed this substance erythronium, since most of the salts turned red when heated. The French chemist Hippolyte Victor Collet-Descotils incorrectly declared that del Río's new element was only impure chromium. Del Río thought himself to be mistaken and accepted the statement of the French chemist that was also backed by del Río's friend Alexander von Humboldt.\n\nIn 1831, Sefström of Sweden rediscovered vanadium in a new oxide he found while working with some iron ores. He chose to call the element vanadin in Swedish (which has become vanadium in other languages including German and English) after the Old Norse \"Vanadís\", another name for the Norse Vanr goddess Freyja, whose facets include connections to beauty and fertility, because of the many beautifully colored chemical compounds it produces. Later that same year Friedrich Wöhler confirmed del Río's earlier work. Later, \nGeorge William Featherstonhaugh, one of the first US geologists, suggested that the element should be named \"rionium\" after del Río, but this never happened.\n\nCharles Hatchett named element 41 columbium in 1801 (Cb), but after the publication of the identity of columbium with tantalum by William Hyde Wollaston in 1802 the claims of discovery of Hattchet were refused.\nIn 1846 Heinrich Rose discovered that tantalite contained an element similar to tantalum and named it niobium.\n\nIUPAC officially adopted niobium in 1950 after 100 years of controversy.\nThis was a compromise of sorts; the IUPAC accepted tungsten (element 74) instead of wolfram (in deference to North American usage) and niobium instead of columbium (in deference to European usage).\n\nGadolinite, a mineral (from Ytterby, a village in Sweden), consists of several compounds (oxides or earths): yttria, erbia (sub-component as ytterbia) and terbia.\n\nIn 1878, Jean Charles Galissard de Marignac assumed that ytterbia consisted of a new element he called ytterbium (but actually there were two new elements).\nIn 1907 Georges Urbain isolated element 70 and element 71 from ytterbia. He called element 70 neoytterbium (\"new ytterbium\") and called element 71 lutecium. \nAt about the same time, Carl Auer von Welsbach also independently isolated these and proposed the names aldebaranium (Ad), after the star Aldebaran (in the constellation of Taurus), for element 70 (ytterbium), and cassiopeium (Cp), after the constellation Cassiopeia, for element 71 (lutetium), but both proposals were rejected.\n\nNeoytterbium (element 70) was eventually reverted to ytterbium (following Marignac) and in 1949 the spelling of lutecium (element 71) was changed to lutetium.\n\nAt the time of their discovery, there was an element naming controversy as to what (particularly) the elements from 103 to 109 were to be called. \nAt last, a committee of the International Union of Pure and Applied Chemistry (IUPAC) resolved the dispute and adopted one name for each element. They also adopted a temporary systematic element name.\n\nIUPAC ratified the name lawrencium (Lr) in honor of Ernest Lawrence during a meeting in Geneva; the name was preferred by the American Chemical Society.\n\nThe Joint Institute for Nuclear Research in Dubna (then USSR, today Russia) named element 104 kurchatovium (Ku) in honor of Igor Kurchatov, father of the Soviet atomic bomb. But the University of California, Berkeley, US, named element 104 rutherfordium (Rf) in honor of Ernest Rutherford. \nIn 1997 a committee of IUPAC recommended that element 104 be named rutherfordium.\n\nThe Joint Institute for Nuclear Research in Dubna (a Russian city north of Moscow), proposed naming element 105 nielsbohrium (Ns) after Niels Bohr, while the University of California, Berkeley suggested the name hahnium (Ha) in honor of Otto Hahn. \nIUPAC recommended that element 105 be named dubnium, after Dubna.\n\nThe element was discovered almost simultaneously by two laboratories. In June 1974, a Soviet team led by G. N. Flyorov at the Joint Institute for Nuclear Research at Dubna reported producing the isotope 106, and in September 1974, an American research team led by Albert Ghiorso at the Lawrence Radiation Laboratory at the University of California, Berkeley reported creating the isotope 106. \nBecause their work was independently confirmed first, the Americans suggested the name seaborgium (Sg) in honor of Glenn T. Seaborg, an American chemist. This name was extremely controversial because Seaborg was still alive.\n\nAn international committee decided in 1992 that the Berkeley and Dubna laboratories should share credit for the discovery. \nAn element naming controversy erupted and as a result IUPAC adopted unnilhexium (Unh) as a temporary, systematic element name.\n\nIn 1994 a committee of IUPAC adopted a rule that no element can be named after a living person. This ruling was fiercely objected to by the American Chemical Society.\n\nSeaborg and Ghiorso pointed out that precedents had been set in the naming of elements 99 and 100 as einsteinium (Es) and fermium (Fm) during the lives of Albert Einstein and Enrico Fermi, although these names were not publicly announced until after Einstein and Fermi's deaths. In 1997, as part of a compromise involving elements 104 to 108, the name seaborgium for element 106 was recognized internationally.\n\nSome suggested the name nielsbohrium (Ns), in honor of Niels Bohr (this was separate from the proposal of the same name for element 105). IUPAC adopted unnilseptium (Uns) as a temporary systematic element name. In 1994 a committee of IUPAC recommended that element 107 be named bohrium (Bh), also in honor of Niels Bohr but using his surname only. While this conforms to the names of other elements honoring individuals where only the surname is taken, it was opposed by many who were concerned that it could be confused with boron, which is called \"borium\" in some languages including Latin. Despite this, the name bohrium for element 107 was recognized internationally in 1997.\n\nIUPAC adopted unniloctium (Uno) as a temporary, systematic element name.\nIn 1997 a committee of IUPAC recommended that element 108 be named hassium (Hs), in honor of the German state of Hesse (or Hassia in Latin). This state includes the city of Darmstadt, which is home to the GSI Helmholtz Centre for Heavy Ion Research where several new elements were discovered or confirmed. The element name was accepted internationally.\n\nIUPAC adopted unnilennium (Une) as a temporary, systematic element name. While \"meitnerium\" was discussed in the naming controversy, it was the only proposal and thus never disputed. In 1997 a committee of IUPAC adopted the name meitnerium in honor of Lise Meitner (Mt).\n\nFurther elements were named without controversy, starting with elements 110 (Ds, darmstadtium) and 111 (Rg, roentgenium), whose names were approved by IUPAC in 2003 and 2004 respectively. More elements followed in 2010 (112: Cn, copernicium), 2012 (114: Fl, flerovium; 116: Lv, livermorium), and 2016 (113: Nh, nihonium; 115: Mc, moscovium; 117, Ts, tennessine; and 118, Og, oganesson), completing the seventh row of the periodic table.\n\n\n"}
{"id": "49626177", "url": "https://en.wikipedia.org/wiki?curid=49626177", "title": "List of rectores magnifici of Maastricht University", "text": "List of rectores magnifici of Maastricht University\n\nA rector of a Dutch university is called a \"rector magnificus\". The following people have been rector magnificus of the Maastricht University:\n"}
{"id": "140829", "url": "https://en.wikipedia.org/wiki?curid=140829", "title": "List of statisticians", "text": "List of statisticians\n\nThis list of statisticians lists people who have made notable contributions to the theories or application of statistics, or to the related fields of probability or machine learning. Also included are actuaries and demographers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "54668072", "url": "https://en.wikipedia.org/wiki?curid=54668072", "title": "MCG+07-33-027", "text": "MCG+07-33-027\n\nMCG+07-33-027 is an isolated spiral galaxy located about 330 million light-years away in the constellation Hercules. It has a very high rate of star formation which would make it a starburst galaxy. Normally, starburst galaxies are triggered by the collision of another galaxy. However most galaxies are in groups or clusters, while MCG+07-33-027 is solitary. Therefore, the cause of the starburst was not due to a collision or by the passing of a nearby galaxy and so the cause of the activity remains unknown.\n\nOn April 2, 2005 a supernova of type Ic was discovered in MCG+07-33-027.\n\n"}
{"id": "54261880", "url": "https://en.wikipedia.org/wiki?curid=54261880", "title": "Mary Andross", "text": "Mary Andross\n\nMary Andross (1893-1968) was a Scottish food chemist. She was one of the leading pioneers of dietetics.\n\nAndross was born on 15 March 1893 in Irvine, Scotland. Her father, Henry Andross, was a cashier. He built a home for his family on Bank Street in 1901, where Mary lived for most of her life.\n\nAndross was an athletic child, but a car accident during her early years meant she lost much of the use of her legs. However, she continued to live an active life and enjoyed fishing on the island of Harris.\n\nAndross enrolled in the University of Glasgow in 1912. She studied various scientific subjects, including anatomy, organic chemistry, zoology, natural philosophy and physiology. She graduated in 1916 with a BSc in Science and Engineering.\n\nAfter graduation she undertook some postgraduate work at the Technical College and then at the University of Glasgow. She taught at Irvine Royal Academy between 1916 and 1917, then worked at the Ministry of Munitions Inspections Department on poison gases from 1917 to 1919. Afterwards, she returned to the University of Glasgow to work as a Chemistry Assistant until 1923.\n\nShe eventually became the Head of the Science Department of the Glasgow and West of Scotland College of Domestic Science, where she remained for 40 years until she retired in 1965.\n\nAndross became a Fellow of the Royal Institute of Chemistry in 1951, of the Institute of Food Science and Technology in 1964, and was a member of the Nutrition Society and the Society of Chemical Industry.\n\nDuring World War II, Andross made three major contributions to the field of dietetics:\n\nAndross died on 22 February 1968.\n"}
{"id": "34375922", "url": "https://en.wikipedia.org/wiki?curid=34375922", "title": "Mattauch isobar rule", "text": "Mattauch isobar rule\n\nThe Mattauch isobar rule, formulated by Josef Mattauch in 1934, states that if two adjacent elements on the periodic table have isotopes of the same mass number, one of these isotopes must be radioactive. Two nuclides that have the same mass number (isobars) can both be stable only if their atomic numbers differ by more than one (In fact, for currently observationally stable nuclides, the difference can only be 2 or 4, and in theory, two nuclides that have the same mass number cannot be both stable (at least to beta decay or double beta decay), but many such nuclides which are theoretically unstable to double beta decay have not been observed to decay, e.g. Xe-134). However, this rule cannot make predictions on the half-lives of these radioisotopes.\n\nA consequence of this rule is that technetium and promethium both have no stable isotopes as each of the elements just before and just after these two unstable elements on the periodic table (molybdenum and ruthenium, and neodymium and samarium, respectively) have a stable isotope for each mass number for the range in which the isotopes of the unstable elements usually would be stable to beta decay (note that although Sm147 is unstable, it is stable to beta decay, thus 147 is not a counterexample). These ranges can be calculated using the liquid drop model (for example the stability of technetium isotopes).\n\nThus no stable nuclides have proton number 43 or 61, by the same reason, no stable nuclides have neutron number 19, 21, 35, 39, 45, 61, 71, 89, 115, or 123.\n\nThe only known exceptions to the Mattauch isobar rule are the cases of antimony-123 and tellurium-123 and of hafnium-180 and tantalum-180m, where both nuclei are observationally stable. It is predicted that Te would undergo electron capture to form Sb, but this decay has not yet been observed; Ta should be able to undergo isomeric transition to Ta, beta decay to W, electron capture to Hf, or alpha decay to Lu, but none of these decay modes have been observed.\n"}
{"id": "6714405", "url": "https://en.wikipedia.org/wiki?curid=6714405", "title": "Military organization", "text": "Military organization\n\nMilitary organization or military organisation is the structuring of the armed forces of a state so as to offer such military capability as a national defense policy may require. In some countries paramilitary forces are included in a nation's armed forces, though not considered military. Armed forces that are not a part of military or paramilitary organizations, such as insurgent forces, often mimic military organizations, or use \"ad hoc\" structures, while formal military organization tends to use hierarchical forms.\n\nThe use of formalized ranks in a hierarchical structure came into widespread use with the Roman Army.\n\nIn modern times, executive control, management and administration of military organization is typically undertaken by governments through a government department within the structure of public administration, often known as a Ministry of Defense, Department of Defense, or Department of War. These in turn manage Armed Services that themselves command formations and units specialising in combat, combat support and combat-service support.\n\nThe usually civilian or partly civilian executive control over the national military organization is exercised in democracies by an elected political leader as a member of the government's Cabinet, usually known as a Minister of Defense. (In presidential systems, such as the United States, the president is the commander-in-chief, and the cabinet-level defense minister is second in command.) Subordinated to that position are often Secretaries for specific major operational divisions of the armed forces as a whole, such as those that provide general support services to the Armed Services, including their dependants.\n\nThen there are the heads of specific departmental agencies responsible for the provision and management of specific skill- and knowledge-based service such as Strategy advice, Capability Development assessment, or Defense Science provision of research, and design and development of technologies. Within each departmental agency will be found administrative branches responsible for further agency business specialization work.\n\nIn most countries the \"armed forces\" are divided into three or four Armed services (also: \"service, military service, or military branch\"): army, navy, and air force.\n\nMany countries have a variation on the standard model of three or four basic Armed Services. Some nations also organize their marines, special forces or strategic missile forces as independent armed services. A nation's coast guard may also be an independent military branch of its military, although in many nations the coast guard is a law enforcement or civil agency. A number of countries have no navy, for geographical reasons. Some other variations include:\n\n\nIn larger armed forces the culture between the different Armed Services of the armed forces can be quite different.\n\nMost smaller countries have a single organization that encompasses all armed forces employed by the country in question. Third-world armies tend to consist primarily of infantry, while first-world armies tend to have larger units manning expensive equipment and only a fraction of personnel in infantry units.\n\nIt is worthwhile to make mention of the term \"joint\". In western militaries, a joint force is defined as a unit or formation comprising representation of combat power from two or more branches of the military.\n\nGendarmeries, including equivalents such as Internal Troops, Paramilitary Forces and similar, are an internal security service common in most of the world, but uncommon in Anglo-Saxon countries where civil police are employed to enforce the law, and there are tight restrictions on how the armed forces may be used to assist.\n\nIt is common, at least in the European and North American militaries, to refer to the building blocks of a military as commands, formations and units.\n\nIn a military context, a command is a collection of units and formations under the control of a single officer. Although during the Second World War a Command was also a name given to a battle group in the US Army, in general it is an administrative and executive strategic headquarters which is responsible to the national government or the national military headquarters. It is not uncommon for a nation's services to each consist of their own command (such as Land Component, Air Component, Naval Component, and Medical Component in the Belgian Army), but this does not preclude the existence of commands which are not service-based.\n\nA formation is defined by the US Department of Defense as \"two or more aircraft, ships, or units proceeding together under a commander\". Formin in the Great Soviet Encyclopedia emphasised its combined-arms nature: \"Formations are those military organisations which are formed from different speciality Arms and Services troop units to create a balanced, combined combat force. The formations only differ in their ability to achieve different scales of application of force to achieve different strategic, operational and tactical goals and mission objectives.\" It is a composite military organization that includes a mixture of integrated and operationally attached sub-units, and is usually combat-capable. Example of formations include: divisions, brigades, battalions, wings, etc. Formation may also refer to tactical formation, the physical arrangement or disposition of troops and weapons. Examples of formation in such usage include: pakfront, panzerkeil, testudo formation, etc.\n\nA typical unit is a homogeneous military organization (either combat, combat-support or non-combat in capability) that includes service personnel predominantly from a single arm of service, or a branch of service, and its administrative and command functions are self-contained. Any unit subordinate to another unit is considered its sub-unit or minor unit. It is not uncommon for unit and formation to be used synonymously in the United States. In Commonwealth practice, formation is not used for smaller organizations like battalions which are instead called \"units\", and their constituent platoons or companies are referred to as sub-units. In the Commonwealth, formations are divisions, brigades, etc.\n\nDifferent armed forces, and even different branches of service of the armed forces, may use the same name to denote different types of organizations. An example is the \"squadron\". In most navies a squadron is a formation of several ships; in most air forces it is a unit; in the U.S. Army it is a battalion-sized cavalry unit; and in Commonwealth armies a squadron is a company-sized sub-unit.\n\nA table of organization and equipment (TOE or TO&E) is a document published by the U.S. Army Force Management Support Agency that prescribes the organization, manning, and equipage of units from divisional size and down, but also including the headquarters of Corps and Armies.\n\nIt also provides information on the mission and capabilities of a unit as well as the unit's current status. A general TOE is applicable to a type of unit (for instance, infantry) rather than a specific unit (the 3rd Infantry Division). In this way, all units of the same branch (such as infantry) follow the same structural guidelines.\n\nMain article: \"Command hierarchy\"\nThe following table gives an overview of some of the terms used to describe army hierarchy in armed forces across the world. Whilst it is recognized that there are differences between armies of different nations, many are modeled on the British or American models, or both. However, many military units and formations go back in history for a long time, and were devised by various military thinkers throughout European history.\n\nFor example, the modern \"Corps\" was first introduced in France about 1805 by Napoleon as a more flexible tactical grouping of two or more divisions during the Napoleonic Wars.\n\nThey have become part of the organization of most armies around the world.\n\nRungs may be skipped in this ladder: for example, typically NATO forces skip from battalion to brigade. Likewise, only large military powers may have organizations at the top levels and different armies and countries may also use traditional names, creating considerable confusion: for example, a British or Canadian armored regiment (battalion) is divided into squadrons (companies) and troops (platoons), whereas an American cavalry squadron (battalion) is divided into troops (companies) and platoons.\n\nArmy, army group, region, and theatre are all large formations that vary significantly between armed forces in size and hierarchy position. While divisions were the traditional level at which support elements (field artillery, hospital, logistics and maintenance, etc.) were added to the unit structure, since World War II, many brigades now have such support units, and since the 1980s, regiments also have been receiving support elements. A regiment with such support elements is called a regimental combat team in US military parlance, or a battle group in the UK and other forces.\n\nDuring World War II the Red Army used the same basic organizational structure. However, in the beginning many units were greatly underpowered and their size was actually one level below on the ladder that is usually used elsewhere; for example, a division in the early-WWII Red Army would have been about the size of most nations' regiments or brigades. At the top of the ladder, what other nations would call an army group, the Red Army called a front. By contrast, during the same period the German Wehrmacht Army Groups, particularly on the Eastern Front, such as Army Group Centre significantly exceeded the above numbers, and were more cognate with the Soviet Strategic Directions.\n\nNaval organization at the flotilla level and higher is less commonly abided by, as ships operate in smaller or larger groups in various situations that may change at a moment's notice. However, there is some common terminology used throughout navies to communicate the general concept of how many vessels might be in a unit.\n\nNavies are generally organized into groups for a specific purpose, usually strategic, and these organizational groupings appear and disappear frequently based on the conditions and demands placed upon a navy. This contrasts with army organization where units remain static, with the same men and equipment, over long periods of time.\n\nThe five-star ranks of Admiral of the Fleet and Fleet Admiral have largely been out of regular use since the 1990s, with the exception of ceremonial or honorary appointments. Currently, all major navies are commanded by an admiral (four-star rank) or vice-admiral (three-star rank) depending on relative size. Smaller naval forces, such as the RNZN, or those navies that are effectively coastguards, are commanded by a rear-admiral (two-star rank), commodore (one-star rank) or even a captain.\n\nAircraft carriers are typically commanded by a captain. Submarines and destroyers are typically commanded by a captain or commander. Some destroyers, particularly smaller destroyers such as frigates (formerly known as destroyer escorts) are usually commanded by officers with the rank of commander. Corvettes, the smallest class of warship, are commanded by officers with the rank of commander or lieutenant-commander. Auxiliary ships, including gunboats, minesweepers, patrol boats, military riverine craft, tenders and torpedo boats are usually commanded by lieutenants, sub-lieutenants or warrant officers. Usually, the smaller the vessel, the lower the rank of the ship's commander. For example, patrol boats are often commanded by ensigns, while frigates are rarely commanded by an officer below the rank of commander.\n\nHistorical navies were far more rigid in structure. Ships were collected in divisions, which in turn were collected in numbered squadrons, which comprised a numbered fleet. Permission for a vessel to leave one unit and join another would have to be approved on paper.\n\nThe modern U.S. Navy is primarily based on a number of standard groupings of vessels, including the carrier strike group and the Expeditionary Strike Group.\n\nAdditionally, Naval organization continues aboard a single ship. The complement forms three or four departments (such as tactical and engineering), each of which has a number of divisions, followed by work centers.\n\nThe organizational structures of air forces vary between nations: some air forces (such as the United States Air Force and the Royal Air Force) are divided into commands, groups and squadrons; others (such as the Soviet Air Force) have an Army-style organizational structure. The modern Royal Canadian Air Force uses Air Division as the formation between wings and the entire air command. Like the RAF, Canadian wings consist of squadrons.\n\nA task force is a unit or formation created as a temporary grouping for a specific operational purpose. Aside from administrative hierarchical forms of organization that have evolved since the early 17th century in Europe, fighting forces have been grouped for specific operational purposes into mission-related organizations such as the German Kampfgruppe or the U.S. Combat Team (Army) and Task Force (Navy) during the Second World War, or the Soviet Operational manoeuvre group during the Cold War. In the British and Commonwealth armies the battlegroup became the usual grouping of companies during the Second World War and the Cold War.\n\nWithin NATO, a Joint Task Force (JTF) would be such a temporary grouping that includes elements from more than one armed service, a Combined Task Force (CTF) would be such a temporary grouping that includes elements from more than one nation, and a Combined Joint Task Force (CJTF) would be such a temporary grouping that includes elements of more than one armed service and more than one nation.\n\n"}
{"id": "11760070", "url": "https://en.wikipedia.org/wiki?curid=11760070", "title": "NatCarb", "text": "NatCarb\n\nThe NatCarb geoportal provides access to geospatial information and tools concerning carbon sequestration in the United States.\n\n\nCarr, T.R., P.M. Rich, and J.D. Bartley. 2007. The NATCARB geoportal: linking distributed data from the Carbon Sequestration Regional Partnerships. \"Journal of Map and Geography Libraries (Geoscapes)\", \"Special Issue on Department of Energy (DOE) Geospatial Science Innovations\". In Press.\n"}
{"id": "5104401", "url": "https://en.wikipedia.org/wiki?curid=5104401", "title": "Outline of computer vision", "text": "Outline of computer vision\n\nThe following outline is provided as an overview of and topical guide to computer vision:\n\nComputer vision – interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring digital images (through image sensors), image processing, and image analysis, to reach an understanding of digital images. In general, it deals with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information that the computer can interpret. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images.\n\n\nHistory of computer vision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50629878", "url": "https://en.wikipedia.org/wiki?curid=50629878", "title": "Plant–soil feedback", "text": "Plant–soil feedback\n\nPlant–soil feedback is a process where plants alter the biotic and abiotic qualities of soil they grow in, which then alters the ability of plants to grow in that soil in the future.\n\nNegative plant–soil feedback occurs when plants are less able to grow in soil that was previously occupied by a member of the same species, and positive plant–soil feedback occurs when plants are more able to grow in soil that was previously occupied by a member of the same species. Although it was originally assumed that negative plant–soil feedback was caused by plants depleting the soil of nutrients, recent work has suggested that a major cause of plant–soil feedback is a buildup of soil-borne pathogens. Mutualism and allelopathy are also thought to cause plant–soil feedback. Studies have shown that, on average, plant–soil feedback tends to be negative; however, there have been many notable exceptions, such as many invasive species.\n\nNegative plant–soil feedback is thought to be an important factor in helping plants to coexist. If a plant is over-abundant, then soil pathogens and other negative factors will become common, hurting its growth. Similarly, if a plant becomes overly rare, then so too will its soil pathogens and other negative factors, helping its growth. This negative feedback will help populations to stay in the community. Negative plant–soil feedback has been called a particular case of the Janzen–Connell hypothesis.\n\nPlant–soil feedback is best measured using Bever's interaction coefficient, \"I\". This value quantifies how much each plant's growth is limited by its own soil community compared to how much it limits others. It is for two-species comparisons. To measure this quantity, one must measure the growth of two plants, both in soil conditioned by members of their own species (\"G\"(home) for species \"x\"), and in soil conditioned by members of the other species (\"G\"(away) for plant species \"x\"). Then, the interaction coefficient is calculated as\n\nIf \"I\" is negative, it means that both species grow worse in their own site compared to their competitor's soil, and therefore plant–soil feedback helps these species to coexist. \n"}
{"id": "35757264", "url": "https://en.wikipedia.org/wiki?curid=35757264", "title": "Prescriptive analytics", "text": "Prescriptive analytics\n\nPrescriptive analytics is the third and final phase of business analytics, which also includes descriptive and predictive analytics.\n\nReferred to as the \"final frontier of analytic capabilities,\" prescriptive analytics entails the application of mathematical and computational sciences and suggests decision options to take advantage of the results of descriptive and predictive analytics. The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting – such as sales, marketing, operations, and finance – uses this type of post-mortem analysis.\n\nThe next phase is predictive analytics. Predictive analytics answers the question what is likely to happen. This is when historical data is combined with rules, algorithms, and occasionally external data to determine the probable future outcome of an event or the likelihood of a situation occurring. The final phase is prescriptive analytics, which goes beyond predicting future outcomes by also suggesting actions to benefit from the predictions and showing the implications of each decision option.\n\nPrescriptive analytics not only anticipates what will happen and when it will happen, but also why it will happen. Further, prescriptive analytics suggests decision options on how to take advantage of a future opportunity or mitigate a future risk and shows the implication of each decision option. Prescriptive analytics can continually take in new data to re-predict and re-prescribe, thus automatically improving prediction accuracy and prescribing better decision options. Prescriptive analytics ingests hybrid data, a combination of structured (numbers, categories) and unstructured data (videos, images, sounds, texts), and business rules to predict what lies ahead and to prescribe how to take advantage of this predicted future without compromising other priorities.\n\nAll three phases of analytics can be performed through professional services or technology or a combination. In order to scale, prescriptive analytics technologies need to be adaptive to take into account the growing volume, velocity, and variety of data that most mission critical processes and their environments may produce.\n\nOne criticism of prescriptive analytics is that its distinction from predictive analytics is ill-defined and therefore ill-conceived. \n\nPrescriptive analytics incorporates both structured and unstructured data, and uses a combination of advanced analytic techniques and disciplines to predict, prescribe, and adapt. While the term prescriptive analytics was first coined by IBM and later trademarked by Ayata, the underlying concepts have been around for hundreds of years. The technology behind prescriptive analytics synergistically combines hybrid data, business rules with mathematical models and computational models. The data inputs to prescriptive analytics may come from multiple sources: internal, such as inside a corporation; and external, also known as environmental data. The data may be structured, which includes numbers and categories, as well as unstructured data, such as texts, images, sounds, and videos. Unstructured data differs from structured data in that its format varies widely and cannot be stored in traditional relational databases without significant effort at data transformation. More than 80% of the world's data today is unstructured, according to IBM.\n\nIn addition to this variety of data types and growing data volume, incoming data can also evolve with respect to velocity, that is, more data being generated at a faster or a variable pace. Business rules define the business process and include objectives constraints, preferences, policies, best practices, and boundaries. Mathematical models and computational models are techniques derived from mathematical sciences, computer science and related disciplines such as applied statistics, machine learning, operations research, natural language processing, computer vision, pattern recognition, image processing, speech recognition, and signal processing. The correct application of all these methods and the verification of their results implies the need for resources on a massive scale including human, computational and temporal for every Prescriptive Analytic project. In order to spare the expense of dozens of people, high performance machines and weeks of work one must consider the reduction of resources and therefore a reduction in the accuracy or reliability of the outcome. The preferable route is a reduction that produces a probabilistic result within acceptable limits.\n\nEnergy is the largest industry in the world ($6 trillion in size). The processes and decisions related to oil and natural gas exploration, development and production generate large amounts of data. Many types of captured data are used to create models and images of the Earth’s structure and layers 5,000 - 35,000 feet below the surface and to describe activities around the wells themselves, such as depositional characteristics, machinery performance, oil flow rates, reservoir temperatures and pressures. Prescriptive analytics software can help with both locating and producing hydrocarbons\nby taking in seismic data, well log data, production data, and other related data sets to prescribe specific recipes for how and where to drill, complete, and produce wells in order to optimize recovery, minimize cost, and reduce environmental footprint.\n\nWith the value of the end product determined by global commodity economics, the basis of competition for operators in upstream E&P is the ability to effectively deploy capital to locate and extract resources more efficiently, effectively, predictably, and safely than their peers. In unconventional resource plays, operational efficiency and effectiveness is diminished by reservoir inconsistencies, and decision-making impaired by high degrees of uncertainty. These challenges manifest themselves in the form of low recovery factors and wide performance variations.\n\nPrescriptive Analytics software can accurately predict production and prescribe optimal configurations of controllable drilling, completion, and production variables by modeling numerous internal and external variables simultaneously, regardless of source, structure, size, or format. Prescriptive analytics software can also provide decision options and show the impact of each decision option so the operations managers can proactively take appropriate actions, on time, to guarantee future exploration and production performance, and maximize the economic value of assets at every point over the course of their serviceable lifetimes.\n\nIn the realm of oilfield equipment maintenance, Prescriptive Analytics can optimize configuration, anticipate and prevent unplanned downtime, optimize field scheduling, and improve maintenance planning. According to General Electric, there are more than 130,000 electric submersible pumps (ESP's) installed globally, accounting for 60% of the world's oil production. Prescriptive Analytics has been deployed to predict when and why an ESP will fail, and recommend the necessary actions to prevent the failure.\n\nIn the area of Health, Safety, and Environment, prescriptive analytics can predict and preempt incidents that can lead to reputational and financial loss for oil and gas companies.\n\nPricing is another area of focus. Natural gas prices fluctuate dramatically depending upon supply, demand, econometrics, geopolitics, and weather conditions. Gas producers, pipeline transmission companies and utility firms have a keen interest in more accurately predicting gas prices so that they can lock in favorable terms while hedging downside risk. Prescriptive analytics software can accurately predict prices by modeling internal and external variables simultaneously and also provide decision options and show the impact of each decision option.\n\nMultiple factors are driving healthcare providers to dramatically improve business processes and operations as the United States healthcare industry embarks on the necessary migration from a largely fee-for service, volume-based system to a fee-for-performance, value-based system. Prescriptive analytics is playing a key role to help improve the performance in a number of areas involving various stakeholders: payers, providers and pharmaceutical companies.\n\nPrescriptive analytics can help providers improve effectiveness of their clinical care delivery to the population they manage and in the process achieve better patient satisfaction and retention. Providers can do better population health management by identifying appropriate intervention models for risk stratified population combining data from the in-facility care episodes and home based telehealth.\n\nPrescriptive analytics can also benefit healthcare providers in their capacity planning by using analytics to leverage operational and usage data combined with data of external factors such as economic data, population demographic trends and population health trends, to more accurately plan for future capital investments such as new facilities and equipment utilization as well as understand the trade-offs between adding additional beds and expanding an existing facility versus building a new one.\n\nPrescriptive analytics can help pharmaceutical companies to expedite their drug development by identifying patient cohorts that are most suitable for the clinical trials worldwide - patients who are expected to be compliant and will not drop out of the trial due to complications. Analytics can tell companies how much time and money they can save if they choose one patient cohort in a specific country vs. another.\n\nIn provider-payer negotiations, providers can improve their negotiating position with health insurers by developing a robust understanding of future service utilization. By accurately predicting utilization, providers can also better allocate personnel.\n\n\n"}
{"id": "267869", "url": "https://en.wikipedia.org/wiki?curid=267869", "title": "Project Genie", "text": "Project Genie\n\nProject Genie was a computer research project started in 1964 at the University of California, Berkeley. \nIt produced an early time-sharing system including the Berkeley Timesharing System, which was then commercialized as the SDS 940.\n\nProject Genie was funded by J. C. R. Licklider, the head of DARPA at that time. The project was a smaller counterpart to MIT's Project MAC.\n\nThe Scientific Data Systems SDS 940 was created by modifying an SDS 930 24-bit commercial computer so that it could be used for timesharing. The work was funded by ARPA and directed by Melvin W. Pirtle at and Wayne Lichtenberger at UC Berkeley. Butler Lampson, Chuck Thacker, and L. Peter Deutsch were among the young technical leaders of that project. When completed and in service, the first 940 ran reliably in spite of its array of tricky mechanical issues such as a huge disk drive driven by hydraulic arms. It served about forty or fifty users at a time and still managed to drive a graphics subsystem that was quite capable for its time. \n\nWhen SDS realized the value of the time sharing system, and that the software was in the public domain (funded by the US federal government), they came back to Berkeley and collected enough information to begin manufacturing. Because SDS manufacturing was overloaded with the 9 series production and the startup of the Sigma Series production, it could not incorporate the 940 modifications into the standard production line. Instead, production of the 940s was turned over to the Systems Engineering Department, which manufactured systems customised to user requirements. To produce a 940, the Systems Engineering Department ordered a 930 from SDS manufacturing, installed the modifications developed by the Berkeley engineers, and shipped machine to the SDS customer as a 940.\n\nProject Genie pioneered several computer hardware techniques, such as commercial time-sharing which allowed end-user programming in machine language, separate protected user modes, memory paging, and protected memory. Concepts from Project Genie influenced the development of the TENEX operating system for the PDP-10, and Unix, which inherited the concept of process forking from it (Unix co-creator Ken Thompson worked on an SDS 940 while at Berkeley). An SDS 940 mainframe was used by Douglas Engelbart's OnLine System at the Stanford Research Institute and was the first computer used by the Community Memory Project at Berkeley.\n\nA follow-on project was called CalTSS for a dual-processor CDC 6400 which ended quickly in 1969.\nSeveral members of project Genie such as Pirtle, Thacker, Deutsch and Lampson left UCB to form the Berkeley Computer Corporation (BCC), which produced one prototype, the BCC-500.\nAfter BCC went bankrupt after the recession of 1969–70, the BCC-500 was transferred to the University of Hawaii, where it continued in use through the 1970s. It became part of the ALOHAnet.\n\nSeveral BCC employees became the core of Xerox PARC's computer research group (Deutsch, Lampson and Thacker) in 1970. Lichtenberger went to the University of Hawaii, and was an early employee at Cisco Systems.\n\nPirtle became technical director for the ILLIAC IV project at NASA Ames Research Center.\n\n\n"}
{"id": "29579190", "url": "https://en.wikipedia.org/wiki?curid=29579190", "title": "Science Online", "text": "Science Online\n\nScience Online was an annual conference held in Durham, North Carolina, that focuses on the role of the internet in science and science communication. It is attended primarily by bloggers and science journalists from North America.\n\nThe conference was held annually, beginning in 2007. Notable attendees have included PZ Myers, Jennifer Ouellette, Rebecca Skloot, Carl Zimmer and others. The conferences have been covered as local news by publications such as the \"Charlotte Observer\", as well as \"new media\" like Boing Boing, professional journalist organizations like the \"Columbia Journalism Review\" and science-oriented publications like \"Scientific American\".\n\nIn October 2014, the ScienceOnline foundation, which organized the conferences, announced that it had become insolvent and consequently was shutting down.\n"}
{"id": "14138784", "url": "https://en.wikipedia.org/wiki?curid=14138784", "title": "Sophus Mads Jørgensen", "text": "Sophus Mads Jørgensen\n\nSophus Mads Jørgensen (4 July 1837 – 1 April 1914) was a Danish chemist. He is considered one of the founders of coordination chemistry, and is known for the debates which he had with Alfred Werner during 1893-1899. While Jørgensen's theories on coordination chemistry were ultimately proven to be incorrect, his experimental work provided much of the basis for Werner's theories. Jørgensen also made major contributions to the chemistry of platinum and rhodium compounds.\n\nJørgensen was a board member of the Carlsberg Foundation from 1885 until his death in 1914, and was elected a member of the Royal Swedish Academy of Sciences in 1899.\n\n"}
{"id": "12173000", "url": "https://en.wikipedia.org/wiki?curid=12173000", "title": "Soyuz 7K-TM", "text": "Soyuz 7K-TM\n\nThe 1975 Apollo–Soyuz Test Project version of the Soyuz spacecraft (Soyuz 7K-TM) served as a technological bridge to the third generation Soyuz-T (T - транспортный, \"Transportnyi\" meaning transport) spacecraft (1976–1986). \n\nThe Soyuz ASTP spacecraft was designed for use during the Apollo Soyuz Test Project as Soyuz 19. It featured design changes to increase compatibility with the American craft. The Soyuz ASTP featured new solar panels for increased mission length, an APAS-75 docking mechanism instead of the standard male mechanism and modifications to the environmental control system to lower the cabin pressure to 0.68 atmospheres (69 kPa) prior to docking with Apollo. The ASTP Soyuz backup craft flew as the Soyuz 22 mission, replacing the docking port with a camera.\n\n\n\n"}
{"id": "9271587", "url": "https://en.wikipedia.org/wiki?curid=9271587", "title": "Staling", "text": "Staling\n\nStaling, or \"going stale\", is a chemical and physical process in bread and other foods that reduces their palatability. Stale bread is dry and leathery.\n\nStaling is not, as is commonly believed, simply a drying-out process due to evaporation. Bread will stale even in a moist environment, and stales most rapidly at temperatures just above freezing. Bread stored in the refrigerator will have increased staling rates, and therefore bread should be kept at room temperature. However, refrigeration delays the growth of mold and extends the shelf life of bread.\n\nOne important mechanism is the migration of moisture from the starch granules into the interstitial spaces, degelatinizing the starch. The starch amylose and amylopectin molecules realign themselves causing recrystalisation. This results in stale bread's leathery, hard texture. Additionally, pleasant \"fresh\" flavor is lost to the air, and often unpleasant flavor is absorbed from it as well, especially in a confined space with other food such as when in a refrigerator.\n\nAnti-staling agents used in bread include wheat gluten, enzymes, and glycerolipids, mainly monoglycerides and diglycerides.\n\nStale bread is an important ingredient in many dishes, some of which were invented for the express purpose of using up otherwise unpalatable stale bread. Examples include bread sauce, bread dumplings, and flummadiddle, an early American savoury pudding. A sweet dish is bread pudding.\n\nThere are many types of bread soups such as gazpacho (in Spanish cuisine), wodzionka (in Silesian cuisine), ribollita (in Italian cuisine).\n\nStale bread can be used to \"stretch\" meat in dishes such as haslet (a type of meatloaf in American cuisine) and garbure (a stew in French cuisine). It can be a subsidiary ingredient in dishes such as fattoush (a type of salad in Levantine cuisine). Stale bread can be used as a base for dips such as skordalia (in Greek cuisine), or substituted with another ingredient. \n\nCubes of stale bread can be dipped in cheese fondue, or seasoned and baked in the oven to become croutons, suitable for scattering in salads or on top of soups. Slices of stale bread soaked in an egg and milk mixture and then fried turn into French toast (known in French as \"pain perdu\" - lost bread).\n\nSometimes the same word can have different meanings in different countries. \"Migas\", in Spanish and Portuguese cuisines, is a breakfast dish using stale bread, and \"leblebi\", in Tunisian cuisine, is a soup of chickpeas and stale bread.\n\nIn medieval cuisine, slices of stale bread, called trenchers, were used instead of plates.\n\nStale bread can be partially destaled by heating to 60 °C (140 °F) in a conventional oven or microwave oven. However, if not eaten before it cools or dries, the bread is even worse than before due to the moisture loss.\n\n"}
{"id": "22965231", "url": "https://en.wikipedia.org/wiki?curid=22965231", "title": "Supersingular prime (for an elliptic curve)", "text": "Supersingular prime (for an elliptic curve)\n\nIn algebraic number theory, a supersingular prime is a prime number with a certain relationship to a given elliptic curve. If the curve \"E\" defined over the rational numbers, then a prime \"p\" is supersingular for \"E\" if the reduction of \"E\" modulo \"p\" is a supersingular elliptic curve over the residue field F.\n\nMore generally, if \"K\" is any global field—i.e., a finite extension either of Q or of F(\"t\")—and \"A\" is an abelian variety defined over \"K\", then a supersingular prime formula_2 for \"A\" is a finite place of \"K\" such that the reduction of \"A\" modulo formula_2 is a supersingular abelian variety.\n\n"}
{"id": "43629419", "url": "https://en.wikipedia.org/wiki?curid=43629419", "title": "Sutton armillary", "text": "Sutton armillary\n\nThe Millennium Dial Armillary is one of six pieces of public art located in the town centre of Sutton in Greater London, England. The others include the Sutton heritage mosaic, the Sutton twin towns mural and the Messenger statue. \n\nThe armillary was dedicated to the town in 2000 by the Rotary Club, and is in the form of an historical timepiece. It serves three purposes: firstly, simply to tell the time; secondly, to commemorate time through various inscriptions including the Rotary motto \"Service Above Self\" and distances to nearby areas such as Kingston upon Thames; and thirdly, to commemorate the work which the Rotary Club has done.\n\nThe inscription on the plinth reads as follows:\n\nThe brief history of the Sutton armillary is that, in the years leading up to the new millennium, the London Borough of Sutton expressed its wish for time-related millennium projects. The Rotary Club responded to this by conceiving, planning and jointly funding the armillary. It was designed to last for years to come, and was originally positioned as the central feature of a Millennium Garden. It was slightly re-positioned in 2011, following a repaving of the pedestrianised High Street area, since when it has stood on the edge of the new central square in the town, directly in front of the Waterstones bookshop. When deciding on the new position, the Rotary Club and the local council had to take account of the need for an adequate supply of sunlight. \n\nThe armillary also had to be removed temporarily in November 2012, when it came off its plinth – this received coverage in the local press in a column headed \"Time stops in Sutton High Street after Armillary removed\".\n\nThe armillary's installation has provided a focus for the town centre, and it will remain as a permanent memorial, marking both the new millennium and the important role the Rotary has played in the welfare of Sutton since 1923.\n"}
{"id": "1998755", "url": "https://en.wikipedia.org/wiki?curid=1998755", "title": "The Global 2000 Report to the President", "text": "The Global 2000 Report to the President\n\nThe Global 2000 Report to the President was a 1980 report on sustainable societal development, commissioned by President Jimmy Carter on May 23, 1977. The report was released at a press conference in the White House on July 24, 1980. The report sold 1.5 million copies in 9 languages. There were many editions of the report.\n\nThe Global 2000 project was directed by Gerald O. Barney, and the main conclusion of the report was: \n\nThe Global 2000 Report was based on the best data and models available from 14 participating government agencies plus the World Bank. Projections were made using computer models. \n\nThe Global 2000 project produced a report, but the Global 2000 project was more than a report. It was the effort by a nation to prepare a 20-year outlook on probable changes in the world’s population, resources, economy, and environment. time the long-term global professionals of an entire national government were assembled as a team to look at the future of the entire world. time that sector-specific global policy models (population, energy, agriculture, forestry, economy, minerals, non-fuel minerals, water, etc.) were assembled and consistency assessed as the “foundation for longer-term planning” that they in fact are. It was time that all previous global studies were assembled and reviewed for their insights and recommendations. It was time that a nation made an assessment of all of its previous efforts to take a long-term view of anything.\n\nGlobal 2000 was not the first computer-based global study. The first computer-based global study, \"The Limits to Growth\" was published in 1972. \"Limits\" questioned growth as the answer to all development and social problems and started a global discussion of global warming, energy scarcity, human population growth, plant and species extinctions , genetic diversity , and a global economic system based on unlimited human wants in contrast to Earth's finite resources, etc. \n\nLibrary of Congress Classification: Volume 1: HC79.E5 G59 1980b\n\n"}
{"id": "32326682", "url": "https://en.wikipedia.org/wiki?curid=32326682", "title": "The National Marriage Project", "text": "The National Marriage Project\n\nThe National Marriage Project is a research project based in the U.S. that investigates how American marriages are formed, maintained and ended, and how society is affected. The project gathers statistical information and analyzes it to provide education to the public, and to formulate recommendations for the future. The project was started in 1997 by David Popenoe, a sociologist at Rutgers University. Since 2009 it has been based at the University of Virginia under the direction of sociologist W. Bradford Wilcox.\n\n"}
{"id": "20203853", "url": "https://en.wikipedia.org/wiki?curid=20203853", "title": "The Psychology of Nuclear Proliferation", "text": "The Psychology of Nuclear Proliferation\n\nThe Psychology of Nuclear Proliferation: Identity, Emotions, and Foreign Policy is a 2006 book by Jacques E. C. Hymans, published by Cambridge University Press. In the book, Hymans draws on the humanities and social sciences to build a model of decision-making that links identity to emotions and ultimately to nuclear energy policy choices. \n\n"}
{"id": "13707020", "url": "https://en.wikipedia.org/wiki?curid=13707020", "title": "Thomas Stanley Westoll", "text": "Thomas Stanley Westoll\n\nThomas Stanley Westoll, FRS FRSE, FGS (3 July 1912 – 19 September 1995) was a British geologist, and the long-time head of the Department of Geology at Newcastle University.\n\nHe was born in West Hartlepool and educated at the local grammar school. He began a brilliant career as zoologist, palaeontologist, but primarily a geologist, when he entered Armstrong College at the age of 17 by means of an open entrance scholarship in 1929. Armstrong College went on eventually to become Newcastle University. He gained a BSc in geology in 1932 and a PhD in 1934 from research on Permian fishes. His association with Newcastle University was to endure throughout his life, his central interest being the study of fossil fish. He was head of department from 1948 until his retirement in 1977. In retirement he remained as a research fellow and Chairman of Convocation.\n\nHe was elected a Fellow of the Royal Society in March 1952. The citation on his application read: \"\"Westoll is a palaeontologist who by his description of new materials and by the introduction of new and fertile ideas into the interpretation of the structure of early fossil vertebrates has greatly increased our understanding of the problems they present. He has introduced new views about the origins of the pectoral fins of craniates and of the Tetrapod limb. He has clarified our ideas about the homologies of the dermal skull bones of vertebrates and made a new and convincing comparison between the skulls of Amphibia and Fish. He has made important contributions towards the solution of the old problems of the origin of the mammalian palate and ear. His monograph of the Haplolepidae sets a new standard for taxonomic work on fossil fish\".\" \n\nHe was on the council of the Royal Society and from 1972 to 1974 was President of the Geological Society of London.\n\nHe died in Newcastle upon Tyne in 1995.\n\nHis research interests were wide-ranging, but he is best known for his work on the evolution of fish. The development of the tetrapod limb and issues with the Silurian-Devonian boundary were some of the topics which occupied him. Throughout a long academic career he made forceful and important contributions in these and other fields\n\n"}
{"id": "866256", "url": "https://en.wikipedia.org/wiki?curid=866256", "title": "User illusion", "text": "User illusion\n\nThe user illusion is the illusion created for the user by a human–computer interface, for example the visual metaphor of a desktop used in many graphical user interfaces. The phrase originated at Xerox PARC.\n\nSome philosophers of mind have argued that consciousness is a form of user illusion. This notion is explored by Tor Nørretranders in his 1991 Danish book \"Mærk verden\", issued in a 1998 English edition as \"The User Illusion: Cutting Consciousness Down to Size\". He introduced the idea of exformation in this book. Philosopher Daniel Dennett has also embraced the view that human consciousness is a \"user-illusion.\"\n\nAccording to this picture, our experience of the world is not immediate, as all sensation requires processing time. It follows that our conscious experience is less a perfect reflection of what is occurring, and more a simulation produced unconsciously by the brain. Therefore, there may be phenomena that exist beyond our peripheries, beyond what consciousness could create to isolate or reduce them.\n\nCritics of the idea of consciousness being a device for justifying preconceptions argue that such a device would consume nutrients without producing any useful results, since it would not change the outcome of any decisions. These critics argue that the existence of social insects with extremely small brains falsifies the notion that social behavior requires consciousness, citing that insects have too small brains to be conscious and yet there are observed behaviors among them that for all functional intents and purposes match those of complex social cooperation and manipulation (including hierarchies where each individual has its place among paper wasps and Jack Jumper ants and honey bees sneaking when they lay eggs). These critics also argue that since social behavior in insects and other extremely small-brained animals have evolved multiple times independently, there is no evolutionary difficulty in simple reaction sociality to impose selection pressure for the more nutrient-consuming path of consciousness for sociality. These critics do point out that other evolutionary paths to consciousness are possible, such as critical evaluation that enhances plasticity by criticizing fallible notions, while pointing out that such a critical consciousness would be quite different from the justificatory type proposed by Nørretranders, differences including that a critical consciousness would make individuals more capable of changing their minds instead of justifying and persuading.\n\n"}
{"id": "50270017", "url": "https://en.wikipedia.org/wiki?curid=50270017", "title": "Von Baer's laws (embryology)", "text": "Von Baer's laws (embryology)\n\nVon Baer's laws of embryology (or laws of development) are four rules discovered by Karl Ernst von Baer to explain the observed pattern of embryonic development in different species.\n\nVon Baer formulated the laws in the book \"Über Entwickelungsgeschichte der Thiere\" (\"On the Developmental History of Animals\"), published in 1828, while working at the University of Königsberg. He specifically intended to rebut Johann Friedrich Meckel's 1808 recapitulation theory. According to that theory, embryos pass through successive stages that represent the adult forms of less complex organisms in the course of development, and that ultimately reflects \"scala naturae\" (the great chain of being). von Baer believed that such linear development is impossible. He posited that instead of linear progression, embryos started from one or a few basic forms that are similar in different animals, and then developed in a branching pattern into increasingly different organisms. Defending his ideas, he was also opposed to Charles Darwin's 1859 theory of common ancestry and descent with modification, and particularly to Ernst Haeckel's revised recapitulation theory with its slogan \"ontogeny recapitulates phylogeny\".\n\nVon Baer's laws are a series of statements generally summarised into four points. As translated by Thomas Henry Huxley in his \"Scientific Memoirs\":\n\n\nVon Baer discovered the blastula (the early hollow ball stage of an embryo) and the development of the notochord (the stiffening rod along the back of all chordates, that forms after the blastula and gastrula stages). From his observations of these stages in different vertebrates, he realised that Johann Friedrich Meckel's recapitulation theory must be wrong. For example, he noticed that the yolk sac is found in birds, but not in frogs. According to the recapitulation theory, such structures should invariably be present in frogs because they were assumed to be at a lower level in the evolutionary tree. Von Baer concluded that while structures like the notochord are recapitulated during embryogenesis, whole organisms are not. He asserted that (as translated):\n\nIn terms of taxonomic hierarchy, according to von Baer, characters in the embryo are formed in top-to-bottom sequence, first from those of the largest and oldest taxon, the phylum, then in turn class, order, family, genus, and finally species.\n\nThe laws received a mixed appreciation. While they were criticised in detail, they formed the foundation of modern embryology. The British zoologist Adam Sedgwick studied the developing embryos of dogfish and chicken, and in 1894 noted a series of differences, such as the green yolk in the dogfish and yellow yolk in the chicken, absence of embryonic rim in chick embryos, absence of blastopore in dogfish, and differences in the gill slits and gill clefts. He concluded:\n\nThe most important supporter of von Baer's laws was Charles Darwin, who wrote in his \"Origin of Species\":\n\nDarwin took up the concept of common descent which formed part of his theory of evolution. But von Baer was a vociferous anti-Darwinist, devoting much of his scholarly effort to criticising Darwinism. His criticism culminated with his last work \"Über Darwins Lehre\" (\"On Darwin's Doctrine\"), published in the year of his death in 1876.\n\n"}
