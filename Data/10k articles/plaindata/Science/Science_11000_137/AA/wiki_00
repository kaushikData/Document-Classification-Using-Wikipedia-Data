{"id": "19918926", "url": "https://en.wikipedia.org/wiki?curid=19918926", "title": "Active surface", "text": "Active surface\n\nAn active surface is a surface of a radio telescope that is under active computer control of its shape.\n\nLarge (more than 10 m in diameter or length) radio telescopes always bend during operation, due to their enormous weight and the fact that even the strongest materials are not perfectly stiff. This bending, in the range of a few millimetres, does not affect low frequency operation much, but dramatically reduces the efficiency of the telescope at higher frequencies where the wavelengths are comparable to the distortion. Typically, the efficiency of a telescope drops appreciably when the deviation from the desired shape is more than 1/10 of the considered wavelength. An active surface uses numerous small actuators to move the surface panels with respect to the underlying frame, and thus maintain the correct shape.\n\nAn active surface can try to compensate for many different types of errors. The first is gravity—this is simplest since previous measurements, or even a mathematical model, can be used to predict (and correct) any bending. More difficult is correction for wind and thermal errors, since these require measuring and correcting in real time.\n\nSome examples of active surfaces are:\n\nThe Chinese Five hundred meter Aperture Spherical Telescope uses a uniquely ambitious form of active surface, not only correcting errors, but applying deflections of up to in order to aim and focus the telescope.\n\n"}
{"id": "5306982", "url": "https://en.wikipedia.org/wiki?curid=5306982", "title": "Aeronomy of Ice in the Mesosphere", "text": "Aeronomy of Ice in the Mesosphere\n\nThe Aeronomy of Ice in the Mesosphere (AIM) is a satellite to conduct a 26-month study of noctilucent clouds (NLCs). It is the ninetieth Explorer program mission and is part of the NASA-funded Small Explorer program (SMEX). On April 25, 2007 AIM was boosted into a high polar orbit by a Pegasus-XL rocket, which was air-launched from the Lockheed L-1011 \"Stargazer\" aircraft operated by Orbital Sciences.\n\nThe AIM satellite is a , by spacecraft, powered by two solar arrays, carrying three instruments:\n\nThe noctilucent clouds AIM is to study, also known as Polar mesospheric clouds, occur in the Earth's atmosphere at high altitudes of roughly above the surface, far higher than other clouds. The AIM mission will help determine what factors — temperature, water vapor, and dust particles — lead to the formation of these clouds. The clouds seem to be a relatively recent phenomenon: they were first seen in 1885, and lately seem to be occurring more frequently. The clouds always occur during the summer season near the poles and the Northern season always starts around the same time. Scientists have found that the start of the Southern season can vary up to a month however.\n\n"}
{"id": "11161239", "url": "https://en.wikipedia.org/wiki?curid=11161239", "title": "Aquarium fish feeder", "text": "Aquarium fish feeder\n\nAquarium fish feeders are electric or electronic devices that are designed to feed aquarium fish at regular intervals. They are often used to feed fish when the aquarist is on vacation or is too busy to maintain a regular feeding schedule.\n\nFish feeders are usually clamped to the wall of the tank just over the water. They consist of a hopper which is loaded with a variety of dry food, a timer which rotates the hopper at regular intervals (dispensing food in the process), and a method of setting the interval between feeding and the amount of food dispensed. \n\nMost feeders can dispense flake, pellet, or freeze dried food.\n\nThe benefits of electronic aquarium feeders are not only that the fish are fed when the aquarist is not at home, but they are also helpful in maintaining the fish' health. Because they are feeding small portions of food at scheduled intervals and precise feedings at appropriate times, the automatic feeders can be successfully used to feed diabetic fish. \n\nAnother concern of aquarists is overeating. Fish are not to be given too much food. It is estimated that fish should be given as much as they can eat in 3 to 5 minutes, and once a day. However, the electronic fish feeders prevent overeating by releasing the right quantity of food, at scheduled times. This way, aquarists who have to get away for few days do not have to ask their neighbours or friends to come over and take care of their pets. It is often impossible to find a reliable person available and willing to do such a favor. The electronic fish feeders are therefore a solution for fish keepers who own aquariums and which ensure that the pets are fed in a healthy way and on schedule.\n\nAnother advantage of these devices is that electronic fish feeders may also be used to feed amphibians, if they are kept in glass cages. \n\nThere are also disadvantages that come with the electronic feeders. First, fish tend to get used to where and when the timer is going to trigger and food is going to fall which can create a feeding frenzy when the feeder drops the food. This usually results in a lot of splashing which may wet the rest of the food. Mold can then grow and the leftover food is likely to go bad or to clog the feeder's mechanism. The humidity and moisture due to close proximity to the water can also cause this type of problem with an electronic feeder. Second, one has to make sure that the food containers are properly sealed and the food is kept fresh. At this time electronic feeders are not able to adjust to the changing needs of the fish over longer periods which may result in either overfeeding or underfeeding. Too much food in the water is not only bad for the fish, but also for their environment.\n\nThe maintenance of electronic aquarium feeders basically consists in keeping the device clean and making sure the electronic part of the feeder does not get wet, which may cause improper functioning.\n\nThough some feeders are designed specifically to keep food dry, many designs allow moisture to seep into the food hopper. This can cause clumping, and can result in the failure of the mechanism. Because fish feeders generally cannot feed frozen or live food, they are not effective options for feeding most predatory fish.\n\n"}
{"id": "8740164", "url": "https://en.wikipedia.org/wiki?curid=8740164", "title": "Bad Astronomy", "text": "Bad Astronomy\n\nBad Astronomy: Misconceptions and Misuses Revealed, from Astrology to the Moon Landing \"Hoax\" is a non-fiction book by the American astronomer Phil Plait, also known as \"the Bad Astronomer\". The book was published in 2002 and deals with various misunderstandings about space and astronomy, such as sounds being audible in space (a misconception because in the vacuum of space, sound has no medium in which to propagate).\n\nPlait's first book received generally favorable reviews within the academic and astronomy communities and was the first volume in the \"Bad Science\" series by John Wiley & Sons Publishing.\n\nInspired by the author's web site, \"Bad Astronomy,\" the book attempts to explore twenty-four common astronomical fallacies and explain the scientific consensus concerning these topics within the field of astronomy.\n\nThe book explains and corrects many ideas relating to space that, according to Plait, are mistaken but nevertheless often portrayed in popular movies. Plait also dedicates much of the book to debunking the idea of a Moon landing hoax and explains why astrology should not be taken seriously. A part of the book describes the Moon's tidal effects and explains the Coriolis effect, why the sky is blue, the Big Bang and other related topics.\n\nMany of the book's topics and arguments also are found on Plait's page at the \"Slate\" magazine blog site, but Plait explores them in greater depth in the book. He states that the book is intended to debunk popular myths and also to describe science in an easily comprehensible way.\n\nTormod Guldvog writes in his review that \"It is indeed a gem when it comes to teaching things about common astronomical phenomena. Plait discusses common ways bad astronomy is communicated, in the media, in the classroom, and perhaps, most of all, in our own minds.\"\n\nReviewing \"Bad Astronomy\" for the National Science Teachers Association, Deborah Teuscher, Director of Pike Planetarium, praised the work as \"interesting, accurate, and fun to read,\" recommending the book as a resource for science teachers, scientifically-interested lay persons, and high school and college students as a supplement to an astronomy unit.\n\n\"Publishers Weekly\" gave a generally favorable review, stating of the planned John Wiley & Sons \"Bad Science\" series that \"[i]f every entry in the series is as entertaining as Plait's, good science may have a fighting chance with the American public.\"\n\nAn April 2002 review for UniSci's \"Daily University Science News\" also praised \"Bad Astronomy\" as the \"ideal accompaniment for International Astronomy Day (April 20)\" and quoted the author, stating that it is \"dangerous to be ignorant about science. Our lives and our livelihoods depend on it.\"\n\nIn an October 2002 review for \"Sky & Telescope\", Bud Sadler praised \"Bad Astronomy\" for its humor, \"easily understood explanations\" and \"simple demonstrations\" to explain what he called \"the most egregious examples of ill-informed astronomy.\"\n\nPart I of \"Bad Astronomy\", \"Bad Astronomy Begins at Home,\" focuses on examples of astronomical misconceptions that are typically associated with the household or classroom, including the effect of the equinox on an egg's ability to balance upright without falling onto its side, the Coriolis effect's rumored effect on direction of whirlpools in household plumbing, and astronomical misunderstandings inherent in common English idioms, such as \"meteoric rise\" and \"dark side of the Moon.\" \"Idiom's Delight,\" the chapter dealing with scientific inaccuracies that appear in everyday expressions, such as the phrase \"light years ahead.\"\n\nPart II of the book, \"From the Earth to the Moon,\" focuses on Earth's orbit and atmosphere and the Moon, with particular emphasis on how photon scattering results in the sky appearing blue, the impact of axial tilt on seasons, the impact of the Moon's presence, and misconceptions regarding the \"Moon Size Illusion,\" explaining why and how the Moon appears larger when closer to the horizon.\n\nPart III, \"Skies at Night are Big and Bright,\" concentrates on the viewing of objects further away than the radius of the Moon's orbit around Earth, including the optical \"twinkle\" effect when viewing some stars, the brightness and color of stars, observation of meteors and asteroids, and using astronomical observations to study the beginning of the universe. Plait's chapter on meteors and asteroids delves into terms and distinctions and explains, for example, \"why small meteors are cold, not hot, when they hit the ground.\"\n\nPart IV, \"Artificial Intelligence,\" attempts to tackle various conspiracy theories and alternate worldviews, including the so-called Moon Landing Hoax, Young-Earth Creationism, Immanuel Velikovsky's book \"Worlds in Collision\" (which asserts that a relatively young Venus was once a part of Jupiter), extraterrestrial claims regarding unidentified flying objects (UFOs), and astrology. In \"Appalled at Apollo,\" the section devoted to Moon landing hoax conspiracy theories, Plait examines aspects of the hoax theory and compares its claims against basic laws of physics. Astronomical Society of the Pacific listed Chapter 17, \"Appalled at Apollo,\" on a list of resources stating it was \"good ammunition for debunking the notion that NASA never went to the Moon point by point.\" In the chapter \"Misidentified Flying Objects,\" Plait discusses various ways that cameras sometimes distort images, which Plait writes are often responsible for examples of evidence presented by extraterrestrial UFO proponents. A chapter devoted to astrology explores the topic, explaining \"why astrology doesn't work.\"\n\nPart V, \"Beam Me Up,\" explores additional topics, such as common misconceptions regarding the Hubble Space Telescope and its funding, star-naming companies, and astronomy myths and inaccuracies perpetuated by Hollywood, providing \"The Top-Ten Examples of Bad Astronomy in Major Motion Pictures.\"\n\n\n\n\n"}
{"id": "3935892", "url": "https://en.wikipedia.org/wiki?curid=3935892", "title": "Bay", "text": "Bay\n\nA bay is a recessed, coastal body of water that directly connects to a larger main body of water, such as an ocean, a lake, or another bay. A large bay is usually called a gulf, sea, sound, or bight. A cove is a type of smaller bay with a circular inlet and narrow entrance. A fjord is a particularly steep bay shaped by glacial activity.\n\nA bay can be the estuary of a river, such as the Chesapeake Bay, an estuary of the Susquehanna River. Bays may also be nested within each other; for example, James Bay is an arm of Hudson Bay in northeastern Canada. Some large bays, such as the Bay of Bengal and Hudson Bay, have varied marine geology.\n\nThe land surrounding a bay often reduces the strength of winds and blocks waves. Bays were significant in the history of human settlement because they provided safe places for fishing. Later they were important in the development of sea trade as the safe anchorage they provide encouraged their selection as ports.\n\nThe United Nations Convention on the Law of the Sea (UNCLOS), also called the Law of the Sea, defines a bay as a well-marked indentation whose penetration is in such proportion to the width of its mouth as to contain land-locked waters and constitute more than a mere curvature of the coast. An indentation shall not, however, be regarded as a bay unless its area is as large as, or larger than, that of the semi-circle whose diameter is a line drawn across the mouth of that indentation.\n\nThere are various ways in which bays can form. The largest bays have developed through plate tectonics. As the super-continent Pangaea broke up along curved and indented fault lines, the continents moved apart and left large bays; these include the Gulf of Guinea, the Gulf of Mexico, and the Bay of Bengal, which is the world's largest bay.\n\nBays also form through coastal erosion by rivers and glaciers. A bay formed by a glacier is a fjord. Rias are created by rivers and are characterised by more gradual slopes. Deposits of softer rocks erode more rapidly, forming bays, while harder rocks erode less quickly, leaving headlands.\n\n"}
{"id": "21904028", "url": "https://en.wikipedia.org/wiki?curid=21904028", "title": "Break Through (book)", "text": "Break Through (book)\n\nBreak Through: From the Death of Environmentalism to the Politics of Possibility, first published in October 2007, is a book written by Ted Nordhaus and Michael Shellenberger, both long-time environmental strategists. \"Break Through\" is an argument for a positive, \"post-environmental\" politics that abandons the traditional environmentalist focus on nature protection for a focus on creating a new sustainable economy. \n\nThe book is based on a controversial October 2004 essay by the same authors, \"The Death of Environmentalism: Global Warming Politics in a Post-Environmental World.\" The essay argues that environmentalism is conceptually and institutionally incapable of dealing with climate change and should \"die\" so that a new politics can be born. The essay was widely discussed among liberals and greens at \"Salon\", \"Grist\", and \"The New York Times\".\n\nAfter the failure of climate legislation in the U.S. Senate for the third time in June 2008, \"Time Magazine\" named Nordhaus and Shellenberger \"Heroes of the Environment,\" calling \"Break Through\" \"prescient\" for its prediction that climate policy should focus not on making fossil fuels expensive through regulation but rather on making clean energy cheap. The book's authors reiterated this argument in a September 2008 op-ed for the \"Los Angeles Times\", arguing for $30–$50bn in annual research subsidies for clean energy.\n\nIn early 2008 \"Break Through\" won the Center for Science Writing's Green Book Award, which comes with a $5000 prize for the author(s).\n\nThe first half of \"Break Through\" is a criticism of the green \"politics of limits.\" The book begins with the birth of environmentalism. Nordhaus and Shellenberger argue that environmentalism in the U.S. emerged from post-war affluence, which they argue is a clue to understanding how ecological movements might emerge in places like China and India. \nProgressive social reforms, from the Civil Rights Act to the Clean Water Act, tend to occur during times of prosperity and rising expectations—not immiseration and declining expectations. Both the environmental movement and the civil rights movement emerged as a consequence of rising prosperity. It was the middle-class, young, and educated black Americans who were on the forefront of the civil rights movement. Poor blacks were active, but the movement was overwhelmingly led by educated, middle-class intellectuals and community leaders (preachers prominent among them). This was also the case with the white supporters of the civil rights movement, who tended to be more highly educated and more affluent than the general American population. In short, the civil rights movement no more emerged because African Americans were suddenly denied their freedom than the environmental movement emerged because America suddenly started polluting.\nChapter two criticizes conservation efforts in Brazil, suggesting that nature protection cannot save the Amazon unless environmentalists provide an alternative way for Brazil to prosper. The authors criticize the environmental justice movement as focusing on low-priority pollution concerns in communities of color, narrowing the movement's focus instead of expanding it to include job creation and public health. And they fault climate activists for seeing climate change as a pollution problem like acid rain and the ozone hole instead of as an economic development and technological innovation challenge. The authors draw on science philosopher Thomas Kuhn to argue that environmentalists are stuck in a \"pollution paradigm\" when it comes to global warming.\nOne of Kuhn’s most famous examples was of the revolution led first by Copernicus and later by Galileo to overthrow the Earth-centered view of the solar system and replace it with our current sun-centered one. But in other instances, new paradigms leave part of the old paradigms intact, such as Einstein’s theory of relativity, which left Newton’s theory of gravity on Earth intact even as it revolutionized our understanding of mass and energy in the rest of the universe.\nPart II of Break Through, \"the politics of possibility,\" is an argument for environmentalism to die and become reborn as a new progressive politics, one capable of winning a new social contract for Americans, so that they are financially secure enough to be able to care about ecological challenges, and a $500 billion public-private investment in clean energy. The last half of the book makes the case for a new social contract for the post-industrial age, one capable of helping Americans overcome \"insecure affluence,\" whereby voters are both more materially wealthy but also more financially insecure than ever before. Nordhaus and Shellenberger say environmentalism should evolve from being a religion into being a church, and they see evangelical churches, with their capacity for providing belonging and fulfilment to their middle-class members, as models for a new \"pre-political\" institution for secular progressives. The authors argue for concrete policies, from \"Global Warming Preparedness,\" and a global clean energy investment strategy modelled on the creation of the European Union after World War II. \n\nIn the final chapter of\" Break Through,\" \"Greatness,\" argues that global warming will reshape national and international politics:\nClimate change and the political response to it is already defining a new fault line in the culture. On one side of that line will be a global NIMBYism that sees the planet as too fragile to support the hopes and dreams of seven billion humans. It will seek to establish and enforce the equivalent of an international caste system in which the poor of the developing world are consigned to energy poverty in perpetuity. This politics of limits will be anti-immigration, anti-globalization, and anti-growth. It will be zero-sum, fiscally conservative, and deficit-oriented. It will combine Malthusian environmentalism with Hobbesian conservatism.\n\"Break Through\" was criticized and praised by both left and right. \"Wired\" magazine wrote that \"Break Through\" \"could turn out to be the best thing to happen to environmentalism since Rachel Carson's \"Silent Spring\".\" \"The Wall Street Journal\" wrote, \"If heeded, Nordhaus and Shellenberger's call for an optimistic outlook -- embracing economic dynamism and creative potential -- will surely do more for the environment than any U.N. report or Nobel Prize.\". NPR's science correspondent Richard Harris listed \"Break Through\" on his \"recommended reading list\" for climate change.\n\nOther reviewers were harshly critical. Joseph Romm, a former US Department of Energy official now with the Center for American Progress, argued that \"Pollution limits are far, far more important than R&D for what really matters -- reducing greenhouse-gas emissions and driving clean technologies into the marketplace.\" (Romm also acknowledged that he had not read the book: \"I won't waste time reading their new instant bestseller, unhelpfully titled \"Break Through,\" and you shouldn't either.\") Reviewers for the \"San Francisco Chronicle\", the \"American Prospect\" and the \"Harvard Law Review\" argued that a critical reevaluation of green politics was unwarranted because global warming had become a high profile issue and the Democratic Congress was preparing to act.\n"}
{"id": "50539179", "url": "https://en.wikipedia.org/wiki?curid=50539179", "title": "Cereal growth staging scales", "text": "Cereal growth staging scales\n\nCereal growth staging scales attempt to objectively measure the growth of cereals.\n\nIn agronomy, the \"BBCH-scale for cereals\"' describes the phenological development of cereals using the BBCH-scale.\n\nThe phenological growth stages and BBCH-identification keys of cereals are:\nwith stages 21\n\nThe Feekes scale is a system to identify the growth and development of cereal crops introduced by the Dutch agronomists Willem Feekes (1907-1979) in 1941. This scale is more widely used in the United States than other similar and more descriptive scales such as the Zadoks scale or the BBCH scale. Like other scales of crop development, the Feekes scale is useful in planning management strategies that incorporate plant growth information for the use of pesticides and fertilizers to avoid damaging the crop and/or maximize crop yield.\n\nThe Zadoks scale is a cereal development scale proposed by the Dutch phytopathologist Jan C. Zadoks that is widely used in cereal research and agriculture.\nKnowing the stages of development of a crop is critical in many management decisions that growers make. They are represented on a scale from 10 to 92. For example, in some countries, nitrogen and herbicide applications must be completed during the tillering stage. In France, the recommendation for the first nitrogen application on wheat is 6 weeks before Z30, with the second application on Z30. Wheat growth regulators are typically applied at Z30. Disease control is most critical in the stem extension and heading stage (Z31, Z32, Z35), in particular as soon as the flag leaf is out (Z37). The crop is also more sensitive to heat or frost at some stages than others (for example, during the meiosis stage the crop is very sensitive to low temperature). Knowing the growth stage of the crop when checking for problems is essential for deciding which control measures should be followed.\n\nExamples of typical stages\n\n\n\n"}
{"id": "1037556", "url": "https://en.wikipedia.org/wiki?curid=1037556", "title": "Certified Health Physicist", "text": "Certified Health Physicist\n\nCertified Health Physicist is an official title granted by the American Board of Health Physics, the certification board for health physicists in the United States. A Certified Health Physicist is designated by the letters CHP or DABHP (Diplomat of the American Board of Health Physics) after his or her name.\n\nA certification by the ABHP is not a license to practice and does not confer any legal qualification to practice health physics. However, the certification is well respected and indicates a high level of achievement by those who obtain it. \n\nCertified Health Physicists are plenary or emeritus members of the American Academy of Health Physics (AAHP). In 2015, the AAHP web site listed over 1500 plenary and emeritus members.\n\nA person certified as a health physicist has a responsibility to uphold the professional integrity associated with the certification to promote the practice and science of radiation safety. It is expected that such a person will always give health physics information based on the highest standards of science and professional ethics. A certified individual has a responsibility to remain professionally active in the health physics field and remain technically competent in the scientific, technical and regulatory developments in the field.\n\nThe requirements for prospective candidates for certification are \n\n\n\n"}
{"id": "3622421", "url": "https://en.wikipedia.org/wiki?curid=3622421", "title": "Cratonic sequence", "text": "Cratonic sequence\n\nA cratonic sequence is a very large-scale lithostratographic sequence that covers a complete marine transgressive-regressive cycle across a craton. They are also known as \"megasequences\", \"stratigraphic sequences\", \"sloss sequence\", \"supersequence\" or simply \"sequences\". In plain English, it is the geological evidence of the sea level rising and then falling, thereby depositing layers of sediment onto an area of ancient rock called a craton. Places such as the Grand Canyon are a good visual example of this, apparent in the layers deposited over time. \n\nCratonic sequences were first proposed by Lawrence Sloss in 1963; each one represents a time when epeiric seas deposited sediments across the craton, while the upper and lower edges of the sequence are bounded by craton-wide unconformities eroded when the seas receded.\n\nThese sequences may in part represent eustatic or global change in sea level; however, when the proper names are used they usually refer to the North American continent. The most likely causes of these cycles is change in mid-ocean ridge volume, which is related to spreading rates. When Earth's mid-ocean ridges spread rapidly, the ridges tend to be longer than usual; also, the greater heat elevates the lithosphere over the ridges. This elevated lithosphere reduces ocean-basin volume and displaces water onto the continents; conversely, when spreading rates decline, the ridges subside, and the seas drain from the cratons. It is also possible that other mechanisms, such as dynamic topography related to mantle mass anomalies, and intraplate stress related to episodes of contractional and extensional tectonics, play a part by causing significant tectonic uplift and subsidence across the craton.\n\nThere have been six cratonic sequences since the beginning of the Cambrian Period. For North America, from oldest to youngest, they are the Sauk, Tippecanoe, Kaskaskia, Absaroka, Zuñi, and the Tejas. Attempts to identify equivalent cratonic sequences on other continents have met with only limited success, suggesting that eustasy is unlikely to be the sole responsible mechanism.\n\n\n"}
{"id": "2770818", "url": "https://en.wikipedia.org/wiki?curid=2770818", "title": "Descriptive botanical names", "text": "Descriptive botanical names\n\nDescriptive botanical names are scientific names of groups of plants that are irregular, not being derived systematically from the name of a type genus. They may describe some characteristics of the group in general or may be a name already in existence before regularised scientific nomenclature.\n\nDescriptive names can occur above or at the rank of family. There is only a single descriptive below the rank of family (the subfamily Papilionoideae).\n\nDescriptive names above the rank of family are governed by Article 16 of the International Code of Nomenclature for algae, fungi, and plants (ICN), which rules that a name above the rank of family may be either be ‘automatically typified’ (such as Magnoliophyta and Magnoliopsida from the type genus \"Magnolia\") or be descriptive.\n\nDescriptive names of this type may be used unchanged at different ranks (without modifying the suffix). These descriptive plant names are decreasing in importance, becoming less common than ‘automatically typified names’, but many are still in use, such as:\n\nMany of these descriptive names have a very long history, often preceding Carl Linnaeus. Some are Classical Latin common nouns in the nominative plural, meaning for instance ‘the plants’, ‘the seaweeds’, ‘the mosses’. Like all names above the rank of family, these names follow the Latin grammatical rules of nouns in the plural, and are written with an initial capital letter.\n\nArticle 18.5 of the ICN allows a descriptive name, of long usage, for the following eight families. For each of these families there also exists a name based on the name of an included genus (an alternative name that is also allowed, here in parentheses):\n\n\nSpecial provision has been made in Article 19.8 for the plant subfamily name Papilionoideae to be an alternative name of the subfamily Faboideae, because the family Papilionaceae had previously been conserved when many botanists considered it to be a separate family from Fabaceae (Leguminosae). This name is not based on any plant genus named \"Papilio\" (a butterfly), but is a descriptive name meaning that the plant has butterfly-like flowers.\n"}
{"id": "4455454", "url": "https://en.wikipedia.org/wiki?curid=4455454", "title": "Dmitri Zaikin", "text": "Dmitri Zaikin\n\nDmitri Alekseyevich Zayikin (April 29, 1932 – October 20, 2013) was a Russian cosmonaut trainer. \nZaikin was born in Yekaterinovka, Rostov Oblast, Russia. He graduated from \"Military Fighter Pilot School\", Armavir (Krasnodar Krai) and Frunze (now Bishkek), in 1955. He was selected for the cosmonaut training in 1960, as one of a group of the twenty Air Force pilots who would train as the first cosmonauts.\n\nZayikin was assigned as backup commander for Voskhod 2. He then went on to graduate from the \"Zhukovsky Military Engineering Academy\", Monino, in 1968. He left the space service on medical grounds (stomach ulcer) while training for Soyuz missions on October 25, 1969. He then became an instructor and lead engineer at the Yuri Gagarin Cosmonauts Training Center.\nHe left the space programme in 1982, and retired from active military duty in 1987.\n\nHe was married with two children.\n\nZaikin died of natural causes on October 20, 2013, as reported by Star City, the Yuri Gagarin Training Centre.\n"}
{"id": "3869283", "url": "https://en.wikipedia.org/wiki?curid=3869283", "title": "Dual inheritance theory", "text": "Dual inheritance theory\n\nDual inheritance theory (DIT), also known as gene–culture coevolution or biocultural evolution, was developed in the 1960s through early 1980s to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. Genes and culture continually interact in a feedback loop, changes in genes can lead to changes in culture which can then influence genetic selection, and vice versa. One of the theory's central claims is that culture evolves partly through a Darwinian selection process, which dual inheritance theorists often describe by analogy to genetic evolution.\n\n'Culture', in this context is defined as 'socially learned behavior', and 'social learning' is defined as copying behaviors observed in others or acquiring behaviors through being taught by others. Most of the modelling done in the field relies on the first dynamic (copying) though it can be extended to teaching. Social learning at its simplest involves blind copying of behaviors from a model (someone observed behaving), though it is also understood to have many potential biases, including success bias (copying from those who are perceived to be better off), status bias (copying from those with higher status), homophily (copying from those most like ourselves), conformist bias (disproportionately picking up behaviors that more people are performing), etc.. Understanding social learning is a system of pattern replication, and understanding that there are different rates of survival for different socially learned cultural variants, this sets up, by definition, an evolutionary structure: cultural evolution.\n\nBecause genetic evolution is relatively well understood, most of DIT examines cultural evolution and the interactions between cultural evolution and genetic evolution.\n\nDIT holds that genetic and cultural evolution interacted in the evolution of \"Homo sapiens\". DIT recognizes that the natural selection of genotypes is an important component of the evolution of human behavior and that cultural traits can be constrained by genetic imperatives. However, DIT also recognizes that genetic evolution has endowed the human species with a parallel evolutionary process of cultural evolution. DIT makes three main claims:\n\nThe human capacity to store and transmit culture arose from genetically evolved psychological mechanisms. This implies that at some point during the evolution of the human species a type of social learning leading to cumulative cultural evolution was evolutionarily advantageous.\n\nSocial learning processes give rise to cultural evolution. Cultural traits are transmitted differently from genetic traits and, therefore, result in different population-level effects on behavioral variation.\n\nCultural traits alter the social and physical environments under which genetic selection operates. For example, the cultural adoptions of agriculture and dairying have, in humans, caused genetic selection for the traits to digest starch and lactose, respectively. As another example, it is likely that once culture became adaptive, genetic selection caused a refinement of the cognitive architecture that stores and transmits cultural information. This refinement may have further influenced the way culture is stored and the biases that govern its transmission.\n\nDIT also predicts that, under certain situations, cultural evolution may select for traits that are genetically maladaptive. An example of this is the demographic transition, which describes the fall of birth rates within industrialized societies. Dual inheritance theorists hypothesize that the demographic transition may be a result of a prestige bias, where individuals that forgo reproduction to gain more influence in industrial societies are more likely to be chosen as cultural models.\n\nPeople have defined the word \"culture\" to describe a large set of different phenomena. A definition that sums up what is meant by \"culture\" in DIT is:\n\nThis view of culture emphasizes population thinking by focusing on the process by which culture is generated and maintained. It also views culture as a dynamic property of individuals, as opposed to a view of culture as a superorganic entity to which individuals must conform. This view's main advantage is that it connects individual-level processes to population-level outcomes.\n\nGenes affect cultural evolution via psychological predispositions on cultural learning. Genes encode much of the information needed to form the human brain. Genes constrain the brain's structure and, hence, the ability of the brain to acquire and store culture. Genes may also endow individuals with certain types of transmission bias (described below).\n\nCulture can profoundly influence gene frequencies in a population.\n\nLactase persistence\n\nOne of the best known examples is the prevalence of the genotype for adult lactose absorption in human populations, such as Northern Europeans and some African societies, with a long history of raising cattle for milk. Until around 7,500 years ago, lactase production stopped shortly after weaning, and in societies which did not develop dairying, such as East Asians and Amerindians, this is still true today. In areas with lactase persistence, it is believed that by domesticating animals, a source of milk became available while an adult and thus strong selection for lactase persistence could occur, in a Scandinavian population the estimated selection coefficient was 0.09-0.19. This implies that the cultural practice of raising cattle first for meat and later for milk led to selection for genetic traits for lactose digestion. Recently, analysis of natural selection on the human genome suggests that civilization has accelerated genetic change in humans over the past 10,000 years.\n\nFood processing\n\nCulture has driven changes to the human digestive systems making many digestive organs, like our teeth or stomach, smaller than expected for primates of a similar size, and has been attributed to one of the reasons why humans have such large brains compared to other great apes. This is due to food processing. Early examples of food processing include pounding, marinating and most notably cooking. Pounding meat breaks down the muscle fibres, hence taking away some of the job from the mouth, teeth and jaw. Marinating emulates the action of the stomach with high acid levels. Cooking partially breaks down food making it more easily digestible. Food enters the body effectively partly digested, and as such food processing reduces the work that the digestive system has to do. This means that there is selection for smaller digestive organs as the tissue is energetically expensive, those with smaller digestive organs can process their food but at a lower energetic cost than those with larger organs. Cooking is notable because the energy available from food increases when cooked and this also means less time is spent looking for food.\n\nHumans living on cooked diets spend only a fraction of their day chewing compared to other extant primates living on raw diets. American girls and boys spent on average 8 and 7 percent of their day chewing respectively, compared to chimpanzees who spend more than 6 hours a day chewing. This frees up time which can be used for hunting. A raw diet means hunting is constrained since time spent hunting is time not spent eating and chewing plant material, but cooking reduces the time required to get the day's energy requirements, allowing for more subsistence activities. Digestibility of cooked carbohydrates is approximately on average 30% higher than digestibility of non cooked carbohydrates. This increased energy intake, more free time and savings made on tissue used in the digestive system allowed for the selection of genes for larger brain size. \n\nDespite its benefits, brain tissue requires a large amount of calories, hence a main constraint in selection for larger brains is calorie intake. A greater calorie intake can support greater quantities of brain tissue. This is argued to explain why human brains can be much larger than other apes, since humans are the only ape to engage in food processing. The cooking of food has influenced genes to the extent that, research suggests, humans cannot live without cooking. A study on 513 individuals consuming long term raw diets found that as the percentage of their diet which was made up of raw food and/or the length they had been on a diet of raw food increased, their BMI decreased. This is despite access to many non thermal processing, like grinding, pounding or heating to 48 deg. c. (118 deg. F). With approximately 86 billion neurons in the human brain and 60–70 kg body mass, an exclusively raw diet close to that of what extant primates have would be not viable as, when modelled, it is argued that it would require an infeasible level of more than nine hours of feeding everyday. However, this is contested, with alternative modelling showing enough calories could be obtained within 5–6 hours per day. Some scientists and anthropologists point to evidence that brain size in the Homo lineage started to increase well before the advent of cooking due to increased consumption of meat and that basic food processing (slicing) accounts for the size reduction in organs related to chewing. Cornélio et al. argues that improving cooperative abilities and a varying of diet to more meat and seeds improved foraging and hunting efficiency. It is this that allowed for the brain expansion, independent of cooking which they argue came much later, a consequence from the complex cognition that developed. Yet this is still an example of a cultural shift in diet and the resulting genetic evolution. Further criticism comes from the controversy of the archaeological evidence available. Some claim there is a lack of evidence of fire control when brain sizes first started expanding. Wrangham argues that anatomical evidence around the time of the origin of \"Homo erectus\" (1.8 million years ago), indicates that the control of fire and hence cooking occurred. At this time, the largest reductions in tooth size in the entirety of human evolution occurred, indicating that softer foods became prevalent in the diet. Also at this time was a narrowing of the pelvis indicating a smaller gut and also there is evidence that there was a loss of the ability to climb which Wrangham argues indicates the control of fire, since sleeping on the ground needs fire to ward off predators. The proposed increases in brain size from food processing will have led to a greater mental capacity for further cultural innovation in food processing which will have increased digestive efficiency further providing more energy for further gains in brain size. This positive feedback loop is argued to have led to the rapid brain size increases seen in the \"Homo\" lineage.\n\nIn DIT, the evolution and maintenance of cultures is described by five major mechanisms: natural selection of cultural variants, random variation, cultural drift, guided variation and transmission bias.\n\nCultural differences among individuals can lead to differential survival of individuals. The patterns of this selective process depend on transmission biases and can result in behavior that is more adaptive to a given environment.\n\nRandom variation arises from errors in the learning, display or recall of cultural information, and is roughly analogous to the process of mutation in genetic evolution.\n\nCultural drift is a process roughly analogous to genetic drift in evolutionary biology. In cultural drift, the frequency of cultural traits in a population may be subject to random fluctuations due to chance variations in which traits are observed and transmitted (sometimes called \"sampling error\"). These fluctuations might cause cultural variants to disappear from a population. This effect should be especially strong in small populations. A model by Hahn and Bentley shows that cultural drift gives a reasonably good approximation to changes in the popularity of American baby names. Drift processes have also been suggested to explain changes in archaeological pottery and technology patent applications. Changes in the songs of song birds are also thought to arise from drift processes, where distinct dialects in different groups occur due to errors in songbird singing and acquisition by successive generations. Cultural drift is also observed in an early computer model of cultural evolution.\n\nCultural traits may be gained in a population through the process of individual learning. Once an individual learns a novel trait, it can be transmitted to other members of the population. The process of guided variation depends on an adaptive standard that determines what cultural variants are learned.\n\nUnderstanding the different ways that culture traits can be transmitted between individuals has been an important part of DIT research since the 1970s. Transmission biases occur when some cultural variants are favored over others during the process of cultural transmission. Boyd and Richerson (1985) defined and analytically modeled a number of possible transmission biases. The list of biases has been refined over the years, especially by Henrich and McElreath.\n\nContent biases result from situations where some aspect of a cultural variant's content makes them more likely to be adopted. Content biases can result from genetic preferences, preferences determined by existing cultural traits, or a combination of the two. For example, food preferences can result from genetic preferences for sugary or fatty foods and socially-learned eating practices and taboos. Content biases are sometimes called \"direct biases.\"\n\nContext biases result from individuals using clues about the social structure of their population to determine what cultural variants to adopt. This determination is made without reference to the content of the variant. There are two major categories of context biases: model-based biases, and frequency-dependent biases.\n\nModel-based biases result when an individual is biased to choose a particular \"cultural model\" to imitate. There are four major categories of model-based biases: prestige bias, skill bias, success bias, and similarity bias. A \"prestige bias\" results when individuals are more likely to imitate cultural models that are seen as having more prestige. A measure of prestige could be the amount of deference shown to a potential cultural model by other individuals. A \"skill bias\" results when individuals can directly observe different cultural models performing a learned skill and are more likely to imitate cultural models that perform better at the specific skill. A \"success bias\" results from individuals preferentially imitating cultural models that they determine are most generally successful (as opposed to successful at a specific skill as in the skill bias.) A \"similarity bias\" results when individuals are more likely to imitate cultural models that are perceived as being similar to the individual based on specific traits.\n\nFrequency-dependent biases result when an individual is biased to choose particular cultural variants based on their perceived frequency in the population. The most explored frequency-dependent bias is the \"conformity bias.\" Conformity biases result when individuals attempt to copy the mean or the mode cultural variant in the population. Another possible frequency dependent bias is the \"rarity bias.\" The rarity bias results when individuals preferentially choose cultural variants that are less common in the population. The rarity bias is also sometimes called a \"nonconformist\" or \"anti-conformist\" bias.\n\nIn DIT, the evolution of culture is dependent on the evolution of social learning. Analytic models show that social learning becomes evolutionarily beneficial when the environment changes with enough frequency that genetic inheritance can not track the changes, but not fast enough that individual learning is more efficient. For environments that have very little variability, social learning is not needed since genes can adapt fast enough to the changes that occur, and innate behaviour is able to deal with the constant environment. In fast changing environments cultural learning would not be useful because what the previous generation knew is now outdated and will provide no benefit in the changed environment, and hence individual learning is more beneficial. It is only in the moderately changing environment where cultural learning becomes useful since each generation shares a mostly similar environment but genes have insufficient time to change to changes in the environment. While other species have social learning, and thus some level of culture, only humans, some birds and chimpanzees are known to have cumulative culture. Boyd and Richerson argue that the evolution of cumulative culture depends on observational learning and is uncommon in other species because it is ineffective when it is rare in a population. They propose that the environmental changes occurring in the Pleistocene may have provided the right environmental conditions. Michael Tomasello argues that cumulative cultural evolution results from a ratchet effect that began when humans developed the cognitive architecture to understand others as mental agents. Furthermore, Tomasello proposed in the 80s that there are some disparities between the observational learning mechanisms found in humans and great apes - which go some way to explain the observable difference between great ape traditions and human types of culture (see Emulation (observational learning)).\n\nAlthough group selection is commonly thought to be nonexistent or unimportant in genetic evolution, DIT predicts that, due to the nature of cultural inheritance, it may be an important force in cultural evolution. Group selection occurs in cultural evolution because conformist biases make it difficult for novel cultural traits to spread through a population (see above section on transmission biases). Conformist bias also helps maintain variation between groups. These two properties, rare in genetic transmission, are necessary for group selection to operate. Based on an earlier model by Cavalli-Sforza and Feldman, Boyd and Richerson show that conformist biases are almost inevitable when traits spread through social learning, implying that group selection is common in cultural evolution. Analysis of small groups in New Guinea imply that cultural group selection might be a good explanation for slowly changing aspects of social structure, but not for rapidly changing fads. The ability of cultural evolution to maintain intergroup diversity is what allows for the study of cultural phylogenetics.\n\nThe idea that human cultures undergo a similar evolutionary process as genetic evolution goes back at least to Darwin In the 1960s, Donald T. Campbell published some of the first theoretical work that adapted principles of evolutionary theory to the evolution of cultures. In 1976, two developments in cultural evolutionary theory set the stage for DIT. In that year Richard Dawkins's \"The Selfish Gene\" introduced ideas of cultural evolution to a popular audience. Although one of the best-selling science books of all time, because of its lack of mathematical rigor, it had little effect on the development of DIT. Also in 1976, geneticists Marcus Feldman and Luigi Luca Cavalli-Sforza published the first dynamic models of gene–culture coevolution. These models were to form the basis for subsequent work on DIT, heralded by the publication of three seminal books in the 1980s.\n\nThe first was Charles Lumsden and E.O. Wilson's \"Genes, Mind and Culture\". This book outlined a series of mathematical models of how genetic evolution might favor the selection of cultural traits and how cultural traits might, in turn, affect the speed of genetic evolution. While it was the first book published describing how genes and culture might coevolve, it had relatively little effect on the further development of DIT. Some critics felt that their models depended too heavily on genetic mechanisms at the expense of cultural mechanisms. Controversy surrounding Wilson's sociobiological theories may also have decreased the lasting effect of this book.\n\nThe second 1981 book was Cavalli-Sforza and Feldman's \"Cultural Transmission and Evolution: A Quantitative Approach\". Borrowing heavily from population genetics and epidemiology, this book built a mathematical theory concerning the spread of cultural traits. It describes the evolutionary implications of vertical transmission, passing cultural traits from parents to offspring; oblique transmission, passing cultural traits from any member of an older generation to a younger generation; and horizontal transmission, passing traits between members of the same population.\n\nThe next significant DIT publication was Robert Boyd and Peter Richerson's 1985 \"Culture and the Evolutionary Process\". This book presents the now-standard mathematical models of the evolution of social learning under different environmental conditions, the population effects of social learning, various forces of selection on cultural learning rules, different forms of biased transmission and their population-level effects, and conflicts between cultural and genetic evolution. The book's conclusion also outlined areas for future research that are still relevant today.\n\nIn their 1985 book, Boyd and Richerson outlined an agenda for future DIT research. This agenda, outlined below, called for the development of both theoretical models and empirical research. DIT has since built a rich tradition of theoretical models over the past two decades. However, there has not been a comparable level of empirical work.\n\nIn a 2006 interview Harvard biologist E. O. Wilson expressed disappointment at the little attention afforded to DIT:\n\nKevin Laland and Gillian Brown attribute this lack of attention to DIT's heavy reliance on formal modeling.\n\nEconomist Herbert Gintis disagrees with this critique, citing empirical work as well as more recent work using techniques from behavioral economics. These behavioral economic techniques have been adapted to test predictions of cultural evolutionary models in laboratory settings as well as studying differences in cooperation in fifteen small-scale societies in the field.\n\nSince one of the goals of DIT is to explain the distribution of human cultural traits, ethnographic and ethnologic techniques may also be useful for testing hypothesis stemming from DIT. Although findings from traditional ethnologic studies have been used to buttress DIT arguments, thus far there have been little ethnographic fieldwork designed to explicitly test these hypotheses.\n\nHerb Gintis has named DIT one of the two major conceptual theories with potential for unifying the behavioral sciences, including economics, biology, anthropology, sociology, psychology and political science. Because it addresses both the genetic and cultural components of human inheritance, Gintis sees DIT models as providing the best explanations for the ultimate cause of human behavior and the best paradigm for integrating those disciplines with evolutionary theory. In a review of competing evolutionary perspectives on human behavior, Laland and Brown see DIT as the best candidate for uniting the other evolutionary perspectives under one theoretical umbrella.\n\nTwo major topics of study in both sociology and cultural anthropology are human cultures and cultural variation.\nHowever, Dual Inheritance theorists charge that both disciplines too often treat culture as a static superorganic entity that dictates human behavior. Cultures are defined by a suite of common traits shared by a large group of people. DIT theorists argue that this doesn't sufficiently explain variation in cultural traits at the individual level. By contrast, DIT models human culture at the individual level and views culture as the result of a dynamic evolutionary process at the population level.\n\nEvolutionary psychologists study the evolved architecture of the human mind. They see it as composed of many different programs that process information, each with assumptions and procedures that were specialized by natural selection to solve a different adaptive problem faced by our hunter-gatherer ancestors (e.g., choosing mates, hunting, avoiding predators, cooperating, using aggression). These evolved programs contain content-rich assumptions about how the world and other people work. As ideas are passed from mind to mind, they are changed by these evolved inference systems (much like messages get changed in a game of telephone). But the changes are not random. Evolved programs add and subtract information, reshaping the ideas in ways that make them more \"intuitive\", more memorable, and more attention-grabbing. In other words, \"memes\" (ideas) are not like genes. Genes are copied faithfully as they are replicated, but ideas are not. It’s not just that ideas mutate every once in awhile, like genes do. Ideas are transformed every time they are passed from mind to mind, because the sender's message is being interpreted by evolved inference systems in the receiver. There is no necessary contradiction between evolutionary psychology and DIT, but evolutionary psychologists argue that the psychology implicit in many DIT models is too simple; evolved programs have a rich inferential structure not captured by the idea of a \"content bias\". They also argue that some of the phenomena DIT models attribute to cultural evolution are cases of \"evoked culture\"—situations in which different evolved programs are activated in different places, in response to cues in the environment.\n\nHuman sociobiologists try to understand how maximizing genetic fitness, in either the modern era or past environments, can explain human behavior. When faced with a trait that seems maladaptive, some sociobiologists try to determine how the trait actually increases genetic fitness (maybe through kin selection or by speculating about early evolutionary environments). Dual inheritance theorists, in contrast, will consider a variety of genetic and cultural processes in addition to natural selection on genes.\n\nHuman behavioral ecology (HBE) and DIT have a similar relationship to what ecology and evolutionary biology have in the biological sciences. HBE is more concerned about ecological process and DIT more focused on historical process. One difference is that human behavioral ecologists often assume that culture is a system that produces the most adaptive outcome in a given environment. This implies that similar behavioral traditions should be found in similar environments. However, this is not always the case. A study of African cultures showed that cultural history was a better predictor of cultural traits than local ecological conditions.\n\nMemetics, which comes from the meme idea described in Dawkins's \"The Selfish Gene\", is similar to DIT in that it treats culture as an evolutionary process that is distinct from genetic transmission. However, there are some philosophical differences between memetics and DIT. One difference is that memetics' focus is on the selection potential of discrete replicators (memes), where DIT allows for transmission of both non-replicators and non-discrete cultural variants. DIT does not assume that replicators are necessary for cumulative adaptive evolution. DIT also more strongly emphasizes the role of genetic inheritance in shaping the capacity for cultural evolution. But perhaps the biggest difference is a difference in academic lineage. Memetics as a label is more influential in popular culture than in academia. Critics of memetics argue that it is lacking in empirical support or is conceptually ill-founded, and question whether there is hope for the memetic research program succeeding. Proponents point out that many cultural traits are discrete, and that many existing models of cultural inheritance assume discrete cultural units, and hence involve memes.\n\nSerious criticisms have been levelled against DIT. Use of the term ‘dual inheritance’ to refer to not just traits that are transmitted by way of a self-assembly code (as in genetic evolution) but also traits that are \"not\" transmitted by way of a self-assembly code (as in cultural evolution) is misleading, because this second use does not capture the algorithmic structure that makes an inheritance system require a particular kind of mathematical framework. The population genetics framework was designed to solve the problem of how does evolution occur--i.e., how are fit traits preserved in a lineage--in a system wherein acquired change is discarded at the end of each generation. Darwin noticed that there are two kinds of traits: acquired traits (e.g., a tattoo, or knowledge of the layout of a particular city) which are discarded, while inherited traits (e.g., blood type) are preserved. His ingenious solution was to develop a population level explanation, and show that evolution was due to differential replication of heritable variation in response to selection. \nWe now know that the reason for the distinction between these two kinds of traits is that some traits (inherited traits) are encoded in genes which collectively constitute self-assembly code and are transmitted to offspring, while all other traits (acquired traits) are shed at the end of a generation with the deaths of those who bore them.\nThus, the reason that horizontal transmission of ideas is algorithmically dissimilar to vertical transmission (reproduction) in genetic evolution is that it does not provide a means of preserving fit traits in a system wherein those traits would otherwise be lost from the lineage. \nProponents of DIT argue that 1) even genetic evolution uses non-vertical transmission through the environmental alteration of the genome during life by acquired circumstance: epigenetics, and 2) genetic evolution is also affected by direct horizontal transmission between separate species of plants and strains of bacteria: horizontal gene transfer. \nHowever, these arguments are irrelevant to the issue of whether a Darwinian (or selectionist, or population genetics) mathematical framework is appropriate to the description of cultural evolution, since these aspects of biological evolution are themselves not accommodated by such a framework. The point is that although it is not essential that ‘inherited traits’ be transmitted by way of \"genes\" (necessarily) for a population genetics framework to be applicable, but they need to be transmitted by way of a self-assembly code, or some other such mechanism that may exist out there in the universe that does the same thing: preserving traits that would otherwise be lost from a lineage due to the death of individuals.\n\nThe above criticism of DIT arises due to the choice of Darwinian selection as an explanatory framework for culture. Cultural evolution does not possess the algorithmic structure of a process that can be modeled in a Darwinian framework as characterized by John von Neumann and used by John Holland to design the genetic algorithm. Forcing culture into a Darwinian framework gives a distorted picture of the process for several reasons. First, Darwinian selection only works as an explanatory framework when variation is randomly generated. To the extent that transmission biases are operative in culture, they mitigate the effect of Darwinian change, i.e. change in the distribution of variants over generations of exposure to selective pressures. Second, since acquired change can accumulate orders of magnitude faster than inherited change, if it is not getting regularly discarded each generation, it quickly overwhelms the population-level mechanism of change identified by Darwin; it ‘swamps the phylogenetic signal’. \nDIT proponents reply that their theory includes a very important role for decision-making forces. As a point of history, Darwin had a rather sophisticated theory of human cultural evolution that depended on natural selection \"to a subordinate degree\" compared to \"laws, customs, and traditions\" supported by public opinion. \nCritics do not see the relevance of this reply to the point they are making. When critics claim that DIT is too \"Darwinian\" they are claiming that culture does not have the algorithmic structure of the kind of process that the formalisms of population genetics were developed to capture.\n\nA related problem stems from the reconstructive manner in which ideas are encoded and retrieved from memory, and the fact that ideas are interpreted in terms of existing conceptual structure and creatively adapted to their bearer's needs, views, and tastes prior to cultural transmission. This means that what biologists call 'acquired change' is ubiquitous in culture, and the population genetics framework was specifically developed to describe the evolution of inherited change in a system where acquired change is regularly discarded from the lineage. Proponents argue \"But if this criticism was valid then it would be comparatively much easier to argue an unpopular or incorrect concepts than it actually is.\" Critics do not know what they mean by this. Proponents also claim, \"In addition, nothing about DIT runs counter to the idea that an internally selective process (some would call creativity) also determines the fitness of ideas received and sent. In fact this decision making is a large part of the territory embraced by DIT proponents but is poorly understood due to limitations in neurobiology (for more information see Neural Darwinism).\" Critics, however, do not view creativity as an \"internally selective process\", and their criticism has nothing to do with whether creativity \"determines the fitness of ideas received and sent.\" (They also point out that there is a vast psychological literature on decision making; it would only appear to be \"poorly understood\" to someone whose only source for papers on decision making is the literature on Neural Darwinism.) The point critics are making in regard to creativity is that creativity introduces acquired change which is not handled by a selectionist, or Darwinian, or population genetics (it doesn't matter what you call it) type of mathematical framework.\n\nRelated criticisms of the effort to frame culture in Darwinian terms have been leveled by Richard Lewontin, Niles Eldredge, and Stuart Kauffman.\n\n\n\n\n\n\n"}
{"id": "44689684", "url": "https://en.wikipedia.org/wiki?curid=44689684", "title": "Energy informatics", "text": "Energy informatics\n\nEnergy Informatics is founded on flow networks that are the major suppliers and consumers of energy. Their efficiency can be improved by collecting and analyzing information.\nEnergy informatics is a research field covering the use of information and communication technology to address energy challenges. Methods used for \"smart\" implementations often combine sensors with artificial intelligence and machine learning.\n\nThe field among other consider application areas within:\n\n\n"}
{"id": "5210185", "url": "https://en.wikipedia.org/wiki?curid=5210185", "title": "Engineering change order", "text": "Engineering change order\n\nEngineering change orders (ECO) are used for changes in components, assemblies, or documents such as processes and work instructions. They may also be used for changes in specifications. Lastly, it can be \"a modification that will have an effect on a manufactured product or manufacturing process.\"\n\nECOs are also called an \"engineering change note\", engineering change notice (ECN), or just an engineering change (EC). \n\nIn a typical system development cycle, the specification or the implementation is likely to change during engineering development or during integration of the system elements. These last-minute design changes are commonly referred to as engineering change orders (ECOs) and affect the functionality of a design after it has been wholly or partially completed. ECOs can compensate for design errors found during debug or changes that are made to the design specification to compensate for design problems in other areas of the system design. When improperly managed, engineering change orders can vastly increase costs. \n\nIn product development the need for change is caused by:\n\n\"[A] document approved by the design activity that describes and authorizes the implementation of an engineering change to the product and its approved configuration documentation\".\n\nAn ECO must contain at least this information:\n\nIn chip design, ECO is the process of inserting a logic change directly into the netlist after it has already been processed by an automatic tool. Before the chip masks are made, ECOs are usually done to save time, by avoiding the need for full ASIC logic synthesis, technology mapping, place, route, layout extraction, and timing verification. EDA tools are often built with incremental modes of operation to facilitate this type of ECO.\n\nAfter masks have been made, ECOs may be done to save money. If a change can be implemented by modifying only a few of the layers (typically metal) then the cost is much less than it would be if the design was re-built from scratch. This is because starting the process from the beginning will almost always require new photomasks for all layers, and each of the 20 or so masks in a modern semiconductor fabrication process is quite expensive. A change implemented by modifying only a few layers is typically called a \"metal-mask\" ECO or a \"post-mask\" ECO. Designers often sprinkle a design with unused logic gates, and EDA tools have specialized commands, to make this process easier.\n\nOne of the most common ECOs in ASIC design is the gate-level netlist ECO. In this flow, engineers manually (and often tediously) hand-edit the gate-level netlist, instead of re-running logic synthesis. The netlist files have to be searched for the logic affected by the change, the files need to be edited to implement the changes up and down the hierarchy, and the changes need to be tracked and verified to make sure exactly what needs to change gets changed and nothing more. This is a very time and resource-intensive process that is easily subject to errors. Therefore formal equivalence checking is normally used after ECOs to ensure the revised implementation matches the revised specification.\n\nWith time-to-market pressures and rising mask costs in the semiconductor industry, several electronic design automation (EDA) companies are beginning to bring more automation into the ECO implementation process. Most popular place and route products have some level of built-in ECO routing to help with implementing physical-level ECOs. Cadence Design Systems has recently announced a product called \"conformal ECO designer\", that automates the creation of Functional ECOs, usually the most tedious process in implementing an ECO. It uses formal equivalence checking and logic synthesis techniques to produce a gate-level ECO netlist based on the changed RTL. Synopsys in the past had a product called \"ECO compiler\" that is now defunct. Synopsys now has \"primetime-ECO\" for dealing with ECOs. Tweaker-F1 & Tweaker-T1 have also come into the limelight in the recent DAC-2012 for their ECO algorithms.\n\nThe telecommunications industry has a formal process that takes elements of the ECO and other considerations and combines them into the \"product change notice\" (PCN). After telecommunications products have been generally available and/or in service for a period of time, it often becomes necessary for suppliers to introduce changes to those products. As a result of implementing these changes – regardless of who performs the actual work – the telecommunications carriers are significantly impacted with respect to labor and resources, etc. Thus, it is imperative that changes to these products are accurately reported and tracked through completion, according to the needs and requirements of the carriers.\n\nThe term \"product change\" includes changes to hardware, software, and firmware that occur over the entire life of a product. Product changes include those considered reportable and non-reportable. These changes may be applied by a supplier, a customer, or a contractor retained by the customer, depending on negotiated agreements. Fundamentally, the customer's goal is to ensure there is a process by which there is accurate and efficient tracking and reporting of changes to products. \n\nChanges are considered reportable when they affect the performance or life span of a product. Such changes include any that affect the form, fit, function, or the product technical specification (i.e., documentation) of the product. The desire for supplier or customer traceability may result in a reportable change.\n\nThe entire PCN process is documented in GR-209, Issue 6, \"Generic Requirements for Product Change Notices (PCNs)\".\n\n"}
{"id": "2545846", "url": "https://en.wikipedia.org/wiki?curid=2545846", "title": "Enstrophy", "text": "Enstrophy\n\nIn fluid dynamics, the enstrophy can be interpreted as another type of potential density; or, more concretely, the quantity directly related to the kinetic energy in the flow model that corresponds to dissipation effects in the fluid. It is particularly useful in the study of turbulent flows, and is often identified in the study of thrusters as well as the field of combustion theory.\n\nThe enstrophy can be described as the integral of the square of the vorticity \"ω\",\n\nor, in terms of the flow velocity,\n\nHere, since the curl gives a scalar field in two dimensions (vortex) corresponding to the vector-valued velocity solving in the incompressible Navier–Stokes equations, we can integrate its square over a surface \"S\" to retrieve a continuous linear operator on the space of possible velocity fields, known as a \"current\". This equation is however somewhat misleading. Here we have chosen a simplified version of the enstrophy derived from the incompressibility condition, which is equivalent to vanishing divergence of the velocity field,\n\nMore generally, when not restricted to the incompressible condition, or to two spatial dimensions, the enstrophy may be computed by:\n\nwhere \n\nis the Frobenius norm of the gradient of the velocity field u.\n\n"}
{"id": "2094725", "url": "https://en.wikipedia.org/wiki?curid=2094725", "title": "Explorer 32", "text": "Explorer 32\n\nExplorer 32, also known as Atmosphere Explorer-B (AE-B), was a satellite launched by the United States to study the Earth's upper atmosphere. It was launched from Cape Canaveral on a Delta-C1 rocket, on May 25, 1966. It was the second of five Atmosphere Explorers, the first being Explorer 17.\n\nExplorer 32 was a stainless steel, vacuum-sealed sphere, in diameter. It carried one ion and two neutral mass spectrometers, three magnetron density gauges, and two electrostatic probes. It used a tape recorder to save data that was acquired when the satellite was not in range of one of the 13 ground stations. It was powered by silver-zinc batteries and a solar cell array.\n\nThe satellite had an operational life of 10 months. The two neutral mass spectrometers failed a few days after launch, but the remaining instruments operated throughout most of the satellite's lifetime. Explorer 32 suffered a depressurization which led to battery failure, at which point it ceased functioning.\n\nExplorer 32 was designed to directly measure the temperature, composition, density, and pressure of the upper atmosphere.\n\nThe \"ion mass spectrometers\" measured the concentrations of different types of ions in the topside ionosphere, principally atomic hydrogen, helium, nitrogen, and oxygen. The concentrations were recorded as a function of time, location, and solar and geomagnetic activity. The satellite was able to perform a global study of the diurnal variation of the atmosphere during nearly two complete diurnal cycles, since the orbit plane precessed one revolution each 5.5 months. The data from the ion mass spectrometers allowed for studies of: (1) the diurnal and seasonal variation of atmospheric ion composition, (2) the effect of atmospheric winds on the atomic hydrogen-atomic oxygen ion transition level, (3) the density and temporal variation of thermospheric atomic hydrogen, and (4) the altitude variation of ion composition in the midlatitude trough region.\n\nThe three \"magnetron density gauges\" measured the density of the neutral atmosphere as a function of altitude, time, latitude, and solar and geomagnetic activity.\n\nThe \"electron temperature and density\" instrument measure the distribution of electron temperatures and densities using a swept voltage electron probe.\n\nThe two \"neutral particle magnetic mass spectrometers\" were intended to measure the composition of the neutral atmosphere at altitudes between . One spectrometer failed after 4 days, and the other failed after 7 days in orbit.\n\nThe \"satellite drag atmospheric density\" experiment measured the density of the upper atmosphere as a function of altitude, latitude, season, and solar activity. The experiment was possible due to the symmetrical shape of Explorer 32. When the satellite was near perigee, it was observed by networks of ground-based Baker-Nunn cameras, as well as being tracked by radio and radar. Sequences of these observations were used to deduce the density of the atmosphere that the satellite was passing through.\n\n\n"}
{"id": "57263837", "url": "https://en.wikipedia.org/wiki?curid=57263837", "title": "Frederic William Harmer", "text": "Frederic William Harmer\n\nFrederic William Harmer FGS, FRMetS (24 April 1835 – 24 April 1923) was an English amateur geologist, palaeontologist, and naturalist.\n\nHe was born in Norwich and was educated at Norwich Grammar School.\n\nHarmer was the mayor of Norwich in 1887–1888 and served there as an alderman from 1880 to 1902. After about a decade of inactivity in geological work, he presented in 1895 at the meeting of the British Association at Ipswich two important papers on the Coralline and Red Crags. From 1895 until his death, he actively pursued field work in geology. He was awarded the Murchison Medal in 1902.\n\nHe made extensive investigations of the Pliocene and Pleistocene deposits in England's eastern and midland counties, as well as those deposits in Belgium and the Netherlands. The \"Société géologique de Belgique\" elected him an honorary member. The University of Cambridge conferred upon him an honorary M.A.\n\nHe married Mary Young Lyon in 1860. The marriage produced several children, among whom were the surgeon William Douglas Harmer and the zoologist Sir Sidney Frederic Harmer.\n"}
{"id": "2874850", "url": "https://en.wikipedia.org/wiki?curid=2874850", "title": "Georg Dionysius Ehret", "text": "Georg Dionysius Ehret\n\nGeorg Dionysius Ehret (30 January 1708 – 9 September 1770) was a botanist and entomologist, and is best known for his botanical illustrations.\n\nEhret was born in Germany to Ferdinand Christian Ehret, a gardener and competent draughtsman, and Anna Maria Ehret. Beginning his working life as a gardener's apprentice near Heidelberg, he became one of the most influential European botanical artists of all time. His first illustrations were in collaboration with Carl Linnaeus and George Clifford in 1735-1736. Clifford, a wealthy Dutch banker and governor of the Dutch East India Company was a keen botanist with a large herbarium. He had the income to attract the talents of botanists such as Linnaeus and artists like Ehret. Together at the Clifford estate, Hartecamp, which is located south of Haarlem in Heemstede near Bennebroek, they produced \"Hortus Cliffortianus\" in 1738, a masterpiece of early botanical literature.\n\nAs a result of exploitation by Johann Wilhelm Weinmann, Ehret finished only 500 plates of a 1,000 plate commission and moved to England where he illustrated many of the more spectacular plants that were in cultivation. His original art work may be found at the Natural History Museum in London, the Royal Botanic Gardens, Kew, The Royal Society, London, the Lindley Library at the Royal Horticultural Society, the Victoria and Albert Museum, at the University Library of Erlangen, and the Hunt Institute for Botanical Documentation in Pittsburgh, PA.\n\nThe genus \"Ehretia\" was named in his honour.\n\n\nA memoir of Georg Dionysius Ehret, written by himself \"Proceedings of the Linnean Society\", London, November 1984 to June 1985.\n\n\n"}
{"id": "18332975", "url": "https://en.wikipedia.org/wiki?curid=18332975", "title": "Giant resonance", "text": "Giant resonance\n\nGiant resonance is a high-frequency collective excitation of atomic nuclei, as a property of many-body quantum systems. In the macroscopic interpretation of such an excitation in terms of an oscillation, the most prominent giant resonance is a collective oscillation of all protons against all neutrons in a nucleus.\n\nIn 1947, G. C. Baldwin and G. S. Klaiber observed the giant dipole resonance (GDR) in photonuclear reactions, and in 1972 the giant quadrupole resonance (GQR) was discovered, and in 1977 the giant monopole resonance (GMR) was discovered in medium and heavy nuclei.\n\nGiant dipole resonances may result in a number of de-excitation events, such as nuclear fission, emission of neutrons or gamma rays, or combinations of these.\n\nGiant dipole resonances can be caused by any mechanism that imparts enough energy to the nucleus. Classical causes are irradiation with gamma rays at energies from 7 to 40 MeV, which couple to nuclei and either cause or increase the dipole moment of the nucleus by adding energy that separates charges in the nucleus. The process is the inverse of gamma decay, but the energies involved are typically much larger, and the dipole moments induced are larger than occur in the excited nuclear states that cause the average gamma decay. \n\nHigh energy electrons of >50 MeV may cause the same phenomenon, by coupling to the nucleus via a \"virtual gamma photon\", in a nuclear reaction that is the inverse (i.e., reverse) of internal conversion decay.\n\n\n\n"}
{"id": "3309479", "url": "https://en.wikipedia.org/wiki?curid=3309479", "title": "Helioscope", "text": "Helioscope\n\nA helioscope is an instrument used in observing the sun and sunspots. \nThe helioscope was first used by Benedetto Castelli (1578-1643) and refined by Galileo (1564–1642). The method involves projecting an image of the sun onto a white sheet of paper suspended in a darkened room with the use of a telescope.\n\nThe first \"machina helioscopica\" or helioscope was designed by Christoph Scheiner (1575 –1650) to assist his sunspot observations.\n\n"}
{"id": "13600", "url": "https://en.wikipedia.org/wiki?curid=13600", "title": "Hipparchus", "text": "Hipparchus\n\nHipparchus of Nicaea (; , \"Hipparkhos\";  ) was a Greek astronomer, geographer, and mathematician. He is considered the founder of trigonometry but is most famous for his incidental discovery of precession of the equinoxes.\n\nHipparchus was born in Nicaea, Bithynia (now İznik, Turkey), and probably died on the island of Rhodes. He is known to have been a working astronomer at least from 162 to 127 . Hipparchus is considered the greatest ancient astronomical observer and, by some, the greatest overall astronomer of antiquity. He was the first whose quantitative and accurate models for the motion of the Sun and Moon survive. For this he certainly made use of the observations and perhaps the mathematical techniques accumulated over centuries by the Babylonians and by Meton of Athens (5th century ), Timocharis, Aristyllus, Aristarchus of Samos and Eratosthenes, among others. He developed trigonometry and constructed trigonometric tables, and he solved several problems of spherical trigonometry. With his solar and lunar theories and his trigonometry, he may have been the first to develop a reliable method to predict solar eclipses. His other reputed achievements include the discovery and measurement of Earth's precession, the compilation of the first comprehensive star catalog of the western world, and possibly the invention of the astrolabe, also of the armillary sphere, which he used during the creation of much of the star catalogue.\n\nRelatively little of Hipparchus's direct work survives into modern times. Although he wrote at least fourteen books, only his commentary on the popular astronomical poem by Aratus was preserved by later copyists. Most of what is known about Hipparchus comes from Strabo's \"Geography\" and Pliny's \"Natural History\" in the 1st century; Ptolemy's 2nd-century \"Almagest\"; and additional references to him in the 4th century by Pappus of Alexandria and Theon of Alexandria in their commentaries on the \"Almagest\".\n\nThere is a strong tradition that Hipparchus was born in Nicaea (Greek \"Νίκαια\"), in the ancient district of Bithynia (modern-day Iznik in province Bursa), in what today is the country Turkey.\n\nThe exact dates of his life are not known, but Ptolemy attributes to him astronomical observations in the period from 147–127 , and some of these are stated as made in Rhodes; earlier observations since 162  might also have been made by him. His birth date ( ) was calculated by Delambre based on clues in his work. Hipparchus must have lived some time after 127  because he analyzed and published his observations from that year. Hipparchus obtained information from Alexandria as well as Babylon, but it is not known when or if he visited these places. He is believed to have died on the island of Rhodes, where he seems to have spent most of his later life.\n\nIt is not known what Hipparchus's economic means were nor how he supported his scientific activities. His appearance is likewise unknown: there are no contemporary portraits. In the 2nd and 3rd centuries coins were made in his honour in Bithynia that bear his name and show him with a globe; this supports the tradition that he was born there.\n\nHipparchus is thought to be the first to calculate a heliocentric system, but he abandoned his work because the calculations showed the orbits were not perfectly circular as believed to be mandatory by the science of the time. As an astronomer of antiquity, his influence, supported by ideas from Aristotle, held sway for nearly 2000 years, until the heliocentric model of Copernicus.\n\nHipparchus's only preserved work is \"Τῶν Ἀράτου καὶ Εὐδόξου φαινομένων ἐξήγησις\" (\"Commentary on the Phaenomena of Eudoxus and Aratus\"). This is a highly critical commentary in the form of two books on a popular poem by Aratus based on the work by Eudoxus. Hipparchus also made a list of his major works, which apparently mentioned about fourteen books, but which is only known from references by later authors. His famous star catalog was incorporated into the one by Ptolemy, and may be almost perfectly reconstructed by subtraction of two and two thirds degrees from the longitudes of Ptolemy's stars. The first trigonometric table was apparently compiled by Hipparchus, who is now consequently known as \"the father of trigonometry\".\n\nHipparchus was in the international news in 2005, when it was again proposed (as in 1898) that the data on the celestial globe of Hipparchus or in his star catalog may have been preserved in the only surviving large ancient celestial globe which depicts the constellations with moderate accuracy, the globe carried by the Farnese Atlas. There are a variety of mis-steps in the more ambitious 2005 paper, thus no specialists in the area accept its widely publicized speculation.\n\nLucio Russo has said that Plutarch, in his work \"On the Face in the Moon\", was reporting some physical theories that we consider to be Newtonian and that these may have come originally from Hipparchus; he goes on to say that Newton may have been influenced by them. According to one book review, both of these claims have been rejected by other scholars.\n\nA line in Plutarch's \"Table Talk\" states that Hipparchus counted 103049 compound propositions that can be formed from ten simple propositions. 103049 is the tenth Schröder–Hipparchus number, which counts the number of ways of adding one or more pairs of parentheses around consecutive subsequences of two or more items in any sequence of ten symbols. This has led to speculation that Hipparchus knew about enumerative combinatorics, a field of mathematics that developed independently in modern mathematics.\n\nEarlier Greek astronomers and mathematicians were influenced by Babylonian astronomy to some extent, for instance the period relations of the Metonic cycle and Saros cycle may have come from Babylonian sources (see \"Babylonian astronomical diaries\"). Hipparchus seems to have been the first to exploit Babylonian astronomical knowledge and techniques systematically. Except for Timocharis and Aristillus, he was the first Greek known to divide the circle in 360 degrees of 60 arc minutes (Eratosthenes before him used a simpler sexagesimal system dividing a circle into 60 parts). He also used the Babylonian unit \"pechus\" (\"cubit\") of about 2° or 2.5°.\n\nHipparchus probably compiled a list of Babylonian astronomical observations; G. J. Toomer, a historian of astronomy, has suggested that Ptolemy's knowledge of eclipse records and other Babylonian observations in the \"Almagest\" came from a list made by Hipparchus. Hipparchus's use of Babylonian sources has always been known in a general way, because of Ptolemy's statements. However, Franz Xaver Kugler demonstrated that the synodic and anomalistic periods that Ptolemy attributes to Hipparchus had already been used in Babylonian ephemerides, specifically the collection of texts nowadays called \"System B\" (sometimes attributed to Kidinnu).\n\nHipparchus's long draconitic lunar period (5,458 months = 5,923 lunar nodal periods) also appears a few times in Babylonian records. But the only such tablet explicitly dated is post-Hipparchus so the direction of transmission is not settled by the tablets.\n\nHipparchus's draconitic lunar motion cannot be solved by the lunar-four arguments that are sometimes proposed to explain his anomalistic motion. A solution that has produced the exact ratio is rejected by most historians though it uses the only anciently attested method of determining such ratios, and it automatically delivers the ratio's four-digit numerator and denominator. Hipparchus initially used (\"Almagest\" 6.9) his 141 BC eclipse with a Babylonian eclipse of 720 BC to find the less accurate ratio 7,160 synodic months = 7,770 draconitic months, simplified by him to 716 = 777 through division by 10. (He similarly found from the 345-year cycle the ratio 4267 synodic months = 4573 anomalistic months and divided by 17 to obtain the standard ratio 251 synodic months = 269 anomalistic months.) If he sought a longer time base for this draconitic investigation he could use his same 141 BC eclipse with a moonrise 1245 BC eclipse from Babylon, an interval of 13,645 synodic months = draconitic months ≈ anomalistic months. Dividing by produces 5458 synodic months = 5923 precisely. The obvious main objection is that the early eclipse is unattested though that is not surprising in itself and there is no consensus on whether Babylonian observations were recorded this remotely. Though Hipparchus's tables formally went back only to 747 BC, 600 years before his era, the tables were actually good back to before the eclipse in question because as only recently noted their use in reverse is no more difficult than forwards.\n\nHipparchus was recognized as the first mathematician known to have possessed a trigonometric table, which he needed when computing the eccentricity of the orbits of the Moon and Sun. He tabulated values for the chord function, which gives the length of the chord for each angle. He did this for a circle with a circumference of 21,600 and a radius (rounded) of 3438 units: this circle has a unit length of 1 arc minute along its perimeter. He tabulated the chords for angles with increments of 7.5°. In modern terms, the chord of an angle equals the radius times twice the sine of half of the angle, i.e.:\n\nHe described the chord table in a work, now lost, called \"Tōn en kuklō eutheiōn\" (\"Of Lines Inside a Circle\") by Theon of Alexandria in his 4th-century commentary on the \"Almagest\" I.10; some claim his table may have survived in astronomical treatises in India, for instance the \"Surya Siddhanta\". Trigonometry was a significant innovation, because it allowed Greek astronomers to solve any triangle, and made it possible to make quantitative astronomical models and predictions using their preferred geometric techniques.\n\nFor his chord table Hipparchus must have used a better approximation for π than the one from Archimedes of between and ; perhaps he had the one later used by Ptolemy: 3;8,30 (sexagesimal) (\"Almagest\" VI.7); but it is not known if he computed an improved value himself.\n\nBut some scholars do not believe Āryabhaṭa's sine table has anything to do with Hipparchus's chord table which does not exist today. Some scholars do not agree with this hypothesis that Hipparchus constructed a chord table. Bo C. Klintberg states \"With mathematical reconstructions and philosophical arguments I show that Toomer's 1973 paper never contained any conclusive evidence for his claims that Hipparchus had a 3438'-based chord table, and that the Indians used that table to compute their sine tables. Recalculating Toomer's reconstructions with a 3600' radius – i.e. the radius of the chord table in Ptolemy's Almagest, expressed in 'minutes' instead of 'degrees' – generates Hipparchan-like ratios similar to those produced by a 3438' radius. It is therefore possible that the radius of Hipparchus's chord table was 3600', and that the Indians independently constructed their 3438'-based sine table.\"\n\nHipparchus could construct his chord table using the Pythagorean theorem and a theorem known to Archimedes. He also might have developed and used the theorem in plane geometry called Ptolemy's theorem, because it was proved by Ptolemy in his \"Almagest\" (I.10) (later elaborated on by Carnot).\n\nHipparchus was the first to show that the stereographic projection is conformal, and that it transforms circles on the sphere that do not pass through the center of projection to circles on the plane. This was the basis for the astrolabe.\n\nBesides geometry, Hipparchus also used arithmetic techniques developed by the Chaldeans. He was one of the first Greek mathematicians to do this, and in this way expanded the techniques available to astronomers and geographers.\n\nThere are several indications that Hipparchus knew spherical trigonometry, but the first surviving text of it is that of Menelaus of Alexandria in the 1st century, who on that basis is now commonly credited with its discovery. (Previous to the finding of the proofs of Menelaus a century ago, Ptolemy was credited with the invention of spherical trigonometry.) Ptolemy later used spherical trigonometry to compute things like the rising and setting points of the ecliptic, or to take account of the lunar parallax. Hipparchus may have used a globe for these tasks, reading values off coordinate grids drawn on it, or he may have made approximations from planar geometry, or perhaps used arithmetical approximations developed by the Chaldeans. He might have used spherical trigonometry.\n\nAubrey Diller has shown that the clima calculations which Strabo preserved from Hipparchus were performed by spherical trigonometry with the sole accurate obliquity known to have been used by ancient astronomers, 23°40'. All thirteen clima figures agree with Diller's proposal. Further confirming his contention is the finding that the big errors in Hipparchus's longitude of Regulus and both longitudes of Spica agree to a few minutes in all three instances with a theory that he took the wrong sign for his correction for parallax when using eclipses for determining stars' positions.\n\nHipparchus also studied the motion of the Moon and confirmed the accurate values for two periods of its motion that Chaldean astronomers are widely presumed to have possessed before him, whatever their ultimate origin. The traditional value (from Babylonian System B) for the mean synodic month is 29 days; 31,50,8,20 (sexagesimal) = 29.5305941... days. Expressed as 29 days + 12 hours +  hours this value has been used later in the Hebrew calendar. The Chaldeans also knew that 251 synodic months ≈ 269 anomalistic months. Hipparchus used the multiple of this period by a factor of 17, because that interval is also an eclipse period, and is also close to an integer number of years (4267 moons : 4573 anomalistic periods : 4630.53 nodal periods : 4611.98 lunar orbits : 344.996 years : 344.982 solar orbits : 126,007.003 days : 126,351.985 rotations). What was so exceptional and useful about the cycle was that all 345-year-interval eclipse pairs occur slightly over 126,007 days apart within a tight range of only about ± hour, guaranteeing (after division by 4267) an estimate of the synodic month correct to one part in order of magnitude 10 million. The 345 year periodicity is why the ancients could conceive of a \"mean\" month and quantify it so accurately that it is even today correct to a fraction of a second of time.\n\nHipparchus could confirm his computations by comparing eclipses from his own time (presumably 27 January 141  and 26 November 139  according to [Toomer 1980]), with eclipses from Babylonian records 345 years earlier (\"Almagest\" IV.2; [A.Jones, 2001]). Already al-Biruni (\"Qanun\" VII.2.II) and Copernicus (\"de revolutionibus\" IV.4) noted that the period of 4,267 moons is actually about 5 minutes longer than the value for the eclipse period that Ptolemy attributes to Hipparchus. However, the timing methods of the Babylonians had an error of no less than 8 minutes. Modern scholars agree that Hipparchus rounded the eclipse period to the nearest hour, and used it to confirm the validity of the traditional values, rather than try to derive an improved value from his own observations. From modern ephemerides and taking account of the change in the length of the day (see ΔT) we estimate that the error in the assumed length of the synodic month was less than 0.2 seconds in the 4th century  and less than 0.1 seconds in Hipparchus's time.\n\nIt had been known for a long time that the motion of the Moon is not uniform: its speed varies. This is called its \"anomaly\", and it repeats with its own period; the anomalistic month. The Chaldeans took account of this arithmetically, and used a table giving the daily motion of the Moon according to the date within a long period. The Greeks however preferred to think in geometrical models of the sky. Apollonius of Perga had at the end of the 3rd century  proposed two models for lunar and planetary motion:\n\nHipparchus devised a geometrical method to find the parameters from three positions of the Moon, at particular phases of its anomaly. In fact, he did this separately for the eccentric and the epicycle model. Ptolemy describes the details in the \"Almagest\" IV.11. Hipparchus used two sets of three lunar eclipse observations, which he carefully selected to satisfy the requirements. The eccentric model he fitted to these eclipses from his Babylonian eclipse list: 22/23 December 383 , 18/19 June 382 , and 12/13 December 382 . The epicycle model he fitted to lunar eclipse observations made in Alexandria at 22 September 201 , 19 March 200 , and 11 September 200 .\nThe somewhat weird numbers are due to the cumbersome unit he used in his chord table according to one group of historians, who explain their reconstruction's inability to agree with these four numbers as partly due to some sloppy rounding and calculation errors by Hipparchus, for which Ptolemy criticised him (he himself made rounding errors too). A simpler alternate reconstruction agrees with all four numbers. Anyway, Hipparchus found inconsistent results; he later used the ratio of the epicycle model ( : ), which is too small (60 : 4;45 sexagesimal). Ptolemy established a ratio of 60 : . (The maximum angular deviation producible by this geometry is the arcsin of divided by 60, or about 5° 1', a figure that is sometimes therefore quoted as the equivalent of the Moon's equation of the center in the Hipparchan model.)\n\nBefore Hipparchus, Meton, Euctemon, and their pupils at Athens had made a solstice observation (i.e., timed the moment of the summer solstice) on 27 June 432  (proleptic Julian calendar). Aristarchus of Samos is said to have done so in 280 , and Hipparchus also had an observation by Archimedes. As shown in a 1991\npaper, in 158 BC Hipparchus computed a very erroneous summer solstice from Callippus's calendar. He observed the summer solstice in 146 and 135  both accurate to a few hours, but observations of the moment of equinox were simpler, and he made twenty during his lifetime. Ptolemy gives an extensive discussion of Hipparchus's work on the length of the year in the \"Almagest\" III.1, and quotes many observations that Hipparchus made or used, spanning 162–128 . Analysis of Hipparchus's seventeen equinox observations made at Rhodes shows that the mean error in declination is positive seven arc minutes, nearly agreeing with the sum of refraction by air and Swerdlow's parallax. The random noise is two arc minutes or more nearly one arcminute if rounding is taken into account which approximately agrees with the sharpness of the eye. Ptolemy quotes an equinox timing by Hipparchus (at 24 March 146  at dawn) that differs by 5 hours from the observation made on Alexandria's large public equatorial ring that same day (at 1 hour before noon): Hipparchus may have visited Alexandria but he did not make his equinox observations there; presumably he was on Rhodes (at nearly the same geographical longitude). He could have used the equatorial ring of his armillary sphere or another equatorial ring for these observations, but Hipparchus (and Ptolemy) knew that observations with these instruments are sensitive to a precise alignment with the equator, so if he were restricted to an armillary, it would make more sense to use its meridian ring as a transit instrument. The problem with an equatorial ring (if an observer is naive enough to trust it very near dawn or dusk) is that atmospheric refraction lifts the Sun significantly above the horizon: so for a northern hemisphere observer its apparent declination is too high, which changes the observed time when the Sun crosses the equator. (Worse, the refraction decreases as the Sun rises and increases as it sets, so it may appear to move in the wrong direction with respect to the equator in the course of the day – as Ptolemy mentions. Ptolemy and Hipparchus apparently did not realize that refraction is the cause.) However, such details have doubtful relation to the data of either man, since there is no textual, scientific, or statistical ground for believing that their equinoxes were taken on an equatorial ring, which is useless for solstices in any case. Not one of two centuries of mathematical investigations of their solar errors has claimed to have traced them to the effect of refraction on use of an equatorial ring. Ptolemy claims his solar observations were on a transit instrument set in the meridian.\n\nRecent expert translation and analysis by Anne Tihon of papyrus P. Fouad 267 A has confirmed the 1991 finding cited above that Hipparchus obtained a summer solstice in 158 BC But the papyrus makes the date June 26, over a day earlier than the 1991 paper's conclusion for June 28. The earlier study's §M found that Hipparchus did not adopt June 26 solstices until 146 BC when he founded the orbit of the sun which Ptolemy later adopted. Dovetailing these data suggests Hipparchus extrapolated the 158 BC June 26 solstice from his 145 solstice 12 years later a procedure that would cause only minuscule error. The papyrus also confirmed that Hipparchus had used Callippic solar motion in 158 BC, a new finding in 1991 but not attested directly until P. Fouad 267 A. Another table on the papyrus is perhaps for sidereal motion and a third table is for Metonic tropical motion, using a previously unknown year of – days. This was presumably found by dividing the 274 years from 432 to 158 BC, into the corresponding interval of 100077 days and hours between Meton's sunrise and Hipparchus's sunset solstices.\n\nAt the end of his career, Hipparchus wrote a book called \"Peri eniausíou megéthous\" (\"On the Length of the Year\") about his results. The established value for the tropical year, introduced by Callippus in or before 330  was days. Speculating a Babylonian origin for the Callippic year is hard to defend, since Babylon did not observe solstices thus the only extant System B year length was based on Greek solstices (see below). Hipparchus's equinox observations gave varying results, but he himself points out (quoted in \"Almagest\" III.1(H195)) that the observation errors by himself and his predecessors may have been as large as day. He used old solstice observations, and determined a difference of about one day in about 300 years. So he set the length of the tropical year to − days (= 365.24666... days = 365 days 5 hours 55 min, which differs from the actual value (modern estimate, including earth spin acceleration) in his time of about 365.2425 days, an error of about 6 min per year, an hour per decade, 10 hours per century.\n\nBetween the solstice observation of Meton and his own, there were 297 years spanning 108,478 days. D. Rawlins noted that this implies a tropical year of 365.24579... days = 365 days;14,44,51 (sexagesimal; = 365 days + + + ) and that this exact year length has been found on one of the few Babylonian clay tablets which explicitly specifies the System B month. This is an indication that Hipparchus's work was known to Chaldeans.\n\nAnother value for the year that is attributed to Hipparchus (by the astrologer Vettius Valens in the 1st century) is 365 + + days (= 365.25347... days = 365 days 6 hours 5 min), but this may be a corruption of another value attributed to a Babylonian source: 365 + + days (= 365.25694... days = 365 days 6 hours 10 min). It is not clear if this would be a value for the sidereal year (actual value at his time (modern estimate) about 365.2565 days), but the difference with Hipparchus's value for the tropical year is consistent with his rate of precession (see below).\n\nBefore Hipparchus, astronomers knew that the lengths of the seasons are not equal. Hipparchus made observations of equinox and solstice, and according to Ptolemy (\"Almagest\" III.4) determined that spring (from spring equinox to summer solstice) lasted 94½ days, and summer (from summer solstice to autumn equinox) days. This is inconsistent with a premise of the Sun moving around the Earth in a circle at uniform speed. Hipparchus's solution was to place the Earth not at the center of the Sun's motion, but at some distance from the center. This model described the apparent motion of the Sun fairly well. It is known today that the planets, including the Earth, move in approximate ellipses around the Sun, but this was not discovered until Johannes Kepler published his first two laws of planetary motion in 1609. The value for the eccentricity attributed to Hipparchus by Ptolemy is that the offset is of the radius of the orbit (which is a little too large), and the direction of the apogee would be at longitude 65.5° from the vernal equinox. Hipparchus may also have used other sets of observations, which would lead to different values. One of his two eclipse trios' solar longitudes are consistent with his having initially adopted inaccurate lengths for spring and summer of and days. His other triplet of solar positions is consistent with and days, an improvement on the results ( and days) attributed to Hipparchus by Ptolemy, which a few scholars still question the authorship of. Ptolemy made no change three centuries later, and expressed lengths for the autumn and winter seasons which were already implicit (as shown, e.g., by A. Aaboe).\n\nHipparchus also undertook to find the distances and sizes of the Sun and the Moon. He published his results in a work of two books called \"Perí megethōn kaí apostēmátōn\" (\"On Sizes and Distances\") by Pappus in his commentary on the \"Almagest\" V.11; Theon of Smyrna (2nd century) mentions the work with the addition \"of the Sun and Moon\".\n\nHipparchus measured the apparent diameters of the Sun and Moon with his \"diopter\". Like others before and after him, he found that the Moon's size varies as it moves on its (eccentric) orbit, but he found no perceptible variation in the apparent diameter of the Sun. He found that at the \"mean\" distance of the Moon, the Sun and Moon had the same apparent diameter; at that distance, the Moon's diameter fits 650 times into the circle, i.e., the mean apparent diameters are = 0°33'14\".\n\nLike others before and after him, he also noticed that the Moon has a noticeable parallax, i.e., that it appears displaced from its calculated position (compared to the Sun or stars), and the difference is greater when closer to the horizon. He knew that this is because in the then-current models the Moon circles the center of the Earth, but the observer is at the surface—the Moon, Earth and observer form a triangle with a sharp angle that changes all the time. From the size of this parallax, the distance of the Moon as measured in Earth radii can be determined. For the Sun however, there was no observable parallax (we now know that it is about 8.8\", several times smaller than the resolution of the unaided eye).\n\nIn the first book, Hipparchus assumes that the parallax of the Sun is 0, as if it is at infinite distance. He then analyzed a solar eclipse, which Toomer (against the opinion of over a century of astronomers) presumes to be the eclipse of 14 March 190 . It was total in the region of the Hellespont (and in his birthplace, Nicaea); at the time Toomer proposes the Romans were preparing for war with Antiochus III in the area, and the eclipse is mentioned by Livy in his \"Ab Urbe Condita Libri\" VIII.2. It was also observed in Alexandria, where the Sun was reported to be obscured 4/5ths by the Moon. Alexandria and Nicaea are on the same meridian. Alexandria is at about 31° North, and the region of the Hellespont about 40° North. (It has been contended that authors like Strabo and Ptolemy had fairly decent values for these geographical positions, so Hipparchus must have known them too. However, Strabo's Hipparchus dependent latitudes for this region are at least 1° too high, and Ptolemy appears to copy them, placing Byzantium 2° high in latitude.) Hipparchus could draw a triangle formed by the two places and the Moon, and from simple geometry was able to establish a distance of the Moon, expressed in Earth radii. Because the eclipse occurred in the morning, the Moon was not in the meridian, and it has been proposed that as a consequence the distance found by Hipparchus was a lower limit. In any case, according to Pappus, Hipparchus found that the least distance is 71 (from this eclipse), and the greatest 81 Earth radii.\n\nIn the second book, Hipparchus starts from the opposite extreme assumption: he assigns a (minimum) distance to the Sun of 490 Earth radii. This would correspond to a parallax of 7', which is apparently the greatest parallax that Hipparchus thought would not be noticed (for comparison: the typical resolution of the human eye is about 2'; Tycho Brahe made naked eye observation with an accuracy down to 1'). In this case, the shadow of the Earth is a cone rather than a cylinder as under the first assumption. Hipparchus observed (at lunar eclipses) that at the mean distance of the Moon, the diameter of the shadow cone is lunar diameters. That apparent diameter is, as he had observed, degrees. With these values and simple geometry, Hipparchus could determine the mean distance; because it was computed for a minimum distance of the Sun, it is the maximum mean distance possible for the Moon. With his value for the eccentricity of the orbit, he could compute the least and greatest distances of the Moon too. According to Pappus, he found a least distance of 62, a mean of , and consequently a greatest distance of Earth radii. With this method, as the parallax of the Sun decreases (i.e., its distance increases), the minimum limit for the mean distance is 59 Earth radii – exactly the mean distance that Ptolemy later derived.\n\nHipparchus thus had the problematic result that his minimum distance (from book 1) was greater than his maximum mean distance (from book 2). He was intellectually honest about this discrepancy, and probably realized that especially the first method is very sensitive to the accuracy of the observations and parameters. (In fact, modern calculations show that the size of the 189  solar eclipse at Alexandria must have been closer to ths and not the reported ths, a fraction more closely matched by the degree of totality at Alexandria of eclipses occurring in 310 and 129  which were also nearly total in the Hellespont and are thought by many to be more likely possibilities for the eclipse Hipparchus used for his computations.)\n\nPtolemy later measured the lunar parallax directly (\"Almagest\" V.13), and used the second method of Hipparchus with lunar eclipses to compute the distance of the Sun (\"Almagest\" V.15). He criticizes Hipparchus for making contradictory assumptions, and obtaining conflicting results (\"Almagest\" V.11): but apparently he failed to understand Hipparchus's strategy to establish limits consistent with the observations, rather than a single value for the distance. His results were the best so far: the actual mean distance of the Moon is 60.3 Earth radii, within his limits from Hipparchus's second book.\n\nTheon of Smyrna wrote that according to Hipparchus, the Sun is 1,880 times the size of the Earth, and the Earth twenty-seven times the size of the Moon; apparently this refers to volumes, not diameters. From the geometry of book 2 it follows that the Sun is at 2,550 Earth radii, and the mean distance of the Moon is radii. Similarly, Cleomedes quotes Hipparchus for the sizes of the Sun and Earth as 1050:1; this leads to a mean lunar distance of 61 radii. Apparently Hipparchus later refined his computations, and derived accurate single values that he could use for predictions of solar eclipses.\n\nSee [Toomer 1974] for a more detailed discussion.\n\nPliny (\"Naturalis Historia\" II.X) tells us that Hipparchus demonstrated that lunar eclipses can occur five months apart, and solar eclipses seven months (instead of the usual six months); and the Sun can be hidden twice in thirty days, but as seen by different nations. Ptolemy discussed this a century later at length in \"Almagest\" VI.6. The geometry, and the limits of the positions of Sun and Moon when a solar or lunar eclipse is possible, are explained in \"Almagest\" VI.5. Hipparchus apparently made similar calculations. The result that two solar eclipses can occur one month apart is important, because this can not be based on observations: one is visible on the northern and the other on the southern hemisphere – as Pliny indicates – and the latter was inaccessible to the Greek.\n\nPrediction of a solar eclipse, i.e., exactly when and where it will be visible, requires a solid lunar theory and proper treatment of the lunar parallax. Hipparchus must have been the first to be able to do this. A rigorous treatment requires spherical trigonometry, thus those who remain certain that Hipparchus lacked it must speculate that he may have made do with planar approximations. He may have discussed these things in \"Perí tēs katá plátos mēniaías tēs selēnēs kinēseōs\" (\"On the monthly motion of the Moon in latitude\"), a work mentioned in the \"Suda\".\n\nPliny also remarks that \"he also discovered for what exact reason, although the shadow causing the eclipse must from sunrise onward be below the earth, it happened once in the past that the moon was eclipsed in the west while both luminaries were visible above the earth\" (translation H. Rackham (1938), Loeb Classical Library 330 p. 207). Toomer (1980) argued that this must refer to the large total lunar eclipse of 26 November 139 , when over a clean sea horizon as seen from Rhodes, the Moon was eclipsed in the northwest just after the Sun rose in the southeast. This would be the second eclipse of the 345-year interval that Hipparchus used to verify the traditional Babylonian periods: this puts a late date to the development of Hipparchus's lunar theory. We do not know what \"exact reason\" Hipparchus found for seeing the Moon eclipsed while apparently it was not in exact opposition to the Sun. Parallax lowers the altitude of the luminaries; refraction raises them, and from a high point of view the horizon is lowered.\n\nHipparchus and his predecessors used various instruments for astronomical calculations and observations, such as the gnomon, the astrolabe, and the armillary sphere.\n\nHipparchus is credited with the invention or improvement of several astronomical instruments, which were used for a long time for naked-eye observations. According to Synesius of Ptolemais (4th century) he made the first \"astrolabion\": this may have been an armillary sphere (which Ptolemy however says he constructed, in \"Almagest\" V.1); or the predecessor of the planar instrument called astrolabe (also mentioned by Theon of Alexandria). With an astrolabe Hipparchus was the first to be able to measure the geographical latitude and time by observing fixed stars. Previously this was done at daytime by measuring the shadow cast by a gnomon, by recording the length of the longest day of the year or with the portable instrument known as a \"scaphe\".\n\nPtolemy mentions (\"Almagest\" V.14) that he used a similar instrument as Hipparchus, called \"dioptra\", to measure the apparent diameter of the Sun and Moon. Pappus of Alexandria described it (in his commentary on the \"Almagest\" of that chapter), as did Proclus (\"Hypotyposis\" IV). It was a 4-foot rod with a scale, a sighting hole at one end, and a wedge that could be moved along the rod to exactly obscure the disk of Sun or Moon.\n\nHipparchus also observed solar equinoxes, which may be done with an equatorial ring: its shadow falls on itself when the Sun is on the equator (i.e., in one of the equinoctial points on the ecliptic), but the shadow falls above or below the opposite side of the ring when the Sun is south or north of the equator. Ptolemy quotes (in \"Almagest\" III.1 (H195)) a description by Hipparchus of an equatorial ring in Alexandria; a little further he describes two such instruments present in Alexandria in his own time.\n\nHipparchus applied his knowledge of spherical angles to the problem of denoting locations on the Earth's surface. Before him a grid system had been used by Dicaearchus of Messana, but Hipparchus was the first to apply mathematical rigor to the determination of the latitude and longitude of places on the Earth. Hipparchus wrote a critique in three books on the work of the geographer Eratosthenes of Cyrene (3rd century ), called \"Pròs tèn 'Eratosthénous geografían\" (\"Against the Geography of Eratosthenes\"). It is known to us from Strabo of Amaseia, who in his turn criticised Hipparchus in his own \"Geografia\". Hipparchus apparently made many detailed corrections to the locations and distances mentioned by Eratosthenes. It seems he did not introduce many improvements in methods, but he did propose a means to determine the geographical longitudes of different cities at lunar eclipses (Strabo \"Geografia\" 1 January 2012). A lunar eclipse is visible simultaneously on half of the Earth, and the difference in longitude between places can be computed from the difference in local time when the eclipse is observed. His approach would give accurate results if it were correctly carried out but the limitations of timekeeping accuracy in his era made this method impractical.\n\nLate in his career (possibly about 135 ) Hipparchus compiled his star catalog, the original of which does not survive. He also constructed a celestial globe depicting the constellations, based on his observations. His interest in the fixed stars may have been inspired by the observation of a supernova (according to Pliny), or by his discovery of precession, according to Ptolemy, who says that Hipparchus could not reconcile his data with earlier observations made by Timocharis and Aristillus. For more information see Discovery of precession. In Raphael's painting \"The School of Athens\", Hipparchus is depicted holding his celestial globe, as the representative figure for astronomy.\n\nPreviously, Eudoxus of Cnidus in the 4th century  had described the stars and constellations in two books called \"Phaenomena\" and \"Entropon\". Aratus wrote a poem called \"Phaenomena\" or \"Arateia\" based on Eudoxus's work. Hipparchus wrote a commentary on the \"Arateia\" – his only preserved work – which contains many stellar positions and times for rising, culmination, and setting of the constellations, and these are likely to have been based on his own measurements.\n\nHipparchus made his measurements with an armillary sphere, and obtained the positions of at least 850 stars. It is disputed which coordinate system(s) he used. Ptolemy's catalog in the \"Almagest\", which is derived from Hipparchus's catalog, is given in ecliptic coordinates. However Delambre in his \"Histoire de l'Astronomie Ancienne\" (1817) concluded that Hipparchus knew and used the equatorial coordinate system, a conclusion challenged by Otto Neugebauer in his \"A History of Ancient Mathematical Astronomy\" (1975). Hipparchus seems to have used a mix of ecliptic coordinates and equatorial coordinates: in his commentary on Eudoxos he provides stars' polar distance (equivalent to the declination in the equatorial system), right ascension (equatorial), longitude (ecliptical), polar longitude (hybrid), but not celestial latitude.\n\nAs with most of his work, Hipparchus's star catalog was adopted and perhaps expanded by Ptolemy. Delambre, in 1817, cast doubt on Ptolemy's work. It was disputed whether the star catalog in the \"Almagest\" is due to Hipparchus, but 1976–2002 statistical and spatial analyses (by R. R. Newton, Dennis Rawlins, Gerd Grasshoff, Keith Pickering and Dennis Duke) have shown conclusively that the \"Almagest\" star catalog is almost entirely Hipparchan. Ptolemy has even (since Brahe, 1598) been accused by astronomers of fraud for stating (\"Syntaxis\", book 7, chapter 4) that he observed all 1025 stars: for almost every star he used Hipparchus's data and precessed it to his own epoch centuries later by adding 2°40' to the longitude, using an erroneously small precession constant of 1° per century.\n\nIn any case the work started by Hipparchus has had a lasting heritage, and was much later updated by Al Sufi (964) and Copernicus (1543). Ulugh Beg reobserved all the Hipparchus stars he could see from Samarkand in 1437 to about the same accuracy as Hipparchus's. The catalog was superseded only in the late 16th century by Brahe and Wilhelm IV of Kassel via superior ruled instruments and spherical trigonometry, which improved accuracy by an order of magnitude even before the invention of the telescope. Hipparchus is considered the greatest observational astronomer from classical antiquity until Brahe.\n\nHipparchus is only conjectured to have ranked the apparent magnitudes of stars on a numerical scale from 1, the brightest, to 6, the faintest. Nevertheless, this system certainly precedes Ptolemy, who used it extensively about 150. This system was made more precise and extended by N. R. Pogson in 1856, who placed the magnitudes on a logarithmic scale, making magnitude 1 stars 100 times brighter than magnitude 6 stars, thus each magnitude is or 2.512 times brighter than the next faintest magnitude.\n\nHipparchus is generally recognized as discoverer of the precession of the equinoxes in 127 . His two books on precession, \"On the Displacement of the Solsticial and Equinoctial Points\" and \"On the Length of the Year\", are both mentioned in the \"Almagest\" of Claudius Ptolemy. According to Ptolemy, Hipparchus measured the longitude of Spica and Regulus and other bright stars. Comparing his measurements with data from his predecessors, Timocharis and Aristillus, he concluded that Spica had moved 2° relative to the autumnal equinox. He also compared the lengths of the tropical year (the time it takes the Sun to return to an equinox) and the sidereal year (the time it takes the Sun to return to a fixed star), and found a slight discrepancy. Hipparchus concluded that the equinoxes were moving (\"precessing\") through the zodiac, and that the rate of precession was not less than 1° in a century.\n\nHipparchus's treatise \"Against the Geography of Eratosthenes\" in three books is not preserved. \nMost of our knowledge of it comes from Strabo, according to whom Hipparchus thoroughly and often unfairly criticized Eratosthenes mainly for internal contradictions and inaccuracy in determining positions of geographical localities. Hipparchus insists that a geographic map must be based only on astronomical measurements of latitudes and longitudes and triangulation for finding unknown distances. \nIn geographic theory and methods Hipparchus introduced three main innovations.\nHe was the first to use the grade grid, to determine geographic latitude from star observations, and not only from the sun’s altitude, a method known long before him, and to suggest that geographic longitude could be determined by means of simultaneous observations of lunar eclipses in distant places. In the practical part of his work, the so-called \"table of climata\", Hipparchus listed latitudes for several tens of localities. In particular, he improved Eratosthenes' values for the latitudes of Athens, Sicily, and southern extremity of India. \nIn calculating latitudes of climata (latitudes correlated with the length of the longest solstitial day), Hipparchus used an unexpectedly accurate value for the obliquity of the ecliptic, 23°40′ (the actual value in the second half of the 2nd century  was approximately 23°43′), whereas all other ancient authors knew only a roughly rounded value 24°, and even Ptolemy used a less accurate value, 23°51′. \nHipparchus opposed the view generally accepted in the Hellenistic period that the Atlantic and Indian Oceans and the Caspian Sea are parts of a single ocean. At the same time he extends the limits of the oikoumene, i.e. the inhabited part of the land, up to the equator and the Arctic Circle. \nHipparchus’ ideas found their reflection in the \"Geography\" of Ptolemy. In essence, Ptolemy's work is an extended attempt to realize Hipparchus’ vision of what geography ought to be.\n\nThe rather cumbersome formal name for the ESA's Hipparcos Space Astrometry Mission was High Precision Parallax Collecting Satellite; it was deliberately named in this way to give an acronym, HiPParCoS, that echoed and commemorated the name of Hipparchus. The lunar crater Hipparchus and the asteroid 4000 Hipparchus are more directly named after him.\n\nThe Astronomer's Monument at the Griffith Observatory in Los Angeles, California, United States features a relief of Hipparchus as one of six of the greatest astronomers of all time and the only one from Antiquity.\n\n\n\n\n\n\n\n"}
{"id": "33949149", "url": "https://en.wikipedia.org/wiki?curid=33949149", "title": "History of hearing aids", "text": "History of hearing aids\n\nThe first hearing aid was created in the 17th century. The movement toward modern hearing aids began with the creation of the telephone, and the first electric hearing aid was created in 1898. By the late 20th century, the digital hearing aid was distributed to the public commercially. Some of the first hearing aids were external hearing aids. External hearing aids directed sounds in front of the ear and blocked all other noises. The apparatus would fit behind or in the ear.\n\nThe invention of the carbon microphone, transmitters, digital signal processing chip or DSP, and the development of computer technology helped transform the hearing aid to its present form.\n\nThe use of ear trumpets for the partially deaf, dates back to the 17th century. By the late 18th century, their use was becoming increasingly common. Collapsible conical ear trumpets were made by instrument makers on a one-off basis for specific clients. Well known models of the period included the Townsend Trumpet (made by the deaf educator John Townshend), the Reynolds Trumpet (specially built for painter Joshua Reynolds) and the Daubeney Trumpet.\n\nThe first firm to begin commercial production of the ear trumpet was established by Frederick C. Rein in London in 1800. As well as producing ear trumpets, Rein also sold hearing fans, and speaking tubes. These instruments helped amplify sounds, while still being portable. However, these devices were generally bulky and had to be physically supported from below. Later, smaller, hand-held ear trumpets and cones were used as hearing aids.\nRein was commissioned to design a special acoustic chair for the ailing King of Portugal, John VI of Portugal in 1819. The throne was designed with ornately carved arms that looked like the open mouths of lions. These holes acted as the receiving area for the acoustics, which were transmitted to the back of the throne via a speaking tube, and into the king's ear.\nFinally in the late 1800s, the acoustic horn, which was a tube that had two ends, a cone that captured sound, and was eventually made to fit in the ear. \n\nToward the late 19th century, hidden hearing aids became increasingly popular. Rein pioneered many notable designs, including his 'acoustic headbands', where the hearing aid device was artfully concealed within the hair or headgear. Reins' \"Aurolese Phones\" were headbands, made in a variety of shapes, that incorporated sound collectors near the ear that would amplify the acoustics. Hearing aids were also hidden in couches, clothing, and accessories. This drive toward ever increasing invisibility was often more about hiding the individual's disability from the public than about helping the individual cope with his problem.\n\nThe first electronic hearing aids were constructed after the invention of the telephone and microphone in the 1870s and 1880s. The technology within the telephone increased how acoustic signal could be altered. Telephones were able to control the loudness, frequency, and distortion of sounds. These abilities were used in the creation of the hearing aid.\n\nThe first electric hearing aid, called the Akouphone, was created by Miller Reese Hutchison in 1898. It used a carbon transmitter, so that the hearing aid could be portable. The carbon transmitter was used to amplify sound by taking a weak signal and using electric current to make it a strong signal. These electronic hearing aids could eventually be shrunk into purses and other accessories. \n\nOne of the first manufacturers of the electronically amplified hearing aid was the Siemens company in 1913. Their hearing aids were bulky and not easily portable. They were about the size of a \"tall cigar box\" and had a speaker that would fit in the ear.\n\nThe first vacuum-tube hearing aid was patented by a Naval engineer Earl Hanson in 1920. It was called the Vactuphone and used the telephone transmitter to turn speech into electrical signals. After the signal was converted, it would be amplified when it moved to the receiver. The hearing aid weighed seven pounds, which made it light enough to be carried. Marconi in England and Western Electric in the US began marketing vacuum tube hearing aids in 1923.\n\nDuring the 1920s and 1930s, the vacuum tube hearing aid became more successful and began to decrease in size with better miniaturization techniques. The Acousticon's Model 56 was created in the mid-1920s and was one of the first portable hearing aid units, although it was quite heavy. The first wearable hearing aid using vacuum tube technology went on sale in England in 1936, and a year later in the United States. By the 1930s, hearing aids were becoming popular to the public. Multitone of London patented the first hearing aid to use automatic gain control. The same company introduced a wearable version in 1948. \n\nMilitary technological advances that occurred in World War II helped the development of hearing aids. One of the major advances that World War II enabled was the idea of miniaturization. This could be seen by Zenith's pocket-sized Miniature 75.\n\nThe development of transistors in 1948 by Bell Laboratories led to major improvements to the hearing aid. The transistor was invented by John Bardeen, Walter Brattain, and William Shockley. Transistors were created to replace vacuum tubes; they were small, required less battery power and had less distortion and heat than their predecessor. These vacuum tubes were typically hot and fragile, so the transistor was the ideal replacement.The 1952 Sonotone 1010 used a transistor stage along with vacuum tubes, to extend battery life. The size of these transistors led to developments in miniature, carbon microphones. These microphones could be mounted on various items, even eyeglasses. In 1951, Raytheon manufactured the transistor and was one of the first companies to mass-produce transistors to throughout America. Raytheon realized that their hearing aid only lasted short-term and began to sell the vacuum-tube hearing aids again along with transistor hearing aids.\n\nThe act of putting transistors into hearing aids was so quick that they were not properly tested. It was later found that transistors could get damp. Because of this dampness, the hearing aid would only last for a few weeks and then die. In order to stop this from happening, a coating had to be put on the transistor to protect it from the dampness. This problem had to be fixed in order for transistors in hearing aids to be successful. \n\nZenith was the first company to realize the problem with transistors was the body heat of individuals. After coming to this conclusion, the first “all-transistor” hearing aids were offered in 1952, called the Microtone Transimatic and the Maico Transist-ear. In 1954, the company, Texas Instruments, produced a silicon transistor, which was much more effective than the previous version. The end of the transistor was marked by the creation of the integrated circuit or IC by Jack Kilby at Texas Instruments in 1958 and the technique was perfected in hearing aids over the next 20 years.\n\nElmer V. Carlson, the author of thirty patents, was instrumental in inventing many of the components of the modern hearing aid.\n\nBeginning in the early 1960s, Bell Telephone Laboratories created a process for creating both speech and audio signals on a large mainframe computer. Because of the size of digital computers, the process of simulating hearing aids was extremely slow. The progression of the audio speech signal took longer than the length of the duration of the signal itself. In order to get the power needed to process the sound, a large mainframe computer was needed. This made it nearly impossible to conceive the idea that hearing aids could be made into something that could be made to fit onto an ear. This research was important for learning about how to develop sounds for those with hearing disabilities. \n\nIn addition, in the 1970s, the microprocessor was created. This microprocessor helped to open up the door to miniaturization of the hearing aid. Moreover, researcher Edgar Villchur developed multi-channel amplitude compression. Amplitude compression enabled audio signal to be separated into frequency bands. These bands were able to adjust sounds so that sounds that were more intensive were weakened and sounds that were weakened would become more intensified. The system of multi-channel amplitude compression would be later used as the fundamental structural design for the first hearing aids that used digital technology.\n\nAnother pioneer in hearing aid development was Daniel Graupe, who developed the six-channel hearing aid. The six-channel hearing aid in 1975 had digital control of the frequency in all channels. In 1979, electro-acoustic features of hearing aids were able to be changed by a simple button. The pressing of this button caused the amplification to be altered to appropriate levels for the environment in which the individual was present. This technique of controlling the hearing aid for the environment is used in some manner throughout all digital hearing aids.\n\nThe creation of high-speed digital-array processors used in minicomputers opened up the door for advances in digital hearing aids. These minicomputers were able to process audio signals at speeds that were equivalent to real-time. In 1982, at the City University of New York, the first real-time all digital hearing aid was created. The equipment contained a digital array processor and minicomputer. This consisted of an FM radio transmitter and receiver. The radio made a connection between the individual through a transmitter on the body to the radio on top of the computer. The transmitter on the body was connected by a wire to the ear microphone and the receiver. Even though this was a major breakthrough in the creation of hearing aids, there were still a few problems. One of these major problems was that while the hearing aid worked, it was extremely heavy and nearly impossible to move.\nThe first commercial digital hearing aid was created in 1987 by the Nicolet Corporation. The hearing aid contained a body-worn processor that had a hardwire connection with an ear mounted transducer. While the Nicolet Corporation’s hearing aid was not publicly successful and the company shortly folded, it was able to start a competition between companies to create more effective hearing aids. Two years later, in 1989, the behind-the-ear (BTE) digital hearing aid was launched.\n\nIn addition to the Nicolet Corporation, Bell Laboratories expanded upon the hearing aid business by developing a hybrid digital-analog hearing aid. This hearing aid used digital circuits to handle a two-channel compression amplifier. Even though early research on this hearing aid was successful, AT&T, the parent company to Bell Laboratories, pulled out of the hearing aid market and sold its rights to Resound Corporation in 1987. When the hearing aid was put on in the market, it was instantaneously successful. This development helped bring major changes to the world of the hearing aid.\n\nAfter the success of the Resound Corporation, other hearing aid companies began putting out hybrid hearing aids that included analog amplifiers, filters, and limiters that were managed digitally. There were many benefits to these hearing aids; which included storing parameter settings, having a capability for paired-comparison testing, having settings for different acoustic environments, and having more advanced methods of signaling; this which included multi-channel compression.\n\nThe next major milestone was creating an all-digital hearing aid. The Oticon Company developed the first digital hearing aid in 1995, but it was only distributed to audiological research centers for research on digital technology in the realm of acoustic amplification. The Senso was the first commercially successful, all-digital hearing aid, and was created by Widex in 1996. After the success of the Senso, Oticon began marketing their own hearing aid, the DigiFocus.\n\nCurrent digital hearing aids are now programmable which enables digital hearing aids to regulate the sound on their own, without using a separate control. The hearing aid can now adjust itself depending on what environment it is in and often does not even need a physical volume control button.\n\nRecently, \"Made for iPhone hearing aids\" (MFi) were introduced by Resound, which enables users of MFi digital hearing aids to stream phone calls, music, and podcasts directly from iOS devices. \n\nDirectly leveraging the audio processing power potential in smartphones, Jacoti BVBA from Belgium developed ListenApp, the first digital hearing aid application to win CE certification and FDA approval as a medical device. \n\nOne of the first digital chips was created by Daniel Graupe. The digital chip, referred to as the Zeta Noise Blocker, routinely adjusted the gain in the frequency channels to help control high levels of noise. The chip was integrated in a number of hearing aids in the 1980s. In addition to the Zeta Noise Blocker, there was a development of digital chips that were devoted to high-speed digital signal processing or DSP. DSP chips became available in 1982, and began to be implemented into hearing aids. By 1988, chips were produced in hearing aids. One of the major contributions of these chips was the ability to process both speech and other types of noises in real time. One major down fall of these chips was that they were massive and used up a lot of battery, which made them nearly impossible to be worn.\n\n\n"}
{"id": "52771514", "url": "https://en.wikipedia.org/wiki?curid=52771514", "title": "IC 3", "text": "IC 3\n\nIC 3 is a compact elliptical galaxy located approximately 228 million light-years away in the constellation of Pisces. The galaxy was discovered by astronomer Stéphane Javelle on August 27, 1892.\n\n"}
{"id": "8151836", "url": "https://en.wikipedia.org/wiki?curid=8151836", "title": "Iota Sigma Pi", "text": "Iota Sigma Pi\n\nIota Sigma Pi (ΙΣΠ) is a national honor society in the United States. It was established in 1902 and specializes in the promotion of women in the sciences, especially chemistry. It also focuses on personal and professional growth for women in these fields. As with all honor societies, they create professional networks along with recognizing achievements of women in chemistry.\n\nThe society was formed during a period when women gained little recognition for their work. Therefore, women began to set up their own awards to highlight their abilities on their resumes.\n\nThe national society was formed in 1902 by Agnes Faye Morgan. She was appointed department chair of the Department of Household Science and Arts at the University of California and was one of the first to integrate chemistry into the curriculum of home economics. She continued to participate in the society throughout her professional life and had a particular focus on research. She also founded a local honour society for women in home economics named Alpha Nu.\n\nThe society goals were to encourage women to pursue chemistry academically, to \"stimulate personal accomplishment in chemical fields\" and to promote the academic, business and social lives of its members.\n\nEarly chapters opened at the University of Washington around 1910 and continued to spread across the country, and eventually held meetings for the American Chemical Society. In the 1930s, there was an offer of amalgamation from the Phi Lambda Upsilon honor society for male chemists but this was refused.\n\nThe highest award from the society is the National Honorary Member which is given to female chemists who have made an exceptional and significant achievement in the field. The certificate is awarded with a prize fund of USD1500. Some of the previous winners include: Marie Sklodowska-Curie, Gerti Cori and Dorothy Hodgkin.\n\nThe Violet Diller Professional Excellence Award, named after a previous member (treasurer and president), is awarded for \"accomplishments in academic, governmental, or industrial chemistry, in education, in administration, or in a combination of these areas\". The award consists of a certificate and USD1000 prize fund. This award was first awarded to Joan P. Lambros in 1984.\n\nThe Agnes Fay Morgan Research Award is given to women who have achieved in the field of chemistry or biochemistry.\n\nThe Centennial Award for Excellence in Undergraduate Teaching is given to those who have excelled in teaching chemistry, biochemistry or a similar subject. The nominee must spend at least 75% of their time teaching undergraduates to qualify for the certificate and USD500 award.\n\nThe Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research is given to the nominee that has demonstrated outstanding chemical research. The nominee must also be a full-time graduate student to get the certification and US$500 reward.\n\nThere are two awards for Undergraduate Excellence in Chemistry; one must go to a first-generation student. Again, the reward is a certificate and US$500.\n\nThe Gladys Anderson Emerson Undergraduate Scholarship is funded from Emerson's estate. She wished for the funding to be given to female Iota Sigma Pi members who are undergraduates in chemistry or biochemistry. The scholarship consists of a US$2000 stipend and a certificate.\n\nThe Members-at-Large Re-entry Award is for those who have been away from academic study for over three years and then returned to degree-level chemistry. This is awarded with a US$1500 cash prize and a certificate of recognition.\n\nAimed at high school students, the Outstanding Young Women in Chemistry is an award given to those who demonstrate high academic ability in chemistry.\n\n"}
{"id": "53853389", "url": "https://en.wikipedia.org/wiki?curid=53853389", "title": "James G. Brasseur", "text": "James G. Brasseur\n\nJames G. Brasseur from the Pennsylvania State University, was awarded the status of Fellow in the American Physical Society, after he was nominated by the Division of Fluid Dynamics in 2009, for \"advancements in knowledge of nonclassical interscale interactions in turbulence and in large-eddy simulation of the high Reynolds number boundary layer, and for interdisciplinary contributions to gastro-intestinal medicine by integrating physiology, mechanics, and mathematical modeling.\"\n"}
{"id": "17929685", "url": "https://en.wikipedia.org/wiki?curid=17929685", "title": "James Paton (seaman)", "text": "James Paton (seaman)\n\nJames \"Scotty\" Paton (1869–1917 or 18) was a Scots-born seaman who sailed to the Antarctic in several major expeditions between 1902 and 1917. His first venture was from 1902 to 1904 as a crewman of \"Captain William Colbeck\"'s SY Morning. This expedition consisted of two voyages and was sent as a relief ship for the Captain Scott's Discovery Expedition. During the first voyage the ship was briefly stalled in the ice between Cape Bird and Beaufort Island. Scotty Paton took the opportunity to leave ship and jump floes a distance of one mile to 'land' of Beaufort Island, the first man to do so. This accomplishment was received with a reprimand. In 1907–09 he was a crew member of Sir Ernest Shackleton's Nimrod Expedition during each of \"Nimrod's\" two southern voyages. From 1910 to 1913 he was a seaman aboard Captain Scott's \"Terra Nova\" during her two voyages between New Zealand and Cape Evans, in support of Scott's ill-fated expedition. In 1914 he joined the Ross Sea party section of Shackleton's Imperial Trans-Antarctic Expedition as boatswain on the \"Aurora\". He was aboard ship on 7 May 1915 when \"Aurora\" was torn from her Cape Evans moorings, drifting in the pack for nine months before limping back to New Zealand. Paton's last Antarctic voyage was with \"Aurora\" on the mission to relieve the stranded Ross Sea party in January 1917.\n\nAfter \"Aurora\"'s return to New Zealand she was sold, and became a coal carrier. Paton signed on as her boatswain, and was aboard when she left Newcastle, New South Wales in June 1917, bound for South America. Her fate is uncertain, but in late 1917 or early 1918 she was lost, and Paton was lost with her. An inquiry established that the German raider \"Wolf\" was laying mines in Cook Strait and in the Tasmanian Sea in June and July 1917 and concluded that the \"Aurora\" likely ran afoul a mine.\n\nPaton's Antarctic voyages are commemorated by Paton Peak, , on Beaufort Island in the Ross Sea, Antarctica, at .\n\n"}
{"id": "54478868", "url": "https://en.wikipedia.org/wiki?curid=54478868", "title": "Kleinjena", "text": "Kleinjena\n\nKleinjena is situated 4 kilometres north of Naumburg on a long spur at the foot of the Finne mountain range above the village of Kleinjena, at the heart of Federal Republic of Germany in the State of Saxony-Anhalt. It has been proposed by Germany for inscription in the List of World Heritage. The World Heritage nomination is representative for the processes that shaped the continent during the High Middle Ages between 1000 and 1300: Christianization, the so-called “Landesausbau” and the dynamics of cultural exchange and transfer characteristic for this very period.\n\nThe castle of Kleinjena is one of the eleven components of the cultural landscape Naumburg Cathedral and the High Medieval Cultural Landscape of the Rivers Saale and Unstrut. As Kleinjena marks the starting point for the vast growth of the region in the 12th century, it is indispensable for an understanding of the history of this cultural landscape as a whole.\n\nThe castle of Kleinjena was built by the influential Ekkehardine dynasty. This dynasty owned estates in the old settlement area in the northern part of Thuringia and in the area of the conﬂuence of the rivers Saale and Unstrut. The family seat was the castle of Gene, which was situated south of the Unstrut River in the former diocese of the Archbishopric of Mainz. Like other Ottonian palaces or imperial castles of the time, the castle complex of Ekkehard I was a segmented complex with two ramparts on a ﬂat spur. Its main rampart and the related trench have ﬂattened to a great extent, but they are still clearly visible in the terrain. Following the murder of Ekkehard I, the site was gradually abandoned as the margravial brothers Hermann and Ekkehard II transferred their family seatfrom Kleinjena to Naumburg (Nuenburch), first mentioned in the records in 1012. \nEncouraged by a privilege granted by Emperor Conrad II in 1033, also the merchants of Kleinjena moved to Naumburg, since they were guaranteed free trade and the heritable, interest-free ownership of their enclosed domicile. The comparative advantage of Naumburg was also a topographical one as that the descent of the long-distance routes coming from the plateau was not as steep as in Kleinjena and Grossjena.\n\nThese archeologically documented remains of the castle of the Markgraves of Meißen, the Ekkehardines, date back to the 10th century. At the European level, these preserved structures are an early evidence of the efforts undertaken by the high nobility to impose its centre of power through large-scale fortifications that eventually served as their memorials.\n\n\n"}
{"id": "48602668", "url": "https://en.wikipedia.org/wiki?curid=48602668", "title": "Lisa Ng", "text": "Lisa Ng\n\nLisa Ng is a virologist. In 2008, she became the first Singaporean and the first woman to win the ASEAN Young Scientist and Technologist Award.\n\nLisa Ng earned a doctorate in molecular virology from the National University of Singapore and later worked in the Genome Institute of Singapore. She works as an Associate Professor at the National University of Singapore and at SIgN where she is the leading virologist. She is undertaking research on infectious diseases, including the Chikungungya virus, and is investigating the improvement of preparedness and prevention based on the use of vaccines and therapeutic antibodies. She has developed diagnostic kits for detecting Avian Influenza and Sars.\n"}
{"id": "2538901", "url": "https://en.wikipedia.org/wiki?curid=2538901", "title": "List of Indian flags", "text": "List of Indian flags\n\nThis is a list of flags used in India. For more information about the national flag, visit the article Flag of India.\n\nJammu and Kashmir is the only state to have \"de jure\" flag.\n\n"}
{"id": "2675183", "url": "https://en.wikipedia.org/wiki?curid=2675183", "title": "List of compounds with carbon number 21", "text": "List of compounds with carbon number 21\n\nThis is a partial list of molecules that contain 21 carbon atoms.\n\n"}
{"id": "49012939", "url": "https://en.wikipedia.org/wiki?curid=49012939", "title": "List of countries by business R&amp;D intensity", "text": "List of countries by business R&amp;D intensity\n\nThis is a list of countries by business research and development intensity as a percentage of value added in the industry, published by the OECD in 2013.\n\n"}
{"id": "24435146", "url": "https://en.wikipedia.org/wiki?curid=24435146", "title": "List of craters on Mars", "text": "List of craters on Mars\n\nThis is a list of craters on Mars. Impact craters on Mars larger than 1 km exist by the hundreds of thousands, but only about one thousand of them have names. Names are assigned by the International Astronomical Union after petitioning by relevant scientists, and in general, only craters that have a significant research interest are given names. Martian craters are named after famous scientists and science fiction authors, or if less than 60 km in diameter, after towns on Earth. Craters cannot be named for living people, and names for small craters are rarely intended to commemorate a specific town. Latitude and longitude are given as planetographic coordinates with west longitude.\n\nThe catalog is divided into three partial lists:\n\nNames are grouped into tables for each letter of the alphabet, containing the crater's name (linked if article exists), coordinates, diameter in kilometers, year of official name adoption (approval), the eponym (\"named after\") and a direct reference to the \"Gazetteer of Planetary Nomenclature\".\n\nAs of 2017, Martian craters account for 21% of all 5,211 named craters in the Solar System. Apart from the Moon, no other body has as many named craters as Mars. Other, non-planetary bodies with numerous named craters include Callisto (141), Ganymede (131), Rhea (128), Vesta (90), Ceres (90), Dione (73), Iapetus (58), Enceladus (53), Tethys (50) and Europa (41). For a full list, \"see List of craters in the Solar System\".\n\nSome of the largest craters on Mars remain unnamed. Diameters differ depending on source data.\n\n\n"}
{"id": "31668977", "url": "https://en.wikipedia.org/wiki?curid=31668977", "title": "List of iPad accessories", "text": "List of iPad accessories\n\nThe iPad is an iOS-based line of tablet computers designed and developed by Apple Inc.; it has a wide variety of accessories made by Apple available for it, including a screen cover specifically for the respective models of iPad called Smart Cover, as well as a number of accessories to allow the iPad to connect to other devices, some of which enable non-touchscreen input.\n\n"}
{"id": "2123336", "url": "https://en.wikipedia.org/wiki?curid=2123336", "title": "List of members of the National Academy of Sciences", "text": "List of members of the National Academy of Sciences\n\nThis list includes approximately 2,000 members and 350 foreign associates of the United States National Academy of Sciences, each of whom is affiliated with one of 31 disciplinary sections. Each person's name, primary institution, and election year are given.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49412547", "url": "https://en.wikipedia.org/wiki?curid=49412547", "title": "Ministry of Energy, Science, Technology, Environment and Climate Change (Malaysia)", "text": "Ministry of Energy, Science, Technology, Environment and Climate Change (Malaysia)\n\nThe Ministry of Energy, Science, Technology, Environment and Climate Change (MESTECC) (), is a ministry of the Government of Malaysia that is responsible for sustainable energy, green technology, water supply, sewerage treatment, renewable energy, water purification, air purification, environmental remediation, solid waste management, eGain forecasting, energy conservation, sustainable engineering.\n\n\n\n\nMESTECC is responsible for administration of several key Acts:\n\n\n\n\nThe objectives of the Ministry are:\n\n"}
{"id": "30230244", "url": "https://en.wikipedia.org/wiki?curid=30230244", "title": "Moderation theory", "text": "Moderation theory\n\nModeration theory is a set of interrelated hypotheses that explain the process through which political groups eschew radical platforms in favour of more moderate policies and prefer electoral, compromising and non-confrontational strategies over non-electoral, exclusive, and confrontational strategies. Moderation can take place at both ideological and behavioural levels that mutually reinforce each other. The origins of the theory go back to the work of Robert Michels who offers a classical study of the Social Democratic Party of Germany in his book \"Political Parties\". The theory offers insights into the transformation of party politics in a great range of cultural and historical cases including socialist and Christian democratic parties in Western Europe and more recently Islamic political groups. In particular, the evolution of Islamic political parties in Turkey since the early 1970s that culminated in the rise of the Justice and Development Party in the 2002 parliamentary elections exemplifies the dynamics highlighted by moderation theory.\n\nThe theory is composed of three causal mechanisms. First, once radical political groups are organized as vote-seeking parties, electoral considerations prevail and these groups abandon revolutionary agendas in favour of vote-maximizing strategies. This expectation is based on the median voter theorem. A second mechanism concerns the vulnerability of radical political groups participating in electoral contest to state repression. The logic of political survival necessitates that these groups avoid openly confronting state elites. The final mechanism involves the effects of organizational resources on group behaviour and suggests that the maintenance of electoral organization is prioritized over original political goals. Once radicals are organized as electoral parties, their original projects of revolutionizing the \npolitical system becomes unachievable simply because of the lack of organizational resources. While moderation of radicals is generally thought to be conducive to democratization, it can also hamper and even hinder democratic progress as radicals are co-opted into the ruling political system and lose their reformist characteristics.\n\nIn contemporary times, moderation theory is further developed and critically refined to understand the evolution of Islamic political parties in Muslim majority countries as diverse as Egypt, Jordan, Indonesia, Iran, and Turkey. The Center Party (\"Hizb al-Wasat\") of Egypt is example of a moderate Islamic organization that was not given license by the ruling regime. Moreover, Muslim Brotherhood of Egypt has transformed into an organization that is responsive to the logic of political competition and survival in an authoritarian regime at the cost of its original ideological commitments. Similarly, the Islamic Action Front of Jordan shows that Islamists can be moderate as a result of participation in pluralistic political process as long as this participation can be justified in Islamic terms.\n\n"}
{"id": "7029928", "url": "https://en.wikipedia.org/wiki?curid=7029928", "title": "National Botanical Research Institute", "text": "National Botanical Research Institute\n\nThe National Botanical Research Institute (NBRI) is a research institute of CSIR in Lucknow. It is engaged in the field of taxonomy and modern biology.\n\nOriginally conceptualised and set up as the National Botanic Gardens (NBG) by Professor Kailas Nath Kaul on behalf of the State Government of Uttar Pradesh, it was taken over by the CSIR in 1953. Dr Triloki Nath Khoshoo joined in 1964 as the Assistant Director, shortly afterwards becoming the Director. Initially engaged in research work in the classical botanical disciplines, the NBG went on laying an increasing emphasis in keeping with the national needs and priorities in the field of plant sciences, on its applied and developmental research activities. Due to the untiring efforts of Dr Khoshoo, the institute rose to the stature of being the National Botanical Research Institute in 1978, reflecting the correct nature and extent of its aims and objectives, functions and R & D activities. Sikandar Bagh is a famous and historic pleasure garden, located in the grounds of the Institute.\n\n\nNational Botanical Research Institute (NBRI) is also the state botanical research institute of South Africa.\n"}
{"id": "13320316", "url": "https://en.wikipedia.org/wiki?curid=13320316", "title": "Ozsváth–Schücking metric", "text": "Ozsváth–Schücking metric\n\nThe Ozsváth–Schücking metric, or the Ozsváth–Schücking solution, is a vacuum solution of the Einstein field equations. The metric was published by István Ozsváth and Engelbert Schücking in 1962. It is noteworthy among vacuum solutions for being the first known solution that is stationary, globally defined, and singularity-free but nevertheless not isometric to the Minkowski metric. This stands in contradiction to a claimed strong Mach principle, which would forbid a vacuum solution from being anything but Minkowski without singularities, where the singularities are to be construed as mass as in the Schwarzschild metric.\n\nWith coordinates formula_1, define the following tetrad:\n\nIt is straightforward to verify that e is timelike, e, e, e are spacelike, that they are all orthogonal, and that there are no singularities. The corresponding proper time is\n\nThe Riemann tensor has only one algebraically independent, nonzero component\n\nwhich shows that the spacetime is Ricci flat but not conformally flat. That is sufficient to conclude that it is a vacuum solution distinct from Minkowski spacetime. Under a suitable coordinate transformation, the metric can be rewritten as\n\nand is therefore an example of a pp-wave spacetime.\n"}
{"id": "7913725", "url": "https://en.wikipedia.org/wiki?curid=7913725", "title": "Rank abundance curve", "text": "Rank abundance curve\n\nA rank abundance curve or Whittaker plot is a chart used by ecologists to display relative species abundance, a component of biodiversity. It can also be used to visualize species richness and species evenness. It overcomes the shortcomings of biodiversity indices that cannot display the relative role different variables played in their calculation.\n\nThe curve is a 2D chart with relative abundance on the Y-axis and the abundance rank on the X-axis.\n\nThe rank abundance curve visually depicts both species richness and species evenness. Species richness can be viewed as the number of different species on the chart i.e., how many species were ranked. Species evenness is reflected in the slope of the line that fits the graph (assuming a linear, i.e. logarithmic series, relationship). A steep gradient indicates low evenness as the high-ranking species have much higher abundances than the low-ranking species. A shallow gradient indicates high evenness as the abundances of different species are similar.\n\nQuantitative comparison of rank abundance curves of different communities can be done using RADanalysis package in R. This package uses the max rank normalization method in which a rank abundance distribution is made by normalization of rank abundance curves of communities to the same number of ranks and then normalize the relative abundances to one.\n\n\n"}
{"id": "50053183", "url": "https://en.wikipedia.org/wiki?curid=50053183", "title": "Richard Pyle", "text": "Richard Pyle\n\nRichard Lawrence Pyle, Ph.D. is a scuba diver and ichthyologist working on Hawaii.\n\nPyle discovered the principle of \"Pyle stops\" when decompressing from many deep dives in search of new species of fish, and has identified hundreds of new species.\n\nHe is the author of over 130 publications.\n\nIn October 2015, he won second prize, an award of €5,000, in the GBIF Ebbe Nielsen Challenge, a Global Biodiversity Information Facility competition, for BioGUID.org, \"a web service that crosslinks identifiers linked to data objects in the biodiversity realm\". At that time, the site contained over one billion (1,000,000,000) identifiers.\n\nRichard Pyle is a member of ZooBank Committee and he is the leader of ZooBank architecture policy working group.\n\n"}
{"id": "3674522", "url": "https://en.wikipedia.org/wiki?curid=3674522", "title": "Rudolf Ruedemann", "text": "Rudolf Ruedemann\n\nRudolf Ruedemann (October 16, 1864–June 18, 1956) was a German American paleontologist, widely known as an expert in graptolites, enigmatic fossil animals. He worked at the New York State Museum for over 40 years, including a decade as State Paleontologist of New York. and was elected to the U.S. National Academy of Sciences in 1928. Born in Georgenthal, Germany, he was educated in Europe, earning a PhD in 1887 from the University of Jena (Ph.D., 1887), and a second doctorate in 1889 from France's University of Strasbourg where he was an assistant in geology from 1887 to 1892. He emigrated to the United States in 1892 and taught at the high schools of Lowville and Dolgeville, New York for several years before joining the State Museum in 1899, where he worked for the remainder of his career. Although his primary interests were in graptolites he also made contributions to other areas of invertebrate paleontology, describing new species of fossil corals, eurypterids (\"sea scorpions\"), trilobites, and cephalopods.\n\nHe was married with a daughter and six sons, and retired in 1937.\n"}
{"id": "47777152", "url": "https://en.wikipedia.org/wiki?curid=47777152", "title": "Shore", "text": "Shore\n\nA shore or a shoreline is the fringe of land at the edge of a large body of water, such as an ocean, sea, or lake. In physical oceanography, a shore is the wider fringe that is geologically modified by the action of the body of water past and present, while the beach is at the edge of the shore, representing the intertidal zone where there is one. In contrast to a coast, a shore can border any body of water, while the coast must border an ocean; in that sense a coast is a type of shore; however, coast often refers to an area far wider than the shore, often stretching miles into the interior.\n\nShores are influenced by the topography of the surrounding landscape, as well as by water induced erosion, such as waves. The geological composition of rock and soil dictates the type of shore which is created.\n\n\"Riviera\" is an Italian word for \"shoreline\", ultimately derived from Latin \"ripa\" (\"riverbank\"). It came to be applied as a proper name to the coast of the Ligurian Sea, in the form \"riviera ligure\", then shortened to \"riviera\". Historically, the Ligurian Riviera extended from Capo Corvo (Punta Bianca) south of Genoa, north and west into what is now French territory past Monaco and sometimes as far as Marseilles. Now it is divided into the Italian Riviera and the French Riviera, although the French use the term \"Riviera\" to refer to the Italian Riviera and call the French portion the \"Côte d'Azur\".\n\nAs a result of the fame of the Ligurian rivieras, the term came into English to refer to any shoreline, especially one that is sunny, topographically diverse and popular with tourists. Such places using the term include the Australian Riviera in Queensland and the Turkish Riviera along the Aegean Sea.\n\n\n\n"}
{"id": "12073433", "url": "https://en.wikipedia.org/wiki?curid=12073433", "title": "The Beginning or the End", "text": "The Beginning or the End\n\nThe Beginning or the End is a 1947 American docudrama film about the development of the atomic bomb in World War II, directed by Norman Taurog, starring Brian Donlevy and Hume Cronyn, and released by Metro-Goldwyn-Mayer. The film dramatizes the creation of the atomic bomb in the Manhattan Project and the bombing of Hiroshima.\n\nThe film originated in October 1945 as a project of actress Donna Reed and her high school science teacher, Edward R. Tompkins, who was a chemist at the Oak Ridge National Laboratory. Bob Considine wrote the treatment, which was sent to MGM script writers. The title was supplied by President Harry S. Truman. At the time there was a legal requirement that permission be obtained to depict living well-known public figures. Many refused, but others, such as J. Robert Oppenheimer, co-operated. Major General Leslie R. Groves, Jr., the director of the Manhattan Project, was hired as a consultant for $10,000 ().\n\nAlthough the filmmakers put considerable effort into historical accuracy, particularly in details, the film is known for some key distortions of history. An entirely fictional sequence was added in which Truman agonizes over whether to authorize the attack; anti-aircraft shells are shown bursting around the \"Enola Gay\" on its bombing run over Hiroshima; and it is said that leaflets were dropped on Hiroshima for ten days in advance of the mission warning the citizens of the forthcoming raid. The film received generally mixed reviews, and was a box office disappointment.\n\nIn 1945, physicist and atomic scientist Dr. J. Robert Oppenheimer (Hume Cronyn) praises the discovery of atomic energy, but also warns of its dangers. American scientists such as Matt Cochran (Tom Drake), working under the guidance of Dr. Enrico Fermi (Joseph Calleia) and Dr. Marré (Victor Francen), have split the atom, and essentially beaten the Germans in the race to create an atomic bomb. With the assistance of Albert Einstein (Ludwig Stössel), they inform President Franklin D. Roosevelt (Godfrey Tearle) that a monumental discovery has been made.\n\nIn 1941, with the United States at war, Roosevelt authorizes up to two billion dollars for the Manhattan Project to develop an atomic bomb. In December 1942, at the University of Chicago, under the watchful eyes of observers such as Lieutenant Colonel Jeff Nixon (Robert Walker) and international experts, scientists create the first chain reaction, under a stadium at the campus.\n\nNixon is assigned to General Leslie Groves (Brian Donlevy), who is placed in charge of the project. Groves has to bring together the scientific, industrial and defense communities to build the atomic bomb. In 1945, following the death of Roosevelt, the new president, Harry S. Truman (Art Baker), continues to support the atomic project, now moved to Los Alamos, New Mexico. When refined uranium-235 is obtained, the first atomic bomb is built and tested successfully in the New Mexico desert. Facing stiff resistance in the Pacific War, Truman orders the use of the atomic bomb against Japan in July 1945.\n\nCochran and Nixon are assigned to accompany the crew transporting the bomb to Tinian. While assembling the bomb, the scientist comes into contact with radioactive material and dies. The following day, on August 6, 1945, the \"Enola Gay\", a Boeing B-29 Superfortress bomber, drops an atomic bomb on Hiroshima. After the mission, Nixon returns home to break the news of her husband's death to Cochran's wife.\n\nThe idea for \"The Beginning or the End\" originated in October 1945 with actress Donna Reed, and her high school science teacher, Edward R. Tompkins, a chemist at the Oak Ridge National Laboratory. According to \"The Hollywood Reporter\" issues of December 1945 and January 1946, MGM, Paramount and 20th Century-Fox were all interested in making a film about the Manhattan Project. Paramount's Hal B. Wallis was already working on his own version, titled \"Top Secret\", but agreed to merge his project with MGM's and hand over his story and research, offering to serve as an adviser on the MGM treatment in return for a fixed fee and a percentage of the box office gross.\n\n\"The Beginning or the End\" had a number of working titles, including \"Atom Bomb\", \"The Manhattan Project\" and \"Top Secret\". Bob Considine was hired to produce a treatment, which was submitted to MGM writers. The script underwent a number of revisions, with Ayn Rand being one of the writers. Her contributions include the montage of Hitler's conquests, a sequence in which a dying informant sends a message to Albert Einstein, and the sequence in which President Franklin Roosevelt authorizes the Manhattan Project. Other writers involved with the script were Robert Smith, Frank \"Spig\" Wead, Norman Krasna, David Hawkins, John Lee Mahin and Glenn Tryon. Producer Samuel Marx wrote the opening narration.\nMarx and Donna Reed's husband Tony Owen met with President Harry S. Truman to secure his approval. At their meeting, Truman is reported to have said: \"Gentlemen, make a motion picture. Tell the people of this nation that for them it is the beginning or the end,\" thereby supplying the movie with its title.\n\nH. T. Wensel from the National Bureau of Standards, Tompkins, and W. Bradford Shank from the Los Alamos National Laboratory acted as technical advisers. Relations between MGM and the scientists soon soured, as the scientists began asking for more accuracy which required multiple script changes, and Tompkins eventually resigned. Oppenheimer sent David Hawkins, a philosophy professor from the University of California to act as a mediator between Marx and the scientists. Although the original intention was that a substantial sum of money would be donated to scientists' associations like the Federation of Atomic Scientists, in the end, no scientific organizations accepted any money. Tompkins received payment of one hundred dollars. (). At the time, there was a legal requirement that permission be obtained to depict living well-known public figures in films. Lise Meitner, Niels Bohr and Sir James Chadwick all refused to allow their names to be used in \"The Beginning or the End\", which Marx regarded as unfortunate, as it made the film's Manhattan Project scenes look like an all-American affair.\n\nThe loss of Bohr caused important sequences to be deleted. The script originally had Bohr, rescued from the Germans in Denmark, bring a shocked Oppenheimer news that the German nuclear weapon project was supplying expertise to its Japanese counterpart. A German submarine carrying a fictional scientist travels to Japan where he joins the Japanese project in Hiroshima. Vannevar Bush objected to the way the script depicted him as having doubts about whether the atomic bomb could be built in time or could fit into an aircraft. Bush insisted that he never had any doubts. The script was changed to soften this. In the film, when Bush tells President Roosevelt that he has a top secret matter to discuss, the President's dog Fala leaves the room.\n\nOppenheimer raised no objection to the sequence in the film in which he informed Brigadier General Thomas Farrell that the odds of a runaway explosion destroying the planet were less than one in a million, although he told MGM that he never said this. The cultured Oppenheimer's main concern was that the script was poor, with characters that were \"stilted, lifeless, and without purpose or insight.\"\nMilitary technical advisers for \"The Beginning or the End\" included Colonel William A. Considine, Groves's assistant in charge of in charge of Security and Public Relations, Major Glen W. Landreth, Major Paul Van Sloun and Lieutenant Colonel Charles W. Sweeney, the pilot of \"Bockscar\", the bomber that dropped the second atomic bomb on Nagasaki. Scientists were alarmed by reports that MGM leading man Clark Gable was being considered for the role of Groves, but were relieved when Brian Donlevy was cast instead. Donlevy usually appeared in villainous supporting roles and indeed, most of the actors cast in the movie were best known for film noir : Hume Cronyn for \"The Postman Always Rings Twice\"; Joseph Calleia, for \"Gilda\" and \"Deadline at Dawn\"; and Ludwig Stössel for Fritz Lang's \"Cloak and Dagger\". Groves's cooperation was secured by hiring him as a primary consultant, for $10,000 (). The portly Groves apparently had no objection to his portrayal by the slim and handsome Donlevy, except for the way in which he was shown bossing industrialists around. He had a scene in which he warned Roosevelt that the invasion of Japan would be opposed by Japanese nuclear weapons deleted.\n\nEleanor Roosevelt objected to the casting of Lionel Barrymore as her late husband, due to political remarks that Barrymore had made about the president in 1944. Marx delayed Barrymore's scenes while she had a chance to read and respond to a letter Barrymore sent her explaining that his remarks had been misinterpreted, but she was not placated, and Barrymore was replaced in the role by Godfrey Tearle. The War Department and the White House reviewed the script, and both asked for changes. The Army had objected to a scene in which an Army major made a pass at a girl and it was cut from the film, as the Army felt that this was poor conduct for an officer. \n\nThe casual way that Truman and Groves were shown to decide to use the bomb, with Truman stating that \"I think more of our American boys than I do of all our enemies\", while accurate, troubled Walter Lippmann, who felt that it could lead to foreigners being fearful of atomic weapons being in American hands. It was replaced with a scene where Truman agonizes over whether to authorize the attack or not. In it, Truman asserts that dropping the bomb will shorten the war, and a \"year less of war will mean life for ... from 300,000 to half a million of America's finest youth\".\n\nThe motion picture censors asked for further cuts. Derogatory references to Mexicans were removed, as was an off-color joke about the effects of exposure to radioactive substances (\"Is it true if you fool around with that stuff you don't like girls anymore?\" \"Not that I've noticed\"), and one about politics (\"I got it confidential−we're makin' the front ends of horses. We ship 'em to Washington to hook on to the other end.\")\n\nPrincipal photography for \"The Beginning or the End\" began on April 29, 1946, and continued until July 25 with retakes beginning on August 9, 1946. The production premiered in Washington, D.C. on February 19, 1947, with the national release of the film following on March 7, 1947.\n\nThe filmmakers put considerable effort into many details for historical accuracy, such as military uniforms, and the details of the \"Enola Gay\" and its crew. Nine of the actors who portrayed the \"Enola Gay\" crew were actual veterans of World War II. However, the technical details of atomic processes and the bomb's design are wildly inaccurate by intention. In 1947, these details were highly classified. Another inaccuracy, independent of necessary military secrecy, is the portrayal of anti-aircraft shells bursting around the aircraft on the bombing run, as the attack on Hiroshima was not opposed.\n\nThe film twice refers to supposed specific leaflet drops on the target for ten days in advance of the mission warning the citizens of the forthcoming raid. \"We've been dropping warning leaflets on them for ten days now\", one crew member remarks, \"That's ten days more warning than they gave us before Pearl Harbor.\" However, there was no leaflet specifically warning of an atomic attack. In his review in the \"Bulletin of the Atomic Scientists\", physicist Harrison Brown called this \"the most horrible falsification of history\". Historians have debated whether any leaflets were dropped at all.\n\nAlthough \"The Beginning or the End\" was the first film to depict the story of the atomic bomb, both critics and the public were confused by the attempt to merge real events in a docudrama form. Bosley Crowther of \"The New York Times\" commented, \"... despite its generally able reenactments, this film is so laced with sentiment of the silliest and most theatrical nature that much of its impressiveness is marred.\" \"Variety\" described the film as a \"portentous tale in broad strokes of masterful scripting and production\", and a \"sum credit of everybody concerned that the documentary values are sufficiently there without becoming static\". In his \"Bulletin of the Atomic Scientists\" review, Harrison Brown considered the movie \"poor\", with a romantic angle \"insipid in the extreme\", but was most troubled by way scientific equipment was \"over-glamorized\" in the film, which he felt gave \"a completely false impression of how scientists work.\" The review in Time was less positive, noting that, \"even as entertainment ... the picture seldom rises above cheery imbecility.\"\n\nAccording to MGM records, \"The Beginning or the End\" was made on a budget of $2,632,000 (), but earned $1,221,000 () in the United States and Canada and $721,000 () elsewhere, resulting in a loss to the studio of $1,596,000 ().\n\n\n"}
{"id": "40594008", "url": "https://en.wikipedia.org/wiki?curid=40594008", "title": "The Planet That Wasn't", "text": "The Planet That Wasn't\n\nThe Planet That Wasn't is a collection of seventeen scientific essays by Isaac Asimov. It was the twelfth of a series of books collecting essays from \"The Magazine of Fantasy and Science Fiction\". These essays were first published between December 1974 and April 1976. It was first published by Doubleday & Company in 1976.\n\n\n"}
{"id": "17426098", "url": "https://en.wikipedia.org/wiki?curid=17426098", "title": "Theodore Paraskevakos", "text": "Theodore Paraskevakos\n\nTheodore George “Ted” Paraskevakos (; born March 25, 1937, in Athens, Greece) is a Greek-American inventor and businessman. Paraskevakos graduated from the Superior College of Electronics in Greece and served for 28 months as communications and electronics instructor in the Hellenic Air Force. He attended a variety of courses for digital engineering in Alabama and in Florida.\nParaskevakos' most notable inventions relate to the transmission of electronic data through telephone lines which formed the original basis for what is now known as caller ID. Paraskevakos began his work in this field in 1968 while working as a communications engineer with SITA and has since been issued over 20 patents worldwide based on this technology. His transmitter and receiver were put into practice in 1971 in a Boeing facility in Huntsville, Alabama.\n\nParaskevakos holds over 50 patents worldwide including a digital alarm communication system, which also covered handheld or portable cardiac alarms automatic meter reading and load management, digital vending machine communications, indoor archery, vertical parking, intelligent currency validation network, and a method for identification of currency used in unlawful activity. He founded, among other companies, Metretek, Inc., DataVend, Inc. and Intelligent Currency Validation Network, Inc.\n"}
{"id": "2398982", "url": "https://en.wikipedia.org/wiki?curid=2398982", "title": "Univariate", "text": "Univariate\n\nIn mathematics, univariate refers to an expression, equation, function or polynomial of only one variable. Objects of any of these types involving more than one variable may be called multivariate. In some cases the distinction between the univariate and multivariate cases is fundamental; for example, the fundamental theorem of algebra and Euclid's algorithm for polynomials are fundamental properties of univariate polynomials that cannot be generalized to multivariate polynomials.\n\nThe term is commonly used in statistics to distinguish a distribution of one variable from a distribution of several variables, although it can be applied in other ways as well. For example, univariate data are composed of a single scalar component. In time series analysis, the term is applied with a whole time series as the object referred to: thus a univariate time series refers to the set of values over time of a single quantity. Correspondingly, a \"multivariate time series\" refers to the changing values over time of several quantities. Thus there is a minor conflict of terminology since the values within a univariate time series may be treated using certain types of multivariate statistical analyses and may be represented using multivariate distributions.\n\n"}
{"id": "742024", "url": "https://en.wikipedia.org/wiki?curid=742024", "title": "William Gascoigne (scientist)", "text": "William Gascoigne (scientist)\n\nWilliam Gascoigne (1612 – 2 July 1644) was an English astronomer, mathematician and maker of scientific instruments from Middleton, Leeds who invented the micrometer. He was one of a group of astronomers in the north of England who followed the astronomy of Johannes Kepler which included, Jeremiah Horrocks and William Crabtree.\n\nGascoigne was born in Middleton, Leeds in 1611, the son of a minor country gentleman. His father was Henry Gascoigne, Esq., of Thorpe-on-the-Hill in the parish of Rothwell, near Leeds, Yorkshire. His mother was Margaret Jane, daughter of William Cartwright. Little is known of his early life. He claimed he was educated at the University of Oxford, although no record of this has been found.\n\nIn the late 1630s, Gascoigne, was working on a Keplerian optical arrangement when a thread from a spider’s web happened to become caught at exactly the combined optical focal points of the two lenses. When he looked through the arrangement Gascoigne saw the web bright and sharp within the field of view. He realized that he could more accurately point the telescope using the line as a guide, and went on to invent the telescopic sight by placing crossed wires at the focal point to define the centre of the field of view. He then added this arrangement to a sextant modelled on the instrument used by Tycho Brahe, although Tycho’s sextant was only a naked-eye instrument. Gascoigne's sextant was five feet in radius, and measured the distance between astronomical bodies to an unprecedented degree of accuracy. Gascoigne then realised that by introducing two points, whose separation could be adjusted using a screw, he could measure the size of the image enclosed by them. Using the known pitch of the screw, and knowing the focal length of the lens producing the image, he could work out the size of the object, such as the Moon or the planets, to a hitherto unattainable degree of accuracy.\n\nGascoigne met the Lancashire astronomer William Crabtree, probably in 1640. After making observations at Gascoigne's home, Crabtree was much taken with these inventions and immediately saw their significance. On his return to his home in Broughton, just outside Manchester, he wrote to Gascoigne asking if he might obtain such instruments and also wrote to his friend Jeremiah Horrocks about them. He wrote again to Gascoigne on 28 December 1640 saying, \nMy friend Mr. Horrox professeth that little touch which I gave him hath ravished his mind quite from itself and left him in an Exstasie between Admiration and Amazement. I beseech you Sir, slack not your Intentions for the Perfection of your begun Wonders.\n\nThis invention was later taken up and improved by the scientist and astronomer Richard Towneley who was the nephew of Gascoigne's friend Christopher Towneley. Towneley later brought the instrument to the attention of Robert Hooke, who used it to calculate the size of comets and other celestial bodies. The micrometer, as it became known, was to lie at the heart of astronomical measurement down to the twentieth century.\n\nIn 1642, civil war broke out in England, and Gascoigne received a commission as Providore for Yorkshire in the army of King Charles I. Crabtree lived in Broughton, just outside Manchester which was on the parliamentary side and all correspondence between the two ceased.\n\nGascoigne died at the Battle of Marston Moor, Yorkshire, on 2 July 1644 as did Charles Towneley, the father of his friend Richard Towneley.\n\nAfter Gascoigne's death some of his papers and fragments of correspondence between Crabtree and Gascoigne came into the possession of Christopher and Richard Towneley. They brought them to the attention of John Flamsteed, the first Astronomer Royal, who came to see Horrocks, Crabtree, and Gascoigne as the founding fathers of British research astronomy and the intellectual heirs of Galileo and Kepler. He began his massive three-folio volume \"Historia Coelestis Britannica\" (1725) by printing five pages of their surviving letters and observations, made between 1638 and 1643.\n\nMany of Gascoigne's papers and correspondence were lost during the English Civil War and later in the Great Fire of London, but most of what is known to remain is kept in the Bodleian Library at the University of Oxford.\n\nIn March 2018 Leeds Civic Trust unveiled a blue plaque in the city honouring Gascoigne. It was unveiled, David Sellers, who has written a biography of Gascoigne, who said:\n\nAlthough his name is known by astronomers, his role as a pioneer in precision astronomy deserves wider public recognition. I hope that this plaque will help to achieve this and will encourage young people to follow his lead and inspire an interest in the natural world.\n\nLocal MP Hilary Benn was also present.\n\n"}
{"id": "6344374", "url": "https://en.wikipedia.org/wiki?curid=6344374", "title": "Yvonne Cagle", "text": "Yvonne Cagle\n\nYvonne Darlene Cagle (born April 24, 1959) is an American astronaut.\n\nBorn in West Point, New York, Yvonne Cagle graduated from Novato High School in Novato, California. She received her bachelor's degree in biochemistry from San Francisco State University in 1981, and a doctor of medicine degree from the University of Washington in 1985. She completed a transitional internship at Highland General Hospital in Oakland, California in 1985 and received a certificate in Aerospace Medicine from the School of Aerospace Medicine at Brooks Air Force Base, Texas, in 1988. She then went on to complete a residency in family practice at Ghent FP at Eastern Virginia Medical School in 1992 and received certification as a senior aviation medical examiner from the Federal Aviation Administration in 1995.\n\nCagle retired from the United States Air Force with the rank of Colonel in 2008. In May 1989 as a commissioned medical officer assigned to the 48th Tactical Hospital, United Kingdom, Cagle served as Air Force Medical Liaison Officer for STS-30 mission to test the Magellan spacecraft, before she became a NASA astronaut. She worked as medical doctor at NASA's Occupational Health Clinic from 1994 to 1996. In 1996 she was selected for astronaut training by NASA.\n\nYvonne Cagle was a member of the Astronaut Class of 1996 (NASA Astronaut Group 16). She is currently assigned to Johnson Space Center's Space and Life Sciences Directorate.\n\nCagle is also an advisor for NASA's Flight Opportunities Program (originally named CRuSR – Commercial Reusable Suborbital Research Program). Currently Dr. Cagle is on faculty and serves as the NASA liaison for exploration and space development with Singularity University. During the workshop, Dr. Cagle was embedded with the crew as a crew training consultant and advisor, providing insights and feedback to both crew and study team from the viewpoint of an astronaut, flight surgeon, space development expert, and science liaison.\n\nShe has recently been selected reserve crew for Hawai'i Space Exploration Analog and Simulation (HI-SEAS), which is part of a study for NASA to determine the best way to keep astronauts well nourished during multiple-year missions to Mars or the moon. Furthermore, Dr. Cagle is also listed as an honorary member of the Danish Astronautical Society.\n\nIn 2014, Cagle was a visiting professor to Fordham University where she was participating in interdisciplinary research in health, environment and human Performance. She was awarded an honorary Ph.D. by Fordham University for her substantial and significant contributions to the fields of science, technology and human health.\n\nAs of June 2018, Cagle is considered a \"NASA Management Astronaut\", which means that she is employed at NASA but is no longer eligible for spaceflight assignments.\n\nIn 2017, she brought Katherine Johnson onto the stage at the Academy Awards.\n\n\n\n"}
{"id": "486565", "url": "https://en.wikipedia.org/wiki?curid=486565", "title": "Zacharias Janssen", "text": "Zacharias Janssen\n\nZacharias Janssen (also Zacharias Jansen or Sacharias Jansen) (1585 – pre-1632) was a Dutch spectacle-maker from Middelburg associated with the invention of the first optical telescope. Janssen is sometimes also credited for inventing the first truly compound microscope. However, the origin of the microscope, just like the origin of the telescope, is a matter of debate.\n\nZacharias Janssen was born in The Hague. Local records seem to indicate he was born in 1580 although a date of birth as early as 1580 or as late a 1588 are also given. His parents were Hans Martens (who may have had the occupation of a peddler) and Maeyken Meertens, both probably from Antwerp, Belgium. He grew up with his sister Sara in Middelburg, at the time the second most important city of the Netherlands. He was known as a \"street seller\" who was constantly in trouble with the local authorities.\n\nBut he stated he was born in The Hague on the marriage file of his first marriage, with Catharina de Haene, on October 23, 1610. When this file was refound by Cornelis de Waard in 1906, De Waard found the following excerpt: \"Sacharias Jansen, j.g. uut Den Haag\", \"Zacharias Jansen, bachelor from The Hague\" Before, it was often thought that Janssen was a native of Middelburg. In 1612, Zacharias and Catharina had a son they named Johannes Zachariassen.\n\nIn 1615 Zacharias was appointed guardian of two children of Lowys Lowyssen \"geseyt Henricxen brilmakers\" (\"called Henry the spectacle maker\"). It is surmised that Zacharias also took possession of Lowys Lowyssen's spectacle-making tools because the first record of Zacharias Janssen being a spectacle maker appears in 1616. The family had to move to Arnemuiden in 1618 after Zacharias's counterfeiting activities were exposed. There Zacharias was again accused of counterfeiting in 1619 causing him to be on the move again, ending up back in Middleburg in 1621.\n\nA year after the death of Janssen's first wife in 1624, he married Anna Couget from Antwerp, who was the wife of Hans lippershey (probably father of Janssen). He moved to Amsterdam in November 1626 with a profession of a spectacle maker, but was bankrupt by 1628. Janssen has been given a death date as late as 1638 although his sister said he was dead in 1632 testimony and his son Johannes declared his parents had died by the time of his marriage in April 1632.\n\nOver the years of existence they have been claim Zacharias Janssen who read the telescope and or the microscope in Middelburg in between 1580 to 1800. Zacharias worked for some period of his life as spectacle-maker (a very competitive and secretive trade) and at one time lived next door to Middelburg spectacle maker Hans Lippershey, also claimed to have invented the telescope. Janssen's attribution to these discoveries is debatable since there is no concrete evidence as to the actual inventor, and there are a whole series of confusing and conflicting claims from the testimony of his son and fellow countrymen.\n\nThe claim that Zacharias Janssen invented the telescope and the microscope dates back to the year 1655. During that time Dutch diplomat Willem Boreel conducted an investigation trying to figure out who invented the telescope. He had a local magistrate in Middelburg follow up on a 45 year old recollection of a spectacle maker named \"Hans\" who told a young Boreel in 1610 about inventing the telescope. In his investigation the magistrate was contacted by a then unknown claimant, Middelburg spectacle maker Johannes Zachariassen, the son of Zacharias Janssen, who testified under oath that his father invented the telescope and the microscope as early as 1590 and that Hans Lippershey had stolen his father's invention of the telescope. This testimony seemed to convincing to Boreel, who modified his recollections, concluding that Zacharias must have been who he remembered. Boreel's conclusion that Zacharias Janssen invented the telescope a little ahead of spectacle maker Hans Lippershey was adopted by Pierre Borel in his 1656 book on the subject. \n\nIn Boreel's investigation Johannes also claimed his father, Zacharias Jansen, invented the compound microscope in 1590. This pushes the date so early it is sometimes assumed, for the claim to be true (Zacharias most likely dates of birth would have made him 2-5 years old at the time) grandfather Hans Martens must have invented it.\n\nOther claims have come forward over the years. Physicist Jean Henri van Swinden's 1822-23 investigation reached the conclusion supporting Janssen and in 1841 a collector named Zacharias Snijder came forward with 4 iron tubes with lenses in them purported to be Janssen original telescopes. In historian Cornelis de Waard's 1906 book on the history of the telescope he recounted his discovery of note written in 1634 by the Dutch philosopher Isaac Beeckman in which Beeckman mentioned that Johannes Zachariassen claimed his father created his first telescope in 1604 (and that it was a copy of an Italian device from 1590). The German astronomer Simon Marius's account to his patron Johan Philip Fuchs von Bimbach about meeting an unnamed Dutchman at the 1608 Autumn Frankfurt Fair who tried to sell him a device that sounded like a broken telescope has led to later speculation this unnamed Dutchman could have been Zacharias Janssen.\n\nThe confusion surrounding the claim to invention of the telescope and the microscope arises in part from the (sometimes conflicting) testimony of Zacharias Janssen's son, Johannes Zachariassen. Johannes claims include that his father invented the telescope in 1590, that his father invented the telescope in 1604, that he and his father invented the telescope in 1618, and that Jacob Metius and Cornelis Drebbel bought a telescope from him and his father in 1620 and copied it. Johannes also seems to have lied about his own date of birth, maybe so he could stake his own claim as inventor of the telescope along with his father.\n\nThe 1655 investigation by William Boreel (who may have been a childhood friend of Zacharias Zachariassen) added to the confusion over invention. The people he had the local magistrate interview were trying to recount details 50 or 60 years after the fact and Boreel may have confused the names of spectacle makers from his childhood. He may have also been confused about a microscope built by another optician for Drebbel, claiming it was built by Zacharias Janssen.\n\nAn investigation begun in 1816 in preparation for a memorial to commemorate Janssen as the inventor of the telescope and microscope turned up further problems with the claim including the Lippershey and Metius patent applications, Janssen late 1585 date of birth, and no record of him being a spectacle maker before 1615. \n\nAlbert Van Helden, Sven Dupré, Rob Van Gent, and Huib Zuidervaart in their book \"Origins of the Telescope\" came to the conclusion that Janssen may not have become an optician until 1616 and that the claims surrounding him as the inventor of the telescope and the microscope were the fabrications of his own son, Johannes Zachariassen, who claimed it as a matter of fame and for possible financial gain.\n\nIn the years 1613–1619, Janssen was tried several times for counterfeiting coins. Janssen grew up right next to the Middleburg mint where his brother-in-law worked. These circumstances made it very easy for Janssen to mimic the process of manufacturing money. He fled to the neighbouring village of Arnemuiden to avoid the high penalties for counterfeiting coins.\n\nHowever, he continued counterfeiting coins in Arnemuiden. In 1619 he was apprehended for owning several devices he counterfeited coins with. Normally, one would have been sentenced to death for this crime. However, since the father of the Arnemuiden bailiff was found to be an accessory, he was pardoned from this punishment. Thanks to this, the process was delayed to such an extent that Janssen was able to flee again. Eventually, the case was dismissed. Janssen returned to Middleburg in 1621.\n\nJanssen's life was documented by the many investigations on the subject before the Second World War. Many of the Middelburg archives were destroyed by a bombing of Middelburg on May 17, 1940, during the Nazi invasion of the Netherlands. Without these earlier studies, very little would be known of Janssen's life at all because all original files were lost in the fires following the bombardment.\n\n"}
