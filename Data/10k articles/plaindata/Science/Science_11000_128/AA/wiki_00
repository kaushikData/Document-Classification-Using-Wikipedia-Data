{"id": "8247796", "url": "https://en.wikipedia.org/wiki?curid=8247796", "title": "A606 road", "text": "A606 road\n\nThe A606 is an A road in England that starts in West Bridgford and heads southeastwards through Leicestershire and the towns of Melton Mowbray and Oakham, terminating at Stamford, Lincolnshire on the former Great North Road.\n\nThe route in earlier times was a far more important route than it is today. It was the main road from Nottingham to London. The turnpike went from Nottingham via Melton and Oakham to Kettering (to join today's A6), and then on to London, run by the Nottingham, Melton and Kettering Turnpike Trust from 1758. The turnpike trusts were abolished in 1880, the year that the railway line to Melton opened. The trains would run until 1968.\n\nThe road begins a few hundred yards south of Nottinghamshire County Hall (built in 1937), in West Bridgford in the district of Rushcliffe, at the traffic lights junction of the A60 (for Loughborough) and the B679 (for Wilford). The section to Melton follows the former railway from Nottingham to Melton, now the Old Dalby Test Track, and to the A46 junction is only a few hundred metres apart. The line then continued from Melton to Oakham, then on to Corby and Kettering, and was the fastest route to Nottingham by train from St Pancras. It followed the exact line of the former turnpike.\n\nIt begins as \"Melton Road\", and passes two churches then crosses the former railway line to Melton, and the BP \"Melton Road Filling Station\". There is traffic lights at a crossroad for \"Valley Road\", to the left, and \"Boundary Road\", to the right (for Rushcliffe Leisure Centre and Rushcliffe School). In Edwalton, the road then crosses the former railway, where a few hundred metres further south it is still accessible by train. To the right is Wheatcrofts garden centre (started by Harry Wheatcroft) and the road meets the A52 at a busy roundabout, where the road exits to the east as a trunk road.\nThere is a 14 ft 6 in low bridge (the former railway), so there is a turning point for high vehicles. For the next mile the road is the parish boundary between Tollerton to the left, and Plumtree, to the right. It passes Tollerton post office and there is traffic lights for \"Tollerton Lane\" (for Nottingham Airport), where there is the TOTAL \"Lane End Garage\". The road becomes the parish boundary between Plumtree and Normanton-on-the-Wolds, to the left. The former A606 used to go through both villages before 1930. At the end of the joint bypass, there is a right turn for the British Geological Survey. It passes through Stanton-on-the-Wolds. At the junction for Keyworth, there is the Murco \"Wolds Service Station\". At a crossroads, there is access to Widmerpool, to the right, and the former Widmerpool railway station. The road meets the dual-carriageway A46 at an interchange. This point is the southern end of the A46 Newark to Widmerpool Improvement.\nAt Hickling Pastures, it passes Turnpike Farm, and there is a left turn for Hickling. The road becomes more hilly, and it passes through Upper Broughton and its \"Golden Fleece\". Crossing the Dalby Brook, the road enters Leicestershire and the district of Melton. In Broughton and Old Dalby, it passes through Nether Broughton, and its \"Anchor Inn\" and \"Red House\". The road is crossed by a pylon line, and ascends Broughton Hill where the road rises 80 metres in 0.5 km.\nOn the top of the hill, there are crossroads, which is highest point of the road at 171 metres, for a former Roman road (\"Six Hills Lane\") that follows the northern ridge of the wolds, where the right turn is for Wartnaby. The road descends down a hill into Ab Kettleby, the former home of Desert Orchid, passing the \"Sugar Loaf\" on the left. The road descends down the side of a valley and up onto Potter Hill. At this point the road enters the parish of Melton Mowbray as \"Nottingham Road\". It passes the Esso \"Brookside Service Station\" and the headquarters of East Midlands Councils, and Melton Borough Council on the right. It meets the A607 and A6006 at traffic lights and crossroads. Previously the road went straight through the town centre, which is now pedestrianised as \"Nottingham Street\". The southbound road now follows the A607 to the east on \"Norman Way\", then follows \"Thorpe End\" and \"Sherrard Street\" to the west, in a convoluted route through the town centre. The northbound route follows \"Leicester Street\" then \"Wilton Road\". Both meet at an awkward junction at the \"Anne of Cleves\", onto \"Burton Road\".\n\nThe road passes Melton Mowbray railway station, and crosses the Birmingham to Peterborough Line and the River Eye. The road climbs the side of the River Eye's valley out of Melton, passing the former King Edward VII School. In Burton and Dalby it passes through Burton Lazars, and St James church, and is crossed by the Jubilee Way. At crossroads there is a left turn for Whissendine, and a right turn for Little Dalby. In Somerby at the top of Leesthorpe Hill, there is the \"Leesthorpe Crossroads]\", with a right turn for Leesthorpe and Pickwell. The road climbs to the top of a hill, where it becomes the boundary between Rutland (to the left) and Leicestershire (to the right), reaching 160 metres at Green's Lodge. Rutland became independent in April 1997. Leaving the Rutland boundary, the road descends to reach Whissendine Brook, and there is a left turn for Whissendine, and a right turn for Cold Overton and Northfield Farm (both in Leicestershire).\n\nIn Langham it ascends the side of Ranksborough Hill (at 191 metres, the second highest in Rutland), to reach 166 metres. Descending down the hill, it passes a right turn for Ranksborough Hall, an activity centre. In Langham, there is an abrupt turn to the left and one to the right, where it meets a road from Cold Overton. Ruddles Brewery was based here before 1997 - the year Rutland finally became independent. As \"Oakham Road\" it meets a roundabout for the bypass in Barleythorpe. The former route through Oakham is now the B640. The £11.6 million bypass opened on Wednesday 10 January 2007, with construction having started in October 2005. The contract had been awarded to Alfred McAlpine Civil Engineering in June 2003. The next roundabout is for the Lands' End clothing company. It crosses the Birmingham to Peterborough Line, and there is a roundabout for the B668 (\"Burley Road\"), close to a Midlands Co-op superstore.\n\nThe bypass is crossed by the Hereward Way, and at the A6003 roundabout the A6003 leaves to the south for Uppingham, and the A606 leaves to the left (east). From here to Barnsdale, the road is followed by the Viking Way and the Macmillan Way. There is a right turn for Hambleton (and Hambleton Hall), which is the former route of the road. When Rutland Water was built, the A606 was diverted to the north. The road passes on the north shore of Rutland Water, and the southern edge of Burley Wood. In the parish of Whitwell, at Barnsdale crossroads, there is a right turn for Barnsdale Hall Hotel and Country Club, and Barnsdale Lodge. The road passes through Whitwell, where it is crossed by the Viking Way, and passes \"The Noel\" (\"Noel Arms\"). The road reaches the end of Rutland Water, the largest reservoir (by surface area) in the UK, owned by Anglian Water, and passes through Empingham, where it is crossed by the Rutland Round. It crosses the River Gwash, and is crossed by the Hereward Way. At Tinwell it meets the A1 at an interchange built in 1960. It enters Lincolnshire, South Kesteven, and Stamford as \"Empingham Road\" losing its trunk road status, passing the Malcolm Sargent primary school (former Exeter secondary modern school), on the left, and the \"Danish Invader\", on the right. There is a right turn for \"Roman Bank\" (former Ermine Street) and it reaches its terminus at \"Scotgate\" - the former Great North Road (B1081).\n\n"}
{"id": "1698066", "url": "https://en.wikipedia.org/wiki?curid=1698066", "title": "Abuse of notation", "text": "Abuse of notation\n\nIn mathematics, abuse of notation occurs when an author uses a mathematical notation in a way that is not formally correct but that seems likely to simplify the exposition or suggest the correct intuition (while being unlikely to introduce errors or cause confusion). However, the concept of formal correctness depends on time and on the context. Therefore, many notations in mathematics are qualified as abuse of notation in some context and are formally correct in other contexts; as many notations were introduced a long time before any formalization of the theory in which they are used, the qualification of abuse of notation is strongly time dependent. Moreover, many abuses of notation may be made formally correct by improving the theory. \"Abuse of notation\" should be contrasted with \"misuse\" of notation, which should be avoided.\n\nMany mathematical objects consist of a set, often called the underlying set, equipped with some additional structure, typically a mathematical operation or a topology. It is a common abuse of notation to use the same notation for the underlying set and the structured object. For example, formula_2 may denote the set of the integers, the group of integers together with addition, or the ring of integers with addition and multiplication. In general, there is no problem with this, and avoiding such an abuse of notation would make mathematical texts pedantic and difficult to read. When this abuse of notation may be confusing, one may distinguish between these structures by denoting formula_3 the group of integers with addition, and formula_4 the ring of integers.\n\nSimilarly, a topological space consists of a set (the underlying set) and a topology formula_5 which is characterized by a set of subsets of (the open sets). Most frequently, one considers only one topology on , and there is no problem to denote by both the underlying set, and the pair consisting of and its topology formula_5 although they are different mathematical objects. Nevertheless, it occurs sometimes that two different topologies are considered simultaneously on the same set; for distinguishing the corresponding topological spaces, one must use notation such as formula_7 and formula_8\n\nOne encounters, in many textbooks, sentences such as \"Let be a function ...\". This is an abuse of notation, as the name of the function is , and denotes normally the value of the function for the element of its domain. The correct phrase would be \"Let be a function of the variable ...\" or \"Let be a function ...\" This abuse of notation is widely used, as it simplifies the formulation, and the systematic use of a correct notation quickly becomes pedantic.\n\nA similar abuse of notation occurs in sentences such as \"Let us consider the function ...\" In fact is not a function. The function is the operation that associates to , often denoted as . Nevertheless, this abuse of notation is widely used since it is generally not confusing.\n\nMany mathematical structures are defined through a characterizing property (often a universal property). Once this desired property is defined, there may be various ways to construct the structure, and the corresponding results are formally different objects, but which have exactly the same properties – they are isomorphic. As there is no way to distinguish these isomorphic objects through their properties, it is standard to consider them as equal, even if this is formally wrong.\n\nOne example of this the Cartesian product, which is often seen as associative:\n\nBut this is not strictly true: if formula_10, formula_11 and formula_12, the identity formula_13 would imply that formula_14 and formula_15, and so formula_16 would mean nothing.\n\nThis notion can be made rigorous in category theory, using the idea of a natural isomorphism.\n\nAnother example occurs in such statements as \"there are two non-Abelian groups of order 8\", which more strictly stated means \"there are two isomorphism classes of non-Abelian groups of order 8\".\n\nReferring to an equivalence class of an equivalence relation by \"x\" instead of [\"x\"] is an abuse of notation. Formally, if a set \"X\" is partitioned by an equivalence relation ~, then for each \"x\" ∈ \"X\", the equivalence class {\"y\" ∈ \"X\" | \"y\" ~ \"x\"} is denoted [\"x\"]. But in practice, if the remainder of the discussion is focused on equivalence classes rather than individual elements of the underlying set, it is common to drop the square brackets in the discussion.\n\nFor example, in modular arithmetic, a finite group of order \"n\" can be formed by partitioning the integers via the equivalence relation \"x\" ~ \"y\" if and only if \"x\" ≡ \"y\" (mod \"n\"). The elements of that group would then be [0], [1], …, [\"n\" − 1], but in practice they are usually just denoted 0, 1, …, \"n\" − 1.\n\nAnother example is the space of (classes of) measurable functions over a measure space, or classes of Lebesgue integrable functions, where the equivalence relation is equality \"almost everywhere\".\n\nThe terms \"abuse of language\" and \"abuse of notation\" depend on context. Writing \"\"f\": \"A\" → \"B\"\" for a partial function from \"A\" to \"B\" is almost always an abuse of notation, but not in a category theoretic context, where \"f\" can be seen as a morphism in the category of sets and partial functions.\n\n"}
{"id": "1158037", "url": "https://en.wikipedia.org/wiki?curid=1158037", "title": "Academy Glacier", "text": "Academy Glacier\n\nAcademy Glacier () is a major Antarctic glacier in the Pensacola Mountains, draining northwestward between the Patuxent and Neptune ranges to enter Foundation Ice Stream.\n\nIt was mapped by the USGS from surveys and US Navy air photos in 1956-66, and was named by the US-ACAN for the National Academy of Sciences which played an important role in the planning of the U.S. program in Antarctica.\n\n"}
{"id": "2406859", "url": "https://en.wikipedia.org/wiki?curid=2406859", "title": "Adelphogamy", "text": "Adelphogamy\n\nAdelphogamy is a form of sexual partnership between sibling eukaryotes, for example in some species of fungi, flowering plants or ants, or in humans. In sociology, the term \"adelphogamy\" may also refer to fraternal polyandry or to a relationship between a brother and sister.\n"}
{"id": "2992945", "url": "https://en.wikipedia.org/wiki?curid=2992945", "title": "Auroral kilometric radiation", "text": "Auroral kilometric radiation\n\nAuroral kilometric radiation (AKR) is the intense radio radiation emitted in the acceleration zone (at a height of three times the radius of the Earth) of the polar lights. The radiation mainly comes from cyclotron radiation from electrons orbiting around the magnetic field lines of the Earth. The radiation has a frequency of between 50 and 500 kHz and a total power of between about 1 million and 10 million watts. The radiation is absorbed by the ionosphere and therefore can only be measured by satellites positioned at vast heights, such as the Fast Auroral Snapshot Explorer (FAST). According to the data of the Cluster mission, it is beamed out in the cosmos in a narrow plane tangent to the magnetic field at the source. The sound produced by playing AKR over an audio device has been described as \"whistles\", \"chirps\", and even \"screams\".\n\nAs some other planets emit cyclotron radiation too, AKR could be used to learn more about Jupiter, Saturn, Uranus and Neptune, and to detect extrasolar planets.\n"}
{"id": "342006", "url": "https://en.wikipedia.org/wiki?curid=342006", "title": "Boosterism", "text": "Boosterism\n\nBoosterism is the act of promoting (\"boosting\") a town, city, or organization, with the goal of improving public perception of it. Boosting can be as simple as talking up the entity at a party or as elaborate as establishing a visitors' bureau. It has been somewhat associated with American small towns. Boosting is also done in political settings, especially in regard to disputed policies or controversial events.\n\nDuring the expansion of the American and Canadian West, boosterism became epidemic as the leaders and owners of small towns made extravagant predictions for their settlement, in the hope of attracting more residents and, not coincidentally, inflating the prices of local real estate.\n\nThe 1871 humorous speech \"The Untold Delights of Duluth\", delivered by Democratic U.S. Representative J. Proctor Knott, lampooned boosterism. Boosterism is also a major theme of two novels by Sinclair Lewis—\"Main Street\" (published 1920) and \"Babbitt\" (1922). As indicated by an editorial that Lewis wrote in 1908 entitled \"The Needful Knocker\", boosting was the opposite of knocking. The editorial explained:\n\n\n"}
{"id": "25583808", "url": "https://en.wikipedia.org/wiki?curid=25583808", "title": "Cakalele (journal)", "text": "Cakalele (journal)\n\nCakalele: \"Maluku Research Journal/Majalah Penelitian Maluku\" is an academic journal that publishes the results of research about Maluku and Maluku communities in Indonesia and the Netherlands. The journal chronicles the growth of Maluku in humanities and the sciences as it expands geographically. \"Cakalele\" is stored on the ScholarSpace institutional repository of the University of Hawaii at Manoa. The journal was founded by linguist James T. Collins.\n\n"}
{"id": "23630641", "url": "https://en.wikipedia.org/wiki?curid=23630641", "title": "Carey mask", "text": "Carey mask\n\nA Carey mask (named after the inventor, George Carey) is a focusing aid for astronomical telescopes. The mask is in the form of a thin card or sheet that is placed over the front aperture of the telescope. There are four series of slits in the mask which form a diffraction pattern in the image plane.\n\nIn this example the two sets of slits on the left are angled at 12 degrees to each other. Those on the right are angled at 10 degrees to each other. Different telescope and imaging combinations may require slightly different angles.\n\nThe diffraction pattern caused by the left hand slits will be in the form of an 'X'.\nThe right hand slits will also form an 'X' shape, but the lines forming the 'X' will cross at a narrower angle.\nWhen perfect focus is achieved the two 'X's will be superimposed and be perfectly symmetrical.\nAny slight error in focus will cause the 'X's to be offset, and this is very noticeable to the naked eye.\n\nIn the example images below, focus error is obvious in the first two images. The third image is very close to perfect focus as shown by the equal spacing between the elongated spikes on the left and the right.\n\nA negative image can sometimes show the diffraction spikes more clearly.\n\nThe enlarged view below shows the left hand spikes to be slightly further apart than those on the right. This is an indication that the focus knob needs to be rotated slightly anti-clockwise. If the right hand spikes were further apart, a clockwise rotation would be needed. The mask should always be placed over the aperture with the same orientation. If the mask was rotated 180 degrees, then the focus knob directions would be reversed. The operator soon becomes familiar with the rotation directions needed for a given setup.\n\nThis indication of the direction of focus change needed removes much of the trial and error that can often be encountered when attempting astrophotography.\n\n\n"}
{"id": "31908462", "url": "https://en.wikipedia.org/wiki?curid=31908462", "title": "Defense Weather Satellite System", "text": "Defense Weather Satellite System\n\nThe Defense Weather Satellite System (DWSS) was a United States Department of Defense weather satellite system to have been built by Northrop Grumman Corporation projected for launch in 2018.\nIn January 2012, the US Air Force cancelled the program. It was replaced with the Weather System Follow-on Microwave (WSF-M).\n\nDWSS was a follow-on for the Defense Meteorological Satellite Program (DMSP) mission. The DWSS, together with the still continuing Joint Polar Satellite System (JPSS) project, was to replace the National Polar-orbiting Operational Environmental Satellite System (NPOESS) project which itself was cancelled in January 2010.\n"}
{"id": "7984866", "url": "https://en.wikipedia.org/wiki?curid=7984866", "title": "European Spatial Development Perspective", "text": "European Spatial Development Perspective\n\nThe European Spatial Development Perspective (ESDP) is a document approved by the Informal Council of Ministers of Spatial Planning of European Commission in Potsdam in 1999. It is a legally non-binding document forming a policy framework with 60 policy options for all tiers of administration with a planning responsibility. The strategic aim is to achieve a balanced and sustainable spatial development strategy.\n\nWith the aim to provide an integrated, multi-sectoral and indicative strategy for the spatial development, the key ideas of ESDP are: \n\n\n\n\n"}
{"id": "6181420", "url": "https://en.wikipedia.org/wiki?curid=6181420", "title": "Fissility (geology)", "text": "Fissility (geology)\n\nIn geology, fissility is the ability or tendency of a rock to split along flat planes of weakness (“parting surfaces”). These planes of weakness are oriented parallel to stratification in sedimentary rocks. Fissility is differentiated from scaly fabric in hand sample by the parting surfaces’ continuously parallel orientations to each other and to stratification. Fissility is distinguished from scaly fabric in thin section by the well-developed orientation of platy minerals such as mica. Fissility is the result of sedimentary or metamorphic processes.\n\nPlanes of weakness are developed in sedimentary rocks such as shale or mudstone by clay particles aligning during compaction. Planes of weakness are developed in metamorphic rocks by the recrystallization and growth of micaceous minerals. A rock’s fissility can be degraded in numerous ways during the geologic process, including clay particles flocculating into a random fabric before compaction, bioturbation during compaction, and weathering during and after uplift. The effect of bioturbation has been documented well in shale cores sampled: past variable critical depths where burrowing organisms can no longer survive, shale fissility will become more pervasive and better defined.\n\nFissility is used by some geologists as the defining characteristic which separates mudstone (no fissility) from shale (fissile). However, some professions, like drilling engineers, continue to use the two terms interchangeably.\n\n"}
{"id": "7647575", "url": "https://en.wikipedia.org/wiki?curid=7647575", "title": "Five hundred meter Aperture Spherical Telescope", "text": "Five hundred meter Aperture Spherical Telescope\n\nThe Five-hundred-meter Aperture Spherical radio Telescope (FAST; ), nicknamed Tianyan (, lit. \"Heavenly Eye\" or \"The Eye of Heaven\") is a radio telescope located in the Dawodang depression (), a natural basin in Pingtang County, Guizhou Province, southwest China. It consists of a fixed diameter dish constructed in a natural depression in the landscape. It is the world's largest filled-aperture radio telescope, and the second-largest single-dish aperture after the sparsely-filled RATAN-600 in Russia.\n\nIt has a novel design, using an active surface made of metal panels that can be tilted by a computer to help change the focus to different areas of the sky. The cabin containing the feed antenna suspended on cables above the dish is also moved using a digitally-controlled winch by the computer control system to steer the instrument to receive from different directions.\n\nConstruction on the FAST project began in 2011 and it achieved first light in September 2016. It is currently undergoing testing and commissioning. It observes at wavelengths of 10 cm to 4.3 m.\n\nThe telescope made its first discovery of two new pulsars in August 2017, barely one year after its first light. The new pulsars PSR J1859-01 and PSR J1931-02, which are also referred to as FAST pulsar #1 and #2 (FP1 and FP2), were detected on 22 and 25 August and are 16,000 and 4,100 light years away, respectively. They were independently confirmed by the Parkes Observatory in Australia on 10 September. The telescope had discovered 44 new pulsars by September, 2018.\n\nThe telescope was first proposed in 1994. The project was approved by the National Development and Reform Commission (NDRC) in July 2007. A 65-person village was relocated from the valley to make room for the telescope and an additional 9,110 people living within a 5 km radius of the telescope were relocated to create a radio-quiet area. About 500 families tried to sue the local government. Villagers accused the government of forced demolitions, unlawful detentions and not giving compensation. The Chinese government spent around $269 million in poverty relief funds and bank loans for the relocation of the local residents, while the construction of the telescope itself cost .\n\nOn 26 December 2008, a foundation laying ceremony was held on the construction site. Construction started in March 2011, and the last panel was installed on the morning of 3 July 2016.\n\nOriginally budgeted for , the final cost was (). Significant difficulties encountered were the site's remote location and poor road access, and the need to add shielding to suppress radio-frequency interference (RFI) from the primary mirror actuators. There are still ongoing problems with the failure rate of the primary mirror actuators.\n\nTesting and commissioning began with first light on 25 September 2016. The first observations are being done without the active primary reflector, configuring it in a fixed shape and using the Earth's rotation to scan the sky. Subsequent early science will take place at lower frequencies while the active surface is brought to its design accuracy; longer wavelengths are less sensitive to errors in reflector shape. It will take three years to calibrate the various instruments so it can become fully operational.\n\nLocal government efforts to develop a tourist industry around the telescope are causing some concern among astronomers worried about nearby mobile telephones acting as sources of RFI. A projected 10 million tourists in 2017 will force officials to decide on the scientific mission versus the economic benefits of tourism.\n\nFAST has a fixed primary reflector located in a natural sinkhole in the landscape (karst), focusing radio waves on a receiving antenna in a \"feed cabin\" suspended above it. The reflector is made of perforated aluminium panels supported by a mesh of steel cables hanging from the rim.\n\nFAST's surface is made of 4450 triangular panels, on a side, in the form of a geodesic dome. 2225 winches located underneath make it an active surface, pulling on joints between panels, deforming the flexible steel cable support into a parabolic antenna aligned with the desired sky direction.\n\nAbove the reflector is a light-weight feed cabin moved by a cable robot using winch servomechanisms on six support towers. The receiving antennas are mounted below this on a Stewart platform which provides fine position control and compensates for disturbances like wind motion. This produces a planned pointing precision of 8 arcseconds.\n\nThe maximum zenith angle is 60 degree when the effective illuminated aperture is reduced to 200 m, while it is 26.4 degree when the effective illuminated aperture is 300 m without loss.\n\nAlthough the reflector diameter is , only a circle of 300 m diameter is used (held in the correct parabolic shape and \"illuminated\" by the receiver) at any one time. Thus, the name is a misnomer: the aperture is not 500 m, nor is it spherical.\n\nIts working frequency range of 70 MHz to 3.0 GHz, with the upper limit set by the precision with which the primary can approximate a parabola. It could be improved slightly, but the size of the triangular segments limits the shortest wavelength which can be received. This range is covered by nine receivers under the feed cabin, with the 1.23–1.53 GHz band around the hydrogen line using a 19-beam receiver built by the CSIRO as part of the ACAMAR collaboration between the Australian Academy of Science and the Chinese Academy of Sciences.\n\nThe Next Generation Archive System (NGAS), developed by the International Center for Radio Astronomy (ICRAR) in Perth, Australia and the European Southern Observatory will store and maintain the large amount of data that it collects.\n\nThe FAST website lists the following science objectives of the radio telescope:\n\nThe FAST telescope joined the Breakthrough Listen SETI project in October 2016 to search for intelligent extraterrestrial communications in the Universe.\n\nThe primary driving force behind the project was Nan Rendong (), a researcher with the Chinese National Astronomical Observatory, part of the Chinese Academy of Sciences. He held the positions of chief scientist and chief engineer of the project. He passed away on 15 September 2017 in Boston due to lung cancer.\n\nThe Academy has been having difficulty finding staff for the telescope. Its size requires a large staff, but they are having difficulty attracting astronomers to its remote location, making it unlikely that the telescope will operate at full capacity for some time. As China has few radio astronomers, they are soliciting staff internationally, but the pay offered is low, and many are worried about heavy-handed management for this high-profile project.\n\nThe Academy has likewise been searching for a qualified director of scientific operations for FAST since May 2017, but has not been able to fill the position. Although an offer of is widely quoted, this is primarily a one-time research grant, not salary or ongoing support.\n\nThe basic design of FAST is similar to the Arecibo Observatory radio telescope. Both are fixed primary reflectors installed in natural hollows, made of perforated aluminum panels with a movable receiver suspended above. And both have an effective aperture smaller than the physical size of the primary. There are, however, five significant differences in addition to the size.\n\nFirst, Arecibo's dish is fixed in a spherical shape. Although it is also suspended from steel cables with supports underneath for fine-tuning the shape, they are manually operated and adjusted only for maintenance. It has a fixed spherical shape and two additional reflectors suspended above to correct for the resultant spherical aberration.\n\nSecond, Arecibo's receiver platform is fixed in place. To support the greater weight of the additional reflectors, the primary support cables are static, with the only motorised portion being three hold-down winches which compensate for thermal expansion. The antennas are mounted on a rotating arm below the platform. This smaller range of motion limits it to viewing objects within 19.7° of the zenith.\n\nThird, Arecibo can receive higher frequencies. The finite size of the triangular panels making up FAST's primary reflector limits the accuracy with which it can approximate a parabola, and thus the shortest wavelength it can focus. Arecibo's more rigid design allows it to maintain sharp focus down to 3 cm wavelength (10 GHz); FAST is limited to 10 cm (3 GHz). Improvements in position control of the secondary might be able to push that to 6 cm (5 GHz), but then the primary reflector becomes a hard limit.\n\nFourth, the FAST dish is significantly deeper, contributing to a wider field of view. Although 100*(500 / - 1) round 0% larger in diameter, FAST's radius of curvature is , barely larger than Arecibo's , so it forms a 360/pi*asin(500/2/300) round 0° arc (vs. 360/pi*asin(1000/2/870) round 0° for Arecibo). Although Arecibo's full aperture of can be used when observing objects at the zenith, the effective aperture for more typical inclined observations is . (This is partially compensated for by Arecibo's location closer to the equator, so the Earth's rotation scans a larger fraction of the sky. Arecibo is located at 18.35° N latitude, while FAST is sited about 7.5° farther north, at about 25.80° N.)\n\nFifth, Arecibo's larger secondary platform also houses several \"transmitters\", making it one of only two instruments in the world capable of radar astronomy. The NASA-funded Planetary Radar System allows Arecibo to study solid objects from Mercury to Saturn, and to perform very accurate orbit determination on near-earth objects, particularly potentially hazardous objects. Arecibo also includes several NSF funded radars for ionospheric studies. These powerful transmitters are too large and heavy for FAST's small receiver cabin, so it will not be able to participate in planetary defense.\n\n\n\n"}
{"id": "27237768", "url": "https://en.wikipedia.org/wiki?curid=27237768", "title": "Foreign Policy and Security Research Center", "text": "Foreign Policy and Security Research Center\n\nForeign Policy and Security Research center established in 2008 by Doctors of History and Ph.D. in history, law, foreign policy and other subjects, currently working in public institutions of Belarus: Belarusian National Technical University, Academy of Public Administration under the aegis of the President of the Republic of Belarus, Belarusian State University (Faculty of International Relations). The official purpose of the Foreign Policy and Security Research center - to increase mutual understanding between Belarus and Europe to help Europeans learn more about Belarus. Foreign Policy and Security Research center emphasizes its status as a nongovernmental organization, it ready to cooperate with all interested persons and organizations.\n\nRozanov Anatoliy Arkadievich – Doctor of History, Professor of International Relations (Faculty of International Relations, BSU, Minsk). Research interests - United States and International Organizations; U.S. foreign policy; the Evolution of NATO; National and International Security.\n\nGancherenok Igor Ivanovich - Vice-Rector of the Academy of Public Administration under the aegis of the President of the Republic of Belarus, Doctor of Physics and Mathematics, Professor. Research interests - International Cooperation in Higher Education; Administration in Higher Education; Polarization Nonlinear Optics and Spectroscopy.\n\nMalevich Ulianna Igorevna - Doctor of Policy, Professor of International Relations (Faculty of International Relations, BSU, Minsk). Research interests - Foreign Policy of Asian and Pacific Rim countries; Foreign Policy of China and Japan; Bilateral Interaction between Belarus-China; human rights issues in contemporary international relations.\n\nPrannik Tatiana Alexandrovna - PhD in History, CEO of the Center for International Cooperation and Educational Programs of the Academy of Management under the President of the Republic of Belarus. Research interests - history of Germany; international relations in the interwar period; international cooperation in higher education.\n\nShadurski Victor Gennadievich - Dean of the Faculty of International Relations(BSU, Minsk), Doctor of History, Professor of International Relations. Research interests - Domestic and Foreign Policy of France; Place and Role of the Baltic States in Modern World Politics; the Impact of World Development Problems on Belarus.\n\nSharapo Alexander Victorovich - Doctor of History, Department Chairman of International Relations (Faculty of International Relations, BSU, Minsk), Professor of International Relations. Research interests - the Belarusian-Russian integration; the political system of Germany; foreign policy of FRG; internal policy of Germany; the political system and foreign policy of Austria; the political system and foreign policy of Switzerland; international relations; global trends in world development.\n\nRusakovich Andrei Vladimirovich - PhD in History, Associate Professor of International Relations (Faculty of International Relations, BSU, Minsk). Research interests - Belarus-Germany Relations; the History of the Foreign Policy of Belarus; Modern International Relations; the Formation and Activities of the CIS; the Regional Organization in Asia, Africa and Latin America.\n\nSelivanov Andrey Vladimirovich - PhD in History, lecturer in International Relations (Faculty of International Relations, BSU, Minsk). Research interests - the UN; the political system and foreign policy of the Nordic countries; migration; refugees.\n\nBrovka Gennady Mikhailovich - Dean of the Faculty of Management Technology and humanization (BNTU, Minsk), PhD Degree in Education, Associate Professor.\nResearch interests - Economic Security; Customs Management; Management of Educational Systems and Institutions; International Activity of Higher Educational Institutions.\n\nTihomirov Alexander Valentinovich - PhD in History, Associated Professor of International Relations (Faculty of International Relations, BSU, Minsk). Research interests - foreign policy of Belarus; foreign policy of Russia; foreign policy of Ukraine; foreign policy of Eastern Europe countries; foreign policy of Western Europe countries; foreign policy of North American countries; international relations.\n\nExecution of contract with the Geneva Centre for Democratic Control of Armed Forces (Switzerland) - 2010\n\n«Economic, legal and informational aspects of customs cooperation» 08-09.04.2010\n\nA joint workshop with NATO «NATO: challenges of present and future» 10.12.2009\n\n«Belarus-Turkey: ways of cooperation» 08.12.2009\n\n«Belarus in modern world» 20.02.2009\n\n"}
{"id": "1320131", "url": "https://en.wikipedia.org/wiki?curid=1320131", "title": "Fort Mandan", "text": "Fort Mandan\n\nFort Mandan was the name of the encampment which the Lewis and Clark Expedition built for wintering over in 1804-1805. The encampment was located on the Missouri River approximately twelve miles from the site of present-day Washburn, North Dakota, which developed later. The precise location is not known for certain and is believed now to be under the water of the river. A replica of the fort has been constructed near the original site.\n\nThe fort was built of cottonwood lumber cut from the riverbanks. It was triangular in shape, with high walls on all sides, an interior open space between structures, and a gate facing the Missouri River, by which the party would normally travel. Storage rooms provided a safe place to keep supplies. Lewis and Clark shared a room. The men of the Corps of Discovery started the fort on November 2, 1804. They wintered there until April 6, 1805. According to the journals, they built the fort slightly downriver from the five villages of the Mandan and Hidatsa nations.\n\nThe winter was very cold, with temperatures sometimes dipping to minus 45 degrees Fahrenheit (-43°C), but the fort provided some protection from the elements. Several of the men of the expedition suffered frostbite due to the severely cold conditions, which affected them even with brief exposure.\n\nIn addition to seeking protection during the winter, Captains Meriwether Lewis and William Clark spent much of this period on diplomatic efforts with the several Native American tribes who lived near the fort. As the expedition established the first official contact between the United States and these nations, President Thomas Jefferson had directed the captains to pursue diplomatic goals. They were to try to establish friendly relationships with as many tribes as possible, and to prepare them for the arrival of United States traders to the region. They were also to claim United States territorial sovereignty over the land, which had been occupied by Native Americans for thousands of years. The historic tribes had differing conceptions of property use than did the European Americans.\n\nThe Teton people had already shown resistance to the expedition. Lewis and Clark gradually adjusted their goals, working to form alliances with the Arikara, Hidatsa, and Mandan against the Teton.\n\nThe Mandan were cautiously favorable toward such an alliance. When the Expedition returned to the area in 1806 while traveling east, the Mandan sent one of their chiefs, \"Sheheke,\" on the trip to Washington, D.C. to meet with Thomas Jefferson. But the Mandan did not commit to trading with the United States at the expense of their previous partnership with Great Britain through Canadian traders. The Hidatsa strongly resisted the American diplomatic efforts, often avoiding meeting with Lewis and Clark.\n\nThe Corps spent much time during the winter to prepare for their travel in the spring, repairing equipment, making clothing, processing dried meats, etc. In addition, on the way to their winter site, they had used maps made by previous explorers. From that point on in their westward journey, they would enter territory unfamiliar to Europeans according to known documentation. Clark noted that he gathered information from chief Sheheke about the route to the west in order to make a preliminary map.\n\nNot knowing if they would survive the journey, Lewis and Clark used the winter to compile their descriptions of tributaries of the Missouri River, their observations about the Native nations encountered, and their descriptions of plant and mineral specimens which they had collected; all were compiled into a manuscript which they called the \"Mandan Miscellany\". In the spring the captains sent a copy of the manuscript to government officials in St. Louis via their large keelboat. The boat would return before their expected arrival at the Mandan area in 1806.\n\nLewis and Clark appear to have first met Sacagawea at Fort Mandan. Her husband Toussaint Charbonneau served as a Hidatsa interpreter for the expedition, and the journals imply that she lived at the fort with him. Their son Jean Baptiste Charbonneau, whom she kept with her throughout the expedition, was born on February 11, possibly at the fort.\n\nWhen the Corps passed back through the area in August 1806 on their return journey to the East, they found the fort had burnt to the ground. The cause is unknown. Since that time, the Missouri River has slowly eroded the bank and shifted course to the east, putting the former site of the fort underwater.\n\nThe Lewis and Clark Fort Mandan Foundation built a replica of the fort along the river, 2.5 miles from the intersection of ND 200A and US 83. Made according to materials and design as described in the expedition's journals, it is located near the North Dakota Lewis and Clark Interpretive Center. The fort replica holds reproduction items, such as \"Meriwether Lewis' field desk, William Clark's map-making tools, bunks the men slept in, equipment they carried in the field, clothes they wore, and the blacksmith's forge.\" In addition, the site has personnel for tours and interpretive programs about the Lewis and Clark Expedition and its significance in United States, state and regional history. Walking trails go along the property and the river.\n\n"}
{"id": "19474055", "url": "https://en.wikipedia.org/wiki?curid=19474055", "title": "Gauss circle problem", "text": "Gauss circle problem\n\nIn mathematics, the Gauss circle problem is the problem of determining how many integer lattice points there are in a circle centered at the origin and with radius \"r\". This number is approximated by the area of the circle, so the real problem is to accurately bound the error term describing how the number of points differs from the area.\nThe first progress on a solution was made by Carl Friedrich Gauss, hence its name.\n\nConsider a circle in R with center at the origin and radius \"r\" ≥ 0. Gauss' circle problem asks how many points there are inside this circle of the form (\"m\",\"n\") where \"m\" and \"n\" are both integers. Since the equation of this circle is given in Cartesian coordinates by \"x\" + \"y\" = \"r\", the question is equivalently asking how many pairs of integers \"m\" and \"n\" there are such that\n\nIf the answer for a given \"r\" is denoted by \"N\"(\"r\") then the following list shows the first few values of \"N\"(\"r\") for \"r\" an integer between 0 and 12 followed by the list of values formula_2 rounded to the nearest integer:\n\nN(r) is roughly π\"r\", the area inside a circle of radius \"r\". This is because on average, each unit square contains one lattice point. Thus, the actual number of lattice points in the circle is approximately equal to its area, π\"r\". So it should be expected that\nfor some error term \"E\"(\"r\") of relatively small absolute value. Finding a correct upper bound for |\"E\"(\"r\")| is thus the form the problem has taken. Note that \"r\" need not be an integer. After formula_4 one has formula_5 At these places formula_6 increases by formula_7 after which it decreases (at a rate of formula_8) until the next time it increases.\n\nGauss managed to prove that\nHardy and, independently, Landau found a lower bound by showing that\nusing the little o-notation. It is conjectured that the correct bound is\n\nWriting |\"E\"(\"r\")| ≤ \"Cr\", the current bounds on \"t\" are\nwith the lower bound from Hardy and Landau in 1915, and the upper bound proved by Huxley in 2000.\n\nThe value of \"N\"(\"r\") can be given by several series. In terms of a sum involving the floor function it can be expressed as:\n\nThis is a consequence of Jacobi's two-square theorem, which follows almost immediately from the Jacobi triple product.\n\nA much simpler sum appears if the sum of squares function \"r\"(\"n\") is defined as the number of ways of writing the number \"n\" as the sum of two squares. Then\n\nAlthough the original problem asks for integer lattice points in a circle, there is no reason not to consider other shapes, for example conics; indeed Dirichlet's divisor problem is the equivalent problem where the circle is replaced by the rectangular hyperbola. Similarly one could extend the question from two dimensions to higher dimensions, and ask for integer points within a sphere or other objects. There is an extensive literature on these problems. If one ignores the geometry and merely considers the problem an algebraic one of Diophantine inequalities then there one could increase the exponents appearing in the problem from squares to cubes, or higher.\n\nAnother generalisation is to calculate the number of coprime integer solutions \"m\", \"n\" to the inequality\nThis problem is known as the primitive circle problem, as it involves searching for primitive solutions to the original circle problem. It can be intuitively understood as the question of how many trees within a distance of r are visible in the Euclid's orchard, standing in the origin. If the number of such solutions is denoted \"V\"(\"r\") then the values of \"V\"(\"r\") for \"r\" taking small integer values are \nUsing the same ideas as the usual Gauss circle problem and the fact that the probability that two integers are coprime is 6/\"π\", it is relatively straightforward to show that\nAs with the usual circle problem, the problematic part of the primitive circle problem is reducing the exponent in the error term. At present the best known exponent is 221/304 + \"ε\" if one assumes the Riemann hypothesis. Without assuming the Riemann hypothesis, the best known upper bound is\nfor a positive constant \"c\". In particular, no bound on the error term of the form 1 − \"ε\" for any \"ε\" > 0 is currently known that does not assume the Riemann Hypothesis.\n"}
{"id": "22290501", "url": "https://en.wikipedia.org/wiki?curid=22290501", "title": "Georg von Tiesenhausen", "text": "Georg von Tiesenhausen\n\nGeorg Heinrich Patrick Baron von Tiesenhausen (May 18, 1914 – June 4, 2018) was a United States-based German rocket scientist. \n\nAfter being brought to the United States in 1953 as part of Operation Paperclip, he was part of Wernher von Braun's team at the United States Army, and later, NASA. He is credited with the first complete design of the Lunar Roving Vehicle and made a variety of other contributions to the space program. He was a member of Baltic German noble family of Tiesenhausen.\n\nTiesenhausen was born in Riga, Latvia, in the Russian Empire to a Baltic German family from his father's side, while his mother was of Scottish ancestry. He studied engineering in Hamburg, but was conscripted to the Luftwaffe in 1939 and sent to the Eastern Front. He was allowed to continue his studies and in 1943 and graduated from University of Hamburg. After his graduation he was sent to the Peenemünde Army Research Center.\n\nTiesenhausen worked with Wernher von Braun developing V-2 rockets in Germany during World War II. He came to America in 1953 as part of Operation Paperclip, where he again worked with von Braun on guided missiles such as the Redstone, this time for the United States Army at Redstone Arsenal in Huntsville, Alabama. He was later transferred to NASA, where he worked on various spaceflight programs, including the Apollo program, which landed men on the Moon. \n\nHe continued to work for NASA well into the Shuttle era. Later he worked on space tether missions.\n\nBetween 1987 and 2010, von Tiesenhausen frequently volunteered at the U.S. Space & Rocket Center in Huntsville, Alabama, lecturing to students in Space Camp programs about the future of space exploration and other topics.\n\nIn 2007, he became one of the original inductees into the Space Camp Hall of Fame. On February 3, 2011, he was presented with the U.S. Space & Rocket Center's Lifetime Achievement Award for Education by Neil Armstrong. \"Dr. von T is one of those rare individuals who has a natural ability to inform and inspire, to educate and motivate, and, most remarkably, to endure,\" Armstrong said.\n\n"}
{"id": "57716134", "url": "https://en.wikipedia.org/wiki?curid=57716134", "title": "H.Y. Mohan Ram", "text": "H.Y. Mohan Ram\n\nHolenarasipur Yoganarasimham Mohan Ram (24 September 1930 - 18 June 2018) was an Indian botanist who influenced numerous students as a professor of Botany at Delhi University. His research areas included studies in floral biology, plant physiology, insectivorous plants and on the family Podostemaceae. He was a brother of H. Y. Sharada Prasad and the father of Indian Ocean's Rahul Ram.\n\nMohan Ram was born in Karnataka and grew up in Mysuru where he studied at the Saradavilas High School (1943-46) and Intermediate College (1946-48), receiving his bachelor of science degree from St. Philomena's College, Mysore and a masters degree in Botany from the Balwant Rajput College, Agra in 1953. He then joined as a lecturer in Botany at University of Delhi and worked on seed development in the Acanthaceae under Professor Panchanan Maheswari. He subsequently worked at Cornell University as a Fulbright Scholar with F.C. Steward and became a specialist in tissue culture. He also worked at the Laboratoire de Physiologie Pluricellulaire with J.P. Nitsch. \n\nProfessor Mohan Ram published over 240 research papers and edited four books while also guiding 32 PhD students. He helped in the establishment of the Department of Genetics and Environmental Biology at Delhi University. He was an honorary scientist of the Indian National Science Academy from 2006 and was a vice-president of Indian Academy of Sciences between 1988 and 1990. He was the chairman of the NCERT biology textbook committee from 1986 to 1988. He was awarded the JC Bose Award in 1979, the Om Prakash Bhasin Award (1986), the Sergei Nawashin Medal of the USSR (1990) and numerous other recognitions.\n\n"}
{"id": "31027378", "url": "https://en.wikipedia.org/wiki?curid=31027378", "title": "HMS Protector (A173)", "text": "HMS Protector (A173)\n\nHMS \"Protector is a Royal Navy ice patrol ship built in Norway in 2001. As MV \"Polarbjørn (Norwegian: \"polar bear\") she operated under charter as a polar research icebreaker and a subsea support vessel. In 2011, she was chartered as a temporary replacement for the ice patrol ship and was purchased outright by the British Ministry of Defence in September 2013.\n\nAn earlier icebreaker \"Polarbjørn\" was bought by Greenpeace in 1995 and renamed .\n\n\"Polarbjørn\" was designed and built for long Antarctic expeditions and for supporting subsea work.\n\"Polarbjørn\" was equipped to DP2 class and had accommodation for 100. Large cargo holds and open deck areas provide storage capacity for ROVs and related equipment. A 50-ton knuckle-boom crane and the 25-ton stern A-frame allow equipment to be deployed over the side and over the stern.\n\n\"Polarbjørn\" worked in the \"spot\" market, on short-term charter. During 2009, the vessel was chartered for electromagnetic survey work in the North Sea, Norwegian Sea and Barents Sea. She was exposed to a downturn in business during 2010, with only a 33% utilisation.\n\nPrior to the Royal Navy charter, she underwent a ten-day refit in Odense, Denmark. The helicopter deck, originally above her bridge, was repositioned over the stern and a multibeam echosounder for survey work was installed. Her engines and gearboxes were overhauled and she was modified to allow the carriage of the ancillary vessels and vehicles (survey boats, all-terrain vehicles) used in support of the British Antarctic Survey.\n\nFrom April 2011, she was chartered to the Royal Navy for three years as a temporary replacement for the ice patrol ship, , and was renamed HMS \"Protector\". The annual cost of the charter was £8.7m. In September 2013 the British Ministry of Defence purchased the ship outright from GC Rieber Shipping, for £51 million. In October 2013 the Ministry of Defence announced that from 1 April 2014 the ship's homeport would change from HMNB Portsmouth to HMNB Devonport, the location of the Hydrography and Meteorology Centre of Specialisation and where the Royal Navy's other survey ships are based.\n\nShe was commissioned into the Navy on 23 June 2011 as HMS \"Protector\". The commissioning ceremony was held on the 50th anniversary of the date that the Antarctic Treaty came \ninto force. During September 2011, \"Protector\" embarked on operational sea training in preparation for her first deployment in November.\n\nIn February 2012, after receiving a distress call from Comandante Ferraz Antarctic Station on King George Island in the South Shetland Islands, \"Protector\" sailed to provide assistance to the Brazilian research station after a large fire had broken out there. 23 of her sailors were put ashore with fire-fighting equipment to tackle the blaze. Two of the researchers died in the incident.\nDuring March and April 2012, the ship operated in the vicinity of Rothera Research Station. During a major visit, she delivered around 170 cubic metres of aviation fuel. At 67° 34′ S, this was the most southerly visit of her career up to that date, nearly from Cape Horn, the southernmost tip of the South America. The crew competed in a 'winter Olympics' with scientists from the British Antarctic Survey.\n\nOn the way to her second Antarctic deployment, in October 2012 \"Protector\" surveyed the wreck of the Dale-class oiler RFA \"Darkdale in James Bay, Saint Helena, as part of an assessment of its possible threat to the island's environment. On arriving in Antarctica in December, her designated Antarctic Treaty Observers supported an international team carrying out inspections of research stations to ensure compliance with the Antarctic Treaty.\n\nThe ship left for her third Antarctic deployment in October 2013. She revisited Rothera and then sailed across Marguerite Bay, reaching a latitude of 68° 12′ S, from Cape Horn.\n\nIn the northern summer of 2014, the ship visited the Caribbean to perform training for humanitarian assistance, and also assisted some community projects in the British Virgin Islands.\n\nIn late 2015, \"Protector\" commenced a 20-month deployment to the Ross Sea for fisheries patrol and hydrographic survey operations. It is the first time that a Royal Navy or British Government vessel has operated in the waters south of Australia and New Zealand since 1936. In addition to the ship's usual equipment, three unmanned aerial vehicles (designed and 3D printed by the University of Southampton) were embarked. Sailing from Devonport, \"Protector\" visited the Seychelles and Diego Garcia en-route (in the latter instance, becoming the first Royal Navy surface ship to visit in eight years) before proceeding to Tasmania, Australia. At the start of December, \"Protector\" departed from Hobart, Tasmania to commence fisheries patrols. In January 2016, the ship completed a five-week patrol of the Ross Sea conducting inspections in support of the Convention for the Conservation of Antarctic Marine Living Resources, with the aid of six embarked Australian and New Zealand specialists. The ship visited Zucchelli Station and reached a latitude of 77° 56′ S. Crew members visited Scott's Hut at Cape Evans. \"Protector\" circled the globe, covering more than 18,500 nautical miles, in 2016.\n\nIn November 2017, following a request for assistance from the Argentine government, \"Protector\" was redeployed to aid international efforts to locate the missing submarine .\n\n\"Protector\" operates several small boats, including the survey motor boat \"James Caird IV\", the ramped work boat \"Terra Nova\" and two Pacific 22 RIBs \"Nimrod\" and \"Aurora\". She also embarks three BV206 all-terrain vehicles and a number of quad-bikes and trailers for activities on Antarctica, such as moving stores and equipment.\n\n\"James Caird IV\" is a , ice-capable survey motor boat built by Mustang Marine in Pembroke Dockyard, based on a design of existing British Antarctic Survey boats. It has a crew of five, plus up to five passengers. The boat was named by Alexandra Shackleton, the granddaughter of Antarctic explorer Ernest Shackleton, during the commissioning ceremony for \"Protector\" on 23 June 2011. The boat's name commemorates the voyage of the \"James Caird\" made by Shackleton in 1916.\n\n\n"}
{"id": "22784944", "url": "https://en.wikipedia.org/wiki?curid=22784944", "title": "Halothermal circulation", "text": "Halothermal circulation\n\nSee Thermohaline Circulation.\nThe term halothermal circulation refers to the part of the large-scale ocean circulation that is driven by global density gradients created by surface heat and evaporation.\n\nThe adjective halothermal derives from \"halo-\" referring to salt content and \"-thermal\" referring to temperature, factors which together determine the density of sea water. Halothermal circulation is driven primarily by salinity changes and secondarily by temperature changes (as opposed to the thermohaline mode in modern oceans). The generation of high salinity surface waters at low latitudes, which were therefore of higher density and thus sank, is thought to have been the dominant ocean circulation driver during greenhouse climates such as the Cretaceous. Similar dynamics operate today in the Mediterranean.\n\nThe formation of bottom waters by halothermal dynamics is considered to be one to two orders of magnitude weaker than in thermohaline systems.\n\n"}
{"id": "49651940", "url": "https://en.wikipedia.org/wiki?curid=49651940", "title": "Hashemite (mineral)", "text": "Hashemite (mineral)\n\nHashemite is a very rare barium chromate mineral with the formula Ba(Cr,S)O4. It is a representative of natural chromates - a relatively small and rare group of minerals. Hashemite is the barium-analogue of tarapacáite. It is also the chromium-analogue of baryte, and belongs to the baryte group of minerals. Hashemite is stoichiometrically similar to crocoite and chromatite. Hashemite is orthorhombic, with space group \"Pnma\". I was found together with chromium-bearing ettringite and an apatite group mineral in the Hatrurim Formation, known for the occurrence of rocks formed due to natural pyrometamorphism. Hashemite is named after the Hashemite Kingdom of Jordan.\n"}
{"id": "34622346", "url": "https://en.wikipedia.org/wiki?curid=34622346", "title": "Herbert Boucher Dobbie", "text": "Herbert Boucher Dobbie\n\nHerbert Boucher Dobbie (13 February 1852–8 August 1940) was a New Zealand engineering draughtsman, botanist, stationmaster, orchardist and writer. He was born in Hayes, Middlesex, England, on 13 February 1852.\n\n"}
{"id": "39480604", "url": "https://en.wikipedia.org/wiki?curid=39480604", "title": "History of anthropometry", "text": "History of anthropometry\n\nThe history of anthropometry includes the use of anthropometry as an early tool of physical anthropology, use for identification, use for the purposes of understanding human physical variation, in paleoanthropology, and in various attempts to correlate physical with racial and psychological traits. At various points in history, certain anthropometrics have been cited by advocates of discrimination and eugenics, often as part of novel social movements or based upon pseudoscientific claims.\n\nIn 1716 Louis-Jean-Marie Daubenton, who wrote many essays on comparative anatomy for the Académie française, published his \"Memoir on the Different Positions of the Occipital Foramen in Man and Animals\" (\"Mémoire sur les différences de la situation du grand trou occipital dans l’homme et dans les animaux\"). Six years later Pieter Camper (1722–1789), distinguished both as an artist and as an anatomist, published some lectures that laid the foundation of much work. Camper invented the \"facial angle,\" a measure meant to determine intelligence among various species. According to this technique, a \"facial angle\" was formed by drawing two lines: one horizontally from the nostril to the ear; and the other perpendicularly from the advancing part of the upper jawbone to the most prominent part of the forehead. Camper's measurements of facial angle were first made to compare the skulls of men with those of other animals. Camper claimed that antique statues presented an angle of 90°, Europeans of 80°, Central Africans of 70° and the orangutan of 58°.\n\nSwedish professor of anatomy Anders Retzius (1796–1860) first used the cephalic index in physical anthropology to classify ancient human remains found in Europe. He classed skulls in three main categories; \"dolichocephalic\" (from the Ancient Greek \"kephalê\", head, and \"dolikhos\", long and thin), \"brachycephalic\" (short and broad) and \"mesocephalic\" (intermediate length and width). Scientific research was continued by Étienne Geoffroy Saint-Hilaire (1772–1844) and Paul Broca (1824–1880), founder of the Anthropological Society in France in 1859. Paleoanthropologists still rely upon craniofacial anthropometry to identify species in the study of fossilized hominid bones. Specimens of \"Homo erectus\" and athletic specimens of \"Homo sapiens\", for example, are virtually identical from the neck down but their skulls can easily be told apart.\n\nSamuel George Morton (1799–1851), whose two major monographs were the \"Crania Americana\" (1839), \"An Inquiry into the Distinctive Characteristics of the Aboriginal Race of America\" and \"Crania Aegyptiaca\" (1844) concluded that the ancient Egyptians were not Negroid but Caucasoid and that Caucasians and Negroes were already distinct three thousand years ago. Since \"The Bible\" indicated that Noah's Ark had washed up on Mount Ararat only a thousand years before this Noah's sons could not account for every race on earth. According to Morton's theory of polygenism the races had been separate from the start. Josiah C. Nott and George Gliddon carried Morton's ideas further. Charles Darwin, who thought the single-origin hypothesis essential to evolutionary theory, opposed Nott and Gliddon in his 1871 \"The Descent of Man\", arguing for monogenism.\n\nIn 1856, workers found in a limestone quarry the skull of a Neanderthal hominid male, thinking it to be the remains of a bear. They gave the material to amateur naturalist Johann Karl Fuhlrott who turned the fossils over to anatomist Hermann Schaaffhausen. The discovery was jointly announced in 1857, giving rise to the discipline of paleoanthropology. By comparing skeletons of apes to man, T. H. Huxley (1825–1895) backed up Charles Darwin's theory of evolution, first expressed in \"On the Origin of Species\" (1859). He also developed the \"Pithecometra principle,\" which stated that man and ape were descended from a common ancestor.\n\nEugène Dubois' (1858–1940) discovery in 1891 in Indonesia of the \"Java Man\", the first specimen of Homo erectus to be discovered, demonstrated mankind's deep ancestry outside Europe. Ernst Haeckel (1834–1919) became famous for his \"recapitulation theory\", according to which each individual mirrors the evolution of the whole species during his life.\n\nIntelligence testing was compared with anthropometrics. Samuel George Morton (1799–1851) collected hundreds of human skulls from all over the world and started trying to find a way to classify them according to some logical criterion. Morton claimed that he could judge intellectual capacity by cranial capacity. A large skull meant a large brain and high intellectual capacity, a small skull indicated a small brain and decreased intellectual capacity. Modern science has since confirmed that there is a correlation between cranium size (measured in various ways) and intelligence as measured by IQ tests, although it is a weak correlation at about 0.2. Today, brain volume as measured with MRI scanners also find a correlation between brain size and intelligence at about 0.4.\n\nCraniometry was also used in phrenology, which purported to determine character, personality traits, and criminality on the basis of the shape of the head. At the turn of the 19th century, Franz Joseph Gall (1758–1822) developed \"cranioscopy\" (Ancient Greek \"kranion\" - \"skull\", \"scopos\" - \"vision\"), a method to determine the personality and development of mental and moral faculties on the basis of the external shape of the skull. Cranioscopy was later renamed phrenology (\"phrenos\": mind, \"logos\": study) by his student Johann Spurzheim (1776–1832), who wrote extensively on \"Drs. Gall and Spurzheim's physiognomical System.\" These all claimed the ability to predict traits or intelligence and were intensively practised in the 19th and the first part of the 20th century.\n\nDuring the 1940s anthropometry was used by William Sheldon when evaluating his somatotypes, according to which characteristics of the body can be translated into characteristics of the mind. Inspired by Cesare Lombroso's criminal anthropology, he also believed that criminality could be predicted according to the body type. A basically anthropometric division of body types into the categories endomorphic, ectomorphic and mesomorphic derived from Sheldon's somatotype theories is today popular among people doing weight training.\n\nIn 1883, Frenchman Alphonse Bertillon introduced a system of identification that was named after him. The \"Bertillonage\" system was based on the finding that several measures of physical features, such as the dimensions of bony structures in the body, that remain fairly constant throughout adult life. Bertillon concluded that when these measurements were made and recorded systematically, every individual would be distinguishable. Bertillon's goal was a way of identifying recidivists (\"repeat offenders\"). Previously police could only record general descriptions. Photography of criminals had become commonplace, but there was no easy way to sort the many thousands of photographs except by name. Bertillon's hope was that, through the use of measurements, a set of identifying numbers could be entered into a filing system installed in a single cabinet.\n\nThe system involved 10 measurements; \"height\", \"stretch\" (distance from left shoulder to middle finger of raised right arm), \"bust\" (torso from head to seat when seated), \"head length\" (crown to forehead) and \"head width\" temple to temple) \"width\" of cheeks, and \"lengths\" of the \"right ear\", the \"left foot, middle finger\", and \"cubit\" (elbow to tip of middle finger). It was possible, by exhaustion, to sort the cards on which these details were recorded (together with a photograph) until a small number produced the measurements of the individual sought, independently of name.\n\nThe system was soon adapted to police methods: it prevented impersonation and could demonstrate wrongdoing.\n\nBertillonage was before long represented in Paris by a collection of some 100,000 cards and became popular in several other countries' justice systems. England followed suit when in 1894, a committee sent to Paris to investigate the methods and its results reported favorably on the use of measurements for primary classification and recommended also the partial adoption of the system of finger prints suggested by Francis Galton, then in use in Bengal, where measurements were abandoned in 1897 after the fingerprint system was adopted throughout British India. Three years later England followed suit, and, as the result of a fresh inquiry ordered by the Home Office, relied upon fingerprints alone.\n\nBertillonage exhibited certain defects and was gradually supplanted by the system of fingerprints and, latterly, genetics. Bertillon originally measured variables he thought were independent - such as forearm length and leg length - but Galton had realized that both were the result of a single causal variable (in this case, stature) and developed the statistical concept of correlation.\n\nOther complications were: it was difficult to tell whether or not individuals arrested were first-time offenders; instruments employed were costly and liable to break down; skilled measurers were needed; errors were frequent and all but irremediable; and it was necessary to repeat measurements three times to arrive at a mean result.\n\nPhysiognomy claimed a correlation between physical features (especially facial features) and character traits. It was made famous by Cesare Lombroso (1835–1909), the founder of anthropological criminology, who claimed to be able to scientifically identify links between the nature of a crime and the personality or physical appearance of the offender. The originator of the concept of a \"born criminal\" and arguing in favor of biological determinism, Lombroso tried to recognize criminals by measurements of their bodies. He concluded that skull and facial features were clues to genetic criminality and that these features could be measured with craniometers and calipers with the results developed into quantitative research. A few of the 14 identified traits of a criminal included large jaws, forward projection of jaw, low sloping forehead; high cheekbones, flattened or upturned nose; handle-shaped ears; hawk-like noses or fleshy lips; hard shifty eyes; scanty beard or baldness; insensitivity to pain; long arms, and so on.\n\nPhylogeography is the science of identifying and tracking major human migrations, especially in prehistoric times. Linguistics can follow the movement of languages and archaeology can follow the movement of artefact styles but neither can tell whether a culture's spread was due to a source population's physically migrating or to a destination population's simply copying the technology and learning the language. Anthropometry was used extensively by anthropologists studying human and racial origins: some attempted racial differentiation and classification, often seeking ways in which certain races were inferior to others. Nott translated Arthur de Gobineau's \"An Essay on the Inequality of the Human Races\" (1853–1855), a founding work of racial segregationism that made three main divisions between races, based not on colour but on climatic conditions and geographic location, and privileged the \"Aryan\" race. Science has tested many theories aligning race and personality, which have been current since Boulainvilliers (1658–1722) contrasted the \"Français\" (French people), alleged descendants of the Nordic Franks, and members of the aristocracy, to the Third Estate, considered to be indigenous Gallo-Roman people subordinated by right of conquest.\n\nFrançois Bernier, Carl Linnaeus and Blumenbach had examined multiple observable human characteristics in search of a typology. Bernier based his racial classification on physical type which included hair shape, nose shape and skin color. Linnaeus based a similar racial classification scheme. As anthropologists gained access to methods of skull measure they developed racial classification based on skull shape.\n\nTheories of scientific racism became popular, one prominent figure being Georges Vacher de Lapouge (1854–1936), who in \"L'Aryen et son rôle social\" (1899 - \"The Aryan and his social role\") divided humanity into various, hierarchized, different \" races\", spanning from the \"Aryan white race, dolichocephalic\" to the \"brachycephalic\" (short and broad-headed) race. Between these Vacher de Lapouge identified the \"\"Homo europaeus\" (Teutonic, Protestant, etc.), the \"\"Homo alpinus\" (Auvergnat, Turkish, etc.) and the \"Homo mediterraneus\"\" (Napolitano, Andalus, etc.). \"Homo africanus\" (Congo, Florida) was excluded from discussion. His racial classification (\"Teutonic\", \"Alpine\" and \"Mediterranean\") was also used by William Z. Ripley (1867–1941) who, in \"The Races of Europe\" (1899), made a map of Europe according to the cephalic index of its inhabitants.\n\nVacher de Lapouge became one of the leading inspirations of Nazi anti-semitism and Nazi ideology. Nazi Germany relied on anthropometric measurements to distinguish Aryans from Jews and many forms of anthropometry were used for the advocacy of eugenics. During the 1920s and 1930s, though, members of the school of cultural anthropology of Franz Boas began to use anthropometric approaches to discredit the concept of fixed biological race. Boas used the cephalic index to show the influence of environmental factors. Researches on skulls and skeletons eventually helped liberate 19th century European science from its ethnocentric bias. This school of physical anthropology generally went into decline during the 1940s.\n\nSeveral studies have demonstrated correlations between race and brain size, with varying results. In some studies, Caucasians were reported to have larger brains than other racial groups, whereas in recent studies and reanalysis of previous studies, East Asians were reported as having larger brains and skulls. More common among the studies was the report that Africans had smaller skulls than either Caucasians or East Asians. Criticisms have been raised against a number of these studies regarding questionable methods.\nIn \"Crania Americana\" Morton claimed that Caucasians had the biggest brains, averaging 87 cubic inches, Indians were in the middle with an average of 82 cubic inches and Negroes had the smallest brains with an average of 78 cubic inches. In 1873 Paul Broca (1824–1880) found the same pattern described by Samuel Morton's \"Crania Americana\" by weighing brains at autopsy. Other historical studies alleging a Black-White difference in brain size include Bean (1906), Mall, (1909), Pearl, (1934) and Vint (1934). But in Germany Rudolf Virchow's study led him to denounce \"Nordic mysticism\" in the 1885 Anthropology Congress in Karlsruhe. Josef Kollmann, a collaborator of Virchow, stated in the same congress that the people of Europe, be them German, Italian, English or French, belonged to a \"mixture of various races,\" furthermore declaring that the \"results of craniology\" led to \"struggle against any theory concerning the superiority of this or that European race\". Virchow later rejected measure of skulls as legitimate means of taxonomy. Paul Kretschmer quoted an 1892 discussion with him concerning these criticisms, also citing Aurel von Törok's 1895 work, who basically proclaimed the failure of craniometry.\n\nStephen Jay Gould (1941–2002) claimed Samuel Morton had fudged data and \"overpacked\" the skulls. A subsequent study by John Michael concluded that \"[c]ontrary to Gould's interpretation... Morton's research was conducted with integrity.\" In 2011 physical anthropologists at the University of Pennsylvania, which owns Morton’s collection, published a study that concluded that \"Morton did not manipulate his data to support his preconceptions, contra Gould.\" They identified and remeasured half of the skulls used in Morton’s reports, finding that in only 2% of cases did Morton’s measurements differ significantly from their own and that these errors either were random or gave a larger than accurate volume to African skulls, the reverse of the bias that Dr. Gould imputed to Morton. Difference in brain size, however, does not necessarily imply differences in intelligence: women tend to have smaller brains than men yet have more neural complexity and loading in certain areas of the brain. This claim has been criticized by, among others, John S. Michael, who reported in 1988 that Morton's analysis was \"conducted with integrity\" while Gould's criticism was \"mistaken\".\n\nSimilar claims were previously made by Ho et al. (1980), who measured 1,261 brains at autopsy, and Beals et al. (1984), who measured approximately 20,000 skulls, finding the same East Asian → European → African pattern but warning against using the findings as indicative of racial traits, \"If one merely lists such means by geographical region or race, causes of similarity by genogroup and ecotype are hopelessly confounded\". Rushton's findings have been criticized for confusing African-Americans with equatorial Africans, who generally have smaller craniums as people from hot climates often have slightly smaller crania. He also compared equatorial Africans from the poorest and least educated areas of Africa with Asians from the wealthiest, most educated areas and colder climates. According to Z. Z. Cernovsky Rushton's own study shows that the average cranial capacity of North American blacks is similar to that of Caucasians from comparable climatic zones, though a previous work by Rushton showed appreciable differences in cranial capacity between North Americans of different race. This is consistent with the findings of Z. Z. Cernovsky that people from different climates tend to have minor differences in brain size.\n\nObservable craniofacial differences included: head shape (mesocephalic, brachycephalic, dolichocephalic) breadth of nasal aperture, nasal root height, sagittal crest appearance, jaw thickness, brow ridge size and forehead slope. Using this skull-based categorization, anthropologists identified three or four racial groups, based upon terminology created by German philosopher Christoph Meiners in his The Outline of History of Mankind (1785);\n\n\nRipley's \"The Races of Europe\" was rewritten in 1939 by Harvard physical anthropologist Carleton S. Coon. Carleton S. Coon, a 20th-century craniofacial anthropometrist, used the technique for his \"The Origin of Races\" (New York: Knopf, 1962). Because of the inconsistencies in the old three-part system (Caucasoid, Mongoloid, Negroid), Coon adopted a five-part scheme. He defined \"Caucasoid\" as a pattern of skull measurements and other phenotypical characteristics typical of populations in Europe, Central Asia, South Asia, West Asia, North Africa, and Northeast Africa (Ethiopia, and Somalia). He discarded the term \"Negroid\" as misleading since it implies skin-tone, which is found at low latitudes around the globe and is a product of adaptation, and defined skulls typical of sub-Saharan Africa as \"Congoid\" and those of Southern Africa as \"Capoid\". Finally, he split \"Australoid\" from \"Mongoloid\" along a line roughly similar to the modern distinction between sinodonts in the north and sundadonts in the south. He argued that these races had developed independently of each other over the past half-million years, developing into Homo Sapiens at different periods of time, resulting in different levels of civilization. This raised considerable controversy and led the American Anthropological Association to reject his approach without mentioning him by name.\n\nIn \"The Races of Europe\" (1939) Coon classified Caucasoids into racial sub-groups named after regions or archaeological sites such as Brünn, Borreby, Alpine, Ladogan, East Baltic, Neo-Danubian, Lappish, Mediterranean, Atlanto-Mediterranean, Irano-Afghan, Nordic, Hallstatt, Keltic, Tronder, Dinaric, Noric and Armenoid. This typological view of race, however, was starting to be seen as out-of-date at the time of publication. Coon eventually resigned from the American Association of Physical Anthropologists, while some of his other works were discounted because he would not agree with the evidence brought forward by Franz Boas, Stephen Jay Gould, Richard Lewontin, Leonard Lieberman and others.\n\nAlthough craniofacial race categorization based on skull indices is unambiguous, races categorized using alternative methods yield different groups, making them non-concordant. Neither will the method pin-point geographic origins reliably, due to variation in skulls within a geographic region. The United States has group ancestries from geographically distant locations, which have generally remained endogamous. As more migrate and Americans become more racially mixed, such craniofacial identification is of reduced utility. About one-third of \"white\" Americans have detectable African DNA markers, and about five percent of \"black\" Americans have no detectable \"negroid\" traits at all, craniofacial or genetic. Given three Americans who self-identify and are socially accepted as white, black and Hispanic, and given that they have precisely the same Afro-European mix of ancestries (one \"mulatto\" grandparent), there is no objective test that will identify their US endogamous group membership without an interview. While this method produces useful results for the population of the United States, it is likely that it would not be reliable for populations from other countries or historical periods.\n\n\n\n"}
{"id": "442404", "url": "https://en.wikipedia.org/wiki?curid=442404", "title": "Ho-Am Prize in Science", "text": "Ho-Am Prize in Science\n\nThe Ho-Am Prize in Science was established in 1990 by Kun-Hee Lee, the Chairman of Samsung, to honour the late Chairman, Lee Byung-chul, the founder of the company. The Ho-Am Prize in Science (previously the Ho-Am Prize in Science & Technology) is one of six prizes awarded annually, covering the five categories of Science, Engineering, Medicine, Arts, and Community Service, plus a Special Prize, which are named after the late Chairman's sobriquet (art-name or pen name), Ho-Am.\n\nThe Ho-Am Prize in Science is presented each year, together with the other prizes, to individuals of Korean heritage who have furthered the welfare of humanity through distinguished accomplishments in the field of Science. \n\nSource: Ho-Am Foundation\n\n"}
{"id": "36134078", "url": "https://en.wikipedia.org/wiki?curid=36134078", "title": "Ice Warrior Project", "text": "Ice Warrior Project\n\nThe Ice Warrior Project is an organisation founded in 2001 by the explorer Jim McNeill. Its remit is to develop people from all walks of life and echelons of society into modern-day explorers; to discover change in the world’s most remote regions under the guidance of partner leading scientific authorities; and deliver these discoveries in an engaging, human manner to audiences around the globe.\n\nThe organisation provides raw, scientific data for others to go on and interpret. Its rationale for doing so is that if we do not monitor these regions, which were described by Nobel peace prize nominee Sheila Watt-Cloutier as \"clearly one of the most important regions when it comes to climate change.\", then how will we know that any action to mitigate the human impact on our Earth is actually working?\n\nThe project's founder Jim McNeill maintains that without such knowledge how can we be true guardians of the planet on which we live? And if we are not, then this ultimately affects our chances of survival as a species. \n\nAs an organisation it has an altruistic project side which relies on corporate sponsorship and involves charitable work and also an underpinning commercial side.\n\n\n"}
{"id": "17985651", "url": "https://en.wikipedia.org/wiki?curid=17985651", "title": "Ionized jewelry", "text": "Ionized jewelry\n\nIonized bracelets, or ionic bracelets, are a type of metal bracelet jewelry purported to affect the chi of the wearer. No claims of effectiveness made by manufacturers have ever been substantiated by independent sources, and the Federal Trade Commission (FTC) has found the bracelets are \"part of a scheme devised to defraud\".\n\nQ-Ray, Balance, Bio-Ray, IRenew, Rayma, and Rico's Bio-Energy brand bracelets are considered to be of the \"ionized\" family. Other alternative health bracelets, such as magnetic or copper therapy bracelets, are considered a different type of product.\n\nIn October 1973, corporate websites claim, Manuel L. Polo began investigating the effects of different metals on humans, believing that some metals offered a benefit when worn. This led directly to his creation of the Bio-Ray (Biomagnetic Regulator), the first ionized bracelet.\n\nIn 1994, Andrew Park bought a Bio-Ray bracelet while visiting Barcelona, Spain. Believing that it had reduced his lower back pain, he was inspired to found QT Inc., which began manufacturing and selling Q-Ray bracelets in the United States by 1996.\n\nWestern interest in the Q-Ray Ionized Bracelet rose as a result of an infomercial campaign by QT Inc. which ran from August 2000 through June 11, 2003. During this time many marketing claims were made regarding the product's alleged effectiveness, most notably regarding relief from pain and arthritis due to manipulation of a body's chi.\n\nIn a Marketplace interview, Charles Park, president of Q-Ray Canada, explains that the term \"ionized\" does not mean the bracelets themselves are ionized, but rather that the term comes from their secret \"ionization process\" which, he asserts, affects the bracelets in undisclosed ways.\n\nThese claims were the topic of a 2003 injunction by the Federal Trade Commission and later a high-profile court ruling in 2006. The court was unable to find any basis for QT Inc.'s claims related to traditional Chinese medicine, concluding that it was \"part of a scheme devised by QT Inc to defraud its consumers\".\n\nA placebo controlled randomized trial study published in the journal \"Mayo Clinic Proceedings\" compared the effect of ionized bracelet produced by Q-ray to an identically appearing placebo bracelet. The study found no difference between the ionized bracelet and control with respect to musculoskeletal pain, suggesting the effects of Q-ray bracelet was due to the placebo effect.\n\n"}
{"id": "401597", "url": "https://en.wikipedia.org/wiki?curid=401597", "title": "John Davis (English explorer)", "text": "John Davis (English explorer)\n\nJohn Davis or Davys (c. 155029 December 1605) (b. 1543?) was one of the chief English navigators of Elizabeth I. He led several voyages to discover the Northwest Passage and served as pilot and captain on both Dutch and English voyages to the East Indies. He discovered the Falkland Islands (today a British Overseas Territory) in August 1592.\n\nIt is important that Captain John Davis of Sandridge should not be confused with a contemporary, Captain John Davis of Limehouse. Both served in the fleet of Captain James Lancaster during the first voyage of the East India Company to the East Indies.\n\nDavis was born in the parish of Stoke Gabriel in Devon circa 1550, and spent his childhood in Sandridge Barton nearby. It has been suggested that he learned much of his seamanship as a child while plying boats along the river Dart, and went to sea at an early age. His childhood neighbours included Adrian Gilbert and Humphrey Gilbert and their half-brother Walter Raleigh. From early on, he also became friends with John Dee.\n\nHe began pitching a voyage in search of the Northwest Passage to the queen's secretary Francis Walsingham in 1583. Two years later, in 1585, the secretary relented and funded the expedition, which traced Frobisher's route to Greenland's east coast, around Cape Farewell, and west towards Baffin Island. In 1586 he returned to the Arctic with four ships, two of which were sent to Greenland's iceberg-calving eastern shore; the other two penetrated the strait which came to bear his name as far as 67°N before being blocked by the Arctic ice cap. \"Sunshine\" attempted (and failed) to circumnavigate the island from the east. The initially amiable approach Davis adopted to the Inuit bringing musicians and having the crew dance and play with them changed after they stole one of his anchors; they were likely irate at having been interrupted during one of their religious ceremonies. Inuit also attacked his ships in Hamilton Inlet (Labrador). A third expedition in 1587 reached 72°12'N and Disko Island before unfavorable winds forced it back. On his return, Davis charted the Davis Inlet in the coast of Labrador. The log of this trip remained a textbook model for later captains for centuries.\n\nIn 1588 he seems to have commanded \"Black Dog\" against the Spanish Armada. In 1589 he joined the Earl of Cumberland as part of the Azores Voyage of 1589. In 1591 he accompanied Thomas Cavendish on Cavendish's last voyage, which sought to discover the Northwest Passage \"upon the back parts of America\" (i.e., from the western entrance). After the rest of Cavendish's expedition returned unsuccessful, Davis continued to attempt on his own account the passage of the Strait of Magellan; though defeated by foul weather, he probably discovered the Falkland Islands in August 1592 aboard \"Desire\". His crew was forced to kill hundreds of penguins for food on the islands, but the stored meat spoiled in the tropics and only fourteen of his 76 men made it home alive.\n\nFrom 1596 to 1597 Davis seems to have sailed with Sir Walter Raleigh to Cádiz and the Azores as master of Raleigh's ship; from 1598 to 1600 he accompanied a Dutch expedition to the East Indies as pilot, sailing from Flushing and returning to Middleburg, while carefully charting and recording geographical details. He narrowly escaped destruction from treachery at Achin on Sumatra.\n\nFrom 1601 to 1603 he accompanied Sir James Lancaster as Pilot-Major on the first voyage of the English East India Company. For his part Davis was to receive £500 (around £1.5 million at 2015 values) if the voyage doubled its original investment, £1,000 if three times, £1,500 if four times and £2,000 if five times.\n\nBefore departure, Davis had told London merchants that pepper could be obtained in Aceh at a price of four reals of eight per hundredweight - whereas it actually cost 20. When the voyage returned, Lancaster complained that Davis had been wrong about both the price and availability of pepper. Unhappy at being made a scapegoat for the situation, on 5 December 1604 Davis sailed again for the East Indies as pilot to Sir Edward Michelborne, an \"interloper\" who had been granted a charter by James I despite the supposed East India Company monopoly on trade with the East. On this journey he was killed off Bintan Island near Singapore by one of his captive \"Japanese\" pirates whose vessel he had just seized.\n\nIn the centuries after his death, the importance of Dutch whalers actually led the settlements along Greenland's western coast to be called \"Straat Davis\" after their name for the Strait, while the name \"Greenland\" was used to refer to the eastern shore, erroneously presumed to be the site of the Norse Eastern Settlement.\n\nDavis's explorations in the Arctic were published by Richard Hakluyt and appeared on his world map. Davis himself published a valuable treatise on practical navigation called \"The Seaman's Secrets\" in 1594 and a more theoretical work called \"The World's Hydrographical Description\" in 1595. The account of Davis's last voyage was written by Michelborne on his return to England in 1606.\n\nHis invention of the backstaff and double quadrant (called the Davis Quadrant after him) remained popular among English seamen until long after Hadley's reflecting quadrant had been introduced.\n\nOn 28 September 1582, Davis married Mistress Faith Fulford, daughter of Sir John Fulford (the High Sheriff of Devon) and Dorithy Bourchier, the daughter of the Earl of Bath. He had five children: his first son, Gilbert was baptised on 27 March 1583; a daughter Elizabeth who died in infancy; Arthur, born 1586; John, born and died 1587; and Philip.\n\n"}
{"id": "42963086", "url": "https://en.wikipedia.org/wiki?curid=42963086", "title": "Judith Massare", "text": "Judith Massare\n\nJudith Massare is a paleontologist specializing in Mesozoic marine reptile research. In 1987, Massare published an analysis of plesiosaur feeding habits. She concluded that the long-necked plesiosauroids ate soft prey. \"Liopleurodon\" and its relatives, on the other hand, had teeth resembling those of killer whales and probably ate larger, bonier prey. The next year, Massare analyzed Mesozoic marine reptile swimming abilities and found that long-necked plesiosaurs would have been significantly slower than pliosaurs due to excess drag incurred from their large round bodies.\n\n"}
{"id": "23439689", "url": "https://en.wikipedia.org/wiki?curid=23439689", "title": "Lecture Notes in Physics", "text": "Lecture Notes in Physics\n\nLecture Notes in Physics (LNP) is a book series published by Springer Science+Business Media in the field of physics, including article related to both research and teaching. It was established in 1969.\n\n\n"}
{"id": "52603934", "url": "https://en.wikipedia.org/wiki?curid=52603934", "title": "Les Aventures électriques de Zeltron", "text": "Les Aventures électriques de Zeltron\n\nLes Aventures électriques de Zeltron is an educational children's television series broadcast in Antenne 2's Récré A2 program in France and TVJQ in Quebec. The show was sponsored by Électricité de France (EDF) and was produced from 1979 to 1982. The companies D-3-mil and CIP Vidéo worked on the show and the staff included Marcel Dupuoy.\n\nThe show features an extraterrestrial named Zeltron (Michel Elias), who resembles a small blue insulator with orange limbs and a bolt on top, who teaches electricity-related subjects and history. Some episodes include his interactions with a boy and girl living in Earth. Episodes end with a question the viewer can answer through postcard. Another character is the robot Voltix.\n\nIn the show's production, Zeltron and Voltix are marionettes with different colors from their appearance in the show. A process was used to achieve animation-like visuals for the two characters. Early in the show's run, the backgrounds were illustrated before switching to reel footage.\n\nThe character Zeltron was created by François Castan and made an earlier appearance in Salon de l'enfance. Zeltron was promoted to children through merchandise such as plastic figures, comics published by GP, Panini figurines, and a key ring with Zeltron with movable eyes. According to Yves Bouvier of the University of Savoy, Zeltron was the most successful EDF effort to teach the concepts of electricity and to promote its brand towards children.\n\n"}
{"id": "7288146", "url": "https://en.wikipedia.org/wiki?curid=7288146", "title": "Lesotho Promise", "text": "Lesotho Promise\n\nThe Lesotho Promise, a 603 carat (121 g) diamond stone of exceptional colour was unearthed on 22 August 2006 at the Letseng diamond mine in the mountain kingdom of Lesotho. Announced on 4 October 2006, it was the largest reported find this century and the 15th largest diamond ever found. The stone is rated 'D', the top colour band for diamonds.\n\nThe diamond was sold at an auction on 9 October 2006 in Antwerp, Belgium, for US$12.4 million. The buyer, The South African Diamond Corporation (SAFDICO), expected to sell the diamond for more than US $20 million after cutting.\n\nIn July 2007 the finished stones were unveiled. The largest gem cut from the crystal is a pear-shaped diamond, and the smallest is a round brilliant. In all, twenty-six stones were fashioned from the rough gem, figuring as seven pear shapes, four emerald cuts, thirteen round brilliants and one heart shape. The finished gems total .\n\nThe Lesotho Brown (usually simply called the Lesotho) at 601 carats (120 g) was the largest diamond previously found at the mine.\n\n\n"}
{"id": "15325578", "url": "https://en.wikipedia.org/wiki?curid=15325578", "title": "Life Nature Library", "text": "Life Nature Library\n\nThe Life Nature Library is a series of 25 hardbound books published by Time-Life between 1961 and 1965, with revisions to 1968. It has been translated from English into eight languages and sold in 90 countries. Each volume explores an important division of the natural world and is written for educated laymen by a primary author (or authors) \"and the Editors of LIFE\".\n\nThe 25 volumes:\n\n"}
{"id": "10794987", "url": "https://en.wikipedia.org/wiki?curid=10794987", "title": "List of Dutch flags", "text": "List of Dutch flags\n\nThis is a list of flags used in the Kingdom of the Netherlands. For more information about the national flag, visit the article Flag of the Netherlands.\n"}
{"id": "3958742", "url": "https://en.wikipedia.org/wiki?curid=3958742", "title": "List of building materials", "text": "List of building materials\n\nThis is a list of building materials. Many types of building materials are used in the construction industry to create .\n\nThese categories of materials and products are used by architects and construction project managers to specify the materials and methods used for building projects.\n\nSome building materials like cold rolled steel framing are considered modern methods of construction, over the traditionally slower methods like blockwork and timber. Many building materials have a variety of uses, therefore it is always a good idea to consult the manufacturer to check if a product is best suited to your requirements.\n\nCatalogs distributed by architectural product suppliers are typically organized into these groups.\n\nThe Construction Specifications Institute maintains the following industry standards:\n\n\n\n"}
{"id": "2674674", "url": "https://en.wikipedia.org/wiki?curid=2674674", "title": "List of compounds with carbon numbers 30–39", "text": "List of compounds with carbon numbers 30–39\n\nThis is a partial list of molecules that contain 30 to 39 carbon atoms.\n\n"}
{"id": "552931", "url": "https://en.wikipedia.org/wiki?curid=552931", "title": "List of countries that border only one other country", "text": "List of countries that border only one other country\n\nThis is the list of countries that border only one other country, with only land borders being counted. Some of these countries have several neighbours \"across the sea\". As an example Denmark \"borders\" Sweden and Norway by sea, and Canada has sea boundaries with Denmark (between Baffin Island and Greenland) and France (between the island of Newfoundland and the territory of St. Pierre and Miquelon). Other countries, such as Sri Lanka, border only one other country by sea.\n\nThere are generally three possible arrangements by which a country can have a single border. The first is with a divided island such a Haiti and the Dominican Republic, or Ireland and the United Kingdom. The second is a peninsular relationship, where the first country borders the second and is otherwise surrounded by sea, while the second country borders other countries, as with Portugal and Spain, Denmark and Germany, or Canada and the United States. The third is the circumstance where the first country is a small country that is landlocked and completely surrounded by the second, larger country, as with The Vatican and Italy, or Lesotho and South Africa.\n\nTerritory leased or ceded by one country to another for perpetual use, but not in sovereignty, such as Guantanamo Bay Naval Base in Cuba, or memorials, such as the American Cemetery in France, do not constitute true territorial borders, because the land occupied remains a formal part of the host country.\n\nThis list is based on the Correlates of War Direct Contiguity data set, with causeways and bridges not being counted. \n\nBorders relevant to this list may include short borders in the middle of man-made constructions.\n\nIn some cases, a dependent territory of one nation borders another nation.\n\n\nThere were many countries that historically had only one neighbour. Some no longer exist while others now have either no land borders or borders with more than one nation due to border changes.\n\n\n"}
{"id": "28292127", "url": "https://en.wikipedia.org/wiki?curid=28292127", "title": "List of eponymous surgical procedures", "text": "List of eponymous surgical procedures\n\nEponymous surgical procedures are generally named after the surgeon or surgeons who performed or reported them first. In some instances they are named after the surgeon who popularised them or refined existing procedures, and occasionally are named after the patient who first underwent the procedure.\n"}
{"id": "660678", "url": "https://en.wikipedia.org/wiki?curid=660678", "title": "List of glaciers", "text": "List of glaciers\n\nA glacier ( ) or () is a persistent body of dense ice that is constantly moving under its own weight; it forms where the accumulation of snow exceeds its ablation (melting and sublimation) over many years, often centuries. Glaciers slowly deform and flow due to stresses induced by their weight, creating crevasses, seracs, and other distinguishing features. Because glacial mass is affected by long-term climate changes, e.g., precipitation, mean temperature, and cloud cover, glacial mass changes are considered among the most sensitive indicators of climate change.\n\nAfrica, specifically East Africa, has contained glacial regions, possibly as far back as the last glacier maximum 10 to 15 thousand years ago. Seasonal snow does exist on the highest peaks of East Africa as well as in the Drakensberg Range of South Africa, the Stormberg Mountains, and the Atlas Mountains in Morocco. Currently, the only remaining glaciers on the continent exist on Mount Kilimanjaro, Mount Kenya, and the Rwenzori.\n\nThere are many glaciers in the Antarctic. This set of lists does not include ice sheets, ice caps or ice fields, such as the Antarctic ice sheet, but includes glacial features that are defined by their flow, rather than general bodies of ice. The lists include outlet glaciers, valley glaciers, cirque glaciers, tidewater glaciers and ice streams. Ice streams are a type of glacier and many of them have \"glacier\" in their name, e.g. Pine Island Glacier. Ice shelves are listed separately in the List of Antarctic ice shelves. For the purposes of these lists, the Antarctic is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty System).\n\nThere are also glaciers in the subantarctic. This includes one snow field (Murray Snowfield). Snow fields are not glaciers in the strict sense of the word, but they are commonly found at the accumulation zone or head of a glacier. For the purposes of this list, Antarctica is defined as any latitude further south than 60° (the continental limit according to the Antarctic Treaty).\n\nThe majority of Europe's glaciers are found in the Alps, Caucasus and the Scandinavian Mountains (mostly Norway) as well as in Iceland. Iceland has the largest glacier in Europe, Vatnajökull glacier, that covers between 8,100-8,300 km² in area and 3,100 km³ in volume. Norway alone has more than 2500 glaciers (including very small ones) covering an estimated 1% of mainland Norway's surface area. Several of mainland Europe's biggest glaciers are found here including; Jostedalsbreen(the largest in mainland Europe at 487 km), Vestre Svartisen(221 km), Søndre Folgefonna(168 km) and Østre Svartisen(148 km). The two Svartisen glaciers used to be one connected entity during the Little Ice Age but has since separated.\n\n\nThere are a number of glaciers existing in North America, currently or in recent centuries. In the United States, these glaciers are located in nine states, all in the Rocky Mountains or further west. The southernmost named glacier among them is the Lilliput Glacier in Tulare County, east of the Central Valley of California.\n\nMexico has about two dozen glaciers, all of which are located on Pico de Orizaba (Citlaltépetl), Popocatépetl and Iztaccíhuatl, the three tallest mountains in the country.\n\n\nGlaciers in South America develop exclusively on the Andes and are subject of the Andes various climatic regimes namely the Tropical Andes, Dry Andes and the Wet Andes. Apart from this there is a wide range of latitudes on which glaciers develop from 5000 m in the Altiplano mountains and volcanoes to reaching sealevel as tidewater glaciers from San Rafael Lagoon (45° S) and southwards. South America hosts two large ice fields, the Northern and Southern Patagonian Ice Fields, of which the second is the largest contiguous body of glaciers in extrapolar regions.\n\nThe glaciers of Chile cover 2.7% (20,188 km) of the land area of the country, excluding Antártica Chilena, and have a considerable impact on its landscape and water supply. By surface 80% of South America’s glaciers lie in Chile. Glaciers develop in the Andes of Chile from 27˚S southwards and in a very few places north of 18°30'S in the extreme north of the country: in between they are absent because of extreme aridity, though rock glaciers formed from permafrost are common. The largest glaciers of Chile are the Northern and Southern Patagonian Ice Fields. From a latitude of 47° S and south some glaciers reach sea level.\n\nApart from height and latitude, the settings of Chilean glaciers depend on precipitation patterns; in this sense two different regions exist: the Dry Andes and the Wet Andes.\n\nNo glaciers remain on the Australia mainland or Tasmania. A few, like the Heard Island glaciers are located in the territory of Heard Island and McDonald Islands in the southern Indian Ocean.\n\nNew Guinea has the Puncak Jaya glacier.\n\nNew Zealand contains many glaciers, mostly located near the Main Divide of the Southern Alps in the South Island. They are classed as mid-latitude mountain glaciers. There are eighteen small glaciers in the North Island on Mount Ruapehu.\n\nAn inventory of South Island glaciers compiled in the 1980s indicated there were about 3,155 glaciers with an area of at least one hectare (2.5 acres). Approximately one sixth of these glaciers covered more than 10 hectares. These include:\n\n\nThe following is the list of longest glaciers in the non-polar regions, generally regarded as between 60 degrees north and 60 degrees south latitude, though some definitions expand it slightly.\n\n"}
{"id": "10702216", "url": "https://en.wikipedia.org/wiki?curid=10702216", "title": "List of important publications in philosophy", "text": "List of important publications in philosophy\n\nThis is a list of important publications in philosophy, organized by field. The publications on this list are regarded as important because they have served or are serving as one or more of the following roles:\n\n\n\n\n\"Thirukkural\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21153493", "url": "https://en.wikipedia.org/wiki?curid=21153493", "title": "List of technical standard organisations", "text": "List of technical standard organisations\n\n\n\n\n\n\n\nThis list is not limited to ISO members.\n\n\n"}
{"id": "6662356", "url": "https://en.wikipedia.org/wiki?curid=6662356", "title": "List of volcanoes in Canada", "text": "List of volcanoes in Canada\n\nA list of volcanoes in Canada.\n\n"}
{"id": "578519", "url": "https://en.wikipedia.org/wiki?curid=578519", "title": "Marcel Benoist Prize", "text": "Marcel Benoist Prize\n\nThe Marcel Benoist Prize, offered by the Marcel Benoist Foundation, is a monetary prize that has been offered annually since 1920 to a scientist of Swiss nationality or residency who has made the most useful scientific discovery. Emphasis is placed on those discoveries affecting human life. Since 1997, candidates in the humanities have also been eligible for the prize.\n\nThe Marcel Benoist Foundation was established by the will of the French lawyer Marcel Benoist, a wartime resident of Lausanne, who died in 1918. It is managed by a group of trustees comprising the Swiss interior minister and heads of the main Swiss universities. It is often dubbed the \"Swiss Nobel Prize.\" The current laureate is Lars-Erik Cederman.\n\nThe first award was given to immunologist Maurice Arthus (1862–1945) at the University of Lausanne. Other winners have included computer scientist Niklaus Wirth, astronomer Michel Mayor, and cardiologist Max Holzmann. , nine Marcel Benoist winners have later also won the Nobel Prize: Paul Karrer, Walter R. Hess, Leopold Ruzicka, Tadeus Reichstein, Vladimir Prelog, Niels Kaj Jerne, Richard R. Ernst, Johannes G. Bednorz & Karl Alexander Müller, and Kurt Wüthrich.\n\nIn 2009, Françoise Gisou van der Goot (École polytechnique fédérale de Lausanne) was the first woman to win the Marcel Benoist Prize.\n\n\n"}
{"id": "76653", "url": "https://en.wikipedia.org/wiki?curid=76653", "title": "Naval architecture", "text": "Naval architecture\n\nNaval architecture, or naval engineering, along with automotive engineering and aerospace engineering, is an engineering discipline branch of vehicle engineering, incorporating elements of mechanical, electrical, electronic, software and safety engineering as applied to the engineering design process, shipbuilding, maintenance, and operation of marine vessels and structures. Naval architecture involves basic and applied research, design, development, design evaluation and calculations during all stages of the life of a marine vehicle. Preliminary design of the vessel, its detailed design, construction, trials, operation and maintenance, launching and dry-docking are the main activities involved. Ship design calculations are also required for ships being modified (by means of conversion, rebuilding, modernization, or repair). Naval architecture also involves formulation of safety regulations and damage-control rules and the approval and certification of ship designs to meet statutory and non-statutory requirements.\nThe word \"vessel\" includes every description of watercraft, including non-displacement craft, WIG craft and seaplanes, used or capable of being used as a means of transportation on water. The principal elements of naval architecture are:\n\nHydrostatics concerns the conditions to which the vessel is subjected while at rest in water and to its ability to remain afloat. This involves computing buoyancy, displacement, and other hydrostatic properties such as trim (the measure of the longitudinal inclination of the vessel) and stability (the ability of a vessel to restore itself to an upright position after being inclined by wind, sea, or loading conditions).\n\nHydrodynamics concerns the flow of water around the ship's hull, bow, and stern, and over bodies such as propeller blades or rudder, or through thruster tunnels. \nResistance – resistance towards motion in water primarily caused due to flow of water around the hull. Powering calculation is done based on this.\nPropulsion – to move the vessel through water using propellers, thrusters, water jets, sails etc. Engine types are mainly internal combustion. Some vessels are electrically powered using nuclear or solar energy.\nShip motions – involves motions of the vessel in seaway and its responses in waves and wind.\nControllability (maneuvering) – involves controlling and maintaining position and direction of the vessel.\n\nWhile atop a liquid surface a floating body has 6 degrees of freedom in its movements, these are categorized in either rotation or translation.\n\nLongitudinal stability for longitudinal inclinations, the stability depends upon the distance between the center of gravity and the longitudinal meta-center. In other words, the basis in which the ship maintains its center of gravity is its distance set equally apart from both the aft and forward section of the ship.\n\nWhile a body floats on a liquid surface it still encounters the force of gravity pushing down on it. In order to stay afloat and avoid sinking there is an opposed force acting against the body known as the hydrostatic pressures. The forces acting on the body must be of the same magnitude and same line of motion in order to maintain the body at equilibrium. This description of equilibrium is only present when a freely floating body is in still water, when other conditions are present the magnitude of which these forces shifts drastically creating the swaying motion of the body.\n\nThe buoyancy force is equal to the weight of the body, in other words, the mass of the body is equal to the mass of the water displaced by the body. This adds an upward force to the body by the amount of surface area times the area displaced in order to create an equilibrium between the surface of the body and the surface of the water.\n\nThe stability of a ship under most conditions is able to overcome any form or restriction or resistance encountered in rough seas; however, ships have undesirable roll characteristics when the balance of oscillations in roll is two times that of oscillations in heave, thus causing the ship to capsize.\n\nStructures involves selection of material of construction, structural analysis of global and local strength of the vessel, vibration of the structural components and structural responses of the vessel during motions in seaway. Depending on the type of ship, the structure and design will vary in what material to use as well as how much of it. Some ships are made from glass reinforced plastics but the vast majority are steel with possibly some aluminium in the superstructure. The complete structure of the ship is designed with panels shaped in a rectangular form consisting of steel plating supported on four edges. Combined in a large surface area the Grillages create the hull of the ship, deck, and bulkheads while still providing mutual support of the frames. Though the structure of the ship is sturdy enough to hold itself together the main force it has to overcome is longitudinal bending creating a strain against its hull, its structure must be designed so that the material is disposed as much forward and aft as possible. The principal longitudinal elements are the deck, shell plating, inner bottom all of which are in the form of grillages, and additional longitudinal stretching to these. The dimensions of the ship are in order to create enough spacing between the stiffeners in prevention of buckling. Warships have used a longitudinal system of stiffening that many modern commercial vessels have adopted. This system was widely used in early merchant ships such as the SS Great Eastern, but later shifted to transversely framed structure another concept in ship hull design that proved more practical. This system was later implemented on modern vessels such as tankers because of its popularity and was then named the Isherwood system. The arrangement of the Isherwood system consists of stiffening decks both side and bottom by longitudinal members, they are separated enough so they have the same distance between them as the frames and beams. This system works by spacing out the transverse members that support the longitudinal by about 3 or 4 meters, with the wide spacing this causes the traverse strength needed by displacing the amount of force the bulkheads provide.\n\nArrangements involves concept design, layout and access, fire protection, allocation of spaces, ergonomics and capacity.\n\nConstruction depends on the material used. When steel or aluminium is used this involves welding of the plates and profiles after rolling, marking, cutting and bending as per the structural design drawings or models, followed by erection and launching. Other joining techniques are used for other materials like fibre reinforced plastic and glass-reinforced plastic.The process of construction is thought-out cautiously while considering all factors like safety, strength of structure, hydrodynamics, and ship arrangement. Each factor considered presents a new option for materials to consider as well as ship orientation. When the strength of the structure is considered the acts of ship collision are considered in the way that the ships structure is altered. Therefore, the properties of materials are considered carefully as applied material on the struck ship has elastic properties, the energy absorbed by the ship being struck is then deflected in the opposite direction, so both ships go through the process of rebounding to prevent further damage.\nTraditionally, naval architecture has been more craft than science. The suitability of a vessel's shape was judged by looking at a half-model of a vessel or a prototype. Ungainly shapes or abrupt transitions were frowned on as being flawed. This included rigging, deck arrangements, and even fixtures. Subjective descriptors such as ungainly, full, and fine were used as a substitute for the more precise terms used today. A vessel was, and still is described as having a ‘fair’ shape. The term ‘fair’ is meant to denote not only a smooth transition from fore to aft but also a shape that was ‘right.’ Determining what is ‘right’ in a particular situation in the absence of definitive supporting analysis encompasses the art of naval architecture to this day.\n\nModern low-cost digital computers and dedicated software, combined with extensive research to correlate full-scale, towing tank and computational data, have enabled naval architects to more accurately predict the performance of a marine vehicle. These tools are used for static stability (intact and damaged), dynamic stability, resistance, powering, hull development, structural analysis, green water modelling, and slamming analysis. Data are regularly shared in international conferences sponsored by RINA, Society of Naval Architects and Marine Engineers (SNAME) and others. Computational Fluid Dynamics is being applied to predict the response of a floating body in a random sea.\n\nDue to the complexity associated with operating in a marine environment, naval architecture is a co-operative effort between groups of technically skilled individuals who are specialists in particular fields, often coordinated by a lead naval architect. This inherent complexity also means that the analytical tools available are much less evolved than those for designing aircraft, cars and even spacecraft. This is due primarily to the paucity of data on the environment the marine vehicle is required to work in and the complexity of the interaction of waves and wind on a marine structure.\n\nA naval architect is an engineer who is responsible for the design, construction, and/or repair of ships, boats, other marine vessels, and offshore structures, both commercial and military, including:\n\nSome of these vessels are amongst the largest (such as supertankers), most complex (such as Aircraft carriers), and highly valued movable structures produced by mankind. They are typically the most efficient method of transporting the world's raw materials and products. Modern engineering on this scale is essentially a team activity conducted by specialists in their respective fields and disciplines. \nNaval architects integrate these activities. This demanding leadership role requires managerial qualities and the ability to bring together the often-conflicting demands of the various design constraints to produce a product which is fit for the purpose.\n\nIn addition to this leadership role, a naval architect also has a specialist function in ensuring that a safe, economic, environmentally sound and seaworthy design is produced. To undertake all these tasks, a naval architect must have an understanding of many branches of engineering and must be in the forefront of high technology areas. He or she must be able to effectively utilize the services provided by scientists, lawyers, accountants, and business people of many kinds.\n\nNaval architects typically work for shipyards, ship owners, design firms and consultancies, equipment manufacturers, Classification societies, regulatory bodies (Admiralty law), navies, and governments.\n\n"}
{"id": "16843758", "url": "https://en.wikipedia.org/wiki?curid=16843758", "title": "Newcomb Cleveland Prize", "text": "Newcomb Cleveland Prize\n\nThe Newcomb Cleveland Prize of the American Association for the Advancement of Science (AAAS) is annually awarded to author(s) of outstanding scientific paper published in the Research Articles or Reports sections of \"Science\". Established in 1923, funded by Newcomb Cleveland who remained anonymous until his death in 1951, and for this period it was known as the AAAS Thousand Dollar Prize. \"The prize was inspired by Mr. Cleveland's belief that it was the scientist who counted and who needed the encouragement an unexpected monetary award could give.\" The present rules were instituted in 1975, previously it had gone to the author(s) of noteworthy papers, representing an outstanding contribution to science, presented in a regular session, sectional or societal, during the AAAS Annual Meeting. It is now sponsored by the Fodor Family Trust and includes a prize of $25,000.\n\nList of winners\n\n"}
{"id": "19077833", "url": "https://en.wikipedia.org/wiki?curid=19077833", "title": "Niklaus Gerber", "text": "Niklaus Gerber\n\nNiklaus Gerber (8 June 1850 – 9 February 1914) was a Swiss dairy chemist and industrialist. He was born in 1850 in Thun, Switzerland. He attended the University of Bern and University of Zurich, studied chemistry in Paris and Munich and spent 2 years at the Swiss-American Milk Co. in Little Falls, New York.\n\nIn 1887, Gerber founded United Dairies of Zurich. At this time, the quality of raw milk was poor due to lack of hygiene. Further, dishonest dairymen would dilute raw milk with water and no means existed to effectively test the milk. In 1892, Dr. Gerber developed a method of analyzing fat content in milk in a relatively fast, simple and reliable manner for the time. Niklaus obtained a patent on this \"Acid-Butyrometry,\" which came to be known as the \"Gerber Method\". Although the method was originally developed for use only by United Dairies, Dr. Gerber began to sell the equipment to milk processors globally and created a separate company to commercialise the Gerber Method.\n\nIn 1904, Gerber founded the \"Dr. N. Gerber's Acid-Butyrometry Ltd., Leipzig\", which later merged with another entity to create \"Dr. N. Gerber's m.b.H Zurich and Leipzig\" to produce and develop the Gerber instruments.\n\nGerber died in 1914.\n\nThe Gerber method remains in wide use throughout much of the world. The Babcock test is similar and is more widely used in the United States.\n"}
{"id": "10811870", "url": "https://en.wikipedia.org/wiki?curid=10811870", "title": "Nina and the Neurons", "text": "Nina and the Neurons\n\nNina and the Neurons is a programme shown on the CBeebies channel aimed at four to six-year-olds to help them understand basic science. Nina is a neuroscientist who enlists the help of five Neurons (animated characters representing the senses) in her brain to answer a scientific question.\n\nThe show is produced by Lucille McLaughlin, who has also produced the children's programmes Balamory, Me Too! and Bits and Bobs. The series is commissioned by CBeebies Controller, Michael Carrington.\n\nMost of the show is based at Glasgow Science Centre, with a small part taking place outdoors. At the start of the show, Nina conducts experiments in front of an unseen audience of children. At one point of the show, Nina is 'contacted' by (usually two or three, but rarely four) children, who appears on a computer screen asking a science-related question (e.g., ‘What makes rainbows appear and disappear?’) Nina then chooses one (or more) of the five Neurons inside her brain based upon which of the senses is most appropriate to answer the question. Once the Neuron has been selected, the children (called the 'experimenters') then visit Nina, using fun experiments and games.\n\nAfterwards, Nina takes the children out to find out more about the answer to the question, sometimes with the help of their friends and family. After they have found out the answer to the question, they travel back to the Glasgow Science Centre to do another experiment. Then, the ‘experimenters’ leave.\n\nAt the end of each show, a song is sung, which changes from series to series depending on the theme of the series. Then, the Neurons discuss what they have done and the individual role they have played. The show ends with Nina and the Neurons bidding farewell to the viewers.\n\nThe main character of Nina is played by Scottish actress Katrina Bryan. She is a scientist who works in the lab. Nina wears either a white lab coat with brightly coloured cuffs and lapels in her lab, or one of a bright yellow coat or blue jacket when outside. In the programme, Nina drives either a pale blue 1970s Volkswagen Type 2 (Transporter) minibus or a New MINI, both bearing numberplates reading 'NINA', or rides a bicycle in the Go ECO! version.\n\nThe Neurons are computer animated characters (stylised with human facial features and body, but no legs) who live inside Nina’s brain, with no legs, and are named to reflect the five senses which they represent:\n\nFelix, voiced by James Dreyfus in the first series but subsequently by Lewis MacLeod represents touch. He is green and speaks in a posh accent and is particular about his appearance.\n\nBelle, voiced by Kelly Harrison represents hearing. She is red but also loud and can be bossy. She also the group vice leader.\n\nLuke, voiced by Patrice Naiambana, represents sight is yellow and is also the group leader. His character is laid back and relaxed.\n\nOllie, voiced by Siobhan Redmond represents smell. She is purple and described as 'sweet, self-assured and a bit of a goth.' She is the elder sister of Bud.\n\nBud, voiced by Sharon Small represents the taste neuron. He is blue and also the youngest Neuron and can be enthusiastic and easily excited. He is the younger brother of Ollie.\n\nBud and Ollie are often chosen together by Nina due to the way taste and smell work together.\n\nFelix and Luke got chosen together by Nina due to the way touch and sight work together.\n\nBelle and Luke got chosen together by Nina due to the way hearing and sight work together.\n\nFelix and Belle got chosen together by Nina due to the way touch and hearing work together.\n\nComposer: Scottish Composer Graham Ness\n\nThere is a themed \"Nina & the Neurons\" activity trail at the Glasgow Science Centre.\n\nThe first series began on 26 February 2007 and ended on 30 March 2007. The 2nd series began airing on Cbeebies on 31 March 2008 and ended on 2 May 2008. It was followed by a third, this time called Nina and the Neurons: Go Eco!, on 13 June 2008 as part of CBeebies' year-long green initiative called EcoBeebies, which ended on 15 August 2008. A fourth series called Nina and the Neurons: Go Inventing started on 18 May 2009, where Nina invites several young inventors to her lab to discover how things work by inventing their own versions. It ended on 19 June 2009. The fifth series, Nina and the Neurons: In The Lab, was broadcast from 27 September 2010 to 10 December 2010. In the show, Nina and the experimenters discover changes and reaction.\n\nA sixth series, called Nina and the Neurons: Brilliant Bodies, premiered on 5 September 2011. The show focuses on the parts of the human body. The seventh series, Nina and the Neurons: Go Engineering, is another series about inventions. It started in 2013. Series 8 is called Nina and the Neurons: Earth Explorers, explores the Earth, the sea and beyond. The series also started in 2013. The last three series, Get Sporty (2014), Go Digital (2015) and Get Building (2016) focus on sport, gadgets and building respectively.\n\n\n\n\n\n\n\nA series about Engineering feats consisting of 25 episodes (15 minutes each) started being broadcast on 14 January 2013. Each episode tries to simplify a tricky science concept into three key experiments which children can participate in. (Episode info from Subtitles Portal's \"Nina and the Neurons\" Microsite)\n\n\nThis series looks at various geographical and geological features such as lakes, caves, volcanoes, etc.\n\n\n\n\n\n"}
{"id": "25392304", "url": "https://en.wikipedia.org/wiki?curid=25392304", "title": "Orgel diagram", "text": "Orgel diagram\n\nOrgel diagrams are correlation diagrams which show the relative energies of electronic terms in transition metal complexes, much like Tanabe–Sugano diagrams. They are named after their creator, Leslie Orgel. Orgel diagrams are restricted to only show weak field (i.e. high spin) cases, and offer no information about strong field (low spin) cases. Because Orgel diagrams are qualitative, no energy calculations can be performed from these diagrams; also, Orgel diagrams only show the symmetry states of the highest spin multiplicity instead of all possible terms, unlike a Tanabe–Sugano diagram. Orgel diagrams will, however, show the number of spin allowed transitions, along with their respective symmetry designations. In an Orgel diagram, the parent term (P, D, or F) in the presence of no ligand field is located in the center of the diagram, with the terms due to that electronic configuration in a ligand field at each side. There are two Orgel diagrams, one for d, d, d, and d configurations and the other with d, d, d, and d configurations.\n\nIn an Orgel diagram lines with the same Russell – Saunders terms will diverge due to the non-crossing rule, but all other lines will be linear. Also, for the D Orgel diagram, the left side contains d and d tetrahedral and d and d octahedral complexes. The right side contains d and d tetrahedral and d and d octahedral complexes. For the F Orgel diagram, the left side contains d and d tetrahedral and d and d octahedral complexes. The right side contains d and d tetrahedral and d and high spin d octahedral complexes.\n\n"}
{"id": "596816", "url": "https://en.wikipedia.org/wiki?curid=596816", "title": "Particle-in-cell", "text": "Particle-in-cell\n\nThe particle-in-cell (PIC) method refers to a technique used to solve a certain class of partial differential equations. In this method, individual particles (or fluid elements) in a Lagrangian frame are tracked in continuous phase space, whereas moments of the distribution such as densities and currents are computed simultaneously on Eulerian (stationary) mesh points.\n\nPIC methods were already in use as early as 1955,\neven before the first Fortran compilers were available. The method gained popularity for plasma simulation in the late 1950s and early 1960s by Buneman, Dawson, Hockney, Birdsall, Morse and others. In plasma physics applications, the method amounts to following the trajectories of charged particles in self-consistent electromagnetic (or electrostatic) fields computed on a fixed mesh. \n\nFor many types of problems, the classical PIC method invented by Buneman, Dawson, Hockney, Birdsall, Morse and others is relatively intuitive and straightforward to implement. This probably accounts for much of its success, particularly for plasma simulation, for which the method typically includes the following procedures:\n\nModels which include interactions of particles only through the average fields are called PM (particle-mesh). Those which include direct binary interactions are PP (particle-particle). Models with both types of interactions are called PP-PM or PM.\n\nSince the early days, it has been recognized that the PIC method is susceptible to error from so-called \"discrete particle noise\".\n\nThis error is statistical in nature, and today it remains less-well understood than for traditional fixed-grid methods, such as Eulerian or semi-Lagrangian schemes.\n\nModern geometric PIC algorithms are based on a very different theoretical framework. These algorithms use tools of discrete manifold, interpolating differential forms, and canonical or non-canonical symplectic integrators to guarantee gauge invariant and conservation of charge, energy-momentum, and more importantly the infinitely dimensional symplectic structure of the particle-field system.\nThese desired features are attributed to the fact that geometric PIC algorithms are built on the more fundamental field-theoretical framework and are directly linked to the perfect form, i.e., the variational principle of physics.\n\nInside the plasma research community, systems of different species (electrons, ions, neutrals, molecules, dust particles, etc.) are investigated. The set of equations associated with PIC codes are therefore the Lorentz force as the equation of motion, solved in the so-called \"pusher\" or \"particle mover\" of the code, and Maxwell's equations determining the electric and magnetic fields, calculated in the \"(field) solver\".\n\nThe real systems studied are often extremely large in terms of the number of particles they contain. In order to make simulations efficient or at all possible, so-called \"super-particles\" are used. A super-particle (or \"macroparticle\") is a computational particle that represents many real particles; it may be millions of electrons or ions in the case of a plasma simulation, or, for instance, a vortex element in a fluid simulation. It is allowed to rescale the number of particles, because the Lorentz force depends only on the charge-to-mass ratio, so a super-particle will follow the same trajectory as a real particle would.\n\nThe number of real particles corresponding to a super-particle must be chosen such that sufficient statistics can be collected on the particle motion. If there is a significant difference between the density of different species in the system (between ions and neutrals, for instance), separate real to super-particle ratios can be used for them.\n\nEven with super-particles, the number of simulated particles is usually very large (> 10), and often the particle mover is the most time consuming part of PIC, since it has to be done for each particle separately. Thus, the pusher is required to be of high accuracy and speed and much effort is spent on optimizing the different schemes.\n\nThe schemes used for the particle mover can be split into two categories, implicit and explicit solvers. While implicit solvers(e.g. implicit Euler scheme) calculate the particle velocity from the already updated fields, explicit solvers use only the old force from the previous time step, and are therefore simpler and faster, but require a smaller time step. Two frequently used schemes are the leapfrog method, and the \"Boris scheme\", \n\nFor plasma applications, the leapfrog method takes the following form:\n\nwhere the subscript formula_3 refers to \"old\" quantities from the previous time step, formula_4 to updated quantities from the next time step (i.e. formula_5), and velocities are calculated in-between the usual time steps formula_6.\n\nIn comparison, the equations of the Boris scheme are:\n\nwith \nand formula_13.\n\nBecause of its excellent long term accuracy, the Boris algorithm is the de facto standard for advancing a charged particle. It was realized that the excellent long term accuracy of nonrelativistic Boris algorithm is due to the fact it conserves phase space volume, even though it is not symplectic. The global bound on energy error typically associated with symplectic algorithms still holds for the Boris algorithm, making it an effective algorithm for the multi-scale dynamics of plasmas. It has also been shown\n\nthat one can improve on the relativistic Boris push to make it both volume preserving and have a constant-velocity solution in crossed E and B fields.\n\nThe most commonly used methods for solving Maxwell's equations (or more generally, partial differential equations (PDE)) belong to one of the following three categories:\n\nWith the FDM, the continuous domain is replaced with a discrete grid of points, on which the electric and magnetic fields are calculated. Derivatives are then approximated with differences between neighboring grid-point values and thus PDEs are turned into algebraic equations.\n\nUsing FEM, the continuous domain is divided into a discrete mesh of elements. The PDEs are treated as an eigenvalue problem and initially a trial solution is calculated using basis functions that are localized in each element. The final solution is then obtained by optimization until the required accuracy is reached.\n\nAlso spectral methods, such as the fast Fourier transform (FFT), transform the PDEs into an eigenvalue problem, but this time the basis functions are high order and defined globally over the whole domain. The domain itself is not discretized in this case, it remains continuous. Again, a trial solution is found by inserting the basis functions into the eigenvalue equation and then optimized to determine the best values of the initial trial parameters.\n\nThe name \"particle-in-cell\" originates in the way that plasma macro-quantities (number density, current density, etc.) are assigned to simulation particles (i.e., the \"particle weighting\"). Particles can be situated anywhere on the continuous domain, but macro-quantities are calculated only on the mesh points, just as the fields are. To obtain the macro-quantities, one assumes that the particles have a given \"shape\" determined by the shape function\n\nwhere formula_15 is the coordinate of the particle and formula_16 the observation point. Perhaps the easiest and most used choice for the shape function is the so-called \"cloud-in-cell\" (CIC) scheme, which is a first order (linear) weighting scheme. Whatever the scheme is, the shape function has to satisfy the following conditions:\n\nspace isotropy, charge conservation, and increasing accuracy (convergence) for higher-order terms.\n\nThe fields obtained from the field solver are determined only on the grid points and can't be used directly in the particle mover to calculate the force acting on particles, but have to be interpolated via the \"field weighting\":\n\nwhere the subscript formula_18 labels the grid point. To ensure that the forces acting on particles are self-consistently obtained, the way of calculating macro-quantities from particle positions on the grid points and interpolating fields from grid points to particle positions has to be consistent, too, since they both appear in Maxwell's equations. Above all, the field interpolation scheme should conserve momentum. This can be achieved by choosing the same weighting scheme for particles and fields and by ensuring the appropriate space symmetry (i.e. no self-force and fulfilling the action-reaction law) of the field solver at the same time\n\nAs the field solver is required to be free of self-forces, inside a cell the field generated by a particle must decrease with decreasing distance from the particle, and hence inter-particle forces inside the cells are underestimated. This can be balanced with the aid of Coulomb collisions between charged particles. Simulating the interaction for every pair of a big system would be computationally too expensive, so several Monte Carlo methods have been developed instead. A widely used method is the \"binary collision model\", in which particles are grouped according to their cell, then these particles are paired randomly, and finally the pairs are collided.\n\nIn a real plasma, many other reactions may play a role, ranging from elastic collisions, such as collisions between charged and neutral particles, over inelastic collisions, such as electron-neutral ionization collision, to chemical reactions; each of them requiring separate treatment. Most of the collision models handling charged-neutral collisions use either the \"direct Monte-Carlo\" scheme, in which all particles carry information about their collision probability, or the \"null-collision\" scheme, which does not analyze all particles but uses the maximum collision probability for each charged species instead.\n\nAs in every simulation method, also in PIC, the time step and the grid size must be well chosen, so that the time and length scale phenomena of interest are properly resolved in the problem. In addition, time step and grid size affect the speed and accuracy of the code.\n\nFor an electrostatic plasma simulation using an explicit time integration scheme (e.g. leapfrog, which is most commonly used), two important conditions regarding the grid size formula_19 and the time step formula_20 should be fulfilled in order to ensure the stability of the solution:\n\nwhich can be derived considering the harmonic oscillations of a one-dimensional unmagnetized plasma. The latter conditions is strictly required but practical considerations related to energy conservation suggest to use a much stricter constraint where the factor 2 is replaced by a number one order of magnitude smaller. The use of formula_23 is typical. Not surprisingly, the natural time scale in the plasma is given by the inverse plasma frequency formula_24 and length scale by the Debye length formula_25.\n\nFor an explicit electromagnetic plasma simulation, the time step must also satisfy the CFL condition:\nwhere formula_27, and formula_28 is the speed of light.\n\nWithin plasma physics, PIC simulation has been used successfully to study laser-plasma interactions, electron acceleration and ion heating in the auroral ionosphere, magnetohydrodynamics, magnetic reconnection, as well as ion-temperature-gradient and other microinstabilities in tokamaks, furthermore vacuum discharges, and dusty plasmas.\n\nHybrid models may use the PIC method for the kinetic treatment of some species, while other species (that are Maxwellian) are simulated with a fluid model.\n\nPIC simulations have also been applied outside of plasma physics to problems in solid and fluid mechanics.\n\n\n"}
{"id": "27262352", "url": "https://en.wikipedia.org/wiki?curid=27262352", "title": "Spatial contextual awareness", "text": "Spatial contextual awareness\n\nSpatial contextual awareness consociates contextual information such as an individual's or sensor's location, activity, the time of day, and proximity to other people or objects and devices. It is also defined as the relationship between and synthesis of information garnered from the spatial environment, a cognitive agent, and a cartographic map. The spatial environment is the physical space in which the orientation or wayfinding task is to be conducted; the cognitive agent is the person or entity charged with completing a task; and the map is the representation of the environment which is used as a tool to complete the task.\n\nAn incomplete view of spatial contextual awareness would render it as simply a contributor to or an element of contextual awareness – that which specifies a point location on the earth. This narrow definition omits the individual cognitive and computational functions involved in a complex geographic system. Rather than defining the myriad of potential factors contributing to context, spatial contextual awareness defined in terms of cognitive processes permits a unique, user-centered perspective in which \"conceptualizations imbue spatial structures with meaning.\"\n\nContext awareness, geographic awareness, and ubiquitous cartography or Ubiquitous Geographic Information (UBGI) all contribute to the understanding of spatial contextual awareness. They are also key elements in a map-based, location-based service, or LBS. In cases in which the user interface for the LBS is a map, cartographic design challenges must be addressed in order to effectively communicate the spatial context to the user.\n\nSpatial contextual awareness can describe present context – the environment of the user at the present time and location, or that of a future context – where the user wants to go and what may be of interest to them in the approaching spatial environment. Some location-based services are proactive systems which can anticipate future context. Augmented reality is an application which guides a user through present and into future context by displaying spatial contextual information in their visual system as they traverse through real space.\n\nNumerous examples of LBS applications exist which require the ability to leverage spatial contextual awareness. These applications are in demand by the general public and are examples of how maps are being used by individuals to help better understand the world and make daily decisions.\n\nContext awareness originated as a term from ubiquitous computing or as so-called pervasive computing which sought to deal with linking changes in the environment with computer systems, which are otherwise static.\n\nContext is defined in multiple ways, most often with location as the cornerstone. One source defines it as \"location and the identity of nearby people and objects.\" Another describes it as \"location, identity, environment and time\". Yet some definitions recognize context awareness as being more inclusive than location.\n\nDey took this broader approach: \"context is any information that can be used to characterize the situation of an entity, where entity means a person, place, or object, which is relevant to the interaction between a user and an application, including the user and the applications themselves.\"\nThe same author defined a system \"to be context-aware if it uses context to provide relevant information and/or services to the user, in which the relevancy depends on the user's task\".\nThe concept of relevancy is described in the following definition of context awareness: \"the set of environmental states and settings that either determines an application's behavior or in which an application event occurs and is interesting to the user\". Different levels of context, in terms of low and high level have also been outlined. Low-level contexts consist of time, location, network bandwidth and orientation. A high-level context consists of the user's current activity and social context.\n\nA three-level model of context awareness (Figure 1) includes the changeable nature of the environment by differentiating between the contributions of static, dynamic, and internal context:\n\nStatic content is driven by stored information while dynamic content is provided and updated by sensors.\n\nContext categories for mobile maps have been identified through pilot user tests. The categories in Table 1 were deemed useful for mobile map services:\n\nGeographic awareness, another term for spatial contextual awareness, clarifies the spatial and geographic aspects of context. Being more than simply present location, it must also include other dimensions and their interdependencies. Figure 2 shows Li's components of context awareness and overlays them on multiple geographic reference systems. To be effective, an LBS application must be able to operate in a heterogeneous space which includes different reference systems. A user of a LBS must be able to seamlessly convert from a Euclidean space (Cartesian Reference Space), to a Linear Reference Space (LRS), to indoor space (to include perhaps the floor, wing, hallway, and room number).\n\nUbiquitous geographic information (UBGI) is geographic information which is provided at any time and any place to users or systems through communication devices. Critical to the understanding of UBGI is that the information provided is based on the context of the user. UBGI is more than data. It includes a set of concepts, practices and standards for spatial and geographic information and processing for applications accessible for use by the general public.\n\nUBGI must also take into account the situation and goals of the user, or cognitive agent. For that purpose, ubiquitous computing concepts employ sensors to collect data on the user's location as well as environmental parameters.\nUbiquitous cartography is \"the ability for users to create and use maps in any place and at any time to resolve geospatial problems\". The users and creators of these maps are more than just highly trained geographers and cartographers, but include the average citizen. In contrast to the accused elitism of the GIS community in the early 80's when many advocated for separate technology because geospatial information was different and unattainable to common users or systems, today's goal of ubiquity is to make the user experience with GIS-enabled devices intuitive and simple to use. These devices and other multimedia cartography tools are playing a major role in the effort to get \"maps out\" to the general public and end the inexcusable practice of perfecting maps as a visualization form only for expert map users operating highly specialized Geographic Information Systems.\n\nThe \"ease-of-use\" objective of ubiquitous cartography can be seen as the fourth generation in the evolution of geographic information. UBGI was preceded by easily accessible of internet maps and the addition of contextual information of LBS and mobile mapping. Digital geographic information was an essential precursor to accessible and mobile maps and these advancements are all an outgrowth of the first generation of paper maps and the effort to better represent and visualize the world (Fig. 3).\n\nA location-based service (LBS) is an information and entertainment service, accessible with mobile devices through the mobile network and utilizing the ability to make use of the geographical position of the mobile device.\n\nLBS services can be used in a variety of contexts, such as health, work, personal life, etc. LBS services include services to identify a location of a person or object, such as discovering the nearest banking cash machine or the whereabouts of a friend or employee. LBS services include parcel tracking and vehicle tracking services. LBS can include mobile commerce when taking the form of coupons or advertising directed at customers based on their current location. They include personalized weather services and even location-based games. They are an example of telecommunication convergence.\n\nLocation Based Services have the ability to exploit knowledge about the location of a user or an information device. Whether the output of the device is a simple text message or an interactive graphic map, the user and the user's location are in some way incorporated into the overall system.\n\nOther distinguishing characteristics of LBS include:\n\nLBS can be used to answer user questions which can be placed into four general categories: location, proximity, navigation, and events. Examples include:\n\nAnother category is \"measurement\" to answer the question, how far away is my destination? This is a routine function of personal automobile navigation devices.\n\nNew, innovative ideas continue to add to the types of questions in which LBS can answer for a user. For example, computer vision and object based indexing can be used to both identify an object and assist a user in navigating from the location. Spatial contextual awareness plays a key role in this process as it provides an initial geo-reference of the location while simplifying the object recognition process to a manageable degree. This category of LBS use can be called \"identification\" and answers the question \"What is it?\"\n\nApplications which require the use of spatial contextual awareness in LBS are confronted with a multitude of cartographic challenges and decisions. Some of these challenges are due to the small displays of the typical PDA user interface and method of use. Other problems result from the large volume of potentially relevant contextual data as difficult choices need to be made on the most important content to display.\n\nA sampling of some of these challenges are:\n\n\n\n\n"}
{"id": "42456725", "url": "https://en.wikipedia.org/wiki?curid=42456725", "title": "Steve Mould", "text": "Steve Mould\n\nSteve Mould (born 5 October 1978) is a British science presenter. Originally from Gateshead, United Kingdom, he is now based in London. He has two children.\n\nMould was born on 5 October 1978 in Gateshead, United Kingdom. He went to St Thomas More Catholic School, Blaydon before going on to study physics at Oxford University.\n\nIn 2014, Mould co-hosted ITV's \"I Never Knew That About Britain\" alongside Paul Martin and Suzannah Lipscomb. He has also appeared as a science expert on \"The Alan Titchmarsh Show\", \"The One Show\" and \"Blue Peter\".\n\nBetween 2008 and 2010, Mould performed three geeky sketch shows at the Edinburgh Festival Fringe with Gemma Arrowsmith.\nSince 2011, Steve has performed live science comedy as part of the comedic trio Festival of the Spoken Nerd, with mathematician Matt Parker and physicist songstress Helen Arney. Festival of the Spoken Nerd has performed at theatres, science and arts festivals.\n"}
{"id": "36901470", "url": "https://en.wikipedia.org/wiki?curid=36901470", "title": "Tunica (biology)", "text": "Tunica (biology)\n\nIn biology, a tunica () (plural tunicae) is a layer, coat, sheath, or similar covering. The word came to English from the New Latin of science and medicine. Its literal sense is about the same as that of the word \"tunic\", with which it is cognate. In biology one of its senses used to be the taxonomic name of a genus of plants, but the nomenclature has been revised and those plants are now included in the genus \"Petrorhagia\". \n\nIn modern biology in general, \"tunica\" occurs as a technical or anatomical term mainly in botany and zoology. It usually refers to membranous structures that line or cover particular organs. In many such contexts \"tunica\" is used interchangeably with \"tunic\" according to preference. An organ or organism that has a tunic(a) may be said to be \"tunicate\", as in a \"tunicate bulb\". This adjective \"tunicate\" is not to be confused with the noun \"tunicate\", which refers to a member of the subphylum \"Tunicata\".\n\nIn botany there are several contexts for the term.\n\nAs an anatomical or morphological reference in zoology, \"tunica\" has a range of applications to membranous structures in anatomy, including human anatomy. Such structures are generally coverings or capsules of organs, but also may be linings of cavities. In some cases, such as the walls of macroscopic blood vessels, layers of the tissue of the walls, whether inner, intermediate, or outer, are called \"tunica\" of one kind or another. Examples follow, but neither the list nor the discussions are exhaustive. \n"}
