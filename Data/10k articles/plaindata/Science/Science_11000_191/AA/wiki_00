{"id": "2200973", "url": "https://en.wikipedia.org/wiki?curid=2200973", "title": "A528 road", "text": "A528 road\n\nThe A528 is a route on the UK highway network that runs from Marchwiel, near Wrexham, in North Wales to Shrewsbury, Shropshire, in England On the way it passes through Ellesmere and Coton Hill. The road follows an old route that was Turnpiked in the 18th century.\n\nThe road starts south east of Wrexham at Marchwiel by a junction on the A525 road. It heads due south through Cock Bank and Overton Bridge before combining with the A539 road and going eastwards through the village of Overton-on-Dee.\n\nIt then heads south east through to Ellesmere, where it shares a small section of road with the A495, before heading south through Crosemere, Harmer Hill, Albrighton and intersecting with the A5124 at Harlescott north east of Shrewsbury.\n\nThe final short section into Shrewsbury goes over the Shrewsbury-Chester railway line and intersects with the A5191 in Shrewsbury town centre. A small stub going northwards only (due to the one-way system in Shrewsbury) starts off the A458 road.\n\nThe A528 is one of Wales' most riskiest and dangerous routes. It is third behind the A44 and the A5 for the number of road casualties per year.\n"}
{"id": "16524953", "url": "https://en.wikipedia.org/wiki?curid=16524953", "title": "Adams Glacier (Victoria Land)", "text": "Adams Glacier (Victoria Land)\n\nAdams Glacier () is a small glacier immediately south of Miers Glacier in Victoria Land. The heads of Adams and Miers glaciers, both located in the Miers Valley, are separated by a low ridge, and the east end of this ridge is almost completely surrounded by the snouts of the two glaciers, which nearly meet in the bottom of the valley, about above Lake Miers, into which they drain. It was named by the New Zealand Northern Survey Party of the Commonwealth Trans-Antarctic Expedition (1956–58) after Lieutenant (later Sir) Jameson Adams, second in command of the shore party of the British Antarctic Expedition (1907–09), who was one of the men to accompany Ernest Shackleton to within of the South Pole.\n\n"}
{"id": "47313457", "url": "https://en.wikipedia.org/wiki?curid=47313457", "title": "Aperiodic frequency", "text": "Aperiodic frequency\n\nAperiodic frequency is the rate of incidence or occurrence of non-cyclic phenomena, including random processes such as radioactive decay. It is expressed in units of measurement of reciprocal seconds (s) or, in the case of radioactivity, becquerels. \n\nIt is defined as a ratio, \"f\" = \"N\"/\"T\", involving the number of times an event happened (\"N\") during a given time duration (\"T\"); it is a physical quantity of type temporal rate.\n\n"}
{"id": "30427069", "url": "https://en.wikipedia.org/wiki?curid=30427069", "title": "Arensky Glacier", "text": "Arensky Glacier\n\nArensky Glacier () is an Antarctic glacier, lying east of Alyabiev Glacier and flows south from Beethoven Peninsula, Alexander Island, into the north end of Boccherini Inlet. The glacier was named by the USSR Academy of Sciences in 1987, after Anton Arensky, the Russian composer.\n\n"}
{"id": "44145355", "url": "https://en.wikipedia.org/wiki?curid=44145355", "title": "Arges project", "text": "Arges project\n\nThe Arges project was a research project in the field of metal-halide lamps, a form of electric lighting. The aim was to achieve a higher degree of energy efficiency in comparison to lamps used at the time. The project was commissioned by Eindhoven University of Technology and Philips Electronics. One of the problems which had to be solved was to let the experimental lamps burn reliably and constantly. Scientists believed that this problem was related to gravity, and to ascertain this experiments had to be performed in zero gravity. Therefore, part of the research took place aboard the International Space Station ISS during the Delta Mission in 2004. Dutch astronaut André Kuipers operated the apparatus transported to the ISS aboard the Soyuz TMA-4.\n\n"}
{"id": "50899232", "url": "https://en.wikipedia.org/wiki?curid=50899232", "title": "Astatic needles", "text": "Astatic needles\n\nAn astatic system comprises two equal and parallel magnetic needles, but with their polarities reversed. This arrangement protects the system from the influence of the terrestrial magnetic field, as the magnetisms of the two needles cancel each other out. Because of this phenomenon, astatic needles were often used in galvanometers.\n\n"}
{"id": "37105058", "url": "https://en.wikipedia.org/wiki?curid=37105058", "title": "Bowers Basebed", "text": "Bowers Basebed\n\nPortland Bowers Basebed is a type of limestone from Bowers Quarry at the Isle of Portland in Dorset, southern England, on the Jurassic Coast, a World Heritage Site.\n\nThe stone is clear of fossils and is the cleanest of the Portland stone types. Bowers Basebed, which is quarried by Albion Stone, has a maximum bed height of 1.95 metres. It is known for being highly durable and being able to withstand the effects of weathering.\n\nBowers Basebed was used to construct \"7–10 Old Bailey\", in the city of London.\n\n"}
{"id": "24953003", "url": "https://en.wikipedia.org/wiki?curid=24953003", "title": "British Pteridological Society", "text": "British Pteridological Society\n\nThe British Pteridological Society is for fern enthusiasts of the British Isles, and was founded in England in 1891.\n\nThe origins and early history of the BPS at the time of \"Pteridomania\" is described in the book \"The Victorian Fern Craze\".\nThe BPS celebrated its centenary in 1991; amongst other things, it was marked by the publication of the book, \"A World of Ferns\".\n\nThe British Pteridological Society is a registered charity: No. 1092399. The BPS has as its Patron HRH The Prince of Wales.\n\nThe British Pteridological Society publishes a number of works, which promote pteridology:\n\nJohn A. Wilson (1831-1914) was elected Chairman of the Society at the first meeting in 1891; subsequently Dr. F.W. Stansfield was invited to become the first President of the Society. He took office in 1892.\n\n"}
{"id": "20546074", "url": "https://en.wikipedia.org/wiki?curid=20546074", "title": "Chief scientific officer", "text": "Chief scientific officer\n\nA chief science officer (C.S.O.) is a position at the head of scientific research operations at organizations or companies performing significant scientific research projects.\n\nA C.S.O. typically is responsible for envisioning and developing research capabilities (human, methodological, and technological), for developing evidence of the validity and utility of research products, and for communicating with the scientific and customer communities concerning capabilities and scientific product offerings. \n\nIn some organizations, the same person may hold this title along with that of chief technology officer (C.T.O.). Alternatively, a company could have one or the other, or both occupied by separate people. Often, C.S.Os exists in heavily research-oriented companies, while C.T.Os exists in product development focused companies. The typical category of research and development that exists in many science and technology companies can be led by either post, depending upon which area is the organization's primary focus. \n\nA C.S.O. almost always has a pure science background and an advanced degree, whereas a C.T.O. often has a background in engineering or business development.\n\nSome academic research organizations, such as the Fox Chase Cancer Center, Dana-Farber Cancer Institute, and the San Diego Supercomputer Center have adopted a similar title of C.S.O. Typically, their role is to evaluate and set scientific priorities and coordinate the administrative structure that supports scientists. A C.S.O. commonly has a scientific or academic background, yet they may or may not be practicing scientists or academics.\n\nIn the National Health Service, the C.S.O. is the head of profession for the 53,000 healthcare scientists working in the organization and its associated bodies. The C.S.O. is one of six NHS professional officers (including the chief medical officer and the chief nursing officer) who are employed within NHS England. These roles lead their own professional groups as well as providing expert knowledge about their specific disciplines to the NHS and wider health and care system.\n\nThe C.S.O. provides professional leadership and expert clinical advice across the health system, as well as working alongside senior clinical leaders within NHS England and the broader commissioning system. The C.S.O. is also responsibility for delivering the government's strategy for a modernised healthcare science workforce, Modernising Scientific Careers.\n\nProfessor Dame Sue Hill has been the C.S.O. since October 2002 first within the department of health and subsequently NHS England . The role was strengthened in March 2013, with the appointment of a deputy C.S.O.\n\n"}
{"id": "1293340", "url": "https://en.wikipedia.org/wiki?curid=1293340", "title": "Classical field theory", "text": "Classical field theory\n\nA classical field theory is a physical theory that predicts how one or more physical fields interact with matter through field equations. The term 'classical field theory' is commonly reserved for describing those physical theories that describe electromagnetism and gravitation, two of the fundamental forces of nature. Theories that incorporate quantum mechanics are called quantum field theories.\n\nA physical field can be thought of as the assignment of a physical quantity at each point of space and time. For example, in a weather forecast, the wind velocity during a day over a country is described by assigning a vector to each point in space. Each vector represents the direction of the movement of air at that point, so the set of all wind vectors in an area at a given point in time constitutes a vector field. As the day progresses, the directions in which the vectors point change as the directions of the wind change. \n\nThe first field theories, Newtonian gravitation and Maxwell's equations of electromagnetic fields were developed in classical physics before the advent of relativity theory in 1905, and had to be revised to be consistent with that theory. Consequently, classical field theories are usually categorized as \"non-relativistic\" and \"relativistic\". Modern field theories are usually expressed using the mathematics of tensor calculus. A more recent alternate mathematical formalism describes classical fields as sections of mathematical objects called fiber bundles .\n\nIn 1839 James MacCullagh presented field equations to describe reflection and refraction in \"An essay toward a dynamical theory of crystalline reflection and refraction\".\n\nSome of the simplest physical fields are vector force fields. Historically, the first time that fields were taken seriously was with Faraday's lines of force when describing the electric field. The gravitational field was then similarly described.\n\nThe first field theory of gravity was Newton's theory of gravitation in which the mutual interaction between two masses obeys an inverse square law. This was very useful for predicting the motion of planets around the Sun.\n\nAny massive body \"M\" has a gravitational field g which describes its influence on other massive bodies. The gravitational field of \"M\" at a point r in space is found by determining the force F that \"M\" exerts on a small test mass \"m\" located at r, and then dividing by \"m\":\nStipulating that \"m\" is much smaller than \"M\" ensures that the presence of \"m\" has a negligible influence on the behavior of \"M\".\n\nAccording to Newton's law of universal gravitation, F(r) is given by\nwhere formula_3 is a unit vector pointing along the line from \"M\" to \"m\", and \"G\" is Newton's gravitational constant. Therefore, the gravitational field of M is\n\nThe experimental observation that inertial mass and gravitational mass are equal to unprecedented levels of accuracy leads to the identification of the gravitational field strength as identical to the acceleration experienced by a particle. This is the starting point of the equivalence principle, which leads to general relativity.\n\nFor a discrete collection of masses, \"M\", located at points, r, the gravitational field at a point r due to the masses is\n\nIf we have a continuous mass distribution \"ρ\" instead, the sum is replaced by an integral,\n\nNote that the direction of the field points from the position r to the position of the masses r; this is ensured by the minus sign. In a nutshell, this means all masses attract.\n\nIn the integral form Gauss's law for gravity is\n\nwhile in differential form it is\n\nTherefore, the gravitational field g can be written in terms of the gradient of a gravitational potential φ(r):\nThis is a consequence of the gravitational force F being conservative.\n\nA charged test particle with charge \"q\" experiences a force F based solely on its charge. We can similarly describe the electric field E so that . Using this and Coulomb's law the electric field due to a single charged particle is\n\nThe electric field is conservative, and hence is given by the gradient of a scalar potential, \"V\"(r)\n\nGauss's law for electricity is in integral form\n\nwhile in differential form\n\nA steady current \"I\" flowing along a path \"ℓ\" will exert a force on nearby charged particles that is quantitatively different from the electric field force described above. The force exerted by \"I\" on a nearby charge \"q\" with velocity v is\nwhere B(r) is the magnetic field, which is determined from \"I\" by the Biot–Savart law:\n\nThe magnetic field is not conservative in general, and hence cannot usually be written in terms of a scalar potential. However, it can be written in terms of a vector potential, A(r):\n\nGauss's law for magnetism in integral form is\n\nwhile in differential form it is\n\nThe physical interpretation is that there are no magnetic monopoles.\n\nIn general, in the presence of both a charge density ρ(r, \"t\") and current density J(r, \"t\"), there will be both an electric and a magnetic field, and both will vary in time. They are determined by Maxwell's equations, a set of differential equations which directly relate E and B to the electric charge density (charge per unit volume) \"ρ\" and current density (electric current per unit area) J.\n\nAlternatively, one can describe the system in terms of its scalar and vector potentials \"V\" and A. A set of integral equations known as \"retarded potentials\" allow one to calculate \"V\" and A from ρ and J, and from there the electric and magnetic fields are determined via the relations\n\nFluid dynamics has fields of pressure, density, and flow rate that are connected by conservation laws for energy and momentum. The mass continuity equation is a continuity equation, representing the conservation of mass\n\nand the Navier–Stokes equations represent the conservation of momentum in the fluid, found from Newton's laws applied to the fluid,\n\nif the density \"ρ\", pressure \"p\", deviatoric stress tensor τ of the fluid, as well as external body forces b, are all given. The velocity field u is the vector field to solve for.\n\nThe term \"potential theory\" arises from the fact that, in 19th century physics, the fundamental forces of nature were believed to be derived from scalar potentials which satisfied Laplace's equation. Poisson addressed the question of the stability of the planetary orbits, which had already been settled by Lagrange to the first degree of approximation from the perturbation forces, and derived the Poisson's equation, named after him. The general form of this equation is\n\nwhere \"σ\" is a source function (as a density, a quantity per unit volume) and φ the scalar potential to solve for.\n\nIn Newtonian gravitation; masses are the sources of the field so that field lines terminate at objects that have mass. Similarly, charges are the sources and sinks of electrostatic fields: positive charges emanate electric field lines, and field lines terminate at negative charges. These field concepts are also illustrated in the general divergence theorem, specifically Gauss's law's for gravity and electricity. For the cases of time-independent gravity and electromagnetism, the fields are gradients of corresponding potentials\n\nso substituting these into Gauss' law for each case obtains\n\nwhere \"ρ\" is the mass density and \"ρ\" the charge density.\n\nIncidentally, this similarity arises from the similarity between Newton's law of gravitation and Coulomb's law.\n\nIn the case where there is no source term (e.g. vacuum, or paired charges), these potentials obey Laplace's equation:\n\nFor a distribution of mass (or charge), the potential can be expanded in a series of spherical harmonics, and the \"n\"th term in the series can be viewed as a potential arising from the 2-moments (see multipole expansion). For many purposes only the monopole, dipole, and quadrupole terms are needed in calculations.\n\nModern formulations of classical field theories generally require Lorentz covariance as this is now recognised as a fundamental aspect of nature. A field theory tends to be expressed mathematically by using Lagrangians. This is a function that, when subjected to an action principle, gives rise to the field equations and a conservation law for the theory. The action is a Lorentz scalar, from which the field equations and symmetries can be readily derived.\n\nThroughout we use units such that the speed of light in vacuum is 1, i.e. \"c\" = 1.\n\nGiven a field tensor \"φ\", a scalar called the Lagrangian density\n\ncan be constructed from \"φ\" and its derivatives.\n\nFrom this density, the action functional can be constructed by integrating over spacetime,\n\nWhere formula_29 is viewed as the 'jacobian' in curved spacetime. formula_30\n\nTherefore, the Lagrangian itself is equal to the integral of the Lagrangian density over all space.\n\nThen by enforcing the action principle, the Euler–Lagrange equations are obtained\n\nTwo of the most well-known Lorentz-covariant classical field theories are now described.\n\nHistorically, the first (classical) field theories were those describing the electric and magnetic fields (separately). After numerous experiments, it was found that these two fields were related, or, in fact, two aspects of the same field: the electromagnetic field. Maxwell's theory of electromagnetism describes the interaction of charged matter with the electromagnetic field. The first formulation of this field theory used vector fields to describe the electric and magnetic fields. With the advent of special relativity, a more complete formulation using tensor fields was found. Instead of using two vector fields describing the electric and magnetic fields, a tensor field representing these two fields together is used.\n\nThe electromagnetic four-potential is defined to be \"A\" = (-\"φ\", A), and the electromagnetic four-current \"j\" = (-\"ρ\", j). The electromagnetic field at any point in spacetime is described by the antisymmetric (0,2)-rank electromagnetic field tensor\n\nTo obtain the dynamics for this field, we try and construct a scalar from the field. In the vacuum, we have\n\nWe can use gauge field theory to get the interaction term, and this gives us\n\nTo obtain the field equations the electromagnetic tensor in the Lagrangian density needs to be replaced by its definition in terms of the 4-potential \"A\", and its this potential which enter the Euler-Lagrange equations. The EM field \"F\" is not varied in the EL equations. Therefore,\n\nEvaluating the derivative of the Lagrangian density with respect to the field components\n\nand the derivatives of the field components\n\nobtains Maxwell's equations in vacuum. The source equations (Gauss' law for electricity and the Maxwell-Ampère law) are\n\nwhile the other two (Gauss' law for magnetism and Faraday's law) are obtained from the fact that \"F\" is the 4-curl of \"A\", or, in other words, from the fact that the bianchi identity holds for the electromagnetic field tensor.\n\nwhere the comma indicates a partial derivative.\n\nAfter Newtonian gravitation was found to be inconsistent with special relativity, Albert Einstein formulated a new theory of gravitation called general relativity. This treats gravitation as a geometric phenomenon ('curved spacetime') caused by masses and represents the gravitational field mathematically by a tensor field called the metric tensor. The Einstein field equations describe how this curvature is produced. Newtonian gravitation is now superseded by Einstein's theory of general relativity, in which gravitation is thought of as being due to a curved spacetime, caused by masses. The Einstein field equation describes how this curvature is produced by masses,\n\nwhere \"κ\" = \"8πG/c\" is a constant which appears in the Einstein field equations (and not the action), and\n\nis the Einstein tensor. An alternate interpretation, due to Arthur Eddington, is that formula_42 is fundamental, formula_43 is merely one aspect of formula_42, and formula_45 is forced by the choice of units.\n\nThe vacuum solution can be obtained by varying the following Einstein–Hilbert action with respect to the metric\n\nThe \"vacuum field equations\" are the field equations written without matter (including sources). Solutions of the vacuum field equations are called vacuum solutions. The field equations may be derived by using the Einstein–Hilbert action. Varying the Lagrangian\n\nwhere \"R\" = \"Rg\" is the Ricci scalar written in terms of the Ricci tensor \"R\" and \"g\" the determinant of the metric tensor \"g\" will yield the vacuum field equations\n\nAttempts to create a unified field theory based on classical physics are classical unified field theories. During the years between the two World Wars, the idea of unification of gravity with electromagnetism was actively pursued by several mathematicians and physicists like Albert Einstein, Theodor Kaluza, Hermann Weyl, Arthur Eddington , Gustav Mie and Ernst Reichenbacher.\n\nEarly attempts to create such theory were based on incorporation of electromagnetic fields into geometry of general relativity. In 1918, the case for the first\ngeometrization of the electromagnetic field was proposed in 1918 by Hermann Weyl. \nIn 1919, an idea of five-dimensional approach suggested by Theodor Kaluza. From that, a theory called Kaluza-Klein Theory was developed. It attempts to unify gravitation and electromagnetism, in a five-dimensional space-time.\nThere are several ways of extending the representational framework for a unified field theory which have been considered by Einstein and other researchers. These extensions in general are based in two options. The first option is based in relaxing the conditions imposed on the original formulation, and the second is based in introduction other mathematical objects into the theory. An example of the first option is relaxing the restrictions to four-dimensional space-time by considering higher-dimensional representations. That is used in Kaluza-Klein Theory. For the second, the most prominent example arises from the concept of the affine connection that was introduced into the theory of general relativity mainly through the work of Tullio Levi-Civita and Hermann Weyl. \n\nFurther development of quantum field theory changed the focus of searching for unified field theory from classical to quantum description. Because of that, many theoretical physicists gave up looking for a classical unified field theory. Quantum field theory would include unification of other two fundamental forces of nature, strong and weak nuclear force which act on subatomic level.\n\n\n"}
{"id": "33936176", "url": "https://en.wikipedia.org/wiki?curid=33936176", "title": "Comprehensive Assessment of Water Management in Agriculture", "text": "Comprehensive Assessment of Water Management in Agriculture\n\nThe report A Comprehensive Assessment of Water Management in Agriculture was published in 2007 by International Water Management Institute and Earthscan in an attempt to answer the question: how can water in agriculture be developed and managed to help end poverty and hunger, ensure environmentally sustainable practices, and find the right balance between food and environmental security?\n\nCompiled after consultation with more than 700 individuals, numerous organisations and networks, it was the first critical evaluation of: \nThe assessment confirmed that agriculture consumes more water resources than any other sector. A key finding was that a third of the world's population live in water-scarce areas. More than 1.2 billion live in areas of physical water scarcity, lacking water resources. Parts of Australia and the United States suffer in this way. A further 1.6 billion people live in areas of economic water scarcity, where there is insufficient human capacity or financial resources for people to effectively make use of water that is available. Here, sub-Saharan Africa is a good example; there is water in the rivers but no dams or pumps to enable people to use it.\n\nThe report's authors forecast that the need for water would double within 50 years, due to global population rise, more people choosing to eat a diet of meat and vegetables rather than primarily consuming cereals, and climate change. Generally, about one litre of liquid water gets converted to water vapour to produce one calorie of food. We each consume between 2,000 and 5,000 liters of water every day, depending on our diet and how the food is produced. This is far more than the two to five litres we drink every day. A heavy meat diet requires much more than a vegetarian diet, because water is used to grow food for the animals as well as being used directly to support the livestock. Economic growth fuels changes in diets; for example, per capita meat demand in China has quadrupled over the last 30 years, and milk and egg products are becoming increasingly popular in India. Growing cities, expanding industry and biofuels are increasingly competing for water with an expanding agriculture.\n\nThe conclusion made by the report's authors was that only by changing the way we use water within agriculture would we be able to meet the acute water, environment and poverty challenges facing us over the next 50 years. They suggested that with wise policies and investments in irrigation, upgrading rainfed agriculture, and trade it would be possible to limit future growth in water withdrawals to 13% and the expansion of cultivated land to 9%. However, the effects of climate change and the increased use of biofuels would complicate matters, making actions necessary to \naddress these. The Assessment found the greatest potential lay in rainfed areas of the world housing the highest number of poor people. Upgrading these rainfed lands through better water management held the greatest potential to increase productivity and decrease poverty. The technology would not necessarily need to be complex; simple measures such as catching water in huge tubular plastic bags and storing roof and road run-off could double or even triple food production in sub-Saharan Africa and south-east Asia, effectively increasing productivity from each raindrop by that amount.\n\nThe report recommended eight policy actions:\n\n"}
{"id": "49411310", "url": "https://en.wikipedia.org/wiki?curid=49411310", "title": "Dark Energy Spectroscopic Instrument", "text": "Dark Energy Spectroscopic Instrument\n\nThe Mid-Scale Dark Energy Spectroscopic Instrument (DESI) is a new instrument for conducting a spectrographic survey of distant galaxies. Its main components are a focal plane containing 5000 fiber-positioning robots, and a bank of spectrographs which are fed by the fibers. The new instrument will enable an experiment to probe the expansion history of the Universe and the mysterious physics of dark energy.\n\nThe instrument is funded by the U.S. Department of Energy Office of Science and currently under construction. It will be located at 6880 ft. on the Mayall Telescope on top of Kitt Peak in the Sonoran Desert 55 miles distant from Tucson, Arizona.\n\nThe expansion history and large-scale structure of the Universe is a key prediction of cosmological models, and DESI observations will permit scientists to probe diverse aspects of cosmology, from dark energy to alternatives to General Relativity to neutrino masses to the early Universe. The data from DESI will be used to create three-dimensional maps of the distribution of matter covering an unprecedented volume of the Universe with unparalleled detail. This will provide insight into the nature of dark energy and establish whether cosmic acceleration is due to a cosmic-scale modification of General Relativity. DESI will be transformative in the understanding of dark energy and the expansion rate of the Universe at early times, one of the greatest mysteries in the understanding of the physical laws.\n\nDESI will measure the expansion history of the Universe using the baryon acoustic oscillations (BAO) imprinted in the clustering of galaxies, quasars, and the intergalactic medium.<ref name=\"seo/eisenstein\"></ref> The BAO technique is a robust way to extract cosmological distance information from the clustering of matter and galaxies. It relies only on very large-scale structure and it does so in a manner that enables scientists to separate the acoustic peak of the BAO signature from uncertainties in most systematic errors in the data. BAO was identified in the 2006 Dark Energy Task Force report as one of the key methods for studying dark energy.<ref name=\"albrecht/etal\"></ref> In May 2014, the High-Energy Physics Advisory Panel, a federal advisory committee, commissioned by the US Department of Energy (DOE) and the National Science Foundation (NSF) endorsed DESI.\n\nThe baryon acoustic oscillations method requires a three-dimensional map of distant galaxies and quasars created from the angular and redshift information of a large statistical sample of cosmologically distant objects. By obtaining spectra of distant galaxies it is possible to determine their distance, via the measurement of their spectroscopic redshift, and thus create a 3-D map of the Universe.<ref name=\"SDSS/Eisenstein\"></ref> The 3-D map of the large-scale structure of the Universe also contains more information about dark energy than just the BAO and is sensitive to the mass of the neutrino and parameters that governed the primordial Universe. During its five-year survey beginning in September 2019, the DESI experiment will observe 35 million galaxies and quasars.\n\nThe DESI instrument will implement a new highly multiplexed optical spectrograph on the Mayall Telescope.<ref name=\"levi/etal\"></ref> A new optical corrector design creates a very large, 8.0 square degree field of view on the sky, which combined with the new focal plane instrumentation will weigh approximately 20,000 lbs. The focal plane accommodates 5,000 small computer controlled fiber positioners on a 10.4 millimeter pitch. The entire focal plane can be reconfigured for the next exposure in less than two minutes while the telescope slews to the next field.\n\nThe instrument fabrication is managed by the Lawrence Berkeley National Laboratory. Construction on the new instrument started in 2015. Funding is provided by the Department of Energy Office of Science, the National Science Foundation, the Science and Technology Facilities Council of the U.K., by the Gordon and Betty Moore Foundation, by the Heising-Simons Foundation, and by collaborating institutions worldwide.\n\n"}
{"id": "27195707", "url": "https://en.wikipedia.org/wiki?curid=27195707", "title": "Derain (crater)", "text": "Derain (crater)\n\nDerain is a crater on Mercury named after \nAndré Derain, a French artist, painter, sculptor and co-founder of Fauvism with Henri Matisse. It has uncommonly dark material within and surrounding the crater. The material is darker than the neighboring terrain such that this crater is easily identified even in a distant global image of Mercury. The dark halo may be material with a mineralogical composition different from the majority of Mercury’s visible surface. Craters with similar dark material on or near their rims were seen on the floor of the Caloris basin during \"MESSENGER\"’s first flyby.\n"}
{"id": "48608070", "url": "https://en.wikipedia.org/wiki?curid=48608070", "title": "Dorothée Le Maître", "text": "Dorothée Le Maître\n\nDorothée Le Maître (; 1 September 1896 – 26 January 1990) was a French paleontologist known for her studies of Devonian flora and fauna in North Africa and Sub-Saharan Africa.\n\nLe Maître was educated at Angers Free University and received her bachelor's degree from the Catholic University of Lille in 1926. She remained there for her doctoral studies and earned her Ph.D. in 1934.\n\nHer first paper was published in 1926, the same year she earned her bachelor's degree. After her Ph.D., Le Maître became a faculty member at the University of Lille, where she was a geology researcher. Her research included work on the \"Spongiomorphides\" and included comparative research of North African and Sub-Saharan fossils to those of Europe.\n\nIn 1941, Le Maître was awarded the Prix Fontannes, and in 1956 she received the Kuhlmann Prize. In 1959, she was honored by the French Academy of Sciences with the Grand Prix Bonnet. She was the president of the Société Géologique du Nord in 1949.\n"}
{"id": "12433418", "url": "https://en.wikipedia.org/wiki?curid=12433418", "title": "Filtering problem (stochastic processes)", "text": "Filtering problem (stochastic processes)\n\nIn the theory of stochastic processes, the filtering problem is a mathematical model for a number of state estimation problems in signal processing and related fields. The general idea is to establish a \"best estimate\" for the true value of some system from an incomplete, potentially noisy set of observations on that system. The problem of optimal non-linear filtering (even for the non-stationary case) was solved by Ruslan L. Stratonovich (1959, 1960), see also Harold J. Kushner's work and Moshe Zakai's, who introduced a simplified dynamics for the unnormalized conditional law of the filter known as Zakai equation. The solution, however, is infinite-dimensional in the general case. Certain approximations and special cases are well understood: for example, the linear filters are optimal for Gaussian random variables, and are known as the Wiener filter and the Kalman-Bucy filter. More generally, as the solution is infinite dimensional, it requires finite dimensional approximations to be implemented in a computer with finite memory. A finite dimensional approximated nonlinear filter may be more based on heuristics, such as the Extended Kalman Filter or the Assumed Density Filters, or more methodologically oriented such as for example the Projection Filters, some sub-families of which are shown to coincide with the Assumed Density Filters.\n\nIn general, if the separation principle applies, then filtering also arises as part of the solution of an optimal control problem. For example, the Kalman filter is the estimation part of the optimal control solution to the linear-quadratic-Gaussian control problem.\n\nConsider a probability space (Ω, Σ, P) and suppose that the (random) state \"Y\" in \"n\"-dimensional Euclidean space R of a system of interest at time \"t\" is a random variable \"Y\" : Ω → R given by the solution to an Itō stochastic differential equation of the form\n\nwhere \"B\" denotes standard \"p\"-dimensional Brownian motion, \"b\" : [0, +∞) × R → R is the drift field, and \"σ\" : [0, +∞) × R → R is the diffusion field. It is assumed that observations \"H\" in R (note that \"m\" and \"n\" may, in general, be unequal) are taken for each time \"t\" according to\n\nAdopting the Itō interpretation of the stochastic differential and setting\n\nthis gives the following stochastic integral representation for the observations \"Z\":\n\nwhere \"W\" denotes standard \"r\"-dimensional Brownian motion, independent of \"B\" and the initial condition \"Y\", and \"c\" : [0, +∞) × R → R and \"γ\" : [0, +∞) × R → R satisfy\n\nfor all \"t\" and \"x\" and some constant \"C\".\n\nThe filtering problem is the following: given observations \"Z\" for 0 ≤ \"s\" ≤ \"t\", what is the best estimate \"Ŷ\" of the true state \"Y\" of the system based on those observations?\n\nBy \"based on those observations\" it is meant that \"Ŷ\" is measurable with respect to the \"σ\"-algebra \"G\" generated by the observations \"Z\", 0 ≤ \"s\" ≤ \"t\". Denote by \"K\" = \"K\"(\"Z\", \"t\") be collection of all R-valued random variables \"Y\" that are square-integrable and \"G\"-measurable:\n\nBy \"best estimate\", it is meant that \"Ŷ\" minimizes the mean-square distance between \"Y\" and all candidates in \"K\":\n\nThe space \"K\"(\"Z\", \"t\") of candidates is a Hilbert space, and the general theory of Hilbert spaces implies that the solution \"Ŷ\" of the minimization problem (M) is given by\n\nwhere \"P\" denotes the orthogonal projection of \"L\"(Ω, Σ, P; R) onto the linear subspace \"K\"(\"Z\", \"t\") = \"L\"(Ω, \"G\", P; R). Furthermore, it is a general fact about conditional expectations that if \"F\" is any sub-\"σ\"-algebra of Σ then the orthogonal projection\n\nis exactly the conditional expectation operator E[·|\"F\"], i.e.,\n\nHence,\n\nThis elementary result is the basis for the general Fujisaki-Kallianpur-Kunita equation of filtering theory.\n\n\n"}
{"id": "26954391", "url": "https://en.wikipedia.org/wiki?curid=26954391", "title": "FoldX", "text": "FoldX\n\nFoldX is a protein design algorithm that uses an empirical force field. It can determine the energetic effect of point mutations as well as the interaction energy of protein complexes (including Protein-DNA). FoldX can mutate protein and DNA side chains using a probability-based rotamer library, while exploring alternative conformations of the surrounding side chains.\n\n\nThe energy function includes terms that have been found to be important for protein stability, where the energy of unfolding (∆G) of a target protein is calculated using the equation:\n\n∆G = ∆G + ∆G + ∆G + ∆G + ∆G + ∆G + ∆S + ∆S\nWhere ∆G is the sum of the Van der Waals contributions of all atoms with respect to the same interactions with the solvent. ∆G and ∆G is the difference in solvation energy for apolar and polar groups, respectively, when going from the unfolded to the folded state. ∆Ghbond is the free energy difference between the formation of an intra-molecular hydrogen-bond compared to inter-molecular hydrogen-bond formation (with solvent). ∆G is the extra stabilizing free energy provided by a water molecule making more than one hydrogen-bond to the protein (water bridges) that cannot be taken into account with non-explicit solvent approximations. ∆G is the electrostatic contribution of charged groups, including the helix dipole. ∆S is the entropy cost for fixing the backbone in the folded state. This term is dependent on the intrinsic tendency of a particular amino acid to adopt certain dihedral angles. ∆S is the entropic cost of fixing a side chain in a particular conformation. The energy values of ∆G, ∆G, ∆G and ∆G attributed to each atom type have been derived from a set of experimental data, and ∆S and ∆S have been taken from theoretical estimates. The Van der Waals contributions are derived from vapor to water energy transfer, while in the protein we are going from solvent to protein.\n\nFor protein-protein interactions, or protein-DNA interactions FoldX calculates ∆∆G of interaction :\n\n∆∆G = ∆G- (∆G + ∆G) + ∆G + ∆S\n\n∆G reflects the effect of electrostatic interactions on the k. ∆S is the loss of translational and rotational entropy upon making the complex.\n\n\nNative FoldX is run from the command line. A FoldX plugin for the YASARA molecular graphics program has been developed to access various FoldX tools inside a graphical environment. The results of e.g. in silico mutations or homology modeling with FoldX can be directly analyzed on screen.\n\n"}
{"id": "6574091", "url": "https://en.wikipedia.org/wiki?curid=6574091", "title": "Gain scheduling", "text": "Gain scheduling\n\nIn control theory, gain scheduling is an approach to control of non-linear systems that uses a family of linear controllers, each of which provides satisfactory control for a different operating point of the system.\n\nOne or more observable variables, called the \"scheduling variables\", are used to determine what operating region the system is currently in and to enable the appropriate linear controller. For example, in an aircraft flight control system, the altitude and Mach number might be the scheduling variables, with different linear controller parameters available (and automatically plugged into the controller) for various combinations of these two variables.\n\nA relatively large scope state of the art about gain scheduling has been published in (Survey of Gain-Scheduling Analysis & Design, D.J.Leith, WE.Leithead).\n\n\n"}
{"id": "22034657", "url": "https://en.wikipedia.org/wiki?curid=22034657", "title": "Gary S. Grest", "text": "Gary S. Grest\n\nGary S. Grest is a computational physicist at Sandia National Laboratories. He received the Aneesur Rahman Prize for Computational Physics from the American Physical Society for his work in computational physics. He was elected to the National Academy of Engineering in 2008.\n"}
{"id": "14941363", "url": "https://en.wikipedia.org/wiki?curid=14941363", "title": "Genomic Standards Consortium", "text": "Genomic Standards Consortium\n\nThe Genomic Standards Consortium (GSC) is an initiative working towards richer descriptions of our collection of genomes, metagenomes and marker genes. Established in September 2005, this international community includes representatives from a range of major sequencing and bioinformatics centres (including NCBI, EMBL, DDBJ, JCVI, JGI, EBI, Sanger, FIG) and research institutions. The goal of the GSC is to promote mechanisms for standardizing the description of (meta)genomes, including the exchange and integration of (meta)genomic data. The number and pace of genomic and metagenomic sequencing projects will only increase as the use of ultra-high-throughput methods becomes common place and standards are vital to scientific progress and data sharing.\n\nCommunity-driven standards have the best chance of success if developed within the auspices of international working groups. Participants in the GSC include biologists, computer scientists, those building genomic databases and conducting large-scale comparative genomic analyses, and those with experience of building community-based standards. The mission of the GSC is to work with the wider community towards: \n\n\nThe GSC has published a “Minimum Information about a (Meta)Genome Sequence” specification and has now completed a \"Minimum Information about an ENvironmental Sequence\" specification. MIGS/MIMS/MIMARKS provides an extension of the minimum information already captured by the primary nucleotide sequence archives (INSDC or DDBJ/ENA/GenBank). The development of any checklist must be an open and iterative process that involves a balanced group of participants. Further, this development process must be supported by providing mechanisms for achieving compliance if a checklist is to be adopted as a tool for the standardization of a particular area of knowledge. Work towards this goal has spawned a set of interlocking projects that are described in more detail here: GSC projects. These include The Genomic Contextual Data Markup Language (GCDML), Genomic Rosetta Stone (GRS), Habitat-Lite. Newer projects include the M5 project.\n\nThe GSC is interested in making and building links with other communities. As stated above, the GSC is engaged in ontology development within the OBO Foundry. The GSC is also a founding member community of the Minimum Information about a Biomedical or Biological Investigation (MIBBI), an umbrella community for supporting and co-ordinating the development of checklists describing Minimum Information Standards.\n\nThe GSC maintains a list of publications on its wiki - GSC Publications. This list includes reports from all workshops, articles from the special issue of the journal OMICS on data standards, and the publications describing the MIGS/MIMS and MIMARKS specifications in the journal \"Nature Biotechnology\" (May 2008 and May 2011 respectively).\n\n"}
{"id": "789480", "url": "https://en.wikipedia.org/wiki?curid=789480", "title": "Harry Edwin Wood", "text": "Harry Edwin Wood\n\nHarry Edwin Wood (3 February 1881 – 27 February 1946) was an English astronomer, director of the Union Observatory in Johannesburg, and discoverer of minor planets.\n\nWood was born in Manchester, graduating from Manchester University in 1902 with first class honours in physics, going on to gain an M.Sc in 1905. In 1906 he was appointed the Chief Assistant at the \"Transvaal Meteorological Observatory\", which soon acquired telescopes and which became known as the Union Observatory and later Republic Observatory. In 1909, he married Mary Ethel Greengrass, also a physics graduate of Manchester University. Wood served as the observatory's director from 1928 to 1941, succeeding Robert Innes. He also served as the president of the \"Astronomical Society of South Africa\" from 1929 to 1930.\n\nWood is credited by the Minor Planet Center with the discovery of 12 numbered asteroids during 1911–1932.\n\nHe died in Mortimer, near Cradock, Eastern Cape, South Africa, in 1946. The asteroid 1660 Wood, discovered by his colleague Jacobus Bruwer at Johannesburg, is named in his honor ().\n\n\n"}
{"id": "43131303", "url": "https://en.wikipedia.org/wiki?curid=43131303", "title": "Historical dynamics", "text": "Historical dynamics\n\nHistorical dynamics broadly includes the scientific modeling of history. This might also be termed computer modeling of history, historical simulation, or simulation of history - allowing for an extensive range of techniques in simulation and estimation. Historical dynamics does not exist as a separate science, but there are individual efforts such as long range planning, population modeling, economic forecasting, demographics, global modeling, country modeling, regional planning, urban planning and many others in the general categories of computer modeling, planning, forecasting, and simulations.\n\nSome examples of \"large\" history where historical dynamics simulations would be helpful include; global history, large structures, , long duration history, philosophy of history, Eurasian history, comparative history, long-range environmental history, world systems theory, non-Western political and economic development, and historical demography.\nWith the rise of technologies like wikis, and internet-wide search engines, some historical and social data can be mined to constrain models of history and society. Data from social media sites, and busy sites, can be mined for human patterns of action. These can provide more and more realistic behavioral models for individuals and groups of any size. Agent-based models and microsimulations of human behavior can be embedded in larger historical simulations. Related subfields are behavioral economics and human behavioral ecology.\n\nIn every sector of human activity, there are extensive databases for transportation data, urban development, health statistics, education data, social data, economic data—along with many projections. See , , , and .\n\nSome examples of database activity include Asian Development Bank statistics, World Bank data, and the International Monetary Fund data.\n\nTime series analysis and econometrics are well established fields for the analysis of trends and forecasting; but, survey data and microdatasets can also be used in forecasts and simulations.\n\nThe United Nations and other organizations routinely project the population of individual countries and regions of the world decades into the future. These demographic models are used by other organizations for projecting demand for services in all sectors of each economy.\n\n\nEach country often has their corresponding modeling groups for each of these major sectors. These can be grouped in separate articles according to sector. Groups include government departments, international aid agencies, as well as nonprofit and non-governmental organizations.\n\nA broad class of models used for economic and social modeling of countries and sectors are the Computable general equilibrium (CGE) model - also called applied general equilibrium models. In the context of time based simulations and policy analysis, see dynamic stochastic general equilibrium models.\n\nPartly because of the controversy over global climate change, there is an extensive network of global climate models, and related social and economic models. These seek to estimate, not only the change in climate and its physical effects, but also the impact on human society and the natural environment. See global economic models, social model, microsimulation, climate model, global climate models, and general circulation model.\n\nThe relationship between the environment and society is examined through environmental social science. human ecology, political ecology, and ecology, in general, can be areas where computer and mathematical modeling over time can be relevant to historical simulation.\n\nWeb-based historical simulations, simulations of history, interactive historical simulations, are increasingly popular for entertainment and educational purposes. The field is expanding rapidly and no central index seems to be available. Another example is \n\nSeveral computer games allow players to interact with the game to model societies over time. The Civilization (series) is one example. Others include Age of Empires, Rise of Nations, Spore, Colonization, Alpha Centauri, Call to Power, and . A longer list of games in historical context, which might include degrees of simulation, are found at .\n\nMilitary simulation is a well-developed field and increasingly accessible on the internet.\n\nComputer models for simulating society fall under artificial society, social simulation, computational sociology, computational social science, and mathematical sociology. There is an interdisciplinary Journal of Artificial Societies and Social Simulation for computer simulation of social processes. The European Social Simulation Association promotes social simulation research in Europe; it is the publisher of JASSS. There is a corresponding Computational Social Science Society of the Americas., and a Pan-Asian Association for Agent-based Approach in Social Systems Sciences. PAAA lists some related Japanese associations.\n\nThe SimSoc (Simulated Society tool) is in its fifth edition.\n\nThere has been extensive research in urban planning, environmental planning and related fields: regional planning, land-use planning, transportation planning, urban studies, and regional science. Journals for these fields are listed at List of planning journals.\n\nSimCity is a game for simulations of artificial cities. It has spawned a range of \"sim\" games. The planning groups try to simulate changes in real cities. The game groups allow experiments with artificial cities. And the two are merging in such efforts as Vizicities\n\nThe profiling of industries is well developed, and most industries make forecasts and plans. See industrial history, history of steel, history of mining, history of construction, history of the petroleum industry, and many other histories of specific industries. See cyclical industrial dynamics for modeling of industries in the sense of \"historical dynamics of industries\". Some related terms are industrial planning, history of industry, industrial evolution, technology change, and technology forecasting. An example of \"history friendly\" industrial models. from the journal, Industrial and Corporate Change.\n\nEconomy-wide models must take into account the interactions between industry and the rest of the economy. See Input–output model, economic planning, and social accounting matrix for some relevant techniques.\n\nMany of the techniques from futures studies are applicable to historical dynamics. Whether projecting forward from a point in the past to the present for validation studies, or projecting backwards from the present into the past - many of the techniques are useful. Likewise, simulations of the past, or alternative pasts, provide a groundwork of techniques for futures studies.\n\n"}
{"id": "3298264", "url": "https://en.wikipedia.org/wiki?curid=3298264", "title": "Human behavioral ecology", "text": "Human behavioral ecology\n\nHuman behavioral ecology (HBE) or human evolutionary ecology applies the principles of evolutionary theory and optimization to the study of human behavioral and cultural diversity. HBE examines the adaptive design of traits, behaviors, and life histories of humans in an ecological context. One aim of modern human behavioral ecology is to determine how ecological and social factors influence and shape behavioral flexibility within and between human populations. Among other things, HBE attempts to explain variation in human behavior as adaptive solutions to the competing life-history demands of growth, development, reproduction, parental care, and mate acquisition.\n\nHBE overlaps with evolutionary psychology, human or cultural ecology, and decision theory. It is most prominent in disciplines such as anthropology and psychology where human evolution is considered relevant for a holistic understanding of\nhuman behavior or in economics where self-interest, methodological individualism, and maximization are key elements in modeling behavioral responses to various ecological factors. \n\nHuman behavioral ecology rests upon a foundation of evolutionary theory. This includes aspects of both general evolutionary theory and established middle-level evolutionary theories, as well. Aspects of general evolutionary theory include:\n\nMiddle-level evolutionary theories used in HBE include:\n\nEcological selectionism refers to the assumption that humans are highly flexible in their behaviors. Furthermore, it assumes that various ecological forces select for various behaviors that optimize humans' inclusive fitness in their given ecological context.\n\nThe piecemeal approach refers to taking a reductionist approach as opposed to a holistic approach in studying human socioecological behavior. Human behavioral ecologists assume that by taking complex social phenomena, (e.g., marriage patterns, foraging behaviors, etc.), and then breaking them down into sets of components involving decisions and constraints that they are in a better position to create models and make predictions involving human behavior. An example would be examining marriage systems by examining the ecological context, mate preferences, the distribution of particular characteristics within the population, and so forth.\n\nHuman behavioral ecologists assume that what might be the most adaptive strategy in one environment might not be the most adaptive strategy in another environment. Conditional strategies, therefore, can be represented in the following statement:\n\nThe phenotypic gambit refers to the simplifying assumption that complex traits, such as behavioural traits, can be modelled as if they were controlled by single distinct alleles, representing alternate strategies. In other words, the phenotypic gambit assumes that \"selection will favour traits with high fitness ...irrespective of the particulars of inheritance.\"\n\nTheoretical models that human behavioral ecologists employ include, but are not limited to:\n\n\n\n\n"}
{"id": "50951733", "url": "https://en.wikipedia.org/wiki?curid=50951733", "title": "Human interactions with microbes", "text": "Human interactions with microbes\n\nHuman interactions with microbes include both practical and symbolic uses of microbes, and negative interactions in the form of human, domestic animal, and crop diseases.\n\nPractical use of microbes began in ancient times with fermentation in food processing; bread, beer and wine have been produced by yeasts from the dawn of civilisation, such as in ancient Egypt. More recently, microbes have been used in activities from biological warfare to the production of chemicals by fermentation, as industrial chemists discover how to manufacture a widening variety of organic chemicals including enzymes and bioactive molecules such as hormones and competitive inhibitors for use as medicines. Fermentation is used, too, to produce substitutes for fossil fuels in forms such as ethanol and methane; fuels may also be produced by algae. Anaerobic microorganisms are important in sewage treatment. In scientific research, yeasts and the bacterium \"Escherichia coli\" serve as model organisms especially in genetics and related fields.\n\nOn the symbolic side, an early poem about brewing is the Sumerian \"Hymn to Ninkasi\", from 1800 BC. In the Middle Ages, Giovanni Boccaccio's \"The Decameron\" and Geoffrey Chaucer's \"The Canterbury Tales\": addressed people's fear of deadly contagion and the moral decline that could result. Novelists have exploited the apocalyptic possibilities of pandemics from Mary Shelley's 1826 \"The Last Man\" and Jack London's 1912 \"The Scarlet Plague\" onwards. Hilaire Belloc wrote a humorous poem to \"The Microbe\" in 1912. Dramatic plagues and mass infection have formed the story lines of many Hollywood films, starting with \"Nosferatu\" in 1922. In 1971, \"The Andromeda Strain\" told the tale of an extraterrestrial microbe threatening life on Earth. Microbiologists since Alexander Fleming have used coloured or fluorescing colonies of bacteria to create miniature artworks.\n\nMicroorganisms such as bacteria and viruses are important as pathogens, causing disease to humans, crop plants, and domestic animals. \n\nCulture consists of the social behaviour and norms found in human societies and transmitted through social learning. Cultural universals in all human societies include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers physical expressions such as technology, architecture and art, whereas immaterial culture includes principles of social organization, mythology, philosophy, literature, and science. This article describes the roles played by microorganisms in human culture.\n\nSince microbes were not known until the Early Modern period, they appear in earlier literature indirectly, through descriptions of baking and brewing. Only with the invention of the microscope, as used by Robert Hooke in his 1665 book \"Micrographia\", and by Antonie Van Leeuwenhoek in the 1670s, the germ theory of disease, and progress in microbiology in the 19th century were microbes observed directly, identified as living organisms, and put to use on a scientific basis. The same knowledge also allowed microbes to appear explicitly in literature and the arts.\n\nControlled fermentation with microbes in brewing, wine making, baking, pickling and cultured dairy products such as yogurt and cheese, is used to modify ingredients to make foods with desirable properties. The principal microbes involved are yeasts, in the case of beer, wine, and ordinary bread; and bacteria, in the case of anaerobically fermented vegetables, dairy products, and sourdough bread. The cultures variously provide flavour and aroma, inhibit pathogens, increase digestibility and palatability, make bread rise, reduce cooking time, and create useful products including alcohol, organic acids, vitamins, amino acids, and carbon dioxide. Safety is maintained with the help of food microbiology.\n\nOxidative sewage treatment processes rely on microorganisms to oxidise organic constituents. Anaerobic microorganisms reduce sludge solids producing methane gas and a sterile mineralised residue. In potable water treatment, one method, the slow sand filter, employs a complex gelatinous layer composed of a wide range of microorganisms to remove both dissolved and particulate material from raw water.\n\nMicroorganisms are used in fermentation to produce ethanol, and in biogas reactors to produce methane. Scientists are researching the use of algae to produce liquid fuels, and bacteria to convert various forms of agricultural and urban waste into usable fuels.\n\nMicroorganisms are used for many commercial and industrial purposes, including the production of chemicals, enzymes and other bioactive molecules, often through protein engineering. For example, acetic acid is produced by the bacterium \"Acetobacter aceti\", while citric acid is produced by the fungus \"Aspergillus niger\". Microorganisms are used to prepare a widening range of bioactive molecules and enzymes. For example, Streptokinase produced by the bacterium \"Streptococcus\" and modified by genetic engineering is used to remove clots from the blood vessels of patients who have suffered a heart attack. Cyclosporin A is an immunosuppressive agent in organ transplantation, while statins produced by the yeast \"Monascus purpureus\" serve as blood cholesterol lowering agents, competitively inhibiting the enzyme that synthesizes cholesterol.\n\nMicroorganisms are essential tools in biotechnology, biochemistry, genetics, and molecular biology. The yeasts brewer's yeast (\"Saccharomyces cerevisiae\") and fission yeast (\"Schizosaccharomyces pombe\") are important model organisms in science, since they are simple eukaryotes that can be grown rapidly in large numbers and are easily manipulated. They are particularly valuable in genetics, genomics and proteomics, for example in protein production. The easily cultured gut bacterium \"Escherichia coli\", a prokaryote, is similarly widely used as a model organism.\n\nMicrobes can form an endosymbiotic relationship with larger organisms. For example, the bacteria that live within the human digestive system contribute to human health through gut immunity, the synthesis of vitamins such as folic acid and biotin, and the fermentation of complex indigestible carbohydrates. Future drugs and food chemicals may need to be tested on the gut microbiota; it is already clear that probiotic supplements can promote health, and that gut microbes are affected by both diet and medicines.\n\nPathogenic microbes, and toxins that they produce, have been developed as possible agents of warfare. Crude forms of biological warfare have been practiced since antiquity. In the 6th century BC, the Assyrians poisoned enemy wells with a fungus said to render the enemy delirious. In 1346, the bodies of Mongol warriors of the Golden Horde who had died of plague were thrown over the walls of the besieged Crimean city of Kaffa, possibly assisting the spread of the Black Death into Europe.\nAdvances in bacteriology in the 20th century increased the sophistication of possible bio-agents in war. Biological sabotage—in the form of anthrax and glanders—was undertaken on behalf of the Imperial German government during World War I, with indifferent results. In World War II, Britain weaponised tularemia, anthrax, brucellosis, and botulism toxins, but never used them.\nThe USA similarly explored biological warfare agents, developing anthrax spores, brucellosis, and botulism toxins for possible military use. Japan developed biological warfare agents, with the use of experiments on human prisoners, and was about to use them when the war ended.\n\nBeing very small, and unknown until the invention of the microscope, microbes do not feature directly in art or literature before Early Modern times (though they appear indirectly in works about brewing and baking), when Antonie van Leeuwenhoek observed microbes in water in 1676; his results were soon confirmed by Robert Hooke. A few major diseases such as tuberculosis appear in literature, art, film, opera and music.\n\nThe literary possibilities of post-apocalyptic stories about pandemics (worldwide outbreaks of disease) have been explored in novels and films from Mary Shelley's 1826 \"The Last Man\" and Jack London's 1912 \"The Scarlet Plague\" onwards. Medieval writings that deal with plague include Giovanni Boccaccio's \"The Decameron\" and Geoffrey Chaucer's \"The Canterbury Tales\": both treat the people's fear of contagion and the resulting moral decline, as well as bodily death.\n\nThe making of beer has been celebrated in verse since the time of Ancient Sumeria, c. 1800 BC, when the \"Hymn to Ninkasi\" was inscribed on a clay tablet. Ninkasi, tutelary goddess of beer, and daughter of the creator Enki and the \"queen of the sacred lake\" Ninki, \"handles the dough and with a big shovel, mixing in a pit, the bappir with [date] honey, ... waters the malt set on the ground, ... soaks the malt in a jar, ... spreads the cooked mash on large reed mats, coolness overcomes, ... holds with both hands the great sweet wort, brewing it with honey\".\n\nWine is a frequent topic in English literature, from the spiced French and Italian \"ypocras\", \"claree\", and \"vernage\" in Chaucer's \"The Merchant's Tale\" onwards. William Shakespeare's Falstaff drank Spanish \"sherris sack\", in contrast to Sir Toby Belch's preference for \"canary\". Wine references in later centuries branch out to more winegrowing regions.\n\n\"The Microbe\" is a humorous 1912 poem by Hilaire Belloc, starting with the lines \"The microbe is so very small / You cannot make him out at all,/ But many sanguine people hope / To see him through a microscope. \"Microbes and Man\" is an admired \"classic\" book, first published in 1969, by the \"father figure of British microbiology\" John Postgate on the whole subject of microorganisms and their relationships with humans.\n\nMicrobes feature in many highly dramatized films. Hollywood was quick to exploit the possibilities of deadly disease, mass infection and drastic government reaction, starting as early as 1922 with \"Nosferatu\", in which a Dracula-like figure, Count Orlok, sleeps in unhallowed ground contaminated with the Black Death, which he brings with him wherever he goes. Another classic film, Ingmar Bergman's 1957 \"The Seventh Seal\", deals with the plague theme very differently, with the grim reaper directly represented by an actor in a hood. More recently, the 1971 \"The Andromeda Strain\", based on a novel by Michael Crichton, portrayed an extraterrestrial microbe contaminating the Earth.\n\n\"A Very Cellular Song,\" a song from the British psychedelic folk band The Incredible String Band's 1968 album \"The Hangman's Beautiful Daughter,\" is told partially from the point of view of an amoeba, a protistan.\n\nMicrobial art is the creation of artworks by culturing bacteria, typically on agar plates, to form desired patterns. These may be chosen to fluoresce under ultraviolet light in different colours. Alexander Fleming, the discoverer of penicillin, created \"germ paintings\" using different species of bacteria that were naturally pigmented in different colours.\n\nAn instance of a protist in an artwork is the artist Louise Bourgeois's bronze sculpture \"Amoeba\". It has a white patina resembling plaster, and was designed in 1963–5, based on drawings of a pregnant woman's belly that she made as early as the 1940s. According to the Tate Gallery, the work \"is a roughly modelled organic form, its bulges and single opening suggesting a moving, living creature in the stages of evolution.\"\n\nMicroorganisms are the causative agents (pathogens) in many infectious diseases of humans and domestic animals. Pathogenic bacteria cause diseases such as plague, tuberculosis and anthrax. Protozoa cause diseases including malaria, sleeping sickness, dysentery and toxoplasmosis. Microscopic fungi cause diseases such as ringworm, candidiasis and histoplasmosis. Pathogenic viruses cause diseases such as influenza, yellow fever and AIDS.\n\nThe practice of hygiene was created to prevent infection or food spoiling by eliminating microbes, especially bacteria, from the surroundings.\n\nMicroorganisms including bacteria, fungi, and viruses are important as plant pathogens, causing disease to crop plants. Fungi cause serious crop diseases such as maize leaf rust, wheat stem rust, and powdery mildew. Bacteria cause plant diseases including leaf spot and crown galls. Viruses cause plant diseases such as leaf mosaic. The oomycete \"Phytophthora infestans\" causes potato blight, contributing to the Great Irish Famine of the 1840s.\n\nThe tulip breaking virus played a role in the tulip mania of the Dutch Golden Age. The famous Semper Augustus tulip, in particular, owed its striking pattern to infection with the plant disease, a kind of mosaic virus, making it the most expensive of all the tulip bulbs sold.\n"}
{"id": "41085587", "url": "https://en.wikipedia.org/wiki?curid=41085587", "title": "Independence hypothesis", "text": "Independence hypothesis\n\nThe Independence hypothesis is a proposed solution to the synoptic problem. It holds that Matthew, Mark, and Luke are each original compositions formed independently of each other, with no documentary relationship.\n\nScholars have long noted that the three synoptic gospels have a great deal in common, not just in content but also in order and precise Greek wording. Most scholars have assumed that this must be due to some sort of literary interrelationship among the gospels, with fragments of text copied from one source to another, but have struggled to find a satisfactory theory of who copied from whom. The independence theory rejects this consensus of documentary dependence; rather, each evangelist has independently drawn from eyewitness accounts and perhaps liturgy and other oral tradition.\n\nThe similarities among the synoptic gospels, the whole basis for the synoptic problem, are held to be, first of all, vastly overstated, and secondly, explainable as artifacts of relying on the same witnesses or of different witnesses to the same events.\n\nThe witnesses to the gospel content, especially apostles such as Peter, would have preached their testimony countless times before contributing to the gospels, and such numerous rehearsals tend to make a story settle into a relatively consistent wording. Any of this material that entered public liturgy (e.g., the Lord's prayer) would become even more stabilized. On the other hand, different witnesses nearly always preserve different details and present numerous minor inconsistencies. So, too, does a single witness consulted on different occasions. Moreover, sayings and anonymous healings may have recurred many times in a similar fashion, so that seemingly similar accounts actually preserve distinct events. What we would expect to see in the gospels according to this method of composition, goes the theory, is exactly what we find: many similar accounts, often with virtually identical wording, but many additions and omissions, a somewhat different selection of content in each, and apparent inconsistencies of order and details.\n\nSome see the independence theory as especially consistent with divine inspiration of the gospels, with the similarities among the gospels explained by the Holy Spirit ensuring a faithful record of Christ's words and deeds.\n\nProtestant theologian Eta Linnemann argues that the reason for four independent Gospels stems from the legal principle of : \"On the evidence of two or three witnesses a matter shall be confirmed.\"\n\n\n"}
{"id": "21338685", "url": "https://en.wikipedia.org/wiki?curid=21338685", "title": "Information scientist", "text": "Information scientist\n\nThe term information scientist developed in the latter part of the twentieth century to describe an individual, usually with a relevant subject degree (such as one in CIS) or high level of subject knowledge, providing focused information to scientific and technical research staff in industry, a role quite distinct from and complementary to that of a librarian. Developments in end-user searching, together with some convergence between the roles of librarian and information scientist, have led to a diminution in its use in this context, and the term information officer or information professional are also now used.\n\nThe term was, and is, also used for an individual carrying out research in information science.\n\nBrian C. Vickery mentions that the Institute of Information Scientists (IIS) was established in London during 1958 and lists the criteria put forward by this institute \"Criteria for Information Science\" (appendix 1) as well as his own \"Areas of study in information science\" (appendix 2). The IIS merged with the Library Association in 2002 to form the Chartered Institute of Library and Information Professionals (CILIP).\n\n\n\n\n\"Pioneers\" of Information Science scrapbook: http://faculty.libsci.sc.edu/bob/ISP/scrapbook.htm\n"}
{"id": "47093432", "url": "https://en.wikipedia.org/wiki?curid=47093432", "title": "Kay Dickersin", "text": "Kay Dickersin\n\nKay Dickersin (born November 10, 1951) is an academic who trained first in cell biology and subsequently epidemiology. She went on to a career studying factors that influence research integrity, in particular publication bias and outcome reporting bias. She is Professor in the Department of Epidemiology at Johns Hopkins Bloomberg School of Public Health and Director of the Center for Clinical Trials and Evidence Synthesis there. She was also Director of the US Cochrane Center and the US Satellite of the Cochrane Eyes and Vision Group within the Cochrane Collaboration. Dickersin has received multiple awards for her research.\n\nDickersin’s formal academic and research training spans the basic, clinical, and public health sciences. Dickersin began an undergraduate degree at Bennington College in Bennington, Vermont, planning to major in art. After two years, she transferred undergraduate institutions to University of California, Berkeley. She received both a Bachelor of Arts and Master of Arts in Zoology (cell biology), from the UC Berkeley, in 1974 and 1975, respectively. She was awarded a PhD in epidemiology from the Johns Hopkins School of Hygiene and Public Health in 1989. While an undergraduate, she received a Howard Hughes Fellowship in Medical Research in 1971, and during her PhD research she received an NIH traineeship.\n\nDickersin spent two months in Dorothy and Claude A. Villee’s research laboratory at Harvard University to complete a \"field work term” at Bennington College. Her new interest in science led her to leave Bennington after 2 years, and to work in Allan Tobin’s lab in developmental biology at Harvard College. Subsequently, she transferred undergraduate institutions to University of California, Berkeley. At Berkeley, where she did undergraduate and master's degree research, she worked in Daniel Mazia’s lab. Her master’s thesis was “Increased Permeability of Sea Urchin Eggs to Adenine After Fertilization, Parthenogenetic Activation, and Exposure to Ammonia”. After her master's degree, Dickersin taught biology at two community colleges in California (West Valley College and Fullerton College) and subsequently worked in Roger Sloboda’s lab at Dartmouth College doing developmental biology research.\n\nWhile at Fullerton College, and through her biology students, Dickersin learned about the field of epidemiology, which she liked because it merges science with societal concerns and involves experimental research (clinical trials). In addition, in the late 1970s Dickersin believed the opportunities for women in laboratory research were limited. These two factors led her to switch research areas to public health, a field that she believed is more friendly to women. Dickersin matriculated at the Johns Hopkins School of Hygiene and Public Health in 1979, focusing on clinical trials, and Curtis L. Meinert became her advisor. Her graduate education was interrupted when the family moved to Boston for 4 years in 1981 so that her husband could attend medical school. During this interlude she worked with two influential mentors, Thomas C. Chalmers and Sir Iain Chalmers, and became interested in research synthesis and related biases. When she returned to Baltimore to complete her PhD, she worked on clinical trials as well as research directly linked to publication bias. Her PhD dissertation, “Publication and the Meta-analysis of Clinical Trials,” was the first in her Department to use the model where a group of related research papers is bound together with linking text, rather than the traditional model of stand-alone document with related chapters.\n\nAfter completing her PhD in 1989, Dickersin moved to the University of Maryland, School of Medicine, in Baltimore, first to the Department of Ophthalmology and later to the Department of Epidemiology and Preventive Medicine. In 1998, she moved to Brown University School of Medicine where she launched the Center for Clinical Trials and Evidence-Based Healthcare. In 2005, she accepted a position directing the Center for Clinical Trials at Johns Hopkins Bloomberg School of Public Health in Baltimore, renamed in 2014 to the Center for Clinical Trials and Evidence Synthesis.\n\nDickersin’s research career has spanned several areas related to clinical trials and systematic reviews of trials. She has led clinical trials, for example she was principal investigator of two federally funded multicenter randomized trials, the data center for the Ischemic Optic Neuropathy Decompression Trial (IONDT) and the Surgical Treatments Outcomes Project for Dysfunctional Uterine Bleeding (STOP-DUB). Dickersin became interested in systematic reviews in the mid-1980s, especially the problem of identifying all relevant research on a topic. While her early research focused on establishing the existence of publication bias and the difficulty of retrieving the clinical trial literature, her later research has explored how selective outcome reporting may distort knowledge from clinical trials[16]. She has also contributed to implementation of “fixes” related to these problems, aimed at increasing research integrity. For example, she has contributed to the establishment of clinical trial registries, promoting public accessibility to trial findings, and establishment of the Cochrane Collaboration in 1993 \n\nOver the course of her career, Dickersin has served on a variety of federal advisory committees, for example, she was appointed by President Clinton to the National Cancer Advisory Board (1994-2000). Internationally, she was instrumental in starting the World Health Organization’s International Clinical Trials Registry Platform (ICTRP) and served as Co-Chair of the Scientific Advisory Group from 2005-2008 when it was disbanded. She has also served on numerous Institute of Medicine and National Research Council committees, which include for example: The Vaccine Safety Committee (1992-3); Committee to Advise the Department of Defense on its FY 1993 Breast Cancer Program (1993); Committee to Study the Reimbursement of Routine Patient Care Costs for Medicare Patients Enrolled in Clinical Trials (1998-9); Committee to Assess the System for Protecting Human Research Subjects (Advisory Consultant) (2001-2); Committee on the Review of Evidence on High Clinical Value Services (2005-7); Committee on Comparative Effectiveness Research Prioritization (2009); Committee for the Handling Missing Data in Clinical Trials (2009–10); Committee on Standards for Systematic Reviews of Clinical Effectiveness Research (2009–11).\n\nDickersin was a founding member of the Cochrane Collaboration in 1993, and was on the Steering Committee from 1993 to 1996. She served on the Information Management Steering Group from 2003 to 2005. She directs the US Cochrane Center, which has evolved from one of four original centers (she headed the Baltimore Cochrane Center, the first US-based center, opening in 1994). She also has directed the Cochrane Eyes and Vision Group, US Satellite, since its inception in 2002.\n\nThe US Cochrane Center coordinated the development of Cochrane’s Central Register of Controlled Trials (CENTRAL) from 1994 to 2005, which was then turned over to The Cochrane Library publisher for further development and maintenance. Dickersin’s papers related to the Cochrane Collaboration are at the National Library of Medicine.\n\nIn 2018, the US Cochrane Center at Johns Hopkins closed and Kay Dickerson stepped down from her position as its director.\n\nIn 1986, one year after resuming her PhD work, Dickersin was diagnosed with invasive breast cancer. This led to her starting a breast cancer support group, Arm-in-Arm, with Marsha Oakley, and engagement in national and international consumer advocacy work, as a founding mother of the National Breast Cancer Coalition in 1991. Her papers related to the start of the NBCC are at the Schlesinger Library in Cambridge, Massachusetts. As the only scientist on the Board of Directors of the NBCC, Dickersin initiated a series of “teach-ins” for the Board designed to expose Board members to the underlying concepts in biology and epidemiology that they would need to be active contributors to the research agenda. Teaching faculty included Francis Collins and others. This project expanded in 1995 to Project LEAD, a flagship science education program offered by NBCC to consumer advocates and still active in 2014. Her longstanding support for consumer and patient engagement in the research process is also evident from her involvement in the Department of Defense (DoD) Breast Cancer Programme.\nIn 2003, Dickersin initiated a coalition of consumer and health advocacy groups (later named Consumers United for Evidence-based Healthcare or CUE. The idea behind CUE was to bring together consumer groups, who traditionally have not worked together, to form a professional organization for learning and networking. In addition to serving these functions, CUE helps scientist groups to identify consumers for meaningful engagement in research and on advisory and guideline panels.\n\n\n\n\nDickersin is married and has two sons, Isaac and Edward and two grandchildren, Eden and Desmond.\n\n"}
{"id": "510257", "url": "https://en.wikipedia.org/wiki?curid=510257", "title": "Last Chance to See", "text": "Last Chance to See\n\nLast Chance to See is a 1989 BBC radio documentary series and its accompanying book, written and presented by Douglas Adams and Mark Carwardine. In the series, Adams and Carwardine travel to various locations in the hope of encountering species on the brink of extinction. The book was published in 1990.\n\nIn 2009, the BBC broadcast a television follow-up series of the same name, with Stephen Fry replacing the late Adams.\n\nIn 1985, Douglas Adams went to Madagascar in search of the (possibly extinct) lemur the aye-aye. The trip was part of a project by the World Wide Fund for Nature and British Sunday newspaper \"The Observer\", sending well-known authors to remote places to seek endangered species and write articles for \"The Observer Magazine\", to help raise awareness of ecological issues. Adams was met in Madagascar by zoologist Mark Carwardine (who was working for the WWF at the time). The \"Observer\" project was successful, and Adams and Carwardine developed a radio series around the same concept for BBC Radio 4. Carwardine later said:\n\nThe journeys undertaken were to see:\nThe aye-aye programme was broadcast on BBC Radio 4 on 1 November 1985 as a pilot; six further episodes were then broadcast in 1989:\n\nThe mountain gorilla and northern white rhino, although the subject of a chapter in the book, did not feature in the radio series.\nIn 1990, an accompanying book was published in the UK, describing the various adventures that duo had encountered on journeys, often with a comic tone. The book covers most of the radio episodes, but excludes the Juan Fernández fur seal and the Amazonia Manatee. It includes some colour photographs taken by Carwardine.\n\nThe first American hardcover edition was published by Harmony Books in 1991 (under ) and the first German paperback edition was published in 1992 by Heyne (under ). These editions carry slightly different photographs of the journeys. An abridged audiobook, read by Adams, was also published.\n\nIn the posthumous biography and essay collection, \"The Salmon of Doubt\", Adams describes \"Last Chance to See\" as his favourite work.\n\nThe Voyager Company also published a 2 CD-ROM set (for Microsoft Windows 3.1 and Macintosh System 6), in 1992, featuring over 800 still photographs, Adams reading the nearly complete book, Carwardine reading fact files on the species they searched like side bars, and extracts from the BBC Radio 4 series.\n\nIn 2009, the BBC broadcast a TV follow-up series, in which Stephen Fry, a friend of the late Adams, accompanies Carwardine on a journey to see what has changed in the 20 years since the radio broadcasts. The series excludes the Rodrigues fruit bat, the Yangtze river dolphin, which is \"in all probability extinct\", and the Juan Fernandez fur seal, which had proved embarrassingly easy for Adams and Carwardine to find.\n\n\n"}
{"id": "220657", "url": "https://en.wikipedia.org/wiki?curid=220657", "title": "Leadbeater's possum", "text": "Leadbeater's possum\n\nThe Leadbeater's possum (\"Gymnobelideus leadbeateri\") is a critically endangered possum largely restricted to small pockets of alpine ash, mountain ash, and snow gum forests in the Central Highlands of Victoria, Australia, north-east of Melbourne. It is primitive, relict, and non-gliding, and, as the only species in the petaurid genus \"Gymnobelideus\", represents an ancestral form. Formerly, Leadbeater's possums were moderately common within the very small areas they inhabited; their requirement for year-round food supplies and tree-holes to take refuge in during the day restricts them to mixed-age wet sclerophyll forest with a dense mid-story of \"Acacia\". The species was named in 1867 after John Leadbeater, the then taxidermist at the Museum Victoria. They also go by the common name of fairy possum. On 2 March 1971, the State of Victoria made the Leadbeater's possum its faunal emblem.\n\nLeadbeater's possum is thought to have evolved about 20 million years ago. It was not discovered until 1867 and was originally known only through five specimens, the last one collected in 1909. From that time on, the fear that it might be extinct gradually grew into near-certainty after the swamps and wetlands in Australia around Bass River in south-west Gippsland were drained for farming in the early 1900s.\n\nBy the time of the 1939 Black Friday fires, the species was thought to have been extinct. Then, on 3 April 1961, a member of the species was rediscovered by naturalist Eric Wilkinson in the forests near Cambarville, and the first specimen in more than 50 years was captured later in the month.\n\nIn 1961, a colony was discovered near Marysville. Extensive searches since then have found the existing population in the highlands. However, the availability of suitable habitat is critical: forest must be neither too old nor too young, with conservation efforts for Leadbeater’s possum involving protection of remaining old-growth stands, and maintenance of younger stands that are allowed to attain hollow-bearing age.\n\nThe combination of 40-year-old regrowth (for food) and large dead trees left still standing after the fires (for shelter and nesting) allowed the Leadbeater's possum population to expand to an estimated peak of about 7500 in the early 1980s. From its peak in the 1980s, the Leadbeater's possum population was expected to further decline rapidly, by as much as 90%, due to a habitat bottleneck. The population has dropped sharply since 1996. Particularly, the February 2009 Black Saturday bushfires destroyed 43% of Leadbeater's possums' habitat in the Central Highlands, halving the wild population to 1,500. A study in 2014 concluded there is a 92% chance the Leadbeater's ecosystem in the Victoria central highlands will collapse within 50 years.\n\nLeadbeater's possums are rarely seen as they are nocturnal, fast-moving, and occupy the upper story of some of the tallest forest trees in the world. They have an average body length of 33 cm (13 inches) with the tail included. They live in small family colonies of up to 12 individuals, including one monogamous breeding pair. Mating occurs only once a year, with a maximum of two joeys being born to each pair. All members sleep together in a nest made out of shredded bark in a tree hollow, anywhere from 6 to 30 metres above ground level and roughly in the centre of a territory of 3 hectares, which they defend actively. The society of Leadbeater's possums is matriarchal: each group is dominated by only one female Leadbeater's possum that is active in expelling outsiders. Other juvenile females are weaned off before they reach sexual maturity. In addition, female Leadbeater's possums are more aggressive in nature, often engaging in frequent fights with other females, including their own daughters. Due to the constant attacks, young females are forced to leave much earlier than their male brothers, which results in the extremely high male to female ratio of 3:1.\n\nSolitary Leadbeater's possums have difficulty surviving: when young males disperse at about 15 months of age, they tend either to join another colony as a supernumerary member, or to gather together into bachelor groups while they wait to find a mate.\n\nAt dusk, Leadbeater's possums emerge from the nest and spread out to forage in the sub-canopy, often making substantial leaps from tree to tree (they require continuous understory to travel). Their diet is omnivorous: feeding on a range of wattle saps and exudates, lerps, and a high proportion of arthropods which they find under the loose bark of eucalypts, including spiders, crickets, termites and beetles. Plant exudates make up 80% of their energy intake, but the protein provided by the arthropods is essential for successful breeding.\n\nBirths are usually timed for the beginning of winter (May and June) or late spring (October and November). Most litters are of one or two young, which stay in the pouch for 80 to 90 days, and first emerge from the nest following this. Young, newly independent Leadbeater's possums are very vulnerable to owls.\n\nLeadbeater's possums and their forest habitat have been the subject of the largest longitudinal study of any species in the world—conducted by David Lindenmayer, a professor at the Australian National University, and his research assistants since 1983. Hundreds of peer reviewed scientific papers, journal articles and books have resulted from the years of data collection by the ANU team. Their findings show that the availability of suitable habitat is critical: forest must be neither too old nor too young, with conservation efforts for Leadbeater’s possums involving protection of remaining old-growth stands, and maintenance of younger stands that are allowed to attain hollow-bearing age. Clearfell logging and salvage logging (after bushfires) have been proven by the researchers to have been the greatest threat to the possums' conservation in the wild over the last three decades of the 20th century.\n\nThe entire Central Highlands population distribution is confined to a 70 by 80 kilometre area. With 43% of its known Central Highlands habitat destroyed in the bushfires of February 2009 – large areas of forest around Toolangi, Marysville, Narbethong, Cambarville and Healesville – the species' status is currently in doubt. Consequently, in December 2012, David Lindenmayer and Zoos Victoria's threatened species biologist, Dan Harley, submitted an application to the federal government for a revision of the species status, providing evidence that it should be relisted as critically endangered. The then minister for the environment, Tony Burke, agreed with the nomination and forwarded the application to the scientific committee of the EPBC Act requesting urgent consideration. On 22 April 2015, it was decided to relist the species as critically endangered.\n\nThe only remaining population outside the Central Highlands is located at Yellingbo Nature Conservation Reserve. Harley has estimated this population to be fewer than 50.\n\nAs the species is endangered and occupies a restricted range, logging continues to pose a critical threat to the Leadbeater's possum. The logging in 1993 of \"much of the possum's habitat, known as zone one\" a five hectare reserve east of Powelltown, followed a \"mapping error\". Author Peter Preuss stated that the possum's population faltered in 1997 with current habitat (limited to a 50-square-kilometre area) under threat from logging. He emphasised the need to relaunch a breeding program.\n\nDespite a joint federal and state government plan to save it, since the 1980s, the Leadbeater's possum population halved to around 2000 even before the Black Saturday fires. Many more were killed early in 2007 when the government-backed enterprise company, VicForests, bulldozed large firebreaks through Leadbeater's monitoring stations following the Christmas fires – firebreaks and clear-felling also prevent breeding with nearby colonies.\nDavid Lindenmayer (Australian National University) has argued that the need for nest boxes indicates that logging practices are not ecologically sustainable for conserving hollow-dependent species like the Leadbeater's possum. Studies have shown that clear-felling operations, such as the logging run in state forest between the Yarra Ranges National Park and Mount Bullfight Conservation Reserve in February 2006, led to the deaths of most possums in the area—\"Adult animals have a strong affinity with their home range and are reluctant to move\".\n\nSalvage logging since the fires has posed a further risk to this extremely diminished population with clear-felling also approved by VicForests in the few remaining unburnt areas, such as the Kalatha Creek area of Toolangi State Forest in 2010, a move opposed by the Yarra Ranges Shire Council.\n\nIn 2012 MyEnvironment challenged VicForests' operations in three planned coupes in the Toolangi forest in the supreme court. The basis of their claim being that \"VicForests did not undertake adequate pre-logging surveys prior to logging in an area that we claim meets Leadbeater’s habitat and therefore should not have been logged.\" The proposed logging is to supply (taxpayer subsidised) pulp to manufacture 'Reflex' copy paper—a product of Australian Paper owned by the Japanese company, Nippon Paper Group. During the case, film was recorded of a Leadbeater's possum in the contested coupe area. The case was lost by MyEnvironment due to inconsistencies in the wording of the Leadbeater's Possum Action Statement (10 years out of date) and the forestry prescriptions adhered to by VicForests. The group immediately appealed the decision by the presiding judge Justice Osborne, and the supreme court accepted there was a sound basis for an appeal to the original determination. A supreme court appeal was heard on 24 June 2013 before three judges and MyEnvironment was represented in court by Julian Burnside QC. The appeal was lost.\n\nPreviously feral cats had only been considered a peripheral threat to Leadbeater's possums, but recent research has found video evidence of cats preying on possums leaving nesting boxes, and of possum remains in stomach contents of trapped feral cats. It is now considered that cats may be a more significant threat to possum populations, particularly in areas already disturbed by logging or bushfires.\n\nOn 27 June 2013 the Napthine led State government passed legislative changes to allow VicForests access to Victoria's forests for the next 25 years and to be self monitoring (this follows the success of other recent cases preventing logging of remaining possum habitat). According to The Wilderness Society, \"the Victorian government ... [is] virtually signing the death warrant of the remaining 500 or so Leadbeater's possums.\" These changes to the \"Sustainable Forests (Timber) Act 2004\" will have implications not only for the Leadbeater's possum but to the biodiversity, carbon storage and water catchments of the forests.\n\nOn 22 April 2015, Greg Hunt, the Minister for the Environment, announced that the Leadbeater's possum would be listed as a critically endangered species under the EPBC Act. Of its ash forest habitat about 30% is protected, while the rest is allocated to logging. The habitat of a small isolated, genetically distinct, population is protected within the Yellingbo Nature Conservation Reserve. In 2013 it was proposed to create the \"Great Forest National Park\" to protect the mountain ash forest habitat. The park would protect the area between Kinglake, Baw Baw and Eildon national parks, which is also important for Melbourne's drinking water and as a carbon sink.\n\nSince 2004, the Friends of Leadbeater's Possum community group has been active in raising the animal's profile and lobbying for its conservation.\n\nThrough a joint community/government program, \"Project Possum\" has installed approximately 200 plastic nest boxes in the wild. Many of these nest boxes were paid for by a community fundraising campaign. The nest boxes are primarily used to assist with ongoing population monitoring and supplement the declining forest habitat. Project Possum has targeted two forest types: montane ash forest (i.e. Mt Ritchie, Dowey Spur, Ben Cairn) and sub-alpine woodland (i.e. Mount Baw Baw, Lake Mountain, and Mount Bullfight). The nest boxes are routinely checked for habitation every one or two years. Nest boxes located in the sub-alpine woodland tend to have a high uptake, while those located in montane ash forest have very limited uptake. An additional 50 nest boxes are due for installation in 2015–16.\n\nThe forestry industry and Barnaby Joyce advocate for the Leadbeater's possum to be taken off the critically endangered list.\n\nDes Hackett is credited as the first person to successfully breed the Leadbeater's possum in captivity. In May 2006, the last Australian specimen at the time, held at Healesville Sanctuary, died. In January 2010, Kasia, at the time the last captive Leadbeater's possum worldwide, died at Toronto Zoo. of the few Lake Mountain Leadbeater's possums remaining after the 2009 bushfire led to three remaining individuals being taken into captivity for their own protection. One animal has since died. There are no plans to release the remaining two animals despite a further two colonies of Leadbeater's possums having recently been located at Lake Mountain in remnant gully vegetation. These two Lake Mountain animals are now on public display in the Nocturnal House as ambassadors for the species. Healesville Sanctuary's captive breeding program for Leadbeater's possums recommenced in May 2012 and now comprises 6 individuals from the genetically distinct Yellingbo population. they are housed as pairs in large enclosures off display, but are yet to breed.\n\n\n"}
{"id": "36217770", "url": "https://en.wikipedia.org/wiki?curid=36217770", "title": "List of Turkic scholars", "text": "List of Turkic scholars\n\nThe following is a non-comprehensive list of Central Asian scientists and engineers who lived from antiquity up until the Ottoman era.\n\n\nThe following is a non-comprehensive list of Ottoman scientists and engineers of Turkic descent who lived in the Ottoman Empire.\n\n\n"}
{"id": "1100194", "url": "https://en.wikipedia.org/wiki?curid=1100194", "title": "List of analyses of categorical data", "text": "List of analyses of categorical data\n\nThis a list of statistical procedures which can be used for the analysis of categorical data, also known as data on the nominal scale and as categorical variables.\n\n\n\n\n\n"}
{"id": "3767575", "url": "https://en.wikipedia.org/wiki?curid=3767575", "title": "List of credentials in psychology", "text": "List of credentials in psychology\n\nThis list is of professional and academic credentials in the field of psychology and allied fields in North America, including psychotherapy, counseling, social work, and family therapy.\n\nAlthough undergraduate (Bachelor's) degrees for psychology and counseling exist, in most jurisdictions the minimum requirement for professional licensure is a graduate degree (master's or doctorate).\n\n\n\nProfessional licenses for mental health providers with a master's degree issued by US states to graduate degree holders which allow them to legally practice (additional requirements/training/hours, for Supervisors \"-S\"):\n\n\nCertifications for licensed providers are offered by various non-profit and for-profit organizations such as the National Board for Certified Counselors and Affiliates. In most states, a license to practice is also required.\n\n"}
{"id": "43334032", "url": "https://en.wikipedia.org/wiki?curid=43334032", "title": "List of fossiliferous stratigraphic units in Central America", "text": "List of fossiliferous stratigraphic units in Central America\n\nThis page lists the fossiliferous stratigraphic units in Central America.\n\n"}
{"id": "24590558", "url": "https://en.wikipedia.org/wiki?curid=24590558", "title": "List of impact craters in Antarctica", "text": "List of impact craters in Antarctica\n\nThis List of impact craters in Antarctica includes only unconfirmed and theoretical impact sites in Antarctica and the surrounding waters of the Southern Ocean. There are not yet any confirmed impact sites in Antarctica according to the Earth Impact Database.\n\nThe following craters are officially considered \"unconfirmed\" because they are not listed in the Earth Impact Database. Due to stringent requirements regarding evidence and peer-reviewed publication, newly discovered craters or those with difficulty collecting evidence generally are known for some time before becoming listed. However, entries on the unconfirmed list could still have an impact origin disproven.\n\n\n"}
{"id": "815092", "url": "https://en.wikipedia.org/wiki?curid=815092", "title": "List of important publications in geology", "text": "List of important publications in geology\n\nThis is a list of important publications in geology, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\nCompilations of important publications can be found in Further reading.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "40236269", "url": "https://en.wikipedia.org/wiki?curid=40236269", "title": "List of things named after Max Born", "text": "List of things named after Max Born\n\nMax Born was a scientist who worked in many fields. Below is a list of things named in his honour.\n\n\n\n\n"}
{"id": "23446323", "url": "https://en.wikipedia.org/wiki?curid=23446323", "title": "MeerKAT", "text": "MeerKAT\n\nMeerKAT, originally the Karoo Array Telescope, is a radio telescope consisting of 64 antennas now being tested and verified in the Northern Cape of South Africa. When fully functional it will be the largest and most sensitive radio telescope in the southern hemisphere until the Square Kilometre Array is completed in approximately 2024. The telescope will be used for research into cosmic magnetism, galactic evolution, the large-scale structure of the cosmos, dark matter and the nature of transient radio sources. It will also serve as a technology demonstrator for South Africa's bid to host the Square Kilometer Array. First light was on 16 July 2016. As of May 2018, all sixty-four 13.5-meter diameter (44.3 feet) dish antennae have been completed and are currently undergoing verification tests .\n\nMeerKAT will consist of 64 dishes of 13.5 metres in diameter each with an offset Gregorian configuration. An offset dish configuration has been chosen because its unblocked aperture provides uncompromised optical performance and sensitivity, excellent imaging quality and good rejection of unwanted radio frequency interference from satellites and terrestrial transmitters. It also facilitates the installation of multiple receiver systems in the primary and secondary focal areas and is the reference design for the mid-band SKA concept.\n\nMeerKAT supports a wide range of observing modes, including deep continuum, polarisation and spectral line imaging, pulsar timing and transient searches. A range of standard data products are provided, including an imaging pipeline. A number of \"data spigots\" are also available to support user-provided instrumentation. Significant design and qualification efforts are planned to ensure high reliability to achieve low operational cost and high availability.\n\nMeerKAT's 64 dishes will be distributed over two components:\n\nFor Phase 2, seven additional antennas will be added to extend the longest baselines to about 20 km.\n\nTo acquire experience in the construction of interferometric telescopes, members of the Karoo Array Telescope constructed the Phased Experimental Demonstrator (PED) at the South African Astronomical Observatory in Cape Town between 2005 and 2007.\n\nDuring 2007, the eXperimental Development Model Telescope (XDM) was built at the Hartebeesthoek Radio Astronomy Observatory to serve as a testbed for MeerKAT.\n\nConstruction of the MeerKAT Precursor Array (MPA – also known as KAT-7), on the site started in August 2009. In April 2010 four of the seven first dishes were linked together as an integrated system to produce its first interferometric image of an astronomical object. In Dec 2010, there was a successful detection of very long baseline interferometry (VLBI) fringes between the Hartebeesthoek Radio Astronomy Observatory 26 m dish and one of the KAT-7 dishes.\n\nDespite original plans to complete MeerKAT by 2012, construction was suspended in late 2010 due to budget restructure. Science Minister Naledi Pandor denied the suspension marked any setback to the SKA project or 'external considerations'. MeerKAT construction received no funding in 2010/11 and 2011/12. The 2012 South African National Budget projected that just 15 MeerKAT antennas would be completed by 2015.\n\nThe last of the reinforced concrete foundations for the MeerKAT antennas was completed on 11 February 2014. Almost 5000 m of concrete and over 570 tonnes of steel were used to build the 64 bases over a 9-month period.\n\nMeerKAT is planned to be completed in three phases. The first phase will include all the antennas but only the first receiver will be fitted. A processing bandwidth of 750 MHz is available. For the second and third phases, the remaining two receivers will be fitted and the processing bandwidth will be increased to at least 2 GHz, with a goal of 4 GHz. With construction of all sixty-four MeerKAT antennas complete, verification tests have begun to ensure the instruments are functioning correctly. Following this, MeerKAT will be commissioned in the second half of 2018 with the array then coming online for science operations. \n\nThe science objectives of the MeerKAT surveys are in line with the prime science drivers for the first phase of the SKA, confirming MeerKAT's designation as an SKA precursor instrument. Five years of observing time on MeerKAT have been allocated to leading astronomers who have applied for time to do research.\nMeerKAT will also participate in global VLBI operations with all major radio astronomy observatories around the world and will add considerably to the sensitivity of the global VLBI network. Further potential science objectives for MeerKAT are to participate in the search for extraterrestrial intelligence and collaborate with NASA on downloading information from space probes.\n\n\n"}
{"id": "46644935", "url": "https://en.wikipedia.org/wiki?curid=46644935", "title": "Microtrends", "text": "Microtrends\n\nMicrotrends: The Small Forces Behind Tomorrow's Big Changes is a non-fiction book by Mark Penn and Kinney Zalesne. The text was initially published by Twelve on September 5, 2007. Mark Penn has been named the winner of the Consumer Insights category in the 2010 Atticus Awards for this book. \n\nThe text focuses on subtle and invisible trends in demography, sociology, business, family life, technologies, human interactions, and many other areas—that are currently shaping the potential future the world and society. Overall, the authors try to categorize approximately 75 microtrends (hence the name of the book) seen in the modern world.\n\nThe book examines how small groups of people can trigger big changes. Specifically, Penn shows how a mere one percent of the American public, or 3 million people, can create a \"microtrend\" capable of launching a major business or even a new cultural movement, changing commercial, political and social landscapes. From December 2008 to December 2009, Penn authored a regular online column for the \"Wall Street Journal\" called \"Microtrends\", focusing on demographic trends in society and business. \"Microtrends\" appeared regularly in the Media & Marketing section of the \"Wall Street Journal\".\n\n—Review by \"The New York Times\"\n\n"}
{"id": "19100297", "url": "https://en.wikipedia.org/wiki?curid=19100297", "title": "Military art (military science)", "text": "Military art (military science)\n\nMilitary art () (lit. art of war) is a field of theoretical research and training methodology in military science used in the conduct of military operations on land, in the maritime or air environments. Military art includes the study and application of the principles of warfare and laws of war that apply equally to the closely interrelated military strategy, operational art and tactics. Exercise of military art is highly dependent on the economics and logistics supporting the armed forces, their military technology and equipment, and reflects the social influences on the military organisation exercising military art. Often misunderstood due to its 19th-century perception as generally \"including the entire subject of war\", it is primarily, as the term implies, the expression of creative thinking on the part of the decision-makers in employing their forces, with the map of the area of operations as a veritable canvas, and the movement of forces commonly marked on the map with arrows, as brush strokes. Less imaginatively it was defined in France during the 19th century as \nThe art of war is the art of concentrating and employing, at the opportune moment, a superior force of troops upon the decisive point.\nThe art of divining the intention of the enemy from slight indications is one which rarely misleads, and is one of the most precious attributes of military genius.\n\nIt is not well known that many of the greatest military leaders in Europe and Asia, notably Japan, were themselves either accomplished artists, or collectors of art.\n\nIn history, schools of thinking about military art can be divided during the ancient and medieval periods by the influence of infantry troops on tactics in Europe, and cavalry on tactics in Asia, and the period commencing with the Early Modern period when firearms and artillery increasingly influenced employment of forces. The need for mobility, opportunity and decisiveness have tended to associate military art with offensive manoeuvre, cavalry, and therefore, before advent of late-19th century firearms, Asia.\n\nIn Europe military art was primarily concerned with time of combat, understanding the best way an occupied position can enhance defensive potential of a small field army largely consisting of relatively low mobility troops, by often using high ground, terrain choke points or field entrenchments. Due to relative lack of economic and logistic support, European military art theorists advocated decisive battles that could bring a military campaign to a quick conclusion, thus reducing the economic cost of war, notably through the shock tactics of the armoured cavalry.\nIn Asia, due to a more developed and widespread horse breeding offering enhanced mobility, military art developed a more offensive-based way of thinking about military tactics, and military art was dominated by considerations of choosing point of attack and military communications.\n\nThe confrontation between these two forms of military art that took place as a result of the Crusades and the Mongol invasion of Europe, and the contemporaneous introduction of artillery into warfare significantly changed thinking about military art in Europe, leading to wide-ranging experiments in tactical formation of troops, use of combined arms and exercise of maneuver warfare concepts and methods not only in tactics, but on a larger scale, including in use of naval forces. This later led to creation of European colonial empires.\n\nAlthough European military art on land can be argued not to have reached its developed stage until the 19th century in the attempt to defeat linear tactics that led to the First World War, and ultimate sophistication during the Second World War and the Cold War, great increases in the firepower of the European firearms and artillery were usually able to negate greater number and mobility of the Asian field forces, no-where more illustrated than during the French Invasion of Egypt in 1798. The defining application of firepower and manoeuvre in military art became expressed during the conduct of strategic operations of the Red Army in World War II that sought to combine the shock of armoured warfare with mobility, and the traditional reliance on infantry in strengthened positions to first stop the German advances and during counter-attacks to achieve breakthrough of the enemy position, and in conducting deep operations to destroy the logistic support, literally starving German troops of supplies and ammunition, and forcing them to surrender.\n\nMilitary art in naval warfare likewise developed along the considerations of position versus speed, initially through use of galleys and later during the Age of Sail. However, increasingly naval tactics became dominated by the relative firepower of individual vessels, and this was demonstrated during the epic Battle of Trafalgar. The use of naval forces in military art significantly expanded the scale of conflicts to global ranges. For much of the 19th and 20th centuries naval military art became concerned primarily with the firepower and the ability of ships to withstand it through use of increasing their armour until the introduction of naval aviation into consideration of naval battles and naval strategy.\n\nIntroduction of aircraft into warfare drastically altered the 19th century understanding of military art, and application of its principles. Aerial warfare became the ultimate solution to manoeuvre in delivery of firepower, and drastically increased the tempo of conflicts, making blitzkrieg possible during the Second World War. Aerial warfare also finally negated the advantage of a defensive position which left the defender as an easily detected and attacked target for the bombers. While German industrial centres became just such targets for the Allied strategic bombing during World War II, on the Eastern Front the Soviet Red Army chose to adapt by developing concepts and methods suggested by theorists during the interwar period to a more offensive-based and dynamic conduct of operations coordinating ground and air offensives.\n\nThe final development in military art came with the increased influence of the electronics and its implications for the ability by infantry to halt the breakthroughs using increasingly powerful infantry support weapons, and the effect on the military communications by a new form of combat, electronic warfare. The potential of losing ability to exercise command and control over forces, particularly strategically significant forces, completely undermined much of the theoretical thinking of the Cold War. Some have argued that this was one of the factors, along with those of economic considerations, that eventually led to the Treaty on Conventional Armed Forces in Europe, and Dissolution of the USSR.\n\nSince the end of the Cold War in Europe, application of military art has been sought primarily by conventional forces in combat operations against unconventional opponents, and this has many attempts to review military history in the effort to find solutions to successful conduct of operations against these forces.\n\n"}
{"id": "34195662", "url": "https://en.wikipedia.org/wiki?curid=34195662", "title": "Peace Research Institute Frankfurt", "text": "Peace Research Institute Frankfurt\n\nEstablished in 1970, the Peace Research Institute Frankfurt (PRIF), or Hessische Stiftung Friedens und Konfliktforschung (HSFK) was founded by the state of Hesse.\n\nWith approximately 70 employees (as of 2011), PRIF is Germany’s largest and oldest peace research institute. PRIF does not carry out any commissioned research.\n\nPRIF investigates the causes of international crisis and conflict, pursues research on peace and conflict studies and disarmament policy. PRIF places a regional focus on the Balkans, the Middle East and Asia, while researching the necessary conditions to increase justice and decrease violence (i.e. establish peace). It places special emphasis in its research work on arms control, non-proliferation and disarmament. From 1987 to 2002, the PRIF trained many young academics in countries having no non-proliferation expertise, organizing a network of European non-proliferation researchers. PRIF researchers have long-standing experience in political advice and consulting, having served, inter alia, in German delegations to NPT, CTBT, CWC, BWC, Ottawa Convention and SALW gatherings, in the UN Conference on Disarmament and in IAEA Expert Groups.\n\nThe Executive Director of the PRIF is Nicole Deitelhoff. Other members of the Executive Board are Christopher Daase, Susanne Boetsch, Peter Kreuzer, Sabine Mannitz and Jonas Wolff.\n\nThe Research Council is a body that includes all PRIF scholars. It makes decisions on the research program and on the projects of the individual research departments.\n\nThe Board of Trustees, consisting of Hesse’s Prime Minister, the Minister for Science and the Arts, and the Finance Minister of Hesse, three public figures and three elected representatives from within the institute, supervises the management of the institution and approves its budget.\n\nThe Advisory Board supports the PRIF on the design and implementation of the institute's research programs. Four-year terms are by appointment of the Board of Trustees which is made up of eight scholars from Germany and abroad. The Advisory Board chairman is currently Thomas Risse.\n\nAt the end of 2004, the PRIF was evaluated by a commission of the German Council of Science and Humanities and recommended for acceptance into the Leibniz Association. On January 1, 2009 the PRIF became a member institute in the scientific organization.\n\nSince 2009, PRIF devotes its work to the research program \"Just Democratic Peace\" which analyses justice-related conflicts and the ways in which peace and justice can be achieved at the same time.\n\nResearch at the PRIF is broken down into seven departments:\n\n\nIn addition to anthologies and monographs in the publication series \"Studies of the Peace Research Institute Frankfurt,\" PRIF publishes approximately ten HSFK-Reports (PRIF Reports in English),as well as the series \"HSFK-Standpunkte – Beiträge zum demokratischen Frieden\" [PRIF Viewpoints – Articles on Democratic Peace, published in German only]. PRIF publications can be downloaded online or ordered directly from the institute’s web site. Print versions are also available in many libraries.\n\nThe State of Peace Report, which has been appearing since 1987, is the yearbook of five German peace research institutes. Besides PRIF, the Institute for Peace Research and Security Policy at the University of Hamburg (IFHS) and the Research Center of the Evangelical Studies Community in Heidelberg FEST are involved. Since 1999 the Bonn International Center for Conversion (BICC) and the Institute for Development and Peace in Duisburg (INEF) have also been involved.\n\nThe State of Peace Yearbook consists of an introductory statement followed by several analyses of current conflicts as well as foreign and security policy, supplemented by concrete policy recommendations. Since 2007, the Deutsche Stiftung Friedensforschung (German Foundation for Peace Research) has provided financial support for the State of Peace Report.\n\n"}
{"id": "20506304", "url": "https://en.wikipedia.org/wiki?curid=20506304", "title": "Property testing", "text": "Property testing\n\nIn computer science, a property testing algorithm for a decision problem is an algorithm whose query complexity to its input is much smaller than the instance size of the problem. Typically property testing algorithms are used to decide if some mathematical object (such as a graph or a boolean function) has a \"global\" property, or is \"far\" from having this property, using only a small number of \"local\" queries to the object.\n\nFor example, the following promise problem admits an algorithm whose query complexity is independent of the instance size (for an arbitrary constant ε > 0):\n\nProperty testing algorithms are central to the definition of probabilistically checkable proofs, as a probabilistically checkable proof is essentially a proof that can be verified by a property testing algorithm.\n\nFormally, a property testing algorithm with query complexity \"q\"(\"n\") and \"proximity parameter\" ε for a decision problem \"L\" is a randomized algorithm that, on input \"x\" (an instance of \"L\") makes at most \"q\"(|\"x\"|) queries to \"x\" and behaves as follows:\n\nHere, \"\"x\" is ε-far from \"L\"\" means that the Hamming distance between \"x\" and any string in \"L\" is at least ε|\"x\"|.\n\nA property testing algorithm is said to have \"one-sided error\" if it satisfies the stronger condition that the accepting probability for instances \"x ∈ L\" is 1 instead of ⅔.\n\nA property testing algorithm is said be \"non-adaptive\" if it performs all its queries before it \"observes\" any answers to previous queries. Such an algorithm can be viewed as operating in the following manner. First the algorithm receives its input. Before looking at the input, using its internal randomness, the algorithm decides which symbols of the input are to be queried. Next, the algorithm observes these symbols. Finally, without making any additional queries (but possibly using its randomness), the algorithm decides whether to accept or reject the input.\n\nThe main efficiency parameter of a property testing algorithm is its query complexity, which is the maximum number of input symbols inspected over all inputs of a given length (and all random choices made by the algorithm). One is interested in designing algorithms whose query complexity is as small as possible. In many cases the running time of property testing algorithms is sublinear in the instance length. Typically, the goal is first to make the query complexity as small as possible as a function of the instance size \"n\", and then study the dependency on the proximity parameter ε.\n\nUnlike other complexity-theoretic settings, the asymptotic query complexity of property testing algorithms is affected dramatically by the representation of instances. For example, when ε = 0.01, the problem of testing bipartiteness of \"dense graphs\" (which are represented by their adjacency matrix) admits an algorithm of constant query complexity. In contrast, sparse graphs on \"n\" vertices (which are represented by their adjacency list) require property testing algorithms of query complexity formula_2.\n\nThe query complexity of property testing algorithms grows as the proximity parameter ε becomes smaller for all non-trivial properties. This dependence on ε is necessary as a change of fewer than ε symbols in the input cannot be detected with constant probability using fewer than O(1/ε) queries. Many interesting properties of dense graphs can be tested using query complexity that depends only on ε and not on the graph size \"n\". However, the query complexity can grow enormously fast as a function of ε. For example, for a long time the best known algorithm for testing if a graph does not contain any triangle had a query complexity which is a tower function of \"poly\"(1/ε), and only in 2010 this has been improved to a tower function of \"log\"(1/ε). One of the reasons for this enormous growth in bounds is that many of the positive results for property testing of graphs are established using the Szemerédi regularity lemma, which also has tower-type bounds in its conclusions.\n\n"}
{"id": "4981467", "url": "https://en.wikipedia.org/wiki?curid=4981467", "title": "Quantum Philosophy", "text": "Quantum Philosophy\n\nQuantum Philosophy is a 2002 book by the physicist Roland Omnès, in which he aims to show the non-specialist reader how modern developments in quantum mechanics allow the recovery of our common sense view of the world.\n\n\nOmnès' project is not quite as it at first sounds. He is not trying to show that quantum mechanics itself can be understood in a common sense framework, quite the opposite. He argues that modern science has, necessarily, become more and more formal, and more and more remote from common sense, as it strives to make itself an accurate reflection of the physical world. But he argues that we have now come near enough to scaling the 'magnificent peaks' of the formal mathematics needed to describe reality for one thing to have finally become clear: it is now possible to demonstrate, formally, and starting from the underlying principles of quantum mechanics, that the laws of classical logic, classical probability and classical dynamics apply to objects at the macroscopic level.\n\nAs Omnès makes explicit, this is the exact opposite of the classical epistemological project. It has always, up to now, been necessary to access reality by first presupposing the laws of classical common sense. Now finally, we can enter the world either at the formal level, or at the classical level, and we find that each entails the other: experiment has led to the quantum formalism; the quantum formalism now, finally, allows the recovery of the framework of classical reasoning under which the experiments took place.\n\nOmnès emphasises throughout that no new principles, other than those described when quantum mechanics was developed in the 1920s, are needed. Moreover, some additional principles which seemed to be required then (such as wavefunction collapse, or its slightly more formal sister, wavefunction reduction) are no longer needed. \"Classical behaviour can now be recovered in a system described entirely by a single, unitary (time-reversible) wavefunction.\"\n\nThe mathematical developments which allowed this progress have taken place in two fields: quantum decoherence and the consistent histories approach to quantum mechanics.\n\nThe consistent histories approach makes mathematically explicit which sets of classical questions can be consistently asked of a single quantum system, and, conversely, which sets of questions are fundamentally inconsistent, and thus meaningless when asked together. We can therefore demonstrate formally why it is that the questions which Einstein, Podolsky and Rosen assumed could be asked together, of a single quantum system, simply cannot be asked together. On the other hand, we can demonstrate that classical, logical reasoning often does apply, even to quantum experiments – but we can now be mathematically exact about the limits of classical logic.\n\nQuantum decoherence, on the other hand (in combination with the consistent histories approach), recovers classical behaviour at the macroscopic level. The formal mathematics of this approach allows us to demonstrate, finally, that is impossible (or rather, massively improbable) for a macroscopic Schrödinger's cat to exist for longer than a minuscule time (related to the macroscopic energy dissipation time by a factor involving the square of Planck's constant) in a quantum superposition of its |alive> and |dead> states. Even for a cat otherwise isolated from the rest of the Universe, \"and even with no observer present\", there are so many unknowns in the quantum state of the whole cat, that the relevant mathematics determine that only the normally observed classical states of the cat are at all probable, except over the very shortest of timescales. This reasoning is developed formally within measurement theory, and applies to any macroscopic, non-super cooled measuring device, whether or not there is an observer to watch it.\n\nOmnès makes clear that others contributed materially to the research described in his book, including Robert Griffiths, Murray Gell-Mann, and James Hartle.\n\nTranslation: This book is translated to Persian by Professor R. Roknizadeh and published in 2016 in Iran."}
{"id": "40707705", "url": "https://en.wikipedia.org/wiki?curid=40707705", "title": "Radical theory", "text": "Radical theory\n\nRadical theory is an obsolete scientific theory in chemistry describing the structure of organic compounds. The theory was pioneered by Justus von Liebig, Friedrich Wöhler and Auguste Laurent around 1830 and is not related to the modern understanding of free radicals. In this theory, organic compounds were thought to exist as combinations of radicals that could be exchanged in chemical reactions just as chemical elements could be interchanged in inorganic compounds.\n\nThe term radical was already in use when radical theory was developed. Louis-Bernard Guyton de Morveau introduced the phrase \"radical\" in 1785 and the phrase was employed by Antoine Lavoisier in 1789 in his Traité Élémentaire de Chimie. A radical was identified as the root base of certain acids (The Latin word \"radix\" meaning \"root\"). The combination of a radical with oxygen would result in an acid. For example the radical of acetic acid was called \"acetic\" and that of muriatic acid (hydrochloric acid) was called \"muriatic\". Joseph Louis Gay-Lussac found evidence for the cyanide radical in 1815 in his work on hydrogen cyanide and a number of cyanide salts he discovered. He also isolated cyanogen ((CN)) not realizing that cyanogen is the cyanide dimer NC-CN. Jean-Baptiste Dumas proposed the ethylene radical from investigations into diethyl ether and ethanol. In his Etherin theory he observed that ether consisted of two equivalents of ethylene and one equivalent of water and that ethylene and ethanol could interconvert in chemical reactions. Ethylene was also the base fragment for a number of other compounds such as ethyl acetate. This Etherin theory was eventually abandoned by Dumas in favor of radical theory. As a radical it should react with an oxide to form the hydrate but it was found that ethylene is resistant to an oxide like calcium oxide. Henri Victor Regnault in 1834 reacted ethylene dichloride (CHCH.Cl) with KOH forming vinyl chloride, water, and KCl. In etherin theory it should not be possible to break up the ethylene fragment in this way.\n\nRadical theory replaced electrochemical dualism which stated that all molecules were to be considered as salts composed of basic and acidic oxides.\n\nLiebig and Wöhler observed in 1832 in an investigation of benzoin resin (benzoic acid) that the compounds almond oil (benzaldehyde), \"Benzoestoff\" (benzyl alcohol), benzoyl chloride and benzamide all share a common CHO fragment and that these compounds could all be synthesized from almond oil by simple substitutions. The CHO fragment was considered a \"radical of benzoic acid\" and called benzoyl. Organic radicals were thus placed on the same level as the inorganic elements. Just like the inorganic elements (\"simple radicals\") the organic radicals (\"compound radicals\") were indivisible. The theory was developed thanks to improvements in elemental analysis by von Liebig. Laurent contributed to the theory by reporting the isolation of benzoyl itself in 1835, however the isolated chemical is today recognised at its dimer dibenzoyl. Raffaele Piria reported the salicyl radical as the base for salicylic acid. Liebig published a definition of a radical in 1838 \n\nBerzelius and Robert Bunsen investigated the radical cacodyl (reaction of cacodyl chloride with zinc) around 1841, now also known as a dimer species (CH)As—As(CH). Edward Frankland and Hermann Kolbe contributed to the radical theory by investigating the ethyl and the methyl radicals. Frankland first reported diethylzinc in 1848. Frankland and Kolbe together investigated the reaction of ethyl cyanide and zinc in 1849 reporting the isolation of not the ethyl radical but the methyl radical (CH) which in fact was ethane. Kolbe also investigated the electrolysis of potassium salts of some fatty acids. Acetic acid was regarded as the combination of the methyl radical and oxalic acid and electrolysis of the salt yielded as gas again ethane misidentified as the liberated methyl radical.\nIn 1850 Frankland investigated ethyl radicals. In the course of this work butane formed by reaction of ethyl iodide and zinc was mistakenly identified as the ethyl radical.\n\nAugust Wilhelm von Hofmann, Auguste Laurent and Charles Frédéric Gerhardt challenged Frankland and Kolbe by suggesting that the ethyl radical was in fact a dimer called dimethyl. Frankland and Kolbe countered that ethyl hydride was also a possibility and in 1864 Carl Schorlemmer proved that dimethyl and ethyl hydride were in fact one and the same compound.\n\nRadical theory was eventually replaced by a number of theories each advocating specific entities. One adaptation of radical theory was called theory of types (theory of residues), advocated by Charles-Adolphe Wurtz, August Wilhelm von Hofmann and Charles Frédéric Gerhardt. Another was water type as promoted by Alexander William Williamson. Jean-Baptiste Dumas and Auguste Laurent (an early supporter of radical theory) challenged radical theory in 1840 with a Law of Substitution (or Theory of Substitution). This law acknowledged that any hydrogen atom even as part of a radical could be substituted by a halogen.\n\nEventually Frankland in 1852 and August Kekulé in 1857 introduced valence theory with as central theme the tetravalency of carbon. making trivalent carbon obsolete for the time being.\n\nIn 1900 Moses Gomberg unexpectedly discovered true trivalent carbon and the first radical in the modern sense of the word in his (unsuccessful) attempt to make hexaphenylethane. In current organic chemistry concepts such as benzoyl and acetyl persist in chemical nomenclature but only to identify a functional group having the same fragment.\n"}
{"id": "9548465", "url": "https://en.wikipedia.org/wiki?curid=9548465", "title": "Sami Solanki", "text": "Sami Solanki\n\nSami Khan Solanki (born 1958 in Karachi, Pakistan) is director of the Max Planck Institute for Solar System Research (MPS), director of the Sun-Heliosphere Department of MPS, a scientific member of the Max Planck Society, and a Chair (and spokesperson) of the International Max Planck Research School on Physical Processes in the Solar System and Beyond at the Universities of Braunschweig and Göttingen.\n\nSolanki is also an Honorary Professor at the Institute of Astronomy at the Swiss Federal Institute of Technology Zurich, and (2) Institute for Geophysics and Extraterrestrial Physics at the Braunschweig University of Technology in Germany. In addition, he is a Distinguished Professor at the Kyung Hee University in Korea.\n\nHe is the editor-in-chief of the \"Living Reviews in Solar Physics\", an exclusively web-based, peer-reviewed journal, publishing reviews of research in all areas of solar and heliospheric physics. \"Living Reviews in Solar Physics\" was recently rated with an impact factor of 17.636 taking the third place in the \"Astronomy & Astrophysics\" category.\n\nSolanki's main topics of research are: \n\nHe has also held these positions: (1) Vice-Chairman and member of the Senate Committee of the German Aerospace Centre (DLR); (2) Member Appointment Committee and Committee of Three of the DLR; (3) Member Extraterrestrial Program Committee of the DLR; (4) Science Advisory Committee of the High Altitude Observatory, Boulder/USA; (5) Science Advisory Board at the Istituto Ricerche Solari (IRSOL), Locarno/Switzerland; and has contributed to the following space/balloon projects:\n\n\n\nIn 2011, Solanki delivered a lecture, “Is the Sun to Blame for Global Warming?,” at the first Starmus Festival in the Canary Islands. His talk was subsequently published in the book \"Starmus: 50 Years of Man in Space\".\n\nSolanki's research has been quoted as being part of the Global warming controversy, for instance in an article in the Telegraph.co.uk in 2004 as taking a sceptical position: But the same research has been quoted as being evidence for global warming in a news release from the Max Planck Society though he is quoted as calling for further investigation, saying:\n\n\n"}
{"id": "181889", "url": "https://en.wikipedia.org/wiki?curid=181889", "title": "Science education", "text": "Science education\n\nScience education is the field concerned with sharing science content and process with individuals not traditionally considered part of the scientific community. The learners may be children, college students, or adults within the general public; the field of science education includes work in science content, science process (the scientific method), some social science, and some teaching pedagogy. The standards for science education provide expectations for the development of understanding for students through the entire course of their K-12 education and beyond. The traditional subjects included in the standards are physical, life, earth, space, and human sciences.\n\nThe first person credited with being employed as a Science teacher in a British public school was William Sharp who left the job at Rugby School in 1850 after establishing Science to the curriculum. Sharp is said to have established a model for Science to be taught throughout the British Public Schools.\n\nThe next step came when the British Academy for the Advancement of Science (BAAS) published a report in 1867. BAAS promoted teaching of \"pure science\" and training of the \"scientific habit of mind.\" The progressive education movement of the time supported the ideology of mental training through the sciences. BAAS emphasized separately pre-professional training in secondary science education. In this way, future BAAS members could be prepared.\n\nThe initial development of science teaching was slowed by the lack of qualified teachers. One key development was the founding of the first London School Board in 1870, which discussed the school curriculum; another was the initiation of courses to supply the country with trained science teachers. In both cases the influence of Thomas Henry Huxley was critical (see especially Thomas Henry Huxley educational influence). John Tyndall was also influential in the teaching of physical science.\n\nIn the US, science education was a scatter of subjects prior to its standardization in the 1890s. The development of a science curriculum in the US emerged gradually after extended debate between two ideologies, citizen science and pre-professional training. As a result of a conference of 30 leading secondary and college educators in Florida, the National Education Association appointed a Committee of Ten in 1892 which had authority to organize future meetings and appoint subject matter committees of the major subjects taught in U.S. secondary schools. The committee was composed of ten educators (all men) and was chaired by Charles Eliot of Harvard University. The Committee of Ten met, and appointed nine conferences committees (Latin, Greek, English, Other Modern Languages, Mathematics, History, Civil Government and Political Economy, and three in science). The three conference committees appointed for science were: physics, astronomy, and chemistry (1); natural history (2); and geography (3). Each committee, appointed by the Committee of Ten, was composed of ten leading specialists from colleges and normal schools, and secondary schools. Each committee met in a different location in the U.S. The three science committees met for three days in the Chicago area. Committee reports were submitted to the Committee of Ten, which met for four days in New York, to create a comprehensive report. In 1894, the NEA published the results of work of these conference committees.\n\nAccording to the Committee of Ten, the goal of high school was to prepare all students to do well in life, contributing to their well-being and the good of society. Another goal was to prepare some students to succeed in college.\n\nThis committee supported the citizen science approach focused on mental training and withheld performance in science studies from consideration for college entrance. The BAAS encouraged their longer standing model in the UK. The US adopted a curriculum was characterized as follows:\n\n\nThe format of shared mental training and pre-professional training consistently dominated the curriculum from its inception to now. However, the movement to incorporate a humanistic approach, such as inclusion of the arts (S.T.E.A.M.), science, technology, society and environment education is growing and being implemented more broadly in the late 20th century (Aikenhead, 1994). Reports by the American Academy for the Advancement of Science (AAAS), including Project 2061, and by the National Committee on Science Education Standards and Assessment detail goals for science education that link classroom science to practical applications and societal implications.\n\nScience is a universal subject that spans the branch of knowledge that examines the structure and behavior of the physical and natural world through observation and experiment. Science education is most commonly broken down into the following three fields: Biology, Chemistry, and Physics.\n\nPhysics education is characterized by the study of science that deals with matter and energy, and their interactions.\n\nPhysics First, a program endorsed by the American Association of Physics Teachers, is a curriculum in which 9th grade students take an introductory physics course. The purpose is to enrich students' understanding of physics, and allow for more detail to be taught in subsequent high school biology and chemistry classes. It also aims to increase the number of students who go on to take 12th grade physics or AP Physics, which are generally elective courses in American high schools.\n\nPhysics education in high schools in the United States has suffered the last twenty years because many states now only require three sciences, which can be satisfied by earth/physical science, chemistry, and biology. The fact that many students do not take physics in high school makes it more difficult for those students to take scientific courses in college.\n\nAt the university/college level, using appropriate technology-related projects to spark non-physics majors’ interest in learning physics has been shown to be successful. This is a potential opportunity to forge the connection between physics and social benefit.\n\nChemistry education is characterized by the study of science that deals with the composition, structure, and properties of substances and the transformations that they undergo.\n\nChemistry is the study of chemicals and the elements and their effects and attributes. Students in chemistry learn the periodic table. The branch of science education known as \"chemistry must be taught in a relevant context in order to promote full understanding of current sustainability issues.\" As this source states chemistry is a very important subject in school as it teaches students to understand issues in the world. As children are interested by the world around them chemistry teachers can attract interest in turn educating the students further. The subject of chemistry is a very practical based subject meaning most of class time is spent working or completing experiments.\n\nBiology education is characterized by the study of structure, function, heredity, and evolution of all living organisms. Biology itself is the study of living organisms, through different fields including morphology, physiology, anatomy, behavior, origin, and distribution. \n\nDepending on the country and education level, there are many approaches to teaching biology. In the United States, there is a growing emphasis on the ability to investigate and analyze biology related questions over an extended period of time.\n\nWhile the public image of science education may be one of simply learning facts by rote, science education in recent history also generally concentrates on the teaching of science concepts and addressing misconceptions that learners may hold regarding science concepts or other content. Science education has been strongly influenced by constructivist thinking. Constructivism in science education has been informed by an extensive research programme into student thinking and learning in science, and in particular exploring how teachers can facilitate conceptual change towards canonical scientific thinking. Constructivism emphasises the active role of the learner, and the significance of current knowledge and understanding in mediating learning, and the importance of teaching that provides an optimal level of guidance to learners.\n\nAlong with John Dewey, Jerome Bruner, and many others, Arthur Koestler offers a critique of contemporary science education and proposes its replacement with the guided-discovery approach: To derive pleasure from the art of discovery, as from the other arts, the consumer—in this case the student—must be made to re-live, to some extent, the creative process. In other words, he must be induced, with proper aid and guidance, to make some of the fundamental discoveries of science by himself, to experience in his own mind some of those flashes of insight which have lightened its path. . . . The traditional method of confronting the student not with the problem but with the finished solution, means depriving him of all excitement, [shutting] off the creative impulse, [reducing] the adventure of mankind to a dusty heap of theorems.Specific hands-on illustrations of this approach are available.\n\nThe practice of science education has been increasingly informed by research into science teaching and learning. Research in science education relies on a wide variety of methodologies, borrowed from many branches of science and engineering such as computer science, cognitive science, cognitive psychology and anthropology. Science education research aims to define or characterize what constitutes learning in science and how it is brought about.\n\nJohn D. Bransford, et al., summarized massive research into student thinking as having three key findings:\n\n\nEducational technologies are being refined to meet the specific needs of science teachers. One research study examining how cellphones are being used in post-secondary science teaching settings showed that mobile technologies can increase student engagement and motivation in the science classroom.\n\nAccording to a bibliography on constructivist-oriented research on teaching and learning science in 2005, about 64 percent of studies documented are carried out in the domain of physics, 21 percent in the domain of biology, and 15 percent in chemistry. The major reason for this dominance of physics in the research on teaching and learning appears to be that understanding physics includes difficulties due to the particular nature of physics.\nResearch on students' conceptions has shown that most pre-instructional (everyday) ideas that students bring to physics instruction are in stark contrast to the physics concepts and principles to be achieved – from kindergarten to the tertiary level. Quite often students' ideas are incompatible with physics views. This also holds true for students’ more general patterns of thinking and reasoning.\n\nAs in England and Wales, science education in Australia is compulsory up until year 11, where students can choose to study one or more of the branches mentioned above. If they wish to no longer study science, they can choose none of the branches. The science stream is one course up until year 11, meaning students learn in all of the branches giving them a broad idea of what science is all about. The National Curriculum Board of Australia (2009) stated that \"The science curriculum will be organised around three interrelated strands: science understanding; science inquiry skills; and science as a human endeavour.\" These strands give teachers and educators the framework of how they should be instructing their students.\n\nA major problem that has befallen science education in Australia over the last decade is a falling interest in science. Fewer year 10 students are choosing to study science for year 11, which is problematic as these are the years where students form attitudes to pursue science careers. This issue is not unique in Australia, but is happening in countries all over the world.\n\nEducational quality in China suffers because a typical classroom contains 50 to 70 students. With over 200 million students, China has the largest educational system in the world. However, only 20% percent of students complete the rigorous ten-year program of formal schooling.\n\nAs in many other countries, the science curriculum includes sequenced courses in physics, chemistry, and biology. Science education is given high priority and is driven by textbooks composed by committees of scientists and teachers. Science education in China places great emphasis on memorization, and gives far less attention to problem solving, application of principles to novel situations, interpretations, and predictions.\n\nIn English and Welsh schools, science is a compulsory subject in the National Curriculum. All pupils from 5 to 16 years of age must study science. It is generally taught as a single subject science until sixth form, then splits into subject-specific A levels (physics, chemistry and biology). However, the government has since expressed its desire that those pupils who achieve well at the age of 14 should be offered the opportunity to study the three separate sciences from September 2008. In Scotland the subjects split into chemistry, physics and biology at the age of 13–15 for National 4/5s in these subjects, and there is also a combined science standard grade qualification which students can sit, provided their school offers it.\n\nIn September 2006 a new science program of study known as 21st Century Science was introduced as a GCSE option in UK schools, designed to \"give all 14 to 16 year old's a worthwhile and inspiring experience of science\". In November 2013, Ofsted's survey of science in schools revealed that practical science teaching was not considered important enough. At the majority of English schools, students have the opportunity to study a separate science program as part of their GCSEs, which results in them taking 6 papers at the end of Year 11; this usually fills one of their option 'blocks' and requires more science lessons than those who choose not to partake in separate science or are not invited. Other students who choose not to follow the compulsory additional science course, which results in them taking 4 papers resulting in 2 GCSEs, opposed to the 3 GCSEs given by taking separate science.\n\nIn many U.S. states, K-12 educators must adhere to rigid standards or frameworks of what content is to be taught to which age groups. This often leads teachers to rush to \"cover\" the material, without truly \"teaching\" it. In addition, the \"process\" of science, including such elements as the scientific method and critical thinking, is often overlooked. This emphasis can produce students who pass standardized tests without having developed complex problem solving skills. Although at the college level American science education tends to be less regulated, it is actually more rigorous, with teachers and professors fitting more content into the same time period.\n\nIn 1996, the U.S. National Academy of Sciences of the U.S. National Academies produced the National Science Education Standards, which is available online for free in multiple forms. Its focus on inquiry-based science, based on the theory of constructivism rather than on direct instruction of facts and methods, remains controversial. Some research suggests that it is more effective as a model for teaching science. \"The Standards call for more than 'science as process,' in which students learn such skills as observing, inferring, and experimenting. Inquiry is central to science learning. When engaging in inquiry, students describe objects and events, ask questions, construct explanations, test those explanations against current scientific knowledge, and communicate their ideas to others. They identify their assumptions, use critical and logical thinking, and consider alternative explanations. In this way, students actively develop their understanding of science by combining scientific knowledge with reasoning and thinking skills.\"\n\nConcern about science education and science standards has often been driven by worries that American students lag behind their peers in international rankings. One notable example was the wave of education reforms implemented after the Soviet Union launched its Sputnik satellite in 1957. The first and most prominent of these reforms was led by the Physical Science Study Committee at MIT. In recent years, business leaders such as Microsoft Chairman Bill Gates have called for more emphasis on science education, saying the United States risks losing its economic edge. To this end, Tapping America's Potential is an organization aimed at getting more students to graduate with science, technology, engineering and mathematics degrees. Public opinion surveys, however, indicate most U.S. parents are complacent about science education and that their level of concern has actually declined in recent years.\n\nProf Sreyashi Jhumki Basu published extensively on the need for equity in Science Education in the United States.\n\nFurthermore, in the recent National Curriculum Survey conducted by ACT, researchers uncovered a possible disconnect among science educators. \"Both middle school/junior high school teachers and post secondary science instructors rate(d) process/inquiry skills as more important than advanced science content topics; high school teachers rate them in exactly the opposite order.\" Perhaps more communication among educators at the different grade levels in necessary to ensure common goals for students.\n\nAccording to a report from the National Academy of Sciences, the fields of science, technology, and education hold a paramount place in the modern world, but there are not enough workers in the United States entering the science, technology, engineering, and math (STEM) professions. In 2012 the National Academy of Sciences Committee on a Conceptual Framework for New K-12 Science Education Standards developed a guiding framework to standardize K-12 science education with the goal of organizing science education systematically across the K-12 years. Titled \"A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas\", the publication promotes standardizing K-12 science education in the United States. It emphasizes science educators to focus on a \"limited number of disciplinary core ideas and crosscutting concepts, be designed so that students continually build on and revise their knowledge and abilities over multiple years, and support the integration of such knowledge and abilities with the practices needed to engage in scientific inquiry and engineering design.\" \n\nThe report says that in the 21st century Americans need science education in order to engage in and \"systematically investigate issues related to their personal and community priorities,\" as well as to reason scientifically and know how to apply science knowledge. The committee that designed this new framework sees this imperative as a matter of educational equity to the diverse set of schoolchildren. Getting more diverse students into STEM education is a matter of social justice as seen by the committee.\n\nIn 2013 a new standards for science education were released that update the national standards released in 1996. Developed by 26 state governments and national organizations of scientists and science teachers, the guidelines, called the Next Generation Science Standards, are intended to \"combat widespread scientific ignorance, to standardize teaching among states, and to raise the number of high school graduates who choose scientific and technical majors in college...\" Included are guidelines for teaching students about topics such as climate change and evolution. An emphasis is teaching the scientific process so that students have a better understanding of the methods of science and can critically evaluate scientific evidence. Organizations that contributed to developing the standards include the National Science Teachers Association, the American Association for the Advancement of Science, the National Research Council, and Achieve, a nonprofit organization that was also involved in developing math and English standards.\n\nInformal science education is the science teaching and learning that occurs outside of the formal school curriculum in places such as museums, the media, and community-based programs. The National Science Teachers Association has created a position statement on Informal Science Education to define and encourage science learning in many contexts and throughout the lifespan. Research in informal science education is funded in the United States by the National Science Foundation. The Center for Advancement of Informal Science Education (CAISE) provides resources for the informal science education community.\n\nExamples of informal science education include science centers, science museums, and new digital learning environments (\"e.g.\" Global Challenge Award), many of which are members of the Association of Science and Technology Centers (ASTC). The Exploratorium in San Francisco and The Franklin Institute in Philadelphia are the oldest of this type of museum in the United States. Media include TV programs such as \"NOVA\", \"Newton's Apple\", \"Bill Nye the Science Guy\",\"Beakman's World\", \"The Magic School Bus\", and \"Dragonfly TV\". Early examples of science education on American television included programs by Daniel Q. Posin, such as \"Dr. Posin's Universe\", \"The Universe Around Us\", \"On the Shoulders of Giants\", and \"Out of This World\". Examples of community-based programs are 4-H Youth Development programs, Hands On Science Outreach, NASA and After school Programs and Girls at the Center. Home education is encouraged through educational products such as the former (1940-1989) Things of Science subscription service.\n\nIn 2010, the National Academies released \"Surrounded by Science: Learning Science in Informal Environments\", based on the National Research Council study, \"Learning Science in Informal Environments: People, Places, and Pursuits\". \"Surrounded by Science\" is a resource book that shows how current research on learning science across informal science settings can guide the thinking, the work, and the discussions among informal science practitioners. This book makes valuable research accessible to those working in informal science: educators, museum professionals, university faculty, youth leaders, media specialists, publishers, broadcast journalists, and many others.\n\n\n\n"}
{"id": "47119924", "url": "https://en.wikipedia.org/wiki?curid=47119924", "title": "Sleights of Mind", "text": "Sleights of Mind\n\nSleights of Mind: What the Neuroscience of Magic Reveals about Our Everyday Deceptions is a 2010 popular science book, written by neuroscientists Stephen Macknik and Susana Martinez-Conde, with science writer Sandra Blakeslee. Working alongside some of the world’s greatest magicians, Macknik and Martinez-Conde studied how conjuring techniques trick the brain. \"Sleights of Mind\" considers the greater implications of magic and misdirection for clinical conditions such as autism, and for everyday life situations, including choice and trust in personal and business relationships.\n\n\"Sleights of Mind\" is the result of Macknik and Martinez-Conde’s yearlong, world-wide exploration of magic and how its principles apply to human behavior. \"Sleights of Mind\" introduces Neuromagic as a new area of scientific endeavor, a discipline aimed to uncover the interaction between brain science and the art of magic. Macknik and Martinez-Conde propose that understanding how the mind perceives magic and illusion will provide a greater understanding of perception and cognition at large.\n\nMacknik and Martinez-Conde say that magic tricks fool us because humans have hardwired processes of attention and awareness that are hackable. Good magicians use our inherent mental and neural limitations against us by leading us to perceive and feel what we are neurologically inclined to. Working with renowned magicians like Apollo Robbins, Teller, Mac King, and James Randi, Macknik and Martinez-Conde research the ways in which the perceptual and cognitive elements of magic relate to more than simple deceits. The authors reveal the neural underpinnings of the magical methods that explain how our brains perceive magic.\n\nThrough their exploration, Macknik and Martinez-Conde uncover how our brains work in everyday situations. They describe how if you have ever bought an expensive item you had sworn you would never buy, the salesperson was creating the “illusion of choice,” a core technique of magic. They also relate the use of magic to Bernie Madoff’s “illusion of trust”. Through these examples, Macknik’s and Martinez-Conde’s Sleights of Mind illuminates the reasons for studying magic, and its implications for research on, and renewed understanding of, perceptual and cognitive processes.\n\nEvening Standard Best Book of the Year List\n\nPrisma Prize\n\nThe book has been reviewed by authors of other science-related books, including Neil deGrasse Tyson, Dan Ariely and JJ Abrams. It has also been reviewed in the New York Times.\n"}
{"id": "48564127", "url": "https://en.wikipedia.org/wiki?curid=48564127", "title": "Sophie Charlotte Ducker", "text": "Sophie Charlotte Ducker\n\nSophie Charlotte Ducker (9 April 1909 – 20 May 2004) was a German-born Australian botanist. She was awarded the Mueller Medal in 1996.\n\nShe was born Sophie Charlotte von Klemperer in Dresden. She studied at the Cheltenham Ladies' College in England. She began the study of botany at the University of Geneva and the University of Stuttgart. She stopped her studies in 1931 when she married Johann Friedrich Ducker. The family left Germany at the outbreak of hostilities and moved to Tehran, Persia. In 1941, they were forced to move again and settled in Australia. \n\nDucker worked as a research assistant for Dr. Ethel Irene McLennan of the botany school at the University of Melbourne. She completed a BSc at there in 1952. In 1957, she became a botany lecturer at the university and, in 1961, a senior lecturer. She specialized in marine botany, especially algae. She retired in 1974 but continued to conduct research, present papers and lecture. After her retirement, she collaborated with Professor Bruce Knox at the University of Melbourne on pollination, particularly that of seagrasses. Ducker received a DSc from the University of Melbourne in 1978. She also published biographies of early Australian botanists. \n\nDucker was a founding member of the Australasian Society for Phycology and Aquatic Botany.\n\nIn 1993, Ducker was awarded an Honorary Doctor of Laws from the University of Melbourne.\n\nIn 1996 she received the Australian and New Zealand Association for the Advancement of Science's Mueller Medal.\n\nDucker died in Melbourne at the age of 95.\n\nDucker, Sophie. \"The contented botanist: letters of W.H. Harvey about Australia and the Pacific.\" Carlton: Melbourne University Press, 1984. \n\nDucker, Sophie C. 'Harvey, William Henry (1811–1866)', \"Australian Dictionary of Biography\", National Centre of Biography, Australian National University, http://adb.anu.edu.au/biography/harvey-william-henry-3732/text5867, published first in hardcopy 1972, accessed online 25 August 2017\n\n"}
{"id": "1217870", "url": "https://en.wikipedia.org/wiki?curid=1217870", "title": "Spectrohelioscope", "text": "Spectrohelioscope\n\nA spectrohelioscope is a type of solar telescope designed by George Ellery Hale in 1924 to allow the Sun to be viewed in a selected wavelength of light. The name comes from three Latin-based words: Spectro, referring to the optical spectrum, Helio, referring to the Sun and Scope, as in telescope.\n\nThe basic spectrohelioscope is a complex machine that uses a spectroscope to scan the surface of the Sun. The image from the objective lens is focused on a narrow slit revealing only a thin portion of the Sun's surface. The light is then passed through a prism or diffraction grating to spread the light into a spectrum. The spectrum is then focused on another slit that allows only a narrow part of the spectrum (the desired wavelength of light for viewing) to pass. The light is finally focused on an eyepiece so the surface of the Sun can be seen. The view, however, would be only a narrow strip of the Sun's surface. The slits are moved in unison to scan across the whole surface of the Sun giving a full image. Independently nodding mirrors can be used instead of moving slits to produce the same scan: the first mirror selects a slice of the Sun, the second selects the desired wavelength.\n\nThe Spectroheliograph is a similar device, but images the Sun at a particular wavelength photographically and is still in use in professional observatories.\n\n\n"}
{"id": "31820187", "url": "https://en.wikipedia.org/wiki?curid=31820187", "title": "The Information: A History, a Theory, a Flood", "text": "The Information: A History, a Theory, a Flood\n\nThe Information: A History, a Theory, a Flood is a book by science history writer James Gleick published in March 2011 which covers the genesis of our current information age. It was on the New York Times best-seller list for three weeks following its debut. \n\n\"The Information\" has also been published in ebook formats by Fourth Estate and Random House, and as an audiobook by Random House Audio. \n\nGleick begins with the tale of colonial European explorers and their fascination with African talking drums and their observed use to send complex and widely understood messages back and forth between villages far apart, and over even longer distances by relay. Gleick transitions from the information implications of such drum signaling to the impact of the arrival of long distance telegraph and then telephone communication to the commercial and social prospects of the industrial age west. Research to improve these technologies ultimately led to our understanding the essentially digital nature of information, quantized down to the unit of the bit (or qubit).\n\nStarting with the development of symbolic written language (and the eventual perceived need for a dictionary), Gleick examines the history of intellectual insights central to information theory, detailing the key figures responsible such as Claude Shannon, Charles Babbage, Ada Byron, Samuel Morse, Alan Turing, Stephen Hawking, Richard Dawkins and John Archibald Wheeler. The author also delves into how digital information is now being understood in relation to physics and genetics. Following the circulation of Claude Shannon's \"A Mathematical Theory of Communication\" and Norbert Wiener's \"\" many disciplines attempted to jump on the information theory bandwagon to varying success. Information theory concepts of data compression and error correction became especially important to the computer and electronics industries.\n\nGleick finally discusses Wikipedia as an emerging internet based Library of Babel, investigating the implications of its expansive user generated content, including the ongoing struggle between inclusionists, deletionists, and vandals. Gleick uses the Jimmy Wales created article for the Cape Town butchery restaurant Mzoli's as a case study of this struggle. The flood of information humanity is now exposed to presents new challenges Gleick says, as we retain more of our information now than at any previous point in human history, it takes much more effort to delete or remove unwanted information than to accumulate it. This is the ultimate entropy cost of generating additional information and the answer to slay Maxwell's Demon.\n\nIn addition to winning major awards for science writing and history, \"The Information\" received mostly positive reviews. In \"The New York Times\", Janet Maslin said it is \"so ambitious, illuminating and sexily theoretical that it will amount to aspirational reading for many of those who have the mettle to tackle it.\" Other admirers were Nicholas Carr for \"The Daily Beast\" and physicist Freeman Dyson for \"The New York Review of Books\". Science fiction author Cory Doctorow in his BoingBoing review called Gleick \"one of the great science writers of all time\", \"Not a biographer of scientists... but a biographer of the \"idea\" itself.\" Tim Wu for \"Slate\" praised \"a mind-bending explanation of theory\" but wished Gleick had examined the economic importance of information more deeply. Ian Pindar writing for \"The Guardian\" complained that \"The Information\" does not fully address the relationship between social control of information (censorship, propaganda) and access to political power.\n\n\n\n"}
{"id": "25063946", "url": "https://en.wikipedia.org/wiki?curid=25063946", "title": "Wagstaffe-Le Fort avulsion fracture", "text": "Wagstaffe-Le Fort avulsion fracture\n\nLe Fort's fracture of the ankle is a vertical fracture of the antero-medial part of the distal fibula with avulsion of the anterior tibiofibular ligament, opposite to a Tillaux-Chaput avulsion fracture\n\nThe injury was described by Léon Clément Le Fort in 1886.\n"}
{"id": "3464419", "url": "https://en.wikipedia.org/wiki?curid=3464419", "title": "ZINDO", "text": "ZINDO\n\nZINDO is a semi-empirical quantum chemistry method used in computational chemistry. It is a development of the INDO method. It stands for Zerner's Intermediate Neglect of Differential Overlap, as it was developed by Michael Zerner and his coworkers in the 1970s. Unlike INDO, which was really restricted to organic molecules and those containing the atoms B to F, ZINDO covers a wide range of the periodic table, even including the rare-earth elements. There are two distinct versions of the method:\n\nThe original BIGSPEC program from the Zerner group is not widely available, but the method is implemented in HyperChem, ORCA, in part, in Gaussian, and in SCIGRESS.\n\nTo obtain good results, it is frequently necessary to fit the parameters to a given molecule, thereby making it ideal only in semi-empirical calculations.\n"}
