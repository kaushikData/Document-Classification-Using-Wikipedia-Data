{"id": "28846952", "url": "https://en.wikipedia.org/wiki?curid=28846952", "title": "ADL astronomical society", "text": "ADL astronomical society\n\nADL, Astronomsko društvo Labod (Slovenian for Cygnus Astronomical Society) is the most active Slovenian astronomical society. It organises astronomy camps, lectures for youths and similar events. \n"}
{"id": "44973208", "url": "https://en.wikipedia.org/wiki?curid=44973208", "title": "Alfred Nicholson Leeds", "text": "Alfred Nicholson Leeds\n\nAlfred Nicholson Leeds (9 March 184725 August 1917) was an English amateur palaeontologist.\n\nHe was born at Eyebury, Peterborough, the youngest of the eight children of Edward Thurlow Leeds (180251) and Eliza Mary Leeds (née Nicholson). He was educated at Warwick School. He had wanted to become a doctor, but circumstances meant that from 1868 he had to take on the management of Eyebury Farm (in The Fens, and historically attached to Peterborough Abbey) as a gentleman farmer.\n\nHis elder brother Charles, a student at Oxford University, had been encouraged by Professor John Phillips to persevere in collecting fossils from near his home. Alfred joined him in these searches, and between them they developed better methods of disinterring, and of scientifically recording, fossils in soft clay than had been used before. (They rewarded the workmen at the clay pits (which served a brickworks in Fletton, Peterborough) for not doing so themselves, but instead sending notice to Eyebury.)\n\nIn 1887, Charles emigrated to New Zealand; but Alfred continued to search for fossils, assisted by his wife and by their second son, Edward Thurlow Leeds (18771955, Keeper of the Ashmolean Museum 192845).\n\nHe amassed one of the largest collections of fossil vertebrates in the world. In 1889, his portrait was painted by the 17-year-old William Nicholson. From 1890 onwards, he began to present his most important specimens of Jurassic fossils from the Oxford Clay near Peterborough to the British Museum. He was a Fellow of the Geological Society; in 1893, he was awarded part of its Lyell Fund.\n\nOther museums in the UK and elsewhere hold items from his collection; including the National Museum of Ireland.\n\nAn extinct genus of fish, \"Leedsichthys\", and several extinct species have been named in his honour.\n\n\n"}
{"id": "1158007", "url": "https://en.wikipedia.org/wiki?curid=1158007", "title": "Amundsen Glacier", "text": "Amundsen Glacier\n\nAmundsen Glacier () is a major Antarctic glacier, about 6 to 10 km (4 to 6 mi) wide and 128 km (80 mi) long, originating on the polar plateau where it drains the area to the south and west of Nilsen Plateau, and descending through the Queen Maud Mountains to enter the Ross Ice Shelf just west of the MacDonald Nunataks. The tributary Blackwall Glacier flows northwest along the northeast side of Hansen Spur to join Amundsen Glacier.\n\nIt was discovered by Rear Admiral Byrd on the South Pole flight in November 1929. The name was proposed for Roald Amundsen by Laurence Gould, leader of the Byrd AE geological party which sledged past the mouth of the glacier in December 1929.\n\n"}
{"id": "631671", "url": "https://en.wikipedia.org/wiki?curid=631671", "title": "Angiosperm Phylogeny Group", "text": "Angiosperm Phylogeny Group\n\nThe Angiosperm Phylogeny Group, or APG, refers to an informal international group of systematic botanists who collaborate to establish a consensus on the taxonomy of flowering plants (angiosperms) that reflects new knowledge about plant relationships discovered through phylogenetic studies.\n\n, four incremental versions of a classification system have resulted from this collaboration, published in 1998, 2003, 2009 and 2016. An important motivation for the group was what they considered deficiencies in prior angiosperm classifications since they were not based on monophyletic groups (i.e., groups that include all the descendants of a common ancestor).\n\nAPG publications are increasingly influential, with a number of major herbaria changing the arrangement of their collections to match the latest APG system.\n\nIn the past, classification systems were typically produced by an individual botanist or by a small group. The result was a large number of systems (see List of systems of plant taxonomy). Different systems and their updates were generally favoured in different countries. Examples are the Engler system in continental Europe, the Bentham & Hooker system in Britain (particularly influential because it was used by Kew), the Takhtajan system in the former Soviet Union and countries within its sphere of influence and the Cronquist system in the United States.\n\nBefore the availability of genetic evidence, the classification of angiosperms (also known as \"flowering plants\", \"Angiospermae\", \"Anthophyta\" or \"Magnoliophyta\") was based on their morphology (particularly of their flower) and biochemistry (the kinds of chemical compounds in the plant).\n\nAfter the 1980s, detailed genetic evidence analysed by phylogenetic methods became available and while confirming or clarifying some relationships in existing classification systems, it radically changed others. This genetic evidence created a rapid increase in knowledge that led to many proposed changes; stability was \"rudely shattered\". This posed problems for all users of classification systems (including encyclopaedists). The impetus came from a major molecular study published in 1993 based on 5000 flowering plants and a photosynthesis gene (rbcL). This produced a number of surprising results in terms of the relationships between groupings of plants, for instance the dicotyledons were not supported as a distinct group. At first there was a reluctance to develop a new system based entirely on a single gene. However, subsequent work continued to support these findings. These research studies involved an unprecedented collaboration between a very large number of scientists. Therefore, rather than naming all the individual contributors a decision was made to adopt the name Angiosperm Phylogeny Group classification, or APG for short. The first publication under this name was in 1998, and attracted considerable media attention. The intention was to provide a widely accepted and more stable point of reference for angiosperm classification.\n\n, three revisions have been published, in 2003 (APG II), in 2009 (APG III) and in 2016 (APG IV), each superseding the previous system. Thirteen researchers have been listed as authors to the three papers, and a further 43 as contributors (see Members of the APG below).\nA classification presents a view at a particular point in time, based on a particular state of research. Independent researchers, including members of the APG, continue to publish their own views on areas of angiosperm taxonomy. Classifications change, however inconvenient this is to users. However, the APG publications are increasingly regarded as an authoritative point of reference and the following are some examples of the influence of the APG system:\n\nThe principles of the APG's approach to classification were set out in the first paper of 1998, and have remained unchanged in subsequent revisions. Briefly, these are:\n\n\nFor a detailed discussion on phylogenetic nomenclature, see Cantino \"et al.\" (2007).)\n\nThe initial 1998 paper by the APG made angiosperms the first large group of organisms to be systematically re-classified primarily on the basis of genetic characteristics. The paper explained the authors' view that there is a need for a classification system for angiosperms at the level of families, orders and above, but that existing classifications were \"outdated\". The main reason why existing systems were rejected was because they were not phylogenetic, i.e. not based on strictly monophyletic groups (groups which consist of \"all\" descendants of a common ancestor). An ordinal classification of flowering plant families was proposed as a \"reference tool of broad utility\". The broad approach adopted to defining the limits of orders resulted in the recognition of 40 orders, compared to, for example, 232 in Takhtajan's 1997 classification.\n\nIn 1998 only a handful of families had been adequately studied, but the primary aim was to obtain a consensus on the naming of higher orders. Such a consensus proved relatively easy to achieve but the resultant tree was highly unresolved. That is, while the relationship of orders was established, their composition was not.\n\nOther features of the proposed classification included:\n\nA major outcome of the classification was the disappearance of the traditional division of the flowering plants into two groups, monocots and dicots. The monocots were recognized as a clade, but the dicots were not, with a number of former dicots being placed in separate groups basal to both monocots and the remaining dicots, the eudicots or 'true dicots'. The overall scheme was relatively simple. This consisted of a grade consisting of isolated taxa (referred to as ANITA), followed by the major angiosperm radiation, clades of monocots, magnolids and eudicots. The last being a large clade with smaller subclades and two main groupings, rosids and asterids, each in turn having two major subclades.\n\nAs the overall relationship between groups of flowering plants became clearer, the focus shifted to the family level, in particular those families generally accepted as problematic. Again, consensus was achieved relatively easily resulting in an updated classification at the family level. The second paper published by the APG in 2003 presented an update to the original classification of 1998. The authors stated that changes were proposed only when there was \"substantial new evidence\" which supported them.\n\nThe classification continued the tradition of seeking broad circumscriptions of taxa, for example trying to place small families containing only one genus in a larger group. The authors stated that they have generally accepted the views of specialists, although noting that specialists \"nearly always favour splitting of groups\" regarded as too varied in their morphology.\n\nAPG II continued and indeed extends the use of alternative 'bracketed' taxa allowing the choice of either a large family or a number of smaller ones. For example, the large Asparagaceae family includes 7 'bracketed' families which can either be considered as part of the Asparagaceae or as separate families.\nSome of the main changes in APG II were:\n\nIn 2007, a paper was published giving a linear ordering of the families in APG II, suitable for ordering herbarium specimens, for example.\n\nThe third paper from the APG updates the system described in the 2003 paper. The broad outline of the system remains unchanged, but the number of previously unplaced families and genera is significantly reduced. This requires the recognition of both new orders and new families compared to the previous classification. The number of orders goes up from 45 to 59; only 10 families are not placed in an order and only two of these (Apodanthaceae and Cynomoriaceae) are left entirely outside the classification. The authors say that they have tried to leave long-recognized families unchanged, while merging families with few genera. They \"hope the classification [...] will not need much further change.\"\n\nA major change is that the paper discontinues the use of 'bracketed' families in favour of larger, more inclusive families. As a result, the APG III system contains only 415 families, rather than the 457 of APG II. For example, the agave family (Agavaceae) and the hyacinth family (Hyacinthaceae) are no longer regarded as distinct from the broader asparagus family (Asparagaceae). The authors say that alternative circumscriptions, as in APG I and II, are likely to cause confusion and that major herbaria which are re-arranging their collections in accordance with the APG approach have all agreed to use the more inclusive families. This approach is being increasingly used in collections in herbaria and botanic gardens.\n\nIn the same volume of the journal, two related papers were published. One gives a linear ordering of the families in APG III; as with the linear ordering published for APG II, this is intended for ordering herbarium specimens, for example. The other paper gives, for the first time, a classification of the families in APG III which uses formal taxonomic ranks; previously only informal clade names were used above the ordinal level.\n\nIn the development of a fourth version there was some controversy over the methodology, and the development of a consensus proved more difficult than in previous iterations. In particular Peter Stevens questioned the validity of discussions regarding family delimitation in the absence of changes of phylogenetic relationships.\n\nFurther progress was made by the use of large banks of genes, including those of plastid, mitochondrial and nuclear ribosomal origin, such as that of Douglas Soltis and colleagues (2011). The fourth version was finally published in 2016. It arose from an international conference hosted at the Royal Botanical Gardens in September 2015 and also an online survey of botanists and other users. The broad outline of the system remains unchanged but several new orders are included (Boraginales, Dilleniales, Icacinales, Metteniusiales and Vahliales), some new families are recognised (Kewaceae, Macarthuriaceae, Maundiaceae, Mazaceae, Microteaceae, Nyssaceae, Peraceae, Petenaeaceae and Petiveriaceae) and some previously recognised families are lumped (Aristolochiaceae now includes Lactoridaceae and Hydnoraceae; Restionaceae now re-includes Anarthriaceae and Centrolepidaceae; and Buxaceae now includes Haptanthaceae). Due to nomenclatural issues, the family name Asphodelaceae is used instead of Xanthorrhoeaceae, and Francoaceae is used instead of Melianthaceae (and now also includes Vivianiaceae). This brings the total number of orders and families recognized in the APG system to 64 and 416, respectively. Two additional informal major clades, superrosids and superasterids, that each comprise the additional orders that are included in the larger clades dominated by the rosids and asterids are also included. APG IV also uses the linear approach (LAPG) as advocated by Haston \"et al.\" (2009) In a supplemental file Byng \"et al.\" provide an alphabetical list of families by orders.\n\nPeter Stevens, one of the authors of all four of the APG papers, maintains a web site, the Angiosperm Phylogeny Website (APWeb), hosted by the Missouri Botanical Garden, which has been regularly updated since 2001, and is a useful source for the latest research in angiosperm phylogeny which follows the APG approach. Other sources include the Angiosperm Phylogeny Poster and The Flowering Plants Handbook.\n\na = listed as an author; c = listed as a contributor\n\nc = listed as a contributor\n\n\n"}
{"id": "56795983", "url": "https://en.wikipedia.org/wiki?curid=56795983", "title": "Bethany Brookshire", "text": "Bethany Brookshire\n\nBethany Brookshire is an American science journalist. She writes for Science News for Students.\n\nBrookshire completed a BA (Philosophy) and BS (Biology) at the College of William & Mary in 2004. She earned a PhD in Physiology and Pharmacology at Wake Forest School of Medicine in 2010, where she worked on ritalin and the serotonin switch with Sara Jones. She began blogging about science in 2008, during her graduate studies. She wrote under the pseudonym \"SciCurious for\" Discover (magazine) and The Guardian. She worked as a postdoctoral researcher at the University of Pennsylvania, where she used social media to discuss the brain and psychiatric illness. Here she worked with Irwin Lucki identifying the mechanisms of antidepressants in action.\n\nIn 2013, Brookshire began blogging in her own name. Today she writes \"Eureka!Lab\" for Science News for Students, and for \"SciCurious\" for Science News. She presents the podcast \"Science for the People\", as well as appearing on other science related shows. She appeared on the \"Story Collider\" in 2015, a show which tells the stories of scientists, where Brookshire discussed her quest for a mentor. In May 2016 she published \"Science Blogging: The Essential Guide\" with Christie Wilcox and Jason Goldman.\n\nShe has written for \"Slate\", \"Scientific American\", and The Open Notebook.\n\n"}
{"id": "4636561", "url": "https://en.wikipedia.org/wiki?curid=4636561", "title": "Charge-transfer insulators", "text": "Charge-transfer insulators\n\nCharge-transfer insulators are a class of materials predicted to be conductors following conventional band theory, but which are in fact insulators due to a charge-transfer process. Unlike Mott insulators, where the insulating properties arise from electrons hopping between unit cells, the electrons in charge-transfer insulators move between atoms within the unit cell. In the Mott-Hubbard case, it's easier for electrons to transfer between two adjacent metal sites (on-site Coulomb interaction U); in the charge-transfer case, it's easier from the anion to the metal (charge-transfer energy Δ). U is determined by repulsive/exchange effects between the cation valence electrons. Δ is tuned by the chemistry between the cation and anion.\n"}
{"id": "41608446", "url": "https://en.wikipedia.org/wiki?curid=41608446", "title": "Chemical Society Located in Taipei", "text": "Chemical Society Located in Taipei\n\nChemical Society Located in Taipei (CSLT; ; literally 'Chinese Chemical Society') is a Taiwanese scholarly organization dedicated to chemistry. The organization traces its roots to the establishment of Chinese Chemical Society in Nanjing in 1932 and was reestablished in Taiwan in 1950. For political reasons, the organization's English name was changed to \"Chemical Society Located in Taipei\" although it still retains the name \"Chinese Chemical Society\" (中國化學會) in Chinese.\n\nCSLT and Wiley publish a monthly periodical - Journal of the Chinese Chemical Society.\n\n"}
{"id": "40333821", "url": "https://en.wikipedia.org/wiki?curid=40333821", "title": "Chief research officer", "text": "Chief research officer\n\nThe chief research officer (CRO), research officer, or research director, is a job title commonly given to the most senior executive in an enterprise responsible for the research that supports enterprise goals. Generally, the CRO reports to the chief executive officer, chief operating officer, or chief financial officer. In educational organizations, they report to the Chancellor or President.\n"}
{"id": "514259", "url": "https://en.wikipedia.org/wiki?curid=514259", "title": "Citizen Lab", "text": "Citizen Lab\n\nThe Citizen Lab is an interdisciplinary laboratory based at the Munk School of Global Affairs at the University of Toronto, Canada. Founded and directed by Professor Ronald Deibert, the Citizen Lab studies information controls—such as network surveillance and content filtering—that impact the openness and security of the Internet and that pose threats to human rights. The Citizen Lab collaborates with research centres, organizations, and individuals around the world, and uses a \"mixed methods\" approach, which combines computer-generated interrogation, data mining and analysis with intensive field research, qualitative social science, and legal and policy analysis methods.\n\nThe Citizen Lab was a founding partner of the OpenNet Initiative (2002-2013) and the Information Warfare Monitor (2002-2012) projects. The organization also developed the original design of the Psiphon censorship circumvention software, which was spun out of the Lab into a private Canadian corporation (Psiphon Inc.) in 2008.\n\nThe Citizen Lab’s research outputs have made global news headlines around the world. For example, front page exclusives in \"The New York Times\", \"The Washington Post\", and \"The Globe and Mail\". In \"Tracking Ghostnet\" (2009) researchers uncovered a suspected cyber espionage network of over 1,295 infected hosts in 103 countries, a high percentage of which were high-value targets, including ministries of foreign affairs, embassies, international organizations, news media, and NGOs. This seminal study was one of the first public reports to reveal a cyber espionage network that targeted civil society and government systems around the world. In \"Shadows in the Cloud\" (2010), researchers documented a complex ecosystem of cyber espionage that systematically compromised government, business, academic, and other computer network systems in India, the offices of the Dalai Lama, the United Nations, and several other countries.\n\nThe Citizen Lab has won a number of awards for its work. It is the first Canadian institution to win the MacArthur Foundation’s MacArthur Award for Creative and Effective Institutions (2014) and the only Canadian institution to receive a \"New Digital Age\" Grant (2014) from Google Executive Chairman Eric Schmidt. Past awards include the Canadian Library Association's Advancement of Intellectual Freedom in Canada Award (2013), the Canadian Committee for World Press Freedom’s Press Freedom Award (2011), and the Canadian Journalists for Free Expression’s Vox Libera Award (2010).\n\nIn July 2014, Citizen Lab was profiled in the Ars Technica article,\" Inside Citizen Lab, the \"Hacker Hothouse\" protecting you from Big Brother.\"\n\nThe Citizen Lab is independent of government or corporate interests. Financial support for the Citizen Lab has come from the Ford Foundation, the Open Society Institute, the Social Sciences and Humanities Research Council of Canada, the International Development Research Centre (IDRC), the Canada Centre for Global Security Studies at the University of Toronto’s Munk School of Global Affairs, the John D. and Catherine T. MacArthur Foundation, the Donner Canadian Foundation, the Open Technology Fund, and The Walter and Duncan Gordon Foundation. The Citizen Lab has received donations of software and support from Palantir Technologies, VirusTotal, and Oculus Info Inc.\n\nThe Citizen Lab’s Targeted Threats research stream seeks to gain a better understanding of the technical and social nature of digital attacks against civil society groups and the political context that may motivate them. The Citizen Lab conducts ongoing comparative analysis of a growing spectrum of online threats, including Internet filtering, denial-of-service attacks, and targeted malware. Targeted Threats reports have covered a number espionage campaigns and information operations against the Tibetan community and diaspora, phishing attempts made against journalists, human rights defenders, political figures, international investigators and anti-corruption advocates in Mexico, and a prominent human rights advocate who was the focus of government surveillance in the United Arab Emirates. Citizen Lab researchers and collaborators like the Electronic Frontier Foundation have also revealed several different malware campaigns targeting Syrian activists and opposition groups in the context of the Syrian Civil War. Many of these findings were translated into Arabic and disseminated along with recommendations for detecting and removing malware.\n\nThe Citizen Lab’s research on threats against civil society organizations has been featured on the front page of \"BusinessWeek\", and covered in Al Jazeera, \"Forbes\", \"Wired\", among other international media outlets.\n\nThe group reports that their work analyzing spyware used to target opposition figures in South America has triggered death threats.\nIn September 2015 members of the group received a pop-up that said:\n\nThe OpenNet Initiative has tested for Internet filtering in 74 countries and found that 42 of them—including both authoritarian and democratic regimes—implement some level of filtering.\n\nThe Citizen Lab is continuing this research area through the Internet Censorship Lab (ICLab), a project to develop new systems and methods for measuring Internet censorship. It is a collaborative effort between The Citizen Lab, Professor Phillipa Gill’s group at Stony Brook University's Department of Computer Science, and Professor Nick Feamster’s Network Operations and Internet Security Group at Princeton University.\n\nThe Citizen Lab studies censorship and surveillance implemented in popular applications including social networks, instant messaging, and search engines.\n\nPrevious work includes investigations of censorship practices of search engines provided by Google, Microsoft, and Yahoo! for the Chinese market along with the domestic Chinese search engine Baidu. In 2008, Nart Villeneuve found that TOM-Skype (the Chinese version of Skype at the time) had collected and stored millions of chat records on a publicly accessible server based in China. In 2013, Citizen Lab researchers collaborated with Professor Jedidiah Crandall and Ph.D. student Jeffrey Knockel at the University of New Mexico to reverse engineering of TOM-Skype and Sina UC, another instant messaging application used in China. The team was able to obtain the URLs and encryption keys for various versions of these two programs and downloaded the keyword blacklists daily. This work analyzed over one year and a half of data from tracking the keyword lists, examined the social and political contexts behind the content of these lists, and analyzed those times when the list had been updated, including correlations with current events.\n\nCurrent research focuses on monitoring information controls on the popular Chinese microblogging service Sina Weibo, Chinese online encyclopedias, and mobile messaging applications popular in Asia. The Asia Chats project utilizes technical investigation of censorship and surveillance, assessment on the use and storage of user data, and comparison of the terms of service and privacy policies of the applications. The first report released from this project examined regional keyword filtering mechanisms that LINE applies to its Chinese users.\n\nAnalysis of a popular cellphone app called \"Smart Sheriff\", by Citizen Lab and the German group Cure53, asserted the app represented a security hole that betrayed the privacy of the children it was meant to protect and that of their parents.\nSouth Korean law required all cellphones sold to those under 18 to contain software designed to protect children, and Smart Sheriff was the most popular government approved app—with 380,000 subscribers. The Citizen Lab/Cure53 report described Smart Sheriff's security holes as \"catastrophic\".\n\nThe Citizen Lab conducts groundbreaking research on the global proliferation of targeted surveillance software and toolkits, including FinFisher, Hacking Team and NSO Group.\n\nFinFisher is a suite of remote intrusion and surveillance software developed by Munich-based Gamma International GmbH and marketed and sold exclusively to law enforcement and intelligence agencies by the UK-based Gamma Group. In 2012, Morgan Marquis-Boire and Bill Marczak provided the first public identification of FinFisher's software. The Citizen Lab and collaborators have done extensive investigations into FinFisher, including revealing its use against Bahraini activists, analyzing variants of the FinFisher suite that target mobile phone operating systems, uncovering targeted spying campaigns against political dissidents in Malaysia and Ethiopia, and documenting FinFisher command and control servers in 36 countries. Citizen Lab's FinFisher research has informed and inspired responses from civil society organizations in Pakistan, Mexico, and the United Kingdom. In Mexico, for example, local activists, and politicians collaborated to demand an investigation into the state’s acquisition of surveillance technologies. In the UK, it led to a crackdown on the sale of the software over worries of misuse by repressive regimes.\n\nHacking Team is a Milan, Italy-based company that provides intrusion and surveillance software called Remote Control System (RCS) to law enforcement and intelligence agencies. The Citizen Lab and collaborators have mapped out RCS network endpoints in 21 countries, and have revealed evidence of RCS being used to target a human rights activist in the United Arab Emirates, a Moroccan citizen journalist organization, and an independent news agency run by members of the Ethiopian diaspora. Following the publication of Hacking Team and the Targeting of Ethiopian Journalists, the Electronic Frontier Foundation and Privacy International both took legal action related to allegations that the Ethiopian government had compromised the computers of Ethiopian expatriates in the United States and UK.\n\nIn 2017, the group released several reports that showcased phishing attempts in Mexico that used NSO Group technology, an Israeli-based \"cyber warfare firm\". The products were used in multiple attempts to gain control of mobile devices of Mexican government officials, journalists, lawyers, human rights advocates and anti-corruption workers. The operations used SMS messages as bait in an attempt to trick targets into clicking on links to the NSO Group’s exploit infrastructure. Clicking on the links would lead to the remote infection of a target’s phone. In one case, the son of one of the journalists—a minor at the time—was also targeted. NSO, who purports to only sell products to governments, also came under the group’s focus when prominent UAE human rights defender Ahmed Mansoor’s mobile phone was targeted. The report on these attempts prompted Apple to release a security update to their iOS 9.3.5.\n\nThe Citizen Lab’s research on surveillance software has been featured on the front pages of \"The Washington Post\" and \"The New York Times\" and covered extensively in news media around the world, including the BBC, Bloomberg, CBC, Slate, and Salon.\n\nThe Citizen Lab’s research on commercial surveillance technologies has resulted in legal and policy impacts. In December 2013, the Wassenaar Arrangement was amended to include two new categories of surveillance systems on its Dual Use control list—\"intrusion software\" and \"IP Network surveillance systems\". The Wassenaar Arrangement seeks to limit the export of conventional arms and dual-use technologies by calling on signatories to exchange information and provide notification on export activities of goods and munitions included in its control lists. The amendments in December 2013 were the product of intense lobbying by civil society organizations and politicians in Europe, whose efforts were informed by Citizen Lab’s research on intrusion software like FinFisher and surveillance systems developed and marketed by Blue Coat Systems.\n\nThe Citizen Lab studies the commercial market for censorship and surveillance technologies, which consists of a range of products that are capable of content filtering as well as passive surveillance.\n\nThe Citizen Lab has been developing and refining methods for performing Internet-wide scans to measure Internet filtering and detect externally visible installations of URL filtering products. The goal of this work is to develop simple, repeatable methodologies for identifying instances of internet filtering and installations of devices used to conduct censorship and surveillance.\n\nThe Citizen Lab has conducted research into companies such as Blue Coat Systems, Netsweeper, and SmartFilter. Major reports include \"Some Devices Wander by Mistake: Planet Blue Coat Redux\" (2013), \"O Pakistan, We Stand on Guard for Thee: An Analysis of Canada-based Netsweeper’s Role in Pakistan’s Censorship Regime\" (2013), and Planet Blue Coat: Mapping Global Censorship and Surveillance Tools (2013).\n\nThis research has been covered in news media around the world, including the front page of \"The Washington Post\", \"The New York Times\", \"The Globe and Mail\", and the Jakarta Post.\n\nFollowing the 2011 publication of \"Behind Blue Coat: Investigations of Commercial Filtering in Syria and Burma\", Blue Coat Systems officially announced that it would no longer provide \"support, updates. or other services\" to software in Syria. In December 2011, the U.S. Department of Commerce's Bureau of Industry and Security reacted to the Blue Coat evidence and imposed a $2.8 million fine on the Emirati company responsible for purchasing filtering products from Blue Coat and exporting them to Syria without a license.\n\nCitizen Lab's Netsweeper research has been cited by Pakistani civil society organizations Bytes for All and Bolo Bhi in public interest litigation against the Pakistani government and in formal complaints to the High Commission (Embassy) of Canada to Pakistan.\n\nThe Citizen Lab is an active participant in various global discussions on Internet governance, such as the Internet Governance Forum, ICANN, and the United Nations Government Group of Experts on Information and Telecommunications.\n\nSince 2010, the Citizen Lab has helped organize the annual Cyber Dialogue conference, hosted by the Munk School of Global Affairs’ Canada Centre, which convenes over 100 individuals from countries around the world who work in government, civil society, academia, and private enterprise in an effort to better understand the most pressing issues in cyberspace. The Cyber Dialogue has a participatory format that engages all attendees in a moderated dialogue on Internet security, governance, and human rights. Other conferences around the world, including a high-level meeting by the Hague-based Scientific Council for Government Policy and the Swedish government’s Stockholm Internet Forum, have taken up themes inspired by discussions at the Cyber Dialogue.\n\nThe Citizen Lab contributes to field building by supporting networks of researchers, advocates, and practitioners around the world, particularly from the Global South. The Citizen Lab has developed regional networks of activists and researchers working on information controls and human rights for the past ten years. These networks are in Asia (OpenNet Asia), the Commonwealth of Independent States (OpenNet Eurasia), and the Middle East and North Africa.\n\nWith the support of the International Development Research Centre (IDRC), the Citizen Lab launched the Cyber Stewards Network in 2012, which consists of South-based researchers, advocates, and practitioners who analyze and impact cybersecurity policies and practices at the local, regional, and international level. The project consists of 24 partners from across Asia, sub-Saharan Africa, Latin America, and the Middle East and North Africa including 7iber, OpenNet, and the Centre for Internet and Society.\n\nCitizen Lab staff also work with local partners to educate and train at-risk communities. For example, in 2013 it collaborated with the Tibet Action Institute to hold public awareness events in Dharamshala, India, for the exiled Tibetan community on cyber espionage campaigns. In the winter of 2013, the Citizen Lab conducted a digital security training session for Russian investigative journalists at the Sakharov Center in Moscow.\n\nThe Citizen Lab's work is often cited in media stories relating to digital security, privacy controls, government policy, human rights, and technology. Since 2006, they have been featured on 22 front-page stories at publications including \"The New York Times\", Washington Post, Globe and Mail and International Herald Tribune.\n\nSince 2013, Citizen Lab has hosted the Summer Institute on Monitoring Internet Openness and Rights as an annual research workshop at the Munk School of Global Affairs, University of Toronto. It brings together researchers and practitioners from academia, civil society, and the private sector who are working on Internet openness, security, and rights. Collaborations formed at CLSI workshops have led to publication of high impact reports on Internet filtering in Zambia, a security audit of child monitoring apps in South Korea, and an analysis of the \"Great Cannon\", an attack tool in China used for large scale distributed-denial of service attacks against Github and GreatFire.org.\n\n"}
{"id": "22973510", "url": "https://en.wikipedia.org/wiki?curid=22973510", "title": "Constantijn Huygens Jr.", "text": "Constantijn Huygens Jr.\n\nConstantijn Huygens Jr., Lord of Zuilichem (10 March 1628 – October 1697) was a Dutch statesman and poet, mostly known for his work on scientific instruments (sometimes in conjunction with his younger brother Christiaan Huygens). But, he was also a chronicler of his times, revealing the importance of gossip. Besides he was an amateur draughtsman of landscapes.\n\nConstantijn was the eldest son of Sir Constantijn Huygens, a poet and statesman and Suzanna van Baerle.\nHe was taught at home by his father and private tutors. In 1637 his mother died. Around 1640 the family was depicted by Adriaen Hanneman. Along with his brother Christiaan, he began his studies at Leiden university in 1645 studying Law and also taking some liberal Arts courses. These studies included the works of Classical authors on history, philosophy, and science, including mathematics from Frans van Schooten.\n\nIn 1649–1650 he accompanied Adriaen Pauw to England and toured through Belgium, France, Switzerland and Italy. In 1655 Constantijn moved to Paris. He joined the circle around Honoré Fabri. In the Hague he also was visiting the salon, organized by the wife of François Caron. In 1668 he married Susanna Rijckaert (1642–1712), a rich woman from Amsterdam. In April 1676, during his stay in Zemst he was visited by David Teniers the Younger. In 1680 Constantijn Jr. visited Celle and moved out of his father's house. It is not sure if it had to do with Abraham de Wicquefort or\nSophia Dorothea of Celle. To stop the gossip his father wrote a poem \"Cluijs-werck\".\n\nWhen William III of England became stadtholder in 1672 Constantijn Jr. had been appointed as his secretary. Constantijn participated in the campaigns against the French, in the Glorious Revolution, but did not attend the crowning. He described the Battle of the Boyne. During the Nine Years' War, Constantijn left for the Southern Netherlands each spring, returning to London each autumn. Constantijn became friends with or wrote about the Groom of the Stole\nWilliam Bentinck, Arnold van Keppel, 1st Earl of Albemarle, William Nassau de Zuylestein, Everhard van Weede Dijkvelt, Coenraad van Beuningen and Adrian Beverland. When William Blathwayt surpassed him as secretary, Constantijn was frustrated and in 1695 he received permission to return to the Dutch Republic.\n\nHugens was buried on 2 November 1697. The couple had one son, who died in 1704; Constantijn had one daughter from an earlier affair.\n\nAround 1650 when Christiaan Huygens became interested in microscopes and telescopes, Constantijn helped him with the construction of the lenses. In 1655 Christiaan discovered Titan, a moon orbiting Saturn. Between 1683 and 1687 Constantijn and his brother continued to make larger and longer focal length telescope objectives culminating in the very large tubeless aerial telescopes. He presented a 7.5 inch (190mm) diameter 123 ft (37.5 m) focal length aerial telescope objective to the Royal Society in 1690 that still bears his signature.\n\nFrom 1673 to 1696, Huygens kept a private diary (now at the Koninklijke Bibliotheek). Between 1649 and 1697 Huygens filled 1,599 pages. In this diary he recorded all aspects of early-modern court life in Holland and England. The book includes chapters on such subjects as the changing perception of time, book collecting, Huygens's role as connoisseur of art, belief in magic and witchcraft, and gossip and sexuality at the court of William and Mary.\nIt provides an insight on the history of human sexuality. Huygens is comparable to his English contemporary, Samuel Pepys, but with an important difference: whereas Pepys mainly describes his own sexual habits, Huygens almost exclusively describes those of others.\n\nLike his ancestor Joris Hoefnagel Constantijn practised his skill in drawing. With the death of his father in 1687, he inherited the Heerlijkheid and the castle of Zuilichem (nearby Zaltbommel), by which he became known as Lord of Zuilichem (in Dutch: Heer van Zuilichem). Huygens Jr. made some drawings and paintings, depicting this castle in the period between 1650 till 1660. They can be seen at the museum Maarten van Rossum at Zaltbommel.\n\nConstantijn was an art connoisseur and advised setting up the gallery in Kensington Palace. He is connected to the Codex Windsor and the Codex Huygens. According to Huygens Romeyn de Hooghe illustrated \" La puttana errante\", an erotic book by Pietro Aretino.\n\n"}
{"id": "161513", "url": "https://en.wikipedia.org/wiki?curid=161513", "title": "Contrasting and categorization of emotions", "text": "Contrasting and categorization of emotions\n\nThe contrasting and categorization of emotions describes how emotions are thought to relate to each other. Several proposals have been made for organizing them into groups.\n\nHumans experience emotion, with evidence used that they influence action, thoughts and behavior. Emotions are categorized into various affects, which correspond to the current situation. An affect is a term used to describe the range of feeling experienced.\n\nMany theories of emotion have been proposed, with contrasting views.\n\n\nA 2009 review of theories of emotion identifies and contrasts fundamental emotions according to three key criteria for mental experiences that:\n\nThe combination of these attributes distinguishes emotions from sensations, feelings and moods.\nThe \"emotion annotation and representation language\" (EARL) proposed by the Human-Machine Interaction Network on Emotion (HUMAINE) classifies 48 emotions.\nA tree-structured list of emotions was described in Shaver et al. (1987), and also featured in Parrott (2001).\n\nIn 1980, Robert Plutchik constructed diagram of emotions visualising eight basic emotions: joy, trust, fear, surprise, sadness, disgust, anger and anticipation. The wheel was inspired by \"Plutchik's Ten Postulates\" Plutchik also theorized twenty-four \"Primary\", \"Secondary\", and \"Tertiary\" dyads (feelings composed of two emotions). The wheel emotions can be paired in four groups:\nEmotions can be mild or intense; for example, distraction is a mild form of surprise, and rage is an intense form of anger. The kinds of relation between each pair of emotions are:\n\nJessica Hagy wrote on her blog that Plutchik's wheel of emotions gave a demonstration on emotions, but needed more levels of intensity in the emotion combinations. She observed that the wheel was a Venn diagram format, and expanded the primary dyads.\n\nThe 2012 book The Hourglass of Emotions was based on Robert Plutchik's model, but categorised the emotions into four sentic dimensions. It contrasted anger, anticipation, joy, and trust as positive emotions, and fear, surprise, sadness and disgust as negative.\n\nTiffany Watt Smith listed 154 different worldwide emotions and feelings.\n\nScientists map twenty-one different facial emotions expanded from Paul Ekman's six basic emotions of anger, disgust, fear, happiness, sadness, and surprise:\n\n"}
{"id": "17410548", "url": "https://en.wikipedia.org/wiki?curid=17410548", "title": "Council for Science and Technology", "text": "Council for Science and Technology\n\nThe Council for Science and Technology (CST) is an advisory non-departmental public body of the United Kingdom government. Its role is to give advice on issues that cut across government departments to the Prime Minister, the First Minister of Scotland and the First Minister for Wales. It was established in 1993 and relaunched in 2003. It is based in London.\n\nThe Council has 17 independent members and two co-chairs. Professor Dame Nancy Rothwell chairs meetings where advice is being developed.\nSir Mark Walport, the Chief Scientific Adviser and head of the Government Office for Science, chairs meetings reporting its advice to government.\n\nThe advisory functions of the CST had previously been performed by the Advisory Council for Applied Research and Development (ACARD), from 1976 to 1987, and the Advisory Council on Science and Technology (ACOST) from 1987 to 1993.\n\n\n"}
{"id": "43397445", "url": "https://en.wikipedia.org/wiki?curid=43397445", "title": "CrAssphage", "text": "CrAssphage\n\ncrAssphage (cross-assembly Phage) is a bacteriophage (virus that infects bacteria) that was discovered in 2014 by computational analysis of publicly accessible scientific data. Its circular genome is around 97 kbp in size and contains 80 predicted open reading frames, and the sequence is commonly found in human faecal samples. The virus is predicted to infect bacteria of the phylum \"Bacteroidetes\" that are common in the intestinal tract of many animals including humans. Based on analysis of metagenomics data, crAssphage sequences have been identified in about half of all sampled humans. The virus was named after the crAss (cross-assembly) software that was used to find the viral genome. CrAssphage is possibly the first organism to be named after a computer program.\n\nWhile crAssphage did not have any known relatives when it was discovered in 2014, a range of related viruses were discovered in 2017. Based on a screen of related sequences in public nucleotide databases and phylogenetic analysis, it was concluded that crAssphage may be part of an expansive bacteriophage family (Podoviridae, order Caudovirales) that is found in a range of environments including human gut and feces, termite gut , terrestrial/groundwater environments, soda lake (hypersaline brine), marine sediment, and plant root environments.\n\nThere is no indication that crAssphage is involved in human health or disease. The virus may outperform indicator bacteria as a marker for human faecal contamination.\n\n"}
{"id": "19106089", "url": "https://en.wikipedia.org/wiki?curid=19106089", "title": "DARPA FORESTER", "text": "DARPA FORESTER\n\nThe DARPA FORESTER is a technology development program sponsored jointly by the Defense Advanced Research Projects Agency (DARPA) and the U.S. Army intended to produce an advanced airborne UHF radar system that can track personnel and vehicles on the ground when they are hidden by foliage. \"FORESTER\" is an acronym for \"FO\"PEN Reconnaissance, Surveillance, Tracking and Engagement Radar (\"FOPEN\" itself is an acronym for FOliage PENetration).\n\nThe FORESTER is a GMTI radar system with a resolution of 6 meters that is mounted inside a long pod and designed to be carried under an A160 Hummingbird helicopter unmanned aerial vehicle (UAV). The system is able to detect vehicles and walking soldiers underneath tree cover from a distance of , giving battle planners the ability to detect potential ambushes. The pod is designed to swivel from its stowed in-line position 90 degrees to its deployed position. From a helicopter UAV hovering at , FORESTER can cover a area.\n\nAccording to FORESTER program manager Lyndall Beamer, \"Employing the sensor system on the DARPA/U.S. Army A160 Hummingbird unmanned aerial vehicle [UAV] helicopter or other suitable platform will provide a robust, wide-area, all-weather, standoff capability.\"\n\nCost is anticipated to run US$2.5 million per unit, with a production goal of US$1 million per unit in quantities of 50 or more.\n\nThe FORESTER program is being managed by DARPA's Information Innovation Office (I2O), and the hardware is manufactured for DARPA by SRC at their Syracuse, NY, headquarters. The initial prototype for the FORESTER was flight tested using a UH-60 Blackhawk helicopter because the A160 had not yet completed its Phase 1 flight test program. Test flights with the A160 began in August 2008.\n\n"}
{"id": "3475463", "url": "https://en.wikipedia.org/wiki?curid=3475463", "title": "Dawes' limit", "text": "Dawes' limit\n\nDawes' limit is a formula to express the maximum resolving power of a microscope or telescope. It is so named after its discoverer, W. R. Dawes\n, although it is also credited to Lord Rayleigh.\n\nThe formula takes different forms depending on the units.\n"}
{"id": "15447", "url": "https://en.wikipedia.org/wiki?curid=15447", "title": "Differential psychology", "text": "Differential psychology\n\nDifferential psychology studies the ways in which individuals differ in their behavior and the processes that underlie it. This is distinguished from other aspects of psychology in that although psychology is ostensibly a study of individuals, modern psychologists often study groups, or attempt to discover general psychological processes that apply to all individuals.\n\nFor example, in evaluating the effectiveness of a new therapy, the mean performance of the therapy in one treatment group might be compared to the mean effectiveness of a placebo (or a well-known therapy) in a second, control group. In this context, differences between individuals in their reaction to the experimental and control manipulations are actually treated as errors rather than as interesting phenomena to study.\n\nThis approach is applied because psychological research depends upon statistical controls that are only defined upon groups of people. Individual difference psychologists usually express their interest in individuals while studying groups by seeking dimensions shared by all individuals but upon which individuals differ. The ergodicity problem impedes correct group-to-individual generalization for most psychological phenomena.\n\nIndividual differences are essential whenever we wish to explain how\nindividuals differ in their behavior. In any study, significant\nvariation exists between individuals. Reaction time, preferences,\nvalues, and health-linked behaviors are just a few examples. Individual\ndifferences in factors such as personality, intelligence,\nmemory, or physical factors such as body size, sex, age, and other\nfactors can be studied and used in understanding this large source of\nvariance. Importantly, individuals can also differ not only in their\ncurrent state, but in the magnitude or even direction of response to a\ngiven stimulus. Such phenomena, often\nexplained in terms of inverted-U response curves,\nplace differential psychology at an important location in such\nendeavours as personalized medicine, in which diagnoses are\ncustomised for an individual's response profile.\n\nIndividual differences research typically includes personality, motivation, intelligence, ability, IQ, interests, values, self-concept, self-efficacy, and self-esteem (to name just a few). There are few remaining \"differential psychology\" programs in the United States, although research in this area is very active. Current researchers are found in a variety of applied and experimental programs, including clinical psychology, educational psychology, Industrial and organizational psychology, personality psychology, social psychology, behavioral genetics, and developmental psychology programs, in the neo-Piagetian theories of cognitive development in particular.\n\n\n"}
{"id": "47778007", "url": "https://en.wikipedia.org/wiki?curid=47778007", "title": "Egypt Urban Forum", "text": "Egypt Urban Forum\n\nEgypt Urban Forum (EUF) is an international forum about urbanization issues held in Cairo, Egypt annually under the auspices of the Ministry of Housing and Urban Communities of Egypt and the UN-Habitat. the First Egypt Urban Forum was in Cairo on June 14–16, 2015. more than 300 Egyptian institutions, decision makers, civil society representatives, scholars and experts, private sector companies and around 50 regional and international partners will be invited to the first forum to discuss issues that related to Egypt cooperation with World Urban Forum.\n\n"}
{"id": "21155519", "url": "https://en.wikipedia.org/wiki?curid=21155519", "title": "Einstein–Cartan–Evans theory", "text": "Einstein–Cartan–Evans theory\n\nEinstein–Cartan–Evans theory or ECE theory was an attempted unified theory of physics proposed by the Welsh chemist and physicist Myron Wyn Evans (born May 26, 1950), which claimed to unify general relativity, quantum mechanics and electromagnetism. The hypothesis was largely published in the journal \"Foundations of Physics Letters\" between 2003 and 2005. Several of Evans' central claims were later shown to be mathematically incorrect and, in 2008, the new editor of \"Foundations of Physics\", Nobel laureate Gerard 't Hooft, published an editorial note effectively retracting the journal's support for the hypothesis.\n\nEarlier versions of the theory were called \"O(3) electrodynamics\". Evans claims that he is able to derive a generally covariant field equation for electromagnetism and gravity, similar to that derived by Mendel Sachs.\n\nEvans argues that Einstein's theory of general relativity does not take into account torsion, which is included in the Einstein–Cartan theory.\n\nIn 1998 Evans founded the Alpha Institute for Advanced Studies (AIAS) to keep developing his theory. Its website collects papers on the theory and recent developments.\n\nThe theory has been used to justify the motionless electromagnetic generator, a perpetual motion machine. In July 2017, Evans claimed (on his blog): \"There is immediate international interest in [papers] UFT382 and UFT383, describing the new energy from spacetime (ES) circuits. There is also great interest in UFT364, the paper that describes the circuit [...] These circuits should be [...] developed into power stations.\" In November 2017, Evans expanded on this point, as follows (again on his blog): \"There is no reasonable doubt that the vacuum (or aether or spacetime) contains a source of inexhaustible, safe and clean energy. This source can be used in patented and replicated circuits such as those of [Evans' self-published papers] UFT311, UFT364, UFT382, and UFT383.\"\n\nEvans' claims are not accepted by the mainstream physics community. In an editorial note in \"Foundations of Physics\" the Nobel laureate Gerard 't Hooft discussed the \"revolutionary paradigm switch in theoretical physics\" promised by ECE theory. He concluded that activities in the subject \"have remained limited to personal web pages and are absent from the standard electronic archives, while no reference to ECE theory can be spotted in any of the peer reviewed scientific journals\".\n\nSeveral of the published contributions in this theory have been shown to be mathematically incorrect. In response to these demonstrations, 't Hooft's editorial note concludes, \"Taking into account the findings of Bruhn, Hehl and Obukhhov, the discussion of ECE theory in the journal \"Foundations of Physics\" will be concluded herewith unless very good arguments are presented to resume the matter.\"\n\n\n\n\n"}
{"id": "22382381", "url": "https://en.wikipedia.org/wiki?curid=22382381", "title": "Ernesto Milá", "text": "Ernesto Milá\n\nErnesto Milá Rodríguez (Barcelona, 1952) is a Spanish far right political activist known for his thesis about the distinction between patriotism, which he supports, and nationalism, which he sees as dangerous and divisive.\n\nBorn in Barcelona to a Catalan father belonging to the Penedès rural bourgeoisie and an Extremaduran immigrant mother (herself the daughter of a Republican military officer), Milá began his political activity as one of the members of the fringe right-wing extremist groups (usually named \"incontrolados\", \"uncontrolled elements\") who rallied against leftist or pro-democratic meetings during late Francoism, usually assuming the role of unofficial mob breakers and violent counter-rioters. His political affiliation began in the relatively short-lived neonazi group PENS (\"Partido Español Nacional Socialista\").\n\nAs Xavier Casals Meseguer explains in \"Los Neonazis en España\" (Editorial Grijalbo, 1995), the PENS terrorist attacks on the headquarters of \"El Ciervo\" (a relatively center-leaning Catholic journal), the\"Taller Picasso\" (1971), Catalan libraries (such as the \"Cinc d'Oros\", 1971), the \"Gran enciclopédia catalana\" (1974) and libraries and public centers in Valencia (1975), among others, were not prosecuted—mainly due to the fact that the Francoist police and the SECED itself, along with a former member of Franco's bodyguard corps (Luis García Rodríguez, later founder of the neo-Nazi group Estado Nacional Europeo), provided active support for said actions.\n\nMilá then became a member of the Círculo José Antonio in Barcelona, subsequently entering Blas Piñar's Fuerza Nueva (1975), from which he was expelled in 1977, mostly on the grounds of his recent civil marriage; he then joined the Fuerza Nueva splinter group Frente Nacional de la Juventud founded by Ramón Graells Bofill, which later merged into the Youth Front.\n\nIn June 1980, an illegal gathering dubbed \"Día de la Patria Española\" and organized by the Milá on behalf of the Youth Front, ended with the assault and arson of the Barcelona Union of the Democratic Centre (UCD) headquarters. The Barcelona Supreme Court ruled that Milá was the main inductor of the demonstration and sentenced him \"in absentia\" to a two-year prison sentence. This formal indictment forced Milá to flee for France, where he was investigated in connection with the Copernicus street synagogue bombing in Paris, which claimed four lives. He was sentenced to three months imprisonment in La Santé for his use of forged documents. During his time in prison, he wrote an apology letter to Blas Piñar in which he assured his return to the Catholic faith, as shown in the documents compiled by Piñar himself in \"Escrito para la historia\" (Fuerza Nueva Editorial, 2000).\n\nIn January 1981, during Milá's absence, more than thirty militants of the \"Frente de la Juventud\" were arrested by the police, and accused of terrorist activities, among them a bombing in Madrid which killed one person and wounded nine others. The party officially dissolved August 29, 1982.\n\nFrom France, he departed to Latin America, where he engaged in (mostly undisclosed) collaboration with a number of activists and regimes of probably diverging political obedience. As he admitted himself in his interview with Manuel Vázquez Montalbán, during his stay in Bolivia he worked as an \"adviser\" (most likely on PSYOPS or plain torture) for the short-lived \"Cocaine-coup\" dictatorship along with infamous neo-fascists such as Stefano delle Chiaie and Nazi war criminal Klaus Barbie, later returning to Spain in 1983.\n\nUpon his return, he was arrested by the police at the Spanish frontier with Ingram machine guns in his possession, indicted on firearms and illegal demonstration charges and sentenced to two years in prison.\n\nThe 22 February 1983 edition of \"El País\" informed of his formal confinement in the Carabanchel prison, along with that of Rafael Tormo Acosta, after the aforementioned arrest. The 14 April article in \"El País\" even qualified Milá's wife's written plea on behalf of her husband as \"pathetic\".\n\nThe 3 October 1985 issue of \"El País\" informed of a new arrest, this time in the streets of Barcelona, and his immediate confinement in the , in order to execute his pending sentence for the 1980 incidents in the UCD headquarters.\n\nUpon release from prison, Milà founded the \"Dissidencias\" journal, after having participated in forming Juntas Españolas along with the \"El Alcázar\" director, Antonio Izquierdo. During that time he directed \"Ediciones Alternativa\", which published the first translations of the works of the Italian esoterist and fascist ideologue Julius Evola, one of whose career highlights was the Italian translation of the anti-Semitic libel \"Protocols of the Elders of Zion\".\n\nIn 2000 he became a militant of the umbrella far-right group \"Democracia Nacional\", already a haven for CEDADE ex-members (such as Christian Ruiz Reguant, Laureano Luna or Joaquín Bochaca) and other Spanish neo-nazis. In 2004 Milà disenfranchised himself from DN after bitter disagreements with its leader, Manuel Canduela.\n\nHis attention then turned to Spain 2000, the controversial far-right platform linked to the \"Asociación Nacional de Empresarios de Locales de Alterne\" or ANELA, Spain's largest pimp syndicate (see \"Los Amos de la prostitución en España\" by Joan Cantarero (Ediciones B, 2007) ). Milá was recently appointed as the party's press secretary, and appeared in the election list for the 2008 General Election, ostensibly an attempt to gain preponderance over other fascist groups in Spain by capitalizing on Milá's extensive history. Milá's presumed relation with the CNI and its predecessors, albeit not as yet admitted by him, has raised found an outcry in most far-right blogs, especially those of Democracia Nacional and the currently marginal entourage of the once infamous Ricardo Saenz de Ynestrillas, who had a row with Milá himself although the latter decided not to press charges.\n\nWidely considered the most intellectually ambitious and well-connected of all Spanish neo-fascists who have escaped successful or permanent legal prosecution, Milá is nowadays devoted to \"cultural\" dissertations in his blog on Julius Evola, theosophy, Esoteric Nazism, purportedly \"mysterious\" aspects of History (such Catharism, Freemasonry and unknown or \"intriguing\" aspects of Barcelona or Gaudí's legacy) and other subjects, most notably surveillance and security. He also uses the blog to extend on his particular view of contemporary topics, such as immigration, as well as to occasionally write film reviews. He has worked jointly with a number of printed and electronic media, with special attention towards esoterism and the occult. His orientation towards these themes has been the object of many a controversy with other journalists He also published in journals such as El Alcázar and Defensa, Más allá de la ciencia, Año Cero, Próximo Milenio, Nueva Dimensión and Historia y Vida, \nsome of them also including articles by historic militants of CEDADE. The information in the articles written by Milá and published in his blog is often inconsistent with the known historical facts (e.g. assuming that it was William the Conqueror, and not Harold Godwinson, who defeated Harald III of Norway at the Battle of \"Stanford\" (sic) Bridge), or punctuated by facts and rumors of which he does seldom presents a proof, such as the different conspiracies whose confluence led to the 1981 coup attempt in Spain.\n\nHe was the chief editor of the \"Saber Más\" magazine, distributed in 1997 with the El Mundo edition in Catalunya—the relation between mainstream, purportedly center-right communication media such as \"El Mundo\", Telemadrid, ABC or the COPE with former or rumoured neo-Nazi activists has long been a matter of controversy.\n\nHe then became chief editor for \"Nuevos Horizontes\", directed by notorious \"occultist\" Sebastià d'Arbó, and then for the \"(In)Seguridad\" magazine. He is currently the editor of the \"Revista IdentidaD\".\n\nHis current line of thought is overtly critical towards Catalan nationalists, whom he accuses of ignoring or even disparaging the Spanish immigration of the 60s and 70s, from whence he partially descends, while being oblivious of the “cultural and anthropological gap” with respect to more recent immigrants (itself another staple in his blog articles).\n\n\n"}
{"id": "32432572", "url": "https://en.wikipedia.org/wiki?curid=32432572", "title": "Eureka (UK TV series)", "text": "Eureka (UK TV series)\n\nEureka (sometimes referred to as Eureka!) is a British educational television series about science and inventiveness which was originally produced and broadcast by the BBC from 1982 to 1986, and repeated until 1987. Devised and written by Clive Doig and Jeremy Beadle, the series told the stories behind the inventions of commonplace objects.\n\nPresented by Jeremy Beadle (series 1, 1982), then Sarah Greene (series 2, 1983), Paul McDowell (series 2 and 3, 1983 & 1985) and Wilf Lunn (series 4, 1986), the show featured an ensemble cast who re-enacted the moments of invention or performed humorous sketches to deliver key facts and information. Notable cast members included Sylvester McCoy, Simon Gipps-Kent, Bernard Holley, Madeline Smith, Mike Savage, Julia Binsted, Philip Fox and Jackie Clarke.\n\nEach show also featured a segment showcasing a madcap and not always reliable invention by Wilf Lunn often to the bemusement of McCoy or another of the regular cast. In the fourth and final series, the format changed slightly and Lunn became the presenter, playing the Doctor of Alternative Invention at the Eureka Museum of Invention.\n\n"}
{"id": "50899666", "url": "https://en.wikipedia.org/wiki?curid=50899666", "title": "Eustachio Porcellotti", "text": "Eustachio Porcellotti\n\nEustachio Porcellotti (19th century) was a Florentine watchmaker.\n\nHe was known for having built, in the second half of the 19th century, several models illustrating the Galilean notion of applying the pendulum to the clock.\n"}
{"id": "12979470", "url": "https://en.wikipedia.org/wiki?curid=12979470", "title": "Filar micrometer", "text": "Filar micrometer\n\nA filar micrometer is a specialized eyepiece used in astronomical telescopes for astrometry measurements, in microscopes for specimen measurements, and in alignment and surveying telescopes for measuring angles and distances on nearby objects. The word \"filar\" derives . It refers to the fine threads or wires used in the device.\n\nA typical filar micrometer consists of a micrometer and a reticle that has two fine parallel wires or threads that can be moved by the observer using a micrometer screw mechanism. The wires are placed in the focal image plane of the eyepiece so they remain sharply superimposed over the object under observation, while the micrometer motion moves the wires across the focal plane. Other designs employ a fixed reticle, against which one wire or a second reticle moves. By rotating the eyepiece assembly in the eyetube, the measurement axis can be aligned to match the orientation of the two points of observation.\n\nAt one time, it was common to use spider silk as a thread.\n\nBy placing one wire over one point of interest and moving the other to a second point, the distance between the two wires can be measured with the micrometer portion of the instrument. Given this precise distance measurement at the image plane, a trigonometric calculation with the objective focal length yields the angular distance between the two points seen in a telescope. In a microscope, a similar calculation yields the spatial distance between two points on a specimen.\n\nIn an alignment telescope, the precise micrometric measurement of the eyepiece image directly indicates the real distance of a nearby observed point from the line of sight. This absolute measurement is independent of the distance to the object, due to the telecentricity principle.\n\nA common use of filar micrometers in astronomical telescopes was measuring the distance between double stars.\n\nFilar micrometers are little used in modern astronomy, having been replaced by digital photographic techniques where digital pixels provide a precise reference for image distance. However, filar eyepieces are still used in teaching astronomy and by some amateur astronomers.\n\nThe precursor to the filar micrometer was the micrometer eyepiece, invented by William Gascoigne.\n\nEarlier measures of angular distances relied on inserting into the eyepiece a thin metal sheet cut in the shape of a narrow, isosceles triangle. The sheet was pushed into the eyepiece until the two adjacent edges of the metal sheet simultaneously occulted the two objects of interest. By carefully measuring the position where the objects were extinguished and knowing the focal length of the eyepiece, the angular distance could be calculated. Christiaan Huygens used such a device.\n\n\n"}
{"id": "45515131", "url": "https://en.wikipedia.org/wiki?curid=45515131", "title": "Finite Volume Community Ocean Model", "text": "Finite Volume Community Ocean Model\n\nThe Finite Volume Community Ocean Model (FVCOM; Formerly Finite Volume Coastal Ocean Model) is a prognostic, unstructured-grid, free-surface, 3-D primitive equation coastal ocean circulation model. The model is developed primarily by researchers at the University of Massachusetts Dartmouth and Woods Hole Oceanographic Institution, and used by researchers worldwide. Originally developed for the estuarine flooding/drying process, FVCOM has been upgraded to the spherical coordinate system for basin and global applications.\n\n"}
{"id": "45464251", "url": "https://en.wikipedia.org/wiki?curid=45464251", "title": "François Josephe Fettig", "text": "François Josephe Fettig\n\nAbbé François Joseph Fettig (10 July 1824, Mothern near Wissembourg – 5 May 1906, Matzenheim)\nwas a French entomologist specialising in Lepidoptera and Coleoptera.\n\nHis collections are shared between Muséum national d'histoire naturelle (Coleoptera), Museum Colmar (Microlepidoptera and larvae, destroyed or badly damaged) and Zoological Museum,Strasbourg (Macrolepidoptera).\n\npartial list\n\n"}
{"id": "1404417", "url": "https://en.wikipedia.org/wiki?curid=1404417", "title": "Grounded theory", "text": "Grounded theory\n\nGrounded theory (GT) is a systematic methodology in the social sciences involving the construction of theories through methodical gathering and analysis of data. Grounded theory is a research methodology which operates inductively, in contrast to the hypothetico-deductive approach. A study using grounded theory is likely to begin with a question, or even just with the collection of qualitative data. As researchers review the data collected, repeated ideas, concepts or elements become apparent, and are tagged with \"codes\", which have been extracted from the data. As more data is collected, and re-reviewed, codes can be grouped into concepts, and then into categories. These categories may become the basis for new theory. Thus, grounded theory is quite different from the traditional model of research, where the researcher chooses an existing theoretical framework, and only then collects data to show how the theory does or does not apply to the phenomenon under study.\n\nGrounded theory is a general methodology, a way of thinking about and conceptualizing data. It focuses on the studies of diverse populations from areas like remarriage after divorce (Cauhape, 1983) and Professional Socialization (Broadhed, 1983). The Grounded Theory method was developed by two sociologists, Barney Glaser and Anselm Strauss. Their collaboration in research on dying hospital patients led them to write \"Awareness of Dying\" in 1965. In this research they developed the constant comparative method, later known as Grounded Theory Method. There were three main purposes behind the publication of \"The Discovery of Grounded Theory\": \n\nThis theory mainly came into existence when there was a wave of criticism towards the fundamentalist and structuralist theories that were deductive and speculative in nature.\n\nAfter two decades, sociologists and psychologists showed some appreciation for the Grounded theory because of its explicit and systematic conceptualization of the theory. \"The Discovery of Grounded Theory\" (1967) was published simultaneously in the United States and the United Kingdom, because of which the theory became well known among qualitative researchers and graduate students of those countries.\n\nThe turning point for this theory came after the publishing of two main monographs/works which dealt with \"dying in hospitals\". This helped the theory to gain some significance in the fields of medical sociology, psychology and psychiatry. From its beginnings in health, the grounded theory method has come to prominence in fields as diverse as drama, management, manufacturing and education.\n\nGrounded theory [Pakyu] combines diverse traditions in sociology, positivism and symbolic interactionism as it is according to Ralph, Birks & Chapman (2015) \"methodologically dynamic\". Glaser's strong training in positivism enabled him to code the qualitative responses, however Strauss's training looked at the \"active\" role of people who live in it. Strauss recognized the profundity and richness of qualitative research regarding social processes and the complexity of social life, Glaser recognized the systematic analysis inherent in quantitative research through line by line examination, followed by the generation of codes, categories, and properties. According to Glaser (1992), the strategy of Grounded Theory is to take the interpretation of meaning in social interaction on board and study \"the interrelationship between meaning in the perception of the subjects and their action\". Therefore, through the meaning of symbols, human beings interpret their world and the actors who interact with them, while Grounded Theory translates and discovers new understandings of human beings' behaviors that are generated from the meaning of symbols. Symbolic interactionism is considered to be one of the most important theories to have influenced grounded theory, according to it understanding the world by interpreting human interaction, which occurs through the use of symbols, such as language. According to Milliken and Schreiber in Aldiabat and Navenec, the grounded theorist's task is to gain knowledge about the socially-shared meaning that forms the behaviors and the reality of the participants being studied.\n\nOnce the data are collected, grounded theory analysis involves the following basic steps:\n\n\nTheorizing is involved in all these steps. One is required to build and test theory all the way through till the end of a project.\n\nGrounded theory method is a systematic generation of theory from data that contains both inductive and deductive thinking. One goal is to formulate hypotheses based on conceptual ideas. Others may try to verify the hypotheses that are generated by constantly comparing conceptualized data on different levels of abstraction, and these comparisons contain deductive steps. Another goal of a grounded theory study is to discover the participants' main concern and how they continually try to resolve it. The questions the researcher repeatedly asks in grounded theory are \"What's going on?\" and \"What is the main problem of the participants, and how are they trying to solve it?\" These questions will be answered by the core variable and its subcores and properties in due course.\n\nGrounded theory method does not aim for the \"truth\" but to conceptualize what is going on by using empirical research. In a way, grounded theory method resembles what many researchers do when retrospectively formulating new hypotheses to fit data. However, when applying the grounded theory method, the researcher does not formulate the hypotheses in advance since preconceived hypotheses result in a theory that is ungrounded from the data.\n\nIf the researcher's goal is an accurate description, then another method should be chosen since grounded theory is not a descriptive method. Instead, it has the goal of generating concepts that explain the way that people resolve their central concerns regardless of time and place. The use of description in a theory generated by the grounded theory method is mainly to illustrate concepts.\n\nIn most behavioral research endeavors, persons or patients are units of analysis, whereas in GT the unit of analysis is the incident. Typically several hundred incidents are analyzed in a grounded theory study since usually every participant reports many incidents.\n\nWhen comparing many incidents in a certain area, the emerging concepts and their relationships are in reality probability statements. Consequently, GT is a general method that can use any kind of data even though the most common use is with qualitative data (Glaser, 2001, 2003). However, although working with probabilities, most GT studies are considered as qualitative since statistical methods are not used, and figures are not presented.\nThe results of GT are not a reporting of statistically significant probabilities but a set of probability statements about the relationship between concepts, or an integrated set of conceptual hypotheses developed from empirical data (Glaser 1998). Validity in its traditional sense is consequently not an issue in GT, which instead should be judged by fit, relevance, workability, and modifiability (Glaser & Strauss 1967, Glaser 1978, Glaser 1998).\n\nFit. A theory that is fitting has concepts that are closely connected to the incidents they are representing; this is related to how thorough the constant comparison of incidents to concepts was done.\n\nRelevance. A relevant study deals with the real concern of participants, evokes \"grab\" (captures the attention) and is not only of academic interest.\n\nWorkability. The theory works when it explains how the problem is being solved with much variation.\n\nModifiability. A modifiable theory can be altered when new relevant data are compared to existing data.\nA GT is never right or wrong, it just has more or less fit, relevance, workability and modifiability.\n\nA concept is the overall element and includes the categories which are conceptual elements standing by themselves, and properties of categories, which are conceptual aspects of categories (Glaser & Strauss, 1967). The core variable explains most of the participants' main concern with as much variation as possible. It has the most powerful properties to picture what's going on, but with as few properties as possible needed to do so. A popular type of core variable can be theoretically modeled as a basic social process that accounts for most of the variation in change over time, context, and behavior in the studied area.\n\"GT is multivariate. It happens sequentially, subsequently, simultaneously, serendipitously, and scheduled\" (Glaser, 1998).\n\nAll is data is a fundamental property of GT which means that everything that the researcher encounters when studying a certain area is data – not only interviews or observations but anything that helps the researcher generating concepts for the emerging theory. According to Ralph, Birks & Chapman (2014) field notes can come from informal interviews, lectures, seminars, expert group meetings, newspaper articles, Internet mail lists, even television shows, conversations with friends etc.\nA related technique consists of conducting self-interviews and treating those interviews like any other data, coding and comparing it to other data and generating concepts from it.\n\nOpen coding or substantive coding is conceptualizing on the first level of abstraction. Written data from field notes or transcripts are conceptualized line by line. In the beginning of a study everything is coded in order to find out about the problem and how it is being resolved. The coding is often done in the margin of the field notes.\nThis phase is often tedious since it involves conceptualizing all the incidents in the data, which yields many concepts. These are compared as more data is coded, merged into new concepts, and eventually renamed and modified.\nThe GT researcher goes back and forth while comparing data, constantly modifying, and sharpening the growing theory at the same time as she follows the build-up schedule of GT's different steps.\n\nSelective coding is done after having found the core variable or what is thought to be the core, the tentative core. The core explains the behavior of the participants in resolving their main concern. The tentative core is never wrong. It just more or less fits with the data.\nAfter the core variable is chosen, researchers selectively code data with the core guiding their coding, not bothering about concepts with little importance to the core and its subcores. Also, they now selectively sample new data with the core in mind, which is called theoretical sampling – a deductive part of GT.\nSelective coding delimits the study, which makes it move fast. This is indeed encouraged while doing GT (Glaser, 1998) since GT is not concerned with data accuracy as in descriptive research but is about generating concepts that are abstract of time, place and people.\nSelective coding could be done by going over old field notes or memos which are already coded once at an earlier stage or by coding newly gathered data.\n\nTheoretical codes integrate the theory by weaving the fractured concepts into hypotheses that work together in a theory explaining the main concern of the participants. Theoretical coding means that the researcher applies a theoretical model to the data. It is important that this model is not forced beforehand but has emerged during the comparative process of GT. So the theoretical codes just as substantives codes should emerge from the process of constantly comparing the data in field notes and memos.\n\nTheoretical memoing is \"the core stage of grounded theory methodology\" (Glaser 1998).\n\"Memos are the theorizing write-up of ideas about substantive codes and their theoretically coded relationships as they emerge during coding, collecting and analyzing data, and during memoing\" (Glaser 1998).\n\nMemoing is also important in the early phase of a GT study such as open coding. The researcher is then conceptualizing incidents, and memoing helps this process. Theoretical memos can be anything written or drawn in the constant comparison that makes up a GT. \nMemos are important tools to both refine and keep track of ideas that develop when researchers compare incidents to incidents and then concepts to concepts in the evolving theory. In memos, they develop ideas about naming concepts and relating them to each other and try the relationships between concepts in two-by-two tables, in diagrams or figures or whatever makes the ideas flow, and generates comparative power.\n\nWithout memoing, the theory is superficial and the concepts generated are not very original. Memoing works as an accumulation of written ideas into a bank of ideas about concepts and how they relate to each other. This bank contains rich parts of what will later be the written theory.\nMemoing is total creative freedom without rules of writing, grammar or style (Glaser 1998). The writing must be an instrument for outflow of ideas, and nothing else. When people write memos, the ideas become more realistic, being converted from thoughts into words, and thus ideas communicable to the afterworld.\n\nIn GT the preconscious processing that occurs when coding and comparing is recognized. The researcher is encouraged to register ideas about the ongoing study that eventually pop up in everyday situations, and awareness of the serendipity of the method is also necessary to achieve good results.\n\nSerendipity is used as a sociological method in grounded theory, building on ideas by sociologist Robert K. Merton, who in \"Social Theory and Social Structure\" (1949) referred to the \"serendipity pattern\" as the fairly common experience of observing an unanticipated, anomalous and strategic datum which becomes the occasion for developing a new theory or for extending an existing theory. Robert K. Merton also coauthored (with Elinor Barber) \"The Travels and Adventures of Serendipity\" which traces the origins and uses of the word \"serendipity\" since it was coined. The book is \"a study in sociological semantics and the sociology of science\", as the subtitle of the book declares. It further develops the idea of serendipity as scientific \"method\" (as juxtaposed with purposeful discovery by experiment or retrospective prophecy).\n\nIn the next step memos are sorted, which is the key to formulate the theory for presentation to others. Sorting puts fractured data back together. During sorting lots of new ideas emerge, which in turn are recorded in new memos giving the memo-on-memos phenomenon.\nSorting memos generates theory that explains the main action in the studied area. A theory written from unsorted memos may be rich in ideas but the connection between concepts is weak.\n\nWriting up the sorted memo piles follows after sorting, and at this stage the theory is close to the written GT product. The different categories are now related to each other and the core variable. The theoretical density should be stratified so that concepts are mixed with description in words, tables, or figures to optimize readability.\n\nIn the later rewriting the relevant literature is woven in to put the theory in a scholarly context. Finally, the GT is edited for style and language and eventually submitted for publication. Most books on grounded theory do not explain what methodology details to include in a scholarly article; however, some guidelines have been suggested.\n\nGT according to Glaser gives the researcher freedom to generate new concepts explaining human behavior. This freedom is optimal when the researcher refrains from taping interviews, doing a pre-research literature review, and talking about the research before it is written up.\nThese rules makes GT different from most other methods using qualitative data.\n\nNo pre-research literature review. Studying the literature of the area under study gives preconceptions about what to find and the researcher gets desensitized by borrowed concepts. Instead, the GT method increases theoretical sensitivity. The literature should instead be read in the sorting stage being treated as more data to code and compare with what has already been coded and generated.\n\nNo taping. Taping and transcribing interviews is common in qualitative research, but is counter-productive and a waste of time in GT which moves fast when the researcher delimits her data by field-noting interviews and soon after generates concepts that fit with data, are relevant and work in explaining what participants are doing to resolve their main concern. However, Kathy Charmaz counters this point, insisting that transcribing, coding, and re-coding are integral to the development of the theory.\n\nNo talk. Talking about the theory before it is written up drains the researcher of motivational energy. Talking can either render praise or criticism, and both diminish the motivational drive to write memos that develop and refine the concepts and the theory (Glaser 1998). Positive feedback makes researchers content with what they have and negative feedback hampers their self-confidence. Talking about the GT should be restricted to persons capable of helping the researcher without influencing her final judgments.\n\nRalph, Birks & Chapman (2015) explain the split in divergence grounded theory methodology in the article \"The Methodological Dynamism of Grounded Theory\" and how grounded theory has been influenced by varying schools of thought over the years.\n\nSince their original publication in 1967, Glaser and Strauss have disagreed on how to apply the grounded theory method, resulting in a split between Straussian and Glaserian paradigms. This split occurred most obviously after Strauss published \"Qualitative Analysis for Social Scientists\" (1987). Thereafter Strauss, together with Juliet Corbin, published \"Basics of Qualitative Research: Grounded Theory Procedures and Techniques\" in 1990. This was followed by a rebuke by Glaser (1992) who set out, chapter by chapter, to highlight the differences in what he argued was original grounded theory and why, according to Glaser, what Strauss and Corbin had written was not grounded theory in its \"intended form\" but was rather a form of qualitative data analysis. This divergence in methodology is a subject of much academic debate, which Glaser (1998) calls a \"rhetorical wrestle\". Glaser continues to write about and teach the original grounded theory method.\n\nAccording to Kelle (2005), \"the controversy between Glaser and Strauss boils down to the question of whether the researcher uses a well-defined 'coding paradigm' and always looks systematically for 'causal conditions,' 'phenomena/context, intervening conditions, action strategies' and 'consequences' in the data, or whether theoretical codes are employed as they emerge in the same way as substantive codes emerge, but drawing on a huge fund of 'coding families.' Both strategies have their pros and cons. Novices who wish to get clear advice on how to structure data material may be satisfied with the use of the coding paradigm. Since the paradigm consists of theoretical terms which carry only limited empirical content the risk is not very high that data are forced by its application. However, it must not be forgotten that it is linked to a certain micro-sociological perspective. Many researchers may concur with that approach especially since qualitative research always had a relation to micro-sociological action theory, but others who want to employ a macro-sociological and system theory perspective may feel that the use of the coding paradigm would lead them astray.\"\n\nGlaser originated the basic process of Grounded theory method described as the constant comparative method where the analyst begins analysis with the first data collected and constantly compares indicators, concepts and categories as the theory emerges.\n\nThe first book, \"The Discovery of Grounded Theory\", published in 1967, was \"developed in close and equal collaboration\" by Glaser and Strauss. Glaser wrote \"Theoretical Sensitivity\" in 1978 and has since written five more books on the method and edited five readers with a collection of grounded theory articles and dissertations.\n\nThe Glaserian method is \"not\" a qualitative research method, but claims the dictum \"all is data\". This means that not only interview or observational data but also surveys or statistical analyses or \"whatever comes the researcher's way while studying a substantive area\" (Glaser quote) can be used in the comparative process as well as literature data from science or media or even fiction. Thus the method according to Glaser is not limited to the realm of qualitative research, which he calls \"QDA\" (Qualitative Data Analysis). QDA is devoted to descriptive accuracy while the Glaserian method emphasizes conceptualization abstract of time, place and people. A theory discovered with the grounded theory method should be easy to use outside of the substantive area where it was generated.\n\nGenerally speaking, grounded theory is an approach for looking systematically at (mostly) qualitative data (like transcripts of interviews or protocols of observations) aiming at the generation of theory. Sometimes, grounded theory is seen as a qualitative method, but grounded theory reaches farther: it combines a specific style of research (or a paradigm) with pragmatic theory of action and with some methodological guidelines.\n\nThis approach was written down and systematized in the 1960s by Anselm Strauss (himself a student of Herbert Blumer) and Barney Glaser (a student of Paul Lazarsfeld), while working together in studying the sociology of illness at the University of California, San Francisco. For and with their studies, they developed a methodology, which was then made explicit and became the foundation stone for an important branch of qualitative sociology.\n\nImportant concepts of grounded theory method are categories, codes and codings. The research principle behind grounded theory method is neither inductive nor deductive, but combines both in a way of abductive reasoning (coming from the works of Charles Sanders Peirce). This leads to a research practice where data sampling, data analysis and theory development are not seen as distinct and disjunct, but as different steps to be repeated until one can describe and explain the phenomenon that is to be researched. This stopping point is reached when new data does not change the emerging theory anymore.\n\nIn an interview that was conducted shortly before Strauss' death (1994), he named three basic elements every grounded theory approach should include (Legewie/Schervier-Legewie (2004)). These three elements are:\n\nGrounded theory method according to Glaser emphasizes induction or emergence, and the individual researcher's creativity within a clear frame of stages, while Strauss is more interested in validation criteria and a systematic approach.\n\nA later version of GT called constructivist GT, which was rooted in pragmatism and relativist epistemology, assumes that neither data nor theories are discovered, but are constructed by the researcher as a result of his or her interactions with the field and its participants. Data are co-constructed by researcher and participants, and colored by the researcher's perspectives, values, privileges, positions, interactions, and geographical locations. This position takes a middle ground between the realist and postmodernist positions by assuming an \"obdurate reality\" at the same time as it assumes multiple perspectives on these realities. Within this approach, a literature review is used in a constructive and data-sensitive way without forcing it on data.\n\nMore recently, a critical realist version of GT has been developed and applied for generating mechanism-based explanations for social phenomena (Kempster and Parry 2011, Oliver 2012, Bunt 2016, Hoddy 2018). Critical realism (CR) is the philosophical approach associated with Roy Bhaskar which argues for a structured and differentiated account of reality in which difference, stratification and change is central. In contrast to positivism and idealism, CR combines a realist ontology with an interpretivist epistemology. A CR approach to GT shares with Strauss and Corbin's approach a commitment to abduction but it includes a retroductive step for identifying causes and conditions. A critical realist grounded theory produces an explanation through an examination of the three domains of social reality: the 'real', as the domain of structures and mechanisms; the 'actual', as the domain of events; and the 'empirical', as the domain of experiences and perceptions.\n\nGrounded theory has provided part of the basis for the research methodology known as the data percolation methodology. While the latter accepts the formulation of hypotheses, it first recommends that the researcher immerses him/herself in ground research free of preconceived ideas or biases. A series of steps are proposed to ensure the research leads to results that are as meaningful as possible after having percolated the mass of data. Notably, the data percolation methodology, unlike grounded theory, accepts the formulation of a so-called emerging model, which, as the name suggests, evolves as the researcher moves from a grounded approach to a hypothetico-deductive method back to testing the emerging model in the research field again. Like the grounded theory, data percolation has also been found effective in social sciences, including in analyzing functional psychopathy.\n\nGrounded theory is \"shaped by the desire to discover social and psychological processes\" However grounded theory is not restricted to these two disciplines of study. As Gibbs points out, the process of grounded theory can be and has been applied to a number of different disciplines such as medicine, law, and economics to name a few. Grounded theory has gone global among the disciplines of nursing, business, and education and less so among other social-psychological-oriented disciplines such as social welfare, psychology, sociology, and art.\n\nGrounded theory focuses more on the procedure and not on the discipline. Rather than being limited to a particular discipline or form of data collection, grounded theory has been found useful across multiple research areas (Wells 1995). Here are some examples: \n\nThe benefits of using grounded theory include:\n\nEcological validity: Ecological validity is the extent to which research findings accurately represent real-world settings. Grounded theories are usually ecologically valid because they are similar to the data from which they were established. Although the constructs in a grounded theory are appropriately abstract (since their goal is to explain other similar phenomenon), they are context-specific, detailed, and tightly connected to the data.\n\nNovelty: Because grounded theories are not tied to any preexisting theory, grounded theories are often fresh and new and have the potential for innovative discoveries in science and other areas.\n\nParsimony: Parsimony involves using the simplest possible definition to explain complex phenomenon. Grounded theories aim to provide practical and simple explanations about complex phenomena by converting them into abstract constructs and hypothesizing their relationships. They offer helpful and relatively easy-to-remember layouts for us to understand our world a little bit better.\n\nGrounded theory has further significance because:\n\n\nGrounded theory methods have earned their place as a standard social research method and have influenced researchers from varied disciplines and professions.\n\nCritiques of grounded theory have focused on:\n\nThese criticisms are summed up by Thomas and James. These authors also suggest that it is impossible to free oneself of preconceptions in the collection and analysis of data in the way that Glaser and Strauss say is necessary. They also point to the formulaic nature of grounded theory method and the lack of congruence of this with open and creative interpretation – which ought to be the hallmark of qualitative inquiry. They suggest that the one element of grounded theory worth keeping is constant comparative method.\n\nGoldthorpe has put forth some criticisms of grounded theory as an effort to synthesize variables oriented as empirical studies and radical choice theory.\nGrounded theory allows for modifications in the formulated hypotheses at the start of the empirical research process. \nIn grounded theory, researchers engage in excessive conceptualization and defend this as \"sensitivity to context.\" Because of this, convergent conceptualization becomes impossible.[?] \nAs a result of these two arguments, grounded theory escapes the testing of theory. There is a very thin line between context and regularities. Goldthorpe supports this criticism in a review of three overlapping literatures: historical sociology, comparative macrosociology, and ethnography. On the one hand, historical sociology is good at analyzing long term processes of structural change, but on the other hand, its reliance on secondary sources opens several possibilities of bias. Comparative macro-sociology may be able to contextualize with reference to institutions and historical path-dependencies, but its focus on constellations of singular causal forces makes it difficult to break with long outdated mechanical models of reasoning. Ethnography may closely analyse actual mechanisms of interaction, but it doesn't provide acceptable knowledge about underlying generative processes, since it is unable to deal with variation within and across locales.\nGoldthorpe's core arguments are in terms of rational action theory and probabilistic statistical models.\nGrounded Theory can be reductive in the search for general patterns across a population, and even the selective coding process does not fully cover the contextual issues.\n\nThe grounded theory approach can be criticized as being empiricist; that it relies too heavily on the empirical data. Considers the fieldwork data as the source of its theories and sets itself against the use of the preceding theories\nStrauss's version of grounded theory has been criticized in several ways-\n\nGrounded theory method was developed in a period when other qualitative methods were often considered unscientific. It achieved wide acceptance of its academic rigor. Thus, especially in American academia, qualitative research is often equated to grounded theory method. This equation is sometimes criticized by qualitative researchers using other methodologies (for example, traditional ethnography, narratology, and storytelling).\n\nOne alternative to grounded theory is engaged theory. It puts an equal emphasis on doing on-the-ground work linked to analytical processes of empirical generalization. However, unlike grounded theory, engaged theory is in the critical theory tradition, locating those processes within a larger theoretical framework that specifies different levels of abstraction at which one can make claims about the world.\n\n\n\n\n\n\n\n"}
{"id": "4957558", "url": "https://en.wikipedia.org/wiki?curid=4957558", "title": "Hubble–Reynolds law", "text": "Hubble–Reynolds law\n\nThe Hubble–Reynolds law models the surface brightness of elliptical galaxies as\n\nWhere formula_2 is the surface brightness at radius formula_3, formula_4 is the central brightness, and formula_5 is the radius at which the surface brightness is diminished by a factor of 1/4. It is asymptotically similar to the De Vaucouleurs' law which is a special case of the Sersic profile for elliptical galaxies.\n\nThe law is named for the astronomers Edwin Hubble and John Henry Reynolds.\n"}
{"id": "53551430", "url": "https://en.wikipedia.org/wiki?curid=53551430", "title": "ISCB Fellow", "text": "ISCB Fellow\n\nISCB Fellowship is an award granted to scientists that the International Society for Computational Biology (ISCB) judges to have made “outstanding contributions to the fields of computational biology and bioinformatics”. , there are 64 Fellows of the ISCB including Michael Ashburner, Alex Bateman, Bonnie Berger, Steven E. Brenner, Janet Kelso, Daphne Koller, Michael Levitt, Sarah Teichmann and Shoshana Wodak. See List of Fellows of the International Society for Computational Biology for a comprehensive listing.\n\nThe first seven fellows of the ISCB were laureates of the ISCB Senior Scientist Award from to :\n\nSince 2009, new fellows have been nominated from the community of ISCB members and voted on annually by a selection committee. New fellows are traditionally inaugurated at the annual Intelligent Systems for Molecular Biology (ISMB) conference.\n"}
{"id": "166689", "url": "https://en.wikipedia.org/wiki?curid=166689", "title": "Interferometry", "text": "Interferometry\n\nInterferometry is a family of techniques in which waves, usually electromagnetic waves, are superimposed causing the phenomenon of interference in order to extract information. Interferometry is an important investigative technique in the fields of astronomy, fiber optics, engineering metrology, optical metrology, oceanography, seismology, spectroscopy (and its applications to chemistry), quantum mechanics, nuclear and particle physics, plasma physics, remote sensing, biomolecular interactions, surface profiling, microfluidics, mechanical stress/strain measurement, velocimetry, and optometry.\n\nInterferometers are widely used in science and industry for the measurement of small displacements, refractive index changes and surface irregularities. In most interferometers, light from a single source is split into two beams that travel different optical paths, then combined again to produce interference, however,\nunder the some circumstances two incoherent sources can also be interfered. The resulting interference fringes give information about the difference in optical path length. In analytical science, interferometers are used to measure lengths and the shape of optical components with nanometer precision; they are the highest precision length measuring instruments existing. In Fourier transform spectroscopy they are used to analyze light containing features of absorption or emission associated with a substance or mixture. An astronomical interferometer consists of two or more separate telescopes that combine their signals, offering a resolution equivalent to that of a telescope of diameter equal to the largest separation between its individual elements.\n\nInterferometry makes use of the principle of superposition to combine waves in a way that will cause the result of their combination to have some meaningful property that is diagnostic of the original state of the waves. This works because when two waves with the same frequency combine, the resulting intensity pattern is determined by the phase difference between the two waves—waves that are in phase will undergo constructive interference while waves that are out of phase will undergo destructive interference. Waves which are not completely in phase nor completely out of phase will have an intermediate intensity pattern, which can be used to determine their relative phase difference. Most interferometers use light or some other form of electromagnetic wave.\n\nTypically (see Fig. 1, the well-known Michelson configuration) a single incoming beam of coherent light will be split into two identical beams by a beam splitter (a partially reflecting mirror). Each of these beams travels a different route, called a path, and they are recombined before arriving at a detector. The path difference, the difference in the distance traveled by each beam, creates a phase difference between them. It is this introduced phase difference that creates the interference pattern between the initially identical waves. If a single beam has been split along two paths, then the phase difference is diagnostic of anything that changes the phase along the paths. This could be a physical change in the path length itself or a change in the refractive index along the path.\n\nAs seen in Fig. 2a and 2b, the observer has a direct view of mirror \"M\" seen through the beam splitter, and sees a reflected image \"M'\" of mirror \"M\". The fringes can be interpreted as the result of interference between light coming from the two virtual images \"S'\" and \"S'\" of the original source \"S\". The characteristics of the interference pattern depend on the nature of the light source and the precise orientation of the mirrors and beam splitter. In Fig. 2a, the optical elements are oriented so that \"S'\" and \"S'\" are in line with the observer, and the resulting interference pattern consists of circles centered on the normal to \"M\" and \"M'\". If, as in Fig. 2b, \"M\" and \"M'\" are tilted with respect to each other, the interference fringes will generally take the shape of conic sections (hyperbolas), but if \"M\" and \"M'\" overlap, the fringes near the axis will be straight, parallel, and equally spaced. If S is an extended source rather than a point source as illustrated, the fringes of Fig. 2a must be observed with a telescope set at infinity, while the fringes of Fig. 2b will be localized on the mirrors.\n\nUse of white light will result in a pattern of colored fringes (see Fig. 3). The central fringe representing equal path length may be light or dark depending on the number of phase inversions experienced by the two beams as they traverse the optical system. (See Michelson interferometer for a discussion of this.)\n\nInterferometers and interferometric techniques may be categorized by a variety of criteria:\n\nIn homodyne detection, the interference occurs between two beams at the same wavelength (or carrier frequency). The phase difference between the two beams results in a change in the intensity of the light on the detector. The resulting intensity of the light after mixing of these two beams is measured, or the pattern of interference fringes is viewed or recorded. Most of the interferometers discussed in this article fall into this category.\n\nThe heterodyne technique is used for (1) shifting an input signal into a new frequency range as well as (2) amplifying a weak input signal (assuming use of an active mixer). A weak input signal of frequency f is mixed with a strong reference frequency f from a local oscillator (LO). The nonlinear combination of the input signals creates two new signals, one at the sum f + f of the two frequencies, and the other at the difference f − f. These new frequencies are called heterodynes. Typically only one of the new frequencies is desired, and the other signal is filtered out of the output of the mixer. The output signal will have an intensity proportional to the product of the amplitudes of the input signals.\n\nThe most important and widely used application of the heterodyne technique is in the superheterodyne receiver (superhet), invented by U.S. engineer Edwin Howard Armstrong in 1918. In this circuit, the incoming radio frequency signal from the antenna is mixed with a signal from a local oscillator (LO) and converted by the heterodyne technique to a lower fixed frequency signal called the intermediate frequency (IF). This IF is amplified and filtered, before being applied to a detector which extracts the audio signal, which is sent to the loudspeaker.\n\nWhile optical heterodyne interferometry is usually done at a single point it is also possible to perform this widefield.\n\nA double path interferometer is one in which the reference beam and sample beam travel along divergent paths. Examples include the Michelson interferometer, the Twyman-Green interferometer, and the Mach-Zehnder interferometer. After being perturbed by interaction with the sample under test, the sample beam is recombined with the reference beam to create an interference pattern which can then be interpreted.\n\nA common path interferometer is a class of interferometer in which the reference beam and sample beam travel along the same path. Fig. 4 illustrates the Sagnac interferometer, the fibre optic gyroscope, the point diffraction interferometer, and the lateral shearing interferometer. Other examples of common path interferometer include the Zernike phase contrast microscope, Fresnel's biprism, the zero-area Sagnac, and the scatterplate interferometer.\n\nA wavefront splitting interferometer divides a light wavefront emerging from a point or a narrow slit (\"i.e.\" spatially coherent light) and, after allowing the two parts of the wavefront to travel through different paths, allows them to recombine. Fig. 5 illustrates Young's interference experiment and Lloyd's mirror. Other examples of wavefront splitting interferometer include the Fresnel biprism, the Billet Bi-Lens, and the Rayleigh interferometer.\n\nIn 1803, Young's interference experiment played a major role in the general acceptance of the wave theory of light. If white light is used in Young's experiment, the result is a white central band of constructive interference corresponding to equal path length from the two slits, surrounded by a symmetrical pattern of colored fringes of diminishing intensity. In addition to continuous electromagnetic radiation, Young's experiment has been performed with individual photons, with electrons, and with buckyball molecules large enough to be seen under an electron microscope.\n\nLloyd's mirror generates interference fringes by combining direct light from a source (blue lines) and light from the source's reflected image (red lines) from a mirror held at grazing incidence. The result is an asymmetrical pattern of fringes. The band of equal path length, nearest the mirror, is dark rather than bright. In 1834, Humphrey Lloyd interpreted this effect as proof that the phase of a front-surface reflected beam is inverted.\n\nAn amplitude splitting interferometer uses a partial reflector to divide the amplitude of the incident wave into separate beams which are separated and recombined. Fig. 6 illustrates the Fizeau, Mach–Zehnder and Fabry–Pérot interferometers. Other examples of amplitude splitting interferometer include the Michelson, Twyman–Green, Laser Unequal Path, and Linnik interferometer.\n\nThe Fizeau interferometer is shown as it might be set up to test an optical flat. A precisely figured reference flat is placed on top of the flat being tested, separated by narrow spacers. The reference flat is slightly beveled (only a fraction of a degree of beveling is necessary) to prevent the rear surface of the flat from producing interference fringes. Separating the test and reference flats allows the two flats to be tilted with respect to each other. By adjusting the tilt, which adds a controlled phase gradient to the fringe pattern, one can control the spacing and direction of the fringes, so that one may obtain an easily interpreted series of nearly parallel fringes rather than a complex swirl of contour lines. Separating the plates, however, necessitates that the illuminating light be collimated. Fig 6 shows a collimated beam of monochromatic light illuminating the two flats and a beam splitter allowing the fringes to be viewed on-axis.\n\nThe Mach–Zehnder interferometer is a more versatile instrument than the Michelson interferometer. Each of the well separated light paths is traversed only once, and the fringes can be adjusted so that they are localized in any desired plane. Typically, the fringes would be adjusted to lie in the same plane as the test object, so that fringes and test object can be photographed together. If it is decided to produce fringes in white light, then, since white light has a limited coherence length, on the order of micrometers, great care must be taken to equalize the optical paths or no fringes will be visible. As illustrated in Fig. 6, a compensating cell would be placed in the path of the reference beam to match the test cell. Note also the precise orientation of the beam splitters. The reflecting surfaces of the beam splitters would be oriented so that the test and reference beams pass through an equal amount of glass. In this orientation, the test and reference beams each experience two front-surface reflections, resulting in the same number of phase inversions. The result is that light traveling an equal optical path length in the test and reference beams produces a white light fringe of constructive interference.\n\nThe heart of the Fabry–Pérot interferometer is a pair of partially silvered glass optical flats spaced several millimeters to centimeters apart with the silvered surfaces facing each other. (Alternatively, a Fabry–Pérot \"etalon\" uses a transparent plate with two parallel reflecting surfaces.) As with the Fizeau interferometer, the flats are slightly beveled. In a typical system, illumination is provided by a diffuse source set at the focal plane of a collimating lens. A focusing lens produces what would be an inverted image of the source if the paired flats were not present; \"i.e.\" in the absence of the paired flats, all light emitted from point A passing through the optical system would be focused at point A'. In Fig. 6, only one ray emitted from point A on the source is traced. As the ray passes through the paired flats, it is multiply reflected to produce multiple transmitted rays which are collected by the focusing lens and brought to point A' on the screen. The complete interference pattern takes the appearance of a set of concentric rings. The sharpness of the rings depends on the reflectivity of the flats. If the reflectivity is high, resulting in a high Q factor (\"i.e.\" high finesse), monochromatic light produces a set of narrow bright rings against a dark background. In Fig. 6, the low-finesse image corresponds to a reflectivity of 0.04 (\"i.e.\" unsilvered surfaces) \"versus\" a reflectivity of 0.95 for the high-finesse image.\n\nMichelson and Morley (1887) and other early experimentalists using interferometric techniques in an attempt to measure the properties of the luminiferous aether, used monochromatic light only for initially setting up their equipment, always switching to white light for the actual measurements. The reason is that measurements were recorded visually. Monochromatic light would result in a uniform fringe pattern. Lacking modern means of environmental temperature control, experimentalists struggled with continual fringe drift even though the interferometer might be set up in a basement. Since the fringes would occasionally disappear due to vibrations by passing horse traffic, distant thunderstorms and the like, it would be easy for an observer to \"get lost\" when the fringes returned to visibility. The advantages of white light, which produced a distinctive colored fringe pattern, far outweighed the difficulties of aligning the apparatus due to its low coherence length. This was an early example of the use of white light to resolve the \"2 pi ambiguity\".\n\nIn physics, one of the most important experiments of the late 19th century was the famous \"failed experiment\" of Michelson and Morley which provided evidence for special relativity. Recent repetitions of the Michelson–Morley experiment perform heterodyne measurements of beat frequencies of crossed cryogenic optical resonators. Fig 7 illustrates a resonator experiment performed by Müller et al. in 2003. Two optical resonators constructed from crystalline sapphire, controlling the frequencies of two lasers, were set at right angles within a helium cryostat. A frequency comparator measured the beat frequency of the combined outputs of the two resonators. , the precision by which anisotropy of the speed of light can be excluded in resonator experiments is at the 10 level.\n\nMichelson interferometers are used in tunable narrow band optical filters and as the core hardware component of Fourier transform spectrometers.\n\nWhen used as a tunable narrow band filter, Michelson interferometers exhibit a number of advantages and disadvantages when compared with competing technologies such as Fabry–Pérot interferometers or Lyot filters. Michelson interferometers have the largest field of view for a specified wavelength, and are relatively simple in operation, since tuning is via mechanical rotation of waveplates rather than via high voltage control of piezoelectric crystals or lithium niobate optical modulators as used in a Fabry–Pérot system. Compared with Lyot filters, which use birefringent elements, Michelson interferometers have a relatively low temperature sensitivity. On the negative side, Michelson interferometers have a relatively restricted wavelength range and require use of prefilters which restrict transmittance.\n\nFig. 8 illustrates the operation of a Fourier transform spectrometer, which is essentially a Michelson interferometer with one mirror movable. (A practical Fourier transform spectrometer would substitute corner cube reflectors for the flat mirrors of the conventional Michelson interferometer, but for simplicity, the illustration does not show this.) An interferogram is generated by making measurements of the signal at many discrete positions of the moving mirror. A Fourier transform converts the interferogram into an actual spectrum.\n\nFig. 9 shows a doppler image of the solar corona made using a tunable Fabry-Pérot interferometer to recover scans of the solar corona at a number of wavelengths near the FeXIV green line. The picture is a color-coded image of the doppler shift of the line, which may be associated with the coronal plasma velocity towards or away from the satellite camera.\n\nFabry-Pérot thin-film etalons are used in narrow bandpass filters capable of selecting a single spectral line for imaging; for example, the H-alpha line or the Ca-K line of the Sun or stars. Fig. 10 shows an Extreme ultraviolet Imaging Telescope (EIT) image of the Sun at 195 Ångströms, corresponding to a spectral line of multiply-ionized iron atoms. EIT used multilayer coated reflective mirrors that were coated with alternate layers of a light \"spacer\" element (such as silicon), and a heavy \"scatterer\" element (such as molybdenum). Approximately 100 layers of each type were placed on each mirror, with a thickness of around 10 nm each. The layer thicknesses were tightly controlled so that at the desired wavelength, reflected photons from each layer interfered constructively.\n\nThe Laser Interferometer Gravitational-Wave Observatory (LIGO) uses two 4-km Michelson-Fabry-Pérot interferometers for the detection of gravitational waves. In this application, the Fabry–Pérot cavity is used to store photons for almost a millisecond while they bounce up and down between the mirrors. This increases the time a gravitational wave can interact with the light, which results in a better sensitivity at low frequencies. Smaller cavities, usually called mode cleaners, are used for spatial filtering and frequency stabilization of the main laser. The first observation of gravitational waves occurred on September 14, 2015.\n\nThe Mach-Zehnder interferometer's relatively large and freely accessible working space, and its flexibility in locating the fringes has made it the interferometer of choice for visualizing flow in wind tunnels, and for flow visualization studies in general. It is frequently used in the fields of aerodynamics, plasma physics and heat transfer to measure pressure, density, and temperature changes in gases.\n\nMach-Zehnder interferometers are also used to study one of the most counterintuitive predictions of quantum mechanics, the phenomenon known as quantum entanglement.\n\nAn astronomical interferometer achieves high-resolution observations using the technique of aperture synthesis, mixing signals from a cluster of comparatively small telescopes rather than a single very expensive monolithic telescope.\n\nEarly radio telescope interferometers used a single baseline for measurement. Later astronomical interferometers, such as the Very Large Array illustrated in Fig 11, used arrays of telescopes arranged in a pattern on the ground. A limited number of baselines will result in insufficient coverage. This was alleviated by using the rotation of the Earth to rotate the array relative to the sky. Thus, a single baseline could measure information in multiple orientations by taking repeated measurements, a technique called \"Earth-rotation synthesis\". Baselines thousands of kilometers long were achieved using very long baseline interferometry.\n\nAstronomical optical interferometry has had to overcome a number of technical issues not shared by radio telescope interferometry. The short wavelengths of light necessitate extreme precision and stability of construction. For example, spatial resolution of 1 milliarcsecond requires 0.5 µm stability in a 100 m baseline. Optical interferometric measurements require high sensitivity, low noise detectors that did not become available until the late 1990s. Astronomical \"seeing\", the turbulence that causes stars to twinkle, introduces rapid, random phase changes in the incoming light, requiring kilohertz data collection rates to be faster than the rate of turbulence. Despite these technical difficulties, roughly a dozen astronomical optical interferometers are now in operation offering resolutions down to the fractional milliarcsecond range. , a binary star system approximately 960 light-years (290 parsecs) away in the constellation Lyra, as observed by the CHARA array with the MIRC instrument. The brighter component is the primary star, or the mass donor. The fainter component is the thick disk surrounding the secondary star, or the mass gainer. The two components are separated by 1 milli-arcsecond. Tidal distortions of the mass donor and the mass gainer are both clearly visible.\n\nThe wave character of matter can be exploited to build interferometers. The first examples of matter interferometers were electron interferometers, later followed by neutron interferometers. Around 1990 the first atom interferometers were demonstrated, later followed by interferometers employing molecules.\n\nElectron holography is an imaging technique that photographically records the electron interference pattern of an object, which is then reconstructed to yield a greatly magnified image of the original object. This technique was developed to enable greater resolution in electron microscopy than is possible using conventional imaging techniques. The resolution of conventional electron microscopy is not limited by electron wavelength, but by the large aberrations of electron lenses.\n\nNeutron interferometry has been used to investigate the Aharonov–Bohm effect, to examine the effects of gravity acting on an elementary particle, and to demonstrate a strange behavior of fermions that is at the basis of the Pauli exclusion principle: Unlike macroscopic objects, when fermions are rotated by 360° about any axis, they do not return to their original state, but develop a minus sign in their wave function. In other words, a fermion needs to be rotated 720° before returning to its original state.\n\nAtom interferometry techniques are reaching sufficient precision to allow laboratory-scale tests of general relativity.\n\nInterferometers are used in atmospheric physics for high-precision measurements of trace gases via remote sounding of the atmosphere. There are several examples of interferometers that utilize either absorption or emission features of trace gases. A typical use would be in continual monitoring of the column concentration of trace gases such as ozone and carbon monoxide above the instrument.\n\nNewton (test plate) interferometry is frequently used in the optical industry for testing the quality of surfaces as they are being shaped and figured. Fig. 13 shows photos of reference flats being used to check two test flats at different stages of completion, showing the different patterns of interference fringes. The reference flats are resting with their bottom surfaces in contact with the test flats, and they are illuminated by a monochromatic light source. The light waves reflected from both surfaces interfere, resulting in a pattern of bright and dark bands. The surface in the left photo is nearly flat, indicated by a pattern of straight parallel interference fringes at equal intervals. The surface in the right photo is uneven, resulting in a pattern of curved fringes. Each pair of adjacent fringes represents a difference in surface elevation of half a wavelength of the light used, so differences in elevation can be measured by counting the fringes. The flatness of the surfaces can be measured to millionths of an inch by this method. To determine whether the surface being tested is concave or convex with respect to the reference optical flat, any of several procedures may be adopted. One can observe how the fringes are displaced when one presses gently on the top flat. If one observes the fringes in white light, the sequence of colors becomes familiar with experience and aids in interpretation. Finally one may compare the appearance of the fringes as one moves ones head from a normal to an oblique viewing position. These sorts of maneuvers, while common in the optical shop, are not suitable in a formal testing environment. When the flats are ready for sale, they will typically be mounted in a Fizeau interferometer for formal testing and certification.\n\nFabry-Pérot etalons are widely used in telecommunications, lasers and spectroscopy to control and measure the wavelengths of light. Dichroic filters are multiple layer thin-film etalons. In telecommunications, wavelength-division multiplexing, the technology that enables the use of multiple wavelengths of light through a single optical fiber, depends on filtering devices that are thin-film etalons. Single-mode lasers employ etalons to suppress all optical cavity modes except the single one of interest.\n\nThe Twyman–Green interferometer, invented by Twyman and Green in 1916, is a variant of the Michelson interferometer widely used to test optical components. The basic characteristics distinguishing it from the Michelson configuration are the use of a monochromatic point light source and a collimator. Michelson (1918) criticized the Twyman-Green configuration as being unsuitable for the testing of large optical components, since the light sources available at the time had limited coherence length. Michelson pointed out that constraints on geometry forced by limited coherence length required the use of a reference mirror of equal size to the test mirror, making the Twyman-Green impractical for many purposes. Decades later, the advent of laser light sources answered Michelson's objections. (A Twyman-Green interferometer using a laser light source and unequal path length is known as a Laser Unequal Path Interferometer, or LUPI.) Fig. 14 illustrates a Twyman-Green interferometer set up to test a lens. Light from a monochromatic point source is expanded by a diverging lens (not shown), then is collimated into a parallel beam. A convex spherical mirror is positioned so that its center of curvature coincides with the focus of the lens being tested. The emergent beam is recorded by an imaging system for analysis.\n\nMach-Zehnder interferometers are being used in integrated optical circuits, in which light interferes between two branches of a waveguide that are externally modulated to vary their relative phase. A slight tilt of one of the beam splitters will result in a path difference and a change in the interference pattern. Mach-Zehnder interferometers are the basis of a wide variety of devices, from RF modulators to sensors to optical switches.\n\nThe latest proposed extremely large astronomical telescopes, such as the Thirty Meter Telescope and the Extremely Large Telescope, will be of segmented design. Their primary mirrors will be built from hundreds of hexagonal mirror segments. Polishing and figuring these highly aspheric and non-rotationally symmetric mirror segments presents a major challenge. Traditional means of optical testing compares a surface against a spherical reference with the aid of a null corrector. In recent years, computer-generated holograms (CGHs) have begun to supplement null correctors in test setups for complex aspheric surfaces. Fig. 15 illustrates how this is done. Unlike the figure, actual CGHs have line spacing on the order of 1 to 10 µm. When laser light is passed through the CGH, the zero-order diffracted beam experiences no wavefront modification. The wavefront of the first-order diffracted beam, however, is modified to match the desired shape of the test surface. In the illustrated Fizeau interferometer test setup, the zero-order diffracted beam is directed towards the spherical reference surface, and the first-order diffracted beam is directed towards the test surface in such a way that the two reflected beams combine to form interference fringes. The same test setup can be used for the innermost mirrors as for the outermost, with only the CGH needing to be exchanged.\n\nRing laser gyroscopes (RLGs) and fibre optic gyroscopes (FOGs) are interferometers used in navigation systems. They operate on the principle of the Sagnac effect. The distinction between RLGs and FOGs is that in a RLG, the entire ring is part of the laser while in a FOG, an external laser injects counter-propagating beams into an optical fiber ring, and rotation of the system then causes a relative phase shift between those beams. In a RLG, the observed phase shift is proportional to the accumulated rotation, while in a FOG, the observed phase shift is proportional to the angular velocity.\n\nIn telecommunication networks, heterodyning is used to move frequencies of individual signals to different channels which may share a single physical transmission line. This is called frequency division multiplexing (FDM). For example, a coaxial cable used by a cable television system can carry 500 television channels at the same time because each one is given a different frequency, so they don't interfere with one another. Continuous wave (CW) doppler radar detectors are basically heterodyne detection devices that compare transmitted and reflected beams.\n\nOptical heterodyne detection is used for coherent Doppler lidar measurements capable of detecting very weak light scattered in the atmosphere and monitoring wind speeds with high accuracy. It has application in optical fiber communications, in various high resolution spectroscopic techniques, and the self-heterodyne method can be used to measure the linewidth of a laser.\n\nOptical heterodyne detection is an essential technique used in high-accuracy measurements of the frequencies of optical sources, as well as in the stabilization of their frequencies. Until a relatively few years ago, lengthy frequency chains were needed to connect the microwave frequency of a cesium or other atomic time source to optical frequencies. At each step of the chain, a frequency multiplier would be used to produce a harmonic of the frequency of that step, which would be compared by heterodyne detection with the next step (the output of a microwave source, far infrared laser, infrared laser, or visible laser). Each measurement of a single spectral line required several years of effort in the construction of a custom frequency chain. Currently, optical frequency combs have provided a much simpler method of measuring optical frequencies. If a mode-locked laser is modulated to form a train of pulses, its spectrum is seen to consist of the carrier frequency surrounded by a closely spaced comb of optical sideband frequencies with a spacing equal to the pulse repetition frequency (Fig. 16). The pulse repetition frequency is locked to that of the frequency standard, and the frequencies of the comb elements at the red end of the spectrum are doubled and heterodyned with the frequencies of the comb elements at the blue end of the spectrum, thus allowing the comb to serve as its own reference. In this manner, locking of the frequency comb output to an atomic standard can be performed in a single step. To measure an unknown frequency, the frequency comb output is dispersed into a spectrum. The unknown frequency is overlapped with the appropriate spectral segment of the comb and the frequency of the resultant heterodyne beats is measured.\n\nOne of the most common industrial applications of optical interferometry is as a versatile measurement tool for the high precision examination of surface topography. Popular interferometric measurement techniques include Phase Shifting Interferometry (PSI), and Vertical Scanning Interferometry(VSI), also known as scanning white light interferometry (SWLI) or by the ISO term Coherence Scanning Interferometry (CSI), CSI exploits coherence to extend the range of capabilities for interference microscopy. These techniques are widely used in micro-electronic and micro-optic fabrication. PSI uses monochromatic light and provides very precise measurements; however it is only usable for surfaces that are very smooth. CSI often uses white light and high numerical apertures, and rather than looking at the phase of the fringes, as does PSI, looks for best position of maximum fringe contrast or some other feature of the overall fringe pattern. In its simplest form, CSI provides less precise measurements than PSI but can be used on rough surfaces. Some configurations of CSI, variously known as Enhanced VSI (EVSI), high-resolution SWLI or Frequency Domain Analysis (FDA), use coherence effects in combination with interference phase to enhance precision.\n\nPhase Shifting Interferometry addresses several issues associated with the classical analysis of static interferograms. Classically, one measures the positions of the fringe centers. As seen in Fig. 13, fringe deviations from straightness and equal spacing provide a measure of the aberration. Errors in determining the location of the fringe centers provide the inherent limit to precision of the classical analysis, and any intensity variations across the interferogram will also introduce error. There is a trade-off between precision and number of data points: closely spaced fringes provide many data points of low precision, while widely spaced fringes provide a low number of high precision data points. Since fringe center data is all that one uses in the classical analysis, all of the other information that might theoretically be obtained by detailed analysis of the intensity variations in an interferogram is thrown away. Finally, with static interferograms, additional information is needed to determine the polarity of the wavefront: In Fig. 13, one can see that the tested surface on the right deviates from flatness, but one cannot tell from this single image whether this deviation from flatness is concave or convex. Traditionally, this information would be obtained using non-automated means, such as by observing the direction that the fringes move when the reference surface is pushed.\n\nPhase shifting interferometry overcomes these limitations by not relying on finding fringe centers, but rather by collecting intensity data from every point of the CCD image sensor. As seen in Fig. 17, multiple interferograms (at least three) are analyzed with the reference optical surface shifted by a precise fraction of a wavelength between each exposure using a piezoelectric transducer (PZT). Alternatively, precise phase shifts can be introduced by modulating the laser frequency. The captured images are processed by a computer to calculate the optical wavefront errors. The precision and reproducibility of PSI is far greater than possible in static interferogram analysis, with measurement repeatabilities of a hundredth of a wavelength being routine. Phase shifting technology has been adapted to a variety of interferometer types such as Twyman-Green, Mach–Zehnder, laser Fizeau, and even common path configurations such as point diffraction and lateral shearing interferometers. More generally, phase shifting techniques can be adapted to almost any system that uses fringes for measurement, such as holographic and speckle interferometry.\n\nIn coherence scanning interferometry, interference is only achieved when the path length delays of the interferometer are matched within the coherence time of the light source. CSI monitors the fringe contrast rather than the phase of the fringes. Fig. 17 illustrates a CSI microscope using a Mirau interferometer in the objective; other forms of interferometer used with white light include the Michelson interferometer (for low magnification objectives, where the reference mirror in a Mirau objective would interrupt too much of the aperture) and the Linnik interferometer (for high magnification objectives with limited working distance). The sample (or alternatively, the objective) is moved vertically over the full height range of the sample, and the position of maximum fringe contrast is found for each pixel. The chief benefit of coherence scanning interferometry is that systems can be designed that do not suffer from the 2 pi ambiguity of coherent interferometry, and as seen in Fig. 18, which scans a 180μm x 140μm x 10μm volume, it is well suited to profiling steps and rough surfaces. The axial resolution of the system is determined in part by the coherence length of the light source. Industrial applications include in-process surface metrology, roughness measurement, 3D surface metrology in hard-to-reach spaces and in hostile environments, profilometry of surfaces with high aspect ratio features (grooves, channels, holes), and film thickness measurement (semi-conductor and optical industries, etc.).\n\nFig. 19 illustrates a Twyman–Green interferometer set up for white light scanning of a macroscopic object.\n\nHolographic interferometry is a technique which uses holography to monitor small deformations in single wavelength implementations. In multi-wavelength implementations, it is used to perform dimensional metrology of large parts and assemblies and to detect larger surface defects.\n\nHolographic interferometry was discovered by accident as a result of mistakes committed during the making of holograms. Early lasers were relatively weak and photographic plates were insensitive, necessitating long exposures during which vibrations or minute shifts might occur in the optical system. The resultant holograms, which showed the holographic subject covered with fringes, were considered ruined.\n\nEventually, several independent groups of experimenters in the mid-60s realized that the fringes encoded important information about dimensional changes occurring in the subject, and began intentionally producing holographic double exposures. The main Holographic interferometry article covers the disputes over priority of discovery that occurred during the issuance of the patent for this method.\n\nDouble- and multi- exposure holography is one of three methods used to create holographic interferograms. A first exposure records the object in an unstressed state. Subsequent exposures on the same photographic plate are made while the object is subjected to some stress. The composite image depicts the difference between the stressed and unstressed states.\n\nReal-time holography is a second method of creating holographic interferograms. A holograph of the unstressed object is created. This holograph is illuminated with a reference beam to generate a hologram image of the object directly superimposed over the original object itself while the object is being subjected to some stress. The object waves from this hologram image will interfere with new waves coming from the object. This technique allows real time monitoring of shape changes.\n\nThe third method, time-average holography, involves creating a holograph while the object is subjected to a periodic stress or vibration. This yields a visual image of the vibration pattern.\n\nInterferometric synthetic aperture radar (InSAR) is a radar technique used in geodesy and remote sensing. Satellite synthetic aperture radar images of a geographic feature are taken on separate days, and changes that have taken place between radar images taken on the separate days are recorded as fringes similar to those obtained in holographic interferometry. The technique can monitor centimeter- to millimeter-scale deformation resulting from earthquakes, volcanoes and landslides, and also has uses in structural engineering, in particular for the monitoring of subsidence and structural stability. Fig 20 shows Kilauea, an active volcano in Hawaii. Data acquired using the space shuttle Endeavour's X-band Synthetic Aperture Radar on April 13, 1994 and October 4, 1994 were used to generate interferometric fringes, which were overlaid on the X-SAR image of Kilauea.\n\nElectronic speckle pattern interferometry (ESPI), also known as TV holography, uses video detection and recording to produce an image of the object upon which is superimposed a fringe pattern which represents the displacement of the object between recordings. (see Fig. 21) The fringes are similar to those obtained in holographic interferometry.\n\nWhen lasers were first invented, laser speckle was considered to be a severe drawback in using lasers to illuminate objects, particularly in holographic imaging because of the grainy image produced. It was later realized that speckle patterns could carry information about the object's surface deformations. Butters and Leendertz developed the technique of speckle pattern interferometry in 1970, and since then, speckle has been exploited in a variety of other applications. A photograph is made of the speckle pattern before deformation, and a second photograph is made of the speckle pattern after deformation. Digital subtraction of the two images results in a correlation fringe pattern, where the fringes represent lines of equal deformation. Short laser pulses in the nanosecond range can be used to capture very fast transient events. A phase problem exists: In the absence of other information, one cannot tell the difference between contour lines indicating a peak \"versus\" contour lines indicating a trough. To resolve the issue of phase ambiguity, ESPI may be combined with phase shifting methods.\n\nA method of establishing precise geodetic baselines, invented by Yrjö Väisälä, exploited the low coherence length of white light. Initially, white light was split in two, with the reference beam \"folded\", bouncing back-and-forth six times between a mirror pair spaced precisely 1 m apart. Only if the test path was precisely 6 times the reference path would fringes be seen. Repeated applications of this procedure allowed precise measurement of distances up to 864 meters. Baselines thus established were used to calibrate geodetic distance measurement equipment, leading to a metrologically traceable scale for geodetic networks measured by these instruments. (This method has been superseded by GPS.)\n\nOther uses of interferometers have been to study dispersion of materials, measurement of complex indices of refraction, and thermal properties. They are also used for three-dimensional motion mapping including mapping vibrational patterns of structures.\n\nOptical interferometry, applied to biology and medicine, provides sensitive metrology capabilities for the measurement of biomolecules, subcellular components, cells and tissues. Many forms of label-free biosensors rely on interferometry because the direct interaction of electromagnetic fields with local molecular polarizability eliminates the need for fluorescent tags or nanoparticle markers. At a larger scale, cellular interferometry shares aspects with phase-contrast microscopy, but comprises a much larger class of phase-sensitive optical configurations that rely on optical interference among cellular constituents through refraction and diffraction. At the tissue scale, partially-coherent forward-scattered light propagation through the micro aberrations and heterogeneity of tissue structure provides opportunities to use phase-sensitive gating (optical coherence tomography) as well as phase-sensitive fluctuation spectroscopy to image subtle structural and dynamical properties.\n\nOptical coherence tomography (OCT) is a medical imaging technique using low-coherence interferometry to provide tomographic visualization of internal tissue microstructures. As seen in Fig. 22, the core of a typical OCT system is a Michelson interferometer. One interferometer arm is focused onto the tissue sample and scans the sample in an X-Y longitudinal raster pattern. The other interferometer arm is bounced off a reference mirror. Reflected light from the tissue sample is combined with reflected light from the reference. Because of the low coherence of the light source, interferometric signal is observed only over a limited depth of sample. X-Y scanning therefore records one thin optical slice of the sample at a time. By performing multiple scans, moving the reference mirror between each scan, an entire three-dimensional image of the tissue can be reconstructed. Recent advances have striven to combine the nanometer phase retrieval of coherent interferometry with the ranging capability of low-coherence interferometry.\n\nPhase contrast and differential interference contrast (DIC) microscopy are important tools in biology and medicine. Most animal cells and single-celled organisms have very little color, and their intracellular organelles are almost totally invisible under simple bright field illumination. These structures can be made visible by staining the specimens, but staining procedures are time-consuming and kill the cells. As seen in Figs. 24 and 25, phase contrast and DIC microscopes allow unstained, living cells to be studied. DIC also has non-biological applications, for example in the analysis of planar silicon semiconductor processing.\n\nAngle-resolved low-coherence interferometry (a/LCI) uses scattered light to measure the sizes of subcellular objects, including cell nuclei. This allows interferometry depth measurements to be combined with density measurements. Various correlations have been found between the state of tissue health and the measurements of subcellular objects. For example, it has been found that as tissue changes from normal to cancerous, the average cell nuclei size increases.\n\nPhase-contrast X-ray imaging (Fig. 26) refers to a variety of techniques that use phase information of a coherent x-ray beam to image soft tissues. (For an elementary discussion, see Phase-contrast x-ray imaging (introduction). For a more in-depth review, see Phase-contrast X-ray imaging.) It has become an important method for visualizing cellular and histological structures in a wide range of biological and medical studies. There are several technologies being used for x-ray phase-contrast imaging, all utilizing different principles to convert phase variations in the x-rays emerging from an object into intensity variations. These include propagation-based phase contrast, talbot interferometry, moiré-based far-field interferometry, refraction-enhanced imaging, and x-ray interferometry. These methods provide higher contrast compared to normal absorption-contrast x-ray imaging, making it possible to see smaller details. A disadvantage is that these methods require more sophisticated equipment, such as synchrotron or microfocus x-ray sources, x-ray optics, or high resolution x-ray detectors.\n\n"}
{"id": "57495119", "url": "https://en.wikipedia.org/wiki?curid=57495119", "title": "James Scott Brown", "text": "James Scott Brown\n\nJames Scott Brown, also known as James Scott, is the head of ICIT, the Institute for Critical Infrastructure Technology, a nonprofit think tank in Washington, DC, a prolific self-published author, and the center of fake Twitter and Youtube accounts that promote his businesses.\n\nBrown is reported as a consultant to \"various intelligence agencies around the world\".\nCurrent and previous businesses include:\n\n\n"}
{"id": "877256", "url": "https://en.wikipedia.org/wiki?curid=877256", "title": "Johann Friedrich Wilhelm Herbst", "text": "Johann Friedrich Wilhelm Herbst\n\nJohann Friedrich Wilhelm Herbst (1 November 1743 – 5 November 1807) was a German naturalist and entomologist from Petershagen, Minden-Ravensberg. He served as a chaplain in the Prussian army. His marriage in Berlin, 1770, with Euphrosyne Luise Sophie (1742–1805), daughter of the Prussian \"Hofrat\" Libert Waldschmidt seems to have been childless.\n\nHe was the joint editor, with Carl Gustav Jablonsky, of \"Naturgeschichte der in- und ausländischen Insekten\" (1785–1806, 10 volumes), which was one of the first attempts at a complete survey of the order Coleoptera. Herbst's \"Naturgeschichte der Krabben und Krebse\", released in installments, was the first full survey of crustaceans.\n\nHerbst's other works included \"Anleitung zur Kenntnis der Insekten\" (1784–86, 3 volumes), \"Naturgeschichte der Krabben und Krebse\" (1782–1804, 3 volumes), \"Einleitung zur Kenntnis der Würmer\" (1787–88, 2 volumes) and \"Natursystem der ungeflügelten Insekten\" (Classification of the unwinged insects) (1797–1800, 4 parts).\n\n\n"}
{"id": "180855", "url": "https://en.wikipedia.org/wiki?curid=180855", "title": "Kalman filter", "text": "Kalman filter\n\nIn statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone, by estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. Kálmán, one of the primary developers of its theory.\n\nThe Kalman filter has numerous applications in technology. A common application is for guidance, navigation, and control of vehicles, particularly aircraft and spacecraft. Furthermore, the Kalman filter is a widely applied concept in time series analysis used in fields such as signal processing and econometrics. Kalman filters also are one of the main topics in the field of robotic motion planning and control, and they are sometimes included in trajectory optimization. The Kalman filter also works for modeling the central nervous system's control of movement. Due to the time delay between issuing motor commands and receiving sensory feedback, use of the Kalman filter supports a realistic model for making estimates of the current state of the motor system and issuing updated commands.\n\nThe algorithm works in a two-step process. In the prediction step, the Kalman filter produces estimates of the current state variables, along with their uncertainties. Once the outcome of the next measurement (necessarily corrupted with some amount of error, including random noise) is observed, these estimates are updated using a weighted average, with more weight being given to estimates with higher certainty. The algorithm is recursive. It can run in real time, using only the present input measurements and the previously calculated state and its uncertainty matrix; no additional past information is required.\n\nUsing a Kalman filter does not assume that the errors are Gaussian. However, the filter yields the exact conditional probability estimate in the special case that all errors are Gaussian.\n\nExtensions and generalizations to the method have also been developed, such as the extended Kalman filter and the unscented Kalman filter which work on nonlinear systems. The underlying model is similar to a hidden Markov model except that the state space of the latent variables is continuous and all latent and observed variables have Gaussian distributions.\n\nThe filter is named after Hungarian émigré Rudolf E. Kálmán, although Thorvald Nicolai Thiele and Peter Swerling developed a similar algorithm earlier. Richard S. Bucy of the University of Southern California contributed to the theory, leading to it sometimes being called the Kalman–Bucy filter.\nStanley F. Schmidt is generally credited with developing the first implementation of a Kalman filter. He realized that the filter could be divided into two distinct parts, with one part for time periods between sensor outputs and another part for incorporating measurements. It was during a visit by Kálmán to the NASA Ames Research Center that Schmidt saw the applicability of Kálmán's ideas to the nonlinear problem of trajectory estimation for the Apollo program leading to its incorporation in the Apollo navigation computer.\nThis Kalman filter was first described and partially developed in technical papers by Swerling (1958), Kalman (1960) and Kalman and Bucy (1961).\n\nKalman filters have been vital in the implementation of the navigation systems of U.S. Navy nuclear ballistic missile submarines, and in the guidance and navigation systems of cruise missiles such as the U.S. Navy's Tomahawk missile and the U.S. Air Force's Air Launched Cruise Missile. They are also used in the guidance and navigation systems of reusable launch vehicles and the attitude control and navigation systems of spacecraft which dock at the International Space Station.\n\nThis digital filter is sometimes called the \"Stratonovich–Kalman–Bucy filter\" because it is a special case of a more general, non-linear filter developed somewhat earlier by the Soviet mathematician Ruslan Stratonovich. In fact, some of the special case linear filter's equations appeared in these papers by Stratonovich that were published before summer 1960, when Kalman met with Stratonovich during a conference in Moscow.\n\nThe Kalman filter uses a system's dynamic model (e.g., physical laws of motion), known control inputs to that system, and multiple sequential measurements (such as from sensors) to form an estimate of the system's varying quantities (its state) that is better than the estimate obtained by using only one measurement alone. As such, it is a common sensor fusion and data fusion algorithm.\n\nNoisy sensor data, approximations in the equations that describe the system evolution, and external factors that are not accounted for all place limits on how well it is possible to determine the system's state. The Kalman filter deals effectively with the uncertainty due to noisy sensor data and to some extent also with random external factors. The Kalman filter produces an estimate of the state of the system as an average of the system's predicted state and of the new measurement using a weighted average. The purpose of the weights is that values with better (i.e., smaller) estimated uncertainty are \"trusted\" more. The weights are calculated from the covariance, a measure of the estimated uncertainty of the prediction of the system's state. The result of the weighted average is a new state estimate that lies between the predicted and measured state, and has a better estimated uncertainty than either alone. This process is repeated at every time step, with the new estimate and its covariance informing the prediction used in the following iteration. This means that the Kalman filter works recursively and requires only the last \"best guess\", rather than the entire history, of a system's state to calculate a new state.\n\nThe relative certainty of the measurements and current state estimate is an important consideration, and it is common to discuss the response of the filter in terms of the Kalman filter's \"gain\". The Kalman gain is the relative weight given to the measurements and current state estimate, and can be \"tuned\" to achieve particular performance. With a high gain, the filter places more weight on the most recent measurements, and thus follows them more responsively. With a low gain, the filter follows the model predictions more closely. At the extremes, a high gain close to one will result in a more jumpy estimated trajectory, while low gain close to zero will smooth out noise but decrease the responsiveness.\n\nWhen performing the actual calculations for the filter (as discussed below), the state estimate and covariances are coded into matrices to handle the multiple dimensions involved in a single set of calculations. This allows for a representation of linear relationships between different state variables (such as position, velocity, and acceleration) in any of the transition models or covariances.\n\nAs an example application, consider the problem of determining the precise location of a truck. The truck can be equipped with a GPS unit that provides an estimate of the position within a few meters. The GPS estimate is likely to be noisy; readings 'jump around' rapidly, though remaining within a few meters of the real position. In addition, since the truck is expected to follow the laws of physics, its position can also be estimated by integrating its velocity over time, determined by keeping track of wheel revolutions and the angle of the steering wheel. This is a technique known as dead reckoning. Typically, the dead reckoning will provide a very smooth estimate of the truck's position, but it will drift over time as small errors accumulate.\n\nIn this example, the Kalman filter can be thought of as operating in two distinct phases: predict and update. In the prediction phase, the truck's old position will be modified according to the physical laws of motion (the dynamic or \"state transition\" model). Not only will a new position estimate be calculated, but a new covariance will be calculated as well. Perhaps the covariance is proportional to the speed of the truck because we are more uncertain about the accuracy of the dead reckoning position estimate at high speeds but very certain about the position estimate when moving slowly. Next, in the update phase, a measurement of the truck's position is taken from the GPS unit. Along with this measurement comes some amount of uncertainty, and its covariance relative to that of the prediction from the previous phase determines how much the new measurement will affect the updated prediction. Ideally, as the dead reckoning estimates tend to drift away from the real position, the GPS measurement should pull the position estimate back towards the real position but not disturb it to the point of becoming rapidly jumping and noisy.\n\nThe Kalman filter is an efficient recursive filter that estimates the internal state of a linear dynamic system from a series of noisy measurements. It is used in a wide range of engineering and econometric applications from radar and computer vision to estimation of structural macroeconomic models, and is an important topic in control theory and control systems engineering. Together with the linear-quadratic regulator (LQR), the Kalman filter solves the linear–quadratic–Gaussian control problem (LQG). The Kalman filter, the linear-quadratic regulator and the linear–quadratic–Gaussian controller are solutions to what arguably are the most fundamental problems in control theory.\n\nIn most applications, the internal state is much larger (more degrees of freedom) than the few \"observable\" parameters which are measured. However, by combining a series of measurements, the Kalman filter can estimate the entire internal state.\n\nIn Dempster–Shafer theory, each state equation or observation is considered a special case of a linear belief function and the Kalman filter is a special case of combining linear belief functions on a join-tree or Markov tree. Additional approaches include belief filters which use Bayes or evidential updates to the state equations.\n\nA wide variety of Kalman filters have now been developed, from Kalman's original formulation, now called the \"simple\" Kalman filter, the Kalman–Bucy filter, Schmidt's \"extended\" filter, the information filter, and a variety of \"square-root\" filters that were developed by Bierman, Thornton and many others. Perhaps the most commonly used type of very simple Kalman filter is the phase-locked loop, which is now ubiquitous in radios, especially frequency modulation (FM) radios, television sets, satellite communications receivers, outer space communications systems, and nearly any other electronic communications equipment.\n\nThe Kalman filters are based on linear dynamical systems discretized in the time domain. They are modeled on a Markov chain built on linear operators perturbed by errors that may include Gaussian noise. The state of the system is represented as a vector of real numbers. At each discrete time increment, a linear operator is applied to the state to generate the new state, with some noise mixed in, and optionally some information from the controls on the system if they are known. Then, another linear operator mixed with more noise generates the observed outputs from the true (\"hidden\") state. The Kalman filter may be regarded as analogous to the hidden Markov model, with the key difference that the hidden state variables take values in a continuous space (as opposed to a discrete state space as in the hidden Markov model). There is a strong analogy between the equations of the Kalman Filter and those of the hidden Markov model. A review of this and other models is given in Roweis and Ghahramani (1999), and Hamilton (1994), Chapter 13.\n\nIn order to use the Kalman filter to estimate the internal state of a process given only a sequence of noisy observations, one must model the process in accordance with the framework of the Kalman filter. This means specifying the following matrices:\n\nThe Kalman filter model assumes the true state at time \"k\" is evolved from the state at (\"k\" − 1) according to\n\nwhere\n\nAt time \"k\" an observation (or measurement) z of the true state x is made according to\n\nwhere\n\nThe initial state, and the noise vectors at each step {x, w, …, w, v … v} are all assumed to be mutually independent.\n\nMany real dynamical systems do not exactly fit this model. In fact, unmodeled dynamics can seriously degrade the filter performance, even when it was supposed to work with unknown stochastic signals as inputs. The reason for this is that the effect of unmodeled dynamics depends on the input, and, therefore, can bring the estimation algorithm to instability (it diverges). On the other hand, independent white noise signals will not make the algorithm diverge. The problem of distinguishing between measurement noise and unmodeled dynamics is a difficult one and is treated in control theory under the framework of robust control.\n\nThe Kalman filter is a recursive estimator. This means that only the estimated state from the previous time step and the current measurement are needed to compute the estimate for the current state. In contrast to batch estimation techniques, no history of observations and/or estimates is required. In what follows, the notation formula_6 represents the estimate of formula_7 at time \"n\" given observations up to and including at time .\n\nThe state of the filter is represented by two variables:\n\nThe Kalman filter can be written as a single equation, however it is most often conceptualized as two distinct phases: \"Predict\" and \"Update\". The predict phase uses the state estimate from the previous timestep to produce an estimate of the state at the current timestep. This predicted state estimate is also known as the \"a priori\" state estimate because, although it is an estimate of the state at the current timestep, it does not include observation information from the current timestep. In the update phase, the current \"a priori\" prediction is combined with current observation information to refine the state estimate. This improved estimate is termed the \"a posteriori\" state estimate.\n\nTypically, the two phases alternate, with the prediction advancing the state until the next scheduled observation, and the update incorporating the observation. However, this is not necessary; if an observation is unavailable for some reason, the update may be skipped and multiple prediction steps performed. Likewise, if multiple independent observations are available at the same time, multiple update steps may be performed (typically with different observation matrices H).\n\nThe formula for the updated (\"a posteriori\") estimate covariance above is valid for any gain K and is sometimes called the Joseph form. For the optimal Kalman gain the formula further simplifies to formula_10, in which form it is most widely used in applications. However, one must keep in mind, that it is valid only for the optimal gain that minimizes the residual error. Proof of the formulae is found in the \"derivations\" section.\n\nIf the model is accurate, and the values for formula_11 and formula_12 accurately reflect the distribution of the initial state values, then the following invariants are preserved:\nwhere formula_14 is the expected value of formula_15. That is, all estimates have a mean error of zero.\n\nAlso:\nso covariance matrices accurately reflect the covariance of estimates.\n\nPractical implementation of the Kalman Filter is often difficult due to the difficulty of getting a good estimate of the noise covariance matrices Q and R. Extensive research has been done in this field to estimate these covariances from data. One practical approach to do this is the \"autocovariance least-squares (ALS)\" technique that uses the time-lagged autocovariances of routine operating data to estimate the covariances. The GNU Octave and Matlab code used to calculate the noise covariance matrices using the ALS technique is available online under the GNU General Public License.\n\nIt follows from theory that the Kalman filter is the optimal linear filter in cases where a) the model perfectly matches the real system, b) the entering noise is white (uncorrelated) and c) the covariances of the noise are exactly known. Several methods for the noise covariance estimation have been proposed during past decades, including ALS, mentioned in the section above. After the covariances are estimated, it is useful to evaluate the performance of the filter; i.e., whether it is possible to improve the state estimation quality. If the Kalman filter works optimally, the innovation sequence (the output prediction error) is a white noise, therefore the whiteness property of the innovations measures filter performance. Several different methods can be used for this purpose. If the noise terms are non-Gaussian distributed, methods for assessing performance of the filter estimate, which use probability inequalities or large-sample theory, are known in the literature.\n\nConsider a truck on frictionless, straight rails. Initially, the truck is stationary at position 0, but it is buffeted this way and that by random uncontrolled forces. We measure the position of the truck every Δ\"t\" seconds, but these measurements are imprecise; we want to maintain a model of where the truck is and what is its velocity. We show here how we derive the model from which we create our Kalman filter.\n\nSince formula_17 are constant, their time indices are dropped.\n\nThe position and velocity of the truck are described by the linear state space\n\nwhere formula_19 is the velocity, that is, the derivative of position with respect to time.\n\nWe assume that between the (\"k\" − 1) and \"k\" timestep uncontrolled forces cause a constant acceleration of \"a\" that is normally distributed, with mean 0 and standard deviation \"σ\". From Newton's laws of motion we conclude that\n\n(note that there is no formula_21 term since we have no known control inputs. Instead, we assume that \"a\" is the effect of an unknown input and formula_22 applies that effect to the state vector) where\n\nso that\n\nwhere\n\nPlease note that the matrix formula_26 is not full rank (it is of rank one if formula_27). Hence, the distribution formula_28 is not absolutely continuous and has no probability density function. Another way to express this, avoiding explicit degenerate distributions is given by\n\nAt each time step, a noisy measurement of the true position of the truck is made. Let us suppose the measurement noise \"v\" is also normally distributed, with mean 0 and standard deviation \"σ\".\n\nwhere\n\nand\n\nWe know the initial starting state of the truck with perfect precision, so we initialize\n\nand to tell the filter that we know the exact position and velocity, we give it a zero covariance matrix:\n\nIf the initial position and velocity are not known perfectly, the covariance matrix should be initialized with suitable variances on its diagonal:\n\nThe filter will then prefer the information from the first measurements over the information already in the model.\n\nStarting with our invariant on the error covariance P as above\n\nsubstitute in the definition of formula_37\n\nand substitute formula_39\n\nand formula_41\n\nand by collecting the error vectors we get\n\nSince the measurement error v is uncorrelated with the other terms, this becomes\n\nby the properties of vector covariance this becomes\n\nwhich, using our invariant on P and the definition of R becomes\n\nThis formula (sometimes known as the Joseph form of the covariance update equation) is valid for any value of K. It turns out that if K is the optimal Kalman gain, this can be simplified further as shown below.\n\nThe Kalman filter is a minimum mean-square error estimator. The error in the \"a posteriori\" state estimation is\n\nWe seek to minimize the expected value of the square of the magnitude of this vector, formula_48. This is equivalent to minimizing the trace of the \"a posteriori\" estimate covariance matrix formula_49. By expanding out the terms in the equation above and collecting, we get:\n\nThe trace is minimized when its matrix derivative with respect to the gain matrix is zero. Using the gradient matrix rules and the symmetry of the matrices involved we find that\n\nSolving this for K yields the Kalman gain:\n\nThis gain, which is known as the \"optimal Kalman gain\", is the one that yields MMSE estimates when used.\n\nThe formula used to calculate the \"a posteriori\" error covariance can be simplified when the Kalman gain equals the optimal value derived above. Multiplying both sides of our Kalman gain formula on the right by SK, it follows that\n\nReferring back to our expanded formula for the \"a posteriori\" error covariance,\nwe find the last two terms cancel out, giving\n\nThis formula is computationally cheaper and thus nearly always used in practice, but is only correct for the optimal gain. If arithmetic precision is unusually low causing problems with numerical stability, or if a non-optimal Kalman gain is deliberately used, this simplification cannot be applied; the \"a posteriori\" error covariance formula as derived above (Joseph form) must be used.\n\nThe Kalman filtering equations provide an estimate of the state formula_8 and its error covariance formula_9 recursively. The estimate and its quality depend on the system parameters and the noise statistics fed as inputs to the estimator. This section analyzes the effect of uncertainties in the statistical inputs to the filter. In the absence of reliable statistics or the true values of noise covariance matrices formula_58 and formula_59, the expression\n\nno longer provides the actual error covariance. In other words, formula_61. In most real-time applications, the covariance matrices that are used in designing the Kalman filter are different from the actual (true) noise covariances matrices. This sensitivity analysis describes the behavior of the estimation error covariance when the noise covariances as well as the system matrices formula_62 and formula_63 that are fed as inputs to the filter are incorrect. Thus, the sensitivity analysis describes the robustness (or sensitivity) of the estimator to misspecified statistical and parametric inputs to the estimator.\n\nThis discussion is limited to the error sensitivity analysis for the case of statistical uncertainties. Here the actual noise covariances are denoted by formula_64 and formula_65 respectively, whereas the design values used in the estimator are formula_66 and formula_59 respectively. The actual error covariance is denoted by formula_68 and formula_69 as computed by the Kalman filter is referred to as the Riccati variable. When formula_70 and formula_71, this means that formula_72. While computing the actual error covariance using formula_73, substituting for formula_74 and using the fact that formula_75 and formula_76, results in the following recursive equations for formula_68 :\n\nand\n\nWhile computing formula_69, by design the filter implicitly assumes that formula_81 and formula_82. Note that the recursive expressions for formula_68 and formula_69 are identical except for the presence of formula_85 and formula_86 in place of the design values formula_66 and formula_59 respectively. Researches have been done to analyze Kalman filter system's robustness.\n\nOne problem with the Kalman filter is its numerical stability. If the process noise covariance Q is small, round-off error often causes a small positive eigenvalue to be computed as a negative number. This renders the numerical representation of the state covariance matrix P indefinite, while its true form is positive-definite.\n\nPositive definite matrices have the property that they have a triangular matrix square root P = S·S. This can be computed efficiently using the Cholesky factorization algorithm, but more importantly, if the covariance is kept in this form, it can never have a negative diagonal or become asymmetric. An equivalent form, which avoids many of the square root operations required by the matrix square root yet preserves the desirable numerical properties, is the U-D decomposition form, P = U·D·U, where U is a unit triangular matrix (with unit diagonal), and D is a diagonal matrix.\n\nBetween the two, the U-D factorization uses the same amount of storage, and somewhat less computation, and is the most commonly used square root form. (Early literature on the relative efficiency is somewhat misleading, as it assumed that square roots were much more time-consuming than divisions, while on 21-st century computers they are only slightly more expensive.)\n\nEfficient algorithms for the Kalman prediction and update steps in the square root form were developed by G. J. Bierman and C. L. Thornton.\n\nThe L·D·L decomposition of the innovation covariance matrix S is the basis for another type of numerically efficient and robust square root filter. The algorithm starts with the LU decomposition as implemented in the Linear Algebra PACKage (LAPACK). These results are further factored into the L·D·L structure with methods given by Golub and Van Loan (algorithm 4.1.2) for a symmetric nonsingular matrix. Any singular covariance matrix is pivoted so that the first diagonal partition is nonsingular and well-conditioned. The pivoting algorithm must retain any portion of the innovation covariance matrix directly corresponding to observed state-variables H·x that are associated with auxiliary observations in\ny. The l·d·l square-root filter requires orthogonalization of the observation vector. This may be done with the inverse square-root of the covariance matrix for the auxiliary variables using Method 2 in Higham (2002, p. 263).\n\nThe Kalman filter can be presented as one of the simplest dynamic Bayesian networks. The Kalman filter calculates estimates of the true values of states recursively over time using incoming measurements and a mathematical process model. Similarly, recursive Bayesian estimation calculates estimates of an unknown probability density function (PDF) recursively over time using incoming measurements and a mathematical process model.\n\nIn recursive Bayesian estimation, the true state is assumed to be an unobserved Markov process, and the measurements are the observed states of a hidden Markov model (HMM).\n\nbecause of the Markov assumption, the true state is conditionally independent of all earlier states given the immediately previous state.\n\nSimilarly, the measurement at the \"k\"-th timestep is dependent only upon the current state and is conditionally independent of all other states given the current state.\n\nUsing these assumptions the probability distribution over all states of the hidden Markov model can be written simply as:\n\nHowever, when the Kalman filter is used to estimate the state x, the probability distribution of interest is that associated with the current states conditioned on the measurements up to the current timestep. This is achieved by marginalizing out the previous states and dividing by the probability of the measurement set.\n\nThis leads to the \"predict\" and \"update\" steps of the Kalman filter written probabilistically. The probability distribution associated with the predicted state is the sum (integral) of the products of the probability distribution associated with the transition from the (\"k\" − 1)-th timestep to the \"k\"-th and the probability distribution associated with the previous state, over all possible formula_92.\n\nThe measurement set up to time \"t\" is\n\nThe probability distribution of the update is proportional to the product of the measurement likelihood and the predicted state.\n\nThe denominator\nis a normalization term.\n\nThe remaining probability density functions are\n\nNote that the PDF at the previous timestep is inductively assumed to be the estimated state and covariance. This is justified because, as an optimal estimator, the Kalman filter makes best use of the measurements, therefore the PDF for formula_98 given the measurements formula_99 is the Kalman filter estimate.\n\nRelated to the recursive Bayesian interpretation described above, the Kalman filter can be viewed as a generative model, i.e., a process for \"generating\" a stream of random observations z = (z, z, z, …). Specifically, the process is\n\n\nNote that this process has identical structure to the hidden Markov model, except that the discrete state and observations are replaced with continuous variables sampled from Gaussian distributions.\n\nIn some applications, it is useful to compute the \"probability\" that a Kalman filter with a given set of parameters (prior distribution, transition and observation models, and control inputs) would generate a particular observed signal. This probability is known as the marginal likelihood because it integrates over (\"marginalizes out\") the values of the hidden state variables, so it can be computed using only the observed signal. The marginal likelihood can be useful to evaluate different parameter choices, or to compare the Kalman filter against other models using Bayesian model comparison.\n\nIt is straightforward to compute the marginal likelihood as a side effect of the recursive filtering computation. By the chain rule, the likelihood can be factored as the product of the probability of each observation given previous observations,\n\nand because the Kalman filter describes a Markov process, all relevant information from previous observations is contained in the current state estimate formula_110 Thus the marginal likelihood is given by\ni.e., a product of Gaussian densities, each corresponding to the density of one observation z under the current filtering distribution formula_112. This can easily be computed as a simple recursive update; however, to avoid numeric underflow, in a practical implementation it is usually desirable to compute the \"log\" marginal likelihood formula_113 instead. Adopting the convention formula_114, this can be done via the recursive update rule\nwhere formula_116 is the dimension of the measurement vector.\n\nAn important application where such a (log) likelihood of the observations (given the filter parameters) is used is multi-target tracking. For example, consider an object tracking scenario where a stream of observations is the input, however, it is unknown how many objects are in the scene (or, the number of objects is known but is greater than one). In such a scenario, it can be unknown apriori which observations/measurements were generated by which object. A multiple hypothesis tracker (MHT) typically will form different track association hypotheses, where each hypothesis can be viewed as a Kalman filter (in the linear Gaussian case) with a specific set of parameters associated with the hypothesized object. Thus, it is important to compute the likelihood of the observations for the different hypotheses under consideration, such that the most-likely one can be found.\n\nIn the information filter, or inverse covariance filter, the estimated covariance and estimated state are replaced by the information matrix and information vector respectively. These are defined as:\n\nSimilarly the predicted covariance and state have equivalent information forms, defined as:\n\nas have the measurement covariance and measurement vector, which are defined as:\n\nThe information update now becomes a trivial sum.\n\nThe main advantage of the information filter is that \"N\" measurements can be filtered at each timestep simply by summing their information matrices and vectors.\n\nTo predict the information filter the information matrix and vector can be converted back to their state space equivalents, or alternatively the information space prediction can be used.\n\nNote that if \"F\" and \"Q\" are time invariant these values can be cached. Note also that \"F\" and \"Q\" need to be invertible.\n\nThe optimal fixed-lag smoother provides the optimal estimate of formula_123 for a given fixed-lag formula_124 using the measurements from formula_125 to formula_41. It can be derived using the previous theory via an augmented state, and the main equation of the filter is the following:\n\nwhere:\n\nIf the estimation error covariance is defined so that\n\nthen we have that the improvement on the estimation of formula_138 is given by:\n\nThe optimal fixed-interval smoother provides the optimal estimate of formula_140 (formula_141) using the measurements from a fixed interval formula_125 to formula_143. This is also called \"Kalman Smoothing\". There are several smoothing algorithms in common use.\n\nThe Rauch–Tung–Striebel (RTS) smoother is an efficient two-pass algorithm for fixed interval smoothing.\n\nThe forward pass is the same as the regular Kalman filter algorithm. These \"filtered\" a-priori and a-posteriori state estimates formula_144, formula_37 and covariances formula_146, formula_69 are saved for use in the backwards pass.\n\nIn the backwards pass, we compute the \"smoothed\" state estimates formula_148 and covariances formula_149. We start at the last time step and proceed backwards in time using the following recursive equations:\nwhere\n\nNote that formula_153 is the a-posteriori state estimate of timestep formula_154 and formula_155 is the a-priori state estimate of timestep formula_156. The same notation applies to the covariance.\n\nAn alternative to the RTS algorithm is the modified Bryson–Frazier (MBF) fixed interval smoother developed by Bierman. This also uses a backward pass that processes data saved from the Kalman filter forward pass. The equations for the backward pass involve the recursive\ncomputation of data which are used at each observation time to compute the smoothed state and covariance.\n\nThe recursive equations are\n\nwhere formula_158 is the residual covariance and formula_159. The smoothed state and covariance can then be found by substitution in the equations\n\nor\n\nAn important advantage of the MBF is that it does not require finding the inverse of the covariance matrix.\n\nThe minimum-variance smoother can attain the best-possible error performance, provided that the models are linear, their parameters and the noise statistics are known precisely. This smoother is a time-varying state-space generalization of the optimal non-causal Wiener filter.\n\nThe smoother calculations are done in two passes. The forward calculations involve a one-step-ahead predictor and are given by\n\nThe above system is known as the inverse Wiener-Hopf factor. The backward recursion is the adjoint of the above forward system. The result of the backward pass formula_164 may be calculated by operating the forward equations on the time-reversed formula_165 and time reversing the result. In the case of output estimation, the smoothed estimate is given by\n\nTaking the causal part of this minimum-variance smoother yields\n\nwhich is identical to the minimum-variance Kalman filter. The above solutions minimize the variance of the output estimation error. Note that the Rauch–Tung–Striebel smoother derivation assumes that the underlying distributions are Gaussian, whereas the minimum-variance solutions do not. Optimal smoothers for state estimation and input estimation can be constructed similarly.\n\nA continuous-time version of the above smoother is described in.\n\nExpectation-maximization algorithms may be employed to calculate approximate maximum likelihood estimates of unknown state-space parameters within minimum-variance filters and smoothers. Often uncertainties remain within problem assumptions. A smoother that accommodates uncertainties can be designed by adding a positive definite term to the Riccati equation.\n\nIn cases where the models are nonlinear, step-wise linearizations may be within the minimum-variance filter and smoother recursions (extended Kalman filtering).\n\nPioneering research on the perception of sounds at different frequencies was conducted by Fletcher and Munson in the 1930s. Their work led to a standard way of weighting measured sound levels within investigations of industrial noise and hearing loss. Frequency weightings have since been used within filter and controller designs to manage performance within bands of interest.\n\nTypically, a frequency shaping function is used to weight the average power of the error spectral density in a specified frequency band. Let formula_168 denote the output estimation error exhibited by a conventional Kalman filter. Also, let formula_169 denote a causal frequency weighting transfer function. The optimum solution which minimizes the variance of formula_170 arises by simply constructing formula_171.\n\nThe design of formula_169 remains an open question. One way of proceeding is to identify a system which generates the estimation error and setting formula_169 equal to the inverse of that system. This procedure may be iterated to obtain mean-square error improvement at the cost of increased filter order. The same technique can be applied to smoothers.\n\nThe basic Kalman filter is limited to a linear assumption. More complex systems, however, can be nonlinear. The non-linearity can be associated either with the process model or with the observation model or with both.\n\nIn the extended Kalman filter (EKF), the state transition and observation models need not be linear functions of the state but may instead be non-linear functions. These functions are of differentiable type.\n\nThe function \"f\" can be used to compute the predicted state from the previous estimate and similarly the function \"h\" can be used to compute the predicted measurement from the predicted state. However, \"f\" and \"h\" cannot be applied to the covariance directly. Instead a matrix of partial derivatives (the Jacobian) is computed.\n\nAt each timestep the Jacobian is evaluated with current predicted states. These matrices can be used in the Kalman filter equations. This process essentially linearizes the non-linear function around the current estimate.\n\nWhen the state transition and observation models—that is, the predict and update functions formula_176 and formula_177—are highly non-linear, the extended Kalman filter can give particularly poor performance. This is because the covariance is propagated through linearization of the underlying non-linear model. The unscented Kalman filter (UKF)  uses a deterministic sampling technique known as the unscented transform (UT) to pick a minimal set of sample points (called sigma points) around the mean. The sigma points are then propagated through the non-linear functions, from which a new mean and covariance estimate are then formed. The resulting filter depends on how the transformed statistics of the UT are calculated and which set of sigma points are used—it should be remarked that is always possible to construct new UKFs in a consistent way. For certain systems, the resulting UKF filter more accurately estimates the true mean and covariance. This can be verified with Monte Carlo sampling or Taylor series expansion of the posterior statistics. In addition, this technique removes the requirement to explicitly calculate Jacobians, which for complex functions can be a difficult task in itself (i.e., requiring complicated derivatives if done analytically or being computationally costly if done numerically), if not impossible (if those functions are not differentiable).\n\n\nAs with the EKF, the UKF prediction can be used independently from the UKF update, in combination with a linear (or indeed EKF) update, or vice versa.\n\nThe estimated state and covariance are augmented with the mean and covariance of the process noise.\n\nA set of 2\"L\" + 1 sigma points is derived from the augmented state and covariance where \"L\" is the dimension of the augmented state.\n\nwhere\n\nis the \"i\"th column of the matrix square root of\n\nusing the definition: square root formula_182 of matrix formula_183 satisfies\n\nThe matrix square root should be calculated using numerically efficient and stable methods such as the Cholesky decomposition.\n\nThe sigma points are propagated through the transition function \"f\".\n\nwhere formula_186. The weighted sigma points are recombined to produce the predicted state and covariance.\n\nwhere the weights for the state and covariance are given by:\n\nformula_189 and formula_190 control the spread of the sigma points. formula_191 is related to the distribution of formula_192.\n\nAppropriate values depend on the problem at hand, but a typical recommendation is formula_193, formula_194 and formula_195. However, a larger value of formula_189 (e.g., formula_197) may be beneficial in order to better capture the spread of the distribution and possible non-linearities, as discussed in, where a detailed inspection of how to choose sigma points can be found. If the true distribution of formula_192 is Gaussian, formula_195 is optimal.\n\n\nThe predicted state and covariance are augmented as before, except now with the mean and covariance of the measurement noise.\n\nformula_200\n\nAs before, a set of 2\"L\" + 1 sigma points is derived from the augmented state and covariance where \"L\" is the dimension of the augmented state.\n\nAlternatively if the UKF prediction has been used the sigma points themselves can be augmented along the following lines\n\nwhere\n\nThe sigma points are projected through the observation function \"h\".\n\nThe weighted sigma points are recombined to produce the predicted measurement and predicted measurement covariance.\n\nThe state-measurement cross-covariance matrix,\n\nis used to compute the UKF Kalman gain.\n\nAs with the Kalman filter, the updated state is the predicted state plus the innovation weighted by the Kalman gain,\n\nAnd the updated covariance is the predicted covariance, minus the predicted measurement covariance, weighted by the Kalman gain.\n\nThe Kalman–Bucy filter (named after Richard Snowden Bucy) is a continuous time version of the Kalman filter.\n\nIt is based on the state space model\n\nwhere formula_211 and formula_212 represent the intensities (or, more accurately: the Power Spectral Density - PSD - matrices) of the two white noise terms formula_213 and formula_214, respectively.\n\nThe filter consists of two differential equations, one for the state estimate and one for the covariance:\n\nwhere the Kalman gain is given by\n\nNote that in this expression for formula_217 the covariance of the observation noise formula_212 represents at the same time the covariance of the prediction error (or \"innovation\") formula_219; these covariances are equal only in the case of continuous time.\n\nThe distinction between the prediction and update steps of discrete-time Kalman filtering does not exist in continuous time.\n\nThe second differential equation, for the covariance, is an example of a Riccati equation.\n\nMost physical systems are represented as continuous-time models while discrete-time measurements are frequently taken for state estimation via a digital processor. Therefore, the system model and measurement model are given by\nwhere\n\n\n\nThe prediction equations are derived from those of continuous-time Kalman filter without update from measurements, i.e., formula_224. The predicted state and covariance are calculated respectively by solving a set of differential equations with the initial value equal to the estimate at the previous step.\n\n\nThe update equations are identical to those of the discrete-time Kalman filter.\n\nThe traditional Kalman filter has also been employed for the recovery of sparse, possibly dynamic, signals from\nnoisy observations. Recent works utilize notions from the theory of compressed sensing/sampling, such as the restricted isometry property and related probabilistic recovery arguments, for sequentially estimating the sparse state in intrinsically low-dimensional systems.\n\n\n"}
{"id": "1792884", "url": "https://en.wikipedia.org/wiki?curid=1792884", "title": "Keraunography", "text": "Keraunography\n\nKeraunography or keranography refers to the belief that lightning, when striking an object (generally a human body), can leave markings which constitute a photographic image of surrounding objects. It is generally considered a myth.\n\nLike most folklore, it is impossible to trace the origins of keraunography. However, it seems to have attracted scientific and media attention in England in the early 19th century, and by Victorian times the term \"keraunography\" had been coined to describe numerous unconnected events. With increasing scientific understanding of electricity and the popularity of photography, the time was right in the 19th century for keraunography, which seems to combine both concepts, to enter the public consciousness. However, it is likely that anecdotal accounts of keraunography had existed long before there was a word for it.\n\nLightning, being a dramatic and often-seen (yet mysterious and poorly understood) aspect of nature, has since ancient times been the subject of mythology and folklore regarding its origins, effects, and various ways to ward it off. As mentioned previously, the 19th century were a breeding ground for myths about electricity and photography. Sympsychography, by which brain waves are used to produce a photographic image, was a deliberate joke, but no less taken as truth by members of the public. Also possibly related to keraunography is the mistaken belief that the last image a dying person sees is burned into their retina, like a photograph taken at the time of death.\n\nAlthough to some degree science has still not fully explained all the behaviours of lightning, very few people currently accept keraunography as truth. It is evident that lightning strikes do indeed produce burn marks, and like any basically random shape (clouds, birthmarks, inkbots, etc.) it is human nature to see shapes in them. The lightning often leaves skin marks in characteristic Lichtenberg figures, sometimes called \"lightning flowers\"; they may persist for hours or days, and are a useful indicator for medical examiners when trying to determine the cause of death. Although humans being struck by lightning is of course rare, it is nonetheless possible that over a wide period of time, certain cases of burn marks would exist which could be said to resemble objects nearby the point of the lightning strike. However, these cases are almost certainly the product of coincidence rather than evidence of any photographic property of lightning. No supposed case of keraunography has been investigated by modern science, and unless further evidence is presented, it remains a strange object of 19th-century British folklore.\n\n"}
{"id": "22179816", "url": "https://en.wikipedia.org/wiki?curid=22179816", "title": "List of 21st-century earthquakes", "text": "List of 21st-century earthquakes\n\nThe following is a list of significant earthquakes during the 21st century, listing earthquakes of magnitude 7 and above, or which caused fatalities. Deaths due to earthquake-caused tsunamis are included. In terms of fatalities, the 2010 Haiti earthquake was the most destructive event, followed by the 2004 Indian Ocean earthquake, 2005 Pakistan earthquake and 2008 Sichuan earthquake.\n\nFor lists of earthquakes by country, which may include smaller and less destructive events than those listed here, see Lists of earthquakes by country. \n\nNotes\n\n\n"}
{"id": "12243838", "url": "https://en.wikipedia.org/wiki?curid=12243838", "title": "List of Macromedia software", "text": "List of Macromedia software\n\nThe following is a list of Adobe Systems products integrated from Macromedia .\n\nFor a list of current product line of Adobe products is List of Adobe software\n"}
{"id": "14485862", "url": "https://en.wikipedia.org/wiki?curid=14485862", "title": "List of members of the National Academy of Sciences (Medical genetics, hematology, and oncology)", "text": "List of members of the National Academy of Sciences (Medical genetics, hematology, and oncology)\n"}
{"id": "15951109", "url": "https://en.wikipedia.org/wiki?curid=15951109", "title": "List of space telescopes", "text": "List of space telescopes\n\nThis list of space telescopes (astronomical space observatories) is grouped by major frequency ranges: gamma ray, x-ray, ultraviolet, visible, infrared, microwave and radio. Telescopes that work in multiple frequency bands are included in all of the appropriate sections. Space telescopes that collect particles, such as cosmic ray nuclei and/or electrons, as well as instruments that aim to detect gravitational waves, are also listed. Missions with specific targets within the Solar System (e.g. our Sun and its planets), are excluded; see List of Solar System probes for these, and List of Earth observation satellites for missions targeting our planet.\nTwo values are provided for the dimensions of the initial orbit. For telescopes in Earth orbit, the min and max altitude are given in kilometers. For telescopes in solar orbit, the minimum distance (periapsis) and the maximum distance (apoapsis) between the telescope and the center of mass of the sun are given in astronomical units (AU).\n\nGamma ray telescopes collect and measure individual, high energy gamma rays from astrophysical sources. These are absorbed by the atmosphere, requiring that observations are done by high-altitude balloons or space missions. Gamma rays can be generated by supernovae, neutron stars, pulsars and black holes. Gamma ray bursts, with extremely high energies, have also been detected but have yet to be identified.\n\nX-ray telescopes measure high-energy photons called X-rays. These can not travel a long distance through the atmosphere, meaning that they can only be observed high in the atmosphere or in space. Several types of astrophysical objects emit X-rays, from galaxy clusters, through black holes in active galactic nuclei to galactic objects such as supernova remnants, stars, and binary stars containing a white dwarf (cataclysmic variable stars), neutron star or black hole (X-ray binaries). Some Solar System bodies emit X-rays, the most notable being the Moon, although most of the X-ray brightness of the Moon arises from reflected solar X-rays. A combination of many unresolved X-ray sources is thought to produce the observed X-ray background.\n\nUltraviolet telescopes make observations at ultraviolet wavelengths, i.e. between approximately 10 and 320 nm. Light at these wavelengths is absorbed by the Earth's atmosphere, so observations at these wavelengths must be performed from the upper atmosphere or from space. Objects emitting ultraviolet radiation include the Sun, other stars and galaxies.\nUV ranges listed at Ultraviolet astronomy#Ultraviolet space telescopes.\n\nThe oldest form of astronomy, optical or visible-light astronomy extends from approximately 400 to 700 nm. Positioning an optical telescope in space means that the telescope does not see any atmospheric effects (see astronomical seeing), providing higher resolution images. Optical telescopes are used to look at stars, galaxies, planetary nebulae and protoplanetary disks, amongst many other things.\n\nInfrared light is of lower energy than visible light, hence is emitted by sources that are either cooler, or moving away from the observer (in present context: Earth) at high speed. As such, the following can be viewed in the infrared: cool stars (including brown dwarves), nebulae, and redshifted galaxies.\n\nMicrowave space telescopes have primarily been used to measure cosmological parameters from the Cosmic Microwave Background. They also measure synchrotron radiation, free-free emission and spinning dust from our Galaxy, as well as extragalactic compact sources and galaxy clusters through the Sunyaev-Zel'dovich effect.\n\nAs the atmosphere is transparent for radio waves, radio telescopes in space are of most use for Very Long Baseline Interferometry; doing simultaneous observations of a source with both a satellite and a ground-based telescope and by correlating their signals to simulate a radio telescope the size of the separation between the two telescopes. Observations can be of supernova remnants, masers, gravitational lenses, starburst galaxies, and many other things.\n\nSpacecraft and space-based modules that do particle detection, looking for cosmic rays and electrons. These can be emitted by the sun (Solar Energetic Particles), our galaxy (Galactic cosmic rays) and extragalactic sources (Extragalactic cosmic rays). There are also Ultra-high-energy cosmic rays from active galactic nuclei, those can be detected by ground-based detectors via their particle showers.\nA type of telescope that detects gravitational waves; ripples in space-time generated by colliding neutron stars or black holes.\nList of proposed space observatories: examples of past and present space observatory plans, concepts, and proposals. For observatories in orbit see, List of space telescopes.\n\n"}
{"id": "33194835", "url": "https://en.wikipedia.org/wiki?curid=33194835", "title": "Logarithmic Schrödinger equation", "text": "Logarithmic Schrödinger equation\n\nIn theoretical physics, the logarithmic Schrödinger equation (sometimes abbreviated as LNSE or LogSE) is one of the nonlinear modifications of Schrödinger's equation. It is a classical wave equation with applications to extensions of quantum mechanics, quantum optics, nuclear physics, transport and diffusion phenomena, open quantum systems and information theory, effective quantum gravity and physical vacuum models and theory of superfluidity and Bose–Einstein condensation. \nIts relativistic version (with D'Alembertian instead of Laplacian and first-order time derivative) was first proposed by G. Rosen.\nIt is an example of an integrable model.\n\nThe logarithmic Schrödinger equation is the partial differential equation. In mathematics and mathematical physics one often uses its dimensionless form:\n\nfor the complex-valued function \"ψ\" = \"ψ\"(x, \"t\") of the particles position vector x = (\"x\", \"y\", \"z\") at time \"t\", and\n\nis the Laplacian of \"ψ\" in Cartesian coordinates. \n\nThe relativistic version of this equation can be obtained by replacing the derivative operator with the D'Alembertian, similarly to the Klein–Gordon equation.\n\n"}
{"id": "37207", "url": "https://en.wikipedia.org/wiki?curid=37207", "title": "Nuclear engineering", "text": "Nuclear engineering\n\nNuclear engineering is the branch of engineering concerned with the application of breaking down atomic nuclei (fission) or of combining atomic nuclei (fusion), or with the application of other sub-atomic processes based on the principles of nuclear physics. In the sub-field of nuclear fission, it particularly includes the design, interaction, and maintenance of systems and components like nuclear reactors, nuclear power plants, or nuclear weapons. The field also includes the study of medical and other applications of radiation, particularly Ionizing radiation, nuclear safety, heat/thermodynamics transport, nuclear fuel, or other related technology (e.g., radioactive waste disposal) and the problems of nuclear proliferation.\n\nThe United States currently generates about 18% of its electricity from nuclear power plants. Nuclear engineers in this field generally work, directly or indirectly, in the nuclear power industry or for national laboratories. Current research in the industry is directed at producing economical and proliferation-resistant reactor designs with passive safety features. Some government (national) labs provide research in the same areas as private industry and in other areas such as nuclear fuels and nuclear fuel cycles, advanced reactor designs, and nuclear weapon design and maintenance. A principal pipeline/source of trained personnel (both military and civilian) for US reactor facilities is the US Navy Nuclear Power Program, including its Nuclear Power School in South Carolina. Employment in nuclear engineering is predicted to grow about nine percent to year 2022 as needed to replace retiring nuclear engineers, provide maintenance and updating of safety systems in power plants, and to advance the applications of nuclear medicine.\n\nMedical physics is an important field of nuclear medicine; its sub-fields include nuclear medicine, radiation therapy, health physics, and diagnostic imaging. Highly specialized and intricately operating equipment, including x-ray machines, MRI and PET scanners and many other devices provide most of modern medicine's diagnostic capability—along with disclosing subtle treatment options. \n\nNuclear materials research focuses on two main subject areas, nuclear fuels and irradiation-induced modification of nuclear materials. Improvement of nuclear fuels is crucial for obtaining increased efficiency from nuclear reactors. Irradiation effects studies have many purposes, including studying structural changes to reactor components and studying nano-modification of metals using ion-beams or particle accelerators.\n\nRadiation measurement is fundamental to the science and practice of radiation protection, sometimes known as radiological protection, which is the protection of people and the environment from the harmful effects of uncontrolled radiation. \n\nNuclear engineers and radiological scientists are interested in developing more advanced systems, and using these advances to improve imaging technologies; these areas include detector design, fabrication and analysis, measurements of fundamental atomic and nuclear parameters, and radiation imaging systems, among others.\n\n\n\n"}
{"id": "17272739", "url": "https://en.wikipedia.org/wiki?curid=17272739", "title": "Participatory modeling", "text": "Participatory modeling\n\nParticipatory modeling is a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representation(s) of reality. In this process, the participants co-formulate the problem and use modeling practices to aid in the description, solution, and decision-making actions of the group. Participatory modeling is often used in environmental and resource management contexts. It can be described as engaging non-scientists in the scientific process. The participants structure the problem, describe the system, create a computer model of the system, use the model to test policy interventions, and propose one or more solutions. Participatory modeling is often used in natural resources management, such as forests or water.\n\nThere are numerous benefits from this type of modeling, including a high degree of ownership and motivation towards change for the people involved in the modeling process. There are two approaches which provide highly different goals for the modeling; continuous modeling and conference modeling.\n\nThe focus here is on the end-user being the active modeler. It can be incorporated into an adaptable, context-sensitive, \"intelligent\" system, which is suited to the individual. This combination is often referred to as \"model generated workplace\" or \"model generated user environment\". The basic concept is that the end-user potentially has the greatest domain knowledge and thus the organization as a whole benefits by obtaining and externalize this knowledge.\n\nConference modeling is an approach where the goal often is of a more social kind, such as motivation, and change management. The idea is to involve a group of diversified people from the domain in question. Then the modeling process is developed in group participation during a fixed period of time.\n\n"}
{"id": "725821", "url": "https://en.wikipedia.org/wiki?curid=725821", "title": "Precalculus", "text": "Precalculus\n\nIn mathematics education, precalculus is a course that includes algebra and trigonometry at a level which is designed to prepare students for the study of calculus. Schools often distinguish between algebra and trigonometry as two separate parts of the coursework.\n\nFor students to succeed at finding the derivatives and antiderivatives of calculus, they will need facility with algebraic expressions, particularly in modification and transformation of such expressions. Leonhard Euler wrote the first precalculus book in 1748 called Introduction to the Analysis of the Infinite, which \"was meant as a survey of concepts and methods in analysis and analytic geometry preliminary to the study of differential and integral calculus.\" He began with the fundamental concepts of variables and functions. His innovation is noted for its use of exponentiation to introduce the transcendental functions. The general logarithm, to an arbitrary positive base, Euler presents as the inverse of an exponential function.\n\nThen the natural logarithm is obtained by taking as base \"the number for which the hyperbolic logarithm is one\", sometimes called Euler's number, and written e. This appropriation of the significant number from Gregoire de Saint-Vincent’s calculus suffices to establish the natural logarithm. This part of precalculus prepares the student for integration of the monomial \"x\" in the instance of p = −1.\n\nToday’s precalculus text computes e as the limit of (1 + 1/\"n\") as \"n\" gets large. An exposition on compounded interest in financial mathematics may motivate this limit. Another difference in the modern text is avoidance of complex numbers, except as they may arise as roots of a quadratic equation with negative discriminant, or in Euler's formula as application of trigonometry. Euler used not only complex numbers but also infinite series in his precalculus. Today’s course may cover arithmetic and geometric sequences and series, but not the application by Saint-Vincent to gain his hyperbolic logarithm, which Euler used to finesse his precalculus.\n\nPrecalculus prepares students for calculus somewhat differently from the way that pre-algebra prepares students for algebra. While pre-algebra often has extensive coverage of basic algebraic concepts, precalculus courses might see only small amounts of calculus concepts, if at all, and often involves covering algebraic topics that might not have been given attention in earlier algebra courses. Some precalculus courses might differ with others in terms of content. For example, an honors-level course might spend more time on conic sections, Euclidean vectors, and other topics needed for calculus, used in fields such as medicine or engineering. A college preparatory/regular class might focus on topics used in business-related careers, such as matrices, or power functions.\n\nA standard course considers functions, function composition, and inverse functions, often in connection with sets and real numbers. In particular, polynomials and rational functions are developed. Algebraic skills are exercised with trigonometric functions and trigonometric identities. The binomial theorem, polar coordinates, parametric equations, and the limits of sequences and series are other common topics of precalculus. Sometimes the mathematical induction method of proof for propositions dependent upon a natural number may be demonstrated, but generally coursework involves exercises rather than theory.\n\n\n"}
{"id": "1648694", "url": "https://en.wikipedia.org/wiki?curid=1648694", "title": "Riken", "text": "Riken\n\nRiken conducts research in many areas of science, including physics, chemistry, biology, medical science, engineering, high-performance computing and computational science, and ranging from basic research to practical applications with 485 partners worldwide. It is almost entirely funded by the Japanese government, and its annual budget is about ¥88 billion (US$760 million).\n\nIn 1913, the well-known scientist Jokichi Takamine first proposed the establishment of a national science research institute in Japan. This task was taken on by Viscount Shibusawa Eiichi, a prominent businessman, and following a resolution by the Diet in 1915, Riken came into existence in March 1917. In its first incarnation, Riken was a private foundation (\"zaidan\"), funded by a combination of industry, the government, and the Imperial Household. It was located in the Komagome district of Tokyo, and its first Director was the mathematician Baron Dairoku Kikuchi.\n\nIn 1927, Viscount Masatoshi Ōkōchi, the third Director, established the Riken Konzern (a zaibatsu). This was a group of spin-off companies that used Riken's scientific achievements for commercial ends and returned the profits to Riken. At its peak in 1939 the Konzern comprised about 121 factories and 63 companies, including Riken Kankōshi, which is now Ricoh.\n\nDuring World War II the Japanese army's atomic bomb program was conducted at Riken. In April 1945 the US bombed Riken's laboratories in Komagome, and in November, after the end of the war, Allied soldiers destroyed its two cyclotrons.\n\nAfter the war, the Allies dissolved Riken as a private foundation, and it was brought back to life as a company called , or . In 1958 the Diet passed the Riken Law, whereby the institute returned to its original name and entered its third incarnation, as a , funded by the government. In 1963 it relocated to a large site in Wakō in Saitama Prefecture, just outside Tokyo.\n\nSince the 1980s Riken has expanded dramatically. New labs, centers, and institutes have been established in Japan and overseas, including:\n\nIn October 2003, Riken's status changed again, to Independent Administrative Institution. As such, Riken is still publicly funded, and it is periodically evaluated by the government, but it has a higher degree of autonomy than before. Riken is regarded as the flagship research institute in Japan and conducts basic and applied experimental research in a wide range of science and technology fields including physics, chemistry, medical science, biology and engineering.\n\nRiken was the subject of international attention in 2014 after the Stimulus-triggered acquisition of pluripotency cell (also known as STAP) publication, investigation, retraction, and suicide of Yoshiki Sasai, the principal investigator. Observers, journalists, and former members of Riken have stated that the organization is riddled with unprofessional and inadequate scientific rigor and consistency, and that this is reflective of serious issues with scientific research in Japan in general.\n\nThe main divisions of Riken are listed here. Purely administrative divisions are omitted.\n\n\n\n<--** Center for Life Science Technologies-->\n\n\n\n\n\n"}
{"id": "11925538", "url": "https://en.wikipedia.org/wiki?curid=11925538", "title": "Scorcher: The Dirty Politics of Climate Change", "text": "Scorcher: The Dirty Politics of Climate Change\n\nScorcher: The Dirty Politics of Climate Change is a 2007 book by Clive Hamilton which contends that Australia rather than the United States is the major stumbling block to a more effective Kyoto Protocol. In the final chapter of the book Hamilton argues that \"the Howard Government has been actively working to destroy the Kyoto Protocol\".\n\nScorcher is an updated version of Hamilton's 2001 book, \"Running from the Storm\". Other books by Clive Hamilton include \"Requiem for a Species\", \"Silencing Dissent\", \"Growth Fetish\", \"Affluenza\" and \"\".\n\n\n\n"}
{"id": "31368179", "url": "https://en.wikipedia.org/wiki?curid=31368179", "title": "Space Launch System", "text": "Space Launch System\n\nThe Space Launch System (SLS) is an American Space Shuttle-derived super heavy-lift expendable launch vehicle. It is part of NASA's deep space exploration plans including a crewed mission to Mars. SLS follows the cancellation of the Constellation program, and is to replace the retired Space Shuttle. The NASA Authorization Act of 2010 envisions the transformation of the Constellation program's Ares I and Ares V vehicle designs into a single launch vehicle usable for both crew and cargo, similar to the Ares IV concept. The SLS is to be the most powerful rocket ever built with a total thrust greater than that of the Saturn V, although Saturn V could carry a greater payload mass.\n\nThe SLS launch vehicle is to be upgraded over time with more powerful versions. Its initial Block 1 version is to lift a payload of 95 metric tons to low Earth orbit (LEO), which will be increased with the debut of Block 1B and the Exploration Upper Stage. Block 2 will replace the initial Shuttle-derived boosters with advanced boosters and is planned to have a LEO capability of more than 130 metric tons to meet the congressional requirement. These upgrades will allow the SLS to lift astronauts and hardware to destinations beyond LEO: on a circumlunar trajectory as part of Exploration Mission 1 with Block 1; to deliver elements of the Lunar Orbital Platform-Gateway (LOP-G) with Block 1B; and to Mars with Block 2. The SLS will launch the Orion Crew and Service Module and may support trips to the International Space Station if necessary. SLS will use the ground operations and launch facilities at NASA's Kennedy Space Center, Florida.\n\nOn September 14, 2011, NASA announced its design selection for the new launch system, declaring that it, in combination with the Orion spacecraft, would take the agency's astronauts farther into space than ever before and provide the cornerstone for future US human space exploration efforts.\n\nDuring the early development of the SLS a number of configurations were considered, including a Block 0 variant with three main engines, a Block 1A variant that would have upgraded the vehicle's boosters instead of its second stage, and a Block 2 with five main engines and a different second stage, the Earth Departure Stage, with up to three J-2X engines. In February 2015, it was reported that NASA evaluations showed \"over performance\" versus the baseline payload for Block 1 and Block 1B configurations.\n\nThree versions of the SLS launch vehicle are planned: Block 1, Block 1B, and Block 2. Each will use the same core stage with four main engines, but Block 1B will feature a more powerful second stage called the Exploration Upper Stage (EUS), and Block 2 will combine the EUS with upgraded boosters. Block 1 has a baseline LEO payload capacity of and Block 1B has a baseline of . The proposed Block 2 will have lift capacity of , which is similar to that of the Saturn V. Some sources state this would make the SLS the most capable heavy lift vehicle built; although the Saturn V lifted approximately to LEO in the Apollo 17 mission.\n\nOn July 31, 2013, the SLS passed the Preliminary Design Review (PDR). The review encompassed all aspects of the SLS's design, not only the rocket and boosters but also ground support and logistical arrangements. On August 7, 2014 the SLS Block 1 passed a milestone known as Key Decision Point C and entered full-scale development, with an estimated launch date of November 2018. In April 2017, NASA announced that the schedule for the maiden flight would slip to 2019. In November 2017, the EM-1 maiden flight slipped further to June 2020.\n\nThe Space Launch System's Core Stage will be in diameter and use four RS-25 engines. Initial flights will use modified RS-25D engines left over from the Space Shuttle program; later flights are expected to switch to a cheaper version of the engine not intended for reuse. The stage's structure will consist of a modified Space Shuttle external tank with the aft section adapted to accept the rocket's Main Propulsion System (MPS) and the top converted to host an interstage structure. It will be fabricated at the Michoud Assembly Facility.\n\nThe core stage will be common across all currently planned evolutions of the SLS. Initial planning included studies of a smaller Block 0 configuration with three RS-25 engines, which was eliminated to avoid the need to substantially redesign the core stage for more powerful variants. Likewise, while early Block 2 plans included five RS-25 engines on the core, it was later baselined with four engines.\n\nBlocks 1 and 1B of the SLS will use two five-segment Solid Rocket Boosters (SRBs), which are based on the four-segment Space Shuttle Solid Rocket Boosters. Modifications for the SLS included the addition of a center booster segment, new avionics, and new insulation which eliminates the Shuttle SRB's asbestos and is lighter. The five-segment SRBs provide approximately 25% more total impulse than the Shuttle SRB and will not be recovered after use.\n\nOrbital ATK (formerly Alliant Techsystems, part of Northrop Grumman since mid-2018) has completed full-duration static fire tests of five-segment SRBs. These include successful firings of three developmental motors (DM-1 to DM-3) from 2009 to 2011. The DM-2 motor was cooled to a core temperature of , and DM-3 was heated to above to validate performance at extreme temperatures. Qualification Motor 1 (QM-1) was tested on March 10, 2015. Qualification Motor 2 was successfully tested on June 28, 2016. It was the final ground test before Exploration Mission 1 (EM-1).\n\nFor Block 2, NASA plans to switch from Shuttle-derived five-segment SRBs to advanced boosters. This will occur after development of the Exploration Upper Stage for Block 1B. Early plans would have developed advanced boosters before an updated second stage; this configuration was called Block 1A. By 2012 NASA planned to select these new boosters through an Advanced Booster Competition which was to be held in 2015. Several companies proposed boosters for this competition:\n\n\nChristopher Crumbly, manager of NASA's SLS advanced development office in January 2013 commented on the booster competition, \"The F-1 has great advantages because it is a gas generator and has a very simple cycle. The oxygen-rich staged combustion cycle [Aerojet's engine] has great advantages because it has a higher specific impulse. The Russians have been flying ox[ygen]-rich for a long time. Either one can work. The solids [of ATK] can work.\"\n\nLater analysis showed the Block 1A configuration would result in high acceleration which would be unsuitable for Orion and could require a costly redesign of the Block 1 core. In 2014, NASA confirmed the development of Block 1B instead of Block 1A and called off the 2015 booster competition. In February 2015, it was reported that SLS is expected to fly with the five-segment SRB until at least the late 2020s, and modifications to Launch Pad 39B, its flame trench, and SLS's Mobile Launcher Platform were evaluated based on SLS launching with solid-fuel boosters.\n\nBlock 1, scheduled to fly Exploration Mission 1 (EM-1) in 2020, will use the Interim Cryogenic Propulsion Stage (ICPS). This stage will be a modified Delta IV 5–meter Delta Cryogenic Second Stage (DCSS), and will be powered by a single RL10B-2. Block 1 will be capable of lifting 95 t in this configuration, however the ICPS will be considered part of the payload and be placed into an initial 1,800 km by -93 km suborbital trajectory to ensure safe disposal of the core stage. ICPS will perform an orbital insertion burn at apogee, and then a translunar injection burn to send the uncrewed Orion on a circumlunar excursion. In May 2018, NASA updated the payload capability of the SLS Block 1 from 70 to 95 metric tons to low Earth orbit.\n\nThe Exploration Upper Stage (EUS) was scheduled to fly on Exploration Mission 2 (EM-2). It was expected to be used by Block 1B and Block 2 and, like the core stage, have a diameter of 8.4 meters. The EUS is to be powered by four RL10 engines, complete the SLS ascent phase and then re-ignite to send its payload to destinations beyond low-Earth orbit, similar to the role performed by the Saturn V's 3rd stage, the J-2 powered S-IVB. Because of delays in building the mobile launch platform needed to hold the more powerful EUS, the EM-2 flight might launch earlier than planned but it will not use the EUS, it will not carry a module for the Lunar Gateway and it will not orbit the Moon.\n\n\nIn mid-November 2014, construction of the first SLS rocket began using the new welding system at NASA's Michoud Assembly Facility, where the Core Stage will be assembled.\n\nThe SLS will have the ability to tolerate a minimum of 13 tanking cycles due to launch scrubs and other launch delays before launch. The assembled rocket is to be able to remain at the launch pad for a minimum of 180 days and can remain in stacked configuration for at least 200 days without destacking.\n\nIn January 2015, NASA began test firing RS-25 engines in preparation for use on SLS. Tests continued throughout Spring of 2015. Further testing was conducted in 2016 and 2017.\n\nMultiple facilities throughout the country have started full scale fabrication of different segments of the launch vehicle. Orbital ATK began casting propellant for the solid rocket boosters and manufacturing parts for the boosters in 2016. The company test fired a solid rocket booster in early 2015, and a second booster in June 2016.\n\nConfidence article builds for the core stage began on January 5, 2016 and were expected to be completed in late January of that year. Once completed the test articles will be sent to ensure structural integrity at Marshall Spaceflight Center. The ICPS for EM-1 was slated for assembly in late January 2016, and a structural test article was delivered to NASA in 2015 for confidence testing.\n\nDuring the joint Senate-NASA presentation in September 2011, it was stated that the SLS program had a projected development cost of $18 billion through 2017, with $10 billion for the SLS rocket, $6 billion for the Orion Multi-Purpose Crew Vehicle and $2 billion for upgrades to the launch pad and other facilities at Kennedy Space Center. These costs and schedule were considered optimistic in an independent 2011 cost assessment report by Booz Allen Hamilton for NASA. An unofficial 2011 NASA document estimated the cost of the program through 2025 to total at least $41bn for four 95 t launches (1 uncrewed, 3 crewed), with the 130 t version ready no earlier than 2030.\n\nThe Human Exploration Framework Team (HEFT) estimated unit costs for Block 0 at $1.6bn and Block 1 at $1.86bn in 2010. However, since these estimates were made the Block 0 SLS vehicle was dropped in late 2011, and the design was not completed. The Space Review estimated the cost per launch at $5 billion, depending on the rate of launches. NASA announced in 2013 that the European Space Agency will build the Orion Service Module.\n\nNASA SLS deputy project manager Jody Singer at Marshall Space Flight Center, Huntsville, Alabama stated in September 2012 that $500 million per launch is a reasonable target cost for SLS, with a relatively minor dependence of costs on launch capability. By comparison, a Saturn V launch cost US$185 to US$189 million in 1969-1971 dollars or roughly $1.23 billion in 2016 dollars adjusted for inflation.\n\nOn July 24, 2014, Government Accountability Office audit predicted that SLS would not launch by the end of 2017 as originally planned since NASA had not been receiving sufficient funding.\n\nIn August 2014, as the SLS program passed its Key Decision Point C review and entered full development, costs from February 2014 until its planned launch in September 2018 were estimated at $7.021 billion. Ground systems modifications and construction would require an additional $1.8 billion over the same time period. the Orion spacecraft was expected to enter its Key Decision Point C review in the first half of 2015.\n\nFor Fiscal Year 2015, NASA received an appropriation of from Congress for SLS, an amount that was approximately greater than the amount requested by the Obama administration.\n\nIn 2018, NASA's inspector general stated in a report that the SLS will cost at least by 2021. That is double the amount that was stated initially.\n\nFor fiscal years 2011 through 2015, the SLS program had expended funding totaling $7.7 billion in nominal dollars. This is equivalent to $8.3 billion adjusting to 2016 dollars using the NASA New Start Inflation Indices.\n\nFor 2016, the SLS program funding, excluding the Exploration Upper Stage (EUS), was enacted at $1,915M with an additional $7,180M planned for 2017 through 2021. The SLS program has a 70% confidence level for initial program completion by 2023 according to the Associate Administrator for NASA, Robert Lightfoot.\n\nThe sum of the prior SLS program funding from 2011 to 2015, funding enacted for 2016. \n\nThese prior SLS costs:\nThere are no NASA estimates for the SLS program recurring yearly costs once operational, for a certain flight rate per year, or for the resulting average costs per flight.\n\nThe Space Access Society, Space Frontier Foundation and The Planetary Society called for cancellation of the project in 2011–12, arguing that SLS will consume the funds for other projects from the NASA budget and will not reduce launch costs. U.S. Representative Dana Rohrabacher and others added that instead, a propellant depot should be developed and the Commercial Crew Development program accelerated. Two studies, one not publicly released from NASA and another from the Georgia Institute of Technology, show this option to be possibly cheaper.\n\nOthers suggest it will cost less to use an existing lower payload capacity rocket (Atlas V, Delta IV, Falcon 9, or the derivative Falcon Heavy), with on-orbit assembly and propellant depots as needed, rather than develop a new launch vehicle for space exploration without competition for the whole design. The Augustine commission proposed an option for a commercial 75 metric ton launcher with lower operating costs, and noted that a 40 to 60 t launcher can support lunar exploration.\n\nMars Society founder Robert Zubrin, who co-authored the Mars Direct concept, suggested that a heavy lift vehicle should be developed for $5 billion on fixed-price requests for proposal. Zubrin also disagrees with those that say the U.S. does not need a heavy-lift vehicle. SpaceX's CEO Elon Musk stated in 2010 that he would \"personally guarantee\" that his company could build a launch vehicle in the 140–150 t payload range, for $2.5 billion, or $300 million per launch, but cautioned that this price tag did not include a potential upper-stage upgrade. SpaceX's privately funded ITS launch vehicle, powered by multiple Raptor engines, has also been proposed for lifting very large payloads from Earth in the 2020s.\n\nRep. Tom McClintock and other groups argue that the Congressional mandates forcing NASA to use Space Shuttle components for SLS amounts to a de facto non-competitive, single source requirement assuring contracts to existing shuttle suppliers, and calling the Government Accountability Office (GAO) to investigate possible violations of the Competition in Contracting Act (CICA). Opponents of the heavy launch vehicle have critically used the name \"Senate launch system\". The Competitive Space Task Force, in September 2011, said that the new government launcher directly violates NASA's charter, the Space Act, and the 1998 Commercial Space Act requirements for NASA to pursue the \"fullest possible engagement of commercial providers\" and to \"seek and encourage, to the maximum extent possible, the fullest commercial use of space\".\n\nIn 2013, Chris Kraft, the NASA mission control leader from the Apollo era, expressed his criticism of the system as well. Lori Garver, former NASA Deputy Administrator, has called for cancelling the program. Phil Plait has voiced his criticism of SLS in light of ongoing budget tradeoffs between Commercial Crew Development and SLS budget, also referring to earlier critique by Garver.\n\nDoubts have also been expressed about the utility and cost of depots. \"Patrick R. Chai and Alan W. Wilhite of Georgia Tech presented a study early in 2011 estimating that depot tanks would lose about $12 million worth of propellant per month in low Earth orbit if protected only with passive insulation.\"\n\nThe Planetary Society accepted that a Mars mission could be had with existing budgets.\n\nThe list below includes only confirmed missions according to NASA plans published in April 2017, and updated in September 2018.\n\n\n"}
{"id": "41940944", "url": "https://en.wikipedia.org/wiki?curid=41940944", "title": "Steam infusion", "text": "Steam infusion\n\nSteam Infusion is a direct-contact heating process in which steam condenses on the surface of a pumpable food product. Its primary use is for the gentle and rapid heating of a variety of food ingredients and products including milk, cream, soymilk, ketchup, soups and sauces.\n\nUnlike steam injection and traditional vesselled steam heating; the steam infusion process surrounds the liquid food product with steam as opposed to passing steam through the liquid.\n\nSteam Infusion cooking technology is believed to be the fastest growing food processing solution in the UK with annual sales up 10 fold to £6 million. Steam Infusion allows food product to be cooked, mixed and pumped within a single unit, often removing the need for multiple stages of processing.\n\n“Steam infusion has allowed us to nearly double our previous throughput rates, furthermore our meat sauce quality and consistency has improved dramatically.” Mark Carnaghan, Factory Manager, Greencore\n\nSteam Infusion has been dubbed low risk by commentators because of the ease with which manufacturers can easily match existing products. \n\nSteam infusion was first used in pasteurization and has since been developed for further liquid heating applications.\n\nIn the 1960s APV PLC launched the first steam infusion system under the Palarisator brand name. This involves a 2-stage process for steam infusion whereby the liquid is cascaded into a large pressurized steam chamber and is sterilized when falling as film or droplets through the chamber. The liquid is then condensed at the chilled bottom of the chamber. Illustrated in the image on the right hand side of the page.\n\nThe Steam Infusion process was first conceived in 2000 as a method for marine propulsion. The process has since been developed to be used for steam infusion. On the right a diagram shows how the process creates an environment of vaporised product surrounded by high energy steam. The supersonic steam flow entrains and vaporises the process flow to form a multiphase flow, which heats the suspended particles by surface conduction and condensation. The condensation of the steam causes the process flow to return to a liquid state. This causes rapid and uniform heating over the unit making it applicable to industrial cooking processes. This process has been used in industry, predominantly in soup and sauces applications. Its possible benefits include reduced cooking times, easier cleaning with no burn-on, ingredient reduction and energy savings.\n\nSteam infusion can be used to pasteurize a variety of dairy products. For ultra pasteurization, also known as ultrahigh-temperature (UHT) pasteurization, the milk is heated to temperatures in the order of 140 °C. During steam infusion, milk is brought into contact with steam at 140 °C for one or two seconds. The heating is instantaneous, and the milk is cooled rapidly by evaporative cooling exposure to a slight vacuum, removing any water added to the milk by condensation of the steam.\n\nSteam infusion is used in cooking applications on fluid based products. The heat addition on particulate level in a low pressure environment makes steam infusion cooking especially applicable to soups and sauces where particle integrity is important. Steam Infusion has shown the potential to improve the nutritional content of food and is being researched by the University of Lincoln\n"}
{"id": "46959296", "url": "https://en.wikipedia.org/wiki?curid=46959296", "title": "The Birth and Death of the Sun", "text": "The Birth and Death of the Sun\n\nThe Birth and Death of the Sun is a popular science book by theoretical physicist and cosmologist George Gamow, first published in 1940, exploring atomic chemistry, stellar evolution, and cosmology. The book is illustrated by Gamow. It was revised in 1952.\n\nCritical reception has been positive. In February 1941, Gerard F. W. Mulders gave a favorable review for \"The Birth and Death of the Sun\", writing that \"[i]t gives authentic information in nontechnical language from which mathematical formulae have been completely eliminated. The entertaining presentation of the most modern developments in physics and astrophysics, the sparkling humor, and the original drawings and graphs will be enjoyed by scientist and amateur alike.\"\n\nIn April 2015, physicist and Nobel laureate Steven Weinberg included \"The Birth and Death of the Sun\" in a personal list of \"the 13 best science books for the general reader\".\n\n"}
{"id": "4009179", "url": "https://en.wikipedia.org/wiki?curid=4009179", "title": "The Closed Circle: An Interpretation of the Arabs", "text": "The Closed Circle: An Interpretation of the Arabs\n\nThe Closed Circle: An interpretation of the Arabs is a 1989 book by author David Pryce-Jones that was published by Harper & Row.\n\nThis book discusses the tribal roots of Arab society which form the basis of its cultural traditions. The author documents the cultural forces which drive the violence and mayhem that, in his view, is characteristic of Arab societies in their dealings with each other and with the West.\n\nThe author argues that the Arab world is stuck in an age-old tribalism and behavior from which it is unable to evolve. In tribal society, loyalty is extended to close kin and other members of the tribe. In the Arab world those who seek power achieve it by plotting secretly and ruthlessly eliminating their rivals.\n\n\n"}
{"id": "58957999", "url": "https://en.wikipedia.org/wiki?curid=58957999", "title": "The double thank-you of capitalism", "text": "The double thank-you of capitalism\n\nThe double thank-you of capitalism is the observation that, when a merchant and a customer exchange money for goods, each thanks the other, showing that the transaction is not only voluntary, but mutually beneficial. This is in contrast to the impression of a \"fixed slice pie\" that occurs in many other situations, with one side taking and the other giving, a thank-you likely to pass only one way. The double thank-you is used in economic discourse to illustrate the primary benefit of a free market, that where all actions are voluntary, any transactions must benefit both sides, and this enriches the general community. \n\nThis idea has been around for some time, at least as far back as Adam Smith and his book, The Wealth of Nations. Of this, Nobel laureate Milton Friedman said:\n\nIn most other parts of life or society, one side tends to give and the other receive. There is a fixed slice pie, where the same amount of wealth exists, and is simply shuffled around. A poor person receiving aid from the a social worker may feel grateful and say thanks, but the official giving the aid has no reason to say it back. A more extreme example is tax collecting, where neither side has any impulse to thank. The one that is receiving does so by threat of force, therefore feels empowered, not grateful.\n\nBut a voluntary exchange is a win-win scenario, where each side feels they have more wealth after the trade than before. The shopkeeper needed money more than his product, but the buyer wanted the product more than the money he paid. \n\nCoining the \"Double Thank-You\" term, John Stossel described it thus:\n\nStossel explains that this applies to not just local commerce, but even international trade. When a buyer in one country sends money to another country, he receives more than the local value of that money in goods, in return. Otherwise he would not have done it. But the seller, likewise, ended up with more money than the value of the goods he sent, as far as his own community is concerned. So a \"trade deficit\" is meaningless, all trade results in both sides being richer. Adam Smith said that nothing \"can be more absurd than this whole doctrine of the balance of trade.\"\n\nJay Nordlinger applies this by saying that trade deals should work out for everyone, to turn out in all parties' best interest. Instead of one side winning and all others losing, everyone wins.\n\nEconomist Steve Horwitz describes the \"Double Thank You\" as a \"habit of cooperation\", where people take for granted that if they act to benefit others, the behavior will be mutual, everyone winning out in the end. He contrasts an encounter with a helpful, even generous deli counter merchant with that of dealing with a TSA bureaucrat. When retrieving one's goods from the TSA inspector, the \"habit of cooperation\" of capitalism may give you the impulse to thank the agent for returning your own, rightful property. But you have no reason to say \"thanks\", because the agent has not given you a gift, not provided you with a service of any value, there was no voluntary exchange or contract. It is pointless \"government monopoly security theater with no exit option and no value delivered.\"\n\nThe way such voluntary transactions create wealth can be understood by imagining two islands. One is rocky and dry, growing mostly grasses that provide significant grazing for herd animals. Meat is plentiful and (therefore) cheap, but fresh vegetables are rare and expensive. On another island, though, the land is rich and soft, the weather rainy, conditions great for vegetables but bad for the hoofed animals commonly raised in the area. So vegetables are cheap, meat expensive and difficult to obtain.\n\nA merchant might buy a large amount of the cheap meat from the rocky island, then make the dangerous voyage to the rainy island with his cargo. He can sell for far more than he paid, while still charging less than any other meat in the area. He can then buy a cargo of very cheap vegetables, return to the rocky island, and sell it at a much higher price that nonetheless seems like a bargain to its populace.\n\nAfter his round trip, not only is he far richer than before, but so are the citizens of each island, having access to \"expensive\" food at what seems to them like a bargain price. This simple trade has increased wealth for each part of two different transactions, without actually producing more goods.\n\nThis creation of wealth does not just involve geographic separation, but also information asymmetry. A trader with resources or knowledge of two different levels of demand for a good can buy in one place while selling in the other, creating wealth in both through what is known in finance and economics as arbitrage.\n"}
{"id": "14676686", "url": "https://en.wikipedia.org/wiki?curid=14676686", "title": "William Lawrence Tower", "text": "William Lawrence Tower\n\nWilliam Lawrence Tower (1872–??) was an American zoologist, born in Halifax, Massachusetts. He was educated at the Lawrence Scientific School (Harvard), the Harvard Graduate School, and the University of Chicago (B. S., 1902), where he taught thereafter, becoming associate professor in 1911.\n\nTower was notable for his experimental work in heredity, investigating the inheritance of acquired characteristics and the laws of heredity in beetles and publishing \"An Investigation of Evolution in Chrysomelid Beetles of the Genus Leptinotarsa\" (1906). This study is probably the first (albeit possibly discredited) of mutation in animals. He published also \"The Development of the Colors and Color Patterns of Coleoptera\" (1903) and, with Coulter, Castle, Davenport and East, an essay on \"Heredity and Eugenics\" (1912).\n\nTower was caught up in personal and professional scandals. He resigned from the University of Chicago in 1917 following a very public divorce, but by then he had become a source of discontent among students and faculty. His professed atheism caused offense to some, including graduate student Warder Clyde Allee. Tower caused political friction within the department and many members distrusted his professional ethics. Experimental results which Tower reported in 1906 and 1910 were found to include serious discrepancies which he declined to explain. His claim that experimental results had been lost in a fire increased his colleagues' skepticism. William Bateson, T. D. A. Cockerell, and R. A. Gortner were particularly critical of his work. A more positive reception came from the botanist Henry Chandler Cowles.\n\nIt was suggested that his research may have been faked. The geneticist William E. Castle who visited Tower's laboratory was not impressed by the experimental conditions. He later concluded that Tower had faked his data. Castle found the fire suspicious and also Tower's claim that a steam leak in his greenhouse had destroyed all his beetle stocks.\n\n\n\n"}
