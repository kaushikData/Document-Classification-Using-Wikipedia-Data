{"id": "1379832", "url": "https://en.wikipedia.org/wiki?curid=1379832", "title": "A Universe of Consciousness", "text": "A Universe of Consciousness\n\nA Universe of Consciousness: How Matter Becomes Imagination is the title of a 2000 book by biologists Gerald Maurice Edelman and Giulio Tononi; published in UK as Consciousness: How Matter Becomes Imagination. This book, written with Giulio Tononi, is the culmination of a series of works by Gerald Edelman on the workings of the brain which include Neural Darwinism and Bright Air, Brilliant Fire.\n\nIt is divided into six sections: the first three cover existing work from philosophical, neurological and Darwinian perspectives. Part IV presents the novel thesis of the work: the Dynamic Core Hypothesis. The remaining two parts explore how it resolves various philosophical and practical issues.\n\nSince Descartes, philosophers have been occupied with the concept of consciousness and its subjective nature has posed a special problem for science. Its nature arises from the neuronal structures in the brain and some understanding of these, together with the experimental tools needed to explore them, is given in the following chapters. They then recapitulate Edelman's still controversial theory of somatic selectionism during early development which controls the topology of a particular brain and enables restructuring in response to experience. They argue that memory is not a symbolic representation but a reflection of how the brain has changed its dynamics in order to achieve motor activity. This leads to a discussion of primary consciousness which integrates with perception into a means of directing immediate behavior and requires significant levels of reentrancy to achieve its effects.\n\nThe problem of integrating, or \"binding\", the activity of functionally segregated areas of the brain in order to concentrate attention on a particular activity in a short amount of time (typically 100-250 msecs) after the presentation of a stimulus is explored by means of large-scale simulations. It is shown that this can only happen if some elements interact more strongly among themselves than with the rest of the system including a large amount of reentrancy. These \"functional clusters\" are only slowly coming into the range of PET or fMRI scanning technology which commonly require much longer time scales.\n\nAt any given time, only a small subset of the neuronal groups in the brain are contributing directly to consciousness and this cluster is called a \"dynamic core\". It represents a single point of view and each different state of consciousness corresponds to a different subset. Some dissociative disorders such as schizophrenia may result in the formation of multiple cores.\n\nOne of the recurring issues in consciousness is the existence of \"qualia\", such as redness, warmth and pain. It is not enough to identify each quale with a particular neuron or neuronal group; what is crucial is all the other groups which are highly influenced by the sensation and will fire at the same time. Thus each conscious state deserves to be called a quale. A small perturbation of a group of neurons can affect the whole in a very short space of time provided the system is kept in a state of readiness by the thalamus. Primary consciousness can build up a bodily based reference space even before language and higher-order consciousness appear.\n\nThere is a preliminary approach to the relationship between conscious and unconscious processes, including sensors and motors, because so little is known\nThe evolution of language centres in the brain leads to higher order consciousness which enhances subjective experience and enables humans to describe qualia which are however experienced by a much wider range of animals. Thinking in humans has a range of representations—including pictorial. In contrast to computers which are Turing machines, brains are based on neuronal group selection.\n\nJohn Cornwell (Sunday Times) One of the most thoughtful books on the topic... While revealing much that is surprising about consciousness, they confirm some deeply held convictions about the power and mystery of human imagination.\n\nPiero Scaruffi: The book advances a theory for what causes the conscious feeling that we experience, but fails to explain how matter can turn into feelings. For all the properties of consciousness that they list, the authors fail to grasp the essence of consciousness: I \"feel\" that I am myself. \nThe results of this pioneering work challenge the conventional wisdom about consciousness.\n"}
{"id": "53839755", "url": "https://en.wikipedia.org/wiki?curid=53839755", "title": "Alice Just Harding", "text": "Alice Just Harding\n\nAlice Just Harding from the NASA Goddard Space Flight Center, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Astrophysics in 1991, for \"pioneering investigation of the theory of pulsar atmospheres, including the pulsar wind and its role in accelerating particles to high energies, and for contributions to the theory of basic electromagnetic interactions in the presence of super-strong magnetic fields.\"\n"}
{"id": "57010933", "url": "https://en.wikipedia.org/wiki?curid=57010933", "title": "Anaerolineaceae", "text": "Anaerolineaceae\n\nAnaerolineaceae is a family of methanogenic bacteria from the order of Anaerolineales.\nAnaerolineaceae bacteria occur in marine sediments.\n\n"}
{"id": "52656779", "url": "https://en.wikipedia.org/wiki?curid=52656779", "title": "Association for Chemoreception Sciences", "text": "Association for Chemoreception Sciences\n\nThe Association for Chemoreception Sciences is an international professional society in the field of chemosensory science. It is a non-profit organization that seeks to promote and advance the interests of the science of senses such as taste and smell. In order to do this, it holds an annual meeting that is a scientific forum for the research community and also provides outreach to the public about olfaction (smell) and gustation (taste). \n\nThe association was founded in 1978 by Maxwell M. Mozell, a neuroscientist at the State University of New York, with the help of a grant from the National Science Foundation. The first research meeting was held in Sarasota, Florida, in April 1979. Officers elected at the first meeting included Linda Bartoshuk, Rose Marie Pangborn and Gary Beauchamp. \n\nA meeting is held in April of each year that is attended by an international cohort of physicians and scientists. This annual meeting consists of presentations on olfaction, gustation, and chemesthesis, as well as workshops sponsored by the National Institute of Health Commercial exhibitors also attend the event. The organization has enjoyed strong support from the National Institute on Deafness and Communicative Disorders and its director. In the past AChemS participated in the International Commission on Olfaction and Taste, until that commission was dissolved in 2002, and hosted its annual symposium every twelve years.In 2004, AChemS member Linda Buck and Richard Axel were awarded the Nobel Prize in Physiology or Medicine “for their discoveries of odorant receptors and the organization of the olfactory system”. To celebrate this honor, at the 2005 AChemS annual meeting, Buck and Axel were keynote speakers , recapping their research published in the journal Cell in 1991, which led to the Nobel Award.  \n\n\"Chemical Senses\", the official journal of the association, is published by Oxford University Press. The editor is Wolfgang Meyerhof; it was edited by Maxwell Mozell from 1992 until 1998.\n\nThe Association gives a series of annual awards, including the Max Mozell Award, the Barry Jacobs Memorial Award, the Ajinomoto Award, and the Polak Young Investigator Award. Travel awards are also given to diverse and young scientists to encourage their attendance at the meeting.\n"}
{"id": "15497991", "url": "https://en.wikipedia.org/wiki?curid=15497991", "title": "BELBIC", "text": "BELBIC\n\nIn recent years, the use of biologically inspired methods such as the evolutionary algorithm have been increasingly employed to solve and analyze complex computational problems. BELBIC (Brain Emotional Learning Based Intelligent Controller) is one such controller which is proposed by Caro Lucas, Danial Shahmirzadi and Nima Sheikholeslami and adopts the network model developed by Moren and Balkenius to mimic those parts of the brain which are known to produce emotion (namely, the amygdala, orbitofrontal cortex, thalamus and sensory input cortex).\n\nTraditionally, the study of learning in biological systems was conducted at the expense of overlooking its lesser known counterparts: motivation and emotion. However these phenomena can not be separated. Motivation is the drive that causes any system to do anything – without it, there is no reason to act. Emotions indicate how successful a course of actions have been and whether another set of actions should have been taken instead – they are a constant feedback to the learning system. Learning on the other hand, guarantees that motivation and emotional subsystems are able to adapt to constantly changing conditions.\n\nThus, in the study of biological organisms, emotions have arisen to prominence as an integral part of any biologically inspired system. But how does any living organism benefit from its emotions? It is crucial to answer this question as we attempt to increasingly employ biologically inspired methods in solving computational problems.\n\nEvery creature has innate abilities that accommodate its survival in the world. It can identify food, shelter, partners, and danger. But these \"simple mappings between stimuli and reactions will not be enough to keep the organisms from encountering problems.\" For example, if a given animal knows that its predator has qualities A, B and C, it will escape \"all\" creatures that have those qualities. And thus waste much of its energy and resources on non-existent danger.\n\nWe can not expect evolution to provide more advanced algorithms for assessing danger, because the predator is also evolving at the same speed. Thus, biological systems need to be equipped with the ability to learn. This learning and re-learning mechanism allows them to adapt to highly complex and advanced situations.\n\nTo learn effectively, every learning organism needs an evaluation of the current situation and also feedback on how beneficial the results of learning were. On the most part, these evaluation mechanisms are built-in. And so we encounter a new problem: whereas creatures take appropriate measures in real time based on their evaluations, these built-in evaluation procedures are developed in evolutionary time. But all creatures need to learn of new evaluation techniques in their lifetime just as they learn the proper reactions.\n\nThis is where the ability to condition emotional reactions comes into play. Biological organisms associate innate emotional stimuli with other stimuli they encounter in the world and thus give them an emotional significance when needed. These evaluations can be monitored to operate at very specific times, specific places or when accompanied by other specific stimuli.\n\nThere is another reason why these observations are so significant and that is the creation of artificial systems. These systems do not evolve over time but are designed with certain abilities from the start. Thus, their adaptability must be built-in.\n\nA model is a simplified description of a phenomenon. It brings to life some aspects of this phenomenon while overlooking others. What aspects are kept in the model and what are overlooked greatly depends on the topic of study. Thus, the nature of a model depends on the purpose the investigator plans to carry out. A computational model is one which can be mathematically analyzed, tested and simulated using computer systems.\n\nTo construct a computational model of emotional learning in the brain requires a thorough analysis of the amygdala and the orbitofrontal cortex and the interaction between them:\n\nIn mammals, emotional responses are processed in a part of the brain called the limbic system which lies in the cerebral cortex. The main components of the limbic system are the amygdala, orbitofrontal cortex, thalamus and the sensory cortex.\n\nThe amygdala is an almond shaped area which is placed such that it can communicate with all other cortices within the limbic system. The primary affective conditioning of the system occurs within the amygdala. That is, the association between a stimulus and its emotional consequence takes place in this region.\n\nIt has been suggested that learning takes place in two fundamental steps. First, a particular stimulus is correlated with an emotional response. This stimulus can be an endless number of phenomena from observing a face, to detecting a scent, hearing a noise, etc. Second, this emotional consequence shapes an association between the stimulus and the response. This analysis is quite influential in part because it was one of the first to suggest that emotions play a key part in learning. In more recent studies, it has been shown that the association between a stimulus and its emotional consequence take place in the amygdala. \"In this region, highly analyzed stimulus representations in the cortex are associated with an emotional value. Therefore, emotions are properties of stimuli\".\n\nThe task of the amygdala is thus to assign a primary emotional value to each stimulus that has been paired with a primary reinforcer – the reinforcer is the reward and punishment that the mammal receives. This task is aided by the orbitofrontal complex. \"In terms of learning theory, the amygdala appears to handle the presentation of primary reinforcement, while the orbitofrontal cortex is involved in the detection of omission of reinforcement.\"\n\nThe first thing we notice in the computational model developed by Moren and Balkenius is that quite a number of interacting learning systems exist in the brain that deal with emotional learning. The computational model is presented below where:\n\n\nThis image shows that the sensory input enters through the thalamus TH. In biological systems, the thalamus takes on the task of initiating the process of a response to stimuli. It does so by passing the signal to the amygdala and the sensory cortex.\n\nThis signal is then analyzed in the cortical area – CX. In biological systems, the sensory cortex operates by distributing the incoming signals appropriately between the amygdala and the orbitofrontal cortex. This sensory representation in CX is then sent to the amygdala A, through the pathway V.\n\nThis is the main pathway for learning in this model. Reward and punishment enter the amygdala to strengthen the connection between the amygdala and the pathway. At a later stage if a similar representation is activated in the cortex, E becomes activated and produces an emotional response.\n\nO, the orbitofrontal cortex, operates based on the difference between the \"perceived\" (i.e. expected) reward/punishment and the actual \"received\" reward/punishment. This perceived reward/punishment is the one that has been developed in the brain over time using learning mechanisms and it reaches the orbitofrontal cortex via the sensory cortex and the amygdala. The received reward/punishment on the other hand, comes courtesy of the outside world and is the \"actual\" reward/punishment that the specie has just obtained. If these two are identical, the output is the same as always through E. If not, the orbitofronal cortex inhibits and restrains emotional response to make way for further learning. So the path W is only activated in such conditions.\n\nIn most industrial processes that contain complex nonlinearities, control algorithms are used to create linearized models. One reason is that these linear models are developed using straightforward methods from process test data.\n\nHowever, if the process is highly complex and nonlinear, subject to frequent disturbances, a nonlinear model will be required. Biologically motivated intelligent controllers have been increasingly employed in these situations. Amongst them, fuzzy logic, neural networks and genetic algorithms are some of the most widely employed tools in control applications with highly complex, nonlinear settings.\n\nBELBIC is one such nonlinear controller – a neuromorphic controller based on the computational learning model shown to produce the control action. This model is employed much like an algorithm in these control engineering applications. In these new approaches, intelligence is not \"given\" to the system from the outside but is actually acquired by the system itself.\n\nThis simple model has been employed as a feedback controller to be applied to control design problems. One logic behind this use in control engineering is a belief held by many experts in the field that there has been too much focus on fully rational deliberative approaches, whereas in many real-world circumstances, we are only provided with a bounded rationality. Factors like computational complexity, multiplicity of objectives and prevalence of uncertainty lead to a desire to obtain more ad-hoc, rule-of-thumb approaches. Emotional decision making is highly capable of addressing these issues because it is neither fully cognitive nor fully behavioral.\n\nBELBIC, which is a model free controller, suffers from the same drawback of all intelligent model free controllers: it cannot be applied on unstable systems or systems with unstable equilibrium point. This is a natural result of the trial and error manner of the learning procedure, i.e. exploration for finding the appropriate control signals can lead to instability. By integrating imitative learning and fuzzy inference systems, BELBIC is generalized in order to be capable of controlling unstable systems.\n\nTo date, BELBIC and its modified versions have been tested on the following applications:\n\n\n\n"}
{"id": "21378384", "url": "https://en.wikipedia.org/wiki?curid=21378384", "title": "Bad Science (book)", "text": "Bad Science (book)\n\nBad Science is a book by Ben Goldacre, criticising mainstream media reporting on health and science issues. It was published by Fourth Estate in September 2008. It has been positively reviewed by the \"British Medical Journal\" and the \"Daily Telegraph\" and has reached the Top 10 bestseller list for Amazon Books. It was shortlisted for the 2009 Samuel Johnson Prize. Bad Science or BadScience is also the title of Goldacre's column in \"The Guardian\" and his website.\n\nA brief introduction (by Goldacre) touching on subjects covered by subsequent chapters. It bemoans the widespread lack of understanding of evidence-based science.\n\nChapter 1 is entitled Matter, but is really concerned with the modern trend for Detoxification. Goldacre looks at three supposed detox treatments: aqua detox (a footbath detox), Hopi Ear Candles, and detox patches. Each of these so-called treatments is intended to remove toxins and impurities from the body. \n\nAccording to Goldacre, the manufacturers of these detox remedies are unable or unwilling to state which toxins exactly are being removed from the body. Goldacre debunks the claims made for each of these products and says that the whole idea of detox is an invention. There are no such toxins floating around the body in excessive quantities, waiting to be removed by detox treatments. \n\nGoldacre has no problem with the idea of someone choosing to give their body a rest after overindulging, say. That is just common sense. But he sees detox treatments as the modern equivalent of religious rituals of purifcation or abstinence. Clearly, such rituals fill a human need in some way - Goldacre has no problem with that. What is wrong, however, is to pretend that these detox rituals are based in science. They are not. At worst, they are supposed quick-fixes that distract from the genuine lifestyle risk factors for ill health that affect us over the long term.\n\nBrain Gym is a set of exercises and activities that are supposed to 'enhance the experience of whole brain learning'. At the time when Goldacre’s book was written, Brain Gym was promoted by the Department for Education and used in hundreds of state schools across the country. But Goldacre says this is pseudoscience dressed up in clever long phrases and jargon. He goes on to describe a 2008 study that suggested that people will tend to believe a bad explanation written in sciencey terms, rather than a good explanation that isn't decorated with sciencey words.\n\nSome of the underlying ideas in Brain Gym are sensible: regular breaks, intermittent light exercise and drinking plenty of water are likely to help children learn. But Goldacre sees the pseudoscientific explanation around it (such as the 26 special movements and the ‘brain buttons’ concept) as an attempt to 'proprietorialise' common sense. That is, turn it into something that you can patent, own, sell and make profit from. He sees this trend particularly strongly among nutritionists. The corrosive side effect of this ‘privatisation of common sense’ is that we become dependent on outside systems and people, instead of taking control ourselves.\n\nIn this short chapter, Goldacre looks at cosmetics - specifically, moisturisers. According to Goldacre, expensive moisturisers tend to contain three groups of ingredients: powerful chemicals that \"were\" effective in making skin look younger, before they had to be watered down because of their side effects; vegetable protein, which does actually shrink wrinkles temporarily; and esoteric chemicals that are meant to 'make you believe that all sorts of claims are being made'. But the manufacturers are very careful to claim only that the moisturiser as a whole will have beneficial effects - they don't make specific claims about their 'magic ingredients', because such claims could be easily challenged by the regulator.\n\nInstead, the magic ingredients (such as the made-up Progenium XY Complex) are only included to make it sound like some complicated science is involved. And that is Goldacre's main complaint: The cosmetics companies sell their products by appealing to the misleading idea that science is complicated, incomprehensible, and impenetrable. This is bad because the target audience who are bombarded with this dubious world view are young women, a group who are under-represented in science.\n\nGoldacre provides an overview of the origins of Homeopathy (its ‘invention’ by Samuel Hahnemann in the late eighteenth century) and the basic ideas that characterise it: ‘like cures like’, the increase in potency by dilution, succussion, proving, and the collation of remedies in a reference book. He shows that the levels of dilution used in preparing homeopathic remedies are so high, that the final ‘medicine’ contains no active ingredient. He dismisses the idea of ‘water memory’, which has been used in more recent times to explain why homeopathic remedies still work, in spite of extreme dilution.\n\nAs far as Goldacre is concerned, it’s fine if someone wants to take a homeopathic remedy because ‘it made me feel better last time’. However, the experience of an individual (or a small group of people) cannot be used to as a basis for saying that homeopathy works or that it is science. First of all, an individual can have no way of knowing if they got better because of the homeopathic remedy they took, the placebo effect or regression to the mean (that is, the natural cycle of the disease). Secondly, homeopathic remedies should be subjected to a ‘fair test’: a placebo-controlled trial. In fact, such tests have been carried out for homeopathic remedies and it has been shown that they are no better than placebo.\n\nGoldacre says that some individual trials have shown that a homeopathic remedy works. But usually these trials are found to have methodological flaws. Typical problems with these trials have included a poor quality approach to blinding or randomisation. Another problem is that trials of homeopathic remedies often don’t provide full information about the methods used. Poor quality research studies tend to exaggerate positive results. Goldacre provides a summary of a paper by Ernst et al, which suggested that this has occurred in studies of homeopathic arnica. That said, Goldacre does concede that the overall experience of going to see a homeopath does seem to have a positive effect on some patients, and that would be worth investigating further.\n\nWhat we really need, says Goldacre, is meta-analyses. This is when the results of smaller research studies are pooled and analysed together as a single group. The Cochrane Collaboration was set up to carry out systematic reviews and meta-analyses. A landmark study by Shang et al (2005), which looked at a vast number of homeopathic trials, again found that homeopathic remedies perform no better than placebo. \n\nGoldacre criticises the homeopathic community for their lack of understanding of how to carry out high quality research, their lack of openness and transparency, their unwillingness to submit their research to full and proper scrutiny, their rejection of justified academic criticism and their overall aggressiveness. Using the specific example of an interview with Elizabeth Thompson, he illustrates how homeopaths will use nuanced language to avoid actually admitting that their pills don’t work.\n\nExamples of the power of the mind over pain, anxiety and depression are presented with studies showing how higher prices, fancy packaging, theatrical procedures and a confident attitude in the doctor all contribute to the relief of symptoms. In patients with no specific diagnosed condition, even a fake diagnosis and prognosis with no other treatment helps recovery, but ethical and time constraints usually prevent doctors from giving this reassurance. Exploiting the placebo effect is presented as possibly justifiable if used in conjunction with effective conventional treatments. The author links its use by alternative medicine practitioners with the diversion of patients away from effective treatments and the undermining of public health campaigns on AIDS and malaria.\n\nNutritionists are accused of misusing science and mystifying diet to bamboozle the public. Misrepresentations of the results of legitimate scientific research to lend bogus authority to nutritionist theories, while ignoring alternative explanations are cited in evidence. The use of weak circumstantial associations between diet and health found in observational studies as if they proved nutritionist claims is criticised. The unjustified over-interpretation of surrogate outcomes in animal (or tissue culture) experiments as proving human health benefits is explored. The cherry picking of published research to support a favoured view is contrasted with the systematic review designed to minimise such bias. The supposed benefits of antioxidants are questioned with studies showing they may be ineffective or even harmful in some cases. The methods used by the food supplement industry to manufacture doubt about any critical scientific reports are likened to those previously used by the tobacco and asbestos\n\nThe Scottish TV diet guru and self-styled \"doctor\" Gillian McKeith and her scientific claims are dissected. Statements exemplifying her scientific knowledge include that the consumption of dark-leaved vegetables like spinach \"will really oxygenate your blood\" as they are high in chlorophyll, and that \"each sprouting seed is packed with the nutritional energy needed to create a fully-grown, healthy plant\". She is described masquerading as a genuine medical doctor on her TV reality/health shows. Her publications are compared with a Melanesian cargo cult; superficially correct but lacking any scientific substance. Her belief in the special nutritional value of plant enzymes (which are broken down in the gut like any other proteins) is ridiculed. The general problems involved in establishing any firm links between diet and health are examined.\n\nThe claim that fish oil capsules make children smarter is examined. The book probes the methodological weaknesses of the widely publicised \"Durham trial\" where the pills were given to children to improve their school performance and behaviour, but without any control groups and wide open to a range of confounding factors. The failure to publish any results and backtracking on earlier claims by the education authorities is slated, with their refusal to divulge any data through Freedom of Information Requests specifically mentioned. The media's preference for simple science stories and role in promoting dubious health products is highlighted. Parallels are drawn between the Equazen company behind the Durham fish oil trials and the Efamol company's promotion of evening primrose oil.\n\nThe influence of the best-selling author, media commentator, businessman and founder of the Institute for Optimum Nutrition (which has trained most of the UK's \"nutrition therapists\") is acknowledged. Holford's success in presenting nutritionism as a scientific discipline in the media, and forging links with some British universities is also noted. The book judges that his success is based on misinterpreting and cherry-picking favourable results from the medical literature, in order to market his vitamin pills. His promotion of vitamin C in preference to AZT as a treatment for AIDS, vitamin E to prevent heart attacks, and vitamin A to treat autism are all condemned as lacking in sound evidential support. His reliance on the work of discredited fellow nutritionist Dr. R.K. Chandra is likewise slated. The Universities of Luton and Teesside are criticised for their past associations with Holford and the ION.\n\nThe book remarks on the relatively low percentage of conventional medical activity (50 to 80%) which could be called \"evidence-based\". The efforts of the medical profession to weed out bad treatments are seen to be hampered by the withholding or distortion of evidence by drug companies. The science and economics of drug development are outlined, with criticism of the lack of independence of industrial research and the neglect of Third World diseases. Some underhand tricks used by drug companies to engineer positive trial results for their products are explored. The publication bias produced by researchers not publishing negative results is illustrated with funnel plots. Examples are made of the SSRI antidepressants and Vioxx drugs. Reform of trials registers to prevent abuses is proposed. The ethics of drug advertising and manipulation of patient advocacy groups are questioned.\n\nThe misrepresentation of science and scientists in the media is attributed to the preponderance of humanities graduates in journalism. The dumbing-down of science to produce easily assimilated wacky, breakthrough or scare stories is criticised. Wacky \"formula stories\" like those for \"the perfect boiled egg\" or \"most depressing day of the year\" are revealed to be the product of PR companies using biddable academics to add weight to their marketing campaigns. Among other examples, the speculation by Dr. Oliver Curry (a political theorist at the LSE) that the human race will evolve into two separate races, presented as a science story across the British media, is exposed as a PR stunt for a men's TV channel. The relative scarcity of sensational medical breakthroughs since a golden age of discovery between 1935 and 1975, is seen as motivating the production of dumbed-down stories which trumpet unpublished research and ill-founded speculation. An inability to evaluate the soundness of scientific evidence is seen to give undeserved prominence to marginal figures with fringe views.\n\nThis chapter is a brief introduction to the research on cognitive biases, which, Goldacre argues, explain some of the appeal of alternative medicine ideas. Biases mentioned include confirmation bias, the availability heuristic, illusory superiority and the clustering illusion (the misperception of random data). It also discusses Solomon Asch's classic study of social conformity.\n\nThis chapter covers the cases of Sally Clark and Lucia de Berk, in which the author says poor understanding and presentation of statistics played an important part in their criminal trials.\n\nIn this chapter, the author claims that the press selectively used a \"laboratory\" that gave positive MRSA results where other pathology labs found none. Creating an \"expert\" from Chris Malyszewicz who worked from a garden shed.\n\nGoldacre notes how the Daily Mirror once managed to combine \"three all-time classic bogus science stories\" into one editorial: the Arpad Pusztai affair of GM crops, Andrew Wakefield and the MMR vaccine controversy and Chris Malyszewicz and the MRSA hoax. On the other hand, journalists were very poor in uncovering or reporting on the thalidomide tragedy - only covering well the ultimate political issue of compensation.\n\nAndrew Wakefield and the MMR vaccine controversy. The author continues to discuss the lab results in previous chapter and discusses the MRSA mix up in hospitals wrong patients get wrong results.\n\nThe hardback and first paperback editions did not include an index. Several indexes were prepared by bloggers, including one prepared by Oliblog. The latest paperback issue includes a full index.\n\nFurther to the release of this book a resolution of the legal status of one of the chapters has come about since Goldacre won a libel case filed against him by Matthias Rath. The post dated 9 April 2009 states: \"This is the 'missing chapter' about vitamin pill salesman Matthias Rath. Sadly I was unable to write about him at the time that book was initially published, as he was suing my ass in the High Court.\"\n\nThe full chapter has been made universally available under a Creative Commons license with the title \"The Doctor Will Sue You Now\". Additionally, this full chapter is included as chapter 10 in the New Paperback Edition.\n\nIn this chapter the author explains its origin, its reasons for being excluded, and describes his personal reasons and tribulations in the said legal resolution. It contains an account of his anger at being gagged due to legal/financial restrictions, his support by the \"Guardian\" (for whom he writes) and his now encyclopedic knowledge of the subject in question.\n\n\n"}
{"id": "17951275", "url": "https://en.wikipedia.org/wiki?curid=17951275", "title": "Banyan merchants", "text": "Banyan merchants\n\nBanyan merchants is an expression used widely in the Indian Ocean trade to refer to Indian merchants who are clearly distinguished from others, by their clothing, by their religious and cultural dietary choices, and by the manner in which they conduct trade.\n"}
{"id": "4481", "url": "https://en.wikipedia.org/wiki?curid=4481", "title": "Beatrix Potter", "text": "Beatrix Potter\n\nHelen Beatrix Potter (British English , North American English also , 28 July 186622 December 1943) was an English writer, illustrator, natural scientist, and conservationist best known for her children's books featuring animals, such as those in \"The Tale of Peter Rabbit\".\n\nBorn into an upper-class household, Potter was educated by governesses and grew up isolated from other children. She had numerous pets and spent holidays in Scotland and the Lake District, developing a love of landscape, flora, and fauna, all of which she closely observed and painted.\n\nThough Potter was typical of women of her generation in having limited opportunities for higher education, her study and watercolours of fungi led to her being widely respected in the field of mycology. In her thirties, Potter self-published the highly successful children's book \"The Tale of Peter Rabbit\". Following this, Potter began writing and illustrating children's books full-time.\n\nIn all, Potter wrote thirty books; the best known being her twenty-three children's tales. With the proceeds from the books and a legacy from an aunt, in 1905 Potter bought Hill Top Farm in Near Sawrey, a village in the Lake District which at that time was in Lancashire. Over the following decades, she purchased additional farms to preserve the unique hill country landscape. In 1913, at the age of 47, she married William Heelis, a respected local solicitor from Hawkshead. Potter was also a prize-winning breeder of Herdwick sheep and a prosperous farmer keenly interested in land preservation. She continued to write and illustrate, and to design spin-off merchandise based on her children's books for British publisher Warne, until the duties of land management and her diminishing eyesight made it difficult to continue.\n\nPotter died of pneumonia and heart disease on 22 December 1943 at her home in Near Sawrey at the age of 77, leaving almost all her property to the National Trust. She is credited with preserving much of the land that now constitutes the Lake District National Park. Potter's books continue to sell throughout the world in many languages with her stories being retold in song, film, ballet, and animation, and her life depicted in a feature film and television film.\n\nPotter's paternal grandfather, Edmund Potter, from Glossop in Derbyshire, owned what was then the largest calico printing works in England, and later served as a Member of Parliament.\n\nBeatrix's father, Rupert William Potter (1832–1914), was educated at Manchester College by the Unitarian philosopher Dr. James Martineau. He then trained as a barrister in London. Rupert practised law, specialising in equity law and conveyancing. He married Helen Leech (1839–1932) on 8 August 1863 at Hyde Unitarian Chapel, Gee Cross. Helen was the daughter of Jane Ashton (1806–1884) and John Leech, a wealthy cotton merchant and shipbuilder from Stalybridge. Helen's first cousins were Harriet Lupton (\"née\" Ashton) and Thomas Ashton, 1st Baron Ashton of Hyde. It was reported in July 2014 that Beatrix had personally given a number of her own original hand-painted illustrations to the two daughters of Dr Arthur and Harriet Lupton, who were cousins to both Beatrix and Catherine, Duchess of Cambridge.\n\nBeatrix's parents lived comfortably at 2 Bolton Gardens, West Brompton, where Helen Beatrix was born on 28 July 1866 and her brother Walter Bertram on 14 March 1872. Beatrix lived in the house until her marriage in 1913. The house was destroyed in the Blitz. Bousfield Primary School now stands where the house once was. A blue plaque on the school building testifies to the former site of The Potter home.\n\nBoth parents were artistically talented, and Rupert was an adept amateur photographer. Rupert had invested in the stock market and by the early 1890s was extremely wealthy.\n\nPotter's family on both sides were from the Manchester area. They were English Unitarians, associated with dissenting Protestant congregations, influential in 19th century England, that affirmed the oneness of God and that rejected the doctrine of the Trinity.\n\nBeatrix was educated by three able governesses, the last of whom was Annie Moore (\"née\" Carter), just three years older than Beatrix, who tutored Beatrix in German as well as acting as lady's companion. She and Beatrix remained friends throughout their lives and Annie's eight children were the recipients of many of Potter's delightful picture letters. It was Annie who later suggested that these letters might make good children's books.\n\nShe and her younger brother Walter Bertram (1872–1918) grew up with few friends outside their large extended family. Her parents were artistic, interested in nature, and enjoyed the countryside. As children, Beatrix and Bertram had numerous small animals as pets which they observed closely and drew endlessly. In their school room, Beatrix and Bertram kept a variety of small pets, mice, rabbits, a hedgehog and some bats, along with collections of butterflies and other insects which they drew and studied. Beatrix was devoted to the care of her small animals, often taking them with her on long holidays. In most of the first fifteen years of her life, Beatrix spent summer holidays at Dalguise, an estate on the River Tay in Perthshire, Scotland. There she sketched and explored an area that nourished her imagination and her observation. Beatrix and her brother were allowed great freedom in the country and both children became adept students of natural history. In 1887, when Dalguise was no longer available, the Potters took their first summer holiday in the Lake District, at Wray Castle near Lake Windermere. Here Beatrix met Hardwicke Rawnsley, vicar of Wray and later the founding secretary of the National Trust, whose interest in the countryside and country life inspired the same in Beatrix and who was to have a lasting impact on her life.\n\nAt about the age of 14, Beatrix began to keep a diary. It was written in a code of her own devising which was a simple letter for letter substitution. Her \"Journal\" was important to the development of her creativity, serving as both sketchbook and literary experiment: in tiny handwriting she reported on society, recorded her impressions of art and artists, recounted stories and observed life around her. The \"Journal\", decoded and transcribed by Leslie Linder in 1958, does not provide an intimate record of her personal life, but it is an invaluable source for understanding a vibrant part of British society in the late 19th century. It describes Potter's maturing artistic and intellectual interests, her often amusing insights on the places she visited, and her unusual ability to observe nature and to describe it. Started in 1881, her journal ends in 1897 when her artistic and intellectual energies were absorbed in scientific study and in efforts to publish her drawings. Precocious but reserved and often bored, she was searching for more independent activities and wished to earn some money of her own whilst dutifully taking care of her parents, dealing with her especially demanding mother, and managing their various households.\n\nBeatrix Potter's parents did not discourage higher education. As was common in the Victorian era, women of her class were privately educated and rarely went to university.\n\nBeatrix Potter was interested in every branch of natural science save astronomy. Botany was a passion for most Victorians and nature study was a popular enthusiasm. Potter was eclectic in her tastes: collecting fossils, studying archeological artefacts from London excavations, and interested in entomology. In all these areas she drew and painted her specimens with increasing skill. By the 1890s her scientific interests centred on mycology. First drawn to fungi because of their colours and evanescence in nature and her delight in painting them, her interest deepened after meeting Charles McIntosh, a revered naturalist and amateur mycologist, during a summer holiday in Dunkeld in Perthshire in 1892. He helped improve the accuracy of her illustrations, taught her taxonomy, and supplied her with live specimens to paint during the winter. Curious as to how fungi reproduced, Potter began microscopic drawings of fungus spores (the agarics) and in 1895 developed a theory of their germination. Through the connections of her uncle Sir Henry Enfield Roscoe, a chemist and vice-chancellor of the University of London, she consulted with botanists at Kew Gardens, convincing George Massee of her ability to germinate spores and her theory of hybridisation. She did not believe in the theory of symbiosis proposed by Simon Schwendener, the German mycologist, as previously thought; rather she proposed a more independent process of reproduction.\n\nRebuffed by William Thiselton-Dyer, the Director at Kew, because of her gender and her amateur status, Beatrix wrote up her conclusions and submitted a paper, \"On the Germination of the Spores of the Agaricineae\", to the Linnean Society in 1897. It was introduced by Massee because, as a female, Potter could not attend proceedings or read her paper. She subsequently withdrew it, realising that some of her samples were contaminated, but continued her microscopic studies for several more years. Her paper has only recently been rediscovered, along with the rich, artistic illustrations and drawings that accompanied it. Her work is only now being properly evaluated. Potter later gave her other mycological and scientific drawings to the Armitt Museum and Library in Ambleside, where mycologists still refer to them to identify fungi. There is also a collection of her fungus paintings at the Perth Museum and Art Gallery in Perth, Scotland, donated by Charles McIntosh. In 1967, the mycologist W.P.K. Findlay included many of Potter's beautifully accurate fungus drawings in his \"Wayside & Woodland Fungi\", thereby fulfilling her desire to one day have her fungus drawings published in a book. In 1997, the Linnean Society issued a posthumous apology to Potter for the sexism displayed in its handling of her research.\n\nPotter's artistic and literary interests were deeply influenced by fairies, fairy tales and fantasy. She was a student of the classic fairy tales of Western Europe. As well as stories from the Old Testament, John Bunyan's \"The Pilgrim's Progress\" and Harriet Beecher Stowe's \"Uncle Tom's Cabin\", she grew up with \"Aesop's Fables\", the fairy tales of the Brothers Grimm and Hans Christian Andersen, Charles Kingsley's \"The Water Babies\", the folk tales and mythology of Scotland, the German Romantics, Shakespeare, and the romances of Sir Walter Scott. As a young child, before the age of eight, Edward Lear's \"Book of Nonsense\", including the much loved \"The Owl and the Pussycat\", and Lewis Carroll's \"Alice in Wonderland\" had made their impression, although she later said of \"Alice\" that she was more interested in Tenniel's illustrations than what they were about. The \"Brer Rabbit\" stories of Joel Chandler Harris had been family favourites, and she later studied his \"Uncle Remus\" stories and illustrated them. She studied book illustration from a young age and developed her own tastes, but the work of the picture book triumvirate Walter Crane, Kate Greenaway and Randolph Caldecott, the last an illustrator whose work was later collected by her father, was a great influence. When she started to illustrate, she chose first the traditional rhymes and stories, \"Cinderella\", \"Sleeping Beauty\", \"Ali Baba and the Forty Thieves\", \"Puss-in-boots\", and \"Red Riding Hood\". But most often her illustrations were fantasies featuring her own pets: mice, rabbits, kittens, and guinea pigs.\n\nIn her teenage years, Potter was a regular visitor to the art galleries of London, particularly enjoying the summer and winter exhibitions at the Royal Academy in London. Her \"Journal\" reveals her growing sophistication as a critic as well as the influence of her father's friend, the artist Sir John Everett Millais, who recognised Beatrix's talent of observation. Although Potter was aware of art and artistic trends, her drawing and her prose style were uniquely her own.\n\nAs a way to earn money in the 1890s, Beatrix and her brother began to print Christmas cards of their own design, as well as cards for special occasions. Mice and rabbits were the most frequent subject of her fantasy paintings. In 1890, the firm of Hildesheimer and Faulkner bought several of her drawings of her rabbit Benjamin Bunny to illustrate verses by Frederic Weatherly titled \"A Happy Pair\". In 1893, the same printer bought several more drawings for Weatherly's \"Our Dear Relations\", another book of rhymes, and the following year Potter sold a series of frog illustrations and verses for \"Changing Pictures\", a popular annual offered by the art publisher Ernest Nister. Potter was pleased by this success and determined to publish her own illustrated stories.\n\nWhenever Potter went on holiday to the Lake District or Scotland, she sent letters to young friends, illustrating them with quick sketches. Many of these letters were written to the children of her former governess Annie Carter Moore, particularly to Moore's eldest son Noel who was often ill. In September 1893, Potter was on holiday at Eastwood in Dunkeld, Perthshire. She had run out of things to say to Noel and so she told him a story about \"four little rabbits whose names were Flopsy, Mopsy, Cottontail and Peter\". It became one of the most famous children's letters ever written and the basis of Potter's future career as a writer-artist-storyteller.\n\nIn 1900, Potter revised her tale about the four little rabbits, and fashioned a dummy book of it – it has been suggested, in imitation of Helen Bannerman's 1899 bestseller \"The Story of Little Black Sambo\". Unable to find a buyer for the work, she published it for family and friends at her own expense in December 1901. It was drawn in black and white with a coloured frontispiece. Family friend Canon Hardwicke Rawnsley had great faith in Potter's tale, recast it in didactic verse, and made the rounds of the London publishing houses. Frederick Warne & Co had previously rejected the tale but, eager to compete in the booming small format children's book market, reconsidered and accepted the \"bunny book\" (as the firm called it) following the recommendation of their prominent children's book artist L. Leslie Brooke. The firm declined Rawnsley's verse in favour of Potter's original prose, and Potter agreed to colour her pen and ink illustrations, choosing the then new Hentschel three-colour process to reproduce her watercolours.\n\nOn 2 October 1902, \"The Tale of Peter Rabbit\" was published, and was an immediate success. It was followed the next year by \"The Tale of Squirrel Nutkin\" and \"The Tailor of Gloucester\", which had also first been written as picture letters to the Moore children. Working with Norman Warne as her editor, Potter published two or three little books each year: 23 books in all. The last book in this format was \"Cecily Parsley's Nursery Rhymes\" in 1922, a collection of favourite rhymes. Although \"The Tale of Little Pig Robinson\" was not published until 1930, it had been written much earlier. Potter continued creating her little books until after the First World War, when her energies were increasingly directed toward her farming, sheep-breeding and land conservation.\n\nThe immense popularity of Potter's books was based on the lively quality of her illustrations, the non-didactic nature of her stories, the depiction of the rural countryside, and the imaginative qualities she lent to her animal characters.\n\nPotter was also a canny businesswoman. As early as 1903, she made and patented a Peter Rabbit doll. It was followed by other \"spin-off\" merchandise over the years, including painting books, board games, wall-paper, figurines, baby blankets and china tea-sets. All were licensed by Frederick Warne & Co and earned Potter an independent income, as well as immense profits for her publisher.\n\nIn 1905, Potter and Norman Warne became unofficially engaged. Potter's parents objected to the match because Warne was \"in trade\" and thus not socially suitable. The engagement lasted only one month until Warne died of pernicious anaemia at age 37. That same year, Potter used some of her income and a small inheritance from an aunt to buy Hill Top Farm in Near Sawrey in the English Lake District near Windermere. Potter and Warne may have hoped that Hill Top Farm would be their holiday home, but after Warne's death, Potter went ahead with its purchase as she had always wanted to own that farm, and live in \"that charming village\".\n\nThe tenant farmer John Cannon and his family agreed to stay on to manage the farm for her while she made physical improvements and learned the techniques of fell farming and of raising livestock, including pigs, cows and chickens; the following year she added sheep. Realising she needed to protect her boundaries, she sought advice from W.H. Heelis & Son, a local firm of solicitors with offices in nearby Hawkshead. With William Heelis acting for her she bought contiguous pasture, and in 1909 the Castle Farm across the road from Hill Top Farm. She visited Hill Top at every opportunity, and her books written during this period (such as \"The Tale of Ginger and Pickles\", about the local shop in Near Sawrey and \"The Tale of Mrs. Tittlemouse\", a wood mouse) reflect her increasing participation in village life and her delight in country living.\n\nOwning and managing these working farms required routine collaboration with the widely respected William Heelis. By the summer of 1912 Heelis had proposed marriage and Beatrix had accepted; although she did not immediately tell her parents, who once again disapproved because Heelis was only a country solicitor. Potter and Heelis were married on 15 October 1913 in London at St Mary Abbots in Kensington. The couple moved immediately to Near Sawrey, residing at Castle Cottage, the renovated farm house on Castle Farm, which was 34 acres large. Hill Top remained a working farm but was now remodelled to allow for the tenant family and Potter's private studio and workshop. At last her own woman, Potter settled into the partnerships that shaped the rest of her life: her country solicitor husband and his large family, her farms, the Sawrey community and the predictable rounds of country life. \"The Tale of Jemima Puddle-Duck\" and \"The Tale of Tom Kitten\" are representative of Hill Top Farm and of her farming life, and reflect her happiness with her country life.\n\nRupert Potter died in 1914 and, with the outbreak of World War I, Potter, now a wealthy woman, persuaded her mother to move to the Lake District and found a property for her to rent in Sawrey. Finding life in Sawrey dull, Helen Potter soon moved to Lindeth Howe (now a 34 bedroomed hotel) a large house the Potters had previously rented for the summer in Bowness, on the other side of Lake Windermere, Potter continued to write stories for Frederick Warne & Co and fully participated in country life. She established a Nursing Trust for local villages, and served on various committees and councils responsible for footpaths and other rural issues.\n\nSoon after acquiring Hill Top Farm, Potter became keenly interested in the breeding and raising of Herdwick sheep, the indigenous fell sheep. In 1923 she bought a large sheep farm in the Troutbeck Valley called Troutbeck Park Farm, formerly a deer park, restoring its land with thousands of Herdwick sheep. This established her as one of the major Herdwick sheep farmers in the county. She was admired by her shepherds and farm managers for her willingness to experiment with the latest biological remedies for the common diseases of sheep, and for her employment of the best shepherds, sheep breeders, and farm managers.\n\nBy the late 1920s Potter and her Hill Top farm manager Tom Storey had made a name for their prize-winning Herdwick flock, which took many prizes at the local agricultural shows, where Potter was often asked to serve as a judge. In 1942 she became President-elect of the Herdwick Sheepbreeders’ Association, the first time a woman had ever been elected, but died before taking office.\n\nPotter had been a disciple of the land conservation and preservation ideals of her long-time friend and mentor, Canon Hardwicke Rawnsley, the first secretary and founding member of the National Trust for Places of Historic Interest or Natural Beauty. She supported the efforts of the National Trust to preserve not just the places of extraordinary beauty but also those heads of valleys and low grazing lands that would be irreparably ruined by development. She was also an authority on the traditional Lakeland crafts, period furniture and stonework. She restored and preserved the farms that she bought or managed, making sure that each farm house had in it a piece of antique Lakeland furniture. Potter was interested in preserving not only the Herdwick sheep, but also the way of life of fell farming. In 1930 the Heelises became partners with the National Trust in buying and managing the fell farms included in the large Monk Coniston Estate. The estate was composed of many farms spread over a wide area of north-western Lancashire, including the Tarn Hows. Potter was the \"de facto\" estate manager for the Trust for seven years until the National Trust could afford to buy most of the property back from her. Her stewardship of these farms earned her wide regard, but she was not without her critics, not the least of which were her contemporaries who felt she used her wealth and the position of her husband to acquire properties in advance of their being made public. She was notable in observing the problems of afforestation, preserving the intake grazing lands, and husbanding the quarries and timber on these farms. All her farms were stocked with Herdwick sheep and frequently with Galloway cattle.\nPotter continued to write stories and to draw, although mostly for her own pleasure. Her books in the late 1920s included the semi-autobiographical \"The Fairy Caravan\", a fanciful tale set in her beloved Troutbeck fells. It was published only in the US during Potter's lifetime, and not until 1952 in the UK. \"Sister Anne\", Potter's version of the story of Bluebeard, was written especially for her American readers, but illustrated by Katharine Sturges. A final folktale, \"Wag by Wall\", was published posthumously by \"The Horn Book Magazine\" in 1944. Potter was a generous patron of the Girl Guides, whose troupes she allowed to make their summer encampments on her land, and whose company she enjoyed as an older woman.\n\nPotter and William Heelis enjoyed a happy marriage of thirty years, continuing their farming and preservation efforts throughout the hard days of World War II. Although they were childless, Potter played an important role in William's large family, particularly enjoying her relationship with several nieces whom she helped educate, and giving comfort and aid to her husband's brothers and sisters.\n\nPotter died of complications from pneumonia and heart disease on 22 December 1943 at Castle Cottage, and her remains were cremated at Carleton Crematorium. She left nearly all her property to the National Trust, including over of land, sixteen farms, cottages and herds of cattle and Herdwick sheep. Hers was the largest gift at that time to the National Trust, and it enabled the preservation of the land now included in the Lake District National Park and the continuation of fell farming. The central office of the National Trust in Swindon was named \"Heelis\" in 2005 in her memory. William Heelis continued his stewardship of their properties and of her literary and artistic work for the eighteen months he survived her. When he died in August 1945 he left the remainder to the National Trust.\n\nPotter left almost all the original illustrations for her books to the National Trust. The copyright to her stories and merchandise was then given to her publisher Frederick Warne & Co, now a division of the Penguin Group. On 1 January 2014, the copyright expired in the UK and other countries with a 70-years-after-death limit. Hill Top Farm was opened to the public by the National Trust in 1946; her artwork was displayed there until 1985 when it was moved to William Heelis's former law offices in Hawkshead, also owned by the National Trust as the Beatrix Potter Gallery.\n\nPotter gave her folios of mycological drawings to the Armitt Library and Museum in Ambleside before her death. \"The Tale of Peter Rabbit\" is owned by Frederick Warne and Company, \"The Tailor of Gloucester\" by the Tate Gallery and \"The Tale of the Flopsy Bunnies\" by the British Museum.\n\nThe largest public collection of her letters and drawings is the Leslie Linder Bequest and Leslie Linder Collection at the Victoria and Albert Museum in London. In the United States, the largest public collections are those in the Rare Book Department of the Free Library of Philadelphia, and the Cotsen Children's Library at Princeton University.\n\nIn 2015 a manuscript for an unpublished book was discovered by Jo Hanks, a publisher at Penguin Random House Children's Books, in the Victoria and Albert Museum archive. The book \"The Tale of Kitty-in-Boots\", with illustrations by Quentin Blake, was published 1 September 2016, to mark the 150th anniversary of Potter's birth.\n\nIn 2017, \"The Art of Beatrix Potter: Sketches, Paintings, and Illustrations\" by Emily Zach was published after San Francisco publisher Chronicle Books decided to mark the 150th anniversary of Beatrix Potter's birth by showing that she was \"far more than a 19th-century weekend painter. She was an artist of astonishing range.\"\n\nIn December 2017, the asteroid 13975 Beatrixpotter, discovered by Belgian astronomer Eric Elst in 1992, was named in her memory.\n\nThere are many interpretations of Potter's literary work, the sources of her art, and her life and times. These include critical evaluations of her corpus of children's literature, and Modernist interpretations of Humphrey Carpenter and Katherine Chandler. Judy Taylor, \"That Naughty Rabbit: Beatrix Potter and Peter Rabbit\" (rev. 2002) tells the story of the first publication and many editions.\n\nPotter’s country life and her farming has also been widely discussed in the work of Susan Denyer and by other authors in the publications of The National Trust.\n\nPotter's work as a scientific illustrator and her work in mycology is highlighted in several chapters in Linda Lear, \"Beatrix Potter: A Life in Nature\", 2007; \"Beatrix Potter: The Extraordinary Life of a Victorian Genius\". 2008.\n\nIn 1971, a ballet film was released, \"The Tales of Beatrix Potter\", directed by Reginald Mills, set to music by John Lanchbery with choreography by Frederick Ashton, and performed in character costume by members of the Royal Ballet and the Royal Opera House orchestra. The ballet of the same name has been performed by other dance companies around the world.\n\nIn 1992, Potter's famous children's book \"The Tale of Benjamin Bunny\" was featured in the film \"Lorenzo's Oil\".\n\nPotter is also featured in Susan Wittig Albert's series of light mysteries called The Cottage Tales of Beatrix Potter. The first of the eight-book series is \"Tale of Hill Top Farm\" (2004), which deals with Potter's life in the Lake District and the village of Near Sawrey between 1905 and 1913.\n\nMore recently, John Patrick is adapting a number of Beatrix Potter's tales into an upcoming live-action/animated musical feature film for his brand-new film studio, called Storybook Studio. The film will be titled Beatrix Potter's The Tales of Peter Rabbit and Friends . English actress Jackie Weiner will play Beatrix Potter herself, with the voices of Sienna Adams as Peter Rabbit, Ronan McCoid as Benjamin Bunny, Ella Bradley as Tom Kitten, Kyle Tanis as Mr. Jeremy Fisher, Leslie Fanelli as Mrs. Tiggy-Winkle and Karen Zikas as Jemima Puddle-Duck. John Patrick has released several clips of his upcoming film to YouTube. \n\nIn 1982, the BBC produced \"The Tale of Beatrix Potter\". This dramatisation of her life was written by John Hawkesworth, directed by Bill Hayes, and starred Holly Aird and Penelope Wilton as the young and adult Beatrix, respectively. \"The World of Peter Rabbit and Friends\", a TV series based on her stories, which starred actress Niamh Cusack as Beatrix Potter, has been released on VHS by Pickwick Video and later Carlton Video.\n\nIn 2006, Chris Noonan directed \"Miss Potter\", a biographical film of Potter's life focusing on her early career and romance with her editor Norman Warne. The film stars Renée Zellweger, Ewan McGregor and Emily Watson.\n\nOn February 9, 2018, Columbia Pictures released \"Peter Rabbit\", directed by Will Gluck, based on the work by Potter.\n\nThe 23 Tales\n\nOther books\n\n\n\n\n\n"}
{"id": "44625033", "url": "https://en.wikipedia.org/wiki?curid=44625033", "title": "Bersame Glacier", "text": "Bersame Glacier\n\nBersame Glacier (, ‘Lednik Bersame’ \\'led-nik 'ber-sa-me\\) is the 2.4 km long and 1.5 km wide glacier on the west side of Urda Ridge on Clarence Island in the South Shetland Islands, Antarctica situated northeast of Giridava Glacier. It drains the slopes of Mount Llana, flows northwestwards and enters the Southern Ocean northeast of the terminus of Giridava Glacier.\n\nThe glacier is named after the Thracian settlement of Bersame in Southeastern Bulgaria.\n\nBersame Glacier is centred at . British mapping in 1972 and 2009.\n\n\n\n"}
{"id": "255954", "url": "https://en.wikipedia.org/wiki?curid=255954", "title": "DNA microarray", "text": "DNA microarray\n\nA DNA microarray (also commonly known as DNA chip or biochip) is a collection of microscopic DNA spots attached to a solid surface. Scientists use DNA microarrays to measure the expression levels of large numbers of genes simultaneously or to genotype multiple regions of a genome. Each DNA spot contains picomoles (10 moles) of a specific DNA sequence, known as \"probes\" (or \"reporters\" or \"oligos\"). These can be a short section of a gene or other DNA element that are used to hybridize a cDNA or cRNA (also called anti-sense RNA) sample (called \"target\") under high-stringency conditions. Probe-target hybridization is usually detected and quantified by detection of fluorophore-, silver-, or chemiluminescence-labeled targets to determine relative abundance of nucleic acid sequences in the target. The original nucleic acid arrays were macro arrays approximately 9 cm × 12 cm and the first computerized image based analysis was published in 1981.\n\nThe core principle behind microarrays is hybridization between two DNA strands, the property of complementary nucleic acid sequences to specifically pair with each other by forming hydrogen bonds between complementary nucleotide base pairs. A high number of complementary base pairs in a nucleotide sequence means tighter non-covalent bonding between the two strands. After washing off non-specific bonding sequences, only strongly paired strands will remain hybridized. Fluorescently labeled target sequences that bind to a probe sequence generate a signal that depends on the hybridization conditions (such as temperature), and washing after hybridization. Total strength of the signal, from a spot (feature), depends upon the amount of target sample binding to the probes present on that spot. Microarrays use relative quantitation in which the intensity of a feature is compared to the intensity of the same feature under a different condition, and the identity of the feature is known by its position.\n\nMany types of arrays exist and the broadest distinction is whether they are spatially arranged on a surface or on coded beads:\n\nDNA microarrays can be used to detect DNA (as in comparative genomic hybridization), or detect RNA (most commonly as cDNA after reverse transcription) that may or may not be translated into proteins. The process of measuring gene expression via cDNA is called expression analysis or expression profiling.\n\nApplications include:\n\nMicroarrays can be manufactured in different ways, depending on the number of probes under examination, costs, customization requirements, and the type of scientific question being asked. Arrays from commercial vendors may have as few as 10 probes or as many as 5 million or more micrometre-scale probes.\nMicroarrays can be fabricated using a variety of technologies, including printing with fine-pointed pins onto glass slides, photolithography using pre-made masks, photolithography using dynamic micromirror devices, ink-jet printing, or electrochemistry on microelectrode arrays.\n\nIn \"spotted microarrays\", the probes are oligonucleotides, cDNA or small fragments of PCR products that correspond to mRNAs. The probes are synthesized prior to deposition on the array surface and are then \"spotted\" onto glass. A common approach utilizes an array of fine pins or needles controlled by a robotic arm that is dipped into wells containing DNA probes and then depositing each probe at designated locations on the array surface. The resulting \"grid\" of probes represents the nucleic acid profiles of the prepared probes and is ready to receive complementary cDNA or cRNA \"targets\" derived from experimental or clinical samples.\nThis technique is used by research scientists around the world to produce \"in-house\" printed microarrays from their own labs. These arrays may be easily customized for each experiment, because researchers can choose the probes and printing locations on the arrays, synthesize the probes in their own lab (or collaborating facility), and spot the arrays. They can then generate their own labeled samples for hybridization, hybridize the samples to the array, and finally scan the arrays with their own equipment. This provides a relatively low-cost microarray that may be customized for each study, and avoids the costs of purchasing often more expensive commercial arrays that may represent vast numbers of genes that are not of interest to the investigator.\nPublications exist which indicate in-house spotted microarrays may not provide the same level of sensitivity compared to commercial oligonucleotide arrays, possibly owing to the small batch sizes and reduced printing efficiencies when compared to industrial manufactures of oligo arrays.\n\nIn \"oligonucleotide microarrays\", the probes are short sequences designed to match parts of the sequence of known or predicted open reading frames. Although oligonucleotide probes are often used in \"spotted\" microarrays, the term \"oligonucleotide array\" most often refers to a specific technique of manufacturing. Oligonucleotide arrays are produced by printing short oligonucleotide sequences designed to represent a single gene or family of gene splice-variants by synthesizing this sequence directly onto the array surface instead of depositing intact sequences. Sequences may be longer (60-mer probes such as the Agilent design) or shorter (25-mer probes produced by Affymetrix) depending on the desired purpose; longer probes are more specific to individual target genes, shorter probes may be spotted in higher density across the array and are cheaper to manufacture.\nOne technique used to produce oligonucleotide arrays include photolithographic synthesis (Affymetrix) on a silica substrate where light and light-sensitive masking agents are used to \"build\" a sequence one nucleotide at a time across the entire array. Each applicable probe is selectively \"unmasked\" prior to bathing the array in a solution of a single nucleotide, then a masking reaction takes place and the next set of probes are unmasked in preparation for a different nucleotide exposure. After many repetitions, the sequences of every probe become fully constructed. More recently, Maskless Array Synthesis from NimbleGen Systems has combined flexibility with large numbers of probes.\n\n\"Two-color microarrays\" or \"two-channel microarrays\" are typically hybridized with cDNA prepared from two samples to be compared (e.g. diseased tissue versus healthy tissue) and that are labeled with two different fluorophores. Fluorescent dyes commonly used for cDNA labeling include Cy3, which has a fluorescence emission wavelength of 570 nm (corresponding to the green part of the light spectrum), and Cy5 with a fluorescence emission wavelength of 670 nm (corresponding to the red part of the light spectrum). The two Cy-labeled cDNA samples are mixed and hybridized to a single microarray that is then scanned in a microarray scanner to visualize fluorescence of the two fluorophores after excitation with a laser beam of a defined wavelength. Relative intensities of each fluorophore may then be used in ratio-based analysis to identify up-regulated and down-regulated genes.\n\nOligonucleotide microarrays often carry control probes designed to hybridize with RNA spike-ins. The degree of hybridization between the spike-ins and the control probes is used to normalize the hybridization measurements for the target probes. Although absolute levels of gene expression may be determined in the two-color array in rare instances, the relative differences in expression among different spots within a sample and between samples is the preferred method of data analysis for the two-color system. Examples of providers for such microarrays includes Agilent with their Dual-Mode platform, Eppendorf with their DualChip platform for colorimetric Silverquant labeling, and TeleChem International with Arrayit.\n\nIn \"single-channel microarrays\" or \"one-color microarrays\", the arrays provide intensity data for each probe or probe set indicating a relative level of hybridization with the labeled target. However, they do not truly indicate abundance levels of a gene but rather relative abundance when compared to other samples or conditions when processed in the same experiment. Each RNA molecule encounters protocol and batch-specific bias during amplification, labeling, and hybridization phases of the experiment making comparisons between genes for the same microarray uninformative. The comparison of two conditions for the same gene requires two separate single-dye hybridizations. Several popular single-channel systems are the Affymetrix \"Gene Chip\", Illumina \"Bead Chip\", Agilent single-channel arrays, the Applied Microarrays \"CodeLink\" arrays, and the Eppendorf \"DualChip & Silverquant\". One strength of the single-dye system lies in the fact that an aberrant sample cannot affect the raw data derived from other samples, because each array chip is exposed to only one sample (as opposed to a two-color system in which a single low-quality sample may drastically impinge on overall data precision even if the other sample was of high quality). Another benefit is that data are more easily compared to arrays from different experiments as long as batch effects have been accounted for.\n\nOne channel microarray may be the only choice in some situations. Suppose formula_1 samples need to be compared: then the number of experiments required using the two channel arrays quickly becomes unfeasible, unless a sample is used as a reference. \nThis is an example of a DNA microarray experiment, detailing a particular case to better explain DNA microarray experiments, while enumerating possible alternatives.\n\n\nThe advent of inexpensive microarray experiments created several specific bioinformatics challenges:\n\n\nDue to the biological complexity of gene expression, the considerations of experimental design that are discussed in the expression profiling article are of critical importance if statistically and biologically valid conclusions are to be drawn from the data.\n\nThere are three main elements to consider when designing a microarray experiment. First, replication of the biological samples is essential for drawing conclusions from the experiment. Second, technical replicates (two RNA samples obtained from each experimental unit) help to ensure precision and allow for testing differences within treatment groups. The biological replicates include independent RNA extractions and technical replicates may be two aliquots of the same extraction. Third, spots of each cDNA clone or oligonucleotide are present as replicates (at least duplicates) on the microarray slide, to provide a measure of technical precision in each hybridization. It is critical that information about the sample preparation and handling is discussed, in order to help identify the independent units in the experiment and to avoid inflated estimates of statistical significance.\n\nMicroarray data is difficult to exchange due to the lack of standardization in platform fabrication, assay protocols, and analysis methods. This presents an interoperability problem in bioinformatics. Various grass-roots open-source projects are trying to ease the exchange and analysis of data produced with non-proprietary chips:\n\n\nMicroarray data sets are commonly very large, and analytical precision is influenced by a number of variables. Statistical challenges include taking into account effects of background noise and appropriate normalization of the data. Normalization methods may be suited to specific platforms and, in the case of commercial platforms, the analysis may be proprietary. Algorithms that affect statistical analysis include:\n\n\n\nMicroarray data may require further processing aimed at reducing the dimensionality of the data to aid comprehension and more focused analysis. Other methods permit analysis of data consisting of a low number of biological or technical replicates; for example, the Local Pooled Error (LPE) test pools standard deviations of genes with similar expression levels in an effort to compensate for insufficient replication.\n\nThe relation between a probe and the mRNA that it is expected to detect is not trivial. Some mRNAs may cross-hybridize probes in the array that are supposed to detect another mRNA. In addition, mRNAs may experience amplification bias that is sequence or molecule-specific. Thirdly, probes that are designed to detect the mRNA of a particular gene may be relying on genomic EST information that is incorrectly associated with that gene.\n\nMicroarray data was found to be more useful when compared to other similar datasets. The sheer volume of data, specialized formats (such as MIAME), and curation efforts associated with the datasets require specialized databases to store the data. A number of open-source data warehousing solutions, such as InterMine and BioMart, have been created for the specific purpose of integrating diverse biological datasets, and also support analysis.\n\nAdvances in massively parallel sequencing has led to the development of RNA-Seq technology, that enables a whole transcriptome shotgun approach to characterize and quantify gene expression. Unlike microarrays, which need a reference genome and transcriptome to be available before the microarray itself can be designed, RNA-Seq can also be used for new model organisms whose genome has not been sequenced yet.\n\n\n\n"}
{"id": "297039", "url": "https://en.wikipedia.org/wiki?curid=297039", "title": "Deposition (chemistry)", "text": "Deposition (chemistry)\n\nIn chemistry, deposition occurs when molecules settle out of a solution.\n\nDeposition can be viewed as a reverse process to dissolution or particle re-entrainment.\n\n"}
{"id": "12528897", "url": "https://en.wikipedia.org/wiki?curid=12528897", "title": "Desert dormouse", "text": "Desert dormouse\n\nThe desert dormouse (\"Selevinia betpakdalaensis\") is a species of rodent in the dormouse family, Gliridae. This species was formerly placed in its own family, Seleviniidae, but it is now considered to be a dormouse, monotypic within the genus Selevinia.\nIt is endemic to Kazakhstan.\n\nThe desert dormouse was first described in 1939 by Belosludov & Bazhanov as \"Selevinia betpakdalaensis\", the specific name being derived from the Betpak-Dala Desert, west of Lake Balkhash in Kazakhstan, where the type specimen was found. They included it in the rat and mouse family Muridae but later proposed placing it in a new family allied to Myoxidae (some taxonomists call this family Gliridae), the dormice. By 1947, they had concluded that it should be placed in Leithiinae, a subfamily of Myoxidae, along with three other dormouse genera.\n\nThis dormouse has a head-and-body length of between and a tail of between . It has a robust, rounded body and soft dense fur, the upper parts being grey and the underparts white. Unusually for a mammal, it sheds the upper layers of its skin with the hairs when it moults, starting at the back of the neck and continuing along the body and flanks, the whole process taking about a month. The tail is well clad with short hairs and the soles of the feet are naked. The upper incisors are large but the cheek teeth are small and barely project through the gums.\n\nThe species lives in sandy or clayey soils in arid habitats. It is mainly a nocturnal animal but does sometimes venture into the open during the day. It mostly walks along in a leisurely fashion but if alarmed, can move faster in a series of short bounds. It lives among desert shrubs such as \"Artemisia maritima\" and \"Salsola laricifolia \" and can climb efficiently. These animals have been little studied in the wild, but in captivity may shelter under rocks and foliage when the weather is warm and only burrow underground during cold periods. It mostly feeds on insects and spiders, but has been known to consume vegetation. It hibernates during the coldest months of the year, only being active between about March and September. Pregnant and lactating females have been found in late May and June, with litter sizes of four to eight young being recorded.\n\n\"S. betpakdalaensis\" has a large range in Kazakhstan but is not abundant, being known from only about forty individuals scattered over about thirty locations. The total population and its trend are unknown and it may be threatened by the loss of the desert shrubs among which it lives. Because of these uncertainties, the International Union for Conservation of Nature has assessed its conservation status as \"data deficient\".\n"}
{"id": "16343931", "url": "https://en.wikipedia.org/wiki?curid=16343931", "title": "Engineering and the Mind's Eye", "text": "Engineering and the Mind's Eye\n\nEngineering and the Mind's Eye (1992) is a book by Eugene S. Ferguson, an engineer and historian of science and technology. It was published by MIT Press. In it, Ferguson discusses the importance of the mind's eye for the practicing engineer, including spatial visualization and visual thinking.\n\nA major argument of the book is summarized as follows in the preface:\nThe book comprises 7 chapters and two additional sections on notes about the text and its figures. The chapters are:\n\n"}
{"id": "20330299", "url": "https://en.wikipedia.org/wiki?curid=20330299", "title": "Environmental politics", "text": "Environmental politics\n\nEnvironmental politics designate both the politics about the environment (see also environmental policy) and an academic field of study focused on three core components:\n\n\nNeil Carter, in his foundational text \"Politics of the Environment\" (2009), suggests that \"environmental\" politics is distinct in at least two ways: first, \"it has a primary concern with the relationship between human society and the natural world\" (page 3); and second, \"unlike most other single issues, it comes replete with its own ideology and political movement\" (page 5, drawing on Michael Jacobs, ed., \"Greening the Millenium?\", 1997).\n\nFurther, he distinguishes between modern and earlier forms of environmental politics, in particular conservationism and preservationism. Contemporary environmental politics \"was driven by the idea of a global ecological crisis that threatened the very existence of humanity.\" And \"modern environmentalism was a political and activist mass movement which demanded a radical transformation in the values and structures of society.\"\n\nEnvironmental concerns were rooted in the vast social changes that took place in the United States after World War II. Although environmentalism can be identified in earlier years, only after the war did it become widely shared social priority. This began with outdoor recreation in the 1950s, extended into the wider field of the protection of natural environments, and then became infused with attempts to cope with air and water pollution and still later with toxic chemical pollutants. After World War II, environmental politics became a major public concern. The development of environmentalism in the United Kingdom emerged in this period following the great London smog of 1952 and the Torrey Canyon oil spill of 1967. This is reflected by the emergence of Green politics in the Western world beginning in the 1970s.\n\nClimate change is slow relative to political cycles of leadership in electoral democracies, which impedes responses by politicians who are elected and re-elected on much shorter timescales.\n\nIn the United States, although \"environmentalism\" was once considered a White phenomenon, scholars have identified \"pro-environment positions among Latino, African-American, and non-Hispanic white respondents,\" with growing environmental concern especially among Latinos. Other scholars have similarly noted that Asian Americans are strongly pro-environmental, with some variation among ethnic subgroups.\n\nEffectively responding to global warming necessitates some form of international environmental governance to achieve shared targets related to energy consumption and environmental usage. Climate change complicates political ideology and practice, affecting conceptions of responsibility for future societies as well as economic systems. Material inequality between nations make technological solutions insufficient for climate change mitigation. Rather, political solutions can navigate the particularities of various facets of environmental crisis. Climate change mitigation strategies can be at odds with democratic priorities of prosperity, progress, and state sovereignty, and instead underscore a collective relationship with the environment.\n\nThe international political community is presently based on liberal principles that prioritize individual freedoms and capitalist systems that make quick and ambitious climate responses difficult. Interest-group liberalism is guided by individual human priorities. Groups unable to voice their self-interest, such as minorities without suffrage, or non-humans, are not included in the political compromise. Addressing environmental crises can be impeded when citizens of liberal democracies do not see environmental problems as impacting their lives, or when they lack the education to evaluate the importance of the problem. The human benefits from environmental exploitation and protection compete. Considering the implications of ecological degradation for future human generations can give environmental concerns a basis in anthropocentric liberal democratic politics.\n\nWilliam Ophuls posits that liberal democracies are unfit to address environmental problems, and that the prioritization of these challenges would involve a transition to more authoritarian forms of government. Others counter this by pointing to the past successes of environmental reform movements to improve water and air quality in liberal societies. In practice, environmentalism can improve democracy rather than necessitate its end, by expanding democratic participation and promoting political innovations.\n\nThe tensions between liberal democracy and environmental goals raise questions about the possible limitations of democracy (or at least democracy as we know it): in its responsiveness to subtle but large-scale problems, its ability to work from a holistic societal perspective, its aptness in coping with environmental crisis relative to other forms of government. Democracies do not have the provisions to make environmental reforms that are not mandated by voters, and many voters lack incentives or desire to demand policies that could compromise immediate prosperity. The question arises as to whether the foundation of politics is morality or practicality. A scheme that conceives of and values the environment beyond its human utility, an environmental ethics, could be crucial for democracies to respond to climate change.\n\nIn political theory, deliberative democracy has been discussed as a political model more compatible with environmental goals. Deliberative democracy is a system in which informed political equals weigh values, information, and expertise, and debate priorities to make decisions, as opposed to a democracy based on interest aggregation. This definition of democracy emphasizes informed discussion among citizens in the decision making process, and encourages decisions to benefit the common good rather than individual interests. Amy Gutmann and Dennis Thompson claimed that reason prevails over self-interest in deliberative democracy, making it a more just system. The broad perspective that this discursive model encourages could lead to a stronger engagement with environmental concerns.\n\nIn political theory, the lottery system is a democratic design that allows governments to address problems with future, rather than immediate, impacts. Deliberative bodies composed of randomly selected representatives can draft environmental policies that have short-term costs without considering the political consequences for re-election.\n\nNew materialism is a strain of thought in philosophy and the social sciences that conceives of all material as having life or agency. It criticizes frameworks of justice that center on human attributes like consciousness as insufficient for modern ethical problems that concern the natural environment. It is a post-humanist consideration of all matter that rejects arguments of utility that privilege humans. This politically relevant social theory combats inequality beyond the interpersonal plane. People are ethically responsible for one another, and for the physical spaces they navigate, including animal and plant life, and the inanimate matter that sustains it, like soil. New materialism encourages political action according to this world vision, even if it is incompatible with economic growth.\n\nJane Bennett uses the term \"vital materialism\" in her book \"Vibrant Matter: A Political Ecology of Things\". She develops the concept of materialism with the aim of providing a stronger basis in political theory for environmental politics.\n\nNew materialists have invoked Derrida and other historical thinkers to trace the emergence of their philosophy and to justify their environmental claims:\"No justice ... seems possible or thinkable without the principle of some responsibility, beyond all living present, within that which disjoins the living present, before the ghosts of those who are not yet born or who are already dead [...]. Without this non-contemporaneity with itself of the living present ... without this responsibility and this respect for justice concerning those who are not there, of those who are no longer or who are not yet present and living, what sense would there be to ask the question 'where?' 'where tomorrow?' 'whither?'\" All material, living and dead, is interrelated in \"the mesh\" as described by Timothy Morton. As all matter is interdependent, humans have obligations to all parts of the material world, including those that are unfamiliar.\n\nNew materialism is related to a shift from the view of the environment as a form of capital to a form of labor (see Ecosystem services).\n\nBrazil, Russia, India, and China (known as the \"BRIC\" nations) are rapidly industrializing, and are increasingly responsible for global carbon emissions and the associated climate change. Other forms of environmental degradation have also accompanied the economic growth in these nations. Environmental degradation tends to motivate action more than the threat of global warming does, since air and water pollution cause immediate health problems, and because pollutants can damage natural resources, hampering economic potential.\n\nWith rising incomes, environmental degradation tends to decrease in industrializing nations, as depicted in the Environmental Kuznets Curve (described in a section of the Kuznets Curve article). Citizens demand better air and water quality, and technology becomes more efficient and clean when incomes increase. The level of income per capita needed to reverse the trend of environmental degradation in industrializing nations varies with the environmental impact indicator. More developed nations can facilitate eco-friendly transitions in emerging economies by investing in the development of clean technologies. \n\nLaws implemented in response to environmental concerns vary by nation (see List of environmental laws by country).\n\nChina's environmental ills include acid rain, severe smog, and a reliance on coal-burning for energy. China has instated environmental policies since the 1970s, and has one of the most extensive environmental conservation programs on paper. However, regulation and enforcement by the central government in Beijing are weak, so solutions are decentralized. Wealthier provinces are far more effective in their preservation and sustainable development efforts than poorer regions. China therefore provides an example of the consequences of environmental damage falling disproportionately on the poor. NGOs, the media, and the international community have all contributed to China's response to environmental problems.\n\nFor history, laws, and policies, see Environmental policy in China.\n\nIn 1976, the Constitution of India was amended to reflect environmental priorities, motivated in part by the potential threat of natural resource depletion to economic growth:\"The State shall endeavour to protect and improve the environment and to safeguard the forests and wildlife.\" (Art. 48A)\"It shall be the duty of every citizen of India [...] to protect and improve the natural environment including forests, lakes, rivers and wildlife, and to have compassion for living creatures.\" (Art. 51A)However, in India, as in China, the implementation of written environmental policies, laws, and amendments has proven challenging. Official legislation by the central government (see a partial list at Environmental policy of the Government of India) is often more symbolic than practical. The Ministry of Environment and Forests was established in 1985, but corruption within bureaucratic agencies, namely the influence of wealthy industry leaders, limited any attempts at enforcement of the policies put in place.\n\nScholarly journals representing this field of study include:\n\n\n"}
{"id": "42572381", "url": "https://en.wikipedia.org/wiki?curid=42572381", "title": "Epitome Astronomiae Copernicanae", "text": "Epitome Astronomiae Copernicanae\n\nThe Epitome Astronomiae Copernicanae was an astronomy book on the heliocentric system published by Johannes Kepler in the period 1617 to 1621. The first volume (books I–III) was printed in 1617, the second (book IV) in 1620, and the third (books V–VII) in 1621.\n\nThe book contained in particular the first version in print of his third law of planetary motion. The work was intended as a textbook, and the first part was written by 1615. Divided into seven books, the \"Epitome\" covers much of Kepler's earlier thinking, as well as his later positions on physics, metaphysics and archetypes. In Book IV he supported the Copernican cosmology. Book V provided mathematics underpinning Kepler's views. Kepler wrote and published this work in parallel with his \"Harmonices Mundi\" (1619), the last Books V to VII appearing in 1621.\n\nThe term \"inertia\" was first introduced in the \"Epitome\".\n\nThe first volume was put on the Index of Prohibited Books on 28th of February 1619.\n\n\n"}
{"id": "53236762", "url": "https://en.wikipedia.org/wiki?curid=53236762", "title": "Epitranscriptomic sequencing", "text": "Epitranscriptomic sequencing\n\nIn epitranscriptomic sequencing, most methods focus on either (1) enrichment and purification of the modified RNA molecules before running on the RNA sequencer, or (2) improving or modifying bioinformatics analysis pipelines to call the modification peaks. Most methods have been adapted and optimized for mRNA molecules, except for modified bisulfite sequencing for profiling 5-methylcytidine which was optimized for tRNAs and rRNAs.\n\nThere are six major classes of chemical modifications found in RNA molecules: N-methyladenosine, N6,2'-O-dimethyladenosine, 5-methylcytidine, 5-hydroxylmethylcytidine, inosine, and pseudouridine. Various sequencing methods have been developed to profile each type of modification. The scale, resolution, sensitivity, and limitations associated with each method and the corresponding bioinformatics tools used will be discussed.\n\nMethylation of adenosine does not affect its ability to base-pair with thymidine or uracil, so N-methyladenosine (mA) cannot be detected using standard sequencing or hybridization methods. This modification is marked by the methylation of the adenosine base at the nitrogen-6 position. It is abundantly found in polyA+ mRNA; also found in tRNA, rRNA, snRNA, and long ncRNA.\n\nIn 2012, the first two methods for m6A sequencing came out that enabled transcriptome-wide profile of m6A in mammalian cells. These two techniques, called m6A-seq and MeRIP-seq (m6A-specific methylated RNA immunoprecipitation), are also the first methods to allow for any type of RNA modification sequencing. These methods were able to detect 10,000 m6A peaks in the mammalian transcriptome; the peaks were found to be enriched in 3’UTR regions, near STOP codons, and within long exons.\n\nThe two methods were optimized to detect methylation peaks in poly(A)+ mRNA, but the protocol could be adapted to profile any type of RNA. Collected RNA sample is fragmented into ~100-nucleotide-long oligonucleotides using a fragmentation buffer, immunoprecipitation with purified anti-m6A antibody, elution and collection of antibody-tagged RNA molecules. The immunoprecipitation procedure in MeRIP-Seq is able to produce >130fold enrichment of m6A sequences. Random primed cDNA library generation was performed, followed by adaptor ligation and Illumina sequencing. Since the RNA strands are randomly chopped up, the m6A site should, in principle, lie somewhere in the center of the regions to which sequence reads align. At extremes, the region would be roughly 200nt wide (100nt up- and downstream of the m6A site).\n\nWhen the first nucleotide of a transcript is an adenosine, in addition to the ribose 2’-O-methylation, this base can be further methylated at the N6 position.\nm6A-seq was confirmed to be able to detect m6Am peaks at transcription start sites. Adapter ligation at both ends of RNA fragment results in reads tending to pileup at the 5’ terminus of the transcript. Schwartz et al. (2015) leveraged this knowledge to detect mTSS sites by picking out sites with a high ratio of the size of pileups in the IP samples compared to input sample. As confirmation, >80% of the highly enriched pileup sites contained adenosine.\n\nThe resolution of these methods is 100-200nt, which was the range of the fragment size. \nThese two methods had several drawbacks: (1) required substantial input material, (2) low resolution which made pinpointing the actual site with the m6A mark difficult, and (3) cannot directly assess false positives.\n\nEspecially in MeRIP-Seq, the bioinformatics tools that are currently available are only able to call 1 site per ~100-200nt wide peak, so a substantial portion of clustered m6As (~64nt between each individual site within a cluster) are missed. Each cluster can contain up to 15 m6A residues.\n\nIn 2013, a modified version of m6A-seq based on the previous two methods m6A-seq and MeRIP-seq came out which aimed to increase resolution, and demonstrated this in the yeast transcriptome. They achieved this by decreasing fragment size and employing a ligation-based strand-specific library preparation protocol capturing both ends of the fragmented RNA, ensuring that the methylated position is within the sequenced fragment. By additionally referencing the m6A consensus motif and eliminating false positive m6A peaks using negative control samples, the m6A profiling in yeast was able to be done at single-base resolution.\n\nUV-induced RNA-antibody crosslinking was added on top of m6A-seq to produce PA-m6A-seq (photo-crosslinking-assisted m6A-seq) which increases resolution up to ~23nt. First, 4-thiourodine (4SU) is incorporated into the RNA by adding 4SU in growth media, some incorporation sites presumably near m6A location. Immunoprecipitation is then performed on full-length RNA using m6A-specific antibody [36]. UV light at 365 nm is then shined onto RNA to activate the crosslinking to the antibody with 4SU. Crosslinked RNA was isolated via competition elution and fragmented further to ~25-30nt; proteinase K was used to dissociate the covalent bond between crosslinking site and antibody. Peptide fragments that remain after antibody removal from RNA cause the base to be read as a C as opposed to a T during reverse transcription, effectively inducing a point mutation at the 4SU crosslinking site. The short fragments are subjected to library construction and Illumina sequencing, followed by finding the consensus methylation sequence.\nThe presence of the T to C mutation helps increase the signal to noise ratio of methylation site detection as well as providing greater resolution to the methylation sequence.\nOne shortcoming of this method is that m6A sites that did not incorporate 4SU can’t be detected.\nAnother caveat is that position of 4SU incorporation can vary relative to any single m6A residue, so it still remains challenging to precisely locate m6A site using the T to C mutation.\n\nm6A-CLIP (crosslinking immunoprecipitation) and miCLIP (m6A individual-nucleotide-resolution crosslinking and immunoprecipitation) are UV-based sequencing techniques. These two methods activate crosslinking at 254 nm, fragments RNA molecules before immunoprecipitation with antibody, and do not depend on the incorporation of photoactivatable ribonucleosides - the antibody directly crosslinks with a base close (very predictable location) to the m6A site. These UV-based strategies uses antibodies that induces consistent and predictable mutational and truncation patterns in the cDNA strand during reverse-transcription that could be leveraged to more precisely locate the m6A site . Though both m6A-CLIP and miCLIP reply on UV induced mutations, m6A-CLIP is distinct by taking advantage that m6A alone can induce cDNA truncation during reverse transcription and generate single-nucleotide mapping for over ten folds more precise m6A sites (MITS, m6A-induced truncation sites), permitting comprehensive and unbiased precise m6A mapping. In contrast, UV-mapped m6A sites by miCLIP is only a small subset of total precise m6A sites. The precise location of tens of thousands of m6A sites in human and mouse mRNAs by m6A-CLIP reveals that m6A is enriched at last exon but not around stop codon.\n\nIn m6A-CLIP and miCLIP , RNA is fragmented to ~20-80nt first, then the 254 nm UV-induced covalent RNA/m6A antibody complex was formed in the fragments containing m6A. The antibody was removed with proteinase K before reverse-transcription, library construction and sequencing. Remnants of peptides at the crosslinking site on the RNA after antibody removal, leads to insertions, truncations, and C to T mutations during reverse transcription to cDNA, especially at the +1 position to the m6A site (5’ to the m6A site) in the sequence reads. \nPositive sites seen using m6A-CLIP and miCLIP had high percent of matches with those detected using SCARLET, which has higher local resolution around a specific site, (see below), implicating m6A-CLIP and miCLIP has high spatial resolution and low false discovery rate. \nmiCLIP has been used to detect m6Am by looking at crosslinking-induced truncation sites at the 5’UTR.\n\nAlthough m6A sites could be profiled at high resolution using UV-based methods, the stoichiometry of m6A sites - the methylation status or the ratio m6A+ to m6A- for each individual site within a type of RNA - is still unknown. SCARLET (2013) and m6A-LAIC-seq (2016) allows for the quantitation of stoichiometry at a specific locus and transcriptome-wide, respectively.\n\nBioinformatics methods used to analyze m6A peaks do not make any prior assumptions about the sequence motifs within which m6A sites are usually found, and take into consideration all possible motifs. Therefore, it is less likely to miss sites.\n\nSCARLET (site-specific cleavage and radioactive-labeling followed by ligation-assisted extraction and thin-layer chromatography) is used determining the fraction of RNA in a sample that carries a methylated adenine at a specific site. One can start with total RNA without having to enrich for the target RNA molecule. Therefore, it is an especially suitable method for quantifying methylation status in low abundance RNAs such as tRNAs. However, it is not suitable or practical for large-scale location of m6A sites.\n\nThe procedure begins with a chimeric DNA oligonucleotide annealing to the target RNA around the candidate modification site. The chimeric ssDNA has 2’OMe/2’H modifications and is complementary to the target sequence. The chimeric oligonucleotide serves as a guide to allow RNase H to cleave the RNA strand precisely at the 5’-end of the candidate site. The cut site is then radiolabeled with phosphorus-32 and splint-ligated to a 116nt ssDNA oligonucleotide using DNA ligase. RNase T1/A is introduced to the sample to digest all RNA, except for the RNA molecules with the 116-mers DNA attached. This radiolabeled product is then isolated and digested by nuclease to generate a mixture of modified and unmodified adenosines (5’P-m6A and 5’-P-A) which is separated using thin layer chromatography. The relative proportions of the two groups can be determined using UV absorption levels.\n\nm6A-LAIC-seq (m6A-level and isoform-characterization sequencing) is a high-throughput approach to quantify methylation status on a whole-transcriptome scale. Full-length RNA samples are used in this method. RNAs are first subjected to immunoprecipitation with an anti-m6A antibody. Excess antibody is added to the mixture to ensure all m6A-containing RNAs are pulled down. The mixture is separated into eluate (m6A+ RNAs) and supernatant (m6A- RNAs) pools. External RNA Controls Consortium (ERCC) spike ins are added to the eluate and supernatant, as well as an independent control arm consisting of just ERCC spike in. After antibody cleavage in the eluate pool, each of the three mixtures are sequenced on a next generation sequencing platform. The m6A levels per site or gene could be quantified by the ERCC-normalized RNA abundances in different pools. Since full-length RNA is used, it is possible to directly compare alternatively spliced isoforms between the m6A+ and m6A- fractions as well as comparing isoform abundance within the m6A+ portion.\n\nDespite the advances in m6A-sequencing, several challenges still remain: (1) A method has yet to be developed that characterizes the stoichiometry between different sites in the same transcript; (2) Analysis results are heavily dependent on the bioinformatics algorithm used to call the peaks; (3) Current methods all use m6A-specific antibodies to tag m6A sites, but it has been reported that the antibodies contain intrinsic bias for RNA sequences.\n\nN6,2'-O-dimethyladenosine, abundant in polyA+ mRNAs, occurs at the first nucleotide after the 5’ cap, when an additional methyl group is added to a 2ʹ-O-methyladenosine residue at the ‘capped’ 5ʹ end of mRNA.\n\nSince m6Am can be recognized by anti-m6A antibodies at transcription start sites, the methods used for m6A profiling can be and were adapted for m6Am profiling, namely m6A-seq, and miCLIP (see m6A-seq and miCLIP descriptions above).\n\n5-methylcytidine, m5C, is abundantly found in mRNA and ncRNAs, especially tRNA and rRNAs. In tRNAs, this modification stabilizes the secondary structure and influences anticodon stem-loop conformation. In rRNAs, m5C affects translational fidelity.\n\nTwo principles have been used to develop m5C sequencing methods. The first one is antibody-based approach (bisuphite sequencing and m5C-RIP), similar to m6C sequencing. The second is detecting targets of m5C RNA methyltransferases by covalently linking the enzyme to its target, and then using IP specific to the target enzyme to enrich for RNA molecules containing the mark (Aza-IP and miCLIP).\n\nModified bisulfite sequencing was optimized for rRNA, tRNA, and miRNA molecules from Drosophila.\nBisulfite treatment has been most widely used to detect dm5C (DNA m5C). The treatment essentially converts a cytosine to a uridine, but methylated cytosines would be unchanged by the treatment.\nPrevious attempts to develop m5C sequencing protocols using bisulfite treatment were not able to effectively address the problem of the harsh treatment of RNA which causes significant degradation of the molecules. Specifically, bisulfite deamination treatment (high pH) of RNA is detrimental to the stability of phosphodiester bonds. As a result, it is difficult to pre-enrich RNA molecules or to obtain enough PCR product of the correct size for deep sequencing.\n\nA modified version of bisulfite sequencing was developed by Schaefer et al. (2009) which decreased the temperature at which bisulfite treatment of RNA from 95 °C to 60 °C. The rationale behind the modification was that since RNA, unlike DNA, is not double-stranded, but rather, consists of regions of single-strandedness, double-stranded stem structures and loops, it could be possible to unwind RNA at a much lower temperature. Indeed, RNA could be treated for 180 minutes at 60C without significant loss of PCR amplicons of the expected size. Deamination rates were determined to be 99% at 180min of treatment.\n\nAfter bisulfite treatment of fragmented RNA, reverse transcription is performed, followed by PCR amplification of the cDNA products, and finally deep sequencing was done using the Roche 454 platform.\n\nSince the developers of the method used the Roche platform, they also used GS Amplicon Variant Analyzer (Roche) for analyzing deep sequencing data to quantify sequence-specific cytosine content.\nHowever, recent papers have suggested that the method have several flaws: (1) Incomplete conversion of regular cytosines in double-stranded regions of RNA; (2) areas containing other modifications that resulted in bisulfite-treatment resistance; and (3) sites containing potential false-positives due to (1) and (2) In addition, it is possible the sequencing depth is still not high enough to correctly detect all methylated sites.\n\nAza-IP 5-azacytidine-mediated RNA immunoprecipitation has been optimized on and used for detecting targets of methyltransferases, particularly NSUN2 and DNMT2 — the two main enzymes responsible for laying down the m5C mark.\n\nFirst, the cell is made to overexpress an epitope-tagged m5C-RNA methytransferase derivative so that the antibody used later on for immunoprecipitation could recognize the enzyme. Second, 5-aza-C is introduced to the cells so that it could be incorporated into nascent RNA in place of cytosine. Normally, the methyltransferases are released (i.e. covalent bond between cytosine and methyltransferase is broken) following methylation of the residue. For 5-aza-C, due to a nitrogen substitution in the C5 position of cytosine, the RNA methytransferase enzyme remains covalently bound to the target RNA molecule at the C6 position.\n\nThird, the cell is lysed and the m5C-RNA methyltransferase of interest is immunoprecipitated along with the RNA molecules that are covalently linked to the protein. The IP step enabled >200-fold enrichment of RNA targets, which were mainly tRNAs. The enriched molecules were then fragmented and purified. cDNA library is then constructed and sequencing is performed.\n\nAn important additional feature is that RNA methyltransferase covalent linkage to the C5 of m-aza-C induces rearrangement and ring opening. This ring opening results in preferential pairing with cytosine and is therefore read as guanosine during sequencing. This C to G transversion allows for base resolution detection of m5C sites. \nOne caveat is that m5C sites not replaced by 5-azacytosine will be missed.\n\nmiCLIP (Methylation induced crosslinking immunoprecipitation) was used to detect NSUN2 targets, which were found to be mostly non-coding RNAs such as tRNA. An induced mutation of C271A in NSUN2 inhibits release of enzyme from RNA target. This mutation was over-expressed in the cells of interest, and the mutated NSUN2 was also tagged with the Myc epitope. The covalently linked RNA-protein complexes are isolated via immunoprecipitation for a Myc-specific antibody. These complexes are confirmed and detected by radiolabeling with phosphorus-32. The RNA is then extracted from the complex, reverse-transcribed, amplified with PCR, and sequenced using next-generation platforms.\n\nBoth miCLIP and Aza-IP, though limited by specific targeting of enzymes, can allow for the detection of low-abundance methylated RNA without deep sequencing.\n\nInosine is created enzymatically when an adenosine residue is modified.\n\nSince the chemical makeup of inosine is a deaminated adenosine, this is one of few methylation alterations that has an accompanying alteration in base pairing, which can be capitalised on. The original adenosine nucleotide will pair with a thymine, whereas the methylated inosine will pair with a cytosine. cDNA sequences obtained by rtPCR can therefore be compared to the corresponding genomic sequences; in sites where A residues are repeatedly interpreted as G, a methylation event can be assumed. At high enough accuracy, it is feasible that the quantity of mRNA molecules in the population that have been methylated can be calculated as a percentage. This method potentially has single-nucleotide resolution. In fact, the abundance of RNA-seq data that is now publicly available can be leveraged to investigate G (in cDNA) versus A (in genome). One particular pipeline, called RNA and DNA differences (RDD), claims to excludes false positives, but only 56.8% of its A-to-I sites were found to be valid by ICE-seq(see below).\n\nThe background noise caused by single nucleotide polymorphisms (SNPs), somatic mutations, pseudogenes and sequencing errors reduce the reliability of the signal, especially in a single-cell context.\n\nThe first method to detect A-to-I RNA modifications, developed in 1997, was inosine-specific cleavage. RNA samples are treated with glyoxal and borate to specifically modify all G bases, and subsequently enzymatically digested to by RNase T1, which cleaves after I sites. The amplification of these fragments then allows analysis of cleavage sites and inference of A-to-I modification.\n. It was used to prove the position of inosine at specific sites rather than identify novel sites or transcriptome-wide profiles.\n\nThe existence of two A-to-I modifications in relatively close proximity, which is common in Alu elements, means the downstream mod is less likely to be detected since the cDNA synthesis will be truncated at a prior nucleotide. The throughput is low, and the initial method required specific primers; the protocol is complicated and labour-intensive.\n\nInosine chemical erasing (ICE) refer to a process in which acrylonitrile is reacted with inosine to form N1-cyanoethylinosine (ce1I). This serves to stall reverse transcriptase and lead to truncated cDNA molecules. This was combined with deep-sequencing in a developed method called ICE-seq. Computational methods for automated analysis of the data are available, the main premise being the comparison of treated and untreated samples to identify truncated transcripts and thus infer an inosine modification by read count, with a step to reduce false positives by comparison to online database dbSNP.\n\nThe original ICE protocol involved an RT-PCR amplification step and therefore required primers and knowledge of the location or regions to be investigated, alongside a maximum cDNA length of 300–500bp.\nThe ICE-seq method is complicated, along with being labour-, reagent- and time-intensive. One protocol from 2015 took 22 days. This shares a limitation with inosine-specific cleavage, in that if there are two A-to-I modifications in relatively close proximity, the downstream mod is less likely to be detected since the cDNA synthesis will be truncated at a prior nucleotide.\nBoth ICE and ICE-seq suffer from a lack of sensitivity to infrequently edited locations: it becomes difficult to distinguish a modification with a frequency of <10% from a false positive. An increase in read depth and quality can increase sensitivity, but also then suffer from further amplification bias.\n\nThe modification of A to I is effected by adenosine deaminases that act on RNA (ADARs), of which in mice there are three. The knockdown of these in the cell, therefore, and the subsequent cell–cell comparison of ADAR+ and ADAR- RNA content would be anticipated to provide a basis for A-to-I modification profiling. However, there are further functions of ADAR enzymes within the cell — for example, they have further roles in RNA processing, and in miRNA biogenesis — which would also be likely to change the landscape of cellular mRNA.\n\nThe high incidence of side effects rule out ADAR knockdown as a categorical A-to-I detection method. Moreover, since ADAR knockout results in embryonic lethality, its utility is restricted to cultured cells.\n\nPseudouridine, or Ψ, the overall most abundant post-translational RNA modification, is created when a uridine base is isomerised. In eukaryotes, this can occur by either of two distinct mechanisms; it is sometimes referred to as the ‘fifth RNA nucleotide’. It is incorporated into stable non-coding RNAs such as tRNA, rRNA, and snRNA, with roles in ribosomal ligand binding and translational fidelity in tRNA, and in fine-tuning branching events and splicing events in snRNAs. Pseudouridine has one more hydrogen bond donor from an imino group and a more stable C–C bond, since a C-glycosidic linkage has replaced the N-glycosidic linkage found in its counterpart (regular uridine). As neither of these changes affect its base-pairing properties, both will have the same output when directly sequenced; therefore methods for its detection involve prior biochemical modification.\n\nThere are multiple pseudouridine detection methods beginning with the addition of N-cyclohexyl-N′-b-(4-methylmorpholinium) ethylcarbodiimide metho-p-toluene-sulfonate (CMCT; also known as CMC), since its reaction with pseudouridine produces CMC-Ψ. CMC-Ψ causes reverse transcriptase to stall one nucleotide in the 3’ direction. These methods have single-nucleotide resolution.\nIn an optimisation step, azido-CMC can confer the ability to add biotinylation; subsequent biotin pulldown will enrich Ψ-containing transcripts, allowing identification of even low-abundance transcripts.\n\nAs with other procedures predicated on biochemical alteration followed by sequencing, the development of high-throughput sequencing has removed the limitations requiring prior knowledge of sites of interest and primer design. The method causes a lot of RNA degradation, so it is necessary to start with a large amount of sample, or use effective normalisation techniques to account for amplification biases. One final limitation is that, for CMC labelling of pseudouridine to be specific, it is not complete, and therefore nor is it quantitative. A new reactant that could achieve a higher sensitivity with specificity would be beneficial.\n\nCytidine residues, modified once to m5C (discussed above), can be further modified: either oxidised once for 5-hydroxylmethylcytidine (hm5C), or oxidised twice for 5-formylcytidine (f5C). Arising from the oxidative processing of m5C enacted in mammals by ten-eleven translocation (TET) family enzymes, hm5C is known to occur in all three kingdoms and to have roles in regulation. While 5-hydroxymethylcytidine (hm5dC) is known to be found in DNA in a widespread manner, hm5C is also found in organisms for which no hm5dC has been detected, indicating it is a separate process with distinct regulatory stipulations. To observe the \"in vivo\" addition of methyl groups to cytosine RNA residues followed by oxidative processing, mice can be fed on a diet incorporating particular isotopes and these be traced by LC-MS/MS analysis. Since the metabolic pathway from nutritional intake to nucleotide incorporation is known to progress from dietary methionine --> S-adenosylmethionine (SAM) --> methyl group on RNA base, the labelling of dietary methionine with C and D means these will end up in hm5C residues that have been altered since the addition of these into the diet. In contrast to m5C, a large quantity of hm5C modifications have been recorded within coding sequences.\n\nhMeRIP-seq is an immunoprecipitation method, in which RNA–protein complexes are crosslinked for stability, and antibodies specific to hm5C are added. Using this method, over 3,000 hm5C peaks have been called in \"Drosophila melanogaster\" S2 cells.\n\nDespite two distinct base-resolution methods being available for hm5dC, there are no base-resolution methods for detection of hm5C.\n"}
{"id": "57641573", "url": "https://en.wikipedia.org/wiki?curid=57641573", "title": "Explorer 15", "text": "Explorer 15\n\nExplorer 15, also called EPE-C, was an American satellite launched as part of Explorers program. Explorer 15 as launched on October 27, on Cape Canaveral Air Force Station, Florida, United States, with Delta rocket.\n\nExplorer 15 was a spin-stabilized, solar-cell powered spacecraft instrumented to study the artificial radiation belt produced by the Starfish high-altitude nuclear burst of July 1962. The backup payload for Explorer 14 was modified and used for Explorer 15. The instrumentation included three sets of particle detectors to study both electrons and protons, and a two-axis fluxgate magnetometer to determine magnetic aspect. A 16-channel PFM/PM time-division multiplexed telemeter was used. The time required to sample the 16 channels (one frame period) was 0.323 s. Half of the channels were used to convey eight-level digital information, and the others were used for analog information. During ground processing of the telemetered data, the analog information was digitized with an accuracy of 1/100th of full scale. \n\nOne analog channel was subcommutated in a pattern 16 frames long and was used to telemeter spacecraft temperatures, power system voltages, electric currents, etc. A digital solar aspect sensor measured the spin period and phase, digitized to 0.041 s, and the angle between the spin axis and the sun direction to about 3° intervals. During launch the spacecraft failed to despin. The spin rate ranged from 72.9 to 73.2 rpm during the life of the spacecraft. The spin axis pointed at right ascension 80.97° and declination 20.9°.\n"}
{"id": "11113", "url": "https://en.wikipedia.org/wiki?curid=11113", "title": "Federal Information Processing Standards", "text": "Federal Information Processing Standards\n\nFederal Information Processing Standards (FIPS) are publicly announced standards developed by the United States federal government for use in computer systems by non-military government agencies and government contractors.\n\nFIPS standards are issued to establish requirements for various purposes such as ensuring computer security and interoperability, and are intended for cases in which suitable industry standards do not already exist. Many FIPS specifications are modified versions of standards used in the technical communities, such as the American National Standards Institute (ANSI), the Institute of Electrical and Electronics Engineers (IEEE), and the International Organization for Standardization (ISO).\n\nThe U.S. government has developed various FIPS specifications to standardize a number of topics including:\n\nSome FIPS standards have related to the security of data processing systems. Some of these included the use of key escrow systems. \n\nSome examples of FIPS Codes for geographical areas include FIPS 10-4 for country codes or region codes and FIPS 5-2 for state codes. These codes were similar to or comparable with, but not the same as, ISO 3166, or the NUTS standard of the European Union. In 2002, the National Institute of Standards and Technology (NIST) withdrew several geographic FIPS code standards, including those for countries (FIPS 10-4), U.S. states (FIPS 5-2), and counties (FIPS 6-4). These are to be replaced by ISO 3166 and INCITS standards 38 and 31, respectively. Some of the codes maintain the previous numerical system, particularly for states.\n\nIn 2008, NIST withdrew the FIPS 55-3 database. This database included 5-digit numeric place codes for cities, towns, and villages, or other centers of population in the United States. The codes were assigned alphabetically to places within each state, and as a result changed frequently in order to maintain the alphabetical sorting. NIST replaced these codes with the more permanent GNIS Feature ID, maintained by the U.S. Board on Geographic Names. The GNIS database is the official geographic names repository database for the United States, and is designated the only source of geographic names and locative attributes for use by the agencies of the Federal Government. FIPS 8-6 \"Metropolitan Areas\" and 9-1 \"Congressional Districts of the U.S.\" were also withdrawn in 2008, to be replaced with INCITS standards 454 and 455, respectively.\n\nThe U.S. Census Bureau used FIPS place codes database to identify legal and statistical entities for county subdivisions, places, and American Indian areas, Alaska Native areas, or Hawaiian home lands when they needed to present census data for these areas.\n\nIn response to the NIST decision, the Census Bureau is in the process of transitioning over to the GNIS Feature ID, which will be completed after the 2010 Census. Until then, previously issued FIPS place codes, renamed \"Census Code,\" will continue to be used, with the Census bureau assigning new codes as needed for their internal use during the transition.\n\n\n"}
{"id": "41921610", "url": "https://en.wikipedia.org/wiki?curid=41921610", "title": "Fennia (journal)", "text": "Fennia (journal)\n\nFennia is a biannual peer-reviewed open access scientific journal published by the Geographical Society of Finland. It covers all aspects of geography. The journal was established in 1889. The editor-in-chief is Kirsi Pauliina Kallio (University of Tampere). The journal is abstracted and indexed in Scopus (Citescore 2016, 0.67). and the Emerging Sources Citation Index.\n\n"}
{"id": "25455946", "url": "https://en.wikipedia.org/wiki?curid=25455946", "title": "FreeHAL", "text": "FreeHAL\n\nFreeHAL is a distributed computing project to build a self-learning chatbot. This project is no longer active.\n\nFirst, the program was called \"JEliza\" referring to the chatbot ELIZA by Joseph Weizenbaum.\nThe \"J\" stood for Java because JEliza has first been programmed in Java. In May 2008, the program has been renamed to \"FreeHAL\" because the programming language has changed. Since that time the name is related to the computer in the film \"\".\n\nFreeHAL uses a semantic network and technologies like pattern recognition, stemming, part of speech databases and Hidden Markov Models in order to imitate a human behaviour.\nFreeHAL learns autonomously. While communicating by keyboard the program extends its database.\nCurrently, English and German are supported.\n\nBy using the BOINC infrastructure, new semantic networks for the program are built. FreeHAL@home appears to have terminated operations.\n\nIn 2008, the program won the first prize in the category \"Most Popular\" at the Chatterbox Challenge, a yearly competition between different similar chatbots.\n\nThere was an article about FreeHAL in the \"Linux Magazine\", Issue 97 from December 2008. In the German magazine \"com!\", the program was on the CD/DVD and in the list of the Top-10-Open-Source programs of the month.\n\n"}
{"id": "39635164", "url": "https://en.wikipedia.org/wiki?curid=39635164", "title": "Growing Up Female", "text": "Growing Up Female\n\nGrowing Up Female: A Personal Photo-Journal (1974) was a \"landmark\" book of photography by photojournalist Abigail Heyman (1942-2013). \n\nHeyman introduced the book, writing \"This book is about women, and their lives as women, from one feminist’s point of view.\" The book collected photographs of Heyman's life, \"challeng[ing] assumptions about being a woman\", and \"documented the female experience from a feminist perspective.\" Andy Grundberg described the book as \"test[ing] the line between reportage and personal expression.\" The book portrayed numerous black-and-white images of women in various activities, including a photograph of Ms. Heyman herself having an abortion. \n\nDuring the 1970s, the work sold more than 35,000 copies, and was a mainstay of women's bookstores and feminist literature displays, along with \"Our Bodies, Ourselves\".\n"}
{"id": "38078804", "url": "https://en.wikipedia.org/wiki?curid=38078804", "title": "Hagiopolitan Octoechos", "text": "Hagiopolitan Octoechos\n\nOktōēchos (here transcribed \"Octoechos\"; Greek: pronounced in koine: ; from ὀκτώ \"eight\" and ἦχος \"sound, mode\" called echos; Slavonic: Осмогласие, \"Osmoglasie\" from о́смь \"eight\" and гласъ \"voice, sound\") is the name of the eight mode system used for the composition of religious chant in Byzantine, Syriac, Armenian, Georgian, Latin and Slavic churches since the Middle Ages. In a modified form the octoechos is still regarded as the foundation of the tradition of monodic Orthodox chant today (Neobyzantine Octoechos).\n\nThe Octoechos as a liturgical concept which established an organization of the calendar into eight-week cycles, was the invention of monastic hymnographers at Mar Saba in Palestine and in Constantinople. It was formally accepted in the Quinisext Council of 692 , which also aimed to replace the exegetic poetry of the kontakion and other homiletic poetry, as it was sung during the morning service (Orthros) of the cathedrals.\n\nOne reason why another eight mode system was established by Frankish reformers during the Carolingian reform, may well have been that Pope Adrian I accepted the seventh-century Eastern reform for the Western Church as well during the 787 synod. The only evidence for this is an abbreviated chant book called a \"tonary\". It was a list of incipits of chants ordered according to the intonation formula of each church tone and its psalmody. Later on, fully notated and theoretical tonaries were also written.\n\nThe Byzantine book Octoechos has originally been part of the sticherarion. It was one of the first hymn books with musical notation and its earliest copies survived from the 10th century. Its redaction follows the Studites reform, during which the sticherarion has been invented.\n\nStudents of Orthodox chant today often memorize the history of Byzantine chant in three periods, identified by the names John of Damascus (675/676-749) as the \"beginning\", John Koukouzeles (c. 1280-1360) as the \"flower\" (Papadic Octoechos), and Chrysanthos of Madytos (c. 1770-c. 1840) as the master of the living tradition today (Neobyzantine Octoechos). The latter has the reputation, that he once connected in his time the current tradition with the past of Byzantine chant, which was in fact the work of at least four generations of teachers at the New Music School of the Patriarchate.\n\nThis division of the history into three periods begins quite late with the 8th century, despite the fact that the octoechos reform had been already accepted some decades ago, before John and Cosmas entered the monastery Mar Saba in Palestine. The earliest sources which gave evidence of the octoechos' use in Byzantine chant, can be dated back to the 6th century.\n\nThe common schedule and the focus on the circle around John of Damascus is confirmed by a ninth-century treatise called \"Hagiopolites\" (from \"hagios polis\", \"Holy City\", referring to Jerusalem) which only survived in a complete form as a late copy. The Hagiopolites treatise served presumably as an introduction of a book called \"tropologion\" – a 9th-century chant book which had been replaced soon by the book octoechos, as part of the sticherarion one of the first chant books fully provided with musical notation. The Hagiopolitan emphasis on John of Damascus was obviously the late result of a 9th-century redaction around the Second Council of Nicaea in 787, so it was part of the later Studites reform between Jerusalem and Constantinople and it was motivated theologically, not only because of his contributions to the tropologion, but also because of the keyrole which John of Damascus' polemic against the iconoclasts had during this Council.\n\nNevertheless, the theological and liturgical concept of an eight-week cycle can be traced back to the cathedral rite of Jerusalem during the 5th century, and originally it was the Christian justification of Sunday as the eighth day after Sabbat. Peter Jeffery assumed a first phase during which the concept existed independently in various places, and a second phase during which Palestine became the leading centre of a monastic hymn reform. It established reform models which were also used later by the generation of John of Damascus. Despite that the first paragraph of the \"Hagiopolites\" ascribes the treatise to John of Damascus, it was probably written about 100 years after his death and went through several redactions during the following centuries.\n\nThere is no doubt that the octoechos reform itself had already taken place by 692, because certain passages of the \"Hagiopolites\" paraphrase certain law texts (the canons of the synodal decree). Eric Werner assumed that the eight-mode system developed in Jerusalem since the late fifth century and that the reform by the hymnographers of Mar Saba were already a synthesis with the Ancient Greek names used for the tropes, applied to a model of Syrian origin already used in the Byzantine tradition of Jerusalem. During the eighth century, long before Ancient Greek treatises were translated into Arabic and Persian dialects between the ninth and the tenth centuries, there was already a great interest among theorists like Abū Yūsuf al-Kindī, whose Arabic terms were obviously translated from the Greek. He adored the universality of the Greek octoechos:\n\nSämtliche Stile aller Völker aber haben Teil an den acht byzantinischen Modi (\"hiya min al-alhān at-tamāniya ar-rūmīya\"), die wir erwähnt haben, denn es gibt nichts unter allem, was man hören kann, das nicht zu einem von ihnen gehörte, sei es die Stimme eines Menschen oder eines anderen Lebewesens, wie das Wiehern eines Pferdes oder das Schreien eines Esels oder das Krähen des Hahns. Alles, was an Formen des Schreis einem jeden Lebewesen/Tier eigen ist, ist danach bekannt, zu welchem Modus der acht es gehört, und es ist nicht möglich, daß es sich außerhalb eines von ihnen [bewegt].\n\nEvery style of any tribe takes part of the Byzantine eight tones (\"hiya min al-alhān at-tamāniya ar-rūmīya\") which I mentioned here. Everything which can be heard, be it the human or be it the animal voice – like the neighing of a horse, the braying of a donkey, or the carking of a cock, can be classified according to one of the eight modes, and it is impossible to find anything outside of the eight mode system.\n\nAl-Kindi demonstrated the intervals on the keyboard of a simple four-stringed oud, starting from the third string as well seven steps in ascending as in descending direction.\n\nAccording to Eckhard Neubauer, there is another Persian system of seven \"advār\" (\"cycles\"), outside the Arabic reception of the Byzantine octoechos, which was possibly a cultural transfer from Sanskrit treatises. Persian and Ancient Greek sources had been the main reference for the transfer of knowledge in Arabian-Islamic science.\n\nAccording to the \"Hagiopolites\" the eight \"echoi\" (\"modes\") were divided in four \"kyrioi\" (authentic) \"echoi\" and their four respective \"plagioi\" (enriched, developed) \"echoi\", which were all in the diatonic genus.\n\nDespite the late copies of the Greek \"Hagiopolites\" treatise, the earliest Latin description of the Greek system of eight \"echoi\" is an eleventh-century treatise compilation called \"alia musica\". \"Echos\" was translated as \"sonus\" by the anonymous compilator, who commented with a comparison of the Byzantine octoechos:\n\nQuorum videlicet troporum, sive etiam sonorum, primus graeca lingua dicitur Protus; secundus Deuterus; tertius Tritus; quartus Tetrardus: qui singuli a suis finalibus deorsum pentachordo, quod est diapente, differunt. Superius vero tetrachordum, quod est diatessaron, requirunt, ut unusquisque suam speciem diapason teneat, per quam evagando, sursum ac deorsum libere currat. Cui scilicet diapason plerumque exterius additur, qui emmelis, id est, aptus melo vocatur.\n\nSciendum quoque, quod Dorius maxime proto regitur, similiter Phrygius deutero, Lydius trito, mixolydius tetrardo. Quos sonos in quibusdam cantilenis suae plagae quodammodo tangendo libant, ut plaga proti tangat protum, deuteri deuterum, triti tritum, tetrardi tetrardum. Et id fas est experiri in gradalibus antiphonis.\nIt is known about the tropes, as to say: the ἦχοι, that the Greek language call the First πρῶτος, the Second δεύτερος, the Third τρίτος, the Fourth τέταρτος. Their Finales were separated by a pentachord, that is: a falling fifth (gr. \"diapente\") [between \"kyrios\" and \"plagios\"]. And above [the pentachord] they require a tetrachord, that is: a fourth (gr. \"diatessaron\"), so that each of them has its octave species, in which it can move freely, rambling down and up. For the full octave (gr. \"diapason\") another tone might be added, which is called ὁ ἐμμελής: \"according to the melos\".\nIt has to be known that the \"Dorian\" <nowiki>[</nowiki>octave species<nowiki>]</nowiki> is usually ruling in the πρῶτος, as the \"Phrygian\" in the δεύτερος, the \"Lydian\" in the τρίτος, or the \"Mixolydian\" in the τέταρτος. Their πλάγιοι are derived by these ἦχοι in that way, that the formula touch them [going down a fifth]. So the πλάγιος τοῦ πρώτου touch the πρῶτος, the plagal Second [τοῦ δευτέρου] the δεύτερος, the plagal Third [βαρύς] the τρίτος, the plagal Fourth [πλάγιος τοῦ τετάρτου] the τέταρτος. And this should be proved by the melodies of the antiphonal graduals as a divine law.\n\nThis Latin description about the octoechos used by Greek singers (\"psaltes\") is very precise, when it says that each \"kyrios\" and \"plagios\" pair used the same octave, divided into a fifth (pentachord) and a fourth (tetrachord): D—a—d in protos, E—b—e in devteros, F—c—f in tritos, and C—G—c in tetartos. While the \"kyrioi\" had the finalis (final, and usually also base note) on the top, the \"plagioi\" had the finalis on bottom of the pentachord.\n\nThe intonation formulas, called \"enechema\" (gr. ἐνήχημα), for the authentic modes or \"kyrioi echoi\", usually descend within the pentachord and turn back to the finalis at the end, while the plagal modes or \"plagioi echoi\" just move to the upper third. The later dialogue treatises (Gr. ἐρωταποκρίσεις, \"erotapokriseis\") refer to the Hagiopolitan diatonic eight modes, when they use the \"kyrioi\" intonations to find those of the \"plagioi\":\n\nΠερὶ πλαγίων\n\nἈπο τοῦ πλαγίου πρώτου ἤχου πάλιν καταβαίνεις τέσσαρας φωνάς, καὶ εὑρίσκεται πάλιν πλάγιος πρώτου· ὅυτως δὲ /\nἄνανε ἄνες ἀνὲ ἄνες·\n\nὉμοίως καὶ ὁ β' ἤχος καταβαίνων φωνάς δ', εὑρίσκεις τὸν πλάγιον αὐτοῦ, ἤγουν τὸν πλάγιον τοῦ δευτέρου.\nπλ Β οὕτως δέ.\n\nὉμοίως πάλιν ὁ τρίτος καταβαίνεις φωνὰς τέσσαρας, καὶ εὑρίσκεται ὁ πλάγιος αὐτοῦ, ἤγουν ὁ βαρύς, οὕτως·\nὉμοίως καὶ ἀπὸ τὸν τέταρτον καταβαίνων φωνὰς τέσσαρας, εὑρίσκεις τὸν πλάγιον αὐτοῦ, ὡς ἐστὶ ὁ πλ δ'οὕτως·\nThe Hagiopolites as \"earliest\" theoretical treatise said, that two additional phthorai (\"destroyers\") were like proper modes which did not fit into the diatonic octoechos system, so the Hagiopolitan octoechos was in fact a system of 10 modes. But the chronology of definitions concerned about two phthorai regarded them first as modes of their own because of their proper melos and that their models had to be sung during the eight-week cycle. These mesoi of tetartos and protos, with a finalis and base between kyrios and plagios, were obviously favoured by composer like John of Damascus and his step-brother Kosmas, while the concept of a transition between echoi was established later. It seems that the construction of the eight diatonic echoi was established later by the generation of Theodore the Studite and his brother Joseph.\n\nThe later Papadikai mentions that changes between the echos tritos and the echos plagios tetartos were bridged by the enharmonic phthora nana, and changes between the echos protos and the echos plagios devteros by the chromatic phthora nenano.\n\nNevertheless, the terminology of the Hagiopolites somehow suggested that nenano and nana as phthorai \"destroy\" one or two diatonic degrees used within one tetarchord of a certain echos, so that the chromatic and enharmonic genera were somehow subordinated and excluded from the diatonic octoechos. This raises the question, when the music in the near eastern Middle Ages became entirely diatonic, since certain melodies were coloured by the other enharmonic and chromatic gene according to the school of Damascus. This is the question about the difference between the Hagiopolitan reform of 692 and in as much it was opposed to the Constantinopolitan tradition and its own modal system.\n\nThe author of the Hagiopolites mentioned an alternative system of 16 echoi \"sung in the Asma,\" with 4 phthorai and 4 mesoi beyond the kyrioi and plagioi of the diatonic Octoechos:\n\nΟἱ μὲν οὖν τέσσαρρεις πρῶτοι οὐκ ἐξ ἄλλων τινων ἀλλ'ἐξ αὐτῶν γινονται. οἱ δὲ τέσσαρεις δεύτεροι, ἤγουν οἱ πλάγιοι, ὁ μὲν πλάγιος πρῶτος ἐκ τῆς ὑπορροῆς τοῦ πρώτου γέγονε. καὶ ἀπὸ τῆς ὑπορροῆς τοῦ πληρώματος τοῦ δευτέρου γέγονεν ὁ πλάγιος δευτέρου· ὡς ἐπὶ τὸ πλεῖστον δὲ καὶ τὰ πληρώματα τοῦ δευτέρου [εἰς τὸν πλάγιον δευτέρου] τελειοῖ. ὁ βαρὺς ὁμοίως καὶ ἀπὸ τοῦ τρίτοῦ· καὶ γὰρ εἰς τὸ ἆσμα ἡ ὑποβολὴ τοῦ βαρέως τρίτος ψάλλεται ἅμα τοῦ τέλους αὐτοῦ. καὶ ἀπὸ τοῦ τετάρτου γέγονεν ὁ πλάγιος τέταρτος. καὶ ἀπὸ τῶν τεσσάρων πλαγίων ἐγεννήθησαν τέσσαρεις μέσοι· καὶ ἀπ'αὐτῶν αἱ τέσσαρες φθοραί. καί ἀνεβιβάσθησαν ἦχοι ις', οἵτινες ψάλλονται εἰς τὸ ἆσμα, οἱ δὲ δέκα ὡς προείπομεν εἰς τὸν Ἁγιοπολίτην.\n\nThe 4 Echoi which come first are generated from themselves, not from others. As to the four which come next, i.e. the Plagal ones, Plagios Prōtos is derived from Prōtos, and Plagios Deuteros from Deuteros – normally Deuteros melodies end in Plagios Deuteros. Similarly, Barys from Tritos – \"for in the Asma Hypobole of Barys is sung as Tritos together with its ending\". From the 4 Plagioi originate the 4 Mesoi, and from these the 4 Phthorai. This makes up the 16 Echoi which are sung in the Asma – as already mentioned, there are sung only 10 in the Hagiopolites.\n\nThese \"echoi of the Asma\" did probably point at the rite at the Patriarchal church or even at the cathedral rite of Constantinople which was also known as \"choral\" or \"sung rite\" (ἀκολουθία ᾀσματική). The Constantinopolitan chant books were called \"asmatikon\" (\"book of the choir\"), \"psaltikon\" (\"book of the soloist called 'monophonaris'\"), and \"kontakarion\" (the name of the psaltikon, if it included the huge collection of kontakia, sung during the morning service).\n\nUnfortunately no early Constantinopolitan chant manual survived, there is only this short paragraph of the Hagiopolites which says, that the singers of the choir followed in their chant books an own modal system, which was distinct from the Hagiopolitan octoechos. A distinction from Constantinople is not the only possible explanation, because Jerusalem had also its own local cathedral rite. Since the 14th century at latest, the monastic rite was not opposed to the cathedral rite, even monks celebrated it on festival occasions, whenever they expected guests.\n\nThe earliest sources are those of the Slavic reception of Constantinople which can be dated back not earlier than to the 12th century, and they used a system of 12 modes. The earliest treatises which mention a modal system, is not a chant manual, but a corpus of alchemic treatises, which testifies a modal system of 24 \"elements\" (στοιχεῖα) or \"aims\" (στοχοὶ):\n\nὭσπερ δὲ τεσσάρων ὄντων μουσικῶν γενικωτάτων στοχῶν, Α Β Γ Δ, γίνονται παρ῾ αὐτῶν τῷ εἴδει διάφοροι στοχοὶ κδ´, κέντροι καὶ ἶσοι καὶ πλάγιοι, καθαροί τε καὶ ἄηχοι <καὶ παράηχοι> · καὶ ἀδύνατον ἄλλως ὑφανθῆναι τὰς κατὰ μέρος ἀπείρους μελῳδίας τῶν ὕμνων ἣ θεραπειῶν, ἤ ἄποκαλύψεων, ἤ ἄλλου σκέλους τῆς ἱερᾶς ἐπιστήμης, καὶ οἷον ῥεύσεως ἤ φθορᾶς ἤ ἄλλων μουσικῶν παθῶν ἐλευθέρας,\n\nAs there are 4 basic elements/targets <nowiki>[</nowiki>earth, water, air, and fire<nowiki>]</nowiki> which created their music, the πρῶτος, the δεύτερος, the τρίτος, and the τέταρτος, and by their formulas the same generate 24 different elements: the <nowiki>[4]</nowiki> κέντροι (central), <nowiki>[4]</nowiki> ἶσοι (basic), and <nowiki>[4]</nowiki> πλάγιοι (plagal), the <nowiki>[4]</nowiki> καθαροί (kathartic), <nowiki>[4]</nowiki> ἄηχοι (aphonic), and <nowiki>[4]</nowiki> παράηχοι (paraphonic). Hence, it is impossible to create something outside those infinite melodies of hymns, treatments, revelations, and of other parts of the Holy Wisdom, which is free from the irregularities and corruptions of other musical emotions (πάθη).\n\nIn the edition of the treatise by Otto Gombosi, the four \"elements\" (α', β', γ', δ') were associated with certain colours—πρῶτος with black (all colours together), δεύτερος with white (no colour at all), τρίτος with yellow (an elementary coulour), and τέταρτος with purple (a combination of elementary colours). These passages could be easily compiled with Zosimos of Panopolis' treatise about the process of bleaching.\n\nThe system favoured 3 four tetrachord sets (either modes by themselves or simply degree of the modes with different functions), called κέντροι, ἷσοι, and πλάγιοι. Kέντρος would be probably an early name for μέσος, if it lied between the ἶσος and πλάγιος, it could be as well used as an early name for κύριος ἦχος, because it is mentioned here first, while ἶσος could mean \"equivalent\", or just basis notes.\n\nThe exact point of reference concerning this 24 mode system was not clarified in the treatise, but it is evident, that there was a canonised wisdom which was connected with an ethical doctrine excluding certain passions (πάθη, \"pathe\") as corruptions. Inside this wisdom, there was a Neoplatonic concept of an ideal and divine existence, which can be found and classified according to a modal scheme based on four elements. The term \"element\" (στοχείον) was less meant as a technical term or modal category, it was rather an alchemistic interpretation of the 24 musical modes.\n\nIn comparison, the Hagiopolitan terminology already included the \"corruption\" (φθορά) as an acceptable modal category in itself, which was neither excluded in the Hagiopolitan Octoechos nor in the modal system of a certain cathedral rite, which was made of 16 echoi. On the other hand, the described system, whether it meant 24 echoi including 12 pathologic echoi, called \"aechoi\" and \"paraechoi\", and associated with 4 \"katharoi\" or just cadential degrees or other modal functions. It is not clear, whether the latter name was simply meant in a geographical or ethnical way or whether it was here connected with a kind of music therapy which included certain \"pathe\" as a kind of antidote. Medical treatises of the Mediterranean had been developed later on by the association of melodic modes with 4 elements and 4 humours.\n\nThe introduction of the eight mode system in Western chant traditions was part of the Carolingian reform. Officially, it was motivated by Pope Adrian I's confirmation of an earlier Eastern chant reform during the synode in 787, during which he accepted the reform for the Western traditions as well. Nevertheless, a Carolingian interest for the Byzantine octoechos can already be dated back to a visit some years earlier, when a Byzantine legacy introduced a series of antiphons sung during a procession for Epiphany. These antiphons served as a model for the eight modes according to the Hagiopolitan system.\n\nThe contemporary invention of a proper Latin version of the eight mode system was mainly studied from two perspectives:\n\n\nLatin theorists who knew the Hellenic tropes only through Boethius' 6th-century translation of Ptolemy (\"De institutione musica\"), made the synthesis of the Ancient Greek music theory with the Octoechos as a system of eight church tones, identified with the \"tropes\". The synthesis had not been done earlier than during the Carolingian reform (usually dated according to Charlemagne's \"admonitio generalis\" which was decreed in 789), before music theory as science was strictly separated from chant transmission and the cantor as a profession dedicated to church music.\n\nThe terms \"tropus\" (transposition octave) and \"modus\" (the octave genre defined by the position of the \"tonus\", the whole tone with the proportion of 9:8, and the \"semitonium\", the half tone with the proportion of 256:243) were taken from Boethius' translation. But the Antique names of the seven \"modi\" were applied to the eight church tones called \"toni\". The first attempt to connect Ancient Greek music theory (as expressed in Boethius) and the theory of plainchant can be found in the treatise \"De harmonica institutione\" by Hucbald of Saint-Amand Abbey, written by the end of the 9th century, in which the author addressed his treatise explicitly to cantors and not to mathematicians, whereas the reduction of 4 \"finales\" which made up the tetrachord D—E—F—G, was already done in Carolingian times in the treatises \"Musica\" and \"Scolica enchiriadis\". \"Musica enchiriadis\" is also the only Latin treatise which testifies to the presence of a tetraphonic tone system, represented by 4 \"Dasia\"-signs and therefore called \"Dasia system\", and even the practical use of transposition (\"metabolē kata tonon\") in plainchant, called \"absonia\". Its name probably derived from \"sonus\", the Latin term for ἦχος, but in the context of this treatise the use of \"absonia\" is reserved to describe a primitive form of polyphony or heterophony, rather than serving as a precise description of transposition in monodic chant, as it was used in certain genres of Byzantine chant.\n\nHucbald used an idiosyncratic Greek letter system which referred to the double octave system (\"systēma teleion\") and called the four elements known as \"finales\" according to the Greek system:\nLycanos ypaton scilicet autentum protum· & plagis eiusdem· id est primum & secundum; Hypate mesonʕ autentum deuterum· & plagis eiusʕ iii & iiii· Parypate mesonʕ autentum tritum & plagis eiusʕ id est v· & vi. Lycanos mesonʕ autentum tetrardum. & plagis eiusʕ id est vii· & viii. Ita ut ad aliquam ipsarum ·quatuor. quamvis ul[tra] citraque variabiliter circumacta necessario omnis quaecumque fuerit redigatur· cantilena· Unde et e[a]edem finales appellatae quod finem in ipsis cuncta quae canuntur accipiant.\n\nΛιχανὸς ὑπάτων [D] is <nowiki>[the φθόγγος of]</nowiki> the autentus protus and its plagal which are <nowiki>[tonus]</nowiki> I and II, ὑπάτη μέσων [E] of the autentus deuterus and its plagal which are <nowiki>[tonus]</nowiki> III and IV, παρυπάτῃ μέσων [F] of the autentus tritus and its plagal which are <nowiki>[tonus]</nowiki> V and VI, λιχανὸς μέσων [G] of the autentus tetrardus and its plagal which are <nowiki>[tonus]</nowiki> VII and VIII, for the reason, that these four very present ones surround necessarily each melody, so that they, however they might be, can be reduced to them. These four <nowiki>[φθόγγοι]</nowiki> are called 'finales', since in all those <nowiki>[melodies]</nowiki> which are sung, they are perceived as their end.\n\nAccording to the Latin synthesis the plagal and authentic tones of \"protus, deuterus, tritus\", and \"tetrardus\" did not use the same ambitus as in the Hagiopolitan Octoechos, but authentic and plagal tones used both the \"finalis\" of the \"plagios\", so that the \"finalis\" of the \"kyrios\", the fifth degree of the mode, was no longer used as \"finalis\", but as \"repercussa\": the recitation tone used in a simple form of psalmody which was another genuine invention by the Carolingian reformers. The ambitus of the authentic tones was made up the same way as used in the Greek Octoechos, while the plagal tones used a lower ambitus: not the tetrachord above the pentachord, but below it. Hence, the hypodorian octave referred the \"tonus secundus\" and was constructed A—D—a, and the dorian as \"tonus primus\" D—a—d, both tones of the protus used D as \"finalis\", the hypophrygian octave was B—E—b and was the ambitus of the \"tonus quartus\", and the phrygian octave E—b—e was related to the \"tonus tertius\" and its \"finalis\" E belonged to the deuterus, the hypolydian octave C—F—c was connected with the \"tonus sixtus\", the lydian octave F—c—f with the \"tonus quintus\" and both shared the \"finalis\" F called \"tritus\", the last was the seventh octave G—d—g called \"mixolydian\" which referred to the \"tonus septimus\" and its \"finalis\" G.\n\nThe earliest chant theory connected with the Carolingian octoechos was related to the book tonary. It played a key role in memorising chant and the earliest tonaries referred to the Greek names as elements of a tetrachord: πρῶτος, δεύτερος, τρίτος, and τέταρτος. They were translated into Latin as \"protus\", \"deuterus\", \"tritus\", and \"tetrardus\", but only the tetrachord D—E—F—G was supposed to contain the final notes (\"finales\") for the eight tones used in the Latin octoechos. Since the 10th century the eight tones were applied to eight simplified models of psalmody, which soon adopted in their terminations the melodic beginnings of the antiphons, which were sung as refrains during psalm recitation. This practice made the transitions smoother, and in the list of the antiphons which can be found since the earliest tonaries, it was enough to refer to the melodic beginnings or incipits of the text. In the earliest tonaries no models of psalmody had been given and incipits from all chant genres were listed, probably just for a modal classification (see the section for the \"Autentus protus\" of the Saint Riquier tonary).\n\nAccording to Michel Huglo, there was a prototype tonary which initiated the Carolingian reform. But in a later study he mentioned an even earlier tonary which was brought as a present by a Byzantine legacy which celebrated procession antiphons for Epiphany in a Latin translation.\n\nAlready during the 10th century tonaries became so widespread in different regions, that they do not only allow to study the difference between local schools according to its modal classification, its redaction of modal patterns, and its own way of using Carolingian psalmody. They also showed a fundamental difference between the written transmission of Latin and Greek chant traditions, as it had developed between the 10th and 12th centuries. The main concern of Latin cantors and their tonaries was a precise and unambiguous classification of whatever melody type according to the local perception of the Octoechos system.\n\nGreek psaltes were not interested at all in this question. They knew the models of each modes by certain simple chant genres as the \"troparion\" and the \"heirmoi\" (the melodic models used to create poetry in the meter of the heirmologic odes), but other genres like \"sticheron\" and \"kontakion\" could change the echos within their melos, so their main interest was the relationship between the echoi to compose elegant and discrete changes between them.\n\nIn contrary, the very particular form and function of the tonary within chant transmission made it evident, that the modal classification of Latin cantors according to the eight tones of the Octoechos had to be done a posteriori, deduced by the modal analysis of the chant and its melodic patterns, while the transmission of the traditional chant itself did not provide any model except of the psalm tones used for the recitation of the psalms and the canticles.\n\nThe tonary was the very heart of the mainly oral chant transmission used during the Carolingian reform and as its medium it must have had a strong impact on the melodic memory of the cantors who used it in order to memorize the Roman chant, after a synode confirmed Charlemagne's \"admonitio generalis\". The written transmission by fully notated chant manuscripts, the object of chant studies today, cannot be dated back to an earlier time than nearly 200 years after the \"admonitio\"—the last third of the 10th century. And it seems that Roman cantors whose tradition had to be learnt, followed at least 100 years later by the transcription of their chant repertory and no document has survived which can testify the use of tonaries among Roman cantors. Pope Adrian I's confirmation of the Eastern octoechos reform had probably no consequences on the tradition of Roman chant, which might be an explanation for the distinct written transmission, as it can be studied between Roman Frankish and Old Roman chant manuscripts.\n\nThe eight sections of the Latin tonary are usually ordered \"Tonus primus Autentus Protus\", \"Tonus secundus Plagi Proti\", \"Tonus tertius Autentus deuterus\" etc. Each section is opened by an intonation formula using the names like \"Noannoeane\" for the authentic and \"Noeagis\" for the plagal tones. In his theoretical tonary \"Musica disciplina\" Aurelian of Réôme asked a Greek about the meaning of the syllables, and reported that they had no meaning, they were rather an expression of joy as used by peasants to communicate with their working animals like horses. There was usually no exact resemblance of the Latin syllables to the names of the Greek intonations or \"enechemata\" which were identified with the diatonic kyrioi and plagioi echoi, but Aurelian's question made it obvious that the practice was taken from Greek singers. Unlike the Hagiopolitan octoechos, which used two additional \"phthorai\" with the syllables Nana and Nenano for changes into the enharmonic and chromatic genus, the enharmonic and chromatic genus was excluded from the Latin octoechos, at least according to Carolingian theorists.\n\nSince the 10th century tonaries also include the mnemic verses of certain model antiphons which memorise each tone by one verse. The most common among all tonaries was also used by Guido of Arezzo in his treatise Micrologus: \"Primum querite regnum dei\", \"Secundum autem simile est huic\" etc. Another characteristic was that melodic melisms called \"neumae\" followed the intonation formulas or mnemic verses. Usually they differed more among different tonaries than the preceding intonations or verses, but they all demonstrated the generative and creative aspect within chant transmission.\n\nIn comparison with Byzantine psaltes who always used notation in a more or less stenographic way, the exact patterns used during the so-called \"thesis of the melos\" belonged to the oral tradition of a local school, its own modal system and its genre. But already the question of chant genre was connected with local traditions in medieval times and the point of reference for the psaltes who performed a certain genre: the Hagiopolitan octoechos and its genres (the odes according to the models of the heirmologion, the troparia of the octoechos or tropologion), or the Constantinopolitan cathedral rite \"(akolouthia asmatike)\" and its books asmatikon, psaltikon, and kontakarion might serve here as examples.\n\nThe exact proportions which divided a tetrachord, had never been a subject of Greek medieval treatises concerned about Byzantine chant. The separation between the mathematical science \"harmonikai\" and chant theory gave space to various speculations, even to the assumption that the same division was used as described in Latin music theory, operating with two diatonic intervals like \"tonus\" (9:8) and \"semitonium\" (256:243). Nevertheless, some treatises referred the tetrachord division into three intervals called the \"great tone\" (μείζων τόνος) which often corresponded to the prominent position of the whole tone (9:8), the \"middle tone\" (ἐλάσσων τόνος) between α and β, and the \"small tone\" (ἐλάχιστος τόνος) between β and γ which was usually a much larger interval than the half tone, and this division was common among most divisions by different ancient Greek theorists that were mentioned by Ptolemy in his \"Harmonics\". Before Chrysanthos' \"Theoretika\" (the \"Eisagoge\" was simply an extract, while the \"Mega Theoretikon\" was published by his student Panagiotes Pelopides), exact proportions were never mentioned in Greek chant theory. His system of 68 commata which is based on a corrupted use of arithmetics, can be traced back to the division of 12:11 x 88:81 x 9:8 = 4:3 between α and δ.\n\nAlthough Chrysanthos did not mention his name, the first who mentioned precisely these proportions starting from the open string of the third or middle chord of the oud, was the Arab theorist Al-Farabi in his \"Kitab al-Musiqa al-Kabir\" which was written during the first half of the 10th century. His explicit references to Persian and Ancient Greek music theory were possible, because they had been recently translated into Arabic and Persian dialects in the library of Baghdad. Thanks to them Al-Farabi had also an excellent knowledge of Ancient Greek music theory. The method of demonstrating the intervals by the frets of the oud keyboard was probably taken from Al-Kindi. Here the intervals are not referred to the Byzantine \"phthongoi\", but to the name of the frets. And the fret corresponding to β was called \"ring finger fret of Zalzal\" (wuṣtā Zalzal), named after the famous Baghdadi oud player Zalzal. It seems that the proportion of the Zalzal fret was a refined one in Bagdad using a large middle tone that came very close to the interval of the small tone, while the Mawsili school used 13:12 instead of 12:11. There is no indication that this division had been of Byzantine origin, so Western scholars felt seduced to ascribe the use of the division called \"soft diatonic\" (\"diatonikos malakos\") and the chromaticism derived from it as an influence of the Ottoman Empire and to regard their view of the \"systema teleion\" also as a norm for the Byzantine tonal system. As Phanariotes (Phanar was the Greek district of Istanbul with the residence of the Patriarchate) who composed as well in the \"makamlar\", the teachers of the New Music School of the Patriarchate around Chrysanthos had certainly exchanges with Sephardic, Armenian, and Sufi musicians, but an intensive exchange between Byzantine, Arab and Persian musicians had already a history of more than 1000 years.\n\nUnlike Latin treatises only a few Greek treatises of chant have survived and their authors wrote nothing about the intervals, about microtonal shifts as part of a certain melos and its echos, or about the practice of ison singing (\"isokratema\"). Nevertheless, these practices remained undisputed, because they are still part of the living tradition today, while Western plainchant became rediscovered during the 19th century. Neither musicians nor musicologists were longer familiar with them which explains why various descriptions, as they can be found in certain Latin treatises, were ignored for quite a long time.\n\nAncient Greek music theory had always been a point of reference in Latin chant treatises, something similar cannot be found in Greek chant treatises before the 14th century, but there were a few Latin treatises of the 11th century which did not only refer to Ancient music theory and the \"systema teleion\" together with the Greek names of its elements, they even had parts dedicated to Byzantine chant. The appreciation for Byzantine chant is surprising, because there were very few authors except Boethius who had really studied Greek treatises and who were also capable to translate them.\n\nThe \"systema teleion\" was present by the Boethian diagram which represented it for the diatonic, the chromatic, and the enharmonic genus. Several tonaries used letters which referred to the positions of this diagram. The most famous example is the letter notation of William of Volpiano which he developed for the Cluniac reforms by the end of the 10th century. In his school a unique tonary was already written, when he was reforming abbot of St. Benignus of Dijon. The tonary shows the Roman-Frankish mass chant written out in neume and pitch notation. The repertory is classified according to the Carolingian tonary and its entirely diatonic octoechos. The use of tyronic letters clearly shows, that the enharmonic diesis was used as a kind of melodic attraction within the diatonic genus, which sharpened the semitonium. Even in Guido of Arezzo's treatise \"Micrologus\", at least in earlier copies, there is still a passage which explains, how the diesis can be found on the monochord. It sharpens the semitonium by replacing the usual whole tone (9:8) between re—mi (D—E, G—a, or a—b) by an even larger one in the proportion of 7:6 which was usually perceived as an attraction towards fa.\n\nBut there were as well other practices which could not be explained by the Boethian diagram and its use of \"tonus\" and \"semitonium\". The authors of one theoretical tonary of the compilation called \"alia musica\" used an alternative intonation with the name AIANEOEANE, the name was obviously taken from a Byzantine \"enechema\" ἅγια νεανὲς, a kind of Mesos tetartos with the finalis and basis on a low E, and applied the Byzantine practice to certain pieces of Roman-Frankish chant which were classified as \"tonus tertius\" or \"Autentus deuterus\". In the following section \"De quarto tono\" the author quotes Aristoxenos' description of the enharmonic and chromatic division of the tetrachord, the remark on it in precisely this section had been probably motivated by the Hagiopolitan concept of the phthora nenano which connected the echos protos on a with the plagios devteros on E.\n\nLatin cantors knew about the theoretical concept of the practice of transposition since Boethius' translation of Ptolemy. Very few can be said, if they ever understood the practical use of it. Nevertheless, there was a rudimentary knowledge which can be found in the Carolingian treatises \"Musica\" and \"Scolica enchiriadis\". The \"Musica enchiriadis\" was also the only Latin treatise which documented a second tone system beside the \"systema teleion\", but it does not explain at all, how these both systems worked together in practice.\n\nThe Hagiopolites did neither explain it nor did it mention any tone system nor the \"metabole kata tonon\", but this was probably, because the hymn reform of Jerusalem was mainly concerned with simple models exemplified by \"heirmoi\" or \"troparia\". Greek protopsaltes used the transposition only in very few compositions of the sticherarion, for instance the compositions passing through all the modes of the Octoechos, or certain melismatic elaborations of troparia in the psaltic style, the soloistic style of the Constantinopolitan cathedral rite. This might explain that Charles Atkinson discussed Carolingian theory in comparison with the later \"papadikai\", in which all possible transpositions were represented by the Koukouzelian wheel or by the \"kanônion\".\n\nWheels are also used in Arabic music theory since the 13th century, and Al-Farabi was the first who started a long tradition of science, which did not only find the proportions of the untransposed diatonic system on the oud keyboard, but also those of all possible transpositions. The use of instruments had to adapt to a very complex tradition which had probably been a rather vocal tradition in its origins.\n\n\n\n\n\n\n\n\n"}
{"id": "803641", "url": "https://en.wikipedia.org/wiki?curid=803641", "title": "History and Technology", "text": "History and Technology\n\nHistory and Technology is a quarterly peer reviewed academic journal devoted to publishing papers on all aspects of the history of technology. It was established in 1983. One of the founding editors was Pietro Redondi. The subjects range from ancient and classical times to the present day. The current publisher is Taylor & Francis.\n"}
{"id": "50607539", "url": "https://en.wikipedia.org/wiki?curid=50607539", "title": "Hortus Haren", "text": "Hortus Haren\n\nHortus Haren is a botanical garden of in Haren, Groningen, Netherlands. First created in 1642, it is the largest botanical garden in the country.\n"}
{"id": "3800795", "url": "https://en.wikipedia.org/wiki?curid=3800795", "title": "Ida Facula", "text": "Ida Facula\n\nIda Facula is a bright mountain on Amalthea, one of Jupiter's smallest moons. It is known to be about 15 kilometers in width, somewhat smaller than the neighboring mountain Lyctos Facula. It was discovered by \"Voyager 1\" in 1979 and in the same year named for Mount Ida, a mountain in Crete where Zeus played as a child. Firstly it was called simply \"Ida\".\n"}
{"id": "5477631", "url": "https://en.wikipedia.org/wiki?curid=5477631", "title": "International Association for Hydro-Environment Engineering and Research", "text": "International Association for Hydro-Environment Engineering and Research\n\nThe International Association for Hydro-Environment Engineering and Research (IAHR), founded in 1935, is a worldwide, non-profit, independent organisation of engineers and water specialists working in fields related to the hydro-environment and in particular with reference to hydraulics and its practical application. IAHR was called the International Association of Hydraulic Engineering and Research until 2009.\n\nActivities range from river and maritime hydraulics to water resources development, flood risk management and eco-hydraulics, through to ice engineering, hydroinformatics and continuing education and training. IAHR stimulates and promotes both research and its application, and by so doing strives to contribute to sustainable development, the optimisation of world water resources management and industrial flow processes. IAHR accomplishes its goals by a wide variety of member activities including: working groups, research agenda, congresses, specialty conferences, workshops and short courses; Journals, Monographs and Proceedings; by collaborating with international organisations such as UN Water, UNESCO, WMO, IDNDR, GWP, ICSU; and by co-operation with other water-related national and international organisations.\n\nIAHR publishes several international scientific journals in collaboration with Taylor & Francis and Elsevier – the \"Journal of Hydraulic Research\", the \"Journal of River Basin Management\", the \"Journal of Water Engineering and Research\", the \"Revista Iberoamericana del Agua RIBAGUA\" jointly with WCCE, the \"Journal of Ecohydraulics\" and the\"Journal of Hydro-Environment Engineering and Research\" with the Korean Water Resources Association. It also publishes a quarterly magazine called Hydrolink, together with several e-zines!.\n\nThe activities of IAHR are carried out by around one hundred volunteer elected officers from around the world supported by a full-time professional secretariat with offices in Madrid, Spain which is hosted by the consortium Spain Water (composed of CEDEX, Direccion General del Agua, Direccion General de Costas, MAPAMA, Spain), and in Beijing, China hosted by IWHR, and sponsored by Suez and Hydromodel.\n\nThe governing body of the association is a council elected by member ballot every two years. The current president is Prof. Peter Goodwin (US). The current vice presidents are: Prof. Silke Wieprecht (Germany) (also chair IAHR Hydraulics), Dr. Arturo Marcano (Venezuela), and Prof James Ball (Australia). Dr. Ramon Gutierrez-Serret and Dr Peng Jing are secretary generals. Dr. Christopher George is executive director of the association.\n\nIAHR is a Scientific Associate of the International Council for Science (ICSU) and is a partner organisation of UN-Water.\n\nThe IAHR World Congress is one of the most important activities of the International Association for Hydro-Environment Engineering and Research (IAHR) which typically attracts between 800 and 1500 participants from around the world. The forthcoming world Congress will be in Panama in 2019.\n\nIAHR publishes the \"Journal of Hydraulic Research\" in partnership with Taylor & Francis.\n\nIAHR publishes the \"International Journal of River Basin Management\" together with the International Association of Hydrological Sciences and INBO and in partnership with Taylor & Francis.\n\nIAHR publishes the \"International Journal of Applied Water Engineering and Research\" together with the World Council of Civil Engineers and in partnership with Taylor & Francis.\n\nThe IAHR Asia Pacific Division publishes the\" Journal of Hydro-Environment Research\" in collaboration with the KWRA, Korean Water Resources Association and Elsevier\n\nThe IAHR Latin America Division publishes the \" Revista Iberoamericana del Agua\" in collaboration with the World Council of Civil Engineers (WCCE)\nhttp://www.elsevier.es/es-revista-ribagua-revista-iberoamericana-del-217\n"}
{"id": "18470213", "url": "https://en.wikipedia.org/wiki?curid=18470213", "title": "International Test and Evaluation Association", "text": "International Test and Evaluation Association\n\nThe International Test and Evaluation Association (ITEA), is a professional body founded in 1980. Its primary purpose is to further the exchange of technical information in the field of test and evaluation. Membership is open to professionals, students and corporate bodies involved in the development and application of policy and techniques used to assess the effectiveness, reliability, and safety of new and existing systems and products.\n\n"}
{"id": "19712569", "url": "https://en.wikipedia.org/wiki?curid=19712569", "title": "List of Russian navy flags", "text": "List of Russian navy flags\n\nThis is a List of naval flags of the Russian Federation, from independence from the Soviet Union in 1991 onward, for Soviet naval flags see List of USSR navy flags.\n\n\n"}
{"id": "53514351", "url": "https://en.wikipedia.org/wiki?curid=53514351", "title": "List of bitcoin forks", "text": "List of bitcoin forks\n\nBitcoin forks are defined variantly as changes in the protocol of the bitcoin network or as the situations that occur \"when two or more blocks have the same block height\". A fork influences the validity of the rules. Forks are typically conducted in order to add new features to a blockchain, to reverse the effects of hacking or catastrophic bugs. Forks require consensus to be resolved or else a permanent split emerges.\n\nThe following are forks of the software client for the bitcoin network:\nAll three software clients attempt to increase transaction capacity of the network. None achieved a majority of the hash power.\n\nHard forks splitting bitcoin (aka \"split coins\") are created via changes of the blockchain rules and sharing a transaction history with bitcoin up to a certain time and date. The first hard fork splitting bitcoin happened on 1 August 2017, resulting in the creation of Bitcoin Cash.\n\nThe following is a list of hard forks splitting bitcoin by date and/or block:\n\n\n\nTwo hard forks were created by \"protocol change\" definition:\n"}
{"id": "20209624", "url": "https://en.wikipedia.org/wiki?curid=20209624", "title": "List of features removed in Windows 7", "text": "List of features removed in Windows 7\n\nWindows 7 contains many new features. However, similarly to the transition from Windows XP to Windows Vista, certain capabilities and programs that are present in Windows Vista are no longer present in Windows 7 or have changed. The following is a list of features that originated in earlier versions of Windows and included up to Windows Vista.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "26777763", "url": "https://en.wikipedia.org/wiki?curid=26777763", "title": "List of large volume volcanic eruptions in the Basin and Range Province", "text": "List of large volume volcanic eruptions in the Basin and Range Province\n\nLarge volume volcanic eruptions in the Basin and Range Province include Basin and Range eruptions in California, Idaho, Colorado, New Mexico, Texas, Arizona, Nevada, Wyoming and Oregon, as well as those of the Long Valley Caldera geological province and the Yellowstone hotspot.\n\nSome of the volcanic fields of the Basin and Range Province are within: Northwestern Nevada, the Modoc Plateau, Central Nevada, the Great Basin, Southwestern Nevada, the Mojave Desert, and the Long Valley Caldera region. Named ones include: Coso Volcanic Field, Mono Lake Volcanic Field, Marysvale Volcanic Field, San Juan volcanic field, Indian Peak, Central Colorado volcanic field, Jemez volcanic lineament, Mogollon-Datil volcanic field, Santa Rosa-Calico, and Boot Heel volcanic field.\n\nMany geological features in Western United States have a Northeastern orientation, the North American craton motion has the same orientation as well. For example: the Trans-Challis fault zone, Idaho; the Snake River in Oregon; the Garlock Fault, California; the Colorado River in Utah; the Colorado Mineral Belt; Crater Flat-Reveille Range-Lunar Crater lineament, the Northwestern Nevada volcanic field; the San Juan caldera cluster, Colorado; the Socorro-Magdalena caldera cluster, New Mexico; Jemez volcanic lineament (Raton hotspot trail); and the Yellowstone hotspot trail. But the Yellowstone hotspot trail was modified through faults and extension.\n\nPrior to the Eocene Epoch (55.8 ±0.2 to 33.9 ±0.1 Ma) the convergence rate of the Farallon and North American Plates was fast and the angle of subduction was shallow. During the Eocene the Farallon Plate subduction-associated compressive forces of the Laramide orogeny ended, plate interactions changed from orthogonal compression to oblique strike-slip, and volcanism in the Basin and Range Province flared up. It is suggested that this plate continued to be underthrust until about 19 Ma, at which time it was completely consumed and volcanic activity ceased, in part. Olivine basalt from the oceanic ridge erupted around 17 Ma and extension began. The extension resulted in roughly north-south-trending faults, the Great Basin, the Walker trough, the Owens graben, and the Rio Grande rift, for instance.\n\nThe large volume eruptions in the Basin and Range Province include:\nThe Rupelian age/stage (Paleogene period/system, Oligocene epoch/series) spans the time between 33.9 ±0.1 Ma and 28.4 ±0.1 Ma (million years ago).\n\n\n\n\n"}
{"id": "39655638", "url": "https://en.wikipedia.org/wiki?curid=39655638", "title": "List of national parks of Somalia", "text": "List of national parks of Somalia\n\nThis is a list of national parks of Somalia.\n\n\n\n"}
{"id": "54798839", "url": "https://en.wikipedia.org/wiki?curid=54798839", "title": "List of the prehistoric life of Utah", "text": "List of the prehistoric life of Utah\n\nThis list of the prehistoric life of Utah contains the various prehistoric life-forms whose fossilized remains have been reported from within the US state of Utah.\n\nThe Paleobiology Database records no known occurrences of Precambrian fossils in Utah.\n\n\n\n"}
{"id": "35440144", "url": "https://en.wikipedia.org/wiki?curid=35440144", "title": "List of types of equilibrium", "text": "List of types of equilibrium\n\nThis is a list of various types of equilibrium, the condition of a system in which all competing influences are balanced.\n\n\n\n\n\n\n\n"}
{"id": "254069", "url": "https://en.wikipedia.org/wiki?curid=254069", "title": "List of words derived from toponyms", "text": "List of words derived from toponyms\n\nThis is a list of English language words derived from toponyms, followed by the place name it derives from. \n\n\n\n\n\nNote: Saskatoon, Saskatchewan is named after the local Saskatoon berry, rather than vice versa.\n\n\n\nThere are some corporations whose name is simply the same as their original location.\n\n\"See\": Chemical elements named after places\n\n\n\n"}
{"id": "43533192", "url": "https://en.wikipedia.org/wiki?curid=43533192", "title": "Lists of fossiliferous stratigraphic units", "text": "Lists of fossiliferous stratigraphic units\n\n\n\n"}
{"id": "5668480", "url": "https://en.wikipedia.org/wiki?curid=5668480", "title": "NASAcast", "text": "NASAcast\n\nNASAcast is the official audio and video podcast of the NASA website. Created in late 2005, the podcast service contains the latest audio and video features from the NASA web site, including NASA TV's \"This Week at NASA\" and educational materials produced by NASA. Additional NASA podcasts, such as Science@NASA, are also featured and give subscribers an in-depth look at content by subject matter.\n\n"}
{"id": "33710742", "url": "https://en.wikipedia.org/wiki?curid=33710742", "title": "Natural units", "text": "Natural units\n\nIn physics, natural units are physical units of measurement based only on universal physical constants. For example, the elementary charge is a natural unit of electric charge, and the speed of light is a natural unit of speed. A purely natural system of units has all of its units defined in this way, and usually such that the numerical values of the selected physical constants in terms of these units are exactly \"dimensionless\" . These constants are then typically omitted from mathematical expressions of physical laws, and while this has the apparent advantage of simplicity, it may entail a loss of clarity due to the loss of information for dimensional analysis. It precludes the interpretation of an expression in terms of fundamental physical constants, such as and , unless it is \"known\" which units (in dimensionful units) the expression is supposed to have. In this case, the reinsertion of the correct powers of , , etc., can be uniquely determined.\n\nNatural units are intended to elegantly simplify particular algebraic expressions appearing in the laws of physics or to normalize some chosen physical quantities that are properties of universal elementary particles and are reasonably believed to be constant. However, there is a choice of which quantities to set to unity in a natural system of units, and quantities which are set to unity in one system may take a different value or even be assumed to vary in another natural unit system.\n\nNatural units are \"natural\" because the origin of their definition comes only from properties of nature and not from any human construct. Planck units are often, without qualification, called \"natural units\", although they constitute only one of several systems of natural units, albeit the best known such system. Planck units (up to a simple multiplier for each unit) might be considered one of the most \"natural\" systems in that the set of units is not based on properties of any prototype, object, or particle but are solely derived from the properties of free space.\n\nAs with other systems of units, the base units of a set of natural units will include definitions and values for length, mass, time, temperature, and electric charge (in lieu of electric current). It is possible to disregard temperature as a fundamental physical quantity, since it states the energy per degree of freedom of a particle, which can be expressed in terms of energy (or mass, length, and time). Virtually every system of natural units normalizes Boltzmann's constant to 1, which can be thought of as simply a way of defining the unit temperature.\n\nIn SI, electric charge is a separate fundamental dimension of physical quantity, but in natural unit systems charge is expressed in terms of the mechanical units of mass, length, and time, similarly to cgs. There are two common ways to relate charge to mass, length, and time: In Lorentz–Heaviside units (also called \"rationalized\"), Coulomb's law is , and in Gaussian units (also called \"non-rationalized\"), Coulomb's law is . Both possibilities are incorporated into different natural unit systems.\n\nwhere:\n\nNatural units are most commonly used by \"setting the units to one\". For example, many natural unit systems include the equation in the unit-system definition, where is the speed of light. If a velocity is half the speed of light, then as and , hence . The equation means \"the velocity has the value one-half when measured in Planck units\", or \"the velocity is one-half the Planck unit of velocity\".\n\nThe equation can be plugged in anywhere else. For example, Einstein's equation can be rewritten in Planck units as . This equation means \"The energy of a particle, measured in Planck units of energy, equals the mass of the particle, measured in Planck units of mass.\"\n\nCompared to SI or other unit systems, natural units have both advantages and disadvantages:\n\n\nOut of the many physical constants, the designer of a system of natural unit systems must choose a few of these constants to normalize (set equal to 1). It is not possible to normalize just \"any\" set of constants. For example, the mass of a proton and the mass of an electron cannot both be normalized: if the mass of an electron is defined to be 1, then the mass of a proton has to be approximately 1836. In a less trivial example, the fine-structure constant, , cannot be set to 1 because it is a dimensionless number defined in terms of other quantities. The fine-structure constant is related to other physical constants through , where is the Coulomb constant, is the elementary charge, is the reduced Planck constant, and is the speed of light. Thus, we cannot set all of , , , and to 1, we can normalize at most three of this set to 1.\n\nIn SI units, electric charge is expressed in coulombs, a separate unit which is additional to the \"mechanical\" units (mass, length, time), even though the traditional definition of the ampere refers to some of these other units. In natural unit systems, however, electric charge has units of .\n\nIn order to build natural units in electromagnetism one can use:\nOf these, Lorentz–Heaviside is somewhat more common, mainly because Maxwell's equations are simpler in Lorentz–Heaviside units than they are in Gaussian units.\n\nIn the two unit systems, the elementary charge satisfies:\nwhere is the reduced Planck constant, is the speed of light, and is the fine-structure constant.\n\nIn a natural unit system where , Lorentz–Heaviside units can be derived from SI units by setting . Gaussian units can be derived from SI units by a more complicated set of transformations, such as multiplying all electric fields by , multiplying all magnetic susceptibilities by , and so on.\n\nPlanck units are defined by\nwhere is the speed of light, is the reduced Planck constant, is the gravitational constant, is the Coulomb constant, and is the Boltzmann constant.\n\nPlanck units are a system of natural units that is not defined in terms of properties of any prototype, physical object, or even elementary particle. They only refer to the basic structure of the laws of physics: and are part of the structure of spacetime in general relativity, and captures the relationship between energy and frequency which is at the foundation of quantum mechanics. This makes Planck units particularly useful and common in theories of quantum gravity, including string theory.\n\nPlanck units may be considered \"more natural\" even than other natural unit systems discussed below, as Planck units are not based on any arbitrarily chosen prototype object or particle. For example, some other systems use the mass of an electron as a parameter to be normalized. But the electron is just one of 16 known massive elementary particles, all with different masses, and there is no compelling reason, within fundamental physics, to emphasize the electron mass over some other elementary particle's mass.\n\nThe original Planck units are based on Gaussian units, thus formula_1 and thus formula_2 and formula_3. However, the Planck units can also based on Lorentz–Heaviside units, thus formula_4 and formula_5 (this is often called reduced Planck units, e.g. reduced Planck energy). Both kinds of Planck units have formula_6.\n\nStoney units are defined by:\nwhere is the speed of light, is the gravitational constant, is the Coulomb constant, is the elementary charge, and is the Boltzmann constant.\n\nGeorge Johnstone Stoney was the first physicist to introduce the concept of natural units. He presented the idea in a lecture entitled \"On the Physical Units of Nature\" delivered to the British Association in 1874. Stoney units differ from Planck units by fixing the elementary charge at 1, instead of the Planck constant (only discovered after Stoney's proposal).\n\nStoney units are rarely used in modern physics for calculations, but they are of historical interest.\n\nThere are two types of atomic units, closely related.\n\nHartree atomic units:\nRydberg atomic units:\n\nCoulomb's constant is generally expressed as \n\nThese units are designed to simplify atomic and molecular physics and chemistry, especially the hydrogen atom, and are widely used in these fields. The Hartree units were first proposed by Douglas Hartree, and are more common than the Rydberg units.\n\nThe units are designed especially to characterize the behavior of an electron in the ground state of a hydrogen atom. For example, using the Hartree convention, in the Bohr model of the hydrogen atom, an electron in the ground state has orbital velocity = 1, orbital radius = 1, angular momentum = 1, ionization energy = , etc.\n\nThe unit of energy is called the Hartree energy in the Hartree system and the Rydberg energy in the Rydberg system. They differ by a factor of 2. The speed of light is relatively large in atomic units (137 in Hartree or 274 in Rydberg), which comes from the fact that an electron in hydrogen tends to move much slower than the speed of light. The gravitational constant is extremely small in atomic units (around 10), which comes from the fact that the gravitational force between two electrons is far weaker than the Coulomb force. The unit length, , is the Bohr radius, .\n\nThe values of and shown above imply that , as in Gaussian units, \"not\" Lorentz–Heaviside units. However, hybrids of the Gaussian and Lorentz–Heaviside units are sometimes used, leading to inconsistent conventions for magnetism-related units.\n\nThe Electron rest mass is replaced with that of the proton. \"Strong units\" are \"convenient for work in QCD and nuclear physics, where quantum mechanics and relativity are omnipresent and the proton is an object of central interest\".\n\nIn particle physics and cosmology, the phrase \"natural units\" generally means:\nwhere is the reduced Planck constant, is the speed of light, and is the Boltzmann constant.\n\nBoth Planck units and QCD units are this type of \"Natural units\". Like the other systems, the electromagnetism units can be based on either Lorentz–Heaviside units or Gaussian units. The unit of charge is different in each.\n\nFinally, one more unit is needed to construct a usable system of units that includes energy and mass. Most commonly, electronvolt (eV) is used, despite the fact that this is not a \"natural\" unit in the sense discussed above – it is defined by a natural property, the elementary charge, and the anthropogenic unit of electric potential, the volt. (The SI prefixed multiples of eV are used as well: keV, MeV, GeV, etc.)\n\nWith the addition of eV (or any other auxiliary unit with the proper dimension), any quantity can be expressed. For example, a distance of 1.0 cm can be expressed in terms of eV, in natural units, as:\n\nThe geometrized unit system, used in general relativity, is not a completely defined system. In this system, the base physical units are chosen so that the speed of light and the gravitational constant are set equal to unity. Other units may be treated however desired. Planck units and Stoney units are examples of geometrized unit systems.\n\n"}
{"id": "5420058", "url": "https://en.wikipedia.org/wiki?curid=5420058", "title": "Nodaway River", "text": "Nodaway River\n\nThe Nodaway River is a river in southwest Iowa and northwest Missouri.\n\nThe river's name (as \"Nodawa\") first appears in the journal of Lewis and Clark, who camped at the mouth of the river on July 8, 1804, but who provide no derivation of the name. The name is an Otoe-Missouria term meaning \"jump over water\". The term would be spelled today in full as Nyi At'ąwe (nyi (water) + a- (on) + t'ąwe (jump)) and would be contracted in regular speech as Nyat'ąwe or Nat'ąwe.\n\nLewis and Clark camped at the river's mouth on Nodaway Island on July 8, 1804, by Nodaway, Missouri, on the border of Holt County, Missouri and Andrew County, Missouri and took note of the river.\n\nLewis and Clark liked the spot enough that they recommended it for the winter headquarters of Astor Expedition of 1810–12 that discovered the South Pass in Wyoming through which hundreds of settlers on the Oregon Trail, California Trail, Mormon Trail were to pass.\n\nThe river is navigable only by shallow fishing and row boats although steam ships navigated just inside its mouth. The river was the primary route for white settlers including Amos Graham and Isaac Hogan following the Platte Purchase of 1836 which opened northwest Missouri for settlement. Nodaway County, which derives its name from the river, was by far the biggest county in the purchase and the fourth largest in the state of Missouri.\n\nMajor tributaries in the Nodaway River basin are Seven Mile Creek, West Nodaway River, East Nodaway River, Middle Nodaway River, Clear Creek, Mill Creek, Elkhorn Creek, and Arapahoe Creek. The biggest town on the river is Clarinda, Iowa.\n\nThe Nodaway begins near Shambaugh, Iowa at the confluence of the East and West Nodaway rivers. The West Nodaway River rises northeast of Massena in eastern Cass County, Iowa, and flows south-southwest past Villisca and Clarinda to its junction with the East Nodaway. The East Nodaway River rises just west of Orient in Adair County and flows southwest past Prescott, Corning, Brooks, and Nodaway to its confluence with the West Nodaway. The Middle Nodaway River rises in Adair County south of Casey and flows southwest past Greenfield, Fontanelle, and Carbon to join the West Nodaway just below Villisca, Iowa, above the West Nodaway's juncture with the East Nodaway. The East and West Nodaway join to form the Nodaway River four miles (6 km) north of the Iowa-Missouri border, and the river enters Missouri near Clearmont, Missouri.\n\nElevations in the Nodaway system range from just under above sea level at the source of the Middle Nodaway, to at the beginning of the main stem, to at its mouth on the Missouri River in Nodaway, Missouri in Andrew County, Missouri.\n\nThe Nodaway River is a sixth order river with a basin area of .\n\nThe Platte River basin is to the east and the Grand River and Des Moines River basins to the northeast, with the latter defining the boundary between the Missouri River and Mississippi River basins. The west side is bound by the Tarkio River basin and in the northwest by the Nishnabotna River basin.\n\nThe Nodaway River basin is prone to extensive flooding and can contribute as much as 20% of the flood crest of the Missouri River near its mouth.\n\nAt Graham, Missouri its normal flow is 1,011 cubic feet per second (28.6 m³/s). But during the Great Flood of 1993 the river was flowing 78,300 ft³/s (2220 m³/s) at Graham.\n\n\n\n"}
{"id": "1633982", "url": "https://en.wikipedia.org/wiki?curid=1633982", "title": "Protozoology", "text": "Protozoology\n\nProtozoology is the study of protozoa, the \"animal-like\" (i.e., motile and heterotrophic) protists. \n\nThis term has become dated as understanding of the evolutionary relationships of the eukaryotes has improved. For example, the Society of Protozoologists, founded in 1947, was renamed International Society of Protistologists in 2005. However, the term persists in some cases (e.g., the Polish journal \"Acta Protozoologica\").\n\n"}
{"id": "30970256", "url": "https://en.wikipedia.org/wiki?curid=30970256", "title": "Romeu Beltrão", "text": "Romeu Beltrão\n\nRomeu Beltrão (1913–1977), was a Brazilian physician, educator, historian and paleontologist. He was born and died in Santa Maria, Rio Grande do Sul, Brazil.\n\nIn 1920, at the age of seven, Beltrão entered the gymnasium (secondary school) at the College of Santa Maria. At fifteen, went to study at the Medical School of Porto Alegre. After graduating in medicine in December 1934, he returned home to Santa Maria. But by February 1935, he had moved to São Pedro do Sul, to practice medicine. While there he courted and married his wife, Nilza Niederauer Alvares, as well as writing opinion pieces for the local newspaper. But he missed the intellectual and cultural life of Santa Maria, so in 1937 he again returned home, where he devoted himself to medicine and teaching. In 1938 he became a Professor of Pharmaceutical Botany at the Universidade Federal de Santa Maria.\n\nBeltrão was strongly influenced by Llewellyn Ivor Price, and he undertook an intense survey of the paleontological finds from in and around Santa Maria. By 1951, he himself was actively engaged in paleontological excavations in the region. And in 1958, he published the book \"Cronologia Histórica de Santa Maria e do extinto município de São Martinho\" which included the history of paleontology in the Paleorrota area from 1787 to 1930. He abandoned his planned second volume which would have brought the history up to about 1960, and instead published his translation of Friedrich von Huene's paleontological work about Santa Maria, as well as a biography of Colonel João Niederauer Sobrinho (1827–1868) a hero of the Paraguayan War. He continued to write for the local newspaper and compiled a geographical dictionary of the Santa Maria municipality, which was never published.\n\nA residential street in Canoas was named \"Rua Romeu Beltrão\" in his honor.\n\n"}
{"id": "10363234", "url": "https://en.wikipedia.org/wiki?curid=10363234", "title": "Ronald Pearson Tripp", "text": "Ronald Pearson Tripp\n\nRonald Pearson Tripp (1914–2001) was a British paleontologist specializing in trilobites. Born in England in 1914, Tripp was self-taught in paleontology, but became an authority on the taxonomy of the trilobite families Encrinuridae, Lichidae, and Lichakephalidae – the latter of which he named. He wrote the section on the superfamily Lichacea for the monumental \"Treatise on Invertebrate Paleontology\", which was published in 1959, and wrote numerous articles and monographs pertaining to those families, as well as publications on entire trilobite faunas. As a Fellow of the Royal Society of Edinburgh, Treasurer of the Palaeontological Association, and Associate of the Natural History Museum, he established many new species and higher taxa of trilobites, particularly from the Ordovician rocks of the United Kingdom, before expanding his research globally. His wife, Doris, died in 1980, and he later married Phyllis Forrest and moved to Toronto, where he continued his trilobite research as Associate of the Royal Ontario Museum, after having become legally blind. Tripp died in Toronto, in 2001. Phyllis died suddenly two months later.\n\n"}
{"id": "4131939", "url": "https://en.wikipedia.org/wiki?curid=4131939", "title": "Saltation (biology)", "text": "Saltation (biology)\n\nIn biology, saltation (from Latin, \"saltus\", \"leap\") is a sudden and large mutational change from one generation to the next, potentially causing single-step speciation. This was historically offered as an alternative to Darwinism. Some forms of mutationism were effectively saltationist, implying large discontinuous jumps.\n\nSpeciation, such as by polyploidy in plants, can sometimes be achieved in a single and in evolutionary terms sudden step. Evidence is accumulating for various forms of saltation in a variety of organisms.\n\nPrior to Charles Darwin most evolutionary scientists had been saltationists. Jean-Baptiste Lamarck was a gradualist but similar to other scientists of the period had written that saltational evolution was possible. Étienne Geoffroy Saint-Hilaire endorsed a theory of saltational evolution that \"monstrosities could become the founding fathers (or mothers) of new species by instantaneous transition from one form to the next.\" Geoffroy wrote that environmental pressures could produce sudden transformations to establish new species instantaneously. In 1864 Albert von Kölliker revived Geoffroy's theory that evolution proceeds by large steps, under the name of heterogenesis.\n\nWith the publication of \"On the Origin of Species\" in 1859 Charles Darwin had denied saltational evolution by writing that evolutionary transformation always proceeds gradually and never in jumps. Darwin insisted on slow accumulation of small steps in evolution and wrote \"natural selection acts solely by accumulating slight successive favourable variations, it can produce no great or sudden modification; it can act only by very short steps\". Darwin continued in this belief throughout his life.\n\nFrom 1860 to 1880 saltation had a minority interest but by 1890 had become a major interest to scientists. In their paper on evolutionary theories in the 20th century (Levit \"et al\". 2008) wrote:\n\nThe advocates of saltationism deny the Darwinian idea of slowly and gradually growing divergence of character as the only source of evolutionary progress. They would not necessarily completely deny gradual variation, but claim that cardinally new ‘body plans’ come into being as a result of saltations (sudden, discontinuous and crucial changes, for example, the series of macromutations). The latter are responsible for the sudden appearance of new higher taxa including classes and orders, while small variation is supposed to be responsible for the fine adaptations below the species level.\n\nIn the early 20th century a mechanism of saltation was proposed as large mutations. It was seen as a much faster alternative to the Darwinian concept of a gradual process of small random variations being acted on by natural selection. It was popular with early geneticists such as Hugo de Vries, who along with Carl Correns helped rediscover Gregor Mendel's laws of inheritance in 1900, William Bateson, a British zoologist who switched to genetics, and early in his career Thomas Hunt Morgan. Some of these geneticists developed it into the mutation theory of evolution. There was also a debate over accounts of the evolution of mimicry and if they could be explained by gradualism or saltation. The geneticist Reginald Punnett supported a saltational theory in his book \"Mimicry in Butterflies\" (1915).\n\nThe mutation theory of evolution held that species went through periods of rapid mutation, possibly as a result of environmental stress, that could produce multiple mutations, and in some cases completely new species, in a single generation. This mutationist view of evolution was later replaced by the reconciliation of Mendelian genetics with natural selection into a gradualistic framework for the neo-Darwinian synthesis. It was the emergence of population thinking in evolution which forced many scientists to adopt gradualism in the early 20th century. According to Ernst Mayr, it wasn't until the development of population genetics in the neo-Darwinian synthesis in the 1940s that demonstrated the explanatory power of natural selection that saltational views of evolution were largely abandoned.\n\nSaltation was originally denied by the \"modern synthesis\" school of neo-Darwinism which favoured gradual evolution but has since been accepted due to recent evidence in evolutionary biology (see the current status section). In recent years there are some prominent proponents of saltation, including Carl Woese. Woese, and colleagues, suggested that the absence of RNA signature continuum between domains of bacteria, archaea, and eukarya constitutes a primary indication that the three primary organismal lineages materialized via one or more major evolutionary saltations from some universal ancestral state involving dramatic change in cellular organization that was significant early in the evolution of life, but in complex organisms gave way to the generally accepted Darwinian mechanisms. The geneticist Barbara McClintock introduced the idea of \"jumping genes\", chromosome transpositions that can produce rapid changes in the genome.\n\nSaltational speciation, also known as abrupt speciation, is the discontinuity in a lineage that occurs through genetic mutations, chromosomal aberrations or other evolutionary mechanisms that cause reproductively isolated individuals to establish a new species population. Polyploidy, karyotypic fission, symbiogenesis and lateral gene transfer are possible mechanisms for saltational speciation.\n\nThe botanist John Christopher Willis proposed an early saltationist theory of evolution. He held that species were formed by large mutations, not gradual evolution by natural selection.\n\nThe German geneticist Richard Goldschmidt was the first scientist to use the term \"hopeful monster\". Goldschmidt thought that small gradual changes could not bridge the hypothetical divide between microevolution and macroevolution. In his book \"The Material Basis of Evolution\" (1940) he wrote \"the change from species to species is not a change involving more and more additional atomistic changes, but a complete change of the primary pattern or reaction system into a new one, which afterwards may again produce intraspecific variation by micromutation.\" Goldschmidt believed the large changes in evolution were caused by macromutations (large mutations). His ideas about macromutations became known as the hopeful monster hypothesis which is considered a type of saltational evolution.\n\nGoldschmidt's thesis however was universally rejected and widely ridiculed within the biological community, which favored the neo-Darwinian explanations of R.A. Fisher, J. B. S. Haldane and Sewall Wright. However, there has been a recent interest in the ideas of Goldschmidt in the field of evolutionary developmental biology as some scientists are convinced he was not entirely wrong.\n\nOtto Schindewolf, a German paleontologist, also supported macromutations as part of his evolutionary theory. He was known for presenting an alternative interpretation of the fossil record based on his ideas of orthogenesis, saltational evolution and extraterrestrial impacts opposed to gradualism but abandoned the view of macromutations in later publications.\n\nSøren Løvtrup, a biochemist and embryologist from Denmark, advocated a similar hypothesis of macromutation to Goldschmidt's in 1974. Lovtrup believed that macromutations interfered with various epigenetic processes, that is, those which affect the causal processes in biological development. This is in contrast to the gradualistic theory of micromutations of Neo-Darwinism, which claims that evolutionary innovations are generally the result of accumulation of numerous very slight modifications. Lovtrup also rejected the punctuated equilibria of Stephen Gould and Niles Eldredge, claiming it was a form of gradualism and not a macromutation theory. Lovtrup defended many of Darwin's critics including Schindewolf, Mivart, Goldschmidt, and Himmelfarb. Mae Wan Ho described Lovtrup's theory as similar to the hopeful monster theory of Richard Goldschmidt.\n\nGoldschmidt presented two mechanisms for how hopeful monsters might work. One mechanism, involved “systemic mutations”, rejected the classical gene concept and is no longer considered by modern science; however, his second mechanism involved “developmental macromutations” in “rate genes” or “controlling genes” that change early development and thus cause large effects in the adult phenotype. These kind of mutations are similar to the ones considered in contemporary evolutionary developmental biology.\n\nOn the subject of Goldschmidt Donald Prothero in his book \"Evolution: What the Fossils Say and Why It Matters\" (2007) wrote:\n\nThe past twenty years have vindicated Goldschmidt to some degree. With the discovery of the importance of regulatory genes, we realize that he was ahead of his time in focusing on the importance of a few genes controlling big changes in the organisms, not small-scales changes in the entire genome as neo-Darwinians thought. In addition, the hopeful monster problem is not so insurmountable after all. Embryology has shown that if you affect an entire population of developing embryos with a stress (such as a heat shock) it can cause many embryos to go through the same new pathway of embryonic development, and then they all become hopeful monsters when they reach reproductive age.\n\nIn 2008 evolutionary biologist Olivia Judson in her article \"The Monster Is Back, and It’s Hopeful\" listed some examples which may support the hopeful monster hypothesis and an article published in the journal Nature in 2010 titled \"Evolution: Revenge of the Hopeful Monster\" reported that studies in stickleback populations in a British Columbia lake and bacteria populations in a Michigan lab have shown that large individual genetic changes can have vast effects on organisms \"without dooming it to the evolutionary rubbish heap\". According to the article \"Single-gene changes that confer a large adaptive value do happen: they are not rare, they are not doomed and, when competing with small-effect mutations, they tend to win. But small-effect mutations still matter — a lot. They provide essential fine-tuning and sometimes pave the way for explosive evolution to follow.\"\n\nA paper by (Page \"et al.\" 2010) have written that the Mexican axolotl (\"Ambystoma mexicanum\") could be classified as a hopeful monster as it exhibits an adaptive and derived mode of development that has evolved rapidly and independently among tiger salamanders. According to the paper there has been an interest in aspects of the hopeful monster hypothesis in recent years:\n\nGoldschmidt proposed that mutations occasionally yield individuals within populations that deviate radically from the norm and referred to such individuals as \"hopeful monsters\". If the novel phenotypes of hopeful monsters arise under the right environmental circumstances, they may become fixed, and the population will found a new species. While this idea was discounted during the Modern synthesis, aspects of the hopeful monster hypothesis have been substantiated in recent years. For example, it is clear that dramatic changes in phenotype can occur from few mutations of key developmental genes and phenotypic differences among species often map to relatively few genetic factors. These findings are motivating renewed interest in the study of hopeful monsters and the perspectives they can provide about the evolution of development. In contrast to mutants that are created in the lab, hopeful monsters have been shaped by natural selection and are therefore more likely to reveal mechanisms of adaptive evolution.\n\nGünter Theißen, a German professor of genetics, has classified homeotic mutants as hopeful monsters and documented many examples of animal and plant lineages that may have originated as hopeful monsters. American biologist Michael Freeling has proposed \"balanced gene drive\" as a saltational mechanism in the mutationist tradition, which could explain trends involving morphological complexity in plant and animal eukaryotic lineages.\n\nExamples of saltational evolution include cases of stabilized hybrids that can reproduce without crossing (such as allotetraploids) and cases of symbiogenesis. Both gene duplication and lateral gene transfer have the capacity to bring about relatively large changes that are saltational. Polyploidy (most common in plants but not unknown in animals) is saltational: a significant change (in gene numbers) can result in speciation in a single generation.\n\nEvidence of phenotypic saltation has been found in the centipede and some scientists have suggested there is evidence for independent instances of saltational evolution in Sphinx moths. Saltational changes have occurred in the buccal cavity of the roundworm \"Caenorhabditis elegans\". Some processes of epigenetic inheritance can also produce changes that are saltational. There has been a controversy over whether mimicry in butterflies and other insects can be explained by gradual or saltational evolution. According to Norrström (2006) there is evidence for saltation in some cases of mimicry. The endosymbiotic theory is considered to be a type of saltational evolution. Symonds and Elgar, 2004 have suggested that pheromone evolution in bark beetles is characterized by large saltational shifts. The mode of evolution of sex pheromones in \"Bactrocera\" has occurred by rapid saltational changes associated with speciation followed by gradual divergence thereafter. Saltational speciation has been recognized in the genus \"Clarkia\" (Lewis, 1966). It has been suggested (Carr, 1980, 2000) that the \"Calycadenia pauciflora\" could have originated directly from an ancestral race through a single saltational event involving multiple chromosome breaks. Specific cases of homeosis in flowers can be caused by saltational evolution. In a study of divergent orchid flowers (Bateman and DiMichele, 2002) wrote how simple homeotic morphs in a population can lead to newly established forms that become fixed and ultimately lead to new species. They described the transformation as a saltational evolutionary process, where a mutation of key developmental genes leads to a profound phenotypic change, producing a new evolutionary lineage within a species.\n\nReviewing the history of macroevolutionary theories, the American evolutionary biologist Douglas J. Futuyma notes that since 1970, two very different alternatives to Darwinian gradualism have been proposed, both by Stephen Jay Gould: mutationism, and punctuated equilibria. Gould's macromutation theory gave a nod to his predecessor with an envisaged \"Goldschmidt break\" between evolution within a species and speciation. His advocacy of Goldschmidt was attacked with \"highly unflattering comments\" by B. Charlesworth and Templeton. Futuyma concludes, following other biologists reviewing the field such as K.Sterelny and A. Minelli, that essentially all the claims of evolution driven by large mutations could be explained within the Darwinian evolutionary synthesis.\n\n\n"}
{"id": "57705944", "url": "https://en.wikipedia.org/wiki?curid=57705944", "title": "Solrad 9", "text": "Solrad 9\n\nSolrad 9, also known Explorer 37 and Explorer SE-B, was one of the SOLRAD (Solar Radiation) program that began in 1960 to provide continuous coverage of solar radiation with a set of standard photometers. \n\nWas launched on March 5, 1968 from Wallops Flight Facility, Virginia, United States, with Scout launch vehicle.\n\nSolrad 9 was a spin stabilized satellite oriented with its spin axis perpendicular to the sun-satellite line so that the 14 solar X-ray and UV photometers pointing radially outward from its equatorial belt viewed the sun with each revolution. Data were simultaneously transmitted via FM/AM telemetry and recorded in a core memory that read out its contents on command. Individual scientists and institutions were invited to receive and use the data transmitted on the 136 MHz telemetry band on the standard IRIG channels three through eight.\n\nIn the time that elapsed between the termination of Solrad 8 operations in August 1967 and the orbiting of Solrad 9, solar activity data were obtained using photometers in satellites OSO-4 and OGO-4.\n\nSolrad 9 was particularly important among the Solrad series satellites because thanks to the collected data were useful to predict the behavior of the sun during the period of the first manned missions of the Apollo Program, starting from the first, Apollo 7, it is therefore useful to draw up a mission program to ensure, from this point of view, the safety of astronauts.\n\nAs of July 1971, it was decided to use the Solrad 10 memory data, put into orbit on the 8th of the same month, and so continued until June 1973, when the Solrad 10 data storage device had a bad operation and NASA began to read data from the memory of Solrad 9. The satellite remained active until February 25, 1974, when the gas reserves useful to maintain control of the facility were over. Once the stability was lost, in fact, the satellite became useless and therefore was turned off. \n\nUnlike Solrad 8, its predecessor, Solrad 9, did not remain in orbit and returned to the atmosphere, disintegrating on November 16, 1990.\n"}
{"id": "2099106", "url": "https://en.wikipedia.org/wiki?curid=2099106", "title": "Space Telescope European Coordinating Facility", "text": "Space Telescope European Coordinating Facility\n\nThe Space Telescope – European Coordinating Facility (ST-ECF) was an institution which provided a number of support and service functions primarily for European observers of the NASA/ESA Hubble Space Telescope (HST). It was established in 1984 by the European Space Agency (ESA) and the European Southern Observatory (ESO), and was located at the ESO headquarters in Garching bei München, Germany. The ST-ECF ceased operations on 31 December 2010.\n\nThe ST-ECF provided detailed technical information about the HST and its science instruments, supported European astronomers who were preparing HST observing proposals and coordinated the development of computer software tuned to the specific data analysis needs of HST users. In all these duties the ST-ECF staff maintained close contacts with the Space Telescope Science Institute (STScI) in Baltimore, which is charged with the scientific operation of the HST observatory.\n\nThe ST-ECF was last headed by Robert (Bob) Fosbury. Deputy was Jeremy Walsh.\n\nFounded before the era of the internet and widely available computer resources the ST-ECF was founded with user support for European astronomers as the main goal.\nWhen internet connections across the Atlantic and powerful computers became the mainstay, the focus of the organization shifted towards data product and dedicated instrument science support.\n\nThe ST-ECF was actively engaged in software development in many areas related to HST data calibration, analysis and visualisation for especially the Hubble instruments Faint Object Spectrograph, STIS, and Advanced Camera for Surveys. For example:\n\n\nTogether with the European Southern Observatory (ESO)\nthe ST-ECF operated and maintained the ESO/ST-ECF Science Archive Facility from where all the scientific data collected by the Hubble Space Telescope could be accessed by the public.\nIn collaboration with the HST archives at the STScI and the Canadian Astronomy Data Centre (CADC) data products and access methods were continuously improved to assure the best possible science data for astronomers around the world. This includes work on the Virtual Observatory, On-The-Fly Calibration, and B Associations. Earlier work pioneered projects like Astrovirtel and Astrophysical Virtual Observatory.\n\nAfter the closure of ST-ECF the European HST Archive were moved to ESA's European Space Astronomy Centre in Spain, where most of ESA's Space Science Archives are located and operated from.\n\nOne of the groups at the ST-ECF was the Hubble European Space Agency Information Centre (HEIC), which, since 1999, has been the leading Hubble outreach activities group in Europe (headed by the science communication specialist Lars Lindberg Christensen and graphic designer Martin Kornmesser). HEIC's mission statement is to fulfill the Hubble Space Telescope outreach and education tasks for the European Space Agency.\n\nHEIC became a very distinguished communication office of experts using the newest software and techniques. After the closure of ST-ECF the Outreach component continued on an ESO contract for ESA, as part of ESO's Education and Public Outreach department.\n\nThe ESA/Hubble office has produced large amounts of astronomical material – press releases, images, brochures, web pages, books etc. – suitable both for educational purposes and wider public consumption. HEIC provided a well-assorted archive that is publicly available on its web page.\n\nThe work is centred on the production of news and photo releases that highlight interesting Hubble science results and images. These are often European in origin, and so not only increase the awareness of European Space Agency’s Hubble share (15%), but the contribution of European scientists to the observatory. Furthermore the group produces video releases, innovative educational material, CD-ROMs, brochures, posters, as well as DVDs and museum information kiosks, and much more.\n\nAll publicised material can be found on spacetelescope.org. The group is home to the FITS Liberator project and the \"Hubble – 15 Years of Discovery\" project that also led to the book Hubble - 15 Years of Discovery.\n\n"}
{"id": "28678181", "url": "https://en.wikipedia.org/wiki?curid=28678181", "title": "Spirit Mound Historic Prairie", "text": "Spirit Mound Historic Prairie\n\nSpirit Mound Historic Prairie is a state park of South Dakota, USA, featuring a prominent hill on the Great Plains. The Plains Indians of the region considered Spirit Mound the home of dangerous spirits or little people; members of the Lewis and Clark Expedition climbed it on August 25, 1804. The park was established in 2002. It is located about north of Vermillion, South Dakota.\n\nStories and religious beliefs about \"Little People\" are common to many if not most Native American tribes in the West. In 1804, the Lewis and Clark Expedition stayed for a time with a band of Wičhíyena Sioux on the Vermillion River in modern-day South Dakota. On August 25, Meriwether Lewis, William Clark, and 10 other men traveled about north of the river's junction with the Missouri River to see the \"mountain of the Little People\". Lewis wrote in his journal that the Little People were \"deavals\" (devils) with very large heads, about high, and very alert to any intrusions into their territory. The Sioux said that the devils carried sharp arrows which could strike at a very long distance, and that they killed anyone who approached their mound. The Little People so terrified the local population, Lewis reported, that the Maha (Omaha), Ottoes (Otoe), and Sioux would not go near the place. The Lakota people who came to live near the \"Spirit Mound\" after the Wičhíyena Sioux have a story no more than 250 years old which describes how a band of 350 warriors came near the mound late at night and were nearly wiped out by the ferocious Little People (the survivors were crippled for life).\n\nDue to extensive damming of the Missouri River, Spirit Mound is one of the few places which historians can identify as a precise spot upon which Lewis and Clark stood .\n\nSpirit Mound was in private hands for many decades, leading to extensive degradation of the site's original status. White settlers first came to the area in 1868, and used it for grazing livestock and for farming. Five separate landowners owned parts of the site in the early 1980s. More than 20 buildings, a feed lot, soybean fields, corn fields, several roads, 1,500 non-native trees, and of fence dotted the site.\n\nSeveral local citizens formed the Spirit Mound Trust in 1986 in an attempt to preserve the site. The group received funding boosts and publicity with the 1996 publication of the book \"Undaunted Courage\" (about the Lewis and Clark Expedition) and the 1997 Ken Burns documentary \"\". After years of lobbying, the federal government's Land and Water Conservation Fund provided $600,000 to acquire the site. The donation required that State of South Dakota establish the site as a state park, while the Spirit Mound Trust would restore it, provide interpretational signage and tours, and raise $500,000 for the site's long-term preservation. The site was purchased from the private landowners in 2001. On July 29, 2001, Senator Tim Johnson presented a symbolic check for $600,000 for the purchase of Spirit Mound and the surrounding land.\n\nIn 2004 the United States Department of the Interior designated the trail leading to the summit of Spirit Mound as a National Recreation Trail.\n\nThe Spirit Mound Creek runs right past the Mound, on its southern side.\n\nSome controversy has surrounded the establishment of the state park. In 2001 a few local Native American leaders expressed dismay that a site sacred and terrifying to local tribes would be treated as a \"fun\" place to visit.\n\n\n"}
{"id": "21743214", "url": "https://en.wikipedia.org/wiki?curid=21743214", "title": "Stop squark", "text": "Stop squark\n\nIn particle physics, a stop squark, symbol , is the superpartner of the top quark as predicted by supersymmetry (SUSY). It is a sfermion, which means it is a spin-0 boson (scalar boson). While the top quark is the heaviest known quark, the stop squark is actually often the lightest squark in many supersymmetry models.\n\nThe stop squark is a key ingredient of a wide range of SUSY models that address the hierarchy problem of the Standard Model (SM) in a natural way. A boson partner to the top quark would stabilize the Higgs boson mass against quadratically divergent quantum corrections, provided its mass is close to the electroweak symmetry breaking energy scale. If this was the case then the stop squark would be accessible at the Large Hadron Collider. In the generic R-parity conserving Minimal Supersymmetric Standard Model (MSSM) the scalar partners of right-handed and left-handed top quarks mix to form two stop mass eigenstates. Depending on the specific details of the SUSY model and the mass hierarchy of the sparticles, the stop might decay into a bottom quark and a chargino, with a subsequent decay of the chargino into the lightest neutralino (which is often the lightest supersymmetric particle).\n\nMany searches for evidence of the stop squark have been performed by both the ATLAS and CMS experiments at the LHC but so far no signal has been discovered. \n"}
{"id": "59151390", "url": "https://en.wikipedia.org/wiki?curid=59151390", "title": "Sulfobacillus", "text": "Sulfobacillus\n\nSulfobacillus is a genus of bacteria containing five named species. Members of the genus are Gram-positive, acidophilic, spore-forming bacteria that are moderately thermophilic or thermotolerant. All species are facultative anaerobes capable of oxidizing sulfur-containing compounds; they differ in optimal growth temperature and metabolic capacity, particularly in their ability to grow on various organic carbon compounds.\n\n\"Sulfobacillus\" was first described in 1978, along with the type species, \"Sulfobacillus thermosulfidooxidans\". Four additional species have since been described, in at least one case discovered after samples believed to be \"S. thermosulfidooxidans\" showed unexpected characteristics.\n\nThe genus is of uncertain taxonomic position. It was originally placed in the Clostridiales. It is likely related to the genus \"Thermaerobacter\" and may represent either a deep branch of the \"Firmicutes\" or a separate phylum.\n\n\"Sulfobacillus\" species are found globally in both natural and artificial acidic environments, such as hot springs, solfatara environments, hydrothermal vents, and in various forms of acid mine drainage. Compared to other bacterial species found in similar acidic environments, \"Sulfobacillus\" species are often present at relatively low abundance.\n\nThe genomes of several \"Sulfobacillus\" species have been sequenced. Differences between members include genome size and gene content related to sulfur oxidation pathways.\n"}
{"id": "11747122", "url": "https://en.wikipedia.org/wiki?curid=11747122", "title": "University of Edinburgh School of Chemistry", "text": "University of Edinburgh School of Chemistry\n\nThe School of Chemistry is an academic unit of the University of Edinburgh, in Scotland, The research rating of the 2008 RAE was one of the highest in the UK.\n\nThe teaching of Chemistry at Edinburgh began in 1713 when James Crawford was appointed to the Chair of Physics and Chemistry. The department has occupied many sites in its history, from a house at the top of Robertson's Close in the city centre, to purpose-built facilities in the central campus at Old College through to its current location at King's Buildings. Each move has brought with it expansions in size and status until the department occupied the position it does now, as one of the world's leading Chemistry teaching and research establishments.\n\nToday the department carries on the traditions of Chemistry at Edinburgh both in teaching and research. The collaborative research School formed with St Andrews Chemistry department to form EaStCHEM has strengthened research in Scotland in the chemical sciences.\n\nThe school has a research staff of approximately 120 individuals, and an academic staff of over 40. Current annual enrollment includes around 200 research students, and 450 taught postgraduate and undergraduate students.\n\nEaStCHEM is the joint research school in chemistry between the Universities of Edinburgh and St Andrews. It has eight research groupings: Chemical Biology; Synthesis; Materials; Structural Chemistry; Chemical Physics; Biophysical Chemistry; Inorganic Chemistry; and Catalysis.\n\nIn the Research Assessment Exercise (RAE) 2008, the most in-depth analysis of research outputs for seven years, EaStCHEM, submitted 73% of all world leading outputs (4*) in Scotland and 12% of world leading outputs in all of the UK. From 31 submissions EastChem was the largest in UK Chemistry. EaStCHEM comes joint 4th in the Grade Point Average (GPA) metric, and first when staff numbers are factored in (the power ranking).\n\nThe School has four major research themes as part of EaStCHEM:\n\nThe Chemistry/Biology Interface area is broad, with particular strengths in the areas of protein structure and function, mechanistic enzymology, proteomics, biologically targeted synthesis, the application of high throughput and combinatorial approaches and biophysical chemistry, which focuses on the development and application of physicochemical techniques to biological systems.\n\nChemical Physics/Physical Chemistry is the fundamental study of molecular properties and processes. Areas of expertise include probing molecular structure in the gas phase, clusters and nanoparticles, the development and application of physicochemical techniques such as mass spectrometry to molecular systems and the EaStCHEM surface science group, who study complex molecules on surfaces, probing the structure property-relationships employed in heterogeneous catalysis. A major feature is In Silico Scotland, a world class research computing facility.\n\nMolecular Synthesis encompasses the synthesis and characterisation at ambient and extreme conditions of organic and inorganic compounds, including those with application in homogeneous catalysis, nanotechnology, supramolecular chemistry, drug discovery and ligand design. The development of innovative synthetic and characterisation methodologies (particularly in structural chemistry) is a key feature.\n\nThe Materials Chemistry group is one of the largest materials chemistry groups in the UK. Areas of strength include the design, synthesis and characterisation of strongly correlated electronic materials, battery and fuel cell materials and devices, porous solids, materials at extreme pressures and temperatures, polymer microarray technologies and technique development for materials and nanomaterials analysis.\n\n"}
{"id": "50081260", "url": "https://en.wikipedia.org/wiki?curid=50081260", "title": "Vera Gromova", "text": "Vera Gromova\n\nVera Isaakovna Gromova (, March 8, 1891 – January 21, 1973) was a Soviet paleontologist known for her studies of fossil ungulates (hoofed mammals). She worked at the Russian Academy of Sciences, where from 1919 to 1942 she was head of osteology, Zoological Museum, and from 1942 to 1960 at the Paleontological Institute , where she was head of mammal laboratory from 1946 onward. Her works include \"The history of horse (genus \"Equus\") in the Old World\" (1949) and \"Fundamentals of Paleontology: Mammals\" (1968).\n"}
