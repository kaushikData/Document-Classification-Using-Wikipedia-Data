{"id": "14754562", "url": "https://en.wikipedia.org/wiki?curid=14754562", "title": "3C 35", "text": "3C 35\n\n3C 35 is a giant Radio galaxy with an active galactic nucleus (AGN). It is classified as a Fanaroff & Riley type II radio galaxy. It is located in the constellation Cassiopeia.\n\nIt is listed as a quasar by the SIMBAD astronomical database.\n\n"}
{"id": "2365712", "url": "https://en.wikipedia.org/wiki?curid=2365712", "title": "Abraum salts", "text": "Abraum salts\n\nAbraum salts is the name given to a mixed deposit of salts, including halite (sodium chloride), carnallite, and kieserite (magnesium sulfate), found in association with rock salt at Aschersleben-Staßfurt in Germany. The term comes from the German \"Abraum-salze\", \"salts to be removed.\"\n\nAbraum, which is red, is used to darken mahogany.\n"}
{"id": "21950576", "url": "https://en.wikipedia.org/wiki?curid=21950576", "title": "Alberta Hail Project", "text": "Alberta Hail Project\n\nThe Alberta Hail Project was a research project sponsored by the Alberta Research Council and Environment Canada to study hailstorm physics and dynamics in order to design and test means for suppressing hail. It ran from 1956 until 1985. The main instrument in this research was an S-band circularly polarized weather radar located at the Red Deer Industrial Airport in central Alberta, Canada. \n\nA vast amount of data were collected from several other platforms to conduct research into precipitation mechanisms, severe storm development, hail suppression, hydrology and microwave propagation. Numerous researchers have used the dataset and during the period 1990 to 1994, 23 publications appeared in journals and conferences, as well as 4 scientific reports were prepared. These papers have included radar meteorology, cloud physics, hydrology/hydrometeorology, computer science, instrumentation, synoptic weather, dynamic and mesoscale meteorology. \n\nThe project area covered 33,700 km and was centered on the Penhold radar site located near the Red Deer Airport (). The program had different ways of evaluating the atmospheric conditions for hail and its detection.\n\nEach spring, approximately 20,000 farmers in the project area would receive cards to record any event of hail, including its size. On days with hail, between 10 % and 20 % of the farmers gave report, an average of one observer per 16–32 km. Telephone surveys were also conducted, resulting in observation densities as high as one report per 3 square kilometers. As a result, it is believed that only a very small percentage of hail reaching the surface went undetected. For the duration of the project, six categories were specified to size the observed hailstones conveniently: diameters less than 0.4 cm, 0.4–1.2 cm, 1.3–2.0 cm; 2.1–3.2 cm, 3.3–5.2 cm, and larger than 5.2 cm.\n\nRadiosondes were released from Penhold at 1715 local time (2315 UTC). Data were used to evaluate the atmospheric conditions on hail days within 3 h (i.e., between 1415 and 2015 LT) and within 100 km from the sounding site. A few of these proximity soundings had to be excluded because of missing data or because they were modified by precipitation or a thunderstorm outflow boundary. In addition to upper-air sounding data, the surface temperature and dewpoint (representative of the storm's inflow) were obtained from a mesonetwork.\n\nThe radar would be used to detect not only the intensity of the precipitation by its reflectivity, but the type of hydrometeors with the circular polarization. These would be correlated with the surface observations to study the structure of the thunderstorms leading to hail formation. Furthermore, at certain times there would be aircraft flight in and around convective areas to gather further information on the atmospheric conditions and sample the clouds.\n\nObservations were also made by sampling vehicles. The vehicles were outfitted with various meteorological instrumentation and hail catching apparatus. They were directed into suspected thunderstorm hail regions by a controller at the radar site. The controller communicated with the vehicles by radio.\n\n"}
{"id": "59225804", "url": "https://en.wikipedia.org/wiki?curid=59225804", "title": "Anna Gelman", "text": "Anna Gelman\n\nAnna Dmitrievna Gelman (also Dmitriyevna/Dmitryevna) (Russian: Гельман Анна Дмитриевна) (18 February 1902 — 29 March 1991) (née Nikitin) was a Russian chemist and engineer. She was the first to synthesise alkene complexes of platinum, discovered heptavalent states of plutonium and other actinides and created and improved processes for radionuclide production.\n\nShe was born in Biysk, Altai Krai, Siberia. Her mother was a seamstress and her father a woodman. Her father died when she was seven, so she helped her mother with her three younger siblings. Aged eight, she went to school and graduated with a merit certificate. She attended Biysk Women's High School and because of her study efforts was released from school fees. She graduated in 1921 but had not received appropriate education to enrol at Tomsk University, 450 km to the north. She underwent two years of self-study while teaching in a Tomsk orphanage, before being accepted on a one-year course in Leningrad, 3400 km from her home town. While there, she married August Ansovich Gelman, a border guard. Thereafter, she lived in different border posts of Siberia and central Asia while teaching.\n\nIn 1928, she was transferred to the Crimea for health reasons. In 1930, she enrolled at the Crimean Pedagogical Institute. She studied in the department of chemistry and worked in a bromine plant in the town of Saki. In the spring of 1932, her husband died. That autumn, she graduated and was recommended for postgraduate studies at the Leningrad Pedagogical Institute. She passed the entrance exams but also took the exams for graduate studies. The course leader, I.I. Chernyaev, told her she had the knowledge but that, aged 30, she was too old to study science. She made it clear she was offended, and Chernyaev took her under his supervision. From 1932-36, she was a graduate student and a senior research associate at the Institute of National Economy and Leningrad State University. In 1934, she published her first work, under her maiden name, Nikitin, as an engineer: the VIAM (All-union Institute for Aviation Materials (Russian:Всесоюзный институт авиационных материалов)) production instruction. She was a part-time assistant professor at the Department of Inorganic Chemistry at the University from 1936-38. Chernyaev invited her to become a doctoral student at the in Moscow in 1938. She gained a Ph.D. in 1941 and worked as a senior researcher at the Institute until 1949.\n\nGelman was the first to find ways of synthesising ethylene (and other alkene) complexes of platinum. Her graduate work in 1941 explained the action of ethylene on platinum (III) salts at low pressure. She found new compounds of platinum (II) and (III) using propylene and butene amongst other alkenes. The Academy of Sciences was evacuated to Kazan in World War II; while there she received a telegram from Josef Stalin in 1943 thanking her for her concern about Soviet forces. A monograph of her doctoral dissertation was published in 1945 after the Allied victory and she received the Order of the Badge of Honour, 1st degree for her scientific work and patronage. She continued with work on platinum-alkene complexes, which were later used in homogeneous catalysis.\n\nIn the winter of 1945, Chernyaev instructed Gelman to develop processes to isolate uranium and thorium compounds from aqueous solutions. This was followed up with the isolation of plutonium from uranium and nuclear fission by-products in 1947. In the absence of plutonium - which was only just being produced in small quantities in the Soviet Union - Gelman and her colleague L.N. Essen replaced plutonium (IV) with thorium, plutonium (III) with lanthanum and used rare-earth element fragments to imitate fission products. This production scheme, in which oxalates and carbonates of plutonium were removed from the solution, was verified at NII-9 (now the All-Russian Scientific Research Institute of Inorganic Materials (VNIINM)) for implementation at Plutonium Combine No.817, which was just being built; the product for delivery to the metallurgical department was purified plutonium dioxide. Gelman was seconded to the plant until 1951; all radiochemical staff - many of whom were young graduates from technical schools and universities - faced considerable risks to their health while production and safety procedures were being developed. Plant workers suffered and died, e.g. from pneumosclerosis, after the inhalation of aerosols of alpha particle-emitting radionuclides produced in processing after the irradiation of the uranium source material. Gelman, with her colleagues Chernyaev, and V.D. Nikolsky, wrote the first regulations for processing plutonium salts; they won state prizes in 1949 and 1951 for creating and improving plutonium refining technology.\n\nGelman returned to Moscow in 1952, securing 50 grammes of plutonium dioxide from Combine No.817 for researchers to use at the Institute of General and Inorganic Chemistry. The compound was used in the doctoral theses of students who went on to become leading researchers at the plutonium plant. She became part-time deputy chief of the plant, working in the radiochemical research section. She married Boris Musrukov, director of the plant and major-general of the engineering service. He was soon transferred to the Ministry of Medium Machine Building and, in 1955, he was appointed director of Arzamas-16, (also known as KB-11 [Design Bureau-11] based at the closed city of Sarov, Nizhny Novgorod oblast. Gelman raised his two children and, after separating from him, his daughter Elena remained with Gelman.\n\nV.I. Spitzyn, director of the Institute of Physics and Chemical Sciences of the Academy of Sciences, suggested in 1954 (with the support of Chernyaev and Ministry of Medium Machine Building (the state organ controlling nuclear facilities) that Gelman transfer there because a new 'hot' laboratory was being built. She accepted and directed the laboratory researching transuranium elements, inviting colleagues from Combine No.817 and the Institute of Microorganisms to join her. She remained at the laboratory for the rest of her career. In the primary research undertaken, various actinides were studied with many different types of ligands in simple and complex compounds, with research published internationally. Gelman, with students N.N. Krot and M.P. Mefod'eva discovered heptavalent states of plutonium and neptunium in 1967 (Gelman and Krot later received the State Prize of the USSR for their discovery of the heptavalency of plutonium, neptunium and americium). Gelman retained and developed connexions with her former workplaces and other radioisotope research centres, particularly the Mayak plutonium production plant and also the Siberian Chemical Combine in Tomsk and the Mining and Chemical Combine in Krasnoyarsk. Methods continued to be improved for the isolation of plutonium and neptunium salts.\n\nIn 1971, Gelman put forward the 38-year-old N.N. Krot as her replacement as the laboratory director, over the objections of some of the Academic Council of the Institute of Physics of the Academy of Sciences. She remained in a consulting role.\n\nShe died on 29 March 1994 in Moscow. She was buried at Novokuntsevo cemetery.\n\n"}
{"id": "739327", "url": "https://en.wikipedia.org/wiki?curid=739327", "title": "Brian Binnie", "text": "Brian Binnie\n\nWilliam Brian Binnie (born 1953) is a former United States Navy officer and one of the test pilots for SpaceShipOne, the experimental spaceplane developed by Scaled Composites and flown from 2003–2004.\n\nBinnie was born in West Lafayette, Indiana, where his Scottish father William P. Binnie was a professor of physics at Purdue University. The family returned to Scotland when Binnie was five, and lived in Aberdeen (his father taught at Aberdeen University) and later in Stirling. When Binnie was a teenager the family moved to Boston.\n\nBinnie, an alumnus of Brown and Princeton Universities, served for 21 years in the United States Navy as a naval aviator flying the A-7 Corsair II, A-6 Intruder, F/A-18 Hornet, and AV-8B Harrier II. He graduated from the U.S. Naval Test Pilot School in 1988. Binnie also copiloted the Atmospheric Test Vehicle of the Rotary Rocket. In 2006, he received an Honorary degree from University of Aberdeen.\n\nOn December 17, 2003, the 100th anniversary of the Wright brothers' first powered flight, Binnie piloted the first powered test flight of SpaceShipOne, flight 11P, which reached a top speed of Mach 1.2 and a height of . On October 4, 2004, he piloted SpaceShipOne's second Ansari X Prize flight, flight 17P, winning the X Prize and becoming the 435th person to go into space. His flight, which peaked at , set a winged aircraft altitude record, breaking the old record set by the North American X-15 in 1963. It also earned him the second set of Astronaut Wings to be given by the FAA for a flight aboard a privately operated commercial spacecraft.\n\nIn 2014 Binnie joined XCOR Aerospace as senior engineer and test pilot, after working as a test pilot and program business manager for Scaled Composites for many years.\n\n"}
{"id": "35927838", "url": "https://en.wikipedia.org/wiki?curid=35927838", "title": "Britten–Davidson model", "text": "Britten–Davidson model\n\nThe Britten–Davidson model, also known as the gene-battery model, is a hypothesis for the regulation of protein synthesis in eukaryotes. Proposed by Roy John Britten and Eric H. Davidson in 1969, the model postulates four classes of DNA sequence: an integrator gene, a producer gene, a receptor site, and a sensor site. The sensor site regulates the integrator gene, responsible for synthesis of activator RNA. The integrator gene cannot synthesize activator RNA unless the sensor site is activated. Activation and deactivation of the sensor site is done by external stimuli, such as hormones. The activator RNA then binds with a nearby receptor site, which stimulates the synthesis of mRNA at the structural gene.\n\nThis theory would explain how several different integrators could be synthesized at the same time. In addition, this theory would explain the pattern of repetitive DNA sequences followed by a unique DNA sequence that exists in genes.,\n\n"}
{"id": "50922281", "url": "https://en.wikipedia.org/wiki?curid=50922281", "title": "Civic Planetarium of Lecco", "text": "Civic Planetarium of Lecco\n\nThe Civic Planetarium of Lecco ( or ) also known as the City Planetarium is located within Palazzo Belgioioso in Lecco, Italy. Planetarium is part of Natural History Museum of Lecco.\n\nThe purpose of the planetarium is to be a place for specialists and people interested in astronomy in the region. It organizes meetings and lessons in which experiences and expertise are shared.\n\nThe Amateur Astronomers Group \"DEEP SPACE Lecco\" helps manage the observatory. The instrument consists of a projector and an aluminum dome, which acts as a screen. It can be used to simulate the movement of celestial bodies on the sky. The dome is eight meters in diameter and can seat sixty observers. Often the shows are accompanied by a lecturer.\n\nThe planetarium accelerates the movement of the moon and stars in order to allow observation of processes that normally take days or months. The Planetarium building consists of various rooms: the ticket office, a library for members, a room dedicated to the exhibition of astronomical instruments and a media room used for conferences. \n\nThe astronomical dome in the Planetarium was inaugurated on September 19, 2011. The small dome on the roof of the building comes from Canada and inside there is a Celestron C11 telescope, a catadioptric remotely controlled Schmidt-Cassegrain, so they can observe the whole sky even in the conference room. \n\nAs of July 2016 the director of the planetarium is Erasmo Bardelli. The ticket costs 3 euros.\n\n"}
{"id": "12865108", "url": "https://en.wikipedia.org/wiki?curid=12865108", "title": "Effort heuristic", "text": "Effort heuristic\n\nThe effort heuristic is a mental rule of thumb in which the quality or worth of an object is determined from the perceived amount of effort that went into producing that object. In brief, the effort heuristic follows a tendency to judge objects that took a longer time to produce to be of higher value. The more effort invested in an object, the better it is deemed to be. This is especially true in situations where value is difficult to assess or the evaluator lacks expertise in the appraisement of an item. People use whatever information is available to them and effort is thought to generally be a reliable indicator of quality.\n\nAn example of this would be the comparison of $100 earned, and $100 found. If someone finds $100 they are more inclined to go spend it on a whim, but if that $100 is part of a hard-earned paycheck, they are less likely to squander it away. Another way that effort heuristic can be considered is the amount of effort a person will put into an action depending on the goal. If the goal is of little importance, the amount of effort a person is willing to put into it is going to be lower.\n\nThere is experimental evidence that supports the notion that people sometimes use the effort that has been put into doing something as an estimate for its quality. The seminal study that investigated this phenomenon was done by Kruger, Wirtz, Van Boven, and Altermatt (2004). They conducted three experiments in which participants made judgments of quality—a poem in Experiment 1, paintings in Experiment 2, and medieval arms and armor in Experiment 3. In each experiment, they manipulated the effort seemingly invested in the objects' creation. Despite the fact that the actual quality of the work remained the same, they anticipated that manipulations in supposed effort would influence perceived quality.\n\nIn the first experiment, participants evaluated a poem with respect to how much they enjoyed it, the overall quality of the poem, and the amount of money a poetry magazine would pay for the poem. They were told that the experiment concerned the way in which people evaluate poetry. The subjects were randomly assigned to one of two condition groups in this study: low effort and high effort. Participants in the low effort condition were told that the writer spent 4 hours on the poem while participants in the high effort condition were told the poet spent 18 hours on the piece. The researchers combined the liking and quality measures into one composite result and found participants provided more favorable evaluations of the poem when they thought it took the poet 18 hours to compose rather than when they thought it took him 4 hours. They also judged the more effortful poem to be worth more money.\n\nIn the second experiment, non-experts and self-identified experts individually evaluated the quality of two paintings by Deborah Kleven: \"12 Lines\" and \"Big Abstract.\" Half of the participants were told that the former took 4 hours to paint and the latter 26 hours, and the other half were told the opposite. After rating each painting separately, participants then compared the two paintings directly.\n\nThe results revealed that participants preferred \"12 Lines\" over \"Big Abstract\" when they thought \"12 Lines\" took longer to paint, but the opposite tended to be true when they thought that \"Big Abstract\" took longer to paint. The effort manipulation had a similar effect on participants estimates of how much the paintings were worth. Participants who thought \"12 Lines\" took longer to produce thought that it was worth more money than \"Big Abstract\", whereas the opposite tended to be true when participants thought that \"Big Abstract\" took longer to paint. The data also indicated that the effect of perceived effort on perceived quality was independent of whether participants had self-professed expertise in the domain. Self-identified art experts did not appear to rely on effort any less than novices, despite the fact that the self-identified experts were presumably more practiced at evaluating art. This points to the generality and intuitive appeal of effort as a heuristic for quality.\n\nIn the third and final experiment, researchers asked participants to rate the quality of several images of medieval arms and armor presented on a computer screen. When rating the final target piece of armor, half of the participants were told that it took the blacksmith 110 hours to complete, and half were told that it took 15 hours. In addition to manipulating the perceived effort invested by the artist, researchers also varied the ambiguity of the stimulus to examine its potential as a moderator in the use of the effort heuristic. This was done by altering the resolution of the image where half of the participants viewed a high-resolution image of the piece, and half viewed a low-resolution image.\n\nExperiment 3 produced similar results as the first two; participants provided higher ratings of the piece when they thought it took the blacksmith longer to produce. The more effort invested in the object, the better it was assumed to be. The influence of effort on judgment was also bigger in the high-ambiguity condition than in the low-ambiguity condition. This was expected because the quality of the armor was more ambiguous in the low-resolution condition. Participants in this condition had less objective information upon which to make a judgment of quality, and thus were more likely to rely on the perceived effort invested by the blacksmith while evaluating.\n\nHuman behavior, like most other animals, is often driven by rewards and directed by the energetic cost of an action. It takes effort to attain rewards, and people accordingly weigh the value of rewards against the amount of effort that is required to attain them. At an early age, children learn that good performance due to high effort is valued by adults and that teachers are most likely to reward those who work hard. With experience, they internalize effort as a valuable commodity.\n\nResearch directly examining and testing the effort heuristic is scarce, however there are other research areas that incorporated this construct and, in some way, manipulated its effects. However, a key distinction between the study done by Kruger et al. and other research on the role of effort on judgment is that the former focused on other-generated effort rather than self-generated effort.\n\nOver half a century ago, social psychologist Leon Festinger developed the theory of cognitive dissonance. It asserts that inconsistencies among a person's beliefs, attitudes, or opinions produce psychological discomfort, leading people to rationalize their behavior or change their attitudes. One concept that stemmed from the dissonance theory is that of effort justification in which the subjective value of an outcome is directly related to the effort that went into obtaining it. When people suffer, work hard or make sacrifices, they will attempt to convince themselves that it is worthwhile. People tend to put the most value on goals or items which have required considerable effort to achieve. This is probably because cognitive dissonance would arise if great effort is made to achieve something that is subsequently evaluated negatively.\n\nAronson and Mills conducted a study where college students underwent a \"severe\" or a \"mild\" initiation to join a discussion group. Subjects in the severe initiation group were required to read a sexually explicit passage out loud in front of the experimenter, whereas those in the mild initiation group read a less embarrassing passage. When subjects were asked to evaluate the discussion group, those in the severe initiation condition rated it higher than those in the mild initiation group. Aronson and Mills interpreted their result in terms of cognitive dissonance. According to Aronson and Mills, to resolve the dissonance produced by reading the embarrassing passage, subjects in the severe initiation group gave more value to the discussion group than did subjects in the mild initiation group. The more difficult the task, the greater the value given to sources of reinforcement that followed task completion.\n\nDaryl Bem (1965) proposed an alternative to the cognitive dissonance theory in explaining how attitudes are shaped. The self-perception theory suggests that people infer their own attitudes, opinions, and other internal states partly by observing their behavior and the circumstances in which that behavior occurs. This pattern of thought unfolds because people are engaged in normal efforts to better understand their own behaviors. Bem suggested that all individuals analyze their own behavior as much as an outside observer might and, as a result of these observations, people make judgments about why they are motivated to do what they do. He originally believed that most findings explained by cognitive dissonance were really due to self-perception. However, studies have shown that self-perception is primarily at work when subjects do not have well-defined attitudes regarding the issue at hand.\n\nResearch has shown that the amount of energy put towards the achievement of a goal may play a role in the development or change of an individual's attitude towards that goal. While evaluating a goal, one is motivated to place more value on a goal that has required greater effort to achieve. Axsom and Cooper (1985) suggested that if an objective or the way in which a goal is obtained is not initially attractive, an individual may later look to their own past behavior to determine their attitude towards that goal. If much effort has been spent in the attainment of a goal, it should come to be seen as worthwhile and therefore more attractive. This resembles the need to justify one's efforts in accordance with the cognitive dissonance theory but there is evidence supporting that other factors may be at play.\n\nOne study proposed that the experienced goal value and consumers' subsequent motivation vary as a function of whether or not the pursuit of the goal is perceived to be one's autonomous choice. They found that when consumers perceive that the goal they pursue is adopted through an autonomous choice, the initial effort investment is experienced as reflecting the value of the goal; therefore, greater effort increased the value of the goal as well as consumers' subsequent motivation. Conversely, if consumers perceive that the goal has been imposed on them, they experience psychological reactance that is proportional to the amount of effort that they expend in pursuing the goal; thus, they devalue the goal as they invest more effort in its pursuit and show lower subsequent motivation.\n\nThe study by Kruger et al. demonstrated that when evaluating an item, people have a tendency to judge objects that took a longer time to produce to be of higher value. With this knowledge, companies can manipulate the manner in which a product is viewed to make their products appear desirable to consumers. Because effort is usually required to get the best outcomes, people looking for the best outcomes presume effort must imply the best possible outcome. Briñol, Petty, and Tormala (2006) suggested that the impact of effort on evaluation depends on the meaning people are overtly directed to assign to effort. For example, if people are told that unintelligent people like ease, outcomes associated with ease are judged less favorably.\n\nThe principles of the effort heuristic display a striking resemblance to the labor theory of value. This theory is a major pillar of traditional Marxian economics. Its basic claim is simple: the value of a commodity can be objectively measured by the average number of labor hours required to produce that commodity. Marx described labor power as a worker's capacity to produce goods and services. He explained that the long-run wage workers receive will depend on the number of labor hours it takes to produce a person who is fit for work.\n\n"}
{"id": "33557313", "url": "https://en.wikipedia.org/wiki?curid=33557313", "title": "European Journal of Cultural Studies", "text": "European Journal of Cultural Studies\n\nThe European Journal of Cultural Studies is a peer-reviewed academic journal that covers the field of cultural studies in areas such as migration, post-colonial criticism and consumer cultures. The journal's editors-in-chief are Pertti Alasuutari (University of Tampere), Jon Cruz (University of California), Ann Gray (University of Lincoln), and Joke Hermes (University of Amsterdam). It was established in 1998 and is currently published by SAGE Publications.\n\nThe \"European Journal of Cultural Studies\" is abstracted and indexed in Scopus and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2016 impact factor is 1.244, ranking it 5th out of 39 journals in the category \"Cultural Studies\".\n"}
{"id": "57707115", "url": "https://en.wikipedia.org/wiki?curid=57707115", "title": "Explorer 38", "text": "Explorer 38\n\nExplorer 38 (also called as Radio Astronomy Explorer A, RAE-A and RAE-1) was the first satellite to study radioastronomy. Explorer 38 was launched as part of the Explorers program, being the first of the 2 satellites RAE. Explorer 38 was launched on 4 July 1968 from Vandenberg Air Force Base, California, United States, with an Delta J rocket.\n\nExplorer 38 measured the intensity of celestial radio sources, particularly the Sun, as a function of time, direction and frequency (0.2 MHz to 20 MHz). The spacecraft was gravity gradient oriented. The spacecraft weight was , and average power consumption was 25 W. It carried 2 long V-antennas, one facing toward the Earth and one facing away from the earth. A long dipole antenna was oriented tangentially with respect to the earth's surface.\n\nThe spacecraft was also equipped with one 136 MHz telemetry turnstile. The onboard experiments consisted of four step-frequency Ryle-Vonberg radiometers operating from 0.45 MHz to 9.18 MHz, two multichannel total power radiometers operating from 0.2 MHz to 5.4 MHz, one step frequency V-antenna impedance probe operating from 0.24 MHz to 7.86 MHz, and one dipole antenna capacitance probe operating from 0.25 MHz to 2.2 MHz. Explorer 38 was designed for a 1 year minimum operating lifetime.\n\nThe spaecraft tape recorder performance began to deteriorate after 2 months in orbit. In spite of several cases of instrument malfunction, good data were obtained on all three antenna systems. The small satellite observed for months the \"radio sky\" in frequencies between 0.2 MHz and 9.2 MHz, but it was subjected to the continuous radio interference coming from our planet, both natural (aurorae, thunderstorms) and artificial.\n\nExplorer 38 has 4 antennas deployed in orbit:\n\n\nThe scientific experiments are:\n\n\nThe following results are reported in 1971:\n\n\n\n"}
{"id": "2829558", "url": "https://en.wikipedia.org/wiki?curid=2829558", "title": "Female hysteria", "text": "Female hysteria\n\nFemale hysteria was once a common medical diagnosis for women. It is no longer recognized by medical authorities as a medical disorder, but still has lasting social implications. Its diagnosis and treatment were routine for hundreds of years in Western Europe. In Western medicine hysteria was considered both common and chronic among women. The American Psychiatric Association dropped the term \"hysteria\" in 1952. Even though it was categorized as a disease, hysteria's symptoms were synonymous with normal functioning female sexuality. Women considered to have it exhibited a wide array of symptoms, including anxiety, shortness of breath, fainting, nervousness, sexual desire, insomnia, fluid retention, heaviness in the abdomen, irritability, loss of appetite for food or sex, (paradoxically) sexually forward behaviour, and a \"tendency to cause trouble\".\nIn extreme cases, the woman may have been forced to enter an insane asylum or to have undergone surgical hysterectomy.\n\nThe history of hysteria can be traced to ancient times. Dating back to 1900 BC in Ancient Egypt, the first descriptions of hysteria within the female body were found recorded on the Kahun Papyri. In this culture, the womb was thought capable of affecting much of the rest of the body, but \"there is no warrant for the fanciful view that the ancient Egyptians believed that a variety of bodily complaints were due to an animate, wandering womb\". Prolapse was also known.\n\nIn ancient Greece, wandering womb was described in the gynecological treatise of the Hippocratic Corpus, \"Diseases of Women\". which dates back to the 5th and 4th centuries BC. Plato's dialogue \"Timaeus\" compares a woman's uterus to a living creature that wanders throughout a woman's body, \"blocking passages, obstructing breathing, and causing disease\". Aretaeus of Cappadocia described the uterus as \"an animal within an animal\" (less emotively, \"a living thing inside a living thing\"), which causes symptoms by wandering around a woman's body putting pressure on other organs. The standard cure for this \"hysterical suffocation\" was scent therapy, in which good smells were placed under a woman's genitals and bad odors at the nose, while sneezing could be also induced to drive the uterus back to its correct place. The concept of a pathological \"wandering womb\" was later viewed as the source of the term \"hysteria\", which stems from the Greek cognate of uterus, ὑστέρα (\"hystera\").\n\nWhile in the Hippocratic texts a wide range of women were susceptible - including in particular the childless - Galen in the 2nd century omitted the childless and saw the most vulnerable group as \"widows, and particularly those who previously menstruated regularly, had been pregnant and were eager to have intercourse, but were now deprived of all this\" (\"On the Affected Parts\", 6.5). He also denied that the womb could \"move from one place to another like a wandering animal\". His treatments included scent therapy and sexual intercourse, but also rubbing in ointments to the external genitalia; this was to be performed by midwives, not physicians. While most Hippocratic writers saw the retention of menstrual blood in the womb as a key problem, for Galen even more serious was the retention of \"female seed\".\n\nAnother cause was thought to be the retention of a supposed female semen, thought to have mingled with male semen during intercourse. The female semen was believed to have been stored in the womb. Hysteria was referred to as \"the widow's disease\", because the female semen was believed to turn venomous if not released through regular climax or intercourse. If the patient was married, this could be completed by intercourse with their spouse. Other than participating in sexual intercourse, it was thought that women could position the uterus back into place with fumigation of both the face and genitals. Fumigating the body with special fragrances would supposedly place the uterus into its natural spot in the female body.\n\nThrough the Middle Ages another cause of dramatic symptoms could be found: demonic possession. It was thought that demoniacal forces were attracted to those who were prone to melancholy, particularly single women and the elderly. When a patient could not be diagnosed, or cured of a disease, it was thought that the symptoms of what would now be diagnosed as mental illness, were actually those of someone possessed by the devil. After the 17th century, the correlation of demonic possession and hysteria were gradually discarded and instead was described as behavioral deviance, a medical issue.\n\nIn the 16th and 17th centuries, hysteria was still believed to be due to retention of humours or fluids in the uterus, sexual deprivation, or by the tendency of the uterus to wander around the female body causing irritability and suffocation. Self-treatment such as masturbation, was not recommended and also considered taboo. Marriage, and regular sexual encounters with her husband, was still the most highly recommended long-term course of treatment for a woman suffering from hysteria. It was thought to purge the uterus of any built up fluid, and semen was thought to have healing properties, ‘In this model ejaculation outside the vagina was conducive to uterine disease, since the female genitalia did not receive the health benefits of male emission. Some physicians regarded all contraceptive practices as injurious to women for this reason’. Giovanni Matteo Ferrari da Gradi cited marriage and childbearing as a cure for the disease. If pleasure was obtained from them then hysteria could be cured. If a woman was unmarried, or widowed, manual stimulation by a midwife involving certain oils and scents was recommended to purge the uterus of any fluid retention. Lack of marriage was also thought to be the cause of most melancholy in single women, such as nuns or widows. Studies of the causes and effects of hysteria were continued in the 16th and 17th century by medical professionals such as Ambroise Pare, Thomas Sydenham, and Abraham Zacuto who published their findings furthering medical knowledge of the disease, and informing treatment. Physician Abraham Zacuto writes in his \"Praxis Medica Admiranda\" from 1637,\n\nThere was continued debate about whether it was morally acceptable for a physician to remove excess female seed through genital manipulation of the female patient; Pieter van Foreest (Forestus) and Giovanni Matteo da Grado (Gradus) insisted on using midwives as intermediaries, and regarded the treatment as the last resort.\n\nIn the 18th century, hysteria slowly became associated with mechanisms in the brain rather than the uterus. French physician Philippe Pinel freed hysteria patients detained in Paris’ Salpêtrière sanatorium on the basis that kindness and sensitivity are needed to formulate good care.\n\nJean-Martin Charcot argued that hysteria derives from a neurological disorder and showed hysteria is more common among men than women. Charcot's theories of hysteria being a physical affliction of the mind and not of the body led to a more scientific and analytical approach to the disease in the 19th century. He dispelled the beliefs that hysteria had anything to do with the supernatural and attempted to define it medically. Charcot's use of photography, and the resulting concretization of women's expressions of health and distress, continue to influence women's experiences of seeking healthcare. Though older ideas persisted during this era, over time female hysteria began to be thought of less as a physical ailment and more of a psychological one.\n\nGeorge Beard, a physician who cataloged an incomplete list including 75 pages of possible symptoms of hysteria, claimed that almost any ailment could fit the diagnosis. Physicians thought that the stress associated with the typical female life at the time caused civilized women to be both more susceptible to nervous disorders and to develop faulty reproductive tracts. One American physician expressed pleasure in the fact that the country was \"catching up\" to Europe in the prevalence of hysteria.\n\nAccording to Pierre Roussel and Jean-Jacques Rousseau, femininity is a natural and essential desire for women, ‘Femininity is for both authors an essential nature, with defined functions, and the disease is explained by the non-fulfillment of natural desire.’ It is during this era of industrial revolution and the major development of cities and modern life, that this natural tendency is thought to be disrupted causing lethargy or melancholy leading to hysteria. This melancholy or lethargy is retrospectively thought to have been caused and aggravated by the restrictive views on female sexuality at the time, which held masturbation as something unhealthy and unchaste. This led to a surge in female patients for medical practitioners who were looking for the massage cure to their hysteria. The rate of hysteria was so high in the socially restrictive industrial era that women were prone to carrying smelling salts about their person in case they swooned, reminiscent of Hippocrates’ theory of smells coercing the uterus back into place. For doctors manual massage treatment was becoming tiring, laborious and time-consuming and they were looking for a way to increase productivity.\n\nRachel Maines hypothesized that doctors from the classical era up until the early 20th century commonly treated hysteria by masturbating female patients to orgasm (termed \"hysterical paroxysm\"), and that the inconvenience of this may have driven the early development of and the market for the vibrator. Although Maines's theory that hysteria was treated by masturbating female patients to orgasm is widely repeated in the literature on female anatomy and sexuality, some historians dispute Maines's claims about the prevalence of this treatment for hysteria and about its relevance to the invention of the vibrator, describing them as a distortion of the evidence or that it was only relevant to an extremely narrow group. Maines has said that her theory should be treated as a hypothesis rather than a fact.\n\nFrederick Hollick was a firm believer that a main cause of hysteria was licentiousness present in women.\n\nDuring the early 20th century, the number of women diagnosed with female hysteria sharply declined. This decline has been attributed to many factors. Some medical authors claim that the decline was due to gaining a greater understanding of the psychology behind conversion disorders such as hysteria.\n\nWith so many possible symptoms, historically hysteria was considered a catchall diagnosis where any unidentifiable ailment could be assigned. As diagnostic techniques improved, the number of ambiguous cases that might have been attributed to hysteria declined. For instance, before the introduction of electroencephalography, epilepsy was frequently confused with hysteria. \n\nSigmund Freud claimed that hysteria was not anything physical at all but an emotional, internal affliction that could affect both males and females, which was caused by previous trauma that led to the afflicted being unable to enjoy sex in the normal way. This would later lead to Freud's development of the Oedipus Complex, which connotes femininity as a failure, or lack of masculinity. Though these earlier studies had shown that men were also prone to suffer from hysteria, including Freud himself, over time, the condition was related mainly to issues of femininity as the continued study of hysteria took place only in women. Many cases that had previously been labeled hysteria were reclassified by Freud as anxiety neuroses. Sigmund Freud was fascinated by cases of hysteria. He thought that hysteria may have been related to the unconscious mind and separate from the conscious mind or the ego. He was convinced that deep conflicts in the mind, some concerning instinctual drives for sex and aggression, were driving the behavior of those with hysteria. Freud developed psychoanalysis in order to help patients that had been diagnosed with hysteria reduce internal conflicts causing physical and emotional suffering. While hysteria was reframed with reference to new laws and was new in principle, its recommended treatment in psychoanalysis would remain what Bernheimer observes it had been for centuries: marrying and having babies and in this way regaining the \"lost\" phallus.\n\nNew theories relating to hysteria came from pure speculation; doctors and physicians could not connect symptoms to the disorder, causing it to decline rapidly as a diagnosis.\n\nToday, female hysteria is no longer a recognized illness, but different manifestations of hysteria are recognized in other conditions such as schizophrenia, borderline personality disorder, conversion disorder, and anxiety attacks.\n\nHysteria was often used as a political tool in the media to impede women's rights movements and invalidate their arguments and desire for equal rights and a larger role in society.\n\nThe most vehement negative statements associating feminism with hysteria came during the militant suffrage campaign. \nIn the 1980s, feminists began to reclaim hysteria, using it as a symbol of the systematic oppression of women and reclaiming the term for themselves. The idea stemmed from the belief that Hysteria was a kind of pre-feminist rebellion against the oppressive defined social roles placed upon women. Feminist writers such as Catherine Clément and Hélène Cixous write in \"The Newly Born Woman\" from a place of opposition to the theories proposed in psychoanalytical works, pushing against the notion that socially constructed femininities and hysteria are natural to being female. Feminist social historians of both genders argue that hysteria is caused by women’s oppressive social roles rather than by their bodies or psyches, and they have sought its sources in cultural myths of femininity and in male domination.’\n\nThe 19th century definition of femininity has far reaching implications in modern thought as it cements the idea of woman as child bearer, and denotes women who do not conform to the established norms of sexuality and psychoanalysis, as wrong or defective.\n\n\n"}
{"id": "47580563", "url": "https://en.wikipedia.org/wiki?curid=47580563", "title": "Glossary of ant terms", "text": "Glossary of ant terms\n\n\n"}
{"id": "49671486", "url": "https://en.wikipedia.org/wiki?curid=49671486", "title": "Hierarchy of Influences", "text": "Hierarchy of Influences\n\nIn mass communication, the Hierarchy of Influences, formally known as the Hierarchical Influences Model, is an organized theoretical framework introduced by Pamela Shoemaker & Stephen D. Reese. It comprises five levels of influence on media content from the macro to micro levels: social systems, social institutions, media organizations, routine practices, and individuals. This framework was introduced in their book \"Mediating the Message: Theories of Influences on Mass Media Content\".\n\nThe framework was proposed for use in media effects research of media content. Based on media sociology and psychological studies perspective, the framework \"takes into account the multiple forces that simultaneously impinge on media and suggests how influence at one level may interact with that at another.\" Whereas most media effects study treat media content as independent variable to understand how audience use media content and how they are influenced by media content, \"Hierarchy of Influences\" framework treats media content as dependent variable and five levels of influences as potential independent variables.\n\nOverall, the framework provides a way to understand the \"media and their links with culture, other organizations, and institutions.\"\n\nThe macro social systems level is the outer-most ring of the model that represent the influences from social systems as a whole. This level focus on how ideological forces shape and influence media content. For this reason, it is often employed in cross-national comparative media studies.\n\nSocial institutional level describes influences coming from larger trans-organizational media field. How media organizations combine into larger institutions that become part of larger structured relationships that compete or depend on each other as powerful social institutions.\n\nMedia organization level is distinguished from routines as this level describes larger organizational and occupational context such as organizational policy, occupational roles, and how the media enterprise itself is structured.\n\nThe routines level has three sources of routines, which constrain and enable communicators in their work process: audiences, organizations, and suppliers of content. Journalists have developed routines from endless pattern of norms in response to common situations. This level is where Mr. Gates or gatekeeping (communication) theory is also applied in journalists' appearance\n\nThe micro individual level is located at the center of the model. On this level, individual communicator's characteristics, on both personal and professional, influence media content. Individual's innate characteristics such as gender, race, religious and political background influence media content indirectly through shaping personal attitude and values (e.g. ethical values) as well as professional roles and education.\n\nHierarchy of influences model has been employed as theoretical framework to explain different levels of influences on media content. Researchers have studied professionalism, journalistic roles, cross-national comparative journalistic roles, comparative media studies, and understanding news production to name a few of closely studies areas.\n\nThomas Hanitzsch took a similar approach to hierarchy of influences model in examining different levels of influences on journalists' reporting process in his \"Worlds of Journalism\" study. This study conducted interviews in 21 countries focusing on differences in journalism culture, influences and trust in public institutions.\n"}
{"id": "41078437", "url": "https://en.wikipedia.org/wiki?curid=41078437", "title": "Horologium Oscillatorium", "text": "Horologium Oscillatorium\n\nHorologium Oscillatorium: sive de motu pendulorum ad horologia aptato demonstrationes geometricae (Latin for \"The Pendulum Clock: or geometrical demonstrations concerning the motion of pendula as applied to clocks\") is a book published by Christiaan Huygens in 1673; it is his major work on pendulums and horology. This work is regarded as one of the three most important works done on mechanics in the 17th century, the other two being Galileo’s \"Discourses and Mathematical Demonstrations Relating to Two New Sciences\" (1638) and Isaac Newton’s \"Philosophiæ Naturalis Principia Mathematica\" (1687).\n\nThe book is divided into five parts, where the first part contains the descriptions of clock designs, while the rest of the book is devoted to the analysis of pendulum motion and a theory of curves. In the second part of the book, Huygens states three hypotheses on the motion of bodies. They are essentially the law of inertia and the law of composition of \"motion\". He uses these three rules to re-derive Galileo's original study of falling bodies, based on clearer logical framework. He then studies constrained fall, obtaining the solution to the tautochrone problem as given by a cycloid curve and not a circle as Galileo had conceived. In the third part of the book, he outlines a theory of evolutes and rectification of curves. The fourth part of the book is concerned with the study of the center of oscillation. The derivations of propositions in this part is based on a single assumption: that the center of gravity of heavy objects cannot lift itself, which Huygens used as a virtual work principle. In the process, Huygens obtained solutions to dynamical problems such as the period of an oscillating pendulum as well as a compound pendulum, center of oscillation and its interchangeability with the pivot point, and the concept of moment of inertia. The last part of the book gives propositions regarding bodies in uniform circular motion, without proof, and states the laws of centrifugal force for uniform circular motion.\n\nThe book is also known for its strangely worded dedication to Louis XIV. The appearance of the book in 1673 was a political issue, since at that time the Netherlands was at war with France; Huygens was anxious to show his allegiance to his patron, which can be seen in the obsequious dedication to Louis XIV.\n"}
{"id": "1741027", "url": "https://en.wikipedia.org/wiki?curid=1741027", "title": "Ibrāhīm al-Fazārī", "text": "Ibrāhīm al-Fazārī\n\nIbrahim al-Fazari (died 777 CE) was an 8th-century Arab mathematician and astronomer at the Abbasid court of the Caliph Al-Mansur (r. 754–775). He should not to be confused with his son Muḥammad ibn Ibrāhīm al-Fazārī, also an astronomer. He composed various astronomical writings (\"on the astrolabe\", \"on the armillary spheres\", \"on the calendar\").\n\nThe Caliph ordered him and his son to translate the Indian astronomical text, The \"Sindhind\" along with Yaʿqūb ibn Ṭāriq, which was completed in Baghdad about 750 CE, and entitled \"Az-Zīj ‛alā Sinī al-‛Arab\". This translation was possibly the vehicle by means of which the Hindu numerals were transmitted from India to Iran.\n\nAt the end of the eighth century, while at the court of the Abbasid Caliphate, this Arab geographer mentioned Ghana, \"the land of gold.\"\n\n\n\n"}
{"id": "32044437", "url": "https://en.wikipedia.org/wiki?curid=32044437", "title": "Indira Gandhi Rashtriya Manav Sangrahalaya", "text": "Indira Gandhi Rashtriya Manav Sangrahalaya\n\nIndira Gandhi Rashtriya Manav Sangrahalaya (IGRMS) or National Museum of Humankind or Museum of Man is an anthropology museum located in Bhopal, India, to present an integrated story of the evolution of man and culture with special reference to India. It spreads over an area of about 200 acres on the Shyamla Hills in the city. This museum depicts the story of mankind in time and space. Located on Bhopal's upper lake, 'Rashtriya Manav Sangrahalaya' can be accessed either from Lake View Road or from another road near Demonstration School. IGRMS has a few permanent exhibitions, broadly categorized as a) Open-exhibitions, b) Indoor galleries (Veethi-Sankul and Bhopal Gallery) and c) Periodical/ Temporary exhibitions. It also has other exhibitions categorized as 1. Online exhibitions, 2. Travelling exhibitions, 3. Special exhibitions and 4. Ongoing exhibitions.\n\nThe following open-air exhibitions partially developed and opened for the public: Tribal Habitat. Coastal Village, Desert Village, Himalayan Village, Mythological Trail, Traditional Technology park.\n\nThe museum also has a regional centre for the South India region at Mysore in Karnataka.\n\n"}
{"id": "1799683", "url": "https://en.wikipedia.org/wiki?curid=1799683", "title": "Informetrics", "text": "Informetrics\n\nInformetrics is the study of quantitative aspects of information. This includes the production, dissemination, and use of all forms of information, regardless of its form or origin. Informetrics encompasses the following fields:\n\n\nThe term informetrics (French: \"Informetrie\") was coined by Nacke in 1979.\n\nIn the western world, 20th century's Informetrics is mostly based on Lotka's law, named after Alfred J. Lotka, Zipf's law, named after George Kingsley Zipf, Bradford's law named after Samuel C. Bradford and on the work of Derek J. de Solla Price, Gerard Salton, Leo Egghe, Ronald Rousseau, Tibor Braun, Olle Persson, Peter Ingwersen, Manfred Bonitz, and Eugene Garfield.\n\nQuantitative analysis of bibliographic data was pioneered by Robert K. Merton in an article called \"Science, Technology, and Society in Seventeenth Century England\" and originally published by Merton in 1938.\n\n\n"}
{"id": "741104", "url": "https://en.wikipedia.org/wiki?curid=741104", "title": "Intelligent control", "text": "Intelligent control\n\nIntelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, evolutionary computation and genetic algorithms.\n\nIntelligent control can be divided into the following major sub-domains:\n\nNew control techniques are created continuously as new models of intelligent behavior are created and computational methods developed to support them.\n\nNeural networks have been used to solve problems in almost all spheres of science and technology. Neural network control basically involves two steps:\n\n\nIt has been shown that a feedforward network with nonlinear, continuous and differentiable activation functions have universal approximation capability. Recurrent networks have also been used for system identification. Given, a set of input-output data pairs, system identification aims to form a mapping among these data pairs. Such a network is supposed to capture the dynamics of a system.\n\nBayesian probability has produced a number of algorithms that are in common use in many advanced control systems, serving as state space estimators of some variables that are used in the controller.\n\nThe Kalman filter and the Particle filter are two examples of popular Bayesian control components. The Bayesian approach to controller design often requires an important effort in deriving the so-called system model and measurement model, which are the mathematical relationships linking the state variables to the sensor measurements available in the controlled system. In this respect, it is very closely linked to the\nsystem-theoretic approach to control design.\n\n\n\n"}
{"id": "39354205", "url": "https://en.wikipedia.org/wiki?curid=39354205", "title": "Irresistible Force (Met the Immovable Object)", "text": "Irresistible Force (Met the Immovable Object)\n\n\"Irresistible Force\" is the second single from American alternative rock band Jane's Addiction's fourth studio album, The Great Escape Artist. The song was released on August 3, 2011.\n\nThe song features TV on the Radio member and multi-instrumentalist Dave Sitek on guitar and bass guitar. It is less rock-laden than the previous Jane's Addiction single, \"End to the Lies\", and focuses more on synth-aided textures and atmospheric soundscapes. The song starts with a spoken word intro and continues on verse-chorus form, which is accompanied by a guitar solo by Dave Navarro. The verses, which were built around \"droning funk\" and \"the rubbery basslines\" were also noted.\n\nLyrically, the song contains references to the irresistible force paradox. In an interview with CNN, the lyricist Perry Farrell indicated that \"the song chronicles how the universe was created\", while the drummer Stephen Perkins described it as \"what pulls him, Dave and Perry together\". Will Hermes of Rolling Stone also commented on the song's lyrics, describing it as \"Perry Farrell sounding like Carl Sagan in an episode of , musing ominously about the stars and some sort of big bang.\"\n\nThe critical reception for the song was positive.\n\nA music video for the song was produced.\n\n"}
{"id": "3673376", "url": "https://en.wikipedia.org/wiki?curid=3673376", "title": "List of Python software", "text": "List of Python software\n\nThe Python programming language is actively used by many people, both in industry and academia for a wide variety of purposes.\n\n\n\n\n\n\n\n\n\n\n\n\nPython is, or can be used as the scripting language in these software products:\n\n\nImplementations of Python include:\n\nHistoric Python implementations include:\n\n"}
{"id": "423750", "url": "https://en.wikipedia.org/wiki?curid=423750", "title": "List of Space Shuttle missions", "text": "List of Space Shuttle missions\n\nThe Space Shuttle was a partially reusable low Earth orbital spacecraft system operated by the U.S. National Aeronautics and Space Administration (NASA). Its official program name was Space Transportation System (STS), taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development. Operational missions launched numerous satellites, conducted science experiments in orbit, and participated in construction and servicing of the International Space Station (ISS). The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982.\n\nFrom 1981 to 2011 a total of 135 missions were flown, all launched from Kennedy Space Center (KSC) in Florida. During that time period the fleet logged 1,322 days, 19 hours, 21 minutes and 23 seconds of flight time. The longest orbital flight of the shuttle was STS-80 at 17 days 15 hours, while the shortest flight was STS-51-L at one minute 13 seconds when the Space Shuttle \"Challenger\" broke apart during launch. The shuttles docked with Russian space station \"Mir\" nine times and visited the ISS thirty-seven times. The highest altitude (apogee) achieved by the shuttle was 350 miles when servicing the Hubble Space Telescope. The program flew a total of 355 people representing 16 countries. The Kennedy Space Center served as the landing site for 78 missions, while 54 missions landed at Edwards Air Force Base in California and 1 mission landed at White Sands, New Mexico.\n\nThe first orbiter, \"Enterprise\", was built solely for atmospheric flight tests and had no orbital capability. Four fully operational orbiters were initially built: \"Columbia\", \"Challenger\", \"Discovery\", and \"Atlantis\". \"Challenger\" and \"Columbia\" were destroyed in mission accidents in 1986 and 2003 respectively, killing a total of fourteen astronauts. A fifth operational orbiter, \"Endeavour\", was built in 1991 to replace \"Challenger\". The Space Shuttle was retired from service upon the conclusion of STS-135 by \"Atlantis\" on 21 July 2011.\n\nThe U.S. Space Shuttle program was officially referred to as the Space Transportation System (STS). Specific shuttle missions were therefore designated with the prefix \"STS\". Initially, the launches were given sequential numbers indicating order of launch, such as STS-7. Subsequent to the Apollo 13 mishap, due to NASA Administrator James M. Beggs's triskaidekaphobia and consequent unwillingness to number a forthcoming flight as STS-13, beginning in 1984, each mission was assigned a code, such as STS-41-B, with the first digit indicating the federal fiscal year offset into the program (so 41-B was scheduled for FY 1984, 51-L originally for FY 1985 and the third flight in FY 1995 would have been named 151-C), the second digit indicating the launch site (1 was Kennedy Space Center and 2 was Space Launch Complex (SLC) 6 at Vandenberg Air Force Base, although Vandenberg was never used), and the letter indicating scheduling sequence. These codes were assigned when the launches were initially scheduled and were not changed as missions were delayed or rescheduled. The codes were adopted from STS-41-B through STS-51-L (although the highest code used was actually STS-61-C), and the sequential numbers were used internally at NASA on all processing paperwork.\n\nAfter the \"Challenger\" disaster, NASA returned to using a sequential numbering system, with the number counting from the beginning of the STS program. Unlike the initial system, however, the numbers were assigned based on the initial mission schedule, and did not always reflect actual launch order. This numbering scheme started at 26, with the first flight as STS-26R—the R suffix stood for \"reflight\" to disambiguate from prior missions. The suffix was used for two years through STS-33R, then the R was dropped. As a result of the changes in systems, flights under different numbering systems could have the same number with one having a letter appended, e.g. flight STS-51 (a mission carried out by \"Discovery\" in 1993) was many years after STS-51-A (\"Discovery's\" second flight in 1984).\n\nThe Approach and Landing Test Programme encompassed 16 separate tests of \"Enterprise\", covering taxi tests, unmanned and manned flights on the Shuttle Carrier Aircraft, and finally the free flight tests. The following list includes the free-flight tests, durations listed count only the orbiter free-flight time. The list does not include total time aloft along with airborne time atop of the Boeing 747 Shuttle Carrier Aircraft (SCA).\n\nOne initial emergency flight abort (RTLS) sub-orbital test mission was canceled due to high risk. Many other planned missions were canceled due to the late development of the shuttle, and the \"Challenger\" and \"Columbia\" disasters.\n\nFour missions were cut short by a day or more while in orbit: STS-2 (equipment failure), STS-35 (weather), STS-44 (equipment failure), and STS-83 (equipment failure, relaunched as STS-94).\n\nSTS-300 was the designation for the Space Shuttle Launch on Need (LON) missions to be launched on short notice for STS-114 and STS-121, in the event that the shuttle became disabled or damaged and could not safely return to Earth. The rescue flight for STS-115, if needed, would have been STS-301. After STS-115, the rescue mission designations were based on the corresponding regular mission that would be replaced should the rescue mission be needed. For example, the STS-116 rescue mission was branded STS-317, because the normal mission scheduled after STS-116 was STS-117. Should the rescue mission have been needed, the crew and vehicle for STS-117 would assume the rescue mission profile and become STS-317. All potential rescue missions were to be launched with a crew of four, and would return with ten or eleven crew members, depending on the number of crew launched on the rescued shuttle. Missions were expected to last approximately eleven days. None of the planned contingency missions was ever flown.\n\nNo contingency mission was planned for STS-135, the final shuttle mission. Instead, NASA planned to effect any required rescues one-by-one, using Russian Soyuz spacecraft.\n\n\n\n"}
{"id": "19874811", "url": "https://en.wikipedia.org/wiki?curid=19874811", "title": "List of Space Shuttle rollbacks", "text": "List of Space Shuttle rollbacks\n\nThis is a list of Space Shuttle rollbacks. \"Rollback\" is the term NASA uses when the Space Shuttle was rolled back from the launch pad atop the Mobile Launcher Platform and Crawler-transporter to the Vehicle Assembly Building (VAB). A variety of factors could require a rollback, from severe weather to the need for repairs that could not be performed at the launch pad. Shuttle rollbacks are listed in chronological order:\n\n"}
{"id": "14485626", "url": "https://en.wikipedia.org/wiki?curid=14485626", "title": "List of members of the National Academy of Sciences (Chemistry)", "text": "List of members of the National Academy of Sciences (Chemistry)\n\nThe designation (d) after the name means the member is deceased. \n"}
{"id": "57880587", "url": "https://en.wikipedia.org/wiki?curid=57880587", "title": "List of sequenced algae genomes", "text": "List of sequenced algae genomes\n\nThis list of sequenced algae genomes contains algae species known to have publicly available complete genome sequences that have been assembled, annotated and published. Unassembled genomes are not included, nor are organelle only sequences. For plant genomes see the list of sequenced plant genomes. For all kingdoms, see the list of sequenced genomes.\n\nSee also List of sequenced protist genomes.\n"}
{"id": "15134308", "url": "https://en.wikipedia.org/wiki?curid=15134308", "title": "List of virtual communities", "text": "List of virtual communities\n\nThis is a list of Wikipedia articles about virtual communities.\n\n\n\n\n\n\n\n\n"}
{"id": "7120033", "url": "https://en.wikipedia.org/wiki?curid=7120033", "title": "List of volcanoes in Iceland", "text": "List of volcanoes in Iceland\n\nThis list of volcanoes in Iceland includes 130 active and extinct volcanic mountains, of which 18 have erupted since human settlement of Iceland began circa 900 CE.\n\nIceland has four volcanic zones: Reykjanes (Reykjanes Ridge, the Mid-Atlantic Ridge South of Iceland), West and North Volcanic Zones (RVZ, WVZ, NVZ) and the East Volcanic Zone (EVZ), (Westman Islands). The Mid-Iceland Belt (MIB) connects them across central Iceland. There are two intraplate belts too (Öræfajökull (ÖVB) and Snæfellsnes (SVB)). North of Iceland the Mid-Atlantic Ridge is called Kolbeinsey Ridge.\n\nThe central volcanoes (Iceland's East Volcanic Zone (EVZ)), Vonarskarð and Hágöngur belong to the same volcanic system, this also applies to Bárðarbunga and Hamarinn, and Grímsvötn and Þórðarhyrna. It is proposed that the line Grímsvötn volcano, Mid-Iceland Belt (MIB) to Snæfellsnes volcanic belt shows the movement of the North American Plate over the Iceland hotspot. \n\n\n"}
{"id": "5542625", "url": "https://en.wikipedia.org/wiki?curid=5542625", "title": "Louis Gabriel d'Antessanty", "text": "Louis Gabriel d'Antessanty\n\nLouis Gabriel d’Antessanty (or Abbé G.) (October 26, 1834, in Troyes – January 6, 1922, in Troyes) was a French entomologist.\n\nHis principal publications are:\n\n\nAnd on general natural history:\n\n\nThe types of the new species of Hemiptera described by d’Antessanty are listed in Royer, M. 1922. Les types de la collection d'Hémipt Pres de l'abbé G. d'Antessanty. \"Bulletin de la Société Entomologique de France\" 1922:268-269.\n"}
{"id": "57336023", "url": "https://en.wikipedia.org/wiki?curid=57336023", "title": "Luigi Torchi (inventor)", "text": "Luigi Torchi (inventor)\n\nLuigi Torchi invented the first direct multiplication machine in 1834. This was also the second key-driven machine in the world, following that of James White (1822).\n\nVery little is known about the inventor and the machine. We only know that he was a carpenter; his machine was awarded of a gold metal from the Imperial-regio istituto lombardo di scienze, lettere e arti in Milan in 1834. A document of such occasion provides the known details of the machine, where a second document shows a drawing of the machine itself. However, no detailed documents about how it worked are known to exist.\n\nThe machine was exhibited in Brera between 1834 and 1837; it was later found by Giovanni Schiaparelli in bad conditions. After that, are not known further information about the machine. \n\n\n"}
{"id": "187360", "url": "https://en.wikipedia.org/wiki?curid=187360", "title": "Magnetic susceptibility", "text": "Magnetic susceptibility\n\nIn electromagnetism, the magnetic susceptibility (Latin: , \"receptive\"; denoted ) is one measure of the magnetic properties of a material. The susceptibility indicates whether a material is attracted into or repelled out of a magnetic field, which in turn has implications for practical applications. Quantitative measures of the magnetic susceptibility also provide insights into the structure of materials, providing insight into bonding and energy levels.\n\nIf the magnetic susceptibility is greater than zero, the substance is said to be \"paramagnetic\"; the magnetization of the substance is higher than that of empty space. If the magnetic susceptibility is less than zero, the substance is \"diamagnetic\"; it tends to exclude a magnetic field from its interior.\n\nMathematically it is the ratio of magnetization (magnetic moment per unit volume) to the applied magnetizing field intensity .\n\nMagnetic susceptibility is a dimensionless proportionality constant that indicates the degree of magnetization of a material in response to an applied magnetic field. A related term is magnetizability, the proportion between magnetic moment and magnetic flux density. A closely related parameter is the permeability, which expresses the total magnetization of material and volume.\n\nThe \"volume magnetic susceptibility\", represented by the symbol (often simply , sometimes  – magnetic, to distinguish from the electric susceptibility), is defined in the International System of Units — in other systems there may be additional constants — by the following relationship:\n\nHere\n\nUsing SI units, the magnetic induction is related to by the relationship\n\nwhere is the vacuum permeability (see table of physical constants), and is the relative permeability of the material. Thus the \"volume magnetic susceptibility\" and the magnetic permeability are related by the following formula:\n\nSometimes an auxiliary quantity called \"intensity of magnetization\" (also referred to as \"magnetic polarisation\" ) and measured in teslas, is defined as\n\nThis allows an alternative description of all magnetization phenomena in terms of the quantities and , as opposed to the commonly used and .\n\nThere are two other measures of susceptibility, the \"mass magnetic susceptibility\" ( or , sometimes ), measured in m/kg (SI) and the \"molar magnetic susceptibility\" () measured in m/mol that are defined below, where is the density in kg/m and is molar mass in kg/mol:\n\nNote that the definitions above are according to SI conventions. However, many tables of magnetic susceptibility give cgs values (more specifically emu-cgs, short for electromagnetic units, or Gaussian-cgs; both are the same in this context). These units rely on a different definition of the permeability of free space:\n\nThe dimensionless cgs value of volume susceptibility is multiplied by 4 to give the dimensionless SI volume susceptibility value:\n\nFor example, the cgs volume magnetic susceptibility of water at 20 °C is , which is using the SI convention.\n\nIn physics it is common to see cgs mass susceptibility given in cm/g or emu/g·Oe, so to convert to SI volume susceptibility we use the conversion \n\nwhere is the density given in g/cm, or\n\nThe molar susceptibility is measured cm/mol or emu/mol·Oe in cgs and is calculated using the molar mass in g/mol.\n\nIf is positive, a material can be paramagnetic. In this case, the magnetic field in the material is strengthened by the induced magnetization. Alternatively, if is negative, the material is diamagnetic. In this case, the magnetic field in the material is weakened by the induced magnetization. Generally, nonmagnetic materials are said to be para- or diamagnetic because they do not possess permanent magnetization without external magnetic field. Ferromagnetic, ferrimagnetic, or antiferromagnetic materials possess permanent magnetization even without external magnetic field and do not have a well defined zero-field susceptibility.\n\nVolume magnetic susceptibility is measured by the force change felt upon a substance when a magnetic field gradient is applied. Early measurements are made using the Gouy balance where a sample is hung between the poles of an electromagnet. The change in weight when the electromagnet is turned on is proportional to the susceptibility. Today, high-end measurement systems use a superconductive magnet. An alternative is to measure the force change on a strong compact magnet upon insertion of the sample. This system, widely used today, is called the Evans balance. For liquid samples, the susceptibility can be measured from the dependence of the NMR frequency of the sample on its shape or orientation.\nAnother method using NMR techniques measures the magnetic field distortion around a sample immersed in water inside an MR scanner. This method is highly accurate for diamagnetic materials with susceptibilities similar to water.\n\nThe magnetic susceptibility of most crystals is not a scalar quantity. Magnetic response is dependent upon the orientation of the sample and can occur in directions other than that of the applied field . In these cases, volume susceptibility is defined as a tensor\n\nwhere and refer to the directions (e.g., and in Cartesian coordinates) of the applied field and magnetization, respectively. The tensor is thus rank 2 (second order), dimension (3,3) describing the component of magnetization in the th direction from the external field applied in the th direction.\n\nIn ferromagnetic crystals, the relationship between and is not linear. To accommodate this, a more general definition of \"differential susceptibility\" is used\n\nwhere is a tensor derived from partial derivatives of components of with respect to components of . When the coercivity of the material parallel to an applied field is the smaller of the two, the differential susceptibility is a function of the applied field and self interactions, such as the magnetic anisotropy. When the material is not saturated, the effect will be nonlinear and dependent upon the domain wall configuration of the material.\n\nWhen the magnetic susceptibility is measured in response to an AC magnetic field (i.e. a magnetic field that varies sinusoidally), this is called \"AC susceptibility\". AC susceptibility (and the closely related \"AC permeability\") are complex number quantities, and various phenomena, such as resonance, can be seen in AC susceptibility that cannot in constant-field (DC) susceptibility. In particular, when an AC field is applied perpendicular to the detection direction (called the \"transverse susceptibility\" regardless of the frequency), the effect has a peak at the ferromagnetic resonance frequency of the material with a given static applied field. Currently, this effect is called the \"microwave permeability\" or \"network ferromagnetic resonance\" in the literature. These results are sensitive to the domain wall configuration of the material and eddy currents.\n\nIn terms of ferromagnetic resonance, the effect of an AC-field applied along the direction of the magnetization is called \"parallel pumping\".\n\nThe CRC Handbook of Chemistry and Physics has one of the only published magnetic susceptibility tables. Some of the data (e.g., for aluminium, bismuth, and diamond) is listed as cgs, which has caused confusion to some readers. \"cgs\" is an abbreviation of \"centimeters–grams–seconds\"; it represents the form of the units, but cgs does not specify units. Correct units of magnetic susceptibility in cgs is cm/mol or cm/g. Molar susceptibility and mass susceptibility are both listed in the CRC. Some table have listed magnetic susceptibility of diamagnets as positives. It is important to check the header of the table for the correct units and sign of magnetic susceptibility readings.\n\n"}
{"id": "30102234", "url": "https://en.wikipedia.org/wiki?curid=30102234", "title": "Marine botany", "text": "Marine botany\n\nMarine botany is the study of aquatic plants and algae that live in seawater of the open ocean and the littoral zone, along shorelines of the intertidal zone, and in brackish water of estuaries. \n\nIt is a branch of marine biology and botany.\n\nMarine ecology and marine botany include:\n\n"}
{"id": "22625115", "url": "https://en.wikipedia.org/wiki?curid=22625115", "title": "Mirror theory", "text": "Mirror theory\n\nIn theoretical linguistics, mirror theory refers to a particular approach to the architecture of the language organ developed by Michael Brody, who claims his theory to be purely representational (unlike most of the current generative theories that are either derivational or combining derivation and representation).\n\nThere are several important respects in which mirror theory is different from more traditional theories of phrase structure in generative linguistics such as X-bar theory or bare phrase structure. The first principle, called \"mirror\", states that the syntactic relation 'X complement of Y' is identical to an inverse-order morphological relation 'X specifier of Y'. Thus, the notions of 'syntactic' and 'morphological' specifiers and complements are crucial for the linearisation of syntactic structure and its mapping to the morphological component.\n\nWhen the structure is pronounced, it linearises in the following order: specifiers precede heads, and heads precede their complements. So when a sentence like that in the diagram below is pronounced, 'John' precedes the V-v-T chain, which in turn precedes 'Mary', the latter being the specifier of V. However, English is a VO language, which means that the morphological word 'loves' associated with the V-v-T chain is spelled in v, deriving the correct word order.\n\nAdger, D., Harbour, D., and Watkins, L. \"Mirrors and Microparameters: Phrase Structure beyond Free Word Order\" \n\nBrody, M. \"Mirror Theory\"\n\nSimilar to mirror theory is the Mirror Principle of the Distributed Morphology framework that deals primarily with issues of affix ordering. In the model of Distributed Morphology, the underlying assumption is such that all morphemes in a morphologically complex word are effectually \"leaves\" on a syntactic tree. These syntactic \"leaves\" undergo operations that lead to the build-up of words, meaning that word-formation is subject to the same principles that govern the syntactic build-up of sentences. The mirror principle surmises that a complex word is generated in a derivation with the root of a given word merging lower than all subsequent suffixes that attach to that word. These suffixes immediately c-command the root word and are ordered in accordance with the principle of semantic scope; morphemes that have scope over other parts of a words. To generate the proper linear structure, Left-Head Movement is applied recursively to include the morphemes that c-command a given root.\n\nEmpirical arguments for the Mirror Principle are provided by Baker's (1985) account for the distribution of a number of valence changing operations in Mohawk, showing that the linear order of morphemes is related to the syntactic hierarchy in that morphemes are linearized in a reverse order to how they appear in a syntactic structure. His findings are summarized in \"The Mirror Generalization\", which states: Morphological derivations must directly reflect syntactic derivations (and vice versa).The generalization is observed in Baker's (1988) account for applicatives and passives in Chichewa:\n\nMbidzi zi-na-perek-a mpiringidzo kwa mtsikana\n\nzebra SP-PST-hand-ASP crowbar to girl\n\n\"The zebras handed the crowbar to the girl.\"\n\nMbidzi zi-na-perek-er-a mpiringidzo mtsikana\n\nzebra SP-PST-hand-APPL-ASP crowbar girl\n\n\"The zebras handed the girl the crowbar.\"\n\nMpiringidzo u-na-perek-edw-a kwa mtsikana ndi mbidzi\n\ncrowbar SP-PST-hand-PASS-ASP to girl by zebras\n\n\"The crowbar was handed to the girl by the zebras.\"\n\nMtsikana a-na-perek-er-edw-a mpiringidzo ndi mbidzi\n\ngirl SP-PST-hand-APPL-PASS-ASP crowbar by zebras\n\n\"The girl was handed the crowbar by the zebras.\"\n\n<nowiki>*</nowiki>Mtskikana a-na-perek-edw-er-a mpiringidzo ndi mbidzi\n\n<nowiki>*</nowiki>girl SP-PST-hand-PASS-APPL-ASP crowbar by zebras\n\nThe above examples attest the ordering of the applicative morpheme and the passive morpheme in Chichewa. In this language, applicatives may be passivized, however, passives can not be made into applicatives. Applying the Mirror Principle to this set of data, we assume that in the underlying deep structure of the syntax, all affixes that occur to the right of the lexical root \"perek\" are projected above this root in the syntax. These affixes are then adjoined to the lexical root via head-movement.\n\nNumerous observations in languages are also available, apart from the mirror principle. Yupik, for example, is able to account for affix order appealing entirely to semantic scope. However, semantic scope and the mirror principle are not mutually exclusive ways to account for affix ordering\n\nChallenges to the Mirror Principle have been offered in analyses of the morpheme orders of Navajo and Cupeño by Harley (2009).\n"}
{"id": "13182486", "url": "https://en.wikipedia.org/wiki?curid=13182486", "title": "Molecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules", "text": "Molecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules\n\nMolecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules, by K. P. Huber and Gerhard Herzberg (Van nostrand Reinhold company, New York, 1979, ), is a classic comprehensive multidisciplinary reference text contains a critical compilation of available data for all diatomic molecules and ions known at the time of publication - over 900 diatomic species in all - including electronic energies, vibrational and rotational constants, and observed transitions. Extensive footnotes discuss the reliability of these data and additional detailed informationon potential energy curves, spin-coupling constants, /\\-type doubling, perturbations between electronic states, hyperfine structure, rotational g factors, dipole moments, radiative lifetimes, oscillator strengths, dissociation energies and ionization potentials when available, and other aspects. Herzberg received the 1971 Nobel Prize in Chemistry; both authors are world-renowned highly respected scientists.\n\nPhysics, engineering, mathematics, kinetics, spectroscopy, astronomy, astrophysics, aeronautics, astronautics, radiation, optics, energy, photometry, spectrometry, electromagnetics, oscillators, thermochemistry, thermodynamics, ionization, X-rays, ESR, photoelectrons, electronics, industry, science and technology.\n"}
{"id": "3763072", "url": "https://en.wikipedia.org/wiki?curid=3763072", "title": "Mongoose-V", "text": "Mongoose-V\n\nThe Mongoose-V 32-bit microprocessor for spacecraft onboard computer applications is a radiation-hardened and expanded 10–15 MHz version of the MIPS R3000 CPU. Mongoose-V was developed by Synova, Inc. of Melbourne, Florida, USA, with support from the NASA Goddard Space Flight Center.\n\nThe Mongoose-V processor first flew on NASA's Earth Observing-1 (EO-1) satellite launched in November 2000 where it functioned as the main flight computer. A second Mongoose-V controlled the satellite's solid-state data recorder.\n\nThe Mongoose-V requires 5 volts and is packaged into a 256-pin ceramic quad flatpack (CQFP).\n\nExamples of spacecraft that use the Mongoose-V include:\n\n\n"}
{"id": "15003160", "url": "https://en.wikipedia.org/wiki?curid=15003160", "title": "Nathaniel Wetherell", "text": "Nathaniel Wetherell\n\nDr. Nathaniel Thomas Wetherell MRCS FGS (September 6, 1800 – December 22, 1875) was a British geologist and surgeon. His work involved the collection of various fossils found in England. He was born, lived, and died in Highgate, England.\n\nWetherell discovered a strange mixture of rocks and fossils of northern provenance in Coldfall Wood, Muswell Hill in 1835. This led subsequently to the recognition that glaciation had affected southern England.\n\n\n"}
{"id": "22693121", "url": "https://en.wikipedia.org/wiki?curid=22693121", "title": "Nuclear collision length", "text": "Nuclear collision length\n\nNuclear collision length is the mean free path of a particle before undergoing a nuclear reaction, for a given particle in a given medium. The collision length is smaller than the nuclear interaction length because the latter excludes the elastic and quasi-elastic (diffractive) reactions from its definition.\n\n\n"}
{"id": "48773241", "url": "https://en.wikipedia.org/wiki?curid=48773241", "title": "Organizational metacognition", "text": "Organizational metacognition\n\nOrganizational metacognition is knowing what an organization knows, a concept related to metacognition, organizational learning, the learning organization and sensemaking. It is used to describe how organizations and teams develop an awareness of their own thinking, learning how to learn, where awareness of ignorance can motivate learning.\n\nThe organizational deutero-learning concept identified by Argyris and Schon defines when organizations learn how to carry out single-loop and double-loop learning. It has also been described as learning how to learn through a process of collaborative inquiry and reflection (evaluative inquiry).\n\n\"When an organization engages in deutero-learning its members learn about the previous context for learning. They reflect on and inquire into previous episodes of organizational learning, or failure to learn. They discover what they did that facilitated or inhibited learning, they invent new strategies for learning, they produce these strategies, and they evaluate and generalize what they have produced\" \n\nLearning what facilitates and inhibits learning enables organizations to develop new strategies to develop their knowledge. For example, identification of a gap between perceived performance (such as satisfaction) and actual performance (outcomes) creates an awareness that makes the organization understand that learning needs to occur, driving appropriate changes to the environment and processes.\n\nWijnhoven (2001) grouped four learning prototypes that best meet learning needs, the match between these needs and learning norms dictating an organization's learning capabilities; deutero-learning is the acquisition of these capabilities.\n\nOrganizational metacognition and organizational deutero-learning have both been described as the concept or phenomenon where organizations learn how to learn. Argyris and Schon (1978) place deutero-learning into their cognitive theory of action framework, neglecting aspects of adaptive behaviour and context core to Bateson’s (1972) original definitions. In order to resolve terminological ambiguities, Visser (2007) reviewed and reformulated the concept of deutero-learning as, \"the behavioral adaptation to patterns of conditioning in relationships in organizational contexts, distinguishing it from meta-learning and planned learning\" (pg. 659).\n\nOrganizational metacognition is considered a key norm to the prescriptive concept of the learning organization. Its significance has been recognized by industry, the military and in disaster response.\n\nExamples of poor metacognition (deutero-learning) have been described in knowledge network environments,\n\n\"Knowledge networking is important to most competitive enterprises today. Enterprise knowledge is becoming ever more specialized in nature, so no single person or organization can know everything in detail. Hence addressing complex, multidisciplinary problems requires developing and accessing a network of knowledgeable people and organizations. The problem is, many otherwise knowledgeable people and organizations are not fully aware of their knowledge networks, and even more problematic, they are not aware that they are not aware. This focuses our attention toward organizational metacognition.\" \n"}
{"id": "33636200", "url": "https://en.wikipedia.org/wiki?curid=33636200", "title": "Ottmar Hofmann", "text": "Ottmar Hofmann\n\nOttmar Hofmann (20 September 1835 in Frankfurt am Main – 22 February 1900 in Regensburg) was a German entomologist. \n\nOttmar Hofmann was a physician. As an entomologist, he worked on Microlepidoptera. His collection was sold to Thomas de Grey, 6th Baron Walsingham and is now in the Natural History Museum (London).\n\n\n"}
{"id": "16233284", "url": "https://en.wikipedia.org/wiki?curid=16233284", "title": "Relative income hypothesis", "text": "Relative income hypothesis\n\nDeveloped by James Duesenberry, the relative income hypothesis states that an individual’s attitude to consumption and saving is dictated more by his income in relation to others than by abstract standard of living; the percentage of income consumed by an individual depends on his percentile position within the income distribution.\n\nSecondly, it hypothesizes that the present consumption is not influenced merely by present levels of absolute and relative income, but also by levels of consumption attained in a previous period. It is difficult for a family to reduce a level of consumption once attained. The aggregate ratio of consumption to income is assumed to depend on the level of present income relative to past peak income.\n\nDuesenberry, J. S. \"Income, Saving and the Theory of Consumer Behaviour\". Cambridge: Harvard University Press, 1949.\n"}
{"id": "391327", "url": "https://en.wikipedia.org/wiki?curid=391327", "title": "Saturn I SA-4", "text": "Saturn I SA-4\n\nSA-4 was the fourth launch of a Saturn I launch vehicle and the last of the initial test phase of the first stage. It was part of the Apollo Program.\n\nSA-4 was the last flight to test only the S-I first stage of the Saturn I rocket. As with the first three launches this would be a suborbital flight and would test the structural integrity of the rocket.\n\nThe major addition to this flight was that, in order to test the rocket's ability to deal with an engine failure during the flight, one of the engines would be programmed to shut down about 100 seconds after launch. If all went well the rocket would reroute the fuel for this engine to the other engines and have the rocket burn longer to compensate for the loss of acceleration. This was used successfully on the later Apollo 6 and Apollo 13 flights (both Saturn V rockets).\n\nAlso on this flight, the dummy second stage was outfitted with the aerodynamic design of the real second stage. This included vent ducts, fairings and dummy camera pods. The rocket also flew with antennae designed for the Block II version of the rocket.\n\nAfter the shortest checkout time of any rocket at 54 days, SA-4 went on to experience the longest series of holds of any mission at 120 minutes. The SA-4 launch would be the final single-stage flight.\n\nThe rocket operated perfectly through the first 100 seconds of the flight, when the No. 5 engine shut off as planned. The rocket then continued to operate properly, the propellant system rerouting the fuel to the other engines. The No. 5 engine did not disintegrate because of heat caused by the lack of cooling propellant as some had predicted. This was an important test proving an important feature of the clustered engine design.\n\nThe rocket reached a maximum height of 129 km and a peak velocity of 5906 km per hour. At this stage it also fired retrorockets that would be used on later missions to separate the rocket stages. On SA-4 the stages were not designed to separate but tested the retrorockets to make sure they would fire.\n\n"}
{"id": "937811", "url": "https://en.wikipedia.org/wiki?curid=937811", "title": "Searles Valentine Wood", "text": "Searles Valentine Wood\n\nSearles Valentine Wood (February 14, 1798 – October 26, 1880) was an English palaeontologist.\n\nWood went to sea in 1811 as a midshipman in the British East India Company's service, which he left in 1826. He then settled at Hasketon near Woodbridge, Suffolk.\n\nWood devoted himself to a study of the mollusca of the Newer Tertiary (now Neogene) of Suffolk and Norfolk, and the Older Tertiary (Eocene) of the Hampshire Basin. On the latter subject he published \"A Monograph of the Eocene Bivalves of England\" (1861–1871), issued by the Palaeontographical Society. His chief work was \"A Monograph of the Crag Mollusca\" (1848–1856), published by the same society, for which he was awarded the Wollaston medal in 1860 by the Geological Society of London; a supplement was issued by him in 1872-1874, a second in 1879, and a third (edited by his son) in 1882. He died at Martlesham, near Woodbridge.\n\nHis son, Searles Valentine Wood (1830-1884), was for some years a solicitor at Woodbridge, but gave up the profession and devoted his energies to geology, studying especially the structure of the deposits of the crag and glacial drifts.\n\n"}
{"id": "203510", "url": "https://en.wikipedia.org/wiki?curid=203510", "title": "Social constructionism", "text": "Social constructionism\n\nSocial constructionism is a theory of knowledge in sociology and communication theory that examines the development of jointly constructed understandings of the world that form the basis for shared assumptions about reality. The theory centers on the notion that meanings are developed in coordination with others rather than separately within each individual.\n\nSocial constructionism questions what is defined by humans and society to be reality. Therefore, social constructs can be different based on the society and the events surrounding the time period in which they exist. An example of a social construct is money or the concept of currency, as people in society have agreed to give it importance/ value. Another example of a social construction is the concept of self/ self-identity. Charles Cooley stated based on his Looking-Glass-Self theory: \"I am not who you think I am; I am not who I think I am; I am who I think you think I am.\" This demonstrates how people in society construct ideas or concepts that may not exist without the existence of people or language to validate those concepts.\n\nThere are weak and strong social constructs. Weak social constructs rely on brute facts (which are fundamental facts that are difficult to explain or understand, such as quarks) or institutional facts (which are formed from social conventions). Strong social constructs rely on the human perspective and knowledge that doesn't just exist, but is rather constructed by society.\n\nA social construct or construction concerns the meaning, notion, or connotation placed on an object or event by a society, and adopted by the inhabitants of that society with respect to how they view or deal with the object or event. In that respect, a social construct as an idea would be widely accepted as natural by the society.\n\nA major focus of social constructionism is to uncover the ways in which individuals and groups participate in the construction of their perceived social reality. It involves looking at the ways social phenomena are developed, institutionalized, known, and made into tradition by humans.\n\nIn terms of background, social constructionism is rooted in \"symbolic interactionism\" and \"phenomenology.\" With Berger and Luckmann's \"The Social Construction of Reality\" published in 1966, this concept found its hold. More than four decades later, a sizable number of theory and research pledged to the basic tenet that people \"make their social and cultural worlds at the same time these worlds make them.\" It is a viewpoint that uproots social processes \"simultaneously playful and serious, by which reality is both revealed and concealed, created and destroyed by our activities.\" It provides a substitute to the \"Western intellectual tradition\" where the researcher \"earnestly seeks certainty in a representation of reality by means of propositions.\"\n\nIn social constructionist terms, \"taken-for-granted realities\" are cultivated from \"interactions between and among social agents;\" furthermore, reality is not some objective truth \"waiting to be uncovered through positivist scientific inquiry.\" Rather, there can be \"multiple realities that compete for truth and legitimacy.\" Social constructionism understands the \"fundamental role of language and communication\" and this understanding has \"contributed to the linguistic turn\" and more recently the \"turn to discourse theory.\" The majority of social constructionists abide by the belief that \"language does not mirror reality; rather, it constitutes [creates] it.\"\n\nA broad definition of social constructionism has its supporters and critics in the organizational sciences. A constructionist approach to various organizational and managerial phenomena appear to be more commonplace and on the rise.\n\nAndy Lock and Tomj Strong trace some of the fundamental tenets of social constructionism back to the work of the 18th century Italian political philosopher, rhetorician, historian, and jurist Giambattista Vico.\n\nBerger and Luckmann give credit to Max Scheler as a large influence as he created the idea of Sociology of knowledge which influenced social construction theory.\n\nAccording to Lock and Strong, other influential thinkers whose work has affected the development of social constructionism are: Edmund Husserl, Alfred Schutz, Maurice Merleau-Ponty, Martin Heidegger, Hans-Georg Gadamer, Paul Ricoeur, Jürgen Habermas, Emmanuel Levinas, Mikhail Bakhtin, Valentin Volosinov, Lev Vygotsky, George Herbert Mead, Ludwig Wittgenstein, Gregory Bateson, Harold Garfinkel, Erving Goffman, Anthony Giddens, Michel Foucault, Ken Gergen, Mary Gergen, Rom Harre, and John Shotter.\n\nSince its appearance in the 1950s, personal construct psychology (PCP) has mainly developed as a constructivist theory of personality and a system of transforming individual meaning-making processes, largely in therapeutic contexts. It was based around the notion of persons as scientists who form and test theories about their worlds. Therefore, it represented one of the first attempts to appreciate the constructive nature of experience and the meaning persons give to their experience. Social constructionism (SC), on the other hand, mainly developed as a form of a critique, aimed to transform the oppressing effects of the social meaning-making processes. Over the years, it has grown into a cluster of different approaches, with no single SC position. However, different approaches under the generic term of SC are loosely linked by some shared assumptions about language, knowledge, and reality.\n\nA usual way of thinking about the relationship between PCP and SC is treating them as two separate entities that are similar in some aspects, but also very different in others. This way of conceptualizing this relationship is a logical result of the circumstantial differences of their emergence. In subsequent analyses these differences between PCP and SC were framed around several points of tension, formulated as binary oppositions: personal/social; individualist/relational; agency/structure; constructivist/constructionist. Although some of the most important issues in contemporary psychology are elaborated in these contributions, the polarized positioning also sustained the idea of a separation between PCP and SC, paving the way for only limited opportunities for dialogue between them.\n\nReframing the relationship between PCP and SC may be of use in both the PCP and the SC communities. On one hand, it extends and enriches SC theory and points to benefits of applying the PCP “toolkit” in constructionist therapy and research. On the other hand, the reframing contributes to PCP theory and points to new ways of addressing social construction in therapeutic conversations.\n\nLike social constructionism, social constructivism states that people work together to construct artifacts. While social constructionism focuses on the artifacts that are created through the social interactions of a group, social constructivism focuses on an individual's learning that takes place because of his or her interactions in a group.\n\nSocial constructivism has been studied by many educational psychologists, who are concerned with its implications for teaching and learning. For more on the psychological dimensions of social constructivism, see the work of Ernst von Glasersfeld and A. Sullivan Palincsar.\n\nSystemic therapy is a form of psychotherapy which seeks to address people as people in relationship, dealing with the interactions of groups and their interactional patterns and dynamics.\n\nA bibliographic review of social constructionism as used within communication studies was published in 2016. It features a good overview of resources from that disciplinary perspective.\n\nThe concepts of \"weak\" and strong as applied to opposing philosophical positions, \"isms\", inform a teleologythe goal-oriented, meaningful or \"final end\" of an interpretation of reality. \"Isms\" are not personal opinions, but the extreme, modal, formulations that actual persons, individuals, can then consider, and take a position between. There are opposing philosophical positions concerning the feasibility of co-creating a common, shared, social reality, called \"weak\" and strong.\n\nJohn R. Searle does not elucidate the terms strong and \"weak\" in his book \"The Construction of Social Reality\", but he clearly uses them in his Chinese room argument, where he debates the feasibility of creating a computing machine with a sharable understanding of reality, and he adds \"We are precisely such machines.\" Strong artificial intelligence (Strong AI) is the bet that computer programmers will somehow eventually achieve a computing machine with a mind of its own, and that it will eventually be more powerful than a human mind. Weak AI bets they won't.\n\nDavid Deutsch in his book \"The Fabric of Reality\" uses a form of strong Turing principle to share Frank Tipler's view of the final state of the universe as an omnipotent (but not omniscient), Omega point, computer. \"But this computer is a society of creative thinkers, or people\" (albeit posthuman transhuman persons), having debates in order to generate information, in the never-ending attempt to attain omniscience of this physicsits evolutionary forms, its computational abilities, and the methods of its epistemologyhaving an eternity to do so. (p. 356)\n\nBecause both the Chinese room argument and the construction of social reality deal with Searle and his debates, and because they both use \"weak\" and strong to denote a philosophical position, and because both debate the programmability of \"the other\", it is worth noting the correspondence that \"strong AI\" is strong social constructionism, and \"weak AI\" is weak social constructivism.\n\nStrong social constructiv\"ism\" says \"none are able to \"communicate\" either a full reality or an accurate ontology, therefore my position must impose, by a sort of divine right, my observer-relative epistemology\", whereas weak social constructiv\"ism\" says \"none are able to \"know\" a full reality, therefore \"we must\" cooperate, informing and conveying an objective ontology as best we can.\"\n\n\"Weak social constructionism\" sees the underlying, objective, \"brute fact\" elements of the class of languages and functional assignments of human, metaphysical, reality. Brute facts are all facts that are not institutional facts (e.g., metaphysical, social agreement). The skeptic portrays the \"weak\" aspect of social constructivism, and wants to spend effort debating the institutional realities.\n\nHarvard psychologist Steven Pinker writes that \"some categories really are social constructions: they exist only because people tacitly agree to act as if they exist. Examples include money, tenure, citizenship, decorations for bravery, and the presidency of the United States.\"\n\nIn a similar vein, Stanley Fish has suggested that baseball's \"balls and strikes\" are social constructions.\n\nBoth Fish and Pinker agree that the sorts of objects indicated here can be described as part of what John Searle calls \"social reality.\" In particular, they are, in Searle's terms, ontologically subjective but epistemologically objective. \"Social facts\" are temporally, ontologically, and logically dependent on \"brute facts.\" For example, \"money\" in the form of its raw materials (rag, pulp, ink) as constituted socially for barter (for example by a banking system) is a social fact of \"money\" by virtue of (i) collectively willing and intending (ii) to impose some particular function (purpose for which), (iii) by constitutive rules atop the \"brute facts.\" \"Social facts have the remarkable feature of having no analogue among physical brute facts\" (34). The existence of language is itself constitutive of the social fact (37), which natural or brute facts do not require. Natural or \"brute\" facts exist independently of language; thus a \"mountain\" is a mountain in every language and in no language; it simply is what it is.\n\nSearle illustrates the evolution of social facts from brute facts by the constitutive rule: X counts as Y in C. \"The Y terms has to assign a new \"status\" that the object does not already have just in virtue of satisfying the Y term; and there has to be collective agreement, or at least acceptance, both in the imposition of that status on the stuff referred to by the X term and about the function that goes with that status. Furthermore, because the physical features brute facts specified by the X term are insufficient by themselves to guarantee the fulfillment of the assigned function specified by the Y term, the new status and its attendant functions have to be the sort of things that can be constituted by collective agreement or acceptance.\"\n\nIt is true or false that language is not a \"brute fact,\" that it is an institutional fact, a human convention, a metaphysical reality (that happens to be physically uttered), but Searle points out that there are language-independent thoughts \"noninstitutional, primitive, biological inclinations and cognitions not requiring any linguistic devices,\" and that there are many \"brute facts\" amongst both humans and animals that are truths that should not be altered in the social constructs because language does not truly constitute them, despite the attempt to institute them for any group's gain: money and property are language dependent, but desires (thirst, hunger) and emotions (fear, rage) are not. (Descartes describes the difference between imagination as a sort of vision, or image, and intellect as conceptualizing things by symbolic manipulation.) Therefore, there is doubt that society or a computer can be completely programmed by language and images, (because there is a programmable, emotive effect of images that derives from the language of judgment towards images).\n\nFinally, against the strong theory and for the weak theory, Searle insists, \"it could not be the case, as some have maintained, that all facts are institutional [i.e., social] facts, that there are no brute facts, because the structure of institutional facts reveals that they are logically dependent on brute facts. To suppose that all facts are institutional [i.e., social] would produce an infinite regress or circularity in the account of institutional facts. In order that some facts are institutional, there must be other facts that are brute [i.e., physical, biological, natural]. This is the consequence of the logical structure of institutional facts.\".\n\nIan Hacking, Canadian philosopher of science, insists, \"the notion that everything is socially constructed has been going the rounds. John Searle [1995] argues vehemently (and in my opinion cogently) against universal constructionism.\" \"Universal social constructionism is descended from the doctrine that I once named linguistic idealism and attributed, only half in jest, to Richard Nixon [Hacking, 1975, p. 182]. Linguistic idealism is the doctrine that only what is talked about exists, nothing has reality until it is spoken of, or written about. This extravagant notion is descended from Berkeley's idea-ism, which we call idealism: the doctrine that all that exists is mental.\" \"They are a part of what John Searle [1995] calls social reality. His book is titled the \"Construction of Social Reality,\" and as I explained elsewhere [Hacking, 1996], that is not a \"social\" construction book at all.\"\n\nHacking observes, \"the label 'social constructionism' is more code than description\" of every Leftist, Marxist, Freudian, and Feminist PostModernist to call into question every moral, sex, gender, power, and deviant claim as just another essentialist claim—including the claim that members of the male and female sex are inherently different, rather than historically and socially constructed. Hacking observes that his 1995 simplistic dismissal of the concept actually revealed to many readers the outrageous implications of the theorists: Is child abuse a real evil, or a social construct, asked Hacking? His dismissive attitude, \"gave some readers a way to see that there need be no clash between construction and reality,\" inasmuch as \"the metaphor of social construction once had excellent shock value, but now it has become tired.\"\n\nInformally, they require human practices to sustain their existence, but they have an effect that is (basically) universally agreed upon. The disagreement lies in whether this category should be called \"socially constructed.\" Ian Hacking argues that it should not. Furthermore, it is not clear that authors who write \"social construction\" analyses ever mean \"social construction\" in Pinker's sense. \". If they never do, then Pinker (probably among others) has misunderstood the point of a social constructionist argument.\n\nTo understand how weak social constructionism can conclude that metaphysics (a human affair) is not the entire \"reality,\" see the arguments against the study of metaphysics. This inability to accurately share the \"full\" reality, even given time for a rational conversation, is similarly proclaimed by \"weak artificial intelligence\".\n\nConstructionism became prominent in the U.S. with Peter L. Berger and Thomas Luckmann's 1966 book, \"The Social Construction of Reality\". Berger and Luckmann argue that all knowledge, including the most basic, taken-for-granted common sense knowledge of everyday reality, is derived from and maintained by social interactions. When people interact, they do so with the understanding that their respective perceptions of reality are related, and as they act upon this understanding their common knowledge of reality becomes reinforced. Since this common sense knowledge is negotiated by people, human typifications, significations and institutions come to be presented as part of an objective reality, particularly for future generations who were not involved in the original process of negotiation. For example, as parents negotiate rules for their children to follow, those rules confront the children as externally produced \"givens\" that they cannot change. Berger and Luckmann's social constructionism has its roots in phenomenology. It links to Heidegger and Edmund Husserl through the teaching of Alfred Schutz, who was also Berger's PhD adviser.\n\nDuring the 1970s and 1980s, social constructionist theory underwent a transformation as constructionist sociologists engaged with the work of Michel Foucault and others as a narrative turn in the social sciences was worked out in practice. This particularly affected the emergent sociology of science and the growing field of science and technology studies. In particular, Karin Knorr-Cetina, Bruno Latour, Barry Barnes, Steve Woolgar, and others used social constructionism to relate what science has typically characterized as objective facts to the processes of social construction, with the goal of showing that human subjectivity imposes itself on those facts we take to be objective, not solely the other way around. A particularly provocative title in this line of thought is Andrew Pickering's \"Constructing Quarks: A Sociological History of Particle Physics\". At the same time, Social Constructionism shaped studies of technology – the Sofield, especially on the Social construction of technology, or SCOT, and authors as Wiebe Bijker, Trevor Pinch, Maarten van Wesel, etc. Despite its common perception as objective, mathematics is not immune to social constructionist accounts. Sociologists such as Sal Restivo and Randall Collins, mathematicians including Reuben Hersh and Philip J. Davis, and philosophers including Paul Ernest have published social constructionist treatments of mathematics.\n\nSocial constructionism can be seen as a source of the postmodern movement, and has been influential in the field of cultural studies. Some have gone so far as to attribute the rise of cultural studies (the cultural turn) to social constructionism. Within the social constructionist strand of postmodernism, the concept of socially constructed reality stresses the ongoing mass-building of worldviews by individuals in dialectical interaction with society at a time. The numerous realities so formed comprise, according to this view, the imagined worlds of human social existence and activity, gradually crystallized by habit into institutions propped up by language conventions, given ongoing legitimacy by mythology, religion and philosophy, maintained by therapies and socialization, and subjectively internalized by upbringing and education to become part of the identity of social citizens.\n\nIn the book \"The Reality of Social Construction\", the British sociologist Dave Elder-Vass places the development of social constructionism as one outcome of the legacy of postmodernism. He writes \"Perhaps the most widespread and influential product of this process [coming to terms with the legacy of postmodernism] is social constructionism, which has been booming [within the domain of social theory] since the 1980s.\"\n\nSocial constructionism falls toward the nurture end of the spectrum of the larger nature and nurture debate. Consequently, critics have argued that it generally ignores biological influences on behaviour or culture, or suggests that they are unimportant to achieve an understanding of human behaviour. The view of most psychologists and social scientists is that behaviour is a complex outcome of both biological and cultural influences. Other disciplines, such as evolutionary psychology, behaviour genetics, behavioural neuroscience, epigenetics, etc., take a nature–nurture interactionism approach to understand behaviour or cultural phenomena.\n\nIn 1996, to illustrate what he believed to be the intellectual weaknesses of social constructionism and postmodernism, physics professor Alan Sokal submitted an article to the academic journal \"Social Text\" deliberately written to be incomprehensible but including phrases and jargon typical of the articles published by the journal. The submission, which was published, was an experiment to see if the journal would \"publish an article liberally salted with nonsense if (a) it sounded good and (b) it flattered the editors' ideological preconceptions.\" The Postmodernism Generator is a computer program that is designed to produce similarly incomprehensible text. In 1999, Sokal, with coauthor Jean Bricmont published the book \"Fashionable Nonsense\", which criticized postmodernism and social constructionism.\n\nPhilosopher Paul Boghossian has also written against social constructionism. He follows Ian Hacking's argument that many adopt social constructionism because of its potentially liberating stance: if things are the way that they are only because of our social conventions, as opposed to being so naturally, then it should be possible to change them into how we would rather have them be. He then states that social constructionists argue that we should refrain from making absolute judgements about what is true and instead state that something is true in the light of this or that theory. Countering this, he states:\nLater in the same work, Boghossian severely constrains the requirements of relativism. He states that instead of believing that any world view is just as true as any other (cultural relativism), we should believe that:\nWoolgar and Pawluch argue that constructionists tend to 'ontological gerrymander' social conditions in and out of their analysis. Following this point, Thibodeaux argued that constructionism can both separate and combine a subject and their effective environment. To resolve this he argued that objective conditions should be used when analyzing how perspectives are motivated.\n\nSocial constructionism has been criticized by psychologists such as University of Toronto Professor Jordan Peterson and evolutionary psychologists, including Steven Pinker in his book \"The Blank Slate\". John Tooby and Leda Cosmides used the term \"standard social science model\" to refer to social-science philosophies that they argue fail to take into account the evolved properties of the brain.\n\n\n\n"}
{"id": "27585", "url": "https://en.wikipedia.org/wiki?curid=27585", "title": "Statistical population", "text": "Statistical population\n\nIn statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all possible hands in a game of poker). A common aim of statistical analysis is to produce information about some chosen population.\n\nIn statistical inference, a subset of the population (a statistical sample) is chosen to represent the population in a statistical analysis. The ratio of the size of this statistical sample to the size of the population is called a sampling fraction. If a sample is chosen properly, characteristics of the entire population that the sample is drawn from can be estimated from corresponding characteristics of the sample.\n\nA subconcept of a population that shares one or more additional properties is called a subpopulation. For example, if the population is all Egyptian people, a subpopulation is all Egyptian males; if the population is all pharmacies in the world, a subpopulation is all pharmacies in Egypt. By contrast, a sample is a subset of a population that is not chosen to share any additional property.\n\nDescriptive statistics may yield different results for different subpopulations. For instance, a particular medicine may have different effects on different subpopulations, and these effects may be obscured or dismissed if such special subpopulations are not identified and examined in isolation.\n\nSimilarly, one can often estimate parameters more accurately if one separates out subpopulations: the distribution of heights among people is better modeled by considering men and women as separate subpopulations, for instance.\n\nPopulations consisting of subpopulations can be modeled by mixture models, which combine the distributions within subpopulations into an overall population distribution. Even if subpopulations are well-modeled by given simple models, the overall population may be poorly fit by a given simple model – poor fit may be evidence for existence of subpopulations. For example, given two equal subpopulations, both normally distributed, if they have the same standard deviation and different means, the overall distribution will exhibit low kurtosis relative to a single normal distribution – the means of the subpopulations fall on the shoulders of the overall distribution. If sufficiently separated, these form a bimodal distribution, otherwise it simply has a wide peak. Further, it will exhibit overdispersion relative to a single normal distribution with the given variation. Alternatively, given two subpopulations with the same mean and different standard deviations, the overall population will exhibit high kurtosis, with a sharper peak and heavier tails (and correspondingly shallower shoulders) than a single distribution\n\n\n"}
{"id": "3261964", "url": "https://en.wikipedia.org/wiki?curid=3261964", "title": "Structure and Interpretation of Classical Mechanics", "text": "Structure and Interpretation of Classical Mechanics\n\nStructure and Interpretation of Classical Mechanics (SICM) is a classical mechanics textbook written by Gerald Jay Sussman and Jack Wisdom with Meinhard E. Mayer. The first edition was published by MIT Press in 2001, and a second edition was released in 2015. The book is used at the Massachusetts Institute of Technology to teach a class in advanced classical mechanics, starting with Lagrange's equations and proceeding through canonical perturbation theory.\n\n\"SICM\" explains some physical phenomena by showing computer programs for simulating them. These programs are written in the Scheme programming language, as were the programs in Sussman's earlier computer science textbook, \"Structure and Interpretation of Computer Programs\".\n\nSussman wrote:\nClassical mechanics is deceptively simple. It is surprisingly easy to get the right answer with fallacious reasoning or without the real understanding. To address this problem Jack Wisdom and I, with help from Hardy Mayer, have written [\"Structure and Interpretation of Classical Mechanics\"] and are teaching a class at MIT that uses computational techniques to communicate a deeper understanding of Classical mechanics. We use computational algorithms to express the methods used to analyze dynamical phenomena. Expressing the methods in a computer language forces them to be unambiguous and computationally effective. Formulating a method as a computer-executable program and debugging that program is a powerful exercise in the learning process. Also, once formalized procedurally, a mathematical idea becomes a tool that can be used directly to compute results.\n\nThe entire text is freely available online from the publisher's website.\n\n\n"}
{"id": "20684211", "url": "https://en.wikipedia.org/wiki?curid=20684211", "title": "The New Politics of Science", "text": "The New Politics of Science\n\nThe New Politics of Science is a 1984 book by David Dickson. The book is about the political relationships which affect science funding. Dickson argues that decisions about science are becoming concentrated in a closed circle of corporate, banking, and military leaders and that America's scientific enterprise is being steadily removed from public decision-making.\n\nDickson was Washington correspondent for the British weekly journal \"Nature\" and European correspondent for the journal \"Science\". \n\n\n"}
{"id": "40170171", "url": "https://en.wikipedia.org/wiki?curid=40170171", "title": "The Path to Degrowth in Overdeveloped Countries", "text": "The Path to Degrowth in Overdeveloped Countries\n\n\"The Path to Degrowth in Overdeveloped Countries\" written by Erik Assadourian is the second chapter of the Worldwatch Institute's \"State of the World\" (2012), available for free online, along with these other chapters from the report: \n\nIn his chapter of the report, Assadourian defines degrowth as an \"essential and urgent\" economic strategy to pursue in countries entrenched in overdevelopment (such as the United States) in order for those countries to be truly sustainable and adapt to \"The rapidly warming Earth and the collapse of ecosystem services\". Furthermore, he hopes to dispel \"the myth that perpetual pursuit of growth is good for economies or the societies of which they are a part\" for the well-being of the planet, of underdeveloped populations, and of the sick, stressed, and overweight populations of overdeveloped countries. Assadourian argues via the principle of plenitude that degrowth will inevitably occur whether we want it to or not because—on a planet of finite resources—economies and populations cannot grow infinitely, and overdeveloped countries are still pursuing more economic growth and overconsuming resources.\n\nAssadourian outlines four policies overdeveloped nations could employ to sufficiently facilitate a planned and controlled contraction of the economy so as to get back in line with planetary boundaries. Each of these, in unison, will eventually foster the creation of a steady-state economy that is in balance with Earth’s limits:\n\n\nAssadourian also wrote a 2-page policy brief on the chapter highlighting the key messages of, the problem regarding, and points to keep in mind moving forward on our path to degrowth.\n\n"}
{"id": "28513034", "url": "https://en.wikipedia.org/wiki?curid=28513034", "title": "The Varieties of Scientific Experience", "text": "The Varieties of Scientific Experience\n\nThe Varieties of Scientific Experience: A Personal View of the Search for God is a book collecting transcribed talks on the subject of natural theology that astronomer Carl Sagan delivered in 1985 at the University of Glasgow as part of the Gifford Lectures. The book was first published posthumously in 2006, 10 years after his death. The title is a reference to \"The Varieties of Religious Experience\" by William James.\n\nThe book was edited by Ann Druyan, who also provided an introduction section. The sixth chapter, \"The God Hypothesis\", was later reprinted in Christopher Hitchens' anthology \"The Portable Atheist\".\n"}
{"id": "25075497", "url": "https://en.wikipedia.org/wiki?curid=25075497", "title": "Uncertainty theory", "text": "Uncertainty theory\n\nUncertainty theory is a branch of mathematics based on normality, monotonicity, self-duality, countable subadditivity, and product measure axioms. It was founded by Baoding Liu in 2007 and refined in 2009.\n\nMathematical measures of the likelihood of an event being true include probability theory, capacity, fuzzy logic, possibility, and credibility, as well as uncertainty.\n\nAxiom 1. (Normality Axiom) formula_1.\n\nAxiom 2. (Self-Duality Axiom) formula_2.\n\nAxiom 3. (Countable Subadditivity Axiom) For every countable sequence of events Λ, Λ, ..., we have\n\nAxiom 4. (Product Measure Axiom) Let formula_4 be uncertainty spaces for formula_5. Then the product uncertain measure formula_6 is an uncertain measure on the product σ-algebra satisfying\n\nPrinciple. (Maximum Uncertainty Principle) For any event, if there are multiple reasonable values that an uncertain measure may take, then the value as close to 0.5 as possible is assigned to the event.\n\nAn uncertain variable is a measurable function ξ from an uncertainty space formula_8 to the set of real numbers, i.e., for any Borel set B of real numbers, the set \nformula_9 is an event.\n\nUncertainty distribution is inducted to describe uncertain variables.\n\nDefinition:The uncertainty distribution formula_10 of an uncertain variable ξ is defined by formula_11.\n\nTheorem(Peng and Iwamura, \"Sufficient and Necessary Condition for Uncertainty Distribution\") A function formula_10 is an uncertain distribution if and only if it is an increasing function except formula_13 and formula_14.\n\nDefinition: The uncertain variables formula_15 are said to be independent if \nfor any Borel sets formula_17 of real numbers.\n\nTheorem 1: The uncertain variables formula_15 are independent if \nfor any Borel sets formula_17 of real numbers.\n\nTheorem 2: Let formula_15 be independent uncertain variables, and formula_22 measurable functions. Then formula_23 are independent uncertain variables.\n\nTheorem 3: Let formula_24 be uncertainty distributions of independent uncertain variables formula_25 respectively, and formula_26 the joint uncertainty distribution of uncertain vector formula_27. If formula_15 are independent, then we have \nfor any real numbers formula_30.\n\nTheorem: Let formula_15 be independent uncertain variables, and formula_32 a measurable function. Then formula_33 is an uncertain variable such that\nwhere formula_35 are Borel sets, and formula_36 meansformula_37 for anyformula_38.\n\nDefinition: Let formula_39 be an uncertain variable. Then the expected value of formula_39 is defined by \nprovided that at least one of the two integrals is finite.\n\nTheorem 1: Let formula_39 be an uncertain variable with uncertainty distribution formula_26. If the expected value exists, then \n\nTheorem 2: Let formula_39 be an uncertain variable with regular uncertainty distribution formula_26. If the expected value exists, then \n\nTheorem 3: Let formula_39 and formula_49 be independent uncertain variables with finite expected values. Then for any real numbers formula_50 and formula_51, we have\n\nDefinition: Let formula_39 be an uncertain variable with finite expected value formula_54. Then the variance of formula_39 is defined by \n\nTheorem: If formula_39 be an uncertain variable with finite expected value, formula_50 and formula_51 are real numbers, then\n\nDefinition: Let formula_39 be an uncertain variable, and formula_62. Then\nis called the α-optimistic value to formula_39, and \nis called the α-pessimistic value to formula_39.\n\nTheorem 1: Let formula_39 be an uncertain variable with regular uncertainty distribution formula_26. Then its α-optimistic value and α-pessimistic value are \n\nTheorem 2: Let formula_39 be an uncertain variable, and formula_62. Then we have\n\nTheorem 3: Suppose that formula_39 and formula_49 are independent uncertain variables, and formula_62. Then we have\n\nformula_80,\n\nformula_81,\n\nformula_82,\n\nformula_83,\n\nformula_84,\n\nformula_85.\n\nDefinition: Let formula_39 be an uncertain variable with uncertainty distribution formula_26. Then its entropy is defined by\nwhere formula_89.\n\nTheorem 1(\"Dai and Chen\"): Let formula_39 be an uncertain variable with regular uncertainty distribution formula_26. Then\n\nTheorem 2: Let formula_39 and formula_49 be independent uncertain variables. Then for any real numbers formula_50 and formula_51, we have\n\nTheorem 3: Let formula_39 be an uncertain variable whose uncertainty distribution is arbitrary but the expected value formula_54 and variance formula_100. Then\n\nTheorem 1(\"Liu\", Markov Inequality): Let formula_39 be an uncertain variable. Then for any given numbers formula_103 and formula_104, we have\n\nTheorem 2 (\"Liu\", Chebyshev Inequality) Let formula_39 be an uncertain variable whose variance formula_107 exists. Then for any given numberformula_108, we have\n\nTheorem 3 (\"Liu\", Holder’s Inequality) Let formula_110 and formula_111 be positive numbers with formula_112, and let formula_39 and formula_49 be independent uncertain variables with formula_115 and formula_116. Then we have\n\nTheorem 4:(Liu [127], Minkowski Inequality) Let formula_110 be a real number with formula_119, and let formula_39 and formula_49 be independent uncertain variables with formula_115 and formula_116. Then we have\n\nDefinition 1: Suppose that formula_125 are uncertain variables defined on the uncertainty space formula_8. The sequence formula_127 is said to be convergent a.s. to formula_39 if there exists an event formula_129 with formula_130 such that\nfor every formula_132. In that case we write formula_133,a.s.\n\nDefinition 2: Suppose that formula_125 are uncertain variables. We say that the sequence formula_127 converges in measure to formula_39 if\nfor every formula_138.\n\nDefinition 3: Suppose that formula_125 are uncertain variables with finite expected values. We say that the sequence formula_127 converges in mean to formula_39 if\n\nDefinition 4: Suppose that formula_143 are uncertainty distributions of uncertain variables formula_125, respectively. We say that the sequence formula_127 converges in distribution to formula_39 if formula_147 at any continuity point of formula_26.\n\nTheorem 1: Convergence in Mean formula_149 Convergence in Measure formula_149 Convergence in Distribution. \nHowever, Convergence in Mean formula_151 Convergence Almost Surely formula_151 Convergence in Distribution.\n\nDefinition 1: Let formula_8 be an uncertainty space, and formula_154. Then the conditional uncertain measure of A given B is defined by\n\nTheorem 1: Let formula_8 be an uncertainty space, and B an event with formula_158. Then M{·|B} defined by Definition 1 is an uncertain measure, and formula_159is an uncertainty space.\n\nDefinition 2: Let formula_39 be an uncertain variable on formula_8. A conditional uncertain variable of formula_39 given B is a measurable function formula_163 from the conditional uncertainty space formula_164 to the set of real numbers such that\n\nDefinition 3: The conditional uncertainty distribution formula_166 of an uncertain variable formula_39 given B is defined by\nprovided that formula_169.\n\nTheorem 2: Let formula_39 be an uncertain variable with regular uncertainty distribution formula_171, and formula_172 a real number with formula_173. Then the conditional uncertainty distribution of formula_39 given formula_175 is\n\nTheorem 3: Let formula_39 be an uncertain variable with regular uncertainty distribution formula_171, and formula_172 a real number with formula_180. Then the conditional uncertainty distribution of formula_39 given formula_182 is\n\nDefinition 4: Let formula_39 be an uncertain variable. Then the conditional expected value of formula_39 given B is defined by\nprovided that at least one of the two integrals is finite.\n\n"}
{"id": "206064", "url": "https://en.wikipedia.org/wiki?curid=206064", "title": "Van der Waals equation", "text": "Van der Waals equation\n\nThe van der Waals equation (or van der Waals equation of state; named after Johannes Diderik van der Waals) is based on plausible reasons that real gases do not follow the ideal gas law. The ideal gas law treats gas molecules as point particles that do not interact with each other but only with their containers. In other words, they do not take up any space, and are not attracted or repelled by other gas molecules and \nthus never lose kinetic energy during collisions. The Ideal Gas Law states that volume (V) occupied by n moles of any gas has a pressure (P) at temperature (T) in Kelvin. The relationship for these variables,\nP V = n R T,\nwhere R is known as the gas constant, is called the ideal gas law or equation of state.\n\nTo account for the volume that a real gas molecule takes up, the van der Waals equation replaces \"V\" in the ideal gas law with \"(V-b)\", where \"b\" is the volume per mole that is occupied by the molecules. This leads to:\n\nThe second modification made to the ideal gas law accounts for the fact that gas molecules do in fact attract each other and that real gases are therefore more compressible than ideal gases. Van der Waals provided for intermolecular attraction by adding to the observed pressure \"P\" in the equation of state a term formula_2, where \"a\" is a constant whose value depends on the gas. The van der Waals equation is therefore written as:\n\nand can also be written as below equation\n\nwhere \"V\" is the molar volume of the gas, \"R\" is the universal gas constant, \"T\" is temperature, \"P\" is pressure, and \"V\" is volume. When the molar volume \"V\" is large, \"b\" becomes negligible in comparison with \"V\", \"a/V\" becomes negligible with respect to \"P\", and the van der Waals equation reduces to the ideal gas law, \"PV=RT\".\n\nIt is available via its traditional derivation (a mechanical equation of state), or via a derivation based in statistical thermodynamics, the latter of which provides the partition function of the system and allows thermodynamic functions to be specified. It successfully approximates the behavior of real fluids above their critical temperatures and is qualitatively reasonable for their liquid and low-pressure gaseous states at low temperatures. However, near the transitions between gas and liquid, in the range of \"p\", \"V\", and \"T\" where the liquid phase and the gas phase are in equilibrium, the \"van der Waals equation\" fails to accurately model observed experimental behaviour, in particular that \"p\" is a constant function of \"V\" at given temperatures. As such, the van der Waals model is not useful only for calculations intended to predict real behavior in regions near the critical point. Empirical corrections to address these predictive deficiencies have been inserted into the van der Waals model, e.g., by James Clerk Maxwell in his equal area rule, and related but distinct theoretical models, e.g., based on the principle of corresponding states, have been developed to achieve better fits to real fluid behaviour in equations of more comparable complexity.\n\nThe van der Waals equation is a thermodynamic equation of state based on the theory that fluids are composed of particles with non-zero volumes, and subject to a (not necessarily pairwise) inter-particle attractive force. It was based on work in theoretical physical chemistry performed in the late 19th century by Johannes Diderik van der Waals, who did related work on the attractive force that also bears his name. The equation is known to be based on a traditional set of derivations deriving from van der Waals' and related efforts, as well as a set of derivation based in statistical thermodynamics, see below.\n\nVan der Waals' early, primary interests were in the field of thermodynamics, where a first influence was the published work by Rudolf Clausius on heat, in 1857; other significant influences were the writings by James Clerk Maxwell, Ludwig Boltzmann, and Willard Gibbs. After initial pursuit of teaching credentials, van der Waals' undergraduate coursework in mathematics and physics at the University of Leiden in the Netherlands led, with significant hurdles, to his acceptance for doctoral studies at Leiden under Pieter Rijke. While his dissertation helps to explain the experimental observation in 1869 by Irish professor of chemistry Thomas Andrews (Queen's University Belfast) of the existence of a critical point in fluids, science historian Martin J. Klein states that it is not clear whether van der Waals was aware of Andrews' results when he began his doctorate work. Van der Waals' doctoral research culminated in an 1873 dissertation that provided a semi-quantitative theory describing the gas-liquid change of state and the origin of a critical temperature, \"Over de Continuïteit van den Gas-en Vloeistof[-]toestand\" (Dutch; in English, \"On the Continuity of the Gas- and Liquid-State\"); it was in this dissertation that the first derivations of what we now refer to as the \"van der Waals equation\" appeared. James Clerk Maxwell reviewed and lauded its published content in the British science journal \"Nature\", and van der Waals began independent work that would result in his receipt of the Nobel Prize in 1910, which emphasized the contribution of his formulation of this \"equation of state for gases and liquids.\"\n\nThe equation relates four state variables: the pressure of the fluid \"p\", the total volume of the fluid's container \"V\", the number of particles \"N\", and the absolute temperature of the system \"T\".\n\nThe intensive, microscopic form of the equation is:\n\nwhere\n\nis the volume of the container occupied by each particle (not the velocity of a particle), and \"k\" is the Boltzmann constant. It introduces two new parameters: \"a&apos;\", a measure of the average attraction between particles, and \"b&apos;\", the volume excluded from \"v\" by one particle.\n\nThe equation can be also written in extensive, molar form:\n\nwhere\n\nis a measure of the average attraction between particles,\n\nis the volume excluded by a mole of particles,\n\nis the number of moles,\n\nis the universal gas constant, \"k\" is Boltzmann's constant, and \"N\" is Avogadro's constant.\n\nA careful distinction must be drawn between the volume \"available to\" a particle and the volume \"of\" a particle. In the intensive equation, \"v\" equals the total space available to each particle, while the parameter \"b&apos;\" is proportional to the proper volume of a single particle - the volume bounded by the atomic radius. This is subtracted from \"v\" because of the space taken up by one particle. In van der Waals' original derivation, given below, \"b&apos;\" is four times the proper volume of the particle. Observe further that the pressure \"p\" goes to infinity when the container is completely filled with particles so that there is no void space left for the particles to move; this occurs when \"V\" = \"nb\". \n\nThe van der Waals equation can also be expressed in terms of reduced properties:\n\nThis yields a critical compressibility factor of 3/8.\n\nThe \"van der Waals equation\" is mathematically simple, but it nevertheless predicts the experimentally observed transition between vapor and liquid, and predicts critical behaviour.\n\nAbove the critical temperature, \"T\", the van der Waals equation is an improvement over the ideal gas law, and for lower temperatures, i.e., \"T\" < \"T\", the equation is also qualitatively reasonable for the liquid and low-pressure gaseous states; however, with respect to the first-order phase transition, i.e., the range of (\"p, V, T\") where a liquid phase and a gas phase would be in equilibrium, the equation appears to fail to predict observed experimental behaviour, in the sense that p is typically observed to be constant as a function of \"V\" for a given temperature in the two-phase region. This apparent discrepancy is resolved in the context of vapour–liquid equilibrium: at a particular temperature, there exist two points on the van der Waals isotherm that have the same chemical potential, and thus a system in thermodynamic equilibrium will appear to traverse a straight line on the \"p\"–\"V\" diagram as the ratio of vapour to liquid changes. However, in such a system, there are really only two points present (the liquid and the vapour) rather than a series of states connected by a line, so connecting the locus of points is incorrect: it is not an equation of multiple states, but an equation of (a single) state. It is indeed possible to compress a gas beyond the point at which it would typically condense, given the right conditions, and it is also possible to expand a liquid beyond the point at which it would usually boil. Such states are called \"metastable\" states. Such behaviour is qualitatively (though perhaps not quantitatively) predicted by the van der Waals equation of state.\n\nHowever, the values of physical quantities as predicted with the van der Waals equation of state \"are in very poor agreement with experiment,\" so the model's utility is limited to qualitative rather than quantitative purposes. Empirically-based corrections can easily be inserted into the van der Waals model (see Maxwell's correction, below), but in so doing, the modified expression is no longer as simple an analytical model; in this regard, other models, such as those based on the principle of corresponding states, achieve a better fit with roughly the same work.\nEven with its acknowledged shortcomings, the pervasive use of the \"van der Waals equation\" in standard university physical chemistry textbooks makes clear its importance as a pedagogic tool to aid understanding fundamental physical chemistry ideas involved in developing theories of vapour–liquid behavior and equations of state. In addition, other (more accurate) equations of state such as the Redlich–Kwong and Peng–Robinson equation of state are essentially modifications of the van der Waals equation of state.\n\nTextbooks in physical chemistry generally give two derivations of the title equation. One is the conventional derivation that goes back to van der Waals, a mechanical equation of state that cannot be used to specify all thermodynamic functions; the other is a statistical mechanics derivation that makes explicit the intermolecular potential neglected in the first derivation. A particular advantage of the statistical mechanical derivation is that it yields the partition function for the system, and allows all thermodynamic functions to be specified (including the mechanical equation of state).\n\nConsider one mole of gas composed of non-interacting point particles that satisfy the ideal gas law:(see any standard Physical Chemistry text, op. cit.)\n\nNext, assume that all particles are hard spheres of the same finite radius \"r\" (the van der Waals radius). The effect of the finite volume of the particles is to decrease the available void space in which the particles are free to move. We must replace \"V\" by \"V\" − \"b\", where \"b\" is called the \"excluded volume\" or \"co-volume\". The corrected equation becomes\n\nThe excluded volume formula_15 is not just equal to the volume occupied by the solid, finite-sized particles, but actually four times that volume. To see this, we must realize that a particle is surrounded by a sphere of radius 2\"r\" (two times the original radius) that is forbidden for the centers of the other particles. If the distance between two particle centers were to be smaller than 2\"r\", it would mean that the two particles penetrate each other, which, by definition, hard spheres are unable to do.\n\nThe excluded volume for the two particles (of average diameter \"d\" or radius \"r\") is\n\nwhich, divided by two (the number of colliding particles), gives the excluded volume per particle:\n\nSo \"b′\" is four times the proper volume of the particle. It was a point of concern to van der Waals that the factor four yields an upper bound; empirical values for \"b′\" are usually lower. Of course, molecules are not infinitely hard, as van der Waals thought, and are often fairly soft.\n\nNext, we introduce a (not necessarily pairwise) attractive force between the particles. van der Waals assumed that, notwithstanding the existence of this force, the density of the fluid is homogeneous; furthermore, he assumed that the range of the attractive force is so small that the great majority of the particles do not feel that the container is of finite size. Given the homogeneity of the fluid, the bulk of the particles do not experience a net force pulling them to the right or to the left. This is different for the particles in surface layers directly adjacent to the walls. They feel a net force from the bulk particles pulling them into the container, because this force is not compensated by particles on the side where the wall is (another assumption here is that there is no interaction between walls and particles, which is not true, as can be seen from the phenomenon of droplet formation; most types of liquid show adhesion). This net force decreases the force exerted onto the wall by the particles in the surface layer. The net force on a surface particle, pulling it into the container, is proportional to the number density\n\nThe number of particles in the surface layers is, again by assuming homogeneity, also proportional to the density. In total, the force on the walls is decreased by a factor proportional to the square of the density, and the pressure (force per unit surface) is decreased by \nso that\n\nUpon writing \"n\" for the number of moles and \"nV\" = \"V\", the equation obtains the second form given above,\n\nIt is of some historical interesting to point out that van der Waals, in his Nobel prize lecture, gave credit to Laplace for the argument that pressure is reduced proportional to the square of the density.\nThe canonical partition function \"Z\" of an ideal gas consisting of \"N = nN\" identical (non-interacting) particles, is:\nwhere formula_23 is the thermal de Broglie wavelength,\nwith the usual definitions: \"h\" is Planck's constant, \"m\" the mass of a particle, \"k\" Boltzmann's constant and \"T\" the absolute temperature. In an ideal gas \"z\" is the partition function of a single particle in a container of volume \"V\". In order to derive the van der Waals equation we assume now that each particle moves independently in an average potential field offered by the other particles. The averaging over the particles is easy because we will assume that the particle density of the van der Waals fluid is homogeneous.\nThe interaction between a pair of particles, which are hard spheres, is taken to be\n\"r\" is the distance between the centers of the spheres and \"d\" is the distance where the hard spheres touch each other (twice the van der Waals radius). The depth of the van der Waals well is formula_26.\n\nBecause the particles are not coupled under the mean field Hamiltonian, the mean field approximation of the total partition function still factorizes,\n\nbut the intermolecular potential necessitates two modifications to \"z\". First, because of the finite size of the particles, not all of \"V\" is available, but only \"V − Nb\"', where (just as in the conventional derivation above)\n\nSecond, we insert a Boltzmann factor\nexp[\" - ϕ/2kT\"] to take care of the average intermolecular potential. We divide here the potential by two because this interaction energy is shared between two particles. Thus\n\nThe total attraction felt by a single particle is\n\nwhere we assumed that in a shell of thickness d\"r\" there are \"N/V\" 4\"π\" \"r\"\"dr\" particles. This is a mean field approximation; the position of the particles is averaged. In reality the density close to the particle is different than far away as can be described by a pair correlation function. Furthermore, it is neglected that the fluid is enclosed \nbetween walls. Performing the integral we get\nHence, we obtain,\nFrom statistical thermodynamics we know that\nso that we only have to differentiate the terms containing V. We get\n\nBelow the critical temperature, the van der Waals equation seems to predict qualitatively incorrect relationships. Unlike for ideal gases, the p-V isotherms oscillate with a relative minimum (\"d\") and a relative maximum (\"e\"). Any pressure between \"p\" and \"p\" appears to have 3 stable volumes, contradicting the experimental observation that two state variables completely determine a one-component system's state. Moreover, the isothermal compressibility is negative between \"d\" and \"e\" (equivalently formula_35), which cannot describe a system at equilibrium.\n\nTo address these problems, James Clerk Maxwell replaced the isotherm between points \"a\" and \"c\" with a horizontal line positioned so that the areas of the two shaded regions would be equal (replacing the \"a\"-\"d\"-\"b\"-\"e\"-\"c\" curve with a straight line from \"a\" to \"c\"); this portion of the isotherm corresponds to the liquid-vapor equilibrium. The regions of the isotherm from \"a\"–\"d\" and from \"c\"–\"e\" are interpreted as metastable states of super-heated liquid and super-cooled vapor, respectively.\n\nwhere \"p\" is the vapor pressure (flat portion of the curve), \"V\" is the volume of the pure liquid phase at point \"a\" on the diagram, and \"V\" is the volume of the pure gas phase at point \"c\" on the diagram. A two-phase mixture at \"p\" will occupy a total volume between \"V\" and \"V\", as determined by Maxwell's lever rule.\n\nMaxwell justified the rule based on the fact that the area on a \"pV\" diagram corresponds to mechanical work, saying that work done on the system in going from \"c\" to \"b\" should equal work released on going from \"a\" to \"b\". This is because the change in free energy \"A\"(\"T\",\"V\") equals the work done during a reversible process, and, as a state variable, the free energy must be path-independent. In particular, the value of \"A\" at point \"b\" should be the same regardless of whether the path taken is from left or right across the horizontal isobar, or follows the original van der Waals isotherm.\n\nThis derivation is not entirely rigorous, since it requires a reversible path through a region of thermodynamic instability, while \"b\" is unstable. Nevertheless, modern derivations from chemical potential reach the same conclusion, and it remains a necessary modification to the van der Waals and to any other analytic equation of state.\n\nThe Maxwell equal area rule can also be derived from an assumption of equal chemical potential \"μ\" of coexisting liquid and vapour phases. On the isotherm shown in the above plot, points \"a\" and \"c\" are the only pair of points which fulfill the equilibrium condition of having equal pressure, temperature and chemical potential. It follows that systems with volumes intermediate between these two points will consist of a mixture of the pure liquid and gas with specific volumes equal to the pure liquid and gas phases at points \"a\" and \"c\".\n\nThe van der Waals equation may be solved for \"V\" and \"V\" as functions of the temperature and the vapor pressure \"p\". Since:\n\nwhere \"A\" is the Helmholtz free energy, it follows that the equal area rule can be expressed as:\n\nSince the gas and liquid volumes are functions of \"p\" and \"T\" only, this equation is then solved numerically to obtain \"p\" as a function of temperature (and number of particles \"N\"), which may then be used to determine the gas and liquid volumes.\n\nA pseudo-3D plot of the locus of liquid and vapor volumes versus temperature and pressure is shown\nin the accompanying figure. One sees that the two locii meet at the critical point (1,1,1) smoothly. An isotherm of the van der Waals fluid taken at \"T \" = 0.90 is also shown where the intersections of the isotherm with the loci illustrate the construct's requirement that the two areas (red and blue, shown) are equal.\n\nWe reiterate that the extensive volume \"V\"  is related to the volume per particle \"v=V/N\"  where \"N = nN\"  is the number of particles in the system.\nThe equation of state does not give us all the thermodynamic parameters of the system. We can take the equation for the Helmholtz energy \"A\" \nFrom the equation derived above for ln\"Q\", we find\n\nWhere Φ is an undetermined constant, which may be taken from the Sackur–Tetrode equation for an ideal gas to be:\n\nThis equation expresses \"A\"  in terms of its natural variables \"V\"  and \"T\" , and therefore gives us all thermodynamic information about the system. The mechanical equation of state was already derived above\n\nThe entropy equation of state yields the entropy (\"S\" )\n\nfrom which we can calculate the internal energy\n\nSimilar equations can be written for the other thermodynamic potential and the chemical potential, but expressing any potential as a function of pressure \"p\"  will require the solution of a third-order polynomial, which yields a complicated expression. Therefore, expressing the enthalpy and the Gibbs energy as functions of their natural variables will be complicated.\nAlthough the material constant \"a\" and \"b\" in the usual form of the van der Waals equation differs for every single fluid considered, the equation can be recast into an invariant form applicable to \"all\" fluids.\n\nDefining the following reduced variables (\"f\", \"f\" are the reduced and critical variable versions of \"f\", respectively),\n\nwhere\n\nas shown by Salzman.\n\nThe first form of the van der Waals equation of state given above can be recast in the following reduced form:\n\nThis equation is \"invariant\" for all fluids; that is, the same reduced form equation of state applies, no matter what \"a\" and \"b\" may be for the particular fluid.\n\nThis invariance may also be understood in terms of the principle of corresponding states. If two fluids have the same reduced pressure, reduced volume, and reduced temperature, we say that their states are corresponding. The states of two fluids may be corresponding even if their measured pressure, volume, and temperature are very different. If the two fluids' states are corresponding, they exist in the same regime of the reduced form equation of state. Therefore, they will respond to changes in roughly the same way, even though their measurable physical characteristics may differ significantly.\n\nThe van der Waals equation is a cubic equation of state; in the reduced formulation the cubic equation is:\n\nAt the critical temperature, where formula_49 we get as expected\n\nFor \"T\" < 1, there are 3 values for \"v\". \nFor \"T\" > 1, there is 1 real value for \"v\".\n\nThe solution of this equation for the case where there are three separate roots may be found at Maxwell construction\n\nThe equation is also usable as a PVT equation for compressible fluids (e.g. polymers). In this case specific volume changes are small and it can be written in a simplified form:\n\nwhere \"p\" is the pressure, \"V\" is specific volume, \"T\" is the temperature and \"A, B, C\" are parameters.\n\n\n\n"}
{"id": "3294759", "url": "https://en.wikipedia.org/wiki?curid=3294759", "title": "Walras's law", "text": "Walras's law\n\nWalras' law is a principle in general equilibrium theory asserting that budget constraints imply that the \"values\" of excess demand (or, conversely, excess market supplies) must sum to zero. That is:\nwhere formula_2 is the price of good \"j\" and formula_3 and formula_4 are the demand and supply respectively of good \"j\".\n\nWalras' law is named after the economist Léon Walras of the University of Lausanne who formulated the concept in his \"Elements of Pure Economics\" of 1874. Although the concept was expressed earlier but in a less mathematically rigorous fashion by John Stuart Mill in his \"Essays on Some Unsettled Questions of Political Economy\" (1844), Walras noted the mathematically equivalent proposition that when considering any particular market, if all other markets in an economy are in equilibrium, then that specific market must also be in equilibrium. The term \"Walras' law\" was coined by Oskar Lange to distinguish it from Say's law. Some economic theorists also use the term to refer to the weaker proposition that the total value of excess demand cannot exceed the total values of excess supply.\n\n\nWalras' law states that the sum of the values of excess demands across all markets must equal zero, whether or not the economy is in a general equilibrium. This implies that if positive excess demand exists in one market, negative excess demand must exist in some other market. Thus, if all markets but one are in equilibrium, then that last market must also be in equilibrium.\n\nThis last implication is often applied in formal general equilibrium models. In particular, to characterize general equilibrium in a model with \"m\" agents and \"n\" commodities, a modeler may impose market clearing for \"n\" – 1 commodities and \"drop the \"n\"-th market-clearing condition.\" In this case, the modeler should include the budget constraints of all \"m\" agents (with equality). Imposing the budget constraints for all \"m\" agents ensures that Walras' law holds, rendering the \"n\"-th market-clearing condition redundant.\n\nIn the former example, suppose that the only commodities in the economy are cherries and apples, and that no other markets exist. This is an exchange economy with no money, so cherries are traded for apples and vice versa. If excess demand for cherries is zero, then by Walras' law, excess demand for apples is also zero. If there is excess demand for cherries, then there will be a surplus (excess supply, or negative excess demand) for apples; and the market value of the excess demand for cherries will equal the market value of the excess supply of apples.\n\nWalras' law is ensured if every agent's budget constraint holds with equality. An agent's budget constraint is an equation stating that the total market value of the agent's planned expenditures, including saving for future consumption, must be less than or equal to the total market value of the agent's expected revenue, including sales of financial assets such as bonds or money. When an agent's budget constraint holds with equality, the agent neither plans to acquire goods for free (e.g., by stealing), nor does the agent plan to give away any goods for free. If every agent's budget constraint holds with equality, then the total market value of \"all\" agents' planned outlays for \"all\" commodities (including saving, which represents future purchases) must equal the total market value of all agents' planned sales of all commodities and assets. It follows that the market value of total excess demand in the economy must be zero, which is the statement of Walras' law. Walras' law implies that if there are \"n\" markets and \"n\" – 1 of these are in equilibrium, then the last market must also be in equilibrium, a property which is essential in the proof of the existence of equilibrium.\n\nConsider an exchange economy with formula_5 agents and formula_6 divisible goods.\n\nFor every agent formula_7, let formula_8 be his initial endowment vector and formula_9 his Marshallian demand function (demand as a function of prices and income).\n\nGiven a price vector formula_10, the income of consumer formula_7 is formula_12. Hence, his demand is formula_13.\n\nThe excess demand function is:\n\nWalras' law can be stated succinctly as:\n\nPROOF: By definition of the excess demand:\n\nThe Marshallian demand is a bundle formula_17 that maximizes the agent's utility, given the budget constraint. The budget constraint here is:\nHence, all terms in the sum are 0 so the sum itself is 0.\n\nNeoclassical macroeconomic reasoning concludes that because of Walras' law, if all markets for goods are in equilibrium, the market for labor must also be in equilibrium. Thus, by neoclassical reasoning, Walras' law contradicts the Keynesian conclusion that negative excess demand and consequently, involuntary unemployment, may exist in the labor market, even when all markets for goods are in equilibrium. The Keynesian rebuttal is that this neoclassical perspective ignores financial markets, which may experience excess demand (such as a \"liquidity trap\") that permits an excess supply of labor and consequently, temporary involuntary unemployment, even if markets for goods are in equilibrium.\n\n\n"}
{"id": "9537177", "url": "https://en.wikipedia.org/wiki?curid=9537177", "title": "William de Alwis", "text": "William de Alwis\n\nWilliam de Alwis (1842–1916) was a Ceylonese artist and entomologist. With his brother George (dates unknown), William made a lasting contribution to the knowledge of the lepidoptera, (butterflies and moths) of Ceylon.\n\nThe brothers' father, Haramanis de Alwis Seneviratne (1792–1894) was a botanical illustrator who worked at the Botanical Gardens in Ceylon at Kalutara between 1818–1822 and Peradeniya from 1822–1861. He illustrated over 2,000 plants. William was appointed to succeed him to continue the work as a botanical artist.\n\nGeorge Thwaites, the Director of the Botanical Garden at Peradeniya between 1849 and 1879, who was impressed by the de Alwis brothers' botanical drawings, recommended to W H Gregory, the Governor of Ceylon, that they should draw from nature the butterflies and moths of Ceylon. Thwaites supervised the drawings, many of which were illustrations of specimens that he had collected himself. The drawings were accurate and later used by a number of authors publishing on the lepidoptera of Ceylon, notably by George Morrison Reid Henry and L G O Woodhouse. The De Alwis drawings are in the Natural History Museum, London.\n\n"}
