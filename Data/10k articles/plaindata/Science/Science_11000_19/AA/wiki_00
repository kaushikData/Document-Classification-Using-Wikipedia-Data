{"id": "57622987", "url": "https://en.wikipedia.org/wiki?curid=57622987", "title": "Akamptisomer", "text": "Akamptisomer\n\nAn akamptisomer is a type of conformational isomer characterized by a hindered inversion of a bond angle. It was first discovered in 2018 in a series of bridged porphyrin molecules.\n"}
{"id": "52548016", "url": "https://en.wikipedia.org/wiki?curid=52548016", "title": "Alison Morrison-Low", "text": "Alison Morrison-Low\n\nDr Alison Morrison-Low is a retired Principal Curator for Science at National Museums Scotland.\n\nMorrison-Low is a director for the Brisbane Observatory Trust and has been a director for the Northern Lighthouse Heritage Trust, the British Society For The History Of Science and the Museum of Scottish Lighthouses.\n\nIn July 2016 she was an invited expert on for \"The Invention of Photography\", In Our Time, BBC Radio 4. She won the Saltire Society Research Book Prize in 2005 for \"Weights and Measures in Scotland: A European Perspective\", and in 2008 the Hans R. Jenemann Foundation's Paul Bunge Prize in 2008 for \"Making Scientific Instruments in the Industrial Revolution\".\n\nIn 2018 Morrison-Low became the first female president of the Royal Scottish Society of the Arts.\n\n\n"}
{"id": "13891942", "url": "https://en.wikipedia.org/wiki?curid=13891942", "title": "Bloch spectrum", "text": "Bloch spectrum\n\nThe Bloch spectrum is a concept in quantum mechanics in the field of theoretical physics; this concept addresses certain energy spectra considerations.\nLet \"H\" be the one-dimensional Schrödinger equation operator\n\nwhere \"U\" is a periodic function of period \"α\". The Bloch spectrum of \"H\" is defined as the set of values \"E\" for which all the solutions of (\"H\" − \"E\")φ = 0 are bounded on the whole real axis. The Bloch spectrum consists of the half-line \"E\" < \"E\" from which certain closed intervals [\"E\", \"E\"] (\"j\" = 1, 2, ...) are omitted. These are forbidden bands (or gaps) so the (\"E\", \"E\") are allowed bands.\n"}
{"id": "47815020", "url": "https://en.wikipedia.org/wiki?curid=47815020", "title": "Brain Balance", "text": "Brain Balance\n\nBrain Balance Achievement Centers are after-school learning centers that offers a program of brain training, exercise, simple physical exercises, skills training, and dietary advice that it says helps children with developmental and learning disabilities.\n\nAs of 2018 there was no good evidence that the company's program helps children. In the scientific and medical community, Brain Balance has been criticized for the lack of scientific evidence for its marketing, as well as its claims about neuroplasticity and other aspects of brain development. That assessment is consistent with a 2015 determination by the Wisconsin Department of Health Services that there was insufficient evidence of effectiveness for the company's claims. and the results of a June 2018 year-long investigation by National Public Radio which cast further doubt on the veracity of claims by the company.\n\nThe Brain Balance program was developed by Robert Melillo, a New York-based chiropractor who later added what he calls \"functional neurology\" to his practice. He began developing the business and its learning centers in 2006, opting for a franchising model. The first center opened in 2007. By early 2018 there were 110 centers in the chain and the business had an annual revenue of $41 million.\n"}
{"id": "27113499", "url": "https://en.wikipedia.org/wiki?curid=27113499", "title": "Cinemeducation", "text": "Cinemeducation\n\nCinemeducation refers to the use of film in medical education. It was originally coined by Drs. Matthew Alexander, Hall, and Pettice in the journal \"Family Medicine\", in 1994 and later used by Drs. Matthew Alexander, Anna Pavlov, and Patricia Lenahan in their text of the same title.\n\n\n"}
{"id": "10013732", "url": "https://en.wikipedia.org/wiki?curid=10013732", "title": "Coherent potential approximation", "text": "Coherent potential approximation\n\nThe coherent potential approximation (or CPA) is a method, in physics, of finding the Green's function of an effective medium. It is a useful concept in understanding how sound waves scatter in a material which displays spatial inhomogeneity.\n\nOne version of the CPA is an extension to random materials of the muffin-tin approximation, used to calculate electronic band structure in solids. A variational implementation of the muffin-tin approximation to crystalline solids using Green's functions was suggested by Korringa and by Kohn and Rostoker, and is often referred to as the KKR method. For random materials, the theory is applied by the introduction of an ordered lattice of effective potentials to replace the varying potentials in the random material. This approach is called the \"KKR coherent potential approximation\".\n\n"}
{"id": "848254", "url": "https://en.wikipedia.org/wiki?curid=848254", "title": "Crescograph", "text": "Crescograph\n\nA crescograph is a device for measuring the growth in plants. It was invented in the early 20th century by Sir Jagadish Chandra Bose.\n\nThe Bose crescograph uses a series of clockwork gears and a smoked glass plate to record the movement of the tip of a plant (or its roots) at magnifications of up to 10,000. Marks are made on the plate at intervals of a few seconds, demonstrating how the rate of growth varies under varying stimuli. Bose experimented with temperature, chemicals, gases, and electricity.\n\nThe electronic crescograph plant movement detector is capable of measurements as small as 1/1,000,000 of an inch. However, its normal operating range is from 1/1000 to 1/10,000 of an inch. The component which actually measures the movement is a differential transformer along with a movable core hinged between two points. A micrometer is used to adjust and calibrate the system. It could record plant growth, magnifying a small movement as much as 10,000,000 times.\n"}
{"id": "8517443", "url": "https://en.wikipedia.org/wiki?curid=8517443", "title": "Cross-cultural psychiatry", "text": "Cross-cultural psychiatry\n\nCross-cultural psychiatry (also known as transcultural psychiatry or cultural psychiatry) is a branch of psychiatry concerned with the cultural context of mental disorders and the challenges of addressing ethnic diversity in psychiatric services. It emerged as a coherent field from several strands of work, including surveys of the prevalence and form of disorders in different cultures or countries; the study of migrant populations and ethnic diversity within countries; and analysis of psychiatry itself as a cultural product.\n\nThe early literature was associated with colonialism and with observations by asylum psychiatrists or anthropologists who tended to assume the universal applicability of Western psychiatric diagnostic categories. A seminal paper by Arthur Kleinman in 1977 followed by a renewed dialogue between anthropology and psychiatry, is seen as having heralded a \"new cross-cultural psychiatry\". However, Kleinman later pointed out that culture often became incorporated in only superficial ways, and that for example 90% of DSM-IV categories are culture-bound to North America and Western Europe, and yet the \"culture-bound syndrome\" label is only applied to \"exotic\" conditions outside Euro-American society.\n\nCultural psychiatry looks at whether psychiatric classifications of disorders are appropriate to different cultures or ethnic groups. It often argues that psychiatric illnesses represent social constructs as well as genuine medical conditions, and as such have social uses peculiar to the social groups in which they are created and legitimized. It studies psychiatric classifications in different cultures, whether informal (e.g. category terms used in different languages) or formal (for example the World Health Organization's ICD, the American Psychiatric Association's DSM, or the Chinese Society of Psychiatry's CCMD).\nThe field has increasingly had to address the process of globalization. It is said every city has a different culture and that the urban environment, and how people adapt or struggle to adapt to it, can play a crucial role in the onset or worsening of mental illness.\n\nHowever, some scholars developing an anthropology of mental illness (Lézé, 2014) consider that attention to culture is not enough if it is decontextualized from historical events, and history in more general sense. An historical and politically informed perspective can counteract some of the risks related to promoting universalized 'global mental health' programs as well as the increasing hegemony of diagnostic categories such as PTSD (Didier Fassin and Richard Rechtman analyze this issue in their book 'The Empire of Trauma'). Roberto Beneduce, who devoted many years to research and clinical practice in West Africa (Mali, among the Dogon) and in Italy with migrants, strongly emphasizes this shift. Inspired by the thought of Frantz Fanon, Beneduce points to forms of historical consciousness and selfhood as well as history-related suffering as central dimensions of a 'critical ethnopsychiatry' or 'critical transcultural psychiatry'.\n\nAs a named field within the larger discipline of psychiatry, cultural psychiatry has a relatively short history. In 1955, a program in transcultural psychiatry was established at McGill University in Montreal by Eric Wittkower from psychiatry and Jacob Fried from the department of anthropology. In 1957, at the International Psychiatric Congress in Zurich, Wittkower organized a meeting that was attended by psychiatrists from 20 countries, including many who became major contributors to the field of cultural psychiatry: Tsung-Yi Lin (Taiwan), Thomas Lambo (Nigeria), Morris Carstairs (Britain), Carlos Alberto Seguin (Peru) and Pow-Meng Yap (Hong Kong). The American Psychiatric Association established a Committee on Transcultural Psychiatry in 1964, followed by the Canadian Psychiatric Association in 1967. H.B.M. Murphy of McGill founded the World Psychiatric Association Section on Transcultural Psychiatry in 1970. By the mid-1970s there were active transcultural psychiatry societies in England, France, Italy and Cuba. There are several scientific journals devoted to cross-cultural issues: \"Transcultural Psychiatry\" (est. 1956, originally as \"Transcultural Psychiatric Research Review\", and now the official journal of the WPA Section on Transcultural Psychiatry), \"Psychopathologie Africaine\" (1965), \"Culture Medicine & Psychiatry\" (1977), \"Curare\" (1978), and \"World Cultural Psychiatry Research Review\" (2006). The Foundation for Psychocultural Research at UCLA has published an important volume on psychocultural aspects of trauma and most recently the landmark volumes entitled \"Formative Experiences: the Interaction of Caregiving, Culture, and Developmental Psychobiology\" edited by Carol Worthman, Paul Plotsky, Daniel Schechter and Constance Cummings. and \"Re-Visioning Psychiatry: Cultural Phenomenology, Critical Neuroscience, and Global Mental Health\" edited by Laurence J. Kirmayer, Robert Lemelson and Constance Cummings.\n\nIt is argued that a cultural perspective can help psychiatrists become aware of the hidden assumptions and limitations of current psychiatric theory and practice and can identify new approaches appropriate for treating the increasingly diverse populations seen in psychiatric services around the world. The recent revision of the nosology of the American Psychiatric Association, DSM-5, includes a Cultural Formulation Interview that aims to help clinicians contextualize diagnostic assessment. A related approach to cultural assessment involves cultural consultation which works with interpreters and cultural brokers to develop a cultural formulation and treatment plan that can assist clinicians.\n\nThe main professional organizations devoted to the field are the WPA Section on Transcultural Psychiatry, the Society for the Study of Psychiatry and Culture, and the World Association for Cultural Psychiatry. Many other mental health organizations have interest groups or sections devoted to issues of culture and mental health.\n\nThere are active research and training programs in cultural psychiatry at several academic centers around the world, notably the Division of Social and Transcultural Psychiatry at McGill University, Harvard University, the University of Toronto, and University College London. Other organizations are devoted to cross-cultural adaptation of research and clinical methods. In 1993 the Transcultural Psychosocial Organization (TPO) was founded. The TPO has developed a system of intervention aimed at countries with little or no mental health care. They train local people to become mental health workers, often using people who previously have provided mental health guidance of some kind. The TPO provides training material that is adapted to local culture, language and distinct traumatic events that might have occurred in the region where the organization is operating. Avoiding Western approaches to mental health, the TPO sets up what becomes a local non-governmental organization that is self-sustainable, as well as economically and politically independent of any state. The TPO projects have been successful in both Uganda and Cambodia.\n"}
{"id": "58497153", "url": "https://en.wikipedia.org/wiki?curid=58497153", "title": "Datasets.load", "text": "Datasets.load\n\ndatasets.load is an R package and RStudio plugin, that provides a both Graphical User Interface (GUI) as well as a Command Line Interface for loading datasets. Normally, R only makes datasets of loaded packages visible, datasets.load shows the list of all installed datasets on the local library, including datasets included with packages that haven't been loaded.\n\nR functionality is extendible using extensions call R packages. The central place to store package is the Central R Archive network (CRAN). From CRAN, R packages can be installed using the command:\nOnce installed, R packages can be loaded using the command:\nAfter a package has been loaded, objects available from the package - such as functions and datasets - can be accessed.\n\nThe available datasets can listed using the command:\nHowever, datasets from packages that are not loaded, are not listed. As a result, many R users have access to datasets on their local install that are never used. The datasets.load packages addresses this by listed all datasets that are in any packages installed (loaded or not loaded).\n\nThe usage of datasets.load's Command Line Interface is demonstrated in the code snippet below.\n\nA video demonstrating the GUI usage is available on YouTube.\n\nThe basic functionality of datasets.load is to expose all installed datasets to the user, including those in packages that are not loaded. There is a Command Line Interface which can be user from any R terminal.\n\nIn addition to this, there is also a Graphical User Interface for the RStudio Integrated Development Environment, using RStudio Addins.\n\nThe initial release of version 0.1.0 took place in December 2016, and averaged a download rate of 1,000 times per month, from the RStudio servers alone. With the release of version 0.3.0 in 2018, the download rate increased to 2,000 times per month, putting the package in the 9th percentile of most popular R packages. The package was reviewed in the 2017 article \"R Packages worth a look\" on Data Analytics & R, which further increased usage. It is also frequently preinstalled on university computers, in order to help make R more accessible to students. \n\nThe RStudio CRAN mirror download logs\n, with a total of over 30,000 downloads since the first release\n, according to RDocumentation.org, this puts the package in the 9th percentile of most popular R packages\n\n"}
{"id": "40628358", "url": "https://en.wikipedia.org/wiki?curid=40628358", "title": "Deliberatorium", "text": "Deliberatorium\n\nA deliberatorium or collaboratorium is a form of online collaborative argument mapping. It was first deployed as the MIT Collaboratorium, and directed at the question of climate change. \n\nIn December 2008, Center for Collective Intelligence at the MIT Mark Klein tested a prototype of the collaboratorium at the University of Naples featuring a 200 student debate on biofuels. \n\nDeleiberatoriums work by deploying a website which allows the public to post the latest scientific results about climate change. Once done, people can debate how to get rid of carbon emissions which is how politicians get feedback on public opinion. \n\nThe site operates similar to Wikipedia for authoritative reports but with a more structured, organized debate featuring an argument tree. These reports for the site come from the Intergovernmental Panel on Climate Change (IPCC). \n\n\n"}
{"id": "58902618", "url": "https://en.wikipedia.org/wiki?curid=58902618", "title": "Denis Burgarella", "text": "Denis Burgarella\n\nDenis Burgarella is a French astrophysicist working at Laboratoire d'astrophysique de Marseille. He was president of SF2A from 2008 to 2010. He was president of the J1 commission of the International Astronomical Union from 2015 to 2018. From 2018 he is president of division J.\n"}
{"id": "21799935", "url": "https://en.wikipedia.org/wiki?curid=21799935", "title": "Downie bodies", "text": "Downie bodies\n\nDownie bodies, also known as a type of A-type inclusion, are a type of inclusion body associated with cowpox.\n\nThey are named for Allan Watt Downie.\n\nIt should not be confused with the term \"Downey bodies\", which refers to a type of T cell observed in infectious mononucleosis.\n"}
{"id": "2111048", "url": "https://en.wikipedia.org/wiki?curid=2111048", "title": "Earthquake engineering", "text": "Earthquake engineering\n\nEarthquake engineering is an interdisciplinary branch of engineering that designs and analyzes structures, such as buildings and bridges, with earthquakes in mind. Its overall goal is to make such structures more resistant to earthquakes. An earthquake (or seismic) engineer aims to construct structures that will not be damaged in minor shaking and will avoid serious damage or collapse in a major earthquake. Earthquake engineering is the scientific field concerned with protecting society, the natural environment, and the man-made environment from earthquakes by limiting the seismic risk to socio-economically acceptable levels. Traditionally, it has been narrowly defined as the study of the behavior of structures and geo-structures subject to seismic loading; it is considered as a subset of structural engineering, geotechnical engineering, mechanical engineering, chemical engineering, applied physics, etc. However, the tremendous costs experienced in recent earthquakes have led to an expansion of its scope to encompass disciplines from the wider field of civil engineering, mechanical engineering and from the social sciences, especially sociology, political science, economics and finance.\n\nThe main objectives of earthquake engineering are:\n\nA properly engineered structure does not necessarily have to be extremely strong or expensive. It has to be properly designed to withstand the seismic effects while sustaining an acceptable level of damage.\n\nSeismic loading means application of an earthquake-generated excitation on a structure (or geo-structure). It happens at contact surfaces of a structure either with the ground, with adjacent structures, or with gravity waves from tsunami. The loading that is expected at a given location on the Earth's surface is estimated by engineering seismology. It is related to the seismic hazard of the location.\n\nEarthquake or seismic performance defines a structure's ability to sustain its main functions, such as its safety and serviceability, \"at\" and \"after\" a particular earthquake exposure. A structure is normally considered \"safe\" if it does not endanger the lives and well-being of those in or around it by partially or completely collapsing. A structure may be considered \"serviceable\" if it is able to fulfill its operational functions for which it was designed.\n\nBasic concepts of the earthquake engineering, implemented in the major building codes, assume that a building should survive a rare, very severe earthquake by sustaining significant damage but without globally collapsing. On the other hand, it should remain operational for more frequent, but less severe seismic events.\n\nEngineers need to know the quantified level of the actual or anticipated seismic performance associated with the direct damage to an individual building subject to a specified ground shaking.\nSuch an assessment may be performed either experimentally or analytically.\n\nExperimental evaluations are expensive tests that are typically done by placing a (scaled) model of the structure on a shake-table that simulates the earth shaking and observing its behavior. Such kinds of experiments were first performed more than a century ago. Only recently has it become possible to perform 1:1 scale testing on full structures.\n\nDue to the costly nature of such tests, they tend to be used mainly for understanding the seismic behavior of structures, validating models and verifying analysis methods. Thus, once properly validated, computational models and numerical procedures tend to carry the major burden for the seismic performance assessment of structures.\n\nSeismic performance assessment or seismic structural analysis is a powerful tool of earthquake engineering which utilizes detailed modelling of the structure together with methods of structural analysis to gain a better understanding of seismic performance of building and non-building structures. The technique as a formal concept is a relatively recent development.\n\nIn general, seismic structural analysis is based on the methods of structural dynamics. For decades, the most prominent instrument of seismic analysis has been the earthquake response spectrum method which also contributed to the proposed building code's concept of today.\n\nHowever, such methods are good only for linear elastic systems, being largely unable to model the structural behavior when damage (i.e., non-linearity) appears. Numerical \"step-by-step integration\" proved to be a more effective method of analysis for multi-degree-of-freedom structural systems with significant non-linearity under a transient process of ground motion excitation.\n\nBasically, numerical analysis is conducted in order to evaluate the seismic performance of buildings. Performance evaluations are generally carried out by using nonlinear static pushover analysis or nonlinear time-history analysis. In such analyses, it is essential to achieve accurate non-linear modeling of structural components such as beams, columns, beam-column joints, shear walls etc. Thus, experimental results play an important role in determining the modeling parameters of individual components, especially those that are subject to significant non-linear deformations. The individual components are then assembled to create a full non-linear model of the structure. Thus created models are analyzed to evaluate the performance of buildings.\n\nThe capabilities of the structural analysis software are a major consideration in the above process as they restrict the possible component models, the analysis methods available and, most importantly, the numerical robustness. The latter becomes a major consideration for structures that venture into the non-linear range and approach global or local collapse as the numerical solution becomes increasingly unstable and thus difficult to reach. There are several commercially available Finite Element Analysis software's such as CSI-SAP2000 and CSI-PERFORM-3D and Scia Engineer-ECtools which can be used for the seismic performance evaluation of buildings. Moreover, there is research-based finite element analysis platforms such as OpenSees, RUAUMOKO and the older DRAIN-2D/3D, several of which are now open source.\n\nResearch for earthquake engineering means both field and analytical investigation or experimentation intended for discovery and scientific explanation of earthquake engineering related facts, revision of conventional concepts in the light of new findings, and practical application of the developed theories.\n\nThe National Science Foundation (NSF) is the main United States government agency that supports fundamental research and education in all fields of earthquake engineering. In particular, it focuses on experimental, analytical and computational research on design and performance enhancement of structural systems.\n\nThe Earthquake Engineering Research Institute (EERI) is a leader in dissemination of earthquake engineering research related information both in the U.S. and globally.\n\nA definitive list of earthquake engineering research related shaking tables around the world may be found in Experimental Facilities for Earthquake Engineering Simulation Worldwide. The most prominent of them is now E-Defense Shake Table in Japan.\n\n NSF also supports the George E. Brown, Jr. Network for Earthquake Engineering Simulation\n\nThe NSF Hazard Mitigation and Structural Engineering program (HMSE) supports research on new technologies for improving the behavior and response of structural systems subject to earthquake hazards; fundamental research on safety and reliability of constructed systems; innovative developments in analysis and model based simulation of structural behavior and response including soil-structure interaction; design concepts that improve structure performance and flexibility; and application of new control techniques for structural systems.\n\n(NEES) that advances knowledge discovery and innovation for earthquakes and tsunami loss reduction of the nation's civil infrastructure and new experimental simulation techniques and instrumentation.\n\nThe NEES network features 14 geographically-distributed, shared-use laboratories that support several types of experimental work: geotechnical centrifuge research, shake-table tests, large-scale structural testing, tsunami wave basin experiments, and field site research. Participating universities include: Cornell University; Lehigh University; Oregon State University; Rensselaer Polytechnic Institute; University at Buffalo, State University of New York; University of California, Berkeley; University of California, Davis; University of California, Los Angeles; University of California, San Diego; University of California, Santa Barbara; University of Illinois, Urbana-Champaign; University of Minnesota; University of Nevada, Reno; and the University of Texas, Austin.\n\nThe equipment sites (labs) and a central data repository are connected to the global earthquake engineering community via the NEEShub website. The NEES website is powered by HUBzero software developed at Purdue University for nanoHUB specifically to help the scientific community share resources and collaborate. The cyberinfrastructure, connected via Internet2, provides interactive simulation tools, a simulation tool development area, a curated central data repository, animated presentations, user support, telepresence, mechanism for uploading and sharing resources, and statistics about users and usage patterns.\n\nThis cyberinfrastructure allows researchers to: securely store, organize and share data within a standardized framework in a central location; remotely observe and participate in experiments through the use of synchronized real-time data and video; collaborate with colleagues to facilitate the planning, performance, analysis, and publication of research experiments; and conduct computational and hybrid simulations that may combine the results of multiple distributed experiments and link physical experiments with computer simulations to enable the investigation of overall system performance.\n\nThese resources jointly provide the means for collaboration and discovery to improve the seismic design and performance of civil and mechanical infrastructure systems.\n\nThe very first earthquake simulations were performed by statically applying some \"horizontal inertia forces\" based on scaled peak ground accelerations to a mathematical model of a building. With the further development of computational technologies, static approaches began to give way to dynamic ones.\n\nDynamic experiments on building and non-building structures may be physical, like shake-table testing, or virtual ones. In both cases, to verify a structure's expected seismic performance, some researchers prefer to deal with so called \"real time-histories\" though the last cannot be \"real\" for a hypothetical earthquake specified by either a building code or by some particular research requirements. Therefore, there is a strong incentive to engage an earthquake simulation which is the seismic input that possesses only essential features of a real event.\n\nSometimes earthquake simulation is understood as a re-creation of local effects of a strong earth shaking.\n\nTheoretical or experimental evaluation of anticipated seismic performance mostly requires a structure simulation which is based on the concept of structural likeness or similarity. Similarity is some degree of analogy or resemblance between two or more objects. The notion of similarity rests either on exact or approximate repetitions of patterns in the compared items.\n\nIn general, a building model is said to have similarity with the real object if the two share \"geometric similarity\", \"kinematic similarity\" and \"dynamic similarity\". The most vivid and effective type of similarity is the \"kinematic\" one. \"Kinematic similarity\" exists when the paths and velocities of moving particles of a model and its prototype are similar.\n\nThe ultimate level of \"kinematic similarity\" is \"kinematic equivalence\" when, in the case of earthquake engineering, time-histories of each story lateral displacements of the model and its prototype would be the same.\n\nSeismic vibration control is a set of technical means aimed to mitigate seismic impacts in building and non-building structures. All seismic vibration control devices may be classified as \"passive\", \"active\" or \"hybrid\" where:\n\n\nWhen ground seismic waves reach up and start to penetrate a base of a building, their energy flow density, due to reflections, reduces dramatically: usually, up to 90%. However, the remaining portions of the incident waves during a major earthquake still bear a huge devastating potential.\n\nAfter the seismic waves enter a superstructure, there are a number of ways to control them in order to soothe their damaging effect and improve the building's seismic performance, for instance:\n\nDevices of the last kind, abbreviated correspondingly as TMD for the tuned (\"passive\"), as AMD for the \"active\", and as HMD for the \"hybrid mass dampers\", have been studied and installed in high-rise buildings, predominantly in Japan, for a quarter of a century.\n\nHowever, there is quite another approach: partial suppression of the seismic energy flow into the superstructure known as seismic or base isolation.\n\nFor this, some pads are inserted into or under all major load-carrying elements in the base of the building which should substantially decouple a superstructure from its substructure resting on a shaking ground.\n\nThe first evidence of earthquake protection by using the principle of base isolation was discovered in Pasargadae, a city in ancient Persia, now Iran, and dates back to the 6th century BCE. Below, there are some samples of seismic vibration control technologies of today.\n\nPeople of Inca civilization were masters of the polished 'dry-stone walls', called ashlar, where blocks of stone were cut to fit together tightly without any mortar. The Incas were among the best stonemasons the world has ever seen and many junctions in their masonry were so perfect that even blades of grass could not fit between the stones.\n\nPeru is a highly seismic land and for centuries the mortar-free construction proved to be apparently more earthquake-resistant than using mortar. The stones of the dry-stone walls built by the Incas could move slightly and resettle without the walls collapsing, a passive structural control technique employing both the principle of energy dissipation (coulomb damping) and that of suppressing resonant amplifications.\n\nTypically the tuned mass dampers are huge concrete blocks mounted in skyscrapers or other structures and move in opposition to the resonance frequency oscillations of the structures by means of some sort of spring mechanism.\n\nThe Taipei 101 skyscraper needs to withstand typhoon winds and earthquake tremors common in this area of Asia/Pacific. For this purpose, a steel pendulum weighing 660 metric tonnes that serves as a tuned mass damper was designed and installed atop the structure. Suspended from the 92nd to the 88th floor, the pendulum sways to decrease resonant amplifications of lateral displacements in the building caused by earthquakes and strong gusts.\nA hysteretic damper is intended to provide better and more reliable seismic performance than that of a conventional structure by increasing the dissipation of seismic input energy. There are five major groups of hysteretic dampers used for the purpose, namely:\n\nViscous Dampers have the benefit of being a supplemental damping system. They have an oval hysteretic loop and the damping is velocity dependent. While some minor maintenance is potentially required, viscous dampers generally do not need to be replaced after an earthquake. While more expensive than other damping technologies they can be used for both seismic and wind loads and are the most commonly used hysteretic damper.\n\nFriction dampers tend to be available in two major types, linear and rotational and dissipate energy by heat. The damper operates on the principle of a coulomb damper. Depending on the design, friction dampers can experience stick-slip phenomenon and Cold welding. The main disadvantage being that friction surfaces can wear over time and for this reason they are not recommended for dissipating wind loads. When used in seismic applications wear is not a problem and there is no required maintenance. They have a rectangular hysteretic loop and as long as the building is sufficiently elastic they tend to settle back to their original positions after an earthquake.\n\nMetallic yielding dampers, as the name implies, yield in order to absorb the earthquake's energy. This type of damper absorbs a large amount of energy however they must be replaced after an earthquake and may prevent the building from settling back to its original position.\n\nViscoelastic dampers are useful in that they can be used for both wind and seismic applications, they are usually limited to small displacements. There is some concern as to the reliability of the technology as some brands have been banned from use in buildings in the United States.\n\nBase isolation seeks to prevent the kinetic energy of the earthquake from being transferred into elastic energy in the building. These technologies do so by isolating the structure from the ground, thus enabling them to move somewhat independently. The degree to which the energy is transferred into the structure and how the energy is dissipated will vary depending on the technology used.\n\nLead rubber bearing or LRB is a type of base isolation employing a heavy damping. It was invented by Bill Robinson, a New Zealander.\n\nHeavy damping mechanism incorporated in vibration control technologies and, particularly, in base isolation devices, is often considered a valuable source of suppressing vibrations thus enhancing a building's seismic performance. However, for the rather pliant systems such as base isolated structures, with a relatively low bearing stiffness but with a high damping, the so-called \"damping force\" may turn out the main pushing force at a strong earthquake. The video shows a Lead Rubber Bearing being tested at the UCSD Caltrans-SRMD facility. The bearing is made of rubber with a lead core. It was a uniaxial test in which the bearing was also under a full structure load. Many buildings and bridges, both in New Zealand and elsewhere, are protected with lead dampers and lead and rubber bearings. Te Papa Tongarewa, the national museum of New Zealand, and the New Zealand Parliament Buildings have been fitted with the bearings. Both are in Wellington which sits on an active fault.\n\nSprings-with-damper base isolator installed under a three-story town-house, Santa Monica, California is shown on the photo taken prior to the 1994 Northridge earthquake exposure. It is a base isolation device conceptually similar to \"Lead Rubber Bearing\".\n\nOne of two three-story town-houses like this, which was well instrumented for recording of both vertical and horizontal accelerations on its floors and the ground, has survived a severe shaking during the Northridge earthquake and left valuable recorded information for further study.\n\nSimple roller bearing is a base isolation device which is intended for protection of various building and non-building structures against potentially damaging lateral impacts of strong earthquakes.\n\nThis metallic bearing support may be adapted, with certain precautions, as a seismic isolator to skyscrapers and buildings on soft ground. Recently, it has been employed under the name of \"metallic roller bearing\" for a housing complex (17 stories) in Tokyo, Japan.\n\nFriction pendulum bearing (FPB) is another name of friction pendulum system (FPS). It is based on three pillars:\n\nSnapshot with the link to video clip of a shake-table testing of FPB system supporting a rigid building model is presented at the right.\n\nSeismic design is based on authorized engineering procedures, principles and criteria meant to design or retrofit structures subject to earthquake exposure. Those criteria are only consistent with the contemporary state of the knowledge about earthquake engineering structures. Therefore, a building design which exactly follows seismic code regulations does not guarantee safety against collapse or serious damage.\n\nThe price of poor seismic design may be enormous. Nevertheless, seismic design has always been a trial and error process whether it was based on physical laws or on empirical knowledge of the structural performance of different shapes and materials.\n\nTo practice seismic design, seismic analysis or seismic evaluation of new and existing civil engineering projects, an engineer should, normally, pass examination on \"Seismic Principles\" which, in the State of California, include:\n\nTo build up complex structural systems, seismic design largely uses the same relatively small number of basic structural elements (to say nothing of vibration control devices) as any non-seismic design project.\n\nNormally, according to building codes, structures are designed to \"withstand\" the largest earthquake of a certain probability that is likely to occur at their location. This means the loss of life should be minimized by preventing collapse of the buildings.\n\nSeismic design is carried out by understanding the possible failure modes of a structure and providing the structure with appropriate strength, stiffness, ductility, and configuration to ensure those modes cannot occur.\n\nSeismic design requirements depend on the type of the structure, locality of the project and its authorities which stipulate applicable seismic design codes and criteria. For instance, California Department of Transportation's requirements called \"The Seismic Design Criteria\" (SDC) and aimed at the design of new bridges in California incorporate an innovative seismic performance-based approach.\n\nThe most significant feature in the SDC design philosophy is a shift from a \"force-based assessment\" of seismic demand to a \"displacement-based assessment\" of demand and capacity. Thus, the newly adopted displacement approach is based on comparing the \"elastic displacement\" demand to the \"inelastic displacement\" capacity of the primary structural components while ensuring a minimum level of inelastic capacity at all potential plastic hinge locations.\n\nIn addition to the designed structure itself, seismic design requirements may include a \"ground stabilization\" underneath the structure: sometimes, heavily shaken ground breaks up which leads to collapse of the structure sitting upon it.\nThe following topics should be of primary concerns: liquefaction; dynamic lateral earth pressures on retaining walls; seismic slope stability; earthquake-induced settlement.\n\nNuclear facilities should not jeopardise their safety in case of earthquakes or other hostile external events. Therefore, their seismic design is based on criteria far more stringent than those applying to non-nuclear facilities. The Fukushima I nuclear accidents and damage to other nuclear facilities that followed the 2011 Tōhoku earthquake and tsunami have, however, drawn attention to ongoing concerns over Japanese nuclear seismic design standards and caused other many governments to re-evaluate their nuclear programs. Doubt has also been expressed over the seismic evaluation and design of certain other plants, including the Fessenheim Nuclear Power Plant in France.\n\nFailure mode is the manner by which an earthquake induced failure is observed. It, generally, describes the way the failure occurs. Though costly and time consuming, learning from each real earthquake failure remains a routine recipe for advancement in \"seismic design\" methods. Below, some typical modes of earthquake-generated failures are presented.\nThe lack of reinforcement coupled with poor mortar and inadequate roof-to-wall ties can result in substantial damage to an unreinforced masonry building. Severely cracked or leaning walls are some of the most common earthquake damage. Also hazardous is the damage that may occur between the walls and roof or floor diaphragms. Separation between the framing and the walls can jeopardize the vertical support of roof and floor systems.\nSoft story effect. Absence of adequate stiffness on the ground level caused damage to this structure. A close examination of the image reveals that the rough board siding, once covered by a brick veneer, has been completely dismantled from the studwall. Only the rigidity of the floor above combined with the support on the two hidden sides by continuous walls, not penetrated with large doors as on the street sides, is preventing full collapse of the structure.\nSoil liquefaction. In the cases where the soil consists of loose granular deposited materials with the tendency to develop excessive hydrostatic pore water pressure of sufficient magnitude and compact, liquefaction of those loose saturated deposits may result in non-uniform settlements and tilting of structures. This caused major damage to thousands of buildings in Niigata, Japan during the 1964 earthquake.\nLandslide rock fall. A landslide is a geological phenomenon which includes a wide range of ground movement, including rock falls. Typically, the action of gravity is the primary driving force for a landslide to occur though in this case there was another contributing factor which affected the original slope stability: the landslide required an \"earthquake trigger\" before being released.\nPounding against adjacent building. This is a photograph of the collapsed five-story tower, St. Joseph's Seminary, Los Altos, California which resulted in one fatality. During Loma Prieta earthquake, the tower pounded against the independently vibrating adjacent building behind. A possibility of pounding depends on both buildings' lateral displacements which should be accurately estimated and accounted for.\nAt Northridge earthquake, the Kaiser Permanente concrete frame office building had joints completely shattered, revealing inadequate confinement steel, which resulted in the second story collapse. In the transverse direction, composite end shear walls, consisting of two wythes of brick and a layer of shotcrete that carried the lateral load, peeled apart because of inadequate through-ties and failed.\n\nSliding off foundations effect of a relatively rigid residential building structure during 1987 Whittier Narrows earthquake. The magnitude 5.9 earthquake pounded the Garvey West Apartment building in Monterey Park, California and shifted its superstructure about 10 inches to the east on its foundation.\nIf a superstructure is not mounted on a base isolation system, its shifting on the basement should be prevented.\nReinforced concrete column burst at Northridge earthquake due to insufficient shear reinforcement mode which allows main reinforcement to buckle outwards. The deck unseated at the hinge and failed in shear. As a result, the La Cienega-Venice underpass section of the 10 Freeway collapsed.\nLoma Prieta earthquake: side view of reinforced concrete support-columns failure which triggered the upper deck collapse onto the lower deck of the two-level Cypress viaduct of Interstate Highway 880, Oakland, CA.\nRetaining wall failure at Loma Prieta earthquake in Santa Cruz Mountains area: prominent northwest-trending extensional cracks up to 12 cm (4.7 in) wide in the concrete spillway to Austrian Dam, the north abutment.\nGround shaking triggered soil liquefaction in a subsurface layer of sand, producing differential lateral and vertical movement in an overlying carapace of unliquified sand and silt. This mode of ground failure, termed lateral spreading, is a principal cause of liquefaction-related earthquake damage.\nSeverely damaged building of Agriculture Development Bank of China after 2008 Sichuan earthquake: most of the beams and pier columns are sheared. Large diagonal cracks in masonry and veneer are due to in-plane loads while abrupt settlement of the right end of the building should be attributed to a landfill which may be hazardous even without any earthquake.\nTwofold tsunami impact: sea waves hydraulic pressure and inundation. Thus, the Indian Ocean earthquake of December 26, 2004, with the epicenter off the west coast of Sumatra, Indonesia, triggered a series of devastating tsunamis, killing more than 230,000 people in eleven countries by inundating surrounding coastal communities with huge waves up to 30 meters (100 feet) high.\n\nEarthquake construction means implementation of seismic design to enable building and non-building structures to live through the anticipated earthquake exposure up to the expectations and in compliance with the applicable building codes.\n\nDesign and construction are intimately related. To achieve a good workmanship, detailing of the members and their connections should be as simple as possible. As any construction in general, earthquake construction is a process that consists of the building, retrofitting or assembling of infrastructure given the construction materials available.\n\nThe destabilizing action of an earthquake on constructions may be \"direct\" (seismic motion of the ground) or \"indirect\" (earthquake-induced landslides, soil liquefaction and waves of tsunami).\n\nA structure might have all the appearances of stability, yet offer nothing but danger when an earthquake occurs. The crucial fact is that, for safety, earthquake-resistant construction techniques are as important as quality control and using correct materials. \"Earthquake contractor\" should be registered in the state/province/country of the project location (depending on local regulations), bonded and insured .\n\nTo minimize possible losses, construction process should be organized with keeping in mind that earthquake may strike any time prior to the end of construction.\n\nEach construction project requires a qualified team of professionals who understand the basic features of seismic performance of different structures as well as construction management.\n\nAround thirty percent of the world's population lives or works in earth-made construction. Adobe type of mud bricks is one of the oldest and most widely used building materials. The use of adobe is very common in some of the world's most hazard-prone regions, traditionally across Latin America, Africa, Indian subcontinent and other parts of Asia, Middle East and Southern Europe.\n\nAdobe buildings are considered very vulnerable at strong quakes. However, multiple ways of seismic strengthening of new and existing adobe buildings are available.\n\nKey factors for the improved seismic performance of\nadobe construction are:\n\nLimestone is very common in architecture, especially in North America and Europe. Many landmarks across the world are made of limestone. Many medieval churches and castles in Europe are made of limestone and sandstone masonry. They are the long-lasting materials but their rather heavy weight is not beneficial for adequate seismic performance.\n\nApplication of modern technology to seismic retrofitting can enhance the survivability of unreinforced masonry structures. As an example, from 1973 to 1989, the Salt Lake City and County Building in Utah was exhaustively renovated and repaired with an emphasis on preserving historical accuracy in appearance. This was done in concert with a seismic upgrade that placed the weak sandstone structure on base isolation foundation to better protect it from earthquake damage.\n\nTimber framing dates back thousands of years, and has been used in many parts of the world during various periods such as ancient Japan, Europe and medieval England in localities where timber was in good supply and building stone and the skills to work it were not.\n\nThe use of timber framing in buildings provides their complete skeletal framing which offers some structural benefits as the timber frame, if properly engineered, lends itself to better \"seismic survivability\".\n\nLight-frame structures usually gain seismic resistance from rigid plywood shear walls and wood structural panel diaphragms. Special provisions for seismic load-resisting systems for all engineered wood structures requires consideration of diaphragm ratios, horizontal and vertical diaphragm shears, and connector/fastener values. In addition, collectors, or drag struts, to distribute shear along a diaphragm length are required.\n\nA construction system where steel reinforcement is embedded in the mortar joints of masonry or placed in holes and after filled with concrete or grout is called reinforced masonry.\n\nThe devastating 1933 Long Beach earthquake revealed that masonry construction should be improved immediately. Then, the California State Code made the reinforced masonry mandatory.\n\nThere are various practices and techniques to achieve reinforced masonry. The most common type is the reinforced hollow unit masonry. The effectiveness of both vertical and horizontal reinforcement strongly depends on the type and quality of the masonry, i.e. masonry units and mortar.\n\nTo achieve a ductile behavior of masonry, it is necessary that the shear strength of the wall is greater than the flexural strength.\n\nReinforced concrete is concrete in which steel reinforcement bars (rebars) or fibers have been incorporated to strengthen a material that would otherwise be brittle. It can be used to produce beams, columns, floors or bridges.\n\nPrestressed concrete is a kind of reinforced concrete used for overcoming concrete's natural weakness in tension. It can be applied to beams, floors or bridges with a longer span than is practical with ordinary reinforced concrete. Prestressing tendons (generally of high tensile steel cable or rods) are used to provide a clamping load which produces a compressive stress that offsets the tensile stress that the concrete compression member would, otherwise, experience due to a bending load.\n\nTo prevent catastrophic collapse in response earth shaking (in the interest of life safety), a traditional reinforced concrete frame should have ductile joints. Depending upon the methods used and the imposed seismic forces, such buildings may be immediately usable, require extensive repair, or may have to be demolished.\n\nPrestressed structure is the one whose overall integrity, stability and security depend, primarily, on a \"prestressing\". \"Prestressing\" means the intentional creation of permanent stresses in a structure for the purpose of improving its performance under various service conditions.\nThere are the following basic types of prestressing:\nToday, the concept of prestressed structure is widely engaged in design of buildings, underground structures, TV towers, power stations, floating storage and offshore facilities, nuclear reactor vessels, and numerous kinds of bridge systems.\n\nA beneficial idea of \"prestressing\" was, apparently, familiar to the ancient Rome architects; look, e.g., at the tall attic wall of Colosseum working as a stabilizing device for the wall piers beneath.\n\nSteel structures are considered mostly earthquake resistant but some failures have occurred. A great number of welded steel moment-resisting frame buildings, which looked earthquake-proof, surprisingly experienced brittle behavior and were hazardously damaged in the 1994 Northridge earthquake. After that, the Federal Emergency Management Agency (FEMA) initiated development of repair techniques and new design approaches to minimize damage to steel moment frame buildings in future earthquakes.\n\nFor structural steel seismic design based on Load and Resistance Factor Design (LRFD) approach, it is very important to assess ability of a structure to develop and maintain its bearing resistance in the inelastic range. A measure of this ability is ductility, which may be observed in a \"material itself\", in a \"structural element\", or to a \"whole structure\".\n\nAs a consequence of Northridge earthquake experience, the American Institute of Steel Construction has introduced AISC 358 \"Pre-Qualified Connections for Special and intermediate Steel Moment Frames.\" The AISC Seismic Design Provisions require that all Steel Moment Resisting Frames employ either connections contained in AISC 358, or the use of connections that have been subjected to pre-qualifying cyclic testing.\n\nEarthquake loss estimation is usually defined as a \"Damage Ratio\" (DR) which is a ratio of the earthquake damage repair cost to the total value of a building. \"Probable Maximum Loss\" (PML) is a common term used for earthquake loss estimation, but it lacks a precise definition. In 1999, ASTM E2026 'Standard Guide for the Estimation of Building Damageability in Earthquakes' was produced in order to standardize the nomenclature for seismic loss estimation, as well as establish guidelines as to the review process and qualifications of the reviewer.\n\nEarthquake loss estimations are also referred to as \"Seismic Risk Assessments\". The risk assessment process generally involves determining the probability of various ground motions coupled with the vulnerability or damage of the building under those ground motions. The results are defined as a percent of building replacement value.\n\n\n"}
{"id": "37355300", "url": "https://en.wikipedia.org/wiki?curid=37355300", "title": "Enterobacteria phage PsP3", "text": "Enterobacteria phage PsP3\n\nEnterobacteria phage PsP3 is a virus of the family Myoviridae, genus \"P2likevirus\".\n\nAs a member of the group I of the Baltimore classification, \"Enterobacteria phage PsP3\" is a dsDNA viruses. All the family Myoviridae members share a nonenveloped morphology consisting of a head and a tail separated by a neck. Its genome is linear. The propagation of the virions includes the attaching to a host cell (a bacterium, as \"Enterobacteria phage PsP3\" is a bacteriophage) and the injection of the double stranded DNA; the host transcribes and translates it to manufacture new particles. To replicate its genetic content requires host cell DNA polymerases and, hence, the process is highly dependent on the cell cycle.\n"}
{"id": "13118658", "url": "https://en.wikipedia.org/wiki?curid=13118658", "title": "Farmland preservation", "text": "Farmland preservation\n\nFarmland preservation is a joint effort by non-governmental organizations and local governments to set aside and protect examples of a region's farmland for the use, education, and enjoyment of future generations. It is often a part of regional planning and national historic preservation.\n\nNew Jersey passed the Farmland Assessment Act of 1964 to mitigate the loss of farmland to rapid suburban development through the use of favorable tax assessments. But by the late 1970s, the value of farmland had outstripped the tax benefits of the act, so the state purchased deed restrictions on farms through the Agriculture Retention and Development Act of 1981. \n\nRegional efforts in Monmouth County, New Jersey include the Navesink Highlands Greenway, a project of the Monmouth County Farmland Preservation Program, which, along with the Monmouth Conservation Foundation, purchased the development rights of the Holly Crest Farm in Middletown in September 2008 for US $2.5 million. Over 20 percent of county farmlands and open spaces are permanently preserved.\n\nAmerican Farmland Trust was established in 1980 to preserve farmland and promote sustainable farming practices.\n\nThe Genesee Valley Conservancy was founded in New York in 1990.\n\nFarmland protection refers to programs in the United States, operated mostly at state and local levels by government agencies or private entities such as land trusts, that are designed to limit conversion of agricultural land to other uses that otherwise might have been more financially attractive to the land owner. Every state provides tax relief through differential (preferential) assessment, and has right-to-farm laws. Less common approaches include establishing agricultural districts, using zoning to protect agricultural land, purchasing development rights, and transferable development rights.\n\nConservation easement is one approach used to manage protected farms.\n\nA transferable development rights program offers landowners financial incentives or bonuses for the conservation and maintenance of agricultural land. Land developers can purchase the development rights of certain properties within a designated \"sending district\" and transfer the rights to another \"receiving district\" to increase the density of their new development. A widely-noted example of a sending district is the Montgomery County, Maryland Agricultural Reserve.\n\n\n\n"}
{"id": "11545", "url": "https://en.wikipedia.org/wiki?curid=11545", "title": "Feedback", "text": "Feedback\n\nFeedback occurs when outputs of a system are routed back as inputs as part of a chain of cause-and-effect that forms a circuit or loop. The system can then be said to \"feed back\" into itself. The notion of cause-and-effect has to be handled carefully when applied to feedback systems:\n\nSelf-regulating mechanisms have existed since antiquity, and the idea of feedback had started to enter economic theory in Britain by the eighteenth century, but it wasn't at that time recognized as a universal abstraction and so didn't have a name.\n\nThe verb phrase \"to feed back\", in the sense of \"returning to an earlier position\" in a mechanical process, was in use in the US by the 1860s, and in 1909, Nobel laureate Karl Ferdinand Braun used the term \"feed-back\" as a noun to refer to (undesired) \"coupling\" between components of an electronic circuit.\n\nBy the end of 1912, researchers using early electronic amplifiers (audions) had discovered that deliberately coupling part of the output signal back to the input circuit would boost the amplification (through regeneration), but would also cause the audion to howl or sing. This action of feeding back of the signal from output to input gave rise to the use of the term \"feedback\" as a distinct word by 1920.\n\nOver the years there has been some dispute as to the best definition of feedback. According to Ashby (1956), mathematicians and theorists interested in the \"principles\" of feedback mechanisms prefer the definition of \"circularity of action\", which keeps the theory simple and consistent. For those with more \"practical\" aims, feedback should be a deliberate effect via some more tangible connection.\nFocusing on uses in management theory, Ramaprasad (1983) defines feedback generally as \"...information about the gap between the actual level and the reference level of a system parameter\" that is used to \"alter the gap in some way.\" He emphasizes that the information by itself is not feedback unless translated into action.\n\nThere are two types of feedback: positive feedback and negative feedback.\n\nAs an example of negative feedback, the diagram might represent a cruise control system in a car, for example, that matches a target speed such as the speed limit. The controlled system is the car; its input includes the combined torque from the engine and from the changing slope of the road (the disturbance). The car's speed (status) is measured by a speedometer. The error signal is the departure of the speed as measured by the speedometer from the target speed (set point). This measured error is interpreted by the controller to adjust the accelerator, commanding the fuel flow to the engine (the effector). The resulting change in engine torque, the feedback, combines with the torque exerted by the changing road grade to reduce the error in speed, minimizing the road disturbance.\n\nThe terms \"positive\" and \"negative\" were first applied to feedback prior to WWII. The idea of positive feedback was already current in the 1920s with the introduction of the regenerative circuit. Friis and Jensen (1924) described regeneration in a set of electronic amplifiers as a case where \"the \"feed-back\" action is positive\" in contrast to negative feed-back action, which they mention only in passing. Harold Stephen Black's classic 1934 paper first details the use of negative feedback in electronic amplifiers. According to Black:\nAccording to Mindell (2002) confusion in the terms arose shortly after this:\nEven prior to the terms being applied, James Clerk Maxwell had described several kinds of \"component motions\" associated with the centrifugal governors used in steam engines, distinguishing between those that lead to a continual \"increase\" in a disturbance or the amplitude of an oscillation, and those that lead to a \"decrease\" of the same.\n\nThe terms positive and negative feedback are defined in different ways within different disciplines.\n\n\nThe two definitions may cause confusion, such as when an incentive (reward) is used to boost poor performance (narrow a gap). Referring to definition 1, some authors use alternative terms, replacing \"positive/negative\" with \"self-reinforcing/self-correcting\", \"reinforcing/balancing\", \"discrepancy-enhancing/discrepancy-reducing\" or \"regenerative/degenerative\" respectively. And for definition 2, some authors advocate describing the action or effect as positive/negative \"reinforcement\" or \"punishment\" rather than feedback.\nYet even within a single discipline an example of feedback can be called either positive or negative, depending on how values are measured or referenced.\n\nThis confusion may arise because feedback can be used for either \"informational\" or \"motivational\" purposes, and often has both a \"qualitative\" and a \"quantitative\" component. As Connellan and Zemke (1993) put it:\nWhile simple systems can sometimes be described as one or the other type, many systems with feedback loops cannot be so easily designated as simply positive or negative, and this is especially true when multiple loops are present.\n\nIn general, feedback systems can have many signals fed back and the feedback loop frequently contain mixtures of positive and negative feedback where positive and negative feedback can dominate at different frequencies or different points in the state space of a system.\n\nThe term bipolar feedback has been coined to refer to biological systems where positive and negative feedback systems can interact, the output of one affecting the input of another, and vice versa.\n\nSome systems with feedback can have very complex behaviors such as chaotic behaviors in non-linear systems, while others have much more predictable behaviors, such as those that are used to make and design digital systems.\n\nFeedback is used extensively in digital systems. For example, binary counters and similar devices employ feedback where the current state and inputs are used to calculate a new state which is then fed back and clocked back into the device to update it.\n\nBy using feedback properties, the behavior of a system can be altered to meet the needs of an application; systems can be made stable, responsive or held constant. It is shown that dynamical systems with a feedback experience an adaptation to the edge of chaos.\n\nIn biological systems such as organisms, ecosystems, or the biosphere, most parameters must stay under control within a narrow range around a certain optimal level under certain environmental conditions. The deviation of the optimal value of the controlled parameter can result from the changes in internal and external environments. A change of some of the environmental conditions may also require change of that range to change for the system to function. The value of the parameter to maintain is recorded by a reception system and conveyed to a regulation module via an information channel. An example of this is insulin oscillations.\n\nBiological systems contain many types of regulatory circuits, both positive and negative. As in other contexts, \"positive\" and \"negative\" do not imply that the feedback causes \"good\" or \"bad\" effects. A negative feedback loop is one that tends to slow down a process, whereas the positive feedback loop tends to accelerate it. The mirror neurons are part of a social feedback system, when an observed action is \"mirrored\" by the brain—like a self-performed action.\n\nNormal tissue integrity is preserved by feedback interactions between diverse cell types mediated by adhesion molecules and secreted molecules that act as mediators; failure of key feedback mechanisms in cancer disrupts tissue function.\nIn an injured or infected tissue, inflammatory mediators elicit feedback responses in cells, which alter gene expression, and change the groups of molecules expressed and secreted, including molecules that induce diverse cells to cooperate and restore tissue structure and function. This type of feedback is important because it enables coordination of immune responses and recovery from infections and injuries. During cancer, key elements of this feedback fail. This disrupts tissue function and immunity.\n\nMechanisms of feedback were first elucidated in bacteria, where a nutrient elicits changes in some of their metabolic functions.\nFeedback is also central to the operations of genes and gene regulatory networks. Repressor (see Lac repressor) and activator proteins are used to create genetic operons, which were identified by Francois Jacob and Jacques Monod in 1961 as \"feedback loops\". These feedback loops may be positive (as in the case of the coupling between a sugar molecule and the proteins that import sugar into a bacterial cell), or negative (as is often the case in metabolic consumption).\n\nOn a larger scale, feedback can have a stabilizing effect on animal populations even when profoundly affected by external changes, although time lags in feedback response can give rise to predator-prey cycles.\n\nIn zymology, feedback serves as regulation of activity of an enzyme by its direct product(s) or downstream metabolite(s) in the metabolic pathway (see Allosteric regulation).\n\nThe hypothalamic–pituitary–adrenal axis is largely controlled by positive and negative feedback, much of which is still unknown.\n\nIn psychology, the body receives a stimulus from the environment or internally that causes the release of hormones. Release of hormones then may cause more of those hormones to be released, causing a positive feedback loop. This cycle is also found in certain behaviour. For example, \"shame loops\" occur in people who blush easily. When they realize that they are blushing, they become even more embarrassed, which leads to further blushing, and so on.\n\nThe climate system is characterized by strong positive and negative feedback loops between processes that affect the state of the atmosphere, ocean, and land. A simple example is the ice-albedo positive feedback loop whereby melting snow exposes more dark ground (of lower albedo), which in turn absorbs heat and causes more snow to melt.\n\nFeedback is extensively used in control theory, using a variety of methods including state space (controls), full state feedback (also known as pole placement), and so forth. Note that in the context of control theory, \"feedback\" is traditionally assumed to specify \"negative feedback\".\n\nThe most common general-purpose controller using a control-loop feedback mechanism is a proportional-integral-derivative (PID) controller. Heuristically, the terms of a PID controller can be interpreted as corresponding to time: the proportional term depends on the \"present\" error, the integral term on the accumulation of \"past\" errors, and the derivative term is a prediction of \"future\" error, based on current rate of change.\n\nIn ancient times, the float valve was used to regulate the flow of water in Greek and Roman water clocks; similar float valves are used to regulate fuel in a carburettor and also used to regulate tank water level in the flush toilet.\n\nThe Dutch inventor Cornelius Drebbel (1572-1633) built thermostats (c1620) to control the temperature of chicken incubators and chemical furnaces. In 1745, the windmill was improved by blacksmith Edmund Lee, who added a fantail to keep the face of the windmill pointing into the wind. In 1787, Thomas Mead regulated the rotation speed of a windmill by using a centrifugal pendulum to adjust the distance between the bedstone and the runner stone (i.e., to adjust the load).\n\nThe use of the centrifugal governor by James Watt in 1788 to regulate the speed of his steam engine was one factor leading to the Industrial Revolution. Steam engines also use float valves and pressure release valves as mechanical regulation devices. A mathematical analysis of Watt's governor was done by James Clerk Maxwell in 1868.\n\nThe \"Great Eastern\" was one of the largest steamships of its time and employed a steam powered rudder with feedback mechanism designed in 1866 by John McFarlane Gray. Joseph Farcot coined the word \"servo\" in 1873 to describe steam-powered steering systems. Hydraulic servos were later used to position guns. Elmer Ambrose Sperry of the Sperry Corporation designed the first autopilot in 1912. Nicolas Minorsky published a theoretical analysis of automatic ship steering in 1922 and described the PID controller.\n\nInternal combustion engines of the late 20th century employed mechanical feedback mechanisms such as the vacuum timing advance but mechanical feedback was replaced by electronic engine management systems once small, robust and powerful single-chip microcontrollers became affordable.\n\nThe use of feedback is widespread in the design of electronic amplifiers, oscillators, and stateful logic circuit elements such as flip-flops and counters. Electronic feedback systems are also very commonly used to control mechanical, thermal and other physical processes.\n\nIf the signal is inverted on its way round the control loop, the system is said to have \"negative feedback\"; otherwise, the feedback is said to be \"positive\". Negative feedback is often deliberately introduced to increase the stability and accuracy of a system by correcting or reducing the influence of unwanted changes. This scheme can fail if the input changes faster than the system can respond to it. When this happens, the lag in arrival of the correcting signal can result in overcorrection, causing the output to oscillate or \"hunt\". While often an unwanted consequence of system behaviour, this effect is used deliberately in electronic oscillators.\n\nHarry Nyquist at Bell Labs derived the Nyquist stability criterion for determining the stability of feedback systems. An easier method, but less general, is to use Bode plots developed by Hendrik Bode to determine the gain margin and phase margin. Design to ensure stability often involves frequency compensation to control the location of the poles of the amplifier.\n\nElectronic feedback loops are used to control the output of electronic devices, such as amplifiers. A feedback loop is created when all or some portion of the output is fed back to the input. A device is said to be operating \"open loop\" if no output feedback is being employed and \"closed loop\" if feedback is being used.\n\nWhen two or more amplifiers are cross-coupled using positive feedback, complex behaviors can be created. These \"multivibrators\" are widely used and include:\n\n\nA Negative feedback occurs when the fed-back output signal has a relative phase of 180° with respect to the input signal (upside down). This situation is sometimes referred to as being \"out of phase\", but that term also is used to indicate other phase separations, as in \"90° out of phase\". Negative feedback can be used to correct output errors or to desensitize a system to unwanted fluctuations. In feedback amplifiers, this correction is generally for waveform distortion reduction or to establish a specified gain level. A general expression for the gain of a negative feedback amplifier is the asymptotic gain model.\n\nPositive feedback occurs when the fed-back signal is in phase with the input signal. Under certain gain conditions, positive feedback reinforces the input signal to the point where the output of the device oscillates between its maximum and minimum possible states. Positive feedback may also introduce hysteresis into a circuit. This can cause the circuit to ignore small signals and respond only to large ones. It is sometimes used to eliminate noise from a digital signal. Under some circumstances, positive feedback may cause a device to latch, i.e., to reach a condition in which the output is locked to its maximum or minimum state. This fact is very widely used in digital electronics to make bistable circuits for volatile storage of information.\n\nThe loud squeals that sometimes occurs in audio systems, PA systems, and rock music are known as audio feedback. If a microphone is in front of a loudspeaker that it is connected to, sound that the microphone picks up comes out of the speaker, and is picked up by the microphone and re-amplified. If the loop gain is sufficient, howling or squealing at the maximum power of the amplifier is possible.\n\nAn electronic oscillator is an electronic circuit that produces a periodic, oscillating electronic signal, often a sine wave or a square wave. Oscillators convert direct current (DC) from a power supply to an alternating current signal. They are widely used in many electronic devices. Common examples of signals generated by oscillators include signals broadcast by radio and television transmitters, clock signals that regulate computers and quartz clocks, and the sounds produced by electronic beepers and video games.\n\nOscillators are often characterized by the frequency of their output signal:\n\n\nOscillators designed to produce a high-power AC output from a DC supply are usually called inverters.\n\nThere are two main types of electronic oscillator: the linear or harmonic oscillator and the nonlinear or relaxation oscillator.\n\nA latch or a flip-flop is a circuit that has two stable states and can be used to store state information. They typically constructed using feedback that crosses over between two arms of the circuit, to provide the circuit with a state. The circuit can be made to change state by signals applied to one or more control inputs and will have one or two outputs. It is the basic storage element in sequential logic. Latches and flip-flops are fundamental building blocks of digital electronics systems used in computers, communications, and many other types of systems.\n\nLatches and flip-flops are used as data storage elements. Such data storage can be used for storage of \"state\", and such a circuit is described as sequential logic. When used in a finite-state machine, the output and next state depend not only on its current input, but also on its current state (and hence, previous inputs). It can also be used for counting of pulses, and for synchronizing variably-timed input signals to some reference timing signal.\n\nFlip-flops can be either simple (transparent or opaque) or clocked (synchronous or edge-triggered). Although the term flip-flop has historically referred generically to both simple and clocked circuits, in modern usage it is common to reserve the term \"flip-flop\" exclusively for discussing clocked circuits; the simple ones are commonly called \"latches\".\n\nUsing this terminology, a latch is level-sensitive, whereas a flip-flop is edge-sensitive. That is, when a latch is enabled it becomes transparent, while a flip flop's output only changes on a single type (positive going or negative going) of clock edge.\n\nFeedback loops provide generic mechanisms for controlling the running, maintenance, and evolution of software and computing systems. Feedback-loops are important models in the engineering of adaptive software, as they define the behaviour of the interactions among the control elements over the adaptation process, to guarantee system properties at run-time. Feedback loops and foundations of control theory have been successfully applied to computing systems. In particular, they have been applied to the development of products such as IBM's Universal Database server and IBM Tivoli. From a software perspective, the autonomic (MAPE, monitor analyze plan execute) loop proposed by researchers of IBM is another valuable contribution to the application of feedback loops to the control of dynamic properties and the design and evolution of autonomic software systems.\n\nFeedback is also a useful design principle for designing user interfaces.\n\nVideo feedback is the video equivalent of acoustic feedback. It involves a loop between a video camera input and a video output, e.g., a television screen or monitor. Aiming the camera at the display produces a complex video image based on the feedback.\n\nThe stock market is an example of a system prone to oscillatory \"hunting\", governed by positive and negative feedback resulting from cognitive and emotional factors among market participants. For example:\n\n\nGeorge Soros used the word \"reflexivity,\" to describe feedback in the financial markets and developed an investment theory based on this principle.\n\nThe conventional economic equilibrium model of supply and demand supports only ideal linear negative feedback and was heavily criticized by Paul Ormerod in his book \"The Death of Economics\", which, in turn, was criticized by traditional economists. This book was part of a change of perspective as economists started to recognise that chaos theory applied to nonlinear feedback systems including financial markets.\n\n"}
{"id": "26979054", "url": "https://en.wikipedia.org/wiki?curid=26979054", "title": "Fullerian Professor of Physiology", "text": "Fullerian Professor of Physiology\n\nThe Fullerian Chairs at the Royal Institution in London, England, were established by John 'Mad Jack' Fuller.\n\n"}
{"id": "15375090", "url": "https://en.wikipedia.org/wiki?curid=15375090", "title": "Galactocentric distance", "text": "Galactocentric distance\n\nGalactocentric distance is a star's distance from the center of a galaxy. For example, our Sun is about 27 kly (8kpc) from the center of the Milky Way. Galactocentric distance may also refer to a galaxy's distance from another galaxy.\n\n"}
{"id": "45341503", "url": "https://en.wikipedia.org/wiki?curid=45341503", "title": "Hand heart", "text": "Hand heart\n\nA hand heart is a gesture in which a person forms a heart shape using their fingers. Hand hearts are usually more popular within the young generation.\n\nThe sign has been popularied by musical artists, such as Taylor Swift, who has used it in her live shows. In South Korea it is a known symbol among K-pop stars and their fans and has popularly been performed using the thumb and index finger. The thumb and index finger gesture has become popular across Asia due to the popularity of Kpop and Korean dramas.\n\nHand hearts are widely used everywhere in the world, and appear often in pictures taken by individuals and celebrities. \n\nDiscovered by Engadget team, Google filed a patent in July 2011 that allowed Google Glass users to use the hand heart in front of any objects, and the gadget automatically recognise the object, takes a picture, and send it to social networks as a \"liked\" image.\n"}
{"id": "21268357", "url": "https://en.wikipedia.org/wiki?curid=21268357", "title": "Hatteras Weather Bureau Station", "text": "Hatteras Weather Bureau Station\n\nThe Hatteras Weather Bureau Station is a wood-frame building in Hatteras, North Carolina built in 1901 for what was then called the U.S. Weather Bureau. The then-remote location on the Outer Banks of North Carolina provided data on conditions in the Atlantic Ocean from a fixed location that was farther into the ocean environment than any on the Atlantic coast. The building served as a weather station from 1902 to 1946, when it was converted to living quarters for Weather Bureau personnel. In 1952 the property was turned over to the U.S. Coast Guard, which used it until 1958, when it was transferred to the National Park Service for use by Cape Hatteras National Seashore. From 1958 to 1976 the building was used as a research station, first by Duke University and later by North Carolina State University for investigations concerning marine invertebrates.\n\nIt was listed on the National Register of Historic Places in 1978.\n"}
{"id": "26066810", "url": "https://en.wikipedia.org/wiki?curid=26066810", "title": "Hilbrand J. Groenewold", "text": "Hilbrand J. Groenewold\n\nHilbrand Johannes \"Hip\" Groenewold (1910–1996) was a Dutch theoretical physicist who pioneered the largely operator-free formulation of quantum mechanics in phase space known as phase-space quantization.\n\nGroenewold was born on 29 June 1910 in Muntendam in the province of Groningen. He graduated from the University of Groningen, with a major in physics and minors in mathematics and mechanics in 1934. After a visit to Cambridge to interact with John von Neumann (1934-5) on the links between classical and quantum mechanics, and a checkered career working with Frits Zernike in Groningen, then Leiden, the Hague, De Bilt, and several addresses in the North of the Netherlands during World War II, he earned his Ph.D. degree in 1946, under the tutelage of Léon Rosenfeld at Utrecht University. In 1951, he obtained a position in Groningen in theoretical physics, first as a lecturer, then as a senior lecturer, and finally as a professor in 1955. He was the initiator and organizer of the Vosbergen Conference in the Netherlands for over two decades.\n\nHis 1946 thesis paper laid the foundations of quantum mechanics in phase space, in unwitting parallel with J. E. Moyal. This treatise was the first to achieve full understanding of the Wigner–Weyl transform as an invertible transform, rather than as an unsatisfactory quantization rule. Significantly, this work further formulated and first appreciated the all-important star-product, the cornerstone of this formulation of the theory, ironically often also associated with Moyal's name, even though it is not featured in Moyal's papers and was not fully understood by Moyal. \n\nMoreover, Groenewold first understood and demonstrated that the Moyal bracket is isomorphic to the quantum commutator, and thus that the latter \"cannot be made to faithfully correspond\" to the Poisson bracket, as had been envisioned by Paul Dirac. This observation and his counterexamples contrasting Poisson brackets to commutators have been generalized and codified to what is now known as the \"Groenewold – Van Hove theorem\". See Groenewold's theorem for one version.\n\n"}
{"id": "48316762", "url": "https://en.wikipedia.org/wiki?curid=48316762", "title": "List of Explorers program missions", "text": "List of Explorers program missions\n\nList of Explorer program missions is a list of missions and spacecraft of the NASA Explorer program. This list was originally based on data from the NSSDC master catalog listing. This does not include Missions of Opportunity (MO), but does include some spacecraft with launch failures or were cancelled.\n"}
{"id": "24925308", "url": "https://en.wikipedia.org/wiki?curid=24925308", "title": "List of Russian inventors", "text": "List of Russian inventors\n\nThis is a list of inventors from the Russian Federation, Soviet Union, Russian Empire, Tsardom of Russia and Grand Duchy of Moscow, including both ethnic Russians and people of other ethnicities.\n\nThis list also includes those who were born in Russia or its predecessor states but later emigrated, and those who were born elsewhere but immigrated to the country or worked there for a considerable time, (producing inventions on Russian soil).\n\nFor Russian inventions in chronological order, see the Timeline of Russian inventions and technology records.\n\n"}
{"id": "42245410", "url": "https://en.wikipedia.org/wiki?curid=42245410", "title": "List of environmental accidents in the fossil fuel industry in Australia", "text": "List of environmental accidents in the fossil fuel industry in Australia\n\n\n\n"}
{"id": "4759160", "url": "https://en.wikipedia.org/wiki?curid=4759160", "title": "List of extremely hazardous substances", "text": "List of extremely hazardous substances\n\nThis is the list of extremely hazardous substances defined in Section 302 of the U.S. Emergency Planning and Community Right-to-Know Act (42 U.S.C. 11002). The list can be found as an appendix to 40 C.F.R. 355. Updates as of 2006 can be seen on the Federal Register, 71 FR 47121 (August 16, 2006).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "41894825", "url": "https://en.wikipedia.org/wiki?curid=41894825", "title": "List of movements of the human body", "text": "List of movements of the human body\n\nThe list below describes such skeletal movements as normally are possible in particular joints of the human body. Other animals have different degrees of movement at their respective joints; this is because of differences in positions of muscles and because structures peculiar to the bodies of humans and other species block motions unsuited to their anatomies. \n\nThe major muscles involved in retraction include the rhomboid major muscle, rhomboid minor muscle and trapezius muscle, whereas the major muscles involved in protraction include the serratus anterior and pectoralis minor muscles.\n\nThe muscles tibialis anterior and tibialis posterior invert the foot. Some sources also state that the triceps surae and extensor hallucis longus invert. Inversion occurs at the subtalar joint and transverse tarsal joint.\n\nEversion of the foot occurs at the subtalar joint. The muscles involved in this include Fibularis longus and fibularis brevis, which are innervated by the superficial fibular nerve. Some sources also state that the fibularis tertius everts.\n\nDorsiflexion of the foot: The muscles involved include those of the Anterior compartment of leg, specifically tibialis anterior muscle, extensor hallucis longus muscle, extensor digitorum longus muscle, and peroneus tertius. The range of motion for dorsiflexion indicated in the literature varies from 12.2 to 18 degrees. Foot drop is a condition, that occurs when dorsiflexion is difficult for an individual who is walking.\n\nPlantarflexion of the foot: Primary muscles for plantar flexion are situated in the Posterior compartment of leg, namely the superficial Gastrocnemius, Soleus and Plantaris (only weak participation), and the deep muscles Flexor hallucis longus, Flexor digitorum longus and Tibialis posterior. Muscles in the Lateral compartment of leg also weakly participate, namely the Fibularis longus and Fibularis brevis muscles. Those in the lateral compartment only have weak participation in plantar flexion though. The range of motion for plantar flexion is usually indicated in the literature as 30° to 40°, but sometimes also 50°. The nerves are primarily from the sacral spinal cord roots S1 and S2. Compression of S1 roots may result in weakness in plantarflexion; these nerves run from the lower back to the bottom of the foot. \n\n\"Pronation\" at the forearm is a rotational movement at the radioulnar joint, or of the foot at the subtalar and talocalcaneonavicular joints. For the forearm, when standing in the anatomical position, pronation will move the palm of the hand from an anterior-facing position to a posterior-facing position without an associated movement at the shoulder joint). This corresponds to a counterclockwise twist for the right forearm and a clockwise twist for the left (when viewed superiorly). In the forearm, this action is performed by pronator quadratus and pronator teres muscle. Brachioradialis puts the forearm into a midpronated/supinated position from either full pronation or supination. For the foot, pronation will cause the sole of the foot to face more laterally than when standing in the anatomical position.\n\nPronation of the foot is a compound movement that combines abduction, eversion, and dorsiflexion. Regarding posture, a pronated foot is one in which the heel bone angles inward and the arch tends to collapse. Pronation is the motion of the inner and outer ball of the foot with the heel bone. One is said to be \"knock-kneed\" if one has overly pronated feet. It flattens the arch as the foot strikes the ground in order to absorb shock when the heel hits the ground, and to assist in balance during mid-stance. If habits develop, this action can lead to foot pain as well as knee pain, shin splints, achilles tendinitis, posterior tibial tendinitis, piriformis syndrome, and plantar fasciitis..\n"}
{"id": "4245884", "url": "https://en.wikipedia.org/wiki?curid=4245884", "title": "List of spreadsheet software", "text": "List of spreadsheet software\n\nThe following is a list of spreadsheets.\n\n\n\n\n\n\n\n\n\n\n-* 32-bit addressable memory on Microsoft Windows, i.e. ~2.5 GB.\n\n\n"}
{"id": "24260834", "url": "https://en.wikipedia.org/wiki?curid=24260834", "title": "Master of biological sciences", "text": "Master of biological sciences\n\nA Master of Science in Biological Sciences is a specific degree to a Master's Degree in the field of the Biological Sciences. This is a higher degree taken up in the Graduate School provided by a university. This degree is usually specific to those who have accomplished their undergraduate studies in any field under the Natural Sciences.\n\nThe advancement in the study of this field offers a lot more specific fields of study such as Plant Biology, Molecular Biology, and Animal Biology. It requires at least 2 years of graduate studies. It may or may not require any thesis or research work, depending on the program offered by the university. There are laboratory works, lectures, and research works involved in this degree. There are also examinations given to the graduate students.\n\nAfter one has finished his Master in Biological Sciences, he can become a researcher, a professor for undergraduate studies, or he can also pursue a higher education in Doctoral Studies.\n"}
{"id": "5020850", "url": "https://en.wikipedia.org/wiki?curid=5020850", "title": "National Institute of Oceanography (Pakistan)", "text": "National Institute of Oceanography (Pakistan)\n\nThe National Institute of Oceanography (NIO), (), is a department of the Government of Pakistan and a major research institute of Ministry of Science and Technology (Pakistan). The NIO is a science and research executive organization located in Karachi, Sindh, Pakistan. The NIO' research and studies are funded by the Federal Government of Pakistan while the facilities are provided by the Sindh Government.\n\nNIO was established in 1981 through a Government Resolution to conduct multidisciplinary research in oceanography in the coastal and offshore areas of Pakistan (EEZ 24,000 km²). The institute has 30 qualified marine scientists working on ocean biology/productivity, marine chemistry and environment, physical oceanography/coastal hydraulics, marine geology and geophysics.\n\n\n\n"}
{"id": "19156682", "url": "https://en.wikipedia.org/wiki?curid=19156682", "title": "On Nuclear Terrorism", "text": "On Nuclear Terrorism\n\nIn his 2007 book On Nuclear Terrorism, author Michael A. Levi surveys the issue of nuclear terrorism and explores the decisions a terrorist leader might take in pursuing a nuclear plot. Levi points out the many obstacles that such a terrorist scheme may encounter, which in turn leads to a host of possible ways that any terrorist plan could be foiled.\n\nProfessor John Mueller's 2010 book \"Atomic Obsession: Nuclear Alarmism From Hiroshima to Al-Qaeda\" is an expansion of the same theme.\n\nMichael Levi is a Senior Fellow for energy and environment at the Council on Foreign Relations, New York.\n\n\n\n"}
{"id": "29367893", "url": "https://en.wikipedia.org/wiki?curid=29367893", "title": "Online communication between school and home", "text": "Online communication between school and home\n\nOnline communication between home and school is the use of digital telecommunication to convey information and ideas between teachers, students, parents, and school administrators. As the use of e-mail and the internet becomes even more widespread, these tools become more valuable and useful in education for the purposes of increasing learning for students, and facilitating conversations between students, parents, and schools.\n\nOnline communication emphasizes 21st century skills, self-directed learning, self-advocacy, global awareness, and thinking skills for learners. Utilizing online communication methods, schools help students develop Netiquette, and technical and computer skills. In addition, teachers can provide parents with frequent information about school programs and their children's progress through automated e-mails, official websites and learning management systems. This communication can be achieved either synchronously or asynchronously, providing greater time flexibility.\n\nWith online communication, learning may occur outside traditional school hours as students participate in collaborative activities, like reading and responding to peer posts in online forums, experiments, group projects, research papers, and current events assignments. In addition, online communication can connect a wide range of individuals and increase the diversity of perspectives that learners are exposed to.\n\nHowever, not all parents, students, or teachers have access to unlimited internet access or the digital technology necessary to participate in online communication, and it may be costly to initially implement the information technology, hardware, and software. Furthermore, schools must provide orientation to the online environments and technical support to ensure that all potential users are ready to participate. Teachers will also need to spend additional time online as active participants in the communication activities (e.g. act as the moderators of discussions). In addition, the immediacy of online communication can lead to students and parents' unreasonable expectations that teachers be 'on-call' at all times.\n\nOnline communication between parents and schools are online methods that serve as a platform for parents and teachers to exchange ideas. For teachers and administrators, online communication makes it easier to reach the parents and build the partnerships with parents.Online communication allows parents to receive real-time information about their child’s performance and activities at school, and flexible opportunities to ask questions and provide information to teachers and school administrators.\n\nIn this recent time of technology and modern educational sector, there are many ways to get instant connectivity with your university or college teachers. In the online education process the communication is playing a major role for both teachers and students. There are many online universities and colleges who offer their education services online and they create a good communication path where all the student and the teacher took participation to discuss with every other and this will also help to get problem resolve very early. For those professional employees who stop their studies and start doing work. With the online education colleges and universities now they have an opportunity to get complete their education and complete their academic profile by having a higher degree on their resume. If you have a years of work experience in any field, but your employer doesn’t recognize you just because of your degree then there is a good news for you that the online universities also giving you the chance for apply your degree on the basis of your work experience.\n\nCreating modes for online communication can increase parent participation in their children's education, which in turn increases students’ interest in their learning. Online communication increases parents’ understanding of classroom procedures, philosophies and policies. Parents then feel more involved in their child's school and more connected to the teacher. In general, online communication improves parents’ attitudes toward conferencing with teachers and administrators.\n\nThis style of communication allows for more asynchronous communication and greater flexibility. With online communication, parents can initiate conversations and express concerns to teachers and school officials easily. In addition, informal communication through online chatting or forums can reduce parents’ anxiety of meeting face-to-face with teachers and/or school officials. When possible, online communication can also offer comfort through anonymity.\n\nThough most of the time, teachers and parents want to establish communication, there are some challenges that teacher and parents need to face together. The most common challenges involve parent’s ability to use the software, their access to consistent internet access and language barriers. There may be financial costs incurred by the school, if they provide training or translation to parents in order to make online communication more inclusive.\n\nOnline communication between teachers and students facilitates the exchange of ideas and e-learning. Online communication allows students to access learning materials beyond school hours and develop relationships with peers and teachers.\n\nThe creation of Web 2.0 and social networks means that knowledge is now collective and readily available online for students to access and contribute to. Promoting online communication between teachers and students creates opportunities for students to receive feedback and assistance from teachers and peers outside the regular school day and classroom. Student can e-mail or post questions, add their opinions to peer-discussions, and check official websites for pertinent information.\n\nThrough online learning communities of teachers and peers, students can build relationships with other users and establish a sense of both connectedness and belonging. Some students, who are less likely to participate in face-to discussions, are more likely to participate in online discussions and activities. This online communication enhances the strength of the relationships between students and both their peers and teachers.\n\nStudent can demonstrate antipathy towards online communication or peer interdependency in internet forums. In order to be productive, online communication between students and teachers requires trust, interactivity, common expectations and shared goals. Some students expect teachers to be on-call 24 hours a day, 7 days a week, placing unreasonable expectations upon teachers. In addition, most students spend participate in online communication from home, which means that parent's help is often required, but may not be available. Finally, students may lack the technical skills or access to the technology necessary to involve themselves in online communication.\n\nTeachers have great responsibility in the establishment of online communication and communities with students, because of their leadership position. Several of their in-class characteristics must extend into the online environment, such as their ability to guide student behavior and learning. With online communication, teachers must model and demonstrate appropriate Netiquette throughout their persistent involvement. Teachers should also encourage their classes to evolve into learning communities in which group processes have the power to influence the behavior of individuals. These online environments should foster a sense of openness, friendliness, and trust, so that problem solving becomes a group function.\n\nTeachers and students can e-mail questions and answers to each other about course content and assignments. Schools and teachers can maintain official websites with important information about events, assignments, and resources that students can utilize outside class.\n\nBoth students and teachers can post messages in online forums as a part of homework assignments. In this way they can present different points of view that they don't have any chance to present in the classroom.\n\nOnline study groups allow students to maintain relationships with their peers from a distance. These study groups can be created within a classroom’s social networking site, allowing users to connect with each other directly, beyond typical chat rooms and forums.\n\nTeachers can develop virtual tours, virtual education, and virtual learning environment for their students in multi-user virtual environments (MUVE).\n\nThe most widely used online communication tool is e-mail between teachers, which provides opportunities for asynchronous communication, instantaneous distribution to a mass audience, mobile access, and file exchange. Teacher-created websites provide online access to administrative information, calendars, links, blogs, etc. Internet forums allow learners and teachers to articulate ideas, give and receive feedback, reflect on the perspectives of others, and receive clarification of concepts. social networking sites are used for focused and open communication between users. Blogs allow individuals to express their ideas in greater detail and with multimedia.\n\nVarious course management systems are designed specifically for facilitating online communication in education. Effective course management software tends to include more information, widgets, functions, and customization options than teacher-created websites.\n\nMost course management systems include:\n\nNotable CMS Software include:\n\n\n"}
{"id": "372619", "url": "https://en.wikipedia.org/wiki?curid=372619", "title": "Optical engineering", "text": "Optical engineering\n\nOptical engineering is the field of study that focuses on applications of optics. Optical engineers design components of optical instruments such as lenses, microscopes, telescopes, and other equipment that utilizes the properties of light. Other devices include optical sensors and measurement systems, lasers, fiber optic communication systems, optical disc systems (e.g. CD, DVD), etc. \n\nBecause optical engineers want to design and build devices that make light do something useful, they must understand and apply the science of optics in substantial detail, in order to know what is physically possible to achieve (physics and chemistry). However, they also must know what is practical in terms of available technology, materials, costs, design methods, etc. As with other fields of engineering, computers are important to many (perhaps most) optical engineers. They are used with instruments, for simulation, in design, and for many other applications. Engineers often use general computer tools such as spreadsheets and programming languages, and they make frequent use of specialized optical software designed specifically for their field.\n\nOptical engineering metrology uses optical methods to measure micro-vibrations with instruments like the laser speckle interferometer or to measure the properties of the various masses with instruments measuring refraction.\n\n\n"}
{"id": "48382760", "url": "https://en.wikipedia.org/wiki?curid=48382760", "title": "Pathological Altruism", "text": "Pathological Altruism\n\nPathological Altruism is a book edited by Barbara Oakley, Ariel Knafo, Guruprasad Madhavan, and David Sloan Wilson. It was published on 5 January 2012 by Oxford University Press, and contains 31 academic papers. Oakley defines pathological altruism as \"altruism in which attempts to promote the welfare of others instead result in unanticipated harm.\"\n\nThe book comprises a collection of essays which discuss negative aspects of altruism and empathy towards others, such as when altruism hurts the altruist, is taken to an unhealthy extreme, or causes more harm than good. Examples given include depression and burnout seen in healthcare professionals, an unhealthy focus on others to the detriment of one's own needs, hoarding of animals, and ineffective philanthropic and social programs that ultimately worsen the situations they are meant to aid. It is considered the first book to explore negative aspects of altruism and empathy.\n\nAccording to Oakley, anorexia, supporting addictions of other people (codependency), animal hoarding, depression, guilt and self-righteousness can be pathological altruism. Oakley has also stated that suicide bombings and genocides can be caused by pathological altruism, when perpetrators of these acts believe they are behaving altruistically towards those who share their ideology. \nOakley further states that some people are naturally \"hypersensitive\" or they have an excessive desire to \"help\" others. According to Oakley, such people are convinced that they are helping others without considering the practical results of their \"help\".\n\n\nThe book was widely reviewed, including reviews in the \"New Scientist\", and \"The Independent\". \"Nursing Standard\" said \"I recommend this book to health professionals looking for a deeper understanding of altruism and its motivation. The arguments are clear and scholarly, and supported by a wealth of references.\"\n\nWriting in \"The New York Times\", Natalie Angier called book a \"scholarly yet surprisingly sprightly volume\". She wrote, pathological altruism is not limited to showcase acts of self-sacrifice... The book is the first comprehensive treatment of the idea that when ostensibly generous 'how can I help you?' behavior is taken to extremes, misapplied or stridently rhapsodized, it can become unhelpful, unproductive and even destructive. Selflessness gone awry may play a role in a broad variety of disorders, including anorexia and animal hoarding, women who put up with abusive partners and men who abide alcoholic ones. Because a certain degree of selfless behavior is essential to the smooth performance of any human group, selflessness run amok can crop up in political contexts. It fosters the exhilarating sensation of righteous indignation, the belief in the purity of your team and your cause and the perfidiousness of all competing teams and causes.\n\n\n"}
{"id": "73357", "url": "https://en.wikipedia.org/wiki?curid=73357", "title": "Pioneer Venus project", "text": "Pioneer Venus project\n\nThe Pioneer Venus project was part of the Pioneer program consisting of two spacecraft, the Pioneer Venus Orbiter and the Pioneer Venus Multiprobe, launched to Venus in 1978. The program was managed by NASA's Ames Research Center. An official Ames documentary film titled \"Venus Pioneers\" is visible on George Van Valkenburg's YouTube Channel. \n\nThe Pioneer Venus Orbiter entered orbit around Venus on December 4, 1978, and performed observations to characterize the atmosphere and surface of Venus. It continued to transmit data until October 1992.\n\nThe Pioneer Venus Multiprobe deployed four small probes into the Venusian atmosphere on December 9, 1978. All four probes transmitted data throughout their descent to the surface. One probe survived landing and transmitted data from the surface for over an hour.\n\nThe Pioneer mission consisted of two components, launched separately: an orbiter and a multiprobe.\n\nThe orbiter was launched on 20 May 1978 with an Atlas-Centaur rocket. The orbiter's mass was . The Pioneer Venus Orbiter was inserted into an elliptical orbit around Venus on December 4, 1978. It carried 17 experiments (with a total mass of 45 kg): \n\nIn May 1992 the orbiter began the final phase of its mission, in which the periapsis was held between 150 and 250 km until the fuel ran out and atmospheric entry destroyed the spacecraft in August 1992.\n\nThe Pioneer Venus Multiprobe was launched on August 8, 1978 on an Atlas-Centaur rocket. It consisted of a 290 kg bus which carried one large (315 kg) and three small atmospheric probes. The large probe was released on November 16, 1978 and the three small probes on November 20. All four probes entered the Venus atmosphere on December 9, followed by the bus.\n\nThe Pioneer Venus large probe was about 1.5 m in diameter and equipped with 7 science experiments. After deceleration from initial atmospheric entry at about 11.5 km/s, a parachute was deployed at 47 km altitude. The science experiments were: \n\nThe three small probes were identical to each other, 0.8 m in diameter and 90 kg each small probe. The small probes were each targeted at different parts of the planet; They had no parachutes and the aeroshells did not separate from the probe.\n\nThe Pioneer Venus bus also carried two experiments, a neutral mass spectrometer and an ion mass spectrometer to study the composition of the atmosphere. With no heat shield or parachute, the bus made measurements only to about 110 km altitude before burning up.\n\n\n"}
{"id": "52095317", "url": "https://en.wikipedia.org/wiki?curid=52095317", "title": "Planning cultures", "text": "Planning cultures\n\nPlanning cultures are the differing customs and practices in the profession of urban and regional planning that exist around the world. The discourse, models, and styles of communication in planning are adapted to the various local conditions of each community such that planning approaches from one part of the world are not necessarily transferrable to other parts of the globe. Planning culture can refer to how planning professionals undertake their practice in a given location, where they are \"affected by both individual and collectively shared cognitive frames\" that shape their view of the world. Planners, as stated by Simone Abram, are \"constantly in the process of actually producing culture\". The concept of planning culture also encompasses how planning actually unfolds within a community, as shaped by its culture and influenced by its people. Differing cultural contexts produce different planning and policy responses to issues \"bound to specific local (cultural) contexts\". Examples of planning cultures include those specific to different countries, regions, and parts of the globe, as well as differing cultures that exist within the same location, such as indigenous planning cultures.\n\nIn general, planning cultures evolved throughout the 20th century from a state-centred attempt at city beautification toward a more collaborative process involving the public. This differs depending on local culture; however, as planning in some locations has remained a top-down process where plans are prescribed by senior government (for example, in China). Planning culture requires treatment of different local circumstances and as such, varies around the world. In Canada, planning culture has evolved from its roots influenced by town planning in Great Britain to become a values-based political process that promotes public involvement in an attempt to give agency to various groups, although social inequalities and colonial legacies are still largely present. Indigenous planning culture, which has evolved over hundreds to thousands of years, is another distinct planning culture, within which exists distinct, fluid, and evolving planning cultures that are unique to land, history, and peoples. Today, the reclamation of Indigenous planning cultures has planning in many countries exploring how non-Indigenous and Indigenous planners can work collaboratively toward a reconciliatory, culturally-responsive, and respectful process.\n\nPlanning cultures operate within the institutions of their locations. As such, planning cultures play out within the institutional spheres of society, spheres which are consistently evolving and changing in a dynamic response to cultural stimuli. Educational institutions across the world differ in their approach to planning, contributing toward the formation of distinct planning cultures and demonstrating transformations within planning cultures through changing subject matter and emphases.\n\nPlanning cultures evolved alongside processes of industrialization and the advancement of societies. Prior to World War II, planning was considered a city beautification movement proposed by architects and politicians. In Europe, with industrialization, planning was conceived as an attempt to provide city leaders with a framework to develop new neighbourhoods and towns outside of the urban core. After World War II, planning expanded from beautification to include many social elements such as economy, history, religion, and public policy. It became understood that such social factors influence how urban infrastructure develops.\n\nThe formative years for planning cultures were the 1940s and 1950s, during a global period of rebuilding after World War II. Planning cultures emerged from planning practices as the public became increasingly aware of the impacts of planning. During this period, planners also started to take into consideration the impacts of planning practices on the public. Efforts began to shift from building functional and aesthetic architecture, to analytical and rational development approaches for resolving issues faced by a city or region.\n\nThe planning culture in many countries has shifted from a design-centric practice led by a small group of professionals to a much broader social activity involving the general public throughout the process.\n\nStarting in the 1960s, the traditional “top-down” and “state-centered” planning paradigm was criticized as being static and unresponsive in dealing with complex issues in local level. Accordingly, planning practice in many industrialized nations began to shift towards a more “bottom-up” and “people-centered” process. During this transitional period, planning evolved from its original object of providing solutions for specific issues to a more comprehensive goal of addressing socioeconomic challenges. This was an attempt to explore more efficient, flexible, and accountable options for complex problems. As planning culture in industrialized states became more inclusive and aware of issues related to the environment, social equality, multiculturalism, and other non-spatial issues, planners felt it necessary to reflect upon their professional views in order to engage with general public and to search common ground across public in different background and interest groups During the same period, some developing countries maintained their “top-down” paradigm in planning, such as China, Cuba, and Iran, where planning still followed comprehensive national strategies. Since the 1980s, planning practices in those and other developing counties have begun to take different directions.\n\nPlanning processes and results are highly subject to political influences and institutional structures, which vary significantly around the world. The process of planning involves many stakeholders, although primarily, it is overseen and approved by governments. As such, the largest influences on planning culture are the variables of: constitutional government structures (unitary states, federal states, multinational states), economic structure (market economies, those in transition from command economies), the pace of urban population growth, political culture (involvement of civil society, role of media, competitiveness among parties, openness of political process), and the level of economic development. In all cases, planning institutions are constantly evolving themselves to adapt to new needs and changes. One such broad shift is the embracing of market-led development, where planning institutions now facilitate market forces in an entrepreneurial fashion instead of restraining them. Several examples of planning cultures are presented below, in alphabetical order by country name.\n\nPlanning culture in Canada is inherently value-laden, and cultural and moral values are important aspects of this practice. Canadian planning culture has been influenced by the planning practices of Great Britain, who partly settled Canada, and by the United States, Canada's neighbour to the South. The culture of planning in Canada has retained its unique identity by focusing on community planning, while Great Britain focuses on town planning, and the United States on urban planning. Canada's legacy as a sparsely populated nation has led to a planning culture that centres on regional planning as a way of tackling disparities between rural and urban areas. The conflict of Canada's regional planning culture lies in the fact that a region can be defined in many different ways. For example, a region could be a large area centred around an urban area, a watershed area, or as an area based on the valley section between mountains.\n\nCombined with the regional approach to planning, the diverse and multicultural nature of the Canadian population has led to some challenges in the way that planning is conducted in Canada. Jill Grant explains that a potential solution to these conflicts that has been adopted by Canadian planners is a planning culture where the professional works proactively to anticipate conflicts and mitigate them before they reach a tipping point. This flexible and proactive approach is useful in Canadian planning culture because it helps balance the needs of different groups living, an often very diverse geographic, political, and social, region to come to positive solutions.\n\nPlanning practice is influenced by politics, and in Canada social justice has been incorporated into progressive planning cultures since the 1960s. Social justice in planning has focused on enabling typically under-resourced groups, such as the poor. The legacy of planning in Canada as a form of social justice has permeated to planning schools, including the School of Community and Regional Planning at the University of British Columbia, where agency, democratization, and sustainability are declared to be important values of the curriculum. Participatory planning culture in Canada is in part inspired by Porto Alegre's participatory budget, which has led to a culture of consensus-building, collaboration, and community agency.\n\nCanada's planning culture is rife with colonial legacies and inequalities and this is not forgotten in today's planning culture. Today, planning culture in Canada has evolved to a values-based practice embedded within the political realm that attempts to give agency to various groups by promoting collaboration and conflict resolution.\n\nIn China, planning activities and planning bureaucracies have expanded significantly in recent decades, with over 60,000 practicing planners in what is now a well-reputed profession. Local development and land use decisions are guided by socio-economic plans developed by senior governments, which in turn are aligned with national 5-year plans. The State Land Administration Bureau is a vertically-integrated organization with branches in every major city that oversees the allocation of land uses and coordinates major projects along with the Ministry of Construction and the Planning Bureau. This is indicative of planning culture in China, which is characterized by centralization of decision making: for example, Civic Mayors are appointed by higher levels of government. Planning is largely seen as a responsibility of government, with little involvement from civil society. Citizens who take issue with planning decisions may sue the local government, file a petition, or engage in public demonstration; however, these actions are largely futile. As a result, planners have significant latitude to implement their ideas without inhibition. Recently; however, media outlets have begun to offer critical perspectives on planning decisions.\n\nRelative to China, planning culture in India is characterized by jurisdictional and administrative autonomy. The civil service in charge of planning is accountable to, yet separate from, the political sphere. Five-year national economic plans guide the direction of development and land use planning at the state and municipal level. With the quadrupling of urban populations over the past 50 years, planning is challenged to keep up with such rapid urbanization. As a result, civil society is still finding its role in balancing planning actors with increasing influence for historically marginalized groups. While planning in India is seen as a vehicle by which to organize and bring significant benefits to society, the planning culture is marred by a socio-economic disconnect between planners and the populations they serve, a highly bureaucratic administration, and the continued marginalization of vast swathes of the urban poor who often live without land rights.\n\nIn Japan, planning was a large part of the dynastic society throughout the known recent history of Japan. During successive heredity an Emperor or the appointed Shogun had the authority to provide centralized planning for the areas of the country under their control. Individual provinces would report to the authority and power of the central government. After the Meiji Restoration the government developed separated functions and planning functions largely centralized on a regional basis. During the 1990s, local governments were empowered to begin producing urban development Master Plans to guide long-term changes in their jurisdictions. While municipalities were still tied financially to the central government, this change in planning was seen as a major innovation, and one that created a planning culture with increased opportunities for localized public participation.\n\nIn Russia, prior to the late 1980s, planning was highly centralized and plans were not publicly available. 30-year plans and designs for most areas were produced in government institutes in the largest cities, far removed from the realities where they were to be applied. After the dissolution of the Soviet Union, local governments were rapidly given increased planning responsibility, but did not yet have the tools and structure in place to effectively regulate land use. Significant political, societal, and economic changes in that period of transition further complicated the challenges faced by local planning authorities. Attention was given to addressing these immediate needs, and less to the long-range planning tasks of the Soviet era.\n\nIn sub-Saharan Africa, including the country of South Africa, urban growth rates remain some of the highest in the world. A severe lack of institutional capacity and financial resources characterizes the context of planning challenges in sub-Saharan states. The planning culture is characterized by a process that focuses in the areas inhabited by urban elites, while the vast majority of urban populations live in informal and squatter settlements, which are technically illegal and beyond the official realm of planning jurisdiction. However, local and national authorities are increasingly directing attention to such settlements, and, combined with foreign financial support, are improving basic infrastructure needs. The precarious nature of these unofficial residential lands limits the ability of residents to obtain services and raise issues with local planning administrations, further maintaining their marginalization. Enforcement of regulations, public participation, and plan execution are notably weak, while nepotism is strong, resulting in much urban development taking place outside of official planning processes.\n\nStudies of planning culture have tended to focus on national scales for comparative analysis; however, a need has been identified to focus on the contextual factors that differentiate planning cultures at the local level. Further research is also needed to understand the underlying forces that affect the core values of planning cultures; while \"planning artifacts\", such as institutional planning structures, are evident and can be easily compared, intangible aspects of the \"social environment\" require more thorough examination.\n\nIndigenous planning culture is an approach to planning that is both by and for indigenous peoples and is a practice that has evolved over hundreds to thousands of years. Its cultural practice is defined by the sets of traditional knowledges, customs, practices, and cultural identities that are unique to the approximately 350 Indigenous peoples worldwide. Indigenous world views ground Indigenous planning cultures, that have evolved and been sustained through time. Indigenous Planning is defined by Dr. Theodore (Ted) Jojola as, \"both an approach to community planning and an ideological movement. What distinguishes indigenous planning from mainstream practice is its reformulation of planning approaches in a manner that incorporates 'traditional' knowledge and cultural identity. Key to the process is the acknowledgement of an indigenous world-view, which not only serves to unite it philosophically, but also to distinguish it from neighbouring, non land-based communities.\" Many scholars, practitioners, and community leaders see Indigenous planning as a cultural practice of reclaiming traditional and reformulating mainstream planning approaches that are respectful, responsive to, and owned by Indigenous communities.\n\nAccording to Sanyal, (p.xxi) \"planning cultures are cultures of planning practice. They are the collective ethos and dominant attitudes of professional planners toward the appropriate roles of the state, market forces, and civil society in urban, regional, and national development.\" As Reimer notes, this ethos refers to the \"basic ideas, traditions, and values of planning professionals.\"\n\nReimer further suggests that planning cultures are \"complex and multi-dimensional institutional matrices comprising formal and informal institutional spheres that need to adapt to ever-changing external and internal circumstances.\" In this context, planning cultures operate in an institutional setting and any evolution of a planning culture must contend with a \"persisting institutional stability\". Reimer evaluates planning cultures as \"an analytical concept (and not as a normative paradigm in theory and practice).\" Planning is embedded and operates in specific cultural contexts, subject to changing planning cultures. This change can be seen as it plays out in institutional spheres, a form of social transformation. Institutional change can be theorized as a process comprising six stages: equilibrium, shock event, deinstitutionalization, preinstitutionalization, theorization, diffusion, and reinstitutionalization.\n\nTransitions in planning cultures can also be seen in the style and focus of education programs for new planners. Traditionally, planning was closely aligned with architecture and engineering; this is evidenced in planning programs in continental Europe, which emphasize urban design and land use planning. In North America, the United Kingdom, and select European countries, the breadth of planning subject matter in education has expanded and directs more attention to environmental sustainability and social justice, while also focusing on built form and land regulation. Planning programs are evolving to train planners to be more creative, innovative, and collaborative. Such skills are essential to meet the increasing challenges faced by planning, for which traditional methods will not suffice. In unprecedented, uncertain, and unstable times, adaptation of planning cultures is seen as key to their efficacy.\n\nThe foundation of Anglo-American planning values has long been guided by a yearning to implement utopian ideals. This starts with planning institutions where academics and practitioners practice, where institutional reform is a constant focus. By reforming institutions, planners believe they can improve core issues in the practice and process of planning.\n\n"}
{"id": "1585092", "url": "https://en.wikipedia.org/wiki?curid=1585092", "title": "Polish Institute of Physical Chemistry", "text": "Polish Institute of Physical Chemistry\n\nPolish Institute of Physical Chemistry (Polish Instytut Chemii Fizycznej) is one of numerous institutes belonging to the Polish Academy of Sciences. As its name suggests, the institute's primary research interests are in the field of physical chemistry. The institute is subdivided into departments, including the Department of Soft Condensed Matter and Fluids, the Department of Physical Chemistry of Supramolecular Complexes, the Department of Photochemistry and Spectroscopy and the Department of Quantum Theory of Solids and Molecules, this is also known as the PIPC.\n\n"}
{"id": "6883009", "url": "https://en.wikipedia.org/wiki?curid=6883009", "title": "RF resonant cavity thruster", "text": "RF resonant cavity thruster\n\nA radio frequency (RF) resonant cavity thruster, also known as an EmDrive, is a hypothesized type of propellant-free thruster that was proposed in 2001 by Roger Shawyer. No plausible theory of operation for such drives has been proposed; the theories that were proposed were shown to be inconsistent with known laws of physics, including conservation of momentum and conservation of energy.\n\nSeveral prototypes of this concept have been constructed and tested, including by the Advanced Propulsion Physics Laboratory at NASA. Initially, a few tests of prototype drives were reported to produce a small apparent thrust, but subsequent testing has failed to reliably reproduce these results.\n\nDue to the lack of both a physically plausible theory of operation and of reliably reproducible evidence, many theoretical physicists and commentators consider the device unworkable, explaining the observed thrust as measurement errors. Various media platforms have referred to the engine as the Impossible Drive or the Impossible Space Drive.\n\nElectromagnetic propulsion designs which operate on the principle of reaction mass have been around since the start of the 20th century. In the 1960s, extensive research was conducted on two designs which emit high-velocity ionized gases in similar ways: ion thrusters that convert propellant to ions and accelerate and eject them via electric potentials, and plasma thrusters that convert propellant to plasma ions and accelerate and eject them via plasma currents. In the latter, plasma can be generated from an intense source of microwave or other radio-frequency (RF) energy, and in combination with a resonant cavity, can be tuned to resonate at a precise frequency.\n\nA low-propellant space drive has long been a goal for space exploration, since the propellant is dead weight that must be lifted and accelerated with the ship all the way from launch until the moment it is used (see Tsiolkovsky rocket equation). Gravity assists, solar sails, and beam-powered propulsion from a spacecraft-remote location such as the ground or in orbit, are useful because they allow a ship to gain speed without propellant. However, some of these methods do not work in deep space. Shining a light out of the ship provides a small force from radiation pressure, i.e., using photons as a form of propellant, but the force is far too weak, for a given amount of input power, to be useful in practice.\n\nA true zero-propellant drive is widely believed to be impossible, but if it existed, it could potentially be used for travel in many environments including deep space. Thus, such drives are a popular concept in science fiction, and their improbability contributes to enthusiasm for exploring such designs.\n\nConventional rocket engines expel propellant, such as when ships move masses of water, aircraft move masses of air, or rockets expel exhaust. A drive which does not expel propellant in order to produce a reaction force, providing thrust while being a closed system with no external interaction, would be a reactionless drive. Such a drive would violate the conservation of momentum and Newton's third law, leading many physicists to believe it to be impossible, labelling the idea pseudoscience. On the other hand, a drive that interacts with an external field would be part of an open system, propellantless but not reactionless, like a sail catching and redirecting existing winds to move a ship.\n\nThe first proposal for an RF resonant cavity thruster came from British engineer Roger Shawyer in 2001. He invented a design with a conical cavity, calling it the EmDrive. Guido Fetta later built the Cannae Drive based on Shawyer's concept a resonant thruster with a pillbox-shaped cavity.\nSince 2008, a few physicists have tested their own models, trying to confirm results claimed by Shawyer and Fetta. Juan Yang at Xi'an's Northwestern Polytechnical University (NWPU) initially reported thrust from a model they built, but retracted her claims in 2016 after a measurement error was identified and an improved setup measured no significant thrust. In 2016, Harold White's group at NASA's Advanced Propulsion Physics Laboratory reported a test of their own model had observed 40–100 μN of thrust from inputs of 40–80 W, in the Journal of Propulsion and Power. In December 2016, Yue Chen, part of the communication satellite division of the China Academy of Space Technology (CAST), said his team had tested several prototypes using an \"experimental verification platform\", observed thrust, and was carrying out in-orbit verification. In September 2017, Chen talked about this CAST project again in an interview on CCTV.\n\nThe plausibility of thrusters that emit no propellant, such as the EmDrive, is controversial, primarily because their operation would violate the conservation of momentum.\n\nMedia coverage of experiments using these designs has been controversial and polarized. The EmDrive first drew attention, both credulous and dismissive, when \"New Scientist\" wrote about it as an \"impossible\" drive in 2006. Media outlets were later criticised for misleading claims that a resonant cavity thruster had been \"validated by NASA\" following White's first tentative test reports in 2014. Scientists have continued to note the lack of unbiased coverage, from both polarized sides.\n\nIn 2006, responding to the \"New Scientist\" piece, mathematical physicist John C. Baez at the University of California, Riverside, and Australian science-fiction writer Greg Egan, said the positive results reported by Shawyer were likely misinterpretations of experimental errors.\n\nIn 2014, White's conference paper suggested that resonant cavity thrusters could work by transferring momentum to the \"quantum vacuum virtual plasma.\" Baez and Carroll criticized this explanation, because in the standard description of vacuum fluctuations, virtual particles do not behave as a plasma; Carroll also noted that the quantum vacuum has no \"rest frame\", providing nothing to push against, so it can't be used for propulsion. In the same way, physicists James F. Woodward and Heidi Fearn published two papers showing that the amount of electron-positron virtual pairs of the quantum vacuum, used by White as a virtual plasma propellant, cannot account for thrusts in any isolated, closed electromagnetic system such as a quantum vacuum thruster.\n\nPhysicists Eric W. Davis at the Institute for Advanced Studies in Austin and Sean M. Carroll at the California Institute of Technology said in 2015 that the thrust reported in papers by both Tajmar and White were indicative of thermal effect errors.\n\nIn May 2018, researchers from the Institute of Aerospace Engineering at Technische Universität Dresden, Germany, concluded that the apparent thrust is clearly an artifact caused by Earth's magnetic field interacting with power cables in the chamber, a result that other experts agree with.\n\nIn 2001, Shawyer founded \"Satellite Propulsion Research Ltd\", in order to work on the EmDrive, a drive that he said used a resonant cavity to produce thrust without propellant. The company was backed by SMART award grants from the UK Department of Trade and Industry. In December 2002, he described a working prototype with an alleged total thrust of about 0.02 newtons powered by an 850 W cavity magnetron. The device could operate for only a few dozen seconds before the magnetron failed, due to overheating.\n\nIn October 2006, Shawyer conducted tests on a new water-cooled prototype and said that it had increased thrust. He planned to have the device ready to use in space by May 2009 and was considering making the resonant cavity a superconductor.\n\n\"New Scientist\" magazine featured the EmDrive on the cover of 8 September 2006 issue. The article portrayed the device as plausible and emphasized the arguments of those who held that point of view. Science fiction author Greg Egan distributed a public letter stating that \"a sensationalist bent and a lack of basic knowledge by its writers\" made the magazine's coverage unreliable, sufficient \"to constitute a real threat to the public understanding of science\". Especially, Egan said he was \"gobsmacked by the level of scientific illiteracy\" in the magazine's coverage, alleging that it used \"meaningless double-talk\" to obfuscate the problem of conservation of momentum. The letter was endorsed by mathematical physicist John C. Baez and posted on his blog. \"New Scientist\" editor Jeremy Webb responded to critics: \"New Scientist\" also published a letter from the former technical director of EADS Astrium: A letter from physicist Paul Friedlander: \n\nIn 2007, the UK Department of Trade and Industry granted SPR an export licence to Boeing in the US. In December 2008, Shawyer was invited to The Pentagon to present on the EmDrive, and in 2009 Boeing confirmed they wanted to license the technology. The UK Ministry of Defence agreed to a technology transfer, and SPR designed, built and tested a thruster for use on a test satellite. According to Shawyer, the 10-month contract was completed by July 2010 and the thruster, giving 18 grams of thrust, was transferred to Boeing. Boeing did not, however, license the technology and communication stopped. Questioned on the matter in 2012, a Boeing representative confirmed that\nBoeing Phantom Works used to explore exotic forms of space propulsion, including Shawyer's drive, but such work has since ceased. They confirmed that \"Phantom Works is not working with Mr. Shawyer,\" adding that the company is no longer pursuing those explorations.\n\nIn 2013 and 2014, Shawyer presented ideas for 'second-generation' EmDrive designs and applications, at the annual International Astronautical Congress. A paper based on his 2014 presentation was published in \"Acta Astronautica\" in 2015. It describes a model for a superconducting resonant cavity and three models for thrusters with multiple cavities, with hypothetical applications for launching space probes.\n\nIn October 2016, a UK patent application describing a new superconducting EmDrive was published, followed by a first international version. Shortly thereafter Shawyer unveiled the creation of \"Universal Propulsion Ltd.\", a new company aimed to develop and commercialise such thrusters, as a joint venture with \"Gilo Industries Group\", a small UK aerospace company designing and selling paramotors and the Parajet Skycar.\n\nThe Cannae Drive (formerly Q-drive), another engine designed to generate propulsion from a resonant cavity without propellant, is another implementation of this idea. Its cavity is also asymmetric, but relatively flat rather than a truncated cone. It was designed by Fetta in 2006 and has been promoted within the US through his company, Cannae LLC, since 2011. In 2016, Fetta announced plans to eventually launch a CubeSat satellite containing a version of the Cannae Drive, which they would run for 6 months to observe how it functions in space.\n\nIn China, researchers working under Yang at NWPU developed their own prototype resonant cavity thruster in 2008, publishing a report in their university's journal on the theory behind such devices. In 2012 they measured thrust from their prototype, however, in 2014 they found this had been an experimental error. A second, improved prototype did not produce any measured thrust.\n\nAt the China Academy of Space Technology, Yue Chen filed several patent applications in 2016 describing various RF resonant cavity thruster designs. These included a method for stacking several short resonant cavities to improve thrust, and a design with a cavity that was a semicylinder instead of a frustum. That December, Chen announced that CAST was conducting tests on a resonant cavity thruster in orbit, without specifying what design was used. In an interview on CCTV in September 2017, Chen Yue showed some testing of a flat cylindrical device corresponding to the patent describing stacked short cavities with internal diaphragms.\n\nThe proposed theory for how the EmDrive works violates the conservation of momentum, which states any interaction cannot have a net force; a consequence of the conservation of momentum is Newton's third law, where for every action there is an equal and opposite reaction. The conservation of momentum is a symmetry of nature.\n\nIn instances where matter appears to violate conservation laws, the apparent non-conservation is in reality an interaction with the vacuum so that overall symmetry in the system is restored. An often cited example of apparent nonconservation of momentum is the Casimir effect; in the standard case where two parallel plates are attracted to each other. However the plates move in opposite directions, so no net momentum is extracted from the vacuum and, moreover, the energy must be put into the system to take the plates apart again.\n\nAssuming homogeneous electric and magnetic fields, it is impossible for the EmDrive, or any other device, to extract a net momentum transfer from either a classical or quantum vacuum.\nExtraction of a net momentum \"from nothing\"\nhas been postulated in an inhomogeneous vacuum, but this remains highly controversial as it will violate Lorentz invariance.\n\nBoth Harold White's\nand Mike McCulloch's theories of how the EmDrive could work rely on these asymmetric or dynamical Casimir effects. However, if these vacuum forces are present, they are expected to be exceptionally tiny based on our current understanding, too small to explain the level of observed thrust.\nIn the event that observed thrust is not due to experimental error, a positive result could indicate new physics.\n\nCritics liken the EmDrive to trying to move a car by getting inside and pushing on the windshield. This violation of the fundamental principles of physics has drawn criticism from the scientific community, leading to various attempts to explain the apparent or observed thrust. However, to date, there is no acceptance or consensus on how or why these cavities produce thrust if they produce thrust at all.\n\nAttempts to explain the thrust (assuming that there is thrust) generally fall into four categories:\n\nThe simplest and most likely explanation is that any thrust detected is due to experimental error or noise. In all of the experiments set up, a very large amount of energy goes into generating a tiny amount of thrust. When attempting to measure a small signal superimposed on a large signal, the noise from the large signal can obscure the small signal and give incorrect results. The strongest early result, from Yang's group in China, was later reported to be caused by an experimental error.\n\nThe largest error source is believed to come from the thermal expansion of the thruster's heat sink; as it expands this would lead to a change in the centre of gravity causing the resonant cavity to move. White's team attempted to model the thermal effect on the overall displacement by using a superposition of the displacements caused by \"thermal effects\" and \"impulsive thrust\" with White saying \"That was the thing we worked the hardest to understand and put in a box\". Despite these efforts, White's team were unable to fully account for the thermal expansion. In an interview with \"Aerospace America\", White comments that \"although maybe we put a little bit of a pencil mark through [thermal errors]... they are certainly not black-Sharpie-crossed-out.\"\n\nTheir method of accounting for thermal effects has been criticized by Millis and Davies, who highlight that there is a lack of both mathematical and empirical detail to justify the assumptions made about those effects. For example, they do not provide data on temperature measurement over time compared to device displacement. The paper includes a graphical chart, but it is based on \"a priori\" assumptions about what the shapes of the \"impulsive thrust\" and \"thermal effects\" should be, and how those signals will superimpose. The model further assumes all noise to be thermal and does not include other effects such as interaction with the chamber wall, power lead forces, and tilting. Because the Eagleworks paper has no explicit model for thrust to compare with the observations, it is ultimately subjective, and its data can be interpreted in more than one way. The Eagleworks test, therefore, does not conclusively show a thrust effect, but cannot rule it out either.\n\nWhite suggested future experiments could run on a Cavendish balance. In such a setup, the thruster could rotate out to much larger angular displacements, letting a thrust (if present) dominate any possible thermal effects. Testing a device in space would also eliminate the center-of-gravity issue.\n\nAnother source of error could have arisen from electromagnetic interaction with the walls of the vacuum chamber. White argued that any wall interaction could only be the result of a well-formed resonance coupling between the device and wall and that the high frequency used imply the chances of this would be highly dependent on the device's geometry. As components get warmer due to thermal expansion, the device's geometry changes, shifting the resonance of the cavity. In order to counter this effect and keep the system in optimal resonance conditions, White used a phase-locked loop system (PLL). Their analysis assumed that using a PLL ruled out significant electromagnetic interaction with the wall.\n\nAnother potential source of error was a Lorentz force arising from power leads. Many previous experiments used cups with Galinstan metal alloy, which is liquid at room temperature, to supply electrical power to the device in lieu of solid wires. Martin Tajmar and his graduate student Fiedler characterized and attempted to quantify possible sources of error in their experiment at Dresden University of Technology. They ran multiple tests on their experimental setup, including measurements of the force along different axes with respect to the power supply current. While eliminating or accounting for many other sources of error in previous experiments, such as replacing a magnetic damping mechanism with an oil damper, less efficient but significantly less interacting with electromagnetic field, the study remained inconclusive as to the effects of electromagnetic interaction with the apparatus' power feed, at the same time noting it as possibly the most significant source of noise. White's power setup may have been different, but their paper does not state if the connections are all coaxially aligned with the stand's rotation axis, which would be required to minimize errors from Lorentz forces, and it gives no data from equivalent tests with power into a dummy load so these influences can be compared with those seen in the Tajmar-Fiedler run.\n\nWhite's 2016 paper went through about a year of peer review involving five referees. Peer review does not mean the results or observations are true, only that the referees looked at the experiment, results and interpretation and found it to be sound and sensible. Brice Cassenti, a professor at the University of Connecticut and an expert in advanced propulsion, spoke to one of the referees, and reported the referee did not believe the results point to any new physics, but that the results were puzzling enough to publish. Cassenti believes there is a mundane explanation for the results, but the probability of the results being valid is slim but not zero.\n\nWhite's paper was published in the \"Journal of Propulsion and Power\". Marc Millis and Eric Davies who ran NASA's previous advanced propulsion project, the Breakthrough Propulsion Physics Program have commented that while White used techniques that would be acceptable for checking the electric propulsion of Hall thrusters, the tests were not sufficient to demonstrate that any new physics effect exists.\n\nIn 2004, Shawyer reported seven independent positive reviews from experts at BAE Systems, EADS Astrium, Siemens and the IEE, but these were disputed. In a letter to \"New Scientist\", the then-technical director of EADS Astrium (Shawyer's former employer) denied this: \n\nIn 2011, Fetta tested a superconducting version of the Cannae drive. The RF resonant cavity was suspended inside a liquid helium-filled dewar. The weight of the cavity was monitored by load cells. Fetta theorized that when the device was activated and produced upward thrust, the load cells would detect the thrust as a change in weight. When the drive was energized by sending 10.5 watt power pulses of RF power into the resonant cavity, there was, as predicted, a reduction in compressive force on the load cells consistent with thrust of 8–10 mN.\n\nNone of these results have been published in the scientific literature, or replicated by independent researchers. They have been posted on their inventors' websites.\n\nIn 2015, Shawyer published an article in \"Acta Astronautica\", summarising existing tests on the EmDrive. Of seven tests, four produced a measured force in the intended direction and three produced thrust in the opposite direction. Furthermore, in one test, thrust could be produced in either direction by varying the spring constants in the measuring apparatus.\n\nIn 2008, a team of Chinese researchers led by Juan Yang (杨涓), professor of propulsion theory and engineering of aeronautics and astronautics at Northwestern Polytechnical University (NWPU) in Xi'an, China, said that they had developed a valid electro-magnetic theory behind a microwave resonant cavity thruster. A demonstration version of the drive was built and tested with different cavity shapes and at higher power levels in 2010. Using an aerospace engine test stand usually used to precisely test spacecraft engines like ion drives, they reported a maximum thrust of 720 mN at 2,500 W of input power. Yang noted that her results were tentative, and said she \"[was] not able to discuss her work until more results are published\". This positive result was over 100x more thrust per input power than any other experiment, and inspired other groups to try to replicate their work.\n\nIn a 2014 follow-up experiment (published in 2016), Yang could not reproduce the 2010 observation and suggested it was due to experimental error. In that experiment they refined their experimental setup, using a three-wire torsion pendulum to measure thrust, and tested two different power setups. In one trial, the power system was outside the cavity, and they observed a \"thrust\" of 8–10 mN. In a second trial, the power system was within the cavity, and they measured no such thrust. Instead they observed an insignificant thrust below their noise threshold of 3 mN, fluctuating between ±0.7 mN with a measurement uncertainty of 80%, with 230 W of input power. They concluded that they were unable to measure significant thrust; that \"thrust\" measured when using external power sources (as in their 2010 experiment) could be noise; and that it was important to use self-contained power systems for these experiments, and more sensitive pendulums with lower torsional stiffness.\n\nSince 2011, White has had a team at NASA known as the Advanced Propulsion Physics Laboratory, or Eagleworks Laboratories, which is devoted to studying exotic propulsion concepts. The group has investigated ideas for a wide range of untested and fringe proposals, including Alcubierre drives, drives that interact with the quantum vacuum, and RF resonant cavity thrusters.\n\nIn 2014, the group began testing resonant cavity thrusters of their own design and sharing some of their results. In November 2016, they published their first peer-reviewed paper on this work, in the \"Journal of Propulsion and Power\".\n\nIn July 2014, White reported tentative positive results for evaluating a tapered RF resonant cavity. Testing was performed using a low-thrust torsion pendulum able to detect force at the micronewton level within a sealed but unevacuated vacuum chamber (the RF power amplifier used an electrolytic capacitor unable to operate in a hard vacuum). The experimenters recorded directional thrust immediately upon application of power.\n\nTheir first tests of this tapered cavity were conducted at very low power (2% of Shawyer's 2002 experiment). A net mean thrust over five runs was measured at 91.2 µN at 17 W of input power. The experiment was criticized for its small data set and for not having been conducted in vacuum, to eliminate thermal air currents.\n\nThe group announced a plan to upgrade their equipment to higher power levels, to use vacuum-capable RF amplifiers with power ranges of up to 125 W, and to design a new tapered cavity that could be in the 0.1 N/kW range. The test article was to be subject to independent verification and validation at Glenn Research Center, the Jet Propulsion Laboratory and the Johns Hopkins University Applied Physics Laboratory. , this validation has not happened.\n\nIn 2015, Paul March from Eagleworks made new results public, measured with a torsional pendulum in a hard vacuum: about 50 µN with 50 W of input power at 5.0×10 torr. The new RF power amplifiers were said to be made for hard vacuum, but failed rapidly due to internal corona discharges. Without funding to replace or upgrade them, measurements were scarce for a time.\n\nThey conducted further experiments in vacuum, a set of 18 observations with 40-80W of input power. They published the results in the American Institute of Aeronautics and Astronautics's peer-reviewed \"Journal of Propulsion and Power\", under the title \"Measurement of Impulsive Thrust from a Closed Radio-Frequency Cavity in Vacuum\". This was released online in November 2016, with print publication in December. The study said that the system was \"consistently performing with a thrust-to-power ratio of 1.2±0.1mN/kW\", and enumerated many potential sources of error.\n\nThe paper suggested that pilot-wave theory (a controversial, non-mainstream deterministic interpretation of quantum mechanics) could explain how the device produces thrust. Commenters pointed out that just because a study reporting consistent thrust was published with peer-review does not necessarily mean that the drive functions as claimed. Physicist Ethan Siegal commented on the paper, saying that the drive most likely does not violate conservation of momentum as this would \"make physics fall apart\" but rather that there is something else going on. He said that \"Whether it's new physics [or] the effect's cause simply hasn't been determined yet, more and better experiments will be the ultimate arbiter\". Physicist Chris Lee was very critical of the work, saying that the paper had a small data set and a number of missing details he described as 'gaping holes'. Electrical Engineer George Hathaway analyzed and criticized the scientific method described in the paper.\n\nWhite's 2014 tests also evaluated two Cannae drive prototypes. One had radial slots engraved along the bottom rim of the resonant cavity interior, as required by Fetta's hypothesis to produce thrust; another \"null\" test article lacked those radial slots. Both drives were equipped with an internal dielectric. A third test article, the experimental control, had an RF load but no resonant cavity interior. These tests took place at atmospheric pressure.\n\nAbout the same net thrust was reported for both the device with radial slots and the device without slots. Thrust was not reported for the experimental control. Some considered the positive result for the non-slotted device a possible flaw in the experiment, as the null test device had been expected to produce less or no thrust based upon Fetta's hypothesis of how thrust was produced by the device. In the complete paper, however, White concluded that the test results proved that \"thrust production was not dependent upon slotting\".\n\nIn July 2015, an aerospace research group at the Dresden University of Technology (TUD) under Martin Tajmar reported results for an evaluation of an RF resonant tapered cavity similar to the EmDrive. Testing was performed first on a knife-edge beam balance able to detect force at the micronewton level, atop an antivibration granite table at ambient air pressure; then on a torsion pendulum with a force resolution of 0.1 mN, inside a vacuum chamber at ambient air pressure and in a hard vacuum at .\n\nThey used a conventional ISM band 2.45 GHz 700 W oven magnetron, and a small cavity with a low Q factor (20 in vacuum tests). They observed small positive thrusts in the positive direction and negative thrusts in the negative direction, of about 20 µN in a hard vacuum. However, when they rotated the cavity upwards as a \"null\" configuration, they observed an anomalous thrust of hundreds of micronewtons, significantly larger than the expected result of zero thrust. This indicated a strong source of noise which they could not identify. This led them to conclude that they could not confirm or refute claims about such a thruster. At the time they considered future experiments with better magnetic shielding, other vacuum tests and improved cavities with higher \"Q\" factors.\n\nIn 2018, the TU Dresden research team presented a conference paper summarizing the results from the most recent experiments on their upgraded test rig, which seemed to show that their measured thrust was a result of experimental error from insufficiently shielded components interacting with the earth's magnetic field. In their experiments, they measured thrust values consistent with previous experiments, and the thrust reversed appropriately when the thruster was rotated by 180°. However, the team also measured thrust perpendicular to the expected direction when the thruster was rotated by 90°, and did not measure a reduction in thrust when an attenuator was used to reduce the power that actually entered the resonant cavity by a factor of 10,000, which they said \"clearly indicates that the \"thrust\" is not coming from the EMDrive but from some electromagnetic interaction.\" They concluded that \"magnetic interaction from not sufficiently shielded cables or thrusters are a major factor that needs to be taken into account for proper μN thrust measurements for these type of devices,\" and they plan on conducting future tests at higher power and at different frequencies, and with improved shielding and cavity geometry.\n\nIn August 2016, Cannae announced plans to launch its thruster on a 6U cubesat which they would run for 6 months to observe how it functions in space. Cannae has formed a company called Theseus for the venture and partnered with LAI International and SpaceQuest Ltd. to launch the satellite. No launch date has yet been announced.\n\nIn November 2016, the \"International Business Times\" published an unconfirmed report that the U.S. government was testing a version of the EmDrive on the Boeing X-37B and that the Chinese government has made plans to incorporate the EmDrive on its orbital space laboratory Tiangong-2. The US Air Force has only confirmed that the X-37B mission in question did an electric propulsion system test using a Hall-effect thruster, a type of ion thruster that uses a gaseous propellant.\n\nIn December 2016, Yue Chen told a reporter at China's \"Science and Technology Daily\" that his team was testing an EmDrive in orbit, and that they had been funding research in the area for five years. Chen noted that their prototype's thrust was at the \"micronewton to millinewton level\", which would have to be scaled up to at least 100–1000 millinewtons for a chance of conclusive experimental results. Despite this, he said his goal was to complete validation of the drive, and then to make such technology available in the field of satellite engineering \"as quickly as possible\".\n\nThe drive is featured in \"Salvation\", an American Sci-Fi suspense drama. It is described as a theoretically impossible piece of technology, but the protagonists manage to make it work using an exotic space-originated crystal extracted from an asteroid.\n\n"}
{"id": "22993438", "url": "https://en.wikipedia.org/wiki?curid=22993438", "title": "Risk parity", "text": "Risk parity\n\nRisk parity (or risk premia parity) is an approach to investment portfolio management which focuses on allocation of risk, usually defined as volatility, rather than allocation of capital. The risk parity approach asserts that when asset allocations are adjusted (leveraged or deleveraged) to the same risk level, the risk parity portfolio can achieve a higher Sharpe ratio and can be more resistant to market downturns than the traditional portfolio.\n\nRoughly speaking, the approach of building a risk parity portfolio is similar to creating a minimum-variance portfolio subject to the constraint that each asset (or asset class, such as bonds, stocks, real estate, etc.) contributes equally to the portfolio overall volatility.\n\nRisk parity can also be a generalized term that denotes a variety of investment systems and techniques that utilize its principles. The principles of risk parity are applied differently according to the investment style and goals of various financial managers and yield different results.\n\nSome of its theoretical components were developed in the 1950s and 1960s but the first risk parity fund, called the \"All Weather\" fund, was pioneered in 1996. In recent years many investment companies have begun offering risk parity funds to their clients. The term, risk parity, came into use in 2005 and was then adopted by the asset management industry. Risk parity can be seen as either a passive or active management strategy.\n\nInterest in the risk parity approach has increased since the late 2000s financial crisis as the risk parity approach fared better than traditionally constructed portfolios, as well as many hedge funds. Some portfolio managers have expressed skepticism about the practical application of the concept and its effectiveness in all types of market conditions but others point to its performance during the financial crisis of 2007-2008 as an indication of its potential success.\n\nRisk parity is a conceptual approach to investing which attempts to provide a lower risk and lower fee alternative to the traditional portfolio allocation of 60% stocks and 40% bonds which carries 90% of its risk in the stock portion of the portfolio (see illustration). The risk parity approach attempts to equalize risk by allocating funds to a wider range of categories such as stocks, government bonds, credit-related securities and inflation hedges (including real assets, commodities, real estate and inflation-protected bonds), while maximizing gains through financial leveraging. According to Bob Prince, CIO at Bridgewater Associates, the defining parameters of a traditional risk parity portfolio are uncorrelated assets, low equity risk, and passive management.\n\nSome scholars contend that a risk parity portfolio requires strong management and continuous oversight to reduce the potential for negative consequences as a result of leverage and allocation building in the form of buying and selling of assets to keep dollar holdings at predetermined and equalized risk levels. For example, if the price of a security goes up or down and risk levels remain the same, the risk parity portfolio will be adjusted to keep its dollar exposure constant. On the other hand, some consider risk parity to be a passive approach, because it does not require the portfolio manager to buy or sell securities on the basis of judgments about future market behavior.\n\nThe principles of risk parity may be applied differently by different financial managers, as they have different methods for categorizing assets into classes, different definitions of risk, different ways of allocating risk within asset classes, different methods for forecasting future risk and different ways of implementing exposure to risk. However, many risk parity funds evolve away from their original intentions, including passive management. The extent to which a risk parity portfolio is managed, is often the distinguishing characteristic between the various kinds of risk parity funds available today.\n\nThe best known version of risk parity is the equally-weighted risk contributions portfolio method.\n\nEqually-weighted risk contributions is not about \"having the same volatility\", it is about having each asset contributing in the same way to the portfolio overall volatility. For this we will have to define the contribution of each asset to the portfolio risk.\n\nConsider a portfolio of formula_1 assets where the weight of asset formula_2 is formula_3. The formula_3 form the allocation vector formula_5. Let us further denote the covariance matrix of the assets by formula_6. The volatility of the portfolio is defined as:\nThe total risk contribution of asset formula_2 is computed as follows:\nSince formula_10 is homogeneous of degree 1 in formula_5 it follows that:\nso that formula_13 can be interpreted as the contribution of asset formula_2 in the portfolio to the overall risk of the portfolio.\n\nEqual risk contribution then means formula_15 for all formula_16 or equivalently\n\nThe solution can be found by either solving the fixed point problem\nor by solving the minimization problem\nboth with some constraint that eliminates the invariance of the problem with the scaling of formula_5.\n\nUsually either the overall portfolio volatility is set to formula_21:\nor the gross investment of the portfolio is set to formula_23\nThe above minimization problem can be efficiently solved by the cyclical coordinate descent method, an open source implementation of which is available in JavaScript.\n\nThe seeds for the risk parity approach were sown when economist and Nobel Prize winner, Harry Markowitz introduced the concept of the efficient frontier into modern portfolio theory in 1952. Then in 1958, Nobel laureate James Tobin concluded that the efficient frontier model could be improved by adding risk-free investments and he advocated leveraging a diversified portfolio to improve its risk/return ratio. The theoretical analysis of combining leverage and minimizing risk amongst multiple assets in a portfolio was also examined by Jack Treynor in 1961, William F. Sharpe in 1964, John Lintner in 1965 and Jan Mossin in 1966. However, the concept was not put into practice due to the difficulties of implementing leverage in the portfolio of a large institution.\n\nAccording to Joe Flaherty, senior vice president at MFS Investment Management, \"the idea of risk parity goes back to the 1990s\". In 1996, Bridgewater Associates launched a risk parity fund called the \"All Weather\" asset allocation strategy. Although Bridgewater Associates was the first to bring a risk parity product to market, they did not coin the term. Instead the term \"risk parity\" was first used by Edward Qian, of PanAgora Asset Management, when he authored a white paper in 2005. In 2008 the name Risk Parity (short for Risk Premia Parity) was given to this portfolio investment category by Andrew Zaytsev at the investment consulting firm Alan Biller and Associates. Soon, the term was adopted by the asset management industry. In time, other firms such as Aquila Capital (2004), Northwater, Wellington, Invesco, First Quadrant, Putnam Investments, ATP (2006), PanAgora Asset Management (2006), BlackRock (2009 - formerly Barclays Global Investors), 1741 Asset Management (2009), Neuberger Berman (2009), AllianceBernstein (2010), AQR Capital Management (2010), Clifton Group (2011), Salient Partners (2012), Schroders (2012), Natixis Asset Management (2013) and Allianz Global Investors (2015) began establishing risk parity funds.\n\nA white paper report from Callan Investments Institute Research in Feb 2010 reported that a \"levered Risk Parity portfolio would have significantly underperformed\" versus a standard institutional portfolio in the 1990s but \"would have significantly outperformed\" a standard institutional portfolio during the decade of 2000 to 2010. According to a 2010 article in the Wall Street Journal \"Risk-parity funds held up relatively well during the financial crisis\" of 2008. For example, AQR's risk parity fund declined 18% to 19% in 2008 compared with the 22% decline in the Vanguard Balanced Index fund. According to a 2013 Wall Street Journal report the risk parity type of fund offered by hedge funds has \"soared in popularity\" and \"consistently outperformed traditional strategies since the financial crisis\". However, mutual funds using the risk parity strategy were reported to have incurred losses of 6.75% during the first half of the year. Proponents of risk parity argue that the value of balancing risks between asset classes will be realized over long periods including periods of recessions, growth and higher inflation regimes. Historical analysis does provide some evidence of better performance than equities in recessionary environments.\n\nWith the bullish stock market of the 1990s, equity-heavy investing approaches outperformed risk parity in the near term. However, after the March 2000 crash, there was an increased interest in risk parity, first among institutional investors in the United States and then in Europe. USA investors include the Wisconsin State Investment Board and the Pennsylvania Public School Employees’ Retirement System (PSERS) which have invested hundreds of millions in the risk parity funds of AQR, BlackRock and Bridgewater Associates. The financial crisis of 2007-2010 was also hard on equity-heavy and Yale Model portfolios, but risk parity funds fared reasonably well.\n\nDespite criticisms from skeptics, the risk parity approach has seen a \"flurry of activity\" following a decade of \"subpar equity performance\". During the period 2005 to 2012 several companies began offering risk parity products including: Barclays Global Investors (now BlackRock), Schroders, First Quadrant, Mellon Capital Management, Neuberger Berman and State Street Global Advisors. A 2011 survey of institutional investors and consultants suggests that over 50% of America-based benefit pension and endowments and foundations are currently using, or considering, risk parity products for their investment portfolios. A survey conducted by Chief Investor Officer magazine in 2014 shows how far the adoption has grown: 46% of institutional investors surveyed are using risk parity and 8% are considering investing.\n\nAccording to a 2011 article in \"Investments & Pensions Europe\", the risk parity approach has \"moderate risks\" which include: communicating its value to boards of directors; unforeseen events like the 2008 market decline; market timing risks associated with implementation; the use of leverage and derivatives and basis risks associated with derivatives. Other critics warn that the use of leverage and relying heavily on fixed income assets may create its own risk. Portfolio manager Ben Inker has criticized risk parity for being a benchmarking approach that gives too much relative weight to bonds when compared to other alternative portfolio approaches. However, proponents of risk parity say that its purpose is to avoid predicting future returns. Inker also says that risk parity requires too much leverage to produce the same expected returns as conventional alternatives. Proponents answer that the reduced risk from additional diversification more than offsets the additional leverage risk and that leverage through publicly traded futures and prime brokerage financing of assets also means a high percentage of cash in the portfolio to cover losses and margin calls. Additionally Inker says that bonds have negative skew, (small probability of large losses and large probability of small gains) which makes them a dangerous investment to leverage. Proponents have countered by saying that their approach calls for reduced exposure to bonds as volatility increases and provides less skew than conventional portfolios.\n\nProponents of the use of leverage argue that using leverage can be risk-reducing rather than risk-increasing provided four conditions are met: (i) enough unencumbered cash is kept to meet any margin calls (ii) leverage is applied to a well-diversified portfolio (iii) assets can be rebalanced frequently and (iv) counterparty risk is minimized.\n\nA 2012 article in the \"Financial Times\" indicated possible challenges for risk parity funds \"at the peak of a 30-year bull market for fixed income\". While advocates point out their diversification amongst bonds as well as \"inflation-linked securities, corporate credit, emerging market debt, commodities and equities, balanced by how each asset class responds to two factors: changes in the expected rate of economic growth and changes to expectations for inflation\". A 2013 article in the \"Financial News\" reported that \"risk parity continues to prosper, as investors come to appreciate the better balance of different risks that it represents in an uncertain world.\"\n\nAfter the sharp fall in bond prices of 2013 (\"taper tantrum\"), investors continued to question the impact of rising rates on risk parity portfolios or other more concentrated equity portfolios. A historical analysis of episodes of rising rates show the value in distinguishing between orderly and disorderly rising rates regimes. Risk parity has weaker performance in disorderly rising rates environments but its performance over time is not dependent on falling bond yields.\n\nRisk parity advocates assert that the unlevered risk parity portfolio is quite close to the tangency portfolio, as close as can be measured given uncertainties and noise in the data. Theoretical and empirical arguments are made in support of this contention. One specific set of assumptions that puts the risk parity portfolio on the efficient frontier is that the individual asset classes are uncorrelated and have identical Sharpe ratios. Risk parity critics rarely contest the claim that the risk parity portfolio is near the tangency portfolio but they say that the leveraged investment line is less steep and that the levered risk parity portfolio has slight or no advantage over 60% stocks / 40% bonds, and carries the disadvantage of greater explicit leverage.\n\n"}
{"id": "50900158", "url": "https://en.wikipedia.org/wiki?curid=50900158", "title": "Robert Banks (optician)", "text": "Robert Banks (optician)\n\nRobert Banks (late 18th century-first half 19th century) was an English maker of optical instruments.\n\nHis workshop was located in the Strand, in central London, 1796-1827. The Museo di Fisica e Storia Naturale of Florence commissioned several optical instruments from him.\n"}
{"id": "3040357", "url": "https://en.wikipedia.org/wiki?curid=3040357", "title": "Robert John Lechmere Guppy", "text": "Robert John Lechmere Guppy\n\nRobert John Lechmere Guppy (15 August 1836 in London – 5 August 1916 in San Fernando, Trinidad and Tobago) was a British-born naturalist after whom the guppy is named. He contributed much to the geology, palaeontology and zoology of the West Indian region, in particular Trinidad.\n\nHe was one of four children of Robert Guppy, a lawyer and the Mayor of San Fernando, and Amelia Elizabeth Guppy, a painter and one of the pioneers of photography, who navigated the Orinoco River accompanied by only a few native Indians. \"Lechmere\", as he was called, was raised by his grandparents, Richard Parkinson and Lucy Lechmere, in Kinnersley Castle, a 13th-century Norman castle in Herefordshire. Richard Parkinson wanted Lechmere to take over the castle, a role in which he had no interest. Having come into an inheritance from another relative, he left England at the age of 18. He visited Australia, Tasmania and was shipwrecked on the coast of New Zealand in 1856. After living with the Māori for two years and mapping the area, Lechmere left New Zealand for Trinidad, where his parents were living. He married Alice Rostant, the daughter of local French Creole planters and a descendant of the Counts of Rostant, French aristocrats who had fled to Trinidad to escape the French Revolution and became Trinidad's first Superintendent of Schools. \n\nHe was a civil engineer by trade and helped in the construction of the Cipero Railway. He later became Chief Inspector of Schools until his retirement in 1891.\n\nAlthough he had no formal training in the sciences Lechmere wrote and published numerous articles on the palaeontology of the region. He served as President of the Scientific Association of Trinidad, as well as of the Royal Victoria Institute Board. He contributed about seventy memoirs or papers between 1863 and 1913. His special interest went to paleoconchology and Recent mollusks, especially the terrestrial and fluviatile species.\n\nThough sometimes rumored to have been a clergyman, Lechmere Guppy was, in fact, an agnostic.\n\nGuppy discovered the Guppy fish in Trinidad in 1866, and the fish was named \"Girardinus guppii\" in his honour by Albert C. L. G. Günther later that year. However, the fish had previously been described in America. Although \"Girardinus guppii\" is now considered a junior synonym of \"Poecilia reticulata\", the common name \"guppy\" still remains.\n\n\n\n"}
{"id": "53843935", "url": "https://en.wikipedia.org/wiki?curid=53843935", "title": "Ruben Gerardo Barrera", "text": "Ruben Gerardo Barrera\n\nRuben Gerardo Barrera from the UNAM, Mexico, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Forum on International Physics in 2001, for \"his significant contributions to the understanding of the optical properties of surfaces and inhomogenous media as well as for his leadership in the establishment and improvement of relations among physicists in the Americas, e.g., helping to create the Latin American Federation of Physics Societies.\"\n"}
{"id": "53399282", "url": "https://en.wikipedia.org/wiki?curid=53399282", "title": "ScienceAtHome", "text": "ScienceAtHome\n\nScienceAtHome is a team of scientists, game developers, designers and visual artists based at Aarhus University, Denmark. ScienceAtHome does research on quantum physics, citizen science and gamification. ScienceAtHome also develops games that contribute to scientific research, and studies how humans interpret information to achieve results superior to some algorithmic approaches.\n\nMost ScienceAtHome games are casual games and require no formal scientific training. Over 150,000. people have contributed to ScienceAtHome citizen science projects by playing games. Research games are also part of a much larger movement of creating serious games that go beyond mere entertainment.\n\nThe premise behind such games is that humans are better than computers at performing certain tasks, because of their intuition and superior visual processing. Video games are now being used to channel these abilities to solve problems in quantum physics. Result from the flagship game, Quantum Moves have been published in scientific magazine, \"Nature\".\n\nThe idea of computer players solving quantum problems came to Jacob Sherson’s mind while he was doing research at Johannes Gutenberg University, Mainz, in the group of Prof. Immanuel Bloch. The first form of ScienceAtHome was announced in 2012 based on the idea that computer game players could solve quantum problems. It was then called CODER - “Pilot Center for Community-driven Research: Game Assisted Quantum Computing” - and it started with a game called Quantum Computer Game. CODER later grew and evolved into ScienceAtHome, and the Quantum Computer Games was renamed as Quantum Moves.\n\nScienceAtHome appeared in both national and international press several times. It has been featured in The Wall Street Journal, The Washington Post, LA Times, The New Yorker, The Sydney Morning Herald and the BBC\n\nJacob Sherson gave a speech at TEDxAarhus 2016 called “How to become a quantum physicist in five minutes”.\n\nPinja Haikka, Postdoctoral Researcher in Theoretical Physics, also introduced ScienceAtHome at Women in Science event at Aarhus University, which was published at local television ITV OJ.\n"}
{"id": "28862415", "url": "https://en.wikipedia.org/wiki?curid=28862415", "title": "Société chimique de France", "text": "Société chimique de France\n\nThe Société Chimique de France (SCF) is a learned society and professional association founded in 1857 to represent the interests of French chemists in a variety of ways in local, national and international contexts. Until 2009 the organization was known as the Société Française de Chimie.\n\nThe Society traces its origins back to an organization of young Parisian chemists who began meeting in May 1857 under the name Société Chimique, with the goal of self-study and mutual education. In 1858 the established chemist Adolphe Wurtz joined the society, and immediately transformed it into a learned society modeled after the Chemical Society of London, which was the precursor of the Royal Society of Chemistry. Like its British counterpart, the French association sought to foster the communication of new ideas and facts throughout France and across international borders.\n\nSupport for the \"Bulletin de la Société Chimique de Paris\" began in 1858. \n\nIn the 21st century, the society has become a member of ChemPubSoc Europe, which is an organization of 16 European chemical societies. This European consortium was established in the late 1990s as many chemical journals owned by national chemical societies were amalgamated. In 2010 they started ChemistryViews.org, their news and information service for chemists and other scientists worldwide.\n\nThe society acknowledges individual achievement with prizes and awards, including:\n\n\n\n"}
{"id": "3753262", "url": "https://en.wikipedia.org/wiki?curid=3753262", "title": "The Sentimental Agents in the Volyen Empire", "text": "The Sentimental Agents in the Volyen Empire\n\n(Documents Relating to) The Sentimental Agents in the Volyen Empire is a 1983 science fiction novel by Doris Lessing. It is the fifth book in her five-book \"Canopus in Argos\" series and comprises a set of documents that describe the final days of the Volyen Empire, located at the edge of our galaxy and under the influence of three other galactic empires, the benevolent Canopus, the tyrannical Sirius, and the malicious Shammat of Puttiora. It was first published in the United States in March 1983 by Alfred A. Knopf, and in the United Kingdom in May 1983 by Jonathan Cape.\n\n\"The Sentimental Agents in the Volyen Empire\" is a social satire written in the tradition of Jonathan Swift and George Orwell, and focuses on the debasement of language in political rhetoric. In Lessing's fictional universe it is propaganda that keeps the fragile empires afloat, and when language becomes too distorted, some of her characters succumb to a condition called \"undulant rhetoric\" and are placed in a Hospital for Rhetorical Diseases.\n\nBecause of its focus on characterisation and social/cultural issues, and the de-emphasis of technological details, this book is not strictly science fiction but soft science fiction, or \"space fiction\" as Lessing calls her \"Canopus in Argos\" series. While \"The Sentimental Agents\" can be read as a stand-alone book, Lessing does continue with the history of the Sirian Empire, picking up from where she left off in \"The Sirian Experiments\" (1980), the third book in the \"Canopus\" series.\n\nEdward Rothstein in a review in \"The New York Times\" describes \"The Sentimental Agents in the Volyen Empire\" as \"a satirical romp through rhetoric in a foreign empire\", but complains that the tone of the book \"wavers uncertainly, mixing farce, cynicism and banal religiosity.\"\n\nIn another New York Times review, Michiko Kakutani writes that in this book Lessing has \"narrowed her cosmic focus to a specific issue, namely the manipulative use of language and words\" which, she believes, was handled with \"more acerbity and more humour\" by George Orwell in \"Animal Farm\" (1945) and \"Nineteen Eighty-Four\" (1949).\n\nThe Volyen Empire is a relatively weak interstellar empire situated at the edge of our galaxy. It comprises the planet Volyen, its two moons, Volyenadna and Volyendesta (also referred to as planets), and two neighbouring planets, Maken and Slovin. Intelligent life evolved independently on each of these five \"planets\", and over time unstable empires formed, where each planet for a period ruled the others. The Volyen Empire is the last of these empires and rules the region with force and repression.\n\nAlthough this system is at the edge of the Canopean Empire's sphere of influence, Canopus sends agents to the region because Volyen's colonisation of Maken and Slovin provoked the Sirian Empire who had earmarked these planets for \"possible expansion\". In addition, Shammat, Canopus's enemy, had established a presence in the region.\n\nDisillusioned and oppressed, the citizens of Volyen and its colonies start speaking out against the Volyen government. Revolutionary groups form and counter the Empire's rhetoric with rhetoric of their own. Krolgul of Shammat, buoyed by the turmoil, encourages anti-government behaviour. Klorathy, a senior Canopean Colonial administrator, is sent to Volyen to observe the unfolding events, and to monitor Incent, one of his agents who has been caught up in the sentiment of the revolutionary rhetoric. Incent has also fallen prey to Krolgul's propaganda and is withdrawn from the field by Klorathy and placed in a Hospital for Rhetorical Diseases on Volyendesta.\n\nSirius is now threatening to invade the region, and this is welcomed by the downtrodden in the Volyen Empire because they are sure that Sirius will set them free. Many citizens became Sirian agents and provide Sirius with information and support. But the Sirian Empire is itself in turmoil. A conflict on Sirius split the governing oligarchy into the Questioners, led by the Five who want Sirius's expansion program halted, and the Conservers, who believe Sirius should continue colonising other planets. The Five were defeated and Sirius resumed its expansion, but this time with an uncontrolled brutality that turned the Sirian Empire into a tyranny. When the Sirian agents learn about Sirius's tyranny, their loyalties are divided between Sirius and Volyen, and they become known as \"sentimental agents\".\n\nSirius invades the Volyen Empire with troops from nearby Sirian occupied planets. The troops, themselves colonial subjects of the now declining Sirian Empire, were told that Volyen is poor and deprived and needs Sirius's help. But when they land they discover that the Volyens are better off than they are, and return home and declare their own planets independent from Sirius. Volyenadna and Volyendesta, with Klorathy's help, become self-sustaining and declare \"their\" independence from the crumbling Volyen Empire.\n\nThe change of circumstances in the region weakens Krolgul and his influence, and he returns to Shammat. Incent, now \"recovered\" from his illness, decides that he is going to help Krolgul. Klorathy, still Incent's custodian, follows him to Shammat.\n\n\n\n\n"}
{"id": "4478297", "url": "https://en.wikipedia.org/wiki?curid=4478297", "title": "Timeline of Macintosh models", "text": "Timeline of Macintosh models\n\nThis timeline of Macintosh models lists all major types of Macintosh computers produced by Apple Inc. in order of introduction date. Macintosh Performa models were often physically identical to other models, in which case they are omitted in favor of the identical twin. Also not listed are model numbers that identify software bundles. For example, the Performa 6115CD and 6116CD differed only in software and were identical to the Power Macintosh 6100, so only the 6100 is listed below. The Apple Network Server and Apple Lisa are included, as they filled high-end niches of the Macintosh line despite not directly running Mac OS.\n\n\"*The Lisa is included, although it is not a Macintosh, due to its compatibility with Macintosh software and common use as an early Macintosh development platform.\"\n\n\"*The Apple Network Server is technically not a Macintosh, but is included here for completeness.\"\n\n\n\n"}
{"id": "22616314", "url": "https://en.wikipedia.org/wiki?curid=22616314", "title": "Titan II GLV", "text": "Titan II GLV\n\nThe Titan II GLV (Gemini Launch Vehicle) or Gemini-Titan II was an American expendable launch system derived from the Titan II missile, which was used to launch twelve Gemini missions for NASA between 1964 and 1966. Two unmanned launches followed by ten manned ones were conducted from Launch Complex 19 at the Cape Canaveral Air Force Station, starting with Gemini 1 on April 8, 1964.\n\nThe Titan II was a two-stage liquid-fuel rocket, using a hypergolic propellant combination of Aerozine 50 fuel and nitrogen tetroxide oxidizer. The first stage was powered by an LR87 engine (with two combustion chambers and nozzles, fed by a single set of turbomachinery), and the second stage was propelled by an LR-91 engine.\n\nIn addition to greater payload capability, the Titan II promised greater reliability than the Atlas LV-3B which had been selected for Project Mercury, because Titan's hypergolic-fueled engines contained far fewer components.\n\nSeveral modifications were made to the Titan missile to man-rate it for Project Gemini:\n\n\nModifications were overseen by the Air Force Systems Command. The Aerojet company, the manufacturer of the Titan's engines, had released a revised model during mid-1963 due to deficiencies in the original design, and also to attempt to improve manufacturing procedures.\n\nFilm footage of Gemini 10's launch revealed that the first stage oxidizer tank ruptured shortly after staging and released a cloud of NO. As first stage telemetry had been terminated at staging, there was no data other than photographic/visual evidence to go by, however the conclusion was that either loose debris struck the oxidizer tank dome or else exhaust from the second stage engine had burned through it.\n\nGemini 12's launch vehicle also experienced a tank rupture after staging and film review of Titan II ICBM launches found several occurrences of this phenomenon. Since this did not appear to pose any safety risks to the astronauts, NASA decided that it was not a concern.\n\nDuring Titan II ICBM development, it had been found that the first stage turbopump gearbox was prone to total failure caused by resonant vibration in the idler gear. This problem had not occurred on actual launches, but only static firing tests. This was considered to be a critical item to fix. Aerojet developed a totally redesigned gearbox, and all of Gemini launch vehicles except for the unmanned Gemini 1 used it.\n\nThere was also a potentially serious problem with the turbopump bearings which led to more design changes, however the odds of failing on a Gemini launch were slim to nil since GLV boosters used specially selected and tested bearings, in addition the turbopumps would be \"hot fired\" as part of prelaunch checks.\n\nCombustion instability in the second stage engine was also a concern although that too had only been witnessed in static firing runs. A new injector with improved baffling was developed for the engine and flight-tested on a Titan IIIC launch; all GLVs from Gemini 8 onwards incorporated it.\n\nAfter a Titan II propellant feed line was found to have some damage during factory inspections, NASA put out the requirement that all GLV propellant lines had to be X-rayed in order to prevent a potentially disastrous fuel leak during launch. X-ray tests later found several more damaged propellant lines, most likely due to careless handling.\n\nThe most significant issue in man-rating the Titan II was resolving problems with resonant vibration known as \"pogo\" (since the action was said to resemble that of a pogo stick) that could produce g-forces sufficient to incapacitate astronauts, but the Air Force were not interested in helping NASA with a problem that did not affect the ICBM program and could potentially delay it, or require major modifications to the design. However, Martin-Marietta argued that the pogo problem could be fixed fairly easily, and also the Air Force began to develop more of an interest in man-rating the Titan II due to the proposed Manned Orbiting Laboratory program. The primary changes made to resolve pogo were adding oxidizer standpipes, increasing the pressure in the propellant tanks, and adding a mechanical accumulator to the fuel suction side.\n\nAnother nuisance problem that occurred during the Gemini program was code-named \"Green Man\" and involved momentary pitch oscillations of the Titan second stage following engine cutoff. This phenomenon had happened on both Gemini and unmanned Titan II/III flights and had resulted in the failure of the ablative skirt on the second stage at least twice (those instances were dubbed \"Brown Man\"). Investigation following skirt failure on a Titan IIIC launch concluded that pressure buildup in the ablative skirt caused the pitch oscillations, but NASA decided that there was probably little chance of loose debris from the skirt contacting the Gemini spacecraft, so no corrective action had to be taken and in any case, the Titan IIIC incident was found to be the result of poor quality control which would not affect the more tightly supervised Gemini program.\n\nThe assembly of these rockets was done at Martin-Marietta's plant in Baltimore, Maryland, so not to interfere with missile work at the one in Denver, Colorado, although this also saved the former plant from a planned shutdown. As with the Mercury-Atlas launch vehicles, a high degree of workmanship was stressed as well as more thorough testing of components and improved handling procedures compared with Titans designed for unmanned flights. \nThe Titan II had a much higher thrust-to-weight ratio than the Saturn V. Astronauts experienced almost 6G before the second stage stopped firing at altitude. Richard F. Gordon Jr. compared the Titan II to \"a young fighter pilot's ride. It's faster than the Saturn's old man's ride.\" Frank Borman said that simulations did not prepare him for the \"almost deafening\" noise, which he compared to a jet's afterburner or large train. Walter Schirra and Gordon Cooper reported that the ride was smoother than on the Atlas, however.\n\n\n"}
{"id": "27359923", "url": "https://en.wikipedia.org/wiki?curid=27359923", "title": "William Abler", "text": "William Abler\n\nWilliam L. Abler or simply known as Bill Abler is a paleontologist who has mostly studied the teeth of dinosaurs.\n\nHe has studied tyrannosaurine teeth and has concluded that \"Tyrannosaurus\" had infectious saliva that could have helped it kill prey. In modern animals this saliva can be seen in many monitor lizards, such as \"Varanus komodoensis\", commonly known as the Komodo dragon or Komodo monitor.\n\nHe has written papers and books on paleontology, one of them is \"The Teeth of the Tyrannosaurs\". He has written a book too, on the human being's place in nature, called \"Structure of Matter, Structure of Mind: Man's Place in Nature, Reconsidered\".\n"}
{"id": "970672", "url": "https://en.wikipedia.org/wiki?curid=970672", "title": "William J. Knight", "text": "William J. Knight\n\nWilliam John \"Pete\" Knight (November 18, 1929 – May 7, 2004), (Col, USAF), was an American aeronautical engineer, politician, Vietnam War combat pilot, test pilot, and astronaut in the X-20 Dyna-Soar and North American X-15 programs. Knight holds the world's speed record for flight in a winged, powered aircraft.\n\nAs a politician, he is noted as the author of California Proposition 22, which forbade the state from performing or recognizing same-sex marriage.\n\nKnight was born November 18, 1929 in Noblesville, Indiana, to parents William T. Knight (1906–1968) and Mary Emma Knight (1909–1959). Following high school, Knight attended Butler University and Purdue University. He graduated with a Bachelor of Science degree in Aeronautical Engineering from the U.S. Air Force Institute of Technology in 1958.\n\nKnight was married to Helena Stone and they had three sons, Steve, Peter, and David. Helena preceded Knight in death. Knight remarried and at his death in 2004 he was survived by his widow Gail, a brother, three sons, four stepchildren and 15 grandchildren.\n\nKnight joined the United States Air Force in 1951. While only a Second Lieutenant, he flew an F-89 at the National Air Show in 1954 and won the Allison Jet Trophy.\n\nStarting in 1958, following his graduation from both U.S. Air Force Institute of Technology and the Air Force Experimental Flight Test Pilot School, Knight served as a test pilot at Edwards Air Force Base, California. He was a project test pilot for the F-100, F-101 Voodoo, F-104 Starfighter and later, T-38 and F-5 test programs. In 1960, he was one of six test pilots selected to fly the X-20 Dyna-Soar, which was slated to become the first winged orbital space vehicle capable of lifting reentries and conventional landings. After the X-20 program was canceled in 1963, he completed the astronaut training curriculum through the Aerospace Research Pilot School at Edwards AFB and was selected to fly the North American X-15.\n\nHe had more than his share of eventful flights in the X-15. While climbing through at Mach 4.17 on June 29, 1967, he suffered a total electrical failure and all onboard systems shut down. After reaching a maximum altitude of , he calmly set up a visual approach and, resorting to old-fashioned \"seat-of-the-pants\" flying, he glided down to a safe emergency landing at Mud Lake, Nevada. For his remarkable feat of airmanship that day, he earned a Distinguished Flying Cross.\nOn October 3, 1967, Knight set a world aircraft speed record for manned aircraft by piloting the X-15A-2 to (Mach 6.72), a record that still stands today. During 16 flights in the aircraft, Knight also became one of only five pilots to earn their Astronaut Wings by flying an airplane in space, reaching an altitude of .\n\nAfter nearly ten years of test flying at Edwards AFB, he went to Southeast Asia in 1968 where he completed a total of 253 combat missions in the F-100 during the Vietnam War. Following his combat tour, he served as test director during development of the F-15 Eagle at Wright Patterson Air Force Base in Dayton, Ohio. He also was the Program Director for the International Fighter (F-5) Program at Wright-Patterson. In 1979, he returned to Edwards AFB, and served as a test pilot for the F-16 Fighting Falcon.\n\nAfter 32 years of service and more than 6,000 hours in the cockpits of more than 100 different aircraft, he retired from the U.S. Air Force as a Colonel in 1982.\n\nIn 1984, he was elected to the city council of Palmdale, California, and four years later became the city's first elected mayor. In 1992, he was elected to serve in the California State Assembly representing the 36th District. He served in the State Senate representing the 17th District from 1996 until his death on May 7, 2004. Knight's youngest son, Steve Knight served as Assemblyman for the 36th Assembly District from 2008 to 2012, the seat previously held by his father.\n\nDuring his term in the Senate, Knight gained statewide attention in 2000 as the author of Proposition 22, a.k.a. the \"Knight Initiative,\" the purpose of which was to ban same-sex marriage: \"Only marriage between a man and a woman is valid or recognized in California.\" The proposition passed with 61.4% approval and 38.6% against. On March 9, 2004, Knight's son, David Knight, who is gay, married his partner during the period when San Francisco performed same-sex marriages in defiance of state law. These marriages were later nullified by the California Supreme Court in 2004. The Court later found Proposition 22 to be unconstitutional in \"In re Marriage Cases\" (2008).\n\nIn addition to his gay son, Knight also had a younger brother who died of AIDS-related complications in 1995 at age 60. Of his younger brother, Knight would only say, \"We never talked about it.\" \n\n\n\nIn the city of Palmdale, Pete Knight High School was opened in his memory. The school began its first year in the school year of 2003-2004 and celebrated its first graduating class in 2007.\n\n\n"}
{"id": "1453546", "url": "https://en.wikipedia.org/wiki?curid=1453546", "title": "Xplanet", "text": "Xplanet\n\nXplanet is a renderer for planetary and solar system images, capable of producing various types of graphics depicting the solar system. It is normally used to create computer wallpapers, which may be updated with the latest cloud maps or the regions of Earth which are in sunlight. Xplanet is free software released under the GNU GPL.\n\nXplanet can be used to produce projected maps of any planet (but typically Earth), for example mollweide projections which show the whole earth at once, or mercator projections with a rectangular appearance suitable for filling the screen.\n\nIt is possible to overlay clouds or text (such as the location of recent events) onto these maps; a popular option is shading areas currently experiencing night.\n\nXplanet can also be used to render more general views of objects in the solar system, such as a view of the Earth from the moon. In more recent versions, Xplanet depicts eclipses, and some of its images show Jupiter's moons casting an eclipse onto the planet.\n\nXplanet runs on Linux, Mac OS X and other Unix operating systems and also on Microsoft Windows, and was derived from an older Unix application called \"xearth\".\n\nIt can either generate wallpaper, save the resulting image, or produce textual output detailing the locations of various objects.\n\nConfiguration is done by modifying a text file. The Windows version comes with a simple editor called winXPlanetBG to assist in updating the configurations and helps to download the cloud maps automatically. OSXplanet is an interactive wallpaper derivative for the Mac OS X.\n\nXplanetFX is a GTK-frontend for Xplanet under Linux. It provides a simple to use GUI to configure Xplanet and schedule renderings. It also claims to produce higher quality renderings. XplanetFX is free software released under a permissive vanity license.\n\n\n"}
