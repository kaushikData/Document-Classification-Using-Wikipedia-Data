{"id": "7974905", "url": "https://en.wikipedia.org/wiki?curid=7974905", "title": "Al-Kuz", "text": "Al-Kuz\n\nAl-Kuz is an impact crater on the anti-Saturn hemisphere of Saturn's moon Enceladus. Al-Kuz was first observed in \"Cassini\" images during that mission's March 2005 flyby of Enceladus. It is located at 18.7° South Latitude, 178.2° West Longitude, and is 9.3 kilometers across. Since the crater's formation, numerous southwest-northeast trending fractures cut across the crater, forming canyons several hundred meters deep along the crater's rim. In addition, a smaller impact occurred along the northern crater wall, forming a crater 4 kilometers wide.\n\nAl-Kuz is named after one of the barber's six brothers in \"The Hunchback's Tale\" in \"The Book of One Thousand and One Nights\".\n"}
{"id": "44210836", "url": "https://en.wikipedia.org/wiki?curid=44210836", "title": "Anna Kikina", "text": "Anna Kikina\n\nAnna Yuryevna Kikina (born 27 August 1984 in Novosibirsk) is a Russian engineer and test cosmonaut, selected in 2012.\n\nKikina graduated with honors from the Novosibirsk State Academy of Water transport.\n"}
{"id": "53905375", "url": "https://en.wikipedia.org/wiki?curid=53905375", "title": "Anthropology of food", "text": "Anthropology of food\n\nAnthropology of food is a sub-discipline of anthropology that connects an ethnographic and historical perspective with contemporary social issues in food production and consumption systems. \n\nAlthough early anthropological accounts often dealt with cooking and eating as part of ritual or daily life, food was rarely regarded as the central point of academic focus. This changed in the later half of the 20th century, when foundational work by Mary Douglas, Marvin Harris, Arjun Appadurai, Jack Goody, and Sidney Mintz cemented the study of food as a key insight into modern social life. Mintz is known as the \"Father of food anthropology\" for his work \"Sweetness and Power (1985),\" which linked British demand for sugar with the creation of empire and exploitative industrial labor conditions. \n\nResearch has traced the material and symbolic importance of food, as well as how they intersect. Examples of ongoing themes are food as a form of differentiation, commensality, and food's role in industrialization and globalizing labor and commodity chains. \n\nSeveral related and interdisciplinary academic programs exist in the US and UK (listed under Food studies institutions).\n\n"}
{"id": "40977001", "url": "https://en.wikipedia.org/wiki?curid=40977001", "title": "Asama virus", "text": "Asama virus\n\nAsama virus (ASAV) is a single-stranded, enveloped, negative-sense RNA hantavirus. The hantavirus was found in Japan after analyzing a Japanese shrew mole. Hantaviruses harbored by shrews are genetically closer to ASAV than to hantaviruses harbored by rodents. Host-switching may be evident in the future due to the viruses closeness to soricine shrew-borne hantaviruses. The detection of the asama virus was the first hantavirus found in the family Talpidae, which includes shrew moles. Thoughts on hantavirus evolutionary history has expanded due to the discovery of ASAV.\n\nAsama virus was isolated through RNA extracts from lung tissues of the Japanese shrew mole (Urotrichus talpoides), captured in Japan between February and April 2008. It is one of the first hantaviruses found in a mole.\n\nAsama virus is genetically closer to other hantaviruses harbored by shrews than by rodents. However, the nucleocapsid protein is similar to that of rodent and shrew-borne hantaviruses. Phylogenetic analyses positions it closest to soricine shrew-borne hantaviruses. This suggests a possible host-switching event in the distant past.\n\nAsama virus is related to soricine shrew-borne hantaviruses that are found in North America, Europe, and Asia. This relation was discovered through phylogenetic analyses. The relationship between the two hantaviruses may suggest parallel evolution associated with cross-species transmission.\n\n"}
{"id": "580", "url": "https://en.wikipedia.org/wiki?curid=580", "title": "Astronomer", "text": "Astronomer\n\nAn astronomer is a scientist in the field of astronomy who focuses their studies on a specific question or field outside the scope of Earth. They observe astronomical objects such as stars, planets, moons, comets, and galaxies – in either observational (by analyzing the data) or theoretical astronomy. Examples of topics or fields astronomers study include planetary science, solar astronomy, the origin or evolution of stars, or the formation of galaxies. Related but distinct subjects like physical cosmology, which studies the Universe as a whole.\n\nAstronomers usually fall under either of two main types: observational and theoretical. Observational astronomers make direct observations of celestial objects and analyze the data. In contrast, theoretical astronomers create and investigate models of things that cannot be observed. Because it takes millions to billions of years for a system of stars or a galaxy to complete a life cycle, astronomers must observe snapshots of different systems at unique points in their evolution to determine how they form, evolve, and die. They use these data to create models or simulations to theorize how different celestial objects work.\n\nFurther subcategories under these two main branches of astronomy include planetary astronomy, galactic astronomy, or physical cosmology.\n\nHistorically, astronomy was more concerned with the classification and description of phenomena in the sky, while astrophysics attempted to explain these phenomena and the differences between them using physical laws. Today, that distinction has mostly disappeared and the terms \"astronomer\" and \"astrophysicist\" are interchangeable. Professional astronomers are highly educated individuals who typically have a Ph.D. in physics or astronomy and are employed by research institutions or universities. They spend the majority of their time working on research, although they quite often have other duties such as teaching, building instruments, or aiding in the operation of an observatory.\n\nThe number of professional astronomers in the United States is actually quite small. The American Astronomical Society, which is the major organization of professional astronomers in North America, has approximately 7,000 members. This number includes scientists from other fields such as physics, geology, and engineering, whose research interests are closely related to astronomy. The International Astronomical Union comprises almost 10,145 members from 70 different countries who are involved in astronomical research at the Ph.D. level and beyond.\nContrary to the classical image of an old astronomer peering through a telescope through the dark hours of the night, it is far more common to use a charge-coupled device (CCD) camera to record a long, deep exposure, allowing a more sensitive image to be created because the light is added over time. Before CCDs, photographic plates were a common method of observation. Modern astronomers spend relatively little time at telescopes usually just a few weeks per year. Analysis of observed phenomena, along with making predictions as to the causes of what they observe, takes the majority of observational astronomers' time.\n\nAstronomers who serve as faculty spend much of their time teaching undergraduate and graduate classes. Most universities also have outreach programs including public telescope time and sometimes planetariums as a public service to encourage interest in the field.\n\nThose who become astronomers usually have a broad background in maths, sciences and computing in high school. Taking courses that teach how to research, write and present papers are also invaluable. In college/university most astronomers get a Ph.D. in astronomy or physics.\n\nWhile there is a relatively low number of professional astronomers, the field is popular among amateurs. Most cities have amateur astronomy clubs that meet on a regular basis and often host star parties. The Astronomical Society of the Pacific is the largest general astronomical society in the world, comprising both professional and amateur astronomers as well as educators from 70 different nations. Like any hobby, most people who think of themselves as amateur astronomers may devote a few hours a month to stargazing and reading the latest developments in research. However, amateurs span the range from so-called \"armchair astronomers\" to the very ambitious, who own science-grade telescopes and instruments with which they are able to make their own discoveries and assist professional astronomers in research.\n\n\n\n"}
{"id": "672259", "url": "https://en.wikipedia.org/wiki?curid=672259", "title": "Batalin–Vilkovisky formalism", "text": "Batalin–Vilkovisky formalism\n\nIn theoretical physics, the Batalin–Vilkovisky (BV) formalism (named for Igor Batalin and Grigori Vilkovisky) was developed as a method for determining the ghost structure for Lagrangian gauge theories, such as gravity and supergravity, whose corresponding Hamiltonian formulation has constraints not related to a Lie algebra (i.e., the role of Lie algebra structure constants are played by more general structure functions). The BV formalism, based on an action that contains both fields and \"antifields\", can be thought of as a vast generalization of the original BRST formalism for pure Yang–Mills theory to an arbitrary Lagrangian gauge theory. Other names for the Batalin–Vilkovisky formalism are field-antifield formalism, Lagrangian BRST formalism, or BV–BRST formalism. It should not be confused with the Batalin–Fradkin–Vilkovisky (BFV) formalism, which is the Hamiltonian counterpart.\n\nIn mathematics, a Batalin–Vilkovisky algebra is a graded supercommutative algebra (with a unit 1) with a second-order nilpotent operator Δ of degree −1. More precisely, it satisfies the identities\n\nOne often also requires normalization:\n\n\nA Batalin–Vilkovisky algebra becomes a Gerstenhaber algebra if one defines the Gerstenhaber bracket by\nOther names for the Gerstenhaber bracket are Buttin bracket, antibracket, or odd Poisson bracket. The antibracket satisfies\n\nThe normalized operator is defined as\nIt is often called the odd Laplacian, in particular in the context of odd Poisson geometry. It \"differentiates\" the antibracket\nThe square formula_16 of the normalized formula_15 operator is a Hamiltonian vector field with odd Hamiltonian Δ(1)\nwhich is also known as the modular vector field. Assuming normalization Δ(1)=0, the odd Laplacian formula_19 is just the Δ operator, and the modular vector field formula_20 vanishes.\n\nIf one introduces the left multiplication operator formula_21 as \nand the supercommutator [,] as \nfor two arbitrary operators \"S\" and \"T\", then the definition of the antibracket may be written compactly as\nand the second order condition for Δ may be written compactly as\nwhere it is understood that the pertinent operator acts on the unit element 1. In other words, formula_26 is a first-order (affine) operator, and formula_27 is a zeroth-order operator.\n\nThe classical master equation for an even degree element \"S\" (called the [[Action (physics)|action]]) of a Batalin–Vilkovisky algebra is the equation\nThe quantum master equation for an even degree element \"W\" of a Batalin–Vilkovisky algebra is the equation\nor equivalently,\nAssuming normalization Δ(1) = 0, the quantum master equation reads\n\nIn the definition of a generalized BV algebra, one drops the second-order assumption for Δ. One may then define an infinite hierarchy of higher brackets of degree −1\nThe brackets are (graded) symmetric\nwhere formula_34 is a permutation, and formula_35 is the [[Koszul sign]] of the permutation \nThe brackets constitute a [[homotopy Lie algebra]], also known as an formula_37 algebra, which satisfies generalized Jacobi identities \nThe first few brackets are:\nIn particular, the one-bracket formula_44 is the odd Laplacian, and the two-bracket formula_45 is the antibracket up to a sign. The first few generalized Jacobi identities are:\nwhere the [[Jacobiator]] for the two-bracket formula_56 is defined as\n\nThe Δ operator is by definition of n'th order if and only if the (\"n\" + 1)-bracket formula_58 vanishes. In that case, one speaks of a BV n-algebra. Thus a BV 2-algebra is by definition just a BV algebra. The Jacobiator formula_59 vanishes within a BV algebra, which means that the antibracket here satisfies the Jacobi identity. A BV 1-algebra that satisfies normalization Δ(1) = 0 is the same as a [[differential graded algebra|differential graded algebra (DGA)]] with differential Δ. A BV 1-algebra has vanishing antibracket.\n\nLet there be given an (n|n) [[supermanifold]] with an odd Poisson bi-vector formula_60 and a Berezin volume density formula_61, also known as a P-structure and an S-structure, respectively. Let the local coordinates be called formula_62. Let the derivatives formula_63 and \ndenote the [[left derivative|left]] and [[right derivative]] of a function \"f\" wrt. formula_62, respectively. The odd Poisson bi-vector formula_60 satisfies more precisely\nUnder change of coordinates formula_70 the odd Poisson bi-vector formula_60 \nand Berezin volume density formula_61 transform as\nwhere \"sdet\" denotes the [[superdeterminant]], also known as the Berezinian.\nThen the odd Poisson bracket is defined as\nA Hamiltonian vector field formula_76 with Hamiltonian \"f\" can be defined as\nThe (super-)[[divergence]] of a vector field formula_78 is defined as\nRecall that Hamiltonian vector fields are divergencefree in even Poisson geometry because of Liouville's Theorem. \nIn odd Poisson geometry the corresponding statement does not hold. The odd Laplacian formula_80 measures the failure of Liouville's Theorem. Up to a sign factor, it is defined as one half the divergence of the corresponding Hamiltonian vector field, \nThe odd Poisson structure formula_60 and Berezin volume density formula_61 are said to be compatible if the modular vector field formula_20 vanishes. In that case the odd Laplacian formula_80 is a BV Δ operator with normalization Δ(1)=0. The corresponding BV algebra is the algebra of functions.\n\nIf the odd Poisson bi-vector formula_60 is invertible, one has an odd [[Symplectic geometry|symplectic]] manifold. In that case, there exists an odd Darboux Theorem. That is, there exist local Darboux coordinates, i.e., coordinates formula_87, and momenta formula_88, of degree\nsuch that the odd Poisson bracket is on Darboux form\nIn [[theoretical physics]], the coordinates formula_91 and momenta formula_92 are called fields and antifields, and are typically denoted formula_93 and formula_94, respectively.\nacts on the vector space of [[semidensities]], and is a globally well-defined operator on the atlas of Darboux neighborhoods. Khudaverdian's formula_96 operator depends only on the P-structure. It is manifestly nilpotent formula_97, and of degree −1. Nevertheless, it is technically not a BV Δ operator as the vector space of semidensities has no multiplication. (The product of two semidensities is a density rather than a semidensity.) Given a fixed density formula_61, one may construct a nilpotent BV Δ operator as\nwhose corresponding BV algebra is the algebra of functions, or equivalently, [[scalar (physics)|scalar]]s. The odd symplectic structure formula_60 and density formula_61 are compatible if and only if Δ(1) is an odd constant.\n\n\n\n\n[[Category:Algebras]]\n[[Category:Quantum field theory]]\n[[Category:Symplectic geometry]]\n[[Category:Theoretical physics]]"}
{"id": "1072857", "url": "https://en.wikipedia.org/wiki?curid=1072857", "title": "Biosignature", "text": "Biosignature\n\nA biosignature (sometimes called chemical fossil or molecular fossil) is any substance – such as an element, isotope, molecule, or phenomenon – that provides scientific evidence of past or present life. Measurable attributes of life include its complex physical and chemical structures and also its utilization of free energy and the production of biomass and wastes. Due to its unique characteristics, a biosignature can be interpreted as having been produced by living organisms; however, it is important that they not be considered definitive because there is no way of knowing in advance which ones are universal to life and which ones are unique to the peculiar circumstances of life on Earth. Nonetheless, life forms are known to shed unique chemicals, including DNA, into the environment as evidence of their presence in a particular location.\n\nThe ancient record on Earth provides an opportunity to see what geochemical signatures are produced by microbial life and how these signatures are preserved over geologic time. Some related disciplines such as geochemistry, geobiology, and geomicrobiology often use biosignatures to determine if living organisms are or were present in a sample. These possible biosignatures include: (a) microfossils and stromatolites; (b) molecular structures (biomarkers) and isotopic compositions of carbon, nitrogen and hydrogen in organic matter; (c) multiple sulfur and oxygen isotope ratios of minerals; and (d) abundance relationships and isotopic compositions of redox sensitive metals (e.g., Fe, Mo, Cr, and rare earth elements).\n\nFor example, the particular fatty acids measured in a sample can indicate which types of bacteria and archaea live in that environment. Another example are the long-chain fatty alcohols with more than 23 atoms that are produced by planktonic bacteria. When used in this sense, geochemists often prefer the term biomarker. Another example is the presence of straight-chain lipids in the form of alkanes, alcohols an fatty acids with 20-36 carbon atoms in soils or sediments. Peat deposits are an indication of originating from the epicuticular wax of higher plants.\n\nLife processes may produce a range of biosignatures such as nucleic acids, lipids, proteins, amino acids, kerogen-like material and various morphological features that are detectable in rocks and sediments.\nMicrobes often interact with geochemical processes, leaving features in the rock record indicative of biosignatures. For example, bacterial micrometer-sized pores in carbonate rocks resemble inclusions under transmitted light, but have distinct size, shapes and patterns (swirling or dendritic) and are distributed differently from common fluid inclusions. A potential biosignature is a phenomenon that \"may\" have been produced by life, but for which alternate abiotic origins may also be possible.\n\nAstrobiological exploration is founded upon the premise that biosignatures encountered in space will be recognizable as extraterrestrial life. The usefulness of a biosignature is determined, not only by the probability of life creating it, but also by the improbability of nonbiological (abiotic) processes producing it. Concluding that evidence of an extraterrestrial life form (past or present) has been discovered, requires proving that a possible biosignature was produced by the activities or remains of life. As with most scientific discoveries, discovery of a biosignature will require of evidence building up until no other explanation exists.\n\nPossible examples of a biosignature might be complex organic molecules and/or structures whose formation is virtually unachievable in the absence of life. For example, cellular and extracellular morphologies, biomolecules in rocks, bio-organic molecular structures, chirality, biogenic minerals, biogenic stable isotope patterns in minerals and organic compounds, atmospheric gases, and remotely detectable features on planetary surfaces, such as photosynthetic pigments, etc.\n\nIn general, biosignatures and habitable environment signatures can be grouped into ten broad categories:\n\nNo single compound will prove life once existed. Rather, it will be distinctive patterns present in any organic compounds showing a process of selection. For example, membrane lipids left behind by degraded cells will be concentrated, have a limited size range, and comprise an even number of carbons. Similarly, life only uses left-handed amino acids. Biosignatures need not be chemical, however, and can also be suggested by a distinctive magnetic biosignature.\n\nOn Mars, surface oxidants and UV radiation will have altered or destroyed organic molecules at or near the surface. One issue that may add ambiguity in such a search is the fact that, throughout Martian history, abiogenic organic-rich chondritic meteorites have undoubtedly rained upon the Martian surface. At the same time, strong oxidants in Martian soil along with exposure to ionizing radiation might alter or destroy molecular signatures from meteorites or organisms. An alternative approach would be to seek concentrations of buried crystalline minerals, such as clays and evaporites, which may protect organic matter from the destructive effects of ionizing radiation and strong oxidants. The search for Martian biosignatures has become\nmore promising due to the discovery that surface and near-surface aqueous environments existed on Mars at the same time when biological organic matter was being preserved in ancient aqueous sediments on Earth.\n\nAnother possible biosignature might be morphology since the shape and size of certain objects may potentially indicate the presence of past or present life. For example, microscopic magnetite crystals in the Martian meteorite ALH84001 were the longest-debated of several potential biosignatures in that specimen because it was believed until recently that only bacteria could create crystals of their specific shape. For example, the possible biomineral studied in the Martian ALH84001 meteorite includes putative microbial fossils, tiny rock-like structures whose shape was a potential biosignature because it resembled known bacteria. Most scientists ultimately concluded that these were far too small to be fossilized cells. A consensus that has emerged from these discussions, and is now seen as a critical requirement, is the demand for further lines of evidence in addition to any morphological data that supports such extraordinary claims. Currently, the scientific consensus is that \"morphology alone cannot be used unambiguously as a tool for primitive life detection.\" Interpretation of morphology is notoriously subjective, and its use alone has led to numerous errors of interpretation.\n\nThe atmospheric properties of exoplanets are of particular importance, as atmospheres provide the most likely observables for the near future, including habitability indicators and biosignatures. Over billions of years, the processes of life on a planet would result in a mixture of chemicals unlike anything that could form in an ordinary chemical equilibrium. For example, large amounts of oxygen and small amounts of methane are generated by life on Earth.\n\nAlso, an exoplanet's color —or reflectance spectrum— might give away the presence of vast colonies of life forms at its surface.\n\nThe presence of methane in the atmosphere of Mars indicates that there must be an active source on the planet, as it is an unstable gas. Furthermore, current photochemical models cannot explain the presence of methane in the atmosphere of Mars and its reported rapid variations in space and time. Neither its fast appearance nor disappearance can be explained yet. To rule out a biogenic origin for the methane, a future probe or lander hosting a mass spectrometer will be needed, as the isotopic proportions of carbon-12 to carbon-14 in methane could distinguish between a biogenic and non-biogenic origin, similarly to the use of the δ13C standard for recognizing biogenic methane on Earth. In June, 2012, scientists reported that measuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, \"...low H/CH ratios (less than approximately 40) indicate that life is likely present and active.\" The planned ExoMars Trace Gas Orbiter, launched in March 2016 to Mars, will study atmospheric trace gases and will attempt to characterize potential biochemical and geochemical processes at work.\n\nOther scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres. Habitability indicators and biosignatures must be interpreted within a planetary and environmental context. For example, the presence of oxygen and methane together could indicate the kind of extreme thermochemical disequilibrium generated by life. Two of the top 14,000 proposed atmospheric biosignatures are dimethyl sulfide and chloromethane (). An alternative biosignature is the combination of methane and carbon dioxide.\n\nScientific observations include the possible identification of biosignatures through indirect observation. For example, electromagnetic information through infrared radiation telescopes, radio-telescopes, space telescopes, etc. From this discipline, the hypothetical electromagnetic radio signatures that SETI scans for would be a biosignature, since a message from intelligent aliens would certainly demonstrate the existence of extraterrestrial life.\n\nThe \"Viking\" missions to Mars in the 1970s conducted the first experiments which were explicitly designed to look for biosignatures on another planet. Each of the two \"Viking\" landers carried three life-detection experiments which looked for signs of metabolism; however, the results were declared inconclusive.\n\n\nThe \"Curiosity\" rover from the Mars Science Laboratory mission, with its \"Curiosity\" rover is currently assessing the potential past and present habitability of the Martian environment and is attempting to detect biosignatures on the surface of Mars. Considering the MSL instrument payload package, the following classes of biosignatures are within the MSL detection window: organism morphologies (cells, body fossils, casts), biofabrics (including microbial mats), diagnostic organic molecules, isotopic signatures, evidence of biomineralization and bioalteration, spatial patterns in chemistry, and biogenic gases. The \"Curiosity\" rover targets outcrops to maximize the probability of detecting 'fossilized' organic matter preserved in sedimentary deposits.\n\nThe 2016 ExoMars Trace Gas Orbiter (TGO) is a Mars telecommunications orbiter and atmospheric gas analyzer mission. It delivered the \"Schiaparelli\" EDM lander and then began to settle into its science orbit to map the sources of methane on Mars and other gases, and in doing so, will help select the landing site for the ExoMars rover to be launched in 2020. The primary objective of the ExoMars rover mission is the search for biosignatures on the surface and subsurface by using a drill able to collect samples down to a depth of , away from the destructive radiation that bathes the surface.\n\n\nThe Mars 2020 rover, planned to launch in 2020, is intended to investigate an astrobiologically relevant ancient environment on Mars, investigate its surface geological processes and history, including the assessment of its past habitability, the possibility of past life on Mars, and potential for preservation of biosignatures within accessible geological materials. In addition, it will cache the most interesting samples for possible future transport to Earth.\n\nThe planned Dragonfly lander/aircraft to launch in 2025, would seek evidence of biosignatures on the organic-rich surface and atmosphere of Titan, as well as study its possible prebiotic primordial soup.\n"}
{"id": "56261151", "url": "https://en.wikipedia.org/wiki?curid=56261151", "title": "CO stripping", "text": "CO stripping\n\nIn electrochemistry, CO stripping is a special process of voltammetry where a monolayer of carbon monoxide already adsorbed on the surface of an electrocatalyst is electrochemically oxidized and thus removed from the surface. A well-known process of this type is CO stripping on Pt/C electrocatalysts in which the electrooxidation peak occurs somewhere between 0.5 to 0.9 V depending on the characteristics and structural properties of the specimen.\n"}
{"id": "21257049", "url": "https://en.wikipedia.org/wiki?curid=21257049", "title": "Chauffeur's fracture", "text": "Chauffeur's fracture\n\nChauffeur's fracture, also known as Hutchinson fracture, is a type of fracture of the forearm, specifically the radial styloid process. The injury is typically caused by compression of the scaphoid bone of the hand against the styloid process of the distal radius. It can be caused by falling onto an outstretched hand (FOOSH). Treatment is often open reduction and internal fixation, which is surgical realignment of the bone fragments and fixation with pins, screws, or plates.\n\nThe name originates from early chauffeurs who sustained these injuries when the car back-fired while the chauffeur was hand-cranking to start the car. The back-fire forced the crank backward into the chauffeur's palm and produced the characteristic styloid fracture.\n"}
{"id": "5285100", "url": "https://en.wikipedia.org/wiki?curid=5285100", "title": "Comparison of mobile phone standards", "text": "Comparison of mobile phone standards\n\nThis is a comparison of standards of mobile phones. A new generation of cellular standards has appeared approximately every tenth year since 1G systems were introduced in 1979 and the early to mid-1980s.\n\nGlobal System for Mobile Communications (GSM, around 80–85% market share) and IS-95 (around 10–15% market share) were the two most prevalent 2G mobile communication technologies in 2007. In 3G, the most prevalent technology was UMTS with CDMA-2000 in close contention.\n\nAll radio access technologies have to solve the same problems: to divide the finite RF spectrum among multiple users as efficiently as possible. GSM uses TDMA and FDMA for user and cell separation. UMTS, IS-95 and CDMA-2000 use CDMA. WiMAX and LTE use OFDM.\n\n\nIn theory, CDMA, TDMA and FDMA have exactly the same spectral efficiency but practically, each has its own challenges – power control in the case of CDMA, timing in the case of TDMA, and frequency generation/filtering in the case of FDMA.\n\nFor a classic example for understanding the fundamental difference of TDMA and CDMA, imagine a cocktail party where couples are talking to each other in a single room. The room represents the available bandwidth:\n\n\n\n\n\nThis graphic compares the market shares of the different mobile standards.\n\nIn a fast-growing market, GSM/3GSM (red) grows faster than the market and is gaining market share, the CDMA family (blue) grows at about the same rate as the market, while other technologies (grey) are being phased out\n\nAs a reference, a comparison of mobile and non-mobile wireless Internet standards follows.\n"}
{"id": "3295052", "url": "https://en.wikipedia.org/wiki?curid=3295052", "title": "Compliance cost", "text": "Compliance cost\n\nA compliance cost is expenditure of time or money in conforming with government requirements such as legislation or regulation. For example, people or organisations registered for value added tax have the extra burden of having to keep detailed records of all input tax and output tax to facilitate the completion of VAT returns. This may necessitate them having to employ someone skilled in this field, which would be regarded a compliance cost.\n\nCompliance costs normally include all costs associated with obeying the law, including planning and administration, in addition to the direct time and money spent filing paperwork.\n\nBylaws with a high cost of compliance can suffer from not being taken seriously & often being broken. For example, jurisdictions which ban smoking in all public areas theoretically have higher rates of people smoking in public areas as it would be inconvenient for them to go all the way home. Lawmakers therefore need to consider cost of compliance.\n\nCompliance with tax laws, such as income tax or sales tax legislation, is a common topic of political debate, primarily because these taxes affect the majority of citizens in a society. By contrast, environmental regulations, such as those on sulfur dioxide emissions, only affect a minority of businesses within an economy.\n\n\n"}
{"id": "3238078", "url": "https://en.wikipedia.org/wiki?curid=3238078", "title": "Coriolis frequency", "text": "Coriolis frequency\n\nThe Coriolis frequency \"ƒ\", also called the Coriolis parameter or Coriolis coefficient, is equal to twice the rotation rate \"Ω\" of the Earth multiplied by the sine of the latitude \"φ\". \n\nThe rotation rate of the Earth (\"Ω\" = 7.2921 × 10 rad/s) can be calculated as 2\"π\" / \"T\" radians per second, where \"T\" is the rotation period of the Earth which is one \"sidereal\" day (23 hr 56 m 4.1 s). In the midlatitudes, the typical value for formula_2 is about 10 rad/s. Inertial oscillations on the surface of the earth have this frequency. These oscillations are the result of the Coriolis effect.\n\nConsider a body (for example a fixed volume of atmosphere) at latitude formula_3 moving at velocity formula_4 in the earth's rotating reference frame. In the local reference frame of the body, the vertical direction is parallel to the radial vector pointing from the center of the earth to the location of the body and the horizontal direction is perpendicular to this vertical direction (and hence in the meridional direction). The Coriolis force (proportional to formula_5), however, is perpendicular to the plane containing both the earth's angular velocity vector formula_6 (where formula_7) and the body's own velocity in the rotating reference frame formula_8. Thus, the Coriolis force is always at an angle formula_3 with the local vertical direction. The local horizontal direction of the Coriolis force is thus formula_10. This force acts to move the body along longitudes or in the meridional directions.\n\nSuppose the body is moving with a velocity formula_4 such that the centripetal and Coriolis (due to formula_12) forces on it are balanced. We then have\n\nwhere formula_14 is the radius of curvature of the path of object (defined by formula_15). Replacing formula_16 we obtain \n\nThus the Coriolis parameter, formula_18, is the angular velocity or frequency required to maintain a body at a fixed circle of latitude or zonal region. If the Coriolis parameter is large, the effect of the earth's rotation on the body is significant since it will need a larger angular frequency to stay in equilibrium with the Coriolis forces. Alternatively, if the Coriolis parameter is small, the effect of the earth's rotation is small since only a small fraction of the centripetal force on the body is canceled by the Coriolis force. Thus the magnitude of formula_18 strongly affects the relevant dynamics contributing to the body's motion. These considerations are captured in the nondimensionalized Rossby number.\n\nIn stability calculations, the rate of change of formula_18 along the meridional direction becomes significant. This is called the Rossby parameter and is usually denoted\n\nwhere formula_22 is the in the local direction of increasing meridian. This parameter becomes important, for example, in calculations involving Rossby waves.\n\n"}
{"id": "9454590", "url": "https://en.wikipedia.org/wiki?curid=9454590", "title": "Crab mentality", "text": "Crab mentality\n\nCrab mentality or crabs in a bucket (also barrel, basket or pot) is a way of thinking best described by the phrase \"if I can't have it, neither can you\". The metaphor refers to a pattern of behaviour noted in crabs when they are trapped in a bucket. While any one crab could easily escape, its efforts will be undermined by others, ensuring the group's collective demise.\n\nThe analogy in human behaviour is claimed to be that members of a group will attempt to reduce the self-confidence of any member who achieves success beyond the others, out of envy, resentment, spite, conspiracy, or competitive feelings, to halt their progress.\n\nThe impact of crab mentality on performance was quantified by a New Zealand study in 2015 which demonstrated up to an 18 per cent average exam result improvement for students when their grades were reported in a way that prevented others from knowing their position in published rankings.\n"}
{"id": "2977958", "url": "https://en.wikipedia.org/wiki?curid=2977958", "title": "Cyanocarbon", "text": "Cyanocarbon\n\nCyanocarbons are a group of chemical compounds that contain several cyanide functional groups. Such substances generally are classified as organic compounds, since they are formally derived from hydrocarbons by replacing one or more hydrogen atoms with a cyanide group. The parent member is C(CN) (tetracyanomethane, also known as carbon tetracyanide). Organic chemists often refer to cyanides as nitriles.\n\nIn general, cyanide is an electronegative substituent. Thus, for example, cyanide-substituted carboxylic acids tend to be stronger than the parents. The cyanide group can also stabilize anions by delocalizing negative charge as revealed by resonance structures.\n\n\"Cyanocarbons are organic compounds bearing enough cyano functional groups to significantly alter their chemical properties.\"\n\nImportant cyanocarbons:\n"}
{"id": "2457060", "url": "https://en.wikipedia.org/wiki?curid=2457060", "title": "De sphaera mundi", "text": "De sphaera mundi\n\nDe sphaera mundi (Latin title meaning \"On the Sphere of the World\", sometimes rendered \"The Sphere of the Cosmos\"; the Latin title is also given as Tractatus de sphaera, Textus de sphaera, or simply De sphaera) is a medieval introduction to the basic elements of astronomy written by Johannes de Sacrobosco (John of Holywood) c. 1230. Based heavily on Ptolemy's \"Almagest\", and drawing additional ideas from Islamic astronomy, it was one of the most influential works of pre-Copernican astronomy in Europe.\n\nSacrobosco's \"De sphaera mundi\" was the most successful of several competing thirteenth-century textbooks on this topic. It was used in universities for hundreds of years and the manuscript copied many times before the invention of the printing press; hundreds of manuscript copies have survived. The first printed edition appeared in 1472 in Ferrara, and at least 84 editions were printed in the next two hundred years. The work was frequently supplemented with commentaries on the original text. The number of copies and commentaries reflects its importance as a university text.\n\nThe 'sphere of the world' is not the earth but the heavens, and Sacrobosco quotes Theodosius saying it is a solid body. It is divided into nine parts: the \"first moved\" (\"primum mobile\"), the sphere of the fixed stars (the firmament), and the seven planets, Saturn, Jupiter, Mars, the sun, Venus, Mercury and the moon. There is a 'right' sphere and an oblique sphere: the right sphere is only observed by those at the equator (if there are such people), everyone else sees the oblique sphere. There are two movements: one of the heavens from east to west on its axis through the Arctic and Antarctic poles, the other of the inferior spheres at 23° in the opposite direction on their own axes.\n\nThe world, or universe, is divided into two parts: the elementary and the ethereal. The elementary consists of four parts: the earth, about which is water, then air, then fire, reaching up to the moon. Above this is the ethereal which is immutable and called the 'fifth essence' by the philosophers. All are mobile except heavy earth which is the center of the world.\n\nSacrobosco spoke of the universe as the \"machina mundi\", the machine of the world, suggesting that the reported eclipse of the Sun at the crucifixion of Jesus was a disturbance of the order of that machine. This concept is similar to the clockwork universe analogy that became very popular centuries later, during the Enlightenment.\n\nThough principally about the universe, \"De sphaera\" contains a clear description of the Earth as a sphere which agrees with widespread opinion in Europe during the higher Middle Ages, in contrast to statements of some 19th- and 20th-century historians that medieval scholars thought the Earth was flat. As proof, he uses the fact that stars rise and set sooner for those in the east, and lunar eclipses happen earlier; that stars near the North Pole are visible to those further north and those in the south can see different ones; that at sea one can see further by climbing up the mast; and that water seeks its natural shape which is round, as a drop.\n\n\n"}
{"id": "15115978", "url": "https://en.wikipedia.org/wiki?curid=15115978", "title": "Evolutionary psychology research groups and centers", "text": "Evolutionary psychology research groups and centers\n\nThe following is a list of evolutionary psychology research groups and centers.\n"}
{"id": "8027371", "url": "https://en.wikipedia.org/wiki?curid=8027371", "title": "Fiocruz Genome Comparison Project", "text": "Fiocruz Genome Comparison Project\n\nThe Fiocruz Genome Comparison Project is a collaborative effort involving Brazil's Oswaldo Cruz Institute and IBM's World Community Grid, designed to produce a database comparing the genes from many genomes with each other using SSEARCH. The program SSEARCH performs a rigorous Smith–Waterman alignment between a protein sequence and another protein sequence, a protein database, a DNA or a DNA library.\nThe nature of the computation in the project allows it to easily take advantage of distributed computing. This, along with the likely humanitarian benefits of the research, has led the World Community Grid (a distributed computing grid that uses idle computer clock time) to run the Fiocruz project. All products are in the public domain by contract with WCG.\n\nThe problem is that a very large information body (structural, functional, cross-references, etc.) is attached to protein database entries. Once entered the information is rarely updated or corrected. This annotation of predicted protein function is often incomplete, uses non-standard nomenclature or can be incorrect when cross referenced from previous sometimes incorrectly annotated sequences. Additionally, many proteins composed of several structural and/or functional domains are overlooked by automated systems. The comparative information today is huge when compared to the early days of genomics. A single error is compounded and then made complex.\n\nThe Genome Comparison Project performs a complete pairwise comparison between all predicted protein sequences, obtaining indices used (together with standardized Gene Ontology) as a reference repository for the annotator community. The project provides invaluable data sources for biologists. The sequence similarity comparison program used in the Genome Comparison Project is called SSEARCH. This program mathematically finds best local alignment between sequence pairs, and is a freely available implementation of the Smith–Waterman algorithm.\n\nSSEARCH's use makes possible a precise annotation, inconsistencies correction, and possible functions assignment to hypothetical proteins of unknown function. Moreover, proteins with multiple domains and functional elements are correctly spotted. Even distant relationships are detected.\n\nComparative genomics\n\n"}
{"id": "4161892", "url": "https://en.wikipedia.org/wiki?curid=4161892", "title": "Frank John Kerr", "text": "Frank John Kerr\n\nFrank John Kerr (8 January 191815 September 2000) was an Australian astronomer and physicist who made contributions to human understanding of the galactic structure of the Milky Way. Born in St Albans to Australian parents, Kerr returned with his family to Australia after the completion of World War I. He received degrees in physics at the University of Melbourne and an MA in astronomy from Harvard University (1951).\n\nIn 1940, Frank had joined the Commonwealth Scientific and Industrial Research Organisation (CSIRO) radiophysics laboratory in Sydney, Australia under the mentorship of Joseph Lade Pawsey. He pioneered the use of the magnetron, and also studied superrefraction.\n\nIn Australia in late 1951, Kerr used a specially built 36-foot transit telescope, the largest dish of its kind in Australia, and started mapping the Magellanic Clouds, discovering considerable amounts of neutral hydrogen and an extended envelope around both clouds. From 1954 to 1955, Kerr was a member of the team that determined the rotation of the Magellanic Clouds and their masses. Kerr coined the term \"galactic warp\" to refer to the distorting effect of the Magellanic Clouds' gravity on the shape of our own galaxy.\n\nOver the years he worked with various astronomers, including Colin Gum and Gart Westerhout.\n\nFrom 1966 to 1979, he was a visiting, then full, professor of astronomy at the University of Maryland, College Park.\n\nHe died of cancer at Silver Spring, Maryland.\n\n"}
{"id": "11062", "url": "https://en.wikipedia.org/wiki?curid=11062", "title": "Friction", "text": "Friction\n\nFriction is the force resisting the relative motion of solid surfaces, fluid layers, and material elements sliding against each other. There are several types of friction:\n\n\n\nWhen surfaces in contact move relative to each other, the friction between the two surfaces converts kinetic energy into thermal energy (that is, it converts work to heat). This property can have dramatic consequences, as illustrated by the use of friction created by rubbing pieces of wood together to start a fire. Kinetic energy is converted to thermal energy whenever motion with friction occurs, for example when a viscous fluid is stirred. Another important consequence of many types of friction can be wear, which may lead to performance degradation or damage to components. Friction is a component of the science of tribology.\n\nFriction is desirable and important in supplying traction to facilitate motion on land. Most land vehicles rely on friction for acceleration, deceleration and changing direction. Sudden reductions in traction can cause loss of control and accidents.\nFriction is not itself a fundamental force. Dry friction arises from a combination of inter-surface adhesion, surface roughness, surface deformation, and surface contamination. The complexity of these interactions makes the calculation of friction from first principles impractical and necessitates the use of empirical methods for analysis and the development of theory.\n\nFriction is a non-conservative force - work done against friction is path dependent. In the presence of friction, some energy is always lost in the form of heat. Thus mechanical energy is not conserved.\n\nThe Greeks, including Aristotle, Vitruvius, and Pliny the Elder, were interested in the cause and mitigation of friction. They were aware of differences between static and kinetic friction with Themistius stating in 350 that \"it is easier to further the motion of a moving body than to move a body at rest\".\n\nThe classic laws of sliding friction were discovered by Leonardo da Vinci in 1493, a pioneer in tribology, but the laws documented in his notebooks, were not published and remained unknown. These laws were rediscovered by Guillaume Amontons in 1699 and became known as Amonton's three laws of dry friction \"(below)\". Amontons presented the nature of friction in terms of surface irregularities and the force required to raise the weight pressing the surfaces together. This view was further elaborated by Bernard Forest de Bélidor and Leonhard Euler (1750), who derived the angle of repose of a weight on an inclined plane and first distinguished between static and kinetic friction.\nJohn Theophilus Desaguliers (1734) first recognized the role of adhesion in friction. Microscopic forces cause surfaces to stick together; he proposed that friction was the force necessary to tear the adhering surfaces apart.\nThe understanding of friction was further developed by Charles-Augustin de Coulomb (1785). Coulomb investigated the influence of four main factors on friction: the nature of the materials in contact and their surface coatings; the extent of the surface area; the normal pressure (or load); and the length of time that the surfaces remained in contact (time of repose). Coulomb further considered the influence of sliding velocity, temperature and humidity, in order to decide between the different explanations on the nature of friction that had been proposed. The distinction between static and dynamic friction is made in Coulomb's friction law (see below), although this distinction was already drawn by Johann Andreas von Segner in 1758.\nThe effect of the time of repose was explained by Pieter van Musschenbroek (1762) by considering the surfaces of fibrous materials, with fibers meshing together, which takes a finite time in which the friction increases.\n\nJohn Leslie (1766–1832) noted a weakness in the views of Amontons and Coulomb: If friction arises from a weight being drawn up the inclined plane of successive asperities, why then isn't it balanced through descending the opposite slope? Leslie was equally skeptical about the role of adhesion proposed by Desaguliers, which should on the whole have the same tendency to accelerate as to retard the motion. In Leslie's view, friction should be seen as a time-dependent process of flattening, pressing down asperities, which creates new obstacles in what were cavities before.\n\nArthur Jules Morin (1833) developed the concept of sliding versus rolling friction. Osborne Reynolds (1866) derived the equation of viscous flow. This completed the classic empirical model of friction (static, kinetic, and fluid) commonly used today in engineering. In 1877, Fleeming Jenkin and J. A. Ewing investigated the continuity between static and kinetic friction.\n\nThe focus of research during the 20th century has been to understand the physical mechanisms behind friction. Frank Philip Bowden and David Tabor (1950) showed that, at a microscopic level, the actual area of contact between surfaces is a very small fraction of the apparent area. This actual area of contact, caused by asperities increases with pressure. The development of the atomic force microscope (ca. 1986) enabled scientists to study friction at the atomic scale, showing that, on that scale, dry friction is the product of the inter-surface shear stress and the contact area. These two discoveries explain Amonton's first law \"(below)\"; the macroscopic proportionality between normal force and static frictional force between dry surfaces.\n\nThe elementary property of sliding (kinetic) friction were discovered by experiment in the 15th to 18th centuries and were expressed as three empirical laws:\n\nDry friction resists relative lateral motion of two solid surfaces in contact. The two regimes of dry friction are 'static friction' (\"stiction\") between non-moving surfaces, and \"kinetic friction\" (sometimes called sliding friction or dynamic friction) between moving surfaces.\n\nCoulomb friction, named after Charles-Augustin de Coulomb, is an approximate model used to calculate the force of dry friction. It is governed by the model:\n\nwhere\n\nThe Coulomb friction formula_2 may take any value from zero up to formula_6, and the direction of the frictional force against a surface is opposite to the motion that surface would experience in the absence of friction. Thus, in the static case, the frictional force is exactly what it must be in order to prevent motion between the surfaces; it balances the net force tending to cause such motion. In this case, rather than providing an estimate of the actual frictional force, the Coulomb approximation provides a threshold value for this force, above which motion would commence. This maximum force is known as traction.\n\nThe force of friction is always exerted in a direction that opposes movement (for kinetic friction) or potential movement (for static friction) between the two surfaces. For example, a curling stone sliding along the ice experiences a kinetic force slowing it down. For an example of potential movement, the drive wheels of an accelerating car experience a frictional force pointing forward; if they did not, the wheels would spin, and the rubber would slide backwards along the pavement. Note that it is not the direction of movement of the vehicle they oppose, it is the direction of (potential) sliding between tire and road.\n\nThe normal force is defined as the net force compressing two parallel surfaces together; and its direction is perpendicular to the surfaces. In the simple case of a mass resting on a horizontal surface, the only component of the normal force is the force due to gravity, where formula_7. In this case, the magnitude of the friction force is the product of the mass of the object, the acceleration due to gravity, and the coefficient of friction. However, the coefficient of friction is not a function of mass or volume; it depends only on the material. For instance, a large aluminum block has the same coefficient of friction as a small aluminum block. However, the magnitude of the friction force itself depends on the normal force, and hence on the mass of the block.\n\nIf an object is on a level surface and the force tending to cause it to slide is horizontal, the normal force formula_8 between the object and the surface is just its weight, which is equal to its mass multiplied by the acceleration due to earth's gravity, \"g\". If the object is on a tilted surface such as an inclined plane, the normal force is less, because less of the force of gravity is perpendicular to the face of the plane. Therefore, the normal force, and ultimately the frictional force, is determined using vector analysis, usually via a free body diagram. Depending on the situation, the calculation of the normal force may include forces other than gravity.\n\nThe coefficient of friction (COF), often symbolized by the Greek letter µ, is a dimensionless scalar value which describes the ratio of the force of friction between two bodies and the force pressing them together. The coefficient of friction depends on the materials used; for example, ice on steel has a low coefficient of friction, while rubber on pavement has a high coefficient of friction. Coefficients of friction range from near zero to greater than one. It is an axiom of the nature of friction between metal surfaces that it is greater between two surfaces of similar metals than between two surfaces of different metals— hence, brass will have a higher coefficient of friction when moved against brass, but less if moved against steel or aluminum.\n\nFor surfaces at rest relative to each other formula_9, where formula_10 is the \"coefficient of static friction\". This is usually larger than its kinetic counterpart. The coefficient of static friction exhibited by a pair of contacting surfaces depends upon the combined effects of material deformation characteristics and surface roughness, both of which have their origins in the chemical bonding between atoms in each of the bulk materials and between the material surfaces and any adsorbed material. The fractality of surfaces, a parameter describing the scaling behavior of surface asperities, is known to play an important role in determining the magnitude of the static friction.\n\nFor surfaces in relative motion formula_11, where formula_12 is the \"coefficient of kinetic friction\". The Coulomb friction is equal to formula_2, and the frictional force on each surface is exerted in the direction opposite to its motion relative to the other surface.\n\nArthur Morin introduced the term and demonstrated the utility of the coefficient of friction. The coefficient of friction is an empirical measurement – it has to be measured experimentally, and cannot be found through calculations. Rougher surfaces tend to have higher effective values. Both static and kinetic coefficients of friction depend on the pair of surfaces in contact; for a given pair of surfaces, the coefficient of static friction is \"usually\" larger than that of kinetic friction; in some sets the two coefficients are equal, such as teflon-on-teflon.\n\nMost dry materials in combination have friction coefficient values between 0.3 and 0.6. Values outside this range are rarer, but teflon, for example, can have a coefficient as low as 0.04. A value of zero would mean no friction at all, an elusive property. Rubber in contact with other surfaces can yield friction coefficients from 1 to 2. Occasionally it is maintained that µ is always < 1, but this is not true. While in most relevant applications µ < 1, a value above 1 merely implies that the force required to slide an object along the surface is greater than the normal force of the surface on the object. For example, silicone rubber or acrylic rubber-coated surfaces have a coefficient of friction that can be substantially larger than 1.\n\nWhile it is often stated that the COF is a \"material property,\" it is better categorized as a \"system property.\" Unlike true material properties (such as conductivity, dielectric constant, yield strength), the COF for any two materials depends on system variables like temperature, velocity, atmosphere and also what are now popularly described as aging and deaging times; as well as on geometric properties of the interface between the materials, namely surface structure. For example, a copper pin sliding against a thick copper plate can have a COF that varies from 0.6 at low speeds (metal sliding against metal) to below 0.2 at high speeds when the copper surface begins to melt due to frictional heating. The latter speed, of course, does not determine the COF uniquely; if the pin diameter is increased so that the frictional heating is removed rapidly, the temperature drops, the pin remains solid and the COF rises to that of a 'low speed' test.\n\nUnder certain conditions some materials have very low friction coefficients. An example is (highly ordered pyrolytic) graphite which can have a friction coefficient below 0.01.\nThis ultralow-friction regime is called superlubricity.\n\nStatic friction is friction between two or more solid objects that are not moving relative to each other. For example, static friction can prevent an object from sliding down a sloped surface. The coefficient of static friction, typically denoted as \"μ\", is usually higher than the coefficient of kinetic friction. Static friction is considered to arise as the result of surface roughness features across multiple length-scales at solid surfaces. These features, known as asperities are present down to nano-scale dimensions and result in true solid to solid contact existing only at a limited number of points accounting for only a fraction of the apparent or nominal contact area. The linearity between applied load and true contact area, arising from asperity deformation, gives rise to the linearity between static frictional force and normal force, found for typical Amonton-Coulomb type friction.\n\nThe static friction force must be overcome by an applied force before an object can move. The maximum possible friction force between two surfaces before sliding begins is the product of the coefficient of static friction and the normal force: formula_14. When there is no sliding occurring, the friction force can have any value from zero up to formula_15. Any force smaller than formula_15 attempting to slide one surface over the other is opposed by a frictional force of equal magnitude and opposite direction. Any force larger than formula_15 overcomes the force of static friction and causes sliding to occur. The instant sliding occurs, static friction is no longer applicable—the friction between the two surfaces is then called kinetic friction. \nAn example of static friction is the force that prevents a car wheel from slipping as it rolls on the ground. Even though the wheel is in motion, the patch of the tire in contact with the ground is stationary relative to the ground, so it is static rather than kinetic friction. \nThe maximum value of static friction, when motion is impending, is sometimes referred to as limiting friction,\nalthough this term is not used universally.\n\nKinetic friction, also known as dynamic friction or sliding friction, occurs when two objects are moving relative to each other and rub together (like a sled on the ground). The coefficient of kinetic friction is typically denoted as \"μ\", and is usually less than the coefficient of static friction for the same materials. However, Richard Feynman comments that \"with dry metals it is very hard to show any difference.\"\nThe friction force between two surfaces after sliding begins is the product of the coefficient of kinetic friction and the normal force: formula_18.\n\nNew models are beginning to show how kinetic friction can be greater than static friction. Kinetic friction is now understood, in many cases, to be primarily caused by chemical bonding between the surfaces, rather than interlocking asperities; however, in many other cases roughness effects are dominant, for example in rubber to road friction. Surface roughness and contact area affect kinetic friction for micro- and nano-scale objects where surface area forces dominate inertial forces.\n\nThe origin of kinetic friction at nanoscale can be explained by thermodynamics. Upon sliding, new surface forms at the back of a sliding true contact, and existing surface disappears at the front of it. Since all surfaces involve the thermodynamic surface energy, work must be spent in creating the new surface, and energy is released as heat in removing the surface. Thus, a force is required to move the back of the contact, and frictional heat is released at the front.\n\nFor certain applications it is more useful to define static friction in terms of the maximum angle before which one of the items will begin sliding. This is called the \"angle of friction\" or \"friction angle\". It is defined as:\n\nwhere \"θ\" is the angle from horizontal and \"µ\" is the static coefficient of friction between the objects. This formula can also be used to calculate \"µ\" from empirical measurements of the friction angle.\n\nDetermining the forces required to move atoms past each other is a challenge in designing nanomachines. In 2008 scientists for the first time were able to move a single atom across a surface, and measure the forces required. Using ultrahigh vacuum and nearly zero temperature (5º K), a modified atomic force microscope was used to drag a cobalt atom, and a carbon monoxide molecule, across surfaces of copper and platinum.\n\nThe Coulomb approximation mathematically follows from the assumptions that surfaces are in atomically close contact only over a small fraction of their overall area, that this contact area is proportional to the normal force (until saturation, which takes place when all area is in atomic contact), and that the frictional force is proportional to the applied normal force, independently of the contact area (you can see the experiments on friction from Leonardo da Vinci). Such reasoning aside, however, the approximation is fundamentally an empirical construct. It is a rule of thumb describing the approximate outcome of an extremely complicated physical interaction. The strength of the approximation is its simplicity and versatility. Though in general the relationship between normal force and frictional force is not exactly linear (and so the frictional force is not entirely independent of the contact area of the surfaces), the Coulomb approximation is an adequate representation of friction for the analysis of many physical systems.\n\nWhen the surfaces are conjoined, Coulomb friction becomes a very poor approximation (for example, adhesive tape resists sliding even when there is no normal force, or a negative normal force). In this case, the frictional force may depend strongly on the area of contact. Some drag racing tires are adhesive for this reason. However, despite the complexity of the fundamental physics behind friction, the relationships are accurate enough to be useful in many applications.\n\n, a single study has demonstrated the potential for an \"effectively negative coefficient of friction in the low-load regime\", meaning that a decrease in normal force leads to an increase in friction. This contradicts everyday experience in which an increase in normal force leads to an increase in friction. This was reported in the journal \"Nature\" in October 2012 and involved the friction encountered by an atomic force microscope stylus when dragged across a graphene sheet in the presence of graphene-adsorbed oxygen.\n\nDespite being a simplified model of friction, the Coulomb model is useful in many numerical simulation applications such as multibody systems and granular material. Even its most simple expression encapsulates the fundamental effects of sticking and sliding which are required in many applied cases, although specific algorithms have to be designed in order to efficiently numerically integrate mechanical systems with Coulomb friction and bilateral or unilateral contact. Some quite nonlinear effects, such as the so-called Painlevé paradoxes, may be encountered with Coulomb friction.\n\nDry friction can induce several types of instabilities in mechanical systems which display a stable behaviour in the absence of friction. \nThese instabilities may be caused by the decrease of the friction force with an increasing velocity of sliding, by material expansion due to heat generation during friction (the thermo-elastic instabilities), or by pure dynamic effects of sliding of two elastic materials (the Adams-Martins instabilities). The latter were originally discovered in 1995 by George G. Adams and João Arménio Correia Martins for smooth surfaces and were later found in periodic rough surfaces. In particular, friction-related dynamical instabilities are thought to be responsible for brake squeal and the 'song' of a glass harp, phenomena which involve stick and slip, modelled as a drop of friction coefficient with velocity.\n\nA practically important case is the self-oscillation of the strings of bowed instruments such as the violin, cello, hurdy-gurdy, erhu, etc.\n\nA connection between dry friction and flutter instability in a simple mechanical system has been discovered, watch the movie for more details.\n\nFrictional instabilities can lead to the formation of new self-organized patterns (or \"secondary structures\") at the sliding interface, such as in-situ formed tribofilms which are utilized for the reduction of friction and wear in so-called self-lubricating materials.\n\nFluid friction occurs between fluid layers that are moving relative to each other. This internal resistance to flow is named \"viscosity\". In everyday terms, the viscosity of a fluid is described as its \"thickness\". Thus, water is \"thin\", having a lower viscosity, while honey is \"thick\", having a higher viscosity. The less viscous the fluid, the greater its ease of deformation or movement.\n\nAll real fluids (except superfluids) offer some resistance to shearing and therefore are viscous. For teaching and explanatory purposes it is helpful to use the concept of an inviscid fluid or an ideal fluid which offers no resistance to shearing and so is not viscous.\n\nLubricated friction is a case of fluid friction where a fluid separates two solid surfaces. Lubrication is a technique employed to reduce wear of one or both surfaces in close proximity moving relative to each another by interposing a substance called a lubricant between the surfaces.\n\nIn most cases the applied load is carried by pressure generated within the fluid due to the frictional viscous resistance to motion of the lubricating fluid between the surfaces. Adequate lubrication allows smooth continuous operation of equipment, with only mild wear, and without excessive stresses or seizures at bearings. When lubrication breaks down, metal or other components can rub destructively over each other, causing heat and possibly damage or failure.\n\nSkin friction arises from the interaction between the fluid and the skin of the body, and is directly related to the area of the surface of the body that is in contact with the fluid. Skin friction follows the drag equation and rises with the square of the velocity.\n\nSkin friction is caused by viscous drag in the boundary layer around the object. There are two ways to decrease skin friction: the first is to shape the moving body so that smooth flow is possible, like an airfoil. The second method is to decrease the length and cross-section of the moving object as much as is practicable.\n\nInternal friction is the force resisting motion between the elements making up a solid material while it undergoes deformation.\n\nPlastic deformation in solids is an irreversible change in the internal molecular structure of an object. This change may be due to either (or both) an applied force or a change in temperature. The change of an object's shape is called strain. The force causing it is called stress.\n\nElastic deformation in solids is reversible change in the internal molecular structure of an object. Stress does not necessarily cause permanent change. As deformation occurs, internal forces oppose the applied force. If the applied stress is not too large these opposing forces may completely resist the applied force, allowing the object to assume a new equilibrium state and to return to its original shape when the force is removed. This is known as elastic deformation or elasticity.\n\nAs a consequence of light pressure, Einstein in 1909 predicted the existence of \"radiation friction\" which would oppose the movement of matter. He wrote, “radiation will exert pressure on both sides of the plate. The forces of pressure exerted on the two sides are equal if the plate is at rest. However, if it is in motion, more radiation will be reflected on the surface that is ahead during the motion (front surface) than on the back surface. The backwardacting force of pressure exerted on the front surface is thus larger than the force of pressure acting on the back. Hence, as the resultant of the two forces, there remains a force that counteracts the motion of the plate and that increases with the velocity of the plate. We will call this resultant 'radiation friction' in brief.”\n\nRolling resistance is the force that resists the rolling of a wheel or other circular object along a surface caused by deformations in the object or surface. Generally the force of rolling resistance is less than that associated with kinetic friction. Typical values for the coefficient of rolling resistance are 0.001.\nOne of the most common examples of rolling resistance is the movement of motor vehicle tires on a road, a process which generates heat and sound as by-products.\n\nAny wheel equipped with a brake is capable of generating a large retarding force, usually for the purpose of slowing and stopping a vehicle or piece of rotating machinery. Braking friction differs from rolling friction because the coefficient of friction for rolling friction is small whereas the coefficient of friction for braking friction is designed to be large by choice of materials for brake pads.\n\nRubbing dissimilar materials against one another can cause a build-up of electrostatic charge, which can be hazardous if flammable gases or vapours are present. When the static build-up discharges, explosions can be caused by ignition of the flammable mixture.\n\nBelt friction is a physical property observed from the forces acting on a belt wrapped around a pulley, when one end is being pulled. The resulting tension, which acts on both ends of the belt, can be modeled by the belt friction equation.\n\nIn practice, the theoretical tension acting on the belt or rope calculated by the belt friction equation can be compared to the maximum tension the belt can support. This helps a designer of such a rig to know how many times the belt or rope must be wrapped around the pulley to prevent it from slipping. Mountain climbers and sailing crews demonstrate a standard knowledge of belt friction when accomplishing basic tasks.\n\nDevices such as wheels, ball bearings, roller bearings, and air cushion or other types of fluid bearings can change sliding friction into a much smaller type of rolling friction.\n\nMany thermoplastic materials such as nylon, HDPE and PTFE are commonly used in low friction bearings. They are especially useful because the coefficient of friction falls with increasing imposed load. For improved wear resistance, very high molecular weight grades are usually specified for heavy duty or critical bearings.\n\nA common way to reduce friction is by using a lubricant, such as oil, water, or grease, which is placed between the two surfaces, often dramatically lessening the coefficient of friction. The science of friction and lubrication is called tribology. Lubricant technology is when lubricants are mixed with the application of science, especially to industrial or commercial objectives.\n\nSuperlubricity, a recently discovered effect, has been observed in graphite: it is the substantial decrease of friction between two sliding objects, approaching zero levels. A very small amount of frictional energy would still be dissipated.\n\nLubricants to overcome friction need not always be thin, turbulent fluids or powdery solids such as graphite and talc; acoustic lubrication actually uses sound as a lubricant.\n\nAnother way to reduce friction between two parts is to superimpose micro-scale vibration to one of the parts. This can be sinusoidal vibration as used in ultrasound-assisted cutting or vibration noise, known as dither.\n\nAccording to the law of conservation of energy, no energy is destroyed due to friction, though it may be lost to the system of concern. Energy is transformed from other forms into thermal energy. A sliding hockey puck comes to rest because friction converts its kinetic energy into heat which raises the thermal energy of the puck and the ice surface. Since heat quickly dissipates, many early philosophers, including Aristotle, wrongly concluded that moving objects lose energy without a driving force.\n\nWhen an object is pushed along a surface along a path C, the energy converted to heat is given by a line integral, in accordance with the definition of work\nwhere\n\nEnergy lost to a system as a result of friction is a classic example of thermodynamic irreversibility.\n\nIn the reference frame of the interface between two surfaces, static friction does \"no\" work, because there is never displacement between the surfaces. In the same reference frame, kinetic friction is always in the direction opposite the motion, and does \"negative\" work. However, friction can do \"positive\" work in certain frames of reference. One can see this by placing a heavy box on a rug, then pulling on the rug quickly. In this case, the box slides backwards relative to the rug, but moves forward relative to the frame of reference in which the floor is stationary. Thus, the kinetic friction between the box and rug accelerates the box in the same direction that the box moves, doing \"positive\" work.\n\nThe work done by friction can translate into deformation, wear, and heat that can affect the contact surface properties (even the coefficient of friction between the surfaces). This can be beneficial as in polishing. The work of friction is used to mix and join materials such as in the process of friction welding. Excessive erosion or wear of mating sliding surfaces occurs when work due to frictional forces rise to unacceptable levels. Harder corrosion particles caught between mating surfaces in relative motion (fretting) exacerbates wear of frictional forces. Bearing seizure or failure may result from excessive wear due to work of friction. As surfaces are worn by work due to friction, fit and surface finish of an object may degrade until it no longer functions properly.\n\nFriction is an important factor in many engineering disciplines.\n\n\n\n\n"}
{"id": "28631731", "url": "https://en.wikipedia.org/wiki?curid=28631731", "title": "Global Online Laboratory Consortium", "text": "Global Online Laboratory Consortium\n\nGOLC Global Online Laboratory Consortium\n\nWorldwide Consortium of online/remote laboratory providers.\n\nMembers are mostly from national and international online/remote laboratory consortia.\nCurrent members include iLabs USA and Africa (MIT), Labshare Australia, the Lila Project European, as well as many other universities (University of Deusto, Carinthia University of Applied Sciences, Technische Universität Graz, University of Queensland, TU Dortmund University, School of Engineering - Polytechnic of Porto, etc.).\n\nThe mission of the Global Online Laboratory Consortium is to increase learning outcomes and scientific productivity through the creation of sharable, online, experimental environments which are accessible, scalable, and efficient.\n\nWork towards interoperability of the different remote laboratory systems.\n\n\n"}
{"id": "9905340", "url": "https://en.wikipedia.org/wiki?curid=9905340", "title": "Good Natured", "text": "Good Natured\n\nGood Natured is a book by primatologist Frans de Waal on animal behavior and the evolution of ethics.\n\nThe book was published in 1996 by Harvard University Press under the full title Good Natured: The Origins of Right and Wrong in Humans and Other Animals. Much of the book details observations of primate behavior, especially that of chimpanzees and bonobos. On the final page, he concludes:\n\n"}
{"id": "10772350", "url": "https://en.wikipedia.org/wiki?curid=10772350", "title": "History", "text": "History\n\nHistory (from Greek , \"historia\", meaning \"inquiry, knowledge acquired by investigation\") is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.\n\nHistory can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing \"perspective\" on the problems of the present.\n\nStories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the \"disinterested investigation\" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is considered within the Western tradition to be the \"father of history\", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals was known to be compiled from as early as 722 BC although only 2nd-century BC texts survived.\n\nAncient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.\n\nThe word \"history\" comes ultimately from Ancient Greek ἱστορία (\"historía\"), meaning \"inquiry\", \"knowledge from inquiry\", or \"judge\". It was in that sense that Aristotle used the word in his (\"Perì Tà Zôa Ηistoríai\" \"Inquiries about Animals\"). The ancestor word is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boiotic inscriptions (in a legal sense, either \"judge\" or \"witness\", or similar).\n\nThe Greek word was borrowed into Classical Latin as \"historia\", meaning \"investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative\". \"History\" was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as \"stær\" ('history, narrative, story'), but this word fell out of use in the late Old English period.\n\nMeanwhile, as Latin became Old French (and Anglo-Norman), \"historia\" developed into forms such as \"istorie\", \"estoire\", and \"historie\", with new developments in the meaning: \"account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)\".\n\nIt was from Anglo-Norman that \"history\" was borrowed into Middle English, and this time the loan stuck. It appears in the thirteenth-century \"Ancrene Wisse\", but seems to have become a common word in the late fourteenth century, with an early attestation appearing in John Gower's \"Confessio Amantis\" of the 1390s (VI.1383): \"I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire\". In Middle English, the meaning of \"history\" was \"story\" in general. The restriction to the meaning \"the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs\" arose in the mid-fifteenth century.\n\nWith the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late sixteenth century, when he wrote about \"Natural History\". For him, \"historia\" was \"the knowledge of objects determined by space and time\", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).\n\nIn an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both \"history\" and \"story\".\n\nThe adjective \"historical\" is attested from 1661, and \"historic\" from 1669.\n\n\"Historian\" in the sense of a \"researcher of history\" is attested from 1531. In all European languages, the substantive \"history\" is still used to mean both \"what happened with men\", and \"the scholarly study of the happened\", the latter sense sometimes distinguished with a capital letter, \"History\", or the word \"historiography\".\n\nHistorians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, \"All history is contemporary history\". History is facilitated by the formation of a \"true discourse of past\" through the production of narrative and analysis of past events relating to the human race. The modern discipline of history is dedicated to the institutional production of this discourse.\n\nAll events that are remembered and preserved in some authentic form constitute the historical record. The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the \"true past\").\n\nThe study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences. It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification. In the 20th century, French historian Fernand Braudel revolutionized the study of history, by using such outside disciplines as economics, anthropology, and geography in the study of global history.\n\nTraditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three. But writing is the marker that separates history from what comes before.\n\nArchaeology is a discipline that is especially helpful in dealing with buried sites and objects, which, once unearthed, contribute to the study of history. But archaeology rarely stands alone. It uses narrative sources to complement its discoveries. However, archaeology is constituted by a range of methodologies and approaches which are independent from history; that is to say, archaeology does not \"fill the gaps\" within textual sources. Indeed, \"historical archaeology\" is a specific branch of archaeology, often contrasting its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, USA; has sought to understand the contradiction between textual documents and the material record, demonstrating the possession of slaves and the inequalities of wealth apparent via the study of the total historical environment, despite the ideology of \"liberty\" inherent in written documents at this time.\n\nThere are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant overlaps are often present, as in \"The International Women's Movement in an Age of Transition, 1830–1975.\" It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.\n\nThe history of the world is the memory of the past experience of \"Homo sapiens sapiens\" around the world, as that experience has been preserved, largely in written records. By \"prehistory\", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world. In 1961, British historian E. H. Carr wrote:\n\nThis definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.\n\nHistoriography has a number of related meanings. Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative towards long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, \"medieval historiography during the 1960s\" means \"Works of medieval history written during the 1960s\"). Thirdly, it may refer to why history is produced: the Philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.\n\nPhilosophy of history is a branch of philosophy concerning the eventual significance, if any, of human history. Furthermore, it speculates as to a possible teleological end to its development—that is, it asks if there is a design, purpose, directive principle, or finality in the processes of human history. Philosophy of history should not be confused with historiography, which is the study of history as an academic discipline, and thus concerns its methods and practices, and its development as a discipline over time. Nor should philosophy of history be confused with the history of philosophy, which is the study of the development of philosophical ideas through time.\n\nThe historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.\n\nHerodotus of Halicarnassus (484 BC – ca.425 BC) has generally been acclaimed as the \"father of history\". However, his contemporary Thucydides (c. 460 BC – ca. 400 BC) is credited with having first approached history with a well-developed historical method in his work the \"History of the Peloponnesian War\". Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention. In his historical method, Thucydides emphasized chronology, a neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.\n\nThere were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (145–90 BC), author of the \"Records of the Grand Historian\" (\"Shiji\"). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his \"Shiji\" as the official format for historical texts, as well as for biographical literature.\n\nSaint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.\n\nIn the preface to his book, the \"Muqaddimah\" (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized \"idle superstition and uncritical acceptance of historical data.\" As a result, he introduced a scientific method to the study of history, and he often referred to it as his \"new science\". His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and he is thus considered to be the \"father of historiography\" or the \"father of the philosophy of history\".\n\nIn the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. The 19th-century historian with greatest influence on methods was Leopold von Ranke in Germany.\n\nIn the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multi-disciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archaeology while Wehler, Bloch, Fischer, Stone, Febvre and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.\n\nIn opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. \"histoire des mentalités\"). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socio-economic groups. Another genre of social history to emerge in the post-WWII era was \"Alltagsgeschichte\" (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.\n\nMarxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as François Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book \"In Defence of History\", Richard J. Evans defended the worth of history. Another defence of history from post-modernist criticism was the Australian historian Keith Windschuttle's 1994 book, \"The Killing of History\".\n\nThe Marxist theory of historical materialism theorises that society is fundamentally determined by the \"material conditions\" at any given time – in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe. Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.\n\nHistorical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow \"organising ideas and classificatory generalisations\" to be used by historians. The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.\n\nThe field of history generally leaves prehistory to the archaeologists, who have entirely different sets of tools and theories. The usual method for periodisation of the distant prehistoric past, in archaeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Here prehistory is divided into a series of \"chapters\" so that periods in history could unfold not only in a relative chronology but also narrative chronology. This narrative content could be in the form of functional-economic interpretation. There are periodisation, however, that do not have this narrative aspect, relying largely on relative chronology and, thus, devoid of any specific meaning.\n\nDespite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used. Periodisation, however, is not viewed as a perfect framework with one account explaining that \"cultural changes do not conveniently start and stop (combinedly) at periodisation boundaries\" and that different trajectories of change are also needed to be studied in their own right before they get intertwined with cultural phenomena. \n\nParticular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book \"Histoire de France\" (1833), \"without geographical basis, the people, the makers of history, seem to be walking on air.\" Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Allen Semple, cited as a crucial influence on the course of history and racial temperament.\n\n\nMilitary history concerns warfare, strategies, battles, weapons, and the psychology of combat. The \"new military history\" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.\n\nThe history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include \"Church History\", \"The Catholic Historical Review\", and \"History of Religions\". Topics range widely from political and cultural and artistic dimensions, to theology and liturgy. This subject studies religions from all regions and areas of the world where humans have lived.\n\n\"Social history\", sometimes called the \"new social history\", is the field that includes history of ordinary people and their strategies and institutions for coping with life. In its \"golden age\" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%) identified themselves with social history while political history came next with 1425 (25%).\nThe \"old\" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were \"social\" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, \"Without social history, economic history is barren and political history unintelligible.\" While the field has often been viewed negatively as history with the politics left out, it has also been defended as \"history with the people put back in.\"\n\nThe chief subfields of social history include:\nSmaller specialties include:\n\nCultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic.\nCultural history includes the study of art in society as well is the study of images and human visual production (iconography).\n\nDiplomatic history focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars. More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of \"political history\" is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, \"diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies.\" She adds that after 1945, the trend reversed, allowing social history to replace it.\n\nAlthough economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments. Business history deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history; Business history is most often taught in business schools.\n\nEnvironmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.\n\nWorld history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States, Japan and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.\n\nIt has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.\n\nThe World History Association publishes the \"Journal of World History\" every quarter since 1990. The H-World discussion list serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.\n\nA people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other type of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.\n\nIntellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.\n\nGender history is a sub-field of History and Gender studies, which looks at the past from the perspective of gender. It is in many ways, an outgrowth of women's history. Despite its relatively short life, Gender History (and its forerunner Women's History) has had a rather significant effect on the general study of history. Since the 1960s, when the initially small field first achieved a measure of acceptance, it has gone through a number of different phases, each with its own challenges and outcomes. Although some of the changes to the study of history have been quite obvious, such as increased numbers of books on famous women or simply the admission of greater numbers of women into the historical profession, other influences are more subtle.\n\nPublic history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.\n\nProfessional and amateur historians discover, collect, organize, and present information about past events.They discover this information through archaeological evidence, written primary sources from the past and other various means such as place names. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.\n\nSince the 20th century, Western historians have disavowed the aspiration to provide the \"judgement of history.\" The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final. A related issue to that of the judgement of history is that of collective memory.\n\nPseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions.\nClosely related to deceptive historical revisionism, works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.\n\nA major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defence of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.\n\nIn the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.\n\nFrom the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.\n\nAt the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.\n\nThe teaching of history in French schools was influenced by the \"Nouvelle histoire\" as disseminated after the 1960s by \"Cahiers pédagogiques and Enseignement\" and other journals for teachers. Also influential was the Institut national de recherche et de documentation pédagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis François, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote \"active methods\" which would give pupils \"the immense happiness of discovery.\" Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.\n\nIn several countries history textbooks are tools to foster nationalism and patriotism, and give students the official line about national enemies.\n\nIn many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favourable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained. It was standard policy in communist countries to present only a rigid Marxist historiography.\n\nIn the United States, especially the southern part history about slavery and the American Civil War are controversial topics. McGraw-Hill Education for example, was criticised for describing Africans brought to American plantations as \"workers\" instead of slaves in a textbook.\n\nAcademic historians have often fought against the politicization of the textbooks, sometimes with success.\n\nIn 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an \"almost pacifistic and deliberately unpatriotic undertone\" and reflects \"principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace.\" The result is that \"German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness.\"\n\n\n\n\n\n"}
{"id": "1141557", "url": "https://en.wikipedia.org/wiki?curid=1141557", "title": "Human Genome Diversity Project", "text": "Human Genome Diversity Project\n\nThe Human Genome Diversity Project (HGDP) was started by Stanford University's Morrison Institute and a collaboration of scientists around the world. It is the result of many years of work by Luigi Cavalli-Sforza, one of the most cited scientists in the world, who has published extensively in the use of genetics to understand human migration and evolution. The HGDP data sets have often been cited in papers on such topics as population genetics, anthropology, and heritable disease research.\n\nThe project has noted the need to record the genetic profiles of indigenous populations, as isolated populations are the best way to understand the genetic frequencies that have clues into our distant past. Knowing about the relationship between such populations makes it possible to infer the journey of humankind from the humans who left Africa and populated the world to the humans of today. The HGDP-CEPH Human Genome Diversity Cell Line Panel is a resource of 1,063 cultured lymphoblastoid cell lines (LCLs) from 1,050 individuals in 52 world populations, banked at the Fondation Jean Dausset-CEPH in Paris.\n\nThe HGDP is not related to the Human Genome Project (HGP) and has attempted to maintain a distinct identity.\n\nThe HGDP includes the 51 populations from around the world.\nA description of the populations that were studied can be found in a 2005 review paper by Cavalli-Sforza:\n\nOne of the most important tenets of the HGDP debate has been the social and ethical implications for indigenous populations, specifically the methods and ethics of informed consent. Some questions include: How would consent be obtained? Would individuals or groups fully understand the project's intentions, particularly with regards to language barriers and differing cultural views? What is 'informed' in a cross-cultural context? Who would be authorized to actually give consent? How would individuals know what happened to their DNA? For how long would their information be kept in DNA databases? These questions are specifically addressed by the HGDP's \"Model Ethical Protocol for Collecting DNA Samples\".\n\nThe scientific community has used the HGDP data to study human migration, mutation rates, relationships between different populations, genes involved in height, and selective pressure. HGDP has been instrumental in assessing human diversity and in providing information about similarities and differences in human populations. The HGDP is the project with the largest scope among the various human diversity databases available.\n\nSo far 148 papers have been published using the HGDP database. Authors using HGDP data work in the US, Russia, Brasil, Ireland, Portugal, France, and other countries.\n\nMore specifically, HGDP data has been used in studies of evolution and expansion of modern human populations.\n\nDiversity research is relevant in various fields of study ranging from disease surveillance to anthropology. Genomewide-association studies (GWAS) try to associate a genetic mutation with a disease; it is becoming clear that these associations are population-dependent and that understanding human diversity will be a major step toward increasing the power to find genes associated with disease.\n\nTo gain a full assessment of human development, scientists must engage in diversity research. This research needs to be conducted as quickly as possible before small native populations such as those in South America become extinct.\n\nAnother benefit of genomic diversity mapping would be in disease research. Diversity research could help explain why certain ethnic populations are vulnerable to or resistant to certain diseases and how populations have adapted to vulnerabilities (see race in biomedicine).\n\nThe study of human populations has been at the forefront of genomic and clinical research since the Human Genome Project (HGP) was completed. Projects similar to HGDP are the 1000 Genomes Project and the HapMap Project. Each has its own specificities and each has been used by scientists to a large extent for overlapping purposes.\n\nDenouncing the project since its outset, some indigenous communities, NGOs, and human rights organizations have objected to the HGDP's goals based on perceived issues of scientific racism, colonialism, biocolonialism (patenting), informed consent, and the prospect of biological warfare.\n\nRacism\nThe Action Group on Erosion, Technology and Concentration (ETC Group) has been a major critic of the HGDP, speculating that issues of racism and stigmatization could occur should the HGDP be completed. One major concern with the research project has been the potential, in certain countries, for racism resulting from use of HGDP data. Critics feel that when governments are armed with genetic data linked to certain racial groups, those governments might deny human rights based on this genetic data. For example, countries could define races purely in genetic terms and deny a certain person right(s) based on lack of conformity to a certain race's genetic model.\n\nUneven Application\nEight of nine DNA groups under Ctrl/South category belong to Pakistan even though India is in the same group with about seven times the population of Pakistan and with far more numerous racial diversities. However, it is noteworthy that Rosenberg et al. found that the sampled Pakistani populations are more genetically diverse than 15 Indian populations that were explicitly compared\n\nUse of Genetic Data for Non-medical Purposes\nUse of HGDP genetic materials for nonmedical purposes not agreed to by indigenous donors, especially purposes that create possibilities for human rights violations, has been a matter of concern. For example, Kidd et al. described the use of DNA samples from indigenous populations to explore a forensic identification capability based on ethnic origins.\n\nCreating Artificial Genetic Distinctions\nAnthropologist Jonathan M. Marks stated: \"As any anthropologist knows, ethnic groups are categories of human invention, not given by nature.\" Some indigenous peoples have refused to take part in the HGDP due to concerns about misuse of the data: \"In December [1993], a World Council of Indigenous Peoples in Guatemala repudiated the HGDP.\"\n\nIn 1995, the National Research Council (NRC) issued its recommendations on the HGDP. The NRC endorsed the concept of diversity research, also pointing out some concerns with the HGDP procedure. The NRC report suggested alternatives such as keeping sample sources anonymous (i.e., sampling genetic data without tying it to specific racial groups). While such approaches would eliminate the concerns discussed above (regarding racism, weapons development, etc.), they would also prevent researchers from achieving many of the benefits that were to be gained from the project.\n\nSome members of the Human Genome Project (HGP) argued in favor of engaging in diversity research on data gleaned from the Human Genome Diversity Project, although most agreed that diversity research should be done by the HGP and not as a separate project.\n\nA number of the principal collaborators in the HGDP have been involved in the privately funded Genographic Project launched in April 2005 with similar aims.\n\nAn independent replication study of the variations of Europe and Middle East & North Africa individuals was published in 2017. Using the publicly available data sets, the study confirmed that different population structures were observed within each geographical region. The corresponding R source code and Docker image were also published to enable the reproducibility of the study.\n\n"}
{"id": "30049129", "url": "https://en.wikipedia.org/wiki?curid=30049129", "title": "Hylogenesis", "text": "Hylogenesis\n\nHylogenesis is a physical theory about the mechanism behind the origins of dark matter and antimatter. It was proposed in August 2010 in a paper by Hooman Davoudiasl, David E. Morrissey, Kris Sigurdson and Sean Tulin.\n\nThe theory involves a fermion X, and its antiparticle , both of which may couple into quarks in the visible sector, and into hidden particles in a hidden sector, a sector which is not part of the Standard Model. The hidden states have masses near a GeV and very weak couplings to particles in the Standard Model. X and respectively decay into either baryonic matter or hidden baryonic matter, and into either antibaryonic matter or hidden antibaryonic matter, violating CP and quark baryon number.\n\nAn excess of baryonic matter is created in the visible sector, and an excess of antimatter is created in the hidden sector. The hidden antimatter is explained as being stable dark matter. The X and particles have a conserved baryon number charge, so equal and opposite charges appear in the visible and hidden sectors. Therefore, the Universe's total baryon charge stays zero.\n\n"}
{"id": "396505", "url": "https://en.wikipedia.org/wiki?curid=396505", "title": "International Code of Nomenclature for algae, fungi, and plants", "text": "International Code of Nomenclature for algae, fungi, and plants\n\nThe International Code of Nomenclature for algae, fungi, and plants (ICN) is the set of rules and recommendations dealing with the formal botanical names that are given to plants, fungi and a few other groups of organisms, all those \"traditionally treated as algae, fungi, or plants\". It was formerly called the International Code of Botanical Nomenclature (ICBN); the name was changed at the International Botanical Congress in Melbourne in July 2011 as part of the \"Melbourne Code\" which replaced the \"Vienna Code\" of 2005.\n\nThe current version of the code is the \"Shenzhen Code\" adopted by the International Botanical Congress held in Shenzhen, China, in July 2017. As with previous codes, it took effect as soon as it was ratified by the congress (on 29 July 2017), but the documentation of the code in its final form was not published until 26 June 2018.\n\nThe name of the \"Code\" is partly capitalized and partly not. The lower-case for \"algae, fungi, and plants\" indicates that these terms are not formal names of clades, but indicate groups of organisms that were historically known by these names and traditionally studied by phycologists, mycologists, and botanists. This includes blue-green algae (Cyanobacteria); fungi, including chytrids, oomycetes, and slime moulds; photosynthetic protists and taxonomically related non-photosynthetic groups. There are special provisions in the \"ICN\" for some of these groups, as there are for fossils.\n\nThe \"ICN\" can only be changed by an International Botanical Congress (IBC), with the International Association for Plant Taxonomy providing the supporting infrastructure. Each new edition supersedes the earlier editions and is retroactive back to 1753, except where different starting dates are specified.\n\nFor the naming of cultivated plants there is a separate code, the \"International Code of Nomenclature for Cultivated Plants\", which gives rules and recommendations that supplement the \"ICN\".\n\n\nThe rules governing botanical nomenclature have a long and tumultuous history, dating back to dissatisfaction with rules that were established in 1843 to govern zoological nomenclature. The first set of international rules was the \"Lois de la nomenclature botanique\" (\"Laws of botanical nomenclature\") that was adopted as the \"best guide to follow for botanical nomenclature\" at an \"International Botanical Congress\" convened in Paris in 1867. Unlike modern codes, it was not enforced. It was organized as six sections with 68 articles in total.\n\nMultiple attempts to bring more \"expedient\" or more equitable practice to botanical nomenclature resulted in several competing codes, which finally reached a compromise with the 1930 congress. In the meantime, the second edition of the international rules followed the Vienna congress in 1905. These rules were published as the \"Règles internationales de la Nomenclature botanique adoptées par le Congrès International de Botanique de Vienne 1905\" (or in English, \"International rules of Botanical Nomenclature adopted by the International Botanical Conference of Vienna 1905\"). Informally they are referred to as the \"Vienna Rules\" (not to be confused with the \"Vienna Code\" of 2006).\n\nSome but not all subsequent meetings of the International Botanical Congress have produced revised versions of these \"Rules\", later called the \"International Code of Botanical Nomenclature\", and then \"International Code of Nomenclature for algae, fungi, and plants\".\n\nThe Nomenclature Section of the 18th International Botanical Congress in Melbourne, Australia (2011) made major changes:\n\n\nSome important versions are listed below.\n\nSpecific to botany\n\nMore general\n"}
{"id": "55773021", "url": "https://en.wikipedia.org/wiki?curid=55773021", "title": "KBS Tuff", "text": "KBS Tuff\n\nThe KBS Tuff (Kay Behrensmeyer Tuff) is an ash layer in East African Rift Valley sediments, derived from a volcanic eruption that occurred approximately 1.87 million years ago (Ma). The tuff is widely distributed geographically, and marks a significant transition between water flow and associated environmental conditions around Lake Turkana shortly after 2 Ma.\n\nBetween 1970-1985 the age of the tuff was the subject of intense academic dispute, with a variety of dates proposed by different geochemical and paleontological laboratories. This dispute came to be known as the KBS Tuff Controversy.\n\nThe KBS Tuff has been described as \"the Turkana Basin’s most celebrated tephrostratigraphic marker.\"\n\nThe KBS Tuff was first reported and described by Kay Behrensmeyer (hence \"KBS\", Kay Behrensmeyer Site) in sediments that belong to Omo Group deposits in southern Ethiopia and northern Kenya. Within this larger group, the KBS has been found in the Shungura Formation in southern Ethiopia, In the Nachukui Formation on the west side of Lake Turkana in northern Kenya, and in the Koobi Fora Formation on the east side of Lake Turkana.\n\nArgon-argon dating has placed the age of the KBS Tuff at 1.869 ± 0.021 Ma. This age estimate is supported by independent fission track and K/Ar dating methods. KBS is situated above the older Kangaki (2.063 Ma), G-3 (2.188 Ma) and Kalochoro (2.331 Ma) tuffs, and below the younger Malbe (1.843 Ma), Morutot (1.607 Ma) and Lower Ileret (1.527 Ma) tuffs. In some locations tuffs with chemical compositions identical to the KBS tuff have been found in multiple, distinct layers, suggesting that deposition of the layers occurred at various times after eruption.\n\nWithin Omo group deposits, the KBS tuff separates distinct sedimentary members of the Nachukui and Foobi Fora formations. In the Nachukui (western) formation, KBS divides the older Kalochoro from the younger Kaitio member. In the Koobi Fora formation, it separates the older Burgi from the younger KBS member.\n\nThe KBS Tuff marks a transition in the Turkana Basin from a stable to a fluctuating lake, partly filled by a river and delta system to the north and east of the basin.\n\nIn Koobi Fora, on the eastern side of Lake Turkana, a substantial number of hominin fossils have been found immediately below or above the KBS Tuff. A partial list is provided below, including a few specimens from north and west Lake Turkana.\n\n\nThe KBS Tuff was first dated in 1969, after Behrensmeyer discovered stone tools at Koobi Fora in the layer of the Tuff. Argon/Argon dating was performed by Frank Fitch at Birckbeck College in London and Jack Miller at the University of Cambridge in Cambridge, UK, who found a most likely age of 2.61 Ma for the KBS eruption. This find had important implications for the anthropological community because it provided a very old age for the tools found by Behresmeyer, and for associated crania including KNM-ER 1470, attributed to the genus \"Homo\".\n\nThe date was called into question because efforts to replicate the findings produced KBS Tuff ages ranging from less than 1 to over 220 Ma. A study of pig molar anatomy from the site, by Vincent Maglio and Basil Cooke, suggested an age closer to 2 Ma or even less, using methods of biogeochronology. Similar investigation using antelope remains by Alan Gentry at the British Museum of Natural History also contradicted the Argon-Argon 2.6 Ma Tuff. Another problem that emerged was that the 2.6 Ma date for the KBS Tuff, provided by Fitch and Miller, made alignment of the Koobi Fora (east Turkana) geochronology with the Omo (north Turkana) geochronology impossible.\n\nThe conflict was resolved after geochemist Garniss Curtis and his student Thure E. Cerling conducted independent investigations of the age of the KBS Tuff using Argon-Argon and Potassium-Argon dating at the Berkeley Geochronology Laboratory. Curtis and Cerling found that the material dated by the Cambridge team actually belonged to two separate tuffs, which they estimated at 1.8 and 1.6 Ma. This date was confirmed by Potassium-Argon dating conducted by Ian McDougall, and later Fission-Track dating conducted by Andy Gleadow.\n\n"}
{"id": "7169703", "url": "https://en.wikipedia.org/wiki?curid=7169703", "title": "Krogh's principle", "text": "Krogh's principle\n\nKrogh's principle states that \"for such a large number of problems there will be some animal of choice, or a few such animals, on which it can be most conveniently studied.\" This concept is central to those disciplines of biology that rely on the comparative method, such as neuroethology, comparative physiology, and more recently functional genomics.\n\nKrogh's principle is named after the Danish physiologist August Krogh, winner of the Nobel Prize in Physiology for his contributions to understanding the anatomy and physiology of the capillary system, who described it in The American Journal of Physiology in 1929. However, the principle was first elucidated nearly 60 years prior to this, and in almost the same words as Krogh, in 1865 by Claude Bernard, the French instigator of experimental medicine, on page 27 of his \"Introduction à l'étude de la médecine expérimentale\":\nKrogh wrote the following in his 1929 treatise on the then current 'status' of physiology (emphasis added):\n\"Krogh's principle\" was not utilized as a formal term until 1975 when the biochemist Hans Adolf Krebs (who initially described the Citric Acid Cycle), first referred to it.\n\nMore recently, at the International Society for Neuroethology meeting in Nyborg, Denmark in 2004, Krogh's principle was cited as a central principle by the group at their 7th Congress. Krogh's principle has also been receiving attention in the area of functional genomics, where there has been increasing pressure and desire to expand genomics research to a more wide variety of organisms beyond the traditional scope of the field.\n\nA central concept to Krogh's principle is evolutionary adaptation. Evolutionary theory maintains that organisms are suited to particular niches, some of which are highly specialized for solving particular biological problems. These adaptations are typically exploited by biologists in several ways:\n\n\n"}
{"id": "1592548", "url": "https://en.wikipedia.org/wiki?curid=1592548", "title": "List of Apple typefaces", "text": "List of Apple typefaces\n\nThis is a list of typefaces made by/for Apple Inc.\n\n\n\n\n\n\n"}
{"id": "11951675", "url": "https://en.wikipedia.org/wiki?curid=11951675", "title": "List of Byzantine scholars", "text": "List of Byzantine scholars\n\nThis is a list of Byzantine scientists and other scholars.\n\nMost important scholars known before the Macedonian Renaissance were active under the Justinian dynasty.\n\n\nThe Macedonian Renaissance occurred in the period of the Macedonian dynasty from 867 to 1056.\n\n\nThe Komnenian period ranged from 1081 to about 1185.\n\n\nThe Palaiologian Renaissance was mostly contemporary with the Renaissance of the 12th century. The Palaiologos dynasty ruled from c. 1260 to 1453. A number of Greek scholars contributed to the establishment of this renaissance also in Western Europe.\n\n\n"}
{"id": "367107", "url": "https://en.wikipedia.org/wiki?curid=367107", "title": "List of CAx companies", "text": "List of CAx companies\n\nThis is a list of computer-aided technologies (CAx) companies and their software products.\nSoftware using computer-aided technologies (CAx) has been produced since the 1970s for a variety of computer platforms. This software may include applications for computer-aided design (CAD), computer-aided engineering (CAE), computer-aided manufacturing (CAM) and product data management (PDM).\n\nThe list is far from complete or representative as the CAD business landscape is very dynamic: almost every month new companies appear, old companies go out of business, companies split and merge. Sometimes some names disappear and reappear again.\n\nAcquired, orphaned, failed or rebranded.\n\n\nDeveloped by companies for their own use. Some are no longer used as the organizations are now using commercial systems.\n\n\n\n\n\n"}
{"id": "11568606", "url": "https://en.wikipedia.org/wiki?curid=11568606", "title": "List of Estonian flags", "text": "List of Estonian flags\n\nThe national flag of Estonia is a tricolour featuring three equal horizontal bands of blue (top), black, and white. The normal size is 105 × 165 cm. In Estonian it is colloquially called the \"sinimustvalge\" (literally \"blue-black-white\"), after the colours of the bands. The flag became associated with Estonian nationalism and was used as the national flag (\"riigilipp\") when the Estonian Declaration of Independence was issued on February 24, 1918. The flag was formally adopted on November 21, 1918. On December 12, 1918 was the first time the flag was raised as the national symbol atop of the Pikk Hermann Tower in Tallinn.\n\nThe following is a list of flags of Estonia.\n\n"}
{"id": "7334163", "url": "https://en.wikipedia.org/wiki?curid=7334163", "title": "List of compilers", "text": "List of compilers\n\nThis page is intended to list all current compilers, compiler generators, interpreters, translators, tool foundations, assemblers, automatable command line interfaces (shells), etc.\n\ncf. ALGOL 68s specification and implementation timeline\n\nThis list is incomplete. A more extensive list of source-to-source compilers can be found here.\n\nNotes:\n\nProduction quality, open source compilers.\n\nResearch compilers are mostly not robust or complete enough to handle real, large applications. They are used mostly for fast prototyping new language features and new optimizations in research areas.\n\n\n"}
{"id": "26075419", "url": "https://en.wikipedia.org/wiki?curid=26075419", "title": "List of countries and territories by land and maritime borders", "text": "List of countries and territories by land and maritime borders\n\nThis is a list of countries and territories by land and maritime borders. For each country or territory, the number and identity of other countries and territories that neighbor it are listed. Land borders and maritime boundaries are included and are tabulated separately and in combination. For purposes of this list, \"maritime boundary\" includes boundaries that are recognized by the United Nations Convention on the Law of the Sea, which includes boundaries of territorial waters, contiguous zones, and exclusive economic zones. However, it does not include lake or river boundaries, which are considered land boundaries.\n\nAlso included is the number of unique sovereign states that a country or territory shares as neighbors. If the number is higher due to multiple dependencies or unrecognized states bordering the state, the larger number is shown in brackets.\n\nFootnotes are provided to provide clarity regarding the status of certain countries and territories.\n\n\n"}
{"id": "39820294", "url": "https://en.wikipedia.org/wiki?curid=39820294", "title": "List of journals published by Sri Lankan universities", "text": "List of journals published by Sri Lankan universities\n\nThis is a list of academic journals published by Sri Lankan universities.\n\n\n\n\n\n\n\n\n\n\n\"Prathimana\"; Ruhuna Journal of Sociology\nVimarshi; Sociological Journal\nGhanagaweshi; journal of Buddhist Philosophy\n\n\nJournal of Management\n\n\n\n"}
{"id": "55819429", "url": "https://en.wikipedia.org/wiki?curid=55819429", "title": "List of metropolitan planning organizations in the United States", "text": "List of metropolitan planning organizations in the United States\n\nThe United States government established planning organizations to provide for the coordination of land use, transportation and infrastructure. These Metropolitan Planning Organizations (MPO) may exist as a separate, independent organization or they may be administered by a city, county, regional planning organization, highway commission or other government organization. Each MPO has its own structure and governance. The following is a list of the current federally designated MPOs.\n\n"}
{"id": "5882744", "url": "https://en.wikipedia.org/wiki?curid=5882744", "title": "List of old Macintosh software", "text": "List of old Macintosh software\n\nThis is a list of old Macintosh software that no longer runs on current Macs. The software might require Mac OS 9 or other versions of the classic Mac OS that can't run on Apple's current Intel machines. Note that most old programs can still be run using emulators, such as SheepShaver, vMac, or Basilisk II.\n\nFor a list of current programs, see List of Macintosh software. Third-party databases include VersionTracker, MacUpdate and iUseThis. Since a list like this might grow too big and become unmanageable, this list is confined to those programs for which a Wikipedia article exists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13987437", "url": "https://en.wikipedia.org/wiki?curid=13987437", "title": "Luigi Bellardi", "text": "Luigi Bellardi\n\nLuigi Bellardi (18 May 1818 – 17 September 1889) was an Italian malacologist and entomologist who specialised in Diptera.\n\nBellardi was born in Genoa and died in Turin. His collection is in the Turin Museum of Natural History\n\nIn 1872, then a Professor at Liceo Gioberti, Luigi Bellardi began \"I molluschi dei terreni terziari del Piemonte della Liguria\", a work on molluscs of the Middle and Early Tertiary in the Mediterranean basin. In 1888 he published the five parts dealing with Cephalopoda, Pteropoda and the first families of Gastropoda. In 1889 his student Prof. Federico Sacco (1864–1948) took over this work and published 25 more sections partly based on Bellardi’s work on this species-rich fossil group.\n\n"}
{"id": "40953480", "url": "https://en.wikipedia.org/wiki?curid=40953480", "title": "Maripa virus", "text": "Maripa virus\n\nMaripa virus is a single-stranded, enveloped, negative-sense RNA hantavirus species in the Bunyavirales order. It is a new variant strain of Rio Mamore virus. It was first isolated from a patient with hantavirus pulmonary syndrome in French Guiana.\n\n\n"}
{"id": "48284539", "url": "https://en.wikipedia.org/wiki?curid=48284539", "title": "Multiomics", "text": "Multiomics\n\nMultiomics refers to a biological analysis approach in which the data sets are multiple \"omes\", such as the genome, proteome, transcriptome, epigenome, and microbiome; in other words, \"the use of multiple omics technologies to study life in a concerted way\". By combining these omes into a set of omes, scientists can analyze complex biololgical big data efficiently enough to easily find relevant biomarkers. In so doing, multiomics integrates diverse omics data to find a coherently matching geno-pheno-envirotype relationship or association, even with reduced numbers of samples, although multiomics usually involves 'big' data.\n\nUsually, a very large number of samples is required to detect functional relationships, but, using multiomics, encompassing multiple data types, researchers try to detect such associations with more confidence. A possible example case would be suicide biomarker detection by taking the blood of depressive patients and performing genomic, transcriptomic, and epigenomic sequencing, combining these omes to find significant markers from the case population.\n\n"}
{"id": "34068431", "url": "https://en.wikipedia.org/wiki?curid=34068431", "title": "Prehistoric technology", "text": "Prehistoric technology\n\nPrehistoric technology is technology that predates recorded history. History is the study of the past using written records. Anything prior to the first written accounts of history is prehistoric, including earlier technologies. About 2.5 million years before writing was developed, technology began with the earliest hominids who used stone tools, which they may have used to start fires, hunt, and bury their dead.\n\nThere are several factors that made the evolution of prehistoric technology possible or necessary. One of the key factors is behavioral modernity of the highly developed brain of \"Homo sapiens\" capable of abstract reasoning, language, introspection, and problem solving. The advent of agriculture resulted in lifestyle changes from nomadic lifestyles to ones lived in homes, with domesticated animals, and land farmed using more varied and sophisticated tools. Art, architecture, music and religion evolved over the course of the prehistoric periods.\n\nThe Stone Age is a broad prehistoric period during which stone was widely used in the manufacture of implements with a sharp edge, a point, or a percussion surface. The period lasted roughly 2.5 million years, from the time of early hominids to \"Homo sapiens\" in the later Pleistocene era, and largely ended between 6000 and 2000 BCE with the advent of metalworking.\n\nThe Stone Age lifestyle was that of hunter-gatherers who traveled to hunt game and gather wild plants, with minimal changes in technology. As the last glacial period of the current ice age neared its end (about 12,500 years ago), large animals like the mammoth and bison antiquus became extinct and the climate changed. Humans adapted by maximizing the resources in local environments, gathering and eating a wider range of wild plants and hunting or catching smaller game. Domestication of plants and animals with early stages in the Old World (Afro-Eurasia) Mesolithic and New World (Americas) Archaic periods led to significant changes and reliance on agriculture in the Old World Neolithic and New World Formative stage. The agricultural life led to more settled existences and significant technological advancements.\n\nAlthough Paleolithic cultures left no written records, the shift from nomadic life to settlement and agriculture can be inferred from a range of archaeological evidence. Such evidence includes ancient tools, cave paintings, and other prehistoric art, such as the Venus of Willendorf. Human remains also provide direct evidence, both through the examination of bones, and the study of mummies. Though concrete evidence is limited, scientists and historians have been able to form significant inferences about the lifestyle and culture of various prehistoric peoples, and the role technology played in their lives.\n\nThe Lower Paleolithic period was the earliest subdivision of the Paleolithic or Old Stone Age. It spans the time from around 2.5 million years ago when the first evidence of craft and use of stone tools by hominids appears in the current archaeological record, until around 300,000 years ago, spanning the Oldowan (\"mode 1\") and Acheulean (\"mode 2\") lithic technology.\n\nEarly human (hominid) used stone tool technology, such as a hand axe that was similar to that used by primates, which are found to have intelligence levels of modern children aged 3 to 5 years. Intelligence and use of technology did not change much for millions of years. The first \"Homo\" species began with \"Homo habilis\" about . \"Homo habilis\" (\"handy man') created stone tools called Oldowan tools. \"Homo ergaster\" lived in eastern and southern Africa about and used more diverse and sophisticated stone tools than its predecessor, \"Homo habilis,\" including having refined the inherited Oldowan tools and developed the first Acheulean bifacial axes.\n\n\"Homo erectus\" (\"upright man\") lived about in West Asia and Africa and is thought to be the first hominid to hunt in coordinated groups, use complex tools, and care for infirm or weaker companions. \"Homo antecessor\" the earliest hominid in Northern Europe lived from 1.2 million to 800,000 years ago and used stone tools. \"Homo heidelbergensis\" lived between 600,000 and 400,000 years ago and used stone tool technology similar the Acheulean tools used by \"Homo erectus\".\n\nEuropean and Asian sites dating back 1.5 million years ago seem to indicate controlled use of fire by \"Homo erectus\". A northern Israel site from about 690,000 to 790,000 years ago suggests that man could light fires. \"Homo heidelbergensis\" may have been the first species to bury their dead about 500,000 years ago.\n\nThe Middle Paleolithic period occurred in Europe and the Near East, during which the Neanderthals lived (c. 300,000–28,000 years ago). The earliest evidence (Mungo Man) of settlement in Australia dates to around 40,000 years ago when modern humans likely crossed from Asia by island-hopping. The Bhimbetka rock shelters exhibit the earliest traces of human life in India, some of which are approximately 30,000 years old.\n\n\"Homo neanderthalensis\" used Mousterian Stone tools that date back to around 300,000 years ago and include smaller, knife-like and scraper tools. They buried their dead in shallow graves along with stone tools and animal bones, although the reasons and significance of the burials are disputed.\n\n\"Homo sapiens\", the only living species in the genus \"Homo\", originated in Africa about 200,000 years ago. As compared to their predecessors, \"Homo sapiens\" had greater mental capability and ability to walk erect, which provided freed hands for manipulating objects and far greater use of tools. There was art created during this period. Intentional burial, particularly with grave goods, may be one of the earliest detectable forms of religious practice since it may signify a \"concern for the dead that transcends daily life.\" The earliest undisputed human burial so far dates back 130,000 years. Human skeletal remains stained with red ochre were discovered in the Skhul cave at Qafzeh, Israel with a variety of grave goods.\n\nDuring the Upper Paleolithic Revolution, advancements in human intelligence and technology changed radically with the advent of Behavioral modernity between 60,000 and 30,000 years ago. Behavioral modernity is a set of traits that distinguish \"Homo sapiens\" from extinct hominid lineages. \"Homo sapiens\" reached full behavior modernity around 50,000 years ago due to a highly developed brain capable of abstract reasoning, language, introspection, and problem solving.\n\nAurignacian tools, such as stone bladed tools, tools made of antlers, and tools made of bones were created during this period. People began creating clothing. What appear to be sewing needles were found around 40,000 years ago and dyed flax fibers dated 36,000 BP were found in a prehistoric cave in the Republic of Georgia. Human beings may have begun wearing clothing as far back as 190,000 years ago.\n\nCultural aspects emerged, such as art of the Upper Paleolithic period, which included cave painting, sculpture such as the Venus figurines, carvings and engravings of bone and ivory. The most common subject matter was large animals that were hunted by the people of the time.\nThe Cave of Altamira and Paleolithic Cave Art of Northern Spain and Côa Valley Paleolithic Art are examples of such artwork. Musical instruments such as flutes emerged during this period.\n\nThe Mesolithic period was a transitional era between the Paleolithic hunter-gatherers, beginning with the Holocene warm period around 11,660 BP and ending with the Neolithic introduction of farming, the date of which varied in each geographical region. Adaptation was required during this period due to climate changes that affected environment and the types of available food.\n\nSmall stone tools called microliths, including small bladelets and microburins, emerged during this period. For instance, spears or arrows were found at the earliest known Mesolithic battle site at Cemetery 117 in the Sudan. Holmegaard bows were found in the bogs of Northern Europe dating from the Mesolithic period. These microliths point to the use of projectile technology since they are widely assumed to have formed the tips and barbs of arrows. This is demonstrated by mesolithic assemblages found in southwest Germany, which revealed two types of projectiles used: arrows with transverse, trapezoidal stone tips and large barbed antler \"harpoons\". These implements indicate the nature of human adaptation to the environment during the period, describing the Mesolithic societies as hunter-gatherers.\n\nThe Neolithic Revolution was the first agricultural revolution, representing a transition from hunting and gathering nomadic life to an agriculture existence. It evolved independently in six separate locations worldwide circa 10,000–7000 years BP (8,000–5,000 BC). The earliest known evidence exists in the tropical and subtropical areas of southwestern/southern Asia, northern/central Africa and Central America.\n\nThere are some key defining characteristics. The Introduction of agriculture resulted in a shift from nomadic to more sedentary lifestyles, and the use of agricultural tools such as the plough, digging stick and hoe (tool) made agricultural labor more efficient. Animals were domesticated, including dogs. Another defining characteristic of the period was the emergence of pottery, and, in the late Neolithic period, the wheel was introduced for making pottery.\n\nNeolithic architecture included houses and villages built of mud-brick and wattle and daub and the construction of storage facilities, tombs and monuments. Copper metalworking was employed as early as 9000 BC in the Middle East; and a copper pendant found in northern Iraq dated to 8700 BC. Ground and polished stone tools continued to be created and used during the Neolithic period.\n\nNumeric record keeping evolved from a system of counting using small clay tokens that began in Sumer about 8000 BC.\n\nThe Stone Age developed into the Bronze Age after the Neolithic Revolution. The Neolithic Revolution involved radical changes in agricultural technology which included development of agriculture, animal domestication, and the adoption of permanent settlements.\n\nThe Bronze Age is characterised by metal smelting of copper and its alloy bronze, an alloy of tin and copper, to create implements and weapons. Polished stone tools continued to be used due to their abundance compared with the less common metals (especially tin).\n\nThis technological trend apparently began in the Fertile Crescent, and spread outward.\n\nThe Iron Age involved the adoption of iron or steel smelting technology, either by casting or forging. Iron replaced bronze, and made it possible to produce tools which were stronger, lighter and cheaper to make than bronze equivalents. The best tools and weapons were made from steel.\n\nOther societal changes often accompanied the introduction of iron, including practice changes in art, religion and agriculture. The Iron Age ends with the beginning of the historic periods, generally marked by the development of written language that enabled creation of historic records.\n\nThe timing of the adoption of iron depended upon \"the availability of iron ore and the state of knowledge\". Iron was smelted in Egypt about 6000 B.C. and iron replaced bronze in the Middle East about 1500 B.C. Chinese began casting iron about 5000 B.C. and their methods for casting iron was the precursor to modern steel manufacturing methods. Most of Asia, however, did not adopt production of iron until the historic period.\n\nIn Europe, iron was introduced about 1100 B.C. and had replaced bronze for creating weapons and tools by 500 B.C. They made iron through the forging smelting process and integrated casting in the Middle Ages. Large hill forts or oppida were built either as a refuge in time of war, or sometimes as permanent settlements. Agricultural practices were made more efficient with more effective and varied iron tools. In Europe, the Iron Age is the last prehistoric period and is followed by the Middle Ages.\n\nIron was extracted from metal ore starting about 2000 B.C. in Africa.\n\nThe New World periods began with the crossing of the Paleo-Indians, Athabaskan, Aleuts and Eskimos along the Bering Land Bridge onto the North American continent.\n\nThe Paleo-Indians were the first people who entered, and subsequently inhabited, the Americas during the final glacial episodes of the late Pleistocene period. Evidence suggests big-game hunters crossed the Bering Strait from Asia into North America over a land and ice bridge (Beringia), that existed between 45,000 BCE – 12,000 BCE, following herds of large herbivores far into Alaska.\n\nIn their book, \"Method and Theory in American Archaeology,\" Gordon Willey and Philip Phillips defined five cultural stages for the Americas, including the three prehistoric Lithic, Archaic and Formative stages. The historic stages are the Classic and Post-Classic stages.\n\nThe Lithic period occurred from 12,000 to 6,000 years before present and included the Clovis, Folsom and Plano cultures. Clovis culture was considered the first culture to use projectile points to hunt on the North American continent. Since then, a pre-Clovis site was found in Manis, Washington that found use of projectile points to hunt mastodons.\n\nThe Archaic period in the Americas was dated from 8,000 to 2,000 years before present. People were hunters of small game, such as deer, antelope and rabbits, and gatherers of wild plants, moving seasonally to hunting and gathering sites. Late in the Archaic period, about 200-500 A.D., corn was introduced into the diet and pottery-making became an occupation for storing and caring food.\n\nThe Formative stage followed the Archaic period in the Americas and continued until there was contact by European people. Some of the cultures from that period include that of the Ancient Pueblo People, Mississippian culture and Olmec cultures.\n\nCultures of the Formative Stage are supposed to possess the technologies of pottery, weaving, and developed food production. Social organization is supposed to involve permanent towns and villages, as well as the first ceremonial centers. Ideologically, an early priestly class or theocracy is often present or in development.\n\n\n\n"}
{"id": "5963240", "url": "https://en.wikipedia.org/wiki?curid=5963240", "title": "RRS Ernest Shackleton", "text": "RRS Ernest Shackleton\n\nRRS \"Ernest Shackleton\" is a Royal Research Ship operated by the British Antarctic Survey. She is primarily a logistics ship used for the resupply of scientific stations in the Antarctic.\n\nLaunched in 1995 as MV \"Polar Queen\" for GC Rieber Shipping, she was operated in the Antarctic by other national programmes. The British Antarctic Survey acquired her on a long-term bareboat charter in August 1999 and renamed her RRS \"Ernest Shackleton\" after the Anglo-Irish polar explorer Sir Ernest Shackleton. She replaced RRS \"Bransfield\". Known to users as the Shack, the ship was chartered to Crystal Cruise Line to escort its 68,000 ton liner Crystal Serenity through Canada's Northwest Passage in late August/September 2016.\n\nRRS \"Ernest Shackleton\" is ice strengthened with a double hull construction and is capable of a wide range of logistic tasks as well as having a scientific capability. She has a cargo tender \"Tula\" on deck for ship to shore transfer of equipment when the ship cannot berth alongside.\n\nRRS \"Ernest Shackleton\" is primarily a logistic ship, used for the resupply of the Survey's Antarctic research stations. She loads cargo and science equipment in the Humber and sails to the Antarctic in September/October each year, returning in May/June. After annual refit/drydock, \"Ernest Shackleton\" is chartered out for commercial use in the northern summer.\n\n\n"}
{"id": "10044864", "url": "https://en.wikipedia.org/wiki?curid=10044864", "title": "Research question", "text": "Research question\n\nSpecifying the research question is the methodological point of departure of scholarly research in both the natural and social sciences. The research will answer the question posed. At an undergraduate level, the answer to the research question is the thesis statement. The answer to a research question will help address a \"research problem\" which is a problem \"readers think is worth solving\". \n\nSpecifying the research question is one of the first methodological steps the investigator has to take when undertaking research. The research question must be accurately and clearly defined.\n\nChoosing a research question is the central element of both quantitative and qualitative research and in some cases it may precede construction of the conceptual framework of study. In all cases, it makes the theoretical assumptions in the framework more explicit, most of all it indicates what the researcher wants to know most and first.\n\nThe student or researcher then carries out the research necessary to answer the research question, whether this involves reading secondary sources over a few days for an undergraduate term paper or carrying out primary research over years for a major project.\n\nWhen the research is complete and the researcher knows the (probable) answer to the research question, writing up can begin (as distinct from writing notes, which is a process that goes on through a research project). In term papers, the answer to the question is normally given in summary in the introduction in the form of a thesis statement.\n\nThe research question serves two purposes:\nTherefore, the writer must first identify the type of study (qualitative, quantitative, or mixed) before the research question is developed.\n\nA qualitative study seeks to learn why or how, so the writer’s research must be directed at determining the what, why and how of the research topic. Therefore, when crafting a research question for a qualitative study, the writer will need to ask a why or how question about the topic. For example: How did the company successfully market its new product? The sources needed for qualitative research typically include print and internet texts (written words), audio and visual media.\n\nHere is Creswell's (2009) example of a script for a qualitative research central question: \n\nA quantitative study seeks to learn where, or when, so the writer’s research must be directed at determining the where, or when of the research topic. Therefore, when crafting a research question for a quantitative study, the writer will need to ask a where, or when question about the topic. For example: Where should the company market its new product? Unlike a qualitative study, a quantitative study is mathematical analysis of the research topic, so the writer’s research will consist of numbers and statistics.\n\nHere is Creswell's (2009) example of a script for a quantitative research question: \n\nAlternatively, a script for a quantitative null hypothesis might be as follows: \n\nQuantitative studies also fall into two categories: \n\nA mixed study integrates both qualitative and quantitative studies, so the writer’s research must be directed at determining the why or how and the what, where, or when of the research topic. Therefore, the writer will need to craft a research question for each study required for the assignment. A typical study may be expected to have between 1 and 6 research questions.\n\nOnce the writer has determined the type of study to be used and the specific objectives the paper will address, the writer must also consider whether the research question passes the ‘so what’ test. The ‘so what’ test means that the writer must construct evidence to convince the audience why the research is expected to add new or useful knowledge to the literature.\n\n\"Problematique\" is a term that functions analogously to the research problem or question used typically when addressing global systemic problems. The term achieved prominence in 1970 when \nHasan Özbekhan, Erich Jantsch and Alexander Christakis conceptualized the original prospectus of the Club of Rome titled \"The Predicament of Mankind\". In this prospectus the authors designated 49 Continuous Critical Problems facing humankind, saying \"We find it virtually impossible to view them as problems that exist in isolation - or as problems capable of being solved in their own terms... It is this generalized meta system of problems, which we call the 'problematique' that inheres in our situation.\"\n\nSituations similar to the global problematique in their complexity are also called problematiques. These situations receive different designations from other authors. C. West Churchman, Rittell and Weber, and Argyris call these situations wicked problems. Russell Ackoff simply called them \"messes.'\"\n\n\n\n"}
{"id": "11319315", "url": "https://en.wikipedia.org/wiki?curid=11319315", "title": "Scientific diving", "text": "Scientific diving\n\nScientific diving is the use of underwater diving techniques by scientists to perform work underwater in the direct pursuit of scientific knowledge. The legal definition of scientific diving varies by jurisdiction. Scientific divers are normally qualified scientists first and divers second, who use diving equipment and techniques as their way to get to the location of their fieldwork. The direct observation and manipulation of marine habitats afforded to scuba-equipped scientists have transformed the marine sciences generally, and marine biology and marine chemistry in particular. Underwater archeology and geology are other examples of sciences pursued underwater. Some scientific diving is carried out by universities in support of undergraduate or postgraduate research programs, and government bodies such as the United States Environmental Protection Agency and the UK Environment Agency carry out scientific diving to recover samples of water, marine organisms and sea, lake or riverbed material to examine for signs of pollution.\n\nEquipment used varies widely in this field, and is generally selected based on cost, effectiveness, availability and risk factors. Open-circuit scuba is most often used as it is widely available and cost-effective, and is the entry level training mode in most places.\n\nScientific diving in the course of employment may be regulated by occupational safety legislation, or may be exempted as self-regulated by a recognised body. The safety record has generally been good. Collection of scientific data by volunteers outside of employment is generally considered to legally be recreational diving.\n\nTraining standards vary throughout the world, and are generally higher than for entry level recreational diving, and in some cases identical to commercial diver training. There are a few international agreements that facilitate scientists from different places working together on projects of common interest, by recognising mutually acceptable minimum levels of competence.\n\nScientific diving is any diving undertaken in the support of science, so activities are widely varied and may include visual counts and measurements of organisms in situ, collection of samples, surveys, photography, videography, video mosaicing, benthic coring, coral coring, placement, maintenance and retrieval of scientific equipment.\n\nUnderwater diving interventions, particularly on scuba, provide the capacity for scientists to make direct observations on site and in real time, which allow for ground-truthing of larger scale observations and occasional serendipitous observations outside the planned experiment. Human dexterity remains less expensive and more adaptable to unexpected complexities in experimental setup than remotely operated and robotic alternatives in the shallower depth ranges. Scuba has also provided insights which would be unlikely to occur without direct observation, where hypotheses produced by deductive reasoning have not predicted interactive and behavioural characteristics of marine organisms, and these would not be likely to be detected from remote sensing or video or other methods which do not provide the full context and detail available to the diver. Scuba allows the scientist to set up the experiment and be present to observe unforeseen alternatives to the hypothesis.\n\nThe field of global change biology includes investigation of evidence relating to global warming and ocean acidification. Many of the measurable changes in global climate occur in the sea. Coral bleaching is an example of an indicator of change, and scuba diving has provided a large amount of low-impact observational data contributing significantly to the large body of knowledge on the subject over several decades.\n\nThe field of ocean acidification and the impact of anthropogenic carbon dioxide emission has seen similar growth and most of the cited articles in this field have relied to a significant extent on data collected during scuba diving operations.\n\nThe field of paleoclimate reconstruction has a major influence on the understanding of evolution and the ecological and biogeographic past, as climate is the most powerful driver of evolution. Coring corals on a reef in the least harmful and focused manner is currently most practicable using scuba technology. This mining of the past makes it possible to attempt to predict future climate.\n\nAdvances in training and accessibility to trimix diving and closed circuit rebreather systems has enabled scientific divers to reach highly diverse deeper mesophotic reefs which may be the corals last refuge from the warming of surface waters.\n\nThe current knowledge of the functioning of the ecologically and economically important hard-bottom communities in the shallow water coastal zones is both limited and particularly difficult to study due to poor accessibility for surface operated instrumentation as a result of topographic and structural complexity which inhibit remote sampling of orgnisms in the benthic boundary layer. In situ assessments by scientific divers remain the most flexible tool for exploring this habitat and allow precise and optimised location of instruments.\n\nThe capacity to dive under polar ice provides an opportunity to advance science in a restricted environment\nat relatively low cost. A small number of holes in the ice can provide access over a large area and high levels of experimental replication. Divers are a flexible and reliable method for deploying, maintaining and retrieving equipment from under‐ice environments, and are relatively cost efficient for researching remote locations that, would otherwise require the use of more expensive research vessels.\n\nThe global threat to marine ecosystems due to over‐exploitation, habitat loss, pollution and climate change is exacerbated by introduction of alien species, which is considered to be one of the leading causes of extinctions and biodiversity loss. Scientific divers are the most competent to detect the presence of potentially invasive species and in some cases can provide a quick response. Monitoring the effectiveness of response also requires diver intervention.\n\nScientific diving may use any mode of diving that is best suited to the project. Scientific diving operations may use amd have used freediving, scuba open circuit, scuba closed circuit, surface oriented surface-supplied systems, saturation diving from surface or underwater habitats, atmospheric suit diving or remotely operated underwater vehicles. Breathing gases used include air, oxygen, nitrox, trimix, heliox and experimental mixtures.\n\n\n\nSeveral citizen science projects use observational input from recreational divers to provide reliable data on presence and distribution of marine organisms. The ready availability of digital underwater cameras makes collection of such observations easy and the permanence of the record allows peer and expert review. Such projects include the Australian-based Reef Life Survey, and the more international iNaturalist project, based in California, which is only partly focused on marine species.\n\nScientific diving operations which are part of the work of an organisation are generally under the control of a diving supervisor or equivalent, and follow procedures similar to other professional diving operations.\n\nA scientific diving operation which follows the usual procedures of a commercial scuba operation will include one or more working divers, a stand-by diver and a supervisor, who will manage the operation from the surface control point. If the divers are tethered, there will generally be a line tender for each tethered diver in the water The stand-by diver may remain out of the water at the surface or may accompany the working diver or divers in the water. Surface supplied and saturation operations will also generally follow standard procedures used by commercial divers.\n\nThe American system has a Diving Control Board taking overall responsibility for all scientific diving work done by an organisation. The Diving Officer is responsible to the board for operational, diving and safety matters.\nFor each dive, one scientist, designated as the Lead Diver, must be present at the site during that entire operation, and is responsible for management of the dive, including dive planning, briefing, emergency planning, equipment and procedures. The divers operate in a strict buddy diving system.\n\nThe standard procedures for scuba and surface supplied diving are essentially the same as for any other similar diving operation using similar equipment in a similar environment, by both recreational, technical and other professional divers. There are a few special cases where scientific diving operations are carried out in places where other divers would generally not go, such as blue-water diving. Scientific dives tend to be more task oriented than recreational dives, as the scientist is primarily there to gather data, and the diving is of secondary importance, as the way to get to the worksite.\n\nThe requirements for qualification as a scientific diver vary with jurisdiction. The European Scientific Diver (ESD) standard is reasonably representative:\n\nBasic skills and underlying knowledge must include:\n\nEmergency skills include competence in:\n\nThe requirements for qualification as a scientific diver vary with jurisdiction. The European Scientific Diver (ESD) standard is reasonably representative:\n\nCompetence in work methods common to scientific projects:\n\nUnderwater navigation by divers is broadly split into three categories. \"Natural navigation\" techniques, and \"orienteering\", which is navigation focused upon the use of an underwater magnetic compass. and following a guideline.\n\nNatural navigation, sometimes known as pilotage, involves orienting by naturally observable phenomena, such as sunlight, water movement, bottom composition (for example, sand ripples run parallel to the direction of the wave front, which tends to run parallel to the shore), bottom contour and noise. Although natural navigation is taught on courses, developing the skills is generally more a matter of experience.\n\nOrienteering, or compass navigation, is a matter of training, practice and familiarity with the use of underwater compasses, combined with various techniques for reckoning distance underwater, including kick cycles (one complete upward and downward sweep of a kick), time, air consumption and occasionally by actual measurement. Kick cycles depend on the diver's finning technique and equipment, but are generally more reliable than time, which is critically dependent on speed, or air consumption, which is critically dependent on depth, work rate, diver fitness, and equipment drag. Techniques for direct measurement also vary, from the use of calibrated distance lines or surveyor's tape measures, to a mechanism like an impeller log, to pacing off the distance along the bottom with the arms.\n\nSkilled underwater navigators use techniques from both of these categories in a seamless combination, using the compass to navigate between landmarks over longer distances and in poor visibility, while making use of the generic oceanographic indicators to help stay on course and as a check that there is no mistake with the bearing, and then recognising landmarks and using them with the remembered topography of a familiar site to confirm position.\n\nGuidelines, also known as cave lines, distance lines, penetration lines and jackstays are permanent or temporary lines laid by divers to mark a route, particularly in caves, wrecks and other areas where the way out from an overhead environment may not be obvious. Guidelines are also useful in the event of silt out.\n\nDistance lines are wound on to a spool or a reel. The length of the distance line used is dependent on the plan for the dive. Reels for distance lines may have a locking mechanism, ratchet or adjustable drag to control deployment of the line and a winding handle to help keep slack line under control and rewind line. The material used for any given distance line will vary based on intended use. The use of guideline for navigation requires careful attention to laying and securing the line, line following, marking, referencing, positioning, teamwork, and communication.\n\nA transect line is a special case of a guideline commonly used in scientific diving. It is a line laid to guide the diver on a survey along the line. In cases where position along the line must be accurately specified, a surveyor's tape or chain may be used as the transect line.\n\nSearches are often required to find the subject of study, or to recover previously placed instrumentation. There are a number of techniques in general use. Some of these are suitable for scuba, and some for surface supplied diving. The choice of search technique will depend on logistical factors, terrain, protocol and diver skills.\n\nAs a general principle, a search method attempts to provide 100% coverage of the search area. this is greatly influenced by the width of the sweep. In conditions of zero visibility this is as far as the diver can feel with his hands while proceeding along the pattern. When visibility is better, it depends on the distance at which the target can be seen from the pattern. In all cases then, the pattern should be accurate and completely cover the search area without excessive redundancy or missed areas. Overlap is needed to compensate for inaccuracy, and may be necessary to avoid gaps in some patterns. Common search patterns include:\n\nMost scientific fieldwork involves some form of data collection. In some cases it is on-site measurement of physical data, and sometimes it involves taking samples, usually recording the circumstances in some detail. Video, still photography and manual listing of measurements and labeling of specimens are common practice. Biological and geological specimens are usually bagged and labelled for positive identification, and the availability of underwater cameras allows in-situ and bagged photographs to be taken for reference Biologica specimens may also be tagged an released, or have small biopsies taken for DNA analysis. When non-extractive measurements are made, video and still photography provide backup for listed data. Recording on prepared sheets is preferred where practicable as writing underwater is relatively inefficient, and often not very legible. Waterproof paper on a clipboard or a waterproof slate are commonly used for written records. Ordinary graphite pencils work fairly well underwater, though the wood tends to split after a while.\n\nTypes of survey:\nMapping of an underwater site may be necessary for analysis of the data. Several methods are available. A map is the two or three dimensional representation of geographic survey data following a standardised format, often using symbols representations of data, and often to a specified scale.\n\nGenerally relatively low risk and good safety record overall, the vast majority of dives are relatively shallow and in reasonably good conditions. Most scientific dives can be deferred when conditions are sub-optimal, and seldom require the use of dangerous equipment. This has allowed a good safety record in spite of relatively relaxed equipment and training requirements for occupational diving.\n\nThe earliest scientific diving safety programme in the US was established at the Scripps Institution of Oceanography in 1954, about 5 years before the development of the national recreational scuba training agencies. Most American scientific diving programmes are based on elements of the original Scripps diving programme.\n\nA survey of some half a million scientific dives reported 7 fatalities and 21 cases of decompression illmess. These rates are lower than those previously reported for military personnel, recreational divers in the UK, recreational divers in the Caribbean, recreational divers in western Canada and wreck divers in cold water.\n\nNitrox has been used for open circuit scientific diving since the early 1970s with no evidence of increased DCS risk in comparison with similar air dives.\n\nA maximum oxygen partial pressure of 1.6 bar has been found generally acceptable for open circuit nitrox diving by the scientific community, and it has not been found necessary to screen for carbon dioxide retention.\n\nInvestigation of the order of dive profiles has shown no statistical increase of decompression sickness risk in reverse profile diving. No validity was found for the rule of diving progressively shallower in successive no-decompression dives imposed by recreational diver training organisations.\n\nAs of 1992 the prevalence of decompression illness in the United States was estimated at one case per 100,000\ndives for the scientific diving community. This may be compared with approximately one case per 1000 dives for commercial diving and one case per 5000 dives for recreational diving. The reported decompression sickness rate of 1:100,000 over 50 years appears to be acceptable to the scientific diving community. Diving profiles resemble recreational diving more than other sectors, but the incident rate in scientific diving is an order of magnitude lower than for recreational diving. This has been attributed to more thorough entry-level and continued training, better supervision and operational procedures and medical and fitness screening.\n\nIn the United States scientific diving is done by research institutions, universities, museums, aquaria, and consulting companies for purposes of research, education and environmental monitoring. As of 2005 there were an estimated 4000 scientific divers, of which a small number are career scientific divers, with an average age of around 40 years, and a larger number of students in the 18 to 34 year age group. There is no specific upper age limit providing the diver remains medically fit to dive. The lower limit is determined by the age of students qualifying for training. About a quarter are female.\n\nScientific diving is generally considered to be occupational diving, and is usually regulated as such except where specifically exempted.\n\nIn the US, scientific diving is exempted from the requirements of the Federal Occupational Safety and Heath regulations, provided that it complies with the requirements specified for the exemption.\n\nScientific diving governance organizations include:\n\nWhen a scientific diving operation is part of the duties of the diver as an employee, the operation may be considered a professional diving operation subject to regulation as such. In these cases the training and registration may follow the same requirements as for other professional divers, or may include training standards specifically intended for scientific diving. In other cases, where the divers are in full control of their own diving operation, including planning and safety, diving as volunteers, the occupational health and safety regulations may not apply.\n\nWhere scientific diving is exempt from commercial diving regulation, training requirements may differ considerably, and in some cases basic scientific diver training and certification may not differ much from entry level recreational diver training.\n\nTechnological advances have made it possible for scientific divers to accomplish more on a dive, but they have also increased the complexity and the task loading of both the diving equipment and the work done, and consequently require higher levels of trainng and preparation to safely and effectively use this technology. It is preferable for effective learning and safety that such specialisation training is done systematically and under controlled conditions, rather than on site and on the job. Environmental conditions for training should include exercises in conditions as close as reasonably practicable to field conditions.\n\nAlthough the first scientific diving expedition in Australia was carried out by Sir Maurice Yonge to the Great Barrier Reef in 1928, most scientific diving did not start until 1952 when the Commonwealth Scientific and Industrial Research Organisation began work to understand the pearl beds of northern Australia in 1957.\nCommercial divers worked under Australian Standard CZ18 \"Work in Compressed Air\" in 1972. This standard applied to caisson workers and divers so the underwater work was drafted into AS 2299 \"Underwater Air Breathing Operations\" in 1979. In 1987, a re-write of AS 2299 included scientific diving in the regulations even though the divers had been self-regulating under the Australian Marine Sciences Association (AMSA). At that time, the AMSA and the Australian Institute for Maritime Archaeology (AIMA) began a collaboration to draft a new standard for scientific diving.\n\nIn the 1960s there were no regulations for scientific diving in Germany, but two fatal accidents in 1969 led to the implementation of guidelines for scientific diving based on the commercial diving guidelines. These define the equipment, training, protocols and legal background for scientific diving for German universities, research institutes and government organisations. Divers trained to these requirements are mostly science students or technicians, and are subsequently registered as scientific divers.\n\nScientific diving is done by a tethered diver in the water, monitored by a dive tender at the surface, controlled by a dive operation leader (supervisor) and with a standby diver on site. Diving equipment includes full-face mask and dry suit, but a buoyancy control device is not obligatory. Most dives do not require decompression stops.\n\nIn South Africa, scientific diving is considered a form of commercial diving and is within the scope of the Diving Regulations 2009 and the Code of Practice for Scientific Diving published by the Chief Inspector of the department of Labour, Under DR 2009 the Codes of Practice are guidance and not compulsory practice. They are provided as recommended good practice, and in theory need not be followed providing an acceptable level of safety is achieved in terms of the Occupational Health and Safety Act #85 of 1993. However, in this case the onus is on the diving contractor to ensure acceptable safety during the diving operation based on risk assessment. The level of safety required is specified in the OHS act as \"reasonably practicable\" taking into account a number of factors, including cost effectiveness, availability of technology for mitigation and available knowledge of hazards. Use of the relatively flexible scientific code rather than the default Code of Practice for Inshore Diving is restricted to clients which are registered as organisations engaged in either scientific research or higher education.\n\nThe qualification required to dive at work in South Africa is linked to the mode of diving, the equipment to be used, and the diving environment. There are six classes of occupational diver registration, all of which may be employed in scientific diving operations within the scope of the specified competence and when supported by the required infrastructure.\nIn each of these classes, the fundamental diving competences include those of the class with the next higher number, though specialist skills may differ from person to person and may have no obvious connection to the registered class.\n\nMost scientific diving in South Africa is done on open circuit scuba by Class 4 and 5 divers as no-stop dives on air or nitrox. The Code of Practice allows for the use of alternative modes and technologies provided appropriate competence is achieved by training and assessment, and the risk of the project is assessed as acceptable by both the organisation and the members of the diving team.\n\nAs diving is an activity that is considered to put the diver at a higher than normal risk to health, in the UK all diving at work, including scientific diving, is regulated through the Diving at Work Regulations, 1997 and the associated approved codes of practice, which are implemented by the Health and Safety Executive. The code of practice for scientific diving also covers archaeological diving and diving in public aquariums. The professional body representing the scientific and archaeological diving sector is the Scientific Diving Supervisory Committee (SDSC), and it is responsible to the Natural Environment Research Council\n\nThe determining factors indicating that a person is diving at work, and therefore are subject to the regulations, are:\nHSE regulations are only enforceable within UK waters, but operations from UK registered merchant vessels may also require adherence to the regulations and codes of practice.\n\nUndergraduate students and volunteers are generally not regarded as being at work, but if diving as part of an organised event or programme, the diving contractor will still have a duty of care. Postgraduate students are more likely to be considered at work when the diving is a significant part of their research.\n\nIn the United States scientific diving is permitted by the Occupational Safety and Health Administration to operate under an alternative consensual standard of practice that is maintained by the American Academy of Underwater Sciences.\n\n29 CFR Part 1910 - Subpart T \"Commercial Diving Operations,\" establishes mandatory occupational safety and health requirements for commercial diving operations which apply wherever OSHA has statutory jurisdiction. This covers the inland and coastal territorial watrs of the United States and possessions. \nThe United Brotherhood of Carpenters and Joiners of America petitioned the Federal Government in 1975 to issue an emergency temporary standard covering all professional diving operations, which was issued on June 15, 1976, to be effective from July 15, 1976. This was challenged in the US Court of Appeals and was withdrawn in November 1976. A permanent standard for commercial diving was subsequently formulated which became effective from October 20, 1977. The American Academy for Underwater Science applied for an exemption for scientific diving, citing 20 years of self-regulation and a lower accident rate than the commercial diving industry. An exemption was issued effective from November 28, 1982, after negotiation.\n\nTo be able to avail itself of the Scientific Diving Exemption the institution under whose auspices the work is carried out must meet four tests:\n\nThe AAUS promulgates and regularly reviews the consensus based Standards for Scientific Diving Certification and Operation of Scientific Diving Programs, which is a guideline for scientific diving programs in the US, and also used in some other countries. this document is currently the \"Standard\" of the scientific diving community and must be followed by all organizational members, these standards allow for reciprocity between institutions, and are widely used throughout the United States and some foreign countries.\n\nThe AAUS uses three levels of scientific diver authorisation:\n\nThere are also depth limitations which may be incrementally increased based on satisfactory experience, for 9 msw, 18 msw, 30 msw, 40 msw 45 msw and 58 msw. A range of specialty qualifications may follow additional training and assessment. These are: decompression diving, surface-supplied diving, mixed-gas diving, nitrox diving, rebreather diving, lock-out and saturation diving, blue-water diving, drysuit diving,\noverhead environment diving, altitude diving, and use of dive computers for decompression monitoring.\n\nVarious methods may be used to allow for international recognition of scientific divers, allowing them to work together on projects. In some cases the professional diver qualifications may be mutually recognised between countries, and in other cases the exemption allows the controlling bodies to make the necessary arrangements.\n\nThe European Scientific Diving Panel (ESDP) is the European platform for the advancement of underwater scientific excellence and to promote and provide a practical support framework for scientific diving at a European scale. The ESDP was initiated in 2008 as a European Marine Board Panel (until April 2017) and currently is receiving organizational support from the European network of Marine Stations (MARS).\nOfficial website of the European Scientific Diving Panel (ESDP): http://ssd.imbe.fr/?lang=en\n\nTwo levels of scientific diver registration are recognised. These represent the minimum level of training and competence required to allow scientists to participate freely throughout the countries of the European Union in underwater research projects diving using scuba. Certification or registration by an authorized national agency is a prerequisite, and depth and breathing gas limitations may apply.\nThis competence may be gained either through a formal training program, by in the field training and experience under appropriate supervision, or by a combination of these methods. These standards specify the minimum basic training and competence for scientific divers, and do not consider any speciality skill requirements by employers. Further training for job-specific competence is additional to the basic competence implied by the registration.\n\nAll member countries of the European Union are expected in terms of directive EEC 92/51 to recognise\none or both of these training levels. An applicant who satisfies the requirements will be issued with either an ESD or an AESD certificate that is valid for five years, and must be renewed every five years by application to the issuing authority. The certificate holders must comply with all national and local rules regarding medical fitness, workplace safety, insurance, and limitations on scientific diving activities when engaged in scientific diving in a host member country. The certificate only indicates previously assessed competence to the training level, and not the current level of competence.\n\n"}
{"id": "55373668", "url": "https://en.wikipedia.org/wiki?curid=55373668", "title": "Soapbox Science", "text": "Soapbox Science\n\nSoapbox Science is a public outreach platform that promotes women working in science and the research that they do. The events turns public spaces into an area for learning and debate, in the spirit of Hyde Park's Speaker's Corner. Soapbox Science encourages scientists to explain their research to members of the public using non-traditional methods (for example, there is no use of a projector or slides). Speakers typically make props at home to explain the processes behind their research. \n\nSoapbox Science launched in London in 2011, led by Seirian Summer and Nathalie Pettorelli and funded by L'Oreal UNESCO For Women In Science Scheme, Zoological society of London and the Science & Technology Facilities Council. Soapbox Science formed a partnership with Speakezee in 2016.\n\nSoapbox science aims to showcase some of the most eminent female scientists across the world.\n\nThe first three annual events 2011-2013 ran in London, in 2014 events ran in London, Bristol, Dublin, and Swansea.\n\nIn 2015 more cities joined including Exeter, Manchester, Newcastle, Belfast and Glasgow.\n\nIn 2016, Cambridge, Cardiff, Edinburgh, Milton Keynes, Oxford, Galway, Reading and Brisbane ran events.\n\nSoapbox Science was established to compliment other initiatives such as Athena SWAN that tackle the low numbers of women in Science, Technology, Engineering and Mathematics (STEM) in the UK.\n\n\n\n"}
{"id": "1418911", "url": "https://en.wikipedia.org/wiki?curid=1418911", "title": "Taxome", "text": "Taxome\n\nThe taxome is the sum of all the described species and higher groups such as genera, families, phyla of all life, or the sum of all valid taxa. The documenting of all this biodiversity is still very incomplete.\n\nMany organisms have already been documented (it is guessed that around 1-2 million species have been described — see Species Inventory in biodiversity article), but probably around nine-tenths of existing species have never been described, and those that have been described may have been redescribed, many times, under different names. Thus, the state of the science of taxonomy (the systematic organization of life) is somewhat confused.\n\nThe system of Linnean nomenclature (named after the 18th century Swedish pioneer Carl Linnaeus) is such that, in principle, the correct scientific name of all organisms described could be adjudicated, by means of historical priority. However, the names of many organisms are published in such obscure books and journals that very little of this information is available to most people. Thus, not only do we have little idea how many species there are, but we are even ignorant of the number of supposedly known species that have been described.\n\nHowever, the situation is not as bad as it is sometimes painted. The sum total of all existing taxonomic and nomenclatural documentation of living things is not large by today's data-rich standards. We know where the books and journals are where species might have been described. We simply need to put in the work to make the text and graphics of species descriptions available in databases, particularly electronic databases. In many groups, species are poorly known, but it is now rare to discover a major undetected phylum or family, so coverage of the kinds of diversity of life is probably good, even if all species are not described. Most of the undiscovered groups are probably reasonably closely related to something we have described. Thus if we could document the existing knowledge, we would have a useful, albeit somewhat pixellated map of the diversity of life.\n\nJust as a sequenced genome of an organism is a map of its DNA, a database documenting the complete taxome would form a map of the described biodiversity of the entire planet. Existing projects are mapping the positions of moons, planets and stars in the universe, as well as the physical geography of our own planet, and the genomes of selected organisms. Projects are now being set up to take advantage of information technology to document and make available the entire body of taxonomy on line. When this information becomes more complete, it should be possible to link this taxome information to all other information about biology.\n"}
{"id": "473083", "url": "https://en.wikipedia.org/wiki?curid=473083", "title": "TiHKAL", "text": "TiHKAL\n\nTIHKAL: The Continuation is a 1997 book written by Alexander Shulgin and Ann Shulgin about a family of psychoactive drugs known as tryptamines. A sequel to \"PIHKAL: A Chemical Love Story\", \"TIHKAL\" is an acronym that stands for \"Tryptamines I Have Known and Loved.\"\n\n\"TIHKAL\", much like its predecessor \"PIHKAL\", is divided into two parts. The first part, for which all rights are reserved, begins with a fictionalized autobiography, picking up where the similar section of \"PIHKAL\" left off; it then continues with a collection of essays on topics ranging from psychotherapy and the Jungian mind to the prevalence of DMT in nature, ayahuasca and the War on Drugs. The second part of \"TIHKAL\", which may be conditionally distributed for non-commercial reproduction (see external links below), is a detailed synthesis manual for 55 psychedelic compounds (many discovered by Alexander Shulgin himself), including their chemical structures, dosage recommendations, and qualitative comments. Shulgin has made the second part freely available on Erowid.org while the first part is available only in the printed text.\n\nLike \"PIHKAL\", the Shulgins were motivated to release the synthesis information as a way to protect the public's access to information about psychedelic compounds, a goal Alexander Shulgin has noted many times. Following a raid of his laboratory in 1994 by the United States DEA, Richard Meyer, spokesman for DEA's San Francisco Field Division, stated that \"It is our opinion that those books [referring to the previous work, \"PIHKAL\"] are pretty much cookbooks on how to make illegal drugs. Agents tell me that in clandestine labs that they have raided, they have found copies of those books.\"\n\n\n"}
{"id": "38064505", "url": "https://en.wikipedia.org/wiki?curid=38064505", "title": "United States-India Science &amp; Technology Endowment Fund", "text": "United States-India Science &amp; Technology Endowment Fund\n\nThe United States-India Science & Technology Endowment Fund is a joint fund established between the United States and India for the purpose of promoting innovation through science and technology. It is governed by a board representing both India and the United States and provides grants of up to $500,000 for joint projects between the U.S. and India. \n\nThe U.S.-India Science & Technology Endowment Fund was established in 2009 with a joint agreement between the United States and India, having an annual budget of approximately $2 to $3 million per year. A board with members from both countries was established to award grants on a semi-annual basis. The board was established through the United States Department of State and the India Department of Science & Technology. \n\nIn May 2012, then United States Secretary of State Hillary Clinton and former Indian Minister of Science and Technology Vilasrao Deshmukh awarded the first grantee of the fund. The first grantees included a partnership between both countries to develop a cold chain storage for produce, a procedure for metabolic screening of newborns, and a shoe specifically built to assist patients with Parkinson's disease. \n\n\n"}
{"id": "35308606", "url": "https://en.wikipedia.org/wiki?curid=35308606", "title": "Welbeck Academy", "text": "Welbeck Academy\n\nThe Welbeck Academy or Welbeck Circle is a name that has been given to the loose intellectual grouping around William Cavendish, 1st Duke of Newcastle-upon-Tyne in the first half of the 17th century. It takes its name from Welbeck Abbey, a country house in Nottinghamshire that was a Cavendish family seat. Another term used is Newcastle Circle. The geographical connection is, however, more notional than real; and these terms have been regarded also as somewhat misleading. Cavendish was Viscount Mansfield in 1620, and moved up the noble ranks to Duke, step by step; \"Newcastle\" applies by 1628.\n\nNewcastle was a royalist exile in continental Europe in the latter part of the First English Civil War and the Interregnum. He then returned to England and lived to 1676. His life shows many instances of cultural and intellectual patronage.\n\nA scientific interest was optics. The group involved in these studies included Charles Cavendish (William's brother), Thomas Hobbes, Robert Payne and Walter Warner. This core \"academy\" group was disrupted when Newcastle took on responsibility for the Prince of Wales, in 1638. At a later point John Pell was in Newcastle's service. \n\nCharles Cavendish's circle included Henry Bond, Richard Reeve or Reeves the instrument-maker, John Twysden and John Wallis. He was a patron of William Oughtred.\n\nNewcastle in the 1630s became a major patron to Ben Jonson. His second wife was Margaret Cavendish, née Lucas, the writer. Newcastle was called \"our English Maecenas\" by Gerard Langbaine the Younger; he was a patron after the Restoration to both John Dryden and Thomas Shadwell. Other writers he supported included William Davenant, William Sampson, James Shirley and John Suckling. He bought sculptures by Francesco Fanelli for Welbeck.\n\nAs a consequence of the royalist defeat at the Battle of Marston Moor in 1644, Newcastle and some of his entourage went into exile. He returned to England only with the Restoration of 1660. Initially he went to Hamburg. By 1645 Newcastle was in Paris: his circle had contacts in Marin Mersenne and Claude Mydorge, whom Charles Cavendish had met in France at least 15 years earlier. In France Newcastle met and married that year Margaret Lucas who was with the exiled court of Queen Henrietta Maria. She studied with Charles Cavendish, and became a writer on natural philosophy, initially a proponent of atomism. Besides Hobbes, who joined them in Paris, the Cavendishes knew at this period René Descartes, Kenelm Digby, and Christiaan Huygens. Much of the latter part of their exile was spent at Antwerp; there, though in debt, they lived in the Rubenshuis. Other associations were with Walter Charleton who came to know Margaret Cavendish (not necessarily abroad, since she returned to England for a time), and William Brereton, 3rd Baron Brereton.\n\nHobbes was employed by another branch of the Cavendish family (the Devonshire Cavendishes, who owned Chatsworth House). His association with Welbeck started at a date that is not completely clear. It was certainly by 1631, when he was tutor to a different Earl of the same name, William Cavendish, 3rd Earl of Devonshire. But possibly Hobbes had met Mansfield (as he then was) by 1627, on a tour of the Peak District, according to surviving poems (his own and by Richard Andrews), as related by Noel Malcolm. Hobbes himself claimed he had been in discussion with the Cavendish brothers by 1630; by 1636 he was engaging in a scientific correspondence with Newcastle. A manuscript work from the Cavendish group of this period, the so-called \"Short Tract on First Principles\", is considered by Malcolm to be by Payne though very much influenced by the issues Hobbes was addressing at the time, and his approach. But the work has also been attributed to Hobbes himself, by scholars from Ferdinand Tönnies (who christened it) onwards.\n"}
{"id": "59217032", "url": "https://en.wikipedia.org/wiki?curid=59217032", "title": "Year Million", "text": "Year Million\n\nYear Million is a six-part documentary and science fiction television series produced by National Geographic, which premiered on May 15, 2017, on their channel. The series received two Emmy Award nominations, \nincluding a Primetime Emmy for its narrator Laurence Fishburne. The series is based on the 2008 book \"Year Million: Science at the Far Edge of Knowledge\" by Damien Broderick. The narrative alternates between the years 2050, 3000, 500,000 and Year Million, using 2016 interviews to explain events unfolding in the story. The series was filmed in Budapest. \n\nInvestigating the ramifications of a variety of potentially world-changing inventions, the series visits a cast of characters representing a typical American family in several different possible timelines through to the year 1,000,000 AD. Ray Kurzweil, Michio Kaku, Peter Diamandis and Brian Greene guide the documentary aspect, discussing possible changes the future might hold based on their research: Artificial Intelligence, Man merging with Machine, the human species becoming an interplanetary entity. Exploring life in both the near and the far future, where artificial intelligence is ubiquitous and advances in science have radically extended our lifespans. The series aims to show that communication, work and education will be revolutionized through virtual telepathy. \n\nThe series' narrator, Laurence Fishburne, was nominated for a Primetime Emmy Award, with a further Craft Emmy Nomination for Outstanding Lighting Direction and Scenic Design \n\nEach episode of the series is broken up into first narrated scenes, then interviews with scientists and futurologists; the docudrama segments fit around the interviews and narration to illustrate how technological changes might impact a regular family.\n\nLaurence Fishburne - Narrator<br>Interviews:<br>Ray Kurzweil<br>Michio Kaku<br>Peter Diamandis<br>Brian Greene<br>Drama:<br>Vinette Robinson - Eva<br>Reece Ritchie - Oscar<br>Olive Gray - Jess<br>Joe Corrigall - Damon<br>Siobahn Dillon - Mother<br>Miklós Bányai - Newscaster\n"}
