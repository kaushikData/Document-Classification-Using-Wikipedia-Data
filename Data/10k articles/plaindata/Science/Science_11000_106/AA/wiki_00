{"id": "14874265", "url": "https://en.wikipedia.org/wiki?curid=14874265", "title": "3C 452", "text": "3C 452\n\n3C 452 is a Seyfert galaxy located in the constellation Lacerta.\n\n"}
{"id": "46352440", "url": "https://en.wikipedia.org/wiki?curid=46352440", "title": "Aby Warburg Prize", "text": "Aby Warburg Prize\n\nThe Aby Warburg Prize (German \"Aby Warburg-Preis\"; formerly \"Aby M. Warburg-Preis\") is a science prize of the city of Hamburg. It was established in 1979. Since 1980, it is donated by the senate of the city for excellence in the humanities and social sciences. It is named after the Hamburg-born art historian Aby Warburg. The prize is worth 25,000 Euros and awarded every four years. Young scientists will receive a scholarship of 10,000 euros.\n\n\n\n \n"}
{"id": "46892362", "url": "https://en.wikipedia.org/wiki?curid=46892362", "title": "BDF-3299", "text": "BDF-3299\n\nBDF-3299 is a remote galaxy with a redshift of z = 7.109 corresponds to a distance traveled by light to come down to Earth of 12.9 billion light years.\n\n"}
{"id": "1545079", "url": "https://en.wikipedia.org/wiki?curid=1545079", "title": "Barnett effect", "text": "Barnett effect\n\nThe Barnett effect is the magnetization of an uncharged body when spun on its axis. It was discovered by American physicist Samuel Barnett in 1915.\n\nAn uncharged object rotating with angular velocity ω tends to spontaneously magnetize, with a magnetization given by:\n\nwith γ = gyromagnetic ratio for the material, χ = magnetic susceptibility.\n\nThe magnetization occurs parallel to the axis of spin. Barnett was motivated by a prediction by Owen Richardson in 1908, later named the Einstein–de Haas effect, that magnetizing a ferromagnet can induce a mechanical rotation. He instead looked for the opposite effect, that is, that spinning a ferromagnet could change its magnetization. He established the effect with a long series of experiments between 1908 and 1915.\n\n\n"}
{"id": "38292780", "url": "https://en.wikipedia.org/wiki?curid=38292780", "title": "Benjamin Ayres (instrument maker)", "text": "Benjamin Ayres (instrument maker)\n\nBenjamin Ayres (died c. 1775) was an English instrument maker.\n\nAyres may have been related to the scientist Thomas Ayres who was nominated to the Royal Society in 1707 and who was later involved in a joint stock company to exploit the Newcomen steam engine.\nAyres was an apprentice and brother-in-law of Jonathan Sisson of London.\n\nAyres was active between 1731 and 1775, making mathematical instruments and compasses.\nHe worked in Amsterdam from 1743 onward, and sold octants to the Dutch East India Company.\nHis devices incorporate technical innovations introduced by Sisson, which were copied by others only after 1750.\nSome of them were included in the collection of instruments made by Gerard Arnout Hasselaer, a board member of the company.\nHasselaer had connections with both Ayres and Sisson.\n\nIn 1734 Caleb Smith invented a \"sea quadrant\" using an unsilvered glass mirror to reflect the image of the sun into the telescope. Ayres produced an instrument based on this design mounted on gimbals over a magnetic compass, with a spirit level for use when the horizon was not visible, \nthe whole contained in a solid wooden case.\n\nAround 1750 Ayres invented and made a sailors' arithmetical instrument, now held in the University Museum of Utrecht.\nIt consisted of a brass disk on which a number of circular logarithmic scales were inscribed, with two radial wires that could each be locked to a point on the circumference.\nUsing this instrument, a sailor could perform various trigonometric calculations by setting the wire to the position of the argument on one of the circular scales and reading the result from another of the circular scales.\n\nAyres made fine, large Azimuth compasses, used in determining how much the magnetic compass deviated from true north.\nA brass mariner's compass in gimbals set in mahogany box, made by Ayres in Amsterdam around 1775, is said to have been the property of Sir Isaac Newton.\n\nCitations\n\nSources\n"}
{"id": "24206741", "url": "https://en.wikipedia.org/wiki?curid=24206741", "title": "Codes for electromagnetic scattering by spheres", "text": "Codes for electromagnetic scattering by spheres\n\nCodes for electromagnetic scattering by spheres - this article list codes for electromagnetic scattering by a homogeneous sphere, layered sphere, and cluster of spheres. \n\nMajority of existing codes for calculation of electromagnetic scattering by a single sphere is based on Mie theory which is an analytical solution of Maxwell's equations in terms of infinite series. Other approximations to scattering by a single sphere include: Debye series, ray tracing (geometrical optics), ray tracing including the effects of interference between rays, Airy theory, Rayleigh scattering, diffraction approximation. There are many phenomena related to light scattering by spherical particles such as resonances, surface waves, plasmons, near-field scattering. Even though Mie theory offers convenient and fast way of solving light scattering problem by homogeneous spherical particles, there are other techniques, such as discrete dipole approximation, FDTD, T-matrix, which can also be used for such tasks.\nThe compilation contains information about the electromagnetic scattering by spherical particles, relevant links, and applications.\n\nAlgorithmic literature includes several contributions\n\n\n\n"}
{"id": "3984015", "url": "https://en.wikipedia.org/wiki?curid=3984015", "title": "Convergence zone", "text": "Convergence zone\n\nA convergence zone in meteorology is a region in the atmosphere where two prevailing flows meet and interact, usually resulting in distinctive weather conditions.\nThis causes a mass accumulation that eventually leads to a vertical movement and to the formation of clouds and precipitation. Large-scale convergence, called synoptic-scale convergence, is associated with weather systems such as baroclinic troughs, low-pressure areas, and cyclones. Small-scale convergence will give phenomena from isolated cumulus clouds to large areas of thunderstorms.\n\nThe inverse of a convergence is a divergence.\n\nAn example of a convergence zone is the Intertropical Convergence Zone (ITCZ), a low pressure area which girdles the Earth at the Equator. Another example is the South Pacific convergence zone that extends from the western Pacific Ocean toward French Polynesia.\n\nConvergence zones also occur at a smaller scale. Some examples are the Puget Sound Convergence Zone which occurs in the Puget Sound region in the U.S. state of Washington; Mohawk–Hudson convergence in the U.S. state of New York; the Elsinore Convergence Zone in the U.S. state of California; the Brown Willy effect which can be generated when south-westerly winds blow over Bodmin Moor in Cornwall; and the Pembrokeshire Dangler which can form when northerly winds blow down the Irish Sea. They can also be associated with sea breeze fronts.\n"}
{"id": "1921539", "url": "https://en.wikipedia.org/wiki?curid=1921539", "title": "Current quark", "text": "Current quark\n\nCurrent quarks (also called naked quarks or bare quarks) are defined as the constituent quark cores (constituent quarks with no covering) of a valence quark.\n\nIf, in one constituent quark, the current quark is hit inside the covering with large force, it accelerates through the covering and leaves it behind. In addition, current quarks possess one asymptotic freedom within the perturbation theory described limits. In quantum chromodynamics, the mass of the current quarks carries the designation \"current quark mass\".\n\nThe local term plays no more role for the description of the hadrons with the light current quarks.\nIn the formula_1-Scheme at formula_2 the quark masses are: \nA description is only possible with the help of relativistic quantum mechanics.\n\nThe current quark mass is also called the mass of the 'naked' quarks.\nThe mass of the current quark is reduced by the term of the constituent quark covering mass.\n\nThe current quark mass is a logical consequence of the mathematical formalism of the quantum field theory (QFT), \nthus it is from a not descriptive origin.\nThe current quark masses of the light current quarks are much smaller than the constituent quark masses. \nReason for this is the missing of the mass of the constituent quark covering.\nThe current quark mass is a parameter to compute sufficiently small color charges.\n\n\"Definition\":\nThe current quark mass means the mass of the constituent quark mass reduced by the mass of the respective constituent quark covering.\n\nThere is almost no difference between current quark mass and constituent quark mass for the heavy quarks (c,b,t).This is not so for the light quarks (u, d, s).\n\nThe comparison of the results of the computations \nwith the experimental data supplies the values for the current quark masses.\n"}
{"id": "43022448", "url": "https://en.wikipedia.org/wiki?curid=43022448", "title": "DARPA Spectrum Challenge", "text": "DARPA Spectrum Challenge\n\nThe DARPA Spectrum Challenge was a competition held by the Defense Advanced Research Projects Agency to demonstrate a radio protocol that can best use a given communication channel in the presence of other dynamic users and interfering signals. \n\nThe Challenge was not focused on developing new radio hardware, but instead was targeted at finding strategies for guaranteeing successful communication in the presence of other radios that may have conflicting co-existence objectives. The Challenge entailed head-to-head competitions between each team's radio protocol and an opponent's in a structured wireless testbed environment, known as ORBIT, that is maintained by the Wireless Information Network Laboratory (WINLAB) at Rutgers University.\n\nThe Challenge awarded first place teams in the September 2013 preliminary event, and first and second place teams in the March 2014 final event with cash prizes totaling $200,000. Each event consisted of a Competitive and Cooperative Tournament.\n\nOut of the 90 teams that registered for the Spectrum Challenge, the top 18 teams were selected to compete in the Preliminary and Final Event:\n"}
{"id": "11555214", "url": "https://en.wikipedia.org/wiki?curid=11555214", "title": "DC Web Women", "text": "DC Web Women\n\nDC Web Women (DCWW) is a nonprofit professional organization for women in technology and new media based in the Washington, DC area. Established in 1999, the organization's vision is \"to educate, inspire and encourage girls and women in the field of technology.\" The organization hosts an email discussion list, and sponsors monthly workshops and networking events.\n\nDC Web Women began in 1999 as a chapter of Webgrrls. The group was started in a coffee shop by Debbie Weil and Catherine Buzzel. This initial meeting between the two women led to a second meeting in a women owned internet café in the Washington DC area with three additional like-minded women: Cathy Ganssle, Shellie Holubek, and Miriam Jaffe. This meeting led to the foundation of the organization DC Web Women.\n\nOn February 15, 1999, the organization registered as a non-profit and was renamed to \"DC Web Women.\"\n\n\n"}
{"id": "21720273", "url": "https://en.wikipedia.org/wiki?curid=21720273", "title": "Desmond H. Collins", "text": "Desmond H. Collins\n\nDesmond H. Collins is a Canadian paleontologist, associate professor of zoology at the University of Toronto and retired curator of invertebrate paleontology at the Royal Ontario Museum.\n\n"}
{"id": "4899809", "url": "https://en.wikipedia.org/wiki?curid=4899809", "title": "Deviant Behavior (journal)", "text": "Deviant Behavior (journal)\n\nDeviant Behavior is a peer-reviewed academic journal which focuses on social deviance, including criminal, sexual, and narcotic behaviors. It is published by Routledge and was established in 1979. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.052, ranking it 48 out of 62 journals in the category \"Psychology, Social\" and 67 out of 143 journals in the category \"Sociology\".\n"}
{"id": "762048", "url": "https://en.wikipedia.org/wiki?curid=762048", "title": "Diminishing returns", "text": "Diminishing returns\n\nIn economics, diminishing returns is the decrease in the marginal (incremental) output of a production process as the amount of a single factor of production is incrementally increased, while the amounts of all other factors of production stay constant.\n\nThe law of diminishing returns states that in all productive processes, adding more of one factor of production, while holding all others constant (\"ceteris paribus\"), will at some point yield lower incremental per-unit returns. The law of diminishing returns does not imply that adding more of a factor will decrease the \"total\" production, a condition known as negative returns, though in fact this is common.\n\nA common example is adding more people to a job, such as the assembly of a car on a factory floor. At some point, adding more workers causes problems such as workers getting in each other's way or frequently finding themselves waiting for access to a part. In all of these processes, producing one more unit of output per unit of time will eventually require increasingly more usage of the input, due to the input being used less effectively. Another well-studied example is throwing more headcount at software development, yielding Brooks's law. \n\nThe law of diminishing returns is a fundamental principle of economics. It plays a central role in production theory.\n\nThe concept of diminishing returns can be traced back to the concerns of early economists such as Johann Heinrich von Thünen, Jacques Turgot, Adam Smith, James Steuart, Thomas Robert Malthus, and David Ricardo. However, classical economists such as Malthus and Ricardo attributed the successive diminishment of output to the decreasing quality of the inputs. Neoclassical economists assume that each \"unit\" of labor is identical. Diminishing returns are due to the disruption of the entire productive process as additional units of labor are added to a fixed amount of capital. The law of diminishing returns remains an important consideration in farming.\n\nAn example is a factory that has a fixed stock of capital, or tools and machines, and a variable supply of labor. As the firm increases the number of workers, the total output of the firm grows but at an ever-decreasing rate. This is because after a certain point, the factory becomes overcrowded and workers begin to form lines to use the machines. The long-run solution to this problem is to increase the stock of capital, that is, to buy more machines and to build more factories.\n\nThere is an inverse relationship between returns of inputs and the cost of production. Suppose that a kilogram of seed costs one dollar, and this price does not change. Although there are other costs, assume they do not vary with the amount of output and are therefore fixed costs. One kilogram of seeds yields one ton of crop, so the first ton of the crop costs one dollar to produce. That is, for the first ton of output, the marginal cost of the output is $1 per ton. If there are no other changes, then if the second kilogram of seeds applied to land produces only half the output of the first, the marginal cost would equal $1 per half ton of output, or $2 per ton. Similarly, if the third kilogram of seeds yields only a quarter ton, then the marginal cost equals $1 per quarter ton or $4 per ton. Thus, diminishing marginal returns imply increasing marginal costs and rising average costs.\n\nCost can also be measured in terms of opportunity cost. In this case the law also applies to societies – the opportunity cost of producing a single unit of a good generally increases as a society attempts to produce more of that good. This explains the bowed-out shape of the production possibilities frontier.\n\n"}
{"id": "40069309", "url": "https://en.wikipedia.org/wiki?curid=40069309", "title": "Discovery Education 3M Young Scientist Challenge", "text": "Discovery Education 3M Young Scientist Challenge\n\nThe Young Scientist Challenge is a youth science and engineering competition administered by Discovery Education and 3M for middle school students in the United States, similar to the European Union Contest for Young Scientists. Students apply by creating a 1-2 minute video detailing their idea for a new invention intended to solve an everyday problem. Ten finalists are chosen annually to work alongside a 3M scientist during a summer mentorship and receive a trip to the 3M Innovation Center in St. Paul, Minnesota, to compete for $25,000 and the title of America’s Top Young Scientist.\n\nThe entry period is from December until April each year. A panel of judges from Discovery Education and its partner organizations, educators, and science professionals score qualifying entry videos and choose 10 finalists and up to 51 merit winners, one from each state and the District of Columbia, based on the following criteria:\n\nStudents are required to address an everyday problem and articulate how the problem directly impacts them, their families, their communities, and/or the global population. The idea must be a new innovation or solution, and cannot be a behavioral change or a new use for an existing product. Judges also look for the level of understanding of scientific concepts and confidence in communicating science in general exhibited in the videos.\n\nThe ten finalists undergo a summer menotoship and in the fall travel to 3M's headquarters in Minnesota to participate in the Young Scientist Challenge Final Event. They visit 3M labs, meet 3M scientists, and tour the 3M Innovation Center, and also participate in a series of other scored challenges to demonstrate their scientific knowledge and communication abilities. For their final challenge, they then present the innovation that they developed during their mentorship. Here the finalists are judged by a panel of judges selected by Discovery Education and its partner organizations according to the following guidelines:\n\n\nAfter the Final Event, participants attend an award ceremony and dinner, at which the winner of the title \"America’s Top Young Scientist\" is announced.\n\nFirst Place (America's Top Young Scientist)\n\nThree Runner-Up Prize winners\n\nSix Second Prize winners\n\nUp to 51 Merit Winners (one from each state and the District of Columbia)\n\nFormerly known as the Discovery Channel Young Scientist Challenge (DCYSC), the Young Scientist challenge was created in 1999 as an engineering research and exhibit competition for students in grades 5 through 8. It was sponsored primarily by Discovery Communications, Society for Science and the Public, and Elmer's Glue. Competitors were originally qualified for DCYSC by entering an International Science and Engineering-affiliated science fair and being nominated by a teacher or professional.\n\nStudents completed an application that included several essays, which were then evaluated for communication abilities by DCYSC judges, who selected 400 semi-finalists and 40 finalists who received an all-expense-paid trip to Washington, D.C. to compete in the final competition. The finals consisted of two parts. The first was a research presentation, accounting for 20% of the total score, held at the Smithsonian's National Museum of Natural History, the National Academy of Sciences, or another academic national association that varied from year to year. The second was a series of six science-related challenges at the National Institutes of Health or the University of Maryland. Each challenge concluded with some type of presentation (e.g., a radio show, a TV show, or a news conference) worth 10% of the students' total score. Students also presented a simple science experiment, known as a Whelmer, in front of cameras for 15% of their score. The remaining 5% came from teamwork, as the finalists were split into eight teams consisting of five members each for the science challenges.\n\nIn 2008 the contest became the Discovery Education 3M Young Scientist Challenge. Students no longer have to be nominated and now submit a 1-2 minute video clip as their form of entry.\n\nSince 2003, themes for the Young Scientist Challenge have followed scientific curiosities and been built on the activities and innovations around them.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50863546", "url": "https://en.wikipedia.org/wiki?curid=50863546", "title": "Elivagar Flumina", "text": "Elivagar Flumina\n\nElivagar Flumina is a network of river channels ranging from 23 km to 210 km in length in the region around the Menrva Crater of Titan. The channel system is at least 120 km wide and shows signs of erosion. At its mouth, an alluvial fan is present. The Elivagar Flumina is interpreted as alluvial due to its closeness to fluvial valleys and as understood from the radar backscatter. Geomorphologic mapping of the Menrva region of Titan has yielded evidence for exogenic processes such as hydrocarbon fluid channelization (in other words flash floods) that are thought to have formed the Flumina network.\n\nThe Elivagar Flumina is named after the Élivágar, a group of poisonous ice rivers in Norse mythology.\n"}
{"id": "18346799", "url": "https://en.wikipedia.org/wiki?curid=18346799", "title": "Folded optics", "text": "Folded optics\n\nFolded optics is an optical system in which the beam is bent in a way to make the optical path much longer than the size of the system. Prismatic binoculars are a well-known example.\n\nAn early conventional film (35 mm) camera was designed by Tessina that used the concept of folded optics.\n\n"}
{"id": "23724785", "url": "https://en.wikipedia.org/wiki?curid=23724785", "title": "Fredkin finite nature hypothesis", "text": "Fredkin finite nature hypothesis\n\nIn digital physics, the Fredkin finite nature hypothesis states that ultimately all quantities of physics, including space and time, are discrete and . All measurable physical quantities arise from some Planck scale substrate for information processing. Also, the amount of information in any small volume of spacetime will be finite and equal to a small number of possibilities.\n\nStephen Wolfram in \"A New Kind of Science\", Chapter 9, considered the possibility that energy and spacetime might be secondary derivations from an informational substrate underlying the Planck scale. Fredkin's \"Finite Nature\" and Wolfram's ideas on the foundations of physics might be relevant to unsolved problems in physics.\n\nAccording to Fredkin, \"the computational substrate of quantum mechanics must have access to some sort of metric to create inertial motion. Whether or not higher level processes in physics can access this process is another story.\" According to Witten, \"string theory leads in a remarkably simple way to a reasonable rough draft of particle physics unified with gravity\"; if Fredkin is correct about inertia, then there is the highly controversial hypothesis that the foundations of physics might depend upon either string theory with the infinite nature hypothesis or some modified version of string theory with Fredkin's finite nature hypothesis, in which inertial mass-energy obeys Milgrom's modified Newtonian dynamics and not the equivalence principle.\n\n\n"}
{"id": "82424", "url": "https://en.wikipedia.org/wiki?curid=82424", "title": "Gnomon", "text": "Gnomon\n\nA gnomon ([ˈnoʊmɒn], from Greek , \"gnōmōn\", literally: \"one that knows or examines\") is the part of a sundial that casts a shadow. The term is used for a variety of purposes in mathematics and other fields.\n\nA painted stick dating from 2300 BC was excavated at the astronomical site of Taosi is the oldest gnomon known in China. The gnomon was widely used in ancient China from the second century BC onward in order determine the changes in seasons, orientation, and geographical latitude. The ancient Chinese used shadow measurements for creating calendars that are mentioned in several ancient texts. According to the collection of Zhou Chinese poetic anthologies \"Classic of Poetry\", one of the distant ancestors of King Wen of the Zhou dynasty used to measure gnomon shadow lengths to determine the orientation around the 14th century BC.\nThe ancient Greek philosopher Anaximander (610–546 BC) is credited with introducing this Babylonian instrument to the Ancient Greeks. The ancient Greek mathematician and astronomer Oenopides used the phrase \"drawn gnomon-wise\" to describe a line drawn perpendicular to another. Later, the term was used for an L-shaped instrument like a steel square used to draw right angles. This shape may explain its use to describe a shape formed by cutting a smaller square from a larger one. Euclid extended the term to the plane figure formed by removing a similar parallelogram from a corner of a larger parallelogram. Indeed, the gnomon is the increment between two successive figurate numbers, including square and triangular numbers. The ancient Greek mathematician and engineer Hero of Alexandria defined a gnomon as that which, when added to an entity (number or shape), makes a new entity similar to the starting entity. In this sense Theon of Smyrna used it to describe a number which added to a polygonal number produces the next one of the same type. The most common use in this sense is an odd integer especially when seen as a figurate number between square numbers.\n\nPerforated gnomons projecting a pinhole image of the sun were described in the Chinese Zhoubi Suanjing writings (1046 BCE—256 BC with material added until circa 220 AD). The location of the bright circle can be measured to tell the time of day and year. In Arab and European cultures its invention was much later attributed to Egyptian astronomer and mathematician Ibn Yunus around 1000 AD. \nItalian astronomer, mathematician and cosmographer Paolo Toscanelli is associated with the 1475 placement of a bronze plate with a round hole in the dome of the Cathedral of Santa Maria del Fiore in Florence to project an image of the sun on the cathedral's floor. With markings on the floor it tells the exact time of each midday (reportedly to within half a second) as well as the date of the summer solstice. Italian mathematician, engineer, astronomer and geographer Leonardo Ximenes reconstructed the gnomon according to his new measurements in 1756.\n\nIn the Northern Hemisphere, the shadow-casting edge of a sundial gnomon is normally oriented so that it points due northward and is parallel to the rotational axis of Earth. That is, it is inclined to the northern horizon at an angle that equals the latitude of the sundial's location. At present, such a gnomon should thus point almost precisely at Polaris, as this is within 1° of the north celestial pole.\n\nOn some sundials, the gnomon is vertical. These were usually used in former times for observing the altitude of the Sun, especially when on the meridian. The style is the part of the gnomon that casts the shadow. This can change as the sun moves. For example, the upper west edge of the gnomon might be the style in the morning and the upper east edge might be the style in the afternoon. A three-dimensional gnomon is commonly used in CAD and computer graphics as an aid to positioning objects in the virtual world. By convention, the \"x\"-axis direction is colored red, the \"y\"-axis green and the \"z\"-axis blue. NASA astronauts used a gnomon as a photographic tool to indicate local vertical and to display a color chart when they were working on the Moon's surface.\n\n\n\n\n<br>\n"}
{"id": "950130", "url": "https://en.wikipedia.org/wiki?curid=950130", "title": "House numbering", "text": "House numbering\n\nHouse numbering is the system of giving a unique number to each building in a street or area, with the intention of making it easier to locate a particular building. The house number is often part of a postal address. The term describes the number of any building (residential or not) with a mailbox, or even a vacant lot.\n\nHouse numbering schemes vary by location, and in many cases even within cities. In some areas of the world, including many remote areas, houses are named but are not assigned numbers.\n\nA house numbering scheme was present in Pont Notre-Dame in Paris in 1512. However, the purpose of the numbering was generally to determine the distribution of property ownership in the city, rather than for the purpose of organization.\n\nIn the 18th century the first street numbering schemes were applied across Europe, to aid in administrative tasks and the provision of services such as mail delivery. The \"New View of London\" reported in 1708 that \"at Prescott Street, Goodman's Fields, instead of signs, the houses are distinguished by numbers\". Parts of the Paris suburbs were numbered in the 1720s; the houses in the Jewish quarter in the city of Prague in the Austrian Empire were numbered in the same decade to aid the authorities in the conscription of the Jews.\n\nStreet numbering took off in the mid 18th century, especially in Prussia, where authorities were ordered to \"fix numbers on the houses ... in little villages on the day before the troops march in\". In the 1750s and 60s, street numbering on a large scale was applied in Madrid, London, Paris, and Vienna, as well as many other cities across Europe. On 1 March 1768, King Louis XV of France decreed that all French houses outside of Paris affix house numbers, primarily for tracking troops quartered in civilian homes.\n\nIn Australia and New Zealand, the current standard (Australia/New Zealand joint standard AS/NZS 4819:2011 - Rural & Urban Addressing) is directed at local governments that have the primary responsibility for addressing and road naming. The standard calls for lots and buildings on newly created streets to be assigned odd numbers (on the left) and even numbers (on the right) when facing in the direction of increasing numbers (the European system) reflecting already common practice. It first came into force in 2003 under AS/NZS 4819:2003 - Geographic Information – Rural & Urban Addressing. Exceptions are where the road forms part of the boundary between different council areas or cities. For example, Underwood Road in Rochedale South, divided between Logan City and the City of Brisbane.\n\nIn New South Wales, the vast majority of streets were numbered before 2003, some with odd numbers assigned to houses on the right of the street when facing the direction along which numbers increase. There is no plan to reassign these numbers.\n\nOn some long urban roads (e.g. Parramatta Road in Sydney) numbers ascend until the road crosses a council or suburb boundary, then start again at 1 or 2, where a street sign gives the name of the relevant area — these streets have repeating numbers. In semi-rural and rural areas, where houses and farms are widely spaced, a numbering system based on tens of metres or (less commonly) metres has been devised. Thus a farm from the start of the road, on the right-hand side would be numbered 230.\n\nBallarat Central, Victoria uses the US system of increasing house numbers by 100 after a major cross street. Streets are designated North or South depending upon their relative position to Sturt Street.\n\nIn Japan and South Korea, a city is divided into small numbered zones. The houses within each zone are then labelled in the order in which they were constructed, or clockwise around the block. This system is comparable to the system of sestieri (\"sixths\") used in Venice. Visitors to a large, complex city like Tokyo often must resort to asking for directions at a local police substation.\n\nIn Hong Kong, a former British colony, the British and European norm to number houses on one side of the street with odd numbers, and the other side with even numbers, is generally followed. Some roads or streets along the coastline may however have numbering only on one side, even if the opposite side is later reclaimed. These roads or streets include Ferry Street, Connaught Road West, and Gloucester Road.\n\nMost mainland Chinese cities use the European system, with odd numbers on one side of the road and even numbers on the opposite side. In high-density old Shanghai, a street number may be either a \"hao\" (\"号\" hào) or \"nong\" (\"弄\" nòng/lòng), both of them being numbered successively. A \"hao\" refers a door rather than a building, for example, if a building with the address 25 Wuming Rd is followed by another building, which has three entrances opening to the street, the latter will be numbered as three different \"hao\", from 27 to 29 Wuming Rd.\n\nA \"nong\", sometimes translated as \"lane\", refers to a block of buildings. So if in the above example the last building is followed by an enclosed compound, it will have the address \"lane 31, Wuming Rd\". A \"nong\" is further subdivided in its own \"hao\", which do not correlate with the \"hao\" of the street, so the full address of an apartment within a compound may look like \"Apartment 5005, no. 7, lane 31, Wuming Rd\".\n\nThe most common street address formats in Vietnam are:\n\n\nAnother scheme is based on residential areas called . A is addressed by house number, road, and , for example \". Some localities still use an older address format based on neighborhood (): for example, in \"\", 7A is the neighborhood number. This confusing format is being gradually phased out in favor of the more modern formats above.\n\nGenerally in Iran and especially in the capital Tehran odd numbers are all on one side and the even numbers opposite along streets. Infrequently, this style confuses people because this is not how it works everywhere in the city and sometimes the numbers get intertwined with each other. In the rural parts, some houses have no number at all and some have their owner's details as the number instead. In some cases, using the number 13 is skipped replacing it with equivalents such as: 12+1 or 14-1\n\nIn Europe the most common house numbering scheme, in this article referred to as the \"European\" scheme, is to number each plot on one side of the road with ascending odd numbers, from 1, and those on the other with ascending even numbers, from 2 (or sometimes 0). The odd numbers are usually on the left side of the road, looking in the direction in which the numbers increase.\n\nWhere additional buildings are inserted or subdivided, these are often suffixed a, b, c, etc. (in Spain and France, \"bis\", \"ter\", \"quater\"). Where buildings are later combined, they may use just one of the original numbers, combine them (\"13/15\"), or give their address as a range (e.g. \"13–17\"; not to be construed as including the even numbers 14 and 16). Buildings with multiple entrances may have a single number for the entire building or a separate number for each entrance.\n\nWhere plots are not built upon gaps may be left in the numbering scheme or marked on maps for the plots. If buildings are added to a stretch of old street the following may be used rather than a long series of suffixes to the existing numbers: a new name for a new estate/block along the street (e.g. \"1-100 Waterloo Place/Platz, Sud St.\".); a new road name inserted along the course of a street either with or without mention of the parent street; unused numbers \"above\" the highest house number may be used (although rarely as this introduces confusing discontinuity), or the upper remainder of the street is renumbered.\n\nOther local numbering schemes are also in use for administrative or historic reasons, including clockwise and anti-clockwise numbering, district-based numbering, distance-based numbering, and double numbering.\n\nThe Finnish numbering system incorporates solutions to the problems which arose with mass urbanization and increase in building density. Addresses always are formatted as street name followed by street address number. With new, infill building, new addresses are created by adding letters representing the new ground level access point within the old street address, and if there are more apartments than ground level access points, a number added for the apartment number within the new development. The original street numbering system followed the pattern of odd numbers on one side and even numbers on the other side of the street, with lower numbers towards the center of town and higher numbers further away from the center.\n\nThe infill numbering system avoids renumbering the entire street when developments are modified. For example, Mannerheimintie 5 (a large mansion house on a large city plot) was demolished and replaced with 4 new buildings each with 2 stairwells all accessible from Mannerheimintie. The 8 new access stairwells are labelled A B C D E F G and H (each with the letter visible above the stairwell). Each stairwell has 4 apartments on 5 floors, so the new development has 160 new addresses in all running from Mannerheimintie 5 A 1 through to Mannerheimintie 5 H 160. The opposite example is where old, narrow buildings have been combined; Iso Roobertinkatu 36, 38 and 40 were demolished in the 1920s and the new building has the address Iso Roobertinkatu 36-40.\n\nIn the rural parts of Finland, a variant of this method is used. As in towns, odd and even numbers are on opposite sides of the road, but many numbers are skipped. Instead, the house number indicates the distance in tens of metres from the start of the road. For example, \"Pengertie 159\" would be 1590 metres from the place where Pengertie starts.\n\nWhen more buildings are constructed than numbers were originally allotted, discontinuity of numbering is avoided by giving multiple adjacent buildings the same number, with a letter suffix starting at \"A\". In Haarlem, Netherlands, red numbers are used for upstairs apartments.\n\nIn Portugal, the European scheme is most commonly used. However, in Porto and several other cities in the Portuguese Northern region, as well as in the Cascais Municipality (near Lisbon), houses are numbered in the North American style, with the number assigned being proportional to the distance in meters from the baseline of the street.\n\nLisbon numbering is European and furthermore 'from the river, odd numbers left'. Because the Tagus borders Lisbon on the south and the east, this means that north-south streets are numbered low from the south, and east-west streets are numbered low from the east.\n\nIn many new planned neighborhoods of Portugal houses and other buildings are identified by a \"lote\" (plot) number without reference to their street. This is in law the \"número de polícia\", which literally means \"police's number\" — the police formerly assigned the numbers rather than the town hall. The \"lote\" is the construction plot number used in the urban plan, a consecutive number series applies to a broad neighborhood. In theory and in most cases, the use of a \"lote\" number system is provisional, being replaced by a traditional street number system some time after the neighborhood is built and inhabited. In some neighborhoods, \"lote\" numbers are kept for many years, some never being replaced by street numbers.\n\nThe relatively new planned neighborhood of Parque das Nações in Lisbon has also a different numbering scheme: each building is referred by its plot, parcel, and building (in Portuguese: \"lote\", \"parcela\", \"prédio\").\n\nThe European system is most widely used. The odd numbers will typically be on the left-hand side as seen from the centre of the town or village, with the lowest numbers at the end of the street closest to the town centre. Intermediate properties usually have a number suffixed A, B, C, etc., much more rarely instead being given a half number, e.g. the old police station at Camberwell Church Street. It is extremely rare for a property (built next to no. 2 after the street had been numbered) to be zero (0) or named Minusone; researchers have found these instances once in Middlesbrough and once in Newbury. In many rural streets, significantly built alongside before 1900, houses remain named (unnumbered).\n\nIn some places, particularly when open land, a river or a large church fronts one side, all plots on one side of a street are numbered consecutively. Such a street if modern and long is more likely to be numbered using odd numbers, starting at 1. Along oldest streets, numbering is usually clockwise and consecutive: for example in Pall Mall, some new towns, and in many villages in Wales. This usually also applies to all culs-de-sacs. For instance, 10 Downing Street, the official home of the Prime Minister, is next door to 11 Downing Street. Houses which surround squares are usually numbered consecutively clockwise.\n\nIn the early/mid 19th century numbering of long urban streets commonly changed (from clockwise, strict consecutive to odds (consecutive) which face evens (consecutive)). Where this took place it presents a street-long pitfall to researchers using historic street directories and other records. A very rare variation may be seen where a high street (main street) continues from a less commercial part — a road which breaks the UK conventions by not starting at 1 or 2. On one side of the main road between Stratford and Leytonstone houses up to no. 122 are \"Leytonstone Road\". The next house is \"124 High Road, Leytonstone\".\n\nDevelopers may avoid the number 13 for house numbering as in Iran, because that number is considered by some to be unlucky.\n\nBlocks of flats (apartments) are treated in two ways:\n\nIn the UK street numbering and street signposts vary across local authorities. Numbering plates (or similar) are overwhelmingly at the discretion of house owners.\n\nIn the UK fanlights in front doors were introduced in the 1720s in which the house number may be engraved. Contemporary architecture and modern house building techniques see alternatively acrylic, aluminium, or glass, ceramic, brass, slate, or stone used.\n\nItaly mostly follows the European scheme described above but there are some exceptions, generally for historical reasons.\n\nIn Venice, houses are numbered within six named series (one per \"sestiere\" district). Similarly, small villages in rural areas may also occasionally use a single progressive series for all house numbers.\n\nIn Genoa, Savona and Florence houses are marked with black (sometimes blue in Florence) numbers; businesses are usually (but not always) given red numbers, giving up to two distinct, numerically overlapping series per street. Those of businesses are denoted in all other writing (documents, online directories, etc.) by the addition of the letter \"r\" (e.g. \"Via dei Servi 21r\").\n\nIn most of Turkey, currently the European house numbering scheme is applied. The Istanbul Metropolitan Municipality introduced new house numbering and street signs in 2007 by two official designers.\n\nIn Central and Eastern Europe, with some exceptions, houses are typically numbered in the European style. Many streets, however, use the \"boustrophedon\" system.\n\nA double numbering system has been used, similar to the system in the Czech Republic and Slovakia, which also were formerly parts of the Austro-Hungarian Empire.\n\nIn some Czech and Slovak cities and settlements, two numbering systems are used concurrently.\n\nThe basic house number is the \"old\" or \"conscription number\" (Czech: \"popisné číslo\", Slovak: \"súpisné číslo\"). The conscription number is unique within the municipal part (a village, a quarter, mostly for one cadastral area) or within a whole small municipality.\n\nFor makeshift and recreational buildings in the Czech Republic, \"registration number\" (\"evidenční číslo\") from a separate number series is used instead of the descriptive number. Typically, this number begins with zero or with the letter \"E\", or has different colour, or contains words like \"nouzová stavba\" (\"makeshift structure\") or \"chata\" (\"weekend house\"), etc.\n\nIn some settlements where streets have names (mostly in cities), \"new\" or \"orientational numbers\" (Czech: \"orientační číslo\", Slovak: \"orientačné číslo\") are also used concurrently. The orientational numbers are arranged sequentially within the street or square. If the building is on a corner or has two sides, it can have two or more orientation numbers, one for each of the adjacent streets or squares. Solitary houses distant from named streets often have no orientation number. In some places, the name of a small quarter is used instead of a street name. If there is a new building between two older numbered ones, the orientation number is distinguished with an additional lower case letter (for example, the sequence could be 5, 7, 9, 9a, 9b, 9c, 11, 13). In the 1930–1950s in Brno, lower case letters were used for separate entrances of modern block houses perpendicular to the street.\n\nEither number may be used in addresses. Sometimes, businesses will use both numbers to avoid confusion, usually putting the descriptive (or registration) number first: \"Hlavní 20/7\". The two (or three) types of numbers are commonly distinguished by colour of the sign. Each municipality can have its own traditional or official rules and colours. In Prague and many other Bohemian cities, descriptive numbers are red, orientation numbers are blue and \"evidential\" numbers are green or yellow or red. In many Bohemian municipalities, descriptive numbers are blue, black, or are not unified. In Brno and some Moravian and Slovak cities, descriptive numbers are white-black signs, orientation numbers are red-white signs. Many cities and municipalities have different rules.\n\nFormerly, Roman numerals signifying the city district were added to the conscription number: e.g. \"125/III\" means conscription number 125 in district number III (in Prague, this was Malá Strana). Roman numerals were used both in cities and in village municipalities with more settlements. Nowadays, the name of the settlement is preferred instead of Roman numerals.\n\nThe first conscription numbering was ordered by Maria Theresa in 1770 and implemented in 1770–1771. The series was given successively as the soldiers went through the settlement describing houses with numbers. Thereafter, every new house was allocated the next number sequentially, irrespective of its location. Most villages still use their original number series from 1770–1771. In cities, houses have been renumbered once or more often in order to be sequentialthe first wave of renumbering came in 1805–1815. In 1857, the Austrian Emperor allowed a new system of numbering by streets. This new system was introduced in the biggest cities (Prague, Brno) in the 1860s. In 1884, land registration books were introduced and they used the old (conscription) numbers as a permanent and stable identifier of buildings. The new (orientation) numbers continue to be used concurrently.\n\nIn Germany, the European scheme (ascending odd/even numbers, see above) is usually used. In most cases, the numbers increase in the direction away from the town/city centre. Some places use a clockwise scheme for historical reasons, called \"Hufeisennummerierung\" (\"horseshoe numbering\") due to the progression of the numbers. This includes Berlin, parts of Hamburg and some other towns in northern Germany.\n\nIn some older streets in northern and eastern Germany, mainly in the former Kingdom of Prussia and adjoining areas, including parts of Berlin and Hamburg, the \"horseshoe\" numbering system (counter-clockwise \"boustrophedon\"-style numbering) was used for the numbering of new streets up until the 1920s, after which the European system was introduced for new streets.\n\nUnder the horseshoe numbering scheme, starting from one end, the buildings on the right side of the street were numbered sequentially from the near end to the far end of the street. The next number was then assigned to the last building on the left, opposite side of the street, the following numbers sequentially doubling back along the left side of the street. The building with the highest number would be the first on the left side, facing building number 1 across the street. The horseshoe numbering system remains in use in older streets in many German cities, notably Berlin, although newer adjoining streets may use modern European numbering. Kurfürstendamm in Berlin is a well-known example of a street where the horseshoe numbering scheme is still in use, although the numbering today starts with 11 at Breitscheidplatz, with number 237 across the street being the highest number.\n\nVery small villages sometimes number all buildings in the village sequentially according to their date of construction, and independent of the street they are on. However, this scheme is being phased out because it makes it hard to find a building by its address.\nIn Russia and many other former USSR countries, the European style is generally used, with numbers starting from the end of the street closest to the town center. Buildings or plots at street intersections may be assigned a composite number, which includes the number along the intersecting street separated by a slash (), like in Нахимова, 14/41 (14 is the number along Nakhimova street and 41 is the number along intersecting street).\n\nThe odd numbers are usually on the left side of the road, looking in the direction in which the numbers increase; though in some cities (including Saint Petersburg) the odd numbers are on the right side. Some cities (for example, Nizhniy Novgorod) have mixed numbering: odd numbers on the right in some parts of the city and on the left in others.\n\nSoviet era housing districts (microdistricts) often have a complicated network of access lanes thought too small to merit their own names. Buildings in these lanes are ascribed to larger streets which may be quite far from their location; a building placed along a street may sometimes be ascribed to another street, which sometimes makes finding a building by its address a challenging task.\n\nIn some cities, especially hosting large scientific or military research centers in Soviet time, the numbering might be different: houses may have numbers related to the block rather than the street, thus 12-й квартал, дом 3 (Block 12, House 3), similar to the Japanese and Korean systems (see below). Aktau is one example of this.\n\nWhen a numbered plot contains multiple buildings, they are assigned an additional component of the street address, called корпус (building), which is usually a sequentially assigned number unique within the plot (but sometimes contains letters as in 15а, 15б, 15в and so on). So, a Russian street address may look like Московское шоссе, дом 23, корпус 2 (Moscow Street, plot 23, building 2), or Льва Толстого, дом 14б (Leo Tolstoy Street, plot 14, building b).\n\nOn very long roads in suburban areas, a kilometer numbering system also may be used (like Australian rural numbering system). For example, 9-й км Воткинского шоссе (9th kilometer of Votkinsk Highway), and Шабердинский тракт, 7-й км (7th kilometer of Shaberdy Road).\n\nIn Latin America, some countries, like Mexico and Uruguay, use systems similar to those in Europe. Houses are numbered in ascending order from downtown to the border of the city. In Mexico, the cities are usually divided in \"Colonias\", which are small or medium areas. The \"colonia\" is commonly included in the address before the postal code. Sometimes when houses merge in a street or new constructions are built after the numbering was made, the address can become ambiguous. When a number is re-used, a letter is added for the newer address. For example, if there are two 35s, one remains as \"35\", and the second one becomes \"35A\" or \"35Bis\".\n\nIt is sometimes common in remote towns or non-planned areas inside the cities, that the streets do not have any name and the houses do not have numbers. In these cases, the address of the houses are usually the name of a person or family, the name of the area or town, or \"Dirección Conocida\" (\"known address\"), which means that the house of the family is known by almost all the community. This kind of addressing is only used in remote towns or small communities near highways. For people living near highways or roads, the usual address is the kilometer distance of the road in which the house is established; if there is more than one address, some references might be written or the \"Dirección Conocida\" may be added.\n\nIn Uruguay, most house numbering starts at a high number among the hundreds or the thousands. The system is similar to the French-Spanish one: when a house is divided the term 'bis' is added with the difference that no single term designates the third: when a house is divided or added between another the term 'bis' is repeated as many divisions have been made or houses added in between, for example '3217 bis bis' corresponds to the third house from the 3217th, and so on, when many houses are merged the lowest number is used, leaving the in-between numbers missing. Also there are cases when no number is assigned, this occurs mostly in peripheral areas inside the cities, low house numbering occurs in small locations and in balneary areas houses are designated by name rather than number.\n\nIn countries like Brazil and Argentina, a scheme is used also for streets in cities, where the house number is the distance measured in meters from the house to the start of the street. In Venezuela, houses, buildings, and streets have names instead of numbers.\n\nIn the United States and Canada, streets are usually numbered with odd on one side and even on the other. The specific ordering of the numbers vary based on the policies of the local municipality. Generally, three different systems exist:\n\n\n\n\nEven within these systems, there is are also two ways to define the starting point for the numbering system. Some places will start the numbering system at the start of the street itself. Other places will define a numbering system based on a defined point or line, such as a municipal or county boundary, or a defined intersection near the center of the municipality, with numbers increasing generally as one gets further from the baseline, regardless of where streets start or stop.\n"}
{"id": "50880883", "url": "https://en.wikipedia.org/wiki?curid=50880883", "title": "International Center for Relativistic Astrophysics", "text": "International Center for Relativistic Astrophysics\n\nICRA, the International Center for Relativistic Astrophysics is an international research institute for relativistic astrophysics and related areas. Its members are seven Universities and four organizations. The center is located in Rome, Italy.\n\nThe International Center for Relativistic Astrophysics (ICRA) was founded in 1985 by Remo Ruffini (University of Rome \"La Sapienza\") together with Riccardo Giacconi (Nobel Prize for Physics 2002), Abdus Salam (Nobel Prize for Physics 1979), Paul Boynton (University of Washington), George Coyne (former director of the Vatican observatory), Francis Everitt (Stanford University), Fang Li-Zhi (University of Science and Technology of China). It became a legal entity in 1991 with the Ministerial Decree 22/11/1991 from the Ministry of Education, Universities and Research.\n\nThe International Center of Relativistic Astrophysics is located in the Department of Physics building at the main Campus of the University of Rome \"Sapienza\".\n\nIn 2005 ICRA has been among the founders of ICRANet, the International Center for Relativistic Astrophysics Network. The national activities of research and teaching in Italy remained operative at ICRA in Rome, while international activities and coordination are now based in ICRANet in Pescara.\n\nPresident: Remo Ruffini\nICRA Council:\n\nPaolo De Bernardis, University of Rome Sapienza\n\nFrancis Everitt, Stanford University\n\nJosè Gabriel Funes, Vatican Observatory\n\nRemo Ruffini, ICRANet\n\nRobert Williams, Space Telescope Science Institute\n\nUniversity of Rome \"Sapienza\" (Italy)\n\nSpace Telescope Institute - Baltimore - Maryland - (USA)\n\nInternational Centre For Theoretical Physics (ICTP) - Trieste (Italy)\n\nThe World Academy of Sciences (TWAS) - Trieste (Italy)\n\nSpecola Vaticana - Castelgandolfo (Vatican City)\n\nStanford University - Stanford, California (USA)\n\nWashington University - Seattle (USA)\n\nUniversity of Hofei (China)\n\nUniversity Campus Bio-Medico of Rome (Italy)\n\nUniversity of Udine (Italy)\n\nUniversity of Insubria (Italy)\n\nCollaboration agreements have been signed between ICRA and scientific institutions worldwide, in particular:\n\nAIGRC (The Australian International Gravitational Research Centre), Australia\n\nARSEC (Astrophysical Research center for the Structure and Evolution of the Cosmos), South Korea\n\nBAO (Beijing Astronomical Observatory), China\n\nCECS (Centro de Estudios Cientificos de Santiago), Chile\n\nUniversidad Nacional de Colombia, Colombia\n\nIHES (Institut Hautes Etudes Scientifiques), France\n\nKSNU (Kyrgiz State National University), Kyrgyzstan\n\nIPM (Keldysh Institute for Applied Mathematics), Russia\n\nMEPhI (Moscow State Engineering Physics Institute), Russia\n\nNCST (National Centre for Science and Technology), Vietnam\n\nOCA (Côte d’Azur Observatory), France\n\nPAO (Pyongyang Astronomical Observatory), North Korea\n\nUniversity of Tirana, Albania\n\nYITP (Yukawa Institute for Theoretical Physics), Japan\n\nUADP (Physics Department, University of Arizona), USA\n\nSince 2002 ICRA co-organizes an International Ph.D. program in Relativistic Astrophysics - International Relativistic Astrophysics Ph.D. Program, IRAP-PhD, the first joint PhD astrophysics program.\n\nMarcel Grossmann meetings\n\nWriting down the equations of General Relativity was no doubt the result of Einstein's brilliant physical intuition, based from the mathematical point of view on the work of Gregorio Ricci Curbastro and Tullio Levi Civita of the University of Padua and the University of Rome \"La Sapienza\". Essential to this work was the intervention of Marcel Grossmann of the University of Zurich who was close to Einstein and who had a deep knowledge of the Italian school of geometry.\nIn order to promote this important collaboration between physics and mathematics, in celebration of that historic event, Remo Ruffini and Abdus Salam established in 1975 the Marcel Grossmann meetings (MG) on Recent Developments in Theoretical and Experimental General Relativity, Gravitation, and Relativistic Field Theories which take place every three years in different countries. MG1 and MG2 were held in 1975 and in 1979 in Trieste; MG3 in 1982 in Shanghai; MG4 in 1985 in Rome; MG5 in 1988 in Perth; MG6 in 1991 in Kyoto; MG7 in 1994 at Stanford; MG8 in 1997 in Jerusalem; MG9 in 2000 in Rome; MG10 in 2003 in Rio de Janeiro; MG11 in 2006 in Berlin; MG12 in 2009 in Paris; MG13 in 2012 in Stockholm; MG14 in 2015 in Rome.\n\nItalian-Korean Meetings on Relativistic Astrophysics\n\nThe Italian-Korean Symposia on Relativistic Astrophysics is a series of biannual meetings organized alternatively in Italy and in Korea since 1987. It has been boosting exchange of information and collaborations between Italian and Korean astrophysicists on new and hot issues in the field of Relativistic Astrophysics. The symposia cover topics in astrophysics and cosmology, such as gamma ray bursts and compact stars, high energy cosmic rays, dark energy and dark matter, general relativity, black holes, and new physics related to cosmology.\n\nWilliam Fairbank Meetings on Relativistic Gravitational Experiments in Space\n\nThe First William Fairbank Meeting on was held at the University of Rome \"La Sapienza,\" in 1990, under the auspices of ICRA with support from ASI (Italian Space Agency), ESA (European Space Agency), the Vatican Observatory, Stanford University and the University of Rome. Almost 80 physicists and engineers in widely diversified fields relativistic gravitation, space research, SQUID technology, large scale cryogenics, clock technology, laser and radar science and other fields - came together in the kinds of free technical exchange so characteristic of William Fairbank, in whose honor the meeting was held. The second meeting was held in Hong Kong and was devoted to relativistic gravitational experiments in space. The third meeting held in Rome and Pescara in 1998 was focused on the Lense-Thirring effect.\n\nFirst William Fairbank Meeting, Rome, 10–14 September 1990, ICRA, University of Rome \"La Sapienza\" - ICRA Network, Pescara.\n\nSecond William Fairbank Meeting, December 13–16, 1993, Hong Kong\n\nThird William Fairbank Meeting. The Lense-Thirring Effect, June 29 - July 4, 1998, ICRA, University of Rome \"La Sapienza\" - ICRA Network, Pescara.\n\nThe Galileo-Xu Guangqi meetings\n\nThe Galileo-Xu Guangqi meetings have been created in the name of Galileo and Xu Guangqi, the collaborator of Matteo Ricci (Ri Ma Dou), generally recognized for bringing to China the works of Euclid and Galileo and for his strong commitment to the process of modernization and scientific development of China. The 1st Galileo - Xu Guangqi Meeting was held in Shanghai, China in 2009. The 2nd Galileo - Xu Guangqi meeting took place in Hanbury Botanic Gardens (Ventimiglia, Italy) and Villa Ratti (Nice, France) in 2010. The 3rd and 4th Galileo - Xu Guangqi meetings were held in Beijing, China in 2011 and 2015, respectively.\n\nINW I: LXV of R. Giacconi, Rome and Castelgandolfo, October 24–26, 1997\n\nINW II: The Chaotic Universe, Rome and Pescara, February 1–5, 1999\n\nINW III: Electrodynamics and Magnetohydrodynamics around Black Holes, Rome and Pescara, July 12–24, 1999\n\nINW IV: Science at new Millennium, UWA, March 10–14, 2000\n\nINW VI: Time structures in Relativistic Astrophysics, Pescara, July 2–14, 2001\n\nINW VIII: Step and General Relativity, Pescara, September 16–21, 2002\n\nINW IX: Fermi and Astrophysics, Rome and Pescara, October 3–7, 2001\n\nINW X: Black Holes, Gravitational Waves and Cosmology, Rome and Pescara, July 15–20, 2002\n\nINW XV: Testing the Equivalence Principle on Ground and in Space, Rome and Pescara, Italy September 20–23, 2004\n\nIn addition to the proceedings of conferences several books have been published, in particular:\nThe history of the relativistic astrophysics group in the Department of Physics (Fisica) of the University of Rome \"La Sapienza\" led by Remo Ruffini, started with his appointment to a chair of theoretical physics there in 1978 is represented here.\n"}
{"id": "5559816", "url": "https://en.wikipedia.org/wiki?curid=5559816", "title": "International Society of Critical Health Psychology", "text": "International Society of Critical Health Psychology\n\nThe International Society of Critical Health Psychology (ISCHP) is a society devoted to debate about critical ideas within health psychology and developing new ways of health psychology practice. ISCHP's members commonly make use of qualitative methods and participatory research methods to address social, political and cultural issues within health psychology.\n\nThe venue for the 2019 ISCHP conference will be Bratislava in Slovakia from the evening of Sunday 14th July 2019 to late afternoon on Wednesday 17th July. More details will be posted on the conference website.\n\nIn July 1999, the First International Conference on Critical and Qualitative Approaches to Health Psychology was held in St. John's, Newfoundland, Canada, organised by Michael Murray. Over 120 critical health psychologists from 20 countries and every continent on the globe attended the conference. It was agreed that there was an urgent need to establish a network to begin to connect those health psychologists throughout the world who were interested in developing a more critical approach to the subject.\n\nIn August 2001, the Second International Conference was held in Birmingham, UK. The International Society of Critical Health Psychology was established at this conference. The founding committee members include (alphabetically) Kerry Chamberlain, Sue Dalton, Antonia Lyons, David Marks, Michael Murray, Alan Radley, Wendy Stainton Rogers and Chris Stephens. Currently, the Chair is Gareth Treharne, the Vice-Chair is Poul Rohleder, the Treasurer is Chris Stephens, the Secretary is Abigail Locke.\n\nISCHP has the following aims:\n\nMembers of the Society take a variety of theoretical and methodological viewpoints. However, as with other critical psychologists, they share a common dissatisfaction with the positivist assumptions of much of mainstream psychology and its ignorance of broader social and political issues. Instead, they share an interest in various critical ideas (e.g. social constructionism, post-modernism, feminism, marxism, etc.) and various qualitative and participatory methods of research (e.g. discourse analysis, grounded theory, action research, ethnography, etc.) and their relevance to understanding health and illness. Further, they share an awareness of the social, political and cultural dimensions of health and illness (e.g. poverty, racism, sexism, political oppression, etc.) and an active commitment to reducing human suffering and promoting improved quality of life, especially among those sections of society most in need.\n\nISCHP welcomes membership from anyone who is aligned with its aims, regardless of disciplinary affiliation. ISCHP has members throughout the world, who are mostly critical health psychology researchers, but members also come from other fields of psychology, as well as from the sociology and anthropology of health, illness and medicine, health sciences, nursing, media studies, communication studies, cultural studies and other disciplines. As of 2015 the society has approximately 850 members in over 36 countries. Details on how to join ISCHP and thus receive the email updates can be found on ISCHP's website.\n\nISCHP has a blog to keep members up to date on current issues in critical health psychology and activities of the Society. ISCHP also has an official Facebook page and an official Twitter profile, on which topical updates are posted about events, jobs and studies.\n\nThe Society organizes a biennial conference. These conferences have been held in: St. John's, Newfoundland, Canada (1999); Birmingham, UK (2001); Auckland, New Zealand (2003); Sheffield, UK (2005); Boston, USA (2007); Lausanne, Switzerland (2009); Adelaide, Australia (2011); Bradford, UK (2013); Grahamstown, South Africa (2015); and most recently in Loughborough, UK (2017). An archive of previous conference abstracts can be found on ISCHP's website. The next conference will be in Bratislava, Slovakia, from the evening of Sunday 14th July 2019 to Wednesday 17th July. Details about abstract submission, keynote speakers, and other aspects of the 2019 conference can be found on the conference website.\n\n\n"}
{"id": "1016857", "url": "https://en.wikipedia.org/wiki?curid=1016857", "title": "Law of effect", "text": "Law of effect\n\nThe law of effect is a psychological principle advanced by Edward Thorndike in 1898 on the matter of behavioral conditioning (not then formulated as such) which states that \"responses that produce a satisfying effect in a particular situation become more likely to occur again in that situation, and responses that produce a discomforting effect become less likely to occur again in that\nsituation.\"\n\nThis notion is very similar to that of the evolutionary theory, if a certain character trait provides an advantage for reproduction then that trait will persist. The terms \"satisfying\" and dissatisfying\" appearing in the definition of the law of effect were eventually replaced by the terms \"reinforcing\" and \"punishing,\" when operant conditioning became known. \"Satisfying\" and \"dissatisfying\" conditions are determined behaviorally, and they cannot be accurately predicted, because each animal has a different idea of these two terms than another animal. The new terms, \"reinforcing\" and \"punishing\" are used differently in psychology than they are colloquially. Something that reinforces a behavior makes it more likely that that behavior will occur again, and something that punishes a behavior makes it less likely that behavior will occur again.\n\nThorndike's law of effect refutes the ideas George Romanes' book \"Animal Intelligence\", stating that anecdotal evidence is weak and is typically not useful. The book stated that animals, like humans, think things through when dealing with a new environment or situation. Instead, Thorndike hypothesized that animals, to understand their physical environment, must physically interact with it using trial and error, until a successful result is obtained. This is illustrated in his cat experiment, in which a cat is placed in a shuttlebox and eventually learns, by interacting with the environment of the box, how to escape.\n\nThis principle, discussed early on by Lloyd Morgan, is usually associated with the connectionism of Edward Thorndike, who said that if an association is followed by a \"satisfying state of affairs\" it will be strengthened and if it is followed by an \"annoying state of affairs\" it will be weakened.\n\nThe modern version of the law of effect is conveyed by the notion of reinforcement as it is found in operant conditioning. The essential idea is that behavior can be modified by its consequences, as Thorndike found in his famous experiments with hungry cats in puzzle boxes. The cat was placed in a box that could be opened if the cat pressed a lever or pulled a loop. Thorndike noted the amount of time it took the cat to free itself on successive trials in the box. He discovered that during the first few trials the cat would respond in many ineffective ways, such as scratching at the door or the ceiling, finally freeing itself with the press or pull by trial-and-error. With each successive trial, it took the cat, on average, less and less time to escape. Thus, in modern terminology, the correct response was reinforced by its consequence, release from the box.\n\nLaw of effect is the belief that a pleasing after-effect strengthens the action that produced it.\n\nThe law of effect was published by Edward Thorndike in 1905 and states that when an S-R association is established in instrumental conditioning between the instrumental response and the contextual stimuli that are present, the response is reinforced and the S-R association holds the sole responsibility for the occurrence of that behavior. Simply put, this means that once the stimulus and response are associated, the response is likely to occur without the stimulus being present.\nIt holds that responses that produce a satisfying or pleasant state of affairs in a particular situation are more likely to occur again in a similar situation. Conversely, responses that produce a discomforting, annoying or unpleasant effect are less likely to occur again in the situation.\n\nPsychologists have been interested in the factors that are important in behavior change and control since psychology emerged as a discipline. One of the first principles associated with learning and behavior was the Law of Effect, which states that behaviors that lead to satisfying outcomes are likely to be repeated, whereas behaviors that lead to undesired outcomes are less likely to recur.\n\nThorndike emphasized the importance of the situation in eliciting a response; the cat would not go about making the lever-pressing movement if it was not in the puzzle box but was merely in a place where the response had never been reinforced. The situation involves not just the cat's location but also the stimuli it is exposed to, for example, the hunger and the desire for freedom. The cat recognizes the inside of the box, the bars, and the lever and remembers what it needs to do to produce the correct response. This shows that learning and the law of effect are context-specific.\n\nIn an influential paper, R. J. Herrnstein (1970) proposed a quantitative relationship between response rate (\"B\") and reinforcement rate (\"Rf\"):\n\n\"B\" = \"k\" \"Rf\" / (\"Rf\" + \"Rf\")\n\nwhere \"k\" and \"Rf\" are constants. Herrnstein proposed that this formula, which he derived from the matching law he had observed in studies of concurrent schedules of reinforcement, should be regarded as a quantification of the law of effect. While the qualitative law of effect may be a tautology, this quantitative version is not.\n\nAn example is often portrayed in drug addiction. When a person uses a substance for the first time and receives a positive outcome, they are likely to repeat the behavior due to the reinforcing consequence. Over time, the person's nervous system will also develop a tolerance to the drug. Thus only by increasing dosage of the drug will provide the same satisfaction, making it dangerous for the user.\n\nThorndike's Law of Effect can be compared to Darwin's theory of natural selection in which successful organisms are more likely to prosper and survive to pass on their genes to the next generation, while the weaker, unsuccessful organisms are gradually replaced and \"stamped out\". It can be said that the environment selects the \"fittest\" behavior for a situation, stamping out any unsuccessful behaviors, in the same way it selects the \"fittest\" individuals of a species. In an experiment that Thorndike conducted, he placed a hungry cat inside a \"puzzle box\", where the animal could only escape and reach the food once it could operate the latch of the door. At first the cats would scratch and claw in order to find a way out, then by chance / accident, the cat would activate the latch to open the door. On successive trials, the behaviour of the animal would become more habitual, to a point where the animal would operate without hesitation. The occurrence of the favourable outcome, reaching the food source, only strengthens the response that it produces.\n\nColwill and Rescorla for example made all rats complete the goal of getting food pellets and liquid sucrose in consistent sessions on identical variable-interval schedules.\n\nThe law of work for psychologist B. F. Skinner almost half a century later on the principles of operant conditioning, \"a learning process by which the effect, or consequence, of a response influences the future rate of production of that response.\" Skinner would later use an updated version of Thorndike's puzzle box, called the operant chamber, or Skinner box, which has contributed immensely to our perception and understanding of the law of effect in modern society and how it relates to operant conditioning. It has allowed a researcher to study the behavior of small organisms in a controlled environment.\n"}
{"id": "52193801", "url": "https://en.wikipedia.org/wiki?curid=52193801", "title": "Lewis Rickinson", "text": "Lewis Rickinson\n\nLewis Raphael Rickinson (born 21 April 1883 (Lewisham), died 16 April 1945 (Newbury, Berkshire)) was an English marine engineer. He is best known for his service in the Imperial Trans-Antarctic Expedition of 1914–1916, for which he was awarded the Silver Polar Medal.\n\nRickinson was born on 21 April 1883 in Lewisham, which was then part of the County of Kent but has since become part of Greater London. His father was Charles Napier Rickinson and his mother was Emma Isaac Rickinson. As a man trained for work with marine engines, he signed on the \"Endurance\" as the chief engineer. Although the \"Endurance\" was rigged as a barquentine, it also had a coal-burning engine and spent much of its time under steam. \n\nUnder the expedition plans and the articles that Rickinson had signed, his job was to work the engines during the Antarctic summer of 1914-1915 to get the \"Endurance\" to the Filchner Ice Shelf. Once the vessel had reached her destination, she and her crew were supposed to unload the expedition leader, Sir Ernest Shackleton, and a shore party for expedition work in the interior of Antarctica. Rickinson and the ship's company were then supposed to steam north toward warmer waters to avoid the worst of the Antarctic winter of 1915. However, when the \"Endurance\" was beset by pack ice in the Weddell Sea, these plans could not be implemented. With all of the other members of the expedition, Rickinson was first forced to spend the winter in the depths of the southern Weddell Sea, and then shared the fate of his fellow explorers as castaways when the mother ship was crushed and sunk by the ice. After camping on the melting ice for some months, the ship's company and shore party were forced to take to lifeboats. Rickinson was assigned to the lifeboat \"Stancomb Wills\".\n\nShackleton was impressed by Rickinson's ability to take on his share of the survival duties of the party. Unfortunately in April 1916, when the lifeboat party was making a hazardous landing on the shore of Elephant Island off the coast of the Antarctic Peninsula, young Rickinson was stricken while wading ashore in the surf. Once all were safely on shore, the expedition doctor diagnosed the 32-year-old engineer with a mild heart attack. He was advised to rest as much as possible in a crude lean-to hut, the \"Snuggery\", built by the men. Meanwhile Shackleton and a picked crew of volunteers had separated from the main party to mount a forlorn open-boat attempt to escape from Antarctica and fetch help for the Elephant Island castaways, including Rickinson. After enduring more than four months of near-starvation rations Rickinson, who was still classified as an invalid, was rescued from Elephant Island with his comrades. His time in the Antarctic was over; it was 30 August 1916.\n\nUpon returning to Britain Rickinson found World War I being fought. Despite his cardiac diagnosis he joined the colours, was passed as fit, and served in the Royal Navy. In 1918 he married Marjorie Kate Snell. Two children, son Lewis F. Rickinson (1919) and daughter Betty Rickinson (1923), were born of this union. With the coming of peace Rickinson chose life on shore. He became a consulting engineer, specializing in the shipbuilding design and installation of marine power units. \n\nWith the coming of World War II, Rickinson rejoined the colours and was assigned to \"HMS Pembroke\", the pseudo-floating naval barracks and training establishment at Chatham in the Medway. He rose to the rank of engineer naval commander and served until his diagnosis with lung cancer. He was seconded to a Berkshire nursing home for hospice care, and died there in April 1945, age 61.\n\nIn 1916–17, Rickinson was awarded the Polar Medal in silver.\n"}
{"id": "40285325", "url": "https://en.wikipedia.org/wiki?curid=40285325", "title": "Light stage", "text": "Light stage\n\nA light stage or light cage is equipment used for shape, texture, reflectance and motion capture often with structured light and a multi-camera setup.\n\nThe reflectance field over a human face was first captured in 2000 by Paul Debevec et al. The method they used to find the light that travels under the skin was based on the existing scientific knowledge that light reflecting off the air-to-oil retains its polarization while light that travels under the skin loses its polarization.\n\nUsing this information, a light stage was built by Debevec et al., consisting of\n\nFollowing great scientific success Debevec et al. constructed more elaborate versions of the light stage at the University of Southern California's (USC)'s Institute for Creative Technologies (ICT). Ghosh et al. built the seventh version of the USC light stage X. In 2014 President Barack Obama had his image and reflectance captured with the USC mobile light stage.\n\n"}
{"id": "28842180", "url": "https://en.wikipedia.org/wiki?curid=28842180", "title": "List of German scientists", "text": "List of German scientists\n\nThis is a list of notable German scientists.\n\n"}
{"id": "21193804", "url": "https://en.wikipedia.org/wiki?curid=21193804", "title": "List of books about the politics of science", "text": "List of books about the politics of science\n\nThis is a list of notable books about the politics of science that have their own articles on Wikipedia.\n\n\n\n"}
{"id": "22314434", "url": "https://en.wikipedia.org/wiki?curid=22314434", "title": "List of countries by median age", "text": "List of countries by median age\n\nThis article is a list of countries by median age. \n\nMedian age is the age that divides a population into two numerically equal groups - that is, half the people are younger than this age and half are older. It is a single index that summarizes the age distribution of a population.\n\nCurrently, the median age ranges from a low of about 15 in Niger to 40 or more in several European countries, Canada and Japan. The median age for women tends to be much greater than that of men in some of the ex-Soviet republics, while in the Global South the difference is far smaller or is reversed.\n\n"}
{"id": "56252311", "url": "https://en.wikipedia.org/wiki?curid=56252311", "title": "List of presidential trips made by Donald Trump during 2018", "text": "List of presidential trips made by Donald Trump during 2018\n\nThis is a list of presidential trips made by Donald Trump during 2018, the second year of his presidency as the 45th President of the United States.\n\nThis list excludes trips made within Washington, D.C., the U.S. federal capital in which the White House, the official residence and principal workplace of the President, is located. Also excluded are trips to Camp David, the country residence of the President. International trips are included. The number of visits per state or territory where he traveled are:\n"}
{"id": "25214307", "url": "https://en.wikipedia.org/wiki?curid=25214307", "title": "List of telescope parts and construction", "text": "List of telescope parts and construction\n\n\n\n\n\nMirrors and lenses are the critical light-bending components of a telescope.\nSubsequent (sometimes optional) components realign, segment, or in some way modify the light of an incoming image:\n\nGenerally applicable to all items:\n\n\n\n"}
{"id": "15433382", "url": "https://en.wikipedia.org/wiki?curid=15433382", "title": "List of unsolved problems in statistics", "text": "List of unsolved problems in statistics\n\nThere are many longstanding unsolved problems in mathematics for which a solution has still not yet been found. The unsolved problems in statistics are generally of a different flavor; according to John Tukey, \"difficulties in identifying problems have delayed statistics far more than difficulties in solving problems.\" A list of \"one or two open problems\" (in fact 22 of them) was given by David Cox.\n\n\n\n\n"}
{"id": "7120249", "url": "https://en.wikipedia.org/wiki?curid=7120249", "title": "List of volcanoes in Saudi Arabia", "text": "List of volcanoes in Saudi Arabia\n\nThis is a partial list of active and extinct volcanoes in Saudi Arabia. A complete list can be found at http://volcano.si.edu/. \n"}
{"id": "46970402", "url": "https://en.wikipedia.org/wiki?curid=46970402", "title": "London International Development Centre", "text": "London International Development Centre\n\nThe London International Development Centre (LIDC) was established in 2007 through funding of £3.7m from HEFCE. LIDC is a collaborative between University of London's Bloomsbury Colleges (Birkbeck, the UCL Institute of Education, the London School of Hygiene & Tropical Medicine, the Royal Veterinary College, and SOAS) and now has 3,000 staff, student and alumni members from its constituent colleges. As of August 2017, Queen Mary, University of London and City, University of London became member colleges of the LIDC.\n\n"}
{"id": "65637", "url": "https://en.wikipedia.org/wiki?curid=65637", "title": "Metrology", "text": "Metrology\n\nMetrology is the science of measurement. It establishes a common understanding of units, crucial in linking human activities. Modern metrology has its roots in the French Revolution's political motivation to standardise units in France, when a length standard taken from a natural source was proposed. This led to the creation of the decimal-based metric system in 1795, establishing a set of standards for other types of measurements. Several other countries adopted the metric system between 1795 and 1875; to ensure conformity between the countries, the Bureau International des Poids et Mesures (BIPM) was established by the Metre Convention. This has evolved into the International System of Units (SI) as a result of a resolution at the 11th Conference Generale des Poids et Mesures (CGPM) in 1960.\n\nMetrology is divided into three basic overlapping activities.\nThe first being the definition of units of measurement, second the realisation of these units of measurement in practice, and last traceability, which is linking measurements made in practice to the reference standards. These overlapping activities are used in varying degrees by the three basic sub-fields of Metrology. The sub-fields are scientific or fundamental metrology, which is concerned with the establishment of units of measurement, Applied, technical or industrial metrology, the application of measurement to manufacturing and other processes in society, and Legal metrology, which covers the regulation and statutory requirements for measuring instruments and the methods of measurement.\n\nIn each country, a national measurement system (NMS) exists as a network of laboratories, calibration facilities and accreditation bodies which implement and maintain its metrology infrastructure. The NMS affects how measurements are made in a country and their recognition by the international community, which has a wide-ranging impact in its society (including economics, energy, environment, health, manufacturing, industry and consumer confidence). The effects of metrology on trade and economy are some of the easiest-observed societal impacts. To facilitate fair trade, there must be an agreed-upon system of measurement.\n\nThe ability to measure alone is insufficient; standardisation is crucial for measurements to be meaningful. The first record of a permanent standard was in 2900 BC, when the royal Egyptian cubit was carved from black granite. The cubit was decreed to be the length of the Pharaoh's forearm plus the width of his hand, and replica standards were given to builders. The success of a standardised length for the building of the pyramids is indicated by the lengths of their bases differing by no more than 0.05 percent.\n\nOther civilizations produced generally accepted measurement standards, with Roman and Greek architecture based on distinct systems of measurement. The collapse of the empires and the Dark Ages which followed them lost much measurement knowledge and standardisation. Although local systems of measurement were common, comparability was difficult since many local systems were incompatible. England established the Assize of Measures to create standards for length measurements in 1196, and the 1215 Magna Carta included a section for the measurement of wine and beer.\n\nModern metrology has its roots in the French Revolution. With a political motivation to harmonise units throughout France, a length standard based on a natural source was proposed. In March 1791, the metre was defined. This led to the creation of the decimal-based metric system in 1795, establishing standards for other types of measurements. Several other countries adopted the metric system between 1795 and 1875; to ensure international conformity, the International Bureau of Weights and Measures (, or BIPM) was established by the Metre Convention. Although the BIPM's original mission was to create international standards for units of measurement and relate them to national standards to ensure conformity, its scope has broadened to include electrical and photometric units and ionizing radiation measurement standards. The metric system was modernised in 1960 with the creation of the International System of Units (SI) as a result of a resolution at the 11th General Conference on Weights and Measures (, or CGPM).\n\nMetrology is defined by the International Bureau of Weights and Measures (BIPM) as \"the science of measurement, embracing both experimental and theoretical determinations at any level of uncertainty in any field of science and technology\". It establishes a common understanding of units, crucial to human activity. Metrology is a wide reaching field, but can be summarized through three basic activities: the definition of internationally accepted units of measurement, the realisation of these units of measurement in practice, and the application of chains of traceability (linking measurements to reference standards). These concepts apply in different degrees to metrology's three main fields: scientific metrology; applied, technical or industrial metrology, and legal metrology.\n\nScientific metrology is concerned with the establishment of units of measurement, the development of new measurement methods, the realisation of measurement standards, and the transfer of traceability from these standards to users in a society. This type of metrology is considered the top level of metrology which strives for the highest degree of accuracy. BIPM maintains a database of the metrological calibration and measurement capabilities of institutes around the world. These institutes, whose activities are peer-reviewed, provide the fundamental reference points for metrological traceability. In the area of measurement, BIPM has identified nine metrology areas, which are acoustics, electricity and magnetism, length, mass and related quantities, photometry and radiometry, ionizing radiation, time and frequency, thermometry, and chemistry.\n\nThere is a proposed redefinition of the SI base units that was formally voted on in November 2018, and will come into effect in May 2019. The motivation in the change of the base units is to make the entire system derivable from physical constants, which requires the removal of the prototype kilogram as it is the last artefact the unit definitions depend on. Scientific metrology plays an important role in this redefinition of the units as precise measurements of the physical constants is required to have accurate definitions of the base units. To redefine the value of a kilogram without an artefact the value of the Planck constant must be known to twenty parts per billion. Scientific metrology, through the development of the Kibble balance and the Avogadro project, has produced a value of Planck constant with low enough uncertainty to allow for a redefinition of the kilogram.\n\nApplied, technical or industrial metrology is concerned with the application of measurement to manufacturing and other processes and their use in society, ensuring the suitability of measurement instruments, their calibration and quality control. Producing good measurements is important in industry as it has an impact on the value and quality of the end product, and a 10-15% impact on production costs. Although the emphasis in this area of metrology is on the measurements themselves, traceability of the measuring-device calibration is necessary to ensure confidence in the measurement. Recognition of the metrological competence in industry can be achieved through mutual recognition agreements, accreditation, or peer review. Industrial metrology is important to a country's economic and industrial development, and the condition of a country's industrial-metrology program can indicate its economic status.\n\nLegal metrology \"concerns activities which result from statutory requirements and concern measurement, units of measurement, measuring instruments and methods of measurement and which are performed by competent bodies\". Such statutory requirements may arise from the need for protection of health, public safety, the environment, enabling taxation, protection of consumers and fair trade. The International Organization for Legal Metrology (OIML) was established to assist in harmonising regulations across national boundaries to ensure that legal requirements do not inhibit trade. This harmonisation ensures that certification of measuring devices in one country is compatible with another countries certification process, allowing the trade of the measuring devices and the products that rely on them. WELMEC was established in 1990 to promote cooperation in the field of legal metrology in the European Union and among European Free Trade Association (EFTA) member states. In the United States legal metrology is under the authority of the Office of Weights and Measures of National Institute of Standards and Technology (NIST), enforced by the individual states.\n\nThe International System of Units (SI) defines seven base units: length, mass, time, electric current, thermodynamic temperature, amount of substance, and luminous intensity. By convention, each of these units are considered to be mutually independent of each other; however, in reality they are interdependent given some definitions contain other base SI Units. All other SI units are derived from the seven base units.\n\nSince the base units are the reference points for all measurements taken in SI units, if the reference value changed all prior measurements would be incorrect. If a piece of the international prototype kilogram snapped off, it would still be defined as a kilogram; all previous measured values of a kilogram would be heavier. The importance of reproducible SI units has led the BIPM to begin defining base SI units in terms of physical constants. By defining base SI units with respect to physical constants, they are realisable with a higher level of precision and reproducibility. With the redefinition of the SI units occurring on May 20th, 2019 the kilogram, ampere, kelvin, and mole will then be defined by setting exact numerical values for the Planck constant ('), the elementary electric charge ('), the Boltzmann constant (\"\"), and the Avogadro constant (), respectively. The metre and candela are already defined by physical constants, subject to correction to their present definitions. The new definitions aim to improve the SI without changing the size of any units, thus ensuring continuity with existing measurements.\n\nThe realisation of a unit of measure is its conversion into reality. Three possible methods of realisation are defined by the (VIM): a physical realisation of the unit from its definition, a highly-reproducible measurement as a reproduction of the definition (such as the quantum Hall effect for the ohm), and the use of a material object as the measurement standard.\n\nA standard (or etalon) is an object, system, or experiment with a defined relationship to a unit of measurement of a physical quantity. Standards are the fundamental reference for a system of weights and measures by realising, preserving, or reproducing a unit against which measuring devices can be compared. There are three levels of standards in the hierarchy of metrology: primary, secondary, and working standards. Primary standards (the highest quality) do not reference any other standards. Secondary standards are calibrated with reference to a primary standard. Working standards, used to calibrate (or check) measuring instruments or other material measures, are calibrated with respect to secondary standards. The hierarchy preserves the quality of the higher standards. An example of a standard would be gauge blocks for length. A gauge block is a block of metal or ceramic with two opposing faces ground precisely flat and parallel, a precise distance apart. The length of the path of light in vacuum during a time interval of 1/299,792,458 of a second is embodied in an artefact standard such as a gauge block; this gauge block is then a primary standard which can be used to calibrate secondary standards through mechanical comparators.\n\nMetrological traceability is defined as the \"property of a measurement result whereby the result can be related to a reference through a documented unbroken chain of calibrations, each contributing to the measurement uncertainty\". It permits the comparison of measurements, whether the result is compared to the previous result in the same laboratory, a measurement result a year ago, or to the result of a measurement performed anywhere else in the world. The chain of traceability allows any measurement to be referenced to higher levels of measurements back to the original definition of the unit.\n\nTraceability is most often obtained by calibration, establishing the relationship between an indication on a measuring instrument (or secondary standard) and the value of the standard. A calibration is an operation that establishes a relation between a measurement standard with a known measurement uncertainty and the device that is being evaluated. The process will determine the measurement value and uncertainty of the device that is being calibrated and create a traceability link to the measurement standard. The four primary reasons for calibrations are to provide traceability, to ensure that the instrument (or standard) is consistent with other measurements, to determine accuracy, and to establish reliability. Traceability works as a pyramid, at the top level there is the international standards, at the next level national metrology institutes calibrate the primary standards through realisation of the units creating the traceability link from the primary standard and the unit definition. Through subsequent calibrations between national metrology institutes, calibration laboratories, and industry and testing laboratories the realisation of the unit definition is propagated down through the pyramid. The traceability chain works upwards from the bottom of the pyramid, where measurements done by industry and testing laboratories can be directly related to the unit definition at the top through the traceability chain created by calibration.\n\nMeasurement uncertainty is a value associated with a measurement which expresses the spread of possible values associated with the measurand—a quantitative expression of the doubt existing in the measurement. There are two components to the uncertainty of a measurement: the width of the uncertainty interval and the confidence level. The uncertainty interval is a range of values that the measurement value is expected to fall within, while the confidence level is how likely the true value is to fall within the uncertainty interval. Uncertainty is generally expressed as follows:\nWhere \"y\" is the measurement value and \"U\" is the uncertainty value and \"k\" is the coverage factor indicates the confidence interval. The upper and lower limit of the uncertainty interval can be determined by adding and subtracting the uncertainty value from the measurement value. The coverage factor of \"k\" = 2 generally indicates a 95% confidence that the measured value will fall inside the uncertainty interval. Other values of \"k\" can be used to indicate a greater or lower confidence on the interval, for example \"k\" = 1 and \"k\" = 3 generally indicate 66% and 99.7% confidence respectively. The uncertainty value is determined through a combination of statistical analysis of the calibration and uncertainty contribution from other errors in measurement process, which can be evaluated from sources such as the instrument history, manufacturer's specifications, or published information.\n\nSeveral international organizations maintain and standardise metrology.\n\nThe Metre Convention created three main international organizations to facilitate standardisation of weights and measures. The first, the General Conference on Weights and Measures (CGPM), provided a forum for representatives of member states. The second, the International Committee for Weights and Measures (CIPM), was an advisory committee of metrologists of high standing. The third, the International Bureau of Weights and Measures (BIPM), provided secretarial and laboratory facilities for the CGPM and CIPM.\n\nThe General Conference on Weights and Measures (, or CGPM) is the convention's principal decision-making body, consisting of delegates from member states and non-voting observers from associate states. The conference usually meets every four to six years to receive and discuss a CIPM report and endorse new developments in the SI as advised by the CIPM. The last meeting was held November 13-16, 2018. On the last day of this conference there was vote on the redefinition of four base units, which the International Committee for Weights and Measures (CIPM) had proposed earlier that year. The new definitions will come into force on 20 May 2019.\n\nThe International Committee for Weights and Measures (, or CIPM) is made up of eighteen (originally fourteen) individuals from a member state of high scientific standing, nominated by the CGPM to advise the CGPM on administrative and technical matters. It is responsible for ten consultative committees (CCs), each of which investigates a different aspect of metrology; one CC discusses the measurement of temperature, another the measurement of mass, and so forth. The CIPM meets annually in Sèvres to discuss reports from the CCs, to submit an annual report to the governments of member states concerning the administration and finances of the BIPM and to advise the CGPM on technical matters as needed. Each member of the CIPM is from a different member state, with France (in recognition of its role in establishing the convention) always having one seat.\n\nThe International Bureau of Weights and Measures (, or BIPM) is an organisation based in Sèvres, France which has custody of the international prototype kilogram, provides metrology services for the CGPM and CIPM, houses the secretariat for the organisations and hosts their meetings. Over the years, international prototype metres and kilograms have been returned to BIPM headquarters for recalibration. The BIPM director is an ex officio member of the CIPM and a member of all consultative committees.\n\nThe International Organization of Legal Metrology (, or OIML), is an intergovernmental organization created in 1955 to promote the global harmonisation of the legal metrology procedures facilitating international trade. This harmonisation of technical requirements, test procedures and test-report formats ensure confidence in measurements for trade and reduces the costs of discrepancies and measurement duplication. The OIML publishes a number of international reports in four categories:\n\nAlthough the OIML has no legal authority to impose its recommendations and guidelines on its member countries, it provides a standardised legal framework for those countries to assist the development of appropriate, harmonised legislation for certification and calibration. OIML provides a mutual acceptance arrangement (MAA) for measuring instruments that are subject to legal metrological control, which upon approval allows the evaluation and test reports of the instrument to be accepted in all participating countries. Issuing participants in the agreement issue MAA Type Evaulation Reports of MAA Certificates upon demonstration of compliance with ISO/IEC 17065 and a peer evaluation system to determine competency. This ensures that certification of measuring devices in one country is compatible with the certification process in other participating countries, allowing the trade of the measuring devices and the products that rely on them.\n\nThe International Laboratory Accreditation Cooperation (ILAC) is an international organisation for accreditators involved in the certification of conformity-assessment bodies. It standardises accreditation practices and procedures, recognising competent calibration facilities and assisting countries developing their own accreditation bodies. ILAC originally began as a conference in 1977 to develop international cooperation for accredited testing and calibration results to facilitate trade. In 2000, 36 members signed the ILAC mutual recognition agreement (MRA), allowing members work to be automatically accepted by other signatories, and in 2012 was expanded to include accrediation of inspection bodies. Through this standardisation, work done in laboratories accredited by signatories is automatically recognised internationally through the MRA. Other work done by ILAC includes promotion of laboratory and inspection body accreditation, and supporting the development of accreditation systems in developing economies.\n\nThe Joint Committee for Guides in Metrology (JCGM) is a committee which created and maintains two metrology guides: \"Guide to the expression of uncertainty in measurement\" (GUM) and \"International vocabulary of metrology - basic and general concepts and associated terms\" (VIM). The JCGM is a collaboration of eight partner organisations:\n\nThe JCGM has two working groups: JCGM-WG1 and JCGM-WG2. JCGM-WG1 is responsible for the GUM, and JCGM-WG2 for the VIM. Each member organization appoints one representative and up to two experts to attend each meeting, and may appoint up to three experts for each working group.\n\nA national measurement system (NMS) is a network of laboratories, calibration facilities and accreditation bodies which implement and maintain a country's measurement infrastructure. The NMS sets measurement standards, ensuring the accuracy, consistency, comparability, and reliability of measurements made in the country. The measurements of member countries of the CIPM Mutual Recognition Arrangement (CIPM MRA), an agreement of national metrology institutes, are recognized by other member countries. As of March 2018, there are 102 signatories of the CIPM MRA, consisting of 58 member states, 40 associate states, and 4 international organizations.\n\nA national metrology institute's (NMI) role in a country's measurement system is to conduct scientific metrology, realise base units, and maintain primary national standards. An NMI provides traceability to international standards for a country, anchoring its national calibration hierarchy. For a national measurement system to be recognized internationally by the CIPM Mutual Recognition Arrangement, an NMI must participate in international comparisons of its measurement capabilities. BIPM maintains a comparison database and a list of calibration and measurement capabilities (CMCs) of the countries participating in the CIPM MRA. Not all countries have a centralised metrology institute; some have a lead NMI and several decentralised institutes specialising in specific national standards. Some examples of NMI's are the National Institute of Standards and Technology (NIST) in the United States, the National Research Council (NRC) in Canada, the Korea Research Institute of Standards and Science (KRISS), and the National Physical Laboratory of India (NPL-India).\n\nCalibration laboratories are generally responsible for calibrations of industrial instrumentation. Calibration laboratories are accredited and provide calibration services to industry firms, which provides a traceability link back to the national metrology institute. Since the calibration laboratories are accredited, they give companies a traceability link to national metrology standards. Examples of calibration laboratories would be ICL Calibration Laboratories, Testo Industrial Services GmbH, and Transcat.\n\nAn organisation is accredited when an authoritative body determines, by assessing the organisation's personnel and management systems, that it is competent to provide its services. For international recognition, a country's accreditation body must comply with international requirements and is generally the product of international and regional cooperation. A laboratory is evaluated according to international standards such as ISO/IEC 17025 general requirements for the competence of testing and calibration laboratories. To ensure objective and technically-credible accreditation, the bodies are independent of other national measurement system institutions. The National Association of Testing Authorities in Australia, the United Kingdom Accreditation Service, and National Accreditation Board for Testing and Calibration Laboratories in India, are examples of accreditation bodies.\n\nMetrology has wide-ranging impacts on a number of sectors, including economics, energy, the environment, health, manufacturing, industry, and consumer confidence. The effects of metrology on trade and the economy are two of its most-apparent societal impacts. To facilitate fair and accurate trade between countries, there must be an agreed-upon system of measurement. Accurate measurement and regulation of water, fuel, food, and electricity are critical for consumer protection and promote the flow of goods and services between trading partners. A common measurement system and quality standards benefit consumer and producer; production at a common standard reduces cost and consumer risk, ensuring that the product meets consumer needs. Transaction costs are reduced through an increased economy of scale. Several studies have indicated that increased standardization in measurement has a positive impact on GDP. In the United Kingdom, an estimated 28.4 percent of GDP growth from 1921 to 2013 was the result of standardisation; in Canada between 1981 and 2004 an estimated nine percent of GDP growth was standardisation-related, and in Germany the annual economic benefit of standardization is an estimated 0.72% of GDP.\n\nLegal metrology has reduced accidental deaths and injuries with measuring devices, such as radar guns and breathalyzers, by improving their efficiency and reliability. Measuring the human body is challenging, with poor repeatability and reproducibility, and advances in metrology help develop new techniques to improve health care and reduce costs. Environmental policy is based on research data, and accurate measurements are important for assessing climate change and environmental regulation. Aside from regulation, metrology is essential in supporting innovation, the ability to measure provides a technical infrastructure and tools that can then be used to pursue further innovation. By providing a technical platform which new ideas can be built upon, easily demonstrated, and shared, measurement standards allow new ideas to be explored and expanded upon.\n\n"}
{"id": "41585002", "url": "https://en.wikipedia.org/wiki?curid=41585002", "title": "Mlpack", "text": "Mlpack\n\nmlpack is a machine learning software library for C++, built on top of the Armadillo library. mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users. Its intended target users are scientists and engineers.\n\nIt is open-source software distributed under the BSD license, making it useful for developing both open source and proprietary software. Releases 1.0.11 and before were released under the LGPL license. The project is supported by the Georgia Institute of Technology and contributions from around the world.\n\nTemplate classes for GRU, LSTM structures are available, thus the library also supports Recurrent Neural Networks.\n\nThere are bindings to R and Python. Its binding system is extensible to other languages.\n\nCurrently \"mlpack\" supports the following algorithms:\n\n\n"}
{"id": "23943695", "url": "https://en.wikipedia.org/wiki?curid=23943695", "title": "Neptec Design Group", "text": "Neptec Design Group\n\nNeptec Design Group is an Ottawa based, Canadian vision systems company, providing machine vision solutions for space, industrial, and military applications. Privately owned and founded in 1990, Neptec is a NASA prime contractor, supplying operational systems to both the Space Shuttle and International Space Station programs. Starting in 2000, Neptec began expanding its technology to include active 3D imaging systems and 3D processing software. This work led directly to the development of Neptec's Laser Camera System, which is an operational system used by NASA to inspect the shuttle's external surfaces during flight. Building on Laser Camera System technology, Neptec has also developed a 3D imaging and tracking system designed for automated on-orbit rendezvous, inspection and docking. The TriDAR combines a high precision, short range triangulation sensor with a long range LIDAR sensor into the same optical path.\n\nThe Laser Camera System (LCS) is short-range, high precision autosynchronous triangulation scanner. The camera uses a laser to measure the distance between itself and points on a target and is able to create a three-dimensional representation of the area it has scanned.\n\nFirst demonstrated on the Shuttle Discovery Mission STS-105 in August 2001, Neptec's prototype LCS was the first dual target tracking and imaging three-dimensional scanner to fly in space. It wasn't until 2004, after the Space Shuttle Columbia tragedy that the LCS became a primary focus to both Neptec and NASA.\n\nTo ensure the safety of future missions, NASA required a means to determine the amount of damage, if any, sustained by a shuttle during the launch phase. In response to this requirement, Neptec Design Group developed the Laser Camera System (LCS) in 14 months. The LCS made its first mission in July 2005 on NASA's STS-114 Return to Flight shuttle mission and was a mandatory system for subsequent shuttle missions.\n\nThe LCS was part of a larger sensor system that is installed on a 50-foot boom extension that provided additional reach for the Remote Manipulator System (Canadarm). This Orbiter Boom Sensor System (OBSS) was used to inspect areas of the shuttle that were previously not visible to the astronauts inside. The LCS was used to scan and characterize the underside of the shuttle while in orbit. While looking at the shuttle's tiles and panels from the height of a first story window, Neptec's scanner was able to detect cracks less than a millimeter thick. Because it was a 3D scanner, it was also able to measure the depth of cracks or holes that were found. The scanner then sent these measurements to Earth where the data was analyzed in detail by Neptec engineers in NASA's Mission Control Center in Houston, Texas. During STS-114, critical on-orbit data was often processed and in the hands of the Space Shuttle mission managers within an hour of being collected on orbit.\n\nNeptec's LCS continued to fly as an operational part of the OBSS on the remaining Space Shuttle Missions.\n\nNeptec LCS Missions:\n"}
{"id": "12462866", "url": "https://en.wikipedia.org/wiki?curid=12462866", "title": "Ordoñezite", "text": "Ordoñezite\n\nOrdoñezite or ordóñezite is a rare tetragonal zinc antimonate mineral with chemical formula: ZnSbO.\n\nOrdóñezite was first discovered and documented by Ezequiel Ordóñez (1867-1950), a Mexican geologist, formerly director of the Geological Institute of Mexico. \nIt was first described in 1953 for an occurrence with cassiterite in veins in rhyolite in the Santín mine which is located about eight kilometres from Santa Catarina, Guanajuato, Mexico. Another locality is El Antimonio, southwest of Agua Prieta, Sonora, Mexico.\n\nOptical properties include: semitransparent, very light to very dark colorless to pearl-gray, light yellowish olive to dark olive.\n\n"}
{"id": "2022213", "url": "https://en.wikipedia.org/wiki?curid=2022213", "title": "PANOSE", "text": "PANOSE\n\nThe PANOSE System is a method for classifying typefaces solely on their visual characteristics, developed by Benjamin Bauermeister. It can be used to identify an unknown font from a sample image or to match a known font to its closest visual neighbor from a font pool. The word \"PANOSE\" is composed of letters taken from the six classes in which the creator of the system organized the Latin alphabet.\n\nThe original PANOSE System was developed in 1985 by Benjamin Bauermeister. In 1988, it was published by Van Nostrand Reinhold Company Inc. under the title \"A Manual of Comparative Typography: The PANOSE System\". This initial version of the PANOSE system consisted of seven classification categories and was based on subjective visual parameters.\n\nIn 1990, the Weight category was added, and the Arm Style category was split off from the Stroke Variation category, bringing the number of classification categories to 9. Objective classification criteria were also added at this time.\n\nIn 1991, the Family Kind category was added, completing the PANOSE 1.0 definition.\n\nIn 1992, there were attempts made to classify Han ideographic typefaces, which allows applications to suggests the most appropriate Latin typeface to match a given Han ideographic typeface. Transliteral mapping could also be used to match between, for example, decorative or script faces and text equivalents.\n\nIn 1993, Mapper Application Interface (MAI) was developed. PANOSE 2.0 was also released in the same year, which is the basis for Hewlett Packard's Infinifont font synthesis technology.\n\nPANOSE was incorporated into a variety of digital font metadata tags in 1992 by ElseWare Corporation. The classification system, its matching algorithms reference databases, classification parameters, and trademarks were purchased by Hewlett Packard in 1995. A font synthesis engine named Infinifont was also purchased by Hewlett Packard at that time.\n\nIt was published in 1988. A PANOSE classification number consists of 10 concatenated values. Each value from a given category was computed from a specific visual metric, such as the weight of the font and the presence or absence of serifs. Special values \"Any\" (0) and \"No Fit\" (1) exist for every category, which have specific meanings to the mapper. \"Any\" means match that digit with any available digit, which allows the mapper to handle distortable typefaces. \"No Fit\" means that the item being classified does not fit within the present system.\n\nThe standard classifies fonts in following categories in following order:\n\n\n\n\n\nFor example, the PANOSE digits for Times New Roman are:\nThe system stores actual measurement data under the Rich Font Description (RFD) rather than bucketing it, which allows the matching system to use mathematical distance rather than penalty tables. It is designed for distortable font technologies (e.g.: Multi Master fonts). The system offers multiple methods for distortable fonts.\n\nThe original classification system was changed from a bucket-based system to an arithmetic system (except the Family from PANOSE 1.0 and derivatives), and was expanded to following categories:\n\nEach PANOSE 2.0 category value is a signed 16-bit number (from −32,768 to 32,767; only ranges between −10,000 and 10,000 are defined), where value zero (0) is considered to be the \"normal\" for the digit. For example, digit zero for the weight represents medium weight. The \"any\" value from PANOSE 1.0 is translated to a \"don't care\" parameter and is replaced by the more comprehensive distortable font descriptions.\n\nThe Family category is replaced by Class and Genre, where Class indicates a font's language and character set, where Genre indicates text faces, display faces, symbol faces, and so on. PANOSE matching software is designed to match fonts with different Class, but same Genre. The same Genre can have different meaning in different Class, so the matching heuristic decides the closeness of fonts based on adjusted values based on Class, rather than raw PANOSE values within the fonts themselves.\n\nClassification Procedures are objective measurement techniques used to assign a PANOSE number to a font.\n\nThe PANOSE Mapper software determines the closest possible font match on any given system by comparing the PANOSE numbers of the requested and available fonts. The individual PANOSE digits are compared, weighted by their typographic importance, and summed to provide a numerical visual distance. Typographic importance is derived by assigning weights to each digit; for example, a font's weight (regular, bold, demibold, etc.) is more important than its contrast (difference between thick and thin strokes).\n\nPANOSE 1.0 table is supported in TrueType font format.\n\nPANOSE 2.0 is used in ElseWare Corporation's Infinifont parametric font generation system.\n\nIn 1996, during the W3C's draft process for CSS1, Hewlett Packard proposed a PANOSE syntax extension for font substitution. It was not included in the final CSS1 recommendation partly because of licensing concerns. Although Hewlett Packard Co. is not interested in profiting from PANOSE, it will negotiate licenses on a time and materials basis.\n\nPANOSE 1.0 is supported in SVG since version 1.0 in the \"font-face\" element under \"panose-1\" attribute. In CSS2, it is used in the \"panose-1\" property, which was removed in CSS2.1, and was reintroduced in \"CSS3 module: Web Fonts\" specifications.\n\nIn Office Open XML, it is part of WordProcessingML.\n\nPANOSE 1.0 is used in Rich Text Format Specification 1.7.\n\n"}
{"id": "40800276", "url": "https://en.wikipedia.org/wiki?curid=40800276", "title": "Patriarchal bargain", "text": "Patriarchal bargain\n\nPatriarchal bargain is a term coined by Turkish author and researcher Deniz Kandiyoti in 1988, which describes a tactic in which a woman chooses to accommodate and uphold patriarchal norms, accepting gender roles that disadvantage women overall but maximizing her own power and options. It has been used to explain why women in patriarchal societies get married, wear veils, and conform to societal norms about sexual behaviour. According to Kandiyoti, patriarchal bargaining changes in nature in different societies based on \"class, caste, and ethnicity.\"\n\nReferences\n"}
{"id": "755413", "url": "https://en.wikipedia.org/wiki?curid=755413", "title": "Pubarche", "text": "Pubarche\n\nPubarche refers to the first appearance of pubic hair at puberty. Pubarche is one of the physical changes of puberty and can occur independently of complete puberty. Pubarche usually results from rising levels of estrogens in females, and androgens in males from the adrenal glands, ovaries, or testes but may also result from exposure to an anabolic steroid.\n\nWhen pubarche occurs prematurely (in early or mid-childhood), it is referred to as \"premature pubarche\" or precocious puberty and may warrant an evaluation. Premature adrenarche is the most common cause of premature pubarche. Early occurrences can arise due to congenital adrenal hyperplasia, androgen-producing tumors of the adrenals or gonads. When adrenarche, central puberty, and all pathologic conditions have been excluded, the term \"isolated premature pubarche\" is used to describe the unexplained development of pubic hair at an early age without other hormonal or physical changes of puberty.\n\nThe average beginning of pubarche varies due to many factors, including climate, nourishment, weight, nurture, and genes. First (and often transient) pubic hair resulting from adrenarche may appear between ages 10-12 preceding puberty.\n\nDuring puberty pubarche tends to occur earlier in girls than boys, as with most puberty stages and puberty as a whole. The average age for females varies between 12-14, and for males, about 13-15. \n\n\n"}
{"id": "8078684", "url": "https://en.wikipedia.org/wiki?curid=8078684", "title": "Reaction step", "text": "Reaction step\n\nA reaction step of a chemical reaction is defined as: \"An elementary reaction, constituting one of the stages of a stepwise reaction in which a reaction intermediate (or, for the first step, the reactants) is converted into the next reaction intermediate (or, for the last step, the products) in the sequence of intermediates between reactants and products\".\n"}
{"id": "1246035", "url": "https://en.wikipedia.org/wiki?curid=1246035", "title": "Research Assessment Exercise", "text": "Research Assessment Exercise\n\nThe Research Assessment Exercise (RAE) was an exercise undertaken approximately every 5 years on behalf of the four UK higher education funding councils (HEFCE, SHEFC, HEFCW, DELNI) to evaluate the quality of research undertaken by British higher education institutions. RAE submissions from each subject area (or \"unit of assessment\") are given a rank by a subject specialist peer review panel. The rankings are used to inform the allocation of quality weighted research funding (QR) each higher education institution receives from their national funding council. Previous RAEs took place in 1986, 1989, 1992, 1996 and 2001. The most recent results were published in December 2008. It was replaced by the Research Excellence Framework in 2014.\n\nVarious media have produced league tables of institutions and disciplines based on the 2008 RAE results. Different methodologies lead to similar but non-identical rankings.\n\nThe first exercise of assessing of research in Higher Education in the UK took place in 1986 under the Margaret Thatcher Government. It was conducted by the University Grants Committee under the chairmanship of the Cambridge mathematician Peter Swinnerton-Dyer. The purpose of the exercise was to determine the allocation of funding to UK Universities at a time of tight budgetary restrictions. The committee received submissions of research statements from 37 subject areas (\"cost centres\") within Universities, along with five selected research outputs. It issued quality rankings labelled \"outstanding\", \"above average\", \"average\" or \"below average\". The research funding allocated to Universities (called \"quality-related\" funding) depended on the quality ratings of the subject areas. According to Swinnerton-Dyer, the objective was to establish a measure of transparency to the allocation of funding at a time of declining budgets.\n\nA subsequent research assessment was conducted in 1989 under the name \"research selectivity exercise\" by the Universities Funding Council. Responding to the complaint of the Universities that they weren't allowed submit their \"full strength,\" Swinnerton-Dyer allowed the submission of two research outputs per every member of staff. The evaluation was also expanded to 152 subject areas (\"units of assessment\"). According to Roger Brown and Helen Carasso, only about 40 per cent of the research-related funding was allocated based on the assessment of the submissions. The rest was allocated based on staff and student numbers and research grant income.\n\nIn 1992, the distinction between Universities and Polytechnics was abolished. The Universities Funding Council was replaced by regionwise funding councils such as the HEFCE. Behram Bekhradnia, the directory of policy at HEFCE, came to the conclusion that the research assessment needed to become \"much more robust and rigorous.\" This led to the institution of the Research Assessment Exercise in 1992. The results of the 1992 results were nevertheless challenged in Court by the Institute of Dental Surgery and the judge warned that the system had to become more transparent. The assessment panels in the subsequent exercises had to be much more explicit about the criteria for evaluation and the working methods. In 1996, all volume-based evaluation was removed to account for the criticism that volume rather than quality was rewarded.\n\nThe 1992 exercise also stipulated that the staff submitted for assessment had to be in post by a specific date (the \"census date\") in order to counter the criticisms that the staff that had moved on were still counted in the assessment. This led to the phenomenon of \"poaching\" of highly qualified staff by other Universities ahead of the census date. In the 2001 exercise, the credit for the staff that moved institutions in the middle of the cycle could be shared between the two institutions. In the 2008 exercise, this was abolished.\n\nThe assessment of 2008 also brought in a major change. Instead of a single grade for an entire subject area (\"unit of assessment\"), a grade was assigned to each research output. This was done to counter the criticism that large departments were able to hide a \"very long tail\" of lesser work and still get high ratings and, conversely, excellent staff in low-graded departments were unable to receive adequate funding. Thus the single grades for units of assessment were replaced by \"quality profiles,\" which indicated the proportion of each department's research against each quality category.\n\nThe 2008 RAE used a four-point quality scale, and returned a profile, rather than a single aggregate quality score, for each unit.\nThe quality levels—based on assessment of research outputs, research environment and indicators of esteem—are defined as:\n\nEach unit of assessment was given a \"quality profile\" - a five-column histogram - indicating the proportion of the research that meets each of four quality levels or is unclassified.\n\nIn 1992, 1996 and 2001, the following descriptions were used for each of the ratings.\n\nThese ratings have been applied to \"units of assessment\", such as French or Chemistry, which often broadly equate to university departments. Various unofficial league tables have been created of university research capability by aggregating the results from units of assessment. Compiling league tables of universities based on the RAE is problematic, as volume and quality are both significant factors.\n\nThe assessment process for the RAE focuses on quality of research outputs (which usually means papers published in academic journals and conference proceedings), research environment, and indicators of esteem. Each subject panel determines precise rules within general guidance. For RAE 2008, institutions are invited to submit four research outputs, published between January 2001 and December 2007, for each full-time member of staff selected for inclusion.\n\nIn response to criticism of earlier assessments, and developments in employment law, the 2008 RAE does more to take into account part-time workers or those new to a sufficient level of seniority to be included in the process.\n\nThe RAE has not been without its critics. In its different iterations, it has divided opinion among researchers, managers and policy makers. Amongst the criticisms is the fact that it explicitly ignores the publications of most full-time researchers in the UK, on the grounds that they are employed on fixed term contracts. According to the RAE 2008 guidelines, most research assistants are \"not eligible to be listed as research active staff\". Publications by researchers on fixed term contracts are excluded from the Assessment Exercise unless those publications can be credited to a member of staff who is eligible for the RAE. This applies even if the member of staff being assessed only made a minor contribution to the article. The opposite pheonomenon is also true, where non-research active staff on permanent contracts, such as lecturers who have been responsible primarily for teaching activities have also found themselves placed under deeper contractual pressure by their employing universities to produce research output. Another issue is that it is doubtful whether panels of experts have the necessary expertise to evaluate the quality of research outputs, as experts perform much less well as soon as they are outside their particular area of specialisation.\n\nSince 1996 the AUT, now incorporated within the UCU, has maintained a policy of opposition to the Research Assessment Exercise. In its view,\nThe official \"Review of Research Assessment,\" the 2003 \"Roberts Report\" commissioned by the UK funding bodies, recommended changes to research assessment, partly in response to such criticisms.\n\nThe House of Commons Science and Technology Select Committee considered the Roberts report, and took a more optimistic view, asserting that, \"\"the RAE had had positive effects: it had stimulated universities into managing their research and had ensured that funds were targeted at areas of research excellence\",\" it concluded that \"there had been a marked improvement in universities' research performance\". Nevertheless, it argued that \"the RAE in its present form had had its day\", and proposed a reformed RAE, largely based on Roberts' recommendations.\n\nIt was announced in the 2006 Budget that after the 2008 exercise a system of metrics would be developed in order to inform future allocations of QR funding. Following initial consultation with the higher education sector, it is thought that the Higher Education Funding Councils will introduce a metrics based system of assessment for subjects in science, technology, engineering and medicine. A process of peer review is likely to remain for mathematics, statistics, arts, humanities and social studies subjects.\n\nHEFCE has developed a new set of arrangements, known as the Research Excellence Framework (REF), which has been introduced as a follow on to the 2008 RAE.\n\n"}
{"id": "26566069", "url": "https://en.wikipedia.org/wiki?curid=26566069", "title": "Research development", "text": "Research development\n\nResearch development (RD) is a set of strategic, proactive, catalytic, and capacity-building activities designed to facilitate individual faculty members, teams of researchers, and central research administrations in attracting extramural research funding, creating relationships, and developing and implementing strategies that increase institutional competitiveness. These activities are typically practiced at universities, but are also in use at a variety of other research institutions.\n\nResearch development includes a diverse set of dynamic activities that vary by institution. These activities include initiating and nurturing partnerships, networks, and alliances between and among faculty at their institutions and funding agencies; and designing and implementing strategic services for their faculty and researcher constituents (such as workshops, trainings, program officer visits, proposal editing, PR communications, funding opportunity searches and dissemination, budget preparation, forms and submission assistance, research team building, and administering campus limited submission reviews).\n\nResearch development professionals initiate and nurture critical partnerships and alliances throughout the institutional research enterprise and between institutions and with their external stakeholders. With the goal of enabling competitive individual and team research and facilitating research excellence, research development professionals build and implement strategic services and collaborative resources that span across disciplinary and administrative barriers within their organizations and beyond.\n\nResearch development differs significantly from university development (institutional fundraising or advancement) in that RD is not aimed at attracting contributions or donations. Rather, RD strengthens research programs and proposals to make them more competitive for extramural contracts and grants from governmental, private and non-profit funding agencies. Similarly, RD should not be confused with research and development (R&D) which refers to investments in (often) corporate scientific and technological research that leads to new products and applications.\n\nRecent contractions in the availability of public and private research funding have intensified competition for fewer resources among universities. This trend has amplified the need for research development assistance and interventions at universities in order to enhance research excellence and competitiveness. These services have not traditionally been offered through university-sponsored research and projects offices that administer the submission of grant proposals and research funds management. In response to these challenges, research development is increasingly becoming a standard practice at universities, particularly research universities (defined, by the Carnegie Classification of Institutions of Higher Education, as universities that place a high priority on research and rely heavily on extramural funding).\n\nResearch development professionals and services are typically housed in a university's central \"Office of Research\" (or similar), or within a more specific department or research unit devoted to a particular discipline or school. Research development activities are also included in some sponsored research and projects offices. There are also independent consulting firms that provide research development services. According to the US-based National Organization of Research Development Professionals (NORDP), there are currently over 800 research development professionals employed at over 300 institutions (colleges/universities, teaching/not-for-profit hospitals, independent not-for-profit research organizations, national laboratories, research organizations wholly organized and administered by a college or university, consortia of colleges and universities, associations/societies with individual or institutional members predominantly from colleges and universities) across the United States and several foreign countries.\n\nStrategic Research Advancement\n\nCommunication of Research and Research Opportunities\n\nEnhancement of Collaboration/Team Science\n\nProposal Support Functions\nAssisting faculty to find funding opportunities\n\nThe National Organization of Research Development Professionals (NORDP) was established in 2010 as part of a grassroots movement to build a peer community of research development professionals. The organization grew from an informal network of over 100 individuals engaged in research development activities at universities and research institutions across the United States. The central goals of NORDP are to serve these professionals, by providing a formal organization to support their professional development, to enhance institutional research competitiveness, and to catalyze new research and institutional collaborations.\n\nTeam science is \"scientific collaboration, i.e., research conducted by more than one individual in an interdependent fashion\". It can involve work by small teams, but the term is more commonly used to describe large multi-, inter- and transdisciplinary collaborative research projects by large teams of scientists that often integrate research with broader goals including education, technology transfer, outreach and diversity enhancement. Research development is an activity that many universities have embraced to enhance the efforts of their faculty and foster the development of collaborative, team-based science as well as compete for large research center and consortia funding opportunities. Research development professionals serve as \"rainmakers\" who catalyze and facilitate team science in response to the external funding landscape.\n\n"}
{"id": "57741272", "url": "https://en.wikipedia.org/wiki?curid=57741272", "title": "Rnn (software)", "text": "Rnn (software)\n\nrnn is an open-source machine learning framework that implements Recurrent Neural Network architectures, such as LSTM and GRU, natively in the R programming language.\n\nThe rnn package is distributed through the Comprehensive R Archive Network under the open-source GPL v3 license.\n\nThe below example from the rnn documentation show how to train a recurrent neural network to solve the problem of bit-by-bit binary addition.\n\nThe sigmoid functions and derivatives used in the package were originally included in the package, from version 0.8.0 onwards, these were released in a separate R package sigmoid, with the intention to enable more general use. The sigmoid package is a dependency of the rnn package and therefore automatically installed with it.\n\nWith the release of version 0.3.0 in April 2016 the use in production and research environments became more widespread. The package was reviewed several months later on the R blog The Beginner Programmer as \"R provides a simple and very user friendly package named rnn for working with recurrent neural networks.\", which further increased usage.\n\nThe book Neural Networks in R by Balaji Venkateswaran and Giuseppe Ciaburro uses rnn to demonstrate recurrent neural networks to R users.\n\nThe RStudio CRAN mirror download logs\n, with a total of over 20,000 downloads since the first release\n, according to RDocumentation.org, this puts the package in the 15th percentile of most popular R packages\n\n"}
{"id": "6507954", "url": "https://en.wikipedia.org/wiki?curid=6507954", "title": "Self-organising heuristic", "text": "Self-organising heuristic\n\nIn computing, a Self-organising heuristic is an algorithm that modifies a data structure such as a linked list in response to use of the data structure.\n\nExamples might be:\n\n\n\"Move to front\", or \"Order by access frequency\", might be used to organize a cache of information, so that frequently used, or recently used information is at the top (and so can be found quickly, without having to traverse the whole list). \n\n\"Order by frequency\" might be used to re-arrange a list of options in a GUI menu, so that the top ones are the ones most commonly selected by the user.\n\n\"Re-insert at random\" or \"Move to back\" might be used to organise a list of mirror servers, so that once a server has been used for downloading, it goes to the back of the queue, to discourage the user from selecting it again.\n"}
{"id": "1183062", "url": "https://en.wikipedia.org/wiki?curid=1183062", "title": "Steven Emerson", "text": "Steven Emerson\n\nSteven Emerson (born June 6, 1954) is an American journalist, author, and pundit on national security, terrorism, and Islamic extremism.\n\nSome have called Emerson a terrorism and intelligence expert, while critics have said that he is an Islamophobe.\n\nEmerson received a Bachelor of Arts from Brown University in 1976, and a Master of Arts in sociology in 1977. He went to Washington, D.C., in 1977 with the intention of putting off his law school studies for a year. He worked on staff as an investigator for the U.S. Senate Foreign Relations Committee until 1982, and as an executive assistant to Democratic Senator Frank Church of Idaho.\n\nEmerson was a freelance writer for \"The New Republic\", for whom he wrote a series of articles in 1982 on the influence of Saudi Arabia on U.S. corporations, law firms, public-relations outfits, and educational institutions. In their pursuit of large contracts with Saudi Arabia, he argued, U.S. businesses became unofficial, unregistered lobbyists for Saudi interests. He expanded this material in 1985 in his first book, \"The American House of Saud: The Secret Petrodollar Connection\". Emerson has contributed commentaries to Newsmax since July 2009, covering terrorism-related topics.\n\nFrom 1986 to 1989 he worked for \"U.S. News and World Report\" as a senior editor specializing in national security issues. In 1988, he published \"Secret Warriors: Inside the Covert Military Operations of the Reagan Era\", a strongly critical review of Ronald Reagan-era efforts to strengthen U.S. covert capabilities. Reviewing the book, \"The New York Times\" wrote: \"Among the grace notes of Mr. Emerson's fine book are many small, well-told stories\".\nIn 1990, he co-authored \"The Fall of Pan Am 103: Inside the Lockerbie Investigation\", which argued for the then-mainstream theory that Iran was behind the bombing of Pan Am Flight 103. Reviewing the book, \"The New York Times\" wrote: \"Mr. Emerson and Mr. Duffy have put together a surpassing account of the investigation to date, rich with drama and studded with the sort of anecdotal details that give the story the appearance of depth and weight.\" The newspaper listed it as an \"editors' choice\" on their Best Sellers List, and cited it as a \"notable book of the year\".\n\nIn 1990, he joined CNN as an investigative correspondent and continued to write about terrorism. In 1991, he published \"Terrorist: The Inside Story of the Highest-Ranking Iraqi Terrorist Ever to Defect to the West\", detailing how Iraq spread and increased its terror network in the 1980s with U.S. support.\n\nEmerson left CNN in 1993 to work on a documentary, \"\", for the Public Broadcasting Service (PBS). It aired as \"a PBS special\" in November 1994.\n\nIn the documentary, he stood in front of the Twin Towers and warned: \"The survivors of the explosion at the World Trade Center in 1993 are still suffering from the trauma, but as far as everyone else is concerned, all this was a spectacular news event that is over. Is it indeed over? The answer is: apparently not. A network of Muslim extremists is committed to a \"jihad\" against America. Their ultimate aim is to establish a Muslim empire.\"\n\nEmerson noted at the outset that \"the overwhelming majority of Muslims are not members of militant groups.\" But the message of the documentary was that Muslim organizations have ties with militants who preach violence against moderate Muslims, as well as against Christians and Jews, and that charitable contributions to those organizations inevitably become \"extremist.\" He documented meetings in American hotels at which Muslims called for a holy war, raised funds for \"terror\" organizations. He also filmed Muslim-American youth training with weapons in summer camps, and interviewed supporters of terror who he claims operated under the cover of charitable organizations.\n\nHe showed videos of Muslim speakers such as Abdullah Azzam in Brooklyn urging his audience to wage \"jihad\" in America (which Azzam explains \"means fighting only, fighting with the sword\"), Fayiz Azzam (a cousin of Abdullah) telling an Atlanta audience: \"Blood must flow. There must be widows; there must be orphans, hands and limbs must be severed, and limbs and blood must be spread everywhere in order that Allah's religion can stand on its feet\",\n\nand Sheik Omar Abdel-Rahman in Detroit (later convicted of conspiring to blow up several New York City landmarks, and sentenced to life in prison) calling for \"jihad\" against the infidel. Sheik Mohammed Al-Asi of Chicago said: \"If the Americans are placing their forces in the Persian Gulf, we should be creating another war front for the Americans in the Muslim world,\" and at a November 1993 Hamas rally in New Jersey hundreds chanted: \"We buy paradise with the blood of the Jews.\"\n\nEmerson added that: \"As the activities of Muslim radicals expand in the United States, future attacks seem inevitable. Combating these groups within the boundaries of the Constitution will be the greatest challenge to law enforcement since the war on organized crime.\"\n\nThe Council on American-Islamic Relations (CAIR), a Muslim organization in Washington, noted that PBS denied requests by Arab and Muslim journalists to screen the program before its showing, and argued that Emerson was promoting \"a wild theory about an Islamic terrorist network in America\". Writing for \"The New York Times\", Walter Goodman opined that the request to change or cancel the documentary was not justified, but that the concerns about Emerson's claims of an Islamic terrorist network were justified \"since 'Jihad in America' is likely to awaken viewers' unease over what some Muslim groups here may be up to.\"\n\nAfter the film aired in South Africa, Emerson said that the Federal Bureau of Investigation (FBI) informed him that a South African Muslim group had dispatched a team to the U.S. to assassinate him. According to \"Slate\", people who visit his Washington, D.C., office are blindfolded \"en route\", and employees call it \"the bat cave\". \n\nHe received the 1994 George Polk Award for \"Best Television Documentary.\" He also received the top prize for best investigative report from the Investigative Reporters and Editors Organization (IRE).\n\nA review of the book by \"The New York Times's \" Ethan Bronner, says that conservatives and some Jewish organizations took Emerson seriously, but that others have dismissed him as \"an obsessive crusader\", and concludes that while Emerson sometimes connects unrelated dots, occasionally he can be wrong; but that as an investigator focusing on radical Islamic groups in the US, his information should be taken seriously but not just at face value.\n\nEmerson elaborated on this subject in his 2006 book, \"Jihad Incorporated: A Guide to Militant Islam in the U.S.\"\n\nWhile he has testified congressional committees on such topics including Al Qaeda, Hamas, Hezbollah and Islamic Jihad, some of his more recent statements concerning Muslims in the US and Europe have been criticized for inaccuracies; in particular, some of his claims during a Fox News segment about the relationship between British Muslims and the city of Birmingham were subsequently rebuked by the then British Prime Minister David Cameron and led to a censure of Fox News by Ofcom for the airing of the comments which the broadcasting regulator characterized as \"materially misleading\" and \"a serious breach for a current affairs programme\".\n\nIt was Emerson's 1994 documentary \"Jihad in America\" that first linked Sami Al-Arian to the Palestinian Islamic Jihad (PIJ). When in February 2003 the U.S. indicted Al-Arian, accusing him of being the North American leader of PIJ and financing and helping support suicide bombings, \"The New York Times\" noted that Emerson \"has complained about Mr. Al-Arian's activities in the United States for nearly a decade.\" In 2006, Al-Arian pleaded guilty to conspiracy to help a \"specially designated terrorist\" organization, PIJ, and was sentenced to 57 months in prison, after a jury deadlocked on 9 charges (8 of which the government agreed to drop as part of the plea bargain) and acquitted him on another 8. Al-Arian said that he knew of the terrorist group's violent acts, though no evidence was admitted at trial showing that he was involved with violent acts.\n\nIn 1995 CBS interviews, prior to any knowledge the bombing of the Oklahoma City Federal Building was perpetrated by Timothy McVeigh, Emerson said \"Oklahoma City, I can tell you, is probably considered one of the largest centers of Islamic radical activity outside the Middle East\", and that the bombing \"was done with the intent to inflict as many casualties as possible. That is a Middle Eastern trait, and something that has been generally not carried out on this soil until we were rudely awakened to it in 1993\". He also told viewers not to believe Islamic groups' denials of their involvement. Emerson has said some critics fail to recite the rest of his statement that references the 1993 World Trade Center attack which was also carried out with a fertilizer truck bomb. Emerson indicated that he was one of many experts interviewed after the bombing who concluded there were similarities between the Oklahoma City bombing and Middle Eastern terrorism. He said the initial reporting did not \"tar the entire Muslim community\", that he referred only to a fanatical minority in the Islamic community. He acknowledged there were outbreaks of harassment which he referred to as unfortunate. In response to claims that all Muslims were blamed Emerson said \"the charge of racism against Muslims is a canard designed to justify radical Islamic activities in this country.\" He supported the media's decision to report the possible link to Middle East terrorism, saying \"There was no doubt\" that the FBI and other law enforcement agencies suspected it.\n\nIn testimony on March 19, 1996, to the Senate Foreign Relations Committee, Emerson described the Holy Land Foundation as \"the main fund-raising arm for Hamas in the United States.\" In 2007, federal prosecutors brought charges against Holy Land for funding Hamas and other Islamic terrorist organizations. In 2009, the founders of Holy Land were given life sentences for \"funneling $12 million to Hamas.\"\n\nIn early 1997, Emerson told the \"Middle East Quarterly\" that the threat of terrorism \"is greater now than before the World Trade Center bombing [in 1993] as the numbers of these groups and their members expands. In fact, I would say that the infrastructure now exists to carry off twenty simultaneous World Trade Center-type bombings across the United States.\"\n\nOn February 24, 1998, Emerson testified before the Senate Judiciary Committee: \"The foreign terrorist threat in the United States is one of the most important issues we face... We now face distinct possibilities of mass civilian murder the likes of which have not been seen since World War II.\" And just a few months before 9/11, he wrote on May 31, 2001: \"Al-Qaeda is ... planning new attacks on the US... [It has] learned, for example, how to destroy large buildings... Al-Qaeda and other terrorist groups ... have silently declared war on the US; in turn, we must fight them as we would in a war.\"\n\nIn January 2001 it was reported that Emerson pointed out that the U.S. had missed clues that would have allowed it to focus on al-Qaeda early on. One of the men convicted in the World Trade Center bombing, Ahmad Ajaj, returned to the U.S. from Pakistan in 1992 with a bomb manual later seized by the U.S. An English translation of the document, entered into evidence in the World Trade Center trial, said that the manual was dated 1982, that it had been published in Amman, Jordan, and that it carried a heading on the front and succeeding pages: \"The Basic Rule\". But those were all errors, as Emerson pointed out. The heading said \"al-Qaeda\" – which translates as \"The Base\". In addition, the document was published in 1989, a year after al-Qaeda was founded, and the place of publication was Afghanistan, not Jordan.\n\nIn 2010, \"The New York Times\" quoted Emerson criticizing the Obama administration's solicitation of Muslim and Arab-American organizations such as the Islamic Society of North America, which was listed as an unindicted co-conspirator in a 2008 case against the Holy Land Foundation for Relief and Development, whose leaders were convicted of funneling money to Hamas, saying: \"I think dialogue is good, but it has to be with genuine moderates. These are the wrong groups to legitimize.\" ISNA denies any links to terrorism.\n\nThe Investigative Project on Terrorism was founded by Emerson in 1995, shortly after the release of his documentary film, \"\", which first aired in the United States in 1994 on PBS. The documentary was faulted for misrepresentation, and Robert Friedman accused Emerson of \"creating mass hysteria against American Arabs.\"\n\nIPT maintains a data center which includes archival information relating to the past activities of known Islamic terrorist groups. They also investigate suspected funding activities and networks of Islamic extremists in the US and abroad. IPT obtains information from a variety of sources, including \"websites, list-serves, publications, informants, undercover recordings, government records, court documents, and so on\". IPT has provided useful evidence to law enforcement and government agencies, and occasionally provides testimonial evidence during special committee hearings of the US Congress. IPT has been criticized by various proponents of Islam. The liberal think-tank, Center for American Progress (CAP), stated that the IPT was one of ten foundations constituting what it called \"the Islamophobia network in America\".\n\nIn January 2014, former congressman and chairman of the House Intelligence Committee, Pete Hoekstra, was named the Shillman Senior Fellow for IPT specializing in national security, international relations, global terrorism and cyber security.\n\nEmerson is also the founder and Executive Director of the Investigative Project on Terrorism, a large intelligence archive on Islamist groups around the world. In 1995, he incorporated his company, SAE Productions, in Delaware, and also established his private think-tank, The Investigative Project, to conduct investigations into radical Islamist groups and terrorist activities. Since September 2001, Emerson has testified numerous times before committees of both houses of Congress on terrorist funding, and the operational structures of groups including al-Qaeda, Hamas, Hezbollah, and Islamic Jihad. He has also given interviews debunking 9/11 conspiracy theories, and was a contributing expert to the \"Counterterrorism Blog\".\n\nIn a television interview after the 1995 Oklahoma City bombing, Emerson incorrectly pointed at Muslim terrorists, suggesting the bombing showed a Middle Eastern trait. Emerson has stated that he was chastened by the experience and learned a lesson. Christopher Bail, author of \"Terrified: How Anti-Muslim Fringe Organizations Became Mainstream\", postulated that Emerson's doctoring of FBI evidence in his film \"Terrorists Among Us\", his speculation about Muslim involvement in the Oklahoma City bombing and widespread criticism from Muslim American organizations resulted in most major media outlets abandoning IPT until the 2001 September 11 attacks.\n\nIn March 2004, \"Newsweek\" ran an article detailing the high level of reliance Richard Clarke, placed on Emerson's information, in lieu of that of the FBI. Clarke wrote in his book \"Against All Enemies\" that Emerson's \"American Jihad\", \"told me more than the FBI ever had about radical Islamic groups in the U.S.\"\n\nIn April 2006, Emerson organized The Investigative Project on Terrorism Foundation as a nonprofit organization, and serves as its executive director. In January 2007, the IRS granted the organization tax exempt status. The organization's nonprofit status received a great deal of scrutiny from critics. According to an article published in the Tennessean by Bob Smietana, allegations of ties between the newly organized charity, and Emerson's for-profit company, SAE, were brought to the attention of the IRS. It was alleged that the foundation's tax free dollars were being funneled to Emerson's production company in violation of the law. A spokesperson for Emerson's SAE Productions said the approach had already been vetted by the group's lawyers and declared legal, that it was set up that way for security reasons, and he further explained that Emerson does not take any profits from SAE Productions. No formal charges were made, or disciplinary actions taken against Emerson. The foundation maintained its nonprofit status.\n\nAccording to Deepa Kumar in her book \"Islamophobia and the Politics of Empire\", Emerson's Investigative Project on Terrorism, together with David Yerushalmi's Society of Americans for National Existence, have forwarded the notion that there is a conspiracy by Muslims to take over the US, that Muslims have infiltrated its society, making no distinctions between Muslims and Islamists, and contend that Muslim Americans have ties to terrorist organizations and want to institute Sharia law in the United States.\n\nIn November 2016, the Project posted a 2010 speech given by U.S. Rep. Keith Ellison, who is being considered to lead the Democratic National Committee, in which he said that United States foreign policy in the Middle East \"is governed by what is good or bad through a country of 7 million people. A region of 350 million all turns on a country of 7 million. Does that make sense? Is that logic?\"\n\nAccording to an article in the Middle East Forum's \"Middle East Quarterly\", \"the IPT has access to information and intelligence to which the government is not privy, and has been instrumental in shutting down more than a dozen Islamic charitable terrorist and nonviolent front-groups since 2001.\"\n\nOn December 2001, CBS: \"48 Hours\" - Erin Moriarity interviewed Steven Emerson, Executive Director of IPT, for the CBS television documentary series, \"48 Hours\". The episode, \"Target Terrorism\", was broadcast on January 30, 2002. Emerson said that Sami al-Arian was running an organization in the United States that \"was one and the same as the Islamic Jihad\". In February 2003, Arian was indicted for alleged fundraising and material support activities on behalf of terrorist organizations, including Hamas and Palestinian Islamic Jihad (PIJ). According to the \"Tampa Bay Times\", Arian signed a plea agreement in which he admitted to \"conspiring to help people associated with Palestinian Islamic Jihad\" and covering up his knowledge of the PIJ associations by lying to Jim Harper, a St. Petersburg reporter covering Al-Arian in the mid-1990s, and others.\n\nIn the 2007 and 2008 Holy Land Foundation Trials - prosecution relied on evidence produced by IPT, one of the three groups responsible for much of the analysis of exhibits and the links from Holy Land Foundation (HLF) to Hamas, the Muslim Brotherhood (MB), and the extended MB network. On May 27, 2009, in federal court in Dallas, \"U.S. District Judge Jorge A. Solis sentenced the Holy Land Foundation for Relief and Development (HLF) and five of its leaders following their convictions by a federal jury in November 2008 on charges of providing material support to Hamas, a designated foreign terrorist organization.\" As a result of IPT's vast archives on the activities of Hamas front groups in the United States Law enforcement officials commented that IPT had an instrumental role in prosecuting and convicting the Holy Land Foundation, a trial that resulted in sweeping convictions for all defendants in 2008.\n\nThe fund-raising arm of the Investigative Project on Terrorism is the Investigative Project on Terrorism Foundation, a 501(c)(3) tax-exempt organization established in 2006 by Steven Emerson. The Foundation is operated for the most part by SAE Productions, a Delaware-based company that was also founded by Emerson in 1994. According to an officer of SAE Productions, the arrangement avoids the need for the kind of public disclosure associated with tax-exemption and is necessary for security reasons: \"The very nature of our work mandates that we protect the organization and its staff from threats posed by those that are the subject or our research by preserving the confidentiality of our methods.\"\n\nAn article by Bob Smietana in the \"Nashville Tennessean\" says that money is transferred from the non-profit foundation to the for-profit production company, SAE. In 2008, the non-profit paid $3,390,000 to SAE Productions for whats was described as \"management services\", while Emerson was SAE's sole officer. IPT published a statement in response noting that, \"At issue in the Tennessean story is the relationship between the IPT Foundation, a tax-exempt charity, and SAE Productions, a for-profit company run by IPT Executive Director Steven Emerson. The foundation accepts private donations and contracts with SAE to manage operations.N\"\nIPT has stated that it \"accepts no funding from outside the United States, or from any governmental agency or political or religious institutions\". In 2002 and 2003, Emerson received a total of $600,000 in grants from the Smith Richardson Foundation, a conservative-leaning policy research foundation.\n\nEmerson has been referred to by \"The New York Times\" as \"an expert on intelligence\", and a \"self-described terrorism expert\", and by the \"New York Post\" as \"the nation's foremost journalistic expert on terrorism\" The \"Los Angeles Times\" referred to Emerson as a terrorism expert, and as a Fox News commentator.\n\nRichard Clarke, former head of counter-terrorism for the United States National Security Council, said of Emerson, \"I think of Steve as the Paul Revere of terrorism ... We'd always learn things [from him] we weren’t hearing from the FBI or CIA, things which almost always proved to be true.\"\n\nPhilip Jenkins, in his 2003 book, \"Images of terror: what we can and can't know about terrorism\" responded that certain groups criticize Emerson in order to silence and delegitimize his views.\n\nStephen Suleyman Schwartz wrote an article defending Emerson that attempted to explain why Islamists dislike him.\n\nA review by Michael Wines in \"The New York Times\" of \"The Fall of Pan Am 103,\" while noting that the authors were \"respected journalists\" and \"not to be lightly dismissed,\" and that they \"talked to 250 people, including senior law enforcement and intelligence officials in seven nations\", opined that charges of Iranian complicity were presented \"without much substantiation\" although Wines did go on to say that: \"They build a convincing circumstantial case against Iran and its terrorist agents.\"\n\nAdrienne Edgar, writing in \"The New York Times Book Review\" described Emerson and Cristina del Sesto's 1991 book \"Terrorist\", as \"marred by factual errors (such as mistranslations of Arabic names) and marked by \"a pervasive anti-Arab and anti-Palestinian bias.\" Emerson and del Sesto responded: \"We defy anyone to point to any passages that suggest such bias... these characterizations of the book are wild figments of Ms. Edgar's political imagination.\"\n\nIn their report \"Fear, Inc.: The Roots of the Islamophobia Network in America\", the Center for American Progress accused Emerson of being an \"misinformation expert\" who, through his testimonies, exaggerates the presence of Sharia law in America and terrorism sympathizers in mosques.\n\nEmerson has been criticized for espousing Islamophobic views by Islamic studies sholars such as Juliane Hamer and Omid Safi, with German media scholar , and Carl Ernst naming Emerson along with Daniel Pipes as the two most prominent Islamophobic voices in the US. Emerson responded to these and similar characterizations in an op-ed for Fox News, stating that criticism of Islam labeled as Islamphophia, and the labeling of \"Islamic terrorism\" as a racist generalization of Muslims, is \"one of the biggest and most dangerous national security frauds of the past 30 years.\"\n\nEmerson's work was cited as an instance of poor reporting on Islam in the Sut Jhally film about Edward Said's \"Orientalism\", specifically his claim after the Oklahoma City bombing that the municipality was a center of Muslim extremism.\n\nEmerson has played a role in criminal prosecutions. In the widely criticized Sami Al-Arian case he was a major source of information and advice to the federal prosecutors and the \"Tampa Tribune\". He has a close relationship to Gordon Kromberg, a federal prosecutor in the Eastern District of Virginia. The Holy Land Foundation prosecution relied on evidence produced by Emerson's Investigative Project.\n\nOn April 17, 2013, Emerson stated on the Fox News program \"Hannity\" that he had been informed by an official in the US Immigration and Customs Enforcement (ICE) that a Saudi national who was present during the Boston Marathon bombing was suspected of playing a role in the bombing. Emerson wondered why a suspect would be deported and not prosecuted. Emerson reasoned that United States handles Saudi nationals differently to appease Saudi Arabia and not to embarrass the country. Homeland Security Secretary Janet Napolitano, whose department supervises the ICE, dismissed Emerson's allegation during a meeting with the House Homeland Security Committee, as being incorrect. United States officials stated that the injured Saudi national was regarded as a witness and not a suspect. A Saudi official at the embassy also stated that there was no known suspect or person of interest that they were aware of. On April 19, 2013, Steve Emerson was featured in an opinion piece on Fox News and referred to the suspects, Tamerlan Tsarnaev and Dzhokhar Tsarnaev, YouTube channels as being similar in tone to Al Qaeda videos. Many local, state and federal officials, including President Barack Obama, cautioned against jumping to conclusions while there's an ongoing investigation.\n\nIn January 2015, following terrorist attacks in Paris, France, Emerson stated in an interview on Fox News that the city of Birmingham was populated entirely by Muslims and was a \"no go area\" for non-Muslims. According to an estimate from the UK Census of 2011, Birmingham is estimated to have 21.8% of its population identify as Muslim, with a Christian population of 46%, and 25% claiming no religion or not giving a religion. In the same interview, he claimed that in London, \"Muslim religious police 'beat' anyone who doesn't dress according to Muslim, religious Muslim attire\". The errors led to four apologies within 12 hours by Fox News. The UK media regulatory authority Ofcom found Fox News to be in breach of the UK's broadcasting code on account of the comments. Ofcom described the comments as \"materially misleading\" and \"a serious breach\".\n\nIn response to these comments, British Prime Minister David Cameron said that he \"choked on his porridge\" when he heard them and observed that Emerson was \"clearly a complete idiot\". Local MP Gisela Stuart described Emerson's remarks as \"stupid\" and that they had \"no redeeming features\". Emerson's remarks, which \"embarrassed\" Fox, extended to other countries, especially regarding supposed exclusion zones in Paris.\n\nEmerson issued an apology for his misinformation stating, \"I have clearly made a terrible error for which I am deeply sorry. My comments about Birmingham were totally in error.\" He further added that he would make a donation to a charity in Birmingham and also place a newspaper ad in Birmingham. It was also reported that Birmingham City Council welcomed his apology, describing Emerson's comments as \"curious\" and clearly without foundation. Sir Albert Bore, the leader of the council mocked Emerson writing \"As I arrived for work at the Council House this morning I was full of awe and admiration for the many commuters who braved the 'no-go area' that is now Birmingham city centre\" and described Emerson's remarks as \"stupid, untrue and damaging...ridiculous\".\n\n\n\n\n\n"}
{"id": "24028642", "url": "https://en.wikipedia.org/wiki?curid=24028642", "title": "Symbol (formal)", "text": "Symbol (formal)\n\nA logical symbol is a fundamental concept in logic, tokens of which may be marks or a configuration of marks which form a particular pattern. Although the term \"symbol\" in common use refers at some times to the idea being symbolized, and at other times to the marks on a piece of paper or chalkboard which are being used to express that idea; in the formal languages studied in mathematics and logic, the term \"symbol\" refers to the idea, and the marks are considered to be a token instance of the symbol. In logic, symbols build literal utility to illustrate ideas.\n\nSymbols of a formal language need not be symbols \"of\" anything. For instance there are logical constants which do not refer to any idea, but rather serve as a form of punctuation in the language (e.g. parentheses). Symbols of a formal language must be capable of being specified without any reference to any interpretation of them.\n\nA symbol or string of symbols may comprise a well-formed formula if it is consistent with the formation rules of the language.\n\nIn a formal system a symbol may be used as a token in formal operations. The set of formal symbols in a formal language is referred to as an alphabet (hence each symbol may be referred to as a \"letter\")\n\nA formal symbol as used in first-order logic may be a variable (member from a universe of discourse), a constant, a function (mapping to another member of universe) or a predicate (mapping to T/F).\n\nFormal symbols are usually thought of as purely syntactic structures, composed into larger structures using a formal grammar, though sometimes they may be associated with an interpretation or model (a formal semantics).\n\nThe move to view units in natural language (e.g. English) as formal symbols was initiated by Noam Chomsky (it was this work that resulted in the Chomsky hierarchy in formal languages). The generative grammar model looked upon syntax as autonomous from semantics. Building on these models, the logician Richard Montague proposed that semantics could also be constructed on top of the formal structure:\nThis is the philosophical premise underlying Montague grammar.\n\nHowever, this attempt to equate linguistic symbols with formal symbols has been challenged widely, particularly in the tradition of cognitive linguistics, by philosophers like Stevan Harnad, and linguists like George Lakoff and Ronald Langacker.\n\n"}
{"id": "31066", "url": "https://en.wikipedia.org/wiki?curid=31066", "title": "The Third Culture", "text": "The Third Culture\n\nThe Third Culture: Beyond the Scientific Revolution is a 1995 book by John Brockman which discusses the work of several well-known scientists who are directly communicating their new, sometimes provocative, ideas to the general public. John Brockman has continued the themes of 'The Third Culture' in the website of the Edge Foundation, where leading scientists and thinkers contribute their thoughts in plain English.\n\nThe title of the book refers to Charles Percy Snow's 1959 work \"The Two Cultures and the Scientific Revolution\", which described the conflict between the cultures of the humanities and science.\n\n23 people were included in the 1995 book:\n\nThe book influenced the reception of popular scientific literature in parts of the world beyond the United States. In Germany, the book inspired several newspapers to integrate scientific reports into their \"Feuilleton\" or \"culture\" sections (such as the \"Frankfurter Allgemeine Zeitung\"). At the same time, the assertions of the book were discussed as a source of controversy, especially the implicit assertion that \"third culture thinking\" is mainly an American development. Critics acknowledge that, whereas in the Anglo-Saxon cultures there is a large tradition of scientists writing popular books, such tradition was absent for a long period in the German and French languages, with journalists often filling the gap. However, some decades ago there were also scientists, like the physicists Heisenberg and Schrödinger and the psychologist Piaget, who fulfill the criteria Brockman named for \"third culture.\" The German author Gabor Paal suggested that the idea of the \"third culture\" is a rather modern version of what Hegel called Realphilosophie (\"philosophy of the real\").\n\nAlso, already during the interwar period, Otto Neurath and other members of the Vienna Circle strongly propagated the need for both the unity of science and the popularization of new scientific concepts. With the rise of the Nazis in Germany and Austria, many of the Vienna Circle's members left for the United States where they taught in several universities, causing their philosophical ideas to spread in the Anglo-Saxon world throughout the 1930s-1940s.\n\n\n\n"}
{"id": "43118579", "url": "https://en.wikipedia.org/wiki?curid=43118579", "title": "The minimal counterintuitiveness effect", "text": "The minimal counterintuitiveness effect\n\nCognitive anthropologist Pascal Boyer argued that minimally counterintuitive concepts (MCI) i.e., concepts that violate a few ontological expectations of a category such as the category of an agent, are more memorable than intuitive and maximally counterintuitive (MXCI) concepts. A number of experimental psychology studies have found support for Boyer's hypothesis. Upal labelled this as the minimal counterintuitiveness effect or the MCI-effect.\n\nBoyer originally did not precisely specify the number of expectation-violations that would render an idea maximally counterintuitive. Early empirical studies including those by Boyer himself and others did not study MXCI concepts. Both these studies only used concepts violating a single expectation (which were labelled as MCI concepts). Atran was the first to study memory for MXCI concepts and labeled concepts violating 2-expectations as maximally counterintuitive. Studies by the I-75 Cognition and Culture Group also labelled ideas violating two expectations as maximally counterintuitive. Barrett argued that ideas violating 1 or 2 ontological expectations should be considered MCI and only ideas violating 3 or more expectations should be labelled MXCI. Subsequent studies of the MCI effect have followed this revised labelling scheme.\n\nUpal has divided the cognitive accounts that explain the MCI effect into two categories: the Context-based model of minimal counterintuitiveness, and content-based view of minimal counterintuitiveness. The context-based view emphasizes the role played by context in making an idea counterintuitive whereas the content-based view ignores the role of context.\n\n"}
{"id": "40077384", "url": "https://en.wikipedia.org/wiki?curid=40077384", "title": "Weber problem", "text": "Weber problem\n\nIn geometry, the Weber problem, named after Alfred Weber, is one of the most famous problems in location theory. It requires finding a point in the plane that minimizes the sum of the transportation costs from this point to \"n\" destination points, where different destination points are associated with different costs per unit distance.\n\nThe Weber problem generalizes the geometric median, which assumes transportation costs per unit distance are the same for all destination points, and the problem of computing the Fermat point, the geometric median of three points. For this reason it is sometimes called the Fermat–Weber problem, although the same name has also been used for the unweighted geometric median problem. The Weber problem is in turn generalized by the attraction–repulsion problem, which allows some of the costs to be negative, so that greater distance from some points is better.\n\nIn the triangle case, the Fermat problem consists in locating a point D with respect to three points A, B, and C in such a way that the sum of the distances between D and each of the three other points is minimized. It was formulated by the famous French mathematician Pierre de Fermat before 1640, and it can be seen as the true beginning of both location theory, and space-economy. Torricelli found a geometrical solution to this problem around 1645, but it still had no direct numerical solution more than 325 years later. Kuhn and Kuenne found an iterative solution for the general Fermat problem in 1962, and, in 1972, Tellier found a direct numerical solution to the Fermat triangle problem, which is trigonometric. Kuhn and Kuenne's solution applies to the case of polygons having more than three sides, which is not the case with Tellier's solution for reasons explained further on.\n\nThe Weber problem consists, in the triangle case, in locating a point D with respect to three points A, B, and C in such a way that the sum of the transportation costs between D and each of the three other points is minimized. The Weber problem is a generalization of the Fermat problem since it involves both equal and unequal attractive forces (see below), while the Fermat problem only deals with equal attractive forces. It was first formulated, and solved geometrically in the triangle case, by Thomas Simpson in 1750. It was later popularized by Alfred Weber in 1909. Kuhn and Kuenne's iterative solution found in 1962, and Tellier's solution found in 1972 apply to the Weber triangle problem as well as to the Fermat one. Kuhn and Kuenne's solution applies also to the case of polygons having more than three sides.\n\nIn its simplest version, the attraction-repulsion problem consists in locating a point D with respect to three points A, A and R in such a way that the attractive forces exerted by points A and A, and the repulsive force exerted by point R cancel each other out as it must do at the optimum. It constitutes a generalization of both the Fermat and Weber problems. It was first formulated and solved, in the triangle case, in 1985 by Luc-Normand Tellier. In 1992, Chen, Hansen, Jaumard and Tuy found a solution to the Tellier problem for the case of polygons having more than three sides.\n\nEvangelista Torricelli’s geometrical solution of the Fermat triangle problem stems from two observations:\n\n1– point D is at its optimal location when any significant move out of that location induces a net increase of the total distance to reference points A, B, and C, which means that the optimal point is the only point where an infinitesimal movement towards one of the three reference points induces a reduction of the distance to that point that is equal to the sum of the induced changes in the distances to the two other points; in fact, in the Fermat problem, the advantage to reduce the distance from A by one kilometer is equal to the advantage to reduce the distance from B by one kilometer or the distance from C by the same length; in other words, the activity to be located at D is equally attracted by A, B, and C;\n\n2– according to an important theorem of Euclidean geometry, in a convex quadrilateral inscribed in a circle, the opposite angles are supplementary (that is their sum is equal to 180°); that theorem can also take the following form: if we cut a circle with a chord AB, we get two circle arcs, let us say AiB and AjB; on arc AiB, any ∠AiB angle is the same for any chosen point i, and, on arc AjB, all the ∠AjB angles are also equal for any chosen point j; moreover, the ∠AiB and ∠AjB angles are supplementary.\n\nIt can be proved that the first observation implies that, at the optimum, the angles between the AD, BD, and CD straight lines must be equal to 360° / 3 = 120°. Torricelli deduced from that conclusion that:\n\n1– if any triangle ABD, whose ∠ADB angle is equal to 120°, generates an ABDE convex quadrilateral inscribed in a circle, the ∠ABE angle of the ABE triangle must be equal to (180° − 120°)= 60°;\n\n2– one way to determine the set of locations of D for which the ∠ADB angle is equal to 120° is to draw an equilateral ABE triangle (because each angle of an equilateral triangle is equal to 60°), where E is located outside the ABC triangle, and draw a circle round that triangle; then all the D’ points of the circumference of that circle that lie within the ABC circle are such that the ∠AD’B angle is equal to 120°;\n\n3– the same reasoning can be made with respect to triangles ACD, and BCD;\n\n4– this leads to draw two other equilateral triangles ACF and BCG, where F and G are located outside the ABC triangle, as well as two other circles round these equilateral triangles, and to determine the location where the three circles intersect; at that location, the angles between the AD, BD, and CD straight lines is necessarily equal to 120°, which proves that it is the optimal location.\n\nSimpson's geometrical solution of the so-called “Weber triangle problem” (which was first formulated by Thomas Simpson in 1750) directly derives from Torricelli's solution. Simpson and Weber stressed the fact that, in a total transportation minimization problem, the advantage to get closer to each attraction point A, B or C depends on what is carried and on its transportation cost. Consequently, the advantage of getting one kilometer closer to A, B or C varies, and the ∠ADB, ∠ADC and ∠BDC angles no more need to be equal to 120°.\n\nSimpson demonstrated that, in the same way as, in the Fermat triangle problem case, the constructed triangles ABE, ACF and BCG were equilateral because the three attractive forces were equal, in the Weber triangle problem case, the constructed triangles ABE, ACF and BCG, where E, F and G are located outside the ABC triangle, must be proportional to the attractive forces of the location system.\n\nThe solution is such that:\n\n1– in the constructed triangle ABE, the AB side is proportional to the attractive force w pointing towards C, the AE side is proportional to the attractive force w pointing towards B, and the BE side is proportional to the attractive force w pointing towards A;\n\n2– in the constructed triangle BCG, the BC side is proportional to the attractive force w pointing towards A, the BG side is proportional to the attractive force w pointing towards B, and the CG side is proportional to the attractive force w pointing towards C;\n\n3– the optimal point D is located at the intersection of the two circumferences drawn round the ABE and BCG constructed triangles.\n\nA third triangle of forces ACF, where F is located outside the ABC triangle, can be drawn based on the AC side, and a third circumference can be traced round that triangle. That third circumference crosses the two previous ones at the same point D.\n\nA geometrical solution exists for the attraction-repulsion triangle problem. Its discovery is rather recent. That geometrical solution differs from the two previous ones since, in this case, the two constructed force triangles overlap the AAR location triangle (where A and A are attraction points, and R, a repulsion one), while, in the preceding cases, they never did.\n\nThis solution is such that:\n\n1– in the constructed triangle RAH, which partly overlaps the AAR location triangle, the RA side is proportional to the attractive force w pointing towards A, the RH side is proportional to the attractive force w pointing towards A, and the AH side is proportional to the repulsive force w pushing away from point R;\n\n2– in the constructed triangle RAI, which partly overlaps the AAR location triangle, the RA side is proportional to the attractive force w pointing towards A, the RI side is proportional to the attractive force w pointing towards A, and the AI side is proportional to the repulsive force w pushing away from point R;\n\n3– the optimal point D is located at the intersection of the two circumferences drawn round the RAH and RAI constructed triangles.\nThis solution is useless if one of the forces is greater than the sum of the two other ones or if the angles are not compatible. In some cases, no force is larger than the two other ones, and the angles are not compatible; then, the optimal location lies at the point that exerts the greater attractive force.\n\nMore than 332 years separate the first formulation of the Fermat triangle problem and the discovery of its non-iterative numerical solution, while a geometrical solution existed for almost all that period of time. Is there an explanation for that? That explanation lies in the possibility of the origins of the three vectors oriented towards the three attraction points not coinciding. If those origins do coincide and lie at the optimum location P, the vectors oriented towards A, B and C, and the sides of the ABC location triangle form the six angles ∠1, ∠2, ∠3, ∠4, ∠5, and ∠6, and the three vectors form the ∠α, ∠α and ∠α angles. It is easy to write the following six equations linking six unknowns (the angles ∠1, ∠2, ∠3, ∠4, ∠5, and ∠6) with six known values (angles ∠A, ∠B, and ∠C, whose values are given, and angles ∠α, ∠α and ∠α, whose values depend only on the relative magnitude of the three attractive forces pointing towards the A, B and C attraction points):\n\nUnfortunately, this system of six simultaneous equations with six unknowns is undetermined, and the possibility of the origins of the three vectors oriented towards the three attraction points not coinciding explains why. In the case of non-coincidence, we observe that all the six equations are still valid. However, the optimal location P has disappeared because of the triangular hole that exists inside the triangle. In fact, as Tellier (1972) has shown, that triangular hole had exactly the same proportions as the “forces triangles” we drew in Simpson's geometrical solution.\n\nIn order to solve the problem, we must add to the six simultaneous equations a seventh requirement, which states that there should be no triangular hole in the middle of the location triangle. In other words, the origins of the three vectors must coincide.\n\nTellier's solution of the Fermat and Weber triangle problems involves three steps:\n\n1– Determine the angles ∠α, ∠α and ∠α that are such that the three attractives forces w, w and w cancel each other to ensure equilibrium. This is done by means of the following independent equations:\n\n2– Determine the value of angle ∠3 (this equation derives from the requirement that point D must coincide with point E):\n\nwhere k = (CB/CA) (sin ∠α / sin ∠α), and k’ = (∠A +∠B + ∠α) − 180° ;\n\n3– Solve the following system of simultaneous equations where ∠3 is now known:\n\nTellier (1985) extended the Fermat–Weber problem to the case of repulsive forces. Let us examine the triangle case where there are two attractive forces w and w, and one repulsive force w. Here as in the previous case, the possibility exists for the origins of the three vectors not to coincide. So the solution must require their coinciding. Tellier's trigonometric solution of this problem is the following:\n\n1– Determine angle ∠e :\n\n2– Determine angle ∠p :\n\n3– Determine angle ∠c :\n\n4– Determine angle ∠d :\n\n5– Determine the value of angle ∠3 (this equation derives from the requirement that point D must coincide with point E):\nwhere x = sin ∠f – (RA/RA)(sin ∠d sin [∠e − ∠b] / sin ∠c) ; \nand y = (RA/RA)(sin ∠d cos [∠e − ∠b] / sin ∠c) − cos ∠f ;\n\n6– Determine ∠1 :\n\n7– Determine ∠5 :\n\n8– Determine ∠2 :\n\nWhen the number of forces is larger than three, it is no longer possible to determine the angles separating the various forces without taking into account the geometry of the location polygon. Geometric and trigonometric methods are then powerless. Iterative optimizing methods are used in such cases. Kuhn and Kuenne (1962) suggested an algorithm based on iteratively reweighted least squares generalizing Weiszfeld's algorithm for the unweighted problem. Their method is valid for the Fermat and Weber problems involving many forces, but not for the attraction–repulsion problem. In this method, to find an approximation to the point \"y\" minimizing the weighted sum of distances\nan initial approximation to the solution \"y\" is found, and then at each stage of the algorithm is moved closer to the optimal solution by setting \"y\" to be the point minimizing the sum of weighted squared distances\nwhere the initial weights \"w\" of the input points are divided by the distances from each point to the approximation from the previous stage.\nAs the unique optimal solution to a weighted least squares problem, each successive approximation may be found as a weighted average:\n\nFor the attraction–repulsion problem one has instead to resort to the algorithm proposed by Chen, Hansen, Jaumard and Tuy (1992).\n\nIn the world of spatial economics, repulsive forces are omnipresent. Land values are the main illustration of them. In fact a substantial portion of land value theory, both rural and urban, can be summed up in the following way.\n\nIn the case where everybody is attracted by a single attraction point (the rural market or the urban central business district), competition between the various bidders who all want to locate at the center will generate land values that will transform the unique attraction point of the system into a repulsion point from the land value point of view, and, at the equilibrium, each inhabitant and activity will be located at the point where the attractive and the repulsive forces exerted by the center on them will cancel out.\n\nThe Tellier problem preceded the emergence of the New Economic Geography. It is seen by Ottaviano and Thisse (2005) as a prelude to the New Economic Geography (NEG) that developed in the 1990s, and earned Paul Krugman a Nobel Memorial Prize in Economic Sciences in 2008. The concept of attractive force is akin to the NEG concept of agglomeration or centripetal force, and the concept of repulsive force is akin to the NEG concept of dispersal or centrifugal force.\n\n"}
{"id": "57987170", "url": "https://en.wikipedia.org/wiki?curid=57987170", "title": "Yasmine Belkaid", "text": "Yasmine Belkaid\n\nYasmine Belkaid (born 1968) is an Algerian immunologist and senior investigator at the National Institute of Allergy and Infectious Diseases (NIAID), as well as an adjunct professor at the University of Pennsylvania. She is best known for her work studying host-microbe interactions in tissues, uncovering factors controlling immune regulation to microbes. Belkaid currently serves as the director of the NIAID Microbiome program.\n\nBelkaid was born and raised in Algiers, Algeria. She received her Bachelor of Science and master's degrees in biochemistry from the University of Sciences and Technology Houari Boumediene. She then received her Master of Advanced Studies from University of Paris-Sud. She received her PhD in immunology from the Pasteur Institute in 1996, where she studied innate immune responses to \"Leishmania\" infection. Following graduate school, she moved to the United States for a postdoctoral fellowship at NIAID's Laboratory of Parasitic Diseases. In 2002, she joined the faculty of the Division of Molecular Immunology in Cincinnati Children's Hospital Medical Center before returning to NIAID in 2005 as a tenure-track investigator in the Laboratory of Parasitic Diseases. In 2008, she became adjunct Professor of Pathology and Laboratory Medicine at the University of Pennsylvania.\n\nBelkaid's research group is working to understand the mechanisms underlying host-microbe interactions in the gastrointestinal tract and on the skin, which are natural barrier sites between the host's inner workings and their external environment. More specifically, Belkaid's team is investigating the role microbiota play in promoting immunity against infection against other harmful pathogens. While microbes are often thought of as harmful infectious agents, her work has contributed to our understanding of the role that microbiota play in immunity—a beneficial relationship known as commensalism whereby the host actually benefits from the microorganisms that live on or in them. One way microbes shield us from harmful pathogens is by competing for space within our bodies, which stops more harmful invaders from setting up shop inside us. Her group has further contributed to our understanding of how the host immune system can distinguish good microbes from the bad.\n\nBelkaid's group found that certain skin microbes play an important role in immune defense. These commensal microbes interact with the skin's immune cells and allow them to produce a certain cell-signaling molecule that is needed to protect against harmful microbes. They carried out this experiment using mice that had no naturally-occurring microbes in their skin or gut so they could colonize those mice with only one strain of \"good\" bacteria. They then infected the colonized and bacteria-free mice with a parasite and found that those without the \"good\" bacteria were unable to fight back against the parasite, while those with the bacteria mounted an effective immune response. Her team has also found that beneficial bacteria living on the surface of the skin can also accelerate wound healing in mice.\n\nBelkaid's group also studies what happens when there are imbalances in our microbiome. A number of diseases are caused by our immune systems running out of whack. Given the roll the microbiome plays in host immunity, Belkaid has played a major part in advancing our understanding of how shifts in microbiota can contribute to disease, particularly chronic inflammatory diseases like Crohn's disease and Psoriasis. Changes in diet and antibiotic use have tinkered with our microbiota composition and may have played a role in increasing the prevalence of certain chronic inflammatory diseases.\n\n"}
