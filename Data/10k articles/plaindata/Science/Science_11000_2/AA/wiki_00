{"id": "145169", "url": "https://en.wikipedia.org/wiki?curid=145169", "title": "ASCI White", "text": "ASCI White\n\nASCI White was a supercomputer at the Lawrence Livermore National Laboratory in California.\n\nIt was a computer cluster based on IBM's commercial RS/6000 SP computer. 512 of these machines were connected together for ASCI White, with 16 processors per node and 8,192 375 MHz processors in total with 6 terabytes of memory and 160 terabytes of disk storage. It was almost exclusively used for large-scale computations requiring dozens, hundreds or thousands of processors. The computer weighed 106 tons and consumed 3 MW of electricity with a further 3 MW needed for cooling. It had a theoretical processing speed of 12.3 teraflops. The system ran IBM's AIX operating system.\n\nASCI White was made up of three individual systems, the 512 node White, the 28 node Ice and the 68 node Frost.\n\nThe system was built in Poughkeepsie, New York. Completed in June 2000 it was transported to specially built facilities in California and officially dedicated on August 15, 2001. Claimed performance was 12,300 gigaflops, although this was not achieved in the widely accepted LINPACK tests. The system cost $110 million.\n\nIt was built as stage three of the Accelerated Strategic Computing Initiative (ASCI) started by the U.S. Department of Energy and the National Nuclear Security Administration to build a simulator to replace live WMD testing following the moratorium on testing started by President George H. W. Bush in 1992 and extended by Bill Clinton in 1993.\n\nThe machine was decommissioned beginning July 27, 2006.\n\n \n"}
{"id": "1665333", "url": "https://en.wikipedia.org/wiki?curid=1665333", "title": "Abyssal zone", "text": "Abyssal zone\n\nThe abyssal zone or abyssopelagic zone is a layer of the pelagic zone of the ocean. \"Abyss\" derives from the Greek word , meaning bottomless. At depths of , this zone remains in perpetual darkness. These regions are also characterized by continuous cold and lack of nutrients. The abyssal zone has temperatures around through the large majority of its mass. It is the deeper part of the \"midnight zone\" which starts in the bathypelagic waters above.\n\nThe area below the abyssal zone is the sparsely inhabited hadal zone. The zone above is the bathyal zone.\n\nThe deep trenches or fissures that plunge down thousands of metres below the ocean floor (for example, the midoceanic trenches such as the Mariana Trench in the Pacific) are almost unexplored. Previously, only the bathyscaphe \"Trieste\", the remote control submarine \"Kaikō\" and the \"Nereus\" have been able to descend to these depths. However, as of March 25, 2012 one vehicle, the \"Deepsea Challenger\" was able to penetrate to a depth of 10,898.4 metres (35,756 ft).\n\nOverall, the deep sea is a very food-limited environment. In the abyssal zone, biomass is directly related to the amount of food either supplied from transporting ocean currents or from the water column above. The biological pump is thought of as the main driver for cycling organic matter (e.g. particulate organic carbon or dissolved organic carbon) from the atmosphere into the surface ocean, and eventually to the deep ocean.\n\nDue to the fact that organisms in the abyssal zone have no access to sunlight, they rely heavily on nutrients sinking from above (known as marine snow). These nutrients can be in the form of sediments, algae, fecal pellets, or fish carcasses, to name a few. Spatially, the abundance of organisms in the deep ocean is hypothesized to be highest where the surface productivity is the highest, as long as there is efficient export from surface ocean to deep ocean. Studies have shown that marine snow alone does not supply an adequate amount of nutrients to support the population of benthic organisms. However, population booms of algae or animals near the surface ocean can result in heavy pulses of particulate organic matter that, in a few weeks, will deliver as many nutrients as would normally be delivered over decadal timescales of marine snow transport.\n\nSome sea floor locations, such as mid-ocean ridges, are unique in that they contain hydrothermal vents. These vents emit geothermically reduced sulfur compounds that allow for microbial primary production, sustaining many benthic organisms in absence of the sunlight required for photosynthesis.\n\n"}
{"id": "14947257", "url": "https://en.wikipedia.org/wiki?curid=14947257", "title": "Acta Sociologica", "text": "Acta Sociologica\n\nActa Sociologica is a quarterly peer-reviewed academic journal covering all areas of sociology. It is an official journal of the Nordic Sociological Association and was established in 1955. It publishes papers on original research, book reviews, and essays and focusses on research comparing Nordic countries with one another or with other countries.\n\n\"Acta Sociologica\" is abstracted and indexed in Scopus and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2016 impact factor is 1.225, ranking it 57th out of 143 journals in the category \"Sociology\".\n\n"}
{"id": "155544", "url": "https://en.wikipedia.org/wiki?curid=155544", "title": "Charles's law", "text": "Charles's law\n\nCharles's law (also known as the law of volumes) is an experimental gas law that describes how gases tend to expand when heated. A modern statement of Charles's law is:\n\nWhen the pressure on a sample of a dry gas is held constant, the Kelvin temperature and the volume will be in direct proportion.\n\nThis directly proportional relationship can be written as:\nor\nwhere:\n\nThis law describes how a gas expands as the temperature increases; conversely, a decrease in temperature will lead to a decrease in volume. For comparing the same substance under two different sets of conditions, the law can be written as:\nThe equation shows that, as absolute temperature increases, the volume of the gas also increases in proportion.\n\nThe law was named after scientist Jacques Charles, who formulated the original law in his unpublished work from the 1780s.\n\nIn two of a series of four essays presented between 2 and 30 October 1801, John Dalton demonstrated by experiment that all the gases and vapours that he studied expanded by the same amount between two fixed points of temperature. The French natural philosopher Joseph Louis Gay-Lussac confirmed the discovery in a presentation to the French National Institute on 31 Jan 1802, although he credited the discovery to unpublished work from the 1780s by Jacques Charles. The basic principles had already been described by Guillaume Amontons and Francis Hauksbee a century earlier.\n\nDalton was the first to demonstrate that the law applied generally to all gases, and to the vapours of volatile liquids if the temperature was well above the boiling point. Gay-Lussac concurred. With measurements only at the two thermometric fixed points of water, Gay-Lussac was unable to show that the equation relating volume to temperature was a linear function. On mathematical grounds alone, Gay-Lussac's paper does not permit the assignment of any law stating the linear relation. Both Dalton's and Gay-Lussac's main conclusions can be expressed mathematically as:\nwhere \"V\" is the volume occupied by a given sample of gas at 100 °C; \"V\" is the volume occupied by the same sample of gas at 0 °C; and \"k\" is a constant which is the same for all gases at constant pressure. This equation does not contain the temperature and so has nothing to do with what became known as Charles' Law. Gay-Lussac's value for \"k\" (), was identical to Dalton's earlier value for vapours and remarkably close to the present-day value of . Gay-Lussac gave credit for this equation to unpublished statements by his fellow Republican citizen J. Charles in 1787. In the absence of a firm record, the gas law relating volume to temperature cannot be named after Charles.\nDalton's measurements had much more scope regarding temperature than Gay-Lussac, not only measuring the volume at the fixed points of water, but also at two intermediate points. Unaware of the inaccuracies of mercury thermometers at the time, which were divided into equal portions between the fixed points, Dalton, after concluding in Essay II that in the case of vapours, “any elastic fluid expands nearly in a uniform manner into 1370 or 1380 parts by 180 degrees (Fahrenheit) of heat”, was unable to confirm it for gases.\n\nCharles' law appears to imply that the volume of a gas will descend to zero at a certain temperature (−266.66 °C according to Gay-Lussac's figures) or −273.15 °C. Gay-Lussac was clear in his description that the law was not applicable at low temperatures:\n\nbut I may mention that this last conclusion cannot be true except so long as the compressed vapours remain entirely in the elastic state; and this requires that their temperature shall be sufficiently elevated to enable them to resist the pressure which tends to make them assume the liquid state.\n\nAt absolute zero temperature the gas possesses zero energy and hence the molecules restrict motion.\nGay-Lussac had no experience of liquid air (first prepared in 1877), although he appears to have believed (as did Dalton) that the \"permanent gases\" such as air and hydrogen could be liquified. Gay-Lussac had also worked with the vapours of volatile liquids in demonstrating Charles' law, and was aware that the law does not apply just above the boiling point of the liquid:\n\nI may however remark that when the temperature of the ether is only a little above its boiling point, its condensation is a little more rapid than that of atmospheric air. This fact is related to a phenomenon which is exhibited by a great many bodies when passing from the liquid to the solid state, but which is no longer sensible at temperatures a few degrees above that at which the transition occurs.\nThe first mention of a temperature at which the volume of a gas might descend to zero was by William Thomson (later known as Lord Kelvin) in 1848:\n\nThis is what we might anticipate, when we reflect that infinite cold must correspond to a finite number of degrees of the air-thermometer below zero; since if we push the strict principle of graduation, stated above, sufficiently far, we should arrive at a point corresponding to the volume of air being reduced to nothing, which would be marked as −273° of the scale (−100/.366, if .366 be the coefficient of expansion); and therefore −273° of the air-thermometer is a point which cannot be reached at any finite temperature, however low.\n\nHowever, the \"absolute zero\" on the Kelvin temperature scale was originally defined in terms of the second law of thermodynamics, which Thomson himself described in 1852. Thomson did not assume that this was equal to the \"zero-volume point\" of Charles' law, merely that Charles' law provided the minimum temperature which could be attained. The two can be shown to be equivalent by Ludwig Boltzmann's statistical view of entropy (1870).\n\nHowever, Charles also stated:\n\nThe kinetic theory of gases relates the macroscopic properties of gases, such as pressure and volume, to the microscopic properties of the molecules which make up the gas, particularly the mass and speed of the molecules. In order to derive Charles' law from kinetic theory, it is necessary to have a microscopic definition of temperature: this can be conveniently taken as the temperature being proportional to the average kinetic energy of the gas molecules, :\n\nUnder this definition, the demonstration of Charles' law is almost trivial. The kinetic theory equivalent of the ideal gas law relates \"PV\" to the average kinetic energy:\n\n\n\n"}
{"id": "2313150", "url": "https://en.wikipedia.org/wiki?curid=2313150", "title": "Cognitive ethology", "text": "Cognitive ethology\n\nCognitive ethology is a branch of ethology concerned with the influence of conscious awareness and intention on the behaviour of an animal. Donald Griffin, a zoology professor in the United States, set up the foundations for researches in the cognitive awareness of animals within their habitats.\n\nThe fusion of cognitive science and classical ethology into cognitive ethology \"emphasizes observing animals under more-or-less natural conditions, with the objective of understanding the evolution, adaptation (function), causation, and development of the species-specific behavioral repertoire\" (Niko Tinbergen 1963).\n\nAccording to Jamieson & Bekoff (1993), \"Tinbergen's four questions about the evolution, adaptation, causation and development of behavior can be applied to the cognitive and mental abilities of animals.\" Allen & Bekoff (1997, chapter 5) attempt to show how cognitive ethology can take on the central questions of cognitive science, taking as their starting point the four questions described by Barbara Von Eckardt in her 1993 book \"What is Cognitive Science?\", generalizing the four questions and adding a fifth. Kingstone, Smilek & Eastwood (2008) suggested that cognitive ethology should include human behavior. They proposed that researchers should firstly study how people behave in their natural, real world environments and then move to the lab. Anthropocentric claims for the ways non-human animals interact in their social and non-social worlds are often used to influence decisions on how the non-human animals can or should be used by humans.\n\nTraditionally, cognitive ethologists have questioned research methods that isolate animals in unnatural surroundings and present them with a limited set of artificial stimuli, arguing that such techniques favor the study of artificial issues that are not relevant to an understanding of the natural behavior of animals. However, many modern researchers favor a judicious combination of field and laboratory methods.\n\nBekoff, M and Allen, C (1997) \"identify three major groups of people (among some of whose members there are blurred distinctions) with different views on cognitive ethology, namely, slayers, skeptics, and proponents.\" The latter seemingly convergent with animal rights thinking in seeing animal experience as worthy in itself.\n\nEthicist Peter Singer is an example of a \"proponent\" in this sense, as is biologist E. O. Wilson who coined the term biophilia to describe the basis of a direct moral cognition, that 'higher' animals would use to perceive moral implications in the environment directly.\n\nAccording to Marc Bekoff, there are three different views towards whether a science of cognitive ethology is even possible. Slayers deny any possibility of success in cognitive ethology, proponents keep an open mind about animal cognition and the utility of cognitive ethological investigation, and skeptics stand somewhere in between.\n\n\n"}
{"id": "3738572", "url": "https://en.wikipedia.org/wiki?curid=3738572", "title": "Combustion analysis", "text": "Combustion analysis\n\nCombustion analysis is a method used in both organic chemistry and analytical chemistry to determine the elemental composition (more precisely empirical formula) of a pure organic compound by combusting the sample under conditions where the resulting combustion products can be quantitatively analyzed. Once the number of moles of each combustion product has been determined the empirical formula or a partial empirical formula of \nthe original compound can be calculated. \n\nApplications for combustion analysis involve only the elements of carbon (C), hydrogen (H), nitrogen (N), and sulfur (S) as combustion of materials containing them convert these elements to their oxidized form (CO, HO, NO or NO, and SO) under high temperature high oxygen conditions. Notable interests for these elements involve measuring total nitrogen in food or feed to determine protein percentage, measuring sulfur in petroleum products, or measuring total organic carbon (TOC) in water.\n\nThe method was invented by Joseph Louis Gay-Lussac. Justus von Liebig studied the method while working with \nGay-Lussac between 1822 and 1824 and improved the method in the following years to a level that it could be used as standard procedure for organic analysis.\n\nA combustion train is an analytical tool for the determination of elemental composition of a chemical compound. With knowledge of elemental composition a chemical formula can be derived. The combustion train allows the determination of carbon and hydrogen in a succession of steps:\n\n\nAnalytical determination of the amounts of water and carbon dioxide produced from a known amount of sample gives the empirical formula. For every hydrogen atom in the compound 1/2 equivalent of water is produced, and for every carbon atom in the compound 1 equivalent of carbon dioxide is produced.\n\nNowadays, modern instruments are sufficiently automated to be able to do these analyses routinely. Samples required are also extremely small — 0.5 mg of sample can be sufficient to give satisfactory CHN analysis.\n\nThe water vapor, carbon dioxide and other products can be separated via gas chromatography and analysed via a thermal conductivity detector.\n\n"}
{"id": "25303792", "url": "https://en.wikipedia.org/wiki?curid=25303792", "title": "Curvaton", "text": "Curvaton\n\nThe curvaton is a hypothetical elementary particle which mediates a scalar field in early universe cosmology. It can generate fluctuations during inflation, but does not itself drive inflation, instead it generates curvature perturbations at late times after the inflaton field has decayed and the decay products have redshifted away, when the curvaton is the dominant component of the energy density. It is used to generate a flat spectrum of CMB perturbations in models of inflation where the potential is otherwise too steep or in alternatives to inflation like the pre-Big Bang scenario.\n\nThe model was proposed almost simultaneously in 2001 by three independent groups: Kari Enqvist and Martin S. Sloth, David Wands and David H. Lyth, Takeo Moroi and Tomo Takahashi.\n\n"}
{"id": "66109", "url": "https://en.wikipedia.org/wiki?curid=66109", "title": "Eclipse cycle", "text": "Eclipse cycle\n\nEclipses may occur repeatedly, separated by certain intervals of time: these intervals are called eclipse cycles. The series of eclipses separated by a repeat of one of these intervals is called an eclipse series.\n\nEclipses may occur when the Earth and the Moon are aligned with the Sun, and the shadow of one body cast by the Sun falls on the other. So at new moon, when the Moon is in conjunction with the Sun, the Moon may pass in front of the Sun as seen from a narrow region on the surface of the Earth and cause a solar eclipse. At full moon, when the Moon is in opposition to the Sun, the Moon may pass through the shadow of the Earth, and a lunar eclipse is visible from the night half of the Earth. Conjunction and opposition of the Moon together have a special name: syzygy (from \"Greek\" for \"junction\"), because of the importance of these lunar phases.\n\nAn eclipse does not happen at every new or full moon, because the plane of the orbit of the Moon around the Earth is tilted with respect to the plane of the orbit of the Earth around the Sun (the ecliptic): so as seen from the Earth, when the Moon is nearest to the Sun (new moon) or at largest distance (full moon), the three bodies usually are not exactly on the same line.\n\nThis inclination is on average about 5° 9′, much larger than the apparent \"mean\" diameter of the Sun (32′ 2″), the Moon as seen from the surface of the Earth right beneath the Moon (31′ 37″), and the shadow of the Earth at the mean lunar distance (1° 23′).\n\nTherefore, at most new moons the Earth passes too far north or south of the lunar shadow, and at most full moons the Moon misses the shadow of the Earth. Also, at most solar eclipses the apparent angular diameter of the Moon is insufficient to fully obscure the solar disc, unless the Moon is near its perigee, \"i.e.\" closer to the Earth and apparently larger than average. In any case, the alignment must be close to perfect to cause an eclipse.\n\nAn eclipse can only occur when the Moon is close to the plane of the orbit of the Earth, i.e. when its ecliptic latitude is small. This happens when the Moon is near one of the two nodes of its orbit on the ecliptic at the time of the syzygy. Of course, to produce an eclipse, the Sun must also be near a node at that time: the same node for a solar eclipse, or the opposite node for a lunar eclipse.\n\nUp to three eclipses may occur during an eclipse season, a one- or two-month period that happens twice a year, around the time when the Sun is near the nodes of the Moon's orbit.\n\nAn eclipse does not occur every month, because one month after an eclipse the relative geometry of the Sun, Moon, and Earth has changed.\n\nAs seen from the Earth, the time it takes for the Moon to return to a node, the draconic month, is less than the time it takes for the Moon to return to the same ecliptic longitude as the Sun: the synodic month. The main reason is that during the time that the Moon has completed an orbit around the Earth, the Earth (and Moon) have completed about of their orbit around the Sun: the Moon has to make up for this in order to come again into conjunction or opposition with the Sun. Secondly, the orbital nodes of the Moon precess westward in ecliptic longitude, completing a full circle in about 18.60 years, so a draconic month is shorter than a sidereal month. In all, the difference in period between synodic and draconic month is nearly days. Likewise, as seen from the Earth, the Sun passes both nodes as it moves along its ecliptic path. The period for the Sun to return to a node is called the eclipse or draconic year: about 346.6201 d, which is about year shorter than a sidereal year because of the precession of the nodes.\n\nIf a solar eclipse occurs at one new moon, which must be close to a node, then at the next full moon the Moon is already more than a day past its opposite node, and may or may not miss the Earth's shadow. By the next new moon it is even further ahead of the node, so it is less likely that there will be a solar eclipse somewhere on Earth. By the next month, there will certainly be no event.\n\nHowever, about 5 or 6 lunations later the new moon will fall close to the opposite node. In that time (half an eclipse year) the Sun will have moved to the opposite node too, so the circumstances will again be suitable for one or more eclipses.\n\nThese are still rather vague predictions. However we know that if an eclipse occurred at some moment, then there will occur an eclipse again \"S\" synodic months later, \"if\" that interval is also \"D\" draconic months, where \"D\" is an integer number (return to same node), or an integer number + ½ (return to opposite node). So an eclipse cycle is any period \"P\" for which approximately holds:\n\nGiven an eclipse, then there is likely to be another eclipse after every period \"P\". This remains true for a limited time, because the relation is only approximate.\n\nAnother thing to consider is that the motion of the Moon is not a perfect circle. Its orbit is distinctly elliptic, so the lunar distance from Earth varies throughout the lunar cycle. This varying distance changes the apparent diameter of the Moon, and therefore influences the chances, duration, and type (partial, annular, total, mixed) of an eclipse. This orbital period is called the anomalistic month, and together with the synodic month causes the so-called \"full moon cycle\" of about 14 lunations in the timings and appearances of full (and new) Moons. The Moon moves faster when it is closer to the Earth (near perigee) and slower when it is near apogee (furthest distance), thus periodically changing the timing of syzygies by up to ±14 hours (relative to their mean timing), and changing the apparent lunar angular diameter by about ±6%. An eclipse cycle must comprise close to an integer number of anomalistic months in order to perform well in predicting eclipses.\n\nThese are the lengths of the various types of months as discussed above (according to the lunar ephemeris ELP2000-85, valid for the epoch J2000.0; taken from (\"e.g.\") Meeus (1991) ):\n\nNote that there are three main moving points: the Sun, the Moon, and the (ascending) node; and that there are three main periods, when each of the three possible pairs of moving points meet one another: the synodic month when the Moon returns to the Sun, the draconic month when the Moon returns to the node, and the eclipse year when the Sun returns to the node. These three 2-way relations are not independent (i.e. both the synodic month and eclipse year are dependent on the apparent motion of the Sun, both the draconic month and eclipse year are dependent on the motion of the nodes), and indeed the eclipse year can be described as the beat period of the synodic and draconic months (i.e. the period of the difference between the synodic and draconic months); in formula:\n\nas can be checked by filling in the numerical values listed above.\n\nEclipse cycles have a period in which a certain number of synodic months closely equals an integer or half-integer number of draconic months: one such period after an eclipse, a syzygy (new moon or full moon) takes place again near a node of the Moon's orbit on the ecliptic, and an eclipse can occur again. However,the synodic and draconic months are incommensurate: their ratio is not an integer number. We need to approximate this ratio by common fractions: the numerators and denominators then give the multiples of the two periods – draconic and synodic months – that (approximately) span the same amount of time, representing an eclipse cycle.\n\nThese fractions can be found by the method of continued fractions: this arithmetical technique provides a series of progressively better approximations of any real numeric value by proper fractions.\n\nSince there may be an eclipse every half draconic month, we need to find approximations for the number of half draconic months per synodic month: so the target ratio to approximate is: SM / (DM/2) = 29.530588853 / (27.212220817/2) = 2.170391682\n\nThe continued fractions expansion for this ratio is:\n\nThe ratio of synodic months per half eclipse year yields the same series:\n\nEach of these is an eclipse cycle. Less accurate cycles may be constructed by combinations of these.\n\nThis table summarizes the characteristics of various eclipse cycles, and can be computed from the numerical results of the preceding paragraphs; \"cf.\" Meeus (1997) Ch.9. More details are given in the comments below, and several notable cycles have their own pages.\n\nAny eclipse cycle, and indeed the interval between any two eclipses, can be expressed as a combination of saros (\"s\") and inex (\"i\") intervals. These are listed in the column \"formula\".\n\n\"Notes\":\n\n\nAny eclipse can be assigned to a given saros series and inex series. The year of a solar eclipse (in the Gregorian calendar) is then given approximately by:\n\nWhen this is greater than 1, the integer part gives the year AD, but when it is negative the year BC is obtained by taking the integer part and adding 2. For instance, the eclipse in saros series zero and inex series zero was in the middle of 2884 BC.\n\n\n\n"}
{"id": "8991562", "url": "https://en.wikipedia.org/wiki?curid=8991562", "title": "Erich Maren Schlaikjer", "text": "Erich Maren Schlaikjer\n\nErich Maren Schlaikjer (; November 22, 1905 in Newtown, Ohio – November 5, 1972) was an American geologist and dinosaur hunter. Assisting Barnum Brown, he co-described \"Pachycephalosaurus\" and what is now \"Montanoceratops\". Other discoveries include \"Miotapirus\" and a new species of \"Mesohippus\".\n\nErich attended Harvard University, where he graduated with a bachelor’s degree in 1929. He received master’s and doctoral degrees from Columbia University in 1931 and in 1935, respectively.\n\nSelected highlights of honors:\n\n"}
{"id": "42668392", "url": "https://en.wikipedia.org/wiki?curid=42668392", "title": "Freud: His Life and His Mind", "text": "Freud: His Life and His Mind\n\nFreud: His Life and His Mind is a 1947 biography of Sigmund Freud, the founder of psychoanalysis, by Helen Walker Puner. The book was reprinted in 1959 with a new foreword by the psychoanalyst Erich Fromm. The work was praised by Fromm, but has also received criticism from scholars.\n\nPuner attempts to explain Freud's life and his accomplishments.\n\n\"Freud: His Life and His Mind\" was first published in 1947 by Dell Publishing. It was reprinted in 1959 with a new foreword by the psychoanalyst Erich Fromm.\n\nThe psychoanalyst Anna Freud was outraged by Puner's work, describing it as \"horrible\" in a letter to Ernest Jones. Oliver Freud had a less negative view of the book, believing that Puner's errors were the result of citing Carl Jung, Wilhelm Stekel, and Fritz Wittels.\n\nThe poet Maurice English gave \"Freud: His Life and His Mind\" a negative review in \"The Nation\", describing it as \"sloppily written\" and printed. English wrote that the book contained \"elementary mistakes in grammar\" and that Puner's claim that Freud's father was stern and aloof was contradicted by \"the examples she gives of his actual behavior toward his son.\"\n\nThe critic Frederick Crews, writing in \"The New York Review of Books\", described Walker's book as \"astute\". According to Crews, Anna Freud commissioned Jones to write \"The Life and Work of Sigmund Freud\" as a response to Puner's book.\n\nFromm, in his foreword to \"Freud: His Life and His Mind\", commented that, \"although it was not written by a professional psychologist, it shows a sensitivity and grasp of Freud's personality and cultural function which is quite unusual.\" He welcomed the book's republication, crediting Puner with having provided \"a more analytical and realistic\" picture of Freud than previous authors and with recognizing both Freud's personal psychological problems and the pseudo-religious character of psychoanalysis. He wrote that Puner's book, \"will help greatly to disseminate a true and inspiring picture of the founder of psychoanalysis.\" The historian Peter Gay wrote in \"\" (1988) that Puner's biography is \"fairly hostile and neither very scholarly nor very reliable\", but that it was sufficiently influential for Ernest Jones to criticize it in \"The Life and Work of Sigmund Freud\" (1953).\n\nThe psychologist Louis Breger called \"Freud: His Life and His Mind\" a perceptive work for its time, writing that Puner managed to construct a balanced account of Freud despite her ignorance of key facts, including the existence of Wilhelm Fliess. Breger considered Puner's version of Freud \"more human\" than those of Jones and Gay.\n\n\n\n"}
{"id": "21568490", "url": "https://en.wikipedia.org/wiki?curid=21568490", "title": "Hyperdeformation", "text": "Hyperdeformation\n\nIn nuclear physics, hyperdeformation is theoretically predicted states of an atomic nucleus with extremely elongated shape and very high angular momentum. Less elongated states, superdeformation, have been well observed, but the experimental evidence for hyperdeformation is more limited. Hyperdeformed states correspond to an axis ratio of 3:1. They would be caused by a third minimum in the potential energy surface, the second causing superdeformation and the first minimum being normal deformation. Hyperdeformation is predicted to be found in Cd.\n"}
{"id": "54100403", "url": "https://en.wikipedia.org/wiki?curid=54100403", "title": "Imre Fényes", "text": "Imre Fényes\n\nImre Fényes (; 29 July 1917 - 13 November 1977) was a Hungarian physicist who was the first to propose a stochastic interpretation of quantum mechanics.\n\n\n"}
{"id": "8814172", "url": "https://en.wikipedia.org/wiki?curid=8814172", "title": "Incompatibility thesis", "text": "Incompatibility thesis\n\nIncompatibility thesis is an argument in research methodology about incompatibility of quantitative research and qualitative research paradigms in the same research. This thesis is based on philosophies of post-structuralism and post-modernism (among others). Arguments from those philosophies support exclusive superiority of one orientation (usually, the qualitative one) are oppose mixing it with quantitative research (as is advocated by the proponents of mixed-method research who support the compatibility thesis).\n\nIncompatibilists maintain that problems arise not so much at the level of practice, but at the level of epistemological paradigms. In particular, they propose that quantitative and qualitative methods are incompatible on an epistemological level; therefore, the two kinds of methods are incompatible. HOWE 1998, argues that a principle implicit in the incompatibilist's argument ‘that abstract paradigms should determine research methods in a one-way fashion—is untenable’. That paradigms must demonstrate their worth in terms of how they inform, and are informed by, research methods that are successfully employed. Given such a two-way relationship between methods and paradigms, paradigms are evaluated in terms of how well they square with the demands of research practice and incompatibilism vanishes.\n\n"}
{"id": "6214103", "url": "https://en.wikipedia.org/wiki?curid=6214103", "title": "Information pollution", "text": "Information pollution\n\nInformation pollution (also referred to as \"info pollution\") is the contamination of information supply with irrelevant, redundant, unsolicited and low-value information. The spread of useless and undesirable information can have a detrimental effect on human activities. It is considered one of the adverse effects of the information revolution.\n\nPollution is a large problem and is growing rapidly. The majority of the modern descriptions of \"information pollution\" apply to computer based communication methods, such as e-mail, instant messaging (IM) and RSS feeds. The term acquired particular relevance in 2003 when Jakob Nielsen, a leading web usability expert, published a number of articles discussing the topic. However, as early as 1971 researchers were expressing doubts about the negative effects of having to recover “valuable nodules from a slurry of garbage in which it is a randomly dispersed minor component.” People use information in order to make decisions and adapt to the circumstances. Yet, cognitive studies have demonstrated that there is only so much information human beings can process before the quality of their decisions begins to deteriorate. The excess of information is commonly known as information overload and it can lead to decision paralysis, where the person is unable to make a judgment as they cannot see what is relevant anymore. Although technology has clearly exacerbated the problem, it is not the only cause of information pollution. Anything that distracts our attention from the essential facts that we need to perform a task or make a decision could be considered an \"information pollutant\".\n\nThe use of the term \"information pollution\" also draws attention to the parallels between the information revolution that began in the last quarter of the 20th century and the industrial revolution of the 18th–19th century. Information pollution is seen as the equivalent of the environmental pollution generated by industrial processes. Some authors claim that we are facing an information overload crisis of global proportions, in the same scale of the threats faced by the environment. Others have expressed the need for the development of an information ecology to mirror environmental management practices.\n\nAlthough information pollution can present itself in many formats, its manifestations can be broadly grouped into those that provoke disruption and those that affect the quality of the information.\n\nTypical examples of disrupting information pollutants include unsolicited electronic messages (spam) and instant messages, particularly when used in the workplace. Mobile phones (the ring tones and also the actual conversation) can be very distracting in certain environments. Disrupting information pollution is not always technology based. A common example is unwanted publicity in any format. Superfluous messages, for example unnecessary labels on a map, also constitute an unnecessary distraction.\n\nAlternatively, the information supply may be polluted when the quality of the information is reduced. This may be due to the information itself being inaccurate or out of date but it also happens when the information is badly presented. For example, when the messages are unfocused or unclear or when they appear in cluttered, wordy or poorly organised documents that make it difficult for the reader to understand their meaning. This type of information pollution can be addressed in the context of information quality. Another example is in government work. Laws and regulations in many agencies are undergoing rapid changes and revisions. Government workers' handbooks and other sources used for interpreting these laws are often outdated ( sometimes years behind the changes ) which can cause the public to be misinformed, and businesses to be out of compliance with regulatory laws.\n\nA number of cultural factors have contributed to the growth of information pollution:\n\nInformation has been seen traditionally as a good thing. We are used to statements like “you cannot have too much information”, “the more information the better” and “information is power”. The publishing and marketing industries have been used to printing excessive copies of books, magazines and brochures regardless of customer demand, just in case they were needed.\n\nAs new technologies made it easier for information to reach the furthest corners of the planets, we have seen a democratisation of information sharing. This is perceived as a sign of progress and individual empowerment, as well as a positive step to bridge the divide between the information poor and the information rich. However, it also has the effect of increasing the volume of information in circulation and making it more difficult to separate valuable from waste. It is a form pollution that makes senses constantly assaulted by marketing and advertising, information pollution from a cultural context is known as \"cultural pollution.\"\n\nAs already mentioned, information pollution can exist without technology, but the technological advances of the 20th century and, in particular, the internet have played a key role in the increase of information pollution. Blogs, social networks, personal websites and mobile technology all contribute to increased “noise” levels. Some technologies are seen as especially intrusive (or polluting), for example instant messaging. Sometimes, the level of pollution caused depends on the environment in which the tool is being used. For example, e-mail is likely to cause more information pollution when used in a corporate environment than in a private setting. Mobile phones are likely to be particularly disruptive when used in a confined space like a train carriage.\n\nThe effects of information pollution can be seen at a number of levels, from the individual to society in general. The impact on a commercial organisation is likely to be particularly detrimental.\n\nAt a personal level, information pollution will affect the capacity of the individual to evaluate options and find adequate solutions. In the most extreme case it can lead to information overload and this in turn to anxiety, decision paralysis and stress. There also seem to be some negative effects on the learning process.\n\nAside from its impact on the individual, some authors argue that information pollution and information overload can cause loss of perspective and moral values. This argument has been used to explain the indifferent behaviour that modern society shows towards certain topics such as scientific discoveries, health warnings or politics. Because of the low quality and large quantity of the information received, people are becoming less sensitive to headlines and more cynical towards new messages.\n\nAs decision making is a key part of the business world. Information pollution may cause employees to become burdened with information overload and stress and therefore make slower or inadequate decisions. Increased information processing time easily translates into loss of productivity and revenue. Flawed decision making will also increase the risk of critical errors taking place.\n\nWork interruptions caused by e-mail and instant messaging can also add considerably to wasted time and efficiency losses.\n\nA number of solutions to the problem of information pollution have been proposed. These range from those based on personal and organisational management techniques to the type based on technology.\n\nInfollution: The term \"infollution\" or \"informatization pollution\" was initially coined by Dr. Paek-Jae Cho, former president & CEO of KTC (Korean Telecommunication Corp.), in a 2002 speech at the International Telecommunications Society (ITS) 14th biennial conference to describe any undesirable side effect brought about by information technology and its applications.\n\n\n"}
{"id": "644528", "url": "https://en.wikipedia.org/wiki?curid=644528", "title": "Interactivity", "text": "Interactivity\n\nAcross the many fields concerned with interactivity, including information science, computer science, human-computer interaction, communication, and industrial design, there is little agreement over the meaning of the term \"interactivity\", although all are related to interaction with computers and other machines with a user interface.\n\nMultiple views on interactivity exist. In the \"contingency view\" of interactivity, there are three levels: \n\nOne body of research has made a strong distinction between interaction and interactivity. As the suffix 'ity' is used to form nouns that denote a quality or condition, this body of research has defined interactivity as the 'quality or condition of interaction'. These researchers suggest that the distinction between interaction and interactivity is important since interaction may be present in any given setting, but the quality of the interaction varies from low and high.\n\nHuman communication is the basic example of interactive communication which involves two different processes; human to human interactivity and human to computer interactivity. Human-Human interactivity is the communication between people.\n\nOn the other hand, human to computer communication is the way that people communicate with new media. According to Rada Roy, the \"Human Computer interaction model might consists of 4 main components which consist of HUMAN, COMPUTER, TASK ENVIRONMENT and MACHINE ENVIRONMENT. The two basic flows of information and control are assumed. The communication between people and computers; one must understand something about both and about the tasks which people perform with computers. A general model of human - computer interface emphasizes the flow of information and control at the human computer interface.\" Human to Human interactivity consists of many conceptualizations which are based on anthropomorphic definitions. For example, complex systems that detect and react to human behavior are sometimes called interactive. Under this perspective, interaction includes responses to human physical manipulation like movement, body language, and/or changes in mental states.\n\nMedia theorist Fernando Arturo Torres defined interactivity as, \"a particular medium's ability to facilitate the properties necessary in an ideal conversation\" (\"Towards A Universal Theory Of Media Interactivity: Developing A Proper Context,\" 1995, \"Definition of Interactivity,\" para. 1). His research determined that interactivity should be defined by \"how well a medium facilitates two-way communication rather than by the technology of the medium.\"\n\nIn the context of communication between a human and an artifact, interactivity refers to the artifact’s interactive behaviour as experienced by the human user. This is different from other aspects of the artifact such as its visual appearance, its internal working, and the meaning of the signs it might mediate. For example, the interactivity of an iPod is not its physical shape and colour (its so-called \"design\"), its ability to play music, or its storage capacity—it is the behaviour of its user interface as experienced by its user. This includes the way the user moves their finger on its input wheel, the way this allows the selection of a tune in the playlist, and the way the user controls the volume.\n\nAn artifact’s interactivity is best perceived through use. A bystander can imagine how it would be like to use an artifact by watching others use it, but it is only through actual use that its interactivity is fully experienced and \"felt\". This is due to the kinesthetic nature of the interactive experience. It is similar to the difference between watching someone drive a car and actually driving it. It is only through the driving that one can experience and \"feel\" how this car differs from others.\n\nNew Media academic Vincent Maher defines interactivity as \"the relation constituted by a symbolic interface between its referential, objective functionality and the subject.\"\n\nThe term \"look and feel\" is often used to refer to the specifics of a computer system's user interface. Using this metaphor, the \"look\" refers to its visual design, while the \"feel\" refers to its interactivity. Indirectly this can be regarded as an informal definition of interactivity.\n\nFor a more detailed discussion of how interactivity has been conceptualized in the human-computer interaction literature, and how the phenomenology of the French philosopher Merleau-Ponty can shed light on the user experience, see (Svanaes 2000).\n\nIn computer science, interactive refers to software which accepts and responds to input from people—for example, data or commands. Interactive software includes most popular programs, such as word processors or spreadsheet applications. By comparison, noninteractive programs operate without human contact; examples of these include compilers and batch processing applications. If the response is complex enough it is said that the system is conducting social interaction and some systems try to achieve this through the implementation of social interfaces.\n\nAlso, there is the notion of kinds of user interaction, like the Rich UI.\n\nWeb page authors can integrate JavaScript coding to create interactive web pages. Sliders, date pickers, drag and dropping are just some of the many enhancements that can be provided.\n\nVarious authoring tools are available for creating various kinds of interactivities. Some common platforms for creating interactivities include Adobe Flash and Microsoft Silverlight. Notable authoring tools for creating interactivities include Harbinger's Elicitus.\n\neLearning makes use of a concept called an interaction model. Using an interaction model, any person can create interactivities in a very short period of time. Some of the interaction models presented with authoring tools fall under various categories like games, puzzles, simulation tools, presentation tools, etc., which can be completely customized.\n\n\n"}
{"id": "43909471", "url": "https://en.wikipedia.org/wiki?curid=43909471", "title": "International Social Cognition Network", "text": "International Social Cognition Network\n\nThe International Social Cognition Network (ISCON) was formed in 2003 as a joint enterprise between the European Social Cognition Network (ESCON) and the Person Memory Interest Group (PMIG) to act as an umbrella society to advance the international study of social cognition. Among the objectives of ISCON are to advance the understanding of social cognition by encouraging research and the preparation of papers and reports, holding meetings for the presentation of scientific papers, sponsoring or issuing publications containing scientific papers and other relevant material, establishing professional honors and awards to recognize excellence in social cognition research, and cooperating with other scientific and professional societies.\n\nThe official journal of ISCON is \"Social Cognition\", published bimonthly by Guilford Press.\n\nISCON sponsors the Social Cognition Preconference that precedes the annual conference of the Society for Personality and Social Psychology. It organizes a similar preconference for the tri-annual meeting of the European Association of Social Psychology. ISCON also is a sponsor of the annual conference of the Person Memory Interest Group that precedes the annual conference of the Society of Experimental Social Psychology.\n\nIn cooperation with the Personal Memory Interest Group, ISCON honors career contributions to the study of social cognition with the Thomas M. Ostrom Award. Contributions of junior scientists are honored annually with the Early Career Award. Each year, ISCON also gives an award for the Best Social Cognition Paper and recognizes outstanding research by graduate students with the Best Poster Award.\n"}
{"id": "30188621", "url": "https://en.wikipedia.org/wiki?curid=30188621", "title": "International Society of Psychiatric Genetics", "text": "International Society of Psychiatric Genetics\n\nThe International Society of Psychiatric Genetics (ISPG) is a learned society that aims to \"promote and facilitate research in the genetics of psychiatric disorders, substance use disorders and allied traits\". To this end, among other things, it organizes an annual \"World Congress of Psychiatric Genetics\". \n\nIt also awards each year the \"Ming Tsuang Lifetime Achievement Award\" for scientists who have made major contributions to the field of psychiatric genetics and the \"Theodore Reich Young Investigator Award\" for work of exceptional merit by researchers under 40 years of age.\n\nThe annual Ming Tsuang Lifetime Achievement Award is given to a distinguished senior scientist who has made significant and sustained contributions to the advancement of the field of psychiatric genetics. It is named for Ming Tsuang, who was the recipient of the award in 1995.\n"}
{"id": "26460150", "url": "https://en.wikipedia.org/wiki?curid=26460150", "title": "Iso-damping", "text": "Iso-damping\n\nIso-damping is a desirable system property referring to a state where the open-loop phase Bode plot is flat—i.e., the phase derivative with respect to the frequency is zero, at a given frequency called the \"tangent frequency\", formula_1. At the \"tangent frequency\" the Nyquist curve of the open-loop system tangentially touches the sensitivity circle and the phase Bode is locally flat which implies that the system will be more robust to gain variations. For systems that exhibit iso-damping property, the overshoots of the closed-loop step responses will remain almost constant for different values of the controller gain. This will ensure that the closed-loop system is robust to gain variations.\n\nThe iso-damping property can be expressed as formula_2\n\nwhere formula_3 is the tangent frequency and formula_4 is the open-loop system transfer function.\n\nIn the middle of the 20th century, Bode proposed the first idea involving the use of fractional-order controllers in a feedback problem by what is known as Bode’s ideal transfer function. Bode proposed that the ideal shape of the Nyquist plot for the open loop frequency response is a straight line in the complex plane, which provides theoretically infinite gain margin. Ideal open-loop transfer function is given by:\n\nwhere formula_6 is the desired gain cross over frequency and formula_7 is the slope of the ideal cut-off characteristic.\n\nThe Bode diagrams of formula_8, formula_9, are very simple. The amplitude curve is a straight line of constant slope formula_10 dB/dec, and the phase curve is a horizontal line at formula_11 rad. The Nyquist curve consists of a straight line through the origin with formula_12 rad.\n\nThe major benefit achieved through this structure is iso-damping, i.e. overshoot being independent of the payload or the system gain. The usage of fractional elements for description of ideal Bode's control loop is one of the most promising applications of fractional calculus in the process control field. Bode's ideal control loop frequency response has the fractional integrator shape and provides the iso-damping property around the gain crossover frequency. This is due to the fact that the phase margin and the maximum overshoot are defined by one parameter only (the fractional power of formula_13), and are independent of open-loop gain.\n\nBode's ideal loop transfer function is probably the first design method that addressed robustness explicitly.\n"}
{"id": "482780", "url": "https://en.wikipedia.org/wiki?curid=482780", "title": "Jean-Pierre Haigneré", "text": "Jean-Pierre Haigneré\n\nJean-Pierre Haigneré (born 19 May 1948) is a French Air Force officer and a former CNES spationaut.\n\nHaigneré was born in Paris, France and joined the French Air Force, where he trained as a test pilot.\n\nHe flew on two missions to the Mir space station in 1993 and 1999. The Mir Altair long-duration mission (186 days) in 1993 also included an EVA.\nIn addition to his duties at the European Space Agency, Haigneré is also involved in a European space tourism initiative, the \"Astronaute Club Européen\" (ACE), which he co-founded with Alain Dupas and Laurent Gathier.\n\nHe is married to former French astronaut Claudie Haigneré. The asteroid 135268 Haigneré is named in their combined honour.\nHe has three children, two from a previous marriage and one with Claudie Haigneré.\n\n"}
{"id": "40826054", "url": "https://en.wikipedia.org/wiki?curid=40826054", "title": "Job characteristic theory", "text": "Job characteristic theory\n\nJob characteristics theory is a theory of work design. It provides “a set of implementing principles for enriching jobs in organizational settings”. The original version of job characteristics theory proposed a model of five “core” job characteristics (i.e. skill variety, task identity, task significance, autonomy, and feedback) that affect five work-related outcomes (i.e. motivation, satisfaction, performance, and absenteeism and turnover) through three psychological states (i.e. experienced meaningfulness, experienced responsibility, and knowledge of results).\n\nWork redesign first got its start in the 1960s. Up until then, the prevailing attitude was that jobs should be simplified in order to maximize production, however it was found that when subjected to highly routinized and repetitive tasks, the benefits of simplification sometimes disappeared due to worker dissatisfaction. It was proposed that jobs should be enriched in ways that boosted motivation, instead of just simplified to a string of repetitive tasks. It is from this viewpoint that Job Characteristics Theory emerged.\n\nIn 1975, Greg R. Oldham and J. Richard Hackman constructed the original version of the Job Characteristics Theory (JCT), which is based on earlier work by Turner and Lawrence and Hackman and Lawler. Turner and Lawrence, provided a foundation of objective characteristics of jobs in work design. Further, Hackman and Lawler indicated the direct effect of job characteristics on employee's work related attitudes and behaviors and, more importantly, the individual differences in need for development, which is called Growth Need Strength in Job Characteristics Theory.\n\nIn 1980, Hackman and Oldham presented the final form of the Job Characteristics Theory in their book \"Work Redesign\". The main changes included the addition of two more moderators- \"Knowledge and Skill\" and \"Context Satisfaction\", removal of the work outcomes of absenteeism and turnover, and increased focus on \"Internal Work Motivation\". Several of the outcome variables were removed or renamed as well. Concentration was shifted to the affective outcomes following results from empirical studies that showed weak support for the relationship between the psychological states and behavioral outcomes.\n\nIn addition to the theory, Oldham and Hackman also created two instruments, the Job Diagnostic Survey (JDS) and the Job Rating Form (JRF), for assessing constructs of the theory. The JDS directly measures jobholders' perceptions of the five core job characteristics, their experienced psychological states, their Growth Need Strength, and outcomes. The JRF was designed to obtain the assessments from external observers, such as supervisors or researchers, of the core job characteristics.\n\nAccording to the final version of the theory, five core job characteristics should prompt three critical psychological states, which lead to many favorable personal and work outcomes. The moderators Growth Need Strength, Knowledge and Skill, and Context Satisfaction should moderate the links between the job characteristics and the psychological states, and the psychological states and the outcomes.\n\n\n\nAdopted from earlier work the personal and work outcomes of the initial theory were: \"Internal Work Motivation\", \"Job Satisfaction\", \"Absenteeism and Turnover\", and \"Performance Quality\". However, the 1980 revisions to the original model included removing absenteeism and turnover, and breaking performance into \"Quality of Work\" and \"Quantity of Work\".\n\n\nThe three critical psychological states of job characteristic theory (JCT) draw upon cognitive motivation theory and some previous work on identifying the presence of certain psychological states could lead to favorable outcomes. JCT provided the chance to systematically assessed the relationship between the previously discovered psychological states ('Experienced Meaningfulness, 'Experienced Responsibility, and Knowledge of Results) and outcomes. More importantly, previous work on work design showed job characteristics can predict individual performance, but did not provide “why” and “how” this relationship existed. Job Characteristics Theory filled this gap by building a bridge between job characteristics and work-related outcomes through the use of the three critical psychological states.\nThe three psychological states, which are also the conceptual core of the theory, include (1) Experienced Meaningfulness of the Work, (2) Experienced Responsibility for the Outcomes of the Work, and (3) Knowledge of the Results of Work Activities. These psychological states are theorized to mediate the relationship between job characteristics and work-related outcomes. According to the theory, these three critical psychological states are noncompensatory conditions, meaning jobholders have to experience all three critical psychological states to achieve the outcomes proposed in the model. For example, when workers experience the three psychological states, they feel good about themselves when they perform well. These positive feelings, in turn, reinforce the workers to keep performing well.\n\nAccording to the theory, certain core job characteristics are responsible for each psychological state: skill variety, task identity, and task significance shape the experienced meaningfulness; autonomy affects experienced responsibility, and feedback contributes to the knowledge of results. Previous research found that four job characteristics (autonomy, variety, identity, and feedback) could increase workers’ performance, satisfaction, and attendance. Task significance was derived from Greg Oldham’s own work experience as an assembly line worker. Though his job did not provide task variety or identity, he still experienced meaningfulness through the realization that others depended on his work. This realization led to the inclusion of task significance as another job characteristic that would influence experienced meaningfulness of the job. Thus, job characteristics theory proposed the five core job characteristics that could predict work related outcomes.\n\nWhen a job has a high score on the five core characteristics, it is likely to generate three psychological states, which can lead to positive work outcomes, such as high internal work motivation, high satisfaction with the work, high quality work performance, and low absenteeism and turnover. This tendency for high levels of job characteristics to lead to positive outcomes can be formulated by the motivating potential score (MPS). Hackman and Oldham explained that the MPS is an index of the “degree to which a job has an overall high standing on the person's degree of motivation...and, therefore, is likely to prompt favorable personal and work outcomes”:\n\nThe motivating potential score (MPS) can be calculated, using the core dimensions discussed above, as follows:\n\nJobs that are high in motivating potential must be also high on at least one of the three factors that lead to experienced meaningfulness, and also must be high on both Autonomy and Feedback. If a job has a high MPS, the job characteristics model predicts that motivation, performance and job satisfaction will be positively affected and the likelihood of negative outcomes, such as absenteeism and turnover, will be reduced.\n\nAccording to the equation above, a low standing on either autonomy or feedback will substantially compromise a job's MPS, because autonomy and feedback are the only job characteristics expected to foster experienced responsibility and knowledge of results, respectively. On the contrary, a low score on one of the three job characteristics that lead to experienced meaningfulness may not necessarily reduce a job's MPS, because a strong presence of one of those three attributes can offset the absence of the others.\n\nIn response to one of the disadvantages of Motivator–Hygiene Theory, Job Characteristics Theory added an individual difference factor into the model. While Herzberg et al. took into account the importance of intrinsically and extrinsically motivating job characteristics there was no consideration of individual differences. The importance of individual differences had been demonstrated by previous work showing that some individuals are more likely to positively respond to an enriched job environment than others. Thus, the original version of the theory posits an individual difference characteristic, Growth Need Strength (GNS), that moderates the effect of the core job characteristics on outcomes. Jobholders with high Growth Need Strength should respond more positively to the opportunities provided by jobs with high levels of the five core characteristics compared to low GNS jobholders.\n\nTaylor’s theory of scientific management emphasized efficiency and productivity through the simplification of tasks and division of labor.\n\nHerzberg et al.’s Motivator–Hygiene Theory, aka Two-factor Theory, an influence on Job Characteristics Theory, sought to increase motivation and satisfaction through enriching jobs.The theory predicts changes in “motivators”, which are intrinsic to the work, (such as recognition, advancement, and achievement) will lead to higher levels of employee motivation and satisfaction; while “hygiene factors”, which are extrinsic to the work itself, (such as company policies and salary) can lead to lower levels of dissatisfaction, but will not actually effect satisfaction or motivation.\n\nSociotechnical systems theory predicts an increase in satisfaction and productivity through designing work that optimized person-technology interactions.\n\nQuality improvement theory is based on the idea that jobs can be improved through the analysis and optimized of work processes.\n\nAdaptive structuration theory provides a way to look at the interaction between technology’s intended and actual use in an organization, and how it can influence different work-related outcomes.\n\nIdaszak and Drasgow provided a corrected version of the Job Diagnostic Survey that corrected for one of the measurement errors in the instrument. It had been suggested that reverse scoring on several of the questions was to blame for the inconsistent studies looking at the factors involved in the Job Diagnostic Survey. Following a factor analysis, Idaszak and Drasgow found six factors rather than the theorized five characteristics proposed by the Job Characteristics Theory. Upon further investigation, they were able to show that the sixth factor was made up of the reverse coded items. The authors rephrased the questions, ran the analysis again, and found it removed the measurement error.\n\nDue to the inconsistent findings about the validity of Growth Need Strength as a moderator of the Job characteristic-outcomes relationship, Graen, Scandura, and Graen proposed the GN–GO model, which added Growth Opportunity as another moderator. They suggested there isn’t a simple positive relationship between motivation and Growth Need Strength, but instead there is an underlying incremental (stairstep) relationship with various levels of Growth Opportunity. Growth Opportunity increments are described as “events that change either the characteristics of the job itself or the understanding of the job itself”. It was hypothesized that as people high in Growth Need Strength met each level of Growth Opportunity they could be motivated to increase their performance, but when people low on Growth Need Strength met these same increments their performance would either maintain or degrade. Field studies found more support for the GN–GO model than the original Growth Need Strength moderation.\n\nHumphrey, Nahrgang, and Morgeson extended the original model by incorporating many different outcomes and job characteristics. The authors divided the revised set of Job Characteristics into three sections- Motivational, Social, and Work Context Characteristics; and the outcomes were portioned out into four parts- Behavioral, Attitudinal, Role Perception, and Well-being Outcomes. Results showed strong relationships between some of the expanded characteristics and outcomes, suggesting that there are more options for enriching jobs than the original theory would suggest.\n\nTaking from earlier empirical research on Job Characteristics Theory and Psychological Ownership, researchers developed a model that combined the two theories. They replaced the psychological states of the Job Characteristics Theory with Psychological Ownership of the job as the mediator between job characteristics and outcomes. In addition to the positive personal and work outcomes of Job Characteristics Theory, negative outcomes (e.g. Territorial Behaviors, Resistance to Change, and Burden of Responsibility) were added.\n\nSince its inception, Job Characteristics Theory has been scrutinized extensively. The first empirical tests of the theory came from Hackman and Oldham themselves. The authors found the “internal consistency reliability of the scales and the discriminant validity of the items” to be “satisfactory”. They also tried to assess the objectivity of the measure by having the supervisors and the researchers evaluate the job in addition to the jobholders. More importantly, the authors reported the relationships predicted by the model were supported by their analysis.\n\nFollowing these publications, over 200 empirical articles were published examining Job Characteristics Theory over the next decade. Fried and Ferris summarized the research on Job Characteristics Theory and found “modest support” overall. Fried and Ferris mentioned seven general areas of criticism in their review, which are discussed below:\n\n\nOver the years since Job Characteristics Theory’s introduction into the organizational literature, there have been many changes to the field and to work itself. Oldham and Hackman suggest that the areas more fruitful for development in work design are social motivation, job crafting, and teams.\n\nSocial sources of motivation are becoming more important due to the changing nature of work in this country. More jobs are requiring higher levels of client-employee interaction, as well as increasing interdependence among employees. With this in mind, it would make sense to investigate the effect the social aspects have on affective and behavioral outcomes.\n\nWhile Job Characteristics Theory was mainly focused on the organization’s responsibility for manipulating job characteristics to enrich jobs there has been a considerable buzz in the literature regarding job crafting. In job crafting the employee has some control over their role in the organization. Hackman and Oldham point out there are many avenues of inquiry regarding job crafting such as: what are the benefits of job crafting, are the benefits due to the job crafting process itself or the actual changes made to the job, and what are the negative effects of job crafting?\n\nFinally, they brought up the potential research directions relevant to team work design. Specifically, they discuss the need to understand when to use work-design aimed at the individual or team level in order to increase performance, and what type of team is best suited to particular tasks.\n\nJob Characteristics Theory is firmly entrenched within the work design (also called job enrichment) literature, moreover the theory has become one of the most cited in all of the organizational behavior field. In practical terms, Job Characteristics Theory provides a framework for increasing employees’ motivation, satisfaction, and performance through enriching job characteristics.\n\nJob Characteristics Theory has been embraced by researchers and used in a multitude of professions and organizations. In the applied domain, Hackman and Oldham have reported that a number of consulting firms have employed their model or modified it to meet their needs.\n\n"}
{"id": "5459889", "url": "https://en.wikipedia.org/wiki?curid=5459889", "title": "Lewis and Clark Highway", "text": "Lewis and Clark Highway\n\nLewis and Clark Highway may refer to:\n\nLewis and Clark Highway may also refer to:\n"}
{"id": "19061266", "url": "https://en.wikipedia.org/wiki?curid=19061266", "title": "List of Wyoming state symbols", "text": "List of Wyoming state symbols\n\nThe following is a list of symbols of the U.S. state of Wyoming.\n"}
{"id": "22387613", "url": "https://en.wikipedia.org/wiki?curid=22387613", "title": "List of highly toxic gases", "text": "List of highly toxic gases\n\nMany gases have toxic properties, which are often assessed using the LC50 (median lethal dose) measure. In the United States, many of these gases have been assigned an NFPA 704 health rating of 4 (may be fatal) or 3 (may cause serious or permanent injury), and/or exposure limits [Threshold limit value|TLV], TWA or STEL) determined by the ACGIH professional association. Some, but by no means all, toxic gases are detectable by odor, which can serve as a warning. Among the best known toxic gases are carbon monoxide, chlorine, nitrogen dioxide and phosgene.\n\n\n\n"}
{"id": "10630128", "url": "https://en.wikipedia.org/wiki?curid=10630128", "title": "List of lakes in South Dakota", "text": "List of lakes in South Dakota\n\nThis is a list of lakes in South Dakota.\n\n\n"}
{"id": "18083942", "url": "https://en.wikipedia.org/wiki?curid=18083942", "title": "List of political and geographic subdivisions by total area (all)", "text": "List of political and geographic subdivisions by total area (all)\n"}
{"id": "11485823", "url": "https://en.wikipedia.org/wiki?curid=11485823", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: C", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: C\n\n\n"}
{"id": "7120290", "url": "https://en.wikipedia.org/wiki?curid=7120290", "title": "List of volcanoes in Samoa", "text": "List of volcanoes in Samoa\n\nThis is a list of active and extinct volcanoes. \n"}
{"id": "23448158", "url": "https://en.wikipedia.org/wiki?curid=23448158", "title": "Michael Ashley (astronomer)", "text": "Michael Ashley (astronomer)\n\nMichael C. B. Ashley is an Australian astronomer and professor at the University of New South Wales, in Sydney. He is most famous for his work in Antarctica, with the study of the seeing capability at Dome C.\n\nIn September 2004, \"Nature\" published a report written by Jon Lawrence, Michael Ashley, Andrei Tokovinin, and Tony Travouillon on the seeing abilities of astronomical telescopes in Antarctica. The paper concluded that Dome C would be \"the best ground-based site to develop a new astronomical observatory.\" The data used in this report was collected by a remote control experiment run through the French-Italian Concordia Station near Dome C. However, Ashley and his team have been to Antarctica on four separate trips, in 1995, 1998, 2001, and 2004 for earlier experiments, such as measurements of the near-infrared quality of the brightness of the sky. It was found that pictures taken from a telescope at Dome C are, on average, 2.5 times better than those taken at observatories elsewhere. This discovery has been lauded as finding the clearest skies on Earth.\n\n"}
{"id": "850009", "url": "https://en.wikipedia.org/wiki?curid=850009", "title": "Micrographia", "text": "Micrographia\n\nMicrographia: or Some Physiological Descriptions of Minute Bodies Made by Magnifying Glasses. With Observations and Inquiries Thereupon. is a historically significant book by Robert Hooke about his observations through various lenses. It is particularly notable for being the first book to illustrate insects, plants etc. as seen through microscopes. Published in January 1665, the first major publication of the Royal Society, it became the first scientific best-seller, inspiring a wide public interest in the new science of microscopy. It is also notable for coining the biological term \"cell\".\n\nHooke most famously describes a fly's eye and a plant cell (where he coined that term because plant cells, which are walled, reminded him of the cells in a honeycomb). Known for its spectacular copperplate engravings of the miniature world, particularly its fold-out plates of insects, the text itself reinforces the tremendous power of the new microscope. The plates of insects fold out to be larger than the large folio itself, the engraving of the louse in particular folding out to four times the size of the book. Although the book is best known for demonstrating the power of the microscope, \"Micrographia\" also describes distant planetary bodies, the wave theory of light, the organic origin of fossils, and other philosophical and scientific interests of its author.\n\nHooke also selected several objects of human origin; among these objects were the jagged edge of a honed razor and the point of a needle, seeming blunt under the microscope. His goal may well have been as a way to contrast the flawed products of mankind with the perfection of nature (and hence, in the spirit of the times, of biblical creation). \nPublished under the aegis of The Royal Society, the popularity of the book helped further the society's image and mission of being the UK's leading scientific organization. \"Micrographia\"'s illustrations of the miniature world captured the public's imagination in a radically new way; Samuel Pepys called it \"the most ingenious book that ever I read in my life.\"\n\nIn 2007, Janice Neri, a professor of art history and visual culture, studied Hooke's artistic influences and processes with the help of some newly rediscovered notes and drawings that appear to show some of his work leading up to \"Micrographia.\" She observes, \"Hooke's use of the term \"schema\" to identify his plates indicates that he approached his images in a diagrammatic manner and implies the study or visual dissection of the objects portrayed.\" Identifying Hooke's schema as 'organization tools,' she emphasizes:\n\nAdditionally: \"Hooke often enclosed the objects he presented within a round frame, thus offering viewers an evocation of the experience of looking through the lens of a microscope.\"\n\n\n"}
{"id": "1397054", "url": "https://en.wikipedia.org/wiki?curid=1397054", "title": "Ohio State University Radio Observatory", "text": "Ohio State University Radio Observatory\n\nThe Ohio State University Radio Observatory was a Kraus-type radio telescope located on the grounds of the Perkins Observatory at Ohio Wesleyan University from 1963 to 1998. Known as Big Ear, the observatory was part of The Ohio State University's Search for Extraterrestrial Intelligence (SETI) project. Construction of the Big Ear began in 1956 and was completed in 1961, and it was finally turned on for the first time in 1963. \n\nThe main reflector of Big Ear The Flat Reflector measured approximately 103 metres by 33 metres, giving it the sensitivity equivalent to a circular dish measuring nearly 53 metres in diameter. \n\nThe observatory completed the Ohio Sky Survey in 1971, and from 1973–1995, Big Ear was used to search for extraterrestrial radio signals, making it the longest running SETI project in history. In 1977, the Big Ear received the Wow! signal. The observatory was disassembled in 1998 when developers purchased the site from the university and used the land to expand a nearby golf course. \n\nFrom 1965–1971, the Big Ear was used to map wideband radio sources for the Ohio Sky Survey, its first sky survey for extraterrestrial radio sources. In 1972, the United States Congress voted to stop funding the Ohio Sky Survey with support from the National Science Foundation resulting in many people losing their jobs. \n\nIn 1977, the Big Ear recorded an unusual and apparently extraterrestrial radio signal, which became known as the Wow! signal. The observation would prove to be unique, since no similar signals were ever detected afterwards.\n\n\nThe Big Ear was listed in the 1995 Guinness Book of World Records under the category of \"Longest Extraterrestrial Search\":\nThe longest-running full-scale SETI (search for extraterrestrial intelligence) project is the Ohio SETI Program at Ohio State University in Columbus, OH, which has searched the universe for extraterrestrial radio signals for 22 years, beginning in 1973. \n\n\n"}
{"id": "213457", "url": "https://en.wikipedia.org/wiki?curid=213457", "title": "PGM-11 Redstone", "text": "PGM-11 Redstone\n\nThe PGM-11 Redstone was the first large American ballistic missile. A short-range ballistic missile (SRBM), it was in active service with the United States Army in West Germany from June 1958 to June 1964 as part of NATO's Cold War defense of Western Europe. It was the first missile to carry a live nuclear warhead, in the 1958 Pacific Ocean weapons test, Hardtack Teak. Chief Engineer Wernher von Braun personally witnessed this historic launch and detonation.\n\nRedstone was a direct descendant of the German V-2 rocket, developed by a team of predominantly German rocket engineers brought to the United States after World War II as part of Operation Paperclip. The design used an upgraded engine from Rocketdyne that allowed the missile to carry the W39 warhead which weighed with its reentry vehicle to a range of about . Redstone's prime contractor was the Chrysler Corporation.\n\nA major effort to improve Redstone's reliability produced one of the most reliable rockets of the era. Dubbed \"the Army's Workhorse\", it spawned an entire rocket family which had an excellent launch record and holds a number of firsts in the US space program, notably launching the first US astronaut. It was retired by the Army in 1964 and replaced by the solid-fueled MGM-31 Pershing. Surplus missiles were widely used for test missions and space launches, including the first US man in space, and in 1967 the launch of Australia's first satellite.\n\nA product of the Army Ballistic Missile Agency (ABMA) at Redstone Arsenal in Huntsville, Alabama under the leadership of Wernher von Braun, Redstone was designed as a surface-to-surface missile for the U.S. Army. It was named for the arsenal on 8 April 1952, which traced its name to the region's red rocks and soil. Chrysler was awarded the prime production contract and began missile and support equipment production in 1952 at the newly renamed Michigan Ordnance Missile Plant in Warren, Michigan. The navy-owned facility was previously known as the Naval Industrial Reserve Aircraft Plant used for jet engine production. Following the cancellation of a planned jet engine program, the facility was made available to the Chrysler Corporation for missile production. Rocketdyne Division of North American Aviation Company provided the rocket engines; Ford Instrument Company, division of Sperry Rand Corporation, produced the guidance and control systems; and Reynolds Metals Company fabricated fuselage assemblies as subcontractors to Chrysler. The first Redstone lifted off from LC-4A at Cape Canaveral on 20 August 1953. It flew for one minute and 20 seconds before suffering an engine failure and falling into the sea. Following this partial success, the second test was conducted on 27 January 1954, this time without a hitch as the missile flew 55 miles. After these first two prototypes were flown, an improved engine was introduced to reduce problems with LOX turbopump cavitation.\n\nThe third Redstone flight on 5 May was a total loss as the engine cut off one second after launch, causing the rocket to fall back on the pad and explode. After this incident, Major General Holger Toftoy pressured Wernher von Braun for the cause of the failure. The latter replied that he had no idea, but they would review telemetry and other data to find out. Toftoy persisted, asking \"Wernher, why did the rocket explode?\" An exasperated von Braun said \"It exploded because the damn sonofabitch blew up!\"\n\nVon Braun pressured the ABMA team to improve reliability and workmanship standards, allegedly remarking that \"Missile reliability will require that the target area is more dangerous than the launch area.\" Subsequent test flights went better and the Army declared Redstone operational in mid-1955. Testing was moved from LC-4 to the bigger LC-5 and LC-6.\n\nIn 1955, the Jupiter-C rocket (not to be confused with the later, unrelated Jupiter IRBM) was developed as an enhanced Redstone for atmospheric and reentry vehicle tests. It had elongated propellant tanks for increased burn time and a new engine that burned a fuel mixture known as hydyne and under the name of the Jupiter C/Juno 1 was used for the first successful US space launch of the Explorer 1 satellite in 1958.\n\nThe Mercury-Redstone Launch Vehicle was a derivation of the Redstone with a fuel tank increased in length by and was used on 5 May 1961 to launch Alan Shepard on his sub-orbital flight to become the second person and first American in space. It retained the Jupiter C's longer propellant tanks, but went back to using ethyl alcohol/water for propellant instead of hydyne.\n\nThe Redstone program proved to be a bone of contention between the Army and Air Force due to their different ideas of nuclear warfare. The Army favored using small warheads on mobile missiles as tactical battlefield weapons while the Air Force, which was responsible for the ICBM program, wanted large cross-continental missiles that could strike Soviet targets and rapidly cripple the USSR's infrastructure and ability to wage war.\n\nWith the arrival of newer solid-fueled missiles that could be stored and not require fueling before launch, Redstone was rendered obsolete and production ended in 1961. The 40th Artillery Group was deactivated in February 1964 and 46th Artillery Group was deactivated in June 1964, as Redstone missiles were replaced by the Pershing missile in the U.S. Army arsenal. All Redstone missiles and equipment deployed to Europe were returned to the United States by the third quarter of 1964. In October 1964, the Redstone missile was ceremonially retired from active service at Redstone Arsenal.\n\nFrom 1966 to 1967, a series of surplus modified Redstones called Spartas were launched from Woomera, South Australia as part of a joint U.S.–United Kingdom–Australian research program aimed at understanding re-entry phenomena. These Redstones had two solid fuel upper stages added. The U.S. donated a spare Sparta for Australia's first satellite launch, WRESAT, in November 1967.\n\nRedstone was capable of flights from to . It consisted of a thrust unit for powered flight and a missile body for overall missile control and payload delivery on target. During powered flight, Redstone burned a fuel mixture of 25 percent water–75 percent ethyl alcohol with liquid oxygen (LOX) used as the oxidizer. Later Redstones used Hydyne, 60% unsymmetrical dimethylhydrazine (UDMH) and 40% diethylenetriamine (DETA), as the fuel. The missile body consisted of an aft unit containing the instrument compartment, and the warhead unit containing the payload compartment and the radar altimeter fuze. The missile body was separated from the thrust unit 20–30 seconds after the termination of powered flight, as determined by the preset range to target. The body continued on a controlled ballistic trajectory to the target impact point. The thrust unit continued on its own uncontrolled ballistic trajectory, impacting short of the designated target.\n\nThe nuclear-armed Redstone carried the W39, either a MK 39Y1 Mod 1 or MK 39Y2 Mod 1, warhead with a yield of 3.8 megatons.\n\n\n\n\n"}
{"id": "739330", "url": "https://en.wikipedia.org/wiki?curid=739330", "title": "Peter Siebold", "text": "Peter Siebold\n\nPeter Siebold (born 1971) is a member of the Scaled Composites astronaut team. He is their Director of Flight Operations, and was one of the test pilots for SpaceShipOne and SpaceShipTwo, the experimental spaceplanes developed by the company. On April 8, 2004, Siebold piloted the second powered test flight of SpaceShipOne, flight 13P, which reached a top speed of Mach 1.6 and an altitude of 32.0 kilometers. On October 31, 2014, Siebold and Michael Alsbury were piloting the SpaceShipTwo VSS \"Enterprise\" on flight PF04, when the craft came apart in mid-air and then crashed, killing Alsbury and injuring Siebold.\n\nPeter Siebold, a 1990 graduate of Davis Senior High School in Davis, California, obtained his pilot's license at age 16. He has been a design engineer at Scaled Composites since 1996. Siebold holds a degree in aerospace engineering from California Polytechnic University at San Luis Obispo, from 2001.\n\nSiebold was responsible for the simulator, navigation system, and ground control system for the SpaceShipOne project at Scaled. Although he was one of four qualified pilots for SpaceShipOne, Siebold did not pilot the craft during the flights later in 2004 to meet the requirements of the Ansari X Prize. Although Siebold flew SpaceShipOne to an altitude of 32 km (just under 20 miles), he did not cross the 100 km Kármán line—the international standard for reaching space.\n\nFor his contribution to the SpaceShipOne project, Siebold, along with Mike Melvill and Brian Binnie, received the 2004 Iven C. Kincheloe Award presented by the Society of Experimental Test Pilots.\n\nSiebold became the Director of Flight Operations at Scaled. He was the pilot who flew the White Knight Two on its maiden flight on December 21, 2008. He won the Iven C. Kincheloe Award a second time in 2009, this time individually, for his work on the first WhiteKnightTwo, VMS \"Eve\", as chief test pilot.\n\nOn October 31, 2014, Siebold was one of the two pilots flying the Virgin Galactic SpaceShipTwo, VSS \"Enterprise\", along with Michael Alsbury, on a test flight, which suffered an anomaly, resulting in the death of Michael Alsbury and loss of \"Enterprise\". VSS \"Enterprise\" crashed in the California Mojave Desert. Thrown clear of the \"Enterprise\" when it broke up in mid-air, Siebold survived a descent from about 50,000 feet at Mach 1 speed with just a flightsuit. His parachute deployed automatically at about 20,000 feet, and, after landing, he was taken to the hospital for treatment.\n\nSiebold's flight suit was saturated with blood from bleeding in his right arm. He did not feel any lower body injuries. As Siebold removed his parachute harness a “clunking noise” was felt in his chest and Siebold became concerned about a potential spinal fracture. Upon arrival at the ER his flight suit was cut away. \n\nSiebold experienced a non-compound four-part fracture of his right humerus. The ball of his ankle was also dislocated and fractured. He had a non-displaced fracture of his right clavicle, a small gash in his right elbow (source of the blood on his flight suit), a deep scrape on his right wrist, and multiple scrapes on the back of his right shoulder. There was considerable bruising on his right chest but he did not know how it occurred. He did not recall any bruising on his left side. He had an abrasion under his chin which he felt was consistent with the location of his chin strap, and had multiple contusions and scrapes on his face. He was diagnosed with corneal scratches and doctors removed a piece of fiberglass from his left eye during a hospital stay. The eyes did not improve so he saw an ophthalmologist after being discharged. The ophthalmologist removed some foreign matter from his left eyelid and a “silver sliver” from his right cornea. His eyes improved immediately post procedure.\n\n"}
{"id": "13049433", "url": "https://en.wikipedia.org/wiki?curid=13049433", "title": "Platyclade", "text": "Platyclade\n\nPlatyclades are flattened, photosynthetic shoots, branches or stems that resemble or perform the function of leaves, as in \"Homalocladium platycladum\" and some cactus genera like \"Opuntia\" and \"Schlumbergera\".\n\nNew Latin \"platycladium\"; from Greek \"platy\", flat + \"klados\", branch.\n"}
{"id": "274440", "url": "https://en.wikipedia.org/wiki?curid=274440", "title": "Popular science", "text": "Popular science\n\nPopular science (also called pop-science or popsci) is an interpretation of science intended for a general audience. While science journalism focuses on recent scientific developments, popular science is more broad-ranging. It may be written by professional science journalists or by scientists themselves. It is presented in many forms, including books, film and television documentaries, magazine articles, and web pages.\n\nPopular science is a bridge between scientific literature as a professional medium of scientific research, and the realms of popular political and cultural discourse. The goal of the genre is often to capture the methods and accuracy of science, while making the language more accessible. Many science-related controversies are discussed in popular science books and publications, such as the long-running debates over biological determinism and the biological components of intelligence, stirred by popular books such as \"The Mismeasure of Man\" and \"The Bell Curve\".\n\nThe purpose of scientific literature is to inform and persuade peers as to the validity of observations and conclusions and the forensic efficacy of methods. Popular science attempts to inform and convince scientific outsiders (sometimes along with scientists in other fields) of the significance of data and conclusions and to celebrate the results. Statements in scientific literature are often qualified and tentative, emphasizing that new observations and results are consistent with and similar to established knowledge wherein qualified scientists are assumed to recognize the relevance. By contrast, popular science emphasizes uniqueness and generality, taking a tone of factual authority absent from the scientific literature. Comparisons between original scientific reports, derivative science journalism and popular science typically reveal at least some level of distortion and oversimplification which can often be quite dramatic, even with politically neutral scientific topics.\n\nPopular science literature can be written by non-scientists who may have a limited understanding of the subject they are interpreting and it can be difficult for non-experts to identify misleading popular science, which may also blur the boundaries between real science and pseudoscience. However, sometimes non-scientists with a fair scientific background make better popular science writers because of their ability to put themselves in the layperson's place more easily.\n\nSome usual features of popular science productions include:\n\nIn alphabetical order by last name:\n \n\n\nNews agencies\n\n\nDaily newspapers\n\nWeeklies\n\nFortnightlies\n\nMonthlies\n\nBimonthlies\n\n"}
{"id": "602003", "url": "https://en.wikipedia.org/wiki?curid=602003", "title": "Population statistics for Israeli settlements in the West Bank", "text": "Population statistics for Israeli settlements in the West Bank\n\nThe population statistics for Israeli settlements in the West Bank are collected by the Israel Central Bureau of Statistics. As such, the data contains only population of settlements recognized by the Israeli authorities. Israeli outposts are not tracked, and their population is hard to establish. In addition to these, Nahal settlements are formally considered military outposts, and their population is counted, but not reported. Once a Nahal settlement becomes a civilian locality, it starts to be reported.\n\nWhile all settlements in the West Bank were advised by the International Court of Justice to be unlawful in 2004, the construction of the West Bank Barrier keeps a significant number of settlements behind it. The largest settlements left beyond the barrier includes Kiryat Arba (population 7,593 in 2012), Kokhav Ya'akov (6,476), Beit El (5,897), Geva Binyamin (4,674), Eli, Mateh Binyamin (3,521), Ofra (3,489), Talmon (3,202), Shilo, Mateh Binyamin (2,706), Tekoa, Gush Etzion (2,518), and Mitzpe Yeriho (2,115). The total number of settlers east of the barrier lines in 2012 was at least 67,702, plus 11,528 in the Jordan Valley. By comparison, the number of Gaza Strip settlers in 2005 who refused to move voluntarily and be compensated, and that were forcibly evicted during the Israeli disengagement from Gaza was around 9,000. The total population of all settlements in the West Bank was nearly 400,000 in 2014, excluding East Jerusalem. As of December 2015, altogether over 800,000 Israeli Jews resided over the 1949 Armistice Lines (including east-Jerusalem neighborhoods), constituting approximately 13% of Israel's Jewish population.\n\nUnreported Nahal settlements:\n\nLocalities of unknown status:\n\nOther localities:\n\n\n"}
{"id": "7173741", "url": "https://en.wikipedia.org/wiki?curid=7173741", "title": "Ronaldo Rogério de Freitas Mourão", "text": "Ronaldo Rogério de Freitas Mourão\n\nRonaldo Rogério de Freitas Mourão (25 May 1935 – 25 July 2014) was a Brazilian astronomer and the founder of the Museum of Astronomy and Related Sciences (Museu de Astronomia e Ciências Afins) (MAST), as well as a researcher and titular partner at the Brazilian History and Geography Institute (Instituto Histórico e Geográfico Brasileiro) (IGHB). He was born and died in Rio de Janeiro.\n\n"}
{"id": "26336661", "url": "https://en.wikipedia.org/wiki?curid=26336661", "title": "Russian and Eurasian Security Network", "text": "Russian and Eurasian Security Network\n\nThe Russian and Eurasian Security Network (RES) was an open-source service that encouraged the exchange of information among international relations and security professionals worldwide. It maintained a large digital library of documents related to Russia and Eurasia and provided the framework for studies of security-related developments in Russia and the states of the Eurasian region (Armenia, Azerbaijan, Georgia, Moldova, Belarus, Ukraine, Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan and Uzbekistan).\n\nIt was based at the Swiss Federal Institute of Technology in Zurich, Switzerland and was part of the International Relations and Security Network (ISN) and the Center for Security Studies (CSS). The RES website was closed down in August 2012.\n\nThe two RES in-house publications were migrated to the website of the Center for Security Studies: \n\n\n"}
{"id": "57057365", "url": "https://en.wikipedia.org/wiki?curid=57057365", "title": "Schauenberg's index", "text": "Schauenberg's index\n\nSchauenberg's index is the ratio of skull length to cranial capacity.\nThis index was introduced by Paul Schauenberg in 1969 as a method to identify European wildcat (\"Felis silvestris\") skulls and distinguish from domestic cat (\"Felis catus\") skulls.\n"}
{"id": "24714543", "url": "https://en.wikipedia.org/wiki?curid=24714543", "title": "Schedule for Affective Disorders and Schizophrenia", "text": "Schedule for Affective Disorders and Schizophrenia\n\nThe Schedule for Affective Disorders and Schizophrenia (SADS) is a collection of psychiatric diagnostic criteria and symptom rating scales originally published in 1978. It is organized as a semi-structured diagnostic interview. The structured aspect is that every interview asks screening questions about the same set of disorders regardless of the presenting problem; and positive screens get explored with a consistent set of symptoms. These features increase the sensitivity of the interview and the inter-rater reliability (or reproducibility) of the resulting diagnoses. The SADS also allows more flexibility than fully structured interviews: Interviewers can use their own words and rephrase questions, and some clinical judgment is used to score responses. There are three versions of the schedule, the regular SADS, the lifetime version (SADS-L) and a version for measuring the change in symptomology (SADS-C). Although largely replaced by more structured interviews that follow diagnostic criteria such as DSM-IV and DSM-5, and specific mood rating scales, versions of the SADS are still used in some research papers today.\n\nThe diagnoses covered by the interview include schizophrenia, schizoaffective disorder, major depressive disorder, bipolar disorder, anxiety disorders and a limited number of other fairly common diagnoses.\n\nThe SADS was developed by the same group of rearchers as the Research Diagnostic Criteria (RDC). While the RDC is a list of diagnostic criteria for psychiatric disorders, the SADS interview allows diagnoses based on RDC criteria to be made, and also rates subject's symptoms and level of functioning.\n\nThe K-SADS (or Kiddie-SADS) is a version of the SADS adapted for school-aged children of 6–18 years. There are various different versions of the K-SADS, each varying slightly in terms of disorders and specific symptoms covered, as well as the scale range used. All of the variations are still semi-structured interviews, giving the interviewer more flexibility about how to phrase and probe items, while still covering a consistent set of disorders.\n\nThe K-SADS-E (Epidemiological version) was developed for epidemiological research. It focused on current issues and episodes only. Most of the items used a four point rating scale.\n\nThe K-SADS-PL (Present and Lifetime version) is administered by interviewing the parent(s), the child, and integrating them into a summary rating that includes parent report, child report, and clinical observations during the interview. The interview covers both present issues (i.e., the reason the family is seeking an evaluation) as well as past episodes of the disorders. Most items use a three point rating scale for severity (not present, subthreshold, and threshold—which combines both moderate and severe presentations). It has been used with preschool as well as school-aged children. A 2009 working draft removed all reference to the DSM-III-R criteria (which were replaced with the publication of the DSM-IV in 1994) and made some other modifications. A DSM-5 version is being prepared and validated.\n\nThe WASH-U K-SADS (Washington University version) added items to the depression and mania modules and used a six point severity rating for severity.\n\n"}
{"id": "1809532", "url": "https://en.wikipedia.org/wiki?curid=1809532", "title": "Smith's fracture", "text": "Smith's fracture\n\nA Smith's fracture, also sometimes known as a reverse Colles' fracture or Goyrand-Smith's, is a fracture of the distal radius. It is caused by a direct blow to the dorsal forearm or falling onto flexed wrists, as opposed to a Colles' fracture which occurs as a result of falling onto wrists in extension. Smith's fractures are less common than Colles' fractures.\n\nThe distal fracture fragment is displaced volarly (ventrally), as opposed to a Colles' fracture which the fragment is displaced dorsally. Depending on the severity of the impact, there may be one or many fragments and it may or may not involve the articular surface of the wrist joint.\n\nThis fracture is named after the orthopedic surgeon, Robert William Smith (1807–1873) in his book \"A Treatise on Fractures in the Vicinity of Joints, and on certain forms of Accidents and Congenital Dislocations\" published in 1847.\n\nTreatment of this fracture depends on the severity of the fracture. An undisplaced fracture may be treated with a cast alone. A fracture with mild angulation and displacement may require closed reduction (putting into place without surgery). Significant angulation and deformity may require an open reduction and internal fixation. An open fracture will always require surgical intervention.\n\n"}
{"id": "55905540", "url": "https://en.wikipedia.org/wiki?curid=55905540", "title": "Sociology of Sport Journal", "text": "Sociology of Sport Journal\n\nThe Sociology of Sport Journal is a quarterly peer-reviewed academic journal covering the sociology of sport. It was established in 1984 and is published by Human Kinetics Publishers on behalf of the North American Society for the Sociology of Sport, of which it is the official journal. The editor-in-chief is Michael D. Giardina (Florida State University). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.000. \n"}
{"id": "29990881", "url": "https://en.wikipedia.org/wiki?curid=29990881", "title": "Stolen and missing Moon rocks", "text": "Stolen and missing Moon rocks\n\nOf the 270 Apollo 11 Moon Rocks and Apollo 17 Goodwill Moon Rocks that were given to the nations of the world by the Nixon Administration, approximately 180 are unaccounted for. Many of the Moon rocks that are accounted for have been locked away in storage for decades. The location of the rocks has been tracked by researchers and hobbyists because of their rarity and the difficulty of obtaining more. Moon rocks have been subjects of theft and forgery as well.\n\nIn 1998, a unique federal law enforcement undercover operation was created to identify and arrest individuals selling bogus Moon rocks. This sting operation was known as Operation Lunar Eclipse. Originally two undercover agents were involved in this sting, Senior Special Agent Joseph Gutheinz of NASA's Office of Inspector General (NASA OIG), posing as Tony Coriasso, and Inspector Bob Cregger of the United States Postal Inspection Service, posing as John Marta. This sting operation was later expanded to include Agents from the United States Customs Service, namely, Special Agent Dwight Weikel and Special Agent Dave Atwood. Agents posted a quarter page advertisement in \"USA Today\" asking for Moon rocks. The Agents were targeting individuals selling bogus Moon rocks, which con artists sell to the elderly and to space enthusiasts. The sting operation was led by NASA OIG Senior Special Agent Joseph Gutheinz. For the first time in history the sting operation recovered an Apollo era Moon rock, the Honduras Goodwill Moon Rock. This Moon rock had been given to Honduras by President Nixon, fallen into private hands, and then offered to the Agents for $5 million. In order to recover this Moon rock, the agents had to come up with the $5 million requested by the seller. Billionaire and one-time Presidential Candidate H. Ross Perot was asked by one of the agents to put up the money, and did so.\n\nAfter leaving NASA for a teaching position at the University of Phoenix, in Arizona, Gutheinz challenged his criminal justice graduate students to locate the goodwill Moon rocks.\n\nHe subsequently extended this project to also cover the missing Apollo 11 Moon rocks President Nixon gave to the states and nations of the world in 1969. Hundreds of graduate students have participated in this project from 2002 to the present and while many Moon rocks have been found, others are now known to be missing, stolen, or destroyed. Gutheinz patterned this college project after NASA's earlier Operation Lunar Eclipse, which he had participated in. Beginning in 2002, his graduate students began reporting to him that both the Cyprus Apollo 11 Moon rock (which is actually a collection of lunar dust in a Lucite ball) and Cyprus Apollo 17 Goodwill Moon Rock (a pebble-size Moon rock) were missing. Operation Lunar Eclipse and the Moon Rock Project were the subject of the book \"The Case of the Missing Moon Rocks\" by Joe Kloc.\n\nThe experts and politicians in New Jersey, including former Governor Brendan Byrne, had no idea of where it was, or of the state even receiving it.\n\nApollo 11 display missing.\n\nApollo 11 display missing.\n\nWhile the Apollo 17 Goodwill Moon rock presented to Cyprus was recovered, the Apollo 11 rock given to the country remains missing.\n\nIn his June 26, 2011 Op/Ed appearing in the \"Cyprus Mail\" entitled \"Houston we have a problem: we didn't give Cyprus its moon rock\", Joseph Gutheinz revealed that after NASA recovered the Cyprus Apollo 17 Goodwill Moon Rock over a year ago they failed to give the Moon rock to its legal owner, the nation of Cyprus.\n\nApollo 11 display missing.\n\nThe Apollo 11 rock presented to Ireland was accidentally discarded in a landfill known as the Dunsink Landfill in October 1977 following a fire that consumed the Meridian room library at the Dublin Dunsink Observatory where the rock was displayed. Cleo Luff, a student from the University of Phoenix, obtained this information after her investigation into the moon rock's location for a class she had with Professor Joseph Gutheinz. The Apollo 17 Goodwill Rock remains with the National Museum of Ireland.\n\nOn May 18, 2004, Malta’s Goodwill Moon Rock was stolen from Malta’s Museum of Natural History in Mdina. According to an Associated Press story appearing in \"USA Today\" \"there are no surveillance cameras and no custodians at the Museum of Natural History because of insufficient funding. The only attendant is the ticket-seller\"… \"A Maltese flag displayed next to the rock — which the U.S. astronauts had taken up with them — was not taken\". Joseph Gutheinz, a retired NASA Office of Inspector General Special Agent who heads up a \"Moon Rock Project\" at the University of Phoenix (where he assigns his students the task of hunting down missing Moon rocks), urged the Maltese authorities to grant an amnesty period to the thieves. He advised that only an amateur thief would have taken the Maltese Goodwill Moon Rock and left the plaque and flag behind, as all three together would have been self-authenticating and eliminated the risk of a geologist needing to authenticate the Moon rock.\n\nMalta’s Goodwill Moon Rock hasn't been recovered and continues to be actively pursued.\n\nApollo 17 display missing.\n\nUniversity of Phoenix graduate students uncovered evidence that the Romania Goodwill Moon Rock may have been auctioned off by the estate of its executed former leader, Nicolae Ceaușescu. Both Nicolae Ceaușescu and his wife, Elena Ceaușescu, were executed by firing squad on December 25, 1989, for the crime of genocide. As late as 2009, Romania believed it only received one Moon rock from the Nixon Administration, the Apollo 11 Moon rock, and took issue with those who argued otherwise. Joseph Gutheinz provided Daniel Ionascu of the Jurnalul information from the U.S. National Archives which showed that the Romanian Goodwill Moon Rock was presented to Romania. Romania’s Apollo 11 Moon Rock is at the National History Museum in Bucharest.\n\nEvidence surfaced that both Spain’s Apollo 11 Moon Rock and Apollo 17 Goodwill Moon Rock which were given to General Francisco Franco’s Administration by the Nixon Administration were missing. Pablo Jáuregui, the Science Editor of \"El Mundo\", a Spanish newspaper, disclosed in a July 20, 2009 story entitled: \"Franco's grandson: My mother lost Moon stone given her by Grandfather\" that the Spanish Apollo 17 Goodwill Moon Rock had finally been given back to the people of Spain in 2007 by the family of Admiral Luis Carrero; and Jáuregui suggested that Spain’s Apollo 11 Moon Rock, as referenced in the title of the story, was last known to be in the Franco’s families hands, and is now unaccounted for. Jáuregui wrote, as translated: \"As for the stone that Kissinger gave Carrero Blanco, confirmed yesterday\" by \"the son of …Admiral Luis Carrero Blanco\"… \"the stone was in possession of the family (first in the home of his widow, and after that of his eldest son ), until in 2007 they decided to donate it to the Naval Museum, where it is\"…on display…. \"today, along with a Spanish flag which traveled aboard the Apollo 17 mission to the moon. \" My son told me that the gift was dedicated to 'Spanish people', so it seemed right to donate it,\" recalls Luis Carrero Blanco.\" Blanco was assassinated while in Office by ETA, a Basque separatist organization recognized as terrorist by Spain, France, the UK, the US, and the European Union.\n\nAs for Spain’s Apollo 11 Moon Rock the trail is more confused. Jáuregui relates the following from Franco’s grandson: \"The grandson of Franco stressed that neither he nor any other member of his family\" had been told \"that there might be some legal or ethical problem\" regarding \"the Moonstone. If you get anything and it's yours, why not going (translation) to sell?\" He said. \"In any case the rock never sold, but according to Franco, now\" he does \"not know where it is. As my mother is a woman with many things in many houses, in a move or redecorate a room, in the end had to go astray,\" he explains. Students assigned to the Moon Rock Project are currently looking for leads to Spain’s Apollo 11 Moon Rock in Switzerland.\n\nApollo 17 display missing.\n\nElizabeth Riker was assigned the task of hunting down the Alaska Apollo 11 Moon Rock by her professor. On August 18, 2010, in a story she wrote about her investigation in the \"Capital City Weekly\" newspaper, of Juneau Alaska, she stated that after conducting a thorough investigation for Alaska's Apollo 11 Moon Rock she has concluded that it is missing. She advised that she planned to continue to look for the Moon rock and asked for the help of the citizens of Alaska to accomplish her goal of finding it.\n\nIn 1973, there was a massive fire at the Alaska Transportation Museum where the Moon rocks were being housed. Coleman Anderson (a crab-fishing captain who was on the TV show \"Deadliest Catch\") claimed to have gone to the museum to scrounge through the garbage from the fire to see if there would be anything worth saving. Anderson, who was a child at the time, claimed to have found the Moon rocks and cleaned them up over the next few years.\n\nTo clear title to the rocks he filed a lawsuit against the State of Alaska, asking the Court to deem the rocks his sole property.\n\nThe missing Moon rocks were returned by Anderson as of December 7, 2012.\n\nIn a front-page story, the \"Arkansas Democrat-Gazette\" listed numerous sources suggesting the Arkansas Goodwill Moon Rock had gone missing noting that the rock was potentially worth five million dollars. The rock was presented to the state by astronaut Richard H. Truly in 1976 at a Boy Scout event in Little Rock. Its whereabouts remained unknown until September 21, 2011, when it was discovered by Michael Hodge, an archivist with the Butler Center for Arkansas Studies, while processing the gubernatorial papers of Bill Clinton.\n\nBased on the investigation of a graduate student, former Governor John Vanderhoof, then age 88, acknowledged he had the Goodwill Moon Rock presented to the people of Colorado in his personal possession and agreed to give it back to the state. On August 25, 2010, the Colorado Goodwill Moon Rock was unveiled at the Colorado School of the Mines Museum by Dr. Bruce Geller, the museum curator.\n\nFlaws in the State of Hawaii inventory control system were highlighted in 2009 when an estimated $10 million in Moon rocks from Apollo 11 and the Apollo 17 Goodwill Rock could not be located. Curators and officials at every museum and university in the state, along with then Governor Linda Lingle’s office, capitol, and state archives, were contacted but none knew of the whereabouts of the items. Both Moon rocks were later found in a \"routine inventory of gifts given to the governor’s office over the years,\" in a locked cabinet in the Governor’s Office. A senior advisor to the governor vowed to increase security and register the items with the state's Foundation of Cultural Arts.\n\nConfusion erupted in 2010 when employees with the Missouri State Museum and the Missouri State Department of Natural Resources claimed that Missouri's Apollo 17 Goodwill Moon Rock was in storage. Photos in news stories about the location of the rock were later identified as coming from Apollo 11. Then Senator Kit Bond, who was the Governor of Missouri when the Apollo 17 Goodwill Moon Rock was gifted to the state, stated that he has no recollection of receiving a Moon rock and The Missouri State Archives, and the State Museum, reversing what they had previously stated, have no information on Missouri having the Apollo 17 Goodwill Moon Rock concluding that it was presumed missing. The rock was later found amongst Bond's possessions by his staff and it was returned to the state.\n\nProfessor Christopher Brown, Director of the N.C. Space Grant and professor at the University of North Carolina Chapel Hill turned the Moon rock over, along with related items, to the North Carolina Museum of Natural Sciences where it is planned for permanent display in the Fall of 2011 when the museum expansion is completed. Brown obtained the rock from a colleague in 2003 who found it in a desk drawer at the state Commerce Department. Brown's colleague received permission to lend the artifact to Brown who used it in presentations on space and space-related science to students over the next several years.\n\nToni Dowdell, a graduate student at the University of Phoenix, was assigned the task of hunting down the Oregon Apollo 11 Moon Rock while two of her teammates were charged with hunting down the Apollo 17 Goodwill Moon Rocks of Oregon and Louisiana. Toni Dowdell and her two teammates received this assignment from her professor, a retired Senior Special Agent with NASA's Office of Inspector General. This assignment was part of an ongoing assignment known as the Moon Rock Project, where students are assigned the task of hunting down Moon rocks all over America and the world. In a February 19, 2010 article Toni Dowdell wrote for the \"Daily News\" of Greenville, Michigan, Dowdell described how her teammates in this investigation discerned that both the Apollo 17 Goodwill Moon Rocks of Oregon and of Louisiana remain unaccounted for, but how she successfully tracked down her assigned Moon rock, the Oregon Apollo 11 Moon Rock. As with many Moon rock gifts the Nixon Administration gave to the states and the nations of the world the first problem she encountered was a lack of a document trail. However, by reaching out to people, to include an operator in the state Capitol, she found the Moon rock hidden in the ceremonial Governor’s Office of Oregon. According to moon rocks researcher Robert Pearlman, the Oregon Apollo 17 rock display is on permanent exhibit in the Earth Science Hall of the Oregon Museum of Science and Industry in Portland.\n\nSandra Shelton, a graduate student at the University of Phoenix, was assigned the task of hunting down the West Virginia Apollo 17 Goodwill Moon Rock by her professor, a retired Senior Special Agent with NASA's Office of Inspector General. This moon rock was presented to West Virginia in 1974 and is valued at $5 million. On May 16, 2010, Rick Steelhammer of the \"Gazette-Mail\" of Charleston, West Virginia wrote a front-page story documenting Sandra Shelton's investigative findings which revealed that the West Virginia Goodwill Moon Rock was missing. Following that story, retired dentist Robert Conner called Shelton and told her that he had the West Virginia Goodwill Moon Rock. Shelton informed her professor, who advised the Governor's Office. Dr. Conner said that his deceased brother was the former business partner of former West Virginia Governor Arch A. Moore, Jr., and that Conner acquired the moon rock upon the death of his brother from his brother's belongings. In her June 29 story appearing in the Denver Post reporter Sarah Horn wrote that Shelton was awarded a certificate by the Governor of West Virginia, Joe Manchin, for her role in recovering the West Virginia Goodwill Moon Rock.\n\nIn 1972, then 13-year-old Jaymie Matthews, now Astronomy Professor at the University of British Columbia, lied about his age in order to compete in an essay contest, the winner of which would serve as a participant in a \"10-day International Youth Science Tour, in which all the countries in the United Nations were invited to offer up \"youth ambassadors\" aged 17 to 21. These youth ambassadors were to witness first-hand the launch in Florida...\" of Apollo 17…\" Eighty countries accepted the invitation, including Canada. Matthews won the contest, and when his true age came out, Canadian officials decided to let him go anyway. As the student ambassador, it was decided that Canada's Goodwill Moon Rock was mailed to him where he kept it at his home. Eventually he was asked to turn the Moon rock over to Canada, which he did. The rock was reportedly stolen in 1978, while on tour.\n\nIn 2003, University of Phoenix graduate students tracked down the rock to a storage facility at the Canadian Museum of Nature. After 30 years of sitting in storage, the Canadian Goodwill Moon Rock finally went on display at the Canada Science and Technology Museum in Ottawa, on July 23, 2009.\n\nMoon rocks from Apollo 11 and Apollo 17 presented to the island nation of Cyprus were believed to have been destroyed or stolen in 1974 during the Turkish invasion. In September 2009, while cooperating with a worldwide hunt for Moon rocks with Associated Press reporter Toby Sterling (Netherlands Bureau) and \"Cyprus Mail\" reporter Lucy Millett, the daughter of the British Ambassador to Cyprus, Gutheinz was advised by his friend and space memorabilia expert Robert Pearlman (CollectSpace.com) that Pearlman had learned in 2003 that the Cyprus Goodwill Moon Rock was never presented to Cyprus, but retained by the son of an American diplomat. The American government was advised about this situation in 2003 and did nothing. Upon learning the truth Gutheinz reached out to both the American Embassy in Cyprus and the Cyprus Government to convey the facts; he then filed a request for a Congressional Inquiry into the case of the missing Cyprus Goodwill Moon Rock. Subsequently, he caused the facts about the Moon rock to be published in the press in order to motivate the person who had the Moon rock to do the right thing, and return it.\nThe diplomat’s son thereafter began negotiating with NASA's Office of Inspector General, and did so for 5 months until the Cyprus Goodwill Moon Rock was recovered. The diplomat's son's name has never been disclosed.\n\nDuring \"Lunar Eclipse\", Florida businessman Alan H. Rosen, attempted to sell agents the 1.142 gram Goodwill Moon rock presented to Honduras for 5 million dollars. After two months of negotiations with Rosen, the Moon rock was seized from a Bank of America vault. The rock immediately became the subject of a 5-year civil suit, \"United States of America v. One Lucite Ball containing Lunar Material (one Moon Rock) and One Ten Inch by Fourteen Inch Wooden Plaque\", which resulted in the forfeiture of the rock to the Federal Government on March 24, 2003. The rock was refurbished at Johnson Space Center, to be once again presented to the people of Honduras. In a September 22, 2003 ceremony at NASA's Headquarters in Washington, D.C., NASA Administrator Sean O'Keefe presented the Moon rock to Honduran Ambassador Mario M. Canahuati. Also in attendance at this ceremony was Joseph Gutheinz, the leader of the sting operation, who gave a first hand account of the rock's recovery to Ambassador Canahuati. On February 28, 2004, O'Keefe flew to Honduras to formally present the Moon rock to Honduran president Ricardo Maduro. In 2007, Gutheinz, a past recipient of the NASA Exceptional Service Medal, was featured in the BBC Two documentary \"Moon for Sale\" talking about the Honduras Goodwill Moon Rock and this unique case. Today the Honduras Goodwill Moon Rock is on display at the Centro Interactivo Chiminike, an education center in Tegucigalpa that receives hundreds of young student visitors per day.\"\n\nThe Irish Apollo 17 Goodwill Moon Rock is located at the National Museum of Ireland. The Apollo 17 Goodwill Moon Rock was given to Irish President Erskine Childers who later died in office. When the widow of President Childers, Rita Dudley Childers, requested the rock as a keepsake of her late husband, the former first lady’s request was denied, as the Irish Government reasoned the Irish Goodwill Moon Rock belonged to the people of Ireland and not just to one individual.\n\nAP reporter Ken Ritter wrote that the Nicaragua Apollo 11 Moon Rock \"given by then-President Richard Nixon to former Nicaraguan dictator Anastasio Somoza Garcia would have been pilfered by a Costa Rican mercenary soldier-turned Contra rebel, traded to a Baptist missionary for unknown items, then sold to a Las Vegas casino mogul who displayed them at his Moon Rock Cafe before squirreling them away in a safety deposit box.\" The Apollo 11 Moon Rock was returned to the people of Nicaragua in November 2012.\n\nIn April 2013, Karen Nelson, an archivist at Lawrence Berkeley National Laboratory, found 20 vials of Moon dust from the Apollo 11 mission with handwritten labels dated \"24 July 1970\" in a warehouse at the Berkeley lab. They had been there for around 40 years and were forgotten about. NASA requested it be returned to the agency.\n\nOn April 23, 2012 at a restaurant in Buffalo, Texas, \"Moon Rock Hunter\" Joe Gutheinz met with a 67-year-old former toy manufacturer named Rafael Navarro, who claimed to have an Apollo 11 Moon rock given to him by \"a maid, now elderly and in failing health, who worked for a Venezuelan diplomat who told people it was a Moon rock\". Navarro was offering shavings from the rock for $300,000 on eBay. After looking at the sample through a microscope and later examining documents given him by Navarro, Gutheinz is skeptical of Navarro's claim, stating \"...this is a train wreck waiting to happen for him, and he's inviting it. He's opening the jail cell door and walking through it.\"\n\nIn an October 23, 1999 story entitled \"Atlanta Man Admits Trying to Sell Bogus Moon Rock\", Reuters reported two brothers, Ronald and Brian Trochelmann, who were previously charged in 1998 in \"U.S. District Court in Manhattan…\"for…\"a scheme to sell a phony moon rock for millions of dollars,\" both plead guilty to wire fraud, a felony, for perpetrating that scheme. \"The brothers claimed that their father had invented a space-food packaging process that was used by the National Aeronautics and Space Administration during the Apollo moon missions of the 1960s. The Trochelmann’s alleged that the rock had been brought from the moon by Apollo 12 astronaut Alan Bean and given to John Glenn. They claimed Glenn, the first American to orbit Earth and later a U.S. senator, had given the rock to their father in recognition of his supposed invention.\" …\" The brothers had negotiated a consignment agreement with Phillips Son & Neale, a Manhattan auction house, to sell the rock in December 1995. However, before the auction took place, the rock was confiscated by FBI agents in December 1995 prior to the scheduled auction.\" This story first broke in a \"New York Times\" Article written by Lawrence Van Gelder on December 2, 1995. At that time NASA expressed the belief that the Moon rock might have been real as it matched the general description of a Moon rock that was stolen in 1970. \"Eileen Hawley, a spokeswoman for NASA, said of the sample offered through Phillips & Neale: We have a rock that is classified as lost, an Apollo 12 lunar sample of approximately the same weight. With that information, we need to look at this—that this might be a true lunar sample. Ms. Hawley said a rock sample collected during the Apollo 12 mission had been part of a shipment of registered and certified mail that was stolen while en route to a researcher at the University of California in Los Angeles in 1970. The space agency received a call on Thursday from the Postal Investigative Service in New York, she said, after articles about the impending auction had been published. The service passed along a tip from the retired inspector, who was not identified, about a possible connection between the theft and the rock to be auctioned.\" This scheme and schemes like it were the inspiration for the undercover sting operation known as Operation Lunar Eclipse, which resulted in the acquisition of the Honduras Goodwill Moon Rock in December 1998.\n\nIn his November 4, 1969 article appearing in the \"Fort Scott Tribune\" entitled \"Fake Lunar Rock Racket Feared\" NEA Staff correspondent Tom Tiede first predicted a market for fake Moon rocks, a market subsequently given extra momentum as Moon rocks began to be reported lost and stolen. Tiede gave a few examples to support his prediction: \"In Miami Florida a housewife had been approached by a door to door salesman dealing in lunar rocks. She bought five dollars worth;\" \"In Redwood City, Calif., a woman [published an advertisement] announcing moon dust for sale. At $1.98 an ounce;\" \"In New York, the Harlem Better Business Bureau [was] cautioning consumers against purchasing any kind of obviously fake moon substances.\"\n\nIn his August 28, 2009 Associated Press story appearing in the \"Brisbane Times\", Toby Sterling recounted how a spokesman for the Dutch National Museum, Amsterdam's Rijksmuseum, acknowledged on August 26, 2009, \"that one of its prized possessions, a rock supposedly brought back from the moon by\"…Apollo 11… \"US astronauts, is just a piece of petrified wood..\"… \"The museum acquired the rock after the death of former prime minister Willem Drees in 1988. Drees received it as a private gift on October 9, 1969 from then-US ambassador J. William Middendorf during a visit by the three Apollo 11 astronauts, part of their ‘Giant Leap’ goodwill tour after the first moon landing.\" The museum acknowledged that though they did vet the moon rock they failed to double check it. The museum was under the incorrect belief that this Moon rock was one of the 135 Apollo 11 Moon rocks that were presented to the nations of the world by the Nixon Administration. \"It's a nondescript, pretty-much-worthless stone,\" said Frank Beunk, a geologist involved in the investigation. The genuine Apollo 11 Moon rock given to the Dutch is in the inventory of a different museum in the Netherlands, which is, in fact, one of the countries where the location of both the Apollo 11 and Apollo 17 gift rocks is known.\n\nIn June 2002, 101 grams of Moon rocks were stolen from the Johnson Space Center by interns Thad Roberts and Tiffany Fowler. The pair used knowledge of the security around the rocks gained during their internship to remove a safe in building 31 North containing the samples. Roberts is a certified pilot and scuba diver who was an ambitious student pursuing degrees in physics, geology, and anthropology who aspired to be an astronaut. Fellow interns Shae Saur and Tiffany Fowler, as well as accomplice Gordon McWhorter were also arrested for their roles in the theft and attempted sale of the rocks. The theft also included a meteorite that may have revealed information about life on Mars.\n\nRoberts advertised the rocks on a Belgian mineralogy club website which was forwarded to the FBI who, with the help of Belgian amateur rock collector Axel Emmermann, set up a sting. On July 20, 2002, the 33rd anniversary of the Apollo 11 landing, a number of FBI agents posed as diners at a chain Italian restaurant on Orlando's International Drive for a pre-arranged meeting with Roberts. Roberts met and spoke with one of the Special Agents at length. A second Agent--posing as the husband of the first--joined the two, and then two of the three accomplices joined them. The meeting moved to a nearby hotel where dozens of FBI agents arrested Roberts, McWhorter, and Fowler and recovered the missing lunar samples. Roberts was also charged with stealing dinosaur bones and other fossils from his school, the University of Utah.\n\nThe theft was the subject of Ben Mezrich's book \"Sex on the Moon: The Amazing Story Behind the Most Audacious Heist in History\". Axel Emmermann, Gordon McWhorter, NASA Principal Investigator Dr. Everett Gibson and investigating officers of the FBI and NASA Office of the Inspector General were interviewed on camera for a National Geographic Channels \"Explorer\" special called \"Million Dollar Moon Rock Heist\", first broadcast in the US on March 4, 2012. Testimony given therein is at odds with some of the key claims made in Mezrich's account.\n\nIn an Aviation and Space Technology article published on September 27, 1976 entitled \"Lunar Sample Damaged by Vandals\" the author addresses a vandalism and possible theft attempt against a 40 gram Apollo 17 Moon rock. The author states that the \"Apollo 17 lunar sample on open display at the Smithsonian Institution's National Air and Space Museum was slightly damaged…during an apparent vandalism attempt. It is possible that theft was the object of the attack on the sample, but both museum and National Aeronautics and Space Administration officials believe vandalism was the primary objective. About two cubic millimeters of the triangular fine-grained basalt were chipped away during the incident that involved a hard blow to the sample with a sharp object. NASA believes no part of the sample was obtained by the vandal. The area around the sample's display case was swept immediately after the incident, and the sweeper bag is now at the Johnson Space Center, where it is being sifted in an attempt to obtain the missing material.\"\n\nThe author stated that \"The 40-gram sample on display is the first touchable moon rock. Museum visitors are able to feel directly the texture of the lunar material, a departure from strict NASA policy that dictates that no individual ever handle lunar samples directly as a guard against contamination. \"\n\nIn an August 8, 1986 article written by United Press International entitled \"Police Look for Stolen Moon Rocks\" the author wrote: \"Memphis police are looking for some moon rocks taken from a NASA van that was stolen.\" The van was assigned to Louis Marshall of Memphis, who conducts education programs for the National Aeronautics and Space Administration. The van was stolen from outside his home Tuesday night, driven to a field and set afire, police said Friday. A space suit in the van was left to burn. But thieves took some lunar rock and soil specimens, police said. Marshall said it was hard to put a value on them. It's stuff that belongs to all of us,' he said.' I'm out of business right now,' said Marshall. It will take a while to replace the items, he said. NASA officials said that out of of Moon rock retrieved through the years, the sample was not a big loss. I don't know what value it would be except just to gloat over it personally,' said NASA spokesman Terry White about the theft. White said theft is not a common problem with the NASA exhibits, which are shown to schools around the country.’ I’d always thought, Who's going to mess with a big red van with NASA on it?' Marshall said.\" There is no indication that this theft was related to a Moon rock theft that followed just a few days later in Louisiana.\n\nA set of six fragments of Moon rocks used in educational programs were stolen from the Louisiana Science and Nature Center by ripping a small safe out of a wall. The case remains unsolved.\n\nOn January 10, 2006, Rudo Kashiri, an education specialist employed by NASA, reported that someone broke into a van that was parked in the driveway of her home in Virginia Beach, Virginia and made off with a collection of NASA Moon rocks. The rocks were in a safe that was bolted to the van. The safe may or may not have been properly locked. As an Education Specialist for NASA, Kashiri’s job involved bringing the Moon rocks to schools and showing them to students. These Moon rocks have not been recovered.\n\n"}
{"id": "46893591", "url": "https://en.wikipedia.org/wiki?curid=46893591", "title": "Structural fold", "text": "Structural fold\n\nStructural folding is the network property of a cohesive group whose membership overlaps with that of another cohesive group. The idea reaches back to Georg Simmel's argument that individuality itself might be the product of unique intersection of network circles.\n\nIt has been proposed that successful firms often cluster together in cohesive groups, as dense ties among the group members reduce transaction cost by providing a basis for trust and coordination amongst the firms. Cohesive ties also enable the firms to implement projects beyond their capacity and cushions them against great uncertainty \nHowever, another logic suggests that business groups might choose to forgo high density within the group in favour of maintaining some weaker ties to other firms outside the group. This way firms can reduce redundant ties and form long-distance links to other firms who can provide novel information to them. This logic rests on the assumption that the conservatizing strategy of in-group cohesion is maladaptive as it risks locking the businesses into early success and strategies, which in the absence of new information can easily become detrimental in a rapidly changing business environment.\n\nA third, different, strategy would be to combine the benefits of the previous two. Such solutions can be termed either “closure and brokerage” or “cohesion and connectivity” and the benefits of the complementarity of these distinctive network features is common to them, especially for entrepreneurship. Actors at structural folds are multiple insiders, benefiting from dense cohesive ties that provide familiarity with the operations of the members in their group. However, due to being part of more than one group they also have access to diverse information. This combination of familiarity and diversity facilitates innovation and creative success through recombining resources.\n\nIntercohesion is a distinctive network structure built from intersecting cohesive groups. It rests on the theoretical principle that cohesive group structures are not necessarily exclusive, but network structures can actually be cohesive and overlapping. This idea originates from Georg Simmel's who, in one of his works, argued that a person is often a member of more than one cohesive group in the same time, and these multiple group memberships are part of both individuation and social integration of the person. Intercohesion thus refers to mutually interpenetrating, cohesive structures, while the resulting distinctive network position at the intersection is a structural fold.\n\nAs actors at structural folds can be considered multiple insiders, who benefit from both dense cohesive ties that provide familiarity with the operations of the members in their group and from access to non-redundant information, they are believed to be in a better position for innovation and creative success. \nBased on the Schumpeterian understanding of the term, entrepreneurship is conceptualised as knowledge production through recombination, rather than just importing new ideas. Thus, actors at structural folds occupy a privileged position for successful innovation and creativity. On the one hand, they are part of a cohesive group that provides deeply familiar access to knowledge bases and productive resources, which are essential for generative recombination. On the other hand, they also have the opportunity to interact across different groups and have access to a diverse set of sources, which are also considered key for innovative recombination of already existing resource. Vedres and Stark (2010) in their study indeed found that structural folding contributed to higher performance of business groups.\n\nMoreover, entrepreneurship also has dynamic properties along the temporal dimension. As Schumpeter observed entrepreneurship also brings along what he called creative destruction. Applied to network analytical terms, structural folding may in fact destabilise groups as overlapping membership can disrupt group coordination and reciprocal trust. On an empirical level, it was found that in the case of Hungarian businesses, groups with more structural folds are more likely to break up, and when they do so they are likely to fragment into smaller groups.\n\nIt has also been argued that structural folding also contributes to creative success in the case of cultural products especially, when the overlapping groups are cognitively distant. For creative innovation, teams need to have a diversity of stylistic elements for recombination. In cultural fields, where teams periodically assemble, dissolve and reassemble the knowledge base of team resides in the previous experience of its members with various styles. Cognitively diverse groups held in tension by structural folds have both a greater repertoire of action and the ability to recontextualize knowledge. Thus, structural folding improves the likelihood of creative success and innovation by increasing the possibility to override things that are taken for granted and to think more reflexively.Therefore cognitively distant but overlapping cohesive group structures are productive because of the mixing, ambiguities and tensions they encounter.\n"}
{"id": "34113223", "url": "https://en.wikipedia.org/wiki?curid=34113223", "title": "Tadeáš Polanský", "text": "Tadeáš Polanský\n\nTadeáš Polanský, , (1713 – 1770) was a Jesuit theologian and scientist in the field of physics.\n\nTadeáš Polanský was since 1734 lecturing at a Gymnasium (school) in Uherské Hradiště. Later he became professor of dogmatics at University of Olomouc. Apart from theology, he was, together with Polanský Jan Nepomuk (1723 – 1776), active in the field of physics, researching lightning and thunder, colors, phases of Venus, primary and secondary rainbow, and other. In years 1760 – 1761, Polanský became dean of the University's Faculty of Theology.\n\nHe was lecturing in Olomouc at the time when Joseph II, Holy Roman Emperor stepped up his fight for absolute power, effectively restricting the Jesuit monopoly in education. At the University of Olomouc, the struggle was taking place particularly between the conservative Jesuits and a proponent of enlightenment ideas Josef Vratislav Monse.\n\nPolanský's book \"Manudictio sacerdotis ad com-modissime subeundum examen pro approbatione et jurisdictione\", printed in 1762, was banned in 1769. Later, in February 1771, a directive explicitly ordered \"Ausrottung\" (extermination) of the book, while another one from May 1771 introduced heavy fines for mere possession of the book.\n\n"}
{"id": "42248286", "url": "https://en.wikipedia.org/wiki?curid=42248286", "title": "The Simpsons and Their Mathematical Secrets", "text": "The Simpsons and Their Mathematical Secrets\n\nThe Simpsons and Their Mathematical Secrets is a 2013 book by Simon Singh, which is based on the premise that \"many of the writers of \"The Simpsons\" are deeply in love with numbers, and their ultimate desire is to drip-feed morsels of mathematics into the subconscious minds of viewers\".\n\nThe book compiles all the mathematical references used throughout the show's run, and analyzes them in detail. Rather than just explaining the mathematical concepts in the context of how they relate to the relevant episodes of \"The Simpsons\", Singh \"uses them as a starting point for lively discussions of mathematical topics, anecdotes and history\".\n\n\"The Guardian\" described it as a \"readable and unthreatening introduction to various mathematical concepts\". \"The New York Times\" described it as a \"highly entertaining book\".\n"}
{"id": "44139004", "url": "https://en.wikipedia.org/wiki?curid=44139004", "title": "Unidentified decedent", "text": "Unidentified decedent\n\nUnidentified decedent or unidentified person (also abbreviated as UID or UP) is a term in American English used to describe a corpse of a person whose identity cannot be established by police and medical examiners. In many cases, it is several years before the identities of some UIDs are found, while in some cases, they are never identified. A UID may remain unidentified due to lack of evidence as well as absence of personal identification such as a driver's license. Where the remains have deteriorated or been mutilated to the point that the body is not easily recognized, a UID's face may be reconstructed to show what it had looked like before death. UIDs are often referred to by the placeholder names \"John Doe\" or \"Jane Doe\".\n\nThere are approximately 40,000 UIDs in the United States, and numerous others elsewhere. A body may go unidentified due to death in a state where the person was unrecorded, an advanced state of decomposition or major facial injuries. In many cases in the United States, teenagers with a history of running away would be removed from missing person files when they would turn 18, thus eliminating potential matches with existing unidentified person listings.\n\nSome UIDs die outside their native state. The Sumter County Does, murdered in South Carolina, may have been Canadian. Barbara Hess Precht died in Ohio in 2006 but was not identified until 2014. She had been living as a transient with her husband in California for decades but returned to her native state of Ohio, where she died of unknown circumstances. In both of these cases, the UIDs were found in a recognizable state and had their fingerprints and dental records taken with ease. It is unknown if the Sumter County Does' DNA was later recovered, since their bodies would require exhumation to recover DNA. In Lourdes, France, a corpse believed to have been native to a different country was discovered. Many illegal immigrants that die in the United States after crossing the border from Mexico remain unidentified.\n\nMany UIDs are found long after they die and have decomposed severely. This significantly changes their facial features and may prevent identification through fingerprints. Environmental conditions often are a major factor in decomposition, as some UIDs are found months after death with little decomposition if their bodies are placed in cold areas. Some are found in warm areas shortly after death, but hot temperatures and scavenging animals deteriorated the features. In some cases, warm temperatures mummify the corpse, which also distorts its features, though the tissues have survived initial decomposition. One example is the \"Persian Princess\", who died in the 1990s but, in an act of archaeological forgery, was untruthfully stated in Pakistan to have been over 2,000 years old. A man found in Idar-Oberstein, Germany, in 1994 had died months before his body was found, yet in some places, his skin had not deteriorated and tattoos were found, which are also used to identify the dead.\n\nPutrefaction often occurs when bacteria decompose the remains and generate gasses inside, causing the corpse to swell and become discolored. In cases such as the Rogers family, who were murdered in 1989 by Oba Chandler, the bodies were deposited in water but surfaced after gasses in their remains caused them to float to the surface. They were deceased a short period of time but were already severely decomposed and unrecognizable, due to putrefaction that occurred while underwater and high temperatures. It was not until a week later that dental records revealed their identities.\n\nSkeletonization occurs when the UID has decayed to the point that bones and possibly some tissues are all that is found, usually when death occurred a significant amount of time before discovery. If a skeletonized body is found, fingerprints and toeprints are impossible to recover, unless they have survived the initial decomposition of the remains. Fingerprints are often used to identify the dead and were used widely before DNA comparison was possible. In some cases, partial remains limit the available information; for example, a woman's skull found in Frankfurt, Germany, was insufficient to estimate her height and weight.\n\nSkeletonized UIDs are often forensically reconstructed if searching dental records and DNA databases is unsuccessful.\n\nSome UIDs experience trauma that significantly alters their appearance, especially those who die in vehicular accidents or were murdered in a violent manner. A facial expression often represents the pain, which shifts the face into a position that would not commonly be seen by the public, including those who have seen the person when they were alive. A body found decapitated would also prove to be unrecognizable. Many UIDs whose heads were not recovered remain unidentified, like in the Whitehall Mystery, which occurred in the United Kingdom.\n\nOften, someone who tries to conceal a body attempts to destroy it or render it unrecognizable. The currently unidentified Yermo John Doe was killed approximately one hour before he was found, but was completely unrecognizable. When Lynn Breeden, a Canadian model, was murdered and set ablaze in a dumpster, her body was so severely damaged that DNA processing and fingerprint analysis were impossible. She was identified sometime later after her unique dentition matched her own dental records, and DNA extracted from her blood at a different scene was matched. Linda Agostini's body was found burned near Albury, Australia in 1934. Her remains were identified ten years later through dental comparison.\n\nUsually, bodies are identified by comparing their usually unique DNA, fingerprints and dental characteristics. DNA is considered the most accurate, but was not as widely used until the 1990s. It is often obtained through hair follicles, blood, tissue and other biological material. Bodies can also be identified with other physical information, such as illnesses, evidence of surgery, breaks and fractures, and height and weight information. A medical examiner will often be involved with identifying a body.\n\nMany police departments and medical examiners have made efforts to identify the deceased by placing mortuary photographs of the UID's face online. In some instances, the mortuary photographs would be retouched of wounds if they are to be released to the public. Dismembered corpses may also be digitally altered to appear attached to the body. This is not considered to be the most effective method, as the nature of death often distorts the UID's face. An example of this is that of \"Grateful Doe,\" who was killed in a vehicular crash in 1995. He sustained extreme trauma that disfigured his face.\n\nA Jane Doe found in a river in Milwaukee, Wisconsin, had died months earlier, but was preserved by the cold temperatures. Her morgue photographs were displayed publicly on a medical examiner's website, but her face had been distorted by swelling after absorbing water, with additional decomposition.\n\nDeath masks have also been used to assist with identification, which have been stated to be more accurate, as they are required to display \"relaxed expressions,\" which often do not illustrate the faces of the UIDs as they were found, such as that of L'Inconnue de la Seine, a French suicide victim found in the late 1800s. However, a death mask will still depict sunken eyes or other characteristics of a long-term illness, which do not often show how they would have looked in life.\n\nWhen a body is found in an advanced state of decomposition or has died violently, reconstructions are sometimes required to receive assistance from the public, when releasing images of a corpse is considered taboo. Often, those in a recognizable state will often be reconstructed due to the same reason.\n\nFaces can be reconstructed with a three-dimensional model or by 2D, which includes sketches or digital reconstructions, similar to facial composites.\n\nSketches have been used in a variety of cases. Forensic artist Karen T. Taylor created her own method during the 1980s, which involved much more precise techniques, such as estimating locations and sizes of the features of a skull. This method has been shown to be fairly successful.\n\nThe National Center for Missing and Exploited Children has developed methods to estimate the likenesses of the faces of UIDs whose remains were too deteriorated to create a two-dimensional sketch or reconstruction due to the lack of tissue on the bones. A skull would be placed through a CT scanner and the image would then be manipulated with a software that was intended for architecture design, to add digital layers of tissue based on the UID's age, sex and race.\n\nThe following gallery depicts various ways UIDs have been reconstructed. None of those shown have been identified.\n\nIn some cases, such as that of Colleen Orsborn, the true identity of the unidentified person is excluded from the case. In Orsborn's case, she had fractured one of the bones in her leg, but a medical examiner who performed the autopsy on her remains was not able to discover evidence of the injury and subsequently excluded her from the case. It was not until 2011 when DNA confirmed Orsborn was the victim found in 1984. In cases such as the Racine County Jane Doe, a rule out has also been subjected to criticism. Aundria Bowman, a teen who disappeared in 1989 who bore a strong resemblance to a body found in 1999, was excluded, according to the National Missing and Unidentified Persons System. On an online forum, known as Websleuths, users have disagreed with this ruling. In the case of Lavender Doe, a mother of a missing girl also disagreed with the exclusion of her missing daughter through DNA, as she claimed the reconstruction of the victim looked very similar to her daughter.\n\n\n\n\n"}
{"id": "46219708", "url": "https://en.wikipedia.org/wiki?curid=46219708", "title": "Unitary theories of memory", "text": "Unitary theories of memory\n\nUnitary theories of memory are hypotheses that attempt to unify mechanisms of short-term and long-term memory. James Nairne proposed the first unitary theory, which criticized Alan Baddeley's working memory model, which is the dominant theory of the functions of short-term memory. Other theories since Nairne have been proposed; they highlight alternative mechanisms that the working memory model initially overlooked.\n\nWorking memory is the system that is responsible for the transient holding and processing of new and already stored information, an important process for reasoning, comprehension, learning and memory updating. Working memory is generally used synonymously with short term memory, but this depends on how the two forms of memory are defined. Working memory includes subsystems that store and manipulate visual images or verbal information, as well as a central executive that coordinates the subsystems. It includes visual representation of the possible moves, and awareness of the flow of information into and out of memory, all stored for a limited amount of time.\n\nIn 1974, Baddeley and Hitch introduced and made popular the multicomponent model of working memory. This theory proposes a central executive that, among other things, is responsible for directing attention to relevant information, suppressing irrelevant information and inappropriate actions, and for coordinating cognitive processes when more than one task must be done at the same time. The central executive has two \"slave systems\" responsible for short-term maintenance of information, and a \"central executive\" is responsible for the supervision of information integration and for coordinating the slave systems. One slave system, the phonological loop (PL), stores phonological information (that is, the sound of language) and prevents its decay by continuously articulating its contents, thereby refreshing the information in a rehearsal loop. It can, for example, maintain a seven-digit telephone number for as long as one repeats the number to oneself again and again. The other slave system, the visuo-spatial sketchpad, stores visual and spatial information. It can be used, for example, for constructing and manipulating visual images, and for the representation of mental maps. The sketchpad can be further broken down into a visual subsystem (dealing with, for instance, shape, colour, and texture), and a spatial subsystem (dealing with location).\n\nIn 2000, Baddeley extended the model by adding a fourth component, the episodic buffer, which holds representations that integrate phonological, visual, and spatial information, and possibly information not covered by the slave systems (e.g., semantic information, musical information). The component is episodic because it is assumed to bind information into a unitary episodic representation. The episodic buffer resembles Tulving's concept of episodic memory, but it differs in that the episodic buffer is a temporary store.\n\nThe feature model was first described by Nairne (1990) The primary feature of this model is the use of cues for both short term memory and long term memory. Cues become associated with a memory and can later be used to retrieve memories from long term storage.\nWhen a memory is formed, cues associated with the formed memory form a constellation of nerve networks in which the memory is stored. While the cues are present, the nerves that form the constellation are active and the memory is being worked on in short term memory. When the cues are no longer present, the constellation is no longer active and is waiting in long term memory storage for when the cues are once again present.\nRather than using the theory of decay for explaining how memories get forgotten, the feature model focuses on item based interference instead. This means that other items that use the same or similar constellations of cues get in the way of remembering other memories. For example, if you are trying to remember a fruit from a list of electronics, you will find that the fruit is easy to recall. Recalling a fruit from a list of other fruit, on the other hand, will prove a far more difficult task. This is the theory behind memory loss used in the feature model.\n\nThe Oscillator Based Associative Recallolation (OSCAR) Model was proposed by Browne, Preece and Hulme in 2000 The OSCAR Model is another cue driven model of memory. In this model, the cues work as a pointer to a memory’s position in the mind. Memories themselves are stored as context vectors on what Brown calls the oscillator part of the theory. While a memory is not being used in short term memory, the context vector oscillates further away from the starting position. When a cue becomes present, the context vector that the cue points to gets oscillated back to the starting point, where the memory can then be used in short term memory. There is no theory of decay in this model, so to account for memory loss or fading the theory states that memories move further and further from the starting point, and retrieval becomes more difficult the further the memory is from the gas station (starting position). Furthermore, some memories oscillate faster than others. The theory behind this is that memories frequently accessed and used will move slower away from the starting point since the probability of the memory being retrieved is relatively high.\n\nUnlike the standard models of short term/working memory, unitary theories do not assume a direct connection between activation level and memory success. They also do not propose role rehearsal, and they focus more on item based interference rather than memory decay. Unitary theories also generally have only one loop or no loop at all, whereas the Working Memory model relies on multiple loops to explain the different forms of memory input.\nBecause of the multiple loop theory, Working Memory has a more difficult time explaining how short term memory gets consolidated into long term memory, whereas the Unitary Theories generally explain long term memory using the same single principle that they use to explain short term memory. On the other hand, Unitary Theories have more trouble explaining situations where short term memory differs such as the word length effect, which can be more easily explained by the Working Memory model.\n\nThe most prevalent criticism of the unitary models is that they try to oversimplify many complex processes into a single mechanism. Studies have shown that short-term memory and long-term memory are two distinct processes that emphasize different levels of activation in the brain among different cortical areas. Furthermore, the rate of decay is much faster in short-term memory as opposed to long-term memory. The OSCAR model in particular does not account for this phenomenon. It also makes an assumption that memories are always circulating to different areas of the brain—never having a definite cortical space that stores the memory. While this transfer of storage is occurring, the rate of decay in the OSCAR model is either constant or non-existent. \nBoth rates are implausible, as a constant rate of decay neglects all external factors that may contribute to a faster rate of decay (e.g. stress, focusing attention on other things). Failing to neglect decay at all is even more implausible as it makes two wrong assertions—memories are not forgotten, and when the information is remembered, it is remembered perfectly every time.\nWhile the unitary theories of memory may give attention to alternate factors that the working memory model had initially overlooked, new data has surfaced and the model has adapted to new findings. It is still the standard model of how short-term memory works, and it will continue to be the standard model until new evidence overturns it.\n\n"}
{"id": "216354", "url": "https://en.wikipedia.org/wiki?curid=216354", "title": "Weed of cultivation", "text": "Weed of cultivation\n\nA weed of cultivation is any plant that is well-adapted to environments in which land is cultivated for growing some other plant. \n\nMany are quite specialised and can only thrive and reproduce where the ground has been broken by plough or spade. They are invariably annual and reproduction is by seed alone, which can in many species lie dormant for years in the soil until brought to the surface during cultivation. They have fast reproduction cycles, usually in one year from seed to seed, though some species can have more than one generation in one season. A few species such as groundsel (\"Senecio vulgaris\"), shepherd's purse (\"Capsella bursa-pastoris\"), red deadnettle (\"Lamium purpureum\") and chickweed (\"Stellaria media\") can survive unharmed through very cold weather and are often able to seed even in winter.\n\n"}
{"id": "28734467", "url": "https://en.wikipedia.org/wiki?curid=28734467", "title": "World Health Organisation Composite International Diagnostic Interview", "text": "World Health Organisation Composite International Diagnostic Interview\n\nThe World Health Organisation Composite International Diagnostic Interview (CIDI) is a structured interview for psychiatric disorders. As the interview is designed for epidemiological studies, it can be administered by those who are not clinically trained and can be completed in a short amount of time. Versions of the CIDI were used in two important studies, the National Comorbidity Survey (NCS)\nand National Comorbidity Survey Replication (NCS-R)\nwhich are often used as a reference for estimates of the rates of psychiatric illness in the USA.\nThe first version of the CIDI was published in 1988, and has been periodically updated to reflect the changing diagnostic criteria of DSM and ICD.\n\nThe Composite International Diagnostic Interview – Short Form (CIDI-SF) was first published by Ronald C. Kessler and colleagues in 1998,\nand the PhenX Toolkit uses this as its adult protocol for general psychiatric assessment. However, the CIDI-SF is no longer supported. According to a 2007 memo by Kessler, this decision was based on decreased need for the CIDI-SF following the introduction of other short interviews (specifically, PRIME-MD and MINI) and a lack of funding to refine the instrument.\n\n"}
