{"id": "2266816", "url": "https://en.wikipedia.org/wiki?curid=2266816", "title": "Abrolhos squall", "text": "Abrolhos squall\n\nAn Abrolhos squall (or Abroholos squall or simply abroholos) typically occurs from May through August (austral winter) near the Abrolhos Islands off the coast of eastern Brazil near 18°S latitude, located between Cabo de São Tomé and Cabo Frio.\n\nThe southeast trade winds of the tropical South Atlantic Ocean acquire heat and moisture traversing the warm Brazilian current offshore, providing moisture for this rain and thundersquall phenomenon. The Abrolhos squall typically occurs along Antarctic cold fronts penetrating into the tropics.\n\n"}
{"id": "23947591", "url": "https://en.wikipedia.org/wiki?curid=23947591", "title": "Abstract rewriting system", "text": "Abstract rewriting system\n\nIn mathematical logic and theoretical computer science, an abstract rewriting system (also (abstract) reduction system or abstract rewrite system; abbreviation ARS) is a formalism that captures the quintessential notion and properties of rewriting systems. In its simplest form, an ARS is simply a set (of \"objects\") together with a binary relation, traditionally denoted with formula_1; this definition can be further refined if we index (label) subsets of the binary relation. Despite its simplicity, an ARS is sufficient to describe important properties of rewriting systems like normal forms, termination, and various notions of confluence.\n\nHistorically, there have been several formalizations of rewriting in an abstract setting, each with its idiosyncrasies. This is due in part to the fact that some notions are equivalent, see below in this article. The formalization that is most commonly encountered in monographs and textbooks, and which is generally followed here, is due to Gérard Huet (1980).\n\nAbstract reduction system, (abbreviated ARS) is the most general (unidimensional) notion about specifying a set of objects and rules that can be applied to transform them. \nMore recently authors use abstract rewriting system as well. (The preference for the word \"reduction\" here instead of \"rewriting\" constitutes a departure from the uniform use of \"rewriting\" in the names of systems that are particularizations of ARS. Because the word \"reduction\" does not appear in the names of more specialized systems, in older texts reduction system is a synonym for ARS).\n\nAn ARS is a set \"A\", whose elements are usually called objects, together with a binary relation on \"A\", traditionally denoted by →, and called the reduction relation, rewrite relation or just reduction. This (entrenched) terminology using \"reduction\" is a little misleading, because the relation is not necessarily reducing some measure of the objects.\nIn some contexts it may be beneficial to distinguish between some subsets of the rules, i.e. some subsets of the reduction relation →, e.g. the entire reduction relation may consist of associativity and commutativity rules. Consequently, some authors define the reduction relation → as the indexed union of some relations; for instance if formula_2, the notation used is (A, →, →).\n\nAs a mathematical object, an ARS is exactly the same as an unlabeled state transition system, and if the relation is considered as an indexed union, then an ARS is the same as a labeled state transition system with the indices being the labels. The focus of the study, and the terminology are different however. In a state transition system one is interested in interpreting the labels as actions, whereas in an ARS the focus is on how objects may be transformed (rewritten) into others.\n\nSuppose the set of objects is \"T\" = {\"a\", \"b\", \"c\"} and the binary relation is given by the rules \"a\" → \"b\", \"b\" → \"a\", \"a\" → \"c\", and \"b\" → \"c\". Observe that these rules can be applied to both \"a\" and \"b\" to get \"c\". Note also, that \"c\" is, in a sense, a \"simplest\" object in the system, since nothing can be applied to \"c\" to transform it any further. Such a property is clearly an important one.\n\nExample 1 leads us to define some important notions in the general setting of an ARS. First we need some basic notions and notations.\n\n\nAn object \"x\" in \"A\" is called reducible if there exist some other \"y\" in \"A\" and formula_16; otherwise it is called irreducible or a normal form. An object \"y\" is called a normal form of \"x\" if formula_17 and \"y\" is irreducible. If \"x\" has a \"unique\" normal form, then this is usually denoted with formula_18. In example 1 above, \"c\" is a normal form, and formula_19. If every object has at least one normal form, the ARS is called normalizing.\n\nOne of the important problems that may be formulated in an ARS is the word problem: given \"x\" and \"y\" are they equivalent under formula_11? This is a very general setting for formulating the word problem for the presentation of an algebraic structure. For instance, the word problem for groups is a particular case of an ARS word problem. Central to an \"easy\" solution for the word problem is the existence of unique normal forms: in this case two objects are equivalent under formula_11 if and only if they have the same normal form. The word problem for an ARS is undecidable in general.\n\nA related, but weaker notion than the existence of normal forms is that of two objects being joinable: \"x\" and \"y\" are said joinable if there exists some \"z\" with the property that formula_22. From this definition, it's apparent one may define the joinability relation as formula_23, where formula_24 is the composition of relations. Joinability is usually denoted, somewhat confusingly, also with formula_25, but in this notation the down arrow is a binary relation, i.e. we write formula_26 if \"x\" and \"y\" are joinable.\n\nAn ARS is said to possess the Church-Rosser property if and only if formula_27 implies formula_26 for all objects \"x\", \"y\". Equivalently, the Church-Rosser property means that the reflexive transitive symmetric closure is contained in the joinability relation. Alonzo Church and J. Barkley Rosser proved in 1936 that lambda calculus has this property; hence the name of the property. (The fact that lambda calculus has this property is also known as the Church-Rosser theorem.) In an ARS with the Church-Rosser property the word problem may be reduced to the search for a common successor. In a Church-Rosser system, an object has \"at most one\" normal form; that is the normal form of an object is unique if it exists, but it may well not exist. In lambda calculus for instance, the expression (λx.xx)(λx.xx) does not have a normal form because there exists an infinite sequence of beta reductions (λx.xx)(λx.xx) → (λx.xx)(λx.xx) → ...\n\nVarious properties, simpler than Church-Rosser, are equivalent to it. The existence of these equivalent properties allows one to prove that a system is Church-Rosser with less work. Furthermore, the notions of confluence can be defined as properties of a particular object, something that's not possible for Church-Rosser. An ARS formula_29 is said to be,\n\nTheorem. For an ARS the following three conditions are equivalent: (i) it has the Church-Rosser property, (ii) it is confluent, (iii) it is semi-confluent.\n\nCorollary. In a confluent ARS if formula_27 then\n\nBecause of these equivalences, a fair bit of variation in definitions is encountered in the literature. For instance, in Terese the Church-Rosser property and confluence are defined to be synonymous and identical to the definition of confluence presented here; Church-Rosser as defined here remains unnamed, but is given as an equivalent property; this departure from other texts is deliberate. Because of the above corollary, one may define a normal form \"y\" of \"x\" as an irreducible \"y\" with the property that formula_27. This definition, found in Book and Otto, is equivalent to common one given here in a confluent system, but it is more inclusive in a non-confluent ARS.\n\nLocal confluence on the other hand is not equivalent with the other notions of confluence given in this section, but it is strictly weaker than confluence. The typical counterexample is formula_39, which is locally confluent but not confluent (cf. picture).\n\nAn abstract rewriting system is said to be terminating or noetherian if there is no infinite chain formula_40. (This is just saying that the rewriting relation is a Noetherian relation.) In a terminating ARS, every object has at least one normal form, thus it is normalizing. The converse is not true. In example 1 for instance, there is an infinite rewriting chain, namely formula_41, even though the system is normalizing. A confluent and terminating ARS is called canonical, or convergent. In a convergent ARS, every object has a unique normal form. But it is sufficient for the system to be confluent and normalizing for a unique normal to exist for every element, as seen in example 1.\n\nTheorem (Newman's Lemma): A terminating ARS is confluent if and only if it is locally confluent.\n\nThe original 1942 proof of this result by Newman was rather complicated. It wasn't until 1980 that Huet published a much simpler proof exploiting the fact that when formula_1 is terminating we can apply well-founded induction.\n\n"}
{"id": "692731", "url": "https://en.wikipedia.org/wiki?curid=692731", "title": "American Association of University Women", "text": "American Association of University Women\n\nThe American Association of University Women (AAUW), officially founded in 1881, is a non-profit organization that advances equity for women and girls through advocacy, education, and research. The organization has a nationwide network of 170,000 members and supporters, 1,000 local branches, and 800 college and university partners. Its headquarters are in Washington, D.C. AAUW's CEO is Kim Churches. \n\nIn 1881 Marion Talbot and Ellen Swallow Richards invited 15 alumnae from 8 colleges to a meeting in Boston, Massachusetts. The purpose of this meeting was to create an organization of women college graduates that would assist women in finding greater opportunities to use their education, as well as promoting and assisting other women's college attendance. The Association of Collegiate Alumnae or ACA, (AAUW's predecessor organization) was officially founded on January 14, 1882. The ACA also worked to improve standards of education for women so that men and women's higher education was more equal in scope and difficulty.\n\nAt the beginning of 1884, the ACA had been meeting only in Boston. However, as more women across the country became interested in its work, the Association saw that expansion into branches was necessary to carry on its work. Washington, D.C., was the first branch to be created in 1884, and New York, Pacific (San Francisco), Philadelphia, and Boston branches followed in 1886.\n\nIn 1885, the organization took on one of its first major projects: they essentially had to justify their right to exist. A common belief held at the time that a college education would harm a woman’s health and result in infertility. This myth was supported by Harvard-educated Boston physician Dr. Edward H. Clarke. An ACA committee led by Annie Howes created a series of questions that were sent to 1,290 ACA members; 705 replies were received. After the results were tabulated, the data demonstrated that higher education did not harm women’s health. The report, \"Health Statistics of Female College Graduates\" was published in 1885 in conjunction with the Massachusetts Bureau of Statistics of Labor. This first research report is one of many conducted by AAUW during its history.\n\nIn 1887, a fellowship program for women was established. Supporting the education of women through fellowships would continually remain a critical part of AAUW’s mission.\n\nBack in 1883, a similar group of college women had considered forming a Chicago, Illinois branch of the ACA; however, they had reconsidered and formed their own independent organization. They formed the Western Association of Collegiate Alumnae (WACA) with Jane M. Bancroft as its first president. WACA was broad in purpose and consisted of five committees: fine arts, outdoor occupations,domestic professions, press and journalism, and higher education of women in the West. In 1888, WACA awarded its first fellowship of $350 to Ida Street, a Vassar College graduate, to conduct research at the University of Michigan. In 1889, WACA merged with the ACA, further expanding the groups' capacity.\n\nIn 1919, the ACA participated in a larger effort led by a group of American women which ultimately raised $156,413 to purchase a gram of radium for Marie Curie for her experiments.\n\nIn 1921, the ACA merged with the Southern Association of College Women to create the AAUW, although local branches continued to be the backbone of AAUW. The policy of expansion greatly increased both the size and the impact of the Association, from a small, local organization to a nationwide network of college educated women, and by 1929, there were 31,647 members and 475 branches.\n\nDuring World War II, AAUW officially began raising money to assist female scholars displaced by the Nazi led occupation who were unable to continue their work. The War Relief Fund received numerous pleas for help and worked tirelessly to find teaching and other positions for refugee women at American schools and universities and in other countries. Individual branch members of AAUW also participated by signing immigration affidavits of support. During 1940, its inaugural year, the War Relief Committee raised $29,950 for distribution with 350 branches contributing.\n\nThe organization was \"largely apolitical\" until the 1960s. On the other hand, women in the workforce had increased to the extent that they made up 38% of workers by the end of the 1960s. Women graduating from college were looking for good employment. Membership in 1960 was at 147,920 women, most of them middle class.\n\nAAUW is one of the world's largest sources of funding exclusively for women who have graduated from college. Each year, AAUW has provided $3.5 to $4 million in fellowships, grants, and awards for women and for community action projects. The Foundation also funds pioneering research on women, girls, and education. The organization funds studies germane to the education of women.\n\nThe AAUW Legal Advocacy Fund (LAF), a program of the Foundation, is the United States' largest legal fund focused solely on sex discrimination against women in higher education. LAF provides funds and a support system for women seeking judicial redress for sex discrimination in higher education. Since 1981, LAF has helped female students, faculty, and administrators challenge sex discrimination, including sexual harassment, pay inequity, denial of tenure and promotion, and inequality in women’s athletics programs.\n\nAAUW sponsors grassroots and advocacy efforts, research, and Campus Action Projects and other educational programs in conjunction with its ongoing programmatic theme, Education as the Gateway to Women's Economic Security. Along with three other organizations, it founded the CTM Madison Family Theatre in 1965. AAUW joined forces with other women's organizations in August 2011 to launch HERVotes to mobilize women voters in 2012 on preserving health and economic rights. In 2011, the AAUW Action Fund launched an initiative to encourage women to vote in the 2012 election. The campaign was aimed to increase the volume and direction of women’s voices.\n\nAAUW's 2011 research report addresses sexual harassment in grades seven through 12.\n\nAAUW's national convention is held biennially. AAUW sponsors a student leadership conference, called the National Conference of College Women Student Leaders (NCCWSL) designed to help women college students access the resources, skills, and networks they need to lead change on campuses and in communities nationwide. The student leadership conference is held annually in Washington, D.C.\n\n\n"}
{"id": "34784308", "url": "https://en.wikipedia.org/wiki?curid=34784308", "title": "BigDFT", "text": "BigDFT\n\nBigDFT is a free software package for physicists and chemists, distributed under the GNU General Public License, whose main program allows the total energy, charge density, and electronic structure of systems made of electrons and nuclei (molecules and periodic/crystalline solids) to be calculated within density functional theory (DFT), using pseudopotentials, and a wavelet basis.\n\nBigDFT implements density functional theory (DFT) by solving the Kohn–Sham equations describing the electrons in a material, expanded in a Daubechies wavelet basis set and using a self-consistent direct minimization or Davidson diagonalisation methods to determine the energy minimum. Computational efficiency is achieved through the use of fast short convolutions \nand pseudopotentials to describe core electrons. In addition to total energy, forces and stresses are also calculated so that geometry optimizations and ab initio molecular dynamics may be carried out.\n\nThe Daubechies wavelet basis sets are an orthogonal systematic basis set as plane wave basis set but has the great advantage to allow adapted mesh with different levels of resolutions (see multi-resolution analysis). Interpolating scaling functions are used also to solve the Poisson's equation with different boundary conditions as isolated or surface systems.\n\nBigDFT was among the first massively parallel density functional theory codes which benefited from graphics processing units (GPU) using CUDA and then OpenCL languages.\n\nBecause the Daubechies wavelets have a compact support, the Hamiltonian application can be done locally which permits to have a linear scaling in function of the number of atoms instead of a cubic scaling for traditional DFT software.\n\n"}
{"id": "20751475", "url": "https://en.wikipedia.org/wiki?curid=20751475", "title": "Blazhko effect", "text": "Blazhko effect\n\nThe Blazhko effect, which is sometimes called long-period modulation, is a variation in period and amplitude in RR Lyrae type variable stars. It was first observed by Sergey Blazhko in 1907 in the star RW Draconis.\n\nThe physics behind the Blazhko effect is currently still a matter of debate, with there being three primary hypotheses.\nIn the first, referred to as the resonance model, the cause of the modulation is a non-linear resonance among either the fundamental or the first overtone pulsation mode of the star and a higher mode. The second, known as the magnetic model, assumes the variation to be caused by the magnetic field being inclined to the rotational axis, deforming the main radial mode. The magnetic model was ruled out in 2004 by high resolution spectro-polarimetric observations. The third model assumes that cycles in the convection cause the alternations and the modulations.\n\nObservational evidence based on Kepler observations indicates much of the Blazhko effect's two-cycle light curve modulation is due to simple period-doubling. Many RR Lyrae stars have a variability period of approximately 12 hours and ground-based astronomers typically make nightly observations about 24 hours apart; thus period-doubling results in brightness maximums during nightly observations that are significantly different than the daytime maximum.\n\n"}
{"id": "42739", "url": "https://en.wikipedia.org/wiki?curid=42739", "title": "Bubble fusion", "text": "Bubble fusion\n\nBubble fusion is the non-technical name for a nuclear fusion reaction hypothesized to occur inside extraordinarily large collapsing gas bubbles created in a liquid during acoustic cavitation. The more technical name is sonofusion.\n\nThe term was coined in 2002 with the release of a report by Rusi Taleyarkhan and collaborators that claimed to have observed evidence of sonofusion. The claim was quickly surrounded by controversy, including allegations ranging from experimental error to academic fraud. Subsequent publications claiming independent verification of sonofusion were also highly controversial.\n\nEventually, an investigation by Purdue University found that Taleyarkhan had engaged in falsification of independent verification, and had included a student as an author on a paper when he had not participated in the research. He was subsequently stripped of his professorship. One of his funders, the Office of Naval Research reviewed the report by Purdue and barred him from federal funding for 28 months.\n\nUS patent 4,333,796, filed by Hugh Flynn in 1978, appears to be the earliest documented reference to a sonofusion-type reaction.\n\nIn the March 8, 2002 issue of the peer-reviewed journal \"Science\", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. The neutron emission was also reported to be coincident with the sonoluminescence pulse, a key indicator that its source was fusion caused by the heat and pressure inside the collapsing bubbles.\n\nThe results were so startling, that the Oak Ridge National Laboratory asked two independent researchers, D. Shapira and M. J. Saltmarsh, to repeat the experiment using more sophisticated neutron detection equipment. They reported that the neutron release was consistent with random coincidence. A rebuttal by Taleyarkhan and the other authors of the original report argued that the Shapira and Saltmarsh report failed to account for significant differences in experimental setup, including over an inch of shielding between the neutron detector and the sonoluminescing acetone. According to Taleyarkhan \"et al.\", when properly considering those differences, the results were consistent with fusion.\n\nAs early as 2002, while experimental work was still in progress, Aaron Galonsky of Michigan State University, in a letter to the journal \"Science\"\nexpressed doubts about the claim made by the Taleyarkhan team. In Galonsky's opinion, the observed neutrons were too high in energy to be from a deuterium-deuterium (d-d) fusion reaction. In their response (published on the same page), the Taleyarkhan team provided detailed counter-arguments and concluded that the energy was \"reasonably close\" to that which was expected from a fusion reaction.\n\nIn February 2005 the documentary series \"Horizon\" commissioned two leading sonoluminescence researchers, Seth Putterman and Kenneth S. Suslick, to reproduce Taleyarkhan's work. Using similar acoustic parameters, deuterated acetone, similar bubble nucleation, and a much more sophisticated neutron detection device, the researchers could find no evidence of a fusion reaction.\n\nIn 2004, new reports of bubble fusion were published by the Taleyarkhan group, claiming that the results of previous experiments had been replicated under more stringent experimental conditions. These results differed from the original results in that fusion was claimed to occur over longer times than previously reported. The original report only claimed neutron emission from the initial bubble collapse following bubble nucleation, whereas this report claimed neutron emission many acoustic cycles later.\n\nIn July 2005, two of Taleyarkhan's students at Purdue University published evidence confirming the previous result. They used the same acoustic chamber, the same deuterated acetone fluid and a similar bubble nucleation system. In this report, no neutron-sonoluminescence coincidence was attempted. An article in \"Nature\" raised issues about the validity of the research and complaints from his Purdue colleagues (see full analysis elsewhere in this page). Charges of misconduct were raised, and Purdue University opened an investigation. It concluded in 2008 that Taleyarkhan's name should have appeared in the author list because of his deep involvement in many steps of the research, that he added one author that had not really participated in the paper just to overcome the criticism of one reviewer, and that this was part of an attempt of \"an effort to falsify the scientific record by assertion of independent confirmation\". The investigation did not address the validity of the experimental results.\n\nIn January 2006, a paper published in the journal \"Physical Review Letters\" by Taleyarkhan in collaboration with researchers from Rensselaer Polytechnic Institute reported statistically significant evidence of fusion.\n\nIn November 2006, in the midst of accusations concerning Taleyarkhan's research standards, two different scientists visited the meta-stable fluids research lab at Purdue University to measure neutrons, using Taleyarkhan's equipment. Dr. Edward R. Forringer and undergraduates David Robbins and Jonathan Martin of LeTourneau University presented two papers at the American Nuclear Society Winter Meeting that reported replication of neutron emission. Their experimental setup was similar to previous experiments in that it used a mixture of deuterated acetone, deuterated benzene, tetrachloroethylene and uranyl nitrate. Notably, however, it operated without an external neutron source and used two types of neutron detectors. They claimed a liquid scintillation detector measured neutron levels at 8 standard deviations above the background level, while plastic detectors measured levels at 3.8 standard deviations above the background. When the same experiment was performed with non-deuterated control liquid, the measurements were within one standard deviation of background, indicating that the neutron production had only occurred during cavitation of the deuterated liquid. William M. Bugg, emeritus physics professor at the University of Tennessee also traveled to Taleyarkhan's lab to repeat the experiment with his equipment. He also reported neutron emission, using plastic neutron detectors. Taleyarkhan claimed these visits counted as independent replications by experts, but Forringer later recognized that he was not an expert, and Bugg later said that Taleyarkhan performed the experiments and he had only watched.\n\nIn March 2006, \"Nature\" published a special report that called into question the validity of the results of the Purdue experiments. The report quotes Brian Naranjo of the University of California, Los Angeles to the effect that neutron energy spectrum reported in the 2006 paper by Taleyarkhan, et al. was statistically inconsistent with neutrons produced by the proposed fusion reaction and instead highly consistent with neutrons produced by the radioactive decay of Californium 252, an isotope commonly used as a laboratory neutron source .\n\nThe response of Taleyarkhan \"et al.\", published in \"Physical Review Letters\", attempts to refute Naranjo's hypothesis as to the cause of the neutrons detected.\n\nTsoukalas, head of the School of Nuclear Engineering at Purdue, and several of his colleagues at Purdue, had convinced Taleyarkhan to move to Purdue and attempt a joint replication. In the 2006 \"Nature\" report they detail several troubling issues when trying to collaborate with Taleyarkhan. He reported positive results from certain set of raw data, but his colleagues had also examined that set and it only contained negative results. He never showed his colleagues the raw data corresponding to the positive results, despite several requests. He moved the equipment from a shared laboratory to his own laboratory, thus impeding review by his colleagues, and he didn't give any advance warning or explanation for the move. Taleyarkhan convinced his colleagues that they shouldn't publish a paper with their negative results. Taleyarkhan then insisted that the university's press release present his experiment as \"peer-reviewed\" and \"independent\", when the co-authors were working in his laboratory under his supervision, and his peers in the faculty were not allowed to review the data. In summary, Taleyarkhan's colleagues at Purdue said he placed obstacles to peer review of his experiments, and they had serious doubts about the validity of the research.\n\n\"Nature\" also revealed that the process of anonymous peer-review had not been followed, and that the journal \"Nuclear Engineering and Design\" was not independent from the authors. Taleyarkhan was co-editor of the journal, and the paper was only peer-reviewed by his co-editor, with Taleyarkhan's knowledge.\n\nIn 2002 Taleyarkhan filed a patent application on behalf of the United States Department of Energy, while working in Oak Ridge. \"Nature\" reported that the patent had been rejected in 2005 by the US Patent Office. The examiner called the experiment a variation of discredited cold fusion, found that there was \"no reputable evidence of record to support any allegations or claims that the invention is capable of operating as indicated\", and found that there was not enough detail for others to replicate the invention. The field of fusion suffered from many flawed claims, thus the examiner asked for additional proof that the radiation was generated from fusion and not from other sources. An appeal was not filed because the Department of Energy had dropped the claim in December 2005.\n\nDoubts among Purdue University's Nuclear Engineering faculty as to whether the positive results reported from sonofusion experiments conducted there were truthful prompted the university to initiate a review of the research, conducted by Purdue's Office of the Vice President for Research. In a March 9, 2006 article entitled \"Evidence for bubble fusion called into question\", \"Nature\" interviewed several of Taleyarkhan's colleagues who suspected something was amiss.\n\nOn February 7, 2007, the Purdue University administration determined that \"the evidence does not support the allegations of research misconduct and that no further investigation of the allegations is warranted\". Their report also stated that \"vigorous, open debate of the scientific merits of this new technology is the most appropriate focus going forward.\" In order to verify that the investigation was properly conducted, House Representative Brad Miller requested full copies of its documents and reports by March 30, 2007. His congressional report concluded that \"Purdue deviated from its own procedures in investigating this case and did not conduct a thorough investigation\"; in response, Purdue announced that it would re-open its investigation.\n\nIn June 2008, a multi-institutional team including Taleyarkhan published a paper in Nuclear Engineering and Design to \"clear up misconceptions generated by a webposting of UCLA which served as the basis for the \"Nature\" article of March 2006\", according to a press release.\n\nOn July 18, 2008, Purdue University announced that a committee with members from five institutions had investigated 12 allegations of research misconduct against Rusi Taleyarkhan. It concluded that two allegations were founded—that Taleyarkhan had claimed independent confirmation of his work when in reality the apparent confirmations were done by Taleyarkhan's former students and was not as \"independent\" as Taleyarkhan implied, and that Taleyarkhan had included a colleague's name on one of his papers who had not actually been involved in the research (\"the sole apparent motivation for the addition of Mr. Butt was a desire to overcome a reviewer's criticism\", the report concluded).\n\nTaleyarkhan's appeal of the report's conclusions was rejected. He said the two allegations of misconduct were trivial administrative issues and had nothing to do with the discovery of bubble nuclear fusion or the underlying science, and that \"all allegations of fraud and fabrication have been dismissed as invalid and without merit — thereby supporting the underlying science and experimental data as being on solid ground\". A researcher questioned by the LA Times said that the report had not clarified whether bubble fusion was real or not, but that the low quality of the papers and the doubts cast by the report had destroyed Taleyarkhan's credibility with the scientific community.\n\nOn August 27, 2008 he was stripped of his named Arden Bement Jr. Professorship, and forbidden to be a thesis advisor for graduate students for at least the next 3 years.\n\nDespite the findings against him, Taleyarkhan received a $185,000 grant from the National Science Foundation between September 2008 and August 2009 to investigate bubble fusion. In 2009 the Office of Naval Research debarred him for 28 months, until September 2011, from receiving U.S. Federal Funding. During that period his name was listed in the 'Excluded Parties List' to prevent him from receiving further grants from any government agency.\n\n\n"}
{"id": "19821268", "url": "https://en.wikipedia.org/wiki?curid=19821268", "title": "Buckingham (unit)", "text": "Buckingham (unit)\n\nThe Buckingham (symbol: B) is a CGS unit of electric quadrupole, named in honour of the chemical physicist A. David Buckingham who was the first to measure a molecular quadrupole moment. It is defined as 1 statcoulomb-centimetre. This is equivalent to 1 Debye-Ångström, where 1 Debye = 1 statcoulomb-centimetre is the cgs unit of molecular dipole moment and 1 Ångström = 1 cm. \n\nOne Buckingham corresponds to the quadrupole moment resulting from two opposing dipole moments but an equal magnitude of 1 Debye which are separated by a distance of 1 Angstrom, a typical bond length. This is analogous to the Debye unit for the dipole moment of two opposing charges of 1 statcoulomb separated by 1 Angstrom, and the name Buckingham for the unit was in fact suggested by Peter Debye in 1963 in honour of Buckingham. \n"}
{"id": "842779", "url": "https://en.wikipedia.org/wiki?curid=842779", "title": "Calculating Space", "text": "Calculating Space\n\nCalculating Space () is Konrad Zuse's 1969 book on digital physics. Zuse proposed that the universe is being computed by some sort of cellular automaton or other discrete computing machinery, challenging the long-held view that some physical laws are continuous by nature. He focused on cellular automata as a possible substrate of the computation, and pointed out (among other things) that the classical notions of entropy and its growth do not make sense in deterministically computed universes.\n\nBell's theorem is sometimes thought to contradict Zuse's hypothesis, but it is not applicable to deterministic universes, as Bell himself pointed out. Similarly, while Heisenberg's uncertainty principle limits in a fundamental way what an observer can observe, when the observer is himself a part of the universe he is trying to observe, that principle does not rule out Zuse's hypothesis, which views any observer as a part of the hypothesized deterministic process. So far there is no unambiguous physical evidence against the possibility that \"everything is just a computation,\" and a large amount has been written about digital physics since Zuse's book appeared.\n\n\n\n"}
{"id": "8231727", "url": "https://en.wikipedia.org/wiki?curid=8231727", "title": "Choice (credit card)", "text": "Choice (credit card)\n\nChoice was a credit card test marketed by Citibank in the United States, announced in 1977, and first issued in 1978. It was one of the first cards to offer a cash refund program, and no annual fee. Choice was intended to create a rival to Visa, MasterCard, and American Express, but proved unsuccessful, and was withdrawn in 1987. Citibank has continued to use the \"Choice\" name on some of its Visa and MasterCard cards.\n\nThe card was introduced in 1977, when Citibank bought NAC, a regional credit card based in Baltimore, renaming it Choice. A subsequent campaign in Maryland in 1980 turned the card into a regional success, earning more than one million cardholders in the Baltimore and Washington, DC, area.\n\nDespite the success of Sears' Discover Card, which offered many of the same features as Choice when it was introduced in 1985 (such as a rebate on purchases and no annual fee), Citibank decided Choice could not compete with Visa and MasterCard in the longer term, and the card was reissued as a Visa at the end of 1987, aimed at entry level customers and those with poor credit. \n\nIt was also said that Citibank's owner, Citicorp, was not willing to accept the eventual estimated costs of establishing another national credit card, after Sears had spent an estimated USD$80 million creating its Discover Card. Its fate was similar to that of Citibank's first credit card, the \"First National City Charge Service\" (or \"The Everything Card\"), introduced on the East Coast in 1967 to compete with BankAmericard (today's Visa) but which became part of Master Charge (now MasterCard) in 1969.\n"}
{"id": "5375984", "url": "https://en.wikipedia.org/wiki?curid=5375984", "title": "Commentariolus", "text": "Commentariolus\n\nThe Commentariolus (\"Little Commentary\") is Nicolaus Copernicus's brief outline of an early version of his revolutionary heliocentric theory of the universe. After further long development of his theory, Copernicus published the mature version in 1543 in his landmark work, \"De revolutionibus orbium coelestium\" (\"On the Revolutions of the Heavenly Spheres\").\n\nCopernicus wrote the \"Commentariolus\" in Latin some time before 1514 and circulated copies to his friends and colleagues. It thus became known among Copernicus's contemporaries, though it was never printed during his lifetime. In 1533, Johann Albrecht Widmannstetter delivered a series of lectures in Rome outlining Copernicus' theory. Pope Clement VII and several Catholic cardinals heard the lectures and were interested in the theory. On 1 November 1536, Nikolaus von Schönberg, Archbishop of Capua and since the preceding year a cardinal, wrote to Copernicus from Rome and asked him for a copy of his writings \"at the earliest possible moment\".\n\nAlthough copies of the \"Commentariolus\" circulated for a time after Copernicus's death, it subsequently lapsed into obscurity, and its previous existence remained known only indirectly, until a surviving manuscript copy was discovered and published in the second half of the nineteenth century.\n\nThe Commentariolus is subdivided into eight sections (or chapters), of which all but the first bear brief descriptive titles. After a brief introduction, the first section states seven postulates from which Copernicus proposes to show that the apparent motion of the planets can be explained systematically.\n\n\nThe remaining seven sections are titled, in order, \"De ordine orbium\" (\"The order of the spheres\"), \"De motibus qui circa solem apparent\" (\"The apparent motions of the Sun\"), \"Quod aequalitas motum non ad aequinoctia sed ad stellas fixas referatur\" (\"Equal motion should be measured not by the equinoxes but by the fixed stars\"), \"De Luna\" (\"The Moon\"), \"De tribus superioribus: Saturno, Jove et Marte\" (\"The outer planets: Saturn, Jupiter and Mars\"), \"De Venere\" (\"Venus\") and \"De Mercurio\" (\"Mercury\").\n\nIn this section, the heavenly spheres are given in order from outermost to innermost. \nThe outermost sphere is that of the fixed stars, which remains perfectly stationary. Then follow those of Saturn, Jupiter, Mars, Earth, Venus and Mercury, which each revolve about the Sun from west to east with successively shorter periods of revolution, Saturn's being between 29 and 30 years, Jupiter's between 11 and 12, Mars's between 2 and 3, Earth's exactly one, Venus's between 8 and 9 months, and Mercury's between 2 and 3 months. The Moon's sphere, however, revolves around the Earth in a period of one month, and moves with it around the Sun like an epicycle.\n\nThis section explains how the apparent motion of the Sun could arise from three separate motions of the Earth. The first motion is a uniform revolution, with a period of one year, from west to east along a circular orbit whose centre is offset from the Sun by 1/25 of the orbit's radius.\n\nThe second motion is the daily rotation about an axis which passes through the Earth's centre and is inclined at an angle of about 23° to the perpendicular to the plane of its orbit.\n\nThe third motion is a precession of the Earth's axis of rotation about an axis perpendicular to the plane of its orbit. Copernicus specified the rate of this precession with respect to the radial line from the Earth to the centre of its orbit as being slightly less than a year, with an implied direction as being from west to east. \"With respect to the fixed stars\", this precession is very slow, and in the opposite direction—from east to west—and explains the phenomenon of the precession of the equinoxes.\n\nHere Copernicus asserts that the motion of the equinoxes and celestial poles has not been uniform, and argues that consequently they should not be used to define the reference frame with respect to which the motions of the planets are measured, and that the periods of the various planetary motions are more accurately determinable if those motions are measured with respect to the fixed stars. He maintains that he had found the length of the sidereal year to have always been 365 days 6 hours and 10 minutes.\n\nIncluding the annual revolution around the Sun, which the Moon shares with the Earth in his system, \nCopernicus explains the Moon's motion as composed of five independent motions. Its motion around the Earth lies in a plane which is inclined at an angle of 5° to the plane of the Earth's orbit, and which precesses from east to west around an axis perpendicular to that plane, with a period of between 18 and 19 years with respect to the fixed stars. The remaining three motions, which take place within this orbital plane, are depicted in the diagram to the right. The first of these is that of the first, and larger, of two epicycles, whose center (represented by the point e1 in the diagram) moves uniformly from west to east around the circumference of a deferent centred on the Earth (represented by point T in the diagram), with a period of one draconitic month. The centre of the second, smaller epicycle (represented by the point e2 in the diagram) moves uniformly from east to west around the circumference of the first so that the period of the angle β in the diagram is one anomalistic month.\n\nThe Moon itself, represented by the point M in the diagram, moves uniformly from west to east around the circumference of the second epicycle so that the period of the angle γ is half a synodic month. Copernicus states that whenever the point e1 lies on the line joining the Earth to the centre of its orbit (represented by the dotted line OTC in the diagram, of which only the point T here lies in the Moon's orbital plane), the Moon M will lie precisely between e1 and e2. However, this can occur only once every 19 years, when this line coincides with the line of nodes WTE. At other times it does not lie in the moon's orbital plane and the point e1 cannot therefore pass through it. In general, then, while the Moon will be \"close to\" conjunction or opposition to the Sun whenever it lies precisely between e1 and e2, these events will not be precisely simultaneous.\n\nThe ratio which Copernicus took as that for the relative lengths of the small epicycle, large epicycle and deferent is 4:19:180.\n\nThe theories Copernicus gives in the \"Commentariolus\" for the motions of the outer planets all have the same general structure, and only differ in the values of the various parameters needed to specify their motions completely. Their orbits are not coplanar with that of the Earth, but do share its centre as their own common centre, and lie in planes that are only slightly inclined to the Earth's orbital plane. Unlike the Moon's orbital plane, those of the superior planets do not precess. Their inclinations to the Earth's orbital plane do oscillate, however, between the limits 0°10′ and 1°50′ for Mars, 1°15′ and 1°40′ for Jupiter, and 2°15′ and 2°40′ for Saturn. Although Copernicus supposes these oscillations to take place around the orbits' lines of nodes that he assumes to remain fixed, the mechanism he uses to model them does cause tiny oscillations in the lines of nodes as well. As Kepler later pointed out, the necessity for assuming oscillations in the inclinations of the outer planets' orbital planes is an artefact of Copernicus's having taken them as passing through the centre of the Earth's orbit. If he had taken them as passing through the Sun, he would not have needed to introduce these oscillations.\n\nLike the Moon's motion, that of the outer planets, represented in the diagram to the right, is produced by a combination of a deferent and two epicycles. The centre of the first, and larger of the two epicycles, represented by the point e1 in the diagram, revolves uniformly from west to east around the circumference of a deferent whose centre is the centre of the Earth's orbit, represented by the point S in the diagram, with a period relative to the fixed stars as given in the section \"The order of the spheres\" above.\n\nThe centre of the second epicycle, represented by the point e2 in the diagram, revolves uniformly from east to west around the circumference of the first, with the same period relative to the radial line joining S to e1. As a consequence, the direction of the radial line joining e1 to e2 remains fixed relative to the fixed stars, parallel to the planet's line of apses EW, and the point e2 describes an eccentric circle whose radius is equal to that of the deferent, and whose centre, represented by the point O in the diagram, is offset from that of the deferent by the radius of the first epiycle. In his later work, \"De revolutionibus orbium coelestium\", Copernicus uses this eccentric circle directly, rather than representing it as a combination of a deferent and an epicycle.\n\nThe planet itself, represented by the point P in the diagram, revolves uniformly from west to east around the circumference of the second epicycle, whose radius is exactly one third of that of the first, at twice the rate of revolution of e1 about S. This device enabled Copernicus to dispense with the equant, a much-criticised feature of Claudius Ptolemy's theories for the motions of the outer planets. In a heliocentric version of Ptolemy's models, his equant would lie at the point Q in the diagram, offset along the line of apses EW from the point S by a distance one and a third times the radius of Copernicus's first epicycle. The centre of the planet's deferent, with the same radius as Copernicus's, would lie at the point C, mid-way between S and Q. The planet itself would lie at the point of intersection of this deferent with the line QP. While this point only coincides exactly with P whenever they are both at an apsis, the difference between their positions is always negligible in comparison with the inaccuracies inherent to both theories.\n\nFor the ratios of the radii of the outer planets' deferents to radius of the Earth, the \"Commentariolus\" gives 1 for Mars, 5 for Jupiter, and 9 for Saturn. For the ratios of the radii of their deferents to the radii of the larger of their epicycles, it gives 6 for Mars, 12 for Jupiter, and 11 for Saturn.\n\nIn the last two sections Copernicus talks about Venus and Mercury. The first has a system of circles and takes 9 months to complete a revolution.\n\nMercury's orbit is harder than any of the other planets' to study because it is visible for only a few days a year. Mercury, just like Venus, has two epicycles, one greater than another. It takes almost three months to complete a revolution.\n\n\n"}
{"id": "34396592", "url": "https://en.wikipedia.org/wiki?curid=34396592", "title": "Cum sole", "text": "Cum sole\n\nCum sole is a Latin phrase meaning \"with the sun\". The term is sometimes used in meteorology and physical oceanography to refer to anticyclonic motion, which is clockwise in the Northern Hemisphere and counterclockwise in the Southern Hemisphere (but \"with\" the sun's apparent motion when facing Equator in both hemispheres) (Pond and Pickard, 1983). \n\nThe opposite of Cum sole is Contra solem (cyclonic motion). Both terms are infrequently used.\n\n"}
{"id": "30734524", "url": "https://en.wikipedia.org/wiki?curid=30734524", "title": "Edward DeLong", "text": "Edward DeLong\n\nEdward Francis DeLong is a marine microbiologist and professor in the Department of Oceanography at the University of Hawaii, Manoa, and is considered a pioneer in the field of metagenomics. He is best known for his discovery of the bacterial use of the rhodopsin protein in converting sunlight to biochemical energy in marine microbial communities.\n\nDeLong was born in Sonoma, California. He studied biology at Santa Rosa Junior College and obtained an A.S. degree in 1980. While continuing his education at the University of California, Davis, DeLong had originally planned on becoming a medical technologist, but after a meeting and working as an undergraduate researcher with bacteriologist Paul Baumann, he found a new interest in marine microbiology. He graduated with a B.S. degree in bacteriology at UCD in 1982 and moved to the Scripps Institution of Oceanography, where he received a Ph.D. in marine biology after finishing doctoral work with Art Yayanos in 1986. DeLong completed his postdoctoral training at Indiana University in Bloomington with Norman Pace, where he surveyed communities of picoplankton via DNA sequencing.\n\nWith Pace and his group at Indiana University, DeLong developed a method that can be used to identify single cells phylogenetically through the use of phylogenetic stains. These rRNA-based probes identify the cells based on the binding of fluorescent probes to individual cells through use of oligonucleotides that are complementary to 16S rRNA sequences of specific phylogenetic groups. The use of multiple probes with different fluorescent dyes allows for the identification of different cell types in the same field.\n\nDeLong subsequently expanded upon this work and applied gene cloning and sequencing to the study of complex marine microbial communities and their role in the biosphere. These techniques carried significance in that microbes could be studied without the use of a standard microbial culture.\n\nAfter receiving an independent study award in 1989, DeLong spent some time at the Woods Hole Oceanographic Institute in Woods Hole, Massachusetts, and would later on become Associate Professor in the Biology and Ecology, Evolution, and Marine Biology Departments at the University of California, Santa Barbara. DeLong’s surveys during his time at UCSB led him to participate in the study of widespread abundance and diversity of marine archaea in the world’s oceans. Prior to 1992, archaea were thought only to exist in the extreme environments of hypersaline lakes, hydrothermal vents, and similar places. This changed the general view of the scientific community on the role of archaea in the biosphere and opened up new possibilities in applied potential of such microbial assemblages.\n\nIn the years following, DeLong’s work took him to the Monterey Bay Aquarium Research Institute and it is during his time there that he made a crucial discovery in the understanding of the Earth’s carbon and energy cycles. A team of microbiologists led by DeLong discovered a gene in several species of bacteria responsible for production of the protein rhodopsin, previously unheard of in the domain Bacteria. These proteins found in the cell membranes are capable of converting light energy to biochemical energy due to a change in configuration of the rhodopsin molecule as sunlight strikes it, causing the pumping of a proton from inside out and a subsequent inflow that generates the energy. In 2004, DeLong moved to the Massachusetts Institute of Technology, where he worked on developing gene expression studies targeting microbial communities in the wild. At MIT, his collaborations with CMORE and Monterey Bay Aquarium Research Institute colleagues, he discovered of highly synchronized microbial populations having oscillating patterns of gene expression across many species. In 2014, DeLong relocated to the University of Hawaii, where he serves as Co-Director for the Center for Microbial Oceanography: Research and Education, C-MORE and the Simons Collaboration on Ocean Processes and Ecology, SCOPE.\n\nHonorary Professorship, University of Queensland, Brisbane, Australia, 1999-2002.\n\nElected Fellow in the American Academy of Microbiology, August 2000.\n\nMoore Investigator in Marine Microbiology, August, 2004.\n\nElected Fellow in the American Academy of Arts and Sciences, May 2005.\n\nIn April 2008, DeLong was presented with the Vladimir Ivanovich Vernadsky Medal for “important contributions to geomicrobiology and biogeochemical cycling through the innovative use of molecular tools and a genomic approach” at the European Geosciences Union.\n\nElected Fellow in the National Academy of Science, April 2008.\n\nThe American Society for Microbiology presented DeLong with the Procter & Gamble Award in Applied and Environmental Microbiology in May 2008 and the D.C. White Research and Mentoring Award in February 2009.\n\nElected Fellow in the American Association for the Advancement of Science, 2011.\n\nUC Davis College of Biological Sciences Outstanding Alumni Award, UC Davis, 2012\n\nMoore Investigator in Marine Microbiology, 2012.\n\nA.G. Huntsman Award for Excellence in the Marine Sciences, 2014\n\nElected Member in the European Molecular Biology Association EMBO, 2015.\n\nElected President of the International Society for Microbial Ecology (ISME)\n\nDuring the years of his childhood in Sonoma, DeLong shared a love for the ocean and learned to skin dive, scuba dive, and paddle a kayak. He moved to Alaska on a soul search after graduating high school before he decided to return to California to start college.\n\n\n"}
{"id": "57781561", "url": "https://en.wikipedia.org/wiki?curid=57781561", "title": "Fedor Georgievich Alekseev", "text": "Fedor Georgievich Alekseev\n\nFedor Georgievich Alekseev (, ) is a Russian and Armenian linguist and journalist of Belarusian origin.\nHe is the editor-in-chief of \"Minority Languages\", a Russian academic journal dedicated to endangered languages of Russia, and \"Kamysh\", a local online magazine based in the city of Astrakhan in Southern Russia.\n"}
{"id": "43238552", "url": "https://en.wikipedia.org/wiki?curid=43238552", "title": "Flamelet generated manifold", "text": "Flamelet generated manifold\n\nFlamelet-Generated Manifold (FGM) is a combustion chemistry reduction technique. The approach of FGM is based on the idea that the most important aspects of the internal structure of the flame front should be taken into account. In this view, a low-dimensional chemical manifold is created on the basis of one-dimensional flame structures, including nearly all of the transport and chemical phenomena as observed in three-dimensional flames. In addition, the progress of the flame is generally described by transport equations for a limited number of control variables.\n\n"}
{"id": "56787518", "url": "https://en.wikipedia.org/wiki?curid=56787518", "title": "Gerard Smets", "text": "Gerard Smets\n\nGerard Smets (\"fl.\" October 31, 1888 – 1890) was a Belgian paleontologist, scientist and abbé known for the misidentification of the plant genus \"Aachenosaurus\", named after the locale of Aachen in Belgium.\n\nSmets was most likely born in or near Brussels, Belgium before 1855.\n\n\"Aachenosaurus\" was found and named by Smets on October 31, 1888, who named the type species \"Aachenosaurus multidens\". Based on these fragments he determined that the specimen was a hadrosaur reaching an estimated 4 to 5 meters in length which might have had dermal spines. He defended this conclusion, citing that the fossils had been examined visually with the naked eye, magnifying lenses and with the microscope. However, his error was soon demonstrated by Louis Dollo. Smets at first tried to defend his original identification but was again proven wrong by a neutral commission and withdrew from science completely from pure embarrassment.\n"}
{"id": "47132182", "url": "https://en.wikipedia.org/wiki?curid=47132182", "title": "Hack Reactor", "text": "Hack Reactor\n\nHack Reactor is a software engineering Coding Bootcamp education program founded in San Francisco by Anthony Phillips, Shawn Drost, Marcus Phillips, and Douglas Calhoun in 2012. Their coding bootcamp is currently offered in 12-week Full-Time and 9-month Part-Time formats, in-person in San Francisco, Los Angeles, New York City and Austin, as well as remotely, live online.\n\nCofounder Drost has described the program as, \"optimized for people who want to be software engineers as their main, day-to-day work. Their life's work.\" The curriculum focuses on JavaScript and associated technologies including the MEAN stack, React and Backbone.\n\nIn 2015 Hack Reactor acquired Austin-based MakerSquare as \"their first deal in a plan to develop a network of coding bootcamps\" in an effort to \"make a large dent in transforming the old education system into one that focuses on student outcomes.\" The following month, a pair of Hack Reactor alumni partnered with the company to open Telegraph Academy \"to teach software engineering to under-represented minorities\" and create a \"growing community of diverse software engineers.\" In November 2016, Hack Reactor rebranded all of its schools to share the Hack Reactor name.\n\nHack Reactor's admissions standard has been described as \"highly selective, only accepting ten to fifteen percent of applicants for each cohort.\" Though many applicants who do not pass the first admission interview are encouraged to try again when they feel they are better prepared.\n\nAs of 2017, Hack Reactor has two admissions paths. Students can either self-study and pass a technical interview or join their Structured Study Program, a live class which includes preparation, readiness assessment and admittance.\n\nThe technical interview tests both technical skills (JavaScript basics such as objects, arrays, functions and the ability to solve basic coding problems using JavaScript) and soft skills, such as the student’s willingness and ability to learn.\n\nHack Reactor has created financial partnerships with SkillsFund and Climb Credit and to assist students with paying tuition. As of 2016, WeFinance and Reactor Core (Hack Reactor's parent company) have launched a platform that allows anyone to lend to incoming students.\n\nAccepted students are assigned pre-course work, which takes \"at least 50-80 hours\" and is due prior to the start of their cohort.\n\nHack Reactor’s course is offered in 12-week full-time and 9-month part-time formats. During the first half of the program, students work in pairs on two-day “sprints.” Pair and group work helps teach communication and collaboration skills. During this part of the course, the day typically starts with a “toy problem,” which is a programming challenge designed to illustrate core concepts. This is followed by a lecture in which the instructor frequently checks in with students to assess how well they understand the material. The JavaScript tools and technologies taught at Hack Reactor include Angular, Node, MongoDB, Express, React, Backbone and ES6. The goal of this part of the course is for students to become “autonomous learners and programmers.”\n\nThe second half of the course focuses on projects. Students complete “increasingly elaborate” coding projects of their own design, using whatever languages and frameworks they choose. Students often adopt technologies not taught in the course using “fundamentals and self-teaching methods” taught in the first half of the course.\n\nCo-Founder Shawn Drost says that Hack Reactor is \"committed to being the leading coding immersive in terms of quality, student experience and student outcomes.\" In 2017, they joined a 501(c)(6) non-profit organization called the Council on Integrity in Results Reporting (CIRR) as a founding member. CIRR requires members to report \"the outcomes of every enrolled student must be reported in a single, simple, clear report\" using common standards and follow \"CIRR Truth in Advertising policies\".\n\nIn July 2014, Hack Reactor launched an online program, Hack Reactor Remote. This program has the same curriculum, course structure and teaching method as Hack Reactor’s onsite program. Students attend and participate in the lectures at the same time as the other students, work on the same assignments, and benefit from the same job search and placement resources as the onsite program. Hack Reactor's Remote program has comparable student outcomes to its in-person campuses.\n\nIn 2017, Hack Reactor began offering a part-time version of their remote program, known as Hack Reactor Remote Part-Time.\n\nIn January 2015, Hack Reactor acquired coding bootcamp MakerSquare, which had locations in Austin and San Francisco. MakerSquare has since expanded into Los Angeles and New York City.\n\nMakerSquare has the same admissions process, hiring partner network, and the same curriculum with a few small modifications. In November 2016, they rebranded to share the Hack Reactor name.\n\nIn collaboration with The Last Mile, Hack Reactor launched Code.7370, a coding program in San Quentin State Prison. Inmates have to apply to be a part of the program. Once accepted, they learn HTML, CSS, Python and JavaScript for 8 hours a day, 4 days a week. Hack Reactor instructors volunteered as teachers. In addition to class time students are also given time to work on personal projects. Because inmates are not permitted access to the internet, Code.7370 operates by a proprietary programming platform that simulates a normal web environment. The goal of Code.7370 is to reduce recidivism and help felons reenter the workforce.\n\nHack Reactor helped launch ReBootKAMP, a coding bootcamp in Jordan that focuses on Syrian refugees. ReBootKAMP uses Hack Reactor’s curriculum, and received volunteer assistance from Hack Reactor staff and alumni. ReBootKAMP executives also received training on coding bootcamp best practices from Hack Reactor and Reactor Core.\n\nIn December 2017, Hack Reactor supported the launch of Hola<nowiki><code/></nowiki>, a coding bootcamp based in Mexico City focused on bringing new opportunities to recent returnees. Hola<nowiki><code/></nowiki> is powered by Hack Reactor curriculum and Hack Reactor alumni have been part of their teaching team.\n\n\n"}
{"id": "1942466", "url": "https://en.wikipedia.org/wiki?curid=1942466", "title": "High resolution electron energy loss spectroscopy", "text": "High resolution electron energy loss spectroscopy\n\nHigh resolution electron energy loss spectroscopy (HREELS) is a tool used in surface science. The inelastic scattering of electrons from surfaces is utilized to study electronic excitations or vibrational modes of the surface or of molecules adsorbed to a surface. Hence in contrast to other electron energy loss spectroscopies (EELS) HREELS deals with small energy losses in the range of 10 eV to 1 eV.\nIt plays an important role in the investigation of surface structure, catalysis, dispersion of surface phonons and the monitoring of epitaxial growth.\n\nIn general electron energy loss spectroscopy is based on the energy losses of electrons when inelastically scattered on matter. An incident beam of electrons with a known energy (E) is scattered on a sample. The scattering of these electrons can excite the electronic structure of the sample. If this is the case the scattered electron loses the specific energy (ΔE) needed to cause the excitation. Those scattering processes are called inelastic. It may be easiest to imagine that the energy loss is for example due to an excitation of an electron from an atomic K-shell to the M-shell. The energy for this excitation is taken away from the electron's kinetic energy.\nThen the energies of the scattered electrons (E) are measured and the energy loss can be calculated. From the measured data an intensity versus energy loss diagram is established. In the case of scattering on phonons the so-called energy loss can also be a gain of energy (see: Raman spectroscopy).\nThese energy losses allow, using comparison to other experiments, or theory, to draw conclusion about surface properties of the sample.\n\nExcitations of the surface structure are usually very low energetic ranging from 10 eV to 10 eV. In spectra electrons with only small energy losses, like also Raman scattering, the interesting features are all located very close together and especially near to the very strong elastic scattering peak. Hence the used spectrometers require a high resolution. Therefore, this regime of EELS is called High Resolution EELS.\nIn this context resolution shall be defined as the energy difference in which two features in a spectrum are just distinguishable divided by the mean energy of those features: formula_1\n\nIn the case of EELS the first thing to think of in order to achieve high resolution is using incident electrons of a very precisely defined energy and a high quality analyzer.\nFurther high resolution is only possible when the energies of the incident electrons are not far bigger than the energy losses. For HREELS the energy of the incident electrons is therefore mostly significantly smaller than 10 eV.\n\nConsidering that 10 eV electrons have a mean free path of around 1 nm (corresponds to a few monolayers), which is decreasing with lower energies, this automatically implies that HREELS is a surface sensitive technique.\nThis is the reason why HREELS must be measured in reflection and has to be implemented in ultra high vacuum (UHV). In contrast to e.g. Core Level EELS which operates at very high energies and can therefore also be found in transmission electron microscopes (TEM).\n\nIn HREELS not only the electron energy loss can be measured, often the angular distribution, of electrons of a certain energy loss, in reference to the specular direction gives interesting insight to the structures on the surface.\n\nAs mentioned above HREELS involves an inelastic scattering process on a surface. For those processes the conservation of energy as well as the conservation of momentum’s projection parallel to the surface hold:\n\nE are energies, k and q are wave vectors and G denotes a reciprocal lattice vector. One should mention at this point that for non perfect surfaces G is not in any case a well defined quantum number, what has to be considered when using the second relation. Variables subscripted with i denote values of incident electrons those subscripted with s values of scattered electrons. “||” denotes parallel to the surface.\n\nFor the description of the inelastic scattering processes due to the excitation of vibrational modes of adsorbates different approaches exist.\nThe simplest approach distinguishes between regimes of small and large scattering angles:\n\nThe so-called dipole scattering can be applied when the scattered beam is very near to the specular direction. In this case a macroscopic theory can be applied to explain the results. It can be approached using the so-called dielectrical theory of which a quantum mechanical treatment was first presented by E. Evans and D.L. Mills in the early 1970s. Or by a more unformular model which exactly only holds for perfect conductors:\nA unit cell at the surface does not have homogenous surrounding hence it is supposed to have an electrical dipole moment. When a molecule is adsorbed to the surface there can be an additional dipole moment and the total dipole moment P is present. This dipole moment causes a long range electronic potential in the vacuum above the surface. On this potential the incident electron can scatter inelastically which means it excites vibrations in the dipole structure. The dipole moment can then be written as formula_2. When the adsorbate sticks to a metal surface imaginary dipoles occur as shown in the figure on the right. Hence for an adsorbed dipole normal to the surface the dipole moment “seen” from the vacuum doubles. Whereas the dipole moment of a parallel to the surface adsorbed dipole vanishes. Hence an incident electron can excite the adsorbed dipole only when it is adsorbed normal to the surface and the vibrational mode can be detected in the energy loss spectrum. If the dipole is adsorbed parallel then no energy losses will be detected and the vibrational modes of the dipole are missing in the energy loss spectrum.\n\nThe dielectric model holds also when the material on which the molecule adsorbs is not a metal. The picture shown above is then the limit for formula_3 where formula_4 denotes the relative dielectrical constant.\nWhen measuring the intensity of the electron energy loss peaks and comparing to other experimental results or to theoretical models it can also be told whether a molecule is adsorbed normal to the surface or tilted by an angle.\n\nAs the incident electron in this model has to be scattered in the region above the surface it does not come to a direct impact at the surface and as the amount of momentum transferred is therefore small the scattering of is very much into the specular direction.\n\nImpact scattering is the regime which deals with electrons that are scattered further away from the specular direction. In those cases no macroscopic theory exists and a microscopic theory like, quantum mechanical dispersion theory, has to be applied. Symmetry considerations then also result in certain selection rules (They assume that the energy loss in the inelastic scattering process is negligible): \nAll those selection rules make it possible to identify the normal coordinates of the adsorbed molecules.\n\nIntermediate negative ion resonance: The electron forms a compound state with an adsorbed molecule during the scattering process. But the lifetime of those states are so short that this type of scattering is barely observed.\nAll of those regimes can at once be described with the help of the single microscopic theory the selection rule find their origins that in symmetry considerations.\n\nA microscopic theory makes it for example possible to approach the selection rule for dipole scattering in a more exact way. The scattering cross section is only none vanishing in the case of a nonzero matrix element\nformula_5.\nWhere i denotes the initial and f the final vibrational mode of the adsorbed molecule and p the z component of its dipole moment.\n\nAs the dipole moment is something like charge times length p has the same symmetry properties as z, which is totally symmetric. Hence the product of i and f has to be a totally symmetric function, too, otherwise the matrix element would vanish. Hence\n\"excitations from the totally symmetrical ground state of a molecule are only possible to a totally symmetric vibrational state.\"\nThis is the surface selection rule for dipole scattering. Note that it says nothing about how big the intensity for scattering is nor that of the displacement of the atoms of the adsorbate but its total dipole moment is the operator in the matrix element. This is important as a vibration of the atoms parallel to the surface can cause a vibration of the dipole moment normal to the surface, too. So the result in the section \"dipole scattering\" was not exactly correct.\n\nWhen trying to gain information from selection rules it has to be carefully considered whether really a pure dipole or impact scattering region is investigated.\nFurther symmetry breaking due to strong bindings to the surface must be considered. \nAnother problem is that in cases of bigger molecules often many vibrational modes are degenerate, which could again be resolved due to strong molecule surface interactions.\nThose interactions can also generate completely new dipole moments which the molecule normally does not have. But when investigating carefully it is due to analysis of normal dipole modes mostly possible to get a very good picture of how the molecule adheres to the surface.\n\nAs the electrons used for HREELS are of low energy they do not only have a very short mean free path length in the sample materials but also under normal atmospheric conditions. Therefore, one has to set up the spectrometer in UHV.\nThe spectrometer is in general a computer simulated design that optimizes the resolution while keeping an acceptable electron flux.\n\nThe electrons are generated in an electron source, by heating a tungsten cathode, which is encapsulated by a negatively charged so called repeller that prevents stray electrons from coming into the detector unit. The electrons can leave the source only through a lens system, like e.g. a slot lens system consisting of several slits all on different potential. The purpose of this system is to focus the electrons on the entrance of the monochromator unit, to get a high initial electron flux.\n\nThe monochromator is usually a concentric hemispherical analyser (CHA). In more sensitive setups an additional pre-monochromator is used. The task of the monochromator is to reduce the energy of the passing electrons to some eV due to the help of electron lenses. It further lets only those electrons pass which have the chosen initial energy. To achieve a good resolution it is already important to have incident electrons of a well defined energy one normally chooses a resolution of formula_6 for the monochromator. This means, the electrons leaving the monochromator with e.g. 10 eV have an energy accurate to 10 eV. The beam’s flux is then in the orders of 10 A to 10 A. The radii of the CHA are in the order of several 10 mm. And the deflector electrodes have a saw tooth profile to backscatter electrons which are reflected from the walls in order to reduce the background of electrons with the wrong E. The electrons are then focused by a lens system onto the sample. These lenses are, in contrary to those of the emitter system very flexible, as it is important is to get a good focus on the sample. To enable measurements of angular distributions all those elements are mounted on a rotate able table with the axis cantered at the sample.Its negative charge causes the electron beam to broaden. What can be prevented by charging the top and bottom plates of the CHA deflectors negative. What again causes a change in the deflection angle and has to be considered when designing the experiment.\n\nIn the scattering process at the sample the electrons can lose energies from several 10 eV up to a few electron volt. The scattered electron beam which is of around 10 lower flux than the incident beam then enters, the analyzer, another CHA.\n\nThe analyzer CHA again allows only electrons of certain energies to pass to the analyzing unit, a channel electron multiplier (CEM). For this analyzing CHA the same facts are valid as for the monochromator. Except that a formula_7 higher resolution as in the monochromator is wanted. Hence the radial dimensions of this CHA are mostly bigger by like a factor 2. Due to aberrations of the lens systems the beam has also broadened. To sustain a high enough electron flux to the analyzer the apertures are also about a factor 2 bigger. To make the analysis more accurate, especially to reduce the background of in the deflector scattered electrons often two analyzers are used, or additional apertures are added behind the analyzers as scattered electrons of the wrong energy normally leave the CHAs under large angles. In this way energy losses of 10 eV to 10 eV can be detected with accuracies of about 10 eV.\n\nDue to the electron flux the apertures can become negatively charged, which makes them effectively smaller for the passing electrons. This has to be considered when doing the design of the setup as it is anyway difficult to keep different potentials, of repeller, lenses, screening elements, and the reflector, constant. \nUnstable potentials on lenses or CHA deflectors would cause fluctuations in the measured signal. Similar problems are caused by external electric or magnetic fields, either they cause fluctuations in the signal, or add a constant offset. That is why the sample is normally shielded by equipotential, metal electrodes to keep the region of the sample field free so that neither the probe electrons nor the sample is affected by external electric fields. Further a cylinder of a material with a high magnetic permeability, e.g. Mu-metal, built around the whole spectrometer to keep magnetic fields or field inhomogeneties at the experiment down to 10 mG or 1mG/cm. \nBecause of the same reason the whole experiment, except the lenses which are normally made of coated copper, is designed in stainless antimagnetic steel and insulating parts are avoided wherever possible.\n\n\n\n"}
{"id": "23223531", "url": "https://en.wikipedia.org/wiki?curid=23223531", "title": "Horror vacui (physics)", "text": "Horror vacui (physics)\n\nIn physics, horror vacui, or plenism (), is commonly stated as \"Nature abhors a vacuum\". It is a postulate attributed to Aristotle, who articulated a belief, later criticized by the atomism of Epicurus and Lucretius, that nature contains no vacuums because the denser surrounding material continuum would immediately fill the rarity of an incipient void. He also argued against the void in a more abstract sense (as \"separable\"), for example, that by definition a void, itself, is nothing, and following Plato, nothing cannot rightly be said to exist. Furthermore, insofar as it would be featureless, it could neither be encountered by the senses, nor could its supposition lend additional explanatory power. Hero of Alexandria challenged the theory in the first century CE, but his attempts to create an artificial vacuum failed. The theory was debated in the context of 17th-century fluid mechanics, by Thomas Hobbes and Robert Boyle, among others, and through the early 18th century by Sir Isaac Newton and Gottfried Leibniz.\n\nPlenism means \"fullness\", from Latin plēnum, English \"plenty\", cognate via Proto-Indo-European to \"full\".\n\nThe idea was restated as \"Natura abhorret vacuum\" by François Rabelais in his series of books titled Gargantua and Pantagruel in the 1530s. The theory was supported and restated by Galileo Galilei in the early 17th century as \"Resistenza del vacuo\". Galileo was surprised by the fact that water could not rise above a certain level in an aspiration tube in his suction pump, leading him to conclude that there is a limit to the phenomenon. René Descartes proposed a plenic interpretation of atomism to eliminate the void, which he considered incompatible with his concept of space. The theory was rejected by later scientists, such as Galileo's pupil Evangelista Torricelli who repeated his experiment with mercury. Blaise Pascal successfully repeated Galileo's and Torricelli's experiment and foresaw no reason why a perfect vacuum could not be achieved in principle. Scottish philosopher Thomas Carlyle mentioned Pascal's experiment in the Edinburgh Encyclopedia in an 1823 article titled \"Pascal\". The Magdeburg hemispheres used by Otto von Guericke in 1650 were seen by some as proof that the theory was not correct.\n\nJames Clerk Maxwell subscribed to this philosophy when he argued forcefully for the existence of the luminiferous aether in his 1861 electromagnetic theory of light.\n\n"}
{"id": "40450007", "url": "https://en.wikipedia.org/wiki?curid=40450007", "title": "Hydroxymatairesinol", "text": "Hydroxymatairesinol\n\nHydroxymatairesinol (HMR) is a lignan found in Norway spruce (\"Picea abies\"). It is an enterolactone precursor with anticancer activities. In rats, HMR decreased the volume of induced tumours and stabilised established tumours, as well as preventing the development of new tumours. It has also shown anti-oxidant properties \"in vitro\".\n\nHMR's chemical structure is similar to matairesinol. At high concentrations, HMR has estrogenic properties, which are considerably weaker than those of estradiol.\n"}
{"id": "15013011", "url": "https://en.wikipedia.org/wiki?curid=15013011", "title": "Ice XV", "text": "Ice XV\n\nIce XV is a crystalline form of ice, the proton-ordered form of ice VI. It is created by cooling water to around 130 K at 1 GPa (9820 atm).\n\nOrdinary water ice is known as ice I, (in the Bridgman nomenclature). Different types of ice, from ice II to ice XVI, have been created in the laboratory at different temperatures and pressures.\n\nOn 14 June, 2009, Christoph Salzmann at the University of Oxford and colleagues reported having created ice XV and say that its properties differ significantly from those predicted. In particular, ice XV is antiferroelectric rather than ferroelectric as had been predicted.\n\n\n"}
{"id": "10766750", "url": "https://en.wikipedia.org/wiki?curid=10766750", "title": "International Union of Crystallography", "text": "International Union of Crystallography\n\nThe International Union of Crystallography (IUCr) is an organisation devoted to the international promotion and coordination of the science of crystallography. The IUCr is a member of the International Council for Science (ICSU).\n\nThe objectives of the IUCr are to promote international cooperation in crystallography and to contribute to all aspects of crystallography, to promote international publication of crystallographic research, to facilitate standardization of methods, units, nomenclatures and symbols, and to form a focus for the relations of crystallography to other sciences.\n\nThe IUCr fulfils these objectives by publishing in print and electronically primary scientific journals through \"Crystallography Journals Online\", the series of reference volumes \"International Tables for Crystallography\", distributing the quarterly \"IUCr Newsletter\", maintaining the online \"World Directory/Database of Crystallographers\", awarding the \"Ewald Prize\" and organising the triennial \"Congress and General Assembly\".\n\nIn 1944 the yearly meeting of the X-ray Analysis Group (XRAG) of the UK Institute of Physics was held in Oxford, and the distinguished German crystallographer Paul Peter Ewald, who then taught at Queen's University Belfast, was invited to give the evening lecture. In it he gave a historical survey of some of the stages in the evolution of X-ray crystallography and ended with a strong plea for the formation of an international society or union which would represent, and unify publication for, the new science. This idea was followed up by the British crystallographers, and particularly by Sir Lawrence Bragg, the Chairman of XRAG. In June 1946, within a year of the termination of fighting in WWII, he arranged for an international meeting of crystallographers in London which was attended by some 120 scientists from most of the allied countries. In that London meeting Ewald was elected Chairman of the Provisional International Crystallographic Committee, which put into action the decision to form the International Union of Crystallography. Sir Lawrence Bragg was the first formally elected President of this IUCr, with Ralph Walter Graystone Wyckoff and Arne Westgren as Vice-Presidents. Ewald was elected as 5th President of the IUCr, the 'international society or union' that he had originally conceived, in 1960.\n\nThe IUCr notation is the notation for the symmetry group adopted by the International Union of Crystallography in 1952. It identifies members of the Wallpaper group with a 4 character name. First it has a \"P\" or \"C\" for \"primitive\" or \"centered\" groups. Groups are denoted by a number 1, 2, 3, 4, or 6 for the highest order of symmetry. Groups can have one or two reflections, denoted as vertical mirrors first (horizontal reflection), and horizontal second (vertical reflection). A simple reflection is denoted by an \"m\" (mirror), and a glide-reflection is denoted by a \"g\". Place holder \"1\" denotes an orthogonal direction with no reflections.\n\n\n"}
{"id": "1126641", "url": "https://en.wikipedia.org/wiki?curid=1126641", "title": "Invariant (physics)", "text": "Invariant (physics)\n\nIn mathematics and theoretical physics, an invariant is a property of a system which remains unchanged under some transformation.\n\nIn the current era, the immobility of Polaris (the North Star) under the diurnal motion of the celestial sphere is a classical illustration of physical invariance.\n\nAnother example of a physical invariant is the speed of light under a Lorentz transformation and time under a Galilean transformation. Such spacetime transformations represent shifts between the reference frames of different observers.\n\nBy Noether's theorem invariance of the action of a physical system under a continuous symmetry represents a fundamental conservation law. For example, invariance under translation leads to conservation of momentum, and invariance in time leads to conservation of energy.\n\nQuantities can be invariant under some common transformations but not under others. For example, the velocity of a particle is invariant when switching from rectangular coordinates to curvilinear coordinates, but is not invariant when transforming between frames of reference that are moving with respect to each other. Other quantities, like the speed of light, are always invariant.\n\nInvariants are important in modern theoretical physics, and many theories are expressed in terms of their symmetries and invariants. \n\nCovariance and contravariance generalize the mathematical properties of invariance in tensor mathematics, and are frequently used in electromagnetism, special relativity, and general relativity.\n\n"}
{"id": "34986220", "url": "https://en.wikipedia.org/wiki?curid=34986220", "title": "Linguistic sequence complexity", "text": "Linguistic sequence complexity\n\nLinguistic sequence complexity (LC) is a measure of the 'vocabulary richness' of a genetic text in gene sequences.\nWhen a nucleotide sequence is written as text using a four-letter alphabet, the repetitiveness of the text, that is, the repetition of its N-grams (words), can be calculated and serves as a measure of sequence complexity. Thus, the more complex a DNA sequence, the richer its oligonucleotide vocabulary, whereas repetitious sequences have relatively lower complexities. Subsequent work improved the original algorithm described in Trifonov (1990), without changing the essence of the linguistic complexity approach.\n\nThe meaning of LC may be better understood by regarding the presentation of a sequence as a tree of all subsequences of the given sequence. The most complex sequences have maximally balanced trees, while the measure of imbalance or tree asymmetry serves as a complexity measure. The number of nodes at the tree level is equal to the actual vocabulary size of words with the length in a given sequence; the number of nodes in the most balanced tree, which corresponds to the most complex sequence of length N, at the tree level is either 4 or N-j+1, whichever is smaller. Complexity () of a sequence fragment (with a length RW) can be directly calculated as the product of vocabulary-usage measures (U):\n\nVocabulary usage for oligomers of a given size can be defined as the ratio of the actual vocabulary size of a given sequence to the maximal possible vocabulary size for a sequence of that length. For example, U for the sequence ACGGGAAGCTGATTCCA = 14/16, as it contains 14 of 16 possible different dinucleotides; U for the same sequence = 15/15, and U=14/14. For the sequence ACACACACACACACACA, U=1/2; U=2/16=0.125, as it has a simple vocabulary of only two dinucleotides; U for this sequence = 2/15. k-tuples with k from two to W considered, while W depends on RW. For RW values less than 18, W is equal to 3; for RW less than 67, W is equal to 4; for RW<260, W=5; for RW<1029, W=6, and so on. The value of provides a measure of sequence complexity in the range 0<C<1 for various DNA sequence fragments of a given length.\nThis formula is different from the original LC measure in two respects: in the way vocabulary usage U is calculated, and because is not in the range of 2 to N-1 but only up to W. This limitation on the range of U makes the algorithm substantially more efficient without loss of power.\nIn was used another modified version, wherein linguistic complexity (LC) is defined as the ratio of the number of substrings of any length present in the string to the maximum possible number of substrings. Maximum vocabulary over word sizes 1 to m can be calculated according to the simple formula .\nThis sequence analysis complexity calculation can be used to search for conserved regions between compared sequences for the detection of low-complexity regions including simple sequence repeats, imperfect direct or inverted repeats, polypurine and polypyrimidine triple-stranded DNA structures, and four-stranded structures (such as G-quadruplexes).\n"}
{"id": "51248620", "url": "https://en.wikipedia.org/wiki?curid=51248620", "title": "List of Citadel paints", "text": "List of Citadel paints\n\nThis table contains a list of the Citadel paint range used for painting Citadel Miniatures, produced by Games Workshop. \n\nCitadel Miniatures are metal, resin, and plastic miniature gaming figures, accessories, and scenery for Games Workshop's range of tabletop wargames, such as their signature \"Warhammer Fantasy Battle\", and \"Warhammer 40,000\" games and \"The Lord of the Rings Strategy Battle Game\".\n\nThe Hex color codes are for those displayed on the Games Workshop website. The codes for the paints marked as \"(Metal)\" and for some in the \"Technical\" range are approximate.\n\n\n"}
{"id": "7028955", "url": "https://en.wikipedia.org/wiki?curid=7028955", "title": "List of South Korean flags", "text": "List of South Korean flags\n\nThis is a list of flags used in South Korea, from 1945 to the present.\n\nAs the South Korean government claims the territory of North Korea as its own, provincial flags also exist for the North Korean provinces that are claimed by South Korea. The following are flags of the five Korean provinces located entirely north of the Military Demarcation Line as according to the South Korean government, as it formally claims to be the sole legitimate government of the entire Korean Peninsula. \n"}
{"id": "26955844", "url": "https://en.wikipedia.org/wiki?curid=26955844", "title": "List of calques", "text": "List of calques\n\nA calque or loan translation is a word or phrase borrowed from another language by literal, word-for-word (Latin: \"verbum pro verbo\") or word-for-word translation. This list contains examples of calques in various languages.\n\n\n\n\n\n\n\n\n\n\nExamples of Romance language expressions calqued from foreign languages include:\n\n\nMany calques found in Southwestern US Spanish, come from English:\nAlso technological terms calqued from English are used throughout the Spanish-speaking world:\n\n\n\n\n\nNote: \"From a technical standpoint, Danish and the bokmål standard of Norwegian are the same language, with minor spelling and pronunciation differences (equivalent to British and American English). For this reason, they will share a section.\"\n\n\n\n\nIn more recent times, the Macedonian language has calqued new words from other prestige languages including German, French and English.\n\n\nSome words were originally calqued into Russian and then absorbed into Macedonian, considering the close relatedness of the two languages. Therefore, many of these calques can also be considered Russianisms.\n\nThe poet Aleksandr Pushkin (1799–1837) was perhaps the most influential among the Russian literary figures who would transform the modern Russian language and vastly expand its ability to handle abstract and scientific concepts by importing the sophisticated vocabulary of Western intellectuals.\n\nAlthough some Western vocabulary entered the language as loanwords – e.g., Italian \"salvietta\", \"napkin\", was simply Russified in sound and spelling to салфетка (\"salfetka\") – Pushkin and those he influenced most often preferred to render foreign borrowings into Russian by calquing. Compound words were broken down to their component roots, which were then translated piece-by-piece to their Slavic equivalents. But not all of the coinages caught on and became permanent additions to the lexicon; for example, любомудрие (\"ljubomudrie\") was promoted by 19th-century Russian intellectuals as a calque of \"philosophy\", but the word eventually fell out of fashion, and modern Russian instead uses the loanword философия (\"filosofija\").\n\n\n\n\nSince Finnish, a Uralic language, differs radically in pronunciation and orthography from Indo-European languages, most loans adopted in Finnish either are calques or soon become such as foreign words are translated into Finnish. Examples include:\n\nWhen Jews immigrate to Israel, they often Hebraize their surnames. One approach to doing so was by calque from the original (often German or Yiddish) surname. For instance, Imi Lichtenfield (itself a half-calque), founder of the martial art Krav Maga, became Imi Sde-Or. Both last names mean \"light field\". For more examples and other approaches, see the article on Hebraization of surnames.\n\nAccording to linguist Ghil'ad Zuckermann, the more contributing languages have a structurally identical expression, the more likely it is to be calqued into the target language. In Israeli (his term for \"Modern Hebrew\") one uses \"má nishmà\", lit. \"what's heard?\", with the meaning of \"what's up?\". Zuckermann argues that this is a calque not only of the Yiddish expression \"Was hört sich?\" (usually pronounced \"v(o)sérts´kh\"), but also of the parallel expressions in Polish, Russian and Romanian. Whereas most revivalists were native Yiddish-speakers, many first speakers of Modern Hebrew spoke Russian and Polish too. So a Polish speaker in the 1930s might have used \"má nishmà\" not (only) due to Yiddish \"Was hört sich?\" but rather (also) due to Polish \"Co słychać?\" A Russian Jew might have used \"ma nishma\" due to \"Что слышно?\" (pronounced \"chto slyshno\") and a Romanian Israeli would echo \"ce se aude\". According to Zuckermann, such multi-sourced calquing is a manifestation of the Congruence principle.\n\nModern Malayalam is replete with calques from English. The calques manifest themselves as idioms and expressions and many have gone on to become clichés. However standalone words are very few. The following is a list of commonly used calque phrases/expressions.All of these are exact translations of the corresponding English phrases.\n\n"}
{"id": "29127472", "url": "https://en.wikipedia.org/wiki?curid=29127472", "title": "List of causes of shortness of breath", "text": "List of causes of shortness of breath\n\nMany different conditions can lead to the feeling of dyspnea (shortness of breath). DiagnosisPro, an online medical expert system, listed 497 in October 2010. The most common cardiovascular causes are acute myocardial infarction and congestive heart failure while common pulmonary causes include: chronic obstructive pulmonary disease, asthma, pneumothorax, and pneumonia.\n"}
{"id": "42030981", "url": "https://en.wikipedia.org/wiki?curid=42030981", "title": "List of glands of the human body", "text": "List of glands of the human body\n\nThis page contains a list of glands of the human body\nThere are several specialized glands within the human integumentary system that are derived from apocrine or sebaceous gland precursors. There are no specialized variants of eccrine glands.\n\nSee List of human endocrine organs and actions\n"}
{"id": "12155934", "url": "https://en.wikipedia.org/wiki?curid=12155934", "title": "List of lakes in Oregon", "text": "List of lakes in Oregon\n\nThis is an incomplete list of the lakes and reservoirs of Oregon.\n\n"}
{"id": "5971828", "url": "https://en.wikipedia.org/wiki?curid=5971828", "title": "List of mathematicians (R)", "text": "List of mathematicians (R)\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "28775637", "url": "https://en.wikipedia.org/wiki?curid=28775637", "title": "List of software package management systems", "text": "List of software package management systems\n\nThis is a list of software package management systems, categorized first by package format (binary, source code, hybrid) and then by operating system family.\n\nThe following package management systems distribute apps in binary package form; i.e., all apps are compiled and ready to be installed and use.\n\n\n\n\n\n\n\n\n\nThe following package management systems distribute the source code of their apps. Either the user must know how to compile the packages, or they come with a script that automates the compilation process. For example, in GoboLinux a recipe file contains information on how to download, unpack, compile and install a package using its Compile tool. In both cases, the user must provide the computing power and time needed to compile the app, and is legally responsible for the consequences of compiling the package.\n\n\n\n\nThe following unify package management for several or all Linux and sometimes Unix variants. These, too, are based on the concept of a recipe file.\n\nA wide variety of package management systems are in common use today by proprietary software operating systems, handling the installation of both proprietary and free packages.\n\n\n"}
{"id": "43720735", "url": "https://en.wikipedia.org/wiki?curid=43720735", "title": "Lists about the pharmaceutical industry", "text": "Lists about the pharmaceutical industry\n\nThese are Wikipedia lists about the pharmaceutical industry. The pharmaceutical industry develops, produces, and markets drugs or pharmaceuticals licensed for use as medications. Pharmaceutical companies are allowed to deal in generic or brand medications and medical devices. They are subject to a variety of laws and regulations regarding the production, testing, and marketing of drugs.\n\n"}
{"id": "44310137", "url": "https://en.wikipedia.org/wiki?curid=44310137", "title": "Metal testing", "text": "Metal testing\n\nMetal testing is a process or procedure used to check composition of an unknown metallic substance. There are destructive processes and nondestructive processes. Metal testing can also include, determining the properties of newly forged metal alloys. With many chemical-property databases readily available, identification of unmarked pure,common metals can be a quick and easy process. Leaving the original sample in complete, re-usable condition. This type of testing is nondestructive. When working with alloys (forged mixtures) of metals however, to determine the exact composition, could result in the original sample being separated into its starting materials, then measured and calculated. After the components are known they can be looked up and matched to known alloys. The original sample would be destroyed in the process. This type of testing is destructive.\n\nIn this kind of testing, the material undergoes mechanical testing and is discarded thereafter. Test results are compared with specifications. Subtypes include:\n\n\nRaw and finished material undergoes testing according to code specifications such as ASME Boiler and Pressure Vessel Code Section V. The tested material is not damaged by the test. Subtypes include:\n\n"}
{"id": "37808739", "url": "https://en.wikipedia.org/wiki?curid=37808739", "title": "Mind Sports South Africa", "text": "Mind Sports South Africa\n\nMind Sports South Africa (MSSA) is recognised by Act of Parliament as the national controlling body for mind sports in South Africa.\n\nMind Sports South Africa (MSSA) is also an affiliate of the International eSports Federation, Fédération Mondiale du Jeu de Dames, and the International Wargames Federation. Due to its membership of such international bodies, the MSSA is the sole authority for the games that it caters for in terms of the Sport and Recreation Act, number 110 of 1998 (as amended).\n\nMind Sports South Africa was formally constituted on 14 December 1985. However, it was not until 1990 that the MSSA became a member of NOCSA in 1990, and in 1991 that the MSSA became affiliated to the Confederation of South African Sports Confederation (COSAS).\n\nThe MSSA was one of the members that encouraged unity during the apartheid era, and thus voted in favour of the unifying of sport. As a consequence of the actions of the many National Federations, the National Sports Council was formed in 1994 and was immediately accepted as a full member of the newly formed body. Upon dissolution of the NSC in 1999, the MSSA played its part in supporting the formation of the South African Sports Commission in 1999.\n\nOnce the South African Sports Commission was formed (by Act of Parliament) the MSSA again was accepted as a full member. Even when the Minister of Sport and Recreation saw the need to ask Parliament to amend the Sports and Recreation Act (number110 of 1998), the official recognition of Mind Sports South Africa remained unchanged as the MSSA became a founding member of the South African Sports Confederation and Olympic Committee (SASCOC).\n\nAll the games promoted by Mind Sports South Africa are accredited as national sports. Such accreditation by the Sports and Recreation Act (number110 of 1998), guarantees the MSSA as being the only authority for the administration and control of the games that fall under the MSSA's jurisdiction.\n\nThe MSSA changed its name in 2005 from the South African Wargames Union (SAWU) to that of Mind Sports South Africa.\n\nMind Sports South Africa, like other sports federations in South Africa, is constituted as a voluntary association.\n\nThe highest authority of the MSSA is the Annual General Meeting which holds all Committees accountable for their actions.\n\nThe Executive Committee meets a minimum of twice a year, and requires a full report from the Management Board.\n\nThe Management Board deals with the day-to-day operations of the MSSA and overseas the different Boards of Control. Clubs are directly affiliated to the MSSA which ensures that there is greater transparency and inclusion.\n\nBack in the early 1990s, the South African Wargames Union (as the MSSA was then known) was invited to participate in the World Team Championships for Wargames. Up until that point the MSSA used the rampant lion as its symbol. However, it was felt by the committee to approach the State Herald to design something specifically for the MSSA that best represented the games as administered by the MSSA. The State Herald Frederick Brownell designed the Janus Knight for the MSSA.\n\nThe logo is made up of the following aspects:\n\nThe double-headed knight chess piece:\nThe knight chess piece is already an international symbol for army battle school. The symbol represents tactical and strategic thought and training. By making the symbol a double-headed knight, it also incorporates the concepts of considering the opponent's moves and shows the mental versatility of gamers.\n\nThe circle:\nThe circle represents the rules to which all the games are played, and the unity of all the games that the MSSA represents.\n\nIn terms of its Constitution, Mind Sports South Africa caters for a wide variety of Mind Sports in South Africa, such as:\n\nBoard gaming\nCard gaming\n\nFigure gaming\neSports\nRobotics\n\nEvery year the MSSA holds an Annual General Meeting (AGM)directly after the South African National Championships. It is at such AGM that the games to be played in the eSports Discipline are selected by the member clubs.\n\n\n\nSouth Africa has produced the following world champions:\nSouth Africa has produced the following medal winners in international championships:\n\nThe following online events were held by member associations and under the jurisdiction of the IeSF:\n\nThe following Test matches were held:\n\nMSSA has had the following committee members for the period 2005 to present:\n\nThe following South Africans have served on international committees:\nMind Sport South Africa is affiliated to the following international federations:\n\n\n"}
{"id": "47795538", "url": "https://en.wikipedia.org/wiki?curid=47795538", "title": "Models of teaching social science", "text": "Models of teaching social science\n\nA model of teaching is a plan or pattern that can be used to shape curriculum's,to design instructional materials and to guide instruction in the classroom.\n\n\n\nIntroduction,importance,types\n"}
{"id": "9145213", "url": "https://en.wikipedia.org/wiki?curid=9145213", "title": "Outline of science", "text": "Outline of science\n\nThe following outline is provided as a topical overview of science:\n\nScience – the systematic effort of acquiring knowledge—through observation and experimentation coupled with logic and reasoning to find out what can be proved or not proved—and the knowledge thus acquired. The word \"science\" comes from the Latin word \"scientia\" meaning knowledge. A practitioner of science is called a \"scientist\". Modern science respects objective logical reasoning, and follows a set of core procedures or rules in order to determine the nature and underlying natural laws of the universe and everything in it. Some scientists do not know of the rules themselves, but follow them through research policies. These procedures are known as the scientific method.\n\n\nScientific method   (outline) – body of techniques for investigating phenomena and acquiring new knowledge, as well as for correcting and integrating previous knowledge. It is based on observable, empirical, measurable evidence, and subject to laws of reasoning, both deductive and inductive.\n\nBranches of science – divisions within science with respect to the entity or system concerned, which typically embodies its own terminology and nomenclature.\n\nNatural science   (outline) – major branch of science, that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: biology, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.\n\nFormal science – branches of knowledge that are concerned with formal systems, such as those under the branches of: logic, mathematics, computer science, statistics, and some aspects of linguistics. Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world, but instead with the properties of formal systems based on definitions and rules.\n\n\n\nSocial science – study of the social world constructed between humans. The social sciences usually limit themselves to an anthropomorphically centric view of these interactions with minimal emphasis on the inadvertent impact of social human behavior on the external environment (physical, biological, ecological, etc.). 'Social' is the concept of exchange/influence of ideas, thoughts, and relationship interactions (resulting in harmony, peace, self enrichment, favoritism, maliciousness, justice seeking, etc.) between humans. The scientific method is utilized in many social sciences, albeit adapted to the needs of the social construct being studied.\n\nApplied science – branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements.\n\n\n\n\n\n\n\nSee – \n\n\n\n\n\n\nThe scientific fields mentioned below are generally described by the science they study.\n\n\n\n\nScience education\n\n\n"}
{"id": "9190286", "url": "https://en.wikipedia.org/wiki?curid=9190286", "title": "Protocol (science)", "text": "Protocol (science)\n\nIn the natural sciences, a protocol is a predefined written procedural method in the design and implementation of experiments. Protocols are written whenever it is desirable to standardize a laboratory method to ensure successful replication of results by others in the same laboratory or by other laboratories. Detailed protocols also facilitate the assessment of results through peer review. In addition to detailed procedures and lists of required equipment and instruments, protocols often include information on safety precautions, the calculation of results and reporting standards, including statistical analysis and rules for predefining and documenting excluded data to avoid bias. Protocols are employed in a wide range of experimental fields, from social science to quantum mechanics. Written protocols are also employed in manufacturing to ensure consistent quality.\n\nFormal protocol is the general rule in fields of applied science, such as environmental and medical studies that require the coordinated, standardized work of many participants. Such predefined protocols are an essential component of Good Laboratory Practice (GLP) and Good Clinical Practice (GCP) regulations. Protocols written for use by a specific laboratory may incorporate or reference standard operating procedures (SOP) governing general practices required by the laboratory. A protocol may also reference applicable laws and regulations that are applicable to the procedures described. Formal protocols typically require approval by a laboratory official before they are implemented for general use.\n\nIn a clinical trial, the protocol is carefully designed to safeguard the health of the participants as well as answer specific research questions. A protocol describes what types of people may participate in the trial; the schedule of tests, procedures, medications, and dosages; and the length of the study. While in a clinical trial, participants following a protocol are seen regularly by research staff to monitor their health and to determine the safety and effectiveness of their treatment. Since 1996, clinical trials conducted are widely expected to conform to and report the information called for in the CONSORT Statement, which provides a framework for designing and reporting protocols. Though tailored to health and medicine, ideas in the CONSORT statement are broadly applicable to other fields where experimental research is used. Clearly defined protocols are also required by research funded by the National Institutes of Health.\n\nSafety precautions are a valuable addition to a protocol, and can range from requiring goggles to provisions for containment of microbes, environmental hazards, toxic substances, and volatile solvents. Procedural contingencies in the event of an accident may be included in a protocol or in a referenced SOP.\n\nProcedural information may include not only safety procedures but also procedures for avoiding contamination, calibration of equipment, equipment testing, documentation, and all other relevant issues. These procedural protocols can be used by skeptics to invalidate any claimed results if flaws are found.\n\nEquipment testing and documentation includes all necessary specifications, calibrations, operating ranges, etc. Environmental factors such as temperature, humidity, barometric pressure, and other factors can often have effects on results. Documenting these factors should be a part of any good procedure.\n\nProtocols for methods that produce numerical results generally include detailed formulae for calculation of results. Formula may also be included for preparation of reagents and other solutions required for the work. Methods of statistical analysis may be included to guide interpretation of the data. \n\nMany protocols include provisions for avoiding bias in the interpretation of results. Approximation error is common to all measurements. These errors can be absolute errors from limitations of the equipment or propagation errors from approximate numbers used in calculations. Sample bias is the most common and sometimes the hardest bias to quantify. Statisticians often go to great lengths to ensure that the sample used is representative. For instance political polls are best when restricted to likely voters and this is one of the reasons why web polls cannot be considered scientific. The sample size is another important concept and can lead to biased data simply due to an unlikely event. A sample size of 10, i.e. polling 10 people, will seldom give valid polling results. Standard deviation and variance are concepts used to quantify the likely relevance of a given sample size. The mass media and the public often use \"average\" and \"mean\" values interchangeably, which can lead to dubious and even misleading arguments. The placebo effect and observer bias often require an experiment to use a double blind protocol and a control group.\n\nA protocol may require blinding to avoid bias. \n\nAn experimenter may have latitude defining procedures for blinding and controls but may be required to justify those choices if the results are published or submitted to a regulatory agency. When it is known during the experiment which data was negative there are often reasons to rationalize why that data shouldn't be included. Positive data are rarely rationalized the same way.\n\nA protocol may specify reporting requirements. Reporting requirements would include all elements of the experiments design and protocols and any environmental factors or mechanical limitations that might affect the validity of the results.\n"}
{"id": "14960344", "url": "https://en.wikipedia.org/wiki?curid=14960344", "title": "R&amp;D intensity", "text": "R&amp;D intensity\n\nResearch and development intensity or simply R&D intensity, is generally defined as expenditures by a firm on its research and development (R&D) divided by the firm's sales. There are two types of R&D intensity: direct and indirect. R&D intensity varies, in general, according to a firm's industry sector, product knowledge, manufacturing, and technology, and is a metric that can be used to gauge the level of a company's investment to spur innovation in and through basic and applied research. A further aim of R&D spending, ultimately, is to increase productivity (e.g., factor productivity) as well as an organization's salable output.\n\nGenerally speaking, R&D is seen as a main driver of societal and business innovation. The OECD's \"Frascati Manual\" describes R&D as \"creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of man, culture and society, and the use of this stock of knowledge to devise new applications.\"\n\nR&D expenditure and R&D intensity are two of the key indicators used to monitor resources devoted to science and technology worldwide. R&D intensity has been defined as \"the ratio of expenditures by a firm on research and development to the firm's sales.\" William Leonard has described research intensity as \"measured usually by ratios of scientific personnel to total employment or by R&D expenditures/sales\" to gains in such variables as productivity, profits, sales, and asset status. R&D intensity is therefore a measure of a company's R&D spending toward activities aimed at expanding sector and product knowledge, manufacturing, and technology, and so aimed at spurring innovation in and through basic and applied research. Furthermore, it is aimed at increasing \"factor productivity and salable output\".\n\nThere are two types of R&D intensity, calculated as follows:\n\nAs the National Science Foundation explains: absolute levels of \"R&D expenditures indicate the level of effort dedicated to producing future products and process improvements while maintaining current market share and increasing operating efficiency. By extension, such expenditures may reflect firms' perceptions of the market's demand for new and improved technology.\" However, R&D intensity is the most frequently used measure \"to gauge the relative importance of R&D across industries and among firms in the same industry.\" Economic research on sixteen industries by William Leonard, \"the relation [between investment and gains] appears two years after R&D spending and increases thereafter\", although research intensity relates \"less effectively\" to \"manpower ratios [ratios of scientific personnel to total employment].\"\n\nR&D intensity differs between different sectors: high-tech sectors (such as aircraft & spacecraft, electrical equipment, and pharmaceuticals) are characterized by the highest R&D intensity, while low-tech sectors (such as food products, iron and steel, and textiles) usually have low R&D intensity. In fact, R&D intensity could be used as the sole indicator to identify high-tech sectors.\n\nR&D intensity for a \"country\" or larger political or geographical entity is defined as its R&D expenditure as a percentage of gross domestic product (GDP) of the entity. Generally speaking, developed countries have higher R&D intensities than developing countries. As Eurostat noted in 2013, for a preceding period,\n\nGERD can be broken down among four sectors of performance: business enterprise, higher education, government, and private not-for-profit institutions serving households (PNP).\n\n"}
{"id": "30865887", "url": "https://en.wikipedia.org/wiki?curid=30865887", "title": "Selectorate theory", "text": "Selectorate theory\n\nThe selectorate theory is detailed in \"The Logic of Political Survival\", authored by Bruce Bueno de Mesquita of New York University (NYU), Alastair Smith of NYU, Randolph M. Siverson of UC Davis, and James D. Morrow of the University of Michigan.\n\nIn selectorate theory, three groups of people affect leaders. These groups are the nominal selectorate, the real selectorate, and the winning coalition. The nominal selectorate, also referred to as the interchangeables, includes every person who has some say in choosing the leader (for example, in an American presidential election, all registered voters). The real selectorate, also referred to as the influentials, are those who really choose the leaders (for example, in an American presidential election, those people who cast a vote). The winning coalition, also referred to as the essentials, are those whose support translates into victory (for example, in an American presidential election, those voters that get a candidate to 270 Electoral College votes). In other countries, leaders may stay in power with the support of much smaller numbers of people, such as senior figures in the security forces, and business oligarchs, in contemporary Russia.\n\nThe fundamental premise in selectorate theory is that the primary goal of a leader is to remain in power. To remain in power, leaders must maintain their winning coalition. When the winning coalition is small, as in autocracies, the leader will tend to use private goods to satisfy the coalition. When the winning coalition is large, as in democracies, the leader will tend to use public goods to satisfy the coalition.\n\nIn \"The Dictator's Handbook\", Bueno de Mesquita and Smith state five rules that leaders should use to stay in power: (1) The smaller the winning coalition the fewer people to satisfy to remain in control. (2) Having a large nominal selectorate gives a pool of potential people to replace dissenters in coalition. (3) Maintain control of revenue flows to redistribute to your friends. (4) But only pay friends enough that they will not consider overthrowing you and at the same time little enough so that they depend on you. (5) Don't take your friends' money and redistribute it to the masses.\n\nA public good is one that everyone enjoys non-exclusively such as national defense or clean water. A private good is a good that is enjoyed exclusively by a select few, usually within the winning coalition, and cannot be shared. An example of such a good would be anything exclusionary, such as cash or legal impunity.\n\nIt can be said, then, that everyone in the selectorate, including the winning coalition, reap the benefits of public goods while only those within the winning coalition enjoy private goods.\n\nAccording to the selectorate theory, a leader has the greatest chance of political survival when the selectorate is large and the winning coalition is small, which occurs in an autocracy. This is because those who are in a winning coalition can easily be replaced by other members of the selectorate who are not in the winning coalition. Thus, the costs of defection for those members of the winning coalition can be potentially large, namely the loss of all private goods. Similarly, the chances of a challenger in replacing the leader are similarly smallest in such an autocratic system since those in the winning coalition would be hard pressed to defect. The ratio of private to public goods as payoff to the winning coalition is the highest in such a system.\n\nA monarchy, where the selectorate is small and the winning coalition is even smaller, provides a challenger with a greater opportunity to overthrow the current leader. This is because the proportion of selectorate members who are also in the winning coalition is relatively large. That is, if a new leader comes to power, chances are a given member of the winning coalition will remain within the coalition. The incentive for defection to attain a greater amount of goods offered by a challenger is not, in this case, outweighed by the risk of not being included in the new winning coalition. Here, the proportion of private goods in relation to public goods is seen declining.\n\nA scenario in which both the winning coalition is large and the selectorate is even larger provides the least amount of stability to a leader’s occupancy of power; such a system is a democracy. Here, the proportion of public goods outweighs private goods simply because of the sheer size of the winning coalition; it would be far too costly to provide private goods to every individual member of the winning coalition when the benefits of public goods would be enjoyed by all. Because of this fact—that the leader cannot convince winning coalition members to remain loyal through private good incentives, which are in turn cost-restrictive—the challenger poses the greatest threat to the incumbent. This degree of loyalty to the incumbent leader, whatever the government structure may be, is called the loyalty norm.\n\nA scenario where the winning coalition is large and the selectorate is small is logically impossible since the winning coalition is a subset of the selectorate.\n\nBruce Bueno de Mesquita and Alastair Smith further applied the selectorate theory to the field of foreign aid. The fundamental reason behind foreign aid practice, as the selectorate theory suggests, is to improve the survival of political leaders in both donor and recipient states. They argued that the size of leader's winning coalition and government revenues affect leader's decision making on policy concession and aid. By analyzing the bilateral aid transfers by Organization for Economic Cooperation and Development (OECD) nations between 1960 and 2001, they discovered that leaders in aid recipient countries are more likely to grant policy concession for donors when the winning coalition is small because leaders with small winning coalitions can easily reimburse supporters for their concession. As a result, relatively poor, small coalition systems are most likely to get aid. The conclusion of their study shows that interest exchange is the primary reason for foreign aid practice and OECD members have little humanitarian motivation for aid giving. Nancy Qian's study supported this conclusion by arguing that “The literature shows that the primary purpose of aid is often not to alleviate poverty and that out of all of the foreign aid flows, only 1.69% to 5.25% are given to the poorest twenty percent of countries in any given year\"\n"}
{"id": "8843067", "url": "https://en.wikipedia.org/wiki?curid=8843067", "title": "Silo (library)", "text": "Silo (library)\n\nSilo is a computer data format and library developed at Lawrence Livermore National Laboratory (LLNL) for storing rectilinear, curvilinear, unstructured, or point meshes in 2D and 3D. It supports data upon those meshes, including scalar, vector, and tensor variables; volume fraction-based materials; and mass fraction-based species. It fully supports block structured adaptive mesh refinement (AMR) meshes by way of mesh blocks structured in a hierarchy. Silo sits on top of other low-level storage libraries such as PDB, NetCDF, and HDF5.\n\nCurrently, VisIt, an open source software package with its start at LLNL, supports the Silo format for visualization and analysis, among many other formats.\n\nAs of Version 4.8, July, 2010, the Silo source code is now available\nunder the standard BSD Open Source License.\n\nThe source code for two compression libraries which have been part of\nprevious releases of the Silo library is not available under the\nterms of the BSD Open Source license. These are the Hzip and FPzip\ncompression libraries.\n\nFor this reason, two different releases of the Silo source code are\nmade available.\n"}
{"id": "3025570", "url": "https://en.wikipedia.org/wiki?curid=3025570", "title": "Stephan Angeloff", "text": "Stephan Angeloff\n\nStephan Angeloff () (1878–1964) was a Bulgarian microbiologist. He was a member of the Bulgarian Academy of Sciences and the Academy of Sciences Leopoldina, and served as rector of Sofia University from 1941 to 1942.\n\n"}
{"id": "20550200", "url": "https://en.wikipedia.org/wiki?curid=20550200", "title": "Stephen Wolff", "text": "Stephen Wolff\n\nStephen Wolff is one of the many fathers of the Internet. He is mainly credited with turning the Internet from a government project into something that proved to have scholarly and commercial interest for the rest of the world. Dr. Wolff realized before most the potential in the Internet and began selling the idea that the Internet could have a profound effect on both the commercial and academic world.\n\nStephen Wolff earned a BSc with Highest Honors in Electrical Engineering from Swarthmore College in 1957, and a Ph.D. in Electrical Engineering from Princeton University in 1961. In 1962 he continued his studies with post-doctoral work at Imperial College. He taught electrical engineering at the Johns Hopkins University for ten years.\n\nFor fourteen years, Wolff worked as a communications and technology researcher for the United States Army. While working for the Army, Wolff introduced the UNIX operating system to Army labs in the early 1980s. Also while working for the Army, Wolff managed a research group that participated in the development of ARPANET, a major technology precursor to the Internet.\n\nIn 1986, Wolff became Division Director for Networking and Communications Research and Infrastructure at the National Science Foundation where he managed the NSFNET project which included a national backbone network in the U.S. that interconnected NSF sponsored supercomputing centers, regional research and education networks, federal agency networks, and international research and education networks. The five super computing centers were located at Princeton, Cornell, the University of California at San Diego, the University of Illinois at Urbana-Champaign, and the University of Pittsburgh. Wolff also managed grants to link the nation's universities together into regional networks that connected to the backbone and so provided universal connectivity to the academic community. The NSFNET was compatible with, interconnected to, and eventually replaced the ARPANET network.\n\nWolff also conceived the Gigabit Testbed, a joint NSF-DARPA project designed to prove the feasibility of IP networking at gigabit speeds.\n\nIn 1994, Wolff left NSF and joined Cisco where he helped with projects such as Internet2 and the Abilene Network. Wolff's career at Cisco began as business development manager for the Academic Research and Technology Initiative program. There Wolff helped advance the University Research Project (URP) which supports academic research candidates with grants to further networking technology. He was named the interim Vice President and Chief Technology Officer of Internet2 on March 31, 2011.\n\nIn 2002 the Internet Society recognized Wolff with its Postel Award. When presenting the award, Internet Society (ISOC) President and CEO Lynn St.Amour said “…Steve helped transform the Internet from an activity that served the specific goals of the research community to a worldwide enterprise which has energized scholarship and commerce throughout the world.”\n\nThe Internet Society also recognized Wolff in 1994 for his courage and leadership in advancing the Internet.\n"}
{"id": "43619770", "url": "https://en.wikipedia.org/wiki?curid=43619770", "title": "The World in Six Songs", "text": "The World in Six Songs\n\nThe World in Six Songs: How the Musical Brain Created Human Nature is a popular science book written by the McGill University neuroscientist Daniel J. Levitin, and first published by Dutton Penguin in the U.S. and Canada in 2008, and updated and released in paperback by Plume in 2009, and translated into six languages. Levitin’s second \"New York Times\" bestseller, following the publication of \"This Is Your Brain on Music\", received praise from a wide variety of readers including Sir George Martin, Sting, Elizabeth Gilbert, and Adam Gopnik. The \"Los Angeles Times\" called it \"masterful\". The \"New York Times\" wrote: \"A lively, ambitious new book whose combined elements can induce feelings of enlightenment and euphoria. Will leave you awestruck.\" \"The Times\" wrote \"Levitin is such an enthusiastic anthropologist, such an exuberant song and dance man, such a natural-born associative thinker, that you gotta love the guy.\" It was named one of the best books of 2008 by the \"Boston Herald\" and by \"Seed Magazine\".\n\n\"The World in Six Songs\" combines science and art to reveal how music shaped humanity across cultures and throughout history. This book leans more heavily on anthropology and evolutionary biology than did \"This Is Your Brain On Music\", which skewed more toward findings in psychoacoustics and neuroscience.\n\nLevitin identifies six fundamental song functions or types (friendship, joy, comfort, religion, knowledge, and love) then shows how each in its own way has enabled the social bonding necessary for human culture and society to evolve. He shows, in effect, how these six song types function in our brains to preserve the emotional and literal history of our lives and species. Levitin illuminates, through songs, how music has been instrumental in the evolution of language, thought and culture. Musical examples ranging from Beethoven to The Beatles, Busta Rhymes to Bach, are used to support the book's propositions.\n\nUsing cutting-edge scientific research from his music cognition lab at McGill University; his own experiences in the music business; and interviews with musicians such as Sting and David Byrne, as well as conductors, anthropologists, and evolutionary biologists.\n"}
{"id": "170479", "url": "https://en.wikipedia.org/wiki?curid=170479", "title": "UFO conspiracy theory", "text": "UFO conspiracy theory\n\nUFO conspiracy theories argue that various governments, and politicians globally, most notably the officials of Washington, D.C., are suppressing evidence of extraterrestrial unidentified flying objects and alien visitors. Such conspiracy theories commonly argue that Earth governments, especially the Government of the United States, are in communication and/or cooperation with extraterrestrials despite public claims to the contrary, and further that some of these theories claim that the governments are explicitly allowing alien abduction.\n\nVarious UFO conspiracy ideas have flourished on the internet and were frequently featured on Art Bell's program, \"Coast to Coast AM\". According to MUFON, the National Enquirer reported that a survey found 76% of participants felt the government was not revealing all it knew about UFOs, 54% thought UFOs definitely or probably existed, and 32% thought UFOs came from outer space.\n\nNotable persons to have publicly stated that UFO evidence is being suppressed include Senator Barry Goldwater, British Admiral Lord Hill-Norton (former NATO head and chief of the British Defence Staff), Brigadier General Arthur Exon (former commanding officer of Wright-Patterson AFB), Vice Admiral Roscoe H. Hillenkoetter (first CIA director), astronauts Gordon Cooper and Edgar Mitchell, and former Canadian Defence Minister Paul Hellyer. Beyond their testimonies and reports they have presented no evidence to substantiate their statements and claims. According to the Committee for Skeptical Inquiry little or no evidence exists to support them despite significant research on the subject by non-governmental scientific agencies.\n\nOn the night before Halloween in 1938, Orson Welles directed \"The Mercury Theatre on the Air\" live radio adaptation of H. G. Wells's classic novel, \"The War of the Worlds\". By mimicking a news broadcast, the show was quite realistic sounding for its time, and some listeners were fooled into thinking that a Martian invasion was underway in the United States. Widespread confusion was followed by outrage and controversy. Some later studies have argued that the contemporary press exaggerated the extent of the panic, but it remains clear that many people were caught up, to some degree, in the confusion.\n\nIn other countries, reactions were similar. In 1949, part of the script for \"The War of the Worlds\" was read out over the radio in Quito, Ecuador without announcement, as if it were a major piece of breaking news. Huge crowds of people emerged onto the streets and sought refuge inside churches with their families. When the radio station was informed of this, its announcers broadcast the fact that no invasion was happening. An angry mob formed and burned the station to the ground, causing between six and twenty deaths. Many other countries also experienced problems when broadcasting \"The War of the Worlds\".\n\nAccording to U.S. Air Force Captain Edward J. Ruppelt, the Air Force's files often mentioned the panicked aftermath of the 1938 \"War of the Worlds\" broadcast as a possible reaction of the public to confirmed evidence of UFOs; however, the files have not been made available to corroborate his assertions.\n\nDonald Keyhoe later began investigating flying saucers for True Magazine. Keyhoe was one of the first significant conspiracy theorists, asserting eventually that the saucers were from outer space and were on some sort of scouting mission. Keyhoe claimed to derive his theory from his contacts in Air Force and Navy intelligence. Project Sign, based at Air Technical Intelligence Command at Wright-Patterson Air Force Base and its successors Project Grudge and Project Blue Book were officially assigned to investigate the flying saucers. Edward Ruppelt's book \"The Report on Unidentified Flying Objects\", reports that many people within these research groups did in fact support the hypothesis that the flying saucers were from outer space.\n\nKeyhoe later founded NICAP, a civilian investigation group that asserted the US government was lying about UFOs and covering up information that should be shared with the public. NICAP had many influential board members, including Roscoe H. Hillenkoetter, the first director of the CIA. To date no substantiating evidence for NICAP's assertions has been presented beyond accounts that are anecdotal and documented hear-say or rumor. \n\n\"The Great Los Angeles Air Raid\" also known as \"The Battle of Los Angeles\" is the name given by contemporary sources to the imaginary enemy attack and subsequent anti-aircraft artillery barrage which took place from late 24 February to early 25 February 1942 over Los Angeles, California.\n\nInitially, the target of the aerial barrage was thought to be an attacking force from Japan, but Secretary of the Navy Frank Knox speaking at a press conference shortly afterward called the incident a \"false alarm.\" A small number of modern-day UFOlogists have suggested the reported targets were extraterrestrial spacecraft.\n\nWhen documenting the incident in 1983, the U.S. Office of Air Force History attributed the event to a case of \"war nerves\" likely triggered by a lost weather balloon and exacerbated by stray flares and shell bursts from adjoining batteries.\n\nIn 1946 and 1947, numerous reports occurred of so-called ghost rockets appearing over Scandinavian countries, primarily Sweden, which then spread into other European countries. One USAF top secret document from 1948 stated that Swedish Air Force Intelligence informed them that some of their investigators felt that the reported objects were not only real but could not be explained as having earthly origins. Similarly, 20 years later, Greek physicist Dr. Paul Santorini publicly stated that in 1947 he was put in charge of a Greek military investigation into reports of ghost rockets sighted over Greece [Timothy Good 1988, p 23; Donald Keyhoe, p 142]. Again, they quickly concluded the objects were real and not of conventional origin. Santorini claimed their investigation was killed by U.S. scientists and high military officials who had already concluded the objects were extraterrestrial in origin and feared public panic because no defense existed.\n\nIn 1947, the United States Air Force issued a press release stating that a \"flying disk\" had been recovered near Roswell, New Mexico. This press release was quickly withdrawn, and officials stated that a weather balloon had been misidentified. The Roswell case quickly faded even from the attention of most UFOlogists until the 1970s. Speculation persisted despite the official denial that an alien spacecraft crashed near Roswell. For example, retired Brigadier General Arthur E. Exon, former commanding officer of Wright-Patterson AFB, told researchers Kevin D. Randle and Donald R. Schmitt that a spacecraft had crashed, alien bodies were recovered, and the event was covered up by the U.S. government. Exon further claimed he was aware of a very secretive UFO controlling committee made up primarily of very high-ranking military officers and intelligence people. His nickname for this group was \"The Unholy Thirteen\" (see also Majestic 12)\n\nIn the 1990s, the US military published two reports disclosing the true nature of the crashed aircraft: a surveillance balloon from Project Mogul. Nevertheless, the Roswell incident continues to be of interest in popular media, and conspiracy theories surrounding the event persist. Roswell has been described as \"the world's most famous, most exhaustively investigated and most thoroughly debunked UFO claim\".\n\nIn 1948, Air Force pilot Thomas Mantell was killed in a crash while pursuing what he described as \"a metallic object...of tremendous size\". Project Blue Book concluded that Mantell had lost control of his aircraft while chasing a then-classified Skyhook balloon. Some UFOlogists reject Bluebook's conclusion because of its initial suggestion that Mantell was chasing \"Venus or a comet\".\n\nThe U.S. Air Force may have planted the seeds of UFO conspiracy theories with Project Sign (established 1947) (which became Project Grudge and Project Blue Book). Edward J. Ruppelt, the first director of Blue Book, characterized the Air Force's public behavior regarding UFOs as \"schizophrenic\": alternately open and transparent, then secretive and dismissive. Ruppelt also revealed that in mid-1948, Project Sign issued a top secret Estimate of the Situation concluding that the flying saucers were not only real but probably extraterrestrial in origin. According to Ruppelt, the Estimate was ordered destroyed by Air Force Chief of Staff Hoyt Vandenberg.\n\nProject Sign's final report, published in early 1949, stated that while some UFOs appeared to represent actual aircraft data were insufficient to determine their origin.\n\nSome UFOlogists have claimed the existence of a U.S. government group called the \"Interplanetary Phenomenon Unit\" allegedly established by General Douglas MacArthur that was \"supposedly formed to investigate crashed and retrieved flying saucers\".\n\nThe 1950s saw an increase in both governmental and civilian investigative efforts and reports of public disinformation and suppression of evidence.\n\nThe UK Ministry of Defence’s UFO Project has its roots in a study commissioned in 1950 by the MOD’s then Chief Scientific Adviser, the great radar scientist Sir Henry Tizard. As a result of his insistence that UFO sightings should not be dismissed without some form of proper scientific study, the Department set up the Flying Saucer Working Party (or FSWP).\n\nIn August 1950, Montanan baseball manager Nicholas Mariana filmed several UFOs with his color 16mm camera. Project Blue Book was called in and, after inspecting the film, Mariana claimed it was returned to him with critical footage removed, clearly showing the objects as disc-shaped. The incident sparked nation-wide media attention.\n\nFrank Scully's 1950 \"Behind the Flying Saucers\" suggested that the U.S. government had recovered a crashed flying saucer and its dead occupants near Aztec, New Mexico, in 1948. It was later revealed that Scully had been the victim of a prank by \"two veteran confidence artists\".\n\nDonald Keyhoe was a retired U.S. Marine who wrote a series of popular books and magazine articles that were very influential in shaping public opinion, arguing that UFOs were indeed real and that the U.S. government was suppressing UFO evidence. Keyhoe's first article on the subject came out in True Magazine, January 1950, and was a national sensation. His first book, \"Flying Saucers Are Real\" also came out in 1950, about the same time as Frank Scully's book, and was a bestseller. In 1956, Keyhoe helped establish NICAP, a powerful civilian UFO investigating group with many inside sources. Keyhoe became its director and continued his attacks on the Air Force. Other contemporary critics also charged that the United States Air Force was perpetrating a cover-up with its Project Blue Book.\n\nCanadian radio engineer Wilbert B. Smith, who worked for the Canadian Department of Transport, was interested in flying saucer propulsion technology and wondered if the assertions in the just-published Scully and Keyhoe books were factual. In September 1950, he had the Canadian embassy in Washington D.C. arrange contact with U.S. officials to try to discover the truth of the matter. Smith was briefed by Dr. Robert Sarbacher, a physicist and consultant to the Defense Department's Research and Development Board. Other correspondence, having to do with Keyhoe needing to get clearance to publish another article on Smith's theories of UFO propulsion, indicated that Bush and his group were operating out of the Research and Development Board. Smith then briefed superiors in the Canadian government, leading to the establishment of Project Magnet, a small Canadian government UFO research effort. Canadian documents and Smith's private papers were uncovered in the late 1970s, and by 1984, other alleged documents emerged claiming the existence of a highly secret UFO oversight committee of scientists and military people called Majestic 12, again naming Vannevar Bush. Sarbacher was also interviewed in the 1980s and corroborated the information in Smith's memos and correspondence. Throughout the 1950s and early 1960s, Smith granted public interviews, and among other things stated that he had been lent crashed UFO material for analysis by a highly secret U.S. government group which he wouldn't name.\n\nA few weeks after the Robertson Panel, the Air Force issued Regulation 200-2, ordering air base officers to publicly discuss UFO incidents only if they were judged to have been solved, and to classify all the unsolved cases to keep them out of the public eye. In addition, UFO investigative duties started to be taken on by the newly formed 4602nd Air Intelligence Squadron (AISS) of the Air Defense Command. The 4602nd AISS was tasked with investigating only the most important UFO cases having intelligence or national security implications. These were deliberately siphoned away from Blue Book, leaving Blue Book to deal with the more trivial reports. (Dolan, 210-211)\n\nIn 1954 an automatic working station for UFO monitoring was installed at Shirley's Bay near Ottawa in Canada. After this station detected the first suspicious event, all data gained by this station was classified as secret, although the cameras of the monitoring station could not make any pictures because of fog.\n\n1956 saw the publication of Gray Barker's \"They Knew Too Much About Flying Saucers\", the book which publicized the idea of sinister Men in Black who appear to UFO witnesses and warn them to keep quiet. There has been continued speculation that the men in black are government agents who harass and threaten UFO witnesses.\n\nAlso in 1956, the group Foundation for Earth-Space Relations, led by film producer Tzadi Sophit, tested their own flying saucer outside the Long Island town of Ridge Landing. It is speculated in Robertson's \"The Long Island Saucer\" that an FBI cover-up silenced witnesses. \n\nOn January 22, 1958, when Donald Keyhoe appeared on CBS television, his statements on UFOs were precensored by the Air Force. During the show when Keyhoe tried to depart from the censored script to \"reveal something that has never been disclosed before\", CBS cut the sound, later stating Keyhoe was about to violate \"predetermined security standards\" and about to say something he wasn't \"authorized to release\". What Keyhoe was about to reveal were four publicly unknown military studies concluding UFOs were interplanetary including the 1948 Project Sign Estimate of the Situation and a 1952 Project Blue Book engineering analysis of UFO motion presented at the Robertson Panel. [Timothy Good, 286-287; Richard Dolan 293-295]\n\nAstronaut Gordon Cooper reported suppression of a flying saucer movie filmed in high clarity by two Edwards AFB range photographers on May 3, 1957. Cooper said he viewed developed negatives of the object, clearly showing a dish-like object with a dome on top and something like holes or ports in the dome. When later interviewed by James McDonald, the photographers and another witness confirmed the story. Cooper said military authorities then picked up the film and neither he nor the photographers ever heard what happened to it. The incident was also reported in a few newspapers, such as the Los Angeles Times. The official explanation was that the photographers had filmed a weather balloon distorted by hot desert air.\n\nThroughout much of the 1960s, atmospheric physicist James E. McDonald suggested—via lectures, articles and letters—that the U.S. Government was mishandling evidence which would support the extraterrestrial hypothesis.\n\nClark notes that many UFO conspiracy theory tales \"can be traced to a mock documentary, Alternative 3, broadcast on British television on June 20, 1977, and subsequently turned into a paperback book.\" (Clark, 213–4)\n\nClark cites a 1973 encounter as perhaps the earliest suggestion that the U.S. government was involved with ETs. That year, Robert Emenegger and Allan Sandler of Los Angeles, California were in contact with officials at Norton Air Force Base in order to make a documentary film. Emenegger and Sandler report that Air Force Officials (including Paul Shartle) suggested incorporating UFO information in the documentary, including as its centerpiece genuine footage of a 1971 UFO landing at Holloman Air Force Base in New Mexico. Furthermore, says Emenegger, he was given a tour of Holloman AFB and was shown where officials conferred with Extraterrestrial Biological Entities (EBEs). This was supposedly not the first time the U.S. had met these aliens, as Emenegger reported that his U.S. military sources had \"been monitoring signals from an alien group with which they were unfamiliar, and did their ET guests know anything about them? The ETs said no\" (Clark 1998, 144). The documentary was released in 1974 as \"UFO's: Past, Present and Future\" (narrated by Rod Serling) containing only a few seconds of the Holloman UFO footage, the remainder of the landing depicted with illustrations and re-enactments.\n\nIn 1988, Shartle said that the film in question was genuine, and that he had seen it several times.\n\nIn 1976 a televised documentary report \"UFOS: It Has Begun\" written by Robert Emenegger was presented by Rod Serling, Burgess Meredith and José Ferrer. Some sequences were recreated based upon the statements of eyewitness observers, together with the findings and conclusions of governmental civil and military investigations. The documentary uses a hypothetical UFO landing at Holloman AFB as a backdrop.\n\nThe late 1970s also saw the beginning of controversy centered on Paul Bennewitz of Albuquerque, New Mexico.\n\nThe so-called Majestic 12 documents surfaced in 1982, suggesting that there was secret, high-level U.S. government interest in UFOs dating to the 1940s.\n\nLinda Moulton Howe is an advocate of conspiracy theories that cattle mutilations are of extraterrestrial origin and speculations that the U.S. government is involved with aliens.\n\nIn the 1980s, Milton William Cooper achieved a degree of prominence due to his conspiratorial writings.\n\nIn November 1989, Bob Lazar appeared in a special interview with investigative reporter George Knapp on Las Vegas TV station KLAS to discuss his alleged employment at S-4. In his interview with Knapp, Lazar said he first thought the saucers were secret, terrestrial aircraft, whose test flights must have been responsible for many UFO reports. Gradually, on closer examination and from having been shown multiple briefing documents, Lazar came to the conclusion that the discs must have been of extraterrestrial origin. He claims that they use moscovium, an element that decays in a fraction of a second, to warp space, and that he has since met Grey aliens from the \"Zeta Reticuli\" star system. According to the \"Los Angeles Times,\" he never obtained the degrees he claims to hold from MIT and Caltech.\n\nOn October 14, 1988, actor Mike Farrell hosted \"U.S. UFO Cover-Up: Live!\", a two-hour television special \"focusing on the government's handling of information regarding UFOs\" and \"whether there has been any suppression of evidence supporting the existence of UFOs\".\n\nThe Mutual UFO Network held their 1989 annual convention in Las Vegas, Nevada, on July 1, 1989.\n\nBill Moore (ufologist) was scheduled as the main speaker, and he refused to submit his paper for review prior to the convention, and also announced that he would not answer any follow-up questions as was common practice. Unlike most of the convention's attendees, Moore did not stay at the same hotel that was hosting the convention.\n\nWhen he spoke, Moore said that he and others had been part of an elaborate, long-term disinformation campaign begun primarily to discredit Paul Bennewitz: \"My role in the affair ... was primarily that of a freelancer providing information on Paul's (Bennewitz) current thinking and activities\" (Clark, 1998, 163). Air Force Sergeant Richard C. Doty was also involved, said Moore, though Moore thought Doty was \"simply a pawn in a much larger game, as was I.\" (ibid.) One of their goals, Moore said, was to disseminate information and watch as it was passed from person to person in order to study information channels.\n\nMoore said that he \"was in a rather unique position\" in the disinformation campaign: \"judging by the positions of the people I knew to be directly involved in it, [the disinformation] definitely had something to do with national security. There was no way I was going to allow the opportunity to pass me by ... I would play the disinformation game, get my hands dirty just often enough to lead those directing the process into believing I was doing what they wanted me to do, and all the while continuing to burrow my way into the matrix so as to learn as much as possible about who was directing it and why.\"(ibid., 164)\n\nOnce he finished the speech, Moore immediately left the hotel. He left Las Vegas that same night.\n\nMoore's claims sent shock waves through the small, tight-knit UFO community, which remains divided as to the reliability of his assertions.\n\nOn November 24, 1992, a UFO reportedly crashed in Southaven Park, Shirley, New York. John Ford, a Long Island MUFON researcher, investigated the crash. Four years later, on June 12, 1996, Ford was arrested and charged with plotting to poison several local politicians by sneaking radium in their toothpaste. On advice of counsel Ford pleaded insanity and was committed to the Mid Hudson Psychiatric Center. Critics say the charges are a frame-up.\n\nThe Branton Files have circulated on the internet at least since the mid-1990s. They essentially recirculate the information presented above, with many asides from \"Branton\", the document's editor.\n\nPhilip Schneider of the patriot movement, an engineer and geologist formerly working for the U.S. government, made a few appearances at UFO conventions in the 1990s, espousing essentially a new version of the theories mentioned above. He claimed to have played a role in the construction of Deep Underground Military Bases (DUMBs) across the United States, and as a result he said that he had been exposed to classified information of various sorts as well as having personal experiences with EBEs. He claimed to have survived the Dulce Base catastrophe and decided to tell his tale. He died by suicide on January 17, 1996, after a series of lectures given in late 1995 on topics including the Black Budget and underground alien bases. Others believe that Schneider did not take his own life and that he was actually murdered by the government. \n\nIn 1999 a group in France published a study, \"UFOs and Defense: What Must We Be Prepared For?\" Among other topics, the study concluded that the United States government has withheld valuable evidence.\n\n2003 saw the publication of \"Alien Encounters\" (), by Chuck Missler and Mark Eastman, which primarily re-stated the notions presented above (especially Cooper's) and presents them as fact.\n\nEight files from 1978 to 1987 on UFO sightings were first released on May 14, 2008, to the National Archives' website by the British Ministry of Defence. 200 files were set to be made public by 2012. The files are correspondence from the public sent to government officials, such as the MoD and Margaret Thatcher. The information can be downloaded. Copies of Lt. Col. Halt's letter regarding the sighting at RAF Woodbridge (see above) to the U.K. Ministry of Defense were routinely released (without addition comment) by the USA's base public affairs staff throughout the 1980s until the base closed. The MoD released the files due to requests under the Freedom of Information Act. The files included, among other things, alien craft flying over Liverpool and Waterloo Bridge in London.\n\nIn the early 2000s, the concept of \"disclosure\" became increasingly popular in the UFO conspiracy community: that the government had suppressed information on alien contact and full disclosure was needed, and was pursued by activist lobbying groups.\n\nIn 1993, Steven M. Greer founded the Disclosure Project to promote the concept. In May 2001, Greer held a press conference at the National Press Club in D.C that demanded Congress hold hearings on \"secret U.S. involvement with UFOs and extraterrestrials\". It was described by an attending BBC reporter as \"the strangest ever news conference hosted by Washington's August National Press Club.\" The Disclosure Project's claims were met with by derision by skeptics and spokespeople for the U. S. Air Force.\n\nIn 2013, the production company CHD2, LLC held a \"Citizen Hearing on Disclosure\" at the National Press Club in D.C from 29 April to 3 May 2013. The group paid former U.S. Senator Mike Gravel and former Representatives Carolyn Cheeks Kilpatrick, Roscoe Bartlett, Merrill Cook, Darlene Hooley, and Lynn Woolsey $20,000 each to participate, and to preside over panels of academics and former government and military officials discussing UFOs and extraterrestrials.\n\nOther such groups include Citizens Against UFO Secrecy, founded in 1977.\n\nAllegations of suppression of UFO related evidence have persisted for many decades. Some conspiracy theories also claim that some governments might have removed and/or destroyed/suppressed physical evidence; some examples follow.\n\nOn July 7, 1947, William Rhodes photographed an unusual object over Phoenix, Arizona. The photos appeared in a Phoenix newspaper and a few other papers. An Army Air Force intelligence officer and an FBI agent interviewed Rhodes on August 29 and convinced him to surrender the negatives, which he did the next day. He was informed he wouldn't be getting them back, but later he tried, unsuccessfully, to retrieve them. The photos were analyzed and subsequently appeared in some classified Air Force UFO intelligence reports. (Randle, 34–45, full account)\n\nA June 27, 1950, movie of a \"flying disk\" over Louisville, Kentucky, taken by a Louisville \"Courier-Journal\" photographer, had the USAF Directors of counterintelligence (AFOSI) and intelligence discussing in memos how to best obtain the movie and interview the photographer without revealing Air Force interest. One memo suggested the FBI be used, then precluded the FBI getting involved. Another memo said \"it would be nice if OSI could arrange to secure a copy of the film in some covert manner,\" but if that wasn't feasible, one of the Air Force scientists might have to negotiate directly with the newspaper. In a recent interview, the photographer confirmed meeting with military intelligence and still having the film in his possession until then, but refused to say what happened to the film after that.\n\nIn another 1950 movie incident from Montana, Nicholas Mariana filmed some unusual aerial objects and eventually turned the film over to the U.S. Air Force, but insisted that the first part of the film, clearly showing the objects as spinning discs, had been removed when it was returned to him. (Clark, 398)\n\nAccording to some conspiracy theorists, during the military investigation of green fireballs in New Mexico, UFOs were photographed by a tracking camera over White Sands Proving Grounds on April 27, 1949. They claim that the final report in 1951 on the green fireball investigation claimed there was insufficient data to determine anything. Conspiracy theorists claim that documents later uncovered by Dr. Bruce Maccabee indicate that triangulation was accomplished. The conspiracy theorists also claim that the data reduction and photographs showed four objects about 30 feet in diameter flying in formation at high speed at an altitude of about 30 miles. According to conspiracy theorists, Maccabee says this result was apparently suppressed from the final report.\n\nOn January 22, 1958, when NICAP director Donald Keyhoe appeared on CBS television, his statements on UFOs were pre-censored by the Air Force. During the show when Keyhoe tried to depart from the censored script to \"reveal something that has never been disclosed before,\" CBS cut the sound, later stating Keyhoe was about to violate \"predetermined security standards\" and about to say something he wasn't \"authorized to release.\" Conspiracy theorists claim that what Keyhoe was about to reveal were four publicly unknown military studies concluding UFOs were interplanetary (including the 1948 Project Sign Estimate of the Situation and Blue Book's 1952 engineering analysis of UFO motion). (Good, 286–287; Dolan 293–295)\n\nA March 1, 1967 memo directed to all USAF divisions, from USAF Lt. General Hewitt Wheless, Assistant Vice Chief of Staff, stated that unverified information indicated that unknown individuals, impersonating USAF officers and other military personnel, had been harassing civilian UFO witnesses, warning them not to talk, and also confiscating film, referring specifically to the Heflin incident. AFOSI was to be notified if any personnel were to become aware of any other incidents. (Document in Fawcett & Greenwood, 236.)\n\nJohn Callahan, former Division Chief of the Accidents and Investigations Branch of the FAA, Washington D.C., also a Disclosure Project witness, said that following the Japan Air Lines flight 1628 incident that involved a giant UFO over Alaska, recorded by air and ground radar, the FAA conducted an investigation. Callahan held a briefing a few days later for President Reagan's Scientific Study Group, the FBI, and CIA. After the briefing, one of the CIA agents told everybody they \"were never there and this never happened,\" adding they were fearful of public panic.\n\nAccording to one theory related to the assassination of President John F. Kennedy, the CIA killed Kennedy in order to prevent him from leaking information to the Soviet Union about a covert program to reverse-engineer alien technology (i.e., Majestic 12).\n\nNick Cook, an aviation investigative journalist for \"Jane's Information Group\" and researcher of \"Billion Dollar Secret\" and author of \"The Hunt for Zero Point\" claims to have uncovered documentary evidence that top-secret US Defense Industry technology has been developed by government-backed Defense Industry programs, beginning in the 1940s using research conducted by Nazi scientists during WWII and recovered by Allied Military Intelligence, then taken to the U.S. and developed further with the collaboration of the same former German scientists at top-secret facilities established at White Sands, New Mexico, and later at Area 51, allegedly resulting in production of real-world prototype operational supersonic craft actually tested and used in clandestine military exercises, with other developments incorporated later into spy aircraft tasked with overflying hostile countries: the UFO story that evidence of alien technology is being suppressed and removed or destroyed was generated and then promoted by the CIA, beginning 1947, as false-lead disinformation to cover it all up for the sake of National Security, particularly during the Cold War, at a time when (his investigations found) the Soviet Union too was developing its own top-secret high-tech UFO craft. Cook's conclusions, alleging suppression of evidence of advanced \"human\" technology instead of alien, together with what he presents as declassified top-secret documents and blueprints, and his interviews of various experts (some of doubtful reliability), was developed and broadcast as a feature documentary on British television in 2005 as \"UFOs: The Secret Evidence\" and in the US in 2006 as a two-part episode on the History Channel's UFO Files, retitled \"An Alien History of Planet Earth\", with an added introduction by actor William Shatner. The \"History Channel\" program teaser promised \"...a look at rumors of classified military aircraft incorporating alien technology into their designs.\"\n\nIn 1993, Steven M. Greer founded the Disclosure project to promote the concept of disclosing allegedly suppressed evidence of extraterrestrials. In May 2001, Greer held a press conference at the National Press Club in D.C that featured \"20 retired Air Force, Federal Aviation Administration and intelligence officers\" who demanded that Congress begin hearings on \"secret U.S. involvement with UFOs and extraterrestrials\"\n\nIn 2013, Sen. Mike Gravel claimed that the government was suppressing evidence of extraterrestrials.\n\nBenjamin Radford has pointed out how unlikely such suppression of evidence is given that \"[t]he UFO coverup conspiracy would have to span decades, cross international borders, and transcend political administrations\" and that \"all of the world's governments, in perpetuity, regardless of which political party is in power and even among enemies, [would] have colluded to continue the coverup.\"\n\nIn fiction, television programs (\"The X-Files\" and \"Stargate\"), films (\"Men in Black\" and \"Independence Day\") and any number of novels have featured elements of UFO conspiracy theories. Fictionalized elements may include the government's sinister operatives from \"Men in Black\", the military bases known as Area 51, RAF Rudloe Manor or Porton Down, a rumored crash site in Roswell, New Mexico, the Rendlesham Forest Incident, a political committee dubbed \"Majestic 12\", or the successor of the UK Ministry of Defence's Flying Saucer Working Party (FSWP). The novel \"The Doomsday Conspiracy\" by Sidney Sheldon includes a UFO conspiracy in its plot.\n\n\n"}
{"id": "31661219", "url": "https://en.wikipedia.org/wiki?curid=31661219", "title": "William Lincoln Bakewell", "text": "William Lincoln Bakewell\n\nWilliam Lincoln Bakewell (November 26, 1888 – May 21, 1969) was the only American aboard the \"Endurance\" during the 1914 to 1916 Imperial Trans-Antarctic Expedition with Sir Ernest Shackleton. William Bakewell joined the \"Endurance\" crew in Buenos Aires, Argentina along with friend Perce Blackborow. Bakewell was hired on as an Able Seaman. Bakewell's adventures, including his time on board the \"Endurance\", are documented in his own words in his memoir \"The American on the Endurance\".\n\nHe was born on November 26, 1888 in Joliet, Illinois. From 1914 to 1916 he participated in the Imperial Trans-Antarctic Expedition. In 1923 he returned to Joliet and worked at Elgin, Joliet and Eastern Railway. He next was a towerman for the Rock Island Railroad. He married Merle in 1925 and they had a daughter, Elizabeth Bakewell. During World War II he worked at the Diesel Electric Plant in La Grange, Illinois. In August 1945 he bought a farm in Michigan.\n\nHe died on May 21, 1969 in Dukes, Michigan. He was buried in Immanuel Lutheran Church Cemetery in Skandia, Michigan.\n\n"}
{"id": "51067177", "url": "https://en.wikipedia.org/wiki?curid=51067177", "title": "Wolfgang Karl Weyrauch", "text": "Wolfgang Karl Weyrauch\n\nWolfgang Karl Weyrauch (1907 - 1970) was a German - Peruvian malacologist and entomologist.\n\nWolfgang Karl Weyrauch was born on December 7, 1970, in Elberfeld, Germany. He received his PhD in Zoology in 1929 from the University of Berlin with a thesis on insect neurophysiology. From 1928 to 1929, he was an assistant of Richard Hesse, and from 1931 to 1943 he worked for the German Council of Scientific Research doing field studied on entomology and ecology.\n\nIn 1938, he worked as an entomologist at the agricultural experimental station (Estación Agrícola de La Molina) in Lima. At the time of the World War II, he moved to Texas, where he did field work in entomology and malacology. In 1946, he was at the Estación experimental Agrícola de Tingo María in Lima. From 1948 on, he worked for the Universidad Mayor de San Marcos in Lima as a Professor of zoology and Genetics at the Museo Nacional de Historia. In addition, he was from 1959 to 1961 Professor of agricultural zoology at the Pontificia Universidad Católica del Lima. In 1962, he went to Argentina and became professor at the Instituto Miguel Lillo in Tucumán. He died of a heart attack in 1970.\n\nWeyrauch studied land and freshwater gastropods of South America, mainly taxa belonging to the families Camaenidae, Charopidae, Clausiliidae, Endodontidae, Helicinidae, “Hydrobiidae”, Orthalicidae, Pupillidae, Scolodontidae, Subulinidae, and Urocoptidae. He left behind many type specimens in museums, of which he published no original description. He was the author of 198 molluscan names.\n\nThe following gastropod species were named after Weyrauch:\n\nAlso, a species of snake is named after Weyrauch:\n\n"}
{"id": "46981402", "url": "https://en.wikipedia.org/wiki?curid=46981402", "title": "World Solar Challenge 2009", "text": "World Solar Challenge 2009\n\nThe 2009 World Solar Challenge was one of a biennial series of solar-powered car races, covering through the Australian Outback, from Darwin, Northern Territory to Adelaide, South Australia.\n\nIn the Challenge class 24 teams started, of which eight completed the course, and the winner was Tokai University of Japan. In the Adventure class seven teams started and two completed the course, the winner being Osaka Sangyo University also of Japan.\n\nNote\n\n\n"}
{"id": "50558", "url": "https://en.wikipedia.org/wiki?curid=50558", "title": "Zooplankton", "text": "Zooplankton\n\nZooplankton (, ) are heterotrophic (sometimes detritivorous) plankton (cf. phytoplankton). Plankton are organisms drifting in oceans, seas, and bodies of fresh water. The word \"zooplankton\" is derived from the Greek \"zoon\" (), meaning \"animal\", and \"\" (), meaning \"wanderer\" or \"drifter\". Individual zooplankton are usually microscopic, but some (such as jellyfish) are larger and visible to the naked eye.\n\nZooplankton is a categorization spanning a range of organism sizes including small protozoans and large metazoans. It includes holoplanktonic organisms whose complete life cycle lies within the plankton, as well as meroplanktonic organisms that spend part of their lives in the plankton before graduating to either the nekton or a sessile, benthic existence. Although zooplankton are primarily transported by ambient water currents, many have locomotion, used to avoid predators (as in diel vertical migration) or to increase prey encounter rate.\n\nEcologically important protozoan zooplankton groups include the foraminiferans, radiolarians and dinoflagellates (the last of these are often mixotrophic). Important metazoan zooplankton include cnidarians such as jellyfish and the Portuguese Man o' War; crustaceans such as copepods, ostracods, isopods, amphipods, mysids and krill; chaetognaths (arrow worms); molluscs such as pteropods; and chordates such as salps and juvenile fish. This wide phylogenetic range includes a similarly wide range in feeding behavior: filter feeding, predation and symbiosis with autotrophic phytoplankton as seen in corals. Zooplankton feed on bacterioplankton, phytoplankton, other zooplankton (sometimes cannibalistically), detritus (or marine snow) and even nektonic organisms. As a result, zooplankton are primarily found in surface waters where food resources (phytoplankton or other zooplankton) are abundant.\n\nJust as any species can be limited within a geographical region, so are zooplankton. However, species of zooplankton are not dispersed uniformly or randomly within a region of the ocean. As with phytoplankton, ‘patches’ of zooplankton species exist throughout the ocean. Though few physical barriers exist above the mesopelagic, specific species of zooplankton are strictly restricted by salinity and temperature gradients; while other species can withstand wide temperature and salinity gradients. Zooplankton patchiness can also be influenced by biological factors, as well as other physical factors. Biological factors include breeding, predation, concentration of phytoplankton, and vertical migration. The physical factor that influences zooplankton distribution the most is mixing of the water column (upwelling and downwelling along the coast and in the open ocean) that affects nutrient availability and, in turn, phytoplankton production.\n\nThrough their consumption and processing of phytoplankton and other food sources, zooplankton play a role in aquatic food webs, as a resource for consumers on higher trophic levels (including fish), and as a conduit for packaging the organic material in the biological pump. Since they are typically small, zooplankton can respond rapidly to increases in phytoplankton abundance, for instance, during the spring bloom.\n\nZooplankton can also act as a disease reservoir. Crustacean zooplankton have been found to house the bacterium \"Vibrio cholerae\", which causes cholera, by allowing the cholera vibrios to attach to their chitinous exoskeletons. This symbiotic relationship enhances the bacterium's ability to survive in an aquatic environment, as the exoskeleton provides the bacterium with carbon and nitrogen.\n\n\n"}
