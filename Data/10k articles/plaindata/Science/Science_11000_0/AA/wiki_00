{"id": "19111605", "url": "https://en.wikipedia.org/wiki?curid=19111605", "title": "1% rule (Internet culture)", "text": "1% rule (Internet culture)\n\nIn Internet culture, the 1% rule is a rule of thumb pertaining to participation in an internet community, stating that only 1% of the users of a website actively create new content, while the other 99% of the participants only lurk. Variants include the \"1–9–90 rule\" (sometimes \"90–9–1 principle\" or the \"89:10:1 ratio\"), which states that in a collaborative website such as a wiki, 90% of the participants of a community only view content, 9% of the participants edit content, and 1% of the participants actively create new content.\n\nSimilar rules are known in information science, such as the 80/20 rule known as the Pareto principle, that 20 percent of a group will produce 80 percent of the activity, however the activity may be defined.\n\nThe 1% rule states that the number of people who create content on the Internet represents approximately 1% of the people who view that content. For example, for every person who posts on a forum, generally about 99 other people view that forum but do not post. The term was coined by authors and bloggers Ben McConnell and Jackie Huba, although earlier references to the same concept did not use this name.\n\nThe terms \"lurk\" and \"lurking\", in reference to online activity, are used to refer to online observation without engaging others in the community, and were first used by veteran print journalist, P. Tomi Austin, circa 1990, when her presence was noticed by other users in chat rooms, who queried her reasons for not engaging in chat. There were repeated inquiries about her identity and her refusal to engage in chat. The etiquette was, apparently, to greet other users upon entry into the chat rooms/sites. At the time, (then in her 30s, surfing among users averaging in their teens and 20s) she was only identified as \"Bilbo\", she explained that she was a mature, but computer-literate, user and novice to chat, and preferred to \"lurk\", or was \"lurking\" to familiarize herself with the chat culture, etiquette, and the sites to which she had logged on. In some instances, she needed to explain her coinage of the term \"lurking\", as the term was new to the online community, but others quickly understood her meaning. To her knowledge, the terms had not been used prior to that period, and there appears to be no earlier dated reference to the coinage.\n\nA 2005 study of radical Jihadist forums found 87% of users had never posted on the forums, 13% had posted at least once, 5% had posted 50 or more times, and only 1% had posted 500 or more times. \n\nA 2014 peer-reviewed paper entitled \"The 1% Rule in Four Digital Health Social Networks: An Observational Study\" empirically examined the 1% rule in health oriented online forums. The paper concluded that the 1% rule was consistent across the four support groups, with a handful of \"Superusers\" generating the vast majority of content. A study later that year, from a separate group of researchers, replicated the 2014 van Mierlo study in an online forum for depression. Results indicated that the distribution frequency of the 1% rule fit followed Zipf's Law, which is a specific type of a power law. \n\nThe \"90–9–1\" version of this rule states that for websites where users can both create and edit content, 1% of people create content, 9% edit or modify that content, and 90% view the content without contributing.\n\nThe actual percentage is likely to vary depending upon the subject matter. For example, if a forum requires content submissions as a condition of entry, the percentage of people who participate will probably be significantly higher than one percent, but the content producers will still be a minority of users. This is validated in a study conducted by Michael Wu, who uses economics techniques to analyze the participation inequality across hundreds of communities segmented by industry, audience type, and community focus.\n\nThe 1% rule is often misunderstood to apply to the Internet in general, but it applies more specifically to any given Internet community. It is for this reason that one can see evidence for the 1% principle on many websites, but aggregated together one can see a different distribution. This latter distribution is still unknown and likely to shift, but various researchers and pundits have speculated on how to characterize the sum total of participation. Research in late 2012 suggested that only 23% of the population (rather than 90 percent) could properly be classified as lurkers, while 17% of the population could be classified as intense contributors of content. Several years prior, results were reported on a sample of students from Chicago where 60 percent of the sample created content in some form.\n\nA similar concept was introduced by Will Hill of AT&T Laboratories and later cited by Jakob Nielsen; this was the earliest known reference to the term \"participation inequality\" in an online context. The term regained public attention in 2006 when it was used in a strictly quantitative context within a blog entry on the topic of marketing.\n\n\n"}
{"id": "40705930", "url": "https://en.wikipedia.org/wiki?curid=40705930", "title": "Aid on the Edge of Chaos", "text": "Aid on the Edge of Chaos\n\nAid on the Edge of Chaos is a 2013 book on applying cutting-edge science and innovation to international development, published by Oxford University Press. Written by global development and humanitarian expert Ben Ramalingam, it focuses on the need to improve foreign aid and the value of complex systems science and research for how global international aid efforts should be designed, implemented and evaluated.\n\nDescribed in a leading development journal as 'one of the most lauded contributions to recent mainstream development thinking', \"Aid on the Edge of Chaos\" has been endorsed by many top scientists and international leaders, including four Nobel Laureates in Medicine, Economics and Chemistry and the heads of Red Cross and United Nations as well as many NGO leaders. It has been positively reviewed by various press outlets, including \"The Economist\", the \"Financial Times\", \"The Guardian\", \"New Scientist\", \"Nature\", Lancet, Harvard Business Review and the British Medical Journal.\n\nIt was also the focus of an interview feature with the author in \"Huffington Post\".\n\"Aid on the Edge of Chaos\" was the subject of a public lecture by Ben Ramalingam at the Royal Society of Arts, London, in December 2013, an event chaired by Geoff Mulgan, CEO of NESTA. The book was discussed by Ramalingam and Sir John Holmes at the Oxford Literary Festival in March 2014, an event chaired by leading British filmmaker and author Bidisha.\n\nMany international aid agencies are applying ideas from the book in their work, including the UK Department for International Development, United States Agency for International Development, the International Rescue Committee, Mercy Corps, UNICEF, World Food Programme, World Vision, the World Bank, the United Nations and Oxfam.\n"}
{"id": "57232345", "url": "https://en.wikipedia.org/wiki?curid=57232345", "title": "Arcus (satellite)", "text": "Arcus (satellite)\n\nArcus is a proposed X-ray space observatory proposed to NASA's Explorer program, Medium Explorer (MIDEX) class. \n\nThe Arcus mission would study galaxies and galaxy clusters using high-resolution X-ray spectroscopy to characterize the interactions between these objects and the diffuse hot gas that permeates them. The Principal investigator is Randall Smith at the Smithsonian Astrophysical Observatory in Cambridge, Massachusetts; the project has significant contributions from the Massachusetts Institute of Technology, cosine Measurement Systems (Warmond NL), PennState, and the Max Planck Institute for Extraterrestrial Physics.\n\nArcus's spectrograph was originally intended to fly on the cancelled International X-ray Observatory (IXO). After the 2011 cancellation, Arcus was proposed as a mission in 2014 to the Small Explorer program (SMEX) but it was not selected for development. After numerous technological advances later, Arcus was again proposed to NASA in 2016 to the Medium Exproler program (MIDEX) and was awarded $2 million to refine their mission concept (Phase A study) over nine-months. If selected, it would be funded $250 million for development.\n\nArcus is an X-ray grating spectrometer space observatory that combines X-ray optics and gratings to disperse the X-rays, much like how a prism separates sunlight into the colors of the rainbow. It would observe astrophysical phenomena in X-ray band over a broad target size range. Its mission includes investigations on the composition of cosmic dust grains, stellar evolution, identify the launching mechanisms of supermassive black hole winds, and structure formation of galaxy clusters. Using new technologies, it offers a resolution improvement of ten times better than existing X-ray observatories.\n\nThe space observatory would orbit Earth in a 4:1 lunar resonance, which would allow significant stability for minimal propellant consumption, which could extend the mission life to about 10 years.\n\nIt is competing for selection for the next Medium-Class Explorers mission, which will take place in 2019, between Arcus and SPHEREx. If selected in 2019 and built, it would launch in 2023.\n"}
{"id": "20651372", "url": "https://en.wikipedia.org/wiki?curid=20651372", "title": "Autoguider", "text": "Autoguider\n\nAn autoguider is an automatic electronic guidance tool used in astronomy to keep a telescope pointed precisely at an object being observed. This prevents the object from drifting across the field of view during long-exposures which would create a blurred or elongated image.\n\nImaging of dim celestial targets, usually deep sky objects, requires exposure times of many minutes, particularly when narrowband images are being taken. In order for the resulting image to maintain usable clarity and sharpness during these exposures, the target must be held at the same position within the telescope's field of view during the whole exposure; any apparent motion would cause point sources of light (such as stars) to appear as streaks, or the object being photographed to appear blurry. Even computer-tracked mounts and GoTo telescopes do not eliminate the need for tracking adjustments for exposures beyond a few minutes, as astrophotography demands an extremely high level of precision that these devices typically cannot achieve, especially if the mount is not properly polar aligned.\n\nTo accomplish this automatically an autoguider is usually attached to either a guidescope or finderscope, which is a smaller telescope oriented in the same direction as the main telescope, or an off-axis guider, which uses a prism to divert some of the light originally headed towards the eyepiece.\n\nThe device has a CCD or CMOS sensor that regularly takes short exposures of an area of sky near the object. After each image is captured, a computer measures the apparent motion of one or more stars within the imaged area and issues the appropriate corrections to the telescope's computerized mount.\n\nSome computer controlled telescope mounts have an autoguiding port that connects directly to the autoguider (usually referred to as an ST-4 port, which works with analog signals).\n\nAn autoguider need not be an independent unit; some high-end CCD imaging units (such as those offered by SBIG) have a second, integrated CCD sensor on the same plane as the main imaging chip that is dedicated to autoguiding. Astronomical video cameras or modified webcams can also serve as an autoguiding unit when used with guiding software such as Guidedog or PHD Guiding, or general-purpose astronomical programs such as MaxDSLR. However, these setups are generally not as sensitive as specialized units.\n\nSince an image of a star can take up more than one pixel on an image sensor due to lens imperfections and other effects, autoguiders use the amount of light falling on each pixel to calculate where the star should actually be located. As a result, most autoguiders have \"subpixel accuracy\". In other words, the star can be tracked to an accuracy better than the angular size represented by one CCD pixel. However, atmospheric effects (astronomical seeing) typically limit accuracy to one arcsecond in most situations. To prevent the telescope from moving in response to changes in the guide star's apparent position caused by seeing, the user can usually adjust a setting called \"aggressiveness\".\n\n"}
{"id": "532208", "url": "https://en.wikipedia.org/wiki?curid=532208", "title": "Battelle Memorial Institute", "text": "Battelle Memorial Institute\n\nBattelle Memorial Institute (more widely known as simply Battelle) is a private nonprofit applied science and technology development company headquartered in Columbus, Ohio. Battelle is a charitable trust organized as a nonprofit corporation under the laws of the State of Ohio and is exempt from taxation under Section 501(c)(3) of the Internal Revenue Code because it is organized for charitable, scientific and education purposes. The institute opened in 1929 but traces its origins to the 1923 will of Ohio industrialist Gordon Battelle which provided for its creation. Originally focusing on contract research and development work in the areas of metals and material science, Battelle is now an international science and technology enterprise that explores emerging areas of science, develops and commercializes technology, and manages laboratories for customers.\n\nBattelle serves the following: \n\nIn addition to its Columbus (Ohio) headquarters, Battelle has offices in Aberdeen (Maryland), West Jefferson (Ohio), Dublin (Ohio), Seattle (Washington), Sequim (Washington), Arlington (Virginia), Duxbury (Massachusetts), Charlottesville (Virginia), Houston (Texas), Anchorage (Alaska) and Egg Harbor Township (New Jersey).\n\nIn addition to operating its own research facilities, as of 2013, Battelle manages or co-manages on behalf of the United States Department of Energy the following national laboratories:\n\nAdditionally, on behalf of the Department of Homeland Security:\n\nNational Science Foundation projects:\n\n\nIn the 1940s, Battelle's Vice-President of Engineering, John Crout made it possible for Battelle researchers, including William Bixby and Paul Andrus, to develop Chester Carlson's concept of dry copying. Carlson had been turned down for funding by more than a dozen agencies including the U.S. Navy. Work led to the first commercial xerographic equipment, and to the formation of Xerox corporation.\n\nBattelle also developed the first nuclear fuel rods for nuclear reactors, numerous advances in metallurgy that helped advance the United States space program, algorithms and coatings that led to the first optical digital recorder developed by James Russell, which paved the way for the first compact disc, and the first generation jet engines using titanium alloys.\n\nOther advances included the armor plating for tanks in World War II; Snopake, the first correction fluid, developed in 1955; the fuel for the first nuclear submarine, the USS Nautilus (SSN-571); development of the Universal Product Code in 1965; cruise control for automobiles in 1970; and the first all-sputtered photovoltaic cell for solar energy in 1974. In 1987, PIRI, a fiber optics venture with Mitsubishi and NTT, was launched, which resulted in a $1.8 billion market. In conjunction with Kevin M. Amula, Battelle Geneva developed \"No-melt\" chocolate in 1988.\n\nBattelle has made numerous medical advances, including a 1972 breakthrough development of special tubing to prevent blood clots during surgical procedures, and more recently, the development of reusable insulin injection pen, including dose memory, with Eli Lilly and Co..\n\nBattelle was the contractor for a computer system on which the Voter News Service relied for tallying exit polling data in the November 2002 U.S. Congressional and Senate elections; the system failed and results were not reported until ten months after the election. The failure led to the disbanding of the VNS and the formation of its replacement, the National Election Pool.\n\nBattelle provides funds for a public policy research center at the John Glenn College of Public Affairs of The Ohio State University to focus on scholarly questions associated with science and technology policy. The Battelle Center for Science and Technology Policy began official operation in July 2011.\n\n"}
{"id": "40322326", "url": "https://en.wikipedia.org/wiki?curid=40322326", "title": "CELAR", "text": "CELAR\n\nCELAR was a research project which successfully developed an open source set of tools designed to provide automatic, multi-grained resource allocation for cloud applications. In this way CELAR developed a solution that competes directly with Ubuntu Juju (software), Openstack Heat and Amazon Web Services. CELAR was developed with funding from the European Commission under the Seventh Framework Programme for Research and Technological Development, sometimes abbreviated to FP7.\n\nCELAR was a noteworthy example of a collaborative research project supported by the European Union involving a number of European partners including the ATHENA Research and Innovation Center (Greece), Flexiant (United Kingdom), PlayGen Limited (United Kingdom), SIXSQ (Switzerland) the University of Cyprus (Cyprus), the Vienna University of Technology (Austria), The University of Manchester (United Kingdom) and The Greek Research and Technology Network (Greece).\n\nThe vision of the CELAR project was to provide automatic, multi-grained resource allocation for cloud applications. This enabled the commitment of just the right amount of resources based on application demand, performance and requirements, results in optimal use of infrastructure resources and significant reductions in administrative costs.\n\nThe outcome of the CELAR project is an open-source toolkit; a set of tools that allows you to automatically, elastically scale your application deployments. Using the CELAR Platform, you the user can perform the following steps:\n\n\nCELAR features a c-Eclipse (Eclipse (software) GUI for defining the application’s topology, deployment and elasticity constraints. User application deployments can be defined in any scripting language with an ability to incorporate any of the configuration management systems (Chef (software), Puppet (software) etc.).\n\nDuring the lifetime of the project a number of scientific papers were published showcasing the innovation and research of the CELAR consortium:\n\n[C1] Automated, Elastic Resource Provisioning for NoSQLClusters Using TIRAMOLA (May 14, 2013) Best Paper Award, D. Tsoumakos, I. Konstantinou, C. Boumpouka, S. Sioutas and N. Koziris (ATHENA), CCGrid 2013\n\n[C2] SYBL: an Extensible Language for Controlling Elasticity in Cloud Applications (May 13–15, 2013), G. Copil, D. Moldovan, H.-L. Truong, S. Dustdar (TUW), CCGrid 2013\n\n[C3] On Estimating Actuation Delays in Elastic Computing Systems (May 20, 2013), A. Gambi, D. Moldovan, G. Copil, H.-L. Truong, S. Dustdar(TUW), SEAMS 2013\n\n[C4] COCCUS: Self-Configured Cost-Based Query Services in the Cloud (June 22–27, 2013), I. Konstantinou, D. Tsoumakos, and N. Koziris (ATHENA), 2013 ACM SIGMOD/PODS International Conference on Management of Data\n\n[C5] Multi-level Elasticity Control of Cloud Services (short paper) (December 2–5, 2013), G. Copil, D. Moldovan, H.-L. Truong, S. Dustdar (TUW), CSOC 2013\n\n[C6] SYBL+MELA: Specifying, Monitoring, and Controlling Elasticity of Cloud Services (demo paper) (December 2–5, 2013), G. Copil, D. Moldovan, H.-L. Truong, S. Dustdar (TUW), ICSOC 2013\n\n[C7] MELA: Monitoring and Analyzing Elasticity of Cloud Services (2–5 December 2013), D. Moldovan, G. Copil, H.-L. Truong, S. Dustdar (TUW), CloudCom 2013\n\n[C8] CoMoT – A Platform-as-a-Service for Elasticity in the Cloud (10–14 March 2014), Hong-Linh Truong, Schahram Dustdar, Georgiana Copil, AlessioGambi, Waldemar Hummer, Duc-Hung Le, Daniel Moldovan(TUW), Future of PaaS 2014\n\n[C9] JCatascopia: Monitoring Elastically Adaptive Applications in the Cloud, D. Trihinas and G. Pallis and M. D. Dikaiakos, 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID2014)\n\n[C10] Managing and Monitoring Elastic Cloud Applications, D. Trihinas and C. Sofokleous and N. Loulloudes and A.Foudoulis and G. Pallis and M. D. Dikaiakos, 14th International Conference on Web Engineering (ICWE 2014) Poster\n\n[C11] c-Eclipse: An Open-Source Management Framework for Cloud Applications, C. Sofokleous and N. Loulloudes and D. Trihinas and G. Pallisand M. Dikaiakos, EuroPar 2014\n\n[C12] On Controlling Cloud Services Elasticity in Heterogeneous Clouds, Georgiana Copil, Daniel Moldovan, Hong-Linh Truong, Schahram Dustdar, 6th Cloud Control Workshop, 7th IEEE/ACM International Conference on Utility and Cloud Computing, 8–11 December, London, 2014\n\n[C13]QUELLE – a Framework for Accelerating the Development of Elastic Systems, Daniel Moldovan, Georgiana Copil, Hong-Linh Truong, Schahram Dustdar, Third European Conference on Service-Oriented and Cloud Computing – ESOCC 2014, 2–4 September, Manchester, United Kingdom\n\n[C14] ADVISE – a Framework for Evaluating Cloud Service Elasticity Behavior (3–6 November 2014) Best Paper Award Georgiana Copil, Demetris Trihinas, Hong-Linh Truong, Daniel Moldovan, George Pallis, Schahram Dustdar, Marios Dikaiakos, 12th International Conference on Service Oriented Computing. Paris, France\n\n[C15] On Analyzing Elasticity Relationships of Cloud Services, (15–18 December 2014) Daniel Moldovan, Georgiana Copil, Hong-Linh Truong, Schahram Dustdar, 6th IEEE International Conference on Cloud Computing Technology and Science, CloudCom 2014, Singapore\n\n[C16] SALSA: a Framework for Dynamic Configuration of Cloud Services, (15–18 December 2014) Duc-Hung Le, Hong-Linh Truong, Georgiana Copil, Stefan Nastic and Schahram Dustdar, 6th International Conference on Cloud Computing Technology and Science, CloudCom 2014, Singapore\n\n[C17] Coordination-aware Elasticity, (8–11 December 2014) Stefano Mariani, Hong-Linh Truong, Georgiana Copil, Andrea Omicini, Schahram Dustdar, 7th IEEE/ACM International Conference on Utility and Cloud Computing, London, 2014\n\n[C18] CELAR: Automated Application Elasticity Platform, (27–30 October) Ioannis Giannakopoulos, Nikolaos Papailiou, Christos Mantas, Ioannis Konstantinou, Dimitrios Tsoumakos and Nectarios Koziris, 2014 IEEE International Conference on Big Data, Washington DC, USA, 2014\n\n[C19] Dependable Horizontal Scaling Based On Probabilistic Model Checking, A. Naskos, E. Stachtiari, A. Gounaris, P. Katsaros, D.Tsoumakos, I. Konstantinou and S. Sioutas, CCGrid 2015 conference\n\n[C20] PANIC: Modeling Application Performance over Virtualized Resources, I. Giannakopoulos, D. Tsoumakos, N. Papailiou and N. Koziris, 2015 IEEE International Conference on Cloud Engineering (IC2E 2015)\n\n[C21] I/O Performance Modeling for Big Data Applications over Cloud Infrastructures, I. Mytilinis, D. Tsoumakos, V. Kantere, A. Nanos and N. Koziris, 2015 IEEE International Conference on Cloud Engineering (IC2E 2015)\n\n[C22] Transforming Vertical Web Applications Into Elastic Cloud Applications, Nikola Tankovic, Tihana Galinac Grbac, Hong-Linh Truong, Schahram Dustdar, International Conference on Cloud Engineering (IC2E 2015), 9–12 March 2015, USA, \n\n[C23] On Developing and Operating of Data Elasticity Management Process, Tien-Dung Nguyen, Hong-Linh Truong, Georgiana Copil, Duc-Hung Le, Daniel Moldovan, Schahram Dustdar, 13th International Conference on Service Oriented Computing (ICSOC) 2015, 16–19 November, Goa, India\n\n[C24] iCOMOT – Toolset for Managing IoT Cloud Systems, Hong-Linh Truong, Georgiana Copil, Schahram Dustdar, Duc-Hung Le, Daniel Moldovan, Stefan Nastic, 16th IEEE International Conference on Mobile Data Management, 15–18 June 2015, Pittsburg, USA. (Demo), http://mdmconferences.org/mdm2015/demos_Accepted.html\n\n[C25] Programming Elasticity and Commitment in Dynamic Processes, Pablo Fernandez, Hong-Linh Truong, Schahram Dustdar, Antonio Ruiz-Cortes, IEEE Internet Computing, Volume 19, Number 2, pp. 68 – 74, \n\n[C26] Principles for Engineering IoT Cloud Systems, Hong-Linh Truong, Schahram Dustdar, IEEE Cloud Computing, Volume 2, Issue 2, pp. 68 – 76, \n\n[C27] Supporting Cloud Service Operation Management for Elasticity, Georgiana Copil, Hong-Linh Truong, Schahram Dustdar, 13th International Conference on Service Oriented Computing (ICSOC) 2015, 16–19 November, Goa, India\n\n[C28] Enabling Interoperable Cloud Application Management through an Open Source Ecosystem, N. Loulloudes, C. Sofokleous, D. Trihinas, M. D. Dikaiakos, G. Pallis, IEEE Internet Computing 19(3): 54-59 (2015)\n\n[C29] Enabling Cloud Application Portability, D. Antoniades, N. Loulloudes, A. Foudoulis, C. Sophokleous, D. Trihinas, G. Pallis, M. Dikaiakos, H. Kornmayer, Proceedings of the Cloud Challenge 2015, in conjunction with 8th IEEE/ACM International Conference on Utility and Cloud Computing (UCC), December 7–10, 2015, Limassol, Cyprus.\n\n[C30] CELAR: Automatic, Multi-grained Elasticity Provisioning for the Cloud, M Dikaiakos N. Loulloudes, G. Pallis, H-L. Truong, D. Tsoumakos, 8th IEEE/ACM International Conference on Utility and Cloud Computing (UCC), December 7–10, 2015, Limassol, Cyprus\n\n[C31] Cloud Application Management Framework (CAMF) Tutorial, N. Loulloudes, 2nd Workshop on Cloud Computing in Cyprus: Opportunities and Challenges, University of Cyprus, 3 June 2015, Nicosia, Cyprus\n\n[C32] Enabling Interoperable Cloud Application Management through an Open Source Ecosystem, Nicholas Loulloudes and Chrystalla Sofokleous and Demetris Trihinas and Marios D. Dikaiakos and George Pallis, IEEE Internet Computing Volume 19, Pages: 54–59, 2015\n\n[C33] Monitoring Elastically Adaptive Multi-Cloud Services, D. Trihinas, G. Pallis, M.D Dikaiakos, IEEE Transactions on Cloud Computing In Revision (under second round review)\n\n[C34] AdaM: an Adaptive Monitoring Framework for Sampling and Filtering on IoT Devices, D. Trihinas, G. Pallis, M.D Dikaiakos, 2015 IEEE International Conference on Big Data (IEEE BigData 2015), October, Santa Clara, USA, 2015\n\n[C35] Analysing Cancer Genomics in the Elastic Cloud, Christopher Smowton, Andoena Balla, Demetris Antoniades, Crispin Miller, GeorgePallis, Marios D. Dikaiakos, Wei Xing, The 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, , May 2015, Shengzheng, China\n\n[C36] A Network Approach for Managing and Processing Big Omic Data in Clouds, Wei Xing, Wei Jie, Dimitrios Tsoumakos, Moustafa Ghanem, Journal of Cluster Computing, Special issue on Big Data Computing. , Print , Online \n\n[C37] A Cost-Effective Approach to Improving Performance of Big Genomic Data Analyses in Clouds, Christopher Smowton, Andoena Balla, Demetris Antoniades, Crispin Miller, GeorgePallis, Marios D. Dikaiakos, Wei Xing, Submitted to Journal of Future Generation Computer Systems (under review)\n\n[C38] SCAN: A Smart Application Platform for Empowering Parallelization of Big Genomic Data Analysis in Clouds, Wei Xing, Jie Wei, Crispin Miller, 44th International Conference on Parallel Processing (ICPP-2015), 2015, Beijing, China\n\n[C39] Genome Analysis in a Dynamically Scaled Hybrid Cloud, Chris Smowton, Georgiana Copil, Hong-Linh Truong, Crispin Miller and Wei Xing, IEEE eScience 2015, Munich, Germany\n\n[C40] A Cloud-based Data Network Approach for Translational Cancer Research, Wei Xing, Dimitrios Tsoumakos, Moustafa Ghanem, GeNeDis 2014 Computational Biology and Bioinformatics, Springer International Publishing, ; DOI: 10.1007/978-3-319-09012-2, 2015\n\n[J1] MELA: Elasticity Analytics for Cloud Services, Daniel Moldovan, Georgiana Copil, Hong-Linh Truong, Schahram Dustdar, to appear in International Journal of Big Data Intelligence 2015, Vol. 2, No. 1, http://www.inderscience.com/info/inarticletoc.php?jcode=ijbdi&year=2015&vol=2&issue=1\n\n[J2] Evaluating cloud service elasticity behaviour, G. Copil and D. Trihinas and H.L Truong and D. Moldovan and G. Pallis and S. Dustdar and M. D. Dikaiakos, International Journal of Cooperative Information Systems 2015.\n\n[J3]: Programming Elasticity in the Cloud, Hong-Linh Truong, Schahram Dustdar, Computer, March 2015\n\n[BC1] On Controlling Elasticity of Cloud Applications in CELAR, Georgiana Copil, Daniel Moldovan, Duc-Hung Le, Hong-Linh Truong, Schahram Dustdar, Chrystalla Sofokleous, Nicholas Loulloudes, Demetris Trihinas, George Pallis, Marios D. Dikaiakos, Craig Sheridan, Evangelos Floros, Christos KK Loverdos, Kam Star, Wei Xing, to appear in Emerging Research in Cloud Distributed Computing Systems, Advances in Systems Analysis, Software Engineering, and High Performance Computing (ASASEHPC) Book Series\n\nKey:\nC – Conference, BC – Book Chapter, J – Journal\n\nAuto Scaling Resources is one of the top obstacles and opportunities for cloud computing: consumers can minimize the execution time of their tasks without exceeding a given budget. Cloud providers maximise their financial gain while keeping their customers satisfied and minimizing administrative costs. Many systems claim to offer adaptive elasticity, yet the “throttling” is usually performed manually, requiring the user to figure out the proper scaling conditions. In order to harvest the benefits of elastic provisioning, it is imperative that it be performed in an automated, fully customizable manner. CELAR delivers a fully automated and highly customisable system for elastic provisioning of resources in cloud computing platforms.\n\nhttps://www.fi-athens.eu/media/celar\nCraig Sheridan, Head of Research Flexiant said in a statement:\n\n“Combining our consultancy and expertise of cloud orchestration with world class academic and industrial partners in this project will offer the consortium the qualitative and quantitative information necessary to gauge platform and application performance to facilitate intelligent deployment decisions.”\n\nThe goal of the CELAR project was to develop methods and open-source tools for applying and controlling multi-grained, elastic resource provisioning for Cloud applications in an automated manner. This resource allocation is to be performed through intelligent decision-making based on:\n\n(a) Cloud and application performance metrics collected and cost-evaluated through a scalable monitoring system and exposed to the user.\n(b) Qualitative and quantitative characterisation of the application‘s performance through modelling of its elastic properties.\n\nNectarios Koziris, Project Coordinator and Associate Professor of the School of Electrical and Computer Engineering at the National Technical University of Athens explains:\n\n“The result of CELAR is a software package that offers organisations the right amount of resources based on application demand, performance and requirements resulting in optimal use of infrastructure resources and significant reductions in administrative costs.”\n\nCELAR covers the three layers required by an application to operate over the Cloud:\n\n\nThe outcome is a modular, completely open-source system that offers elastic programmability for the user and automatic elasticity at the platform level. This outcome can be bundled in a single software package for one-click installation of any application alongside its automated resource provisioning over a Cloud IaaS.\n\nTwo exemplary applications that showcase and validate the aforementioned technology will be developed: The first will showcase the use of CELAR technology for massive data management and large-scale collaboration required in the on-line gaming realm, while the second will focus on the area of scientific computing, requiring compute- and storage-intensive genome computations.\n\nThe CELAR consortium – under the lead of ATHENA Research and Innovation Center – achieved its objectives by bringing together a team of leading researchers in the large-scale technologies such as Cloud/Grid Computing, service-oriented architectures, virtualization, analytics, Web 2.0 and the world of the Semantic Web. These are combined with active industrial and leading user organizations that offer expertise in the cloud application domain and production-level service provisioning.\n"}
{"id": "57830594", "url": "https://en.wikipedia.org/wiki?curid=57830594", "title": "Candida hypersensitivity", "text": "Candida hypersensitivity\n\nCandida hypersensitivity is a pseudoscientific disease promoted by William G. Crook, M.D. It is spuriously claimed that chronic yeast infections are responsible for many common disorders and non-specific symptoms including fatigue, weight gain, constipation, dizziness, muscle and joint pain, asthma, and others.\n\n\"Candida albicans\" is a fungus that colonizes a large majority of the population (meaning it is present in the body but not causing an infection or any problems). Under certain conditions, however, it can cause an infection. The most common manifestations are thrush (a superficial \"Candida\" infection in the mouth) and vaginitis, also commonly referred to as a yeast infection. \"Candida\" can also cause serious systemic infection, but this is almost always restricted to those with compromised immune systems, such as patients undergoing chemotherapy or with advanced AIDS.\n\nAfter reading publications by C. Orian Truss, M.D., Crook proposed the idea that a condition he termed systemic candidiasis, or Candida hypersensitivity, was responsible for a long list of common conditions and non-specific symptoms including fatigue, asthma, psoriasis, sexual dysfunction, and many others. The list of symptoms is similar to that of multiple chemical sensitivity. Many patients presenting with symptoms of environmental sensitivity claim to suffer from multiple \"fashionable\" syndromes.\n\nThe American Academy of Allergy, Asthma, and Immunology strongly criticized the concept of \"candidiasis hypersensitivity syndrome\" and the diagnostic and treatment approaches its proponents use. AAAAI's position statement concludes: (1) the concept of candidiasis hypersensitivity is speculative and unproven; (2) its basic elements would apply to almost all sick patients at some time because its supposed symptoms are essentially universal; (3) overuse of oral antifungal agents could lead to the development of resistant germs that could menace others; (4) adverse effects of oral antifungal agents are rare, but some inevitably will occur; and (5) neither patients nor doctors can determine effectiveness (as opposed to coincidence) without controlled trials. Because allergic symptoms can be influenced by many factors, including emotions, experiments must be designed to separate the effects of the procedure being tested from the effects of other factors.\n\nBy 2005, scientists were taking note of \"a large pseudoscientific cult\" that had developed around the topic of yeast infections, with claims that up to one in three people were affected by yeast-related illnesses including Candida hypersensitivity. \n\nSome practitioners of alternative medicine have promoted dietary supplements as supposed cures for this non-existent illness, rendering themselves liable to prosecution. In 1990, alternative health vendor Nature's Way signed a FTC consent agreement not to misrepresent in advertising any self-diagnostic test concerning yeast conditions or to make any unsubstantiated representation concerning any food or supplement's ability to control yeast conditions, with a fine of US$30,000 payable to the National Institutes of Health for research in genuine candidiasis.\n\n"}
{"id": "50901086", "url": "https://en.wikipedia.org/wiki?curid=50901086", "title": "Charles François Delamarche", "text": "Charles François Delamarche\n\nCharles-François Delamarche (1740 - 1817) was a French geographer and mapmaker.\n\nOne of the most important French geographers and mapmakers of the second half of the eighteenth century. Successor to Nicolas Sanson (1600 – 1667), Robert de Vaugondy (1686 – 1766), and Rigobert Bonne (1727 – 1794), whose atlases he reprinted. Also taught geography. In addition to maps and globes, his works include a treatise on the use of the sphere and celestial and terrestrial globes. In the treatise, he illustrates both the Ptolemaic and Copernican systems, as well as listing all the ancient and modern constellations. His son Félix Delamarche (18th century – 1st half 19th century) continued his work.\n\n"}
{"id": "36522675", "url": "https://en.wikipedia.org/wiki?curid=36522675", "title": "Chemisches Zentralblatt", "text": "Chemisches Zentralblatt\n\nChemisches Zentralblatt is the first and oldest abstracts journal published in the field of chemistry. It covers the chemical literature from 1830 to 1969 and describes therefore the “birth” of chemistry as science, in contrast to alchemy. The information contained in this German journal is comparable with the content of the leading source of chemical information Chemical Abstracts Service (CAS), which started publishing abstracts in English in 1907.\n\n\"Chemisches Zentralblatt\" was founded as \"Pharmaceutisches Centralblatt\" by Gustav Theodor Fechner and published by Leopold Voß in Leipzig in 1830. In the first year, 544 pages containing 400 abstracts were published, reporting all relevant research results in pharmaceutical chemistry. In the following 20 years the relevance of chemistry grew so much that in 1850 the title changed in \"Chemisch-Pharmazeutisch Zentralblatt\", and in 1856 it became \"Chemisches Zentralblatt\". In 1969, after 140 years the expenses for the collection of primary literature in many languages and the production of abstracts were too high and the publication of \"Chemisches Zentralblatt ceased\".\n\nIn these 140 years, scientific editors reported research progresses in chemistry with approximately 2 million abstracts, publishing over 650,000 pages. Additional 180,000 pages contain indexes such as index of authors, subject indexes, general indexes, register of patents, and formula register.\n\n\"Chemisches Zentralblatt\" was completely digitized by \"FIZ Chemie\" in Berlin. \"FIZ Chemie\" scanned the entire work and developed a full text searchable database for the web. In addition the database can be purchased and integrated in Intranets. The chemical software company InfoChem, based in Munich, developed an Internet-based database, the \"Chemisches Zentralblatt\" Structural Database. This database provides access to the chemical content within the \"Chemisches Zentralblatt\" by performing chemical structure and substructure searches.\n\n"}
{"id": "50061533", "url": "https://en.wikipedia.org/wiki?curid=50061533", "title": "Community perceptions in natural resource management", "text": "Community perceptions in natural resource management\n\nThere has been contested view with regard to the position of the local community on their resources that surrounds their vicinity. Most people urge that the local community's perceptions and aspirations has to be taken on board in the course of planning and managing the resources that are close to their surroundings from which they acquire their day to day livelihood means.According to Perceptions of the local community towards a given natural resource management programme is very essential and hence need not to be underestimated. Understanding of community perceptions is of paramount importance in natural resources management.Several studies stress the importance of using local people’s perceptions as an input for designing and applying appropriate management plans for sustainable development, particularly in protected areas \n"}
{"id": "1930784", "url": "https://en.wikipedia.org/wiki?curid=1930784", "title": "Crookes tube", "text": "Crookes tube\n\nA Crookes tube (also Crookes–Hittorf tube) is an early experimental electrical discharge tube, with partial vacuum, invented by English physicist William Crookes and others around 1869-1875, in which cathode rays, streams of electrons, were discovered.\n\nDeveloped from the earlier Geissler tube, the Crookes tube consists of a partially evacuated glass bulb of various shapes, with two metal electrodes, the cathode and the anode, one at either end. When a high voltage is applied between the electrodes, cathode rays (electrons) are projected in straight lines from the cathode. It was used by Crookes, Johann Hittorf, Julius Plücker, Eugen Goldstein, Heinrich Hertz, Philipp Lenard, Kristian Birkeland and others to discover the properties of cathode rays, culminating in J.J. Thomson's 1897 identification of cathode rays as negatively charged particles, which were later named \"electrons\". Crookes tubes are now used only for demonstrating cathode rays.\n\nWilhelm Röntgen discovered X-rays using the Crookes tube in 1895. The term \"Crookes tube\" is also used for the first generation, cold cathode X-ray tubes, which evolved from the experimental Crookes tubes and were used until about 1920.\n\nCrookes tubes are cold cathode tubes, meaning that they do not have a heated filament in them that releases electrons as the later electronic vacuum tubes usually do. Instead, electrons are generated by the ionization of the residual air by a high DC voltage (from a few kilovolts to about 100 kilovolts) applied between the cathode and anode electrodes in the tube, usually by an induction coil (a \"Ruhmkorff coil\"). The Crookes tubes require a small amount of air in them to function, from about 10 to 5×10 atmosphere (7×10 - 4×10 torr or 0.1-0.005 pascal).\n\nWhen high voltage is applied to the tube, the electric field accelerates the small number of electrically charged ions and free electrons always present in the gas, created by natural processes like photoionization and radioactivity. The electrons collide with other gas molecules, knocking electrons off them and creating more positive ions. The electrons go on to create more ions and electrons in a chain reaction called a Townsend discharge. All the positive ions are attracted to the cathode or negative electrode. When they strike it, they knock large numbers of electrons out of the surface of the metal, which in turn are repelled by the cathode and attracted to the anode or positive electrode. These are the cathode rays.\n\nEnough of the air has been removed from the tube that most of the electrons can travel the length of the tube without striking a gas molecule. The high voltage accelerates these low-mass particles to a high velocity (about 37,000 miles per second, or 59,000 km/s, about 20 percent of the speed of light, for a typical tube voltage of 10 kV). When they get to the anode end of the tube, they have so much momentum that, although they are attracted to the anode, many fly past it and strike the end wall of the tube. When they strike atoms in the glass, they knock their orbital electrons into a higher energy level. When the electrons fall back to their original energy level, they emit light. This process, called fluorescence, causes the glass to glow, usually yellow-green. The electrons themselves are invisible, but the glow reveals where the beam of electrons strikes the glass. Later on, researchers painted the inside back wall of the tube with a phosphor, a fluorescent chemical such as zinc sulfide, in order to make the glow more visible. After striking the wall, the electrons eventually make their way to the anode, flow through the anode wire, the power supply, and back to the cathode.\n\nThe above only describes the motion of the electrons. The full details of the action in a Crookes tube are complicated, because it contains a nonequilibrium plasma of positively charged ions, electrons, and neutral atoms which are constantly interacting. At higher gas pressures, above 10 atm (0.1 Pa), this creates a glow discharge; a pattern of different colored glowing regions in the gas, depending on the pressure in the tube (see diagram). The details were not fully understood until the development of plasma physics in the early 20th century.\n\nCrookes tubes evolved from the earlier Geissler tubes, experimental tubes which are similar to modern neon tube lights. Geissler tubes had only a low vacuum, around 10 atm (100 Pa), and the electrons in them could only travel a short distance before hitting a gas molecule. So the current of electrons moved in a slow diffusion process, constantly colliding with gas molecules, never gaining much energy. These tubes did not create beams of cathode rays, only a colorful glow discharge that filled the tube as the electrons struck the gas molecules and excited them, producing light.\n\nBy the 1870s, Crookes (among other researchers) was able to evacuate his tubes to a lower pressure, 10 to 5x10 atm, using an improved Sprengel mercury vacuum pump invented by his coworker Charles A. Gimingham. He found that as he pumped more air out of his tubes, a dark area in the glowing gas formed next to the cathode. As the pressure got lower, the dark area, now called the \"Crookes dark space\", spread down the tube, until the inside of the tube was totally dark. However, the glass envelope of the tube began to glow at the anode end.\n\nWhat was happening was that as more air was pumped out of the tube, there were fewer gas molecules to obstruct the motion of the electrons from the cathode, so they could travel a longer distance, on average, before they struck one. By the time the inside of the tube became dark, they were able to travel in straight lines from the cathode to the anode, without a collision. They were accelerated to a high velocity by the electric field between the electrodes, both because they did not lose energy to collisions, and also because Crookes tubes were operated at a higher voltage. By the time they reached the anode end of the tube, they were going so fast that many flew past the anode and hit the glass wall. The electrons themselves were invisible, but when they hit the glass walls of the tube they excited the atoms in the glass, making them give off light or fluoresce, usually yellow-green. Later experimenters painted the back wall of Crookes tubes with fluorescent paint, to make the beams more visible.\n\nThis accidental fluorescence allowed researchers to notice that objects in the tube, such as the anode, cast a sharp-edged shadow on the tube wall. Johann Hittorf was first to recognise in 1869 that something must be travelling in straight lines from the cathode to cast the shadow. In 1876, Eugen Goldstein proved that they came from the cathode, and named them \"cathode rays\" (\"Kathodenstrahlen\").\n\nAt the time, atoms were the smallest particles known, the electron was unknown, and what carried electric currents was a mystery. Many ingenious types of Crookes tubes were built to determine the properties of cathode rays (see below). The high energy beams of pure electrons in the tubes revealed their properties much better than electrons flowing in wires. The colorful glowing tubes were also popular in public lectures to demonstrate the mysteries of the new science of electricity. Decorative tubes were made with fluorescent minerals, or butterfly figures painted with fluorescent paint, sealed inside. When power was applied, the fluorescent materials lit up with many glowing colors.\n\nIn 1895, Wilhelm Röntgen discovered X-rays emanating from Crookes tubes. The many uses for X-rays were immediately apparent, the first practical application for Crookes tubes. Medical manufacturers began to produce specialized Crookes tubes to generate X-rays, the first X-ray tubes. \n\nCrookes tubes were unreliable and temperamental. Both the energy and the quantity of cathode rays produced depended on the pressure of residual gas in the tube. Over time the gas was absorbed by the walls of the tube, reducing the pressure. This reduced the amount of cathode rays produced and caused the voltage across the tube to increase, creating more energetic cathode rays. In Crookes X-ray tubes this phenomenon was called \"hardening\" because the higher voltage produced \"harder\", more penetrating X-rays; a tube with a higher vacuum was called a \"hard\" tube, while one with lower vacuum was a \"soft\" tube. Eventually the pressure got so low the tube stopped working entirely. To prevent this, in heavily used tubes such as X-ray tubes various \"softener\" devices were incorporated that released a small amount of gas, restoring the tube's function.\n\nThe electronic vacuum tubes invented later around 1906 superseded the Crookes tube. These operate at a still lower pressure, around 10 atm (10 Pa), at which there are so few gas molecules that they do not conduct by ionization. Instead, they use a more reliable and controllable source of electrons, a heated filament or hot cathode which releases electrons by thermionic emission. The ionization method of creating cathode rays used in Crookes tubes is today only used in a few specialized gas discharge tubes such as thyratrons.\n\nThe technology of manipulating electron beams pioneered in Crookes tubes was applied practically in the design of vacuum tubes, and particularly in the invention of the cathode ray tube by Ferdinand Braun in 1897.\n\nWhen the voltage applied to a Crookes tube is high enough, around 5,000 volts or greater, it can accelerate the electrons to a high enough velocity to create X-rays when they hit the anode or the glass wall of the tube. The fast electrons emit X-rays when their path is bent sharply as they pass near the high electric charge of an atom's nucleus, a process called bremsstrahlung, or they knock an atom's inner electrons into a higher energy level, and these in turn emit X-rays as they return to their former energy level, a process called X-ray fluorescence. Many early Crookes tubes undoubtedly generated X-rays, because early researchers such as Ivan Pulyui had noticed that they could make foggy marks on nearby unexposed photographic plates. On November 8, 1895, Wilhelm Röntgen was operating a Crookes tube covered with black cardboard when he noticed that a nearby fluorescent screen glowed faintly. He realized that some unknown invisible rays from the tube were able to pass through the cardboard and make the screen fluoresce. He found that they could pass through books and papers on his desk. Röntgen began to investigate the rays full-time, and on December 28, 1895, published the first scientific research paper on X-rays. Röntgen\nwas awarded the first Nobel Prize in Physics (in 1901) for his discoveries.\n\nThe many applications of X-rays created the first practical use for Crookes tubes, and workshops began manufacturing specialized Crookes tubes to generate X-rays, the first X-ray tubes. The anode was made of a heavy metal, usually platinum, which generated more X-rays, and was tilted at an angle to the cathode, so the X-rays would radiate through the side of the tube. The cathode had a concave spherical surface which focused the electrons into a small spot around 1 mm in diameter on the anode, in order to approximate a point source of X-rays, which gave the sharpest radiographs. These cold cathode type X-ray tubes were used until about 1920, when they were superseded by the hot cathode Coolidge X-ray tube.\n\nDuring the last quarter of the 19th century Crookes tubes were used in dozens of historic experiments to try to find out what cathode rays were. There were two theories: British scientists Crookes and Cromwell Varley believed they were particles of 'radiant matter', that is, electrically charged atoms. German researchers E. Wiedemann, Heinrich Hertz, and Eugen Goldstein believed they were 'aether vibrations', some new form of electromagnetic waves, and were separate from what carried the current through the tube. The debate continued until J.J. Thomson measured their mass, proving they were a previously unknown negatively charged particle, the first subatomic particle, which he called a 'corpuscle' but was later renamed the 'electron'.\n\nJulius Plücker in 1869 built an anode shaped like a Maltese Cross in the tube. It was hinged, so it could fold down against the floor of the tube. When the tube was turned on, it cast a sharp cross-shaped shadow on the fluorescence on the back face of the tube, showing that the rays moved in straight lines. This fluorescence was used as an argument that cathode rays were electromagnetic waves, since the only thing known to cause fluorescence at the time was ultraviolet light. After a while the fluorescence would get 'tired' and decrease. If the cross was folded down out of the path of the rays, it no longer cast a shadow, and the previously shadowed area would fluoresce more strongly than the area around it.\n\nEugen Goldstein in 1876 found that cathode rays were always emitted perpendicular to the cathode's surface. If the cathode was a flat plate, the rays were shot out in straight lines perpendicular to the plane of the plate. This was evidence that they were particles, because a luminous object, like a red hot metal plate, emits light in all directions, while a charged particle will be repelled by the cathode in a perpendicular direction. If the electrode was made in the form of a concave spherical dish, the cathode rays would be focused to a spot in front of the dish. This could be used to heat samples to a high temperature.\n\nHeinrich Hertz built a tube with a second pair of metal plates to either side of the cathode ray beam, a crude CRT. If the cathode rays were charged particles, their path should be bent by the electric field created when a voltage was applied to the plates, causing the spot of light where the rays hit to move sideways. He did not find any bending, but it was later determined that his tube was insufficiently evacuated, causing accumulations of surface charge which masked the electric field. Later Arthur Shuster repeated the experiment with a higher vacuum. He found that the rays were attracted toward a positively charged plate and repelled by a negative one, bending the beam. This was evidence they were negatively charged, and therefore not electromagnetic waves.\nCrookes put a magnet across the neck of the tube, so that the North pole was on one side of the beam and the South pole was on the other, and the beam travelled through the magnetic field between them. The beam was bent down, perpendicular to the magnetic field. This effect (now called the Lorentz force) was similar to the behaviour of electric currents in an electric motor and showed that the cathode rays obeyed Faraday's law of induction like currents in wires. Both electric and magnetic deflection were evidence for the particle theory, because electric and magnetic fields have no effect on a beam of light waves.\n\nCrookes put a tiny vaned turbine or paddlewheel in the path of the cathode rays, and found that it rotated when the rays hit it. The paddlewheel turned in a direction away from the cathode side of the tube, suggesting that the force of the cathode rays striking the paddles was causing the rotation. Crookes concluded at the time that this showed that cathode rays had momentum, so the rays were likely matter particles. However, later it was concluded that the paddle wheel turned not due to the momentum of the particles (or electrons) hitting the paddle wheel but due to the radiometric effect. When the rays hit the paddle surface they heated it, and the heat caused the gas next to it to expand, pushing the paddle. This was proven in 1903 by J. J. Thomson who calculated that the momentum of the electrons hitting the paddle wheel would only be sufficient to turn the wheel one revolution per minute. All this experiment really showed was that cathode rays were able to heat surfaces.\nJean-Baptiste Perrin wanted to determine whether the cathode rays actually carried negative charge, or whether they just accompanied the charge carriers, as the Germans thought. In 1895 he constructed a tube with a 'catcher', a closed aluminum cylinder with a small hole in the end facing the cathode, to collect the cathode rays. The catcher was attached to an electroscope to measure its charge. The electroscope showed a negative charge, proving that cathode rays really carry negative electricity.\n\nGoldstein found in 1886 that if the cathode is made with small holes in it, streams of a faint luminous glow will be seen issuing from the holes on the back side of the cathode, facing away from the anode. It was found that in an electric field these anode rays bend in the opposite direction from cathode rays, toward a negatively charged plate, indicating that they carry a positive charge. These were the positive ions which were attracted to the cathode, and created the cathode rays. They were named \"canal rays\" (\"Kanalstrahlen\") by Goldstein.\nEugen Goldstein thought he had figured out a method of measuring the speed of cathode rays. If the glow discharge seen in the gas of Crookes tubes was produced by the moving cathode rays, the light radiated from them in the direction they were moving, down the tube, would be shifted in frequency due to the Doppler effect. This could be detected with a spectroscope because the emission line spectrum would be shifted. He built a tube shaped like an \"L\", with a spectroscope pointed through the glass of the elbow down one of the arms. He measured the spectrum of the glow when the spectroscope was pointed toward the cathode end, then switched the power supply connections so the cathode became the anode and the electrons were moving in the other direction, and again observed the spectrum looking for a shift. He did not find one, which he calculated meant that the rays were traveling very slowly. It is now recognized that the glow in Crookes tubes is emitted from gas atoms hit by the electrons, not the electrons themselves. Since the atoms are thousands of times more massive than the electrons, they move much slower, accounting for the lack of Doppler shift.\n\nPhilipp Lenard wanted to see if cathode rays could pass out of the Crookes tube into the air. See diagram. He built a tube with a \"window\" \"(W)\" in the glass envelope made of aluminum foil just thick enough to hold the atmospheric pressure out (later called a \"Lenard window\") facing the cathode \"(C)\" so the cathode rays would hit it. He found that something did come through. Holding a fluorescent screen up to the window caused it to fluoresce, even though no light reached it. A photographic plate held up to it would be darkened, even though it was not exposed to light. The effect had a very short range of about . He measured the ability of cathode rays to penetrate sheets of material, and found they could penetrate much farther than moving atoms could. Since atoms were the smallest particles known at the time, this was first taken as evidence that cathode rays were waves. Later it was realized that electrons were much smaller than atoms, accounting for their greater penetration ability. Lenard was awarded the Nobel Prize in Physics in 1905 for his work.\n\n\n"}
{"id": "9014", "url": "https://en.wikipedia.org/wiki?curid=9014", "title": "Developmental psychology", "text": "Developmental psychology\n\nDevelopmental psychology is the scientific study of how and why human beings change over the course of their life. Originally concerned with infants and children, the field has expanded to include adolescence, adult development, aging, and the entire lifespan. Developmental psychologists aim to explain how thinking, feeling, and behaviors change throughout life. This field examines change across three major dimensions: physical development, cognitive development, and socioemotional development. Within these three dimensions are a broad range of topics including motor skills, executive functions, moral understanding, language acquisition, social change, personality, emotional development, self-concept, and identity formation.\n\nDevelopmental psychology examines the influences of nature \"and\" nurture on the process of human development, and processes of change in context and across time. Many researchers are interested in the interactions among personal characteristics, the individual's behavior, and environmental factors, including the social context and the built environment. Ongoing debates include biological essentialism vs. neuroplasticity and stages of development vs. dynamic systems of development.\n\nDevelopmental psychology involves a range of fields, such as educational psychology, child psychopathology, forensic developmental psychology, child development, cognitive psychology, ecological psychology, and cultural psychology. Influential developmental psychologists from the 20th century include Urie Bronfenbrenner, Erik Erikson, Sigmund Freud, Jean Piaget, Barbara Rogoff, Esther Thelen, and Lev Vygotsky.\n\nJohn B. Watson and Jean-Jacques Rousseau are typically cited as providing the foundations for modern developmental psychology. In the mid-18th century Jean Jacques Rousseau described three stages of development: \"infants\" (infancy), \"puer\" (childhood) and \"adolescence\" in \"\". Rousseau's ideas were taken up strongly by educators at the time.\n\nIt generally focuses on how and why certain modifications throughout an individual’s life-cycle (cognitive, social, intellectual, personality) and human growth change over time. There are many theorists that have made a profound contribution to this area of psychology. For example, Erik Erikson developed a model of eight stages of psychological development. He believed that humans developed in stages throughout their lifetimes and this would affect their behaviors (Similar ideas to Sigmund Freud)\n\nIn the late 19th century, psychologists familiar with the evolutionary theory of Darwin began seeking an evolutionary description of psychological development; prominent here was the pioneering psychologist G. Stanley Hall, who attempted to correlate ages of childhood with previous ages of humanity. James Mark Baldwin who wrote essays on topics that included \"Imitation: A Chapter in the Natural History of Consciousness\" and \"Mental Development in the Child and the Race: Methods and Processes\". James Mark Baldwin was heavily involved in the theory of developmental psychology. Sigmund Freud, whose concepts were developmental, significantly affected public perceptions.\n\nSigmund Freud believed that we all had a conscious, preconscious, and unconscious level. In the conscious, we are aware of our mental process. The preconscious involves information that, though not currently in our thoughts, can be brought into consciousness. Lastly, the unconscious includes mental processes we are unaware of.\n\nHe believed there is tension between the conscious and unconscious because the conscious tries to hold back what the unconscious tries to express. To explain this he developed three personality structures: the id, ego, and superego. The id, the most primitive of the three, functions according to the pleasure principle: seek pleasure and avoid pain. The superego plays the critical and moralizing role; and the ego is the organized, realistic part that mediates between the desires of the id and the superego.\n\nBased on this, he proposed five universal stages of development, that each is characterized by the erogenous zone that is the source of the child's psychosexual energy. The first is the \"oral stage\", which occurs from birth to 12 months of age. During the oral stage, \"the libido is centered in a baby's mouth.\" The baby is able to suck. The second is the \"anal stage\", from one to three years of age. During the anal stage, the child defecates from the anus and is often fascinated with their defecation. The third is the \"phallic stage\", which occurs from three to five years of age (most of a person's personality forms by this age). During the phallic stage, the child is aware of their sexual organs. The fourth is the \"latency stage\", which occurs from age five until puberty. During the latency stage, the child's sexual interests are repressed. Stage five is the \"genital stage\", which takes place from puberty until adulthood. During the genital stage, puberty starts happening.\n\nJean Piaget, a Swiss theorist, posited that children learn by actively constructing knowledge through hands-on experience. He suggested that the adult's role in helping the child learn was to provide appropriate materials that the child can interact with and use to construct. He used Socratic questioning to get children to reflect on what they were doing, and he tried to get them to see contradictions in their explanations.\n\nPiaget believed that intellectual development takes place through a series of stages, which he described in his theory on cognitive development. Each stage consists of steps the child must master before moving to the next step. He believed that these stages are not separate from one another, but rather that each stage builds on the previous one in a continuous learning process. He proposed four stages: \"sensorimotor\", \"pre-operational\", \"concrete operational\", and \"formal operational\". Though he did not believe these stages occurred at any given age, many studies have determined when these cognitive abilities should take place.\n\nPiaget claimed that logic and morality develop through constructive stages. Expanding on Piaget's work, Lawrence Kohlberg determined that the process of moral development was principally concerned with justice, and that it continued throughout the individual's lifetime.\n\nHe suggested three levels of moral reasoning; pre-conventional moral reasoning, conventional moral reasoning, and post-conventional moral reasoning. The pre-conventional moral reasoning is typical of children and is characterized by reasoning that is based on rewards and punishments associated with different courses of action. Conventional moral reason occurs during late childhood and early adolescence and is characterized by reasoning based on rules and conventions of society. Lastly, post-conventional moral reasoning is a stage during which the individual sees society's rules and conventions as relative and subjective, rather than as authoritative.\n\nKohlberg used the Heinz Dilemma to apply to his stages of moral development. The Heinz Dilemma involves Heinz's wife dying from cancer and Heinz having the dilemma to save his wife by stealing a drug. Preconventional morality, conventional morality, and post-conventional morality applies to Heinz's situation.\n\nGerman-American psychologist Erik Erikson and his collaborator and wife, Joan Erikson, conceptualized eight stages of psychosocial development that they theorized healthy individuals pass through as they develop from infancy to adulthood. The first stage is called \"Trust vs. Mistrust\" takes place in infancy. The best virtue for the first stage is hope, in the infant learning who to trust and having hope for a supportive group of people to be there for him/her. The second stage is \"Autonomy vs. Shame and Doubt\" with the best virtue being will. This takes place in early childhood where the child learns to become more independent by discovering what they are capable of where if the child is overly controlled, they believe to feel inadequate on surviving by themselves, which can lead to low self-esteem and doubt. The third stage is \"Initiative vs. Guilt\". The basic virtue that would be gained is the purpose and takes place in the play age. This is the stage where the child will be curious and have many interactions with other kids. They will ask many questions as their curiosity grows. If too much guilt is present, the child may have a slower and harder time interacting with other children. The fourth stage is \"Industry (competence) vs. Inferiority\". The basic virtue for this stage is competency which happens at the school age. This stage is when the child will try to win the approval of others and fit in and understand the value of their accomplishments. The fifth stage is \"Identity vs. Role Confusion\". The basic virtue gained is fidelity which takes place in adolescence. This is where the child will start to find who he/she is as a person in society. What sex role he/she picks. The sixth stage is \"Intimacy vs. Isolation\", which happens in young adults and the virtue gained is love. This is where the person will start to share his/her life with someone else intimately and emotionally. In not doing so, it could lead to isolation. The seventh stage is \"Generativity vs. Stagnation\". This happens in adulthood and the virtue gained would be care. We become stable and start to give back by raising a family and becoming involved in the community. The eighth stage is \"Ego Integrity vs. Despair\". This happens during maturity and wisdom is gained. When one grows old and they contemplate and look back and see the success or failure of their life. This is also the stage where one can also have closure and accept death without fearing anything.\n\nMichael Commons enhanced and simplified Bärbel Inhelder and Piaget's developmental theory and offers a standard method of examining the universal pattern of development. The Model of Hierarchical Complexity (MHC) is not based on the assessment of domain-specific information, It divides the Order of Hierarchical Complexity of tasks to be addressed from the Stage performance on those tasks. A stage is the order hierarchical complexity of the tasks the participant's successfully addresses. He expanded Piaget's original eight stage (counting the half stages) to fifteen stages. The stages are : 0 Calculatory; 1 Sensory & Motor; 2 Circular sensory-motor; 3 Sensory-motor; 4 Nominal; 5 Sentential; 6 Preoperational; 7 Primary; 8 Concrete; 9 Abstract; 10 Formal; 11 Systematic; 12 Metasystematic; 13 Paradigmatic; 14 Cross-paradigmatic; 15 Meta-Cross-paradigmatic. The order of hierarchical complexity of tasks predicts how difficult the performance is with an R ranging from 0.9 to 0.98.\n\nIn the MHC, there are three main axioms for an order to meet in order for the higher order task to coordinate the next lower order task. Axioms are rules that are followed to determine how the MHC orders actions to form a hierarchy. These axioms are: a) defined in terms of tasks at the next lower order of hierarchical complexity task action; b) defined as the higher order task action that organizes two or more less complex actions; that is, the more complex action specifies the way in which the less complex actions combine; c) defined as the lower order task actions have to be carried out non-arbitrarily.\n\nEcological systems theory, originally formulated by Urie Bronfenbrenner, specifies four types of nested environmental systems, with bi-directional influences within and between the systems. The four systems are microsystem, mesosystem, exosystem, and macrosystem. Each system contains roles, norms and rules that can powerfully shape development. The microsystem is the direct environment in our lives such as our home and school. Mesosystem is how relationships connect to the microsystem. Exosystem is a larger social system where the child plays no role. Macrosystem refers to the cultural values, customs and laws of society.\n\nThe microsystem is the immediate environment surrounding and influencing the individual (example: school or the home setting). The mesosystem is the combination of two microsystems and how they influence each other (example: sibling relationships at home vs. peer relationships at school). The exosystem is the interaction among two or more settings that are indirectly linked (example: a father's job requiring more overtime ends up influencing his daughter's performance in school because he can no longer help with her homework). The macrosystem is broader taking into account social economic status, culture, beliefs, customs and morals (example: a child from a wealthier family sees a peer from a less wealthy family as inferior for that reason). Lastly, the chronosystem refers to the chronological nature of life events and how they interact and change the individual and their circumstances through transition (example: a mother losing her own mother to illness and no longer having that support in her life).\n\nSince its publication in 1979, Bronfenbrenner's major statement of this theory, \"The Ecology of Human Development\" has had widespread influence on the way psychologists and others approach the study of human beings and their environments. As a result of this conceptualization of development, these environments—from the family to economic and political structures—have come to be viewed as part of the life course from childhood through to adulthood.\n\nLev Vygotsky was a Russian theorist from the Soviet era, who posited that children learn through hands-on experience and social interactions with members of his/her culture. Unlike Piaget, he claimed that timely and sensitive intervention by adults when a child is on the edge of learning a new task (called the \"zone of proximal development\") could help children learn new tasks. This adult role is often referred to as the skilled \"master,\" whereas the child is considered the learning apprentice through an educational process often termed \"cognitive apprenticeship\" Martin Hill stated that \"The world of reality does not apply to the mind of a child.\" This technique is called \"scaffolding,\" because it builds upon knowledge children already have with new knowledge that adults can help the child learn. Vygotsky was strongly focused on the role of culture in determining the child's pattern of development, arguing that development moves from the social level to the individual level. In other words, Vygotsky claimed that psychology should focus on the progress of human consciousness through the relationship of an individual and their environment. He felt that if scholars continued to disregard this connection, then this disregard would inhibit the full comprehension of the human consciousness.\n\nConstructivism is a paradigm in psychology that characterizes learning as a process of actively constructing knowledge. Individuals create meaning for themselves or make sense of new information by selecting, organizing, and integrating information with other knowledge, often in the context of social interactions. Constructivism can occur in two ways: individual and social. Individual constructivism is when a person constructs knowledge through cognitive processes of their own experiences rather than by memorizing facts provided by others. Social constructivism is when individuals construct knowledge through an interaction between the knowledge they bring to a situation and social or cultural exchanges within that content.\n\nJean Piaget, a Swiss developmental psychologist, proposed that learning is an active process because children learn through experience and make mistakes and solve problems. Piaget proposed that learning should be whole by helping students understand that meaning is constructed.\n\nEvolutionary developmental psychology is a research paradigm that applies the basic principles of Darwinian evolution, particularly natural selection, to understand the development of human behavior and cognition. It involves the study of both the genetic and environmental mechanisms that underlie the development of social and cognitive competencies, as well as the epigenetic (gene-environment interactions) processes that adapt these competencies to local conditions.\n\nEDP considers both the reliably developing, species-typical features of ontogeny (developmental adaptations), as well as individual differences in behavior, from an evolutionary perspective. While evolutionary views tend to regard most individual differences as the result of either random genetic noise (evolutionary byproducts) and/or idiosyncrasies (for example, peer groups, education, neighborhoods, and chance encounters) rather than products of natural selection, EDP asserts that natural selection can favor the emergence of individual differences via \"adaptive developmental plasticity.\" From this perspective, human development follows alternative life-history strategies in response to environmental variability, rather than following one species-typical pattern of development.\n\nEDP is closely linked to the theoretical framework of evolutionary psychology (EP), but is also distinct from EP in several domains, including research emphasis (EDP focuses on adaptations of ontogeny, as opposed to adaptations of adulthood) and consideration of proximate ontogenetic and environmental factors (i.e., how development happens) in addition to more ultimate factors (i.e., why development happens), which are the focus of mainstream evolutionary psychology.\n\nAttachment theory, originally developed by John Bowlby, focuses on the importance of open, intimate, emotionally meaningful relationships. Attachment is described as a biological system or powerful survival impulse that evolved to ensure the survival of the infant. A child who is threatened or stressed will move toward caregivers who create a sense of physical, emotional and psychological safety for the individual. Attachment feeds on body contact and familiarity. Later Mary Ainsworth developed the Strange Situation protocol and the concept of the secure base.\n\nTheorists have proposed four types of attachment styles: secure, anxious-avoidant, anxious-resistant, and disorganized. Secure attachment is a healthy attachment between the infant and the caregiver. It is characterized by trust. Anxious-avoidant is an insecure attachment between an infant and a caregiver. This is characterized by the infant's indifference toward the caregiver. Anxious-resistant is an insecure attachment between the infant and the caregiver characterized by distress from the infant when separated and anger when reunited. Disorganized is an attachment style without a consistent pattern of responses upon return of the parent.\n\nA child can be hindered in its natural tendency to form attachments. Some babies are raised without the stimulation and attention of a regular caregiver or locked away under conditions of abuse or extreme neglect. The possible short-term effects of this deprivation are anger, despair, detachment, and temporary delay in intellectual development. Long-term effects include increased aggression, clinging behavior, detachment, psychosomatic disorders, and an increased risk of depression as an adult.\n\nAttachment style can affect the relationships between people. Attachment is established in early childhood and attachment continues into adulthood. An example of secure attachment continuing in adulthood would be when the person feels confident and is able to meet their own needs. An example of anxious attachment during adulthood is when the adult chooses a partner with anxious-avoidant attachment.\n\nA significant issue in developmental psychology is the relationship between innateness and environmental influence in regard to any particular aspect of development. This is often referred to as \"nature and nurture\" or nativism versus empiricism. A nativist account of development would argue that the processes in question are innate, that is, they are specified by the organism's genes.\n\nAn empiricist perspective would argue that those processes are acquired in interaction with the environment. Today developmental psychologists rarely take such polarised positions with regard to most aspects of development; rather they investigate, among many other things, the relationship between innate and environmental influences. One of the ways this relationship has been explored in recent years is through the emerging field of evolutionary developmental psychology.\n\nOne area where this innateness debate has been prominently portrayed is in research on language acquisition. A major question in this area is whether or not certain properties of human language are specified genetically or can be acquired through learning. The empiricist position on the issue of language acquisition suggests that the language input provides the necessary information required for learning the structure of language and that infants acquire language through a process of statistical learning. From this perspective, language can be acquired via general learning methods that also apply to other aspects of development, such as perceptual learning.\n\nThe nativist position argues that the input from language is too impoverished for infants and children to acquire the structure of language. Linguist Noam Chomsky asserts that, evidenced by the lack of sufficient information in the language input, there is a universal grammar that applies to all human languages and is pre-specified. This has led to the idea that there is a special cognitive module suited for learning language, often called the language acquisition device. Chomsky's critique of the behaviorist model of language acquisition is regarded by many as a key turning point in the decline in the prominence of the theory of behaviorism generally. But Skinner's conception of \"Verbal Behavior\" has not died, perhaps in part because it has generated successful practical applications.\n\nSince theorists believe that development is a smooth, continuous process, individuals gradually add more of the same types of skills throughout their lives. Other theorists, however, think that development takes place in discontinuous stages. People change rapidly and step up to a new level, and then change very little for a while. With each new step, the person shows interest and responds to the world qualitatively.\n\nThis issue involves the degree to which we become older renditions of our early experience or whether we develop into something different from who we were at an earlier point in development. It considers the extent to which early experiences ( especially infancy) or later experiences are the key determinants of a person's development.\n\nMost lifespan developmentalists, recognise that extreme positions are unwise. Therefore, the key to a comprehensive understanding of development at any stage requires the ·interaction of different factors and not only one.\n\nDevelopmental psychology is concerned not only with describing the characteristics of psychological change over time but also seeks to explain the principles and internal workings underlying these changes. Psychologists have attempted to better understand these factors by using models. Developmental models are sometimes computational, but they do not need to be.\n\nA model must simply account for the means by which a process takes place. This is sometimes done in reference to changes in the brain that may correspond to changes in behavior over the course of the development. Computational accounts of development often use either symbolic, connectionist (neural network), or dynamical systems models to explain the Mechanisms of Development.\n\nCognitive development is primarily concerned with the ways that infants and children acquire, develop, and use internal mental capabilities such as: problem-solving, memory, and language. Major topics in cognitive development are the study of language acquisition and the development of perceptual and motor skills. Piaget was one of the influential early psychologists to study the development of cognitive abilities. His theory suggests that development proceeds through a set of stages from infancy to adulthood and that there is an end point or goal.\n\nOther accounts, such as that of Lev Vygotsky, have suggested that development does not progress through stages, but rather that the developmental process that begins at birth and continues until death is too complex for such structure and finality. Rather, from this viewpoint, developmental processes proceed more continuously. Thus, development should be analyzed, instead of treated as a product to obtain.\n\nK. Warner Schaie has expanded the study of cognitive development into adulthood. Rather than being stable from adolescence, Schaie sees adults as progressing in the application of their cognitive abilities.\n\nModern cognitive development has integrated the considerations of cognitive psychology and the psychology of individual differences into the interpretation and modeling of development. Specifically, the neo-Piagetian theories of cognitive development showed that the successive levels or stages of cognitive development are associated with increasing processing efficiency and working memory capacity. These increases explain differences between stages, progression to higher stages, and individual differences of children who are the same-age and of the same grade-level. However, other theories have moved away from Piagetian stage theories, and are influenced by accounts of domain-specific information processing, which posit that development is guided by innate evolutionarily-specified and content-specific information processing mechanisms.\n\nDevelopmental psychologists who are interested in social development examine how individuals develop social and emotional competencies. For example, they study how children form friendships, how they understand and deal with emotions, and how identity develops. Research in this area may involve study of the relationship between cognition or cognitive development and social behavior.\n\nEmotional regulation or ER refers to an individual's ability to modulate emotional responses across a variety of contexts. In young children, this modulation is in part controlled externally, by parents and other authority figures. As children develop, they take on more and more responsibility for their internal state. Studies have shown that the development of ER is affected by the emotional regulation children observe in parents and caretakers, the emotional climate in the home, and the reaction of parents and caretakers to the child's emotions.\n\nMusic also has an influence on stimulating and enhancing the senses of a child through self-expression.\n\nA child's social and emotional development can be disrupted by motor coordination problems as evidenced by the environmental stress hypothesis. The environmental hypothesis explains how children with coordination problems and developmental coordination disorder are exposed to several psychosocial consequences which act as secondary stressors, leading to an increase in internalizing symptoms such as depression and anxiety. Motor coordination problems affect fine and gross motor movement as well as perceptual-motor skills. Secondary stressors commonly identified include the tendency for children with poor motor skills to be less likely to participate in organized play with other children and more likely to feel socially isolated.\n\nSocial and emotional development focuses on 5 keys areas: Self-Awareness, Self Management, Social Awareness, Relationship Skills and Responsible Decision Making.\n\nPhysical development concerns the physical maturation of an individual's body until it reaches the adult stature. Although physical growth is a highly regular process, all children differ tremendously in the timing of their growth spurts. Studies are being done to analyze how the differences in these timings affect and are related to other variables of developmental psychology such as information processing speed. Traditional measures of physical maturity using x-rays are less in practice nowadays, compared to simple measurements of body parts such as height, weight, head circumference, and arm span.\n\nA few other studies and practices with physical developmental psychology are the phonological abilities of mature 5- to 11-year-olds, and the controversial hypotheses of left-handers being maturationally delayed compared to right-handers. A study by Eaton, Chipperfield, Ritchot, and Kostiuk in 1996 found in three different samples that there was no difference between right- and left-handers.\n\nResearchers interested in memory development look at the way our memory develops from childhood and onward. According to Fuzzy-trace theory, we have two separate memory processes: verbatim and gist. These two traces begin to develop at different times as well as at a different pace. Children as young as 4 years-old have verbatim memory, memory for surface information, which increases up to early adulthood, at which point it begins to decline. On the other hand, our capacity for gist memory, memory for semantic information, increases up to early adulthood, at which point it is consistent through old age. Furthermore, our reliance on gist memory traces increases as we age.\n\nDevelopmental psychology employs many of the research methods used in other areas of psychology. However, infants and children cannot be tested in the same ways as adults, so different methods are often used to study their development.\n\nDevelopmental psychologists have a number of methods to study changes in individuals over time. Common research methods include systematic observation, including naturalistic observation or structured observation; self-reports, which could be clinical interviews or structured interviews; clinical or case study method; and ethnography or participant observation. These methods differ in the extent of control researchers impose on study conditions, and how they construct ideas about which variables to study. Every developmental investigation can be characterized in terms of whether its underlying strategy involves the \"experimental\", \"correlational\", or \"case study\" approach. The experimental method involves \"actual manipulation of various treatments, circumstances, or events to which the participant or subject is exposed; the \"experimental design\" points to cause-and-effect relationships. This method allows for strong inferences to be made of causal relationships between the manipulation of one or more independent variables and subsequent behavior, as measured by the dependent variable. The advantage of using this research method is that it permits determination of cause-and-effect relationships among variables. On the other hand, the limitation is that data obtained in an artificial environment may lack generalizability. The correlational method explores the relationship between two or more events by gathering information about these variables without researcher intervention. The advantage of using a correlational design is that it estimates the strength and direction of relationships among variables in the natural environment; however, the limitation is that it does not permit determination of cause-and-effect relationships among variables. The case study approach allows investigations to obtain an in-depth understanding of an individual participant by collecting data based on interviews, structured questionnaires, observations, and test scores. Each of these methods have its strengths and weaknesses but the experimental method when appropriate is the preferred method of developmental scientists because it provides a controlled situation and conclusions to be drawn about cause-and-effect relationships.\n\nMost developmental studies, regardless of whether they employ the experimental, correlational, or case study method, can also be constructed using research designs. Research designs are logical frameworks used to make key comparisons within research studies such as:\n\nIn a longitudinal study, a researcher observes many individuals born at or around the same time (a cohort) and carries out new observations as members of the cohort age. This method can be used to draw conclusions about which types of development are universal (or normative) and occur in most members of a cohort. As an example a longitudinal study of early literacy development examined in detail the early literacy experiences of one child in each of 30 families.\n\nResearchers may also observe ways that development varies between individuals, and hypothesize about the causes of variation in their data. Longitudinal studies often require large amounts of time and funding, making them unfeasible in some situations. Also, because members of a cohort all experience historical events unique to their generation, apparently normative developmental trends may, in fact, be universal only to their cohort.\n\nIn a cross-sectional study, a researcher observes differences between individuals of different ages at the same time. This generally requires fewer resources than the longitudinal method, and because the individuals come from different cohorts, shared historical events are not so much of a confounding factor. By the same token, however, cross-sectional research may not be the most effective way to study differences between participants, as these differences may result not from their different ages but from their exposure to \"different\" historical events.\n\nA third study design, the sequential design, combines both methodologies. Here, a researcher observes members of different birth cohorts at the same time, and then tracks all participants over time, charting changes in the groups. While much more resource-intensive, the format aids in a clearer distinction between what changes can be attributed to an individual or historical environment from those that are truly universal.\n\nBecause every method has some weaknesses, developmental psychologists rarely rely on one study or even one method to reach conclusions by finding consistent evidence from as many converging sources as possible.\n\nPrenatal development is of interest to psychologists investigating the context of early psychological development. The whole prenatal development involves three main stages: germinal stage, embryonic stage and fetal stage. Germinal stage begins at conception until 2 weeks; embryonic stage means the development from 2 weeks to 8 weeks; fetal stage represents 9 weeks until birth of the baby. The senses develop in the womb itself: a fetus can both see and hear by the second trimester (13 to 24 weeks of age). The sense of touch develops in the embryonic stage (5 to 8 weeks). Most of the brain's billions of neurons also are developed by the second trimester. Babies are hence born with some odor, taste and sound preferences, largely related to the mother's environment.\n\nSome primitive reflexes too arise before birth and are still present in newborns. One hypothesis is that these reflexes are vestigial and have limited use in early human life. Piaget's theory of cognitive development suggested that some early reflexes are building blocks for infant sensorimotor development. For example, the tonic neck reflex may help development by bringing objects into the infant's field of view.\n\nOther reflexes, such as the walking reflex appear to be replaced by more sophisticated voluntary control later in infancy. This may be because the infant gains too much weight after birth to be strong enough to use the reflex, or because the reflex and subsequent development are functionally different. It has also been suggested that some reflexes (for example the moro and walking reflexes) are predominantly adaptations to life in the womb with little connection to early infant development. Primitive reflexes reappear in adults under certain conditions, such as neurological conditions like dementia or traumatic lesions.\n\nUltrasound has shown that infants are capable of a range of movements in the womb, many of which appear to be more than simple reflexes. By the time they are born, infants can recognize and have a preference for their mother's voice suggesting some prenatal development of auditory perception. Prenatal development and birth complications may also be connected to neurodevelopmental disorders, for example in schizophrenia. With the advent of cognitive neuroscience, embryology and the neuroscience of prenatal development is of increasing interest to developmental psychology research.\n\nSeveral environmental agents—teratogens—can cause damage during the prenatal period. These include prescription and nonprescription drugs, illegal drugs, tobacco, alcohol, environmental pollutants, infectious disease agents such as the rubella virus and the toxoplasmosis parasite, maternal malnutrition, maternal emotional stress, and Rh factor blood incompatibility between mother and child. There are many statistics which prove the effects of the aforementioned substances. A leading example of this would be that, in America alone, approximately 100,000-375,000 'cocaine babies' are born on an annual basis. This is a result of an expectant mother abusing the drug while pregnant. 'Cocaine babies' are proven to have quite severe and lasting difficulties which persist throughout infancy and right throughout childhood. The drug also encourages behavioural problems in the affected children, as well as defects of various vital organs.\n\nFrom birth until the first year, the child is referred to as an infant. Developmental psychologists vary widely in their assessment of infant psychology, and the influence the outside world has upon it, but certain aspects are relatively clear.\n\nThe majority of a newborn infant's time is spent in sleep. At first, this sleep is evenly spread throughout the day and night, but after a couple of months, infants generally become diurnal.\n\nInfants can be seen to have six states, grouped into pairs:\n\nInfant perception is what a newborn can see, hear, smell, taste, and touch. These five features are better known as one's \"five senses\". Infants respond to stimuli differently in these different states.\n\n\nBabies are born with the ability to discriminate virtually all sounds of all human languages. Infants of around six months can differentiate between phonemes in their own language, but not between similar phonemes in another language. At this stage infants also start to babble, producing phonemes.\n\nPiaget suggested that an infant's perception and understanding of the world depended on their motor development, which was required for the infant to link visual, tactile and motor representations of objects. According to this view, it is through touching and handling objects that infants develop object permanence, the understanding that objects are solid, permanent, and continue to exist when out of sight.\nPiaget's sensorimotor stage comprised six sub-stages (see sensorimotor stages for more detail). In the early stages, development arises out of movements caused by primitive reflexes. Discovery of new behaviors results from classical and operant conditioning, and the formation of habits. From eight months the infant is able to uncover a hidden object but will persevere when the object is moved.\n\nPiaget came to his conclusion that infants lacked a complete understanding of object permanence before 18 months after observing infants' failure before this age to look for an object where it was last seen. Instead, infants continue to look for an object where it was first seen, committing the \"A-not-B error.\" Some researchers have suggested that before the age of eight to nine months, infants' inability to understand object permanence extends to people, which explains why infants at this age do not cry when their mothers are gone (\"Out of sight, out of mind\").\n\nIn the 1980s and 1990s, researchers have developed many new methods of assessing infants' understanding of the world with far more precision and subtlety than Piaget was able to do in his time. Since then, many studies based on these methods suggest that young infants understand far more about the world than first thought.\n\nBased on recent findings, some researchers (such as Elizabeth Spelke and Renee Baillargeon) have proposed that an understanding of object permanence is not learned at all, but rather comprises part of the innate cognitive capacities of our species.\n\nOther research has suggested that young infants in their first six months of life may possess an understanding of numerous aspects of the world around them, including:\n\n\nThere are critical periods in infancy and childhood during which development of certain perceptual, sensorimotor, social and language systems depends crucially on environmental stimulation. Feral children such as Genie, deprived of adequate stimulation, fail to acquire important skills and are unable to learn in later childhood. The concept of critical periods is also well-established in neurophysiology, from the work of Hubel and Wiesel among others.\n\nChildren with developmental delays (DD) are at heightened risk for developing clinically significant behavioral and emotional difficulties as compared to children with typical development (TD). However, nearly all studies comparing psychopathology in youth with DD employ TD control groups of the same chronological age (CA).This comorbidity of DD and a mental disorder is often referred to as dual diagnosis. Epidemiological studies indicate that 30–50% of youth with DD meet the clinical cutoff for behavioral and emotional problems and/or diagnosable mental disorder. Studies that include comparison samples of children with typical development (TD) highlight the considerable difference in risk for psychopathology, with the relative risk for youth with DD (to youth with TD) ranging from 2.8–4.1 to 1.\n\nInfants shift between ages of one and two to a developmental stage known as toddlerhood. In this stage, an infant's transition into toddlerhood is highlighted through self-awareness, developing maturity in language use, and presence of memory and imagination.\n\nDuring toddlerhood, babies begin learning how to walk, talk, and make decisions for themselves. An important characteristic of this age period is the development of language, where children are learning how to communicate and express their emotions and desires through the use of vocal sounds, babbling, and eventually words. Self-control also begins to develop. At this age, children take initiative to explore, experiment and learn from making mistakes. Caretakers who encourage toddlers to try new things and test their limits, help the child become autonomous, self-reliant, and confident. If the caretaker is overprotective or disapproving of independent actions, the toddler may begin to doubt their abilities and feel ashamed of the desire for independence. The child's autonomic development is inhibited, leaving them less prepared to deal with the world in the future. Toddlers also begin to identify themselves in gender roles, acting according to their perception of what a man or woman should do.\n\nSocially, the period of toddler-hood is commonly called the \"terrible twos\". Toddlers often use their new-found language abilities to voice their desires, but are often misunderstood by parents due to their language skills just beginning to develop. A person at this stage testing their independence is another reason behind the stage's infamous label. Tantrums in a fit of frustration are also common.\n\nErik Erikson divides childhood into four stages, each with its distinct social crisis:\n\nPlay (or preschool) ages 3–5.\nIn the earliest years, children are \"completely dependent on the care of others.\" Therefore, they develop a \"social relationship\" with their care givers and, later, with family members. During their preschool years (3-5), they \"enlarge their social horizons\" to include people outside the family.\n\nPreoperational and then operational thinking develops, which means actions are reversible, and egocentric thought diminishes.\n\nThe motor skills of preschoolers increase so they can do more things for themselves. They become more independent. No longer completely dependent on the care of others, the world of this age group expands. More people have a role in shaping their individual personalities. Preschoolers explore and question their world. For Jean Piaget, the child is \"\"a little scientist\" exploring and reflecting on these explorations to increase competence\" and this is done in \"a very independent way.\"\n\nPlay is a major activity for ages 3–5. For Piaget, through play \"a child reaches higher levels of cognitive development.\"\n\nIn their expanded world, children in the 3-5 age group attempt to find their own way. If this is done in a socially acceptable way, the child develops the initiative. If not, the child develops guilt. Children who develop \"guilt\" rather than \"initiative\" have failed Erikson's psychosocial crisis for the 3-5 age group.\n\nMiddle childhood ages 6–12.\nFor Erik Erikson, the psychosocial crisis during middle childhood is Industry vs. Inferiority which, if successfully met, instills a sense of Competency in the child.\n\nIn all cultures, middle childhood is a time for developing \"skills that will be needed in their society.\" School offers an arena in which children can gain a view of themselves as \"industrious (and worthy).\" They are \"graded for their school work and often for their industry.\" They can also develop industry outside of school in sports, games, and doing volunteer work. Children who achieve \"success in school or games might develop a feeling of competence.\"\n\nThe \"peril during this period is that feelings of inadequacy and inferiority will develop. Parents and teachers can \"undermine\" a child's development by failing to recognize accomplishments or being overly critical of a child's efforts.\nChildren who are \"encouraged and praised\" develop a belief in their competence. Lack of encouragement or ability to excel lead to \"feelings of inadequacy and inferiority\".\n\nThe Centers for Disease Control (the CDC) divides Middle Childhood into two stages, 6–8 years and 9–11 years, and gives \"developmental milestones for each stage.\"\n\n\"Middle Childhood (7-10).\"\nEntering elementary school, children in this age group begin to thinks about the future and their \"place in the world.\" Working with other students and wanting their friendship and acceptance become more important. This leads to \"more independence from parents and family.\" As students, they develop the mental and verbal skills \"to describe experiences and talk about thoughts and feelings\". They become less self-centered and show \"more concern for others\".\n\n\"Middle Childhood (9-11).\"\nFor children ages 9–11 \"friendships and peer relationships\" increase in strength, complexity, and importance. This results in greater \"peer pressure.\" They grow even less dependent on their families and they are challenged academically. To meet this challenge, they increase their attention span and learn to see other points of view.\n\nAdolescence is the period of life between the onset of puberty and the full commitment to an adult social role, such as worker, parent, and/or citizen. It is the period known for the formation of personal and social identity (see Erik Erikson) and the discovery of moral purpose (see William Damon). Intelligence is demonstrated through the logical use of symbols related to abstract concepts and formal reasoning. A return to egocentric thought often occurs early in the period. Only 35% develop the capacity to reason formally during adolescence or adulthood. (Huitt, W. and Hummel, J. January 1998)\n\nIt is divided into three parts, namely:\n\nThe adolescent unconsciously explores questions such as \"Who am I? Who do I want to be?\" Like toddlers, adolescents must explore, test limits, become autonomous, and commit to an identity, or sense of self. Different roles, behaviors and ideologies must be tried out to select an identity. Role confusion and inability to choose vocation can result from a failure to achieve a sense of identity through, for example, friends.\n\nEarly adulthood generally refers to the period between ages 18 to 25, and according to theorists such as Erik Erikson, is a stage where development is mainly focused on maintaining relationships. Examples include creating bond of intimacy, sustaining friendships, and ultimately making a family. Some theorists state that development of intimacy skills rely on the resolution of previous developmental stages. A sense of identity gained in the previous stages is also necessary for intimacy to develop. If this skill is not learned the alternative is alienation, isolation, a fear of commitment, and the inability to depend on others.\n\nA related framework for studying this part of the lifespan is that of emerging adulthood. Scholars of emerging adulthood, such as Jeffrey Arnett, are not necessarily interested in relationship development. Instead, this concept suggests that people transition after their teenage years into a period not characterized as relationship building and an overall sense of constancy with life, but with years of living with parents, phases of self-discovery, and experimentation.\n\nMiddle adulthood generally refers to the period between ages 25 to 69. During this period, middle-aged adults experience a conflict between generativity and stagnation. They may either feel a sense of contributing to society, the next generation, or their immediate community; or develop a sense of purposelessness.\n\nPhysically, the middle-aged experience a decline in muscular strength, reaction time, sensory keenness, and cardiac output. Also, women experience menopause and a sharp drop in the hormone estrogen. Men experience an equivalent endocrine system event to menopause. Andropause in males is a hormone fluctuation with physical and psychological effects that can be similar to those seen in menopausal females. As men age lowered testosterone levels can contribute to mood swings and a decline in sperm count. Sexual responsiveness can also be affected, including delays in erection and longer periods of penile stimulation required to achieve ejaculation.\n\nThe World Health Organization finds \"no general agreement on the age at which a person becomes old.\" Most \"developed countries\" set the age as 60 or 65. However, in developing countries inability to make \"active contribution\" to society, not chronological age, marks the beginning of old age. According to Erikson's stages of psychosocial development, old age is the stage in which individuals assess the quality of their lives. In reflecting on their lives, people in this age group develop a feeling of integrity if deciding that their lives were successful or a feeling of despair if evaluation of one's life indicates a failure to achieve goals.\n\nPhysically, older people experience a decline in muscular strength, reaction time, stamina, hearing, distance perception, and the sense of smell. They also are more susceptible to diseases such as cancer and pneumonia due to a weakened immune system. Programs aimed at balance, muscle strength, and mobility have been shown to reduce disability among mildly (but not more severely) disabled elderly.\n\nSexual expression depends in large part upon the emotional and physical health of the individual. Many older adults continue to be sexually active and satisfied with their sexual activity.\n\nMental disintegration may also occur, leading to dementia or ailments such as Alzheimer's disease. It is generally believed that crystallized intelligence increases up to old age, while fluid intelligence decreases with age. Whether or not normal intelligence increases or decreases with age depends on the measure and study. Longitudinal studies show that perceptual speed, inductive reasoning, and spatial orientation decline. An article on adult cognitive development reports that cross-sectional studies show that \"some abilities remained stable into early old age.\"\n\nParenting variables alone have typically accounted for 20 to 50 percent of the variance in child outcomes.\n\nAll parents have their own parenting styles. Parenting styles, according to Kimberly Kopko, are \"based upon two aspects of parenting behavior; control and warmth. Parental control refers to the degree to which parents manage their children's behavior. Parental warmth refers to the degree to which parents are accepting and responsive to their children's behavior.\"\n\nThe following parenting styles have been described in the child development literature:\n\n\nParenting roles in child development have typically focused on the role of the mother. Recent literature, however, has looked toward the father as having an important role in child development. Affirming a role for fathers, studies have shown that children as young as 15 months benefit significantly from substantial engagement with their father. In particular, a study in the U.S. and New Zealand found the presence of the natural father was the most significant factor in reducing rates of early sexual activity and rates of teenage pregnancy in girls. Furthermore, another argument is that neither a mother nor a father is actually essential in successful parenting, and that single parents as well as homosexual couples can support positive child outcomes. According to this set of research, children need at least one consistently responsible adult with whom the child can have a positive emotional connection. Having more than one of these figures contributes to a higher likelihood of positive child outcomes.\n\nAnother parental factor often debated in terms of its effects on child development is divorce. Divorce in itself is not a determining factor of negative child outcomes. In fact, the majority of children from divorcing families fall into the normal range on measures of psychological and cognitive functioning. A number of mediating factors play a role in determining the effects divorce has on a child, for example, divorcing families with young children often face harsher consequences in terms of demographic, social, and economic changes than do families with older children. Positive coparenting after divorce is part of a pattern associated with positive child coping, while hostile parenting behaviors lead to a destructive pattern leaving children at risk. Additionally, direct parental relationship with the child also affects the development of a child after a divorce. Overall, protective factors facilitating positive child development after a divorce are maternal warmth, positive father-child relationship, and cooperation between parents.\n\n\nProminent journals in developmental psychology include:\n\n"}
{"id": "22851883", "url": "https://en.wikipedia.org/wiki?curid=22851883", "title": "Don't Be Such a Scientist", "text": "Don't Be Such a Scientist\n\nDon't Be Such A Scientist: Talking Substance in an Age of Style is a book published by Island Press written by scientist-turned-filmmaker Randy Olson, Ph.D. which arises from a talk of the same title Olson gave to science audiences at universities and museums for five years preceding its publication. The focus of the book is the challenge scientists face in communicating to the general public in an age of information-overload. Olson draws on his two careers, first as a marine biologist who achieved a tenured professorship, then his second career which began when he then resigned to attend film school and acting classes, eventually becoming an independent feature filmmaker. Among other topics, the book addresses the role of spontaneity, storytelling, and likeability in the mass communication of science.\n\n"}
{"id": "39383501", "url": "https://en.wikipedia.org/wiki?curid=39383501", "title": "Elliott formula", "text": "Elliott formula\n\nThe Elliott formula describes analytically, or with few adjustable parameters such as the dephasing constant, the light absorption or emission spectra of solids. It was originally derived by Roger James Elliott to describe linear absorption based on properties of a single electron–hole pair. The analysis can be extended to a many-body investigation with full predictive powers when all parameters are computed microscopically using, e.g., the semiconductor Bloch equations (abbreviated as SBEs) or the semiconductor luminescence equations (abbreviated as SLEs).\n\nOne of the most accurate theories of semiconductor absorption and photoluminescence is provided by the SBEs and SLEs, respectively. Both of them are systematically derived starting from the many-body/quantum-optical system Hamiltonian and fully describe the resulting quantum dynamics of optical and quantum-optical observables such as optical polarization (SBEs) and photoluminescence intensity (SLEs). All relevant many-body effects can be systematically included by using various techniques such as the cluster-expansion approach.\n\nBoth the SBEs and SLEs contain an identical homogeneous part driven either by a classical field (SBEs) or by a spontaneous-emission source (SLEs). This homogeneous part yields an eigenvalue problem that can be expressed through the generalized Wannier equation that can be solved analytically in special cases. In particular, the low-density Wannier equation is analogous to bound solutions of the hydrogen problem of quantum mechanics. These are often referred to as exciton solutions and they formally describe Coulombic binding by oppositely charged electrons and holes. The actual physical meaning of excitonic states is discussed further in connection with the SBEs and SLEs. The exciton eigenfunctions are denoted by formula_1 where formula_2 labels the exciton state with eigenenergy formula_3 and formula_4 is the crystal momentum of charge carriers in the solid.\n\nThese exciton eigenstates provide valuable insight to SBEs and SLEs, especially, when one analyses the linear semiconductor absorption spectrum or photoluminescence at steady-state conditions. One simply uses the constructed eigenstates to diagonalize the homogeneous parts of the SBEs and SLEs. Under the steady-state conditions, the resulting equations can be solved analytically when one further approximates dephasing due to higher-order many-body effects. When such effects are fully included, one must resort to a numeric approach. After the exciton states are obtained, one can eventually express the linear absorption and steady-state photoluminescence analytically.\n\nThe same approach can be applied to compute absorption spectrum for fields that are in the terahertz (abbreviated as THz) range of electromagnetic radiation. Since the THz-photon energy lies within the meV range, it is mostly resonant with the many-body states, not the interband transitions that are typically in the eV range. Technically, the THz investigations are an extension of the ordinary SBEs and/or involve solving the dynamics of two-particle correlations explicitly. Like for the optical absorption and emission problem, one can diagonalize the homogeneous parts that emerge analytically with the help of the exciton eigenstates. Once the diagonalization is completed, one can then compute the THz absorption analytically.\nAll of these derivations rely on the steady-state conditions and the analytic knowledge of the exciton states. Furthermore, the effect of further many-body contributions, such as the excitation-induced dephasing, can be included microscopically to the Wannier solver, which removes the need to introduce phenomenological dephasing constant, energy shifts, or screening of the Coulomb interaction.\n\nLinear absorption of broadband weak optical probe can then be expressed as\n\nwhere formula_5 is the probe-photon energy, formula_6 is the oscillator strength of the exciton state formula_2, and formula_8 is the dephasing constant associated with the exciton state formula_2. For a phenomenological description, formula_8 can be used as a single fit parameter, i.e., formula_11. However, a full microscopic computation generally produces formula_12 that depends on both exciton index formula_2 and photon frequency. As a general tendency, formula_12 increases for elevated formula_3 while the formula_16 dependence is often weak.\n\nEach of the exciton resonances can produce a peak to the absorption spectrum when the photon energy matches with formula_3. For direct-gap semiconductors, the oscillator strength is proportional to the product of dipole-matrix element squared and formula_18 that vanishes for all states except for those that are spherically symmetric. In other words, formula_6 is nonvanishing only for the formula_20-like states, following the quantum-number convention of the hydrogen problem. Therefore, optical spectrum of direct-gap semiconductors produces an absorption resonance only for the formula_20-like state. The width of the resonance is determined by the corresponding dephasing constant.\n\nIn general, the exciton eigen energies consist of a series of bound states that emerge energetically well below the fundamental bandgap energy and a continuum of unbound states that appear for energies above the bandgap. Therefore, a typical semiconductor's low-density absorption spectrum shows a series of exciton resonances and then a continuum-absorption tail. For realistic situations, formula_8 increases more rapidly than the exciton-state spacing so that one typically resolves only few lowest exciton resonances in actual experiments.\n\nThe concentration of charge carriers influence the shape of the absorption spectrum considerably. For high enough densities, all formula_3 energies correspond to continuum states and some of the oscillators strengths may become negative-valued due to the Pauli-blocking effect. Physically, this can be understood as the elementary property of Fermions; if a given electronic state is already excited it cannot be excited a second time due to the Pauli exclusion among Fermions. Therefore, the corresponding electronic states can produce only photon emission that is seen as negative absorption, i.e., gain that is the prerequisite to realizing semiconductor lasers.\n\nEven though one can understand the principal behavior of semiconductor absorption on the basis of the Elliott formula, detailed predictions of the exact formula_3, formula_6, and formula_12 requires a full many-body computation already for moderate carrier densities.\n\nAfter the semiconductor becomes electronically excited, the carrier system relaxes into a quasiequilibrium. At the same time, vacuum-field fluctuations trigger spontaneous recombination of electrons and holes (electronic vacancies) via spontaneous emission of photons. At quasiequilibrium, this yields a steady-state photon flux emitted by the semiconductor. By starting from the SLEs, the steady-state photoluminescence (abbreviated as PL) can be cast into the form\n\nthat is very similar to the Elliott formula for the optical absorption. As a major difference, the numerator has a new contribution – the spontaneous-emission source\n\nformula_27\n\nthat contains electron and hole distributions formula_28 and formula_29, respectively, where formula_30 is the carrier momentum. Additionally, formula_31 contains also a direct contribution from exciton populations formula_32 that describes truly bound electron–hole pairs.\n\nThe formula_33 term defines the probability to find an electron and a hole with same formula_34. Such a form is expected for a probability of two uncorrelated events to occur simultaneously at a desired formula_34 value. Therefore, formula_33 is the spontaneous-emission source originating from uncorrelated electron–hole plasma. The possibility to have truly correlated electron–hole pairs is defined by a two-particle exciton correlation formula_32; the corresponding probability is directly proportional to the correlation. Nevertheless, both the presence of electron–hole plasma and excitons can equivalently induce the spontaneous emission. A further discussion of the relative weight and nature of plasma vs. exciton sources is presented in connection with the SLEs.\n\nLike for the absorption, a direct-gap semiconductor emits light only at the resonances corresponding to the formula_20-like states. As a typical trend, a quasiequilibrium emission is strongly peaked around the 1\"s\" resonance because formula_31 is usually largest for the formula_40 ground state. This emission peak often remains well below the fundamental bandgap energy even at the high excitations where all states are continuum states. This demonstrates that semiconductors are often subjects to massive Coulomb-induced renormalizations even when the system appears to have only electron–hole plasma states as emission resonances. To make an accurate prediction of the exact position and shape at elevated carrier densities, one must resort to the full SLEs.\n\nAs discussed above, it is often meaningful to tune the electromagnetic field to be resonant with the transitions between two many-body states. For example, one can follow how a bound exciton is excited from its 1\"s\" ground state to a 2\"p\" state. In several semiconductor systems, one needs THz fields to induce such transitions. By starting from a steady-state configuration of electron–hole correlations, the diagonalization of THz-induced dynamics yields a THz absorption spectrum\n\n(\\omega) = \\mathrm{Im}\\left[ \\frac{\\sum_{\\nu, \\lambda} S^{\\nu, \\lambda} (\\omega) \\Delta N_{\\nu,\\lambda} - \\left[ S^{\\nu, \\lambda}(-\\omega) \\Delta N_{\\nu,\\lambda}\\right]^{\\star} }{ \\omega (\\hbar \\omega + \\mathrm{i} \\gamma(\\omega))} \\right]\\;.\n</math>\n\nIn this notation, the diagonal contributions formula_41 determine the population of formula_2 excitons. The off-diagonal formula_43 elements formally determine transition amplitudes between two exciton states formula_44 and formula_45. For elevated densities, formula_43 build up spontaneously and they describe correlated electron–hole plasma that is a state where electrons and holes move with respect to each other without forming bound pairs.\n\nIn contrast to optical absorption and photoluminescence, THz absorption may involve all exciton states. This can be seen from the spectral response function\n\nformula_47\n\nthat contains the current-matrix elements formula_48 between two exciton states. The unit vector formula_49 is determined by the direction of the THz field. This leads to dipole selection rules among exciton states, in full analog to the atomic dipole selection rules. Each allowed transition produces a resonance in formula_50 and the resonance width is determined by a dephasing constant formula_51 that generally depends on exciton states involved and the THz frequency formula_16. The THz response also contains formula_53 that stems from the decay constant of macroscopic THz currents.\n\nIn contrast to optical and photoluminescence spectroscopy, THz absorption can directly measure the presence of exciton populations in full analogy to atomic spectroscopy. For example, the presence of a pronounced 1\"s\"-to-2\"p\" resonance in THz absorption uniquely identifies the presence of excitons as detected experimentally in Ref. As a major difference to atomic spectroscopy, semiconductor resonances contain a strong excitation-induced dephasing that produces much broader resonances than in atomic spectroscopy. In fact, one typically can resolve only one 1\"s\"-to-2\"p\" resonance because the dephasing constant formula_54 is broader than energetic spacing of n-\"p\" and (n+1)-\"p\" states making 1\"s\"-to-n-\"p\" and 1\"s\"-to-(n+1)\"p\" resonances merge into one asymmetric tail.\n\n\n"}
{"id": "11750132", "url": "https://en.wikipedia.org/wiki?curid=11750132", "title": "GeoJournal", "text": "GeoJournal\n\nGeoJournal is a peer-reviewed international academic journal on all aspects of geography founded in 1977. Twelve issues (three volumes) a year were published by Springer Netherlands (formerly Kluwer) until December 2009 and can be accessed via SpringerLink. Starting February 2010, \"GeoJournal\" was relaunched as an international journal for spatially integrated social sciences and humanities with six issues a year. The journal's editor-in-chief is currently Daniel Z. Sui (Center for Urban and Regional Analysis, Department of Geography, The Ohio State University).\n\n"}
{"id": "4786644", "url": "https://en.wikipedia.org/wiki?curid=4786644", "title": "His Religion and Hers", "text": "His Religion and Hers\n\nHis Religion And Hers is a book written by Charlotte Perkins Gilman in 1922, after she had moved with her husband from New York City to Norwich, Connecticut. In the book, she planned a religion freed from the dictates of oppressive patriarchal instincts.\n"}
{"id": "31485534", "url": "https://en.wikipedia.org/wiki?curid=31485534", "title": "ISCB Senior Scientist Award", "text": "ISCB Senior Scientist Award\n\nThe ISCB Accomplishment by a Senior Scientist Award is an annual prize awarded by the International Society for Computational Biology for contributions to the field of computational biology.\n\n"}
{"id": "32190088", "url": "https://en.wikipedia.org/wiki?curid=32190088", "title": "International Association for Vegetation Science", "text": "International Association for Vegetation Science\n\nThe International Association for Vegetation Science (IAVS) promotes contact between scientists and others interested in the study of vegetation ecology, promotes research and publication of research results. In 1939 the International Phytosociological Society (IPS) was founded, with its headquarters in Montpellier, France. After the Second World War it was reconstituted as the Internationale Vereinigung für Vegetationskunde (IVV), which adopted a constitution at the International Botanical Congress of 1954. The current name was adopted in 1981–82.\n\nThe society publishes:\n\nThe Alexander von Humboldt Medal is a prize awarded biennially from 2011 onwards by the association. The award is intended to honor scientists who have contributed greatly to the intellectual development and advancement of vegetation science and plant community ecology. Honorary membership is also bestowed by the society.\n\n\n"}
{"id": "5835763", "url": "https://en.wikipedia.org/wiki?curid=5835763", "title": "International Society for the Study of Individual Differences", "text": "International Society for the Study of Individual Differences\n\nThe International Society for the Study of Individual Differences (ISSID) is a scientific society founded in 1983 that fosters research on the measurement, structure, dynamics and biological bases of individual differences in temperament, intelligence, attitudes, and abilities. Its first president (and one of its founders) was Hans Eysenck. The society investigates the major dimensions of individual differences in the context of experimental, physiological, pharmacological, clinical, medical, genetical, statistical, and social psychology. \n\nISSID holds \"Personality and Individual Differences\" (\"PAID\") as its official scientific journal and hosts a conference on individual differences every other year. The current president is Philip Corr, professor of psychology at City University London. \n\n"}
{"id": "42261", "url": "https://en.wikipedia.org/wiki?curid=42261", "title": "Irrigation", "text": "Irrigation\n\nIrrigation is the application of controlled amounts of water to plants at needed intervals. Irrigation helps to grow agricultural crops, maintain landscapes, and revegetate disturbed soils in dry areas and during periods of less than average rainfall. Irrigation also has other uses in crop production, including frost protection, suppressing weed growth in grain fields and preventing soil consolidation. In contrast, agriculture that relies only on direct rainfall is referred to as rain-fed or dry land farming.\n\nIrrigation systems are also used for cooling livestock, dust suppression, disposal of sewage, and in mining. Irrigation is often studied together with drainage, which is the removal of surface and sub-surface water from a given area.\nIrrigation has been a central feature of agriculture for over 5,000 years and is the product of many cultures. Historically, it was the basis for economies and societies across the globe, from Asia to the Southwestern United States.\n\nArchaeological investigation has found evidence of irrigation where natural rainfall was insufficient to support crops for rainfed agriculture.\n\nIrrigation was used as a means of manipulation of water in the alluvial plains of the Indus valley civilization, the application of it is estimated to have begun around 4500 BC and drastically increased the size and prosperity of their agricultural settlements.Sophisticated irrigation and water storage systems were developed by the Indus Valley Civilization, including artificial reservoirs at Girnar dated to 3000 BCE, and an early canal irrigation system from c. 2600 BCE. Large scale agriculture was practiced and an extensive network of canals was used for the purpose of irrigation.\n\n\"Perennial irrigation\" was practiced in the Mesopotamian plain whereby crops were regularly watered throughout the growing season by coaxing water through a matrix of small channels formed in the field. Ancient Egyptians practiced \"Basin irrigation\" using the flooding of the Nile to inundate land plots which had been surrounded by dykes. The flood water was held until the fertile sediment had settled before the surplus was returned to the watercourse. There is evidence of the ancient Egyptian pharaoh Amenemhet III in the twelfth dynasty (about 1800 BCE) using the natural lake of the Faiyum Oasis as a reservoir to store surpluses of water for use during the dry seasons. The lake swelled annually from flooding of the Nile.\n\nThe Ancient Nubians developed a form of irrigation by using a waterwheel-like device called a \"sakia\". Irrigation began in Nubia some time between the third and second millennium BCE. It largely depended upon the flood waters that would flow through the Nile River and other rivers in what is now the Sudan.\nIn sub-Saharan Africa irrigation reached the Niger River region cultures and civilizations by the first or second millennium BCE and was based on wet season flooding and water harvesting.\n\n\"Terrace irrigation\" is evidenced in pre-Columbian America, early Syria, India, and China. In the Zana Valley of the Andes Mountains in Peru, archaeologists found remains of three irrigation canals radiocarbon dated from the 4th millennium BCE, the 3rd millennium BCE and the 9th century CE. These canals are the earliest record of irrigation in the New World. Traces of a canal possibly dating from the 5th millennium BCE were found under the 4th millennium canal.\n\nAncient Persia (modern day Iran) used irrigation as far back as the 6th millennium BCE to grow barley in areas where natural rainfall was insufficient. The Qanats, developed in ancient Persia in about 800 BCE, are among the oldest known irrigation methods still in use today. They are now found in Asia, the Middle East and North Africa. The system comprises a network of vertical wells and gently sloping tunnels driven into the sides of cliffs and steep hills to tap groundwater. The noria, a water wheel with clay pots around the rim powered by the flow of the stream (or by animals where the water source was still), was first brought into use at about this time by Roman settlers in North Africa. By 150 BCE the pots were fitted with valves to allow smoother filling as they were forced into the water.\n\nThe irrigation works of ancient Sri Lanka, the earliest dating from about 300 BCE, in the reign of King Pandukabhaya and under continuous development for the next thousand years, were one of the most complex irrigation systems of the ancient world. In addition to underground canals, the Sinhalese were the first to build completely artificial reservoirs to store water. These reservoirs and canal systems were used primarily to irrigate paddy fields which require a lot of water to cultivate. Most of these irrigation systems still exist undamaged up to now, in Anuradhapura and Polonnaruwa, because of the advanced and precise engineering. The system was extensively restored and further extended during the reign of King Parakrama Bahu (1153–1186 CE).\n\nThe oldest known hydraulic engineers of China were Sunshu Ao (6th century BCE) of the Spring and Autumn period and Ximen Bao (5th century BCE) of the Warring States period, both of whom worked on large irrigation projects. In the Sichuan region belonging to the State of Qin of ancient China, the Dujiangyan Irrigation System devised by the Qin Chinese hydrologist and irrigation engineer Li Bing was built in 256 BCE to irrigate a vast area of farmland that today still supplies water. By the 2nd century AD, during the Han Dynasty, the Chinese also used chain pumps that lifted water from a lower elevation to a higher one. These were powered by manual foot pedal, hydraulic waterwheels, or rotating mechanical wheels pulled by oxen. The water was used for public works of providing water for urban residential quarters and palace gardens, but mostly for irrigation of farmland canals and channels in the fields.\n\nIn 15th century Korea, the world's first rain gauge, \"uryanggye\" (Korean:우량계), was invented in 1441. The inventor was Jang Yeong-sil, a Korean engineer of the Joseon Dynasty, under the active direction of the king, Sejong the Great. It was installed in irrigation tanks as part of a nationwide system to measure and collect rainfall for agricultural applications. With this instrument, planners and farmers could make better use of the information gathered in the survey.\n\nThe earliest agricultural irrigation canal system known in the U.S. dates to between 1200 B.C. and 800 B.C. and was discovered in Marana, Arizona (adjacent to Tucson) in 2009. The irrigation canal system predates the Hohokam culture by two thousand years and belongs to an unidentified culture. In North America, the Hohokam were the only culture known to rely on irrigation canals to water their crops, and their irrigation systems supported the largest population in the Southwest by AD 1300. The Hohokam constructed an assortment of simple canals combined with weirs in their various agricultural pursuits. Between the 7th and 14th centuries, they also built and maintained extensive irrigation networks along the lower Salt and middle Gila rivers that rivaled the complexity of those used in the ancient Near East, Egypt, and China. These were constructed using relatively simple excavation tools, without the benefit of advanced engineering technologies, and achieved drops of a few feet per mile, balancing erosion and siltation. The Hohokam cultivated varieties of cotton, tobacco, maize, beans and squash, as well as harvested an assortment of wild plants. Late in the Hohokam Chronological Sequence, they also used extensive dry-farming systems, primarily to grow agave for food and fiber. Their reliance on agricultural strategies based on canal irrigation, vital in their less than hospitable desert environment and arid climate, provided the basis for the aggregation of rural populations into stable urban centers.\n\nIn year 2000, the total fertile land was 2,788,000 km² (689 million acres) and it was equipped with irrigation infrastructure worldwide. About 68% of this area is in Asia, 17% in the Americas, 9% in Europe, 5% in Africa and 1% in Oceania. The largest contiguous areas of high irrigation density are found:\n\nSmaller irrigation areas are spread across almost all populated parts of the world.\n\nOnly eight years later, in 2008, the area of irrigated land had increased to an estimated total of 3,245,566 km² (802 million acres), which is nearly the size of India.\n\nThere are several methods of irrigation. They vary in how the water is supplied to the plants. The goal is to apply the water to the plants as uniformly as possible, so that each plant has the amount of water it needs, neither too much nor too little.\n\nSurface irrigation is the oldest form of irrigation and has been in use for thousands of years. In \"surface\" (\"furrow\", \"flood\", or \"level basin\") irrigation systems, water moves across the surface of an agricultural lands, in order to wet it and infiltrate into the soil. Surface irrigation can be subdivided into furrow,\" borderstrip or basin irrigation\". It is often called \"flood irrigation\" when the irrigation results in flooding or near flooding of the cultivated land. Historically, this has been the most common method of irrigating agricultural land and is still used in most parts of the world.\n\nWhere water levels from the irrigation source permit, the levels are controlled by dikes, usually plugged by soil. This is often seen in terraced rice fields (rice paddies), where the method is used to flood or control the level of water in each distinct field. In some cases, the water is pumped, or lifted by human or animal power to the level of the land. The water application efficiency of surface irrigation is typically lower than other forms of irrigation.\n\nSurface irrigation is even used to water landscapes in certain areas, for example, in and around Phoenix, Arizona. The irrigated area is surrounded by a berm and the water is delivered according to a schedule set by a local irrigation district.\n\n\"Micro-irrigation\", sometimes called localized irrigation, low volume irrigation, or trickle irrigation is a system where water is distributed under low pressure through a piped network, in a pre-determined pattern, and applied as a small discharge to each plant or adjacent to it. Traditional drip irrigation using individual emitters, subsurface drip irrigation (SDI), micro-spray or micro-sprinkler irrigation, and mini-bubbler irrigation all belong to this category of irrigation methods.\n\nDrip (or micro) irrigation, also known as trickle irrigation, functions as its name suggests. In this system water falls drop by drop just at the position of roots. Water is delivered at or near the root zone of plants, drop by drop. This method can be the most water-efficient method of irrigation, if managed properly, evaporation and runoff are minimized. The field water efficiency of drip irrigation is typically in the range of 80 to 90 percent when managed correctly.\n\nIn modern agriculture, drip irrigation is often combined with plastic mulch, further reducing evaporation, and is also the means of delivery of fertilizer. The process is known as \"fertigation\".\n\nDeep percolation, where water moves below the root zone, can occur if a drip system is operated for too long or if the delivery rate is too high. Drip irrigation methods range from very high-tech and computerized to low-tech and labor-intensive. Lower water pressures are usually needed than for most other types of systems, with the exception of low energy center pivot systems and surface irrigation systems, and the system can be designed for uniformity throughout a field or for precise water delivery to individual plants in a landscape containing a mix of plant species. Although it is difficult to regulate pressure on steep slopes, pressure compensating emitters are available, so the field does not have to be level. High-tech solutions involve precisely calibrated emitters located along lines of tubing that extend from a computerized set of valves.\n\nIn \"sprinkler\" or overhead irrigation, water is piped to one or more central locations within the field and distributed by overhead high-pressure sprinklers or guns. A system using sprinklers, sprays, or guns mounted overhead on permanently installed risers is often referred to as a \"solid-set\" irrigation system. Higher pressure sprinklers that rotate are called \"rotors\" and are driven by a ball drive, gear drive, or impact mechanism. Rotors can be designed to rotate in a full or partial circle. Guns are similar to rotors, except that they generally operate at very high pressures of 275 to 900 kPa (40 to 130 psi) and flows of 3 to 76 L/s (50 to 1200 US gal/min), usually with nozzle diameters in the range of 10 to 50 mm (0.5 to 1.9 in). Guns are used not only for irrigation, but also for industrial applications such as dust suppression and logging.\n\nSprinklers can also be mounted on moving platforms connected to the water source by a hose. Automatically moving wheeled systems known as \"traveling sprinklers\" may irrigate areas such as small farms, sports fields, parks, pastures, and cemeteries unattended. Most of these use a length of polyethylene tubing wound on a steel drum. As the tubing is wound on the drum powered by the irrigation water or a small gas engine, the sprinkler is pulled across the field. When the sprinkler arrives back at the reel the system shuts off. This type of system is known to most people as a \"waterreel\" traveling irrigation sprinkler and they are used extensively for dust suppression, irrigation, and land application of waste water.\n\nOther travelers use a flat rubber hose that is dragged along behind while the sprinkler platform is pulled by a cable.\n\nCenter pivot irrigation is a form of sprinkler irrigation utilising several segments of pipe (usually galvanized steel or aluminium) joined together and supported by trusses, mounted on wheeled towers with sprinklers positioned along its length.\nThe system moves in a circular pattern and is fed with water from the pivot point at the center of the arc. These systems are found and used in all parts of the world and allow irrigation of all types of terrain. Newer systems have drop sprinkler heads as shown in the image that follows.\n\nA \"series of pipes, each with a wheel\" of about 1.5 m diameter permanently affixed to its midpoint, and sprinklers along its length, are coupled together. Water is supplied at one end using a large hose. After sufficient irrigation has been applied to one strip of the field, the hose is removed, the water drained from the system, and the assembly rolled either by hand or with a purpose-built mechanism, so that the sprinklers are moved to a different position across the field. The hose is reconnected. The process is repeated in a pattern until the whole field has been irrigated.\n\nThis system is less expensive to install than a center pivot, but much more labor-intensive to operate – it does not travel automatically across the field: it applies water in a stationary strip, must be drained, and then rolled to a new strip. Most systems use 100 or 130 mm (4 or 5 inch) diameter aluminum pipe. The pipe doubles both as water transport and as an axle for rotating all the wheels. A drive system (often found near the centre of the wheel line) rotates the clamped-together pipe sections as a single axle, rolling the whole wheel line. Manual adjustment of individual wheel positions may be necessary if the system becomes misaligned.\n\nWheel line systems are limited in the amount of water they can carry, and limited in the height of crops that can be irrigated. One useful feature of a lateral move system is that it consists of sections that can be easily disconnected, adapting to field shape as the line is moved. They are most often used for small, rectilinear, or oddly-shaped fields, hilly or mountainous regions, or in regions where labor is inexpensive.\n\nA lawn sprinkler system is permanently installed, as opposed to a hose-end sprinkler, which is portable. Sprinkler systems are installed in residential lawns, in commercial landscapes, for churches and schools, in public parks and cemeteries, and on golf courses. Most of the components of these irrigation systems are hidden under ground, since aesthetics are important in a landscape. A typical lawn sprinkler system will consist of one or more zones, limited in size by the capacity of the water source. Each zone will cover a designated portion of the landscape. Sections of the landscape will usually be divided by microclimate, type of plant material, and type of irrigation equipment. A landscape irrigation system may also include zones containing drip irrigation, bubblers, or other types of equipment besides sprinklers.\n\nAlthough manual systems are still used, most lawn sprinkler systems may be operated automatically using an irrigation controller, sometimes called a clock or timer. Most automatic systems employ electric solenoid valves. Each zone has one or more of these valves that are wired to the controller. When the controller sends power to the valve, the valve opens, allowing water to flow to the sprinklers in that zone.\n\nThere are two main types of sprinklers used in lawn irrigation, pop-up spray heads and rotors. Spray heads have a fixed spray pattern, while rotors have one or more streams that rotate. Spray heads are used to cover smaller areas, while rotors are used for larger areas. Golf course rotors are sometimes so large that a single sprinkler is combined with a valve and called a 'valve in head'. When used in a turf area, the sprinklers are installed with the top of the head flush with the ground surface. When the system is pressurized, the head will pop up out of the ground and water the desired area until the valve closes and shuts off that zone. Once there is no more pressure in the lateral line, the sprinkler head will retract back into the ground. In flower beds or shrub areas, sprinklers may be mounted on above ground risers or even taller pop-up sprinklers may be used and installed flush as in a lawn area.\n\nThere are many types of hose-end sprinklers. Many of them are smaller versions of larger agricultural and landscape sprinklers, sized to work with a typical garden hose. Some have a spiked base allowing them to be temporarily stuck in the ground, while others have a sled base designed to be dragged while attached to the hose.\n\nSubirrigation has been used for many years in field crops in areas with high water tables. It is a method of artificially raising the water table to allow the soil to be moistened from below the plants' root zone. Often those systems are located on permanent grasslands in lowlands or river valleys and combined with drainage infrastructure. A system of pumping stations, canals, weirs and gates allows it to increase or decrease the water level in a network of ditches and thereby control the water table.\n\nSubirrigation is also used in commercial greenhouse production, usually for potted plants. Water is delivered from below, absorbed upwards, and the excess collected for recycling. Typically, a solution of water and nutrients floods a container or flows through a trough for a short period of time, 10–20 minutes, and is then pumped back into a holding tank for reuse. Sub-irrigation in greenhouses requires fairly sophisticated, expensive equipment and management. Advantages are water and nutrient conservation, and labor savings through reduced system maintenance and automation. It is similar in principle and action to subsurface basin irrigation.\n\nAnother type of subirrigation is the self-watering container, also known as a sub-irrigated planter. This consists of a planter suspended over a reservoir with some type of wicking material such as a polyester rope. The water is drawn up the wick through capillary action.\n\nSubsurface Textile Irrigation (SSTI) is a technology designed specifically for subirrigation in all soil textures from desert sands to heavy clays. A typical subsurface textile irrigation system has an impermeable base layer (usually polyethylene or polypropylene), a drip line running along that base, a layer of geotextile on top of the drip line and, finally, a narrow impermeable layer on top of the geotextile (see diagram). Unlike standard drip irrigation, the spacing of emitters in the drip pipe is not critical as the geotextile moves the water along the fabric up to 2 m from the dripper. The impermeable layer effectively creates an artificial water table.\n\nIrrigation water can come from groundwater (extracted from springs or by using wells), from surface water (withdrawn from rivers, lakes or reservoirs) or from non-conventional sources like treated wastewater, desalinated water, drainage water, or fog collection. A special form of irrigation using surface water is spate irrigation, also called floodwater harvesting. In case of a flood (spate), water is diverted to normally dry river beds (wadis) using a network of dams, gates and channels and spread over large areas. The moisture stored in the soil will be used thereafter to grow crops. Spate irrigation areas are in particular located in semi-arid or arid, mountainous regions. While floodwater harvesting belongs to the accepted irrigation methods, rainwater harvesting is usually not considered as a form of irrigation. Rainwater harvesting is the collection of runoff water from roofs or unused land and the concentration of this.\n\nAround 90% of wastewater produced globally remains untreated, causing widespread water pollution, especially in low-income countries. Increasingly, agriculture uses untreated wastewater as a source of irrigation water. Cities provide lucrative markets for fresh produce, so are attractive to farmers. However, because agriculture has to compete for increasingly scarce water resources with industry and municipal users (see Water scarcity below), there is often no alternative for farmers but to use water polluted with urban waste, including sewage, directly to water their crops. Significant health hazards can result from using water loaded with pathogens in this way, especially if people eat raw vegetables that have been irrigated with the polluted water. The International Water Management Institute has worked in India, Pakistan, Vietnam, Ghana, Ethiopia, Mexico and other countries on various projects aimed at assessing and reducing risks of wastewater irrigation. They advocate a 'multiple-barrier' approach to wastewater use, where farmers are encouraged to adopt various risk-reducing behaviours. These include ceasing irrigation a few days before harvesting to allow pathogens to die off in the sunlight, applying water carefully so it does not contaminate leaves likely to be eaten raw, cleaning vegetables with disinfectant or allowing fecal sludge used in farming to dry before being used as a human manure. The World Health Organization has developed guidelines for safe water use.\n\nThere are numerous benefits of using recycled water for irrigation, including the low cost (when compared to other sources, particularly in an urban area), consistency of supply (regardless of season, climatic conditions and associated water restrictions), and general consistency of quality. Irrigation of recycled wastewater is also considered as a means for plant fertilization and particularly nutrient supplementation. This approach carries with it a risk of soil and water pollution through excessive wastewater application. Hence, a detailed understanding of soil water conditions is essential for effective utilization of wastewater for irrigation.\n\nIn countries where humid air sweeps through at night, water can be obtained by condensation onto cold surfaces. This is practiced in the vineyards at Lanzarote using stones to condense water. Fog collectors are also made of canvas or foil sheets. Using condensate from air conditioning units as a water source is also becoming more popular in large urban areas.\n\nModern irrigation methods are efficient enough to supply the entire field uniformly with water, so that each plant has the amount of water it needs, neither too much nor too little. Water use efficiency in the field can be determined as follows:\n\n\nUntil 1960s, the common perception was that water was an infinite resource. At that time, there were fewer than half the current number of people on the planet. People were not as wealthy as today, consumed fewer calories and ate less meat, so less water was needed to produce their food. They required a third of the volume of water we presently take from rivers. Today, the competition for water resources is much more intense. This is because there are now more than seven billion people on the planet, their consumption of water-thirsty meat and vegetables is rising, and there is increasing competition for water from industry, urbanisation and biofuel crops. To avoid a global water crisis, farmers will have to strive to increase productivity to meet growing demands for food, while industry and cities find ways to use water more efficiently.\n\nSuccessful agriculture is dependent upon farmers having sufficient access to water. However, water scarcity is already a critical constraint to farming in many parts of the world. With regards to agriculture, the World Bank targets food production and water management as an increasingly global issue that is fostering a growing debate. Physical water scarcity is where there is not enough water to meet all demands, including that needed for ecosystems to function effectively. Arid regions frequently suffer from physical water scarcity. It also occurs where water seems abundant but where resources are over-committed. This can happen where there is overdevelopment of hydraulic infrastructure, usually for irrigation. Symptoms of physical water scarcity include environmental degradation and declining groundwater. Economic scarcity, meanwhile, is caused by a lack of investment in water or insufficient human capacity to satisfy the demand for water. Symptoms of economic water scarcity include a lack of infrastructure, with people often having to fetch water from rivers for domestic and agricultural uses. Some 2.8 billion people currently live in water-scarce areas.\n\nIrrigation schemes involve solving numerous engineering and economic problems while minimizing negative environmental impact.\n\n\nA 2016 study found that countries whose agriculture depended on irrigation are more likely to be autocratic than other countries. The authors of the study \"argue that the effect has historical origins: irrigation allowed landed elites in arid areas to monopolize water and arable land. This made elites more powerful and better able to oppose democratization.\"\n\n\n\n\n<br>\n"}
{"id": "40547081", "url": "https://en.wikipedia.org/wiki?curid=40547081", "title": "Kew Rule", "text": "Kew Rule\n\nThe Kew Rule was used by some authors to determine the application of synonymous names in botanical nomenclature up to about 1906, but was and still is contrary to codes of botanical nomenclature including the International Code of Nomenclature for algae, fungi, and plants. Index Kewensis, a publication that aimed to list all botanical names for seed plants at the ranks of species and genus, used the Kew Rule until its \"Supplement IV\" was published in 1913 (prepared 1906–1910).\n\nThe Kew Rule applied rules of priority in a more flexible way, so that when transferring a species to a new genus, there was no requirement to retain the epithet of the original species name, and future priority of the new name was counted from the time the species was transferred to the new genus. The effect has been summarized as \"nomenclature used by an established monographer or in a major publication should be adopted\". This is contrary to the modern article 11.4 of the Code of Nomenclature.\n\nThe first discussion in print of what was to become known as the Kew Rule appears to have occurred in 1877 between Henry Trimen and Alphonse Pyramus de Candolle. Trimen did not think it was reasonable for older names discovered in the literature to destabilize the nomenclature that had been well accepted:Probably all botanists are agreed that it is very desirable to retain when possible old specific names, but some of the best authors do not certainly consider themselves bound by any generally accepted rule in this matter. Still less will they be inclined to allow that a writer is at liberty, as M. de Candolle thinks, to reject the specific appellations made by an author whose genera are accepted, in favour of older ones in other genera. It will appear to such that to do this is to needlessly create in each case another synonym.\n\nThe first botanical code of nomenclature that declared itself to be binding was the 1906 \"Règles internationales de la nomenclature botanique adoptées par le Congres International de Botanique de Vienne 1905\" that followed from the 1905 International Botanical Congress. The Kew Rule was outlawed by this code.\n\nThe end of the Kew Rule brought about considerable upheaval in botanical nomenclature. Many new species names were coined to resurrect older epithets, for example, in 1917 Willis Jepson wrote:\n\"\"The plant so long known as \"Brodiaea grandiflora\" Smith ... [was] first published as \"Hookera coronaria\" Salisbury (1806). The correct name, then, is \"Brodiaea coronaria\" Jepson, \"n. comb.\"\"\n\nNames that had previously been conserved to improve the stability of well-known plant names often now no longer required conservation, and other names that had been formed using the Kew Rule and had become well known, were illegitimate. The entire previous list of conserved and rejected names was consequently replaced in 1959 with a reworked list.\n\nPreviously overlooked botanical literature has continued to yield new examples of forgotten older names for more than 100 years since the Kew Rule was banished from the International Code of Nomenclature.\n"}
{"id": "18340", "url": "https://en.wikipedia.org/wiki?curid=18340", "title": "Law of averages", "text": "Law of averages\n\nThe law of averages is the fallacious belief that a particular outcome or event is inevitable or certain simply because it is statistically possible. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability. This notion can lead to the gambler's fallacy when one becomes convinced that a particular outcome must come soon simply because it has not occurred recently (e.g. believing that because three consecutive coin flips yielded \"heads\", the next coin flip must be virtually guaranteed to be \"tails\").\n\nAs invoked in everyday life, the \"law\" usually reflects wishful thinking or a poor understanding of statistics rather than any mathematical principle. While there is a real theorem that a random variable will reflect its underlying probability over a very large sample, the law of averages typically assumes that unnatural short-term \"balance\" must occur. Typical applications also generally assume no bias in the underlying probability distribution, which is frequently at odds with the empirical evidence.\n\nThe gambler's fallacy is a particular misapplication of the law of averages in which the gambler believes that a particular outcome is more likely because it has not happened recently, or (conversely) that because a particular outcome has recently occurred, it will be less likely in the immediate future.\n\nAs an example, consider a roulette wheel that has landed on red in three consecutive spins. An onlooker might apply the law of averages to conclude that on its next spin it must (or at least is much more likely to) land on black. Of course, the wheel has no memory and its probabilities do not change according to past results. So even if the wheel has landed on red in ten or a hundred consecutive spins, the probability that the next spin will be black is still no more than 48.6% (assuming a \"fair\" European wheel with only one green zero; it would be exactly 50% if there were no green zero and the wheel were fair, and 47.4% for a fair American wheel with one green \"0\" and one green \"00\"). Similarly, there is no statistical basis for the belief that lottery numbers which haven't appeared recently are due to appear soon. (There is some value in choosing lottery numbers that are, in general, less \"popular\" than others — not because they are any more or less likely to come up, but because the largest prizes are usually shared among all of the people who chose the winning numbers. The unpopular numbers are just as likely to come up as the popular numbers are, and in the event of a big win, one would likely have to share it with fewer other people.)\n\nOn the other hand, in some locales, modern slot machines are rigged so they \"do\" give wins a certain proportion of the time — the results are not truly random. This is carefully managed so as to encourage people to keep playing, while the casino takes its designated amount of profit.\n\nAnother application of the law of averages is a belief that a sample's behaviour must line up with the expected value based on population statistics. For example, suppose a fair coin is flipped 100 times. Using the law of averages, one might predict that there will be 50 heads and 50 tails. While this is the single most likely outcome, there is only an 8% chance of it occurring. Predictions based on the law of averages are even less useful if the sample does not reflect the population.\n\nIn this example, one tries to increase the probability of a rare event occurring at least once by carrying out more trials. For example, a job seeker might argue, \"If I send my résumé to enough places, the law of averages says that someone will eventually hire me.\" Assuming a non-zero probability, it is true that conducting more trials increases the overall likelihood of the desired outcome. However, there is no particular number of trials that guarantees that outcome; rather, the probability that it will already have occurred approaches but never quite reaches 100%.\n\nThe Steve Goodman song \"A Dying Cub Fan's Last Request\" mentions the Law of Averages in reference to the Chicago Cubs lack of championship success. At the time Goodman recorded the song in 1981, the Cubs had not won a National League championship since the year the United States dropped the atomic bomb on Japan (1945), and had not won a World Series since 1908. This futility would continue until the Cubs would finally win both in 2016.\n\n"}
{"id": "40001291", "url": "https://en.wikipedia.org/wiki?curid=40001291", "title": "List of ARM Cortex-M development tools", "text": "List of ARM Cortex-M development tools\n\nThis is a list of development tools for 32-bit ARM Cortex-M-based microcontrollers, which consists of Cortex-M0, Cortex-M0+, Cortex-M1, Cortex-M3, Cortex-M4, Cortex-M7, Cortex-M23, Cortex-M33 cores.\n\nIDE, compiler, linker, debugger, flashing (in alphabetical order):\n\n\nNotes:\n\nJTAG and/or SWD debug interface host adapters (in alphabetical order):\n\nDebugging tools and/or debugging plug-ins (in alphabetical order):\n\nCommonly referred to as RTOS:\nThe following are free C/C++ libraries:\n\n\n\n\n\n"}
{"id": "11421155", "url": "https://en.wikipedia.org/wiki?curid=11421155", "title": "List of University of Chicago Press journals", "text": "List of University of Chicago Press journals\n\nThe Journals Division of the University of Chicago Press, in partnership with 27 learned and professional societies and associations, foundations, museums, and other not-for-profit organizations, currently publishes and distributes 68 peer-reviewed academic journal titles. These influential scholarly publications present original research in the social sciences, the humanities, education, and the biological, medical, and physical sciences. The following list includes the journals currently published by the University of Chicago Press.\n\n\n\n\n\n\n\n\n\n"}
{"id": "330950", "url": "https://en.wikipedia.org/wiki?curid=330950", "title": "List of coleopterists", "text": "List of coleopterists\n\nNotable students of coleopterology (beetles) include the following.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32429598", "url": "https://en.wikipedia.org/wiki?curid=32429598", "title": "List of zoonotic primate viruses", "text": "List of zoonotic primate viruses\n\nThe following list of primate viruses is not exhaustive. Many viruses specific to non-human primates nevertheless are known to \"jump\" and infect humans and, thus, become known as zoonoses.\n"}
{"id": "56018994", "url": "https://en.wikipedia.org/wiki?curid=56018994", "title": "Mars Organic Molecule Analyser", "text": "Mars Organic Molecule Analyser\n\nThe Mars Organic Molecule Analyser (MOMA) is a mass spectrometer-based instrument on board the ExoMars rover to be launched in July 2020 to Mars on an astrobiology mission. It will search for organic compounds (carbon-containing molecules) in the collected soil samples. By characterizing the molecular structures of detected organics, MOMA can provide insights into potential molecular biosignatures. MOMA will be able to detect organic molecules at concentrations as low as 10 parts-per-billion by weight (ppbw). MOMA examines solid crushed samples exclusively; it does not perform atmospheric analyses.\n\nThe Principal Investigator is Fred Goesmann, from the Max Planck Institute for Solar System Research in Germany.\n\nThe goal of MOMA is to seek signs of past life on Mars (biosignatures) by analysing a wide range of organic compounds that may be found in drilled samples acquired from 2  meters below the Martian surface by the ExoMars rover. MOMA examines solid crushed samples only; it does not perform atmospheric analyses.\n\nMOMA will first volatilize solid organic compounds so that they can be analysed by a mass spectrometer; this volatilization of organic material is achieved by two different techniques: laser desorption and thermal volatilisation, followed by separation using four GC-MS columns. The identification of the organic molecules is then performed with an ion trap mass spectrometer.\n\nWhile there is no unambiguous Martian biosignature to look for, a pragmatic approach is to look out for certain molecules such as lipids and phospholipids that may be forming cell membranes which can be preserved over geological timescales. Lipids and other organic molecules may exhibit biogenic features that are not present in abiogenic organic material. If biogenic (synthesized by a life form), such compounds may be found at high concentrations only over a narrow range of molecular weights, unlike in carbonaceous meteorites where these compounds are detected over a broader range of molecular weights. In the case of sugars and amino acids, excessive molecular homochirality (asymmetry) is another important clue of their biological origin. The assumption is that life on Mars would be carbon-based and cellular as on Earth, so there are expected common building blocks such as chains of amino acids (peptides and proteins) and chains of nucleobases (RNA, DNA, or their analogs). Also, some isomers of high molecular weight organics can be potential biosignatures when identified in context with other supporting evidence. Other compounds targeted for detection will include fatty acids, sterols, and hopanoids.\n\nThe surface of Mars is expected to have accumulated significant quantities of large organic molecules delivered by interplanetary dust particles and carbonaceous meteorites. MOMA's characterization of this fraction, may determine not only the abundance of this potential background for trace biomarker detection, but also the degree of decomposition of this matter by radiation and oxidation as a function of depth. This is essential in order to interpret the samples' origin in the local geological and geochemical context.\n\nThe components of MOMA related to GC-MS have heritage from the \"Viking\" landers, the COSAC on board the comet lander \"Philae\", and SAM on board the \"Curiosity\" rover. But the methods applied in the past on board the \"Viking\" landers and the \"Curiosity\" rover are mostly destructive (pyrolysis), and consequently important information of the organic material is lost. Also, only volatile molecules can be detected and, only nonpolar molecules can get through the GC columns to the detector. MOMA will combine pyrolysis–derivatization with a less destructive method: LDMS (Laser Desorption Mass Spectrometry), which allows large and intact molecular fragments to be detected and characterized by the mass spectrometer (MS). The LDMS technique is not affected by these drawbacks, and it is unaffected to the presence of perchlorates, known to be abundant on the surface of Mars. Tandem mass spectrometry can then be used to further characterize these molecules.\n\nThe Max Planck Institute for Solar System Research is leading the development. International partners include NASA. The mass spectrometer (MS) and the main electronics of MOMA are provided by NASA's Goddard Space Flight Center, while the gas chromatography (GC) is provided by the two French institutes LISA and LATMOS. The UV-Laser is being developed by the Laser Zentrum Hannover. MOMA does not form a single compact unit, but is modular with numerous mechanical and thermal interfaces within the rover. The final integration and verification will be performed at Thales Alenia Space in Italy.\n\n"}
{"id": "19568", "url": "https://en.wikipedia.org/wiki?curid=19568", "title": "Microscope", "text": "Microscope\n\nA microscope (from the , \"mikrós\", \"small\" and , \"skopeîn\", \"to look\" or \"see\") is an instrument used to see objects that are too small to be seen by the naked eye. Microscopy is the science of investigating small objects and structures using such an instrument. Microscopic means invisible to the eye unless aided by a microscope.\n\nThere are many types of microscopes, and they may be grouped in different ways. One way is to describe the way the instruments interact with a sample to create images, either by sending a beam of light or electrons to a sample in its optical path, or by scanning across, and a short distance from, the surface of a sample using a probe. The most common microscope (and the first to be invented) is the optical microscope, which uses light to pass through a sample to produce an image. Other major types of microscopes are the fluorescence microscope, the electron microscope (both, the transmission electron microscope and the scanning electron microscope) and the various types of scanning probe microscopes.\n\nAlthough objects resembling lenses date back 4000 years and there are Greek accounts of the optical properties of water-filled spheres (5th century BC) followed by many centuries of writings on optics, the earliest known use of simple microscopes (magnifying glasses) dates back to the widespread use of lenses in eyeglasses in the 13th century. The earliest known examples of compound microscopes, which combine an objective lens near the specimen with an eyepiece to view a real image, appeared in Europe around 1620. The inventor is unknown although many claims have been made over the years. Several revolve around the spectacle-making centers in the Netherlands including claims it was invented in 1590 by Zacharias Janssen (claim made by his son) and/or Zacharias' father, Hans Martens, claims it was invented by their neighbor and rival spectacle maker, Hans Lippershey (who applied for the first telescope patent in 1608), and claims it was invented by expatriate Cornelis Drebbel who was noted to have a version in London in 1619. Galileo Galilei (also sometimes cited as compound microscope inventor) seems to have found after 1610 that he could close focus his telescope to view small objects and, after seeing a compound microscope built by Drebbel exhibited in Rome in 1624, built his own improved version. Giovanni Faber coined the name \"microscope\" for the compound microscope Galileo submitted to the Accademia dei Lincei in 1625 (Galileo had called it the \"\"occhiolino\" or \"little eye\"\").\n\nThe first detailed account of the microscopic anatomy of organic tissue based on the use of a microscope did not appear until 1644, in Giambattista Odierna's \"L'occhio della mosca\", or \"The Fly's Eye\".\n\nThe microscope was still largely a novelty until the 1660s and 1670s when naturalists in Italy, the Netherlands and England began using them to study biology, both organisms and their ultrastructure. Italian scientist Marcello Malpighi, called the father of histology by some historians of biology, began his analysis of biological structures with the lungs. Robert Hooke's \"Micrographia\" had a huge impact, largely because of its impressive illustrations. A significant contribution came from Antonie van Leeuwenhoek who achieved up to 300 times magnification using a simple single lens microscope. He sandwiched a very small glass ball lens between the holes in two metal plates riveted together, and with an adjustable-by-screws needle attached to mount the specimen. Then, Van Leeuwenhoek re-discovered red blood cells (after Jan Swammerdam) and spermatozoa, and helped popularise the use of microscopes to view biological ultrastructure. On 9 October 1676, van Leeuwenhoek reported the discovery of micro-organisms.\n\nThe performance of a light microscope depends on the quality and correct use of the condensor lens system to focus light on the specimen and the objective lens to capture the light from the specimen and form an image. Early instruments were limited until this principle was fully appreciated and developed from the late 19th to very early 20th century, and until electric lamps were available as light sources. In 1893 August Köhler developed a key principle of sample illumination, Köhler illumination, which is central to achieving the theoretical limits of resolution for the light microscope. This method of sample illumination produces even lighting and overcomes the limited contrast and resolution imposed by early techniques of sample illumination. Further developments in sample illumination came from the discovery of phase contrast by Frits Zernike in 1953, and differential interference contrast illumination by Georges Nomarski in 1955; both of which allow imaging of unstained, transparent samples.\n\nIn the early 20th century a significant alternative to the light microscope was developed, an instrument that uses a beam of electrons rather than light to generate an image. The German physicist, Ernst Ruska, working with electrical engineer Max Knoll, developed the first prototype electron microscope in 1931, a transmission electron microscope (TEM). The transmission electron microscope works on similar principles to an optical microscope but uses electrons in the place of light and electromagnets in the place of glass lenses. Use of electrons, instead of light, allows for much higher resolution.\n\nDevelopment of the transmission electron microscope was quickly followed in 1935 by the development of the scanning electron microscope by Max Knoll. Although TEMs were being used for research before WWII, and became popular afterwards, the SEM was not commercially available until 1965.\n\nTransmission electron microscopes became popular following the Second World War. Ernst Ruska, working at Siemens, developed the first commercial transmission electron microscope and, in the 1950s, major scientific conferences on electron microscopy started being held. In 1965, the first commercial scanning electron microscope was developed by Professor Sir Charles Oatley and his postgraduate student Gary Stewart, and marketed by the Cambridge Instrument Company as the \"Stereoscan\".\n\nOne of the latest discoveries made about using an electron microscope is the ability to identify a virus. Since this microscope produces a visible, clear image of small organelles, in an electron microscope there is no need for reagents to see the virus or harmful cells, resulting in a more efficient way to detect pathogens.\n\nFrom 1981 to 1983 Gerd Binnig and Heinrich Rohrer worked at IBM in Zurich, Switzerland to study the quantum tunnelling phenomenon. They created a practical instrument, a scanning probe microscope from quantum tunnelling theory, that read very small forces exchanged between a probe and the surface of a sample. The probe approaches the surface so closely that electrons can flow continuously between probe and sample, making a current from surface to probe. The microscope was not initially well received due to the complex nature of the underlying theoretical explanations. In 1984 Jerry Tersoff and D.R. Hamann, while at AT&T's Bell Laboratories in Murray Hill, New Jersey began publishing articles that tied theory to the experimental results obtained by the instrument. This was closely followed in 1985 with functioning commercial instruments, and in 1986 with Gerd Binnig, Quate, and Gerber's invention of the atomic force microscope, then Binnig's and Rohrer's Nobel Prize in Physics for the SPM.\n\nNew types of scanning probe microscope have continued to be developed as the ability to machine ultra-fine probes and tips has advanced.\n\nThe most recent developments in light microscope largely centre on the rise of fluorescence microscopy in biology. During the last decades of the 20th century, particularly in the post-genomic era, many techniques for fluorescent staining of cellular structures were developed. The main groups of techniques involve targeted chemical staining of particular cell structures, for example, the chemical compound DAPI to label DNA, use of antibodies conjugated to fluorescent reporters, see\nimmunofluorescence, and fluorescent proteins, such as green fluorescent protein. These techniques use these different fluorophores for analysis of cell structure at a molecular level in both live and fixed samples.\n\nThe rise of fluorescence microscopy drove the development of a major modern microscope design, the confocal microscope. The principle was patented in 1957 by Marvin Minsky, although laser technology limited practical application of the technique. It was not until 1978 when Thomas and Christoph Cremer developed the first practical confocal laser scanning microscope and the technique rapidly gained popularity through the 1980s.\n\nMuch current research (in the early 21st century) on optical microscope techniques is focused on development of superresolution analysis of fluorescently labelled samples. Structured illumination can improve resolution by around two to four times and techniques like stimulated emission depletion (STED) microscopy are approaching the resolution of electron microscopes. This occurs because the diffraction limit is occurred from light or excitation, which makes the resolution must be doubled to become super saturated. Stefan Hell was awarded the 2014 Nobel Prize in Chemistry for the development of the STED technique, along with Eric Betzig and William Moerner who adapted fluorescnence microscopy for single-molecule viusalization.\n\nX-ray microscopes are instruments that use electromagnetic radiation usually in the soft X-ray band to image objects. Technological advances in x-ray lens optics in the early 1970s made the instrument a viable imaging choice. They are often used in tomography (see micro-computed tomography) to produce three dimensional images of objects, including biological materials that have not been chemically fixed. Currently research is being done to improve optics for hard x-rays which have greater penetrating power.\n\nMicroscopes can be separated into several different classes. One grouping is based on what interacts with the sample to generate the image, i.e., light or photons (optical microscopes), electrons (electron microscopes) or a probe (scanning probe microscopes). Alternatively, microscopes can be classified based on whether they analyze the sample via a scanning point (confocal optical microscopes, scanning electron microscopes and scanning probe microscopes) or analyze the sample all at once (wide field optical microscopes and transmission electron microscopes).\n\nWide field optical microscopes and transmission electron microscopes both use the theory of lenses (optics for light microscopes and electromagnet lenses for electron microscopes) in order to magnify the image generated by the passage of a wave transmitted through the sample, or reflected by the sample. The waves used are electromagnetic (in optical microscopes) or electron beams (in electron microscopes). Resolution in these microscopes is limited by the wavelength of the radiation used to image the sample, where shorter wavelengths allow for a higher resolution.\n\nScanning optical and electron microscopes, like the confocal microscope and scanning electron microscope, use lenses to focus a spot of light or electrons onto the sample then analyze the signals generated by the beam interacting with the sample. The point is then scanned over the sample to analyze a rectangular region. Magnification of the image is achieved by displaying the data from scanning a physically small sample area on a relatively large screen. These microscopes have the same resolution limit as wide field optical, probe, and electron microscopes.\n\nScanning probe microscopes also analyze a single point in the sample and then scan the probe over a rectangular sample region to build up an image. As these microscopes do not use electromagnetic or electron radiation for imaging they are not subject to the same resolution limit as the optical and electron microscopes described above.\n\nThe most common type of microscope (and the first invented) is the optical microscope. This is an optical instrument containing one or more lenses producing an enlarged image of a sample placed in the focal plane. Optical microscopes have refractive glass (occasionally plastic or quartz), to focus light on the eye or on to another light detector. Mirror-based optical microscopes operate in the same manner. Typical magnification of a light microscope, assuming visible range light, is up to 1250x with a theoretical resolution limit of around 0.250 micrometres or 250 nanometres. This limits practical magnification to ~1500x. Specialized techniques (e.g., scanning confocal microscopy, Vertico SMI) may exceed this magnification but the resolution is diffraction limited. The use of shorter wavelengths of light, such as ultraviolet, is one way to improve the spatial resolution of the optical microscope, as are devices such as the near-field scanning optical microscope.\n\nSarfus is a recent optical technique that increases the sensitivity of a standard optical microscope to a point where it is possible to directly visualize nanometric films (down to 0.3 nanometre) and isolated nano-objects (down to 2 nm-diameter). The technique is based on the use of non-reflecting substrates for cross-polarized reflected light microscopy.\n\nUltraviolet light enables the resolution of microscopic features as well as the imaging of samples that are transparent to the eye. Near infrared light can be used to visualize circuitry embedded in bonded silicon devices, since silicon is transparent in this region of wavelengths.\n\nIn fluorescence microscopy many wavelengths of light ranging from the ultraviolet to the visible can be used to cause samples to fluoresce which allows viewing by eye or with specifically sensitive cameras. Phase contrast microscopy is an optical microscopy illumination technique in which small phase shifts in the light passing through a transparent specimen are converted into amplitude or contrast changes in the image. The use of phase contrast does not require staining to view the slide. This microscope technique made it possible to study the cell cycle in live cells.\n\nThe traditional optical microscope has more recently evolved into the digital microscope. In addition to, or instead of, directly viewing the object through the eyepieces, a type of sensor similar to those used in a digital camera is used to obtain an image, which is then displayed on a computer monitor. These sensors may use CMOS or charge-coupled device (CCD) technology, depending on the application.\n\nDigital microscopy with very low light levels to avoid damage to vulnerable biological samples is available using sensitive photon-counting digital cameras. It has been demonstrated that a light source providing pairs of entangled photons may minimize the risk of damage to the most light-sensitive samples. In this application of ghost imaging to photon-sparse microscopy, the sample is illuminated with infrared photons, each of which is spatially correlated with an entangled partner in the visible band for efficient imaging by a photon-counting camera.\n\nThe two major types of electron microscopes are transmission electron microscopes (TEMs) and scanning electron microscopes (SEMs). They both have series of electromagnetic and electrostatic lenses to focus a high energy beam of electrons on a sample. In a TEM the electrons pass through the sample, analogous to basic optical microscopy. This requires careful sample preparation, since electrons are scattered strongly by most materials. The samples must also be very thin (50–100 nm) in order for the electrons to pass through it. Cross-sections of cells stained with osmium and heavy metals reveal clear organelle membranes and proteins such as ribosomes. With a 0.1 nm level of resolution, detailed views of viruses (20–300 nm) and a strand of DNA (2 nm in width) can be obtained. In contrast, the SEM has raster coils to scan the surface of bulk objects with a fine electron beam. Therefore, the specimen do not necessarily need to be sectioned, but require coating with a substance such as a heavy metal. This allows three-dimensional views of the surface of samples.\n\nThe different types of scanning probe microscopes arise from the many different types of interactions that occur when a small probe of some type is scanned over and interacts with a specimen. These interactions or modes can be recorded or mapped as function of location on the surface to form a characterization map. The three most common types of scanning probe microscopes are atomic force microscopes (AFM), near-field scanning optical microscopes (MSOM or SNOM, scanning near-field optical microscopy), and scanning tunneling microscopes (STM). An atomic force microscope has a fine probe, usually of silicon or silicon nitride, attached to a cantilever; the probe is scanned over the surface of the sample, and the forces that cause an interaction between the probe and the surface of the sample are measured and mapped. A near-field scanning optical microscope is similar to an AFM but its probe consists of a light source in an optical fiber covered with a tip that has usually an aperture for the light to pass through. The microscope can capture either transmitted or reflected light to measure very localized optical properties of the surface, commonly of a biological specimen. Scanning tunneling microscopes have a metal tip with a single apical atom; the tip is attached to a tube through which a current flows. The tip is scanned over the surface of a conductive sample until a tunneling current flows; the current is kept constant by computer movement of the tip and an image is formed by the recorded movements of the tip.\nScanning acoustic microscopes use sound waves to measure variations in acoustic impedance. Similar to Sonar in principle, they are used for such jobs as detecting defects in the subsurfaces of materials including those found in integrated circuits. On February 4, 2013, Australian engineers built a \"quantum microscope\" which provides unparalleled precision.\n\n"}
{"id": "29686577", "url": "https://en.wikipedia.org/wiki?curid=29686577", "title": "Myrsini Malakou", "text": "Myrsini Malakou\n\nMyrsini Malakou is a Greek biologist. She was awarded the Goldman Environmental Prize in 2001, for her contributions to the protection of the wetlands of Préspa, jointly with fellow biologist Giorgos Catsadorakis.\n"}
{"id": "337301", "url": "https://en.wikipedia.org/wiki?curid=337301", "title": "National Ignition Facility", "text": "National Ignition Facility\n\nThe National Ignition Facility (NIF), is a large laser-based inertial confinement fusion (ICF) research device, located at the Lawrence Livermore National Laboratory in Livermore, California. NIF uses lasers to heat and compress a small amount of hydrogen fuel with the goal of inducing nuclear fusion reactions. NIF's mission is to achieve fusion ignition with high energy gain, and to support nuclear weapon maintenance and design by studying the behavior of matter under the conditions found within nuclear weapons. NIF is the largest and most energetic ICF device built to date, and the largest laser in the world.\n\nConstruction on the NIF began in 1997 but management problems and technical delays slowed progress into the early 2000s. Progress after 2000 was smoother, but compared to initial estimates, NIF was completed five years behind schedule and was almost four times more expensive than originally budgeted. Construction was certified complete on 31 March 2009 by the U.S. Department of Energy, and a dedication ceremony took place on 29 May 2009. The first large-scale laser target experiments were performed in June 2009 and the first \"integrated ignition experiments\" (which tested the laser's power) were declared completed in October 2010.\n\nBringing the system to its full potential was a lengthy process that was carried out from 2009 to 2012. During this period a number of experiments were worked into the process under the National Ignition Campaign, with the goal of reaching ignition just after the laser reached full power, some time in the second half of 2012. The Campaign officially ended in September 2012, at about the conditions needed for ignition. Experiments since then have pushed this closer to , but considerable theoretical and practical work is required if the system is ever to reach ignition. Since 2012, NIF has been used primarily for materials science and weapons research.\n\nInertial confinement fusion (ICF) devices use \"drivers\" to rapidly heat the outer layers of a \"target\" in order to compress it. The target is a small spherical pellet containing a few milligrams of fusion fuel, typically a mix of deuterium (D) and tritium (T). The energy of the laser heats the surface of the pellet into a plasma, which explodes off the surface. The remaining portion of the target is driven inward, eventually compressing it into a small point of extremely high density. The rapid blowoff also creates a shock wave that travels toward the center of the compressed fuel from all sides. When it reaches the center of the fuel, a small volume is further heated and compressed to a greater degree. When the temperature and density of that small spot are raised high enough, fusion reactions occur and release energy.\n\nThe fusion reactions release high-energy particles, some of which, primarily alpha particles, collide with the surrounding high density fuel and heat it further. If this process deposits enough energy in a given area it can cause that fuel to undergo fusion as well. However, the fuel is also losing heat through x-ray losses and hot electrons leaving the fuel area, so the rate of alpha heating must be greater than these losses, a condition known as \"bootstrapping\". Given the right overall conditions of the compressed fuel—high enough density and temperature—this bootstrapping process will result in a chain reaction, burning outward from the center where the shock wave started the reaction. This is a condition known as \"ignition\", which will lead to a significant portion of the fuel in the target undergoing fusion and releasing large amounts of energy.\n\nTo date most ICF experiments have used lasers to heat the target. Calculations show that the energy must be delivered quickly in order to compress the core before it disassembles. The laser energy also must be focused extremely evenly across the target's outer surface in order to collapse the fuel into a symmetric core. Although other drivers have been suggested, notably heavy ions driven in particle accelerators, lasers are currently the only devices with the right combination of features.\n\nNIF aims to create a single 500 terawatt (TW) peak flash of light that reaches the target from numerous directions at the same time, within a few picoseconds. The design uses 192 beamlines in a parallel system of flashlamp-pumped, neodymium-doped phosphate glass lasers.\n\nTo ensure that the output of the beamlines is uniform, the initial laser light is amplified from a single source in the Injection Laser System (ILS). This starts with a low-power flash of 1053-nanometer (nm) infra-red light generated in an ytterbium-doped optical fiber laser known as the Master Oscillator. The light from the Master Oscillator is split and directed into 48 Preamplifier Modules (PAMs). Each PAM contains a two-stage amplification process. The first stage is a regenerative amplifier in which the pulse circulates 30 to 60 times, increasing in energy from nanojoules to tens of millijoules. The light then passes four times through a circuit containing a neodymium glass amplifier similar to (but much smaller than) the ones used in the main beamlines, boosting the nanojoules of light created in the Master Oscillator to about 6 joules. According to Lawrence Livermore National Laboratory (LLNL), the design of the PAMs was one of the major challenges during construction. Improvements to the design since then have allowed them to surpass their initial design goals.\n\nThe main amplification takes place in a series of glass amplifiers located at one end of the beamlines. Before firing, the amplifiers are first optically pumped by a total of 7,680 xenon flash lamps (the PAMs have their own smaller flash lamps as well). The lamps are powered by a capacitor bank which stores a total of 422 MJ (117 kWh) of electrical energy. When the wavefront passes through them, the amplifiers release some of the light energy stored in them into the beam. To improve the energy transfer the beams are sent though the main amplifier section four times, using an optical switch located in a mirrored cavity. In total these amplifiers boost the original 6 J provided by the PAMs to a nominal 4 MJ. Given the time scale of a few billionths of a second, the peak UV power delivered to the target is correspondingly very high, 500 TW.\n\nNear the center of each beamline, and taking up the majority of the total length, are \"spatial filters\". These consist of long tubes with small telescopes at the end that focus the laser beam down to a tiny point in the center of the tube, where a mask cuts off any stray light outside the focal point. The filters ensure that the image of the beam when it reaches the target is extremely uniform, removing any light that was misfocused by imperfections in the optics upstream. Spatial filters were a major step forward in ICF work when they were introduced in the Cyclops laser, an earlier LLNL experiment.\n\nThe total length of the path the laser beam propagates from one end to the other, including switches, is about . The various optical elements in the beamlines are generally packaged into Line Replaceable Units (LRUs), standardized boxes about the size of a vending machine that can be dropped out of the beamline for replacement from below.\n\nAfter the amplification is complete the light is switched back into the beamline, where it runs to the far end of the building to the \"target chamber\". The target chamber is a multi-piece steel sphere weighing . Just before reaching the target chamber, the light is reflected off various mirrors in the \"switchyard\" and target area in order to impinge on the target from different directions. Since the length of the overall path from the Master Oscillator to the target is different for each of the beamlines, optics are used to delay the light in order to ensure all of them reach the center within a few picoseconds of each other. NIF normally directs the laser into the chamber from the top and bottom. The target area and switchyard system can be reconfigured by moving half of the 48 beamlines to alternate positions closer to the equator of the target chamber.\n\nOne of the last steps in the process before reaching the target chamber is to convert the infrared (IR) light at 1053 nm into the ultraviolet (UV) at 351 nm in a device known as a frequency converter. These are made of thin sheets (about 1 cm thick) cut from a single crystal of potassium dihydrogen phosphate. When the 1053 nm (IR) light passes through the first of two of these sheets, frequency addition converts a large fraction of the light into 527 nm light (green). On passing through the second sheet, frequency combination converts much of the 527 nm light and the remaining 1053 nm light into 351 nm (UV) light. Infrared (IR) light is much less effective than UV at heating the targets, because IR couples more strongly with hot electrons which will absorb a considerable amount of energy and interfere with compression. The conversion process can reach peak efficiencies of about 80 percent for a laser pulse that has a flat temporal shape, but the temporal shape needed for ignition varies significantly over the duration of the pulse. The actual conversion process is about 50 percent efficient, reducing delivered energy to a nominal 1.8 MJ.\n\nOne important aspect of any ICF research project is ensuring that experiments can actually be carried out on a timely basis. Previous devices generally had to cool down for many hours to allow the flashlamps and laser glass to regain their shapes after firing (due to thermal expansion), limiting use to one or fewer firings a day. One of the goals for NIF is to reduce this time to less than four hours, in order to allow 700 firings a year.\n\nThe name National Ignition Facility refers to the goal of igniting the fusion fuel, a long-sought threshold in fusion research. In existing (non-weapon) fusion experiments the heat produced by the fusion reactions rapidly escapes from the plasma, meaning that external heating must be applied continually in order to keep the reactions going. Ignition refers to the point at which the energy given off in the fusion reactions currently underway is high enough to sustain the temperature of the fuel against those losses. This causes a chain-reaction that allows the majority of the fuel to undergo a nuclear \"burn\". Ignition is considered a key requirement if fusion power is to ever become practical.\n\nNIF is designed primarily to use the \"indirect drive\" method of operation, in which the laser heats a small metal cylinder instead of the capsule inside it. The heat causes the cylinder, known as a hohlraum (German for \"hollow room\", or cavity), to re-emit the energy as intense X-rays, which are more evenly distributed and symmetrical than the original laser beams. Experimental systems, including the OMEGA and Nova lasers, validated this approach through the late 1980s. In the case of the NIF, the large delivered power allows for the use of a much larger target; the baseline pellet design is about 2 mm in diameter, chilled to about 18 kelvins (−255 °C) and lined with a layer of frozen DT fuel. The hollow interior also contains a small amount of DT gas.\n\nIn a typical experiment, the laser will generate 3 MJ of infrared laser energy of a possible 4. About 1.5 MJ of this is left after conversion to UV, and about 15 percent of this is lost in the x-ray conversion in the hohlraum. About 15 percent of the resulting x-rays, about 150 kJ, will be absorbed by the outer layers of the target. The resulting inward directed compression is expected to compress the fuel in the center of the target to a density of about 1,000 g/cm (or 1,000,000 kg/m); for comparison, lead has a normal density of about 11 g/cm (11,340 kg/m). The pressure is the equivalent of 300 billion atmospheres.\n\nIt is expected this will cause about 20 MJ of fusion energy to be released, resulting in a net fusion energy gain of about 15 (G=Fusion energy/UV laser energy). Improvements in both the laser system and hohlraum design are expected to improve the energy absorbed by the capsule to about 420 kJ, which, in turn, could generate up to 100-150 MJ of fusion energy. However, the baseline design allows for a maximum of about 45 MJ of fusion energy release, due to the design of the target chamber. This is the equivalent of about 11 kg of TNT exploding.\n\nThese output energies are still less than the 422 MJ of input energy required to charge the system's capacitors that power the laser amplifiers. The net wall-plug efficiency of NIF (UV laser energy out divided by the energy required to pump the lasers from an external source) is less than one percent, and the total wall-to-fusion efficiency is under 10% at its maximum performance. An economical fusion reactor would require that the fusion output be at least an order of magnitude more than this input. Commercial laser fusion systems would use the much more efficient diode-pumped solid state lasers, where wall-plug efficiencies of 10 percent have been demonstrated, and efficiencies 16-18 percent are expected with advanced concepts under development.\n\nNIF is also exploring new types of targets. Previous experiments generally used plastic ablators, typically polystyrene (CH). NIF's targets also are constructed by coating a plastic form with a layer of sputtered beryllium or beryllium-copper alloys, and then oxidizing the plastic out of the center. In comparison to traditional plastic targets, beryllium targets offer higher overall implosion efficiencies for the indirect-drive mode where the incoming energy is in the form of x-rays.\n\nAlthough NIF was primarily designed as an indirect drive device, the energy in the laser is high enough to be used as a \"direct drive\" system as well, where the laser shines directly on the target. Even at UV wavelengths the power delivered by NIF is estimated to be more than enough to cause ignition, resulting in fusion energy gains of about 40 times, somewhat higher than the indirect drive system. A more uniform beam layout suitable for direct drive experiments can be arranged through changes in the switchyard that move half of the beamlines to locations closer to the middle of the target chamber.\n\nIt has been shown, using scaled implosions on the OMEGA laser and computer simulations, that NIF should also be capable of igniting a capsule using the so-called \"polar direct drive\" (PDD) configuration where the target is irradiated directly by the laser, but only from the top and bottom, with no changes to the NIF beamline layout. In this configuration the target suffers either a \"pancake\" or \"cigar\" anisotropy on implosion, reducing the maximum temperature at the core.\n\nOther targets, called \"saturn targets\", are specifically designed to reduce the anisotropy and improve the implosion. They feature a small plastic ring around the \"equator\" of the target, which quickly vaporizes into a plasma when hit by the laser. Some of the laser light is refracted through this plasma back towards the equator of the target, evening out the heating. Ignition with gains of just over thirty-five times are thought to be possible using these targets at NIF, producing results almost as good as the fully symmetric direct drive approach.\n\nLawrence Livermore National Laboratory's (LLNL) history with the ICF program starts with physicist John Nuckolls, who started considering the problem after a 1957 meeting on the peaceful use of nuclear weapons arranged by Edward Teller at LLNL. During these meetings, the idea later known as PACER first developed. PACER envisioned the explosion of small hydrogen bombs in large underground caverns to generate steam that would be converted into electrical power. After identifying several problems with this approach, Nuckolls became interested in understanding how small a bomb could be made that would still generate net positive power.\n\nThere are two parts to a typical hydrogen bomb, a plutonium-based atomic bomb known as the \"primary\", and a cylindrical arrangement of fusion fuels known as the \"secondary\". The primary releases significant amounts of x-rays, which are trapped within the bomb casing and heat and compress the secondary until it undergoes ignition. The secondary consists of lithium deuteride fuel, which requires an external neutron source to begin the reaction. This is normally in the form of a D-T \"spark plug\" in the center of the fuel. Nuckolls's idea was to explore how small the secondary could be made, and what effects this would have on the energy needed from the primary to cause ignition. The simplest change is to replace the LiD fuel with D-T gas, essentially making the spark plug the entire secondary. At that point there is no theoretical smallest size - as the secondary got smaller, so did the amount of energy needed to reach ignition. At the milligram level, the energy levels started to approach those available through several known devices.\n\nBy the early 1960s, Nuckolls and several other weapons designers had developed the outlines of the ICF approach. The D-T fuel would be placed in a small capsule, designed to rapidly ablate when heated and thereby maximize compression and shock wave formation. This capsule would be placed within an engineered shell, the hohlraum, which acted similar to the bomb casing. However, the hohlraum did not have to be heated by x-rays; any source of energy could be used as long as it delivered enough energy to cause the hohlraum itself to heat up and start giving off x-rays. Ideally the energy source would be located some distance away, to mechanically isolate both ends of the reaction. A small atomic bomb could be used as the energy source, as it is in a hydrogen bomb, but ideally smaller energy sources would be used. Using computer simulations, the teams estimated that about 5 MJ of energy would be needed from the primary, generating a 1 MJ beam. To put this in perspective, a small fission primary of 0.5 kt releases 2 million MJ in total.\n\nWhile Nuckolls and LLNL were working on hohlraum-based concepts, former weapon designer Ray Kidder was working on the direct drive concept, using a large number of laser beams to evenly heat the target capsule. In the early 1970s, Kidder formed KMS Fusion to directly commercialize this concept. This sparked off intense rivalry between Kidder and the weapons labs. Formerly ignored, ICF was now a hot topic and most of the labs soon started ICF efforts of their own. LLNL decided early on to concentrate on glass lasers, while other facilities studied gas lasers using carbon dioxide (e.g. ANTARES, Los Alamos National Laboratory) or KrF (e.g. Nike laser, Naval Research Laboratory).\n\nThroughout these early stages of development, much of the understanding of the fusion process was the result of computer simulations, primarily LASNEX. LASNEX greatly simplified the reaction to a 2-dimensional simulation, which was all that was possible given the amount of computing power at the time. According to LASNEX, laser drivers in the kJ range would have the required properties to reach low gain, which was just within the state of the art. This led to the Shiva laser project which was completed in 1977. Contrary to predictions, Shiva fell far short of its goals, and the densities reached were thousands of times smaller than predicted. This was traced to issues with the way the laser delivered heat to the target, which delivered most of its energy to electrons rather than the entire fuel mass. Further experiments and simulations demonstrated that this process could be dramatically improved by using shorter wavelengths of laser light.\n\nFurther upgrades to the simulation programs, accounting for these effects, predicted a new design that would reach ignition. This new system emerged as the 20-beam 200 kJ Nova laser concept. During the initial construction phase, Nuckolls found an error in his calculations, and an October 1979 review chaired by John Foster Jr. of TRW confirmed that there was no way Nova would reach ignition. The Nova design was then modified into a smaller 10-beam design that added frequency conversion to 351 nm light, which would increase coupling efficiency. In operation, Nova was able to deliver about 30 kJ of UV laser energy, about half of what was initially expected, primarily due to limits set by optical damage to the final focusing optics. Even at those levels, it was clear that the predictions for fusion production were still wrong; even at the limited powers available, fusion yields were far below predictions.\n\nWith each experiment, the predicted energy needed to reach ignition rose, and it was not clear that post-Nova predictions were any more accurate than earlier ones. The Department of Energy (DOE) decided that direct experimentation was the best way to settle the issue, and in 1978 they started a series of underground experiments at the Nevada Test Site that used small nuclear bombs to illuminate ICF targets. The tests were known as Halite or Centurion depending on which lab ran it, LLNL or LANL.\n\nEach test was able to simultaneously illuminate many targets, allowing them to test the amount of x-ray energy needed by placing the targets at different distances from the bomb. Another question was how large the fuel assembly had to be in order for the fuel to self-heat from the fusion reactions and thus reach ignition. Initial data were available by mid-1984, and the testing ceased in 1988. Ignition was achieved for the first time during these tests, but the amount of energy and the size of the fuel targets needed to reach ignition was far higher than predicted. During this same period, experiments began on Nova using similar targets to understand their behaviour under laser illumination, allowing direct comparison against the results obtained from the bomb tests.\n\nData from the tests suggested that about 10 MJ of x-ray energy would be needed to reach ignition. If this energy is supplied by an IR laser to a hohlraum, as in Nova or NIF, this corresponds to an original laser energy on the order of 100 MJ, well beyond the reach of existing technologies.\n\nA great debate broke out in the ICF establishment as a result. One group suggested that they attempt to build a laser of this power; Leonardo Mascheroni and Claude Phipps designed a new type of hydrogen fluoride laser that was pumped by high-energy electrons that would be able to reach the 100 MJ limit. Others used the same data and new versions of their computer simulations based on these experiments that suggested that careful shaping of the laser pulse and using more beams spread more evenly showed that ignition and net energy gains could be achieved with a laser between 5 and 10 MJ.\n\nThese results prompted the DOE to request a custom military ICF facility they called the \"Laboratory Microfusion Facility\" (LMF). LMF would use a driver on the order of 10 MJ, delivering fusion yields of between 100 and 1,000 MJ. A 1989/90 review of this concept by the National Academy of Sciences suggested that the LMF was too large a step to make at once, and that fundamental physics issues still needed to be explored. They recommended further experiments before attempting to move to a 10 MJ system. Nevertheless, the authors were aware of the potential for higher energy requirements, and noted \"Indeed, if it did turn out that a 100 MJ driver were required for ignition and gain, one would have to rethink the entire approach to, and rationale for, ICF\".\n\nBuilding the LMF was estimated to cost about $1 billion. LLNL initially submitted a design with a 5 MJ 350 nm (UV) driver laser that would be able to reach about 200 MJ yield, which was enough to attain the majority of the LMF goals. The program was estimated to cost about $600 million FY 1989 dollars, and an additional $250 million to upgrade it to a full 1,000 MJ if needed, and would grow to well over $1 billion if LMF was to meet all of the goals requested by the DOE. Other labs also proposed their own LMF designs using other technologies.\n\nThe National Academy of Sciences review led to a reevaluation of these plans, and in July 1990, LLNL responded with the Nova Upgrade, which would reuse the majority of the existing Nova facility, along with the adjacent Shiva facility. The resulting system would be much lower power than the LMF concept, with a driver of about 1 MJ. The new design included a number of features that advanced the state of the art in the driver section, including the multi-pass design in the main amplifiers, and 18 beamlines (up from 10) that were split into 288 \"beamlets\" as they entered the target area in order to improve the uniformity of illumination. The plans called for the installation of two main banks of laser beamlines, one in the existing Nova beamline room, and the other in the older Shiva building next door, extending through its laser bay and target area into an upgraded Nova target area. The lasers would deliver about 500 TW in a 4 ns pulse. The upgrades were expected to allow the new Nova to produce fusion yields of between 2 and 10 MJ. The initial estimates from 1992 estimated construction costs around $400 million, with construction taking place from 1995 to 1999.\n\nThroughout this period, the ending of the Cold War led to dramatic changes in defense funding and priorities. As the need for nuclear weapons was greatly reduced and various arms limitation agreements led to a reduction in warhead count, the US was faced with the prospect of losing a generation of nuclear weapon designers able to maintain the existing stockpiles, or design new weapons. At the same time, progress was being made on what would become the Comprehensive Nuclear-Test-Ban Treaty, which would ban all criticality testing. This would make the reliable development of newer generations of nuclear weapons much more difficult.\nOut of these changes came the Stockpile Stewardship and Management Program (SSMP), which, among other things, included funds for the development of methods to design and build nuclear weapons that would work without having to be explosively tested. In a series of meetings that started in 1995, an agreement formed between the labs to divide up the SSMP efforts. An important part of this would be confirmation of computer models using low-yield ICF experiments. The Nova Upgrade was too small to use for these experiments, and a redesign emerged as NIF in 1994. The estimated cost of the project remained just over $1 billion, with completion in 2002.\n\nIn spite of the agreement, the large project cost combined with the ending of similar projects at other labs resulted in several highly critical comments by scientists at other weapons labs, Sandia National Laboratories in particular. In May 1997, Sandia fusion scientist Rick Spielman publicly stated that NIF had \"virtually no internal peer review on the technical issues\" and that \"Livermore essentially picked the panel to review themselves\". A retired Sandia manager, Bob Puerifoy, was even more blunt than Spielman: \"NIF is worthless ... it can't be used to maintain the stockpile, period\".\n\nA contrasting view was expressed by Victor Reis, assistant secretary for Defense Programs within DOE and the chief architect of the Stockpile Stewardship Program. Reis told the U.S. House Armed Services Committee in 1997 that NIF was \"designed to produce, for the first time in a laboratory setting, conditions of temperature and density of matter close to those that occur in the detonation of nuclear weapons. The ability to study the behavior of matter and the transfer of energy and radiation under these conditions is key to understanding the basic physics of nuclear weapons and predicting their performance without underground nuclear testing. Two JASON panels, which are composed of scientific and technical national security experts, have stated that the NIF is the most scientifically valuable of all programs proposed for science-based stockpile stewardship.\n\nDespite the initial criticism, Sandia, as well as Los Alamos, provided support in the development of many NIF technologies, and both laboratories later became partners with NIF in the National Ignition Campaign.\n\nWork on the NIF started with a single beamline demonstrator, Beamlet. Beamlet operated between 1994 and 1997 and was entirely successful. It was then sent to Sandia National Laboratories as a light source in their Z machine. A full-sized demonstrator then followed, in AMPLAB, which started operations in 1997. The official groundbreaking on the main NIF site was in May 29, 1997.\n\nAt the time, the DOE was estimating that the NIF would cost approximately $1.1 billion and another $1 billion for related research, and would be complete as early as 2002. Later in 1997 the DOE approved an additional $100 million in funding and pushed the operational date back to 2004. As late as 1998 LLNL's public documents stated the overall price was $1.2 billion, with the first eight lasers coming online in 2001 and full completion in 2003.\n\nThe physical scale of the facility alone made the construction project challenging. By the time the \"conventional facility\" (the shell for the laser) was complete in 2001, more than 210,000 cubic yards of soil had been excavated, more than 73,000 cubic yards of concrete had been poured, 7,600 tons of reinforcing steel rebar had been placed, and more than 5,000 tons of structural steel had been erected. In addition to its sheer size, building NIF presented a number of unique challenges. To isolate the laser system from vibration, the foundation of each laser bay was made independent of the rest of the structure. Three-foot-thick, 420-foot-long and 80-foot-wide slabs, each containing 3,800 cubic yards of concrete, required continuous concrete pours to achieve their specifications.\n\nThere were also unexpected challenges to cope with: In November, 1997, an El Niño weather front dumped two inches of rain in two hours, flooding the NIF site with 200,000 gallons of water just three days before the scheduled concrete foundation pour. The earth was so soaked that the framing for the retaining wall sank six inches, forcing the crew to disassemble and reassemble it in order to pour the concrete. Construction was halted in December, 1997, when 16,000-year-old mammoth bones were discovered on the construction site. Paleontologists were called in to remove and preserve the bones, and construction restarted within four days.\n\nA variety of research and development, technology and engineering challenges also had to be overcome, such as working with the optics industry to create a precision large optics fabrication capability to supply the laser glass for NIF's 7,500 meter-sized optics. State-of-the-art optics measurement, coating and finishing techniques were needed to withstand NIF's high-energy lasers, as were methods for amplifying the laser beams to the needed energy levels. Continuous-pour glass, rapid-growth crystals, innovative optical switches, and deformable mirrors were among the technology innovations developed for NIF.\n\nSandia, with extensive experience in pulsed power delivery, designed the capacitor banks used to feed the flashlamps, completing the first unit in October 1998. To everyone's surprise, the Pulsed Power Conditioning Modules (PCMs) suffered capacitor failures that led to explosions. This required a redesign of the module to contain the debris, but since the concrete structure of the buildings holding them had already been poured, this left the new modules so tightly packed that there was no way to do maintenance in-place. Yet another redesign followed, this time allowing the modules to be removed from the bays for servicing. Continuing problems of this sort further delayed the operational start of the project, and in September 1999, an updated DOE report stated that NIF would require up to $350 million more and completion would be pushed back to 2006.\n\nThroughout this period the problems with NIF were not being reported up the management chain. In 1999 then Secretary of Energy Bill Richardson reported to Congress that the NIF project was on time and budget, following the information that had been passed onto him by NIF's management. In August that year it was revealed that NIF management had misled Richardson, and in fact neither claim was close to the truth. As the GAO would later note, \"Furthermore, the Laboratory's former laser director, who oversaw NIF and all other laser activities, assured Laboratory managers, DOE, the university, and the Congress that the NIF project was adequately funded and staffed and was continuing on cost and schedule, even while he was briefed on clear and growing evidence that NIF had serious problems\". Richardson later commented \"I have been very concerned about the management of this facility... bad management has overtaken good science. I don't want this to ever happen again\". A DOE Task Force reporting to Richardson late in January 2000 summarized that \"organizations of the NIF project failed to implement program and project management procedures and processes commensurate with a major research and development project... [and that] ...no one gets a passing grade on NIF Management: not the DOE's office of Defense Programs, not the Lawrence Livermore National Laboratory and not the University of California\".\n\nGiven the budget problems, the US Congress requested an independent review by the General Accounting Office (GAO). They returned a highly critical report in August 2000 stating that the budget was likely $3.9 billion, including R&D, and that the facility was unlikely to be completed anywhere near on time. The report, \"\"Management and Oversight Failures Caused Major Cost Overruns and Schedule Delays\",\" identified management problems for the overruns, and also criticized the program for failing to include a considerable amount of money dedicated to target fabrication in the budget, including it in operational costs instead of development.\n\nEarly technical delays and project management issues caused the DOE to begin a comprehensive \"Rebaseline Validation Review of the National Ignition Facility Project\" in 2000, which took a critical look at the project, identifying areas of concern and adjusting the schedule and budget to ensure completion. John Gordon, National Nuclear Security Administrator, stated \"We have prepared a detailed bottom-up cost and schedule to complete the NIF project... The independent review supports our position that the NIF management team has made significant progress and resolved earlier problems\". The report revised their budget estimate to $2.25 billion, not including related R&D which pushed it to $3.3 billion total, and pushed back the completion date to 2006 with the first lines coming online in 2004. A follow-up report the next year included all of these items, pushing the budget to $4.2 billion, and the completion date to around 2008.\n\nA new management team took over the NIF project in September 1999, headed by George Miller (who later became LLNL director 2006-2011), who was named acting associate director for lasers. Ed Moses, former head of the Atomic Vapor Laser Isotope Separation (AVLIS) program at LLNL, became NIF project manager. Since the rebaselining, NIF's management has received many positive reviews and the project has met the budgets and schedules approved by Congress. In October 2010, the project was named \"Project of the Year\" by the Project Management Institute, which cited NIF as a \"stellar example of how properly applied project management excellence can bring together global teams to deliver a project of this scale and importance efficiently.\"\n\nRecent reviews of the project have been positive, generally in keeping with the post-GAO Rebaseline schedules and budgets. However, there were lingering concerns about the NIF's ability to reach ignition, at least in the short term. An independent review by the JASON Defense Advisory Group was generally positive about NIF's prospects over the long term, but concluded that \"The scientific and technical challenges in such a complex activity suggest that success in the early attempts at ignition in 2010, while possible, is unlikely\". The group suggested a number of changes to the completion timeline to bring NIF to its full design power as soon as possible, skipping over a testing period at lower powers that they felt had little value.\n\nIn May 2003, the NIF achieved \"first light\" on a bundle of four beams, producing a 10.4 kJ pulse of IR light in a single beamline. In 2005 the first eight beams (a full bundle) were fired producing 153 kJ of infrared light, thus eclipsing OMEGA as the highest energy laser (per pulse) on the planet. By January 2007 all of the LRUs in the Master Oscillator Room (MOOR) were complete and the computer room had been installed. By August 2007 96 laser lines were completed and commissioned, and \"A total infrared energy of more than 2.5 megajoules has now been fired. This is more than 40 times what the Nova laser typically operated at the time it was the world's largest laser\".\n\nOn January 26, 2009, the final line replaceable unit (LRU) was installed, completing one of the final major milestones of the NIF construction project and meaning that construction was unofficially completed. On February 26, 2009, for the first time NIF fired all 192 laser beams into the target chamber. On March 10, 2009, NIF became the first laser to break the megajoule barrier, firing all 192 beams and delivering 1.1 MJ of ultraviolet light, known as 3ω, to the target chamber center in a shaped ignition pulse. The main laser delivered 1.952 MJ of infrared energy.\n\nOn 29 May 2009 the NIF was dedicated in a ceremony attended by thousands, including California Governor Arnold Schwarzenegger and Senator Dianne Feinstein. The first laser shots into a hohlraum target were fired in late June 2009.\n\nOn January 28, 2010, the facility published a paper reporting the delivery of a 669 kJ pulse to a gold hohlraum, setting new records for power delivery by a laser, and leading to analysis suggesting that suspected interference by generated plasma would not be a problem in igniting a fusion reaction. Due to the size of the test hohlraums, laser/plasma interactions produced plasma-optics gratings, acting like tiny prisms, which produced symmetric X-ray drive on the capsule inside the hohlraum.\n\nAfter gradually altering the wavelength of the laser, they were able to compress a spherical capsule evenly, and were able to heat it up to 3.3 million kelvins (285 eV). The capsule contained cryogenically cooled gas, acting as a substitute for the deuterium and tritium fuel capsules that will be used later on. Plasma Physics Group Leader Dr. Siegfried Glenzer said they've shown they can maintain the precise fuel layers needed in the lab, but not yet within the laser system.\n\nAs of January 2010, the NIF could run as high as 1.8 megajoules. Glenzer said that experiments with slightly larger hohlraums containing fusion-ready fuel pellets would begin before May 2010, slowly ramping up to 1.2 megajoules — enough for ignition according to calculations. But first the target chamber needed to be equipped with shields to block neutrons that a fusion reaction would produce. On June 5, 2010 the NIF team fired lasers at the target chamber for the first time in six months; realignment of the beams took place later in June in preparation for further high-energy operation.\n\nWith the main construction complete, NIF started working on the \"National Ignition Campaign\" (NIC), the quest to reach ignition. By this time, so sure were the experimenters that ignition would be reached that articles began appearing in science magazines stating that it would be announced only a short time after the article was published. Scientific American started a 2010 review article with the statement \"Ignition is close now. Within a year or two...\"\n\nThe first test was carried out on 8 October 2010 at slightly over 1 MJ. However, a number of problems slowed the drive toward ignition-level laser energies in the 1.4 to 1.5 MJ range.\n\nProgress was initially slowed by the potential for damage from overheating due to a concentration of energy on optical components that is greater than anything previously attempted. Other issues included problems layering the fuel inside the targets, and minute quantities of dust being found on the capsule surface.\n\nAs the power was increased and targets of increasing sophistication were used, another problem appeared that was causing an asymmetric implosion. This was eventually traced to minute amounts of water vapor in the target chamber which froze to the windows on the ends of the hohlraums. This was solved by re-designing the hohlraum with two layers of glass on either end, in effect creating a storm window. Steven Koonin, DOE undersecretary for science, visited the lab for an update on the NIC on 23 April, the day after the window problem was announced as solved. On 10 March he had described the NIC as \"a goal of overriding importance for the DOE\" and expressed that progress to date \"was not as rapid as I had hoped\".\n\nNIC shots halted in February 2011, as the machine was turned over to SSMP materials experiments. As these experiments wound down, a series of planned upgrades were carried out, notably a series of improved diagnostic and measurement instruments. Among these changes were the addition of the ARC (Advanced Radiographic Capability) system, which uses 4 of the NIF's 192 beams as a backlighting source for high-speed imaging of the implosion sequence.\n\nARC is essentially a petawatt-class laser with peak power exceeding a quadrillion (10) watts. It is designed to produce brighter, more penetrating, higher-energy x rays than can be obtained with conventional radiographic techniques. When complete, ARC will be the world's highest-energy short-pulse laser, capable of creating picosecond-duration laser pulses to produce energetic x rays in the range of 50-100 keV for backlighting NIF experiments.\n\nNIC runs restarted in May 2011 with the goal of timing the four laser shock waves that compress the fusion target to very high precision. The shots tested the symmetry of the X-ray drive during the first three nanoseconds. Full-system shots fired in the second half of May achieved unprecedented peak pressures of 50 megabars.\n\nIn January 2012, Mike Dunne, director of NIF's laser fusion energy program, predicted in a Photonics West 2012 plenary talk that ignition would be achieved at NIF by October 2012. In the same month, the NIF fired a record high of 57 shots, more than in any month up to that point. On March 15, 2012, NIF produced a laser pulse with 411 trillion watts of peak power. On July 5, 2012, it produced a shorter pulse of 1.85 MJ and increased power of 500 TW.\n\nThe NIC campaign has been periodically reviewed by a team led by Steven E. Koonin, Under Secretary of Science. The 6th review, May 31, 2012 was chaired by David H. Crandall, Advisor on National Security and Inertial Fusion, Koonin being precluded to chair the review because of a conflict of interest. The review was conducted with the same external reviewers, who had previously served Koonin. Each provided their report independently, with their own estimate of the probability of achieving ignition within the plan, i.e. before December 31, 2012. The conclusion of the review was published on July 19, 2012.\n\nThe previous review dated January 31, 2012, identified a number of experimental improvements that have been completed or are under way. The new report unanimously praised the quality of the installation: lasers, optics, targets, diagnostics, operations have all been outstanding, however:\n\nFurther, the report members express deep concerns on the gaps between observed performance and ICF simulation codes such that the current codes are of a limited utility going forward. Specifically, they found a lack of predictive ability of the radiation drive to the capsule and inadequately modeled laser-plasma interactions. These effects lead to pressure being one half to one third of that required for ignition, far below the predicted values. The memo page 5 discusses the mix of ablator material and capsule fuel due likely to hydrodynamics instabilities in the outer surface of the ablator.\n\nThe report goes on to suggest that using a thicker ablator may improve performance, but this increases its inertia. To keep the required implosion speed, they request that the NIF energy be increased to 2MJ. One must also keep in mind that can withstand only a limited amount of energy or risk permanent damage to the optical quality of the lasing medium. The reviewers question whether or not the energy of NIF is sufficient to indirectly compress a large enough capsule to avoid the mix limit and reach ignition. The report concluded that ignition within the calendar year 2012 is 'highly unlikely'.\n\nThe NIC officially ended on September 30, 2012 without achieving ignition. According to numerous articles in the press, Congress was concerned about the project's progress and funding arguments may begin anew. These reports also suggested that NIF will shift its focus away from ignition back toward materials research.\n\nIn 2008, as NIF was reaching completion, LLNL began the Laser Inertial Fusion Energy program, or LIFE, to explore ways to use the NIF technologies as the basis for a commercial power plant design. Early studies considered the fission-fusion hybrid concept, but from 2009 the focus was on pure fusion devices, incorporating a number of technologies that were being developed in parallel with NIF that would greatly improve the performance of the design. All of these, however, were based on the idea that NIF would achieve ignition, and that only minor changes to the basic design would be required to improve performance. In April 2014, Livermore decided to end the LIFE efforts. Bret Knapp, Livermore acting director was quoted as saying that \"The focus of our inertial confinement fusion efforts is on understanding ignition on NIF rather than on the LIFE concept.\"\n\nA memo sent on 29 September 2013 by Ed Moses describes a fusion shot that took place at 5:15 a.m. on 28 September. It produced 5×10 neutrons, 75% more than any previous shot. Alpha heating, a key component of ignition, was clearly seen. It also noted that the reaction released more energy than the \"energy being absorbed by the fuel\", a condition the memo referred to as \"scientific breakeven\". This received significant press coverage as it appeared to suggest a key threshold had been achieved, which was referred to as a \"milestone\".\n\nHowever, a number of researchers pointed out that the experiment was far below ignition, and did not represent a breakthrough as reported. Others noted that the definition of breakeven as recorded in many references, and directly stated by Moses in the past, was when the fusion output was equal to the laser input. In this release, the term was changed to refer to the energy deposited in the fuel, not the energy of the laser. The method used to reach these levels, known as the \"high foot\", is not suitable for general ignition, and as a result, it is still unclear whether NIF will ever reach this goal.\n\nSince that time, NIF has returned to materials studies. Experiments beginning in 2015 FY have used plutonium targets, with a schedule containing 10 to 12 shots for 2015, and as many as 120 over the next 10 years. Plutonium shots simulate the compression of the primary in a nuclear bomb by high explosives, which has not seen direct testing since the Comprehensive Test Ban. Tiny amounts of plutonium are used in these tests, ranging from less than a milligram to 10 milligrams. Similar experiments are also carried out on Sandia's Z machine. The director of LLNL's Primary Nuclear Design Program, Mike Dunning, noted that \"This is an opportunity for us to get high-quality data using a regime that was previously unavailable to us\".\n\nOne key development on NIF since the Ignition Campaign has been an increase in the shot rate. Although designed to allow shots as often as every 4 hours, in 2014 FY NIF performed 191 shots, slightly more than one every two days. This has been continuously improved, and in April 2015 NIF was on track to meet its goal of 300 laser shots in 2015 FY, almost one a day.\n\nOn 28 January 2016, NIF successfully executed its first gas pipe experiment intended to study the absorption of large amounts of laser light within long targets relevant to high-gain Magnetized Liner Inertial Fusion (MagLIF). In order to investigate key aspects of the propagation, stability, and efficiency of laser energy coupling at full scale for high-gain MagLIF target designs, a single quad of NIF was used to deliver 30 kJ of energy to a target during a 13 nanosecond shaped pulse. Data return was very favorable and analysis is ongoing by scientific staff at Lawrence Livermore and Sandia National Laboratories.\n\nSome similar experimental ICF projects are:\n\nThe NIF was used as the set for the starship \"Enterprise\"'s warp core in the 2013 movie \"Star Trek Into Darkness\".\n\n\n"}
{"id": "39051035", "url": "https://en.wikipedia.org/wiki?curid=39051035", "title": "Neutron Star Interior Composition Explorer", "text": "Neutron Star Interior Composition Explorer\n\nThe Neutron star Interior Composition Explorer (NICER) is a NASA Explorers program Mission of Opportunity dedicated to the study of the extraordinary gravitational, electromagnetic, and nuclear physics environments embodied by neutron stars, exploring the exotic states of matter where density and pressure are higher than in atomic nuclei. \"NICER\" will enable rotation-resolved spectroscopy of the thermal and non-thermal emissions of neutron stars in the soft (0.2–12 keV) X-ray band with unprecedented sensitivity, probing interior structure, the origins of dynamic phenomena, and the mechanisms that underlie the most powerful cosmic particle accelerators known. \"NICER\" will achieve these goals by deploying, following launch, an X-ray timing and spectroscopy instrument as an attached payload aboard the International Space Station (ISS). \"NICER\" was selected by NASA to proceed to formulation phase in April 2013.\n\nNICER-SEXTANT uses the same instrument to test X-ray timing for positioning and navigation, and MXS is a test of X-ray timing communication. In January 2018, X-ray navigation was demonstrated using NICER on ISS.\n\nBy May 2015, \"NICER\" was on track for a 2016 launch, having passed its critical design review and resolved an issue with the power being supplied by the ISS. Following the CRS-7 loss in June 2015, which delayed future missions by several months, \"NICER\" was finally launched on 3 June 2017, with the SpaceX CRS-11 ISS resupply mission aboard a Falcon 9 v1.2 rocket.\n\n\"NICER\" primary science instrument, called the X-ray Timing Instrument (XTI), is an array of 56 X-ray photon detectors. These detectors record the energies of the collected photons as well as with their time of arrival. A GPS receiver enables accurate timing and positioning measurements.\n\nDuring each ISS orbit, \"NICER\" will observe two to four targets. Gimbaling and a star tracker allow \"NICER\" to track specific targets while collecting science data. In order to achieve its science objectives, \"NICER\" will take over 15 million seconds of exposures over an 18-month period.\n\nAn enhancement to the \"NICER\" mission, the \"Station Explorer for X-ray Timing and Navigation Technology\" (\"SEXTANT\"), will act as a technology demonstrator for X-ray pulsar-based navigation (XNAV) techniques that may one day be used for deep-space navigation.\n\nAs part of \"NICER\" testing, a rapid-modulation X-ray device was developed called Modulated X-ray Source (MXS), which is being used to create an X-ray communication system (XCOM) demonstration. If approved and installed on the ISS, XCOM will transmit data encoded into X-ray bursts to the \"NICER\" platform, which may lead to the development of technologies that allow for gigabit bandwidth communication throughout the Solar System.\n\nIn May 2018, NICER discovered a X-ray pulsar in the fastest stellar orbit yet discovered. The pulsar and its companion star were found to orbit each other every 38 minutes.\n\n\n"}
{"id": "44497219", "url": "https://en.wikipedia.org/wiki?curid=44497219", "title": "Nocturnal bottleneck", "text": "Nocturnal bottleneck\n\nThe nocturnal bottleneck hypothesis is a hypothesis to explain several mammalian traits. In 1942, Gordon Lynn Walls described this concept which states that placental mammals were mainly or even exclusively nocturnal through most of their evolutionary story, starting with their origin 225 million years ago, and only ending with the demise of the dinosaurs 66 million years ago. While some mammal groups have later evolved to fill diurnal niches, the approximately 160 million years spent as nocturnal animals has left a lasting legacy on basal anatomy and physiology, and most mammals are still nocturnal.\n\nMammals evolved from cynodonts, a group of superficially dog-like mammal-like reptiles in the wake of the Permian–Triassic mass extinction. The emerging archosaurian groups that flourished after the extinction, including crocodiles and dinosaurs and their ancestors, drove the remaining larger cynodonts into extinction, leaving only the smaller forms. The surviving cynodonts could only succeed in niches with minimal competition from the diurnal dinosaurs, evolving into the typical small-bodied insectivorous dwellers of the nocturnal undergrowth. While the early mammals continued to develop into several probably quite common groups of animals during the Mesozoic, they all remained nocturnal.\n\nOnly with the massive extinction at the end of the Cretaceous did the dinosaurs leave the stage open for the establishment of a new fauna of mammals. Despite this, mammals continued to be small-bodied for millions of years. While all the largest animals alive today are mammals, the majority of mammals are still small nocturnal animals.\n\nSeveral different features of mammalian physiology appear to be adaptations to a nocturnal lifestyle, mainly related to the sensory organs. These include:\n\n\n\n"}
{"id": "40976184", "url": "https://en.wikipedia.org/wiki?curid=40976184", "title": "Nova virus", "text": "Nova virus\n\nNova virus (NVAV) is a single-stranded, negative-sense, enveloped RNA orthohantavirus. It is phylogenetically related to other European and Asian hantaviruses that cause hantavirus hemorrhagic fever with renal syndrome. No known human cases of infection have been reported.\n\nNova virus was first isolated in European moles (\"Talpa europaea\") found in Hungary and France. Previously it was believed that rodents were the principal reservoir hosts, but field trapping has discovered hantavirus species in insectivoire bats, shrews, and moles (\"Soricidae\" and \"Talpidae\").\n\n"}
{"id": "21847877", "url": "https://en.wikipedia.org/wiki?curid=21847877", "title": "Otto Laporte Award", "text": "Otto Laporte Award\n\nThe Otto Laporte Award (1972–2003) was an annual award by the American Physical Society (APS) to \"recognize outstanding contributions to fluid dynamics\" and to honour Otto Laporte (1902–1971). It was established as the Otto Laporte Memorial Lectureship by the APS Division of Fluid Dynamics in 1972, and became an APS award in 1985. The Otto Laporte Award was merged into the Fluid Dynamics Prize in 2004, in order to obtain one major prize in fluid dynamics by the APS. \n\n\n"}
{"id": "303802", "url": "https://en.wikipedia.org/wiki?curid=303802", "title": "Phase rule", "text": "Phase rule\n\nThe phase rule is a general principle governing systems in thermodynamic equilibrium. If \"F\" is the number of degrees of freedom, \"C\" is the number of components and \"P\" is the number of phases, then \nIt was derived by Josiah Willard Gibbs in his landmark paper titled \"On the Equilibrium of Heterogeneous Substances\", published in parts between 1875 and 1878.\nThe rule assumes the components do not react with each other.\n\nThe number of degrees of freedom is the number of independent intensive variables, i.e. the largest number of thermodynamic parameters such as temperature or pressure that can be varied simultaneously and arbitrarily without determining one another. An example of one-component system is a system involving one pure chemical, while two-component systems, such as mixtures of water and ethanol, have two chemically independent components, and so on. Typical phases are solids, liquids and gases.\n\n\nThe basis for the rule (Atkins and de Paula, justification 6.1) is that equilibrium between phases places a constraint on the intensive variables. More rigorously, since the phases are in thermodynamic equilibrium with each other, the chemical potentials of the phases must be equal. The number of equality relationships determines the number of degrees of freedom. For example, if the chemical potentials of a liquid and of its vapour depend on temperature (\"T\") and pressure (\"p\"), the equality of chemical potentials will mean that each of those variables will be dependent on the other. Mathematically, the equation , where \"μ\" = chemical potential, defines temperature as a function of pressure or vice versa. (Caution: do not confuse \"p\" = pressure with \"P\" = number of phases.)\n\nTo be more specific, the composition of each phase is determined by intensive variables (such as mole fractions) in each phase. The total number of variables is , where the extra two are temperature \"T\" and pressure \"p\". The number of constraints is , since the chemical potential of each component must be equal in all phases. Subtract the number of constraints from the number of variables to obtain the number of degrees of freedom as .\n\nThe rule is valid provided the equilibrium between phases is not influenced by gravitational, electrical or magnetic forces, or by surface area, and only by temperature, pressure, and concentration.\n\nFor pure substances so that . In a single phase () condition of a pure component system, two variables (), such as temperature and pressure, can be chosen independently to be any pair of values consistent with the phase. However, if the temperature and pressure combination ranges to a point where the pure component undergoes a separation into two phases (), decreases from 2 to 1. When the system enters the two-phase region, it becomes no longer possible to independently control temperature and pressure.\n\nThe critical point is the black dot at the end of the liquid–gas boundary. As this point is approached, the liquid and gas phases become progressively more similar until, at the critical point, there is no longer a separation into two phases. Above the critical point and away from the phase boundary curve, and the temperature and pressure can be controlled independently. Hence there is only one phase, and it has the physical properties of a dense gas, but is also referred to as a supercritical fluid.\n\nOf the other two-boundary curves, one is the solid–liquid boundary or melting point curve which indicates the conditions for equilibrium between these two phases, and the other at lower temperature and pressure is the solid–gas boundary.\n\nEven for a pure substance, it is possible that three phases, such as solid, liquid and vapour, can exist together in equilibrium (). If there is only one component, there are no degrees of freedom () when there are three phases. Therefore, in a single-component system, this three-phase mixture can only exist at a single temperature and pressure, which is known as a triple point. Here there are two equations , which are sufficient to determine the two variables T and p. In the diagram for CO the triple point is the point at which the solid, liquid and gas phases come together, at 5.2 bar and 217 K. It is also possible for other sets of phases to form a triple point, for example in the water system there is a triple point where ice I, ice III and liquid can coexist.\n\nIf four phases of a pure substance were in equilibrium (), the phase rule would give , which is meaningless, since there cannot be −1 independent variables. This explains the fact that four phases of a pure substance (such as ice I, ice III, liquid water and water vapour) are not found in equilibrium at any temperature and pressure. In terms of chemical potentials there are now three equations, which cannot in general be satisfied by any values of the two variables \"T\" and \"p\", although in principle they might be solved in a special case where one equation is mathematically dependent on the other two. In practice, however, the coexistence of more phases than allowed by the phase rule normally means that the phases are not all in true equilibrium.\n\nFor binary mixtures of two chemically independent components, so that . In addition to temperature and pressure, the other degree of freedom is the composition of each phase, often expressed as mole fraction or mass fraction of one component.\nAs an example, consider the system of two completely miscible liquids such as toluene and benzene, in equilibrium with their vapours. This system may be described by a boiling-point diagram which shows the composition (mole fraction) of the two phases in equilibrium as functions of temperature (at a fixed pressure).\n\nFour thermodynamic variables which may describe the system include temperature (\"T\"), pressure (\"p\"), mole fraction of component 1 (toluene) in the liquid phase (\"x\"), and mole fraction of component 1 in the vapour phase (\"x\"). However, since two phases are present () in equilibrium, only two of these variables can be independent (). This is because the four variables are constrained by two relations: the equality of the chemical potentials of liquid toluene and toluene vapour, and the corresponding equality for benzene.\n\nFor given \"T\" and \"p\", there will be two phases at equilibrium when the overall composition of the system (system point) lies in between the two curves. A horizontal line (isotherm or tie line) can be drawn through any such system point, and intersects the curve for each phase at its equilibrium composition. The quantity of each phase is given by the lever rule (expressed in the variable corresponding to the \"x\"-axis, here mole fraction).\n\nFor the analysis of fractional distillation, the two independent variables are instead considered to be liquid-phase composition (x) and pressure. In that case the phase rule implies that the equilibrium temperature (boiling point) and vapour-phase composition are determined.\n\nLiquid–vapour phase diagrams for other systems may have azeotropes (maxima or minima) in the composition curves, but the application of the phase rule is unchanged. The only difference is that the compositions of the two phases are equal exactly at the azeotropic composition.\n\nFor applications in materials science dealing with phase changes between different solid structures, pressure is often imagined to be constant (for example at one atmosphere), and is ignored as a degree of freedom, so the rule becomes\nThis is sometimes misleadingly called the \"condensed phase rule\", but it is not applicable to condensed systems which are subject to high pressures (for example, in geology), since the effects of these pressures can be important.\n\n"}
{"id": "30542341", "url": "https://en.wikipedia.org/wiki?curid=30542341", "title": "RST model", "text": "RST model\n\nThe Russo–Susskind–Thorlacius model or RST model in short is a modification of the CGHS model to take care of conformal anomalies. In the CGHS model, if we include Faddeev-Popov ghosts to gauge-fix diffeomorphisms in the conformal gauge, they contribute an anomaly of -24. Each matter field contributes an anomaly of 1. So, unless N=24, we will have gravitational anomalies.\nTo the CGHS action\nis added, where \"κ\" is either formula_3 or formula_4 depending upon whether ghosts are considered. The nonlocal term leads to nonlocality.\nIn the conformal gauge,\n\nIt might appear as if the theory is local in the conformal gauge, but this overlooks the fact that the Raychaudhuri equations are still nonlocal.\n"}
{"id": "58796512", "url": "https://en.wikipedia.org/wiki?curid=58796512", "title": "Rajni Rawat", "text": "Rajni Rawat\n\nRajni Rawat is the only transgender politician in Uttarakhand, served as the Vice president of Women Empowerment & Child Development (Women Commission, Uttarakhand).\n\n"}
{"id": "3424567", "url": "https://en.wikipedia.org/wiki?curid=3424567", "title": "Ranked society", "text": "Ranked society\n\nA ranked society in anthropology is one that ranks individuals in terms of their genealogical distance from the chief. Closer relatives of the chief have higher rank or social status than more distant ones. When individuals and groups rank about equally, competition for positions of leadership may occur. In some cases rank is assigned to entire villages rather than individuals or families. The idea of a ranked society was criticized by Max Weber and Karl Marx. Ranks in ranked society are the different levels, platforms, or classes that determine someone’s influence on political aspects, votes, decision making, etc. A person’s ranking also gives them societal power ( power within their civilisation ). \n\n"}
{"id": "44984325", "url": "https://en.wikipedia.org/wiki?curid=44984325", "title": "Replication crisis", "text": "Replication crisis\n\nThe replication crisis (or replicability crisis or reproducibility crisis) is an ongoing (2018) methodological crisis primarily affecting the social sciences in which scholars have found that the results of many scientific studies are difficult or impossible to replicate or reproduce on subsequent investigation, either by independent researchers or by the original researchers themselves. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem.\n\nBecause the reproducibility of experiments is an essential part of the scientific method, the inability to replicate the studies of others has potentially grave consequences for many fields of science in which significant theories are grounded on unreproducible experimental work.\n\nThe replication crisis has been particularly widely discussed in the field of psychology (and in particular, social psychology) and in medicine, where a number of efforts have been made to re-investigate classic results, and to attempt to determine both the reliability of the results, and, if found to be unreliable, the reasons for the failure of replication.\n\nAccording to a 2016 poll of 1,500 scientists reported in the journal \"Nature\", 70% of them had failed to reproduce at least one other scientist's experiment (50% had failed to reproduce one of their own experiments).\n\nIn 2009, 2% of scientists admitted to falsifying studies at least once and 14% admitted to personally knowing someone who did. Misconducts were reported more frequently by medical researchers than others.\n\nSeveral factors have combined to put psychology at the center of the controversy. Much of the focus has been on the area of social psychology, although other areas of psychology such as clinical psychology have also been implicated.\n\nFirstly, questionable research practices (QRPs) have been identified as common in the field. Such practices, while not intentionally fraudulent, involve capitalizing on the gray area of acceptable scientific practices or exploiting flexibility in data collection, analysis, and reporting, often in an effort to obtain a desired outcome. Examples of QRPs include selective reporting or partial publication of data (reporting only some of the study conditions or collected dependent measures in a publication), optional stopping (choosing when to stop data collection, often based on statistical significance of tests), \"p\"-value rounding (rounding \"p\"-values down to 0.05 to suggest statistical significance), file drawer effect (nonpublication of data), post-hoc storytelling (framing exploratory analyses as confirmatory analyses), and manipulation of outliers (either removing outliers or leaving outliers in a dataset to cause a statistical test to be significant). A survey of over 2,000 psychologists indicated that a majority of respondents admitted to using at least one QRP. False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are an inherent hazard in the field, requiring a certain degree of skepticism on the part of readers.\n\nSecondly, psychology and social psychology in particular, has found itself at the center of several scandals involving outright fraudulent research, most notably the admitted data fabrication by Diederik Stapel as well as allegations against others. However, most scholars acknowledge that fraud is, perhaps, the lesser contribution to replication crises.\n\nThird, several effects in psychological science have been found to be difficult to replicate even before the current replication crisis. For example the scientific journal \"Judgment and Decision Making\" has published several studies over the years that fail to provide support for the unconscious thought theory. Replications appear particularly difficult when research trials are pre-registered and conducted by research groups not highly invested in the theory under questioning.\n\nThese three elements together have resulted in renewed attention for replication supported by Kahneman. Scrutiny of many effects have shown that several core beliefs are hard to replicate. A recent special edition of the journal \"Social Psychology\" focused on replication studies and a number of previously held beliefs were found to be difficult to replicate. A 2012 special edition of the journal \"Perspectives on Psychological Science\" also focused on issues ranging from publication bias to null-aversion that contribute to the replication crises in psychology. In 2015, the first open empirical study of reproducibility in Psychology was published, called the Reproducibility Project. Researchers from around the world collaborated to replicate 100 empirical studies from three top Psychology journals. Fewer than half of the attempted replications were successful at producing statistically significant results in the expected directions, though most of the attempted replications did produce trends in the expected directions.\n\nMany research trials and meta-analyses are compromised by poor quality and conflicts of interest that involve both authors and professional advocacy organizations, resulting in many false positives regarding the effectiveness of certain types of psychotherapy.\n\nAlthough the British newspaper \"The Independent\" wrote that the results of the reproducibility project show that much of the published research is just \"psycho-babble\", the replication crisis does not necessarily mean that psychology is unscientific. Rather this process is a healthy if sometimes acrimonious part of the scientific process in which old ideas or those that cannot withstand careful scrutiny are pruned, although this pruning process is not always effective. The consequence is that some areas of psychology once considered solid, such as social priming, have come under increased scrutiny due to failed replications.\n\nNobel laureate and professor emeritus in psychology Daniel Kahneman argued that the original authors should be involved in the replication effort because the published methods are often too vague. Others such as Dr. Andrew Wilson disagree and argue that the methods should be written down in detail. An investigation of replication rates in psychology in 2012 indicated higher success rates of replication in replication studies when there was author overlap with the original authors of a study (91.7% successful replication rates in studies with author overlap compared to 64.6% success replication rates without author overlap).\n\nA report by the Open Science Collaboration in August 2015 that was coordinated by Brian Nosek estimated the reproducibility of 100 studies in psychological science from three high-ranking psychology journals. Overall, 36% of the replications yielded significant findings (\"p\" value below 0.05) compared to 97% of the original studies that had significant effects. The mean effect size in the replications was approximately half the magnitude of the effects reported in the original studies.\n\nThe same paper examined the reproducibility rates and effect sizes by journal (\"Journal of Personality and Social Psychology\" [JPSP], \"\" [JEP:LMC], \"Psychological Science\" [PSCI]) and discipline (social psychology, cognitive psychology). Study replication rates were 23% for JPSP, 38% for JEP:LMC, and 38% for PSCI. Studies in the field of cognitive psychology had a higher replication rate (50%) than studies in the field of social psychology (25%).\n\nAn analysis of the publication history in the top 100 psychology journals between 1900 and 2012 indicated that approximately 1.6% of all psychology publications were replication attempts. Articles were considered a replication attempt if the term \"replication\" appeared in the text. A subset of those studies (500 studies) was randomly selected for further examination and yielded a lower replication rate of 1.07% (342 of the 500 studies [68.4%] were actually replications). In the subset of 500 studies, analysis indicated that 78.9% of published replication attempts were successful. The rate of successful replication was significantly higher when at least one author of the original study was part of the replication attempt (91.7% relative to 64.6%).\n\nA study published in 2018 in \"Nature Human Behaviour\" sought to replicate 21 social and behavioral science papers from \"Nature\" and \"Science,\" finding that only 13 could be successfully replicated.\n\nHighlighting the social structure that discourages replication in psychology, Brian D. Earp and Jim A. C. Everett enumerated five points as to why replication attempts are uncommon\n\nWith the replication crisis of psychology earning attention, Princeton University psychologist Susan Fiske drew controversy for calling out critics of psychology. She called these unnamed \"adversaries\" names such as \"methodological terrorist\" and \"self-appointed data police\", and said that criticism of psychology should only be expressed in private or through contacting the journals. Columbia University statistician and political scientist Andrew Gelman, \"well-respected among the researchers driving the replication debate\", responded to Fiske, saying that she had found herself willing to tolerate the \"dead paradigm\" of faulty statistics and had refused to retract publications even when errors were pointed out. He added that her tenure as editor has been abysmal and that a number of published papers edited by her were found to be based on extremely weak statistics; one of Fiske's own published papers had a major statistical error and \"impossible\" conclusions.\n\nOut of 49 medical studies from 1990–2003, with more than 1000 citations, 45 claimed that studied therapy was effective. Out of these studies, 16% were contradicted by subsequent studies, 16% had found stronger effects than did subsequent studies, 44% were replicated, and 24% remained largely unchallenged. Food and Drug Administration in 1977–90 found flaws in 10–20% of medical studies. In a paper published in 2012, Glenn Begley, a biotech consultant working at Amgen, and Lee Ellis, at the University of Texas, argued that only 11% of the pre-clinical cancer studies could be replicated.\n\nA 2016 article by John Ioannidis, Professor of Medicine and of Health Research and Policy at Stanford University School of Medicine and a Professor of Statistics at Stanford University School of Humanities and Sciences, elaborated on \"Why Most Clinical Research Is Not Useful\". In the article Ioannidis laid out some of the problems and called for reform, characterizing certain points for medical research to be useful again – one example he made was the need for medicine to be \"Patient Centered\" (e.g. in the form of the Patient-Centered Outcomes Research Institute) instead of the current practice to mainly take care of \"the needs of physicians, investigators, or sponsors\". Ioannidis is known for his research focus on science itself since the 2005 paper \"Why Most Published Research Findings Are False\".\n\nMarketing is another discipline with a \"desperate need\" for replication. Many famous marketing studies fail to be repeated upon replication, a notable example being the \"too-many-choices\" effect, in which a high number of choices of product makes a consumer less likely to purchase. In addition to the previously mentioned arguments, replications studies in marketing are needed to examine the applicability of theories and models across countries and cultures, which is especially important because of possible influences of globalization.\n\nA 2016 study in the journal \"Science\" found that two-thirds of 18 experimental studies from two top-tier economics journals (\"American Economic Review\" and the \"Quarterly Journal of Economics\") successfully replicated. A 2017 study in the \"Economic Journal\" suggested that \"the majority of the average effects in the empirical economics literature are exaggerated by a factor of at least 2 and at least one-third are exaggerated by a factor of 4 or more\".\n\nA 2018 study took the field of exercise and sports science to task for insufficient replication studies, limited reporting of null results and trivial results, and insufficient research transparency. Statisticians have criticized sports science for common use of a controversial statistical method called \"magnitude-based inference\" that has allowed sports scientists to extract apparently significant results from noisy data where ordinary hypothesis testing would have found none.\n\nIn a work published in 2015 Glenn Begley and John Ioannidis offer five bullets as to summarize the present predicaments:\n\nIn fact some predictions of a possible crisis in the quality control mechanism of science can be traced back several decades, especially among scholars in science and technology studies (STS). Derek de Solla Price – considered the father of scientometrics – predicted that science could reach 'senility' as a result of its own exponential growth. Some present day literature seems to vindicate this 'overflow' prophesy, lamenting at decay in both attention and quality.\n\nPhilosopher and historian of science Jerome R. Ravetz predicted in his 1971 book \"Scientific Knowledge and Its Social Problems\" that science – in moving from the little science made of restricted communities of scientists to big science or techno-science – would suffer major problems in its internal system of quality control. Ravetz anticipated that modern science's system of rewarding scientists for research might become dysfunctional, the present 'publish or perish' challenge, creating perverse incentives to publish any findings however dubious. For Ravetz quality in science is maintained when there is a community of scholars linked by norms and standards, and a willingness to stand by these.\n\nHistorian Philip Mirowski offered more recently a similar diagnosis in his 2011 book \"Science Mart\" (2011). 'Mart' is here a reference to the retail giant 'Walmart' and an allusion to the commodification of science. In the analysis of Mirowski, when science becomes a commodity being traded in a market, its quality collapses. Mirowski argues his case by tracing the decay of science to the decision of major corporations to close their in-house laboratories in order to outsource their work to universities, and subsequently to move their research away from universities to even cheaper contract research organizations (CRO).\n\nThe crisis of science's quality control system is affecting the use of science for policy. This is the thesis of a recent work by a group of STS scholars, who identify in 'evidence based (or informed) policy' a point of present tension. Economist Noah Smith suggests that a factor in the crisis has been the overvaluing of research in academia and undervaluing of teaching ability, especially in fields with few major recent discoveries.\n\nReplication has been referred to as \"the cornerstone of science\". Replication studies attempt to evaluate whether published results reflect true findings or false positives. The integrity of scientific findings and reproducibility of research are important as they form the knowledge foundation on which future studies are built.\n\nMain article: Registered report\n\nA recent innovation in scientific publishing to address the replication crisis is through the use of registered reports. The registered report format requires authors to submit a description of the study methods and analyses prior to data collection. Once the method and analysis plan is vetted through peer-review, publication of the findings is provisionally guaranteed, based on whether the authors follow the proposed protocol. One goal of registered reports is to circumvent the publication bias toward significant findings that can lead to implementation of questionable research practices and to encourage publication of studies with rigorous methods.\n\nThe journal \"Psychological Science\" has encouraged the preregistration of studies and the reporting of effect sizes and confidence intervals. The editor in chief also noted that the editorial staff will be asking for replication of studies with surprising findings from examinations using small sample sizes before allowing the manuscripts to be published.\n\nMoreover, only a very small proportion of academic journals in psychology and neurosciences explicitly stated that they welcome submissions of replication studies in their aim and scope or instructions to authors. This phenomenon does not encourage the reporting or even attempt on replication studies.\n\nBased on coursework in experimental methods at MIT and Stanford, it has been suggested that methods courses in psychology emphasize replication attempts rather than original studies. Such an approach would help students learn scientific methodology and provide numerous independent replications of meaningful scientific findings that would test the replicability of scientific findings. Some have recommended that graduate students should be required to publish a high-quality replication attempt on a topic related to their doctoral research prior to graduation.\n\nMany publications require a \"p\"-value of \"p\" < 0.05 to claim statistical significance. The paper \"Redefine statistical significance\", signed by a large number of scientists and mathematicians, proposes that in \"fields where the threshold for defining statistical significance for new discoveries is \"P\" < 0.05, we propose a change to \"P\" < 0.005. This simple step would immediately improve the reproducibility of scientific research in many fields.\"\n\nTheir rationale is that \"a leading cause of non-reproducibility (is that the) statistical standards of evidence for claiming new discoveries in many fields of science are simply too low. Associating 'statistically significant' findings with \"P\" < 0.05 results in a high rate of false positives even in the absence of other experimental, procedural and reporting problems.\"\n\nAlthough statisticians are unanimous that use of the \"p\" < 0.05 provides weaker evidence than is generally appreciated, there is a lack of unanimity about what should be done about it. Some have advocated that Bayesian methods should replace \"p\"-values. This has not happened on a wide scale, partly because it is complicated, and partly because many users distrust the specification of prior distributions in the absence of hard data. A simplified version of the Bayesian argument, based on testing a point null hypothesis was suggested by Colquhoun (2014, 2017). The logical problems of inductive inference were discussed in \"The problem with p-values\" (2016).\n\nThe hazards of reliance on \"p\"-values were emphasized by pointing out that even observation of \"p\" = 0.001 was not necessarily strong evidence against the null hypothesis. Despite the fact that the likelihood ratio in favour of the alternative hypothesis over the null is close to 100, if the hypothesis was implausible, with a prior probability of a real effect being 0.1, even the observation of \"p\" = 0.001 would have a false positive risk of 8 percent. It wouldn't even reach the 5 percent level.\n\nIt was recommended that the terms \"significant\" and \"non-significant\" should not be used. \"p\"-values and confidence intervals should still be specified, but they should be accompanied by an indication of the false positive risk. It was suggested that the best way to do this is to calculate the prior probability that would be necessary to believe in order to achieve a false positive risk of, say, 5%. The calculations can be done with R scripts that are provided, or, more simply, with a web calculator. This so-called reverse Bayesian approach, which was suggested by Matthews (2001), is one way to avoid the problem that the prior probability is rarely known.\n\nTo improve the quality of replications, larger sample sizes than those used in the original study are often needed. Larger sample sizes are needed because estimates of effect sizes in published work are often exaggerated due to publication bias and large sampling variability associated with small sample sizes in an original study. Further, using significance thresholds usually leads to inflated effects, because particularly with small sample sizes, only the largest effects will become significant.\n\nOnline repositories where data, protocols, and findings can be stored and evaluated by the public seek to improve the integrity and reproducibility of research. Examples of such repositories include the Open Science Framework, Registry of Research Data Repositories, and Psychfiledrawer.org. Sites like Open Science Framework offer badges for using open science practices in an effort to incentivize scientists. However, there has been concern that those who are most likely to provide their data and code for analyses are the researchers that are likely the most sophisticated. John Ioannidis at Stanford University suggested that \"the paradox may arise that the most meticulous and sophisticated and method-savvy and careful researchers may become more susceptible to criticism and reputation attacks by reanalyzers who hunt for errors, no matter how negligible these errors are\".\n\nIn July 2016 the Netherlands Organisation for Scientific Research made 3 million Euros available for replication studies. The funding is for replication based on reanalysis of existing data and replication by collecting and analysing new data. Funding is available in the areas of social sciences, health research and healthcare innovation.\n\nIn 2013 the Laura and John Arnold Foundation funded the launch of The Center for Open Science with a $5.25 million grant and by 2017 had provided an additional $10 million in funding. It also funded the launch of the Meta-Research Innovation Center at Stanford at Stanford University run by John Ioannidis and Steven Goodman to study ways to improve scientific research. It also provided funding for the AllTrials initiative led in part by Ben Goldacre.\n\nMarcus R. Munafò and George Davey Smith argue, in a piece published by \"Nature\", that research should emphasize triangulation, not just replication. They claim that,\n\"replication alone will get us only so far (and) might actually make matters worse... We believe that an essential protection against flawed ideas is triangulation. This is the strategic use of multiple approaches to address one question. Each approach has its own unrelated assumptions, strengths and weaknesses. Results that agree across different methodologies are less likely to be artefacts... Maybe one reason replication has captured so much interest is the often-repeated idea that falsification is at the heart of the scientific enterprise. This idea was popularized by Karl Popper's 1950s maxim that theories can never be proved, only falsified. Yet an overemphasis on repeating experiments could provide an unfounded sense of certainty about findings that rely on a single approach... philosophers of science have moved on since Popper. Better descriptions of how scientists actually work include what epistemologist Peter Lipton called in 1991 \"inference to the best explanation\".\"\n\n\n"}
{"id": "57870343", "url": "https://en.wikipedia.org/wiki?curid=57870343", "title": "Rocket Men", "text": "Rocket Men\n\nRocket Men: The Daring Odyssey of Apollo 8 and the Astronauts Who Made Man's First Journey to the Moon is a 2018 nonfiction book by Robert Kurson recounting NASA's 1968 Apollo 8 mission, which was the first manned spacecraft to reach the Moon and return safely to Earth. The book is Kurson's fourth, and it debuted on the \"New York Times\" bestseller list.\n\nKurson drew on hundreds of hours of one-on-one interviews with NASA staff, industry experts, astronauts (including all three Apollo 8 astronauts) and their families as source material for the book.\n\nIn November 2017 \"The Martian\" author Andy Weir reported that he read an advanced copy of \"Rocket Men\".\n\n\"Rocket Men\" is an account of the Apollo 8 mission with focus on Frank Borman, Jim Lovell and William Anders, the three astronauts aboard during the mission. The book also places an emphasis on the astronauts' families during the mission.\n\nFrom The \"Washington Post\":\n\nThe book includes chapters dedicated to each astronaut, the Space Race itself, and background and chronological progress of the mission including critical maneuvers and mission setbacks. It is set against the backdrop of 1968, considered by many to be among the most divisive and violent years in American history.\n\nThe book reached #7 on the \"New York Times\" bestseller list and has received positive reviews from critics. The \"USA Today\" called \"Rocket Men\" a \"first-rate account of this remarkable spaceflight\" and added, \"There are many pieces to the Apollo 8 story, but Kurson brings them together effortlessly.\" The \"New York Times\" called the book \"gripping\" and \"a riveting introduction to the [Apollo 8] flight\" in which \"Kurson details the mission in crisp, suspenseful scenes.\" Writing for The \"Washington Post\", Mary Roach compared the book to Alfred Lansing's \"\" and Jon Krakauer's \"Into Thin Air\" and called Kurson's writing style, \"as close to a movie as writing gets.\"\n\nThe film rights to \"Rocket Men\" were secured by Makeready prior to the book's publication.\n\n"}
{"id": "406618", "url": "https://en.wikipedia.org/wiki?curid=406618", "title": "Scientific literature", "text": "Scientific literature\n\nScientific literature comprises scholarly publications that report original empirical and theoretical work in the natural and social sciences, and within an academic field, often abbreviated as the literature. Academic publishing is the process of contributing the results of one's research into the literature, which often requires a peer-review process. Original scientific research published for the first time in scientific journals is called the primary literature. Patents and technical reports, for minor research results and engineering and design work (including computer software), can also be considered primary literature. Secondary sources include review articles (which summarize the findings of published studies to highlight advances and new lines of research) and books (for large projects or broad arguments, including compilations of articles). Tertiary sources might include encyclopedias and similar works intended for broad public consumption.\n\nScientific literature can include the following kinds of publications:\n\n\nLiterature may also be published in areas considered to be \"grey\", as they are published outside of traditional channels. This material is traditionally not indexed by major databases and can include manuals, theses and dissertations, or newsletters and bulletins. \n\nThe significance of different types of the scientific publications can vary between disciplines and change over time. According to James G. Speight and Russell Foote, peer-reviewed journals are the most prominent and prestigious form of publication. Generally books published by university presses are usually considered more prestigious than those published by commercial presses. The status of working papers and conference proceedings depends on the discipline; they are typically more important in the applied sciences. The value of publication as a preprint or scientific report on the web has in the past been low, but in some subjects, such as mathematics or high energy physics, it is now an accepted alternative.\n\nThere are ten different types of scientific papers. Eight of these carry specific objectives, while the other two can vary depending on the style and the intended goal.\n\nPapers that carry specific objectives are: \n\n\nThese two have varying types of scientific classifications and can range from historical articles to speeches:\n\n\nThe actual day-to-day records of scientific information are kept in research notebooks or logbooks. These are usually kept indefinitely as the basic evidence of the work, and are often kept in duplicate, signed, notarized, and archived. The purpose is to preserve the evidence for scientific priority, and in particular for priority for obtaining patents. They have also been used in scientific disputes. Since the availability of computers, the notebooks in some data-intensive fields have been kept as database records, and appropriate software is commercially available.\n\nThe work on a project is typically published as one or more technical reports, or articles. In some fields both are used, with preliminary reports, working papers, or preprints followed by a formal article. Articles are usually prepared at the end of a project, or at the end of components of a particularly large one. In preparing such an article vigorous rules for scientific writing have to be followed.\n\nOften, career advancement depends upon publishing in high-impact journals, which, especially in hard and applied sciences, are usually published in English. Consequently, scientists with poor English writing skills are at a disadvantage when trying to publish in these journals, regardless of the quality of the scientific study itself. Yet many international universities require publication in these high-impact journals by both their students and faculty. One way that some international authors are beginning to overcome this problem is by contracting with freelance medical copy editors who are native speakers of English and specialize in ESL (English as a second language) editing to polish their manuscripts' English to a level that high-impact journals will accept.\n\nA scientific article has a standardized structure, which varies only slightly in different subjects. Although the IMRAD structure emphasizes the organization of content and in scientific journal articles, each section (Introduction, Methods, Results, and Discussion) has unique conventions for scientific writing style.\n\nUltimately, it is not the format that is important, but what lies behind it--the content. However, several key formatting requirements need to be met:\n\n\nThough peer review and the learned journal format are not themselves an essential part of scientific literature, they are both convenient ways of ensuring that the above fundamental criteria are met. They are essentially a means of quality control, a term which also encompasses other means towards the same goal.\n\nThe \"quality\" being referred to here is the scientific one, which consists of transparency and repeatability of the research for independent verification, the validity of the conclusions and interpretations drawn from the reported data, overall importance for advance within a given field of knowledge, novelty, and in certain fields applicability as well. The lack of peer review is what makes most technical reports and World Wide Web publications unacceptable as contributions to the literature. The relatively weak peer review often applied to books and chapters in edited books means that their status is also second-tier, unless an author's personal standing is so high that prior achievement and a continued stake in one's reputation within the scientific community signals a clear expectation of quality.\n\nThe emergence of institutional digital repositories where scholars can post their work as it is submitted to a print-based journal has taken formal peer review into a state of flux. Though publicizing a preprint online does not prevent it from being peer reviewed, it does allow an unreviewed copy to be widely circulated. On the positive side this change has led to faster dissemination of novel work within the scientific community; on the negative it has made it more difficult to discern a valid scientific contribution from the unmeritorious.\n\nIncreasing reliance on abstracting services, especially on those available electronically, means that the effective criterion for whether a publication format forms part of the established, trusted literature is whether it is covered by these services; in particular, by the specialised service for the discipline concerned such as Chemical Abstracts Service, and by the major interdisciplinary services such as those marketed by the Institute for Scientific Information.\n\nThe transfer of copyright from author to publisher, used by some journals, can be controversial because many authors want to propagate their ideas more widely and re-use their material elsewhere without the need for permission. Usually an author or authors circumvent that problem by rewriting an article and using other pictures. Some publishers may also want publicity for their journal so will approve facsimile reproduction unconditionally; other publishers are more resistant. \n\nIn terms of research publications, a number of key issues include and are not restricted to:\nThe first recorded editorial pre-publication peer-review occurred in 1665 by the founding editor of \"Philosophical Transactions of the Royal Society\", Henry Oldenburg.\n\nTechnical and scientific books were a specialty of David Van Nostrand, and his Engineering Magazine re-published contemporary scientific articles.\n\n\n"}
{"id": "37308162", "url": "https://en.wikipedia.org/wiki?curid=37308162", "title": "Station P (ocean measurement site)", "text": "Station P (ocean measurement site)\n\nStation P is an ocean measurement site, located at 50 degrees north latitude, 145 degrees west longitude (water depth, 4220 meters).\n\nThe site was established by the US Navy in 1943. In 1951, US funding to maintain continual presence ran out and observational responsibility was passed to Canada. The site was manned continuously until 1981. Starting in 2007, automated observations have been made by the National Oceanic and Atmospheric Administration \n"}
{"id": "27659451", "url": "https://en.wikipedia.org/wiki?curid=27659451", "title": "The Cat in the Hat Knows a Lot About That!", "text": "The Cat in the Hat Knows a Lot About That!\n\nThe Cat in the Hat Knows a Lot About That! is a British-Canadian-American Animated television series that premiered on August 7, 2010 on Treehouse TV in Canada, on September 6, 2010 on PBS Kids in the US and also in the UK on CITV and Tiny Pop. The award-winning series is based on Random House's Beginner Books franchise and \"The Cat in the Hat's Learning Library\", and is developed by Portfolio Entertainment, Random House Children Entertainment and Collingwood O'Hare Productions in conjunction with KQED and CBC Kids. The first season has 40 half-hour episodes. PBS Kids renewed it for a second season of 20 episodes which premiered on September 10, 2012. A 20 episode third season is currently in development.\n\nEach episode features The Cat in the Hat (voiced by Martin Short), who leads neighbours Nick and Sally, the Fish, and Thing One and Thing Two on a variety of adventures in his \"Thinga-ma-jigger\", a Seussian contraption that can sprout wings, pontoons, booster rockets, change size, and do just about anything else necessary to further the adventure. The adventures are prompted by a question posed by either Nick or Sally at the beginning of each episode, which will inevitably lead them around the globe to \"make natural science discoveries.\" Similar to other PBS Kids series such as \"Curious George\" and \"Sid the Science Kid\", \"The Cat in the Hat Knows a Lot About That!\" focuses on introducing preschoolers to various science and learning concepts.\n\n\n\n\nVarious DVDs were released by NCircle Entertainment beginning in late October 2010 where every DVD is in region 1, subtitled in English and Spanish, approximately 60 minutes except for a few, in Dolby Digital, in 5.1 surround and Dolby surround stereo in Spanish. Every DVD has the bonus segments from the show.\n\n\n\n"}
{"id": "8368868", "url": "https://en.wikipedia.org/wiki?curid=8368868", "title": "The Ethos Effect", "text": "The Ethos Effect\n\nThe Ethos Effect (2003) is a science fiction novel by American writer L. E. Modesitt, Jr., a sequel to \"The Parafaith War\". It is set in a future where humanity has spread to the stars and divided into several factions. Many factions including the Eco-Tech Coalition, the Revenants of the Prophet (\"revs\") and the Taran Empire are engaged in escalating conflict over territory and their competing social philosophies. Against this background, former Taran Empire officer Van C. Albert is recruited by the mysterious Trystin Desoll to work for the equally mysterious Integrated Information Systems.\n\nWhile this novel shares many themes that can be found in other novels by Modesitt, \"Ethos\" adds elements of Societal Ethics rather than only involving personal ethics. This is one of the only books written by Modesitt where a person enacts major political and structural changes in the culture into which the protagonist was raised.\n\nJackie Cassada in her review for \"The Library Journal\" called this novel a \"fast-paced yet complexly plotted work of military sf portrays three-dimensional characters caught up in internal and external battles as they search for ways to combine personal integrity with their professional duties. A thoughtfully written, hard-hitting tale that belongs in most collections.\" Peter Cannon in his review for Publishers Weekly said that \"despite some expository lumps and wooden characterization, thoughtful SF readers will appreciate this weighty tale of humanitarian intentions and social speculations.\" Frieda Murray reviewing for Booklist said \"Modesitt knows well how to\nkeep this kind of soup at a boil.\"\n\n"}
{"id": "11236140", "url": "https://en.wikipedia.org/wiki?curid=11236140", "title": "The New Green Consumer Guide", "text": "The New Green Consumer Guide\n\nThe New Green Consumer Guide is a practical lifestyle book written by Julia Hailes and published by Simon & Schuster on May 21, 2007. The aim of the book is to make the ordinary layman engage in the role that he or she must play in the green mission of today.\n\nJulia Hailes is a freelance consultant and well-known speaker on social, environmental and ethical issues. Her work includes environmental reviews, management briefings and advising companies on policy, strategy and communications.\n\nJulia wrote the original Green Consumer Guide in 1987. It was the first book of its kind and sold a million copies worldwide.\n\nThe New Green Consumer Guide is a completely new book, full colour, redesigned. This is a practical lifestyle book and its overall message is that living a modern lifestyle doesn’t mean we have to waste energy and resources.\n\nOne reviewer concluded \"This is a sensible and rational primer for anyone who doesn't want to be patronised but does want a practical and down-to-earth set of suggestions for making environmental lifestyle changes.\".\n\n"}
{"id": "52590", "url": "https://en.wikipedia.org/wiki?curid=52590", "title": "The Population Bomb", "text": "The Population Bomb\n\nThe Population Bomb is a best-selling book written by Stanford University Professor Paul R. Ehrlich and his wife, Anne Ehrlich (who was uncredited), in 1968. It warned of mass starvation of humans in the 1970s and 1980s due to overpopulation, as well as other major societal upheavals, and advocated immediate action to limit population growth. Fears of a \"population explosion\" were widespread in the 1950s and 1960s, but the book and its author brought the idea to an even wider audience.\"\"' \n\nThe book has been criticized since its publishing for its alarmist tone, and in recent decades for its inaccurate predictions. The Ehrlichs stand by the basic ideas in the book, stating in 2009 that \"perhaps the most serious flaw in \"The Bomb\" was that it was much too optimistic about the future\" and believe that it achieved their goals because \"it alerted people to the importance of environmental issues and brought human numbers into the debate on the human future.\"\n\n\"The Population Bomb\" was written at the suggestion of David Brower the executive director of the environmentalist Sierra Club, and Ian Ballantine of Ballantine Books following various public appearances Ehrlich had made regarding population issues and their relation to the environment. Although the Ehrlichs collaborated on the book, the publisher insisted that a single author be credited, and also asked to change their preferred title: \"Population, Resources, and Environment.\" The title \"Population Bomb\" was taken (with permission) from General William H. Draper, founder of the Population Crisis Committee and a pamphlet issued in 1954 by the Hugh Moore Fund. The Ehrlichs regret the choice of title, which they admit was a perfect choice from a marketing perspective, but think that \"it led Paul to be miscategorized as solely focused on human numbers, despite our interest in all the factors affecting the human trajectory.\"\n\nEarly editions of \"The Population Bomb\" began with the statement:\nMuch of the book is spent describing the state of the environment and the food security situation, which is described as increasingly dire. Ehrlich argues that as the existing population was not being fed adequately, and as it was growing rapidly it was unreasonable to expect sufficient improvements in food production to feed everyone. He further argued that the growing population placed escalating strains on all aspects of the natural world. \"What needs to be done?\" he wrote, \"We must rapidly bring the world population under control, reducing the growth rate to zero or making it negative. Conscious regulation of human numbers must be achieved. Simultaneously we must, at least temporarily, greatly increase our food production.\" Ehrlich described a number of \"ideas on how these goals \"might\" be reached.\" He believed that the United States should take a leading role in population control, both because it was already consuming much more than the rest of the world, and therefore had a moral duty to reduce its impact, and because the US would have to lead international efforts due to its prominence in the world. In order to avoid charges of hypocrisy or racism it would have to take the lead in population reduction efforts. Ehrlich floats the idea of adding \"temporary sterilants\" to the water supply or staple foods. However, he rejects the idea as unpractical due to \"criminal inadequacy of biomedical research in this area.\" He suggests a tax scheme in which additional children would add to a family's tax burden at increasing rates for more children, as well as luxury taxes on childcare goods. He suggests incentives for men who agree to permanent sterilization before they have two children, as well as a variety of other monetary incentives. He proposes a powerful Department of Population and Environment which \"should be set up with the power to take whatever steps are necessary to establish a reasonable population size in the United States and to put an end to the steady deterioration of our environment.\" The department should support research into population control, such as better contraceptives, mass sterilizing agents, and prenatal sex discernment (because families often continue to have children until a male is born. Ehrlich suggested that if they could choose a male child this would reduce the birthrate). Legislation should be enacted guaranteeing the right to an abortion, and sex education should be expanded.\n\nAfter explaining the domestic policies the US should pursue, he discusses foreign policy. He advocates a system of \"triage,\" such as that suggested by William and Paul Paddock in \"Famine 1975!\". Under this system countries would be divided into categories based on their abilities to feed themselves going forward. Countries with sufficient programmes in place to limit population growth, and the ability to become self-sufficient in the future would continue to receive food aid. Countries, for example India, which were \"so far behind in the population-food game that there is no hope that our food aid will see them through to self-sufficiency\" would have their food aid eliminated. Ehrlich argued that this was the only realistic strategy in the long-term. Ehrlich applauds the Paddocks' \"courage and foresight\" in proposing such a solution. Ehrlich further discusses the need to set up public education programs and agricultural development schemes in developing countries. He argues that the scheme would likely have to be implemented outside the framework of the United Nations due to the necessity of being selective regarding the targeted regions and countries, and suggests that within countries certain regions should be prioritized to the extent that cooperative separatist movements should be encouraged if they are an improvement over the existing authority. He mentions his support for government mandated sterilization of Indian males with three or more children.\n\nIn the rest of the book Ehrlich discusses things which readers can do to help. This is focused primarily on changing public opinion to create pressure on politicians to enact the policies he suggests, which he believed were not politically possible in 1968. At the end of the book he discusses the possibility that his forecasts may be wrong, a fact which he felt he must acknowledge as a scientist. However, he believes that humanity will only be better off if it follows his prescriptions, so that even if he is incorrect it is the right course of action.\n\nThe book sold over two million copies, raised the general awareness of population and environmental issues, and influenced 1960s and 1970s public policy.\n\nIn 1948, two widely read books were published that would inspire a \"neo-Malthusian\" debate on population and the environment: Fairfield Osborn’s \"Our Plundered Planet\" and William Vogt’s \"Road to Survival\". Although, they are now much less well known than \"Population Bomb\", they inspired many works such as the original \"Population Bomb\" pamphlet by Hugh Everett Moore in 1954 that inspired the name of Ehrlich's book, as well as some of the original societies concerned with population and environmental matters. D.B. Luten has said that although the book is often seen as a seminal work in the field, the \"Population Bomb\" is actually best understood as \"climaxing and in a sense terminating the debate of the 1950s and 1960s.” Ehrlich has said that he traced his own Malthusian beliefs to a lecture he heard Vogt give when he was attending university in the early 1950s. For Ehrlich, these writers provided “a global framework for things he had observed as a young naturalist.\"\n\nThe \"Population Bomb\" has been characterized by critics as primarily a repetition of the Malthusian catastrophe argument that population growth will outpace agricultural growth unless controlled. Ehrlich observed that since about 1930 the population of the world had doubled within a single generation, from 2 billion to nearly 4 billion, and was on track to do so again. He assumed that available resources on the other hand, and in particular food, were nearly at their limits. Some critics compare Ehrlich unfavorably to Malthus, saying that although Thomas Malthus did not make a firm prediction of imminent catastrophe, Ehrlich warned of a potential massive disaster within the next decade or two. In addition, critics state that unlike Malthus, Ehrlich did not see any means of avoiding the disaster entirely (although some mitigation was possible), and proposed solutions that were much more radical than those discussed by Malthus, such as starving whole countries that refused to implement population control measures.\n\nEhrlich was certainly not unique in his neo-Malthusian predictions, and there was a widespread belief in the 1960s and 70s that increasingly catastrophic famines were on their way.\n\nThe Ehrlichs made a number of specific predictions that did not come to pass, for which they have received criticism. They have acknowledged that some predictions were incorrect. However, they maintain that their general argument remains intact, that their predictions were merely illustrative, that their and others' warnings caused preventive action, or that many of their predictions may yet come true (see Ehrlich's response below). Still other commentators have criticized the Ehrlichs' perceived inability to acknowledge mistakes, evasiveness, and refusal to alter their arguments in the face of contrary evidence. In 2015 Ehrlich told Retro Report, \"I do not think my language was too apocalyptic in \"The Population Bomb.\" My language would be even more apocalyptic today.\"\n\nIn \"The Population Bomb\"s opening lines the authors state that nothing can prevent famines in which hundreds of millions of people will die during the 1970s (amended to 1970s and 80s in later editions), and that there would be \"a substantial increase in the world death rate.\" Although many lives could be saved through dramatic action, it was already too late to prevent a substantial increase in the global death rate. However, in reality the global death rate has continued to decline substantially since then, from 13/1000 in 1965–74 to 10/1000 from 1985–1990. Meanwhile, the population of the world has more than doubled, while calories consumed/person have increased 24%. The UN does not keep official death-by-hunger statistics so it is hard to measure whether the \"hundreds of millions of deaths\" number is correct. Ehrlich himself suggested in 2009 that between 200-300 million had died of hunger since 1968. However, that is measured over 40 years rather than the ten to twenty foreseen in the book, so it can be seen as significantly fewer than predicted.\n\nFamine has not been eliminated, but its root cause has been political instability, not global food shortage. The Indian economist and Nobel Prize winner, Amartya Sen, has argued that nations with democracy and a free press have virtually never suffered from extended famines. And while a 2010 UN report stated that 925 million of the world's population of nearly seven billion people were in a constant state of hunger, it also notes that the percentage of the world's population who qualify as \"undernourished\" has fallen by more than half, from 33 percent to about 16 percent, since Ehrlich published \"The Population Bomb.\"\n\nEhrlich writes: \"I don't see how India could possibly feed two hundred million more people by 1980.\" This view was widely held at the time, as another statement of his, later in the book: \"I have yet to meet anyone familiar with the situation who thinks that India will be self-sufficient in food by 1971.\" In the book's 1971 edition, the latter prediction was removed, as the food situation in India suddenly improved.\n\nAs of 2010, India had almost 1.2 billion people, having nearly tripled its population from around 400 million in 1960. India's Total Fertility Rate in 2008 was calculated to be 2.6. While the absolute numbers of malnourished children in India is high, the rates of malnutrition and poverty in India have declined from approximately 90% at the time of India's independence, to less than 40% today. Ehrlich's prediction about famines were found to be false, although food security is an issue in India. However, most epidemiologists, public health physicians and demographers identify corruption as the chief cause of malnutrition, not \"overpopulation\". As Nobel Prize–winning economist Amartya Sen noted, India frequently had famines during British colonial rule. However, since India became a democracy, there have been no recorded famines.\n\nJournalist Dan Gardner has criticized Ehrlich both for his overconfident predictions and his refusal to acknowledge his errors. \"In two lengthy interviews, Ehrlich admitted making not a single major error in the popular works he published in the late 1960s and early 1970s … the only flat-out mistake Ehrlich acknowledges is missing the destruction of the rain forests, which happens to be a point that supports and strengthens his world view—and is therefore, in cognitive dissonance terms, not a mistake at all. Beyond that, he was by his account, off a little here and there, but only because the information he got from others was wrong. Basically, he was right across the board.\"\n\nJonathan Last called it \"one of the most spectacularly foolish books ever published\".\n\nOne frequent criticism of \"The Population Bomb\" is that it focused on spectacle and exaggeration at the expense of accuracy. Pierre Desrochers and Christine Hoffbauer remark that \"at the time of writing \"The Population Bomb\", Paul and Anne Ehrlich should have been more cautious and revised their tone and rhetoric, in light of the undeniable and already apparent errors and shortcomings of Osborn and Vogt’s analyses.\" Charles Rubin has written that it was precisely because Ehrlich was largely unoriginal and wrote in a clear emotionally gripping style that it became so popular. He quotes a review from \"Natural History\" noting that Ehrlich does not try to \"convince intellectually by mind dulling statistics,\" but rather roars \"like an Old Testament Prophet.\" Gardner says, \"as much as the events and culture of the era, Paul Ehrlich's style explain the enormous audience he attracted.\" Indeed, an appearance on \"The Tonight Show Starring Johnny Carson\" helped to propel the success of the book, as well as Ehrlich's celebrity. Desrochers and Hoffbauer go on to conclude that it seems hard to deny that using an alarmist tone and emotional appeal were the main lessons that the present generation of environmentalists learned from Ehrlich's success.\n\nOn the political left the book received criticism that it was focusing on \"the wrong problem\", and that the real issue was one of distribution of resources rather than of overpopulation. Marxists worried that Ehrlich's work could be used to justify genocide and imperial control, as well as oppression of minorities and disadvantaged groups or even a return to eugenics. Barry Commoner argued that the Ehrlichs were too focused on overpopulation as the source of environmental problems, and that their proposed solutions were politically unacceptable because of the coercion that they implied, and because the cost would fall disproportionately on the poor. He argued that technological, and above all social development would lead to a natural decrease in both population growth and environmental damage.\n\nIn a 2004 \"Grist Magazine\" interview, Ehrlich acknowledged some specific predictions he had made, in the years around the time \"The Population Bomb\" was published, that had \"not\" come to pass. However, as to a number of his fundamental ideas and assertions he maintained that facts and science proved them correct.\n\nIn answer to the question: \"Were your predictions in \"The Population Bomb\" right?\", Ehrlich responded:\n\nIn another retrospective article published in 2009, Ehrlich said, in response to criticism that many of his predictions had not come to pass:\n\n\n\n"}
{"id": "32975011", "url": "https://en.wikipedia.org/wiki?curid=32975011", "title": "The Principles of Quantum Mechanics", "text": "The Principles of Quantum Mechanics\n\nThe Principles of Quantum Mechanics is an influential monograph on quantum mechanics written by Paul Dirac and first published by Oxford University Press in 1930.\nDirac gives an account of quantum mechanics by \"demonstrating how to construct a completely new theoretical framework from scratch\"; \"problems were tackled top-down, by working on the great principles, with the details left to look after themselves\". It leaves classical physics behind after the first chapter, presenting the subject with a logical structure. Its 82 sections contain 785 equations with no diagrams.\n\nDirac is credited with developing the subject \"particularly in Cambridge and Göttingen between 1925–1927\" (Farmelo).\n\nThe first and second editions of the book were published in 1930 and 1935.\n\nIn 1947 the third edition of the book was published, in which the chapter on quantum electrodynamics was rewritten particularly with the inclusion of electron-positron creation.\n\nIn the fourth edition, 1958, the same chapter was revised, adding new sections on interpretation and applications. Later a revised fourth edition appeared in 1967.\n\nBeginning with the third edition (1947), the mathematical descriptions of quantum states and operators were changed to use the Bra–ket notation, introduced in 1939 and largely developed by Dirac himself \n\n\n"}
{"id": "2786441", "url": "https://en.wikipedia.org/wiki?curid=2786441", "title": "Thermal Emission Spectrometer", "text": "Thermal Emission Spectrometer\n\nThe Thermal Emission Spectrometer (TES) is an instrument on board Mars Global Surveyor. TES collects two types of data, hyperspectral thermal infrared data from 6 to 50 micrometres (μm) and bolometric visible-NIR (0.3 to 2.9 μm) measurements. TES has six detectors arranged in a 2x3 array, and each detector has a field of view of approximately 3 × 6 km on the surface of Mars.\nThe TES instrument uses the natural harmonic vibrations of the chemical bonds in materials to determine the composition of gases, liquids, and solids.\nTES identified a large (30,000 square-kilometer) area that contained the mineral olivine. Olivine was found in the Nili Fossae formation. It is thought that the ancient impact that created the Isidis basin resulted in faults that exposed the olivine. Olivine is present in many mafic volcanic rocks. In the presence of water it weathers into minerals such as goethite, chlorite, smectite, maghemite, and hematite. Olivine was also discovered in many other small outcrops within 60 degrees north and south of the equator. Olivine has also been found in the SNC (shergottite, nakhlite, and chassigny) meteorites that are generally accepted to have come from Mars. Later studies have found the olivine-rich rocks to cover over 113,000 square kilometers. That is 11 times larger than the five volcanoes on the Big Island of Hawaii.\n\n\n"}
{"id": "5772843", "url": "https://en.wikipedia.org/wiki?curid=5772843", "title": "Transportation Research Board", "text": "Transportation Research Board\n\nThe Transportation Research Board (TRB) is a division of the National Research Council of the United States which serves as an independent adviser to the President of the United States, the Congress and federal agencies on scientific and technical questions of national importance. It is jointly administered by the National Academy of Sciences, the National Academy of Engineering, and the National Academy of Medicine.\n\nAs one of seven major divisions of the National Academies of Sciences, Engineering, and Medicine, the TRB promotes innovation and progress in transportation through research in an objective and interdisciplinary setting. It stimulates research and offers research management services that promote technical excellence; provides expert advice on transportation policy and programs; and disseminates research results broadly and encourages their implementation. The TRB hosts some 200 standing committees that address specific aspects of transport and the annual TRB conference attracts more than 13,000 attendees.\n\nThe Transportation Research Board was established in 1920 as 'National Advisory Board on Highway Research', as the 'Highway Research Board' from 1925 until 1974 when it was renamed again as the 'Transportation Research Board'. Initially being solely involved in the sharing of information, it has commissioned ad-hoc research since 1950, became more involved in multi-modal transport in the 1960s and has extended its operations further more recently.\n\nTRB fulfills this mission through the work of its more than 200 standing committees and task forces addressing all modes and aspects of transportation; publication and dissemination of reports and peer-reviewed technical papers on research findings; management of cooperative research and other research programs; conduct of special studies on transportation policy issues at the request of the U.S. Congress and government agencies; operation of an on-line computerized file of transportation research information; and the hosting of an annual meeting that typically attracts more than 13,000 transportation professionals from throughout the United States and abroad.\n\nThe Board's activities are organized as follows:\n\n\nTRB's varied activities annually draw on over 7,000 engineers, scientists, and other transportation researchers from the public and private sectors and academia, who contribute expertise in the public interest by participating on TRB committees, panels, and task forces. The program is supported by state transportation departments, the various administrations of the U.S. Department of Transportation and other federal agencies, industry associations, and other organizations and individuals interested in the development of transportation.\n\nPublications include the Highway Capacity Manual and \"Transportation Research Record: Journal of the Transportation Research Board\". Transportation Research Information Services (TRIS) offers several databases for researchers:\n\n"}
{"id": "36584185", "url": "https://en.wikipedia.org/wiki?curid=36584185", "title": "Vasiliy Lindholm", "text": "Vasiliy Lindholm\n\nVasiliy Adolfovich Lindholm (; 1874 – September 17, 1935), also published as Wilhelm Adolf Lindholm, was a Russian malacologist and herpetologist.\n\nLindholm was a curator at the Zoological Museum of the Zoological Institute of the Russian Academy of Sciences in Leningrad. He published works on the molluscs of Lake Baikal, the Crimea, the Caucasus and other parts of the U.S.S.R., and on Palaearctic molluscs generally. He also studied amphibians and reptiles, and described three new species of reptiles. A frog \"Afrixalus lindholmi\" is named for him, sometimes known as Lindholm's banana frog.\n"}
