{"id": "8247788", "url": "https://en.wikipedia.org/wiki?curid=8247788", "title": "A660 road", "text": "A660 road\n\nThe A660 is a major road in the Leeds, and Bradford districts of West Yorkshire, England that runs from Leeds city centre to Burley-in-Wharfedale where it meets the A65. (The A65 also starts in Leeds and runs parallel to, and south of, the A660. It continues to Ilkley, Skipton, Settle and Kendal as a main trans-Pennine route.) The A660 is approximately long, and crosses the watershed from Airedale to lower Wharfedale. For most of its length the road is in the metropolitan district of the City of Leeds; the last is in City of Bradford district.\n\nThe A660 starts in Leeds city centre, officially at the junction of Woodhouse Lane and Claypit Lane. The northbound carriageway passes Leeds Metropolitan University's \"civic quarter\" campus, crosses the A58(M) (Leeds Inner Ring Road), which is in a cutting, and passes the University of Leeds with its landmark tower. The southbound carriageway is separated, at one stage by several terraced streets. The carriageways combine before crossing Woodhouse Moor to Hyde Park. The road passes the buildings of Leeds Girls' High School and climbs to Headingley, passing St Michael's Church and many shops and bars including the Arndale Centre. Shortly after the church, a plaque shows the site of the original oak tree which gave its name to the wapentake of Skyrack (\"Shire oak\"): this is reflected in two pub names and nearby \"Shire Oak Road\". The road continues through Far Headingley and West Park.\n\nThis stretch of the A660 was the route of the first tram service in Leeds (to Far Headingley) and is still a major bus corridor, with several peak hour bus lanes. It suffers from high congestion, and there were historic plans to create a \"Headingley bypass\". Line 2 of the proposed Leeds Supertram was to be on this road, as is the \"North Route\" proposed in 2009 for New Generation Transport trolleybus service. \n\nMany students live near the road, and its many licensed premises host the Otley Run pub crawl.\n\nThe road has several names in this section: \"Woodhouse Lane\" from \"Claypit Lane\" to Hyde Park (but \"Blenheim Walk\" for the separated southbound carriageway), \"Headingley Lane\" between Hyde Park and St Michael's church, Headingley, and \"Otley Road\" beyond this point.\n\nThe A660 crosses the A6120 (Leeds Ring Road - officially \"Leeds Outer Ring Road\") at Lawnswood, and then passes the western part of Adel. At Golden Acre Park the road crosses the Leeds Country Way footpath, which uses the underpass taking pedestrians from the car park to the park. The road passes east of the main part of Bramhope village, and the next major intersection is the A658 Bradford to Harrogate road: \"Pool Bank\" is a steep hill leading north-east from this junction down to Pool-in-Wharfedale. The road skirts the northern slopes of The Chevin before bypassing Otley to the south along the line of the former \"Otley and Ilkley Joint Railway\" (opened 1865, closed 1967). At the end of the Otley bypass the road crosses the A6038 (Otley to Bradford), and then follows the River Wharfe before joining the A65 at the roundabout at the start of the Burley-in-Wharfedale bypass. \n\nAbout before its end, the road crosses into City of Bradford metropolitan district, having been in the City of Leeds district for the rest of its length. The road is only separated from the county of North Yorkshire by a few yards and the width of the river Wharfe, which here forms the county boundary.\n\nThe road is known as \"Otley Road\" until Golden Acre Park, then \"Leeds Road\" until the start of the Otley bypass.\n\nIn the early 18th century the main road from Leeds to Otley ran via Burley and Cookridge and over the top of The Chevin, while the road from Leeds to Headingley was only a country lane. The Leeds-Otley Turnpike Trust was established in 1755, and improved the road from Leeds to Headingley and thence to Cookridge to join the existing road. There were toll bars in Woodhouse Lane and at Otley, and in 1775 a third toll house was built in Headingley village. In 1836 the trustees commissioned George Hayward to design a new route to bypass the steep slopes of The Chevin. The necessary Act of Parliament was passed in 1837, and the new stretch of road was opened in 1842, following today's route along the north side of the hill. Tolls on the road were abolished in January 1867, following the Leeds Improvement Act of 1866.\n\nThe first suburban horse-drawn bus service in Leeds was that to Far Headingley along this road, started in 1838, providing five services daily. The first horse-drawn tram in Leeds followed, in 1871. Fittingly, the number 1 bus service operated by First Leeds still runs on a route which includes the A660 from the city centre past Headingley and Lawnswood before turning off towards Holt Park.\n\n"}
{"id": "37339103", "url": "https://en.wikipedia.org/wiki?curid=37339103", "title": "Aeromonas phage phiO18P", "text": "Aeromonas phage phiO18P\n\nAeromonas phage phiO18P is a virus of the family Myoviridae, genus \"Hp1likevirus\".\n"}
{"id": "24020274", "url": "https://en.wikipedia.org/wiki?curid=24020274", "title": "Akrochordite", "text": "Akrochordite\n\nAkrochordite is an exceptionally rare hydrated hydrous arsenate mineral of the formula (Mn,Mg)(AsO)(OH)4HO and represents a small group of rare in the nature manganese (Mn) arsenates and, similarly to most other Mn-bearing arsenates, possess pinkish colour. It is typically associated with metamorphic Mn deposits.\n"}
{"id": "3887135", "url": "https://en.wikipedia.org/wiki?curid=3887135", "title": "Albert Einstein Medal", "text": "Albert Einstein Medal\n\nThe Albert Einstein Medal is an award presented by the Albert Einstein Society in Bern. First given in 1979, the award is presented to people for \"scientific findings, works, or publications related to Albert Einstein\" each year.\n\n\n"}
{"id": "873819", "url": "https://en.wikipedia.org/wiki?curid=873819", "title": "Aleksandr Pavlovich Aleksandrov", "text": "Aleksandr Pavlovich Aleksandrov\n\nAleksandr Pavlovich Aleksandrov (; born February 20, 1943) is a former Soviet cosmonaut and twice Hero of the Soviet Union (November 23, 1983, and December 29, 1987).\n\nBorn in Moscow, Russia, he graduated from Moscow Bauman-Highschool in 1969 with a doctorate degree, specialised on spacecraft steering systems.\n\nHe was selected as cosmonaut on December 1, 1978. For his first spaceflight, he flew as Flight Engineer on Soyuz T-9, which lasted from June to November 1983. For his second spaceflight, he replaced one of the long-duration crew members of Mir EO-2. For the spaceflight, he was launched with the spacecraft Soyuz TM-3 in July 1987, and landed with the same spacecraft in December 1987. All together he spent 309 days, 18 hours, 2 minutes in space. He served as backup for Soyuz T-8, Soyuz T-13, and Soyuz T-15.\n\nHe resigned from the cosmonaut team on October 26, 1993, when he became chief of NPOE Cosmonaut-group; since 1996 he is Chief flight test directorate of RKKE. He is married with two children.\n\n"}
{"id": "925845", "url": "https://en.wikipedia.org/wiki?curid=925845", "title": "Alexander Keyserling", "text": "Alexander Keyserling\n\nCount Alexander Friedrich Michael Lebrecht Nikolaus Arthur von Keyserling (15 August 1815 in Kabile Parish – 8 May 1891 in Raikküla) was a Baltic German geologist and paleontologist from the Keyserlingk family of Baltic German nobility.\n\nA descendant of Herman Karl von Keyserling, Alexander is considered to be one of the founders of Russian geology. He made many expeditions on behalf of Nicholas I of Russia in Estonia, northern Russia, and the Urals (1839-1846).\n\nHe was also a botanist and zoologist who wrote \"Die wirbelthiere Europa's\" (\"Vertebrates of Europe\") with Johann Heinrich Blasius. This work was published in 1840.\n\nAlexander's nephews include diplomat and writer Eduard von Keyserling. , the first leader of the Latvian Navy, was his brother Eduard Ernst Hermann von Keyserling's grandson. Philosopher Hermann von Keyserling was his grandson.\n\nKeyserling was an advocate of the transmutation of species. In 1853, he wrote an article which suggested that species arose from the activity of \"alien molecules\" acting on the embryo. He believed that such molecules were transported by miasma. In the third edition of \"On the Origin of Species\" published in 1861, Charles Darwin added a \"Historical Sketch\" that acknowledged the ideas of Keyserling.\n\nDarwin sent a copy of his book to Keyserling who was skeptical about the role of natural selection in evolution. By 1886, however, he embraced most of Darwin's ideas claiming \"I renounced my views which contradicted Darwin's theory, and I consider that the changes of the embryo arise not by means of external action of certain molecules but by the influence of selection and heredity.\"\n\nKeyserling is commemorated in the scientific name of a species of gecko, \"Teratoscincus keyserlingi\".\n\n"}
{"id": "18690003", "url": "https://en.wikipedia.org/wiki?curid=18690003", "title": "Arp 104", "text": "Arp 104\n\nArp 104, also known as Keenan's system, is entry 104 in Halton Arp's Atlas of Peculiar Galaxies catalog for spiral galaxy NGC 5216 and globular galaxy NGC 5218. The two galaxies are joined by a bridge of galactic material spanning 22 000 light years.\n\nIn 1790 William Herschel discovered the galaxies, and in 1926 they were studied by Edwin Hubble. In 1935 Philip C. Keenan first published a paper about the bridge connecting the galaxies, which was rediscovered in 1958 at the Lick and Palomar observatories.\n\n"}
{"id": "24212673", "url": "https://en.wikipedia.org/wiki?curid=24212673", "title": "Associative group analysis", "text": "Associative group analysis\n\nAssociative Group Analysis (AGA) is an inferential approach to analyze people’s mental representations, focusing on subjective meanings and images to assess similarities and differences across cultures and belief systems. Culture can be regarded as \"a group-specific cognitive organization or world view composed of the mosaic elements of meanings \". A language, as a communication tool in daily life, contains culturally specific meanings for people who use it. The words people use reflect not only their cognitions, but also their affections and behavioral intentions. To understand differences in psychological meaning across cultures, it is useful to analyze words in a language. The words people use reflect their thinking or feeling. Thinking, or more precisely the cognitive process, together with feeling, guides most of human behavior. By using AGA, we are able to understand how different groups organize and integrate their perceptions and understandings of the world around them.\n\nAGA assumes a close relationship between people’s subjective understandings and their behavior. The verbal associations are determined largely by a decoding of meaning reaction. The disposition of associations then guides the overt reaction. AGA defines the stimulus word as the unit of analysis (rather than individuals, groups, or society, etc.) and as the key unit in the perceptual representational system. By analyzing free verbal associations, researchers can determine the vertical and horizontal structure of the belief system.\n\nThe perceptual representational system includes what people perceive and think about an issue, object, behavior, etc. It is an inclusive worldview, composed of interdependent, representational units. There are three characteristics central to the perceptual-representational system.\n\nAmong the representational units, some are more salient or dominant than others. For example, \"Free market\" is more salient to capitalistic countries than to communist countries.\n\nSome units cluster into a larger category, sharing similar meanings and thus increasing the strength of selected views and beliefs. For example, the theme \"Self\" to some groups denotes individual self since people might associate this word with \"Me,\" \"Individual,\" \"Esteem,\" \"Person,\" etc. However, for other groups of people, the concept of self is a social self. They associate it with \"Society,\" \"Family,\" \"Responsibility,\" etc. These clusters identify the culture, beliefs, and assumptions that can help us predict areas of motivation, vulnerability, need and concern within the group.\n\nThe representational units are tinted with emotions, feelings and evaluations. E.g., \"Marijuana\" may convey negative images like \"hell\" or \"illegal\" for some groups of people where its usage is illegal, but neutral meanings for others. \n\nFrom the above three characteristics, the AGA method focuses on three main categories of information:\n\nAGA is not used as a survey instrument. It is a sociological approach, with the primary goal of assessing people’s subjective representation of their experiences as conveyed by their priorities, perceptions, and meanings. Therefore, the AGA approach is closer to anthropological strategies that intensively assess culturally representative small groups rather than to strategies that use carefully organized large samples. Since statistical significance is not the primary concern, a sample of 50 to 100 respondents is sufficient. However, if the group is quite heterogeneous with considerable variation among subjects, a larger number of subjects is needed ().\n\nSubjects are given a card with stimulus word (theme) in their native language. Each card lists one theme on multiple lines and includes space for writing down subjects’ free associations to the stimulus word. Cards are given in a random order and subjects are told to give any response that occurred to them in the context of the theme within one minute. After one minute, another card is given. To conduct a reasonably comprehensive study, 50 to 100 themes should be presented. For an in-depth study, 100 to 200 systematically selected themes are required.\n\nAfter collecting data, scores are assigned to responses to indicate the relative importance of that response to the theme’s psychological meaning. The weights are assigned to each response according to the proximity of the response to the stimulus word, in a consecutive order of 6, 5, 4, 3, 3, 3, 3, 2, 2, 1, 1……\n\nThe group responses contain a rich source of culturally-specific information. The dominant mindset is the group’s most salient themes configured with their themes of closest affinity, presented by semantographs.\n\nDifferences in meaning of individual themes can be shown by using semantographs. Figure 2 shows how Russian and American managers associate the theme “Freedom.” American associations are indicated in blue, and the Russian associations are indicated in red. The vertical axis contains the associated words for the two groups, and the horizontal axis represents the weighted score for each associated word.\n\nThe characteristics of the AGA method make it well suited for research on cultural/belief change and comparative studies of cultural differences among national groups. Kelly used AGA in a curriculum study. In a curriculum development project called “Justice and the City,” to evaluate whether the concept of justice is learnt by students from the curriculum project, 41 themes grouped into four basic domains (Basic Values, Means/End, Analytical Units, and Political-Economic Orientation domains) were given to students. A content analysis of the responses to the stimulus word “justice,” revealed that the meaning of justice was substantially changed in the experiment. Students in all urban affairs/public policy classes listed specific kinds of justice (e.g., religious, corporate, natural, liberal, Marxist) and public policy issues in the cards while these responses did not appear in the pretest. \n\nAnother AGA study was used in assessing the cultural adaption of Filipinos who had been in the U.S. Navy. Three groups of Filipinos were compared to similarly composed groups of Americans: those who were newly recruited to the U.S. Navy, those who had been in the Navy between 1 and 10 years, and those who had been in the Navy from 11 to 25 years. The results indicated that cultural adaptation occurred most in domains such as “Work” and “Service.” Cultural adaptation occurred least in domains such as “Family,” “Friends,” “Society,” “Interpersonal Relations,” and “Religion.” These domains are most influenced by tradition and early socialization. This study also revealed that cultural adaption is also a function of time spent in the host environment. The change from the first group (new recruits) to the second group (those who had been in the Navy 1–10 years) is faster than the change from the second group to the third group (those who had been in the Navy 11–25 years).\n\n(1) The method’s authors do not offer a measure of significance.\n(2) This method reveals the power, density, and constraints of a theme, but the interpretation of a word might be context dependent. For example, Russians associated “innovation” with “change,” but we don’t know the change is interpreted as negative or positive.\n"}
{"id": "34783649", "url": "https://en.wikipedia.org/wiki?curid=34783649", "title": "Automated Meteorological Data Acquisition System", "text": "Automated Meteorological Data Acquisition System\n\nAMeDAS (\"A\"utomated \"Me\"teorological \"D\"ata \"A\"cquisition \"S\"ystem), commonly known in Japanese as \"アメダス\" (\"amedasu\"), is a high-resolution surface observation network developed by the Japan Meteorological Agency (JMA) used for gathering regional weather data and verifying forecast performance. The system began operating on 1 November 1974, and currently comprises 1,300 stations throughout Japan (of which over 1,100 are unmanned), with an average separation of .\n\nObservations at manned stations cover weather, wind direction and speed, types and amounts of precipitation, types and base heights of clouds, visibility, air temperature, humidity, sunshine duration, and atmospheric pressure. All of these (except weather, visibility and cloud-related meteorological elements) are observed automatically.\n\nAt unmanned stations, observations are performed every 10 minutes. About 700 of the unmanned stations observe precipitation, air temperature, wind direction and speed, and sunshine duration, while the other stations observe only precipitation.\n\nFor about 280 stations (manned or unmanned) located in areas of heavy snowfall, snow depth is also observed.\n\nAll the observational data is transmitted to the AMeDAS Center at JMA Headquarters in Tokyo on a real time basis via dedicated telephone lines. The data is then delivered to the whole country after a quality check.\n\nAs well as weather conditions, AMeDAS is also used in the observation of natural disasters. Temporary observation points are set up in areas where there are signs of volcanic eruptions or earthquakes.\n\n\n"}
{"id": "29333000", "url": "https://en.wikipedia.org/wiki?curid=29333000", "title": "Boschert Glacier", "text": "Boschert Glacier\n\nBoschert Glacier () is a glacier to the southeast of Hayden Peak, flowing southwest from Bear Peninsula into Dotson Ice Shelf, on Walgreen Coast, Marie Byrd Land. It was mapped by the United States Geological Survey (USGS) from U.S. Navy aerial photographs taken 1966, and named in 1977 by the Advisory Committee on Antarctic Names after Ralph G. Boschert, USGS cartographer, a member of the USGS satellite surveying team at South Pole Station, winter party 1975.\n"}
{"id": "56962777", "url": "https://en.wikipedia.org/wiki?curid=56962777", "title": "Caroline Frances Finch", "text": "Caroline Frances Finch\n\nCaroline Frances Finch AO is widely regarded as Australia’s leading sports injury epidemiologist and sports injury prevention researcher. Her research has been adopted and used to directly inform safety policy by Government Departments of Sport and Health, health promotion and injury prevention agencies, and peak sports bodies both within Australia and internationally. Her injury prevention research has been applied to falls in older people, road safety, workplace safety and injuries in children.\n\nFinch graduated from Monash University, Melbourne in 1983 with a BSc (Hons, 1st Class) majoring in statistics. In 1985 Latrobe University awarded her a MSc in Statistics. In 1995 she was awarded a PhD in Mathematical Statistics from Monash University for a thesis titled: \"Fasting plasma glucose distributions and their implications for the diagnostic criteria for non-insulin dependent diabetes mellitus in Pacific populations\".\n\nAt high school Finch was interested in disease prevention. As an undergraduate student she realised this interest could be combined with her strong mathematics and statistics skills which led to her career as an epidemiologist and biostatistician.\n\nFinch began work as a Researcher at the Monash University Accident Research Centre training in injury research from 1992 until 1997 and continued from 2001-2003 within the Monash University, Department of Epidemiology and Preventive Medicine.\n\nFrom 2003 to 2006 Finch was Professor and Director, NSW Injury Risk Management Research Centre, University of New South Wales and then held the position of Research Professor, School of Human Movement and Sport Sciences, University of Ballarat until 2010.\n\nIn 2010 she returned to Monash Injury Research Institute as Research Professor and National Health and Medical Research Council (NHMRC) Principal Research Fellow in the Australian Centre for Research into Injury in Sport and its Prevention (ACRISP). ACRISP is one of only nine centres worldwide recognised as International Research Centres for Prevention of Injury and Protection of Athlete Health and supported by the International Olympic Committee (IOC).\n\nFinch became the Robert HT Smith Professor and Personal Chair at Federation University, Ballarat, Australia in 2013. In December 2017 she was appointed Deputy Vice-Chancellor (Research) at Edith Cowan University, Perth, Western Australia.\n\nFinch has held positions as a sports injury prevention adviser to the Commonwealth Department of Health and Aged Care, the Australian Sports Commission, Sports Medicine Australia, Sport and Recreation Victoria, Department of Human Services (Victoria), the Victorian Health Promotion Foundation, The Australian Football League and other national and state sports bodies. She is a continuing Board Member, Sports Medicine Australia, since 2015, a Member of the Concussion Advisory Group for World Rugby, since 2014, a Member of the Victorian Government Sports Injury Prevention Taskforce from 2011 to 2013 and a Member of the National Sports Safety Framework Committee in 1996,1997 and 2003.\n\nFunding for Finch's research has come from the NHMRC, Australian Research Council, VicHealth, the International Olympic Committee (IOC), the US and Canadian National Institutes of Health, Australian Federal and State government departments for health and sport and from peak sports bodies including the International Rugby Board, Fédération Internationale de Football Association (FIFA), the Australian Football League and Cricket Australia.\n\nFinch has worked to improve community safety in sport and to drive significant change around children's and women's sport and in the Australian school system, to improve safety equipment and training methods in individual sports and to create a database for sports injuries. The work of Finch and her teams has led to government departments of health and peak sport bodies recognising that they have a duty of care to everyone, not just the elite athletes and that sports safety is there business as well.\n\nFinch is a Senior Associate Editor (Injury Prevention) for the British Journal of Sports Medicine and the Injury Prevention and Health Promotion BJSM Series and a member of the Editorial Boards of the international journals: Journal of Science and Medicine in Sport, Injury Epidemiology and Sports Medicine. She has published over 250 articles.\n\nFinch was awarded the 2015 International Distinguished Career Award by the American Public Health Association's (APHA) Injury Control and Emergency Health Services (ICEHS) Section. The award recognised her \"outstanding dedication and leadership in injury/violence prevention and control and emergency health services internationally with contributions and achievements that have a significant and long term impact on the field\".\n\nIn January 2018 Finch was made an Officer of the Order of Australia (AO), \"for distinguished service to sports medicine, particularly in the area of injury prevention, as an educator, researcher and author, and to the promotion of improved health in athletes and those who exercise.\"\n\nProjects Finch has been involved with include:\n\nCarried out from 2010 to 2013, NoGAPS (National Guidance for Australian football Partnerships and Safety) was a NHMRC Partnerships Project which aimed to develop, deliver, implement and evaluate new evidence-based guidelines for exercise training programs to prevent lower limb injuries in community Australian football. It aimed to identify factors that affect the application of evidence-based injury prevention interventions into practice in community sport, and to find evidence for the effectiveness of an evidence-based exercise-training program for lower limb injury prevention in community Australian football. The project involved partnerships with the Australian Football League, Victorian Health Promotion Foundation, NSW Sporting Injuries Committee, JLT Sport, a division of Jardine Lloyd Thompson Australia Pty Ltd; Department of Planning and Community Development - Sport and Recreation Victoria Division; and Sports Medicine Australia - National and Victorian Branches (SMA).\n\nFinch was the project leader of the 2015 PAFIX study which followed 18 community-level Australian football clubs in Western Australia and Victoria through an entire season aiming to understand and prevent knee injuries in community Australian football. The large scale PAFIX project was unique in its use of a multi-level approach to understand the cause and prevention of knee injuries in community Australian football. Published results provide information to assist coaches and sports clubs to implement injury prevention programs. Data was also collected during the project focusing on concussion with the aim to understand and prevent head injuries within a community Australian football setting.\n\nSince the mid 1990s Finch has carried out studies relating to use of helmets for both pedal and motor cyclists.\n\nIn 2013 Finch jointly published results of a study of associations between helmet use and brain injuries amongst injured pedal- and motor-cyclists. The study was carried out at the University of New South Wales, School of Risk and Safety Sciences under funding by an Australian Research Council (ARC) Linkage Grant: Pedal and Motor Cycle Helmet Performance Study. The project partners were: the Commonwealth Department of Infrastructure and Transport, NSW Roads and Traffic Authority (NSW RTA), Transport Accident Commission Victoria, NRMA Motoring and Services, NRMA-ACT Road Safety Trust and DVExperts International.\n\nFinch has also been involved in research into the efficacy and methods of improvement of protective headgear in rugby union, rugby league, and Australian rules football.\n\nFinch's research has also encompassed several aspects of child safety including vehicular safety restraints, parent/caregiver supervision and water safety.\n\nFinch has been involved in research around falls prevention and implementation of fall prevention strategies in older people.\n\n"}
{"id": "12414720", "url": "https://en.wikipedia.org/wiki?curid=12414720", "title": "Chile Darwin's frog", "text": "Chile Darwin's frog\n\nThe Chile Darwin's frog (\"Rhinoderma rufum\"), also called the northern Darwin's frog, is one of only two members of the family Rhinodermatidae. It is endemic to central Chile, although it might well be extinct.\n\nThe Chile Darwin's frog has a snout to vent length of about . It has a fleshy proboscis, slender limbs and feet webbed between the first and second, and the second and third toes. The dorsal colour is variable but is usually some shade of brown or green, or a mixture of the two. The ventral surface is mottled in black and white.\n\nThe Chile Darwin's frog is diurnal and feeds on small insects and other invertebrates. The female lays a small clutch of eggs on moist ground. About a week later the embryos are beginning to move within the eggs and the male picks them up and stores them in his vocal sac. He keeps them there until they have developed a functioning gut and then transports them to a suitable water body and releases them. The tadpoles grow further in the water and undergo metamorphosis there. This development is in contrast to that of the Darwin's frog (\"Rhinoderma darwinii\") tadpoles which complete their development in their parent's vocal sac.\n\nThe Chile Darwin's frog has a very restricted range in central Chile, being found in Talca Province and southwards to Bío Bío Province, between from 33° 30'S to 37° 50'S. Very little is known about this species, but its natural habitats are probably temperate forests, rivers and swamps. It has been found in wet beech forests at altitudes of between above sea level. The species was sympatric with Rhinoderma darwinii in the region surrounding Concepcion, near the southern extremities of Rhinoderma rufum's distribution.\n\nThe Chile Darwin's frog is currently listed as \"Critically Endangered\" by the IUCN, but as there have been no confirmed sightings since around 1981, it may already be extinct. The main threats it faces are destruction of the pine forests in which it lives and building work but its steep decline is unexplained. It may be the victim of disease such as chytridiomycosis but this had not been reported in Chile when the decline started. If still extant, it is likely to be threatened by habitat loss, pollution, and infection from \"Batrachochytrium dendrobatidis\".\n\nOn January 21, 2008, Evolutionarily Distinct and Globally Endangered (EDGE), per chief Helen Meredith identified nature's most weird, wonderful and endangered species: \"The EDGE amphibians are amongst the most remarkable and unusual species on the planet and yet an alarming 85% of the top 100 are receiving little or no conservation attention.\" The top 10 endangered species (in the List of endangered animal species include: the Chinese giant salamander (\"Andrias davidianus\"), a relative of the hellbender (\"Cryptobranchus alleganiensus\"); the tiny Gardiner's Seychelles frog (\"Sooglossus gardineri\"); the limbless Sagalla caecilian (\"Boulengerula niedeni\"); the Table Mountain ghost frog (\"Heleophryne rosei\"); the Mexican lungless salamanders; the Malagasy rainbow frog (\"Scaphiophryne gottlebei\"); the Chile Darwin's frog (\"Rhinoderma rufum\"); and the Betic midwife toad (\"Alytes dickhilleni\").\n\n"}
{"id": "33956397", "url": "https://en.wikipedia.org/wiki?curid=33956397", "title": "Cognitive holding power", "text": "Cognitive holding power\n\nCognitive holding power is a concept measured by John C. Stevenson in 1994 using a questionnaire, the Cognitive Holding Power Questionnaire (CHPQ). This tool is assesses first- or second-order cognitive processing preferences.\n\nStudies using holding power have suggest improvements to mathematical education.\n"}
{"id": "25473531", "url": "https://en.wikipedia.org/wiki?curid=25473531", "title": "Deformation bands", "text": "Deformation bands\n\nDeformation bands are small faults with very small displacements. In the past, these bands have been called Luder's bands or braided shear fractures. They often precede large faults. They develop in porous rocks, such as sandstone. Material in a deformation band has a much smaller grain size, poorer sorting, and a lower porosity than the original sandstone. They can restrict and/or change the flow of fluids like water and oil. They are common in the Colorado Plateau, where examples occur in the Entrada Sandstone in the San Rafael Swell in Utah.\n\nDeformation bands are present in a variety of porous rock types such as sandstones, limestones, siltstones, poorly welded volcanic tuffs, and breccias. The cataclastic and compactional bands often form seals and prevent the flow of groundwater or oil. In their formation grains shift their packing and are crushed.\nThe Mars Reconnaissance Orbiter showed deformation bands in Capen Crater, located in the Arabia quadrangle. The bands represent failure by localized frictional sliding.\n"}
{"id": "54405493", "url": "https://en.wikipedia.org/wiki?curid=54405493", "title": "Digital phenotyping", "text": "Digital phenotyping\n\nDigital phenotyping is a multidisciplinary field of science, defined by Jukka-Pekka Onnela as the “moment-by-moment quantification of the individual-level human phenotype \"in situ\" using data from personal digital devices,” in particular smartphones. The data can be divided into two subgroups, called active data and passive data, where the former refers to data that requires active input from the users to be generated, whereas passive data, such as sensor data and phone usage patterns, are collected without requiring any active participation from the user. The term was first introduced in a paper in Nature Biotechnology by Sachin H. Jain and John Brownstein.\n\nSmartphones are well suited to digital phenotyping given their widespread adoption and ownership, the extent to which users engage with the devices, and richness of data that may be collected from them. Smartphone data can be used to study behavioral patterns, social interactions, physical mobility, gross motor activity, and speech production, among others. Smartphone ownership has been in steady rise globally over the past few years. For example, in the U.S., smartphone ownership among adults increased from 35% in 2011 to 64% in 2015, and in 2017 an estimated 95% of Americans own a cellphone of some kind and 77% own a smartphone.\n\nThe use of passive data collection from smartphone devices can provide granular information relevant to psychiatric and other illness phenotypes. Types of relevant passive data include GPS data to monitor spatial location, accelerometer data to record movement and gross motor activity, and call and messaging logs to document social engagement with others.\n\n"}
{"id": "33602543", "url": "https://en.wikipedia.org/wiki?curid=33602543", "title": "Economics of scientific knowledge", "text": "Economics of scientific knowledge\n\nThe economics of scientific knowledge (ESK) is an approach to understanding science which is predicated on the need to understand scientific knowledge creation and dissemination in economic terms.\n\nThe approach has been developed as a contrast to the sociology of scientific knowledge (SSK) which places scientists in their social context and examines their behavior using social theory. ESK typically involves thinking of scientists as having economic interests with these being thought of as utility maximisation and science as being a market process. Modelling strategies might use any of a variety of approaches including the neoclasscial, game theoretic, behavioural (bounded rationality) information theoretic and transaction costs. Boumans and Davis (2010) mention Dasgupta and David (1994) as being an interesting early example of work in this area.\n\n"}
{"id": "866752", "url": "https://en.wikipedia.org/wiki?curid=866752", "title": "Extended order", "text": "Extended order\n\nExtended order is an economics and sociology concept introduced by Friedrich Hayek in his book \"The Fatal Conceit\". It is a description of what happens when a system embraces specialization and trade and \"constitutes an information gathering process, able to call up, and put to use, widely dispersed information that no central planning agency, let alone any individual, could know as a whole, possess or control.” The result is an interconnected web where people can benefit from the actions and knowledge of those they don't know. This is possible and efficient because a proper legal framework replaces trust, which is only practical in small circles of people who know each other socially. The extended order is at the heart of Hayek's thesis, in \"The Fatal Conceit\", where he argues that \"our civilization depends, not only for its origin but also for its preservation, on what can be precisely described only as the extended order of human cooperation, an order more commonly, if somewhat misleading, known as capitalism.”\n\nThe \"extended order\" \"is a framework of institutions – economic, legal, and moral – into which we fit ourselves by obeying certain rules of conduct that \"we never made\", and which \"we have never understood\" in the sense of which we understand how the things that we manufacture function.” This \"order resulted \"not\" from human design or intention but spontaneously: it arose from unintentionally conforming to certain traditional & largely moral practices, many of which men tend to dislike, whose significance they usually fail to understand, whose validity they cannot prove, and which have nonetheless fairly rapidly spread by means of an evolutionary selection – the comparative increase in population & wealth – of those groups that happened to follow them.”\n\nThe adoption of these practices, by these groups, “increased their access to valuable information of all sorts, & enabled them to be 'fruitful, and multiply, and replenish the earth, and subdue it' (Genesis 1:28). This process is perhaps the least appreciated facet of human evolution.”\n\nThe extended order's formation “required individuals to change their ‘natural’ or instinctual’ responses to others, something strongly resisted\", whereas any & all \"constraints on the practices of the small group, it must be emphasized & repeated, are \"hated\".” This is because man “knows so many objects that seem desirable but for which he is not permitted to grasp, and he cannot see how other beneficial features of his environment depend on the discipline to which he is forced to submit – a discipline forbidding him to reach out for these same appealing objects. Disliking these constraints so much, we can hardly be said to have selected them; rather, these constraints selected us: they enabled us to survive.”\n\nThe evolutionary process of the extended order can be stimulated by increases in individual freedom and has even realized some of its greatest advances during times of anarchy, however it can (and quite often has throughout history) been hindered by government constraint, as Hayek says, \"Protection of several property, not the direction of its use by government, laid the foundations for the growth of the dense network of exchange of services that shaped the extended order.\" The extended order is \"not a creation of man's reason but a distinct second endowment conferred on him by cultural evolution.\"\n\nNot being genetically transferred, the continuing cultural evolution of the extended order requires teaching & passing on to each new generation the prevailing traditions, customs, morality & rules. This cultural evolutionary requirement was also analyzed by Will and Ariel Durant who said: \"Civilization is not inherited; it has to be learned and earned by each generation anew; if the transmission should be interrupted for one century, civilization would die, and we should be savages again.\"\n\n"}
{"id": "640533", "url": "https://en.wikipedia.org/wiki?curid=640533", "title": "George Brettingham Sowerby III", "text": "George Brettingham Sowerby III\n\nGeorge Brettingham Sowerby III (16 September 1843 – 31 January 1921) was a British conchologist, publisher, and illustrator.\n\nHe, too, worked (like his father George Brettingham Sowerby II and his grandfather George Brettingham Sowerby I) on the \"Thesaurus Conchyliorium\", a comprehensive, beautifully illustrated work on molluscs. He was colour blind, and thus his daughter did most of the colouring of his engravings.\n\n\n"}
{"id": "1739273", "url": "https://en.wikipedia.org/wiki?curid=1739273", "title": "Grigore Antipa", "text": "Grigore Antipa\n\nGrigore Antipa (; 27 November 1867 in Botoşani – 9 March 1944 in Bucharest) was a Romanian Darwinist biologist who studied the fauna of the Danube Delta and the Black Sea. Between 1892 and 1944 he was the director of the Bucharest Natural History Museum, which now bears his name.\n\nHe is also considered to be the first person to modernize the diorama by emphasizing the three-dimensional aspect and first to use dioramas in a museum setting.\n\nAdditionally, Antipa was a specialist in zoology, ichthyology, ecology and oceanography, and was a university professor.\n\nHe was elected as member of the Romanian Academy in 1910 and was also a member of several foreign academies. He founded a school of hydrobiology and ichthyology in Romania.\n"}
{"id": "17052696", "url": "https://en.wikipedia.org/wiki?curid=17052696", "title": "History of Solar System formation and evolution hypotheses", "text": "History of Solar System formation and evolution hypotheses\n\nThe history of scientific thought about the Formation and evolution of the Solar System begins with the Copernican Revolution.\nThe first recorded use of the term \"Solar System\" dates from 1704.\n\nThe most widely accepted theory of planetary formation, known as the nebular hypothesis, maintains that 4.6 billion years ago, the Solar System formed from the gravitational collapse of a giant molecular cloud which was light years across. Several stars, including the Sun, formed within the collapsing cloud. The gas that formed the Solar System was slightly more massive than the Sun itself. Most of the mass collected in the centre, forming the Sun; the rest of the mass flattened into a protoplanetary disc, out of which the planets and other bodies in the Solar System formed.\n\nThere are, however, arguments against this hypothesis.\n\nFrench philosopher and mathematician René Descartes was the first to propose a model for the origin of the Solar System in his \"Le Monde\" (ou Traité de lumière) which he wrote in 1632 and 1633 and for which he delayed publication because of the Inquisition and it was published only after his death in 1664. In his view, the Universe was filled with vortices of swirling particles and the Sun and planets had condensed from a particularly large vortex that had somehow contracted, which explained the circular motion of the planets and was on the right track with condensation and contraction. However, this was before Newton's theory of gravity and we now know matter does not behave in this fashion.\n\nThe vortex model of 1944, formulated by German physicist and philosopher Baron Carl Friedrich von Weizsäcker, which harkens back to the Cartesian model, involved a pattern of turbulence-induced eddies in a Laplacian nebular disc. In it a suitable combination of clockwise rotation of each vortex and anti-clockwise rotation of the whole system can lead to individual elements moving around the central mass in Keplerian orbits so there would be little dissipation of energy due to the overall motion of the system but material would be colliding at high relative velocity in the inter-vortex boundaries and in these regions small roller-bearing eddies would coalesce to give annular condensations. It was much criticized as turbulence is a phenomenon associated with disorder and would not spontaneously produce the highly ordered structure required by the hypothesis. As well, it does not provide a solution to the angular momentum problem and does not explain lunar formation nor other very basic characteristics of the Solar System.\n\nThe Weizsäcker model was modified in 1948 by Dutch theoretical physicist Dirk Ter Haar, in that regular eddies were discarded and replaced by random turbulence which would lead to a very thick nebula where gravitational instability would not occur. He concluded the planets must have formed by accretion and explained the compositional difference (solid and liquid planets) as due to the temperature difference between the inner and outer regions, the former being hotter and the latter being cooler, so only refractories (non-volatiles) condensed in the inner region. A major difficulty is that in this supposition turbulent dissipation takes place in a time scale of only about a millennium which does not give enough time for planets to form.\n\nThe nebular hypothesis was first proposed in 1734 by Emanuel Swedenborg and later elaborated and expanded upon by Immanuel Kant in 1755. A similar theory was independently formulated by Pierre-Simon Laplace in 1796.\n\nIn 1749, Georges-Louis Leclerc, Comte de Buffon conceived the idea that the planets were formed when a comet collided with the Sun, sending matter out to form the planets. However, Laplace refuted this idea in 1796, showing that any planets formed in such a way would eventually crash into the Sun. Laplace felt that the near-circular orbits of the planets were a necessary consequence of their formation. Today, comets are known to be far too small to have created the Solar System in this way.\n\nIn 1755, Immanuel Kant speculated that observed nebulae may in fact be regions of star and planet formation. In 1796, Laplace elaborated by arguing that the nebula collapsed into a star, and, as it did so, the remaining material gradually spun outward into a flat disc, which then formed the planets.\n\nHowever plausible it may appear at first sight, the nebular hypothesis still faces the obstacle of angular momentum; if the Sun had indeed formed from the collapse of such a cloud, the planets should be rotating far more slowly. The Sun, though it contains almost 99.9 percent of the system's mass, contains just 1 percent of its angular momentum. This means that the Sun should be spinning much more rapidly.\n\nAttempts to resolve the angular momentum problem led to the temporary abandonment of the nebular hypothesis in favour of a return to \"two-body\" theories. For several decades, many astronomers preferred the \"tidal\" or \"near-collision\" hypothesis put forward by James Jeans in 1917, in which the planets were considered to have been formed due to the approach of some other star to the Sun. This near-miss would have drawn large amounts of matter out of the Sun and the other star by their mutual tidal forces, which could have then condensed into planets. However, in 1929 astronomer Harold Jeffreys countered that such a near-collision was massively unlikely. Objections to the hypothesis were also raised by the American astronomer Henry Norris Russell, who showed that it ran into problems with angular momentum for the outer planets, with the planets struggling to avoid being reabsorbed by the Sun.\n\nForest Moulton in 1900 had also shown that the nebular hypothesis was inconsistent with observations because of the angular momentum. Moulton and Chamberlin in 1904 originated the planetesimal hypothesis(see Chamberlin–Moulton planetesimal hypothesis). Along with many astronomers of the day they came to believe the pictures of \"spiral nebulas\" from the Lick Observatory were direct evidence of forming solar systems. These turned out to be galaxies instead but the Shapley-Curtis debate about these was still 16 years in the future. One of the most fundamental issues in the history of astronomy was distinguishing between nebulas and galaxies.\n\nMoulton and Chamberlin suggested that a star had passed close to the Sun early in its life to cause tidal bulges and that this, along with the internal process that leads to solar prominences, resulted in the ejection of filaments of matter from both stars. While most of the material would have fallen back, part of it would remain in orbit. The filaments cooled into numerous, tiny, solid fragments, ‘planetesimals’, and a few larger protoplanets. This model received favourable support for about 3 decades but passed out of favour by the late '30s and was discarded in the '40s by the realization it was incompatible with the angular momentum of Jupiter, but a part of it, planetesimal accretion, was retained.\n\nIn 1937 and 1940, Ray Lyttleton postulated that a companion star to the Sun collided with a passing star. Such a scenario was already suggested and rejected by Henry Russell in 1935. Lyttleton showed terrestrial planets were too small to condense on their own so suggested one very large proto-planet broke in two because of rotational instability, forming Jupiter and Saturn, with a connecting filament from which the other planets formed. A later model, from 1940 and 1941, involves a triple star system, a binary plus the Sun, in which the binary merges and later breaks up because of rotational instability and escapes from the system leaving a filament that formed between them to be captured by the Sun. Objections of Lyman Spitzer apply to this model also.\n\nIn 1954, 1975, and 1978 Swedish astrophysicist Hannes Alfvén included electromagnetic effects in equations of particle motions, and angular momentum distribution and compositional differences were explained. In 1954 he first proposed the band structure in which he distinguished an A-cloud, containing mostly helium, but with some solid- particle impurities (\"meteor rain\"), a B-cloud, with mostly hydrogen, a C-cloud, having mainly carbon, and a D-cloud, made mainly of silicon and iron. Impurities in the A-cloud form Mars and the Moon (later captured by Earth), in the B-cloud they condense into Mercury, Venus, and Earth, in the C-cloud they condense into the outer planets, and Pluto and Triton may have formed from the D-cloud.\n\nIn 1943, the Soviet astronomer Otto Schmidt proposed that the Sun, in its present form, passed through a dense interstellar cloud, emerging enveloped in a cloud of dust and gas, from which the planets eventually formed. This solved the angular momentum problem by assuming that the Sun's slow rotation was peculiar to it, and that the planets did not form at the same time as the Sun. Extensions of the model, together forming the Russian school, include Gurevich and Lebedinsky (in 1950), Safronov (in 1967,1969), Safronov and Vityazeff (in 1985), Safronov and Ruskol (in 1994), and Ruskol (in 1981), among others However, this hypothesis was severely dented by Victor Safronov who showed that the amount of time required to form the planets from such a diffuse envelope would far exceed the Solar System's determined age.\n\nRay Lyttleton modified the theory by showing that a 3rd body was not necessary and proposing that a mechanism of line accretion described by Bondi and Hoyle in 1944 would enable cloud material to be captured by the star (Williams and Cremin, 1968, loc. cit.)\n\nIn this model (from 1944) the companion went nova with ejected material captured by the Sun and planets forming from this material. In a version a year later it was a supernova. In 1955 he proposed a similar system to Laplace, and with more mathematical detail in 1960. It differs from Laplace in that a magnetic torque occurs between the disk and the Sun, which comes into effect immediately or else more and more matter would be ejected resulting in a much too massive planetary system, one comparable to the Sun. The torque causes a magnetic coupling and acts to transfer angular momentum from the Sun to the disk. The magnetic field strength would have to be 1 gauss. The existence of torque depends on magnetic lines of force being frozen into the disk (a consequence of a well-known MHD (magnetohydrodynamic) theorem on frozen-in lines of force). As the solar condensation temperature when the disk was ejected could not be much more than 1000 degrees K., a number of refractories must be solid, probably as fine smoke particles, which would grow with condensation and accretion. These particles would be swept out with the disk only if their diameter at the Earth's orbit was less than 1 meter so as the disk moved outward a subsidiary disk consisting of only refractories remains behind where the terrestrial planets would form. The model is in good agreement with the mass and composition of the planets and angular momentum distribution provided the magnetic coupling is an acceptable idea, but not explained are twinning, the low mass of Mars and Mercury, and the planetoid belts. It was Alfvén who formulated the concept of frozen-in magnetic field lines.\n\nGerard Kuiper (in 1944) argued, like Ter Haar, that regular eddies would be impossible and postulated that large gravitational instabilities might occur in the solar nebula, forming condensations. In this, the solar nebula could be either co-genetic with the Sun or captured by it. Density distribution would determine what could form: either a planetary system or a stellar companion. The 2 types of planets were assumed to be due to the Roche limit. No explanation was offered for the Sun's slow rotation which Kuiper saw as a larger G-star problem.\n\nIn Fred Whipple's 1948 scenario a smoke cloud about 60,000 AU in diameter and with 1 solar mass () contracts and produces the Sun. It has a negligible angular momentum thus accounting for the Sun's similar property. This smoke cloud captures a smaller one with a large angular momentum. The collapse time for the large smoke and gas nebula is about 100 million years and the rate is slow at first, increasing in later stages. The planets would condense from small clouds developed in, or captured by, the 2nd cloud, the orbits would be nearly circular because accretion would reduce eccentricity due to the influence of the resisting medium, orbital orientations would be similar because the small cloud was originally small and the motions would be in a common direction. The protoplanets might have heated up to such high degrees that the more volatile compounds would have been lost and the orbital velocity decreases with increasing distance so that the terrestrial planets would have been more affected. The weaknesses of this scenario are that practically all the final regularities are introduced as a priori assumptions and most of the hypothesizing was not supported by quantitative calculations. For these reasons it did not gain wide acceptance.\n\nAmerican chemist Harold Urey, who founded cosmochemistry, put forward a scenario in 1951, 1952, 1956, and 1966 based largely on meteorites and using Chandrasekhar's stability equations and obtained density distribution in the gas and dust disk surrounding the primitive Sun. In order that volatile elements like mercury could be retained by the terrestrial planets he postulated a moderately thick gas and dust halo shielding the planets from the Sun. In order to form diamonds, pure carbon crystals, Moon-size objects, gas spheres that became gravitationally unstable, would have to form in the disk with the gas and dust dissipating at a later stage. Pressure fell as gas was lost and diamonds were converted to graphite, while the gas became illuminated by the Sun. Under these conditions considerable ionization would be present and the gas would be accelerated by magnetic fields, hence the angular momentum could be transferred from the Sun. He postulated that these lunar-size bodies were destroyed by collisions, with the gas dissipating, leaving behind solids collected at the core, with the resulting smaller fragments pushed far out into space and the larger fragments staying behind and accreting into planets. He suggested the Moon was just such a surviving core.\n\nIn 1960, 1963, and 1978, W. H. McCrea proposed the \"protoplanet theory\", in which the Sun and planets individually coalesced from matter within the same cloud, with the smaller planets later captured by the Sun's larger gravity. It includes fission in a protoplanetary nebula and there is no solar nebula. Agglomerations of floccules (which are presumed to compose the supersonic turbulence assumed to occur in the interstellar material from which stars are born) formed the Sun and protoplanets, the latter splitting to form planets. The 2 portions can not remain gravitationally bound to each other, are at a mass ratio of at least 8 to 1, and for inner planets they go into independent orbits while for outer planets one of the portions exits the Solar System. The inner protoplanets were Venus-Mercury and Earth-Mars. The moons of the greater planets were formed from \"droplets\" in the neck connecting the 2 portions of the dividing protoplanet and these droplets could account for some of the asteroids. Terrestrial planets would have no major moons which does not account for Luna. It predicts certain observations such as the similar angular velocity of Mars and Earth with similar rotation periods and axial tilts. In this scheme there are 6 principal planets: 2 terrestrial, Venus and Earth, 2 major, Jupiter and Saturn, and 2 outer, Uranus and Neptune; and 3 lesser planets: Mercury, Mars, and Pluto.\n\nThis theory has a number of problems, such as explaining the fact that the planets all orbit the Sun in the same direction, which would appear highly unlikely if they were each individually captured.\n\nIn American astronomer Alastair G. W. Cameron's hypothesis (from 1962 and 1963), the protosun has a mass of about 1–2 Suns with a diameter of around 100,000 AU is gravitationally unstable, collapses, and breaks up into smaller subunits. The magnetic field is of the order of 1/100,000 gauss. During the collapse the magnetic lines of force are twisted. The collapse is fast and is done by the dissociation of H molecules followed by the ionization of H and the double ionization of He. Angular momentum leads to rotational instability which produces a Laplacean disk. At this stage radiation will remove excess energy and the disk will be quite cool in a relatively short period (about 1 mln. yrs.) and the condensation into what Whipple calls cometismals takes place. Aggregation of these produces giant planets which in turn produce disks during their formation from which evolve into lunar systems. The formation of terrestrial planets, comets, and asteroids involved disintegration, heating, melting, solidification, etc. He also formulated the Big Splat or Giant Impactor Hypothesis for the origin of the Moon.\n\nThe \"capture theory\", proposed by Michael Mark Woolfson in 1964, posits that the Solar System formed from tidal interactions between the Sun and a low-density protostar. The Sun's gravity would have drawn material from the diffuse atmosphere of the protostar, which would then have collapsed to form the planets. However, the capture theory predicts a different age for the Sun than for the planets, whereas the similar ages of the Sun and the rest of the Solar System indicate that they formed at roughly the same time.\n\nAs captured planets would have initially eccentric orbits Dormand and Woolfson in 1974 and 1977 and Woolfson proposed the possibility of a collision. A filament is thrown out by a passing proto-star which is captured by the Sun and planets form from it. In this there were 6 original planets, corresponding to 6 point-masses in the filament, with planets A and B, the 2 innermost, colliding, the former at twice the mass of Neptune, and ejecting out of the Solar System, and the latter at 1/3 the mass of Uranus, and splitting into Earth and Venus. Mars and the Moon are former moons of A. Mercury is either a fragment of B or an escaped moon of A. The collision also produced the asteroid belt and the comets.\n\nT.J.J. See was an American astronomer and Navy Captain who at one time worked under Ellery Hale at the Lowell Observatory. He had a cult following largely because of his many (some 60) articles in \"Popular Astronomy\" but also in \"Astronomische Nachrichte\" (Astronomical News) (mostly in English). While at the USNO's Mare Island, Cal. station, he developed a model which he called capture theory, published in 1910, in his \"Researches on the Evolution of the Stellar Systems: v. 2. The capture theory of cosmical evolution, founded on dynamical principles and illustrated by phenomena observed in the spiral nebulae, the planetary system, the double and multiple stars and clusters and the star-clouds of the Milky Way\", which proposed that the planets formed in the outer Solar System and were captured by the Sun; the moons were formed in thus manner and were captured by the planets. This caused a feud with Forest Moulton, who co-developed the planetesimal hypothesis. A preview was presented in 1909 at a meeting of the ASP (Astronomical Society of the Pacific) at the Chabot Observatory in Oakland, Cal., and newspaper headlines blared \"Prof. See's Paper Causes Sensation\" (San Francisco Call) and \"Scientists in Furore Over Nebulae\" (San Francisco Examiner). Our current knowledge of dynamics makes capture most unlikely as it requires special conditions.\n\nSwiss astronomer Louis Jacot (in 1951, 1962, 1981), like Weisacker and Ter Haar, continued the Cartesian idea of vortices but proposed a hierarchy of vortices or vortices within vortices, i.e., a lunar system vortex, a Solar System vortex, and a galactic vortex. He put forward the notion that planetary orbits are spirals, not circles or ellipses. Jacot also proposed the expansion of galaxies (stars move away from the hub), and that moons move away from their planets.\n\nHe also maintained that planets were expelled, one at a time, from the Sun, specifically from an equatorial bulge caused by rotation, and that one of them shattered in this expulsion leaving the asteroid belt. The Kuiper Belt was unknown at the time, but presumably it, too, would be the result of the same kind of shattering. The moons, like the planets, originated as equatorial expulsions, but, of course, from their parent planets, with some shattering, leaving the rings, and Earth is supposed to eventually expel another moon.\n\nIn this model there were 4 phases to the planets: no rotation and keeping the same side to the Sun \"as Mercury does now\" (we've known, of course, since 1965, that it doesn't), very slow, accelerated, and finally, daily rotation.\n\nHe explained the differences between inner and outer planets and inner and outer moons through vortex behaviour. Mercury's eccentric orbit was explained by its recent expulsion from the Sun and Venus' slow rotation as its being in the \"slow rotation phase\", having been expelled second to last.\n\nThe Tom Van Flandern model was first proposed in 1993 in the first edition of his book. In the revised version from 1999 and later, the original Solar System had 6 pairs of twin planets each fissioned off from the equatorial bulges of an overspinning Sun (outward centrifugal forces exceed the inward gravitational force) at different times so having different temperatures, sizes, and compositions, and having condensed thereafter with the nebular disk dissipating after some 100 million years, with 6 planets exploding. Four of these were helium dominated, fluid, and unstable (helium class planets). These were V (Bellatrix) (V standing for the 5th planet, the first 4 including Mercury and Mars), K (Krypton), T (transneptunian), and Planet X. In these cases, the smaller moons exploded because of tidal stresses leaving the 4 component belts of the 2 major planetoid zones. Planet LHB-A, the explosion for which is postulated to have caused the Late Heavy Bombardment (about 4 eons ago), was twinned with Jupiter, and LHB-B, the explosion for which is postulated to have caused another LHB, was twinned with Saturn. In planets LHB-A, Jupiter, LHB-B, and Saturn, being gigantic, Jovian planets, the inner and smaller partner in each pair was subjected to enormous tidal stresses causing it to blow up. The explosions took place before they were able to fission off moons. As the 6 were fluid they left no trace. Solid planets fission off only one moon and Mercury was a moon of Venus but drifted away because of the Sun's gravitational influence. Mars was a moon of Bellatrix.\nOne major argument against exploding planets and moons is that there would not be an energy source powerful enough to cause such explosions. \nIn J. Marvin Herndon's model,\ninner (large-core) planets form by condensation and raining-out from within giant gaseous protoplanets at high pressures and high temperatures. Earth's complete condensation included a c. 300 Earth-mass gas/ice shell that compressed the rocky kernel to about 66% of Earth's present diameter (Jupiter equates to about 300 Earth masses, which equals c. 2000 trillion trillion kg; Earth is at about 6 trillion trillion kg). T Tauri (see T Tauri type stars) eruptions of the Sun stripped the gases away from the inner planets. Mercury was incompletely condensed and a portion of its gases were stripped away and transported to the region between Mars and Jupiter, where it fused with in-falling oxidized condensate from the outer reaches of the Solar System and formed the parent material for ordinary chondrite meteorites, the Main-Belt asteroids, and veneer for the inner planets, especially Mars. The differences between the inner planets are primarily the consequence of different degrees of protoplanetary compression. There are two types of responses to decompression-driven planetary volume increases: cracks, which form to increase surface area, and folding, creating mountain ranges, to accommodate changes in curvature.\n\nThis planetary formation theory represents an extension of the Whole-Earth Decompression Dynamics (WEDD) model,\nwhich includes natural nuclear-fission reactors in planetary cores; Herndon elaborates, expounds, and elucidates it in 11 articles in \"Current Science\" from 2005 to 2013 and in five books published from 2008 to 2012. He refers to his model as \"indivisible\" – meaning that the fundamental aspects of Earth are connected logically and causally, and can be deduced from its early formation as a Jupiter-like giant.\n\nIn 1944 the German chemist and physicist Arnold Eucken considered the thermodynamics of Earth condensing and raining-out within a giant protoplanet at pressures of 100–1000 atm. In the 1950s and early 1960s discussion of planetary formation at such pressures took place, but Cameron's 1963 low-pressure (c. 4–10 atm.) model largely supplanted the idea.\n\nJeans, in 1931, divided the various models into 2 groups: those where the material for planet formation came from the Sun and those where it didn't and may be concurrent or consecutive.<ref name=\"ads.abs.harvard.edu/abs\">Williams, I.O., Cremin, A.W. 1968. A survey of theories relating to the origin of the solar system. Qtly. Rev. RAS 9: 40–62. ads.abs.harvard.edu/abs</ref> \nWilliam McCrea, in 1963, divided them into 2 groups also: those that relate the formation of the planets to the formation of the Sun and those where it is independent of the formation of the Sun, where the planets form after the Sun becomes a normal star.\n\nTer Haar and Cameron distinguished between those theories that consider a closed system, which is a development of the Sun and possibly a solar envelope, that starts with a protosun rather than the Sun itself, and state that Belot calls these theories monistic; and those that consider an open system, which is where there is an interaction between the Sun and some foreign body that is supposed to have been the first step in the developments leading to the planetary system, and state that Belot calls these theories dualistic.\n\nHervé Reeves' classification also categorizes them as co-genetic with the Sun or not but also as formed from altered or unaltered stellar/interstellar material. He as well recognizes 4 groups: 1) models based on the solar nebula, originated by Swedenborg, Kant, and Laplace in the 1700s; 2) the ones proposing a cloud captured from interstellar space, major proponents being Alfvén and Gustaf Arrhenius (in 1978) and Alfvén and Arrhenius; 3) the binary hypotheses which propose that a sister star somehow disintegrated and a portion of its dissipating material was captured by the Sun, principal hypothesizer being Lyttleton in the '40s; 4) and the close-approach-filament ideas of Jeans, Jeffreys, and Woolfson and Dormand.\n\nIn Williams and Cremin the categories are: (1) models that regard the origin and formation of the planets as being essentially related to the Sun, with the 2 formation processes taking place concurrently or consecutively, (2) models that regard formation of the planets as being independent of the formation process of the Sun, the planets forming after the Sun becomes a normal star; this has 2 subcategories: a) where the material for the formation of the planets is extracted either from the Sun or another star, b) where the material is acquired from interstellar space. They conclude that the best models are Hoyle's magnetic coupling and McCrea's floccules.\n\nWoolfson recognized 1) monistic, which included Laplace, Descartes, Kant, and Weisacker, and 2) dualistc, which included Leclerc (comte de Buffon), Chamberlin-Moulton, Jeans, Jeffreys, and Schmidt-Lyttleton.\n\nIn 1978, astronomer A. J. R. Prentice revived the Laplacian nebular model in his Modern Laplacian Theory by suggesting that the angular momentum problem could be resolved by drag created by dust grains in the original disc which slowed down the rotation in the centre. Prentice also suggested that the young Sun transferred some angular momentum to the protoplanetary disc and planetesimals through supersonic ejections understood to occur in T Tauri stars. However, his contention that such formation would occur in toruses or rings has been questioned, as any such rings would disperse before collapsing into planets.\n\nThe birth of the modern widely accepted theory of planetary formation—the Solar Nebular Disk Model (SNDM)—can be traced to the works of Soviet astronomer Victor Safronov. His book \"Evolution of the protoplanetary cloud and formation of the Earth and the planets\", which was translated to English in 1972, had a long-lasting effect on the way scientists thought about the formation of the planets. In this book almost all major problems of the planetary formation process were formulated and some of them solved. Safronov's ideas were further developed in the works of George Wetherill, who discovered \"runaway accretion\". By the early 1980s, the nebular hypothesis in the form of SNDM had come back into favour, led by two major discoveries in astronomy. First, a number of apparently young stars, such as Beta Pictoris, were found to be surrounded by discs of cool dust, much as was predicted by the nebular hypothesis. Second, the Infrared Astronomical Satellite, launched in 1983, observed that many stars had an excess of infrared radiation that could be explained if they were orbited by discs of cooler material.\n\nWhile the broad picture of the nebular hypothesis is widely accepted, many of the details are not well understood and continue to be refined.\n\nThe refined nebular model was developed entirely on the basis of observations of the Solar System because it was the only one known until the mid-1990s. It was not confidently assumed to be widely applicable to other planetary systems, although scientists were anxious to test the nebular model by finding of protoplanetary discs or even planets around other stars. As of August 30, 2013, the discovery of 941 extrasolar planets has turned up many surprises, and the nebular model must be revised to account for these discovered planetary systems, or new models considered.\n\nAmong the extrasolar planets discovered to date are planets the size of Jupiter or larger but possessing very short orbital periods of only a few hours. Such planets would have to orbit very closely to their stars; so closely that their atmospheres would be gradually stripped away by solar radiation. There is no consensus on how to explain these so-called hot Jupiters, but one leading idea is that of planetary migration, similar to the process which is thought to have moved Uranus and Neptune to their current, distant orbit. Possible processes that cause the migration include orbital friction while the protoplanetary disk is still full of hydrogen and helium gas\nand exchange of angular momentum between giant planets and the particles in the protoplanetary disc.\n\nThe detailed features of the planets are another problem. The solar nebula hypothesis predicts that all planets will form exactly in the ecliptic plane. Instead, the orbits of the classical planets have various (but small) inclinations with respect to the ecliptic. Furthermore, for the gas giants it is predicted that their rotations and moon systems will also not be inclined with respect to the ecliptic plane. However, most gas giants have substantial axial tilts with respect to the ecliptic, with Uranus having a 98° tilt. The Moon being relatively large with respect to the Earth and other moons which are in irregular orbits with respect to their planet is yet another issue. It is now believed these observations are explained by events which happened after the initial formation of the Solar System.\n\nAttempts to isolate the physical source of the Sun's energy, and thus determine when and how it might ultimately run out, began in the 19th century. At that time, the prevailing scientific view on the source of the Sun's heat was that it was generated by gravitational contraction. In the 1840s, astronomers J. R. Mayer and J. J. Waterson first proposed that the Sun's massive weight causes it to collapse in on itself, generating heat, an idea expounded upon in 1854 by both Hermann von Helmholtz and Lord Kelvin, who further elaborated on the idea by suggesting that heat may also be produced by the impact of meteors onto the Sun's surface. However, the Sun only has enough gravitational potential energy to power its luminosity by this mechanism for about 30 million years—far less than the age of the Earth. (This collapse time is known as the Kelvin–Helmholtz timescale.)\n\nAlbert Einstein's development of the theory of relativity in 1905 led to the understanding that nuclear reactions could create new elements from smaller precursors, with the loss of energy. In his treatise \"Stars and Atoms\", Arthur Eddington suggested that pressures and temperatures within stars were great enough for hydrogen nuclei to fuse into helium; a process which could produce the massive amounts of energy required to power the Sun. In 1935, Eddington went further and suggested that other elements might also form within stars. Spectral evidence collected after 1945 showed that the distribution of the commonest chemical elements, carbon, hydrogen, oxygen, nitrogen, neon, iron etc., was fairly uniform across the galaxy. This suggested that these elements had a common origin. A number of anomalies in the proportions hinted at an underlying mechanism for creation. Lead has a higher atomic weight than gold, but is far more common. Hydrogen and helium (elements 1 and 2) are virtually ubiquitous yet lithium and beryllium (elements 3 and 4) are extremely rare.\n\nWhile the unusual spectra of red giant stars had been known since the 19th century, it was George Gamow who, in the 1940s, first understood that they were stars of roughly solar mass that had run out of hydrogen in their cores and had resorted to burning the hydrogen in their outer shells. This allowed Martin Schwarzschild to draw the connection between red giants and the finite lifespans of stars. It is now understood that red giants are stars in the last stages of their life cycles.\n\nFred Hoyle noted that, even while the distribution of elements was fairly uniform, different stars had varying amounts of each element. To Hoyle, this indicated that they must have originated within the stars themselves. The abundance of elements peaked around the atomic number for iron, an element that could only have been formed under intense pressures and temperatures. Hoyle concluded that iron must have formed within giant stars. From this, in 1945 and 1946, Hoyle constructed the final stages of a star's life cycle. As the star dies, it collapses under its own weight, leading to a stratified chain of fusion reactions: carbon-12 fuses with helium to form oxygen-16; oxygen-16 fuses with helium to produce neon-20, and so on up to iron. There was, however, no known method by which carbon-12 could be produced. Isotopes of beryllium produced via fusion were too unstable to form carbon, and for three helium atoms to form carbon-12 was so unlikely as to have been impossible over the age of the Universe. However, in 1952 the physicist Ed Salpeter showed that a short enough time existed between the formation and the decay of the beryllium isotope that another helium had a small chance to form carbon, but only if their combined mass/energy amounts were equal to that of carbon-12. Hoyle, employing the anthropic principle, showed that it must be so, since he himself was made of carbon, and he existed. When the matter/energy level of carbon-12 was finally determined, it was found to be within a few percent of Hoyle's prediction.\n\nThe first white dwarf discovered was in the triple star system of 40 Eridani, which contains the relatively bright main sequence star 40 Eridani A, orbited at a distance by the closer binary system of the white dwarf 40 Eridani B and the main sequence red dwarf 40 Eridani C. The pair 40 Eridani B/C was discovered by William Herschel on January 31, 1783; it was again observed by Friedrich Georg Wilhelm Struve in 1825 and by Otto Wilhelm von Struve in 1851. In 1910, it was discovered by Henry Norris Russell, Edward Charles Pickering and Williamina Fleming that despite being a dim star, 40 Eridani B was of spectral type A, or white.\n\nWhite dwarfs were found to be extremely dense soon after their discovery. If a star is in a binary system, as is the case for Sirius B and 40 Eridani B, it is possible to estimate its mass from observations of the binary orbit. This was done for Sirius B by 1910, yielding a mass estimate of . (A more modern estimate is .) Since hotter bodies radiate more than colder ones, a star's surface brightness can be estimated from its effective surface temperature, and hence from its spectrum. If the star's distance is known, its overall luminosity can also be estimated. Comparison of the two figures yields the star's radius. Reasoning of this sort led to the realization, puzzling to astronomers at the time, that Sirius B and 40 Eridani B must be very dense. For example, when Ernst Öpik estimated the density of a number of visual binary stars in 1916, he found that 40 Eridani B had a density of over 25,000 times the Sun's, which was so high that he called it \"impossible\".\n\nSuch densities are possible because white dwarf material is not composed of atoms bound by chemical bonds, but rather consists of a plasma of unbound nuclei and electrons. There is therefore no obstacle to placing nuclei closer to each other than electron orbitals—the regions occupied by electrons bound to an atom—would normally allow. Eddington, however, wondered what would happen when this plasma cooled and the energy which kept the atoms ionized was no longer present. This paradox was resolved by R. H. Fowler in 1926 by an application of the newly devised quantum mechanics. Since electrons obey the Pauli exclusion principle, no two electrons can occupy the same state, and they must obey Fermi–Dirac statistics, also introduced in 1926 to determine the statistical distribution of particles which satisfy the Pauli exclusion principle. At zero temperature, therefore, electrons could not all occupy the lowest-energy, or \"ground\", state; some of them had to occupy higher-energy states, forming a band of lowest-available energy states, the \"Fermi sea\". This state of the electrons, called \"degenerate\", meant that a white dwarf could cool to zero temperature and still possess high energy.\n\nPlanetary nebulae are generally faint objects, and none are visible to the naked eye. The first planetary nebula discovered was the Dumbbell Nebula in the constellation of Vulpecula, observed by Charles Messier in 1764 and listed as M27 in his catalogue of nebulous objects. To early observers with low-resolution telescopes, M27 and subsequently discovered planetary nebulae somewhat resembled the gas giants, and William Herschel, discoverer of Uranus, eventually coined the term 'planetary nebula' for them, although, as we now know, they are very different from planets.\n\nThe central stars of planetary nebulae are very hot. Their luminosity, though, is very low, implying that they must be very small. Only once a star has exhausted all its nuclear fuel can it collapse to such a small size, and so planetary nebulae came to be understood as a final stage of stellar evolution. Spectroscopic observations show that all planetary nebulae are expanding, and so the idea arose that planetary nebulae were caused by a star's outer layers being thrown into space at the end of its life.\n\nOver the centuries, many scientific hypotheses have been advanced concerning the origin of Earth's Moon. One of the earliest was the so-called \"binary accretion model\", which concluded that the Moon accreted from material in orbit around the Earth left over from its formation. Another, the \"fission model\", was developed by George Darwin (son of Charles Darwin), who noted that, as the Moon is gradually receding from the Earth at a rate of about 4 cm per year, so at one point in the distant past it must have been part of the Earth, but was flung outward by the momentum of Earth's then–much faster rotation. This hypothesis is also supported by the fact that the Moon's density, while less than Earth's, is about equal to that of Earth's rocky mantle, suggesting that, unlike the Earth, it lacks a dense iron core. A third hypothesis, known as the \"capture model\", suggested that the Moon was an independently orbiting body that had been snared into orbit by Earth's gravity.\n\nHowever, these hypotheses were all refuted by the late 1960s and early 1970s \"Apollo\" lunar missions, which introduced a stream of new scientific evidence; specifically concerning the Moon's composition, its age, and its history. These lines of evidence contradict many predictions made by these earlier models. The rocks brought back from the Moon showed a marked decrease in water relative to rocks elsewhere in the Solar System, and also evidence of an ocean of magma early in its history, indicating that its formation must have produced a great deal of energy. Also, oxygen isotopes in lunar rocks showed a marked similarity to those on Earth, suggesting that they formed at a similar location in the solar nebula. The capture model fails to explain the similarity in these isotopes (if the Moon had originated in another part of the Solar System, those isotopes would have been different), while the co-accretion model cannot adequately explain the loss of water (if the Moon formed in a similar fashion to the Earth, the amount of water trapped in its mineral structure would also be roughly similar). Conversely, the fission model, while it can account for the similarity in chemical composition and the lack of iron in the Moon, cannot adequately explain its high orbital inclination and, in particular, the large amount of angular momentum in the Earth–Moon system, more than any other planet–satellite pair in the Solar System.\n\nFor many years after \"Apollo\", the binary accretion model was settled on as the best hypothesis for explaining the Moon's origins, even though it was known to be flawed. Then, at a conference in Kona, Hawaii in 1984, a compromise model was composed that accounted for all of the observed discrepancies. Originally formulated by two independent research groups in 1976, the \"giant impact\" model supposed that a massive planetary object, the size of Mars, had collided with Earth early in its history. The impact would have melted Earth's crust, and the other planet's heavy core would have sunk inward and merged with Earth's. The superheated vapour produced by the impact would have risen into orbit around the planet, coalescing into the Moon. This explained the lack of water (the vapour cloud was too hot for water to condense), the similarity in composition (since the Moon had formed from part of the Earth), the lower density (since the Moon had formed from the Earth's crust and mantle, rather than its core), and the Moon's unusual orbit (since an oblique strike would have imparted a massive amount of angular momentum to the Earth–Moon system).\n\nHowever, the giant impact model has been criticised for being too explanatory; it can be expanded to explain any future discoveries and as such, is unfalsifiable. Also, many claim that much of the material from the impactor would have ended up in the Moon, meaning that the isotope levels would be different, but they are not. Also, while some volatile compounds such as water are absent from the Moon's crust, many others, such as manganese, are not.\n\nWhile the co-accretion and capture models are not currently accepted as valid explanations for the existence of the Moon, they have been employed to explain the formation of other natural satellites in the Solar System. Jupiter's Galilean satellites are believed to have formed via co-accretion, while the Solar System's irregular satellites, such as Triton, are all believed to have been captured.\n"}
{"id": "1597220", "url": "https://en.wikipedia.org/wiki?curid=1597220", "title": "International Society for Rock Mechanics", "text": "International Society for Rock Mechanics\n\nThe International Society for Rock Mechanics - ISRM was founded in Salzburg in 1962 as a result of the enlargement of the \"Salzburger Kreis\". Its foundation is mainly owed to Prof. Leopold Müller who acted as President of the Society till September 1966. The ISRM is a non-profit scientific association supported by the fees of the members and grants that do not impair its free action. In 2012 the Society has 6,800 members and 49 National Groups.\n\nThe field of Rock Mechanics is taken to include all studies relative to the physical and mechanical behaviour of rocks and rock masses and the applications of this knowledge for the better understanding of geological processes and in the fields of Engineering.\n\nThe main objectives and purposes of the Society are:\n\nThe main activities carried out by the Society in order to achieve its objectives are: \nThe Society is ruled by a Council, consisting of representatives of the National Groups, the Board and the Past Presidents. The current President is Dr Eda Freitas de Quadros, from Brazil.\n\nThe ISRM Secretariat has been headquartered in Lisbon, Portugal, at the Laboratório Nacional de Engenharia Civil - LNEC since 1966, date of the first ISRM Congress, when Prof. Manuel Rocha was elected as President of the Society.\n\nDetails of the Society can be obtained from its website at www.isrm.net\n"}
{"id": "2302152", "url": "https://en.wikipedia.org/wiki?curid=2302152", "title": "Joseph Jean Baptiste Xavier Fournet", "text": "Joseph Jean Baptiste Xavier Fournet\n\nJoseph Jean Baptiste Xavier Fournet (May 15, 1801 – January 8, 1869), French geologist and metallurgist, was born at Strasbourg.\n\nHe was educated at the \"École des Mines\" in Paris, and after considerable experience as a mining engineer he was in 1834 appointed professor of geology at Lyon.\n\nHe was a man of wide knowledge and extensive research, and wrote memoirs on chemical and mineralogical subjects, on eruptive rocks, on the structure of the Jura, the metamorphism of the Western Alps, on the formation of oolitic limestones, on kaolinization and on metalliferous veins. On metallurgical subjects he also was an acknowledged authority; and he published observations on the order of sulphurability of metals (\"loi de Fournet\").\n\nHe died in Lyon. His chief publications were: \"Études sur les dépôts métallifères\" (Paris, 1834); \"Histoire de la dolomie\" (Lyon, 1847); \"De l'extension des terrains houillers\" (1855); and \"Géologie lyonnaise\" (Lyon, 1861).\n\n"}
{"id": "22146885", "url": "https://en.wikipedia.org/wiki?curid=22146885", "title": "Kumi Lizard", "text": "Kumi Lizard\n\nIn New Zealand folklore, the Kumi Lizard is a purported reptile, possibly a giant monitor lizard, which allegedly once lived in New Zealand.\n\nIn \"New Zealand Mysteries\", author Robyn Gosset refers to a sighting of a Kumi in 1898 by a Maori bushman. Its length was estimated at 1.5 metres. In the first edition of the book, Gosset refers to several more accounts of the lizard which are absent from the second edition. These include an account from captain James Cook, who was told by Maori in Queen Charlotte Sound that huge, arboreal lizards were present in the surrounding bushland, and that they were greatly feared, as well as a sighting from 1875 of a large lizard washed up in a flooded Hokianga river and the discovery of bones possibly from the animal that same year.\n\nMore recent reports both come from 1898, one describing a large reptile seen near Gisbourne, the other a huge creature akin to a monitor lizard which advanced toward a bushman in Arowhana before retreating into a Rata tree. Although the animal itself was not spotted again, photographs of its footprints were taken.\n"}
{"id": "24362213", "url": "https://en.wikipedia.org/wiki?curid=24362213", "title": "L'impermeable", "text": "L'impermeable\n\nL'Impermeable is the name of the first waterproof watch invented at the end of the 19th century and manufactured by the \"West End Watch Company\", one of the oldest Swiss brands still active.\n\nWaterproof (or water-resistant) describes objects unaffected by water or resisting water passage, or which are covered with a material that resists or does not allow water passage.\n\nIn horology, the waterproofness of a watch is defined by its resistance under pressure. The manufacturers indicate mostly the degree of waterproofness in metres (m), feet (ft) or atmospheres (atm). Watches with the \"waterproof\" name, with or without indication of overpressure, have to be complied and have to undergo successfully the tests planned in the standard ISO-2281. These watches are intended for a current daily use and have to resist to the water during exercises such as the short-term swimming.\n\nSo finally watches said waterresistant must : resist to a dive in the water in a depth of at least 100 metres (330 ft), have a system of control of time and answer all the criteria planned by the standard ISO 6425: luminosity, shock resistance, resistance in magnetic fields, solidity of the bracelet.\n\nThe watchmaker company \"Alcide Droz & Sons\", established in St-Imier (Berne) since 1864, developed the first watch attested waterproof. They called it \"L'Impermeable\".\n\nThey had the idea to place a seal in the crown of winder, which is screwed on the counterpart. \"L'Impermeable\" was born: it is the very first waterproof watch intended especially to protect the movement from dust and humidity.\n\nThis pocket watch is nowadays on display at the International Watchmaking Museum in La Chaux-de-Fonds.\n\n\n"}
{"id": "985963", "url": "https://en.wikipedia.org/wiki?curid=985963", "title": "Lambda-CDM model", "text": "Lambda-CDM model\n\nThe ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos:\n\nThe model assumes that general relativity is the correct theory of gravity on cosmological scales.\nIt emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.\n\nThe ΛCDM model can be extended by adding cosmological inflation, quintessence and other elements that are current areas of speculation and research in cosmology.\n\nSome alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, modified gravity, theories of large-scale variations in the matter density of the universe, and scale invariance of empty space.\n\nMost modern cosmological models are based on the cosmological principle, which states that our observational location in the universe is not unusual or special; on a large-enough scale, the universe looks the same in all directions (isotropy) and from every location (homogeneity).\n\nThe model includes an expansion of metric space that is well documented both as the red shift of prominent spectral absorption or emission lines in the light from distant galaxies and as the time dilation in the light decay of supernova luminosity curves. Both effects are attributed to a Doppler shift in electromagnetic radiation as it travels across expanding space. Although this expansion increases the distance between objects that are not under shared gravitational influence, it does not increase the size of the objects (e.g. galaxies) in space. It also allows for distant galaxies to recede from each other at speeds greater than the speed of light; local expansion is less than the speed of light, but expansion summed across great distances can collectively exceed the speed of light.\n\nThe letter formula_1 (lambda) represents the cosmological constant, which is currently associated with a vacuum energy or dark energy in empty space that is used to explain the contemporary accelerating expansion of space against the attractive effects of gravity. A cosmological constant has negative pressure, formula_2, which contributes to the stress-energy tensor that, according to the general theory of relativity, causes accelerating expansion. The fraction of the total energy density of our (flat or almost flat) universe that is dark energy, formula_3, is currently (2015) estimated to be 0.692 ± 0.012, or even 0.6911 ± 0.0062 based on Planck satellite data.\n\nDark matter is postulated in order to account for gravitational effects observed in very large-scale structures (the \"flat\" rotation curves of galaxies; the gravitational lensing of light by galaxy clusters; and enhanced clustering of galaxies) that cannot be accounted for by the quantity of observed matter. Cold dark matter is \"non-baryonic\", i.e. it consists of matter other than protons and neutrons (and electrons, by convention, although electrons are not baryons); \"cold\", i.e. its velocity is far less than the speed of light at the epoch of radiation-matter equality (thus neutrinos are excluded, being non-baryonic but not cold); \"dissipationless\", i.e. it cannot cool by radiating photons; and \"collisionless\", i.e. the dark matter particles interact with each other and other particles only through gravity and possibly the weak force. The dark matter component is currently (2013) estimated to constitute about 26.8% of the mass-energy density of the universe.\n\nThe remaining 4.9% (2013) comprises all ordinary matter observed as atoms, chemical elements, gas and plasma, the stuff of which visible planets, stars and galaxies are made. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10% of the ordinary matter contribution to the mass-energy density of the universe.\n\nAlso, the energy density includes a very small fraction (~ 0.01%) in cosmic microwave background radiation, and not more than 0.5% in relic neutrinos. Although very small today, these were much more important in the distant past, dominating the matter at redshift > 3200.\n\nThe model includes a single originating event, the \"Big Bang\", which was not an explosion but the abrupt appearance of expanding space-time containing radiation at temperatures of around 10 K. This was immediately (within 10 seconds) followed by an exponential expansion of space by a scale multiplier of 10 or more, known as cosmic inflation. The early universe remained hot (above 10,000 K) for several hundred thousand years, a state that is detectable as a residual cosmic microwave background, or CMB, a very low energy radiation emanating from all parts of the sky. The \"Big Bang\" scenario, with cosmic inflation and standard particle physics, is the only current cosmological model consistent with the observed continuing expansion of space, the observed distribution of lighter elements in the universe (hydrogen, helium, and lithium), and the spatial texture of minute irregularities (anisotropies) in the CMB radiation. Cosmic inflation also addresses the \"horizon problem\" in the CMB; indeed, it seems likely that the universe is larger than the observable particle horizon.\n\nThe model uses the Friedmann–Lemaître–Robertson–Walker metric, the Friedmann equations and the cosmological equations of state to describe the observable universe from right after the inflationary epoch to present and future.\n\nThe expansion of the universe is parametrized by a dimensionless scale factor formula_4 (with time formula_5 counted from the birth of the universe), defined relative to the present day, so formula_6; the usual convention in cosmology is that subscript 0 denotes present-day values, so formula_7 is the current age of the universe. The scale factor is related to the observed redshift formula_8 of the light emitted at time formula_9 by\n\nThe expansion rate is described by the time-dependent Hubble parameter, formula_11, defined as\nwhere formula_13 is the time-derivative of the scale factor. The first Friedmann equation gives the expansion rate in terms of the matter+radiation density the curvature and the cosmological constant \n\nwhere as usual is the speed of light and is the gravitational constant. \nA critical density formula_15 is the present-day density, which gives zero curvature formula_16, assuming the cosmological constant formula_1 is zero, regardless of its actual value. Substituting these conditions to the Friedmann equation gives\n\nwhere formula_19 is the reduced Hubble constant.\nIf the cosmological constant were actually zero, the critical density would also mark the dividing line between eventual recollapse of the universe to a Big Crunch, or unlimited expansion. For the Lambda-CDM model with a positive cosmological constant (as observed), the universe is predicted to expand forever regardless of whether the total density is slightly above or below the critical density; though other outcomes are possible in extended models where the dark energy is not constant but actually time-dependent.\n\nIt is standard to define the present-day density parameter formula_20 for various species as the dimensionless ratio\nwhere the subscript formula_22 is one of formula_23 for baryons, formula_24 for cold dark matter, formula_25 for radiation (photons plus relativistic neutrinos), and formula_26 or formula_1 for dark energy.\n\nSince the densities of various species scale as different powers of formula_28, e.g. formula_29 for matter etc.,\nthe Friedmann equation can be conveniently rewritten in terms of the various density parameters as\nwhere w is the equation of state of dark energy, and assuming negligible neutrino mass (significant neutrino mass requires a more complex equation). The various formula_31 parameters add up to formula_32 by construction.\nIn the general case this is integrated by computer to give\nthe expansion history formula_33 and also observable distance-redshift relations for any chosen values of the cosmological parameters, which can then be compared with observations such as supernovae and baryon acoustic oscillations.\n\nIn the minimal 6-parameter Lambda-CDM model, it is assumed that curvature formula_34 is zero and formula_35, so this simplifies to\n\nObservations show that the radiation density is very small today, formula_37; if this term is neglected\nthe above has an analytic solution\nwhere formula_39\nthis is fairly accurate for formula_40 or formula_41million years.\nSolving for formula_42 gives the present age of the universe formula_43 in terms of the other parameters.\n\nIt follows that the transition from decelerating to accelerating expansion (the second derivative formula_44 crossing zero) occurred when\n\nwhich evaluates to formula_46 or formula_47 for the best-fit parameters estimated from the Planck spacecraft.\n\nThe discovery of the Cosmic Microwave Background (CMB) in 1964 confirmed a key prediction of the Big Bang cosmology. From that point on, it was generally accepted that the universe started in a hot, dense state and has been expanding over time. The rate of expansion depends on the types of matter and energy present in the universe, and in particular, whether the total density is above or below the so-called critical density. During the 1970s, most attention focused on pure-baryonic models, but there were serious challenges explaining the formation of galaxies, given the small anisotropies in the CMB (upper limits at that time). In the early 1980s, it was realized that this could be resolved if cold dark matter dominated over the baryons, and the theory of cosmic inflation motivated models with critical density. During the 1980s, most research focused on cold dark matter with critical density in matter, around 95% CDM and 5% baryons: these showed success at forming galaxies and clusters of galaxies, but problems remained; notably, the model required a Hubble constant lower than preferred by observations, and observations around 1988-1990 showed more large-scale galaxy clustering than predicted. These difficulties sharpened with the discovery of CMB anisotropy by COBE in 1992, and several modified CDM models, including ΛCDM and mixed cold and hot dark matter, came under active consideration through the mid-1990s. The ΛCDM model then became the leading model following the observations of accelerating expansion in 1998, and was quickly supported by other observations: in 2000, the BOOMERanG microwave background experiment measured the total (matter–energy) density to be close to 100% of critical, whereas in 2001 the 2dFGRS galaxy redshift survey measured the matter density to be near 25%; the large difference between these values supports a positive Λ or dark energy. Much more precise spacecraft measurements of the microwave background from WMAP in 2003 – 2010 and Planck in 2013 - 2015 have continued to support the model and pin down the parameter values, most of which are now constrained below 1 percent uncertainty.\n\nThere is currently active research into many aspects of the ΛCDM model, both to refine the parameters and possibly detect deviations. In addition, ΛCDM has no explicit physical theory for the origin or physical nature of dark matter or dark energy; the nearly scale-invariant spectrum of the CMB perturbations, and their image across the celestial sphere, are believed to result from very small thermal and acoustic irregularities at the point of recombination. A large majority of astronomers and astrophysicists support the ΛCDM model or close relatives of it, but Milgrom, McGaugh, and Kroupa are leading critics, attacking the dark matter portions of the theory from the perspective of galaxy formation models and supporting the alternative MOND theory, which requires a modification of the Einstein field equations and the Friedmann equations as seen in proposals such as MOG theory or TeVeS theory. Other proposals by theoretical astrophysicists of cosmological alternatives to Einstein's general relativity that attempt to account for dark energy or dark matter include f(R) gravity, scalar–tensor theories such as galileon theories, brane cosmologies, the DGP model, and massive gravity and its extensions such as bimetric gravity.\n\nIn addition to explaining pre-2000 observations,\nthe model has made a number of successful predictions: notably the existence of the\nbaryon acoustic oscillation feature, discovered in 2005 in the predicted location; and the statistics of weak gravitational lensing, first observed in 2000 by several teams. The polarization of the CMB, discovered in 2002 by DASI is now a dramatic success: in the 2015 Planck data release, there are seven observed peaks in the temperature (TT) power spectrum, six peaks in the temperature-polarization (TE) cross spectrum, and five peaks in the polarization (EE) spectrum. The six free parameters can be well constrained by the TT spectrum alone, and then the TE and EE spectra can be predicted theoretically to few-percent precision with no further adjustments allowed: comparison of theory and observations shows an excellent match.\n\nExtensive searches for dark matter particles have so far shown no well-agreed detection;\nthe dark energy may be almost impossible to detect in a laboratory, and its value is unnaturally small compared to naive theoretical predictions.\n\nComparison of the model with observations is very successful on large scales (larger than galaxies, up to the observable horizon), but may have some problems on sub-galaxy scales, possibly predicting too many dwarf galaxies and too much dark matter in the innermost regions of galaxies. This problem is called the \"small scale crisis\". These small scales are harder to resolve in computer simulations, so it is not yet clear whether the problem is the simulations, non-standard properties of dark matter, or a more radical error in the model.\n\nIt has been argued that the ΛCDM model is built upon a foundation of conventionalist stratagems, rendering it unfalsifiable in the sense defined by Karl Popper.\n\nThe simple ΛCDM model is based on six parameters: physical baryon density parameter; physical dark matter density parameter; the age of the universe; scalar spectral index; curvature fluctuation amplitude; and reionization optical depth. In accordance with Occam's razor, six is the smallest number of parameters needed to give an acceptable fit to current observations; other possible parameters are fixed at \"natural\" values, e.g. total density parameter = 1.00, dark energy equation of state = −1. (See below for extended models that allow these to vary.)\n\nThe values of these six parameters are mostly not predicted by current theory (though, ideally, they may be related by a future \"Theory of Everything\"), except that most versions of cosmic inflation predict the scalar spectral index should be slightly smaller than 1, consistent with the estimated value 0.96. The parameter values, and uncertainties, are estimated using large computer searches to locate the region of parameter space providing an acceptable match to cosmological observations. From these six parameters, the other model values, such as the Hubble constant and the dark energy density, can be readily calculated.\n\nCommonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less precisely measured at present.\n\nParameter values listed below are from the Planck Collaboration Cosmological parameters 68% confidence limits for the base ΛCDM model from Planck CMB power spectra, in combination with lensing reconstruction and external data (BAO + JLA + H). See also Planck (spacecraft).\n\nMassimo Persic and Paolo Salucci firstly estimated the baryonic density today present in ellipticals, spirals, groups and clusters of galaxies.\nThey performed an integration of the baryonic mass-to-light ratio over luminosity (in the following formula_48), weighted with the luminosity function formula_49 over the previously mentioned classes of astrophysical objects: \n\nThe result was:\n\nwhere formula_52.\n\nNote that this value is much lower than the prediction of standard cosmic nucleosynthesis formula_53, so that stars and gas in galaxies and in galaxy groups and clusters account for less than 10% of the primordially synthesized baryons. This issue is known as the problem of the \"missing baryons\".\n\nExtended models allow one or more of the \"fixed\" parameters above to vary, in addition to the basic six; so these models join smoothly to the basic six-parameter model in the limit that the additional parameter(s) approach the default values. For example, possible extensions of the simplest ΛCDM model allow for spatial curvature (formula_54 may be different from 1); or quintessence rather than a cosmological constant where the equation of state of dark energy is allowed to differ from −1. Cosmic inflation predicts tensor fluctuations (gravitational waves). Their amplitude is parameterized by the tensor-to-scalar ratio (denoted formula_55), which is determined by the unknown energy scale of inflation. Other modifications allow hot dark matter in the form of neutrinos more massive than the minimal value, or a running spectral index; the latter is generally not favoured by simple cosmic inflation models.\n\nAllowing additional variable parameter(s) will generally \"increase\" the uncertainties in the standard six parameters quoted above, and may also shift the central values slightly. The Table below shows results for each of the possible \"6+1\" scenarios with one additional variable parameter; this indicates that, as of 2015, there is no convincing evidence that any additional parameter is different from its default value.\n\nSome researchers have suggested that there is a running spectral index, but no statistically significant study has revealed one. Theoretical expectations suggest that the tensor-to-scalar ratio formula_55 should be between 0 and 0.3, and the latest results are now within those limits.\n\n\n"}
{"id": "1264179", "url": "https://en.wikipedia.org/wiki?curid=1264179", "title": "List of Scottish scientists", "text": "List of Scottish scientists\n\nList of Scottish engineers and scientists is a list of Scottish scientists born in Scotland or associated with Scotland.\n"}
{"id": "10840672", "url": "https://en.wikipedia.org/wiki?curid=10840672", "title": "List of Superfund sites in Connecticut", "text": "List of Superfund sites in Connecticut\n\nThis is a list of Superfund sites in Connecticut designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of December 16, 2010, there were 14 Superfund sites on the National Priorities List in Connecticut. One additional site has been proposed for entry on the list. Three sites has been cleaned up and removed from the list.\n\n\n"}
{"id": "36952234", "url": "https://en.wikipedia.org/wiki?curid=36952234", "title": "List of dental abnormalities associated with cutaneous conditions", "text": "List of dental abnormalities associated with cutaneous conditions\n\nMany conditions of or affecting the human integumentary system have associated abnormalities of the teeth.\n\n\n"}
{"id": "14725773", "url": "https://en.wikipedia.org/wiki?curid=14725773", "title": "List of group-0 ISBN publisher codes", "text": "List of group-0 ISBN publisher codes\n\nA list of publisher codes for (978) International Standard Book Numbers with a group code of zero.\n\nThe group-0 publisher codes are assigned as follows:\n\n"}
{"id": "28969168", "url": "https://en.wikipedia.org/wiki?curid=28969168", "title": "List of medical schools in Indonesia", "text": "List of medical schools in Indonesia\n\nThis is a list of medical schools located in Indonesia.\n\nThe top 15 medical schools in Indonesia, as of the year 2016 based on the study by PDAT (Pusat Data Analisa Tempo / Tempo Center of Data Analysis):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "30817170", "url": "https://en.wikipedia.org/wiki?curid=30817170", "title": "List of public relations journals", "text": "List of public relations journals\n\nThis is a list of peer-reviewed, English language academic journals in public relations.\n\n\n"}
{"id": "11485593", "url": "https://en.wikipedia.org/wiki?curid=11485593", "title": "List of towns and cities with 100,000 or more inhabitants/country: P-Q-R-S", "text": "List of towns and cities with 100,000 or more inhabitants/country: P-Q-R-S\n\n\n\nThe Philippines has 188 cities and/or towns having more than 100,000 inhabitants. Population figures below are from the \"2015 Census of Population\" by the Philippine Statistics Authority.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSangrilla\n"}
{"id": "7120005", "url": "https://en.wikipedia.org/wiki?curid=7120005", "title": "List of volcanoes in Ethiopia", "text": "List of volcanoes in Ethiopia\n\nThis is a list of volcanoes in Ethiopia. It includes both active and extinct vents.\n\n"}
{"id": "56240903", "url": "https://en.wikipedia.org/wiki?curid=56240903", "title": "Marianne Plehn", "text": "Marianne Plehn\n\nMarianne Plehn (30 Oktober 1863 – 18 January 1946) was a German zoologist. She was the first woman to be awarded a doctorate at the ETH Zurich and the first woman to be appointed as professor in Bavaria in 1914. Plehn is commemorated in the names of three polyclads and 12 disease agents of fishes. The breadth of her research on diseases of fishes defined the scientific study in this area. She published 114 scientific papers on the subject. She worked with Bruno Hofer and has been honoured as one of the founders of fish pathology.\n\nPlehn was born 1863 in Lubochen, then part of west Prussia. She had five siblings and her father was a manor lord. Her older brothers studied medicine. At the age of 27 Plehn moved to Zurich to take up university education in zoology, botany and geology. With financial support from her uncle she studied at the ETH Zurich, where she passed the examination as instructor for natural science in 1893. In 1896 she received her PhD in zoology, making her the first women who was awarded a doctorate by the ETH Zurich. Natural science was part of the philosophy department of the University of Zurich and while preparing for her doctorate she met and formed a lifelong friendship with Ricarda Huch, with whom she continued to exchange letters.\n\nIn 1898 she was appointed as assistant lecturer at the Bavarian Biological Experimental Institute (\"Bayerische Biologische Versuchsanstalt\"), at the School of Veterinary Medicine in Munich, Bavaria. There she worked with Bruno Hofer, who is considered as the founder of fish pathology. Plehn dedicated her life to research and her working day was spent in the laboratory and in the field. She turned down an appointment in Vienna and continued to work in Hofer's shadow, who as director presented their work in public appearances. From 1899 Plehn published her findings on fish disease and parasites, often producing several papers a year. At the time knowledge of fish physiology was limited, and little information was available on diseases and methods for their prevention. Very few scientists in Germany or abroad carried out research in fish pathology. Plehn's research agenda was determined by the needs of breeders supplying stock to lakes and streams in and around Munich and upper Bavaria. Her research was pioneering and often conducted under time pressure as breeders faces considerable financial losses. She carried out work at the institute's research station in Starnberg, which had been established in 1900 prompted by criticism that the institute did not diagnose deadly diseases and issued advice on their control quickly enough. Her most important tools were the microscope and her dissecting instruments, as she tried to determine the reasons for fish die-offs.\n\nOne of her early published study was an investigation of raised scales in minnows in 1902. Between 1903 and 1904 she identified the agent causing red disease in carp species, the \"Bacterium cyprinicida\", and demonstrated that more hygienic conditions reduced the problem. She also conducted research into fish parasitology. In 1903 she discovered the previously unknown \"Trypanoplasma cyprini\" (now called \"Cryptobia cyprini\" Plehn) in carp blood, while investigating protozona parasites. Between 1904 and 1906 she published five papers on stagger disease in Salmonidae (the salmon and trout group). Her later research on the subject prompted her to designate a new genus of parasites, the \"Lentospora\", now \"Myxobolus\". In 1906 she published the book \"Die Fische des Meeres und der Binnengewässer\" (\"Fishes of the Lakes and Inland Waters\") with illustrations aimed at a general public and fish breeders. In 1905 and 1908 she published the results of two studies on the trematode worm \"Sanguinicola armata und inermis\". She published her findings on kidney disease in 1908. Her findings on liver disease in Salmonidae were published in 1909 and 1915.\n\nPlehn considered her employment in Munich and the research opportunities in Starnberg ideal. But perhaps it was the only one available to her in an area that only a small number of scientists worked in. She frequently complained of overwork. In 1909 she was promoted to the position of \"Konservatorin\" (curator) at the Starnberg research station and her annual salary was raised to 3,000 Mark. In Munich she moved to a two-room apartment, which accommodated her large library and her piano. She received visits from her sister, many Zurich friends and from Huch, who also lived in Munich for a number of years with her young daughter. She spent much of her working life at Starnberg and took her holidays outside the summer season as she and her colleagues had to ensure that the research station was always staffed. While many of her friends took part in the women's movement, she did not and instead focused on her research and the prolific publication of research findings. Until 1939 she had a work room at the research station, even after official retirement.\n\nBacterial infections in Salmonidae became a major focus in her research and she contonued to study and publish on the subject over a long period of time. She examined in great detail the bacteria causing epidemics of furunculoses (abscesses) and fluorescence (open sores). Other areas of interest included studies of algal infections in carp, and two of the agents involved in bacterial infections, which now bear her name - the \"Branchiomyces sanguinis\" Plehn (the agent in gill rot) and \"Nephromyces piscium\" Plehn (now called \"Penicillium piscium\" Plehn).\n\nPlehn over many years investigated skeletal malformations in fishes and cancerous growths in fishes and other cold-blooded animals. This research was pioneering and she demonstrated that tumours in cold-blooded animals were similar to those in warm-blooded animals. Her research was followed with interest by scientists in cancer research and she spoke on the subject at the international conference on cancer in Paris in 1910 and later in Vienna. Two of her most important papers on the subject were published as monographs after they appeared in scientific journals. As a result of her research she was made an honorary member of the International Society for Cancer Research.\n\nIn Germany it was not possible for women to qualify as lecturers, thus Plehn was awarded the title \"royal professor\" in 1914 by King Ludwig III of Bavaria for her great contribution to fish pathology. This made her the first woman to be granted a professorship in Bavaria. But a teaching certificate was not linked to this title, and it only became possible for women to acquire such qualification in 1919. Plehn did not seek such qualification. Instead she continued to focus on research at the Bavarian Biological Experimental Institute. In 1920 Plehn published reports on two skin and grill parasites, the \"Ichthyochytrium vulgare\" and the \"Mucophilus cryprini\" (lates identified as a rickettsia).\n\nIn 1924 she published her second book \"Praktikum der Fischkrankheiten\" as practical guide for the use of fish breeders, hatchery managers, fishermen, fish biologists and veterinarians. Her publisher initially wished to publish the book as an update of Hofer's 1904 \"Handbuch der Fischkrankheiten\". But the field had changed so much in 20 years that Plehn took a different approach. Plehn focused on the major brood of fish in the Bavarian region, the Salmonidae and Cyprinidae (salmon/trout and carp) and included illustrations and practical guidance based on 20 years of her work at Starnberg. The book became a standard work for the use in fisheries, and the basis for subsequent research by other scientists. The illustrations and photographs continued to be used for decades.\n\nAlthough she was a professor, her teaching activities were limited to courses at the Fisheries School at Starnberg, which was attached to the research station. In 1927 at the age of 64 she was promoted to the position of \"Hauptkonservatorin\" (chief curator) at the research station. She had long been the foremost authority on fish disease, and she was known all over Germany. She was known as the \"Fischdoktorin\" and had a reputation for being cultivated and charming, with a quiet sense of humour. She retired on a small pension in 1928, and was awarded an honourable doctorate from the faculty of veterinary medicine at the Ludwig Maximilian University, making her the first women to receive such an honourable PhD from the LMU. She continued to be a presence at the Starnberg research station, giving lectures and putting her knowledge at the disposal of young colleagues. Plehn worked on an expanded edition of \"Praktikum der Fischkrankheiten\" until 1942. Although she was an outspoken critic of Adolf Hitler, the Nazi authorities did not force her to stop her teaching and research work at the station.\n\nBut she was only given a leading position when male researchers were drafted for military service during World War II. At the age of almost 80, she took over the management of institute. The institute was bombed out in 1943, the research station in Starnberg was bombed out in January 1944 and her apartment in Munich was destroyed six months later. By that time she had moved in with friends in Grafrath. In 1946 she died in Grafrath shortly after the war had ended. Her urn was buried in the Munich Nordfriedhof and moved to a mass grave in 1970.\n\nPlehn is commemorated in the names of three polyclads and 12 disease agents of fishes. Not all of her research findings stood the test of time as research methods advanced, but the tremendous breadth of her research on diseases of fishes defined the scientific study in this area. All in all she published 114 scientific papers on the subject. The \"Allgemeine Fischerei-Zeitung\", in which she had frequently published, commemorated her in 1963 on the 100th anniversar of her birth. She was honoured as one of the founders of fish pathology.\n"}
{"id": "25474577", "url": "https://en.wikipedia.org/wiki?curid=25474577", "title": "N-slit interferometric equation", "text": "N-slit interferometric equation\n\nQuantum mechanics was first applied to optics, and interference in particular, by Paul Dirac. Richard Feynman, in his Lectures on Physics, uses Dirac's notation to describe thought experiments on double-slit interference of electrons. Feynman's approach was extended to -slit interferometers for either single-photon illumination, or narrow-linewidth laser illumination, that is, illumination by indistinguishable photons, by Frank Duarte. The -slit interferometer was first applied in the generation and measurement of complex interference patterns.\n\nIn this article the generalized -slit interferometric equation, derived via Dirac's notation, is described. Although originally derived to reproduce and predict -slit interferograms, this equation also has applications to other areas of optics.\n\nIn this approach the probability amplitude for the propagation of a photon from a source to an interference plane , via an array of slits , is given using Dirac's bra–ket notation as\n\nThis equation represents the probability amplitude of a photon propagating from to via an array of slits. Using a wavefunction representation for probability amplitudes, and defining the probability amplitudes as\n\nwhere and are the incidence and diffraction phase angles, respectively. Thus, the overall probability amplitude can be rewritten as\n\nwhere\n\nand\n\nafter some algebra, the corresponding probability becomes\n\nwhere is the total number of slits in the array, or transmission grating, and the term in parentheses represents the phase that is directly related to the exact path differences derived from the geometry of the -slit array (), the intra interferometric distance, and the interferometric plane . In its simplest version, the phase term can be related to the geometry using\n\nwhere is the wavenumber, and and represent the exact path differences. Here it should be noted that the Dirac–Duarte (DD) \"interferometric equation\" is a probability distribution that is related to the intensity distribution measured experimentally. The calculations are performed numerically.\n\nThe DD interferometric equation applies to the propagation of a single photon, or the propagation of an ensemble of indistinguishable photons, and enables the accurate prediction of measured -slit interferometric patterns continuously from the near to the far field. Interferograms generated with this equation have been shown to compare well with measured interferograms for both even () and odd () values of from 2 to 1600.\n\nAt a practical level, the -slit interferometric equation was introduced for imaging applications and is routinely applied to predict -slit laser interferograms, both in the near and far field. Thus, it has become a valuable tool in the alignment of large, and very large, -slit laser interferometers used in the study of clear air turbulence and the propagation of \"interferometric characters\" for secure laser communications in space. Other analytical applications are described below.\n\nThe -slit interferometric equation has been applied to describe classical phenomena such as interference, diffraction, refraction (Snell's law), and reflection, in a rational and unified approach, using quantum mechanics principles. In particular, this interferometric approach has been used to derive generalized refraction equations for both positive and negative refraction, thus providing a clear link between diffraction theory and generalized refraction.\n\nFrom the phase term, of the interferometric equation, the expression\n\ncan be obtained, where .\n\nFor , this equation can be written as\n\nwhich is the generalized diffraction grating equation. Here, is the angle of incidence, is the angle of diffraction, is the wavelength, and is the order of diffraction.\n\nUnder certain conditions, , which can be readily obtained experimentally, the phase term becomes\n\nwhich is the generalized refraction equation, where is the angle of incidence, and now becomes the angle of refraction.\n\nFurthermore, the -slit interferometric equation has been applied to derive the cavity linewidth equation applicable to dispersive oscillators, such as the multiple-prism grating laser oscillators:\n\nIn this equation, is the beam divergence and the overall intracavity angular dispersion is the quantity in parentheses.\n\nResearchers working on Fourier-transform ghost imaging consider the -slit interferometric equation as an avenue to investigate the quantum nature of ghost imaging. Also, the -slit interferometric approach is one of several approaches applied to describe basic optical phenomena in a cohesive and unified manner.\n\nNote: given the various terminologies in use, for -slit interferometry, it should be made explicit that the -slit interferometric equation applies to two-slit interference, three-slit interference, four-slit interference, etc.\n\nThe Dirac principles and probabilistic methodology used to derive the -slit interferometric equation have also been used to derive the polarization quantum entanglement probability amplitude\n\nand corresponding probability amplitudes depicting the propagation of multiple pairs of quanta.\n\nA comparison of the Dirac approach with classical methods, in the performance of interferometric calculations, has been done by Travis S. Taylor \"et al\". These authors concluded that the interferometric equation, derived via the Dirac formalism, was advantageous in the very near field.\n\nSome differences between the DD interferometic equation and classical formalisms can be summarized as follows:\n\n\nSo far there has been no published comparison with more general classical approaches based on the Huygens–Fresnel principle or Kirchhoff's diffraction formula.\n\n"}
{"id": "13658011", "url": "https://en.wikipedia.org/wiki?curid=13658011", "title": "On Thermonuclear War", "text": "On Thermonuclear War\n\nOn Thermonuclear War is a book by Herman Kahn, a military strategist at the RAND Corporation, although it was written only a year before he left RAND to form the Hudson Institute. It is a controversial treatise on the nature and theory of war in the thermonuclear weapon age. In it, Kahn addresses the strategic doctrines of nuclear war and its effect on the international balance of power.\n\nKahn introduced the Doomsday Machine as a rhetorical device to show the limits of John von Neumann's strategy of mutual assured destruction or MAD. The book helped popularize the term megadeath, which Kahn coined in 1953.\n\nKahn's stated purpose in writing the book was \"avoiding disaster and buying time, without specifying the use of this time.\" The title of the book was inspired by the classic volume \"On War\", by Carl von Clausewitz.\n\nWidely read on both sides of the Iron Curtain—the book sold 30,000 copies in hardcover—it is noteworthy for its views on the lack of credibility of a purely thermonuclear deterrent and how a country could \"win\" a nuclear war.\n\nOf the book, Hubert H. Humphrey said: \"New thoughts, particularly those which contradict current assumptions, are always painful for the human mind to contemplate. \"On Thermonuclear War\" is filled with such thoughts.\"\n\nLines from the character General Buck Turgidson in the Stanley Kubrick's 1964 film Dr. Strangelove directly mimic passages from this book, such as Turgidson's phrase \"two admittedly regrettable, but nevertheless, distinguishable post-war environments\" which reflects a chart from this book labeled \"Tragic but Distinguishable Postwar States\" (also discussed in the related article Megadeath). Indeed, the folder that General Turgidson holds while reading a report on projected nuclear war casualties is titled \"Global Targets in Megadeaths\".\n\nFirst published in 1960 by the Princeton University Press (), it was republished as a paperback by Transaction Publishers in 2007 ().\n\n\n"}
{"id": "7260569", "url": "https://en.wikipedia.org/wiki?curid=7260569", "title": "Outline of thought", "text": "Outline of thought\n\nThe following outline is provided as an overview of and topical guide to thought (thinking):\n\nThought (also called thinking) – the mental process in which beings form psychological associations and models of the world. Thinking is manipulating information, as when we form concepts, engage in problem solving, reason and make decisions. Thought, the act of thinking, produces thoughts. A thought may be an idea, an image, a sound or even an emotional feeling that arises from the brain.\n\nThought (or thinking) can be described as all of the following:\n\n\nListed below are types of thought, also known as thinking processes.\n\nHuman thought\n\n\nEmotional intelligence\n\nProblem solving\n\nReasoning\n\nOrganizational thought (thinking by organizations)\n\n\nAspects of the thinker which may affect (help or hamper) his or her thinking:\n\n\nHistory of reasoning\n\nNootropic\nSubstances that improve mental performance:\n\n\n\n\n\n\n\n\n\n\n\nMiscellaneous\n\nThinking \n\nLists\n\n"}
{"id": "41246804", "url": "https://en.wikipedia.org/wiki?curid=41246804", "title": "Oxyyttropyrochlore-(Y)", "text": "Oxyyttropyrochlore-(Y)\n\nOxyyttropyrochlore-(Y), also referred to as \"obruchevite\" or \"yttropyrochlore-(Y)\", is a potential (not yet accepted) zero-valent-dominant mineral of the pyrochlore group. Its formula can be written as (Y,◻)NbOO.\n\nThe name \"yttropyrochlore-(Y)\" for this compound was used by Kalita (1957), and Ercit et al. (2003), but it has become obsolete and the mineral status is not yet clear. The yttropyrochlore-(Y) as mentioned by Tindle & Breaks (1998) is in fact \"oxyyttropyrochlore-(Y)\".\n\n"}
{"id": "48625218", "url": "https://en.wikipedia.org/wiki?curid=48625218", "title": "Pebble accretion", "text": "Pebble accretion\n\nIn pebble accretion the accretion of objects ranging from cm's up to meters in diameter onto planetesimals in a protoplanetary disk is enhanced by aerodynamic drag. The rapid growth of the planetesimals via pebble accretion allows for the formation of giant planet cores in the outer Solar System before the dispersal of the gas disk. A reduction in the size of pebbles as they lose water ice after crossing the ice line and a declining density of gas with distance from the sun slow the rates of pebble accretion in the inner Solar System resulting in smaller terrestrial planets, a small mass of Mars and a low mass asteroid belt.\n\nPebbles ranging in size from cm's up to a meter in size are accreted at an enhanced rate in a protoplanetary disk. A protoplanetary disk is made up of a mix of gas and solids including dust, pebbles, planetesimals, and protoplanets. Gas in a protoplanetary disk is pressure supported and as a result orbits at a velocity slower than large objects. The gas affects the motions of the solids in varying ways depending on their size, with dust moving with the gas and the largest planetesimals orbiting largely unaffected by the gas. Pebbles are an intermediate case, aerodynamic drag causes them to settle toward the central plane of the disk and to orbit at a sub-keplerian velocity resulting in radial drift toward the central star. The pebbles frequently encounter planetesimals as a result of their lower velocities and inward drift. If their motions were unaffected by the gas only a small fraction, determined by gravitational focusing and the cross-section of the planetesimals, would be accreted by the planetesimals. The remainder would follow hyperbolic paths, accelerating toward the planetesimal on their approach and decelerating as they recede. However, the drag the pebbles experience grows as their velocities increase, slowing some enough that they become gravitationally bound to the planetesimal. These pebbles continue to lose energy as they orbit the planetesimal causing them to spiral toward and be accreted by the planetesimal.\n\nSmall planetesimals accrete pebbles that are drifting past them at the relative velocity of the gas. Those pebbles with stopping times similar to the planetesimal's Bondi time are accreted from within its Bondi radius. In this context the Bondi radius is defined as the distance at which an object approaching a planetesimal at the relative velocity of the gas is deflected by one radian; the stopping time is the exponential timescale for the deceleration of an object due to gas drag, and the Bondi time is the time required for an object to cross the Bondi radius. Since the Bondi radius and Bondi time increase with the size of the planetesimal, and the stopping time increases with the size of the pebble, the optimal pebble size increases with size of planetesimal. Smaller objects, with ratios of stopping times to Bondi times less than 0.1, are pulled from the flow past the planetesimal and accreted from a smaller radius which declines with the square root of this ratio. Larger, weakly coupled pebbles are also accreted less efficiently due to three body effects with the radius accreted from declining rapidly between ratios of 10 and 100. The Bondi radius is proportional to the mass of the planetesimal so the relative growth rates is proportional to mass squared resulting in runaway growth. The aerodynamic deflection of the gas around the planetesimal reduces the efficiency of pebble accretion resulting in a maximum growth timescale at 100 km.\n\nLarger planetesimals, above a transition mass of roughly Ceres mass in the inner solar system and Pluto mass in the outer solar system, accrete pebbles with Stoke's numbers near one from their Hill radii. The Stokes number in this context is the product of stopping time and the keplerian frequency. As with small planetesimals the radius from which pebbles accrete declines for smaller and larger pebble sizes. The optimal pebble size for large planetesimals measures in cm's due to a combination of the accretion radius and the radial drift rates of the pebbles. As objects grow their accretion changes from 3-D, with accretion from part of the thickness of the pebble disk, to 2D with accretion from full thickness of pebble disk. The relative growth rate in 2-D accretion is proportional to mass^(2/3) leading to oligarchical growth and the formation of similar sized bodies. Pebble accretion can result in doubling of mass of an Earth-massed core in as little as 5500 years, reducing the timescales for growth of the cores of giant planets by 2 or 3 orders of magnitude relative to planetesimal accretion. The gravitational influence of these massive bodies can create a partial gap in the gas disk altering the pressure gradient. The velocity of gas then becomes super-keplerian outside the gap stopping the inward drift of pebbles and ending pebble accretion.\n\nIf the formation of pebbles is slow, pebble accretion leads to the formation of a few gas giants in the outer Solar System. The formation of the gas giants is a long-standing problem in planetary science. The accretion of the cores of giant planets via the collision and mergers of planetesimals is slow and may be difficult to complete before the gas disk dissipates. (Although formation via planetesimal collisions can be accomplished within the typical lifetime of a protoplanetary disc.) The largest planetesimals can grow much faster via pebble accretion, but if the formation or delivery of pebbles is rapid numerous Earth-mass planets form instead of a few giant planet cores. As the largest objects approach Earth-mass the radius from which pebbles are accreted is limited by the Hill radius. This slows their growth relative to their neighbors (growth becomes oligarchic) and allows many objects to accrete similar masses of pebbles. However, if the formation or the delivery of pebbles is slow growth timescales becomes longer than the time required for gravitationally stirring. The largest planetesimals then excite the inclinations and eccentricities of the smaller planetesimals. Their inclined orbits keep small planetesimals outside of the narrow disk of pebbles during most of their orbits, limiting their growth. The period of runaway growth is then extended and the largest objects are able to accrete a sizable fraction of the pebbles and grow into giant planet cores. As the cores grow larger some reach masses sufficient to create partial gaps in the gas disk, altering its pressure gradient and blocking the inward drift of pebbles. Accretion of pebbles is then halted and the gas envelope surrounding the core cools and collapses allowing for the rapid accretion of gas and the formation of a gas giant. Cores that do not grow massive enough to clear gaps in the pebble disk are only able to accrete small gas envelopes and instead become ice giants. The rapid growth via pebble accretion allows the cores to grow large enough to accrete massive gas envelopes forming gas giants while avoiding migrating very close to the star. In simulations cold gas giants like Jupiter and Saturn can form via pebble accretion if their initial embryos began growing beyond 20 AU. This distant formation offers a potential explanation for Jupiter's enrichment in noble gases. However, dedicated formation models indicate that it is difficult to reconcile growth via pebble accretion with the final mass and composition of the solar system ice giants Uranus and Neptune.\n\nThe terrestrial planets may be much smaller than the giant planets due to the sublimation of water ice as pebbles crossed the ice line. The radial drift of pebbles causes them to cross the ice line where water ice sublimates releasing silicate grains. The silicate grains are less sticky than icy grains resulting in bouncing, or fragmentation during collisions and the formation of smaller pebbles. These smaller pebbles are dispersed into a thicker disk by the turbulence in the gas disk. The mass flow of solids drifting through the terrestrial region is also reduced by half by the loss of water ice. In combination these two factors significantly reduce the rate at which mass is accreted by planetesimals in the inner Solar System relative to the outer Solar System. As a result, lunar mass planetary embryos in the inner Solar System are able to grow only to around Mars-mass, whereas in the outer Solar System they are able to grow to more than 10x Earth-mass forming the cores of giant planets. Beginning instead with planetesimals formed via streaming instabilities yields similar results in the inner Solar System. In the asteroid belt the largest planetesimals grow into Mars-massed embryos. These embryos stir the smaller planetesimals, increasing their inclinations, causing them to leave the pebble disk. The growth of these smaller planetesimals is stalled at this point, freezing their size distribution near that of the current asteroid belt. The variation of accretion efficiency with pebble size during this process results in the size sorting of the chondrules observed in the primitive meteorites. In the terrestrial zone pebble accretion plays a smaller role. Here growth is due to a mix of pebble and planetesimal accretion until an oligarchical configuration of isolate lunar-massed embryos forms. Continued growth due to the accretion of inward drifting chondrules increases the mass of these embryos until their orbits are destabilized, leading to giant impacts between the embryos and the formation of Mars-sized embryos. The cutoff of the inward drift of icy pebbles by the formation of Jupiter before the ice line moved into the terrestrial region would limit the water fraction of the planets formed from these embryos.\n\nThe small mass of Mars the low mass asteroid belt may be the result of pebble accretion becoming less efficient as the density of gas in the protoplanetary disk decreases. The protoplanetary disk from which the Solar System formed is believed to have had a surface density that decreased with distance from the Sun and have been flared, with an increasing thickness with distance from the Sun. As a result, the density of the gas and the aerodynamic drag felt by pebbles embedded in the disk would have decreased significantly with distance. If the pebbles were large the efficiency of pebble accretion would decline with distance from the Sun as the aerodynamic drag becomes too weak for the pebbles to be captured during encounters with the largest objects. An object that grows rapidly at Earth's orbital distance would only grow slowly in Mars's orbit and very little in the asteroid belt. The formation of Jupiter's core could also reduce the mass of the asteroid belt by creating a gap in the pebble disk and halting the inward drift of pebbles from beyond the ice line. Objects in the asteroid belt would then be deprived of pebbles early while objects in the terrestrial region continued to accrete pebbles that drifted in from the asteroid region.\n"}
{"id": "2653867", "url": "https://en.wikipedia.org/wiki?curid=2653867", "title": "Peter Quinn (astronomer)", "text": "Peter Quinn (astronomer)\n\nProfessor Peter Quinn is Executive Director of the International Centre for Radio Astronomy Research (ICRAR) in Perth, Western Australia and was previously the head of the Data Management and Operations Division at the European Southern Observatory (ESO).\n\nProfessor Quinn has authored more than 300 published scientific papers and was awarded the 'Western Australian Scientist of the Year' in 2012. Professor Quinn is also a Fellow of Australian Academy of Technological Sciences and Engineering.\n\nAt the ESO Quinn was working towards the creation of the Astrophysical Virtual Observatory, a massive virtual database of astronomical data, designed to allow astrophysicists to carry out research using existing data without using valuable telescope time.\n\nPrior to working at ESO, Dr. Quinn taught astrophysics at Caltech for several years, working with Gerald Jay Sussman. His scientific interests include the study of dark matter, computational cosmology, galactic formation, and MACHOs.\n"}
{"id": "82010", "url": "https://en.wikipedia.org/wiki?curid=82010", "title": "Pioneer 4", "text": "Pioneer 4\n\nPioneer 4 was an American spin-stabilized unmanned spacecraft launched as part of the Pioneer program on a lunar flyby trajectory and into a heliocentric orbit making it the first probe of the United States to escape from the Earth's gravity. It carried a payload similar to \"Pioneer 3\": a lunar radiation environment experiment using a Geiger–Müller tube detector and a lunar photography experiment. It passed within 58,983 km of the Moon's surface. However, \"Pioneer 4\" did not come close enough to trigger its photoelectric sensor. The spacecraft was still in solar orbit as of 1969. It was the only successful lunar probe launched by the U.S. in 12 attempts between 1958–63; only in 1964 would Ranger 7 surpass its success by accomplishing all of its mission objectives.\n\nAfter the Soviet Luna 1 probe conducted the first successful flyby of the Moon on January 3, 1959, the pressure felt by the US to succeed with a lunar mission was enormous, especially since American mission failures were entirely public while the Soviet failures were kept a secret.\n\n\"Pioneer 4\" was a cone-shaped probe 51 cm high and 23 cm in diameter at its base. The cone was composed of a thin fiberglass shell coated with a gold wash to make it electrically conducting and painted with white stripes to maintain the temperature between 10 and 50 degrees Celsius. At the tip of the cone was a small probe which combined with the cone itself to act as an antenna. At the base of the cone a ring of mercury batteries provided power. A photoelectric sensor protruded from the center of the ring. The sensor was designed with two photocells which would be triggered by the light of the Moon when the probe was within about 30,000 km of the Moon. At the center of the cone was a voltage supply tube and two Geiger–Müller tubes. The Laboratory's Microlock system, used for communicating with earlier Explorer satellites, did not have sufficient range to perform this mission. Therefore a new radio system called TRAC(E) \"Tracking And Communication (Extraterrestrial)\" was designed. TRAC(E) was an integral part of the Goldstone Deep Space Communications Complex. A transmitter with a mass of 0.5 kilograms delivered a phase modulated signal of 0.1 W at a frequency of 960.05 MHz. The modulated carrier power was 0.08 W and the total effective radiated power 0.18 W. A despin mechanism consisted of two 7 gram weights which spooled out to the end of two 150 cm wires when triggered by a hydraulic timer 10 hours after launch. The weights were designed to slow the spacecraft spin from 400 rpm to 6 rpm, and then weights and wires were released. Pioneer 4 received a few small modifications over its predecessor, namely added lead shielding around the Geiger tubes and modifications to the telemetry system to improve its reliability and signal strength. The probe had S/N #4, with probe #3 recalled from launch due to technical issues.\n\n\"Pioneer 4\" was launched with a Juno II launch vehicle, which also launched \"Pioneer 3\". Juno II closely resembled the Juno I (Jupiter-C based) vehicle that launched Explorer 1. Its first stage was a 19.51 m elongated Jupiter IRBM missile that was used by the U.S. Army. On top of the Jupiter propulsion section was a guidance and control compartment that supported a rotating tub containing the rocket stages 2, 3 and 4. \"Pioneer 4\" was mounted on top of stage 4.\n\nAt 12:10 AM EST on the night of March 2-3 of 1959, Pioneer 4 lifted from LC-5. This time, the booster performed almost perfectly so that \"Pioneer 4\" achieved its primary objective (an Earth-Moon trajectory), returned radiation data and provided a valuable tracking exercise. A slightly longer than nominal second stage burn however was enough to induce small trajectory and velocity errors, so that the probe passed within 60,000 km of the Moon's surface (7.2° E, 5.7° S) on 4 March 1959 at 22:25 UT (5:25 p.m. EST) at a speed of 7,230 km/h. The distance was not close enough to trigger the photoelectric sensor. The probe continued transmitting radiation data for 82.5 hours, to a distance of 409,000 miles (658,000 km), and reached perihelion on 18 March 1959 at 01:00 UT. The cylindrical fourth stage casing (173 cm long, 15 cm diameter, 4.65 kg) went into orbit with the probe. The communication system had worked well, and it was estimated that signals could have been received out to 680,000 miles (1 million kilometers) had there been enough battery power.\n\n\n"}
{"id": "4121452", "url": "https://en.wikipedia.org/wiki?curid=4121452", "title": "Politics of Nature", "text": "Politics of Nature\n\nPolitics of Nature: How to Bring the Sciences Into Democracy (2004, ) is a book by the French theorist and philosopher of science Bruno Latour. The book is an English translation by Catherine Porter of the French book, \"Politiques de la nature\". It is published by Harvard University Press.\n\nIn the book, Latour argues for a new and better take on political ecology (not the discipline but the ecological political movements, e.g. greens) that embraces his feeling that, \"political ecology has nothing to do with nature\". In fact, Latour argues that the idea of nature is unfair because it unfairly allows those engaged in political discourse to \"short-circuit\" discussions. Latour uses Plato's metaphor of \"the cave\" to describe the current role of nature and science in separating facts from values which is the role of politics and non-scientists. Building on the arguments levelled in his previous works, Latour argues that this distinction between facts and values is rarely useful and in many situations dangerous. He claims that it leads to a system that ignores nature's socially constructed status and creates a political order without \"due process of individual will\".\n\nInstead, he calls for a \"new Constitution\" where different individuals can assemble democratically without the definitions of facts and values influenced by current attitudes towards nature and scientific knowledge. Latour describes an alternate set of rules by which this assembly, or collective as he calls it, might come together and be constituted. He also describes the way that entities will be allowed in or out in the future. In describing this collective, Latour draws attention to the role of the spokesperson, who must be doubted but who must speak for otherwise mute things in order to ensure that the collective involves both \"humans and non-humans\". This is also an important aspect of Actor-network theory (ANT) that can be found in his main sociological works.\n\nThe book includes a short summary at the end and a glossary of terms.\n\nSal Restivo emphasises that the book is reproducing the insights from Science Studies, which Bruno Latour himself has greatly contributed to. However, Sal Restivo questions whether Latour understood social constructivism and what sociologists actually do.\n\n\n"}
{"id": "36607703", "url": "https://en.wikipedia.org/wiki?curid=36607703", "title": "PyLadies", "text": "PyLadies\n\nPyLadies is an international mentorship group which focuses on helping more women become active participants in the Python open-source community. It is part of the Python Software Foundation. It was started in Los Angeles in 2011. The mission of the group is to create a diverse Python community through outreach, education, conferences and social gatherings. PyLadies also provides funding for women to attend open source conferences. The aim of PyLadies is increasing the participation of women in computing. PyLadies became a multi-chapter organization with the founding of the Washington (D.C.) chapter in August 2011. The group currently has more than 40 chapters around the world. \n\nThe organization was created in Los Angeles in April 2011 by seven women: Audrey Roy Greenfeld, Christine Cheung, Esther Nam, Jessica Stanton, Katharine Jarmul, Sandy Strong, and Sophia Viklund. Around 2012, the organization filed for nonprofit status.\n\nAnother PyLadies chapter opened in Dublin in 2013. PyLadies started a chapter in Tokyo in 2014. In 2018, PyLadies opened up a chapter in St. Petersburg.\n\nPyLadies has conducted outreach events for both beginners and experienced users. PyLadies has conducted hackathons, ladies' nights and workshops for Python enthusiasts.\n\nEach chapter is free to run themselves as they wish as long as they are focused on the goal of empowering women. Women make up the majority of the group, but membership is not limited to women and the group is open to helping people who identify as other gender identities as well. \n\n"}
{"id": "51804976", "url": "https://en.wikipedia.org/wiki?curid=51804976", "title": "Quanta Magazine", "text": "Quanta Magazine\n\nQuanta Magazine is an editorially independent online publication of the Simons Foundation covering developments in mathematics, theoretical physics, theoretical computer science and the basic life sciences.\n\nThe articles in the magazine are freely available to read online. Several publications like \"Scientific American\", \"Wired\", and \"The Atlantic\", as well as international science publications like \"Spektrum der Wissenschaft\", have reprinted articles from \"Quanta Magazine\". \"Undark Magazine\" described \"Quanta Magazine\" as \"highly regarded for its masterful coverage of complex topics in science and math.\" The science news aggregator \"RealClearScience\" ranked \"Quanta Magazine\" No. 2 on its list of \"The Top 10 Websites for Science in 2016.\" Initially launched as \"Simons Science News\" in October 2012, it was renamed to its current title in July 2013. \"Quanta\" was founded by the former New York Times journalist Thomas Lin, who is the magazine's first editor-in-chief. \"Quanta's\" two deputy editors are John Rennie (editor) and Michael Moyer, formerly of \"Scientific American\". Notable writers include K.C. Cole, Dan Falk, Kevin Hartnett, Erica Klarreich, George Musser, Jennifer Ouellette, Frank Wilczek, Natalie Wolchover and Carl Zimmer. On 2 May 2017 \"Quanta\" launched its redesigned and rebranded magazine website.\n"}
{"id": "3918214", "url": "https://en.wikipedia.org/wiki?curid=3918214", "title": "Ranjit Chandra", "text": "Ranjit Chandra\n\nRanjit Kumar Chandra (रंजीत कुमार चंद्रा) is a researcher in the field of nutrition and immunology who has been accused by the \"British Medical Journal\" of committing scientific fraud. His fraud was also the subject of a 2006 documentary by the Canadian Broadcasting Corporation. A jury trial in July, 2015, concluded that the allegations of fraud were truthful. Due to these allegations, a number of his scientific articles have been subject to retractions.\n\nChandra is listed in the official directory of the Royal College of Physicians and Surgeons of Canada with a specialization in pediatrics, acquired 8 November 1976, in Gurgaon, India.\n\nChandra is listed in the official directory of the College of Physicians and Surgeons of Ontario with a revocation of registration effective 18 June 2018 and a summary of a Discipline Committee hearing of 8 December 2017.\n\nIn the late 1980s, Chandra was hired by Ross Laboratories, US manufacturer of Isomil and Similac, to study whether infant formulas could help babies to avoid allergy problems. It was the job of Chandra's nurse, Marilyn Harvey, to find newborns in the St. John's, Newfoundland area whose parents had allergies and who were willing to participate in the research. Nestlé (\"Good Start\") and Mead Johnson had also contacted him for similar studies on their baby formulas. Harvey was the first to raise concerns about Chandra's research data, disputing the accuracy of the number of infants in those studies. Chandra found that the Nestlé and Mead Johnson formulas could protect infants from allergies, but the Ross formula could not, despite nearly identical ingredients in the three studied formulas. In his defense, Chandra later cited study design problems, although he had designed the studies himself. He then claimed that he had not been paid enough money to properly perform the studies.\n\nIn 1994, Memorial University, at which Chandra was a professor, investigated him for research fraud but its findings were kept private.\n\nA study published in the September 2001 edition of \"Nutrition\" claimed that his patented multivitamin formula could reverse memory problems in people over the age of 65. However, the same study had been submitted to the \"British Medical Journal\" in 2000 and rejected after a review by a statistical expert, who stated that the study had \"all the hallmarks of having been completely invented.\" The BMJ asked Memorial University to investigate. When they, too, found that the study could not possibly have been completed as claimed, they asked Chandra to produce his data. He refused, claiming the university had lost it, and resigned.\n\nAs claims from Chandra's studies gained widespread attention from sources such as \"The New York Times\", several other world-renowned scientists began examining his published results and realised they were completely fabricated. By 2005, his vitamin study had been completely debunked and retracted.\n\nAt one point, Chandra published a study by someone named Amrit Jain in \"Nutrition Research\" confirming his previous results. Amrit Jain was supposedly affiliated with the Medical Clinic and Nursing Home, Jaipur, India; however, this entity has never been referred to anywhere except in Amrit Jain's paper. Amrit Jain's mailing address is not in India, but is a rented post office box in Canada. No known attempts to verify Jain's identity or existence have ever been successful.\n\nAlthough Chandra retired from Memorial University of Newfoundland under a cloud of suspicion, university officials did not charge him with research fraud. At least one university administrator admits that Chandra's allegations of bias and threats of a lawsuit led to the termination of the university's investigation. Marilyn Harvey would later enter into legal proceedings against Memorial University, claiming that MUN led people to erroneously believe that her allegations against Chandra were unfounded. In response to the scandal, Memorial University created the \"Marilyn Harvey Award to Recognize the Importance of Research Ethics\" in honour of Ms. Harvey.\n\nChandra filed a lawsuit against the Canadian Broadcasting Corporation seeking damages for libel in relation to a news segment entitled \"The Secret Life of Dr. Chandra\". On July 16, 2015, the Ontario Superior Court ruled that the Canadian Broadcasting Corporation's defences of fair comment and responsible journalism were allowed to go to the jury. On July 24, 2015, the jury returned its verdict and found that the substance of the CBC broadcast was in fact true. Further, the jury dismissed Chandra's claim that the CBC had invaded his privacy in preparing the program.\n\nThe presiding judge, Mr. Justice Graeme Mew, wrote in the opinion of the court:Tactically, Dr. Chandra played a high stakes game. The phrase, \"live by the sword, die by the sword\" comes to mind.In the end, he failed abjectly.\n\nHe was Appointed as an Officer in the Order of Canada in 1989. His membership in the Order of Canada was terminated on December 3, 2015.\n\nAccording to media reports from 24 August 2016, Chandra was charged with fraud over $5,000 and an arrest warrant has been issued for him in conjunction with an alleged scheme involving Ontario health insurance billings.\n\n\n"}
{"id": "23854465", "url": "https://en.wikipedia.org/wiki?curid=23854465", "title": "Real-time Control System", "text": "Real-time Control System\n\nReal-time Control System (RCS) is a reference model architecture, suitable for many software-intensive, real-time control problem domains. RCS is a reference model architecture that defines the types of functions that are required in a real-time intelligent control system, and how these functions are related to each other. \nRCS is not a system design, nor is it a specification of how to implement specific systems. RCS prescribes a hierarchical control model based on a set of well-founded engineering principles to organize system complexity. All the control nodes at all levels share a generic node model.\n\nAlso RCS provides a comprehensive methodology for designing, engineering, integrating, and testing control systems. Architects iteratively partition system tasks and information into finer, finite subsets that are controllable and efficient. RCS focuses on intelligent control that adapts to uncertain and unstructured operating environments. The key concerns are sensing, perception, knowledge, costs, learning, planning, and execution.\n\nA reference model architecture is a canonical form, not a system design specification. The RCS reference model architecture combines real-time motion planning and control with high level task planning, problem solving, world modeling, recursive state estimation, tactile and visual image processing, and acoustic signature analysis. In fact, the evolution of the RCS concept has been driven by an effort to include the best properties and capabilities of most, if not all, the intelligent control systems currently known in the literature, from subsumption to SOAR, from blackboards to object-oriented programming. \n\nRCS (real-time control system) is developed into an intelligent agent architecture designed to enable any level of intelligent behavior, up to and including human levels of performance. RCS was inspired by a theoretical model of the cerebellum, the portion of the brain responsible for fine motor coordination and control of conscious motions. It was originally designed for sensory-interactive goal-directed control of laboratory manipulators. Over three decades, it has evolved into a real-time control architecture for intelligent machine tools, factory automation systems, and intelligent autonomous vehicles.\n\nRCS applies to many problem domains including manufacturing examples and vehicle systems examples. Systems based on the RCS architecture have been designed and implemented to varying degrees for a wide variety of applications that include loading and unloading of parts and tools in machine tools, controlling machining workstations, performing robotic deburring and chamfering, and controlling space station telerobots, multiple autonomous undersea vehicles, unmanned land vehicles, coal mining automation systems, postal service mail handling systems, and submarine operational automation systems.\n\nRCS has evolved through a variety of versions over a number of years as understanding of the complexity and sophistication of intelligent behavior has increased. The first implementation was designed for sensory-interactive robotics by Barbera in the mid 1970s.\n\nIn RCS-1, the emphasis was on combining commands with sensory feedback so as to compute the proper response to every combination of goals and states. The application was to control a robot arm with a structured light vision system in visual pursuit tasks. RCS-1 was heavily influenced by biological models such as the Marr-Albus model, and the Cerebellar Model Arithmetic Computer (CMAC). of the cerebellum.\n\nCMAC becomes a state machine when some of its outputs are fed directly back to the input, so RCS-1 was implemented as a set of state-machines arranged in a hierarchy of control levels. At each level, the input command effectively selects a behavior that is driven by feedback in stimulus-response fashion. CMAC thus became the reference model building block of RCS-1, as shown in the figure. \n\nA hierarchy of these building blocks was used to implement a hierarchy of behaviors such as observed by Tinbergen and others. RCS-1 is similar in many respects to Brooks' subsumption architecture, except that RCS selects behaviors before the fact through goals expressed in commands, rather than after the fact through subsumption.\n\nThe next generation, RCS-2, was developed by Barbera, Fitzgerald, Kent, and others for manufacturing control in the NIST Automated Manufacturing Research Facility (AMRF) during the early 1980s. The basic building block of RCS-2 is shown in the figure. \n\nThe H function remained a finite state machine state-table executor. The new feature of RCS-2 was the inclusion of the G function consisting of a number of sensory processing algorithms including structured light and blob analysis algorithms. RCS-2 was used to define an eight level hierarchy consisting of Servo, Coordinate Transform, E-Move, Task, Workstation, Cell, Shop, and Facility levels of control. \n\nOnly the first six levels were actually built. Two of the AMRF workstations fully implemented five levels of RCS-2. The control system for the Army Field Material Handling Robot (FMR) was also implemented in RCS-2, as was the Army TMAP semi-autonomous land vehicle project.\n\nRCS-3 was designed for the NBS/DARPA Multiple Autonomous Undersea Vehicle (MAUV) project and was adapted for the NASA/NBS Standard Reference Model Telerobot Control System Architecture (NASREM) developed for the space station Flight Telerobotic Servicer The basic building block of RCS-3 is shown in the figure. \n\nThe principal new features introduced in RCS-3 are the World Model and the operator interface. The inclusion of the World Model provides the basis for task planning and for model-based sensory processing. This led to refinement of the task decomposition (TD) modules so that each have a job assigner, and planner and executor for each of the subsystems assigned a job. This corresponds roughly to Saridis' three level control hierarchy.\n\nRCS-4 is developed since the 1990s by the NIST Robot Systems Division. The basic building block is shown in the figure). The principal new feature in RCS-4 is the explicit representation of the Value Judgment (VJ) system. VJ modules provide to the RCS-4 control system the type of functions provided to the biological brain by the limbic system. The VJ modules contain processes that compute cost, benefit, and risk of planned actions, and that place value on objects, materials, territory, situations, events, and outcomes. Value state-variables define what goals are important and what objects or regions should be attended to, attacked, defended, assisted, or otherwise acted upon. Value judgments, or evaluation functions, are an essential part of any form of planning or learning. The application of value judgments to intelligent control systems has been addressed by George Pugh. The structure and function of VJ modules are developed more completely developed in Albus (1991).\n\nRCS-4 also uses the term behavior generation (BG) in place of the RCS-3 term task 5 decomposition (TD). The purpose of this change is to emphasize the degree of autonomous decision making. RCS-4 is designed to address highly autonomous applications in unstructured environments where high bandwidth communications are impossible, such as unmanned vehicles operating on the battlefield, deep undersea, or on distant planets. These applications require autonomous value judgments and sophisticated real-time perceptual capabilities. RCS-3 will continue to be used for less demanding applications, such as manufacturing, construction, or telerobotics for near-space, or shallow undersea operations, where environments are more structured and communication bandwidth to a human interface is less restricted. In these applications, value judgments are often represented implicitly in task planning processes, or in human operator input.\n\nIn the figure, an example of the RCS methodology for designing a control system for autonomous onroad driving under everyday traffic conditions is summarized in six steps. \n\n\nThe result of step 3 is that each organizational unit has for each input command a state-table of ordered production rules, each suitable for execution by an extended finite state automaton (FSA). The sequence of output subcommands required to accomplish the input command is generated by situations (i.e., branching conditions) that cause the FSA to transition from one output subcommand to the next.\n\nBased on the RCS Reference Model Architecture the NIST has developed a Real-time Control System Software Library. This is an archive of free C++, Java and Ada code, scripts, tools, makefiles, and documentation developed to aid programmers of software to be used in real-time control systems, especially those using the Reference Model Architecture for Intelligent Systems Design.\n\n\n"}
{"id": "2657405", "url": "https://en.wikipedia.org/wiki?curid=2657405", "title": "Relational frame theory", "text": "Relational frame theory\n\nRelational frame theory (RFT) is a psychological theory of human language. It was developed originally by Steven C. Hayes of University of Nevada, Reno and has been extended in research notably by Dermot Barnes-Holmes of Ghent University.\n\nRelational frame theory argues that the building block of human language and higher cognition is relating, i.e. the human ability to create bidirectional links between things. It can be contrasted with associative learning, which discusses how animals form links between stimuli in the form of the strength of associations in memory. However, relational frame theory argues that natural human language typically specifies not just the strength of a link between stimuli but also the type of relation as well as the dimension along which they are to be related. For example, a tennis ball is not just associated with an orange, but can be said to be the same shape, but a different colour and not edible. In the preceding sentence, 'same', 'different' and 'not' are cues in the environment that specify the type of relation between the stimuli, and 'shape', 'colour' and 'edible' specify the dimension along which each relation is to be made. Relational frame theory argues that while there are an arbitrary number of types of relations and number of dimensions along which stimuli can be related, the core unit of relating is an essential building block for much of what is commonly referred to as human language or higher cognition.\n\nSeveral hundred studies have explored many testable aspects and implications of the theory such as the emergence of specific frames in childhood, how individual frames can be combined to create verbally complex phenomena such as metaphors and analogies, and how the rigidity or automaticity of relating within certain domains is related to psychopathology. In attempting to describe a fundamental building block of human language and higher cognition, RFT explicitly states that its goal is to provide a general theory of psychology that can provide a bedrock for multiple domains and levels of analysis.\n\nRelational frame theory focuses on how humans learn language (i.e., communication) through interactions with the environment and is based on a philosophical approach referred to as functional contextualism.\n\nRFT is a behavioral account of language and higher cognition. In his 1957 book \"Verbal Behavior\", B.F. Skinner presented an interpretation of language. However, this account was intended to be an interpretation as opposed to an experimental research program, and researchers commonly acknowledge that the research products are somewhat limited in scope. For example, Skinner's behavioral interpretation of language has been useful in some aspects of language training in developmentally disabled children, but it has not led to a robust research program in the range of areas relevant to language and cognition, such as problem-solving, reasoning, metaphor, logic, and so on. RFT advocates are fairly bold in stating that their goal is an experimental behavioral research program in all such areas, and RFT research has indeed emerged in a large number of these areas, including grammar.\n\nIn a review of Skinner's book, linguist Noam Chomsky argued that the generativity of language shows that it cannot simply be learned, that there must be some innate \"language acquisition device\". Many have seen this review as a turning point, when cognitivism took the place of behaviorism as the mainstream in psychology. Behavior analysts generally viewed the criticism as somewhat off point, but it is undeniable that psychology turned its attention elsewhere and the review was very influential in helping to produce the rise of cognitive psychology.\n\nDespite the lack of attention from the mainstream, behavior analysis is alive and growing. Its application has been extended to areas such as language and cognitive training. Behavior analysis has long been extended as well to animal training, business and school settings, as well as hospitals and areas of research.\n\nRFT distinguishes itself from Skinner's work by identifying and defining a particular type of operant conditioning known as \"arbitrarily applicable derived relational responding\" (AADRR). In essence the theory argues that language is not associative but is learned and relational. For example, young children learn relations of coordination between names and objects; followed by relations of difference, opposition, before and after, so on. These are \"frames\" in the sense that once relating of that kind is learned, any event can be related in that way mutually and in combination with other relations, given a cue to do so. This is a learning process that to date appears to occur only in humans possessing a capacity for language: to date relational framing has not yet been shown unambiguously in non-human animals despite many attempts to do so. AADRR is theorized to be a pervasive influence on almost all aspects of human behavior. The theory represents an attempt to provide a more empirically progressive account of complex human behavior while preserving the naturalistic approach of behavior analysis.\n\nApproximately 300 studies have tested RFT ideas. Supportive data exists in the areas needed to show that an action is \"operant\" such as the importance of multiple examples in training derived relational responding, the role of context, and the importance of consequences. Derived relational responding has also been shown to alter other behavioral processes such as classical conditioning, an empirical result that RFT theorists point to in explaining why relational operants modify existing behavioristic interpretations of complex human behavior. Empirical advances have also been made by RFT researchers in the analysis and understanding of such topics as metaphor, perspective taking, and reasoning.\n\nProponents of RFT often indicate the failure to establish a vigorous experimental program in language and cognition as the key reason why behavior analysis fell out of the mainstream of psychology despite its many contributions, and argue that RFT might provide a way forward. The theory is still somewhat controversial within behavioral psychology, however. At the current time the controversy is not primarily empirical since RFT studies publish regularly in mainstream behavioral journals and few empirical studies have yet claimed to contradict RFT findings. Rather the controversy seems to revolve around whether RFT is a positive step forward, especially given that its implications seem to go beyond many existing interpretations and extensions from within this intellectual tradition.\n\nRFT has been argued to be central to the development of the psychotherapeutic tradition known as acceptance and commitment therapy and clinical behavior analysis more generally. Indeed, the psychologist Steven C Hayes was involved with the creation of both acceptance and commitment therapy and RFT, and has credited them as inspirations for one another. However, the extent and exact nature of the interaction between RFT as basic behavioral science and applications such as ACT has been an ongoing point of discussion within the field.\n\nRFT provides conceptual and procedural guidance for enhancing the cognitive and language development capability (through its detailed treatment and analysis of derived relational responding and the transformation of function) of early intensive behavior intervention (EIBI) programs for young children with autism and related disorders. The Promoting the Emergence of Advanced Knowledge (PEAK) Relational Training System is heavily influenced by RFT.\n\nMore recently, RFT has also been proposed as a way to guide discussion of language processes within evolution science, whether within evolutionary biology or evolutionary psychology, toward a more informed understanding of the role of language in shaping human social behavior. The effort at integrating RFT into evolution science has been led by, among others, Steven C. Hayes, a co-developer of RFT, and David Sloan Wilson, an evolutionary biologist at Binghamton University. For example, in 2011, Hayes presented at a seminar at Binghamton, on the topic of \"Symbolic Behavior, Behavioral Psychology, and the Clinical Importance of Evolution Science\", while Wilson likewise presented at a symposium at the annual conference in Parma, Italy, of the Association for Contextual Behavioral Science, the parent organization sponsoring RFT research, on the topic of \"Evolution for Everyone, Including Contextual Psychology\". Hayes, Wilson, and colleagues have recently linked RFT to the concept of a symbotype and an evolutionarily sensible way that relational framing could have developed has been described.\n\n\n"}
{"id": "42045081", "url": "https://en.wikipedia.org/wiki?curid=42045081", "title": "Siu Shih Chang", "text": "Siu Shih Chang\n\nSiu Shih Chang (Xui Shi Zhang) (born 1918) is a Chinese botanist and plant collector. The elm species \"Ulmus changii\" was named for him after he discovered it in 1936.\n\n"}
{"id": "24152467", "url": "https://en.wikipedia.org/wiki?curid=24152467", "title": "Swing timing", "text": "Swing timing\n\nSwing timing is concerned with the inter-relationships between ball sports hitting implements and the related anatomical parts of players, in the motor skill of striking sports balls as used in various games.\n\nMore specifically, swing timing commonly refers to the efficiency of ball hitting or kicking skills by achieving optimum power at instant of ball impact. Furthermore, swing timing also refers to the inter-relationship between optimum power as generated and the intended point of ball contact.\n\nPerfect swing timing is therefore achieved when ball is struck with optimum power, with least energy expenditure, and at contact point that achieves the desired direction and velocity of ball after impact.\n\nOf all the motor skill components essential for expert ball play, swing timing is the most difficult to acquire, and teach, because of the essential millisecond co-ordination of major and minor muscle groups to impart optimal progressive acceleration to the hitting implement until it has reached greatest velocity and optimum angle at moment of ball impact.\n\nArguably the best ball players have superior swing timing to achieve greatest ball impact velocities for greatest speed and/or distance, and a highly tuned muscle memory, or ‘feel’, to more consistently repeat or adjust swing as determined by the changing success requirements which vary from one ‘shot’ to another, and the ability to fine-tune swing velocity and impact point and angle.\n\nThe answer still eluding even top teaching professionals; be it in golf, tennis, baseball, racketball, squash, cricket and even ball-kicking sports including European, American and Australian football, and rugby; is how to more effectively teach correct swing timing, the essence of the ‘perfect swing technique’.\n\nSome common and fairly accurate perceptions of correct Swing Timing include a relaxed posture and muscle groups, which are only called into action in precisely timed intervals, and which relax instantly after ball contact is made; and an apparent ‘80% Only’ effort, which appears to promote the desired relaxed state during the whole swing.\n\nBut the inability to more accurately observe these split-second, highly complex correlations and co-ordination; therefore, more often than not, encourages the teaching of specific and isolated swing components only, which tends to disrupt ‘natural’ swing mechanics and timing rather than enhancing these.\n\nFor this reason, correct swing timing, still remains confined to trial and error techniques, where the naturally gifted player eliminates errors more quickly, and acquires accurate muscle memory faster.\n\nAs visual feedback of actual swing remains elusive, an increasingly popular technique is to focus on correct swing rhythm, which can be more easily internalized, thereby accelerating essential muscle memory. Therefore one of the most effective teaching and practice tools is music, where the rhythm of music and ‘perfect swing’ are identical for a given player. In this method there is a more identifiable correlation of actual swing versus ideal by the degree of synchronization achieved.\n\nOther audio and tactile swing timing feedback devices have already been developed. These devices have proven to be highly effective in teaching ‘near perfect swing timing’ even to youngsters in a natural play learning experience.\n\nIn the existing related literature, these two terms are freely interchanged. Yet it appears that failure to recognize the key difference is another reason for the difficulty in their understanding, and thereby the difficulty in achieving correct swing mechanics, timing and rhythm.\n\nWhereas swing timing is pertinent to the sequence of vital swing components coming into effect for optimum ball contact, Swing rhythm is the tempo of the complete swing, which traditionally includes the backswing, and a vital pause between backswing and final swing towards ball impact.\n\nIn swing timing the emphasis is therefore on the correct sequence of the various elements of the total swing. Whereas in swing rhythm the focus is on the tempo, and relative 'weight' of each component within that tempo of the ideal personal swing.\n\nFocus on swing rhythm, either in teaching or personal practice, is considered preferable. Correct swing rhythm results in faster 'feel' of correct timing mechanics, and reduces interference with that vital process by eliminating risk of undue emphasis on individual components of the total swing. To achieve perfect swing timing in this approach, it only remains to fine-tune this rhythm to ensure that the 'down beat' coincides perfectly with ball impact.\n\nIt is more important to understand this rhythm aspect of swing timing, then to focus on its individual components. And regardless of which sports specific skill, this concept remains constant. First, movement occurs in the most distal of the total swing lever system and progressively moves to each component one increment closer to the final impact point. And this movement sequence is common for tennis, golf, baseball and similar hitting skills. First the knees, which initiate rotation and forward movement of hips, which in turn initiate upper trunk and shoulder rotation, before progressively bringing upper and lower arms into play, followed by wrist and fingers which impart the final control and speed to the club, racket or stick. For ultimate success it is equally important that perfect swing timing occurs in the correct Swing Plane as determined by anticipated ball contact point, intended direction of ball flight, and the desired spin effect to be imparted to ball.\n\nMathematical models can easily demonstrated the effect on racket or clubhead velocity, for each swing component which is subject to exponential acceleration caused by the cumulative effect of all prior components in the total acceleration chain. In simple terminology and concept, this complex action and its effect is comparable to cracking a whip. It is therefore self-evident that if any one component is out of ideal swing synchronization, the resultant detrimental impact will be a similarly exponential increase in loss of power and direction at ball impact.\n\nWhile this is an appropriate model for better understanding of the biomechanics and kinesiology involved in perfect swing timing; it is not an ideal basis for teaching same. It is simply too complex for anyone to consciously synchronize all the component in this chain, especially in only the fraction of a second available to complete this final swing sequence; or indeed observe it or feel each individual component coming into play sequentially. Thus tempo and rhythm, still remain as the key variables in the achievement of correct swing timing.\n\nTherefore any theory of correct swing timing, in practice and teaching, must rely on correct swing rhythm emphasis, including learning, teaching and practice devices that provide instant feedback of correct swing rhythm for every swing.\n\nThe importance of the pause of the hitting implement at the end of the backswing, that is, \"before\" the actual swing aimed at perfect swing timing is commenced; is generally agreed upon, regardless of sport. This Pause is the moment of final deliberation as to when to commence a hitchless and optimum swing. Many swing timing problems can thus be traced back to miscalculations during this critical split-second pause. The continuing 'traditional' emphasis on correct backswing check points, arcs and planes, therefore appears to be seriously misplaced, and simply adds to the overall complexity of Swing Timing.\n\nIt would therefore also appear that only two things are important in any backswing instructions, First, that the racket or clubhead pauses in the correct place on the desired swing arc and plane as determined by the intended 'shot', and secondly, that the racket or club face has assumed the optimal angle relative to swing arc and plane, so as to require minimal further adjustment in the Final Swing Sequence, and thereby eliminating another adjustment variable and potential cause of error.\n\nThe achievement of correct swing timing therefore remains over-complicated by breaking down a swing that needs to be continuous and rhythmic, into its sub-components. Instead it can and should be 'discovered' much more easily through an emphasis on correct swing rhythm in ways already described.\n"}
{"id": "48951229", "url": "https://en.wikipedia.org/wiki?curid=48951229", "title": "Teknisk Tidskrift", "text": "Teknisk Tidskrift\n\nTeknisk Tidskrift (1871-March 1872 Illustrerad Teknisk Tidning), was founded in 1871 by the Swedish marine engineer Wilhelm Hoffstedt (1841–1907). The forerunner to \"Ny Teknik\", it has since its establishment been considered one of the leading journals in Sweden for the publication of findings in technology and engineering.\n\nThe journal was divided into a general part and various specialized sections on mechanical engineering, electrical engineering, chemistry and mining engineering (including metallurgy), shipbuilding, architecture and civil engineering. Specialist sections representing the various departments of the Swedish Teknologföreningen reflected the educational structure of the Royal Institute of Technology. A supplement on \"Arkitektur och dekorativ konst\" (architecture and decorative arts, 1901-1922) developed into an independent journal \"Arkitektur\". \"Teknisk Tidskrift\"'s general edition was published once a week and the specialized editions once a month. In the autumn of 1967, the name of the weekly edition was changed to \"Ny Teknik\" which had a more popular scientific format.\n\nIn 1967, the publishing house \"Teknisk Tidskrifts förlag\" changed its name to \"Ingenjörsförlaget\" and in 1990 to \"E + T Förlag\" (\"Ekonomi & Teknik Förlag AB)\", after a merger with \"Affärsvärlden\". In October 2005, \"E + T Förlag\" was sold to the Finnish Talentum Oy, publisher of the business newspaper \"Talouselämä\".\n\n"}
