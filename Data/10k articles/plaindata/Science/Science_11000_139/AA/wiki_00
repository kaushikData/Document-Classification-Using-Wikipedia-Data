{"id": "51500671", "url": "https://en.wikipedia.org/wiki?curid=51500671", "title": "Abdulaziz Karimov", "text": "Abdulaziz Karimov\n\nhttps://en.wikipedia.org/wiki/International_Biographical_Centre\n\nAbdulaziz Karimov \"()\" (born January 11, 1944) is an Uzbek researcher in the field of semiconductors. He is the author of more than 40 patents and inventions in semiconductor optoelectronics. Doctor of Science Certificate for his research “Physical phenomena in GaAs-structures with thin quasiperiodic transition”. Professor in the field of Physics of semiconductors and dielectrics.\nAbdulaziz Karimov Vakhitovich was born in Tashkent, Uzbekistan in 1944. He was brought up in the family of a serviceman. From 1951 to 1962, he studied at Maxim Gorky school no. 90 in Tashkent. He graduated from 1964 to 1970 the Tashkent Institute of Education, Faculty of Mathematics and Physics, a principal research institute of the Academy of Sciences of Uzbekistan with a degree of Doctor of physical and mathematical sciences. From 1962 to 1964 he used to be an employee of the Tashkent Cable Factory. From 1964 to 1967 he served in the army. Having finished his army service, he enrolled as a senior engineer in Tashkent State University in 1967-1968. Beginning from 1968 and in the meantime he is an employee of the Physical-Technical Institute of the Academy of Sciences of Uzbekistan. In the period from 1968 to 1971 he used to be a junior researcher, from 1971 to 1986 as a senior researcher, and from 1986 to present times he is a leading scientist of the Institute.\n\nHis work is the study of physical phenomena and processes in epitaxial multilayer structures and the development of their physical foundations. The scientific activity of Dr. A.V. Karimov is closely related to the investigation of physical phenomena and processes in epitaxial multilayer structures and development of their physical basics as well as creation of a novel class of semiconductor devices. In 1980 Dr. A.V. Karimov was granted his PhD Certifictae for his research titled “Development of physical basics of technology of production of GaAs-structures for field transistors and investigation of their thermal and photoelectrical properties”. His research was primarily devoted to the problems of obtaining the semiconductor epitaxial layers and properties of semiconductor structures on their basis. In 1991 he was granted the title of a senior scientific researcher. His scientific activity at the Physical-Technical Institute is connected to studying the physical phenomena and processes that take place in epitaxial multilayer structures and development of their physical basics, creation of a novel class of samples.\n\nIn 1995 Doctor A.V. Karimov was granted his Doctor of Science Certificate for his research “Physical phenomena in GaAs-structures with thin quasiperiodic transition”. In 2006 he received the title of professor in the field of Physics of semiconductors and dielectrics.\n\nHe has published more than 450 scientific articles, two monograph and one academic textbook and invented - author more than 40 devices. Under his leadership, the physical basis of technologies have been developed, as well as the original design of multi-barrier photodiode structures with internal photoelectric amplifications. Invention three-barrier photodiode Karimov.\n\nHe discovered that double-barrier rAlGaInAs-n GaAs-Au-injection structures and field effects are the basis of injection-field photodiodes with an operating voltage spectral range of A.\n\nHis research results have been published in \"Technical Physics Letters\", \"Radioelectronics and communicatios systems,\" \"Journal of Semiconductor Physics, Quantum Electronics, Optoelectronics and\" \"Journal of Engineering Physics\").\n\nNoteworthy results include the spectral characteristics of the double-barrier structure with a corrugated surface of the photodetector as well as sensitivity to the impurity region of the spectrum.\n\nIn comparison with the classical single-barrier photodiode, he obtained high values of photosensitivity from 5 to 15 A / W, which is one - one and a half orders of magnitude higher than (0.5-1 A / W) in the known analogues.\n\nDr. A.V. Karimov regularly participates at various part in high-profile international scientific conferences, including lectures and latest publicationsin in Prague, 13–17 October 2002, Berlin, 23–25 June 2003, Paris, 7–11 June 2004, Kiev and Odessa 2004-2007 at international conferences physical processes in phototransformator, photodiodes, phototransistors, and the problems of the physical processes in the pn-junctions. He was a member of the organizing committee of the conference held in Baku and Odessa (2004-2007 biennium) on Information and electronic technologies.\n\nDr. A.V. Karimov was an active participant as a research manager of international projects UZB-31 “Development of solar energy photoconverters based on novel structures: ion-beam modified and diamond-like layers of polycrystalline silicon” (01.04.2000-31.06.2002), and UZB-56(J) “Development and investigation of micro-layer photoconverters with InGaAs/GaAs and AlGaAs/GaAs heteroepitaxial junctions” (01.10.2002-31.12.2004) jointly ith Ukrainian Science and Technology Center.\n\nThe physical basics of the technology and original designs of multiple barrier photodiode structures with internal photoelectrical amplification were developed under his guidance in Uzbekistan in the early 90th [Karimov A.V. Three-barrier photodiode of A.V. Karimov. USSR Patent. ٭1676399. 08.05., 1991. Karimov A.V. Three-barrier photodiode. Uzb Patent # 933. 15.04., 1994., Academy of Sciences reports of Uzbek SSR, # 12, 1990, pp. 17–18]. Multibarrier structures replenish from year to year by new constructions with extended functional possibilities. Discovered in two-barrier p+ AlGaInAs-n0 GaAs-Au-structures injection and field effects make up the basis of injection-field photodiode with spectral range switched from the working voltage [A.V. Karimov, D.M. Yodgorova. An injection-type field-emission photodiode// Radioelectronics and communication systems, #2, 2006, pp. 55–59]. D.M. Yodgorova, A.V. Karimov, F.A. Giyasova, D.A. Karimova. Research of photo-voltaic effect in two-base Ag-N AlGaAs-n GaAs-n GaInAs-Au-structure with various thickness of base area. //Semiconductor Physics, Quantum Electronics and Optoelectronics. #1. 2008. V. 11. рр. 75-79.His multiple scientific results were published in various journals (Electronics, Instruments and experimental technologies, Semiconductors, Technical Physics Letters, Radioelectronics and communication systems, Journal of engineering physics and thermophysics) worldwide. The results of investigation of spectral characteristics of two-barrier structures with corrugated surface are of special interest [A.V. Karimov, D.M. Yodgorova, E.N. Yakubov. Research of structures with corrugated photoreceiving surface// Journal of Semiconductor Physics and Quantum Electronics and Optoelectronics. 2004, Vol. 7, #4, pp. 378–382]. Compared to the classical one-barrier photodiode structures, Dr. A.V. Karimov obtained high value of photosensitivity ranging from 5 to 15 A/V, which is by 1-1.5 fold higher than that (0.5-1 A/V) in the existing samples.\n\nDr. A.V. Karimov, as a leading expert and professional in his field, he is listed in the Biographical Collection 10th Anniversary Edition of Who's Who in Science and Engineering/ Marquis Who's Who / Who's Who in Science and Engineering, and also in International list of the International Biographical Centre Melrose Press Ltd (registered in Great Britain under #965274 with an office at Saint Thomas Place, ELY, Cambridgeshire, CB7 4GG, Great Britain). He was admitted as an International scientist for the year 2008.\n\nAt present Dr. A.V. Karimov together with his team works on creating a new generation of photodetector structures for receiving and processing information (a novel generation of photo-receiver structures for information processing and storage).\n\n"}
{"id": "35272599", "url": "https://en.wikipedia.org/wiki?curid=35272599", "title": "Abhayavapi", "text": "Abhayavapi\n\nAbhayavapi (Sinhalese: ) or commonly Abhaya Wewa (Sinhalese: ) is a reservoir built by King Pandukabhaya who ruled in Anuradhapura from 437 BC to 367 BC, after constructing the city. This is now popularly known as \"Basawakkulama\" (Sinhalese: ) tank which is not the original name.\n\nArea is ; length of the \"Waw Kandiya\" (Sinhalese: English: embankment) is and height is . Width of the top of the embankment is to .\n\nBuilt inside the ancient Anuradhapura, it supplied water to then city population.\n\n"}
{"id": "43482107", "url": "https://en.wikipedia.org/wiki?curid=43482107", "title": "Advanced metrics", "text": "Advanced metrics\n\nAdvanced Metrics is the term for the empirical analysis of sports, particularly statistics that measure in-game productivity and efficiency. Advanced metrics were first employed in baseball by Bill James, a pioneer in the field who is considered the father and public face of the practice.\n\nThe basic principles of advanced metrics were outlined in \"The Sabermetric Manifesto\" by David Grabiner (1994). In the piece, Grabiner explains that Bill James defined sabermetrics, the first advanced metric, as the search for objective knowledge about sports.\n\nTo glean objective knowledge, advanced metrics practitioners gather and synthesize thousands of situational game data points from a wide variety of sources beyond high level box score information. A large volume of situational data is then distilled into sophisticated statistics that helps sports enthusiasts better understand, compare, and appreciate the on-field performances by athletes. Advanced metrics provide more objective answers to questions such as \"which player on the Red Sox contributed the most to the team's offense?\" or \"who is the best wing player in the NBA?\" or \"how many touchdowns is Dez Bryant likely to score for the Cowboys?\". By removing the subjective observations of sports media members and the emotional opinions of fans, advanced metrics offer a more clinical, rational prism from which to enjoy sporting events and consume sports media coverage.\n\nAdvanced metrics were first posited in the mid-1900s, and Earnshaw Cook was one its earliest practitioners. Cook's research for his 1964 work, \"Percentage Baseball\" was the first publication siting advanced metrics to garner national media attention. Despite the competitive advantage offered by advanced metrics, the practice was unanimously dismissed by major sports organizations for most of the twentieth century.\n\nIn the late 1970s, Bill James helped bring SABR, which stands for the Society for American Baseball Research, and the empirical analysis of athletic performance, to national prominence. The analytical perspective on sports championed by James and SABR developed a wider following after Sports Illustrated featured James in the article \"He Does It By The Numbers\" by Daniel Okrent (1981).\n\nFormer Major League Baseball second baseman Davey Johnson was the first known member of a major sports organization to advocate for the use of advanced metrics. During his time with the Baltimore Orioles, he used an IBM System/360 to write a FORTRAN baseball computer simulation to determine the team's optimal starting lineup. When he proposed his findings to Orioles manager Earl Weaver, Johnson's proposal was summarily dismissed. A decade later, after becoming the New York Mets' manager in 1984, Johnson tasked a team employee with writing a dBASE II application to run sophisticated statistical models in order to better understand the capabilities and tendencies of the team's opponents. At the close of the twentieth century, advanced metrics had gained significant acceptance by the management of many Major League Baseball clubs, notably the Oakland A's, Boston Red Sox and Cleveland Indians.\n\nThe adoption of advanced metrics by professional baseball franchises has been largely outpaced by sports fans and sports media. Bill James' first published his \"Baseball Abstract\" in the early 1980s. In 1996, Baseball Prospectus sought to build upon Bill James' work when it launched the website \"BaseballProspectus.com\" in order to present sabermetric research and related findings as well as publish advanced metrics such as EqA, the Davenport Translations (DT's), and VORP. Today, Baseball Prospectus has grown into a multi-channel sports media organization employing a team of statisticians and writers who publish \"New York Times Best Selling\" books and host weekly radio shows and podcasts.\n\nNorth Carolina, under coach Frank McGuire, was the first known basketball organization to utilized advanced possession metrics to gain a competitive advantage. Since then, advanced metrics enthusiasts in basketball have borrowed aspects of Bill James' philosophy in order to create weighted statistics that measure each player and each team's on-court efficiency. Most basketball-specific advanced metrics feature a per-minute measurement to ensure that a player's incremental team contributions are measured irrespective of usage volume.\n\nBeginning in the 1990s, statistician Dean Oliver and ESPN sports writer John Hollinger first popularized the use of advanced metrics in basketball. Oliver's book \"Basketball On Paper\" and Hollinger's \"Pro Basketball Forecast\" are credited with the advancement of basketball's version of sabermetrics, APBRmetrics. Major sports media proponents, such as Hollinger, have helped basketball evolve more quickly from rudimentary statistics to advanced metrics than some other major American sports.\n\nHouston Rockets' Daryl Morey was the first NBA general manager to implement advanced metrics as a key aspect of player evaluation. In the years that followed Morey's hiring, the NBA moved quickly to adopt advanced metrics-based player evaluation practices. In 2012, John Holliger left ESPN to become VP of Basketball Operations for the Memphis Grizzlies.\n\nBeyond professional basketball front offices, major sports media websites such as \"basketballreference.com\" and \"hoopsdata.com\" are now dedicated to the collection, synthesis, and dissemination of advanced metrics to pro and college basketball organizations, sports media members, and fans.\n\nThe adoption of advanced metrics by college and professional football organizations has been a gradual evolution. While the sabermetrics revolution in baseball has already inspired a bestselling book, \"Moneyball\" by Michael Lewis, which also became a critically acclaimed film, the advanced metrics movement in football has had no such prominence.\n\nIn 2003, the advanced metrics-focused website \"FootballOutsiders.com\" pioneered football's first comprehensive advanced metric, DVOA (defense-adjusted value over average), which compares a player's success on each play to the league average based on a number of variables including down, distance, location on field, current score gap, quarter, and strength of opponent. Football Outsiders' work has since been widely cited by analytical members the sports media establishment. A few years later, Pro Football Focus launched a comprehensive statistical database, which soon featured a sophisticated player grading system. Advanced Football Analytics (originally Advanced NFL Stats) has its EPA (expected points added) and WPA (win probability added) for NFL players.\n\nGrantland lead football writer Bill Barnwell created the first advanced metrics focused on predicting the future performance of an individual player, the Speed Score, which he referenced in a piece written for \"Pro Football Prospectus\". After analyzing data pertaining to running back success, Barnwell discovered that the most successful running backs at the NFL level were both fast and heavy, therefore, Speed Score weights 40-yard dash times by assigning a premium to bigger, often stronger, running backs.\n\nThe fantasy football community has been one of the driving forces in the evolution of advanced metrics in American football. Fantasy sports writer, and author of \"How To Think Like A Fantasy Football Winner\", C.D. Carter is a leading advocate for the use of advanced metrics in fantasy football analysis. He and peers at XN Sports, NumberFire, and the long-form fantasy football analysis site, Rotoviz.com, have established an informal subculture of fantasy football sports writers who refer to themselves as \"degens.\" The degen movement is responsible for the creation of numerous American football efficiency metrics that better explain past football performances and attempt to predict future player production. Height-adjusted Speed Score, College Dominator Rating, Target Premium, Catch Radius, Net Expected Points (NEP), and Production Premium were recently created and disseminated by degen writers and mathematicians. Building on the work of these writers, sites such as PlayerProfiler.com distill a wide variety of established advanced metrics into a single player snapshot designed to be palatable to the casual sports fan.\n\n\n\n\n"}
{"id": "44311039", "url": "https://en.wikipedia.org/wiki?curid=44311039", "title": "Air pollution episode", "text": "Air pollution episode\n\nAn air pollution episode is an unusual combination of emissions and meteorology that gives rise to high levels of air pollution over a large area. Examples of air pollution episodes include:\n\nSmog, Haze, Ozone, PM, Air Pollution, Inversion\n"}
{"id": "697484", "url": "https://en.wikipedia.org/wiki?curid=697484", "title": "Andrew Wilkie", "text": "Andrew Wilkie\n\nAndrew Damien Wilkie (born 8 November 1961 in Tamworth, New South Wales, Australia) is an Australian politician and independent federal member for Denison. Before entering politics Wilkie was an infantry officer in the Australian Army.\n\nWilkie served with the Australian Army from 1980 to 2004. An officer with the Royal Australian Infantry Corps who had earlier commanded a company of the 6th Battalion, Royal Australian Regiment, at the time of his entry to public life Wilkie was posted to Australia's Office of National Assessments as an intelligence analyst. In 2003, in the lead-up to the Iraq War, he resigned from his position at ONA because he feared the humanitarian consequences of invasion, such as Saddam Hussein using his weapons of mass destruction or assisting terrorists. Following his resignation he said: Iraq's \"weapons of mass destruction program is very disjointed and contained by the regime that's been in place since the last Gulf War. And there is no hard intelligence linking the Iraqi regime to al-Qaeda in any substantial or worrisome way.\" He opposed Australia's contribution to the 2003 invasion of Iraq under the Howard government. Wilkie later argued the Iraq War was based on a \"lie\".\n\nWilkie has been active in politics since 2003. He was a Greens candidate for the federal Division of Bennelong in the 2004 federal election and for the Senate in Tasmania at the 2007 federal election. In 2010 he stood as an independent candidate for the state seat of Denison at the Tasmanian state election, narrowly missing out on the final vacancy. Later in the year, again as an independent candidate, he ran for the federal seat of Denison at the 2010 federal election and won, finishing third on the primary vote but winning the seat after the distribution of preferences. Wilkie finished first on the primary vote at the 2013 federal election and increased his margin.\n\nWilkie trained at the Royal Military College, Duntroon and graduated in 1984. He joined the Young Liberals while a cadet. He also studied at the University of New South Wales, and holds a Bachelor of Arts degree, a Graduate Diploma of Management, and a Graduate Diploma of Defence Studies. After graduation and being stationed in Brisbane, he joined the Liberal Party before allowing his membership to lapse. His military career spanned 1980–2001 and he rose to the rank of lieutenant colonel. He was seconded to the ONA, an Australian intelligence agency, from 1999 until late 2000. After a stint with US defence company Raytheon, Wilkie returned to the ONA shortly after the 11 September attacks.\n\nWilkie joined the Army in 1980, and was first stationed in Brisbane, Queensland. He served in the Royal Australian Infantry Corps and achieved the rank of Lieutenant Colonel. He was discharged in 2001.\n\nIn the aftermath of the September 11 Terror Attacks, the United States called upon Australia to assist in enforcing the 1991 Gulf War peace treaty which had been repeatedly breached by Saddam Hussein's Iraq. Iraq failed to comply with demands to allow unfettered arms inspections, and the Howard Government elected to send forces to support the 2003 US invasion of Iraq. While the Government was considering the case for war, Wilkie was asked to report on humanitarian considerations. According to a leaked report published in the \"Herald Sun\", in December 2002 Wilkie had submitted an ONA report on the humanitarian implications of war in Iraq to the Government. In it he cautioned against unpredictable but potentially serious humanitarian consequences of war against Saddam, such as the use of weapons of mass destruction against civilians.\n\nIn response to widespread opposition to the war, Wilkie gave extensive television interviews and accepted numerous offers of public speaking engagements. He subsequently gave evidence to official British and Australian inquiries into the government's case for involvement in the Iraq war. In 2004, Wilkie published \"Axis of Deceit\", an account of the reasons for his decision and its results. He describes his views on the nature of intelligence agencies and the analyst's work, the history of the Iraq war, the untruths of politicians and the attempts to suppress the truth.\n\nFollowing Britain's 2016 Chilcot Report which criticised the Blair Government's prosecution of the war, Wilkie attributed all terror attacks against Australians since the Iraq invasion to the nation's involvement in the Iraq War. Wilkie said that Howard, Bush and Blair should be brought before an international court, and called for Australia to hold another inquiry into the war. Howard rejected Wilkie's proposition and called him \"irrational\", telling the media: \"Andrew Wilkie said the Iraq invasion was responsible for the Bali attack of 2005. What about the Bali attack of 2002? And he blamed it [the Iraq War] on the Lindt Cafe siege. I mean, this is irrational.\"\n\nOn 11 March 2003, Wilkie resigned from the ONA, asserting that while Iraq likely did possess weapons of mass destruction, its program in this area was contained, that international sanctions were having an effect, and therefore an invasion was premature and also reckless in potentially provoking Saddam Hussein to use those weapons and possibly even begin supporting terrorism. He told the ABC: \"I think that invading Iraq at this time would be wrong. For a start, Iraq does not pose a security threat to any other country at this point in time. Its military is very weak, it's a fraction of the size of the military at the time of the invasion of Kuwait. Its weapons of mass destruction program is very disjointed and contained by the regime that's been in place since the last Gulf War. And there is no hard intelligence linking the Iraqi regime to al-Qaeda in any substantial or worrisome way.\" Wilkie later told the press that in the lead up to his resignation he had increasingly encountered ethical conflict between his duty as an intelligence officer and his \"respect for the truth\". During his later political career, Wilkie said that the notion that Iraq had weapons of mass destruction and co-operated with terrorists had been \"a lie\" and denounced other politicians who had made the claim.\n\nWilkie was a member of the Australian Greens by 2004 and stood unsuccessfully on the Tasmanian Senate ticket in 2007. He resigned from the party in 2008, criticising it for a lack of professionalism.\n\nWilkie stood as the Australian Greens candidate for Bennelong against John Howard in the Australian House of Representatives in the 2004 federal election. He was a supporter of the 'Not happy John!' campaign which ran during the election campaign. Polling 16 per cent of the primary vote, Wilkie achieved the fifth-highest Green vote percentage across the nation. This result was a considerable increase from the Greens' previous (2001) election figure of 5%. Although Wilkie's vote was nowhere near enough to win the seat, there was a swing of 3.18% against John Howard, the sitting Liberal Party member and prime minister, who achieved a primary vote of 49.89%, which resulted in the seat being decided on preferences. The seat reverted to a margin of 5% in 2007, but Green votes were amongst the preferences that saw Labor's Maxine McKew defeat Howard.\n\nHe was nominated as the Greens' second Tasmanian candidate for the Australian Senate at the 2007 federal election, behind the Greens federal leader, Bob Brown.\n\nWilkie stood as an independent candidate in the state Division of Denison, based around central Hobart, in the 2010 Tasmanian state election. He won 8.44 per cent of first preference votes, and was beaten by 315 votes by Liberal candidate Elise Archer after distribution of preferences.\n\nWilkie stood as an independent for the federal Division of Denison, which has the same boundaries as the state division, in the 2010 federal election and won more than 20 per cent of the primary vote. The Australian Broadcasting Corporation declared Wilkie the winner on election night, predicting that Wilkie would be vaulted into second place on Green preferences and ultimately take the seat on Liberal preferences. On the third count, he picked up enough Green preferences to put him in second place, ahead of the Liberal candidate. On the fourth count, more than 79 percent of the Liberal candidate's preferences flowed to Wilkie, allowing him to win the seat with just over 51 per cent of the two-candidate-preferred vote. Reportedly, Wilkie benefited from a lacklustre campaign by Labor's candidate, Jonathan Jackson, the son of former longtime state Labor minister Judy Jackson; Labor lost almost a quarter of its primary vote from 2007, and Labor theoretically tallied a two-party vote of more than 65 percent.\n\nFollowing the election, he declared that he would back the Australian Labor Party minority government, in return for the Gillard government committing $340 million to the Royal Hobart Hospital and a commitment to reduce problem gambling. In contrast the Coalition offered A$1 billion in funding for the same hospital in their offer to Wilkie, which was perceived by Wilkie as \"almost reckless\". Wilkie described this as being part of the evidence that Labor would better be able to offer a more stable, competent and ethical government than the Coalition. The agreement to support the government only extended to issues of supply and no confidence motions.\n\nWilkie was unexpectedly admitted to hospital on 12 November 2010 to have his gall bladder removed. This did not interfere with his ability to attend Parliamentary sittings and he was present at the debate on same-sex marriage on 15 November, where he seconded the motion raised by Greens member Adam Bandt.\n\nIn April 2011 during push for gambling reform initiated by Wilkie, News Limited media reported allegations by a former Duntroon army cadet that in 1983 Wilkie had forced junior cadets to salute to Adolf Hitler on the 50th anniversary of the latter's rise to power. In response, Wilkie said he had been \"involved in bastardisation of teenage army cadets\" at Duntroon during the 1980s and apologised for this \"inappropriate behaviour\" but could not recall the specific incident alleged. With regard to the allegation and its publication, he accused pro-Pokies advocates of running a smear campaign against him.\n\nOn 21 January 2012 Wilkie announced that he was withdrawing his support for the Labor government after it broke the agreement he had signed with Julia Gillard to implement mandatory pre-commitment for all poker machines by 2014. He stated that he would support the government's alternative plan to trial pre-commitment in the ACT and require that pre-commitment technology be installed in all poker machines built from 2013, but that this fell short of what he had been promised in return for supporting the government. Gillard and Minister for Families, Housing, Community Services and Indigenous Affairs Jenny Macklin argued that there was not enough support in the House of Representatives for Wilkie's preferred option for it to be passed, and that they had been advised it was technically infeasible to implement mandatory commitment within the time frame he had specified. In making his announcement, Wilkie stated that he would only support motions of no confidence against the government \"in the event of serious misconduct\" and would \"consider budget measures on their merits\".\n\n\nWilkie was reelected in the 2013 federal election, gaining a swing of 15 points to increase his majority to 65 percent.\n\nIn October 2014, Wilkie wrote to the International Criminal Court, seeking to prosecute Prime Minister Tony Abbott and the 19 members of his cabinet for crimes against humanity, with particular concerns relating to the treatment of asylum seekers.\n\nWilkie made the removal of poker machines his primary campaign issue in the 2010 Tasmanian state election. He strongly opposed to the Gunns pulp mill in the Tamar Valley. Wilkie is a supporter of voluntary euthanasia, provided that there are safeguards in place, he is also in favour of same-sex marriage and access to abortion. He supported a National Broadband Network and opposed the Howard Government's WorkChoices industrial relations reforms.\n\nDuring Wilkie's maiden speech to federal parliament on 30 September 2010, he called for withdrawal of Australian troops from Afghanistan. He said Australia should be more willing to say \"no\" more often to the United States. He said that there could be no hope for peace in Afghanistan until foreign troops are withdrawn: \"No-one should be fooled by the Australian Government's periodic efforts to tinker around the edges with Australia's commitment to Afghanistan\" and that \"The reality is that the best plan the Australian Government can come up with so far is simply to continue to support whatever the US Government comes up with and that alone is no plan—it's just reinforcing failure.\"\n\nWilkie's comments came amid opposition calls for more support for Australia's troops in Afghanistan. During his speech Wilkie also canvassed his push for legislation to protect whistleblowers, measures to tackle problem gambling and a more humane approach to asylum seekers.\n\nIn March 2011 he called Liberal MPs Cory Bernardi and Scott Morrison \"a disgrace to high office\" calling on Tony Abbott to sack them both and referring to endemic racism in the Liberal party.\n\nWilkie campaigned heavily against poker machines (colloquially \"pokies\") at the 2010 federal election and immediately began forging ties with independent anti-pokies Senator Nick Xenophon. Wilkie claimed that problem gamblers in Australia lose $5 billion each year on pokies. The Labor government gave two commitments regarding pokies in exchange for Wilkie's support. The first was mandatory \"pre-commitment\" technology, which required a better to commit how much they were willing to bet before starting. The second commitment was to introduce $1 maximum bet per spin machines that would not require pre-commitment. Wilkie argued these $1 maximum machines would be safer. The Abbott Coalition opposed the plans, with Abbott saying \"it is not Liberal Party policy\" and it will be \"expensive and ineffective\". According to polling, Wilkie's proposals were supported by a clear majority of voters across the spectrum. Wilkie and Xenophon argued that \"$12 billion a year is lost on the pokies. 100,000 Australians are problem gamblers and an additional 200,000 are significantly at risk of developing a full-blown addiction\", and that the legislation is necessary to \"[help] those who sometimes lose up to $1200 an hour on the pokies.\" The Labor government withdrew their support for Wilkies's plan when their strength in parliament improved through a change of Speaker.\n\nThe plan came under sustained attack from clubs, hotels and other businesses which financially benefit from pokies. Xenophon responded by accusing them of misrepresenting plans and creating hype around the issue. Strategy papers erroneously placed on the Clubs Queensland public website seem to indicate that clubs are deliberately and purposefully exaggerating the impact that the pre-commitment reforms will have on their services. The same papers outlined some strategies that the clubs could use to exploit loopholes in the proposed reform.\n\nThe National Rugby League (NRL) aligned themselves with the campaign in opposing the pre-commitment plans, as did some prominent Australian Football League (AFL) people. Commentators from the Nine Network gave planned political arguments without disclosure during commentary of a Semi-final NRL game, prompting the Australian Communications and Media Authority (ACMA) to investigate, stating \"Channel Nine broadcast political material without adequately identifying it as such during the NRL first preliminary final\". One of the accused commentators stated that the remarks were a \"directive from up top that it be read by at least somebody\". Investigations were predicted to take months. Andrew Demetriou, chief executive officer of the AFL, rejected suggestions that the AFL was joining Clubs Australia in their media campaign despite opposition to the plan by Collingwood president Eddie McGuire. Other high-profile club bosses including Jeff Kennett (Hawthorn) and David Smorgon (Bulldogs), and also stated \"The fellow from Clubs Australia, I don't even know his name, but please, stop talking on our behalf, just shut up, that'd be a good help\". Activist group GetUp! attempted to counter the anti-pre-commitment campaign by running political commercials during the NRL grand final but all three major commercial television stations refused to air more of them.\n\nWilkie married Kate Burton in 2004 and had two daughters. However, they separated in 2012 and divorced the following year, with Wilkie citing stress from the 2010–2013 hung parliament as the main cause of the breakdown.\n\nWilkie was married to a fellow army officer, Simone Wilkie (née Burt), from 1991 to 2003.\n\n"}
{"id": "21210612", "url": "https://en.wikipedia.org/wiki?curid=21210612", "title": "Austrophile", "text": "Austrophile\n\nAn Austrophile is somebody who is fond of Austrian culture and Austria in general but not born there. Historically it could be applied to the wider Austrian Empire, but since 1918, it has applied to the more limited boundaries of the modern nation-state of Austria. It was later sometimes taken as part of a wider Germanophile attitude and generally linked to the admiration of the Germanic culture of the German-speaking world or countries, mainly Germany, Austria, Switzerland and Liechtenstein.\n\nThe term ”austrophile” carried different meanings throughout history. The term was used to refer to 19th Century Austrian nationalist societies that were trying to resist the strong cultural influence that the German Empire carried in the Hapsburg Empire. \n\nAdditionally, the term was also used to describe citizens of the Austrian Empire who were not ethnically Austrian, but were strong supporters of the Austrian control over their native region. Similarly, during the War of the Spanish Succession, supporters of the Hapsburg House and its pretender to the throne, Archduke Charles, were known as Austrophiles.\n\nIn Britain, during the 18th century, there were a number of prominent Austrophiles, including Prime Minister Thomas Pelham-Holles, 1st Duke of Newcastle-upon-Tyne. Austrophiles sought an alliance with Austria against France. Opposed to the Francophiles, who saw French dominance in Europe as inevitable both culturally and militarily, they obtained the Anglo-Austrian Alliance.The movement led to British support for the Austrian Empress Maria Theresa during the Austrian War of Succession. They were opposed by the Austrophobes, who tried to draw attention to Austria's perceived autocracy and suppression of Protestant minorities.\n"}
{"id": "223063", "url": "https://en.wikipedia.org/wiki?curid=223063", "title": "Benchmarking", "text": "Benchmarking\n\nBenchmarking is comparing ones business processes and performance metrics to industry bests and best practices from other companies. In project management benchmarking can also support the selection, planning and delivery of projects. Dimensions typically measured are quality, time and cost. In the process of best practice benchmarking, management identifies the best firms in their industry, or in another industry where similar processes exist, and compares the results and processes of those studied (the \"targets\") to one's own results and processes. In this way, they learn how well the targets perform and, more importantly, the business processes that explain why these firms are successful. According to National Council on Measurement in Education, benchmark assessments are short assessments used by teachers at various times throughout the school year to monitor student progress in some area of the school curriculum. These also are known as interim assessments.\n\nBenchmarking is used to measure performance using a specific indicator (cost per unit of measure, productivity per unit of measure, cycle time of x per unit of measure or defects per unit of measure) resulting in a metric of performance that is then compared to others.\n\nAlso referred to as \"best practice benchmarking\" or \"process benchmarking\", this process is used in management which particularly shows VEMR strategic management, in which organizations evaluate various aspects of their processes in relation to best practice companies' processes, usually within a peer group defined for the purposes of comparison. This then allows organizations to develop plans on how to make improvements or adapt specific best practices, usually with the aim of increasing some aspect of performance. Benchmarking may be a one-off event, but is often treated as a continuous process in which organizations continually seek to improve their practices.\n\nThe term benchmark, originates from the history of guns and ammunition, and with the same aim as for the business term; comparison and improved performance. The introduction of gunpowder arms replaced the bow and arrow from the archer, the soldier who used the bow. The archer now had to adapt to the new situation, and learn to handle the gun. The new weapon left only a mark on the target, where the arrow used to be visible, and with the bow gone, the soldiers title changed to marksman, the man who put the mark. The gun was improved already in the early beginning, with rifling of the barrel, and the rifle was born. With the industrialization of the weapon-industry in the mid-1800's, The mass production of ammunition as a cartridge replaced the manual loading of black-powder and bullet into the gun. Now, with standardized production of both the high-precision rifle, as well as the cartridge, the marksman was now the uncertain variable, and with different qualities and specifications on both rifle as well as ammunition, there was a need for a method of finding the best combination. The rifled weapon was fixed in a bench, making it possible to fire several identical shots at a target to measure the spread. This is benchmarking, as it is still done today, both at the gun-factory, the ammunition-factory, as well as for a lot of sportsmen who look for the best ammunition to bring to competition. \n\nBenchmarking is most used to measure performance using a specific indicator (cost per unit of measure, productivity per unit of measure, cycle time of x per unit of measure or defects per unit of measure) resulting in a metric of performance that is then compared to others. In 1994, one of the first technical journal named \"Benchmarking: An International Journal\" was published.\n\nIn 2008, a comprehensive survey on benchmarking was commissioned by The Global Benchmarking Network, a network of benchmarking centres representing 22 countries.\n\n\nThere is no single benchmarking process that has been universally adopted. The wide appeal and acceptance of benchmarking has led to the emergence of benchmarking methodologies. One seminal book is Boxwell's \"Benchmarking for Competitive Advantage\" (1994). The first book on benchmarking, written and published by Kaiser Associates, is a practical guide and offers a seven-step approach. Robert Camp (who wrote one of the earliest books on benchmarking in 1989) developed a 12-stage approach to benchmarking.\n\nThe 12 stage methodology consists of:\n\nThe following is an example of a typical benchmarking methodology:\n\n\nThe three main types of costs in benchmarking are:\n\nThe cost of benchmarking can substantially be reduced through utilizing the many internet resources that have sprung up over the last few years. These aim to capture benchmarks and best practices from organizations, business sectors and countries to make the benchmarking process much quicker and cheaper.\n\nThe technique initially used to compare existing corporate strategies with a view to achieving the best possible performance in new situations (see above), has recently been extended to the comparison of technical products. This process is usually referred to as \"technical benchmarking\" or \"product benchmarking\". Its use is well-developed within the automotive industry (\"automotive benchmarking\"), where it is vital to design products that match precise user expectations, at minimal cost, by applying the best technologies available worldwide. Data is obtained by fully disassembling existing cars and their systems. Such analyses were initially carried out in-house by car makers and their suppliers. However, as these analyses are expensive, they are increasingly being outsourced to companies who specialize in this area. Outsourcing has enabled a drastic decrease in costs for each company (by cost sharing) and the development of efficient tools (standards, software).\n\nBenchmarking can be internal (comparing performance between different groups or teams within an organization) or external (comparing performance with companies in a specific industry or across industries). Within these broader categories, there are three specific types of benchmarking: 1) Process benchmarking, 2) Performance benchmarking and 3) Strategic benchmarking. These can be further detailed as follows:\n\nBenchmarking software can be used to organize large and complex amounts of information. Software packages can extend the concept of benchmarking and competitive analysis by allowing individuals to handle such large and complex amounts or strategies. Such tools support different types of benchmarking (see above) and can reduce the above costs significantly.\n\nThe emerging technology of benchmarking engines automates the stage of going from data to noteworthy comparative insights, sometimes even expressing the insights in English sentences.\n\nAnother approach to making comparisons involves using more aggregative cost or production information to identify strong and weak performing units. The two most common forms of quantitative analysis used in metric benchmarking are data envelope analysis (DEA) and regression analysis. DEA estimates the cost level an efficient firm should be able to achieve in a particular market. In infrastructure regulation, DEA can be used to reward companies/operators whose costs are near the efficient frontier with additional profits. Regression analysis estimates what the average firm should be able to achieve. With regression analysis, firms that performed better than average can be rewarded while firms that performed worse than average can be penalized. Such benchmarking studies are used to create yardstick comparisons, allowing outsiders to evaluate the performance of operators in an industry. Advanced statistical techniques, including stochastic frontier analysis, have been used to identify high and weak performers in industries, including applications to schools, hospitals, water utilities, and electric utilities.\n\nOne of the biggest challenges for metric benchmarking is the variety of metric definitions used among companies or divisions. Definitions may change over time within the same organization due to changes in leadership and priorities. The most useful comparisons can be made when metrics definitions are common between compared units and do not change so improvements can be changed\n\n"}
{"id": "57967446", "url": "https://en.wikipedia.org/wiki?curid=57967446", "title": "Blaire Van Valkenburgh", "text": "Blaire Van Valkenburgh\n\nBlaire Van Valkenburgh is an American paleontologist and holds the Donald R. Dickey Chair in Vertebrate Biology in the Department of Ecology and Evolutionary Biology at University of California Los Angeles. She has served as chair of the department and as associate dean of academic programs in the life sciences at UCLA.\n\nThe focus of her research is the paleobiology and paleoecology of Carnivora. Her contributions include quantification of guild structure in fossil carnivore communities and the study of iterative evolution in carnivore feeding adaptations.\n\nVan Valkenburgh received a bachelors degree from Stockton State College in New Jersey in 1974, a Ph.D. in Vertebrate Paleobiology from Johns Hopkins University in 1984, where she worked with Robert Bakker, after which she worked as a postdoctoral fellow with Alan Walker at Johns Hopkins before moving to UCLA in 1986. Van Valkenburgh served as president of the Society of Vertebrate Paleontology from 2008 to 2010 and as associate editor of the Journal of Vertebrate Paleontology from 2011 to 2017.\n"}
{"id": "36976124", "url": "https://en.wikipedia.org/wiki?curid=36976124", "title": "Carl Julius Meyer von Klinggräff", "text": "Carl Julius Meyer von Klinggräff\n\nCarl Julius Meyer von Klinggräff (26 March 1809 in Klein Watkowitz in Kreis Stuhm – 1879) was a German botanist. He was an older brother to bryologist Hugo Erich Meyer von Klinggräff (1820–1902, H.Klinggr.).\n\nFrom 1828 to 1832, he studied medicine and botany at the University of Königsberg, where he was influenced by Ernst Heinrich Friedrich Meyer. In 1832 he received his doctorate in medicine and surgery with the thesis \"De carie vertebrarum\". Following graduation, he travelled to his parents' home near Zagreb, and on the journey, conducted studies of Alpine flora and plants native to the Adriatic coast. He visited Fiume, Trieste, islands in the Gulf of Quarnero and made the acquaintanceship of botanists Bartolomeo Biasoletto, Friedrich Wilhelm Noë and Mutius von Tommasini.\n\nIn 1834 he returned to Prussia, settling in Marienwerder as a general practitioner. In 1836 he relocated to the town of Paleschken, from where he concentrated on botanical research. During his career, he made numerous trips throughout the Province of Prussia, and in 1844, took an extended journey to Austria, Switzerland and northern Italy.\n\nHe made major contributions to the knowledge of Prussian flora, conducting research in the fields of phytogeography, plant systematics and climatology as it pertained to botany.\n"}
{"id": "12948848", "url": "https://en.wikipedia.org/wiki?curid=12948848", "title": "ChaHα8", "text": "ChaHα8\n\nChaHα8 (also written ChaHa8 when Greek letters are unavailable) is a brown dwarf 522 light years from Earth discovered in 2000. It was found in 2007 to have a low-mass substellar companion in orbit around it. The companion has a mass of 25–31 Jupiter masses and orbits with a period of 5.2 years and an eccentricity of 0.59.\n\n"}
{"id": "38533535", "url": "https://en.wikipedia.org/wiki?curid=38533535", "title": "Crater of eternal darkness", "text": "Crater of eternal darkness\n\nA crater of eternal darkness is a depression on a body in the Solar System within which lies a point that is always in darkness. Such a crater must be located at high latitude (close to a pole), and be on a body with very small axial tilt.\nCraters of eternal darkness might be advantageous for space exploration and colonization, as they could potentially preserve sources of water.\n\n\n"}
{"id": "40328988", "url": "https://en.wikipedia.org/wiki?curid=40328988", "title": "Deferribacter thermophilus", "text": "Deferribacter thermophilus\n\nDeferribacter thermophilus is an iron-reducing bacteria. It is a manganese- and iron-reducing bacterium. It is thermophilic and anaerobic bacterium, its type strain being designated as strain BMAT. The cells are straight to bent rods (1 to 5 by 0.3 to 0.5 μm).\n\n"}
{"id": "57267117", "url": "https://en.wikipedia.org/wiki?curid=57267117", "title": "Dick Barkinji", "text": "Dick Barkinji\n\nDick of the Barkinji people, also known as \"Mountain\", was an Australian explorer who was on the Burke and Wills support expedition.\n\nDick was a member of the Barkinji people from the Darling River, who were on good terms with the Europeans.\nDick probably came into contact with British settlers who were establishing farms along the Darling River during his youth.\n\nHe was on the Burke and Wills expedition support expedition from the Murray River to Coopers Creek, in September 1860, when Trouper Lyons and Alexander MacPherson became lost in the Gibber desert, south of Coopers Creek.\n\nA brass plate and 5 guinies was presented to Dick in September 1861 by Governor of Victoria Henry Barkley.\n"}
{"id": "18239398", "url": "https://en.wikipedia.org/wiki?curid=18239398", "title": "Education informatics", "text": "Education informatics\n\nEducation Informatics is a sub-field of informatics. The primary focus is on computer applications, systems and networks that support research in and delivery of education. Education informatics is based upon information science, computer science and education but particularly addresses the intersection of these broad areas. Note that it is distinct from Informatics Education, a term that relates more to the practice of teaching/learning about informatics, rather than the use of information science and technology in the support of teaching and learning. The term has been in use since at least 1980.\n"}
{"id": "4769216", "url": "https://en.wikipedia.org/wiki?curid=4769216", "title": "Electromagnetic mass", "text": "Electromagnetic mass\n\nElectromagnetic mass was initially a concept of classical mechanics, denoting as to how much the electromagnetic field, or the self-energy, is contributing to the mass of charged particles. It was first derived by J. J. Thomson in 1881 and was for some time also considered as a dynamical explanation of inertial mass \"per se\". Today, the relation of mass, momentum, velocity and all forms of energy, including electromagnetic energy, is analyzed on the basis of Albert Einstein's special relativity and mass–energy equivalence. As to the cause of mass of elementary particles, the Higgs mechanism in the framework of the relativistic Standard Model is currently used. In addition, some problems concerning the electromagnetic mass and self-energy of charged particles are still studied.\n\nIt was recognized by J. J. Thomson in 1881 that a charged sphere moving in a space filled with a medium of a specific inductive capacity (the electromagnetic aether of James Clerk Maxwell), is harder to set in motion than an uncharged body. (Similar considerations were already made by George Gabriel Stokes (1843) with respect to hydrodynamics, who showed that the inertia of a body moving in an incompressible perfect fluid is increased.) So due to this self-induction effect, electrostatic energy behaves as having some sort of momentum and \"apparent\" electromagnetic mass, which can increase the ordinary mechanical mass of the bodies, or in more modern terms, the increase should arise from their electromagnetic self-energy. This idea was worked out in more detail by Oliver Heaviside (1889), Thomson (1893), George Frederick Charles Searle (1897), Max Abraham (1902), Hendrik Lorentz (1892, 1904), and was directly applied to the electron by using the Abraham–Lorentz force. Now, the electrostatic energy formula_1 and mass formula_2 of an electron at rest was calculated to be \n\nwhere formula_4 is the charge, uniformly distributed on the surface of a sphere, and formula_5 is the classical electron radius, which must be nonzero to avoid infinite energy accumulation. Thus the formula for this electromagnetic energy–mass relation is\n\nThis was discussed in connection with the proposal of the electrical origin of matter, so Wilhelm Wien (1900), and Max Abraham (1902), came to the conclusion that the total mass of the bodies is identical to its electromagnetic mass. Wien stated, that if it is assumed that gravitation is an electromagnetic effect too, then there has to be a proportionality between electromagnetic energy, inertial mass, and gravitational mass. When one body attracts another one, the electromagnetic energy store of gravitation is according to Wien diminished by the amount (where formula_7 is the attracted mass, formula_8 the gravitational constant, formula_9 the distance):\n\nHenri Poincaré in 1906 argued that when mass is in fact the product of the electromagnetic field in the aether – implying that no \"real\" mass exists – and because matter is inseparably connected with mass, then also matter doesn't exist at all and electrons are only concavities in the aether.\n\nThomson (1893) noticed that electromagnetic momentum and energy of charged bodies, and therefore their masses, depend on the speed of the bodies as well. He wrote:\n\nIn 1897, Searle gave a more precise formula for the electromagnetic energy of charged sphere in motion:\n\nand like Thomson he concluded:\nFrom Searle's formula, Walter Kaufmann (1901) and Abraham (1902) derived the formula for the electromagnetic mass of moving bodies:\n\nHowever, it was shown by Abraham (1902), that this value is only valid in the longitudinal direction (\"longitudinal mass\"), i.e., that the electromagnetic mass also depends on the direction of the moving bodies with respect to the aether. Thus Abraham also derived the \"transverse mass\":\n\nOn the other hand, already in 1899 Lorentz assumed that the electrons undergo length contraction in the line of motion, which leads to results for the acceleration of moving electrons that differ from those given by Abraham. Lorentz obtained factors of formula_14 parallel to the direction of motion and formula_15 perpendicular to the direction of motion, where formula_16 and formula_17 is an undetermined factor. Lorentz expanded his 1899 ideas in his famous 1904 paper, where he set the factor formula_17 to unity, thus:\n\nSo, eventually Lorentz arrived at the same conclusion as Thomson in 1893: no body can reach the speed of light because the mass becomes infinitely large at this velocity.\n\nAdditionally, a third electron model was developed by Alfred Bucherer and Paul Langevin, in which the electron contracts in the line of motion, and expands perpendicular to it, so that the volume remains constant. This gives:\n\nThe predictions of the theories of Abraham and Lorentz were supported by the experiments of Walter Kaufmann (1901), but the experiments were not precise enough to distinguish between them. In 1905 Kaufmann conducted another series of experiments (Kaufmann–Bucherer–Neumann experiments) which confirmed Abraham's and Bucherer's predictions, but contradicted Lorentz's theory and the \"fundamental assumption of Lorentz and Einstein\", \"i.e.\", the relativity principle. In the following years experiments by Alfred Bucherer (1908), Gunther Neumann (1914) and others seemed to confirm Lorentz's mass formula. It was later pointed out that the Bucherer–Neumann experiments were also not precise enough to distinguish between the theories – it lasted until 1940 when the precision required was achieved to eventually prove Lorentz's formula and to refute Abraham's by these kinds of experiments. (However, other experiments of different kind already refuted Abraham's and Bucherer's formulas long before.)\n\nThe idea of an electromagnetic nature of matter, however, had to be given up. Abraham (1904, 1905) argued that non-electromagnetic forces were necessary to prevent Lorentz's contractile electrons from exploding. He also showed that different results for the longitudinal electromagnetic mass can be obtained in Lorentz's theory, depending on whether the mass is calculated from its energy or its momentum, so a non-electromagnetic potential (corresponding to 1/3 of the electron's electromagnetic energy) was necessary to render these masses equal. Abraham doubted whether it was possible to develop a model satisfying all of these properties.\n\nTo solve those problems, Henri Poincaré in 1905 and 1906 introduced some sort of pressure (\"Poincaré stresses\") of non-electromagnetic nature. As required by Abraham, these stresses contribute non-electromagnetic energy to the electrons, amounting to 1/4 of their total energy or to 1/3 of their electromagnetic energy. So, the Poincaré stresses remove the contradiction in the derivation of the longitudinal electromagnetic mass, they prevent the electron from exploding, they remain unaltered by a Lorentz transformation (\"i.e.\" they are Lorentz invariant), and were also thought as a dynamical explanation of length contraction. However, Poincaré still assumed that only the electromagnetic energy contributes to the mass of the bodies.\n\nAs it was later noted, the problem lies in the 4/3 factor of electromagnetic rest mass – given above as formula_21 when derived from the Abraham–Lorentz equations. However, when it is derived from the electron's electrostatic energy alone, we have formula_22 where the 4/3 factor is missing. This can be solved by adding the non-electromagnetic energy formula_23 of the Poincaré stresses to formula_1, the electron's total energy formula_25 now becomes:\n\nThus the missing 4/3 factor is restored when the mass is related to its electromagnetic energy, and it disappears when the total energy is considered.\n\nAnother way of deriving some sort of electromagnetic mass was based on the concept of radiation pressure. These pressures or tensions in the electromagnetic field were derived by James Clerk Maxwell (1874) and Adolfo Bartoli (1876). Lorentz recognized in 1895 that those tensions also arise in his theory of the stationary aether. So if the electromagnetic field of the aether is able to set bodies in motion, the action/reaction principle demands that the aether must be set in motion by matter as well. However, Lorentz pointed out that any tension in the aether requires the mobility of the aether parts, which in not possible since in his theory the aether is immobile. This represents a violation of the reaction principle that was accepted by Lorentz consciously. He continued by saying, that one can only speak about \"fictitious\" tensions, since they are only mathematical models in his theory to ease the description of the electrodynamic interactions.\n\nIn 1900 Poincaré studied the conflict between the action/reaction principle and Lorentz's theory. He tried to determine whether the center of gravity still moves with a uniform velocity when electromagnetic fields and radiation are involved. He noticed that the action/reaction principle does not hold for matter alone, but that the electromagnetic field has its own momentum (such a momentum was also derived by Thomson in 1893 in a more complicated way). Poincaré concluded, the electromagnetic field energy behaves like a fictitious fluid („fluide fictif“) with a mass density of formula_27 (in other words formula_28). Now, if the center of mass frame (COM-frame) is defined by both the mass of matter \"and\" the mass of the fictitious fluid, and if the fictitious fluid is indestructible – it is neither created or destroyed – then the motion of the center of mass frame remains uniform.\n\nBut this electromagnetic fluid is not indestructible, because it can be absorbed by matter (which according to Poincaré was the reason why he regarded the em-fluid as \"fictitious\" rather than \"real\"). Thus the COM-principle would be violated again. As it was later done by Einstein, an easy solution of this would be to assume that the mass of em-field is transferred to matter in the absorption process. But Poincaré created another solution: He assumed that there exists an immobile non-electromagnetic energy fluid at each point in space, also carrying a mass proportional to its energy. When the fictitious em-fluid is destroyed or absorbed, its electromagnetic energy and mass is not carried away by moving matter, but is transferred into the non-electromagnetic fluid and remains at exactly the same place in that fluid. (Poincaré added that one should not be too surprised by these assumptions, since they are only mathematical fictions.) In this way, the motion of the COM-frame, incl. matter, fictitious em-fluid, and fictitious non-em-fluid, at least \"theoretically\" remains uniform.\n\nHowever, since only matter and electromagnetic energy are directly observable by experiment (not the non-em-fluid), Poincaré's resolution still violates the reaction principle and the COM-theorem, when an emission/absorption process is \"practically\" considered. This leads to a paradox when changing frames: if waves are radiated in a certain direction, the device will suffer a recoil from the momentum of the fictitious fluid. Then, Poincaré performed a Lorentz boost (to first order in v/c) to the frame of the moving source. He noted that energy conservation holds in both frames, but that the law of conservation of momentum is violated. This would allow perpetual motion, a notion which he abhorred. The laws of nature would have to be different in the frames of reference, and the relativity principle would not hold. Therefore, he argued that also in this case there has to be another compensating mechanism in the ether.\n\nPoincaré came back to this topic in 1904. This time he rejected his own solution that motions in the ether can compensate the motion of matter, because any such motion is unobservable and therefore scientifically worthless. He also abandoned the concept that energy carries mass and wrote in connection to the above-mentioned recoil:\n\nHowever, Poincaré's idea of momentum and mass associated with radiation proved to be fruitful, when Max Abraham introduced the term „electromagnetic momentum“, having a field density of formula_27 per cm and formula_30 per cm. Contrary to Lorentz and Poincaré, who considered momentum as a fictitious force, he argued that it is a real physical entity, and therefore conservation of momentum is guaranteed.\n\nIn 1904, Friedrich Hasenöhrl specifically associated inertia with \"radiation\" by studying the dynamics of a moving cavity. Hasenöhrl suggested that part of the mass of a body (which he called \"apparent mass\") can be thought of as radiation bouncing around a cavity. The apparent mass of radiation depends on the temperature (because every heated body emits radiation) and is proportional to its energy, and he first concluded that formula_31. However, in 1905 Hasenöhrl published a summary of a letter, which was written by Abraham to him. Abraham concluded that Hasenöhrl's formula of the apparent mass of radiation is not correct, and on the basis of his definition of electromagnetic momentum and longitudinal electromagnetic mass Abraham changed it to formula_32, the same value for the electromagnetic mass for a body at rest. Hasenöhrl recalculated his own derivation and verified Abraham's result. He also noticed the similarity between the apparent mass and the electromagnetic mass. However, Hasenöhrl stated that this energy-apparent-mass relation \"only\" holds as long a body radiates, i.e. if the temperature of a body is greater than 0 K.\n\nThe idea that the principal relations between mass, energy, momentum and velocity can only be considered on the basis of dynamical interactions of matter was superseded, when Albert Einstein found out in 1905 that considerations based on the special principle of relativity require that all forms of energy (not only electromagnetic) contribute to the mass of bodies (mass–energy equivalence). That is, the entire mass of a body is a measure of its energy content by formula_33, and Einstein's considerations were independent from assumptions about the constitution of matter. By this equivalence, Poincaré's radiation paradox can be solved without using \"compensating forces\", because the mass of matter itself (not the non-electromagnetic aether fluid as suggested by Poincaré) is increased or diminished by the mass of electromagnetic energy in the course of the emission/absorption process. Also the idea of an electromagnetic explanation of gravitation was superseded in the course of developing general relativity.\n\nSo every theory dealing with the mass of a body must be formulated in a relativistic way from the outset. This is for example the case in the current quantum field explanation of mass of elementary particles in the framework of the Standard Model, the Higgs mechanism. Because of this, the idea that any form of mass is \"completely\" caused by interactions with electromagnetic fields, is not relevant any more.\n\nThe concepts of longitudinal and transverse mass (equivalent to those of Lorentz) were also used by Einstein in his first papers on relativity. However, in special relativity they apply to the entire mass of matter, not only to the electromagnetic part. Later it was shown by physicists like Richard Chace Tolman that expressing mass as the ratio of force and acceleration is not advantageous. Therefore, a similar concept without direction dependent terms, in which force is defined as formula_34, was used as relativistic mass\n\nThis concept is sometimes still used in modern physics textbooks, although the term 'mass' is now considered by many to refer to invariant mass, see mass in special relativity.\n\nWhen the special case of the electromagnetic self-energy or self-force of charged particles is discussed, also in modern texts some sort of \"effective\" electromagnetic mass is sometimes introduced – not as an explanation of mass \"per se\", but in addition to the ordinary mass of bodies. Many different reformulations of the Abraham–Lorentz force have been derived – for instance, in order to deal with the 4/3-problem (see next section) and other problems that arose from this concept. Such questions are discussed in connection with renormalization, and on the basis of quantum mechanics and quantum field theory, which must be applied when the electron is considered physically point-like. At distances located in the classical domain, the classical concepts again come into play. A rigorous derivation of the electromagnetic self-force, including the contribution to the mass of the body, was published by Gralla et al. (2009).\n\nMax von Laue in 1911 also used the Abraham–Lorentz equations of motion in his development of special relativistic dynamics, so that also in special relativity the 4/3-factor is present when the electromagnetic mass of a charged sphere is calculated. This contradicts the mass–energy equivalence formula, which requires the relation formula_28 without the 4/3 factor, or in other words, four-momentum doesn't properly transform like a four-vector when the 4/3 factor is present. Laue found a solution equivalent to Poincaré's introduction of a non-electromagnetic potential (Poincaré stresses), but Laue showed its deeper, relativistic meaning by employing and advancing Hermann Minkowski's spacetime formalism. Laue's formalism required that there are additional components and forces, which guarantee that spatially extended systems (where both electromagnetic and non-electromagnetic energies are combined) are forming a stable or \"closed system\" and transform as a four-vector. That is, the 4/3 factor arises only with respect to electromagnetic mass, while the closed system has total rest mass and energy of formula_37.\n\nAnother solution was found by authors such as Enrico Fermi (1922), Paul Dirac (1938) Fritz Rohrlich (1960), or Julian Schwinger (1983), who pointed out that the electron's stability and the 4/3-problem are two different things. They showed that the preceding definitions of four-momentum are non-relativistic \"per se\", and by changing the definition into a relativistic form, the electromagnetic mass can simply be written as formula_28 and thus the 4/3 factor doesn't appear at all. So every part of the system, not only \"closed\" systems, properly transforms as a four-vector. However, binding forces like the Poincaré stresses are still necessary to prevent the electron from exploding due to Coulomb repulsion. But on the basis of the Fermi–Rohrlich definition, this is only a dynamical problem and has nothing to do with the transformation properties any more.\n\nAlso other solutions have been proposed, for instance, Valery Morozov (2011) gave consideration to movement of an imponderable charged sphere. It turned out that a flux of nonelectromagnetic energy exists in the sphere body. This flux has an impulse exactly equal to 1/3 of the sphere electromagnetic impulse regardless of a sphere internal structure or a material, it is made of. The problem was solved without attraction of any additional hypotheses. In this model, sphere tensions are not connected with its mass.\n\n\n"}
{"id": "4532196", "url": "https://en.wikipedia.org/wiki?curid=4532196", "title": "Etheric force", "text": "Etheric force\n\nEtheric force is a term Thomas Edison coined to describe a phenomenon later understood as high frequency electromagnetic waves—effectively, radio. Edison believed it was the mysterious force that some believed pervaded the ether.\n\nAt the end of 1875, Edison and his assistants were experimenting with the Acoustic Telegraph when they noticed that a rapidly vibrating spark gap produced a spark in an adjacent relay. Subsequent investigation showed that the phenomenon could be made to occur at a distance of several feet without interconnecting cables. Edison, with this small amount of evidence, announced that it was \"a true unknown force\" since he believed that the spark transmitted electricity without carrying any charge. Edison concluded that this discovery had the potential to cheapen telegraphic communication and to allow transatlantic cables to be laid without insulation. He was also interested in finding new forces as a means for providing scientific explanations for spiritualist, occult and other allegedly supernatural phenomena following his disenchantment with Helena Blavatsky's Theosophy.\n\nEdison's apparatus consisted of a spark gap vibrating at a high frequency powered by batteries and connected to tin foil sheet about 12 by 8 inches, effectively acting as an antenna. A similar tin foil sheet, connected to ground was located at about eight feet away with two more similar, un-grounded tin foil sheets between. Sparks could be seen at the \"receiver\" sheets. Effectively, Edison had observed wireless transmission and was later to regret that he had not pursued it.\n\nEdison's last laboratory notebook entry on etheric force in 1875, which shows his experimental apparatus, can be seen at The Edison Papers.\n\nThomas Edison announced the discovery, which he called \"etheric force\", to the press and reports began to appear in Newark newspapers from November 29, 1875. While etheric force initially met with an enthusiastic reception, sceptics began to question whether it truly was a new phenomenon or merely a consequence of some already known phenomenon such as electromagnetic induction. Leaders among the doubters were James Ashley, editor of the \"Telegrapher\", the inventor Elihu Thomson and Edwin Houston, a high school teacher with whom Thomson had studied. Thomson and Houston conducted a series of careful experiments where they discovered that the sparks actually carried a charge, and they announced their results, not in the popular press as Edison had done, but in a scientific journal, the \"Journal of the Franklin Institute\". This prompted a reply from Edison, more experiments and more scientific papers.\n\nNevertheless, Edison found support from a fellow diner at Delmonico's: George Miller Beard had experimented with electrophysiology, and enthusiastically promoted Edison's new force, most significantly in the pages of Scientific American.\n\nScepticism is not easily explained on scientific grounds as James Clerk Maxwell had predicted such waves in 1864 (confirmed by Heinrich Hertz in 1889). The negative reception is perhaps better understood as a result of Edison's uneasy relationship with the professional scientific community.\n\nEventually the controversy increased to the point where Edison was pressured by his principal financial backers, Western Union, to desist from etheric force research and publicity over it, and to devote himself to what Western Union saw as more commercially viable projects. Edison abandoned work despite having been able to send signals twenty to thirty feet. He also drafted, but did not file, a patent application for an \"etheric telegraph\" before he abandoned etheric force.\n\nIn 1885 Edison again took up investigation of transmission by spark while working on a railway telegraph system and was able to get transmission of five hundred feet. Edison thought it might be suitable for ship-to-ship and ship-to-shore communication, taking out a patent for the system. Edison did not develop the patent commercially but according to his former assistant Francis Jehl, sold it to Guglielmo Marconi who developed it into radio.\n"}
{"id": "52082270", "url": "https://en.wikipedia.org/wiki?curid=52082270", "title": "Feminization of language", "text": "Feminization of language\n\nIn linguistics, feminization has two mutually independent meanings.\n\nFirst, it refers to the process of re-classifying nouns and adjectives which as such refer to male beings, including occupational terms, as feminine. This is done most of the time by adding inflectional suffixes denoting a female (such as the standard suffix \"-ess\" in English, and its equivalent \"-a\" in Spanish).\n\nIn some languages with grammatical gender, for example Dutch, there is a tendency to assign the feminine gender to certain – in particular abstract – nouns which are originally masculine or neuter. This also happened to some words in Middle English (which, in contrast to Modern English, had grammatical gender) which denoted virtue and vice. In Modern English, in spite of it being a gender-neutral language, certain non-human things that are usually neuter are still sometimes feminized by way of figure of speech, especially countries and ships (see also , ).\n\nFeminists believe the use of the generic masculine to refer to someone who's gender is unknown erases women and should be abolished. There are a number of arguments against such prescriptive rules however.\n\nDouble gender marking is prevalent in radical political pamphlets and manifestos. This is difficult to track, however, as these types of publications are written by many groups and tend to be published by organizations that don't keep detailed records of their activities.\n\nFemale members of a profession can be referred to with the masculine ending -e (eg. presidente) or the feminine -essa (eg. presidentessa). A 2001 study by Mucchi-Faina and Barro showed that women professionals are more persuasive when using the masculine ending while a 2012 study by Merkel et al. show there was no difference in perception.\n\nNoun declension is asymmetrical in Russian. Women can be referred to with suffixes of the first or second declension but men can only be referred to with first declension suffixes.\n\n\"Man\" is commonly used to mean 'one' and is frequently used in general statements. It is similar to English indefinite \"you\" or \"one.\"\nFeminine job titles are usually created by adding -in to the grammatically masculine word in question. Informatiker (singular or plural). The feminine form is Informatikerin (singular) and Informatikerinnen (plural).\n\n"}
{"id": "413102", "url": "https://en.wikipedia.org/wiki?curid=413102", "title": "Folding@home", "text": "Folding@home\n\nFolding@home (FAH or F@h) is a distributed computing project for disease research that simulates protein folding, computational drug design, and other types of molecular dynamics. The project uses the idle processing resources of thousands of personal computers owned by volunteers who have installed the software on their systems. Its main purpose is to determine the mechanisms of protein folding, which is the process by which proteins reach their final three-dimensional structure, and to examine the causes of protein misfolding. This is of significant academic interest with major implications for medical research into Alzheimer's disease, Huntington's disease, and many forms of cancer, among other diseases. To a lesser extent, Folding@home also tries to predict a protein's final structure and determine how other molecules may interact with it, which has applications in drug design. Folding@home is developed and operated by the Pande Laboratory at Stanford University, under the direction of Prof. Vijay Pande, and is shared by various scientific institutions and research laboratories across the world.\n\nThe project has pioneered the use of graphics processing units (GPUs), PlayStation 3s, Message Passing Interface (used for computing on multi-core processors), and some Sony Xperia smartphones for distributed computing and scientific research. The project uses statistical simulation methodology that is a paradigm shift from traditional computing methods. As part of the client–server model network architecture, the volunteered machines each receive pieces of a simulation (work units), complete them, and return them to the project's database servers, where the units are compiled into an overall simulation. Volunteers can track their contributions on the Folding@home website, which makes volunteers' participation competitive and encourages long-term involvement.\n\nFolding@home is one of the world's fastest computing systems, with a speed of approximately 135 petaFLOPS . This performance from its large-scale computing network has allowed researchers to run computationally costly atomic-level simulations of protein folding thousands of times longer than formerly achieved. Since its launch on October 1, 2000, the Pande Lab has produced 139 scientific research papers as a direct result of Folding@home. Results from the project's simulations agree well with experiments.\n\nProteins are an essential component to many biological functions and participate in virtually all processes within biological cells. They often act as enzymes, performing biochemical reactions including cell signaling, molecular transportation, and cellular regulation. As structural elements, some proteins act as a type of skeleton for cells, and as antibodies, while other proteins participate in the immune system. Before a protein can take on these roles, it must fold into a functional three-dimensional structure, a process that often occurs spontaneously and is dependent on interactions within its amino acid sequence and interactions of the amino acids with their surroundings. Protein folding is driven by the search to find the most energetically favorable conformation of the protein, i.e., its native state. Thus, understanding protein folding is critical to understanding what a protein does and how it works, and is considered a \"holy grail\" of computational biology. Despite folding occurring within a crowded cellular environment, it typically proceeds smoothly. However, due to a protein's chemical properties or other factors, proteins may misfold, that is, fold down the wrong pathway and end up misshapen. Unless cellular mechanisms can destroy or refold misfolded proteins, they can subsequently aggregate and cause a variety of debilitating diseases. Laboratory experiments studying these processes can be limited in scope and atomic detail, leading scientists to use physics-based computing models that, when complementing experiments, seek to provide a more complete picture of protein folding, misfolding, and aggregation.\n\nDue to the complexity of proteins' conformation or configuration space (the set of possible shapes a protein can take), and limits in computing power, all-atom molecular dynamics simulations have been severely limited in the timescales which they can study. While most proteins typically fold in the order of milliseconds, before 2010 simulations could only reach nanosecond to microsecond timescales. General-purpose supercomputers have been used to simulate protein folding, but such systems are intrinsically costly and typically shared among many research groups. Further, because the computations in kinetic models occur serially, strong scaling of traditional molecular simulations to these architectures is exceptionally difficult. Moreover, as protein folding is a stochastic process and can statistically vary over time, it is challenging computationally to use long simulations for comprehensive views of the folding process.\nProtein folding does not occur in one step. Instead, proteins spend most of their folding time, nearly 96% in some cases, \"waiting\" in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the number of processors available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.\n\nBetween 2000 and 2010, the length of the proteins Folding@home has studied have increased by a factor of four, while its timescales for protein folding simulations have increased by six orders of magnitude. In 2002, Folding@home used Markov state models to complete approximately a million CPU days of simulations over the span of several months, and in 2011, MSMs parallelized another simulation that required an aggregate 10 million CPU hours of computing. In January 2010, Folding@home used MSMs to simulate the dynamics of the slow-folding 32-residue NTL9 protein out to 1.52 milliseconds, a timescale consistent with experimental folding rate predictions but a thousand times longer than formerly achieved. The model consisted of many individual trajectories, each two orders of magnitude shorter, and provided an unprecedented level of detail into the protein's energy landscape. In 2010, Folding@home researcher Gregory Bowman was awarded the Thomas Kuhn Paradigm Shift Award from the American Chemical Society for the development of the open-source MSMBuilder software and for attaining quantitative agreement between theory and experiment. For his work, Pande was awarded the 2012 Michael and Kate Bárány Award for Young Investigators for \"developing field-defining and field-changing computational methods to produce leading theoretical models for protein and RNA folding\", and the 2006 Irving Sigal Young Investigator Award for his simulation results which \"have stimulated a re-examination of the meaning of both ensemble and single-molecule measurements, making Dr. Pande's efforts pioneering contributions to simulation methodology.\"\n\nProtein misfolding can result in a variety of diseases including Alzheimer's disease, cancer, Creutzfeldt–Jakob disease, cystic fibrosis, Huntington's disease, sickle-cell anemia, and type II diabetes. Cellular infection by viruses such as HIV and influenza also involve folding events on cell membranes. Once protein misfolding is better understood, therapies can be developed that augment cells' natural ability to regulate protein folding. Such therapies include the use of engineered molecules to alter the production of a given protein, help destroy a misfolded protein, or assist in the folding process. The combination of computational molecular modeling and experimental analysis has the possibility to fundamentally shape the future of molecular medicine and the rational design of therapeutics, such as expediting and lowering the costs of drug discovery. The goal of the first five years of Folding@home was to make advances in understanding folding, while the current goal is to understand misfolding and related disease, especially Alzheimer's.\n\nThe simulations run on Folding@home are used in conjunction with laboratory experiments, but researchers can use them to study how folding \"in vitro\" differs from folding in native cellular environments. This is advantageous in studying aspects of folding, misfolding, and their relationships to disease that are difficult to observe experimentally. For example, in 2011 Folding@home simulated protein folding inside a ribosomal exit tunnel, to help scientists better understand how natural confinement and crowding might influence the folding process. Furthermore, scientists typically employ chemical denaturants to unfold proteins from their stable native state. It is not generally known how the denaturant affects the protein's refolding, and it is difficult to experimentally determine if these denatured states contain residual structures which may influence folding behavior. In 2010, Folding@home used GPUs to simulate the unfolded states of Protein L, and predicted its collapse rate in strong agreement with experimental results.\n\nThe Pande Lab is part of Stanford University, a non-profit entity, and does not sell the results generated by Folding@home. The large data sets from the project are freely available for other researchers to use upon request and some can be accessed from the Folding@home website. The Pande lab has collaborated with other molecular dynamics systems such as the Blue Gene supercomputer, and they share Folding@home's key software with other researchers, so that the algorithms which benefited Folding@home may aid other scientific areas. In 2011, they released the open-source Copernicus software, which is based on Folding@home's MSM and other parallelizing methods and aims to improve the efficiency and scaling of molecular simulations on large computer clusters or supercomputers. Summaries of all scientific findings from Folding@home are posted on the Folding@home website after publication.\n\nAlzheimer's disease is an incurable neurodegenerative disease which most often affects the elderly and accounts for more than half of all cases of dementia. Its exact cause remains unknown, but the disease is identified as a protein misfolding disease. Alzheimer's is associated with toxic aggregations of the amyloid beta (Aβ) peptide, caused by Aβ misfolding and clumping together with other Aβ peptides. These Aβ aggregates then grow into significantly larger senile plaques, a pathological marker of Alzheimer's disease. Due to the heterogeneous nature of these aggregates, experimental methods such as X-ray crystallography and nuclear magnetic resonance (NMR) have had difficulty characterizing their structures. Moreover, atomic simulations of Aβ aggregation are highly demanding computationally due to their size and complexity.\n\nPreventing Aβ aggregation is a promising method to developing therapeutic drugs for Alzheimer's disease, according to Drs. Naeem and Fazili in a literature review article. In 2008, Folding@home simulated the dynamics of Aβ aggregation in atomic detail over timescales of the order of tens of seconds. Prior studies were only able to simulate about 10 microseconds. Folding@home was able to simulate Aβ folding for six orders of magnitude longer than formerly possible. Researchers used the results of this study to identify a beta hairpin that was a major source of molecular interactions within the structure. The study helped prepare the Pande lab for future aggregation studies and for further research to find a small peptide which may stabilize the aggregation process.\n\nIn December 2008, Folding@home found several small drug candidates which appear to inhibit the toxicity of Aβ aggregates. In 2010, in close cooperation with the Center for Protein Folding Machinery, these drug leads began to be tested on biological tissue. In 2011, Folding@home completed simulations of several mutations of Aβ that appear to stabilize the aggregate formation, which could aid in the development of therapeutic drug therapies for the disease and greatly assist with experimental nuclear magnetic resonance spectroscopy studies of Aβ oligomers. Later that year, Folding@home began simulations of various Aβ fragments to determine how various natural enzymes affect the structure and folding of Aβ.\n\nHuntington's disease is a neurodegenerative genetic disorder that is associated with protein misfolding and aggregation. Excessive repeats of the glutamine amino acid at the N-terminus of the Huntingtin protein cause aggregation, and although the behavior of the repeats is not completely understood, it does lead to the cognitive decline associated with the disease. As with other aggregates, there is difficulty in experimentally determining its structure. Scientists are using Folding@home to study the structure of the Huntingtin protein aggregate and to predict how it forms, assisting with rational drug design methods to stop the aggregate formation. The N17 fragment of the Huntington protein accelerates this aggregation, and while there have been several mechanisms proposed, its exact role in this process remains largely unknown. Folding@home has simulated this and other fragments to clarify their roles in the disease. Since 2008, its drug design methods for Alzheimer's disease have been applied to Huntington's.\n\nMore than half of all known cancers involve mutations of p53, a tumor suppressor protein present in every cell which regulates the cell cycle and signals for cell death in the event of damage to DNA. Specific mutations in p53 can disrupt these functions, allowing an abnormal cell to continue growing unchecked, resulting in the development of tumors. Analysis of these mutations helps explain the root causes of p53-related cancers. In 2004, Folding@home was used to perform the first molecular dynamics study of the refolding of p53's protein dimer in an all-atom simulation of water. The simulation's results agreed with experimental observations and gave insights into the refolding of the dimer that were formerly unobtainable. This was the first peer reviewed publication on cancer from a distributed computing project. The following year, Folding@home powered a new method to identify the amino acids crucial for the stability of a given protein, which was then used to study mutations of p53. The method was reasonably successful in identifying cancer-promoting mutations and determined the effects of specific mutations which could not otherwise be measured experimentally.\n\nFolding@home is also used to study protein chaperones, heat shock proteins which play essential roles in cell survival by assisting with the folding of other proteins in the crowded and chemically stressful environment within a cell. Rapidly growing cancer cells rely on specific chaperones, and some chaperones play key roles in chemotherapy resistance. Inhibitions to these specific chaperones are seen as potential modes of action for efficient chemotherapy drugs or for reducing the spread of cancer. Using Folding@home and working closely with the Center for Protein Folding Machinery, the Pande lab hopes to find a drug which inhibits those chaperones involved in cancerous cells. Researchers are also using Folding@home to study other molecules related to cancer, such as the enzyme Src kinase, and some forms of the engrailed homeodomain: a large protein which may be involved in many diseases, including cancer. In 2011, Folding@home began simulations of the dynamics of the small knottin protein EETI, which can identify carcinomas in imaging scans by binding to surface receptors of cancer cells.\n\nInterleukin 2 (IL-2) is a protein that helps T cells of the immune system attack pathogens and tumors. Unfortunately, its use as a cancer treatment is restricted due to serious side effects such as pulmonary edema. IL-2 binds to these pulmonary cells differently than it does to T cells, so IL-2 research involves understanding the differences between these binding mechanisms. In 2012, Folding@home assisted with the discovery of a form of IL-2 which is three hundred times more effective in its immune system role but carries fewer side effects. In experiments, this altered form significantly outperformed natural IL-2 in impeding tumor growth. Pharmaceutical companies have expressed interest in the mutant molecule, and the National Institutes of Health are testing it against a large variety of tumor models to try to accelerate its development as a therapeutic.\n\nOsteogenesis imperfecta, known as brittle bone disease, is an incurable genetic bone disorder which can be lethal. Those with the disease are unable to make functional connective bone tissue. This is most commonly due to a mutation in Type-I collagen, which fulfills a variety of structural roles and is the most abundant protein in mammals. The mutation causes a deformation in collagen's triple helix structure, which if not naturally destroyed, leads to abnormal and weakened bone tissue. In 2005, Folding@home tested a new quantum mechanical method that improved upon prior simulation methods, and which may be useful for future computing studies of collagen. Although researchers have used Folding@home to study collagen folding and misfolding, the interest stands as a pilot project compared to Alzheimer's and Huntington's research.\n\nFolding@home is assisting in research towards preventing some viruses, such as influenza and HIV, from recognizing and entering biological cells. In 2011, Folding@home began simulations of the dynamics of the enzyme RNase H, a key component of HIV, to try to design drugs to deactivate it. Folding@home has also been used to study membrane fusion, an essential event for viral infection and a wide range of biological functions. This fusion involves conformational changes of viral fusion proteins and protein docking, but the exact molecular mechanisms behind fusion remain largely unknown. Fusion events may consist of over a half million atoms interacting for hundreds of microseconds. This complexity limits typical computer simulations to about ten thousand atoms over tens of nanoseconds: a difference of several orders of magnitude. The development of models to predict the mechanisms of membrane fusion will assist in the scientific understanding of how to target the process with antiviral drugs. In 2006, scientists applied Markov state models and the Folding@home network to discover two pathways for fusion and gain other mechanistic insights.\n\nFollowing detailed simulations from Folding@home of small cells known as vesicles, in 2007, the Pande lab introduced a new computing method to measure the topology of its structural changes during fusion. In 2009, researchers used Folding@home to study mutations of influenza hemagglutinin, a protein that attaches a virus to its host cell and assists with viral entry. Mutations to hemagglutinin affect how well the protein binds to a host's cell surface receptor molecules, which determines how infective the virus strain is to the host organism. Knowledge of the effects of hemagglutinin mutations assists in the development of antiviral drugs. As of 2012, Folding@home continues to simulate the folding and interactions of hemagglutinin, complementing experimental studies at the University of Virginia.\n\nDrugs function by binding to specific locations on target molecules and causing some desired change, such as disabling a target or causing a conformational change. Ideally, a drug should act very specifically, and bind only to its target without interfering with other biological functions. However, it is difficult to precisely determine where and how tightly two molecules will bind. Due to limits in computing power, current \"in silico\" methods usually must trade speed for accuracy; e.g., use rapid protein docking methods instead of computationally costly free energy calculations. Folding@home's computing performance allows researchers to use both methods, and evaluate their efficiency and reliability. Computer-assisted drug design has the potential to expedite and lower the costs of drug discovery. In 2010, Folding@home used MSMs and free energy calculations to predict the native state of the villin protein to within 1.8 angstrom (Å) root mean square deviation (RMSD) from the crystalline structure experimentally determined through X-ray crystallography. This accuracy has implications to future protein structure prediction methods, including for intrinsically unstructured proteins. Scientists have used Folding@home to research drug resistance by studying vancomycin, an antibiotic drug of last resort, and beta-lactamase, a protein that can break down antibiotics like penicillin.\n\nChemical activity occurs along a protein's active site. Traditional drug design methods involve tightly binding to this site and blocking its activity, under the assumption that the target protein exists in one rigid structure. However, this approach works for approximately only 15% of all proteins. Proteins contain allosteric sites which, when bound to by small molecules, can alter a protein's conformation and ultimately affect the protein's activity. These sites are attractive drug targets, but locating them is very computationally costly. In 2012, Folding@home and MSMs were used to identify allosteric sites in three medically relevant proteins: beta-lactamase, interleukin-2, and RNase H.\n\nApproximately half of all known antibiotics interfere with the workings of a bacteria's ribosome, a large and complex biochemical machine that performs protein biosynthesis by translating messenger RNA into proteins. Macrolide antibiotics clog the ribosome's exit tunnel, preventing synthesis of essential bacterial proteins. In 2007, the Pande lab received a grant to study and design new antibiotics. In 2008, they used Folding@home to study the interior of this tunnel and how specific molecules may affect it. The full structure of the ribosome was determined only as of 2011, and Folding@home has also simulated ribosomal proteins, as many of their functions remain largely unknown.\n\nIn addition to reporting active processors, Folding@home determines its computing performance as measured in floating point operations per second (FLOPS) based on the actual execution time of its calculations. Originally this was reported as native FLOPS: the raw performance from each given type of processing hardware. In March 2009 Folding@home began reporting the performance in native and x86 FLOPS, the latter being an estimation of how many FLOPS the calculation would take on a standard x86 CPU architecture, which is commonly used as a performance reference. Specialized hardware such as GPUs can efficiently perform some complex functions in one floating point operation which otherwise needs multiple operations on the x86 architecture. The x86 measurement attempts to even out these hardware differences. Despite conservative conversions, the GPU clients' x86 FLOPS are consistently greater than their native FLOPS and comprise a large majority of Folding@home's measured computing performance.\n\nIn 2007, Guinness World Records recognized Folding@home as the most powerful distributed computing network. As of September 30, 2014, the project has 107,708 active CPU cores and 63,977 active GPUs for a total of 40.190 x86 petaFLOPS (19.282 native petaFLOPS). At the same time, the combined efforts of all distributed computing projects under BOINC totals 7.924 petaFLOPS. In November 2012, Folding@home updated its accounting of FLOPS, especially for GPUs, and now reports the number of active processor cores and physical processors. Using the Markov state model method, Folding@home achieves strong scaling across its user base and gains a linear speedup for every added processor. This network allows Folding@home to do work that was formerly impractical computationally.\n\nIn March 2002, Google cofounder Sergey Brin launched Google Compute as an add-on for the Google Toolbar. Although limited in function and scope, it increased participation in Folding@home from 10,000, up to about 30,000 active CPUs. The program ended in October 2005, in favor of the official Folding@home clients, and is no longer available for the Toolbar. Folding@home also gained participants from Genome@home, another distributed computing project from the Pande lab and a sister project to Folding@home. The goal of Genome@home was protein design and associated applications. Following its official conclusion in March 2004, users were asked to donate computing power to Folding@home instead.\n\nOn September 16, 2007, due in large part to the participation of PlayStation 3 consoles, the Folding@home project officially attained a sustained performance level higher than one native petaFLOPS, becoming the first computing system of any kind to do so. Top500's fastest supercomputer at the time was BlueGene/L, at 0.280 petaFLOPS. The following year, on May 7, 2008, the project attained a sustained performance level higher than two native petaFLOPS, followed by the three and four native petaFLOPS milestones on August 2008 and September 28, 2008 respectively. On February 18, 2009, Folding@home achieved five native petaFLOPS, and was the first computing project to meet these five levels. In comparison, November 2008's fastest supercomputer was IBM's Roadrunner at 1.105 petaFLOPS. On November 10, 2011, Folding@home's performance exceeded six native petaFLOPS with the equivalent of nearly eight x86 petaFLOPS. In mid-May 2013, Folding@home attained over seven native petaFLOPS, with the equivalent of 14.87 x86 petaFLOPS. It then reached eight native petaFLOPS on June 21, followed by nine on September 9 of that year, with 17.9 x86 petaFLOPS. On May 11, 2016 Folding@home announced that it was moving towards reaching the 100 x86 petaFLOPS mark.\n\nSimilarly to other distributed computing projects, Folding@home quantitatively assesses user computing contributions to the project through a credit system. All units from a given protein project have uniform base credit, which is determined by benchmarking one or more work units from that project on an official reference machine before the project is released. Each user receives these base points for completing every work unit, though through the use of a passkey they can receive added bonus points for reliably and rapidly completing units which are more demanding computationally or have a greater scientific priority. Users may also receive credit for their work by clients on multiple machines. This point system attempts to align awarded credit with the value of the scientific results.\n\nUsers can register their contributions under a team, which combine the points of all their members. A user can start their own team, or they can join an existing team. In some cases, a team may have their own community-driven sources of help or recruitment such as an Internet forum. The points can foster friendly competition between individuals and teams to compute the most for the project, which can benefit the folding community and accelerate scientific research. Individual and team statistics are posted on the Folding@home website.\n\nIf a user does not form a new team, or does not join an existing team, that user automatically becomes part of a \"Default\" team. This \"Default\" team has a team number of \"0\". Statistics are accumulated for this \"Default\" team as well as for specially named teams.\n\nFolding@home software at the user's end involves three primary components: work units, cores, and a client.\n\nA work unit is the protein data that the client is asked to process. Work units are a fraction of the simulation between the states in a Markov state model. After the work unit has been downloaded and completely processed by a volunteer's computer, it is returned to Folding@home servers, which then award the volunteer the credit points. This cycle repeats automatically. All work units have associated deadlines, and if this deadline is exceeded, the user may not get credit and the unit will be automatically reissued to another participant. As protein folding occurs serially, and many work units are generated from their predecessors, this allows the overall simulation process to proceed normally if a work unit is not returned after a reasonable period of time. Due to these deadlines, the minimum system requirement for Folding@home is a Pentium 3 450 MHz CPU with Streaming SIMD Extensions (SSE). However, work units for high-performance clients have a much shorter deadline than those for the uniprocessor client, as a major part of the scientific benefit is dependent on rapidly completing simulations.\n\nBefore public release, work units go through several quality assurance steps to keep problematic ones from becoming fully available. These testing stages include internal, beta, and advanced, before a final full release across Folding@home. Folding@home's work units are normally processed only once, except in the rare event that errors occur during processing. If this occurs for three different users, the unit is automatically pulled from distribution. The Folding@home support forum can be used to differentiate between issues arising from problematic hardware and bad work units.\n\nSpecialized molecular dynamics programs, referred to as \"FahCores\" and often abbreviated \"cores\", perform the calculations on the work unit as a background process. A large majority of Folding@home's cores are based on GROMACS, one of the fastest and most popular molecular dynamics software packages, which largely consists of manually optimized assembly language code and hardware optimizations. Although GROMACS is open-source software and there is a cooperative effort between the Pande lab and GROMACS developers, Folding@home uses a closed-source license to help ensure data validity. Less active cores include ProtoMol and SHARPEN. Folding@home has used AMBER, CPMD, Desmond, and TINKER, but these have since been retired and are no longer in active service. Some of these cores perform explicit solvation calculations in which the surrounding solvent (usually water) is modeled atom-by-atom; while others perform implicit solvation methods, where the solvent is treated as a mathematical continuum. The core is separate from the client to enable the scientific methods to be updated automatically without requiring a client update. The cores periodically create calculation checkpoints so that if they are interrupted they can resume work from that point upon startup.\n\nA Folding@home participant installs a client program on their personal computer. The user interacts with the client, which manages the other software components in the background. Through the client, the user may pause the folding process, open an event log, check the work progress, or view personal statistics. The computer clients run continuously in the background at a very low priority, using idle processing power so that normal computer use is unaffected. The maximum CPU use can be adjusted via client settings. The client connects to a Folding@home server and retrieves a work unit and may also download the appropriate core for the client's settings, operating system, and the underlying hardware architecture. After processing, the work unit is returned to the Folding@home servers. Computer clients are tailored to uniprocessor and multi-core processor systems, and graphics processing units. The diversity and power of each hardware architecture provides Folding@home with the ability to efficiently complete many types of simulations in a timely manner (in a few weeks or months rather than years), which is of significant scientific value. Together, these clients allow researchers to study biomedical questions formerly considered impractical to tackle computationally.\n\nProfessional software developers are responsible for most of Folding@home's code, both for the client and server-side. The development team includes programmers from Nvidia, ATI, Sony, and Cauldron Development. Clients can be downloaded only from the official Folding@home website or its commercial partners, and will only interact with Folding@home computer files. They will upload and download data with Stanford's Folding@home data servers (over port 8080, with 80 as an alternate), and the communication is verified using 2048-bit digital signatures. While the client's graphical user interface (GUI) is open-source, the client is proprietary software citing security and scientific integrity as the reasons.\n\nHowever, this rationale of using proprietary software is disputed since while the license could be enforceable in the legal domain retrospectively, it doesn't practically prevent the modification (also known as patching) of the executable binary files. Likewise, binary-only distribution does not prevent the malicious modification of executable binary-code, either through a man-in-the-middle attack while being downloaded via the internet, or by the redistribution of binaries by a third-party that have been previously modified either in their binary state (i.e. patched), or by decompiling and recompiling them after modification. Unless the binary files - and/or the transport channel - are signed and the recipient person/system is able to verify the digital signature, in which case unwarranted modifications should be detectable, but not always. Either way, since in the case of Folding@Home the input data and output result processed by the client-software are both digitally signed, the integrity of work can be verified independently from the integrity of the client software itself.\n\nFolding@home uses the Cosm software libraries for networking. Folding@home was launched on October 1, 2000, and was the first distributed computing project aimed at bio-molecular systems. Its first client was a screensaver, which would run while the computer was not otherwise in use. In 2004, the Pande lab collaborated with David P. Anderson to test a supplemental client on the open-source BOINC framework. This client was released to closed beta in April 2005; however, the method became unworkable and was shelved in June 2006.\n\nThe specialized hardware of graphics processing units (GPU) is designed to accelerate rendering of 3-D graphics applications such as video games and can significantly outperform CPUs for some types of calculations. GPUs are one of the most powerful and rapidly growing computing platforms, and many scientists and researchers are pursuing general-purpose computing on graphics processing units (GPGPU). However, GPU hardware is difficult to use for non-graphics tasks and usually requires significant algorithm restructuring and an advanced understanding of the underlying architecture. Such customization is challenging, more so to researchers with limited software development resources. Folding@home uses the open-source OpenMM library, which uses a bridge design pattern with two application programming interface (API) levels to interface molecular simulation software to an underlying hardware architecture. With the addition of hardware optimizations, OpenMM-based GPU simulations need no significant modification but achieve performance nearly equal to hand-tuned GPU code, and greatly outperform CPU implementations.\n\nBefore 2010, the computing reliability of GPGPU consumer-grade hardware was largely unknown, and circumstantial evidence related to the lack of built-in error detection and correction in GPU memory raised reliability concerns. In the first large-scale test of GPU scientific accuracy, a 2010 study of over 20,000 hosts on the Folding@home network detected soft errors in the memory subsystems of two-thirds of the tested GPUs. These errors strongly correlated to board architecture, though the study concluded that reliable GPU computing was very feasible as long as attention is paid to the hardware traits, such as software-side error detection.\n\nThe first generation of Folding@home's GPU client (GPU1) was released to the public on October 2, 2006, delivering a 20-30X speedup for some calculations over its CPU-based GROMACS counterparts. It was the first time GPUs had been used for either distributed computing or major molecular dynamics calculations. GPU1 gave researchers significant knowledge and experience with the development of GPGPU software, but in response to scientific inaccuracies with DirectX, on April 10, 2008 it was succeeded by GPU2, the second generation of the client. Following the introduction of GPU2, GPU1 was officially retired on June 6. Compared to GPU1, GPU2 was more scientifically reliable and productive, ran on ATI and CUDA-enabled Nvidia GPUs, and supported more advanced algorithms, larger proteins, and real-time visualization of the protein simulation. Following this, the third generation of Folding@home's GPU client (GPU3) was released on May 25, 2010. While backward compatible with GPU2, GPU3 was more stable, efficient, and flexibile in its scientific abilities, and used OpenMM on top of an OpenCL framework. Although these GPU3 clients did not natively support the operating systems Linux and macOS, Linux users with Nvidia graphics cards were able to run them through the Wine software application. GPUs remain Folding@home's most powerful platform in FLOPS. As of November 2012, GPU clients account for 87% of the entire project's x86 FLOPS throughput.\n\nNative support for Nvidia and AMD graphics cards under Linux was introduced with FahCore 17, which uses OpenCL rather than CUDA.\n\nFrom March 2007 until November 2012, Folding@home took advantage of the computing power of PlayStation 3s. At the time of its inception, its main streaming Cell processor delivered a 20x speed increase over PCs for some calculations, processing power which could not be found on other systems such as the Xbox 360. The PS3's high speed and efficiency introduced other opportunities for worthwhile optimizations according to Amdahl's law, and significantly changed the tradeoff between computing efficiency and overall accuracy, allowing the use of more complex molecular models at little added computing cost. This allowed Folding@home to run biomedical calculations that would have been otherwise infeasible computationally.\n\nThe PS3 client was developed in a collaborative effort between Sony and the Pande lab and was first released as a standalone client on March 23, 2007. Its release made Folding@home the first distributed computing project to use PS3s. On September 18 of the following year, the PS3 client became a channel of Life with PlayStation on its launch. In the types of calculations it can perform, at the time of its introduction, the client fit in between a CPU's flexibility and a GPU's speed. However, unlike CPUs and GPUs, users were unable to perform other activities on their PS3 while running Folding@home. The PS3's uniform console environment made technical support easier and made Folding@home more user friendly. The PS3 also has the ability to stream data quickly to its GPU, which was used for real-time atomic-level visualizing of the current protein dynamics.\n\nOn November 6, 2012, Sony concluded support for the Folding@home PS3 client and other services available under Life with PlayStation. Over its lifetime of five years and 7 months, more than 15 million users contributed over 100 million hours of computing to Folding@home, greatly assisting the project with disease research. Following discussions with the Pande lab, Sony decided to terminate the application. Pande considered the PlayStation 3 client a \"game changer\" for the project.\n\nFolding@home can use the parallel computing abilities of modern multi-core processors. The ability to use several CPU cores simultaneously allows completing the full simulation far faster. Working together, these CPU cores complete single work units proportionately faster than the standard uniprocessor client. This method is scientifically valuable because it enables much longer simulation trajectories to be performed in the same amount of time, and reduces the traditional difficulties of scaling a large simulation to many separate processors. A 2007 publication in the \"Journal of Molecular Biology\" relied on multi-core processing to simulate the folding of part of the villin protein approximately 10 times longer than was possible with a single-processor client, in agreement with experimental folding rates.\n\nIn November 2006, first-generation symmetric multiprocessing (SMP) clients were publicly released for open beta testing, referred to as SMP1. These clients used Message Passing Interface (MPI) communication protocols for parallel processing, as at that time the GROMACS cores were not designed to be used with multiple threads. This was the first time a distributed computing project had used MPI. Although the clients performed well in Unix-based operating systems such as Linux and macOS, they were troublesome under Windows. On January 24, 2010, SMP2, the second generation of the SMP clients and the successor to SMP1, was released as an open beta and replaced the complex MPI with a more reliable thread-based implementation.\n\nSMP2 supports a trial of a special category of \"bigadv\" work units, designed to simulate proteins that are unusually large and computationally intensive and have a great scientific priority. These units originally required a minimum of eight CPU cores, which was raised to sixteen later, on February 7, 2012. Along with these added hardware requirements over standard SMP2 work units, they require more system resources such as random-access memory (RAM) and Internet bandwidth. In return, users who run these are rewarded with a 20% increase over SMP2's bonus point system. The bigadv category allows Folding@home to run especially demanding simulations for long times that had formerly required use of supercomputing clusters and could not be performed anywhere else on Folding@home. Many users with hardware able to run bigadv units have later had their hardware setup deemed ineligible for bigadv work units when CPU core minimums were increased, leaving them only able to run the normal SMP work units. This frustrated many users who invested significant amounts of money into the program only to have their hardware be obsolete for bigadv purposes shortly after. As a result, Vijay Pande announced in January 2014 that the bigadv program would end on January 31, 2015.\n\nThe V7 client is the seventh and latest generation of the Folding@home client software, and is a full rewrite and unification of the prior clients for Windows, macOS, and Linux operating systems. It was released on March 22, 2012. Like its predecessors, V7 can run Folding@home in the background at a very low priority, allowing other applications to use CPU resources as they need. It is designed to make the installation, start-up, and operation more user-friendly for novices, and offer greater scientific flexibility to researchers than prior clients. V7 uses Trac for managing its bug tickets so that users can see its development process and provide feedback.\n\nV7 consists of four integrated elements. The user typically interacts with V7's open-source GUI, named FAHControl. This has Novice, Advanced, and Expert user interface modes, and has the ability to monitor, configure, and control many remote folding clients from one computer. FAHControl directs FAHClient, a back-end application that in turn manages each FAHSlot (or \"slot\"). Each slot acts as replacement for the formerly distinct Folding@home v6 uniprocessor, SMP, or GPU computer clients, as it can download, process, and upload work units independently. The FAHViewer function, modeled after the PS3's viewer, displays a real-time 3-D rendering, if available, of the protein currently being processed.\n\nIn 2014, a client for the Google Chrome and Chromium web browsers was released, allowing users to run Folding@home in their web browser. The client uses Google's Native Client (NaCl) feature on Chromium-based web browsers to run the Folding@Home code at near-native speed in a sandbox on the user's machine.\n\nIn July 2015, a client for Android mobile phones was released on Google Play for devices running Android 4.4 KitKat or newer.\n\nRosetta@home is a distributed computing project aimed at protein structure prediction and is one of the most accurate tertiary structure predictors. The conformational states from Rosetta's software can be used to initialize a Markov state model as starting points for Folding@home simulations. Conversely, structure prediction algorithms can be improved from thermodynamic and kinetic models and the sampling aspects of protein folding simulations. As Rosetta only tries to predict the final folded state, and not how folding proceeds, Rosetta@home and Folding@home are complementary and address very different molecular questions.\n\nAnton is a special-purpose supercomputer built for molecular dynamics simulations. In October 2011, Anton and Folding@home were the two most powerful molecular dynamics systems. Anton is unique in its ability to produce single ultra-long computationally costly molecular trajectories, such as one in 2010 which reached the millisecond range. These long trajectories may be especially helpful for some types of biochemical problems. However, Anton does not use Markov state models (MSM) for analysis. In 2011, the Pande lab constructed a MSM from two 100-µs Anton simulations and found alternative folding pathways that were not visible through Anton's traditional analysis. They concluded that there was little difference between MSMs constructed from a limited number of long trajectories or one assembled from many shorter trajectories. In June 2011 Folding@home began added sampling of an Anton simulation in an effort to better determine how its methods compare to Anton's. However, unlike Folding@home's shorter trajectories, which are more amenable to distributed computing and other parallelizing methods, longer trajectories do not require adaptive sampling to sufficiently sample the protein's phase space. Due to this, it is possible that a combination of Anton's and Folding@home's simulation methods would provide a more thorough sampling of this space.\n\n\nNote 1: Supercomputer FLOPS performance is assessed by running the legacy LINPACK benchmark. This short-term testing has difficulty in accurately reflecting sustained performance on real-world tasks because LINPACK more efficiently maps to supercomputer hardware. Computing systems vary in architecture and design, so direct comparison is difficult. Despite this, FLOPS remain the primary speed metric used in supercomputing. In contrast, Folding@home determines its FLOPS using wall clock time by measuring how much time its work units take to complete.\n"}
{"id": "27257302", "url": "https://en.wikipedia.org/wiki?curid=27257302", "title": "GeneQuant", "text": "GeneQuant\n\nGeneQuant Spectrophotometer is an RNA/DNA calculator tool that measures absorbance at 230, 260, 280, 320, 546, 562, 595 and 600 nm. It can determine the concentration and purity of nucleic acids after PCR amplification, calculate annealing temperatures for primers prior to PCR upon entry of the oligonucleotide sequence and concentration and the total buffer molarity, measure Optical Density (OD) 600 for bacterial cell culture solutions, and it may be used for protein determination using the Bradford, Biuret, and BCA protein assays.\n\nA solution of DNA or RNA with an optical density of 1.0 has a concentration of 50 or 40 ug/mL, respectively, in a 10 mm pathlength cell. Oligonucleotides have a corresponding factor of 33 ug/mL which can be calculated if the base sequence is known.\n\nConcentration = Abs260 * Factor\n\nThe instrument uses factors 50, 40 and 33 as defaults for RNA, DNA, and Oligonucleotides, respectively, and compensates for dilution and use of calls which do not have 10 mm pathlength.\n\nDefault units are converted using:\n1 ug/mL = 1 ng/uL = 0.001 ug/uL\npmol/uL = (ug/mL * 1000)/ MW of oligo\npmol phosphate = (nucleotide concentration, ug/mL) / 315\n\nMolecular weight of a DNA oligonucleotide is calculated:\nMW (g/mole) = [(dA * 312.2) + (dC * 288.2) + (dG * 328.2) + (dT * 303.2)] + [(MW counter-ion) * (length of oligo in bases)]\n\nThe MW is calculated using the following equation and adjusted for the contribution of the atoms at 5' and 3' ends:\nPhosphorylated oligos add: [17 + (2 * MW counter-ion)]\nNon-phosphorylated add: [61 + (MW counter-ion)]\nMW (g/mole) of the most common counter-ions are NA, 23.0, K, 39.1, and TEA, 102.2.\n"}
{"id": "18761076", "url": "https://en.wikipedia.org/wiki?curid=18761076", "title": "General chemistry", "text": "General chemistry\n\nGeneral chemistry\n(sometimes called \"gen chem\" for short) is a course often taught at the high school and introductory university level. It is intended to serve as a broad introduction to a variety of concepts in chemistry and is widely taught. At the university level, it is also sometimes used as a \"weed out\" course for disciplines (sometimes related, sometimes not) which are perceived to require a high level of intellectual rigor or large course loads. It is also one of the few chemistry courses in most universities that does not explicitly explore a particular discipline such as organic chemistry or analytical chemistry.\n\nGeneral chemistry courses typically introduce concepts such as stoichiometry, prediction of reaction products, thermodynamics, nuclear chemistry, electrochemistry, chemical kinetics, and many of the rudiments of physical chemistry. Though the list of subjects covered is typically broad, leading some to criticize both the class and the discipline as encouraging memorization, most general chemistry courses are firmly grounded in several fundamental physical rules for which the primary challenge is understanding when the rules are applicable.\n\nThe concepts taught in a typical general chemistry course are as follows:\n"}
{"id": "4387617", "url": "https://en.wikipedia.org/wiki?curid=4387617", "title": "Grapefruit–drug interactions", "text": "Grapefruit–drug interactions\n\nSome fruit juices and fruits can interact with numerous drugs, in many cases causing adverse effects. The effect was first discovered by accident, when a test of drug interactions with alcohol used grapefruit juice to hide the taste of the ethanol.\n\nIt is still best-studied with grapefruit and grapefruit juice, but similar effects have more recently been seen with some (not all) other citrus fruits. One medical review advises patients to avoid all citrus juices until further research clarifies the risks. The interacting chemicals are found in many plants, and so many other foods may be affected; effects have been observed with apple juice, but their clinical significance is not yet known.\n\nNormal amounts of food and drink, such as one whole grapefruit or a small glass () of grapefruit juice, can cause drug overdose toxicity. Fruit consumed three days before the medicine can still have an effect. The relative risks of different types of citrus fruit have not been systematically studied. Affected drugs typically have an auxiliary label saying “Do not take with grapefruit” on the container, and the interaction is elaborated on in the package insert. People are also advised to ask their physician or pharmacist about drug interactions.\nThe effects are caused by furanocoumarins (and, to a lesser extent, flavonoids). These chemicals inhibit key drug metabolizing enzymes, such as cytochrome P450 3A4 (CYP3A4). CYP3A4 is a metabolizing enzyme for almost 50% of drugs, and is found in the liver and small intestinal epithelial cells. As a result, many drugs are affected. Inhibition of enzymes can have two different effects, depending on whether the drug is either\n\n\nIf the active drug is metabolized by the inhibited enzyme, then the fruit will stop the drug being metabolized, leaving elevated concentrations of the medication in the body, which can cause adverse effects. Conversely, if the medication is a prodrug, it needs to be metabolised to be converted to the active drug. Compromising its metabolism lowers concentrations of the active drug, reducing its therapeutic effect, and risking therapeutic failure.\n\nLow drug concentrations can also be caused when the fruit suppresses drug absorption from the intestine.\n\nThe effect of grapefruit juice with regard to drug absorption was originally discovered in 1989. The first published report on grapefruit drug interactions was in 1991 in the Lancet entitled \"Interactions of Citrus Juices with Felodipine and Nifedipine,\" and was the first reported food-drug interaction clinically. However, the effect only became well-publicized after being responsible for a number of bad interactions with various medications.\n\nCitrus fruits may contain a number of polyphenol compounds, including the flavonoid naringin and furanocoumarins (such as bergamottin, dihydroxybergamottin, bergapten, and bergaptol). These are natural chemicals. They may be present in all forms of the fruit, including freshly squeezed juice, frozen concentrate, and whole fruit.\n\nGrapefruit, Seville oranges, bergamot, and possibly other citrus also contain large amounts of naringin. It can take up to 72 hours before the effects of the naringin on the CYP3A4 enzyme are seen. This is problematic as a 4 oz portion of grapefruit contains enough naringin to inhibit the metabolism of substrates of CYP3A4. Naringin is a flavonoid which contributes to the bitter flavour of grapefruit.\n\nFuranocoumarins seem to have a stronger effect than naringin under some circumstances.\n\nOrganic compounds that are derivatives of furanocoumarin interfere with liver and intestinal enzyme CYP3A4 and are believed to be primarily responsible for the effects of grapefruit on the enzyme. Cytochrome isoforms affected by grapefruit components also include CYP1A2, CYP2C9, and CYP2D6. Bioactive compounds in grapefruit juice may also interfere with MDR1 (multidrug resistance protein 1) and OATP (organic anion transporting polypeptides), either increasing or decreasing the bioavailability of a number of drugs. Drugs that are metabolized by these enzymes may have interactions with citrus chemicals.\n\nWhen drugs are taken orally, they enter the gut lumen to be absorbed in the small intestine and sometimes, in the stomach. In order for drugs to be absorbed, they must pass through the epithelial cells that line the lumen wall before they can enter the hepatic portal circulation to be distributed systemically in blood circulation. Drugs are metabolized by drug-specific metabolizing enzymes in the epithelial cells. Metabolizing enzymes transform these drugs into metabolites. The primary purpose for drug metabolism is to detoxify, inactivate, solubilize and eliminate these drugs. As a result, the amount of the drug in its original form that reaches systemic circulation is reduced due to this first-pass metabolism.\n\nFuranocoumarins (see section above) irreversibly inhibit a metabolizing enzyme cytochrome P450 3A4 (CYP3A4). CYP3A4 is a metabolizing enzyme for almost 50% of drugs, and is found in the liver and small intestinal epithelial cells. As a result, many drugs are impacted by consumption of citrus juice. When the metabolizing enzyme is inhibited, less of the drug will be metabolized by it in the epithelial cells. A decrease in drug metabolism means more of the original form of the drug could pass unchanged to systemic blood circulation. An unexpected high dose of the drug in the blood could lead to fatal drug toxicity.\nThe CYP3A4 is located in both the liver and the enterocytes. Many oral drugs undergo first-pass (presystemic) metabolism by the enzyme. Several organic compounds (see section above) found in citrus and specifically in grapefruit juice exert inhibitory action on drug metabolism by the enzyme.\n\nThis interaction is particularly dangerous when the drug in question has a low therapeutic index, so that a small increase in blood concentration can be the difference between therapeutic effect and toxicity. Citrus juice inhibits the enzyme only within the intestines if consumed in small amounts. Intestinal enzyme inhibition will only affect the potency of orally administrated drugs.\n\nWhen larger amounts are consumed they may also inhibit the enzyme in the liver. The hepatic enzyme inhibition may cause an additional increase in potency and a prolonged metabolic half-life (prolonged metabolic half-life for all ways of drug administration). The degree of the effect varies widely between individuals and between samples of juice, and therefore cannot be accounted for \"a priori\".\n\nAnother mechanism of interaction is possibly through the MDR1 (multidrug resistance protein 1) that is localized in the apical brush border of the enterocytes. P-glycoprotein (Pgp) transports lipophilic molecules out of the enterocyte back into the intestinal lumen. Drugs that possess lipophilic properties are either metabolised by CYP3A4 or removed into the intestine by the Pgp transporter. Both the Pgp and CYP3A4 may act synergistically as a barrier to many orally administered drugs. Therefore, their inhibition (both or alone) can markedly increase the bioavailability of a drug.\n\nGrapefruit–drug interactions that affect the pre-systemic metabolism (i.e., the metabolism that occurs before the drug enters the blood) of drugs have a different duration of action than interactions that work by other mechanisms, such as on absorption, discussed below.\n\nThe interaction is greatest when the juice is ingested with the drug or up to 4 hours before the drug.\n\nThe location of the inhibition occurs in the lining of the intestines, not within the liver. The effects last because grapefruit-mediated inhibition of drug metabolizing enzymes, like CYP3A4, is irreversible; that is, once the grapefruit has \"broken\" the enzyme, the intestinal cells must produce more of the enzyme to restore their capacity to metabolize drugs that the enzyme is used to metabolize. It takes around 24 hours to regain 50% of the cell's baseline enzyme activity and it can take 72 hours for the enzyme activity to completely return to baseline. For this reason, simply separating citrus consumption and medications taken daily does not avoid the drug interaction.\n\nFor medications that interact due to inhibition of OATP (organic anion-transporting polypeptides), a relative short period of time is needed to avoid this interaction, and a 4-hour interval between grapefruit consumption and the medication should suffice. For drugs recently sold on the market, drugs have information pages (monographs) that provide information on any potential interaction between a medication and grapefruit juice. Because there is a growing number of medications that are known to interact with citrus, patients should consult a pharmacist or physician before consuming citrus while taking their medications.\n\nGrapefruit is not the only citrus fruit that can interact with medications; one medical review advised patients to avoid all citrus.\n\nThere are three ways to test if a fruit interacts with drugs:\n\n\nThe first approach involves risk to trial volunteers. The first and second approaches have another problem: the same fruit cultivar could be tested twice with different results. Depending on growing and processing conditions, concentrations of the interacting polyphenol compounds can vary dramatically. The third approach is hampered by a paucity of knowledge of the genes in question.\n\nA descendant of citrus cultivars that cannot produce the problematic polyphenol compounds would presumably also lack the genes to produce them. Many citrus cultivars are hybrids of a small number of ancestral species, which have now been fully genetically sequenced.\n\nMany traditional citrus groups, such as true sweet oranges and lemons, seem to be bud sports, mutant descendants of a single hybrid ancestor. In theory, cultivars in a bud sport group would be either all safe or all problematic. However, new citrus varieties arriving on the market are increasingly likely to be sexually created hybrids, not asexually created sports.\n\nThe ancestry of a hybrid cultivar may not be known. Even if it is known, it is not possible to be certain that a cultivar will not interact with drugs on the basis of taxonomy, as it is not known which ancestors lack the capacity to make the problematic polyphenol compounds. However, many of the citrus cultivars known to be problematic seem to be closely related.\n\nPomelo (the Asian fruit that was crossed with an orange to produce grapefruit) contains high amounts of furanocoumarin derivatives. Grapefruit relatives and other varieties of pomelo have variable amounts of furanocoumarin.\n\nThe Dancy cultivar has a small amount of pomelo ancestry, but is genetically close to a non-hybrid true mandarin (unlike most commercial mandarins, which may have much more extensive hybridization). It has been tested once for furanocoumarins; none were detectable.\n\nNo citron or papeda seems to have been tested.\n\nBoth sweet oranges and bitter oranges are mandarin-pomelo hybrids. Bitter oranges (such as the Seville oranges often used in marmalade) can interfere with drugs including etoposide, a chemotherapy drug, some beta blocker drugs used to treat high blood pressure, and cyclosporine, taken by transplant patients to prevent rejection of their new organs. Evidence on sweet oranges is more mixed.\n\nTests on some tangelos (hybrids of mandarins/tangerines and pomelo or grapefruit) have not shown significant amounts of furanocoumarin.\n\nCommon lemons are the product of orange/citron hybridization, and hence have pomelo ancestry, and although Key limes are papeda/citron hybrids, the more commercially prevalent Persian limes and similar varieties are crosses of the Key lime with lemons, and hence likewise have pomelo ancestry. These limes can also inhibit drug metabolism. Other less-common citrus species also referred to as lemons or limes are genetically distinct from the more common varieties, with different proportions of pomelo ancestry.\n\nMarketing classifications often do not correspond to taxonomic ones. The \"Ambersweet\" cultivar is classified and sold as an orange, but does not descend from the same common ancestor as sweet oranges; it has grapefruit, orange, and mandarin ancestry. Fruits are often sold as mandarin, tangerine, or satsuma (which may be synonyms). Fruit sold under these names include many which are, like Sunbursts and Murcotts, hybrids with grapefruit ancestry. The diversity of fruits called limes is remarkable; some, like the Spanish lime and Wild lime, are not even citrus fruit.\n\nIn some countries, citrus fruit must be labelled with the name of a registered cultivar. Juice is often not so labelled. Some medical literature also names the cultivar tested.\n\nThe discovery that flavonoids are responsible for some interactions make it plausible that other fruit and vegetables are affected.\n\nApple juice, especially commercially produced products, interferes with the action of OATPs. This interference can decrease the absorption of a variety of commonly used medications, including beta blockers like atenolol, antibiotics like ciprofloxacin, and antihistamines like montelukast.\nApple juice has been implicated in interfering with etoposide, a chemotherapy drug, and cyclosporine, taken by transplant patients to prevent rejection of their new organs.\n\nPomegranate juice inhibits the action of the drug metabolizing enzymes CYP2C9 and CYP3A4. However, as of 2014, the currently available literature does not appear to indicate a clinically relevant impact of pomegranate juice on drugs that are metabolized by CYP2C9 and CYP3A4.\n\nResearchers have identified over 85 drugs with which grapefruit is known to have an adverse reaction. According to a review done by the Canadian Medical Association, there is an increase in the number of potential drugs that can interact with grapefruit juice, and of the number of fruit types that can interact with those drugs. From 2008 to 2012, the number of drugs known to potentially interact with grapefruit, with risk of harmful or even dangerous effects (gastrointestinal bleeding, nephrotoxicity), increased from 17 to 43.\n\nThe interaction between citrus and medication depends on the individual drug, and not the class of the drug. Drugs that interact usually share three common features: they are taken orally, normally only a small amount enters systemic blood circulation, and they are metabolized by CYP3A4. However, the effects on the CYP3A4 in the liver could in principle cause interactions with non-oral drugs, and non-CYP3A4-meditated effects also exist.\n\nCytochrome isoforms affected by grapefruit components include CYP3A4, CYP1A2, CYP2C9, and CYP2D6. Drugs that are metabolized by these enzymes may have interactions with components of grapefruit.\n\nAn easy way to tell if a medication may be affected by grapefruit juice is by researching whether another known CYP3A4 inhibitor drug is already contraindicated with the active drug of the medication in question. Examples of such known CYP3A4 inhibitors include cisapride (Propulsid), erythromycin, itraconazole (Sporanox), ketoconazole (Nizoral), and mibefradil (Posicor).\n\nDrugs that interact with grapefruit compounds at CYP3A4 include\n\n\nDrugs that interact with grapefruit compounds at CYP1A2 include\n\nDrugs that interact with grapefruit compounds at CYP2D6 include\nResearch has been done on the interaction between amphetamines and CYP2D6 enzyme, and researchers concluded that some parts of substrate molecules contribute to the binding of the enzyme.\n\nAdditional drugs found to be affected by grapefruit juice include, but are not limited to\n"}
{"id": "58540839", "url": "https://en.wikipedia.org/wiki?curid=58540839", "title": "Groupism", "text": "Groupism\n\nGroupism is a theoretical approach in sociology that posits that conformity to the laws/norms of a group such as family, kinship, race, ethnicity, religion and nationality brings reciprocal benefits such as recognition, right, power and security. It is the principle that a persons primary or prioritised identity is that of membership in a social network. Groupists assume that individuals in a group tend to have stronger affinity and obligation to a particular group when the influence of an authority figure brings a common goal. The concept of groupism can be defined and criticized in varied ways for disciplines such as sociology, social psychology, anthropology, political history and philosophy. Group-ism is defined in most dictionaries as the behavior of a member of a group where they think and act as the group norm at the expense of individualism. The term originated around mid 19th century and the first known use of the word recorded was in 1851. It is a general definition often used in Indian English as the tendency to form factions in a system setting. The term had also been used for “the principles or practices of Oxford Group movement” which is now historical and rare.\n\n\"The tendency to take discrete, sharply differentiated, internally homogeneous and externally bounded groups as basic constituents of social life, chief protagonists of social conflicts, and fundamental units of social analysis\" \n\nGroupism has been a deeply entrenched and fundamental aspect of social analysis of nationalism, ethnicity, race, religion, gender, sexuality, age, class or even groups with combination of these categories but with a common interest in other universal categories like sports, music and values. It is commonly seen on everyday context in media reports and even academic research leading to policy analysis. According to Brubaker, it is the view that division among humans such as ethnicity is an absolute, unchanging entity rather than a changing conceptual variable subject to time and context. It is the tendency to take discrete groups as chief protagonists of social conflicts, to reify such groups as if they were unitary collective actors. It is considered a \"process in which individuals are reduced to specific group characteristic which are politicised as boundaries\". Conceptual groupism involves essentialising groups without taking into account that such habit of giving groups performative character taken into account in ethnopolitical conflict analysis could lead to intentional/unintentional framing of conflict between groups. In the example of ethnic groups, it involves viewing an individual in the group as the collective representative of the values or conflicts associated with the social norm of that group. American sociologist Rogers Brubaker has criticised conceptual groupism for its stereotypical approach in social and political analysis which leads to decisions that trivialize individual need to protect the labelled collective interest of a political group or institution. Brubaker suggests that instead of taking \"groups\" as substantial entities, social and political analysis of the \"groupness\" should be brought about interms of political, social, cultural and psychological processes listed below\n\n\nAccording to Norwegian anthropologist Fredric Barth, ethnicity, race and nationality categories are a matter of factors such as self identification/external categorisation, at individual/institutional level or formal/ informal context. Research on how ethnic, racial and national groups are classified and categorised began through studies on the colonial and post colonial societies. Brubaker suggests that there should be a focus on categories so that ethnicity, race and nationhood can co exist without ethnic 'groups' as separate entities as transforming groups into categories reduces the extent to which misleading political and legal analysis based on groupism turns to a framed reality .\n\nLegal groupism is the construction of groups in legal matter as defined by the German legal scholar Susanne Baer. Legal groupism also posits that equal right should be given to groups hence assuming that people always belong to a \"distinguishable\" group rather than many. This concept is regarded as problematic because most groups have unclear and shifting boundaries due to individuals living multiple identities and group characteristics. Therefore, Legal groupism come into conflict with the idea of individual rights when human rights issues are constructed as group issues in law and complete autonomy given to major religious institution in the name of religious freedom then curtail individual human right issues. For example when constitutional law allows churches and religious communities to self determine matters conflicting with human rights without considering the rights of diverse individuals within a seemingly homogeneous group, such overlapping between human rights and religion has led to lack of legal intervention in matters like sexual discrimination. For example religious authority power to internally handle child abuse cases, the European Union's exemption of religious organisation from fundamental rights in the \"EU proposal for a new Directive against discrimination 2008.\" \n\nIt has been estimated that there are over 300 million indigenous people around the world some among which includes the Red Indians, Sami in Northern Europe, the Aborigines and Torres Strait Islanders of Australia and the Maori of New Zealand, Ainu people of Japan, Bantu in Somalia, Assyrians of the Middle east, the Kazakhs, Mongols, Tajik, Tibetans, Ugyur, and Eurasian Nomads of Kazakhstan, eastern Russia. In order to acquire group rights indigenous people also happen to be under the regulation of national and international law. Although International customary law at the United nation and elsewhere has regarded indigenous people as a category since the 20th century, the definition of indigenous people has been a subject of ongoing debate at the national level since their classification as a group by the national law has interfered with their traditional group making processes. For example, The Finland government ratified ILO convention no.169 in 1989 which involved declaration of right for indigenous people of Finland such as the following definition of legal requirement to vote to the election of the Sami parliament. \n\n\"For the purpose of this Act, a Sami means a person who considers himself a Sami, provided: \n\n1. That he himself or at least one of his parents or grandparents has learnt Sami as his first language; \n\n2. That he is a descendant of a person who has been entered in a land, taxation or population register as a mountain, forest or fishing Lapp; or \n\n3. That at least one of his parents has or could have been registered as an elector for an election to the Sami Delegation or the Sami Parliament.\" \n\nThis definition has led to the Sámi debate in Finland a problem of defining the legal status of Sámi indigenous people as the groupist system of political and legal analysis has framed indigeinity as a political requirement rather than an ethnocultural reality hence the inconsistency between the definition that Sami people has held for their kinship.\n\nGroupism is viewed as deeply rooted part of the Japanese group oriented society known for their high productivity, cooperative attitude and surpassing international competitive strength. Some of the key aspects of groupism in Japanese society has been discussed in the journal article, 'The Paradox of Japan's Groupism: Threat to Future Competitiveness' by Kanji Haitani.\n\n\nThe ryūha-iemoto system refers to social organisation in Japanese traditional music making which is shown to be dominated by the hierarchical form of groupism. This involves an authoritative group called \"Miyagi- ha\" at the top of hierarchy who gets the greatest benefits of musical freedom, recognition, power and money. This group is known for transmission of their composition to subgroups down in the hierarchy. One of the characteristics of this form of social organisation involves lifetime affiliation as a subgroup member whereby there is no \"graduation\" or \"becoming a free musician\".\n\nA form of groupism which generally began as a sense of security provided by national identity to an unquestioning acceptance of a political agenda. It is based on an abstract mentality of “victory” in one’s own group and considering the “other” as a separate entity. The irrational obedience of German citizens during the Nazi regime of the 1930s and 1940s holocaust is a well known example of the negative consequences rooted in groupism. In terms of Legal studies, international lawyer Philip Allot has criticized the concept of state sovereignty in the current international laws in promoting groupism and the lack of reform in the 21 century context.\n\nPrejudice against certain groups of people is rooted in groupism whereby conclusions or attitude about a group of people is drawn without evaluating the evidence and often leads to discrimination which refers to the behaviour of treating other groups in a different way than one's own group.\n\n\nPersuasion through individuals, sub-groups or the group as a whole leads to behavioural change without rational choice. It also involves the power of normative influence from one's heritage, culture and tradition to which people comply with its social norms to gain recognition or to avoid other's disapproval. These concepts are broadly summarised in terms of groupism and can have both positive and negative connotation based on varying scale of groupism in a formal or informal system. Extreme forms of groupism well known in the past and present includes racism, bigotry, terrorism, genocide, dictatorship and war.\n\nSocial Psychology brings the concept of how individual psyche is shaped by the sociological constructs. In summary, social constructs formed by prejudice, discrimination, racism and nationalism can be ascribed to groupism which an individual can obtain throughout life based on their socio-cultural and historical context that leads to psychological development as a child. Social loafing is the phenomenon when the presence of other members in a group causes some to avoid responsibilities and exert less effort towards a group goal. Social disruption whereby the presence of others negatively influences the performance of tasks. Social facilitation which is a phenomenon proposed by Robert Zajonc is another example of the positive aspects of groupism where the presence and influence of diverse groups enhances the performance of a task. This is the opposite of social disruption whereby the presence of others negatively influences the performance of tasks that are relatively difficult.\n\nScientific evidence from early hominids in Africa that shows human have evolved as small social groups that are predisposed to include or exclude others in an instinctual manner. Evolution of humans as a unitary social species has led to the social status and sense of belonging that comes with identifying oneself or being identified as an individual in different categories of group. Research by anthropologist Robin Dunbar suggested that the ratio of the size of the neocortex to the brain size determines the amount of social relationship in different species and found that humans have relatively high social brain that can have tendency to form greater interpersonal networks of small groups than animals such as chimpanzees and dolphins with smaller neocortex size to brain size ratio hence smaller number of relationships. Groupism has been explained in terms of a biological need to form social bonds according to the \"need to belong theory\" whereby deprivation of this need has been shown to have Bio-psycho-social consequences. From the perspective of evolution, social influences on the individual based on natural selection has led to better adaptation and survival in various environments.\n"}
{"id": "15639049", "url": "https://en.wikipedia.org/wiki?curid=15639049", "title": "High probability instruction", "text": "High probability instruction\n\nThe High Probability Instruction (HPI) treatment is a behaviorist psychological treatment based on the idea of positive reinforcement.\nIt consists of the idea of reinforcing an instruction with a low probability of compliance by using the reinforcement of an instruction with a high probability \n\nLuce Doze (2005), under the direction of Ph.D Esteve Freixa i Bacquet (University of Picardie, France) and Mrs. Rivière (University of Lille-III, France). Treatment by HPI of an autistic child - (An example of complimentarity between fundamental research and clinical practice from an autistic child case)\nResults shown at the 2005 seminar of the Association Picardie de Pratiques Cognitives et Comportementales (Picard Association of Cognitive and Behaviorist Practices)\n\nArdoin, S. P., Martens, B. K., & Wolfe, L. A. (1999). Using high-probability instruction sequences with fading to increase student compliance during transitions. Journal of Applied Behavior Analysis, 32, 339-351.\n"}
{"id": "20708927", "url": "https://en.wikipedia.org/wiki?curid=20708927", "title": "Hill people", "text": "Hill people\n\nHill people is a general term for people who live in hills and mountains. There are a wide variety of hill people around the world, many of whom live by small scale pastoralism or on small farms. Musical instruments of hill people, such as various forms of horn, are notable for their ability to be heard at great distances. Political borders often split hill peoples between countries, and they are sometimes minorities in their countries with a tradition of resisting control by central government.\n\nThe Drakensberg are the highest mountain range in Southern Africa, rising up to in height. The people of these mountains are mostly Bantu-speaking people who moved into the area from the north a thousand years ago, displacing the original Khoisan people. They include the Swazi, Xhosa and Zulu. Traditionally these people lived by cattle herding and small-scale farming, growing crops such as sorghum, maize, corn, pumpkins, beans and vegetables.\n\nAt the time the Europeans reached this part of South Africa (around 1830), the Zulu were temporarily in the ascendancy after a series of wars between the people of the region. The region is divided between South Africa, Lesotho and Swaziland, but the people move relatively freely between these states.\n\nEthiopia has a high central plateau that varies from above sea level, with the highest mountain reaching . Near the equator but high up, the climate is temperate all year round. The heavy rains from June until mid-September feed the Blue Nile, which waters Egypt.\n\nThe country's population is highly diverse. Most of its people speak Afro-Asiatic languages of the Cushitic or Semitic branches. The former includes Oromiffa, spoken by the Oromo people, and Somali, spoken by the Somali people; the latter includes Amharic, spoken by the Amhara people, and Tigrinya, spoken by the Tigray-Tigrinya people. Together, these four groups make up about three-quarters of Ethiopia's population. Other Afro-Asiatic languages with a significant number of speakers include the Cushitic Sidamo, Afar, Hadiyya and Agaw languages, as well as the Semitic Gurage, Harari, Silt'e and Argobba tongues. Principal crops include coffee, pulses (e.g. beans), oilseeds, cereals, potatoes, sugarcane, and vegetables. Many of the people traditionally herded cattle, goat or sheep.\n\nThe Ethiopian state is ancient, dating back to at least the Aksumite Empire (which officially used the name \"Ethiopia\" in the 4th century) and its predecessor state, D`mt (with early 1st millennium BC roots). Despite two attempts at conquest by the Italians, the country has remained independent for almost all its history. The Coptic Christian church has a long history in Ethiopia. It was founded in 316 AD by Meropius, a Christian philosopher from Tyre.\n\nThe Atlas is a mountain range across a northern stretch of Africa extending about through Morocco, Algeria, and Tunisia, or the Maghreb. The highest peak is Jbel Toubkal, with an elevation of in southwestern Morocco. The second highest mountain is the M'Goun of .\n\nThe population of the Atlas Mountains are mainly Berber tribes, speakers of an Afro-Asiatic language, including the Kabyle, Chaoui, Mozabite, Riffians, Chleuh, Sous, Tuareg and Zenata. Although nominally subject at times to Carthage, the Roman Empire, the Fatimid dynasty of Egypt (who originated in this area), the Ottoman Turks and more recently the French, for most of their history the tribes of the Atlas mountains have effectively been independent.\n\nMost Berbers are farmers, living in mountains relatively close to the Mediterranean coast, but the Tuareg and Zenaga of the southern Sahara are nomadic. The Berbers are sometimes treated as minorities in their countries, which are dominated by their Arabic-speaking relatives nearer to the coast.\n\nThe Pyrenees are a range of mountains in southwest Europe that form a natural border between France and Spain. Lower mountains extend to the west (Cantabrian Mountains) into Galicia. The inhabitants of the Pyrenees speak Spanish, French, Catalan (in Catalonia and Andorra) and the Basque language. A few speak the Occitan language (the Gascon and Languedocien dialects in France and the Aranese dialect in the Aran Valley), and Aragonese. In the Cantabrian Mountains Basque, Asturian and the Cantabrian dialect are spoken, apart from Spanish. In fact Cantabrians have traditionally also been referred to as \"montañeses\" (highlanders or mountain people)\n\nIt is thought that Basque people are a remnant of the early inhabitants of Western Europe. Basque tribes were already mentioned in Roman times by Strabo and Pliny, including the Vascones, the Aquitani and others. Their language is quite distinct from the Indo-European languages spoken in most of Europe, and possibly (although there is much debate) is related to Northeast Caucasian languages.\n\nThe Alps are one of the great mountain ranges of Europe. They stretch from Austria and Slovenia in the east, through Italy, Switzerland, Liechtenstein and Germany, to France in the west. The highest mountain is Mont Blanc, at , on the Italian–French border. The people of the Alps speak German, French, Italian, Slovene and Romansh. The German spoken in the German speaking part of Switzerland is a range of Swiss dialects.\n\nEarly Alpine tribes included the Helvetii and Alemanni. The Swiss Confederacy was an alliance among the valley communities of the central Alps founded by three cantons in 1291, later expanding to form the nucleus of modern Switzerland. Disciplined Swiss mercenaries armed with pikes gained a formidable reputation during the perennial European wars of the Middle Ages.\n\nIn the Swiss peasant war of 1653 the people rose up against the nobles and authorities. Although defeated, the revolt resulted in reforms that prevented the rise of absolutism as in other parts of Europe. The Swiss to this day have an unusual form of government where almost all major decisions are voted upon by the people.\n\nThe Alpine people have at times been subject in part or whole to neighboring countries, but many in the central Alps maintained the independence of their communities until 1848, when they combined into the federal state of Switzerland.\n\nThe Italian Alps also contain a diverse array of mountain cultures and valley towns. In the southern Alps in Northern Italy, the Camuni people are famous for their rock drawings in Valcamonica.\n\nThe Apennine mountains in Italy were home to pre-Roman cultures and Italic tribes for centuries, most notably the Etruscans, the mountainous peoples known as the Praetutii and Picentes, the Vestini, the Aequi, and Oscan-Umbrian tribes, including the Sabines, Umbri, Samnites, Marrucini, Hernici, Marsi, and Paeligni. The Apennines were notably the birthplace of pre-Roman Apennine culture, a cultural complex throughout the Apennines in central and southern Italy spanning much of the Bronze Age. The people of the Apennine culture were cattle herdsmen grazing their ungulates over the meadows and groves of mountainous central Italy, though their range was not confined to the hills. These tribes lived in small hamlets located in defensible places. On the move between summer pastures they built temporary camps or lived in caves and rock shelters. Their pottery has been found on the Capitoline Hill in Rome.\n\nThe Apennines stretch from modern day Liguria to Reggio Calabria, and mountain towns and communities can be found throughout the whole of the Apennine mountain range. However, certain regions, especially Abruzzo, Molise, eastern Lazio, Umbria, and Marche, are home to larger, more prototypical mountain cultures, and these cultures often remained largely isolated from the wider Italian peninsular culture until modern times. The region of Abruzzo in particular is well known for its mountain shepherd culture, with Teramo, Chieti, and L'Aquila being the most mountainous of Abruzzo's provinces. Abruzzo holds many of Italy's best-preserved and most beautiful medieval and Renaissance hill towns. The region is also the location of Gran Sasso and Abruzzo National Park, a haven for mountain-dwelling fauna such as the Abruzzo chamois, Italian wolf, and the Apennine brown bear.\n\nShepherd culture, which often thrives in mountainous regions, has significantly influenced the cultures of the Abruzzese peoples and other central Apennine peoples. For instance, bagpipes known as zampogna, traditionally made from sheep or goat skin or organs, are prominent in traditional Abruzzese and central and southern Apennine folk music. Sheep dishes such as arrosticini are also common in Abruzzese cuisine and central Apennine cuisine. The region is also home to mountain dogs and sheep dogs such as the Pastore Maremmano-Abruzzese, Italian Shepherd, and Abruzzese Mastiff, dog breeds used to protect sheep and livestock from predators in mountainous terrain. Further north, in Emilia-Romagna, the shepherds in the Apennines near Reggio Emilia developed the shepherd dog breed Cane Lupino del Gigante. The Piccolo Segugio dell'Appennino is another Apennine dog breed found throughout the mountain range.\n\nThe Scottish Highlands include the rugged and mountainous regions of Scotland north and west of the Highland Boundary Fault. They are inhabited by people of the Scottish Gaelic culture, although most now speak English. Traditionally the people lived by crofting, or small-scale farming, growing crops such as oats, barley and kale, and supplemented their diet with fish and red deer.\n\nOn festive occasions, the people eat haggis, made from a sheep's heart, liver and lungs, minced with onion, oatmeal, suet, spices, and salt, mixed with stock, and boiled in the animal's stomach for three hours. The haggis was immortalized by the lowland poet Robert Burns in his \"Address to a Haggis\".\n\nThe Scottish clans have a tradition of feuding among each other and resisting the authority of the central government based in the lowland city of Edinburgh. The Highland regiments went to war wearing tartan kilts accompanied by bagpipe players – a sight and sound calculated to strike fear into the hearts of their enemies.\n\nThe Balkan Mountains run 560 km from the Vrashka Chuka Peak on the border between Bulgaria and eastern Serbia eastward through central Bulgaria to Cape Emine on the Black Sea. The highest peaks of the Stara planina are in central Bulgaria. The highest peak is Botev (2,376 m).\n\nLying between Austria, Italy, Turkey and the Ukraine, the Balkans have had a turbulent history. At different times different parts of the Balkans were controlled by the Macedonian, Roman, Byzantine, Ottoman and Austrian empires.\n\nThe Balkans are home to people speaking Slavic languages (Serbs, Croats, Bulgarians and Macedonians), Romanian (related to Italian), Turkish, Greek, Albanian and other languages. Through its history many other ethnic groups with their own languages lived in the area, among them Thracians, Illyrians, Romans, Pechenegs, Cumans, Avars, Celts, Germans and various Germanic tribes. In addition to language, the people are divided by religion, adhering to the Roman Catholic, Greek Orthodox, and Muslim religions, among others.\n\nThe Caucasus Mountains is a range between the Black sea and the Caspian sea. It is home to Europe's highest mountain (Mount Elbrus). Modern countries of the Caucasus are Chechnya, Ingushetia, Dagestan, Adyghea, Kabardino-Balkaria, Karachay–Cherkessia, North Ossetia, Krasnodar Krai and Stavropol Krai, all parts of the Russian Federation, and the independent countries of Georgia (including the disputed Abkhazia and South Ossetia), Armenia, Azerbaijan and Turkey.\n\nThe people of these mountains have very diverse origins, with cultures influenced by periods of Turkic, Iranian and Russian control. Some, including the Georgians and Armenians, are mostly Christian, while others are Muslim in religion.\n\nThe Hindu Kush is a mountain range located between Afghanistan and Pakistan. It is the westernmost extension of the Pamir Mountains, the Karakoram Range, and is a sub-range of the Himalayas. Lower mountains including the Sulaiman Range extend south to the Arabian Sea.\n\nThe people of this region generally speak Indo-Iranian languages such as Pashto, Balochi, and languages of the Dardic and Nuristani groups. The Brahui people speak a Dravidian language. There are many distinct tribes. The history of the region has been turbulent, lying in the path of invasions from the Persians, the Greeks under Alexander the Great, the Arabs, Mughals and British. Given the rugged geography, central governments have generally had limited control over the tribes, a situation that persists to this day.\n\nThe Pamir Mountains are in Central Asia, formed by the junction of the Tian Shan, Karakoram, Kunlun, and Hindu Kush ranges. They are among the world's highest mountains and in Victorian times they were known as the 'Roof of the World'. The people of this region are mostly Tajik people, and speak Pamir languages, Tajik (closely related to Persian), Uzbek and Kyrgyz.\n\nThe Himalayas lie between the Indian subcontinent and the Tibetan Plateau, thrown up by the collision of two tectonic plates. They include world's highest peaks including Mount Everest and K2. The Himalayas stretch across Bhutan, China, India, Nepal, Pakistan and Afghanistan.\n\nPeople have filtered into the mountains from the northern plateau and from the southern plains and intermingled. They mostly speak Tibeto-Burman languages such as Gurung and Nepal Bhasa or Indo-Aryan languages such as Garhwali, Shina, Kashmiri, Dogri, Pahari (which means \"language of the mountain people\"), Kumaoni, Nepali and Asamiya. The Gurkha people of Nepal are well known for their history of bravery and strength, and still serve in foreign armies such as the British Army's Brigade of Gurkhas and the Indian Army's Gorkha regiments.\n\nFruit, vegetables and sheep flourish in the fertile valley of Kashmir. The people have evolved the unique and famous Kashmiri cuisine with Iranian, Indian and Turkic origins. While the stricter Hindus may restrict themselves to rice with vegetables, curd, milk and fresh fruit, the Muslims eat pilaf and mutton stuffed with spices, apricots stuffed with mutton, roast kid, fish and honey rice.\n\nFurther east, a typical Nepalese meal includes \"Dal\", a spicy lentil soup, served over \"bhat\" (boiled rice) with \"tarkari\" (curried vegetables) together with \"achar\" (pickles) or \"chutni\" (spicy condiment made from fresh ingredients). In the hills of Nepal, people enjoy their own kind of music, playing sarangi (a string instrument), madal (a hand drum) and flute. They also have many popular folk songs known as lok geet and lok dohari.\n\nThe mainland portion of Southeast Asia consists of Cambodia, Laos, Burma, Thailand and Vietnam, and Assam in India. The hills of this region include tribes of Tibeto-Burman peoples including the Akha, Bodo-Kachari, Lahu, Karen, Hmong Mien, Mizo (Mizo is under the sub-group of Zomi )and Lisu. It is thought that they drifted down into the area from Tibet.\n\nMany of the tribes are spread across more than one country; the Karen people live in Burma and Thailand, the Zomi people live in the Chin State, Sagaing Region of Burma and the Mizoram State, Manipur in India and Bangladesh. The Zomi are known as Chin-Kuki-Mizo. Their language is common or overlap approximately more than 40%. They live with three countries: India, Burma and Bangladesh. The Akha people live in small villages among the mountains of China, Laos, Burma, and northern Thailand. They often retain their traditional way of life but are under increasing pressure to do so. As minorities, they tend to be less well off than the majority of people in the countries where they live.\n\nInteresting theorisations about the hill people from this region include the notions of \"Southeast Asian Massif\" and \"Zomia\". Southeast Asian Massif was proposed by Laval University anthropologist Jean Michaud in 1997. This high region extending eastward from the Himalayas and the Tibetan Plateau covers more than 2.5 million square kilometers and comprises nearly one hundred million marginal peoples. This large area is inside the fringe of ten states and at the middle of none, stretching across the standard regional designations (South Asia, East Asia, and Southeast Asia); along with its ecological diversity and its relation to states, it arouses a lot of interest. It stands for an original entity of study, a type of international Appalachia, and a different way in which to study regions. Zomia is a geographical term coined in 2002 by historian Willem van Schendel of the University of Amsterdam to refer to the huge massif of mainland Southeast Asia that has historically been beyond the control of governments based in the population centers of the lowlands. The massif arose during the Alpine orogeny, when the African, Indian and Cimmerian Plates, and smaller Asian terrains, collided with that of Eurasia. The name is from Zomi, a term for highlander common to several related Tibeto-Burman languages spoken in the India-Bangladesh-Burma border area.\n\nThe exact boundaries of Zomia differ among scholars: all would include the highlands of north Indochina (north Vietnam and all Laos,) Thailand, the Shan State of northern Burma, and the mountains of Southwest China, others extend the region as far west as Tibet, north India, Pakistan, and Afghanistan. These areas share a common elevated, rugged terrain, and have been the home of ethnic minorities that have preserved their local cultures by residing far from state control and influence. Other scholars have used the term to discuss the similar ways that Southeast Asian governments have handled minority groups. Once such is Yale University Professor James C. Scott, who address the issue in his 2009 book \"The Art of Not Being Governed: An Anarchist History of Upland Southeast Asia\". From the Preface:\n[Hill tribes] seen from the valley kingdoms as “our living ancestors,” “what we were like before we discovered wet-rice cultivation, Buddhism, and civilization” [are on the contrary] best understood as runaway, fugitive, maroon communities who have, over the course of two millennia, been fleeing the oppressions of state-making projects in the valleys — slavery, conscription, taxes, corvée labor, epidemics, and warfare.\n\nA 2013 newspaper article from the Bangkok Post said that \"Nearly a million hill people and forest dwellers are still treated as outsiders, criminals even, since most live in protected forests. Viewed as national security threats, hundreds of thousands of them are refused citizenship although many are natives to the land.\"\n\nThe Central Highlands of Vietnam have traditionally been populated by a majority of hill tribes, all of whom are culturally distinct from the ethnic Vietnamese population of the lowlands. Collectively known as Montagnard (French for \"mountain people\"), they are also referred derogatorily as \"mọi\" (literally \"savage\" in Vietnamese). While Vietnamese expansion into the Central Highlands dates back to the French colonial period, large-scale settlement did not occur until after the Vietnam War. Today, the Montagnards form a minority in much of the Central Highlands, with the exception of a few areas.\n\nLocal Vietnamese assisted French colonial armies in suppressing rebellions among the hill tribes during the early 20th century, with some calls for completely ridding the Central Highlands of the \"uncivilized\" hill tribes for large-scale Vietnamese and French settlement. Political and economic barriers prevented hill tribes from obtaining high levels of education and entering commerce, although outright legal discrimination against Montagnards was made illegal by the colonial government. During the Vietnam War, American Special Forces trained many hill tribes in Vietnam to stop Viet Cong activity in the region and to prevent them from aligning with the Viet Cong.\n\nNew Guinea, located just north of Australia, is the world's second-largest island. A central east-west mountain range dominates the geography of New Guinea and contains the highest mountains in Oceania, rising up to 4884 m high. The island is populated by very nearly a thousand different tribal groups and a near-equivalent number of separate languages, which makes New Guinea the most linguistically diverse area in the world. They fall into one of two groups, speakers of the Papuan languages and later arrivals who speak Austronesian languages.\n\nThe gardens of the New Guinea Highlands are ancient, intensive permacultures, adapted to high population densities, very high rainfalls (as high as 10,000 mm/yr (400 in/yr)), earthquakes, hilly land, and occasional frost. The people use complex mulches, crop rotations and tillages on terraces with complex irrigation systems. Native gardeners are as or more successful than most scientific farmers in raising certain crops. Early garden crops – many of which are indigenous – included sugarcane, Pacific bananas, yams, and taro, while sago and pandanus were two commonly exploited native forest crops. Today's staples – sweet potatoes and pigs – are later arrivals.\n\nThe people of New Guinea were traditionally warlike, with constant feuds over territory, although violence has greatly reduced in recent years.\n\nThe Appalachian Mountains are a vast system of mountains in eastern North America. Before the Europeans reached the area around 1650, the mountains were inhabited by Algonquian people; speakers of Iroquoian languages such as the Cherokee; and Siouan people such as the Occaneechi, Tutelo and Monacan. Upon European arrival to the mountains, the American Indians were driven west through a series of laws, wars, and treaties. Many of these early inhabitants also died from European diseases (e.g. smallpox). Few of the early inhabitants remain, but surviving communities include the Ramapough Mountain Indians and the Eastern Band of Cherokee Indians.\n\nEuropean migration was primarily led by the Scotch-Irish, a group of people from northern England, the Scottish Lowlands (versus the Scottish Highlands) and Ulster, Ireland. These yeoman farmers were romanticized for their independence, ruggedness, and self-sufficiency, perhaps best represented by the frontiersman Daniel Boone. The Appalachian settlers often had a hostile relationship with their lowland coastal neighbors, sometimes culminating in revolts like the Regulator Movement and the Whiskey Rebellion. During the American Civil War, tensions were so high between hill peoples and lowland peoples that 55 Appalachian counties broke away from the state of Virginia to form West Virginia.\n\nFollowing the war, negative stereotypes of the region and its people began to develop. The Hatfield–McCoy feud (1878–1891) was widely reported and encouraged the stereotype of violent, vengeful and impoverished people. The 1972 film \"Deliverance\" depicted the hill people as backward, hostile to outsiders, and potentially inbred (a stereotype of the \"Hillbilly\" culture). Following the creation of the Appalachian Regional Commission in 1965, the economic situation of the region has improved dramatically, and some aspects of the hillbilly stereotype have disappeared. However, the people of Appalachia are still commonly viewed as uneducated and unrefined.\n\nThe Rocky Mountains stretch more than from northernmost British Columbia, in Canada, to New Mexico, in the United States. The range's highest peak is Mount Elbert in Colorado at above sea level.\n\nAs the Europeans continued to move west, they took most of the lands of the people who lived in the Rockies, forcing the Ute, Navajo, Hopi, Apache, Yaqui and other tribes to assimilate or move onto reservations. Some adapted to farming or sheep herding, but they are the most impoverished of all ethnic groups in the USA.\n\nThe Ancient Pueblo peoples who lived in the present-day Four Corners area of the Southwestern United States were noted for their distinctive pottery and dwelling construction styles. The cultural group is often referred to as the Anasazi. Some of their unusual cliff dwellings still stand today.\n\nIshi (c. 1860 – March 25, 1916) was presumed to be the last member of the Yahi people, the last surviving group of the Yana people in the U.S. state of California. What is now the Western United States has long been home to indigenous peoples of the Great Basin, who live mainly in desert regions and share characteristic with hill people.\n\nThe American Cordillera in Mexico includes the Sierra Madre Occidental, Sierra Madre Oriental and Sierra Madre del Sur. The mountain valleys have a temperate climate and fertile soil, and were the cradles of Mesoamerican civilizations that flourished long before Europeans discovered the area, including the Olmec, Teotihuacán, Toltec, Aztec, Zapotec and Mixtec cultures. The bold patterns and deep saturated colors of the Mesoamerican artists are echoed in modern Mexican art and architecture.\n\nAfter the Spanish invasion of 1519, led by Hernán Cortés, most people eventually adopted the Spanish language, but there remain many groups who retain their original languages and live traditional ways of life. These include the Nahuatl, Zapotec, Mixtec, Otomí, Tzotzil, Tzeltal, Mazahua, Mazateco, Chinantec, Purépecha, Mixe and Tlapanec.\n\nThe traditional Mexican diet is based on maize, beans, chili peppers, tomatoes, cactus and other native plants, supplemented by iguana, rattlesnake, fish and venison. Mexican cuisine today is one of the most rich and varied in the world. Before the Spanish came, the inhabitants of the land used drums, flutes, maracas, seashells and voices to make music and dances. This ancient music is still played in some parts of Mexico.\n\nThe Andes form the world's longest exposed mountain range, a continuous chain of highland along the western coast of South America. The range is over long, wide (widest between 18° to 20°S latitude), and of an average height of about .\n\nBefore the Spanish invasion led by Francisco Pizarro in 1532 the Andes were primarily inhabited by Quechua peoples such as the Inca, Chancas, Huancas and Cañari in the north, and Aymaras further south. Their economy was based on the potato crop, which originated in the area, and the llama and alpaca, used for their wool and meat. Unlike most mountain people, the inhabitants of the Andes appear to have been generally unwarlike.\n\n"}
{"id": "57686971", "url": "https://en.wikipedia.org/wiki?curid=57686971", "title": "Ice Sheet Mass Balance Inter-comparison Exercise", "text": "Ice Sheet Mass Balance Inter-comparison Exercise\n\nThe Ice Sheet Mass Balance Inter-comparison Exercise (IMBIE) is an international scientific collaboration attempting to improve estimates of the amounts of ice contained in ice sheets around the world and of their contribution to sea level rise and to publish data and analyses concerning these subjects. IMBIE was founded in 2011 and is a collaboration between the European Space Agency (ESA) and the National Aeronautics and Space Administration (NASA) of the United States.\n\n"}
{"id": "14122398", "url": "https://en.wikipedia.org/wiki?curid=14122398", "title": "Informal logic", "text": "Informal logic\n\nInformal logic, intuitively, refers to the principles of logic and logical thought outside of a formal setting. However, perhaps because of the \"informal\" in the title, the precise definition of \"informal logic\" is a matter of some dispute. Ralph H. Johnson and J. Anthony Blair define informal logic as \"a branch of logic whose task is to develop non-formal standards, criteria, procedures for the analysis, interpretation, evaluation, criticism and construction of argumentation.\" This definition reflects what had been implicit in their practice and what others were doing in their informal logic texts.\n\nInformal logic is associated with (informal) fallacies, critical thinking, the thinking skills movement and the interdisciplinary inquiry known as argumentation theory. Frans H. van Eemeren writes that the label \"informal logic\" covers a \"collection of normative approaches to the study of reasoning in ordinary language that remain closer to the practice of argumentation than formal logic.\"\n\nInformal logic as a distinguished enterprise under this name emerged roughly in the late 1970s as a sub-field of philosophy. The naming of the field was preceded by the appearance of a number of textbooks that rejected the symbolic approach to logic on pedagogical grounds as inappropriate and unhelpful for introductory textbooks on logic for a general audience, for example Howard Kahane's \"Logic and Contemporary Rhetoric\", subtitled \"The Use of Reason in Everyday Life\", first published in 1971. Kahane's textbook was described on the notice of his death in the \"Proceedings And Addresses of the American Philosophical Association\" (2002) as \"a text in informal logic, [that] was intended to enable students to cope with the misleading rhetoric one frequently finds in the media and in political discourse. It was organized around a discussion of fallacies, and was meant to be a practical instrument for dealing with the problems of everyday life. [It has] ... gone through many editions; [it is] ... still in print; and the thousands upon thousands of students who have taken courses in which his text [was] ... used can thank Howard for contributing to their ability to dissect arguments and avoid the deceptions of deceitful rhetoric. He tried to put into practice the ideal of discourse that aims at truth rather than merely at persuasion. (Hausman et al. 2002)\" Other textbooks from the era taking this approach were Michael Scriven's \"Reasoning\" (Edgepress, 1976) and \"Logical Self-Defense\" by Ralph Johnson and J. Anthony Blair, first published in 1977. Earlier precursors in this tradition can be considered Monroe Beardsley's \"Practical Logic\" (1950) and Stephen Toulmin's \"The Uses of Argument\" (1958).\n\nThe field perhaps became recognized under its current name with the \"First International Symposium on Informal Logic\" held in 1978. Although initially motivated by a new pedagogical approach to undergraduate logic textbooks, the scope of the field was basically defined by a list of 13 problems and issues which Blair and Johnson included as an appendix to their keynote address at this symposium:\n\n\nDavid Hitchcock argues that the naming of the field was unfortunate, and that \"philosophy of argument\" would have been more appropriate. He argues that more undergraduate students in North America study informal logic than any other branch of philosophy, but that as of 2003 informal logic (or philosophy of argument) was not recognized as separate sub-field by the World Congress of Philosophy. Frans H. van Eemeren wrote that \"informal logic\" is mainly an approach to argumentation advanced by a group of US and Canadian philosophers and largely based on the previous works of Stephen Toulmin and to a lesser extent those of Chaïm Perelman.\n\nAlongside the symposia, since 1983 the journal \"Informal Logic\" has been the publication of record of the field, with Blair and Johnson as initial editors, with the editorial board now including two other colleagues from the University of Windsor—Christopher Tindale and Hans V. Hansen. Other journals that regularly publish articles on informal logic include \"Argumentation\" (founded in 1986), \"Philosophy and Rhetoric\", \"Argumentation and Advocacy\" (the journal of the American Forensic Association), and \"\" (founded in 1988).\n\nJohnson and Blair (2000) proposed the following definition: \"Informal logic designates that branch of logic whose task is to develop non-formal standards, criteria, procedures for the analysis, interpretation, evaluation, critique and construction of argumentation in everyday discourse.\" Their meaning of non-formal is taken from Barth and Krabbe (1982), which is explained below.\n\nTo understand the definition above, one must understand \"informal\" which takes its meaning in contrast to its counterpart \"formal.\" (This point was not made for a very long time, hence the nature of informal logic remained opaque, even to those involved in it, for a period of time.) Here it is helpful to have recourse to Barth and Krabbe (1982:14f) where they distinguish three senses of the term \"form.\" By \"form,\" Barth and Krabbe mean the sense of the term which derives from the Platonic idea of form—the ultimate metaphysical unit. Barth and Krabbe claim that most traditional logic is formal in this sense. That is, syllogistic logic is a logic of terms where the terms could naturally be understood as place-holders for Platonic (or Aristotelian) forms. In this first sense of \"form,\" almost all logic is informal (not-formal). Understanding informal logic this way would be much too broad to be useful.\n\nBy \"form,\" Barth and Krabbe mean the form of sentences and statements as these are understood in modern systems of logic. Here validity is the focus: if the premises are true, the conclusion must then also be true. Now validity has to do with the logical form of the statement that makes up the argument. In this sense of \"formal,\" most modern and contemporary logic is \"formal.\" That is, such logics canonize the notion of logical form, and the notion of validity plays the central normative role. In this second sense of form, informal logic is not-formal, because it abandons the notion of logical form as the key to understanding the structure of arguments, and likewise retires validity as normative for the purposes of the evaluation of argument. It seems to many that validity is too stringent a requirement, that there are good arguments in which the conclusion is supported by the premises even though it does not follow necessarily from them (as validity requires). An argument in which the conclusion is thought to be \"beyond reasonable doubt, given the premises\" is sufficient in law to cause a person to be sentenced to death, even though it does not meet the standard of logical validity. This type of argument, based on accumulation of evidence rather than pure deduction, is called a conductive argument.\n\nBy \"form,\" Barth and Krabbe mean to refer to \"procedures which are somehow regulated or regimented, which take place according to some set of rules.\" Barth and Krabbe say that \"we do not defend formality of all kinds and under all circumstances.\" Rather \"we defend the thesis that verbal dialectics must have a certain form (i.e., must proceed according to certain rules) in order that one can speak of the discussion as being won or lost\" (19). In this third sense of \"form\", informal logic can be formal, for there is nothing in the informal logic enterprise that stands opposed to the idea that argumentative discourse should be subject to norms, i.e., subject to rules, criteria, standards or procedures. Informal logic does present standards for the evaluation of argument, procedures for detecting missing premises etc.\n\nJohnson and Blair (2000) noticed a limitation of their own definition, particularly with respect to \"everyday discourse\", which could indicate that it does not seek to understand specialized, domain-specific arguments made in natural languages. Consequently, they have argued that the crucial divide is between arguments made in formal languages and those made in natural languages.\n\nFisher and Scriven (1997) proposed a more encompassing definition, seeing informal logic as \"the discipline which studies the practice of critical thinking and provides its intellectual spine\". By \"critical thinking\" they understand \"skilled and active interpretation and evaluation of observations and communications, information and argumentation.\"\n\nSome hold the view that informal logic is not a branch or subdiscipline of logic, or even the view that there cannot be such a thing as informal logic. Massey criticizes informal logic on the grounds that it has no theory underpinning it. Informal logic, he says, requires detailed classification schemes to organize it, which in other disciplines is provided by the underlying theory. He maintains that there is no method of establishing the invalidity of an argument aside from the formal method, and that the study of fallacies may be of more interest to other disciplines, like psychology, than to philosophy and logic.\n\nSince the 1980s, informal logic has been partnered and even equated, in the minds of many, with critical thinking. The precise definition of \"critical thinking\" is a subject of much dispute. Critical thinking, as defined by Johnson, is the evaluation of an intellectual product (an argument, an explanation, a theory) in terms of its strengths and weaknesses. While critical thinking will include evaluation of arguments and hence require skills of argumentation including informal logic, critical thinking requires additional abilities not supplied by informal logic, such as the ability to obtain and assess information and to clarify meaning. Also, many believe that critical thinking requires certain dispositions. Understood in this way, \"critical thinking\" is a broad term for the attitudes and skills that are involved in analyzing and evaluating arguments. The critical thinking movement promotes critical thinking as an educational ideal. The movement emerged with great force in the '80s in North America as part of an ongoing critique of education as regards the thinking skills not being taught.\n\nThe social, communicative practice of argumentation can and should be distinguished from implication (or entailment)—a relationship between propositions; and from inference—a mental activity typically thought of as the drawing of a conclusion from premises. Informal logic may thus be said to be a logic of argumentation, as distinguished from implication and inference.\n\nArgumentation theory is interdisciplinary in the sense that no one discipline will be able to provide a complete account. A full appreciation of argumentation requires insights from logic (both formal and informal), rhetoric, communication theory, linguistics, psychology, and, increasingly, computer science. Since the 1970s, there has been significant agreement that there are three basic approaches to argumentation theory: the logical, the rhetorical and the dialectical. According to Wenzel, the logical approach deals with the product, the dialectical with the process, and the rhetorical with the procedure. Thus, informal logic is one contributor to this inquiry, being most especially concerned with the norms of argument.\n\n\n\nThe open access issue 20(2) of \"Informal Logic\" from year 2000 groups a number of papers addressing foundational issues, based on the Panel on Informal Logic that was held at the 1998 World Congress of Philosophy, including:\n\n"}
{"id": "30608068", "url": "https://en.wikipedia.org/wiki?curid=30608068", "title": "Ithaka Science Center", "text": "Ithaka Science Center\n\nIthaka Science Center, a hands-on science education center in Tegelen in the Netherlands was founded in 2008 as a non-profit organization by Vaggelis E. Fragiadakis and Margriet van Tulder and funded as a public-private partnership. Its goal: to present interactive exhibits, programs, public lectures, and other events for schools and the general public. \n\nThe science center location at Raadhuislaan 11, in Tegelen, the Netherlands, is a historic building that served as the city hall for Tegelen. The historic façade and many internal architectural details have been preserved.\n\nA major feature of the center is a mixture of permanent and temporary exhibits that demonstrate scientific and technical principles. Exhibits are labeled in Dutch, English and in German, as the center is within 2 km of the border with Germany, and was designed to host visitors and schools from both countries. The first series of exhibits dealt with energy, light, and gravity. The center integrated science exhibits with art displays inspired by scientific themes. The art shown changes periodically.\n\nBefore the center moved into the former townhall, Ithaka created science education programs and conferences, such as energy exhibitions, science clubs, school activities, master classes, and other special events.\n\nFor classroom teachers, the Ithaka Science Center has assembled and provides science activity kits which included instruments, materials and instructions for topics including electricity, air, mechanics, magnetism, properties of solids, liquids and gases.\n\nThe center has hosted numerous speakers on science, technology, and science education. Among them have been Nobel Laureates George Smoot, Douglas Osheroff, and Eric Maskin.\n\nThe center has online activities for children, and had produced a children's science exploration television program.\n\n"}
{"id": "17186415", "url": "https://en.wikipedia.org/wiki?curid=17186415", "title": "Jacques Huber", "text": "Jacques Huber\n\nJacques Huber (13 October 1867 Schleitheim – 18 February 1914 Belém) was a Swiss-Brazilian botanist who did pioneering work on the flora of the Amazon from 1895 until his death. He created and organized the herbarium and arboretum in Belém, Brazil, and was director of the Museu Paraense Emílio Goeldi from 1907 until he died.\n\n\n"}
{"id": "4672204", "url": "https://en.wikipedia.org/wiki?curid=4672204", "title": "Kaye effect", "text": "Kaye effect\n\nThe Kaye effect is a property of complex liquids which was first described by the British engineer Alan Kaye in 1963.\n\nWhile pouring one viscous mixture of an organic liquid onto a surface, the surface suddenly spouted an upcoming jet of liquid which merged with the downgoing one.\n\nThis phenomenon has since been discovered to be common in all shear-thinning liquids (liquids which thin under shear stress). Common household liquids with this property are liquid hand soaps, shampoos and non-drip paint. The effect usually goes unnoticed, however, because it seldom lasts more than about 300 milliseconds. The effect can be sustained by pouring the liquid onto a slanted surface, preventing the outgoing jet from intersecting the downward one (which tends to end the effect).\n\nIt is thought to occur when the downgoing stream \"slips\" off the pile it is forming, and due to a thin layer of shear-thinned liquid acting as a lubricant, does not combine with the pile. When the slipping stream reaches a dimple in the pile, it will shoot off it like a ramp, creating the effect.\n\n"}
{"id": "1955923", "url": "https://en.wikipedia.org/wiki?curid=1955923", "title": "L5 Society", "text": "L5 Society\n\nThe L5 Society was founded in 1975 by Carolyn Meinel and Keith Henson to promote the space colony ideas of Gerard K. O'Neill.\n\nIn 1987 the L5 Society merged with the National Space Institute to form the National Space Society.\n\nThe name comes from the and Lagrangian points in the Earth–Moon system proposed as locations for the huge rotating space habitats that O'Neill envisioned. and are points of stable gravitational equilibrium located along the path of the moon's orbit, 60 degrees ahead or behind it.\n\nAn object placed in orbit around (or ) will remain there indefinitely without having to expend fuel to keep its position, whereas an object placed at , or (all points of unstable equilibrium) may have to expend fuel if it drifts off the point.\n\nO'Neill's first published paper on the subject, \"The Colonization of Space\", appeared in the magazine \"Physics Today\" in September 1974. A number of people who later became leaders of the Society got their first exposure to the idea from this article. Among these were a couple from Tucson, Arizona, Carolyn Meinel and Keith Henson. The Hensons corresponded with O'Neill and were invited to present a paper on \"Closed Ecosystems of High Agricultural Yield\" at the 1975 Princeton Conference on Space Manufacturing Facilities, which was organized by O'Neill.\n\nAt this conference, O'Neill merged the Solar Power Satellite (SPS) ideas of Peter Glaser with his space habitat concepts.\n\nThe Hensons incorporated the Society in August 1975, and sent its first 4-page newsletter in September to a sign up list from the conference and O'Neill's mailing list. The first newsletter included a letter of support from Morris Udall (then a contender for US president) and said \"our clearly stated long range goal will be to disband the Society in a mass meeting at .\"\n\nThe peak of 's influence was the defeat of the Moon Treaty in the U.S. Senate in 1980 (\"... L-5 took on the biggest political fight of its short life, and won\"). Specifically, Society activists campaigned for awareness of the provisions against any form of sovereignty or private property in outer space that would make space colonization impossible and the provisions against any alteration of the environment of any celestial body prohibiting terraforming. Leight \"Ratiner [a Washington lawyer/lobbyist] played the key role in the lobbying effort, although he had energetic help from L-5 activists, notably Eric Drexler and Chris Peterson.\"\n\nAlthough economic analysis indicated the SPS/space colony concept had merit, it foundered on short political and economic horizons and the fact that the transport cost to space was about 300 times too high for individuals to fund when compared to the Plymouth Rock and Mormon colonies.\n\nIn 1986, the Society, which had grown to about 10,000 members, merged with the 25,000 member National Space Institute, to form the present-day National Space Society. The National Space Institute had been founded in 1972 by Wernher von Braun, the former German rocket engineer of the WW II Nazi V-2 rocket/ballistic missile program, and of NASA's Marshall Space Flight Center and Project Apollo program manager. \n\nWhile the Society failed to achieve the goal of human settlements in space, it served as a focal point for many of the people who later became known in fields such as nanotechnology, memetics, extropianism, cryonics, transhumanism, artificial intelligence, and tether propulsion, such as K. Eric Drexler, Robert Forward, and Hans Moravec.\n\nThe \"L5 News\" was the newsletter of the Society reporting on space habitat development and related space issues. The News was published from September 1975 until April 1987, when the merger with the National Space Institute was completed and the newly formed National Space Society began publication of its own magazine, \"Ad Astra\".\n\n\n"}
{"id": "58405077", "url": "https://en.wikipedia.org/wiki?curid=58405077", "title": "Late Elongated Hypocotyl", "text": "Late Elongated Hypocotyl\n\nThe Late Elongated Hypocotyl gene (LHY), is an oscillating gene found in plants that functions as part of their circadian clock. LHY encodes components of mutually regulatory negative feedback loops with Circadian Clock Associated 1 (CCA1) in which overexpression of either results in dampening of both of their expression. This negative feedback loop affects the rhythmicity of multiple outputs creating a daytime protein complex. LHY was one of the first genes identified in the plant clock, along with TOC1 and CCA1. LHY and CCA1 have similar patterns of expression, which is capable of being induced by light. Single loss-of-function mutants in both genes result in seemingly identical phenotypes, but LHY cannot fully rescue the rhythm when CCA1 is absent, indicating that they may only be partially functionally redundant. Under constant light conditions, CCA1 and LHY double loss-of-function mutants fail to maintain rhythms in clock-controlled RNAs.\n\nThe circadian clock in plants has completely different components to those in the animal, fungus or bacterial clocks. The plant clock does have a conceptual similarity to the animal clock in that it consists of a series of interlocking transcriptional feedback loops. The genes involved in the clock show their peak expression at a fixed time of day. The peak expression of the CCA1 and LHY genes occurs at dawn, and the peak expression of the TOC1 gene occurs roughly at dusk. CCA1/LHY and TOC1 proteins repress the expression of each others genes. The result is that as CCA1/LHY protein levels start to reduce after dawn, it releases the repression on the TOC1 gene, allowing TOC1 expression and TOC1 protein levels to increase. As TOC1 protein levels increase, it further suppresses the expression of the CCA1 and LHY genes. The opposite of this sequence occurs overnight to re-establish the peak expression of CCA1 and LHY genes at dawn. \n\nCCA1 is generally a more significant component of this oscillator. Light induces its transcription, and mRNA levels peak at dawn along with LHY. CCA1 and LHY associate to inhibit transcription of the Evening Complex (EC) proteins: ELF4, ELF3 and LUX, which suppresses their accumulation until dusk when LHY and CCA1 protein levels are at their lowest. Four primary pseudo-response regulator proteins (PRR9, PRR7, PRR5 and TOC1/PRR1) perform the majority of interactions with other proteins within the circadian oscillator, and another (PRR3) that has limited function. These genes are all paralogs of each other, and all repress the transcription of CCA1 and LHY at various times throughout the day.\n\nPlants that have lost function of LHY and CCA1 lose the ability to stably maintain circadian rhythm and other output phenomena. In one study, such plants showed photoperiod- insensitive early flowering under long- day (16 hours of light/ 8 hours of dark) conditions and short day (8 hours of light, 16 hours of dark conditions), and arrhythmicity under constant light conditions. However they retain some circadian function in light/dark cycles, showing that \"Arabidopsis\" circadian clock is not completely dependent on CCA1 and LHY activity. Plants with non-functioning LHY and CCA1 show a wavy leaf phenotype in constant light conditions. Mutants also have increased vascular pattern complexity in their leaves, with more areoles, branch points and free ends than wild-type \"Arabidopsis\".\n\nThe function of LHY was initially demonstrated by a group in the Steve Kay lab, including Andrew Millar.\n"}
{"id": "2679928", "url": "https://en.wikipedia.org/wiki?curid=2679928", "title": "List of compounds with carbon number 13", "text": "List of compounds with carbon number 13\n\nThis is a partial list of molecules that contain 13 carbon atoms.\n\n"}
{"id": "2679893", "url": "https://en.wikipedia.org/wiki?curid=2679893", "title": "List of compounds with carbon number 14", "text": "List of compounds with carbon number 14\n\nThis is a partial list of molecules that contain 14 carbon atoms.\n\n"}
{"id": "2680894", "url": "https://en.wikipedia.org/wiki?curid=2680894", "title": "List of compounds with carbon number 3", "text": "List of compounds with carbon number 3\n\nThis is a partial list of molecules that contain 3 carbon atoms.\n\n"}
{"id": "5612136", "url": "https://en.wikipedia.org/wiki?curid=5612136", "title": "List of object-relational mapping software", "text": "List of object-relational mapping software\n\nThis is a list of well-known object-relational mapping software. It is not up-to-date or all-inclusive.\n\nAthena Framework, open source Flex ORM, native support for multitenancy\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "348624", "url": "https://en.wikipedia.org/wiki?curid=348624", "title": "List of order theory topics", "text": "List of order theory topics\n\nOrder theory is a branch of mathematics that studies various kinds of objects (often binary relations) that capture the intuitive notion of ordering, providing a framework for saying when one thing is \"less than\" or \"precedes\" another.\n\nAn alphabetical list of many notions of order theory can be found in the order theory glossary. See also inequality, extreme value and mathematical optimization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "361207", "url": "https://en.wikipedia.org/wiki?curid=361207", "title": "List of ornithologists", "text": "List of ornithologists\n\nThis is a list of ornithologists who have articles, in alphabetical order by surname. See also .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22122214", "url": "https://en.wikipedia.org/wiki?curid=22122214", "title": "Method (patent)", "text": "Method (patent)\n\nIn United States patent law, a method, also called \"process\", is one of the four principal categories of things that may be patented through \"utility patents\". The other three are a machine, an article of manufacture (also termed a \"manufacture\"), and a composition of matter.\n\nIn that context, a method is a series of steps for performing a function or accomplishing a result. While the terms \"method\" and \"process\" are largely interchangeable, \"method\" usually refers to a way to use a product to accomplish a given result, and \"process\" usually refers to a series of steps in manufacture. Thus, one might speak about a method for curing headaches that comprises the administration of a therapeutically effective dose of aspirin or speak about a process for making soap or candles.\n\nNot all methods, in the dictionary sense, are methods for purposes of United States patent law. The case law \"forecloses a purely literal reading of § 101.\" The concept is elaborated in the article machine-or-transformation test.\n\nA method patent claim can only be infringed when a single person or entity (including contractually obligated agents, if any) practices all of the claimed steps. Neither a physical device, such as a product that can be used to practice the method, nor instructions for practicing the method, are infringing until they are used by a single person to perform all the steps together. A potential exception to this rule for indirect infringement was implicit in the circuit court of appeal's ruling in Akamai Tech. v. Limelight Networks (Fed. Cir. 2012). The U.S. Supreme Court reversed the circuit court's ruling on Monday, June 2, 2014 (docket number 12-786), holding that the circuit court had misread patent law to reach its decision. The court noted that the statute explicitly defines a method patent to cover only the entirety of the method, and doesn't confer any rights in the individual steps that make up the method.\n\nThe European Patent Convention does not mention method patents (called process patents) so prominently, and the same applies to the TRIPS Agreement. The prime characteristic of process patents in these treaties is that \"the protection conferred by the patent shall extend to the products directly obtained by such process\". Art. 28(1)(b) TRIPS provides a similar rule. This shows the historical background of process patents in chemistry, where there was a need to protect new processes to manufacture known substances.\n\n"}
{"id": "948587", "url": "https://en.wikipedia.org/wiki?curid=948587", "title": "Operation Buster–Jangle", "text": "Operation Buster–Jangle\n\nOperation Buster–Jangle was a series of seven (six atmospheric, one cratering) nuclear weapons tests conducted by the United States in late 1951 at the Nevada Test Site. \"Buster-Jangle\" was the first joint test program between the DOD (Operation \"Buster\") and Los Alamos National Laboratories (Operation \"Jangle\"). As part of Operation \"Buster\", 6,500 troops were involved in the Operation Desert Rock I, II, and III exercises in conjunction with the tests. The last two tests, Operation \"Jangle\", evaluated the cratering effects of low-yield nuclear devices. This series preceded \"Operation Tumbler-Snapper\" and followed \"Operation Greenhouse\".\n\nFour U.S. Army units took part in the Operation Buster–Jangle \"Dog\" test for combat maneuvers after the detonation of a nuclear weapon took place. These units consisted of:\nPersonnel were instructed to create foxholes, construct gun emplacements and bunkers in a defensive position 11 km south of the detonation area. After the nuclear bomb was detonated, the troops were ordered to move forward towards the affected area. While traveling closer to ground zero, troops witnessed the nuclear weapon's effects on the fortifications that were placed in the location in preparation for the tests. The ground troops got as close as 900 meters from ground zero before they were instructed to move out of the area. The Human Resources Research Office was tasked with gathering data on the psychological experiences of the troops after witnessing such a detonation and moving closer towards the affected area.\n\nFor the Operation Buster–Jangle series of tests, the Atomic Energy Commission created a set of criteria that must be followed if exposing armed forces, or civilians to the harmful effects of ionizing radiation.\nA majority of the personnel that took part in the exercise received around 3 R, with pilots receiving an average of 3.9 R. These estimates vary given the differing data provided by the Department of Defense over the years.\n"}
{"id": "17322397", "url": "https://en.wikipedia.org/wiki?curid=17322397", "title": "Otto Porsch", "text": "Otto Porsch\n\nOtto Porsch 12 September 1875 – 2 January 1959) was an Austrian biologist. \n\nAfter his Ph.D he worked with Gottlieb Haberlandt in Graz and did his habilitation with Richard Wettstein in Vienna. He became first director of the botanical garden in Czernowitz (now Chernivtsi, Ukraine) and later professor at the University of Czernowitz (now Chernivtsi University). Porsch became director of the botanical institute in Vienna in 1920. He retired in 1945 and died in 1959.\n\n"}
{"id": "350204", "url": "https://en.wikipedia.org/wiki?curid=350204", "title": "Outline of combinatorics", "text": "Outline of combinatorics\n\nCombinatorics is a branch of mathematics concerning the study of finite or countable discrete structures.\n\n\n\n\nHistory of combinatorics\n\n\n\n\n\n\n\n\n\n"}
{"id": "24241", "url": "https://en.wikipedia.org/wiki?curid=24241", "title": "Protoscience", "text": "Protoscience\n\nIn the philosophy of science, there are several definitions of protoscience.\n\nIts simplest meaning (most closely reflecting its roots of \"proto-\" + \"science\") involves the earliest eras of the history of science, when the scientific method was still nascent. Thus, in the late 17th century and early 18th century, Isaac Newton contributed to the dawning sciences of chemistry and physics, even though he was also an alchemist who sought chrysopoeia in various ways including some that were unscientific. \n\nAnother meaning extends this idea into the present, with protoscience being an emerging field of study which is still not completely scientific, but later becomes a proper science. An example of it would the general theory of relativity, which started being a protoscience (a theoretical work which had not been tested), but later was experimentally verified and became fully scientitific. Protoscience in this sense is distinguished from pseudoscience by a genuine willingness to be changed through new evidence, as opposed to having a theory that can always find a way to rationalize a predetermined belief.\n\nPhilosopher of chemistry Jaap Brakel defines protoscience as \"the study of \"normative\" criteria for the use of experimental technology in science.\"\nThomas Kuhn said that protosciences \"generate testable conclusions but ... nevertheless resemble philosophy and the arts rather than the established sciences in their developmental patterns. I think, for example, of fields like chemistry and electricity before the mid-18th century, of the study of heredity and phylogeny before the mid-nineteenth, or of many of the social sciences today.\" While noting that they meet the demarcation criteria of falsifiability from Popper, he questions whether the discussion in protoscience fields \"result[s] in clear-cut progress\". Kuhn concluded that protoscience, \"like the arts and philosophy, lack some element which, in the mature sciences, permits the more obvious forms of progress. It is not, however, anything that a methodological prescription can provide. ... I claim no therapy to assist the transformation of a proto-science to a science, nor do I suppose anything of this sort is to be had\".\n\nThe term \"prescientific\" means at root \"relating to an era before science existed\". For example, traditional medicine existed for thousands of years before medical science did, and thus many aspects of it can be described as prescientific. In a related but somewhat different sense, protoscientific topics (such as the alchemy of Newton's day) can be called prescientific, in which case the \"proto-\" and \"pre-\" labels can function more or less synonymously (the latter focusing more sharply on the idea that nothing but science is science).\n\nCompare fringe science, which is considered highly speculative or even strongly refuted. Some protosciences go on to become an accepted part of mainstream science.\n\n\n\n"}
{"id": "58793134", "url": "https://en.wikipedia.org/wiki?curid=58793134", "title": "RV Sprightly", "text": "RV Sprightly\n\nRV Sprightly was a 42m research vessel owned by the Australian Commonwealth Scientific Research Organisation (CSIRO). \"Sprightly\" originally served as a salvage tug in the North Atlantic in World War II. Following the war it was purchased by the CSIRO where it spent 40 years on scientific duties before being retired and replaced by the \"RV Franklin\".\n"}
{"id": "58152972", "url": "https://en.wikipedia.org/wiki?curid=58152972", "title": "Ru Chih C Huang", "text": "Ru Chih C Huang\n\nRu Chih Huang (born 1932) is a biology professor at Johns Hopkins University. She is a biochemist who worked with James F. Bonner and Doug Fambrough to characterize and discern functions for nuclear histones in the early 1960s when the field lacked a consensus on types and functions of individual histone proteins. Later she made discoveries about the molecular biology of cancer and of viral gene regulation.\n\nRu Chih Chow was born April 2, 1932 in Nanjing, China (then called Nanking). She moved to the US in 1954. She was inspired to go into science by reading a biography of Marie Curie. She received the BS from National Taiwan University in 1953, the MS from Virginia Polytechnical Institute in 1956, and the PhD from Ohio State University in 1960. She was a postdoctoral fellow with James F. Bonner at California Institute of Technology from 1960- 1965.\n\nRu Chih Huang joined the faculty at Johns Hopkins University in 1971. She was the first female science professor hired there. Huang was an Assistant Professor from 1965-1971, an Associate Professor from 1971-1975, and a Professor from 1975 to the present. Her title is McElroy Honorary Research Professor in 2018. She served as Chairman of a Gordon Research Conference in1980, Chairman of the Board of Science Counselors of the National Institute on Aging, National Institutes of Health, from 1980 to 1984, a member of the Board of Directors of the Institute of Molecular Biology, Taiwan from 1987 to 1988, and a member of the Science Advisory Board of the National Cancer Institute.\n\nIn 1962, Ru Chih Huang and James Bonner published their finding that histone suppresses chromosomal RNA synthesis. That paper became a Citation Classic. In the interview recognizing her paper as a Citation Classic, Huang said, \"The work ... was done when little was known about the molecular approach to gene expression. The term chromatin as an interphase state of chromosome was beginning to be accepted as a biochemical working usage. The isolation and purification of chromatin was still approached with more art than science, until the pea embryo was chosen.\" James Bonner commented about the state of histone knowledge in the early 1960s when Huang began to work in his laboratory in a biographical article published in 1994, saying, \"Study of the literature on histones made it clear that not much was known about how many kinds of histones there were or whether there were different histones in different creatures or in different specialized cells. Nobody knew what histones were for and no one had studied histones in plants.\" Bonner got funding from NSF and other places to organize a conference on histones, trying to include all known histone researchers. After the conference he summarized the overview they were left with as chaotic. Estimates of the number of histone types ranged up into the thousands and there was no consensus about homology of histones between different species or even between different cells within one species.\n\nRu Chih Huang showed that histone proteins inhibited transcription of pea embryo and calf thymus DNA and worked out repeatable methods for studying individual histones while she was a postdoctoral fellow with James Bonner. Bonner's laboratory later purified and sequenced each histone type from peas in collaboration with Emil Smith. In the years that followed at Johns Hopkins, her research focused upon gene regulation in cancer, regulation of viral gene expression, expression of viral genome fragments in mammalian genomes, and molecular biology of aging cells.\n\nRu Chih Huang PhD is co-founder and Chief Scientific Advisor of  Erimos Pharmacueticals, LLC of Houston, TX. This firm is developing her recently discovered mutation-insensitive antiviral and anticancer compounds which are derivatives of the plant lignan NDGA. This work was somewhat controversial when she first took it up.\n\nRu Chih Chow married PC Huang on June 10, 1956. In an article in 2001, her son is quoted about her wedding day at Virginia Tech as follows, '\"The same day she received her degree, she changed into a wedding dress in a Shell gas station in Blacksburg, Va., and they got married in the university president's office, with just the president and two witnesses,\"said Suber Huang.\"They knew literally no one in this country. But they survived through their wit, intelligence and drive.\"'\n\n"}
{"id": "9225460", "url": "https://en.wikipedia.org/wiki?curid=9225460", "title": "Steve Van Dyck", "text": "Steve Van Dyck\n\nDr. Steve Van Dyck is the Senior Curator of vertebrates at Queensland Museum, in Brisbane, Queensland, Australia.\n\nSteve Van Dyck was instrumental in the rediscovery of the endangered mahogany glider in 1989.\n\nSteve Van Dyck has been awarded the 2008 Queensland Museum Medal for Research.\n\nVan Dyck was awarded the medal for his considerable talent in discovering a number of new species and also for behavioural studies of Australian mammals.\n\nThe medal was presented to Dr Steve Van Dyck by the Queensland Arts Minister Rod Welford in February, 2008.\n\nSteve Van Dyck's professional publications include:\n\n\n"}
{"id": "17058216", "url": "https://en.wikipedia.org/wiki?curid=17058216", "title": "Stochastic control", "text": "Stochastic control\n\nStochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. The system designer assumes, in a Bayesian probability-driven fashion, that random noise with known probability distribution affects the evolution and observation of the state variables. Stochastic control aims to design the time path of the controlled variables that performs the desired control task with minimum cost, somehow defined, despite the presence of this noise. The context may be either discrete time or continuous time.\n\nAn extremely well-studied formulation in stochastic control is that of linear quadratic Gaussian control. Here the model is linear, the objective function is the expected value of a quadratic form, and the disturbances are purely additive. A basic result for discrete-time centralized systems with only additive uncertainty is the certainty equivalence property: that the optimal control solution in this case is the same as would be obtained in the absence of the additive disturbances. This property is applicable to all centralized systems with linear equations of evolution, quadratic cost function, and noise entering the model only additively; the quadratic assumption allows for the optimal control laws, which follow the certainty-equivalence property, to be linear functions of the observations of the controllers.\n\nAny deviation from the above assumptions—a nonlinear state equation, a non-quadratic objective function, noise in the multiplicative parameters of the model, or decentralization of control—causes the certainty equivalence property not to hold. For example, its failure to hold for decentralized control was demonstrated in Witsenhausen's counterexample.\n\nIn a discrete-time context, the decision-maker observes the state variable, possibly with observational noise, in each time period. The objective may be to optimize the sum of expected values of a nonlinear (possibly quadratic) objective function over all the time periods from the present to the final period of concern, or to optimize the value of the objective function as of the final period only. At each time period new observations are made, and the control variables are to be adjusted optimally. Finding the optimal solution for the present time may involve iterating a matrix Riccati equation backwards in time from the last period to the present period.\n\nIn the discrete-time case with uncertainty about the parameter values in the transition matrix (giving the effect of current values of the state variables on their own evolution) and/or the control response matrix of the state equation, but still with a linear state equation and quadratic objective function, a Riccati equation can still be obtained for iterating backward to each period's solution even though certainty equivalence does not apply. The discrete-time case of a non-quadratic loss function but only additive disturbances can also be handled, albeit with more complications.\n\nA typical specification of the discrete-time stochastic linear quadratic control problem is to minimize\n\nwhere E is the expected value operator conditional on \"y\", superscript \"T\" indicates a matrix transpose, and \"S\" is the time horizon, subject to the state equation\n\nwhere \"y\" is an \"n\" × 1 vector of observable state variables, \"u\" is a \"k\" × 1 vector of control variables, \"A\" is the time \"t\" realization of the stochastic \"n\" × \"n\" state transition matrix, \"B\" is the time \"t\" realization of the stochastic \"n\" × \"k\" matrix of control multipliers, and \"Q\" (\"n\" × \"n\") and \"R\" (\"k\" × \"k\") are known symmetric positive definite cost matrices. We assume that each element of \"A\" and \"B\" is jointly independently and identically distributed through time, so the expected value operations need not be time-conditional.\n\nInduction backwards in time can be used to obtain the optimal control solution at each time,\n\nwith the symmetric positive definite cost-to-go matrix \"X\" evolving backwards in time from formula_4 according to\n\nwhich is known as the discrete-time dynamic Riccati equation of this problem. The only information needed regarding the unknown parameters in the \"A\" and \"B\" matrices is the expected value and variance of each element of each matrix and the covariances among elements of the same matrix and among elements across matrices.\n\nThe optimal control solution is unaffected if zero-mean, i.i.d. additive shocks also appear in the state equation, so long as they are uncorrelated with the parameters in the \"A\" and \"B\" matrices. But if they are so correlated, then the optimal control solution for each period contains an additional additive constant vector. If an additive constant vector appears in the state equation, then again the optimal control solution for each period contains an additional additive constant vector.\n\nThe steady-state characterization of \"X\" (if it exists), relevant for the infinite-horizon problem in which \"S\" goes to infinity, can be found by iterating the dynamic equation for \"X\" repeatedly until it converges; then \"X\" is characterized by removing the time subscripts from its dynamic equation.\n\nIf the model is in continuous time, the controller knows the state of the system at each instant of time. The objective is to maximize either an integral of, for example, a concave function of a state variable over a horizon from time zero (the present) to a terminal time \"T\", or a concave function of a state variable at some future date \"T\". As time evolves, new observations are continuously made and the control variables are continuously adjusted in optimal fashion.\n\nIn the literature, there are two types of MPCs for stochastic systems; Robust model predictive control and Stochastic Model Predictive Control (SMPC). Robust model predictive control is a more conservative method which considers the worst scenario in the optimization procedure. However, this method, similar to other robust controls, deteriorates the overall controller's performance and also is applicable only for systems with bounded uncertainties. The alternative method, SMPC, considers soft constraints which limit the risk of violation by a probabilistic inequality.\n\nIn a continuous time approach in a finance context, the state variable in the stochastic differential equation is usually wealth or net worth, and the controls are the shares placed at each time in the various assets. Given the asset allocation chosen at any time, the determinants of the change in wealth are usually the stochastic returns to assets and the interest rate on the risk-free asset. The field of stochastic control has developed greatly since the 1970s, particularly in its applications to finance. Robert Merton used stochastic control to study optimal portfolios of safe and risky assets. His work and that of Black–Scholes changed the nature of the finance literature. Influential mathematical textbook treatments were by Fleming and Rishel, and by Fleming and Soner. These techniques were applied by Stein to the financial crisis of 2007–08.\n\nThe maximization, say of the expected logarithm of net worth at a terminal date \"T\", is subject to stochastic processes on the components of wealth. In this case, in continuous time Itô's equation is the main tool of analysis. In the case where the maximization is an integral of a concave function of utility over an horizon (0,\"T\"), dynamic programming is used. There is no certainty equivalence as in the older literature, because the coefficients of the control variables—that is, the returns received by the chosen shares of assets—are stochastic.\n\n"}
{"id": "13225306", "url": "https://en.wikipedia.org/wiki?curid=13225306", "title": "The Sea Around Us", "text": "The Sea Around Us\n\nThe Sea Around Us is a prize-winning and best-selling book by the American marine biologist Rachel Carson, first published as a whole by Oxford University Press in 1951. It reveals the science and poetry of the sea while ranging from its primeval beginnings to the latest scientific probings. Often described as \"poetic,\" it was Carson's second published book and the one that launched her into the public eye and a second career as a writer and conservationist; in retrospect it is counted the second book of her so-called sea trilogy.\n\n\"The Sea Around Us\" won both the 1952 National Book Award for Nonfiction\nand a Burroughs Medal in nature writing. It remained on the \"New York Times\" Best Seller List for 86 weeks and it has been translated into 28 languages.\n\nSimon & Schuster had published her first book \"Under the Sea Wind\" in 1941; it was reviewed favorably but it sold poorly. Carson initially planned to call the sequel \"Return to the Sea\", and began writing in 1948, just after hiring Marie Rodell as her literary agent. Carson began by writing a single chapter (what would be \"The Birth of an Island\") along with a detailed outline, which Rodell used to pitch the book to publishers. During research for the book, Carson met with a number of oceanographers to discuss current research. Carson and Rodell had little initial success with magazines as outlets for the islands chapter, nor for a second chapter titled \"Another Beachhead.\" In April 1949, with about a third of the chapters complete, Rodell began trying to find a publisher for the entire book. By June she had arranged a contract with Oxford University Press that promised completion of the manuscript by March 1, 1950. Carson continued to write and research through 1949 and into 1950, despite unexpected health and financial difficulties. In part the research involved a trip aboard a U.S. Fish and Wildlife Service ship, \"Albatross III\". After revising the completion date, Carson completed the manuscript in June 1950. By that time, several periodicals (\"The New Yorker\", \"Science Digest\", and \"The Yale Review\") were interested in publishing some of the chapters.\nNine of fourteen chapters were serialized in \"The New Yorker\" beginning on June 2, 1951, and the book was published on July 2 by Oxford University Press. The serialization created a very large popular response, and the book was the subject of the feature review in \"The New York Times Book Review\" the day before publication. One chapter (\"The Birth of an Island\") was published in \"The Yale Review\"; it won the George Westinghouse Science Writing prize from the American Association for the Advancement of Science.\n\nAfter the book's release, Carson was inundated with an unexpected volume of fan mail and media attention. She was soon the object of attention from \"the literary crowd,\" and because of a subsequent condensation in \"Reader's Digest\", a broad general audience as well. The book sold more than 250,000 copies in 1951, in addition to the condensation and excerpts published elsewhere.\n\nA film version was filmed in 1952 and released in 1953; it won the 1953 Oscar for Best Documentary (though Carson was extremely disappointed with the script and would never sell film rights to her work again).\n\n\n"}
{"id": "48807280", "url": "https://en.wikipedia.org/wiki?curid=48807280", "title": "Vindhyan Ecology and Natural History Foundation", "text": "Vindhyan Ecology and Natural History Foundation\n\nThe Vindhyan Ecology and Natural History Foundation (VENHF) is a registered non-profit organisation (2012) with its headquarter in Mirzapur, Uttar Pradesh, India working for protection and conservation of the nature, natural resources and rights of the nature dependent communities in the ecologically fragile landscape of Vindhya Range in India. Vindhya Bachao Abhiyan is the flagship campaign of the organization.\n\nVindhya Bachao Abhiyan ( English meaning: \"Save Vindhya Campaign\") is the flagship program of VENHF which works towards environmental equity and bringing ecological justice through research-based environmental litigation, strengthening grass-root environmental movements, supporting institution of local governance and protecting the rights of nature dependent indigenous communities.\n\nVindhya Bachao Abhiyan exposed the illegalities in environment clearance and forest clearance surrounding the controversial Kanhar Dam Project in Sonbhadra district, Uttar Pradesh on Kanhar River. The information collected by Vindhya Bachao using Right to Information Act, 2005 was the basis of challenging the construction of the dam. Members of Vindhya Bachao and People's Union for Civil Liberties challenged the project in National Green Tribunal, New Delhi. The construction of the dam was thereafter stayed by the National Green Tribunal in December, 2014.\n\nThe Chief Secretary of Chhattisgarh government in April 2015 took note of the irregularities highlighted by Vindhya Bachao Abhiyan and asked the Uttar Pradesh government to stop the construction until the survey and compensation for the affected villages are completed.\n\nThe National Green Tribunal passed its final judgement on 7 May 2015 staying any new construction to be undertaken but allowed the construction already underway. The court also formed a high-level committee under chairmanship of Principal Chief Conservator of Forests, Uttar Pradesh to report on the directions issued in the judgment. The members filed a review petition against the judgment passed by the Tribunal, following which the court gave a direction on 7 July 2015 that \"This Application is disposed of with an observation that upon filing of the report by High Power Committee; constituted under the Judgment of the Tribunal, the Tribunal will pass further directions after hearing the parties in regard to all matters as mentioned in the Judgment including Environmental Clearance and Forest Clearance.\" The Tribunal through its order dated 21 September 2015 issued a show cause notice to the Principal Chief Conservator of Forests, Uttar Pradesh for not submitting the report within the deadline. In one of the article published by Vindhya Bachao Abhiyan on its portal in December, 2015 states that the petitioners are unsatisfied with the report submitted by the committee and alleged that the State government is violating the judgment passed by the Tribunal in its 7 May 2015 order.\n\nVindhya Bachao Website has a separate portal Kanhar Dam Resource Page for sharing latest updates on Kanhar Dam case.\n\n VENHF under the banner of Vindhya Bachao opposed the 1320 MW Coal Based Thermal Power Station in Mirzapur proposed by Ms Welspun Energy U.P. Private Limited since the year 2013. In a site visit report published by Vindhya Bachao in September, 2013 it was claimed that the project proponent concealed the information on the presence of forests and several Schedule I species under Wildlife Protection Act, 1972 in the EIA Report it submitted to the Ministry of Environment and Forests (India). In 2013 it was reported by Down to Earth that the plan was \"mired in controversy following allegations that the company concealed information about the presence of forestland and endangered wildlife at the project site. The farmers in the region have also been protesting against the project, alleging the company bought land for the project by cheating them.\" Debadityo Sinha, founder of VENHF in his articles claimed that the project will be a threat to river Ganga and the upper Khajuri Reservoir for drinking and irrigation. It was apprehended that the project if comes into existence will also threat a historic waterfall of Mirzapur known as Wyndham Fall and will also jeopardise the drinking water supply of the newly established Rajiv Gandhi South campus of Banaras Hindu University. He also made an allegation that the Public Hearing process for the project was greatly compromised and local people were prohibited from entering the public hearing premises. In a research paper published by VENHF in an international open source scientific journal \"Present Environment and Sustainable Development\" of Walter de Gruyter in its October, 2015 edition, the land use land cover map of the project site submitted by the company is contradicted.\n\nThe National Green Tribunal, New Delhi quashed the Environmental Clearance granted to the project in its judgment dated 21 December 2016 in a matter filed by Vindhya Bachao members Debadityo Sinha, Shiva Kumar Upadhyaya and Mukesh Kumar.\n\nVindhya Bachao Website has a separate portal Mirzapur Thermal Power Plant Resource Page with extensive information resources on the project, including site visit reports, minutes of MoEF meetings discussing the project, accounts of protests, and documents submitted by Welspun Energy.\n\nMembers of Vindhya Bachao along with Bharat Jhunjhunwala and other environmentalists wrote to the Ministry of Water Resources (India), World Bank and other states of India on the ecological and cultural impacts of reviving the National Waterway 1 on river Ganges.\n\nVENHF sent a representation to the Ministry of Environment, Forests and Climate Change, Government of India on the proposed draft notification declaring 1 km Eco-sensitive zone around the Kaimoor Wildlife Sanctuary. The representation was endorsed by renowned wildlife experts Mike Pandey and Asad Rahmani.\n\nVENHF hosts an information portal called Saving the Habitat which shares information on wildlife of Mirzapur. In December, 2014 the organization sent a representation to the Government of India demanding some areas of Mirzapur Forest Division to be declared as Protected areas of India.\n\nIn June, 2015 VENHF reviewed the Draft Notification on Emission Standards for Thermal Power Plants in India and sent a representation to Government of India.\n\nIn October, 2015 VENHF sent a representation on the Draft Environment Laws (Amendment) Bill, 2015 to Government of India in which it claimed that the bill will dilute the Environment Protection Act, 1986.\n\nVENHF is partner of EKOenergy and Global Call for Climate Action \n"}
{"id": "9651556", "url": "https://en.wikipedia.org/wiki?curid=9651556", "title": "Volunteer computing", "text": "Volunteer computing\n\nVolunteer computing is a type of distributed computing, \"an arrangement in which people, so-called volunteers, provide computing resources to projects, which use the resources to do distributed computing and/or storage\". Thus, computer owners or users donate their computing resources (such as processing power and storage) to one or more \"projects\".\n\nVolunteers are frequently members of the general public in the possession of their own personal computers with a Internet connection, but also organizations can act as volunteers and provide their computing resources.\n\nProjects in this context are mostly science-related projects executed by universities or academia in general.\n\nThe first volunteer computing project was the Great Internet Mersenne Prime Search, which was started in January 1996. It was followed in 1997 by distributed.net. In 1997 and 1998, several academic research projects developed Java-based systems for volunteer computing; examples include Bayanihan, Popcorn, Superweb, and Charlotte.\n\nThe term \"volunteer computing\" was coined by Luis F. G. Sarmenta, the developer of Bayanihan. It is also appealing for global efforts on social responsibility, or Corporate Social Responsibility as reported in a Harvard Business Review or used in the Responsible IT forum.\n\nIn 1999, the SETI@home and Folding@home projects were launched. These projects received considerable media coverage, and each one attracted several hundred thousand volunteers.\n\nBetween 1998 and 2002, several companies were formed with business models involving volunteer computing. Examples include Popular Power, Porivo, Entropia, and United Devices.\n\nIn 2002, the Berkeley Open Infrastructure for Network Computing (BOINC) project was founded at University of California, Berkeley Space Sciences Laboratory, funded by the National Science Foundation. BOINC provides a complete middleware system for volunteer computing, including a client, client GUI, application runtime system, server software, and software implementing a project web site. The first project based on BOINC was Predictor@home, based at the Scripps Research Institute, which began operation in 2004. Soon thereafter, SETI@home and ClimatePrediction.net began using BOINC. A number of new BOINC-based projects were created over the next few years, including Rosetta@home, Einstein@home, and AQUA@home. In 2007, IBM World Community Grid switched from the United Devices platform to BOINC.\n\nThe client software of the early volunteer computing projects consisted of a single program that combined the scientific computation and the distributed computing infrastructure. This monolithic architecture was inflexible. For example, it was difficult to deploy new application versions.\n\nMore recently, volunteer computing has moved to middleware systems that provide a distributed computing infrastructure independent from the scientific computation. Examples include:\n\n\nMost of these systems have the same basic structure: a client program runs on the volunteer's computer. It periodically contacts project-operated servers over the Internet, requesting jobs and reporting the results of completed jobs. This \"pull\" model is necessary because many volunteer computers are behind firewalls that do not allow incoming connections. The system keeps track of each user's \"credit\", a numerical measure of how much work that user's computers have done for the project.\n\nVolunteer computing systems must deal with several issues involving volunteered computers: their heterogeneity, their churn (the tendency of individual computers to join and leave the network over time), their sporadic availability, and the need to not interfere with their performance during regular use.\n\nIn addition, volunteer computing systems must deal with problems related to correctness:\n\n\nOne common approach to these problems is replicated computing, in which each job is performed on at least two computers. The results (and the corresponding credit) are accepted only if they agree sufficiently.\n\n\nThese effects may or may not be noticeable, and even if they are noticeable, the volunteer might choose to continue participating. However, the increased power consumption can be remedied to some extent by setting an option to limit the percentage of the processor used by the client, which is available in some client software.\n\n\nAlthough there are issues such as lack of accountability and trust between participants and researchers while implementing the projects, volunteer computing is crucially important, especially to projects that have limited funding.\n\n\n"}
{"id": "24293838", "url": "https://en.wikipedia.org/wiki?curid=24293838", "title": "Wigner rotation", "text": "Wigner rotation\n\nIn theoretical physics, the composition of two non-collinear Lorentz boosts results in a Lorentz transformation that is not a pure boost but is the composition of a boost and a rotation. This rotation is called Thomas rotation, Thomas–Wigner rotation or Wigner rotation. The rotation was discovered by Llewellyn Thomas in 1926, and derived by Wigner in 1939. If a sequence of non-collinear boosts returns an object to its initial velocity, then the sequence of Wigner rotations can combine to produce a net rotation called the Thomas precession.\n\nThere are still ongoing discussions about the correct form of equations for the Thomas rotation in different reference systems with contradicting results. Goldstein:\nEinstein's principle of velocity reciprocity (EPVR) reads\nWith less careful interpretation, the EPVR is seemingly violated in some models. There is, of course, no true paradox present.\n\nWhen studying the Thomas rotation at the fundamental level, one typically uses a setup with three coordinate frames, . Frame has velocity relative to frame , and frame has velocity relative to frame .\n\nThe axes are, by construction, oriented as follows. Viewed from , the axes of and are parallel (the same holds true for the pair of frames when viewed from .) Also viewed from , the spatial axes of and are parallel (and the same holds true for the pair of frames when viewed from .) This is an application of EVPR: If is the velocity of relative to , then is the velocity of relative to . The velocity makes the \"same\" angles with respect to coordinate axes in both the primed and unprimed systems. This does \"not\" represent a snapshot taken in any of the two frames of the combined system at any particular time, as should be clear from the detailed description below.\n\nThis is possible, since a boost in, say, the positive , preserves orthogonality of the coordinate axes. A general boost can be expressed as , where is a rotation taking the into the direction of and is a boost in the new . Each rotation retains the property that the spatial coordinate axes are orthogonal. The boost will stretch the (intermediate) by a factor , while leaving the and in place. The fact that coordinate axes are non-parallel in this construction after \"two\" consecutive non-collinear boosts is a precise expression of the phenomenon of Thomas precession.\n\nThe velocity of as seen in is denoted , where ⊕ refers to the relativistic addition of velocity (and not ordinary vector addition), given by\n\n\\frac{1}{c^2}\\frac{\\gamma_\\mathbf{u}}{1+\\gamma_\\mathbf{u}} \\mathbf u \\cdot \\mathbf v \\right)\\mathbf u +\n\nand\n\nwhich still completely defines the direction of the axis without loss of information.\n\nThe rotation is simply a \"static\" rotation and there is no relative rotational motion between the frames, there is relative translational motion in the boost. However, if the frames accelerate, then the rotated frame rotates with an angular velocity. This effect is known as the Thomas precession, and arises purely from the kinematics of successive Lorentz boosts.\n\nIn principle, it is pretty easy. Since every Lorentz transformation is a product of a boost and a rotation, the consecutive application of two pure boosts is a pure boost, either followed by or preceded by a pure rotation. Thus suppose\n\nThe task is to glean from this equation the boost velocity and the rotation from the matrix entries of . The coordinates of events are related by\n\nInverting this relation yields\n\nor\n\nSet Then will record the spacetime position of the origin of the primed system,\n\nor\n\nBut\n\nMultiplying this matrix with a pure rotation will not affect the zeroth columns and rows, and\n\nwhich could have been anticipated from the formula for a simple boost in the -direction, and for the relative velocity vector\n\nThus given with , one obtains and by little more than inspection of . (Of course, can also be found using velocity addition per above.) From , construct . The solution for is then\n\nWith the ansatz\n\none finds by the same means\n\nFinding a formal solution in terms of velocity parameters and involves first \"formally\" multiplying , formally inverting, then reading off form the result, \"formally\" building from the result, and, finally, formally multiplying . It should be clear that this is a daunting task, and it is difficult to interpret/identify the result as a rotation, though it is clear a priori that it is. It is these difficulties that the Goldstein quote at the top refers to. The problem has been thoroughly studied under simplifying assumptions over the years.\n\nAnother way to explain the origin of the rotation is by looking at the generators of the Lorentz group.\n\nThe passage from a velocity to a boost is obtained as follows. An arbitrary boost is given by\n\nwhere is a triple of real numbers serving as coordinates on the boost subspace of the Lie algebra spanned by the matrices\n\nThe vector\n\nis called the \"boost parameter\" or \"boost vector\", its norm is the rapidity. Here is the \"velocity parameter\", the magnitude of the vector . While for one has the is confined by , and hence . Thus\n\nThe set of velocities satisfying is an open ball in and is called the space of admissible velocities in the literature. It is endowed with a hyperbolic geometry described in the linked article.\n\nThe generators of boosts, in different directions do not commute. This has the effect that two consecutive boosts is not a pure boost in general, but a rotation preceding a boost.\n\nConsider a succession of boosts in the x direction, then the y direction, expanding each boost to first order\n\nthen\n\nand the group commutator is\n\nThree of the commutation relations of the Lorentz generators are\n\nwhere the bracket is a binary operation known as the \"commutator\", and the other relations can be found by taking cyclic permutations of x, y, z components (i.e. change x to y, y to z, and z to x, repeat).\n\nReturning to the group commutator, the commutation relations of the boost generators imply for a boost along the x then y directions, there will be a rotation about the z axis. In terms of the rapidities, the rotation angle is given by\n\nThe familiar notion of vector addition for velocities in the Euclidean plane can be done in a triangular formation, or since vector addition is commutative, the vectors in both orderings geometrically form a parallelogram (see \"parallelogram law\"). This does not hold for relativistic velocity addition; instead a hyperbolic triangle arises whose edges are related to the rapidities of the boosts. Changing the order of the boost velocities, one does not find the resultant boost velocities to coincide.\n\n\n\n"}
