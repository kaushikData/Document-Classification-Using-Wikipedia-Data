{"id": "41355223", "url": "https://en.wikipedia.org/wiki?curid=41355223", "title": "A2261-BCG", "text": "A2261-BCG\n\nA2261-BCG (short for Abell 2261 Brightest Cluster Galaxy) is a huge elliptical galaxy in the cluster Abell 2261. One of the largest galaxies known, A2261-BCG is estimated to have a diameter of a million light-years, some 10 times larger than the Milky Way. It is the brightest and the most massive galaxy in the cluster, and has the largest galactic core ever observed, spanning more than 10,000 light-years.\n\nThe cD elliptical galaxy, located at least 3 billion light-years from Earth, is also well known as a radio source. Its core is highly populated by a dense number of old stars, but is mysteriously diffuse, giving it a large core.\nOn September 10, 2012, using Hubble Space Telescope's Wide Field Camera 3, scientists found out that there was no supermassive black hole present in the center. This may be the likely cause of its diffuse and large core, but it contradicts modern galactic evolutionary theories. A huge cD galaxy like A2261-BCG would be expected to have a supermassive black hole concentrated at its center.\n"}
{"id": "14508968", "url": "https://en.wikipedia.org/wiki?curid=14508968", "title": "Acceptance credit", "text": "Acceptance credit\n\nAn acceptance credit is a type of letter of credit that is paid by a time draft authorizing payment on or after a specific date, if the terms of the letter of credit have been complied with. There are two types of acceptance credit, confirmed and unconfirmed. Unconfirmed acceptance credit means that the seller takes the risk that payment will not be made, due to any number of contingencies such as shipment non-delivery, confiscation by customs authorities, or any other problems. Confirmed acceptance credit means that the bank upon which the credit has been issued, essentially guarantees payment as long as the terms of the letter of credit have been complied with.\n\nConfirmed acceptance credit is more expensive to establish than unconfirmed acceptance credit because the issuing bank is effectively guaranteeing payment. It also transfers the risk of non-delivery to the recipient, because once the seller places the product in the hands of the shipping company, the seller has complied and will be paid; if the shipment does not arrive, is delayed, or other problems occur, the buyer cannot stop payment or otherwise prevent redemption of the acceptance credit.\n\nBanks may also create an acceptance credit facility allowing a company to issue time drafts not linked to specific shipments in order to provide general working capital finance. Under the arrangement the issuing company presents bills of exchange to the bank for acceptance, confirmation and sale at a discount to face value (representing the finance cost until maturity). The discounted sum is made available to the issuing company until the bill's maturity when it is obliged to repay the bill's full face value to the bank. The process may then be repeated to provide a so-called rolling facility. While popular in the pre-electronic era, such facilities have since been widely replaced by financing arrangements which do not require the issue of paper.\n"}
{"id": "56584052", "url": "https://en.wikipedia.org/wiki?curid=56584052", "title": "Ahi Pepe MothNet", "text": "Ahi Pepe MothNet\n\nAhi Pepe MothNet (styled \"Ahi Pepe | MothNet\") is a citizen science initiative based in Otago, New Zealand and led by Manaaki Whenua Landcare Research that aims to raise the awareness of moths among teachers and students.\n\nAhi Pepe (Māori for \"moth fire\") refers a traditional proverb (whakataukī) of Te Whiti o Rongomai about firelight attracting moths (pepe) instead of muttonbirds (tītī).\n\nAhi Pepe began in 2015 as a project with four Otago schools, and continued with finding from Landcare Research, Unlocking Curious Minds, and the Biological Heritage National Science Challenge. Collaborators include Otago Museum, Orokonui Ecosanctuary, the University of Otago, Te Rūnanga o Ngāi Tahu, and schools across the South Island.\n\nIn October 2016, Ahi Pepe worked with Orokonui Ecosanctuary to install moth traps inside and outside the predator-proof fence, and schoolchildren worked alongside entomologists to pin and identify the moths caught. The project has produced educational resources for schools in both English and Te Reo Māori. The South Island guide is the first educational resource to be written in the Kāi Tahu dialect.\n\nIn 2017, public donations enabled a delegation of Otago schoolchildren to give a presentation on Ahi Pepe at the World Indigenous Peoples Conference on Education in Toronto.\n\n2015 – \"Beginner’s Guide to the Otago Macro Moths\"\n\n2016 – \"Puka Whakamārama o Te Pepe Nui - Beginners' Guide to the Macro Moths\" (South Island)\n\n2017 – \"Puka Whakamārama o Te Pepe Nui - Beginners' Guide to the Macro Moths\" (North Island)\n\nAll publications are produced in English and Te Reo Māori editions.\n\n"}
{"id": "5585946", "url": "https://en.wikipedia.org/wiki?curid=5585946", "title": "An Urchin in the Storm", "text": "An Urchin in the Storm\n\nAn Urchin in the Storm is a 1987 essay collection from paleontologist and science writer Stephen Jay Gould.\n\nAll but one of the essays had originally appeared in \"The New York Review of Books\". Grouped by theme, the sections of the book deal respectively with the irreducibility of history (and the pleasures and challenges of contingency) in its two principal domains of life and the earth, nature's complexity, the theory and consequences of biological determinism, and rationalism in explanation. Thus it is philosophically the most important of Gould's works - as befits a book dedicated to Peter Medawar and especially Isaiah Berlin, since the latter shares with Gould a commitment against determinism, even though Gould had a Marxist background while Berlin is quintessentially anti-Marxist.\n\nIt was reviewed in \"The New York Times\" by Michiko Kakutani, who noted that although the pieces were technically book reviews, Gould \"tends to use the subject at hand as a jumping-off point for more general discussions\". Gould argued against Creationism and biological determinism, criticizing sociobiology and Arthur Jensen's theories about race and intelligence. Other subjects discussed included the nature of geological time and change, the 19th century Devonian controversy, which involved the identification of a major period of earth history, and the work of biologists Barbara McClintock, E. E. Just, G. E. Hutchinson, and Lewis Thomas.\n"}
{"id": "25579396", "url": "https://en.wikipedia.org/wiki?curid=25579396", "title": "Army engineering maintenance", "text": "Army engineering maintenance\n\nArmy engineering maintenance consists of those engineers, technicians, and military organizations responsible for the expert repair and maintenance of army vehicles, weapon systems, and other equipment.\n\nArmy engineering maintenance should not be confused with military engineering which is distinctly separate and analogous to civil engineering while the former analogous to mechanical engineering and electrical engineering.\n\nAt the operational and tactical levels, army engineering maintenance is focused on the repair and scheduled maintenance work required to keep army equipment fleets operational.\n\nAt the strategic level, army engineering maintenance is closely linked to military logistics. At this level, it includes work such as the design, development, and testing of new vehicles and weapon systems. It also includes lifecycle management activities once new systems become operational.\n\n\n"}
{"id": "10092550", "url": "https://en.wikipedia.org/wiki?curid=10092550", "title": "Body force", "text": "Body force\n\nA body force is a force that acts throughout the volume of a body. Forces due to gravity, electric fields and magnetic fields are examples of body forces. Body forces contrast with contact forces or surface forces which are exerted to the surface of an object.\n\nNormal forces and shear forces between objects are surface forces as they are exerted to the surface of an object. All cohesive surface attraction and contact forces between objects are also considered as surface forces.\n\nFictitious forces such as the centrifugal force, Euler force, and the Coriolis effect are also examples of body forces.\n\nA body force is simply a type of force, and so it has the same dimensions as force, [M][L][T]. However, it is often convenient to talk about a body force in terms of either the force per unit volume or the force per unit mass. If the force per unit volume is of interest, it is referred to as the force density throughout the system.\n\nA body force is distinct from a contact force in that the force does not require contact for transmission. Thus, common forces associated with pressure gradients and conductive and convective heat transmission are not body forces as they require contact between systems to exist. Radiation heat transfer, on the other hand, is a perfect example of a body force.\n\nMore examples of common body forces include;\n\nFictitious forces (or inertial forces) can be viewed as body forces. Common inertial forces are,\n\n\nHowever, fictitious forces are not actually forces. Rather they are corrections to Newton's second law when it is formulated in an accelerating reference frame.\n\nThe body force density is defined so that the volume integral (throughout a volume of interest) of it gives the total force acting throughout the body;\n\nwhere d\"V\" is an infinitesimal volume element, \"ρ\" is the mass density, and g is the \"external body force density field\" acting on the system.\n\nLike any other force, a body force will cause an object to accelerate. For a non-rigid object, Newton's second law applied to a small volume element is\n\nwhere \"ρ\"(r) is the mass density of the substance, ƒ the force density, and a(r) all at point r. \n\nIn the case of gravity on a planet surface, g(r) is simply the approximately constant and uniform gravitational field, like on Earth where:\n\n"}
{"id": "4923690", "url": "https://en.wikipedia.org/wiki?curid=4923690", "title": "Body hair", "text": "Body hair\n\nBody hair, or androgenic hair, is the terminal hair that develops on the human body during and after puberty. It is differentiated from the head hair and less visible vellus hair, which are much finer and lighter in color. The growth of androgenic hair is related to the level of androgens (often referred to as male hormones) and the density of androgen receptors in the dermal papillae. Both must reach a threshold for the proliferation of hair follicle cells.\n\nFrom childhood onward, regardless of sex, vellus hair covers almost the entire area of the human body. Exceptions exclude the lips; the backs of the ears; the palms of hands; the soles of the feet; certain external genital areas; the navel; and scar tissue. The density of hair – i.e. the number of hair follicles per unit area of skin – varies from person to person. In many cases, areas on the human body that contain vellus hair will begin to produce darker and thicker body hair, such as the first growth of beard hair on a male and female adolescent's previously smooth chin; although it may appear thinner on the female.\n\nAndrogenic hair follows the same growth pattern as the hair that grows on the scalp, but with a shorter anagen phase and longer telogen phase. While the anagen phase for the hair on one's head lasts for years, the androgenic hair growth phase for body hair lasts a few months. The telogen phase for body hair lasts close to a year. This shortened growing period and extended dormant period explains why the hair on the head tends to be much longer than other hair found on the body. Differences in length seen in comparing the hair on the back of the hand and pubic hair, for example, can be explained by varied growth cycles in those two regions. The same goes for differences in body hair length seen in different people, especially when comparing men and women.\n\nLike much of the hair on the human body, leg, arm, chest, and back hair begin as vellus hair. As people age, the hair in these regions will often begin to grow darker and more abundantly. This will typically happen during or after puberty. Men will often have more abundant, coarser hair on the arms and back, while women tend to have a less drastic change in the hair growth in these areas but do experience a significant change in thickness of hairs. However, some women will grow darker, longer hair in one or more of these regions.\n\nVellus hair grows on the chest and abdomen of both sexes at all stages of development. After puberty and extending into adulthood, most males grow increasing amounts of terminal hair over the chest and abdomen areas. Adult women also typically can grow terminal hairs around the areola though in many cultures these hairs are typically removed.\nArm hair grows on a human's forearms, sometimes even on the elbow area. Terminal arm hair is concentrated on the wrist end of the forearm, extending over the hand. Terminal hair growth in adolescent males is often much more intense than that in females, particularly for individuals with dark hair. In some cultures, it is common for women to remove arm hair, though this practice is less frequent than that of leg hair removal.\n\nTerminal hair growth on arms is a secondary sexual characteristic in boys and appears in the last stages of puberty. Vellus arm hair is usually concentrated on the elbow end of the forearm and often ends on the lower part of the upper arm. This type of intense arm vellus hair growth sometimes occurs in young women and people of both sexes until puberty. Even though this causes the arms to appear hairy, it is not caused solely by testosterone. The hair is softer and different from men's arm hair, in texture.\n\nVisible hair appearing on the top surfaces of the feet and toes generally begins with the onset of puberty. Terminal hair growth on the feet is typically more intense in adult and adolescent males than in females.\n\nLeg hair sometimes appears at the onset of adulthood, with the legs of men more often hairier than those of women. For a variety of reasons, people may shave their leg hair, including cultural practice or individual needs. Around the world, women generally shave their leg hair more regularly than men, to conform with the social norms of many cultures, many of which perceive smooth skin as a sign of youth, beauty, and in some cultures, hygiene. However, athletes of both sexes – swimmers, runners, cyclists and bodybuilders in particular – may shave their androgenic hair to reduce friction, highlight muscular development or to make it easier to get into and out of skin-tight clothing.\n\nPubic hair is a collection of coarse hair found in the pubic region. It will often also grow on the thighs and abdomen. Zoologist Desmond Morris disputes theories that it developed to signal sexual maturity or protect the skin from chafing during copulation, and prefers the explanation that pubic hair acts as a scent trap. Also, both sexes having thick pubic hairs act as a sort of cushion during intercourse.\n\nThe genital area of males and females are first inhabited by shorter, lighter vellus hairs that are next to invisible and only begin to develop into darker, thicker pubic hair at puberty. At this time, the pituitary gland secretes gonadotropin hormones which trigger the production of testosterone in the testicles and ovaries, promoting pubic hair growth. The average ages pubic hair begins to grow in males and females are 12 and 11, respectively. However, in some females, pubic hair has been known to start growing as early as age 7.\n\nJust as individual people differ in scalp hair color, they can also differ in pubic hair color. Differences in thickness, growth rate, and length are also evident.\n\nIslamic hygienical jurisprudence teaches that pubic and armpit hair must be pulled out or shaven to be considered as Sunnah. Trimming is acceptable. In Western culture, beginning in the 1980s, there has been a trend among women to remove or trim pubic hair, either partially or completely; the trend later spread to men as well. Fashions for both men and women to either shape or remove pubic hair rise and fall throughout much of history, though it is more often considered as a practice for women alone.\n\nUnderarm hair normally starts to appear at the beginning of puberty, with growth usually completed by the end of the teenage years.\n\nToday in much of the world, it is common for women to regularly shave their underarm hair. The prevalence of this practice varies widely, though. The practice became popular for cosmetic reasons around 1915 in the United States and United Kingdom, when one or more magazines showed a woman in a dress with shaved underarms. As women's armpit hair contrasts more noticeably with her other body hair than does a man's, it is a significant indicator of sexual maturity and therefore unwelcome in conventional western polite society. Regular shaving became feasible with the introduction of the safety razor at the beginning of the 20th century. While underarm shaving was quickly adopted in some English speaking countries, especially in the US and Canada, it did not become widespread in Europe until well after World War II. Since then the practice has spread worldwide. Some men also choose to shave their armpits.\n\nFacial hair, as the name suggests, grows primarily on or around one's face. Both men and women experience facial hair growth. Like pubic hair, non-vellus facial hair will begin to grow in around puberty. Moustaches in young men usually begin to grow in at around the age of puberty, although some men may not grow a moustache until they reach late teens or at all. For some cases facial hair development may take longer to mature than in the late teens but a lot of men experience no facial development even at an older age. Facial hair development has often been associated with stress levels.\n\nIt is common for many women to develop a few facial hairs under or around the chin, along the sides of the face (in the area of sideburns), or on the upper lip. These may appear at any age after puberty but are often seen in women after menopause due to decreased levels of estrogen. A darkening of the vellus hair of the upper lip in women is not considered true facial hair, though it is often referred to as a \"moustache\"; the appearance of these dark vellus hairs may be lessened by bleaching. A relatively small number of women are able to grow enough facial hair to have a distinct beard. In some cases, female beard growth is the result of a hormonal imbalance (usually androgen excess), or a rare genetic disorder known as hypertrichosis. Sometimes it is caused by use of anabolic steroids. Cultural pressure leads most women to remove facial hair, as it may be viewed as a social stigma.\n\nHair follicles are to varying degrees sensitive to androgen, primarily testosterone and its derivatives, particularly dihydrotestosterone, with different areas on the body having different sensitivity. As androgen levels increase, the rate of hair growth and the weight of the hairs increase. Genetic factors determine both individual levels of androgen and the hair follicle's sensitivity to androgen, as well as other characteristics such as hair colour, type of hair and hair retention.\n\nRising levels of androgen during puberty cause vellus hair to transform into terminal hair over many areas of the body. The sequence of appearance of terminal hair reflects the level of androgen sensitivity, with pubic hair being the first to appear due to the area's special sensitivity to androgen. The appearance of pubic hair in both sexes is usually seen as an indication of the start of a person's puberty. There is a sexual differentiation in the amount and distribution of androgenic hair, with men tending to have more terminal hair in more areas. This includes facial hair, chest hair, abdominal hair, leg hair, arm hair, and foot hair. Women retain more of the less visible vellus hair, although leg, arm, and foot hair can be noticeable on women. It is not unusual for women to have a few terminal hairs around their nipples as well.\nIn the later decades of life, especially after the 5th decade, there begins a noticeable reduction in body hair especially in the legs. The reason for this is not known but it could be due to poorer circulation, lower free circulating hormone amounts or other reasons.\n\nAndrogenic hair provides tactile sensory input by transferring hair movement and vibration via the shaft to sensory nerves within the skin. Follicular nerves detect displacement of hair shafts and other nerve endings in the surrounding skin detect vibration and distortions of the skin around the follicles. Androgenic hair extends the sense of touch beyond the surface of the skin into the air and space surrounding it, detecting air movements as well as hair displacement from contact by insects or objects.\n\nDetermining the evolutionary function of androgenic hair must take into account both human evolution and the thermal properties of hair itself.\n\nThe thermodynamic properties of hair are based on the properties of the keratin strands and amino acids that combine into a 'coiled' structure. This structure lends to many of the properties of hair, such as its ability to stretch and return to its original length. This coiled structure does not predispose curly or frizzy hair, both of which are defined by oval or triangular hair follicle cross-sections.\n\nHair is a very good thermal conductor and aids heat transfer both into and out of the body. When goose bumps are observed, small muscles (Arrector pili muscle) contract to raise the hairs both to provide insulation, by reducing cooling by air convection of the skin, as well as in response to central nervous stimulus, similar to the feeling of 'hairs standing up on the back of your neck'. This phenomenon also occurs when static charge is built up and stored in the hair. Keratin however can easily be damaged by excessive heat and dryness, suggesting that extreme sun exposure, perhaps due to a lack of clothing, would result in perpetual hair destruction, eventually resulting in the genes being bred out in favor of high skin pigmentation. It is also true that parasites can live on and in hair thus peoples who preserved their body hair would have required greater general hygiene to prevent diseases.\n\nMarkus J. Rantala of the Department of Biological and Environmental Science, University of Jyväskylä, Finland, said humans evolved by \"natural selection\" to be hairless when the trade off of \"having fewer parasites\" became more important than having a \"warming, furry coat\".\n\nP.E. Wheeler of the Department of Biology at Liverpool Polytechnic said quadrupedal savannah mammals of similar volume to humans have body hair to keep warm while only larger quadrupedal savannah mammals lack body hair, because their body volume itself is enough to keep them warm. Therefore, Wheeler said humans who should have body hair based on predictions of body volume alone for savannah mammals evolved no body hair after evolving bipedalism which he said reduced the amount of body area exposed to the sun by 40%, reducing the solar warming effect on the human body.\n\nLoss of fur occurred at least 2 million years ago, but possibly as early as 3.3 million years ago judging from the divergence of head and pubic lice, and aided persistence hunting (the ability to catch prey in very long distance chases) in the warm savannas where hominins first evolved. The two main advantages are felt to be bipedal locomotion and greater thermal load dissipation capacity due to better sweating and less hair.\n\nMarkus J. Rantala of the Department of Biological and Environmental Science, University of Jyväskylä, Finland, said the existence of androgen dependent hair on men could be explained by sexual attraction whereby hair on the genitals would trap pheromones and hair on the chin would make the chin appear more massive.\n\nIn 1876, Oscar Peschel wrote that North Asiatic Mongols, Native Americans, Malays, Hottentots and Bushmen have little to no body hair, while Semitics, Indo-Europeans, and Southern Europeans (especially the Portuguese and Spanish) have extensive body hair.\n\nAccording to Ashley Montagu who taught anthropology at Princeton University, Asian people and black people such as the San people are less hairy than white people. Montagu said that the hairless feature is a neotenous trait.\n\nRodney P.R. Dawber of the Oxford Hair Foundation and Clinical Lecturer in Dermatology said East Asian males have little or no facial or body hair and Dawber also said that Mediterranean males are covered with an exuberant pelage.\n\nAnthropologist Arnold Henry Savage Landor described the Ainu as having hairy bodies.\n\nC.H. Danforth and Mildred Trotter of the Department of Anatomy at Washington University did a study using army soldiers of European origin where they concluded that dark-haired white men are generally more hairy than fair-haired white men.\n\nH. Harris who published in the \"British Journal of Dermatology\" said American Indians have the least body hair, Chinese and black people have little body hair, white people have more body hair than blacks and Ainu have the most body hair.\n\nMilkica Nešić et al. of the Department of Physiology at the University of Niš, Serbia, cited prior studies to indicate that the frequency of hair on the middle finger joint (mid-phalangeal hair) in whites is significantly higher than in black populations.\n\nEike-Meinrad Winkler and Kerrin Christiansen of the Institut für Humanbiologie, Hamburg, Germany, did a study using Kavango people and !Kung people of body hair and hormone levels to investigate the reason black Africans did not have bodies as hairy as Europeans. Winkler and Christiansen concluded the difference in hairiness between black Africans and Europeans had to do with differences in androgen or estradiol production, in androgen metabolism, and in sex hormone action in the target cells.\n\nValerie Anne Randall of the Department of Biomedical Sciences, University of Bradford, said beard growth in Caucasian men increases until the mid-thirties due to a delay caused by growth cycles changing from vellus hair to terminal hair. Randall said white men and women are hairier than Japanese men and women even with the same total plasma androgen levels. Randall says that the reason for some people being hairy and some people not being hairy is unclear, but that it probably is related to differing sensitivity of hair follicles to 5α-reductase.\n\nStewart W. Hindley and Albert Damon of the Department of Anthropology at Stanford University have studied the frequency of hair on the middle finger joint (mid-phalangeal hair) of Solomon Islanders, as a part of a series of anthropometric studies of these populations. They summarize other studies on prevalence of this trait as reporting, in general, that Caucasoids are more likely to have hair on the middle finger joint than Negroids and Mongoloids, and collect the following frequencies from previously published literature: Andamanese 0%, Eskimo 1%, African American 16% or 28%, Ethiopians 25.6%, Mexicans of the Yucatan 20.9%, Penobscot and Shinnecock 22.7%, Gurkha 33.6%, Japanese 44.6%, various Hindus 40–50%, Egyptians 52.3%, Near Eastern peoples 62–71%, various Europeans 60–80%. Although they never made an Androgenic hair map.\n\nAnthropologist Joseph Deniker said that the very hairy peoples are the Ainus, Iranians, Australian aborigines (Arnhem Land being less hairy), Toda, Dravidians and Melanesians, while the most glabrous peoples are the American Indians, San, and East Asians, who include Chinese, Mongols, and Malays. Deniker said that hairy peoples tend to have thicker beards, eyelashes, and eyebrows but fewer hairs on their scalp.\n\nIt has shown that individuals can be uniquely identified by their androgenic hair patterns, e.g. even when face and tattoos, etc., are covered, persons can still be identified by hair on other parts of the body.\n\n"}
{"id": "56857842", "url": "https://en.wikipedia.org/wiki?curid=56857842", "title": "Cosmic-Ray Extremely Distributed Observatory", "text": "Cosmic-Ray Extremely Distributed Observatory\n\nCosmic-Ray Extremely Distributed Observatory (CREDO) is a scientific project initiated at the end of August 2016 by Polish scientists from the Institute of Nuclear Physics in Kraków (researchers from the Czech Republic, Slovakia and Hungary also joined the project) whose purpose is the detection of cosmic rays and the search for dark matter. Its aim is to involve as many people as possible in the construction of a global system of cosmic ray detectors, from which it will be possible to examine the essence of dark matter. Having a camera and a GPS module, a smartphone works well as a detector of particles from space.\n\nThe main objective of CREDO is the detection and analysis of extended cosmic ray phenomena, so-called super-preshowers (SPS), using existing as well as new infrastructure (cosmic-ray observatories, educational detectors, single detectors etc.). The search for ensembles of cosmic ray events initiated by SPS is yet an untouched topic, in contrast to the current state-of-the-art analysis, which is focused on the detection of single cosmic ray events. Theoretical explanation of SPS could be given either within classical (e.g., photon-photon interaction) or exotic (e.g., Super Heavy Dark Matter decay or annihilation) scenarios, thus detection of SPS would provide a better understanding of particle physics, high energy astrophysics and cosmology. The ensembles of cosmic rays can be classified based on the spatial and temporal extent of particles constituting the ensemble. Some classes of SPS are predicted to have huge spatial distribution, a unique signature detectable only with a facility of global size. Since development and commissioning of a completely new facility with such requirements is economically unwarranted and time-consuming, the global analysis goals are achievable when all types of existing detectors are merged into a worldwide network. The idea to use the instruments in operation is based on a novel trigger algorithm: in parallel to looking for neighbour surface detectors receiving the signal simultaneously, one should also look for spatially isolated stations clustered in a small time window. On the other hand, CREDO's strategy is also aimed at an active engagement of a large number of participants, who will contribute to the project by using common electronic devices (e.g. smartphones), capable of detecting cosmic rays. It will help not only in expanding the geographical spread of CREDO, but also in managing a large manpower necessary for a more efficient crowd-sourced pattern recognition scheme to identify and classify SPS. A worldwide network of cosmic-ray detectors could not only become a unique tool to study fundamental physics, it will also provide a number of other opportunities, including space-weather or geophysics studies. Among the latter, one can list the potential to predict earthquakes by monitoring the rate of low energy cosmic-ray events. The diversity of goals motivates us to advertise this concept across the astroparticle physics community. \n\nThe user must install an application that turns their phone into a cosmic ray detector, connect it to the charger and arrange it horizontally; for example, put it on a table or bedside cabinet. It is also important that the cameras of the device are well covered, for example with a piece of black adhesive tape, and notifications indicated by the blinking of lights are turned off. If a radiation particle passes through a photosensitive matrix in the phone, it will stimulate several pixels, which will be noticed by the program that sends information to the server. Thanks to the GPS module, the time and place of the event is also known. \n\nAll data from smartphones will then be analyzed together in the Academic Computer Center Cyfronet AGH, which will keep participants informed about the progress of the search for signs of high-energy particles.\n\nThe application is still under testing and may not produce the expected results on some mobile devices.\n\nAll traces of particles registered by smartphones can be viewed on a dedicated website. Their size and shape depends on the type and energy of the captured particle and the direction from which it came.\n\n"}
{"id": "35276245", "url": "https://en.wikipedia.org/wiki?curid=35276245", "title": "Doris Holmes Blake", "text": "Doris Holmes Blake\n\nDoris Holmes Blake, \"née\" Doris Mildred Holmes (January 11, 1892 – December 3, 1978), was an American entomologist and scientific illustrator. \n\nShe was an expert on chrysomelidae (leaf beetles).\n\nDoris Holmes was raised in a middle-class family in Stoughton, Massachusetts. She earned a B.A. in 1913 from Boston University and an M.A. in Zoology and Psychology from Radcliffe College in 1917. While at Boston University she became a member of Alpha Delta Pi. Marrying the botanical taxonomist Sidney Fay Blake in 1918, she worked for the Bureau of Entomology of the United States Department of Agriculture from 1919 to 1928. From 1928 she worked at the Department of Entomology of the United States National Museum. Forced to resign in 1933 by her husband's employment at the Department (the law prohibited more than one member of a family holding a government position), she continued studying beetles as an unpaid Associate of the Smithsonian Institution at Washington, D.C. until her death.\n\nSome of Blake's entomological and botanical sketches, as well as her non-academic writing, are also included. The papers also include a number of photographs of Blake and her family and of entomologists, both at the Smithsonian and at USDA. Her papers are held by the Smithsonian Institution.\n\n\n"}
{"id": "11270885", "url": "https://en.wikipedia.org/wiki?curid=11270885", "title": "Fuzzy cognitive map", "text": "Fuzzy cognitive map\n\nA fuzzy cognitive map is a cognitive map within which the relations between the elements (e.g. concepts, events, project resources) of a \"mental landscape\" can be used to compute the \"strength of impact\" of these elements. Fuzzy cognitive maps were introduced by Bart Kosko. Ron Axelrod introduced cognitive maps as a formal way of representing social scientific knowledge and modeling decision making in social and political systems, then brought in the computation fuzzy logic.\n\nFuzzy cognitive maps are signed fuzzy digraphs. They may look at first blush like Hasse diagrams but they are not.\nSpreadsheets or tables are used to map FCMs into matrices for further computation.\nFCM is a technique used for causal knowledge acquisition and representation, it supports causal knowledge reasoning process and belong to the neuro-fuzzy system that aim at solving decision making problems, modeling and simulate complex systems . \nLearning algorithms have been proposed for training and updating FCMs weights mostly based on ideas coming from the field of Artificial Neural Networks . Adaptation and learning methodologies used to adapt the FCM model and adjust its weights. Kosko and Dickerson (Dickerson & Kosko, 1994) suggested the Differential Hebbian Learning (DHL) to train FCM. There have been proposed algorithms based on the initial Hebbian algorithm; others algorithms come from the field of genetic algorithms, swarm intelligence and evolutionary computation. Learning algorithms are used to overcome the shortcomings that the traditional FCM present i.e. decreasing the human intervention by suggested automated FCM candidates; or by activating only the most relevant concepts every execution time; or by making models more transparent and dynamic.\n\nFuzzy cognitive maps (FCMs) have gained considerable research interest due to their ability in representing structured knowledge and model complex systems in various fields. This growing interest led to the need for enhancement and making more reliable models that can better represent real situations.\nA first simple application of FCMs is described in a book of William R. Taylor, where the war in Afghanistan and Iraq is analyzed. In Bart Kosko's book \"Fuzzy Thinking\", several Hasse diagrams illustrate the use of FCMs. As an example, one FCM quoted from Rod Taber describes 11 factors of the American cocaine market and the relations between these factors. For computations, Taylor uses pentavalent logic (scalar values out of {-1,-0.5,0,+0.5,+1}). That particular map of Taber uses trivalent logic (scalar values out of {-1,0,+1}). Taber et al. also illustrate the dynamics of map fusion and give a theorem on the convergence of combination in a related article.\n\nWhile applications in social sciences introduced FCMs to the public, they are used in a much wider range of applications, which all have to deal with creating and using models of uncertainty and complex processes and systems. Examples:\n\nFCMappers is an international online community for the analysis and the visualization of fuzzy cognitive maps. FCMappers offer support for starting with FCM and also provide a Microsoft Excel-based tool that is able to check and analyse FCMs. The output is saved as Pajek file and can be visualized within third party software like Pajek, Visone, etc. They also offer to adapt the software to specific research needs.\n\nAdditional FCM software tools, such as Mental Modeler, have recently been developed as a decision-support tool for use in social science research, collaborative decision-making, and natural resource planning.\n\nFuzzy cognitive maps have been further extended to bipolar fuzzy cognitive maps based on bipolar fuzzy sets and bipolar cognitive mapping. Bipolar fuzzy set theory as an equilibrium-based extension to fuzzy sets is recognized by L. A. Zadeh.\n\n"}
{"id": "3376610", "url": "https://en.wikipedia.org/wiki?curid=3376610", "title": "Garabed T. K. Giragossian", "text": "Garabed T. K. Giragossian\n\nGarabed T. K. Giragossian was an Armenian living in Boston who is remembered for developing a perpetual motion device shortly after the turn of the 20th century. He immigrated to America in 1891. In 1917, Giragossian claimed, reportedly fraudulently, to have developed a \"free energy machine\". The assignment of the patent to the United States government was conditionally accepted in Pub. Res. 65-21, , enacted on February 8, 1918, and authorized the Secretary of the Interior to form a scientific committee to investigate the machine. The committee issued a report on July 1, 1918 finding that the principles were not sound. According to an editorial in the Scientific American in 1918, Giragossian claimed that the machine, named Garabed, took energy out of the cosmos and turned it into mechanical motion.\n\nEditors of the Journal of the American Medical Association compared him and his methods with quacks in the medical world; they pointed out that he could not answer the question what qualifications he had to undertake his work, and that he repeatedly only replied, he was an honest man and that he could prove it with signatures of friends and sponsors. They deplored the waste of time of \"scientific men in investigating alleged discoveries by men who are utterly lacking in the fundamental qualifications needed [...].\"\n\nSupposedly involved in a conspiracy, Woodrow Wilson signed a resolution offering him protection. The device was a giant flywheel that was charged up with energy slowly and put out a lot of energy for just a second.\n\n\n"}
{"id": "240953", "url": "https://en.wikipedia.org/wiki?curid=240953", "title": "Gaussian year", "text": "Gaussian year\n\nA Gaussian year is defined as 365.2568983 days. It was adopted by Carl Friedrich Gauss as the length of the sidereal year in his studies of the dynamics of the solar system.\nA slightly different value is now accepted as the length of the sidereal year,\nand the value accepted by Gauss is given a special name.\n\nA particle of negligible mass, that orbits a body of 1 solar mass in this period, has a mean axis for its orbit of 1 astronomical unit by definition. The value is derived from Kepler's third law as\n\nwhere\n"}
{"id": "20445622", "url": "https://en.wikipedia.org/wiki?curid=20445622", "title": "Geography of Halloween", "text": "Geography of Halloween\n\nHalloween, a contraction of \"All Hallows' Eve\", is a celebration observed on 31 October, the day before the feast of All Hallows'. The celebrations and observances of this day occur primarily in regions of the Western world, although some traditions vary significantly between geographical areas.\n\nHalloween, also spelled as Hallowe'en or Allhallowe'en, is a contraction of All Hallows' Eve, the eve or vigil before the Western Christian feast of All Hallows (or All Saints) which is observed on 1 November. This day begins the triduum of Hallowtide, which culminates with All Souls' Day. In the Middle Ages, many Christians held a folk belief that All Hallows' Eve was the \"night where the veil between the material world and the afterlife was at its most transparent.\"\n\nThe Chinese celebrate the \"Hungry Ghost Festival\" in mid-July, when it is customary to float river lanterns to remember those who have died. By contrast, Halloween is often called \"All Saints' Day\" (\"Wànshèngjié\", 萬聖節), or (less commonly, but more correctly) \"All Saints' Eve\" (\"Wànshèngyè\", 萬聖夜) or \"Eve of All Saints' Day\" (\"Wànshèngjié Qiányè\", \"萬聖節前夕\"), stemming from the term \"All Hallows Eve\" (hallow referring to the souls of holy saints). Chinese Christian churches hold religious celebrations. Non-religious celebrations are dominated by expatriate Americans or Canadians, but costume parties are also popular for Chinese young adults, especially in large cities. Hong Kong Disneyland and Ocean Park (Halloween Bash) host annual Halloween shows.\n\nMainland China has been less influenced by Anglo traditions than Hong Kong and Halloween is generally considered \"foreign\". As Halloween has become more popular globally it has also become more popular in China, however, particularly amongst children attending private or international schools with many foreign teachers from North America.\n\nTraditional \"door-to-door\" trick or treating is not commonly practiced in Hong Kong due to the vast majority of Hong Kong residents living in high-rise apartment blocks. However, in many buildings catering to expatriates, Halloween parties and limited trick or treating is arranged by the management. Instances of street-level trick or treating in Hong Kong occur in ultra-exclusive gated housing communities such as The Beverly Hills populated by Hong Kong's super-rich and in expatriate areas like Discovery Bay and the Red Hill Peninsula. For the general public, there are events at Tsim Sha Tsui's Avenue of the Stars that try to mimic the celebration. In the Lan Kwai Fong area of Hong Kong, known as a major entertainment district for the international community, a Halloween celebration and parade has taken place for over 20 years, with many people dressing in costume and making their way around the streets to various drinking establishments. Many international schools also celebrate Halloween with costumes, and some put an academic twist on the celebrations such as the \"Book-o-ween\" celebrations at Hong Kong International School where students dress as favorite literary characters.\n\nHalloween arrived in Japan mainly as a result of American pop culture. As recently as 2009, it was not appreciated and only celebrated by expats. The wearing of elaborate costumes by young adults at night is recently very popular in areas such as Amerikamura in Osaka and Shibuya in Tokyo, where, in October 2012, about 1700 people dressed in costumes to take part in the Halloween Festival. The holiday has become popular with young adults as a costume party and club event. Trick or Treating for Japanese children has taken hold in some areas. The Yakuza have taken advantage of the festival by hosting parties and giving snacks and sweets to children, a tradition which goes back at least for 20 years.\n\nThe period from 31 October through 2 November is a time for remembering dead family members and friends. Many Filipinos travel back to their hometowns for family gatherings of festive remembrance.\n\nTrick-or treating is gradually replacing the dying tradition of \"Pangangaluluwâ\", a local analogue of the old English custom of souling. People in the provinces still observe \"Pangangaluluwâ\" by going in groups to every house and offering a song in exchange for money or food. The participants, usually children, would sing carols about the souls in Purgatory, with the \"abúloy\" (alms for the dead) used to pay for Masses for these souls. Along with the requested alms, householders sometimes gave the children \"suman\" (rice cakes). During the night, various small items, such as clothing, plants, etc., would \"mysteriously\" disappear, only to be discovered the next morning in the yard or in the middle of the street. In older times, it was believed that the spirits of ancestors and loved ones visited the living on this night, manifesting their presence by taking an item.\n\nAs the observation of Christmas traditions in the Philippines begins as early as September, it is a common sight to see Halloween decorations next to Christmas decorations in urban settings.\n\nAround mid-July Singapore Chinese celebrate \"Zhong Yuan Jie / Yu Lan Jie\" (Hungry Ghosts Festival), a time when it is believed that the spirits of the dead come back to visit their families. In recent years, Halloween celebrations are becoming more popular, with influence from the west. In 2012, there were over 19 major Halloween celebration events around Singapore. SCAPE's Museum of Horrors held its fourth scare fest in 2014. Universal Studios Singapore hosts \"Halloween Horror Nights\".\n\nWhile not traditionally a part of Australian culture, non-religious celebrations of Halloween modeled on North American festivities are growing increasingly popular in Australia, in spite of seasonal differences and the transition from spring to summer. Criticism stems largely from the fact that Halloween has little relevance to Australian culture. It is also considered, by some Australians, to be an unwanted American influence; as although Halloween does have Celtic/European origins, its increasing popularity in Australia is largely as a result of American pop-culture influence. Supporters of the event claim that the critics fail to see that the event is not entirely American, but rather Celtic and is no different to embracing other cultural traditions such as Saint Patrick's Day.\n\nDue to the opposition to Halloween by some people, there is a growing movement where people are inviting trick-or-treaters to take part by putting a balloon or decoration on their letter box, to indicate that they are welcome to come knocking. In the past decade, the popularity of Halloween in Australia has grown.\n\nIn New Zealand, as in neighbouring Australia, Halloween is not celebrated to the same extent as in North America, although in recent years the non-religious celebrations have been achieving some popularity especially among young children. Trick-or-treat has become increasingly popular with minors in New Zealand over the years, despite being not a \"British or Kiwi event\" that purely is only influenced by American globalization. Critics of Halloween in New Zealand believe that commercialization of Halloween by the popular store The Warehouse has pushed the popularity of Halloween into an unofficial national holiday.\n\nOver the years, Halloween has become more successful in Europe and has been partially ousting some older customs like the \"\" (), Martinisingen, and others. \n\nOn All Hallow's Eve, a Requiem Mass is widely attended every year at Uppsala Cathedral, part of the Lutheran Church of Sweden.\n\nThroughout the period of Allhallowtide, starting with All Hallow's Eve, Swedish families visit churchyards and adorn the graves of their family members with lit candles and wreaths fashioned from pine branches.\n\nAmong children, the practice of dressing in costume and collecting candy has gained popularity in recent years.\n\nHalloween is a work day in Bosnia and Herzegovina, and not celebrated until recently. For the past few years, it has been popular among younger generations. Since wearing masks has become highly popular among children and teenagers, e.g. in many Bosnian schools, both elementary as well as high schools (secondary schools and vocational), students will usually wear costumes and masks on Halloween. There it is called \"Noć vještica\" ().\n\nHalloween was not generally observed in Germany prior to the 1990s, but has been increasing in popularity. It has been associated with the influence of United States culture, and \"Trick or Treating\" (in German, \"Süßes sonst gibt's Saures\") has been occurring in various German cities, especially in areas such as the Dahlem neighborhood in Berlin, which was part of the American zone during the Cold War. Today, Halloween in Germany brings in 200 million euros a year, through multiple industries. Halloween is celebrated by both children and adults. Adults celebrate at themed costume parties and clubs, while children go trick or treating. Complaints of vandalism associated with Halloween \"Tricks\" are increasing, particularly from many elderly Germans unfamiliar with \"Trick or Treating\".\n\nHalloween has been increasing in popularity in Greece during the 2010s, as a commercial and secular celebration. It has been associated with the influence of the western culture. Bars, nightclubs and fun parks organise Halloween parties.\n\nOn Halloween night, adults and children dress up as ghosts, ghouls, zombies, witches, and goblins, light bonfires, and enjoy spectacular fireworks displays – in particular, the city of Derry is home to the largest organized Halloween celebration on the island, in the form of a street carnival and fireworks display.\nGames are often played, such as bobbing for apples, in which apples, peanuts, and other nuts and fruit and some small coins are placed in a basin of water. Everyone takes turns catching as many items possible using only their mouths. Another common game involves the hands-free eating of an apple hung on a string attached to the ceiling. Games of divination are also played at Halloween. Colcannon is traditionally served on Halloween.\n\n31 October is the busiest day of the year for the Emergency Services. Bangers and fireworks are illegal in the Republic of Ireland; however, they are commonly smuggled in from Northern Ireland where they are legal. Bonfires are frequently built around Halloween. Trick-or-treating is popular amongst children on 31 October and Halloween parties and events are commonplace.\n\nIn Italy All Saints' Day is a public holiday. On 1 November, \"Tutti i Morti\" or All Souls' Day, families remember loved ones who have died. These are still the main holidays. In some Italian tradition, children would awake on the morning of All Saints or All Souls to find small gifts from their deceased ancestors. In Sardinia, \"Concas de Mortu\" (Head of the deads), carved pumpkins that look like skulls, with candles inside are displayed. Halloween is, however, gaining in popularity, and involves costume parties for young adults. The traditions to carve pumpkins in a skull figure, lighting candles inside, or to beg for small gifts for the deads e.g. sweets or nuts, also belong to North Italy. In Veneto these carved pumpkins were called \"lumère\" (lanterns) or \"suche dei morti\" (deads' pumpkins).\n\nRomanians observe the Feast of St. Andrew, patron saint of Romania, on 30 November. On St. Andrew's Eve ghosts are said to be about. A number of customs related to divination, in other places connected to Halloween, are associated with this night. However, with the popularity of Dracula in western Europe, around Halloween the Romanian tourist industry promotes trips to locations connected to the historical Vlad Tepeș and the more fanciful Dracula of Bram Stoker. The most successful Halloween Party in Transylvania takes place in Sighișoara, the citadel where Vlad the Impaler was born.\n\nBoth the Catholic and Orthodox Churches in Romania discourage Halloween celebrations, advising their parishioners to focus rather on the \"Day of the Dead\" on 1 November, when special religious observances are held for the souls of the deceased. Opposition by religious and nationalist groups, including calls to ban costumes and decorations in schools in 2015, have been met with criticism. Halloween parties are popular in bars and nightclubs.\n\nIn Russia most Christians are Orthodox, and in the Orthodox Church Halloween is on the Saturday after Pentecost and therefore 4–5 months before western Halloween. Celebration of western Halloween began in the 1990s around the downfall of the Soviet regime, when costume and ghoulish parties spread throughout night clubs throughout Russia. Halloween is generally celebrated by younger generations and is not widely celebrated in civic society (e.g. theaters or libraries). In fact, Halloween is among the Western celebrations that the Russian government and politicians—which have grown increasingly anti-Western in the early 2010s—are trying to eliminate from public celebration.\n\nSince the fall of Communism in 1989, Halloween has become increasingly popular in Poland and the Czech Republic. Particularly, it is celebrated among younger people. The influx of Western tourists and expats throughout the 1990s introduced the costume party aspect of Hallowe'en celebrations, particularly in clubs and at private house parties. Door-to-door trick or treating is not common. Pumpkin carving is becoming more evident, following a strong North American version of the tradition.\n\nIn Switzerland, Halloween, after first becoming popular in 1999, is on the wane, and is most popular with young adults who attend parties. Switzerland already has a \"festival overload\" and even though Swiss people like to dress up for any occasion, they do prefer a traditional element, such as in the Fasnacht tradition of chasing away winter using noise and masks.\n\nIn the past, on All Souls' Eve families would stay up late, and little \"soul cakes\" were eaten. At the stroke of midnight, there was solemn silence among households, which had candles burning in every room to guide the souls back to visit their earthly homes and a glass of wine on the table to refresh them. The tradition of giving soul cakes that originated in Great Britain and Ireland was known as souling, often seen as the origin of modern trick or treating in North America, and souling continued in parts of England as late as the 1930s, with children going from door to door singing songs and saying prayers for the dead in return for cakes or money.\n\nAmerican-style Halloween celebrations have become increasingly popular with shops decorated with witches and pumpkins, and young people attending costume parties.\n\nThe name \"Halloween\" is first attested in the 16th century as a Scottish shortening of the fuller \"All-Hallow-Even\", that is, the night before All Hallows' Day. Dumfries poet John Mayne's 1780 poem made note of pranks at Halloween \"What fearfu' pranks ensue!\". Scottish poet Robert Burns was influenced by Maynes composition, and portrayed some of the customs in his poem \"Halloween\" (1785). According to Burns, Halloween is \"thought to be a night when witches, devils, and other mischief-making beings are all abroad on their baneful midnight errands\".\n\nAmong the earliest record of Guising at Halloween in Scotland is in 1895, where masqueraders in disguise carrying lanterns made out of scooped out turnips, visit homes to be rewarded with cakes, fruit and money. If children approached the door of a house, they were given offerings of food. The children's practice of \"guising\", going from door to door in costumes for food or coins, is a traditional Halloween custom in Scotland. These days children who knock on their neighbours doors have to sing a song or tell stories for a gift of sweets or money.\n\nA traditional Halloween game includes apple \"dooking\", or \"dunking\" or (i.e., retrieving one from a bucket of water using only one's mouth), and attempting to eat, while blindfolded, a treacle/jam-coated scone hanging on a piece of string.\n\nTraditional customs and lore include divination practices, ways of trying to predict the future. A traditional Scottish form of divining one's future spouse is to carve an apple in one long strip, then toss the peel over one's shoulder. The peel is believed to land in the shape of the first letter of the future spouse's name.\n\nScottish emigration, primarily to Canada before 1870 and to the United States thereafter, brought the Scottish version of the holiday to each country. The earliest known reference to ritual begging on Halloween in English speaking North America occurs in 1911, when a newspaper in Kingston, Ontario reported that it was normal for the smaller children to go street \"guising\" on Halloween between 6 and 7 p.m., visiting shops and neighbours to be rewarded with nuts and candies for their rhymes and songs. Canadians spend more on candy at Halloween than at any time apart from Christmas. Halloween is also a time for charitable contributions. Until 2006 when UNICEF moved to an online donation system, collecting small change for was very much a part of Canadian trick-or-treating. Quebec offers themed tours of parts of the old city and historic cemeteries in the area. In 2014 the hamlet of Arviat, Nunavut moved their Halloween festivities to the community hall, cancelling the practice of door-to-door \"trick or treating\", due to the risk of roaming polar bears. In British Columbia it is a tradition to set off fireworks at Halloween.\n\nIn the United States, where lingering Puritan tradition restricted the observance of many holidays, Halloween did not become a holiday until the 19th century. The transatlantic migration of nearly two million Irish following the Irish Great Famine (1845–1849) brought the holiday to the United States.\n\nAmerican librarian and author Ruth Edna Kelley wrote the first book length history of the holiday in the U.S., \"The Book of Hallowe'en\" (1919), and references souling in the chapter \"Hallowe'en in America\": \"All Hallowe'en customs in the United States are borrowed directly or adapted from those of other countries. The taste in Hallowe'en festivities now is to study old traditions, and hold a Scotch party, using Robert Burns's poem \"Halloween\" as a guide; or to go a-souling as the English used. In short, no custom that was once honored at Hallowe'en is out of fashion now.\" The main event for children of modern Halloween in the United States and Canada is trick-or-treating, in which children, teenagers, (sometimes) young adults, and parents (accompanying their children) disguise themselves in costumes and go door-to-door in their neighborhoods, ringing each doorbell and yelling \"Trick or treat!\" to solicit a gift of candy or similar items. Teenagers and adults will more frequently attend Halloween-themed costume parties typically hosted by friends or themed events at nightclubs either on Halloween itself or a weekend close to the holiday.\nAt the turn of the 20th century, Halloween had turned into a night of vandalism, with destruction of property and cruelty to animals and people. Around 1912, the Boy Scouts, Boys Clubs, and other neighborhood organizations came together to encourage a safe celebration that would end the destruction that had become so common on this night.\n\nThe commercialization of Halloween in the United States did not start until the 20th century, beginning perhaps with Halloween postcards (featuring hundreds of designs), which were most popular between 1905 and 1915. Dennison Manufacturing Company (which published its first Halloween catalog in 1909) and the Beistle Company were pioneers in commercially made Halloween decorations, particularly die-cut paper items. German manufacturers specialised in Halloween figurines that were exported to the United States in the period between the two World Wars.\nHalloween is now the United States' second most popular holiday (after Christmas) for decorating; the sale of candy and costumes is also extremely common during the holiday, which is marketed to children and adults alike. The National Confectioners Association (NCA) reported in 2005 that 80% of American adults planned to give out candy to trick-or-treaters.\nThe NCA reported in 2005 that 93% of children planned to go trick-or-treating. According to the National Retail Federation, the most popular Halloween costume themes for adults are, in order: witch, pirate, vampire, cat, and clown. Each year, popular costumes are dictated by various current events and pop culture icons. On many college campuses, Halloween is a major celebration, with the Friday and Saturday nearest 31 October hosting many costume parties. Other popular activities are watching horror movies and visiting haunted houses. Total spending on Halloween is estimated to be $8.4 billion.\n\nMany theme parks stage Halloween events annually, such as Halloween Horror Nights at Universal Studios Hollywood and Universal Orlando, Mickey's Halloween Party and Mickey's Not-So-Scary Halloween Party at Disneyland Resort and Magic Kingdom respectively, and Knott's Scary Farm at Knott's Berry Farm. One of the more notable parades is New York's Village Halloween Parade. Each year approximately 50,000 costumed marchers parade up Sixth Avenue. Salem, Massachusetts, site of the Salem witch trials, celebrates Halloween throughout the month of October with tours, plays, concerts, and other activities. A number of venues in New York's lower Hudson Valley host various events to showcase a connection with Washington Irving's \"Legend of Sleepy Hollow\". Van Cortlandt Manor stages the \"Great Jack o' Lantern Blaze\" featuring thousands of lighted carved pumpkins.\n\nSome locales have had to modify their celebrations due to disruptive behavior on the part of young adults. Madison, Wisconsin hosts an annual Halloween celebrations. In 2002, due to the large crowds in the State Street area, a riot broke out, necessitating the use of mounted police and tear gas to disperse the crowds. Likewise, Chapel Hill, site of the University of North Carolina, has a downtown street party which in 2007 drew a crowd estimated at 80,000 on downtown Franklin Street, in a town with a population of just 54,000. In 2008, in an effort to curb the influx of out-of-towners, mayor Kevin Foy put measures in place to make commuting downtown more difficult on Halloween. In 2014, large crowds of college students rioted at the Keene, New Hampshire Pumpkin Fest, whereupon the City Council voted not to grant a permit for the following year's festival, and organizers moved the event to Laconia for 2015.\n\nIn Saint Helena, Halloween is actively celebrated, largely along the American model, with ghosts, skeletons, devils, vampires, witches and the like. Imitation pumpkins are used instead of real pumpkins because the pumpkin harvesting season in Saint Helena's hemisphere is not near Halloween. Trick-or-treating is widespread. Party venues provide entertainment for adults.\n\nThe \"Wild Wadi Waterpark\" in Dubai hosts a \"Spooktacular Halloween\". The annual Halloween masquerade ballroom dancing party takes place at \"Dance For You\" studio.\n\nIn the Dominican Republic it has been gaining popularity, largely due to many Dominicans living in the States and then bringing it to the island. In the larger cities of Santiago or Santo Domingo it has become more common to see children trick-or-treating, but in smaller towns and villages it is almost entirely absent, partly due to religious opposition. Tourist areas such as Sosua and Punta Cana feature many venues with Halloween celebrations, predominantly geared towards adults.\n"}
{"id": "275051", "url": "https://en.wikipedia.org/wiki?curid=275051", "title": "HIV/AIDS denialism", "text": "HIV/AIDS denialism\n\nHIV/AIDS denialism is the belief, contradicted by conclusive medical and scientific evidence, that human immunodeficiency virus (HIV) does not cause acquired immune deficiency syndrome (AIDS). Some of its proponents reject the existence of HIV, while others accept that HIV exists but argue that it is a harmless passenger virus and not the cause of AIDS. Insofar as they acknowledge AIDS as a real disease, they attribute it to some combination of sexual behavior, recreational drugs, malnutrition, poor sanitation, haemophilia, or the effects of the drugs used to treat HIV infection.\n\nThe scientific consensus is that the evidence showing HIV to be the cause of AIDS is conclusive and that HIV/AIDS denialist claims are pseudoscience based on conspiracy theories, faulty reasoning, cherry picking, and misrepresentation of mainly outdated scientific data. With the rejection of these arguments by the scientific community, HIV/AIDS denialist material is now targeted at less scientifically sophisticated audiences and spread mainly through the Internet.\n\nDespite its lack of scientific acceptance, HIV/AIDS denialism has had a significant political impact, especially in South Africa under the presidency of Thabo Mbeki. Scientists and physicians have raised alarm at the human cost of HIV/AIDS denialism, which discourages HIV-positive people from using proven treatments. Public health researchers have attributed 330,000 to 340,000 AIDS-related deaths, along with 171,000 other HIV infections and 35,000 infant HIV infections, to the South African government's former embrace of HIV/AIDS denialism. The interrupted use of antiretroviral treatments is also a major global concern as it potentially increases the likelihood of the emergence of antiretroviral-resistant strains of the virus.\nA constellation of symptoms named \"Gay-related immune deficiency\" was noted in 1982. In 1983, a group of scientists and doctors at the Pasteur Institute in France, led by Luc Montagnier, discovered a new virus in a patient with signs and symptoms that often preceded AIDS. They named the virus \"lymphadenopathy-associated virus\", or LAV, and sent samples to Robert Gallo's team in the United States. Their findings were peer reviewed and slated for publication in \"Science\".\n\nAt a 23 April 1984 press conference in Washington, D.C., Margaret Heckler, Secretary of Health and Human Services, announced that Gallo and his co-workers had discovered a virus that is the \"probable\" cause of AIDS. This virus was initially named HTLV-III. That same year, Casper Schmidt responded to Gallo's papers with \"The Group-Fantasy Origins of AIDS\", \"Journal of Psychohistory\". Schmidt posited that AIDS was not an actual disease, but rather an example of \"epidemic hysteria\" in which groups of people are subconsciously acting out social conflicts. Schmidt compared AIDS to documented cases of epidemic hysteria in the past which were mistakenly thought to be infectious. (Schmidt himself would later die of AIDS in 1994.)\n\nIn 1986, the viruses discovered by Montagnier and Gallo, found to be genetically indistinguishable, were renamed HIV.\n\nIn 1987, the molecular biologist Peter Duesberg questioned the link between HIV and AIDS in the journal \"Cancer Research\". Duesberg's publication coincided with the start of major public health campaigns and the development of zidovudine (AZT) as a treatment for HIV/AIDS.\n\nIn 1988, a panel of the Institute of Medicine of the U.S. National Academy of Sciences found that \"the evidence that HIV causes AIDS is scientifically conclusive.\" That same year, \"Science\" published Blattner, Gallo, and Temin's \"HIV causes AIDS\", and Duesberg's \"HIV is not the cause of AIDS\". Also that same year, the Perth Group, a group of denialists based in Perth, Western Australia led by Eleni Papadopulos-Eleopulos, published in the non-peer-reviewed journal \"Medical Hypotheses\" their first article questioning aspects of HIV/AIDS research, arguing that there was \"no compelling reason for preferring the viral hypothesis of AIDS to one based on the activity of oxidising agents.\"\n\nIn 1989, Duesberg exercised his right, as a member of the National Academy of Sciences, to bypass the peer review process and publish his arguments in \"Proceedings of the National Academy of Sciences of the United States of America\" (\"PNAS\") unreviewed. The editor of \"PNAS\" initially resisted, but ultimately allowed Duesberg to publish, saying, \"If you wish to make these unsupported, vague, and prejudicial statements in print, so be it. But I cannot see how this would be convincing to any scientifically trained reader.\"\n\nIn 1990, Robert Root-Bernstein published his first peer-reviewed article detailing his objections to the mainstream view of AIDS and HIV. In it, he questioned both the mainstream view and the \"dissident\" view as potentially inaccurate.\n\nIn 1991, The Group for the Scientific Reappraisal of the HIV-AIDS Hypothesis, comprising twelve scientists, doctors, and activists, submitted a short letter to various journals, but the letter was rejected.\n\nIn 1993, \"Nature\" published an editorial arguing that Duesberg had forfeited his right of reply by engaging in disingenuous rhetorical techniques and ignoring any evidence that conflicted with his claims. That same year, Papadopulos-Eleopulos et al. of the Perth Group, alleged in the journal \"Nature Biotechnology\" (then edited by fellow denialist Harvey Bialy) that the Western blot test for HIV was not standardized, non-reproducible, and of unknown specificity due to a claimed lack of a \"gold standard\".\n\nOn 28 October 1994, Robert Willner, a physician whose medical license had been revoked for, among other things, treating an AIDS patient with ozone therapy, publicly jabbed his finger with blood he said was from an HIV-infected patient. Willner died in 1995 of a heart attack.\n\nIn 1995, The Group for the Scientific Reappraisal of the HIV-AIDS Hypothesis in 1991 published a letter in \"Science\" similar to the one they had attempted to publish in 1991. That same year, Continuum, a denialist group, placed an advertisement in the British gay and lesbian magazine \"The Pink Paper\" offering a £1,000 reward to \"the first person finding one scientific paper establishing actual isolation of HIV\", according to a set of seven steps they claimed to have been drawn up by the Pasteur Institute in 1973. The challenge was later dismissed by various scientists, including Duesberg, asserting that HIV undoubtedly exists. Stefan Lanka argued in the same year that HIV does not exist. Also that year, the National Institute of Allergy and Infectious Diseases released a report concluding that \"abundant epidemiologic, virologic and immunologic data support the conclusion that infection with the human immunodeficiency virus (HIV) is the underlying cause of AIDS.\"\n\nIn 1996, the \"British Medical Journal\" published \"Response: arguments contradict the \"foreign protein-zidovudine\" hypothesis\" as a response to a petition by Duesberg: \"In 1991 Duesberg challenged researchers… We and Darby et al. have provided that evidence\". The paper argued that Duesberg was wrong regarding the cause of AIDS in haemophiliacs.\nIn 1997, The Perth Group questioned the existence of HIV, and speculated that the production of antibodies recognizing HIV proteins can be caused by allogenic stimuli and autoimmune disorders. They continued to repeat this speculation through at least 2006.\n\nIn 1998, Joan Shenton published the book \"Positively False – Exposing the Myths Around HIV and AIDS\", which promotes AIDS denialism. In the book, Shenton claims that AIDS is a conspiracy created by pharmaceutical companies to make money from selling antiretroviral drugs.\n\nIn 2006, Celia Farber, a journalist and prominent HIV/AIDS denialist, published an essay in the March issue of \"Harper's Magazine\" entitled \"Out of Control: AIDS and the Corruption of Medical Science\", in which she summarized a number of arguments for HIV/AIDS denialism and alleged incompetence, conspiracy, and fraud on the part of the medical community. Scientists and AIDS activists extensively criticized the article as inaccurate, misleading, and poorly fact-checked.\n\nIn 2007, members of the Perth Group testified at an appeals hearing for Andre Chad Parenzee, asserting that HIV could not be transmitted by heterosexual sex. The judge concluded, \"I reject the evidence of Ms Papadopulos-Eleopulos and Dr Turner. I conclude… that they are not qualified to give expert opinions.\"\n\nIn 2009, a paper was published in the then non-peer-reviewed journal \"Medical Hypotheses\" by Duesberg and four other researchers which criticized a 2008 study by Chigwedere et al. which found that HIV/AIDS denialism in South Africa resulted in hundreds of thousands of preventable deaths from HIV/AIDS, because the government delayed the provision of antiretroviral drugs. The paper concluded that \"the claims that HIV has caused huge losses of African lives are unconfirmed and that HIV is not sufficient or even necessary to cause the previously known diseases, now called AIDS in the presence of antibody against HIV.\" Later that year, the paper was withdrawn from the journal on the grounds of it having methodological flaws, and that it contained assertions \"that could potentially be damaging to global public health”.\n\nIn 1998, HIV/AIDS denialism and parental rights clashed with the medical establishment in court when Maine resident Valerie Emerson fought for the right to refuse to give AZT to her four-year-old son, Nikolas Emerson, after she witnessed the death of her daughter Tia, who died at the age of three in 1996. Her right to stop treatment was upheld by the court in light of \"her unique experience.\" Nikolas Emerson died eight years later. The family refused to reveal whether the death was AIDS related.\n\nIn 2000, South Africa's President Thabo Mbeki invited several HIV/AIDS denialists to join his Presidential AIDS Advisory Panel. A response named the Durban Declaration was issued affirming the scientific consensus that HIV causes AIDS:\n\nIn 2008, University of Cape Town researcher Nicoli Nattrass, and later that year a group of Harvard scientists led by Zimbabwean physician Pride Chigwedere each independently estimated that Thabo Mbeki's denialist policies led to the early deaths of more than 330,000 South Africans. Barbara Hogan, the health minister appointed by Mbeki's successor, voiced shame over the studies' findings and stated: \"The era of denialism is over completely in South Africa.\"\n\nIn 2009, Fraser McNeill wrote an article arguing that South Africa's reluctance to openly address HIV/AIDS resulted from social conventions that prevent people from talking about causes of death in certain situations, rather than from Mbeki's denialist views. Similarly, political scientist Anthony Butler has argued that \"South African HIV/AIDS policy can be explained without appeals to leadership irrationality or wider cultural denialism.\"\n\nIn July 2016 Aaron Motsoaledi, the Health Minister of South Africa, wrote an article for the Centre for Health Journalism in which he criticised past South African leaders for their denialism, describing it as an \"unlucky moment\" in a country which has since become a leader in treatment and prevention.\n\nAlthough members of the HIV/AIDS denialist community are united by their disagreement with the scientific finding that HIV is the cause of AIDS, the specific positions taken by various groups differ. Denialists claim many incompatible things: HIV does not exist; HIV has not been adequately isolated, HIV does not fulfill Koch's postulates, HIV testing is inaccurate, and that antibodies to HIV neutralize the virus and render it harmless. Suggested alternative causes of AIDS include recreational drugs, malnutrition, and the very antiretroviral drugs used to treat the syndrome.\n\nSuch claims have been examined extensively in the peer-reviewed medical and scientific literature; a scientific consensus has arisen that denialist claims have been convincingly disproved, and that HIV does indeed cause AIDS. In the cases cited by Duesberg where HIV \"cannot be isolated\", PCR or other techniques demonstrate the presence of the virus, and denialist claims of HIV test inaccuracy result from an incorrect or outdated understanding of how HIV antibody testing is performed and interpreted. Regarding Koch's postulates, \"New Scientist\" reported: \"It is debatable how appropriate it is to focus on a set of principles devised for bacterial infections in a century when viruses had not yet been discovered. HIV does, however, meet Koch's postulates as long as they are not applied in a ridiculously stringent way\". The author then demonstrated how each postulate has been met – the suspected cause is strongly associated with the disease, the suspected pathogen can be both isolated and spread outside the host, and when the suspected pathogen is transmitted to a new and uninfected host, that host develops the disease. The latter was proven in a number of tragic accidents, including an instance when multiple scientific technicians with no other known risk factors were exposed to concentrated HIV virus in a laboratory accident, and transmission by a dentist to patients, the majority of whom had no other known risk factor or source of exposure except the same dentist in common. In 2010, Chigwedere and Max Essex demonstrated in the medical journal \"AIDS and Behavior\" that HIV as the cause of AIDS fulfills both Koch's postulates and the Bradford Hill criteria for causality.\n\nEarly denialist arguments held that the HIV/AIDS paradigm was flawed because it had not led to effective treatments. However, the introduction of highly active antiretroviral therapy in the mid-1990s and dramatic improvements in survival of HIV/AIDS patients reversed this argument, as these treatments were based directly on anti-viral activity and the HIV/AIDS paradigm. The development of effective anti-AIDS therapies based on targeting of the HIV virus has been a major factor in convincing some denialist scientists to accept the causative role of HIV in AIDS.\n\nIn a 2010 article on conspiracy theories in science, Ted Goertzel lists HIV/AIDS denialism as an example where scientific findings are being disputed on irrational grounds. He describes proponents as relying on rhetoric, appeal to fairness, and the right to a dissenting opinion rather than on evidence. They frequently invoke the meme of a \"courageous independent scientist resisting orthodoxy\", invoking the name of persecuted physicist and astronomer Galileo Galilei. Regarding this comparison, Goertzel states:\nDenialists often use their critique of the link between HIV and AIDS to promote alternative medicine as a cure, and attempt to convince HIV-infected individuals to avoid ARV therapy in favour of vitamins, massage, yoga and other unproven treatments. Despite this promotion, denialists will often downplay any association with alternative therapies, and attempt to portray themselves as \"dissidents\". An article in the \"Skeptical Inquirer\" stated:\nSeveral scientists have been associated with HIV/AIDS denialism, although they have not themselves studied AIDS or HIV. One of the most famous and influential is Duesberg, professor of molecular and cell biology at the University of California, Berkeley, who since 1987 has disputed that the scientific evidence shows that HIV causes AIDS. Other scientists associated with HIV/AIDS denialism include biochemists David Rasnick and Harvey Bialy. Kary Mullis, who was awarded a Nobel Prize for his role in the development of the polymerase chain reaction, has expressed sympathy for denialist theories. Biologist Lynn Margulis argued that \"there's no evidence that HIV is an infectious virus\" and that AIDS symptoms \"overlap...completely\" with those of syphilis. Pathologist Étienne de Harven also expressed sympathy for HIV/AIDS denial.\n\nAdditional notable HIV/AIDS denialists include Australian academic ethicist Hiram Caton, the late mathematician Serge Lang, former college administrator Henry Bauer, journalist Celia Farber, American talk radio host and author on alternative and complementary medicine and nutrition Gary Null, and the late activist Christine Maggiore, who encouraged HIV-positive mothers to forgo anti-HIV treatment and whose 3-year-old daughter died of complications of untreated AIDS. Nate Mendel, bassist with the rock band Foo Fighters, expressed support for HIV/AIDS denialist ideas and organized a benefit concert in January 2000 for Maggiore's organization Alive & Well AIDS Alternatives. Organizations of HIV/AIDS denialists include the Perth Group, composed of several Australian hospital workers, and the Immunity Resource Foundation.\n\nHIV/AIDS denialism has received some support from political conservatives in the United States. Duesberg's work has been published in \"Policy Review\", a journal once published by The Heritage Foundation but later acquired by the Hoover Institution, and by Regnery Publishing. Regnery published Duesberg's \"Inventing the AIDS Virus\" in 1996, and journalist Tom Bethell's \"The Politically Incorrect Guide to Science\", in which he endorses HIV/AIDS denialism, in 2005. Law professor Phillip E. Johnson has accused the Centers for Disease Control of \"fraud\" in relation to HIV/AIDS. Describing the political aspects of the HIV/AIDS denialism movement, Sociology professor Steven Epstein wrote in \"Impure Science\" that \"... the appeal of Duesberg's views to conservatives—certainly including those with little sympathy for the gay movement—cannot be denied.\" The blog LewRockwell.com has also published articles supportive of HIV/AIDS denialism.\n\nIn a follow-up article in \"Skeptical Inquirer\", Nattrass overviewed the prominent members of the HIV/AIDS denialist community and discussed the reasons of the intractable staying power of HIV/AIDS denialism in spite of scientific and medical consensus supported by over two decades of evidence. She observed that despite being a disparate group of people with very different background and professions, the HIV/AIDS denialists self-organize to fill four important roles:\nSome of them had overlapping roles as board members of Rethinking AIDS and Alive and Well AIDS Alternatives, were involved in the film \"House of Numbers\", \"The Other Side of AIDS\" or on Thabo Mbeki's AIDS Advisory Panel.\nNattrass argued that HIV/AIDS denialism gains social traction through powerful community-building effects where these four organized characters form \"a symbiotic connection between AIDS denialism and alternative healing modalities\" and they are \"facilitated by a shared conspiratorial stance toward HIV science\".\n\nSeveral of the few prominent scientists who once voiced doubts about HIV/AIDS have since changed their views and accepted the fact that HIV plays a role in causing AIDS, in response to an accumulation of newer studies and data. Root-Bernstein, author of \"Rethinking AIDS: The Tragic Cost of Premature Consensus\" and formerly a critic of the causative role of HIV in AIDS, has since distanced himself from the HIV/AIDS denialist movement, saying, \"Both the camp that says HIV is a pussycat and the people who claim AIDS is all HIV are wrong...The denialists make claims that are clearly inconsistent with existing studies.\"\n\nJoseph Sonnabend, who until the late 1990s regarded the issue of AIDS causation as unresolved, has reconsidered in light of the success of newer antiretroviral drugs, stating, \"The evidence now strongly supports a role for HIV… Drugs that can save your life can also under different circumstances kill you. This is a distinction that denialists do not seem to understand.\" Sonnabend has also criticized HIV/AIDS denialists for falsely implying that he supports their position, saying:\nA former denialist wrote in the \"Journal of Medical Ethics\" in 2004:\nIn 2007, aidstruth.org, a website run by HIV researchers to counter denialist claims, published a partial list of HIV/AIDS denialists who had died of AIDS-related causes. For example, the editors of the magazine \"Continuum\" consistently denied the existence of HIV/AIDS. The magazine shut down after both editors died of AIDS-related causes. In each case, the HIV/AIDS denialist community attributed the deaths to unknown causes, secret drug use, or stress rather than HIV/AIDS. Similarly, several HIV-positive former dissidents have reported being ostracized by the AIDS-denialist community after they developed AIDS and decided to pursue effective antiretroviral treatment.\n\nIn 2008, activist Christine Maggiore died at the age of 52 while under a doctor's care for pneumonia. Maggiore, mother of two children, had founded an organisation to help other HIV-positive mothers avoid taking antiretroviral drugs that reduce the risk of HIV transmission from mother to child. After her three-year-old daughter died of AIDS-related pneumonia in 2005, Maggiore continued to believe that HIV is not the cause of AIDS, and she and her husband Robin Scovill sued Los Angeles County and others on behalf of their daughter's estate, for allegedly violating Eliza Scovill's civil rights by releasing an autopsy report that listed her cause of death as AIDS-related pneumonia. The litigants settled out of court, with the county paying Scovill $15,000 in March 2009, with no admission of wrongdoing. The L.A. coroner's ruling that Eliza Jane Scovill died of AIDS remains standing as the official verdict.\n\nAustralia: In 2009 representing the Australian Vaccination-Skeptics Network, President Meryl Dorey signed a petition claiming that \"the AIDS industry and the media\" had tricked the public and the media into believing that the HIV virus causes AIDS.\n\nCanada: The Alberta Reappraising AIDS Society created the petition in March 2000 and has reportedly since attracted \"2,951 doubters\" representing groups and individuals through the globe. Signatories reportedly deny the theory \"that Aids is heterosexually transmitted\".\n\nAIDS-denialist claims have failed to attract support in the scientific community, where the evidence for the causative role of HIV in AIDS is considered conclusive. However, the movement has had a significant impact in the political sphere, culminating with former South African President Thabo Mbeki's embrace of AIDS-denialist claims. The resulting governmental refusal to provide effective anti-HIV treatment in South Africa has been blamed for hundreds of thousands of premature AIDS-related deaths in South Africa.\n\nSkepticism about HIV being the cause of AIDS began almost immediately after the discovery of HIV was announced. One of the earliest prominent skeptics was the journalist John Lauritsen, who argued in his writings for the \"New York Native\" that AIDS was caused by amyl nitrite poppers, and that the government had conspired to hide the truth. Lauritsen's \"The AIDS War\" was published in 1993.\n\nThe publication of Duesberg's first AIDS paper in 1987 provided visibility for denialist claims. Shortly afterwards, the journal \"Science\" reported that Duesberg's remarks had won him \"a large amount of media attention, particularly in the gay press where he is something of a hero.\" However, Duesberg's support in the gay community diminished as he made a series of statements perceived as homophobic; in an interview with \"The Village Voice\" in 1988, Duesberg stated his belief that the AIDS epidemic was \"caused by a lifestyle that was criminal twenty years ago.\"\n\nIn the following few years, others became skeptical of the HIV theory as researchers initially failed to produce an effective treatment or vaccine for AIDS. Journalists such as Neville Hodgkinson and Celia Farber regularly promoted denialist ideas in the American and British media; several television documentaries were also produced to increase awareness of the alternative viewpoint. In 1992–1993, \"The Sunday Times\", where Hodgkinson served as scientific editor, ran a series of articles arguing that the AIDS epidemic in Africa was a myth. These articles stressed Duesberg's claims and argued that antiviral therapy was ineffective, HIV testing unreliable, and that AIDS was not a threat to heterosexuals. The \"Sunday Times\" coverage was heavily criticized as slanted, misleading, and potentially dangerous; the scientific journal \"Nature\" took the unusual step of printing a 1993 editorial calling the paper's coverage of HIV/AIDS \"seriously mistaken, and probably disastrous.\"\n\nFinding difficulty in publishing his arguments in the scientific literature, Duesberg exercised his right as a member of the National Academy of Sciences to publish in \"Proceedings of the National Academy of Sciences of the United States of America\" (\"PNAS\") without going through the peer review process. However, Duesberg's paper raised a \"red flag\" at the journal and was submitted by the editor for non-binding review. All of the reviewers found major flaws in Duesberg's paper; the reviewer specifically chosen by Duesberg noted the presence of \"misleading arguments\", \"nonlogical statements\", \"misrepresentations\", and political overtones. Ultimately, the editor of \"PNAS\" acquiesced to publication, writing to Duesberg: \"If you wish to make these unsupported, vague, and prejudicial statements in print, so be it. But I cannot see how this would be convincing to any scientifically trained reader.\"\n\nHIV/AIDS denialists often resort to special pleading to support their assertion, arguing for different causes of AIDS in different locations and subpopulations. In North America, AIDS is blamed on the health effects of unprotected anal sex and poppers on homosexual men, an argument which does not account for AIDS in drug-free heterosexual women who deny participating in anal sex. In this case, HIV/AIDS denialists claim the women are having anal sex but refuse to disclose it. In haemophiliac North American children who contracted AIDS from blood transfusions, the haemophilia itself or its treatment is claimed to cause AIDS. In Africa, AIDS is blamed on poor nutrition and sanitation due to poverty. For wealthy populations in South Africa with adequate nutrition and sanitation, it is claimed that the antiretroviral drugs used to treat AIDS cause the condition. In each case, the most parsimonious explanation and uniting factor – HIV positive status – is ignored, as are the thousands of studies that converge on the common conclusion that AIDS is caused by HIV infection.\n\nHaemophilia is considered the best test of the HIV-AIDS hypothesis by both denialists and AIDS researchers. While Duesberg claims AIDS in haemophiliacs is caused by contaminated clotting factors and HIV is a harmless passenger virus, this result is contradicted by large studies on haemophiliac patients who received contaminated blood. A comparison of groups receiving high, medium and low levels of contaminated clotting factors found the death rates differed significantly depending on HIV status. Of 396 HIV positive haemophiliacs followed between 1985 and 1993, 153 died. The comparative figure for the HIV negative group was one out of 66, despite comparable doses of contaminated clotting factors. A comparison of individuals receiving blood donations also supports the results; in 1994 there were 6888 individuals with AIDS who had their HIV infection traced to blood transfusions. Since the introduction of HIV testing, the number of individuals whose AIDS status can be traced to blood transfusions was only 29 (as of 1994).\n\nWith the introduction of highly active antiretroviral therapy (HAART) in 1996–1997, the survival and general health of people with HIV improved significantly. The positive response to treatment with anti-HIV medication cemented the scientific acceptance of the HIV/AIDS paradigm, and led several prominent HIV/AIDS denialists to accept the causative role of HIV. Finding their arguments increasingly discredited by the scientific community, denialists took their message to the popular press. A former denialist wrote:\nIn addition to elements of the popular and alternative press, AIDS denialist ideas are propagated largely via the Internet.\n\nA 2007 article in \"PLoS Medicine\" noted:\nAIDS activists have expressed concern that denialist arguments about HIV's harmlessness may be responsible for an upsurge in HIV infections. Denialist claims continue to exert a significant influence in some communities; a survey conducted at minority gay pride events in four American cities in 2005 found that 33% of attendees doubted that HIV caused AIDS. Similarly, a 2010 survey of 343 people living with HIV/AIDS found that one in five of them thought that there was no proof that HIV caused AIDS, and that HIV treatments did more harm than good. According to Stephen Thomas, director of the University of Pittsburgh Center for Minority Health, \"people are focusing on the wrong thing. They're focusing on conspiracies rather than protecting themselves, rather than getting tested and seeking out appropriate care and treatment.\" African Americans are exceptionally likely to believe that HIV does not cause AIDS, partly because they sometimes perceive the role of HIV in the disease as part of a racist agenda. A 2012 survey of young adults in Cape Town, South Africa found that belief in AIDS denialism was strongly related to an increased probability of engaging in unsafe sex.\n\nHIV/AIDS denialist claims have had a major political, social, and public health impact in South Africa. The government of then President Thabo Mbeki was sympathetic to the views of HIV/AIDS denialists, with critics charging that denialist influence was responsible for the slow and ineffective governmental response to the country's massive AIDS epidemic.\n\nIndependent studies have arrived at almost identical estimates of the human costs of HIV/AIDS denialism in South Africa. According to a paper written by researchers from the Harvard School of Public Health, between 2000 and 2005, more than 330,000 deaths and an estimated 35,000 infant HIV infections occurred \"because of a failure to accept the use of available [antiretroviral drugs] to prevent and treat HIV/AIDS in a timely manner.\" Nicoli Nattrass of the University of Cape Town estimates that 343,000 excess AIDS-related deaths and 171,000 infections resulted from the Mbeki administration's policies, an outcome she refers to in the words of Peter Mandelson as \"genocide by sloth\".\n\nIn 2000, when the International AIDS Conference was held in Durban, Mbeki convened a Presidential Advisory Panel containing a number of HIV/AIDS denialists, including Duesberg and David Rasnick. The Advisory Panel meetings were closed to the general press; an invited reporter from the \"Village Voice\" wrote that Rasnick advocated that HIV testing be legally banned and denied that he had seen \"any evidence\" of an AIDS catastrophe in South Africa, while Duesberg \"gave a presentation so removed from African medical reality that it left several local doctors shaking their heads.\"\n\nIn his address to the International AIDS Conference, Mbeki reiterated his view that HIV was not wholly responsible for AIDS, leading hundreds of delegates to walk out on his speech. Mbeki also sent a letter to a number of world leaders likening the mainstream AIDS research community to supporters of the apartheid regime. The tone and content of Mbeki's letter led diplomats in the U.S. to initially question whether it was a hoax.\n\nAIDS scientists and activists were dismayed at the president's behavior and responded with the Durban Declaration, a document affirming that HIV causes AIDS, signed by over 5,000 scientists and physicians.\n\nThe former South African health minister Manto Tshabalala-Msimang also attracted heavy criticism, as she often promoted nutritional remedies such as garlic, lemons, beetroot and olive oil, to people suffering from AIDS, while emphasizing possible toxicities of antiretroviral drugs, which she has referred to as \"poison\". The South African Medical Association has accused Tshabalala-Msimang of \"confusing a vulnerable public\". In September 2006, a group of over 80 scientists and academics called for \"the immediate removal of Dr. Tshabalala-Msimang as minister of health and for an end to the disastrous, pseudoscientific policies that have characterized the South African government's response to HIV/AIDS.\" In December 2006, deputy health minister Nozizwe Madlala-Routledge described \"denial at the very highest levels\" over AIDS.\n\nFormer South African president Thabo Mbeki's government was widely criticized for delaying the rollout of programs to provide antiretroviral drugs to people with advanced HIV disease and to HIV-positive pregnant women. The national treatment program began only after the Treatment Action Campaign (TAC) brought a legal case against Government ministers, claiming they were responsible for the deaths of 600 HIV-positive people a day who could not access medication. South Africa was one of the last countries in the region to begin such a treatment program, and roll-out has been much slower than planned.\n\nAt the XVI International AIDS Conference, Stephen Lewis, UN special envoy for AIDS in Africa, attacked Mbeki's government for its slow response to the AIDS epidemic and reliance on denialist claims:\nIn 2002, Mbeki requested that HIV/AIDS denialists no longer use his name in their literature and stop signing documents with \"Member of President Mbeki's AIDS Advisory Panel\". This coincided with the South African government's statement accompanying its 2002 AIDS campaign, that \"...in conducting this campaign, government's starting point is based on the premise that HIV causes AIDS\". Nonetheless, Mbeki himself continued to promote and defend AIDS-denialist claims. His loyalists attacked former President Nelson Mandela in 2002 when Mandela questioned the government's AIDS policy, and Mbeki attacked Malegapuru William Makgoba, one of South Africa's leading scientists, as a racist defender of \"Western science\" for opposing HIV/AIDS denialism.\n\nIn early 2005, former South African President Nelson Mandela announced that his son had died of complications of AIDS. Mandela's public announcement was seen as both an effort to combat the stigma associated with AIDS, and as a \"political statement designed to… force the President [Mbeki] out of his denial.\"\n\nIn 2008, Mbeki was ousted from power and replaced as President of South Africa by Kgalema Motlanthe. On Motlanthe's first day in office, he removed Manto Tshabalala-Msimang, the controversial health minister who had promoted AIDS-denialist claims and recommended garlic, beetroot, and lemon juice as treatments for AIDS. Barbara Hogan, newly appointed as health minister, voiced shame at the Mbeki government's embrace of HIV/AIDS denialism and vowed a new course, stating: \"The era of denialism is over completely in South Africa.\"\n\n\n\nNational Institute of Allergy and Infectious Diseases pages:\n"}
{"id": "12515238", "url": "https://en.wikipedia.org/wiki?curid=12515238", "title": "Heavy-browed mouse opossum", "text": "Heavy-browed mouse opossum\n\nThe heavy-browed mouse opossum (\"Marmosa andersoni\"), or Anderson's mouse opossum, is a species of opossum in the family Didelphidae. It is endemic to a restricted range in southern Peru. This opossum inhabits forests; it is nocturnal and probably arboreal.\n\nAs only seven individuals have been observed, the species as a whole is limited in description. The dark grey fur on the upper parts is relatively long and tipped with reddish-brown, while the underside is paler. The hair on the cheeks and chin is cream colored, and prominent black rings surround the eyes. Anderson's mouse opossum has large thin ears, providing acute hearing. The tail, which is longer than the head and body, is furry at the base, with bristles that become longer and more slender towards the tip. Each foot has five digits and the big toe on the hindfoot is opposable, which, along with its prehensile tail, makes Anderson's mouse opossums well adapted for a life in the trees.\n\nLittle is known about the ecology of this species. It is nocturnal and may be arboreal. The diet of Marmosa consists largely of insects and fruits but also lizards, bird eggs and small rodents. The second locality where was captured this species is a mixture of forest and bamboo.\n\nAnderson's mouse opossum is known from only three localities, within a narrow strip along the base of the Andes, in Cusco, and southern Peru.\n\nAnderson's mouse opossum is known only from one individual collected in 1954, and several more specimens caught in the late 1990s, and thus very little is known about the biology of this incredibly rare animal. However, much can be deduced from studies of closely related species. It is likely to be nocturnal, and spend most of its time in trees. Like all marsupials, gestation is probably short, with females' giving birth to poorly developed young and most of the development taking place during lactation. It is likely that reproduction is similar to that of \"Marmosa robinsoni\", which gives birth to 6 to 14 young after a gestation period of just 14 days. The tiny young, measuring only up to 12 millimeters, attach themselves to the mother's mammae where they may remain for around 30 days. Unlike many marsupials, female mouse opossums do not possess a pouch to protect the young as they develop. The young are so undeveloped their eyes do not open until 39 to 40 days. It is likely that the young are completely weaned after around 65 days, and they may have an incredibly short life span of only one year. Marmosa species build nests for shelter, or use abandoned bird nests, holes in trees, or banana stalks. These nest sites are unlikely to be permanent; rather, the opossum will use whatever site is available as the sun begins to rise. Like \"M. robinsoni\", it is likely that Anderson's mouse opossum is insectivorous, with fruit also playing an important role in the diet.\n\nThere are no major threats. In the one Western locality, there are natural gas fields in the Camisea region, which are being extracted; however, this is localized. Within the projected range, there is not a high rate of deforestation. In the vicinity of the eastern locality, there are threats including expanding coca crop.\n\n"}
{"id": "40146721", "url": "https://en.wikipedia.org/wiki?curid=40146721", "title": "Historical metrology", "text": "Historical metrology\n\nHistorical metrology is the science that is dealing with the fundamental units of measurement, systems of units formerly in use in various countries, and the development of monetary units throughout their history.\n\nThe interest for historical metrology began by the early antiquarianism movement in Europe during 16th -17th centuries, when studies of the measurement systems in various ancient cultures became common, e.g. De Mensuris et ponderibus Romanis et Graecis, published in 1533 by Georgius Agricola (1494–1555). From the late 18th century and throughout the 19th century a plethora of books, with the aims to collate and clarify the relationships between different measurement systems and compile systems used in ancient cultures as well as in modern time, were published.\n\nResearch efforts were later also used to investigate measurement system in certain cultures or countries. Here it is appropriate to mention Hultsch, who published two monumental works, Griechische und römische Metrologie (1862) and Metrologicorum scriptorium reliquiae, collegit, recensuit, partim nunc primum edidit Fridericus Hultsch (1864/66).\n\nOther works by scholars worth mentioning, is by Aravaca y Torrent (Spain, 1867), Balbin (Argentina, 1881), Falkman (Sweden, 1884/85), Ferrand (Portuguese Territories of Indian Ocean, 1920), Hinz (Islamic world, 1955), Wu \n\n(China, 1957), Skinner (Britain, 1967), Rasmussen (Denmark, 1967), Zupko\n\n(Britain, France and Italy, 1968–90), Pankhurst (Ethiopia, 1969/70), Шостьин (Russia, 1975), Witthöft (Germany, 1979-1994), Bogdán\n\n(Hungary, 1990), Charbonnier\n\n(France, 1990-2006), Donaldson (Oman, 1994), Petersen (Denmark, 2002), Grönros et al. (Finland, 2003), Seabra Lopes (Portugal, 1998-2005), and Connor and Simpson (Scotland, 2004),\n\nFor some countries, principal divisions of executive governments have published reports that compile formerly used weights and measures. For example, this has been done for Bolivia, Great Britain, Costa Rica, Mexico, Portugal, Spain, Tanzania and the United States. In 1954, 1955, and 1966, the United Nation compiled reports that were aimed at giving an overview of the non-metric units then in use in different parts of the world. In 2018, the first of three volumes of the book \"Encyclopaedia of Historical Metrology, Weights, and Measures\", were published. The book addresses the myriad units of measurement that have arisen through the ages, from weights used by ancient cultures to the scientific units of the modern world.\n\nAmong the representatives of historical metrology since the second half of the 20th century may be mentioned, among others Chengluo Wu, Florian Huber, Harald Witthöft, István Bogdán, Jan Gyllenbok, Jean-Claude Hocquet, Luís Seabra Lopes, Pierre Charbonnier, Richard Keir Pethick Pankhurst, Rolf C A Rottländer, Ronald Edward Zupko and Walther Hinz.\n"}
{"id": "12589320", "url": "https://en.wikipedia.org/wiki?curid=12589320", "title": "Horometry", "text": "Horometry\n\nHorometry is the art, practice or method by which time is measured in hours or by smaller units. By this account, a \"horometer\" () is an instrument that measures time.\n\n"}
{"id": "12270519", "url": "https://en.wikipedia.org/wiki?curid=12270519", "title": "Hot Dog (TV series)", "text": "Hot Dog (TV series)\n\nHot Dog is a Saturday morning documentary series for children, seen on NBC from September 12, 1970 to September 4, 1971. Created by Frank Buxton and co-produced by Buxton and Lee Mendelson, the program was notable for its hosts – Jo Anne Worley, comedian Jonathan Winters and writer and actor Woody Allen. The pilot, televised on March 28, 1970, starred Worley, Allen and Tom Smothers, who was replaced with Winters when the show became a series.\n\nBased on Buxton's travels as a comedian (and later, as host of the ABC series, \"Discovery\"), which took him on tours to various factories, \"Hot Dog\" explained, in a humorous manner, how we do things (such as snore) and how things were made (such as the eponymous hot dogs and their buns, plus condiments like mustard).\n\nSeventy topics were covered during the course of this series, which lasted thirteen episodes and rerun the rest of the season. NBC won a Peabody award for the series in 1970.\n\nSome of the music in this series was performed by The Youngbloods.\n\nReruns of \"Hot Dog\" were syndicated during the 1977–1978 television season, at a time when Allen had firmly established himself as a motion picture star, director, and writer. Portions of \"Hot Dog\" were also seen on a local KNBC children's program in Los Angeles, \"That's Cat\", which debuted in 1976.\n\nIn 1971 the Individual topic segments were sold to schools on 16mm film.\n\n\n"}
{"id": "57190122", "url": "https://en.wikipedia.org/wiki?curid=57190122", "title": "How to Make a Spaceship", "text": "How to Make a Spaceship\n\nHow to Make a Spaceship\": A Band of Renegades, An Epic Race, And the Birth of Private Spaceflight\" () is a bestselling award-winning 2016 non-fiction book by journalist Julian Guthrie about the origins of the X Prize Foundation and Peter Diamandis, the first X Prize, the Ansari X Prize and Anousheh Ansari, the entrants into that suborbital spaceflight competition, and the winning team, Mojave Aerospace Ventures of Vulcan Inc., Paul G. Allen, Scaled Composites, Burt Rutan, and their platform of Tier One of SpaceShipOne and WhiteKnightOne.\n\nThe book is an overview of what lead to the creation of the X Prize, and the running of that first X Prize. Profiles of all the major players in the X Prize saga are included in the book. It chronologically starts with the influences that weighed upon Peter Diamandis, and his progression into the space industry. It also covers the process to get funding, rejections, and the arrival of the Ansaris, becoming title sponsors. The book surveys several of the teams that entered into the competition to win the Ansari X Prize. The team that is focused on most is that which won the X Prize in 2004, the one headed by Paul Allen and Burt Rutan, of SpaceShipOne. The book ends with an epilogue about Richard Branson's Virgin Galactic scooping up the SpaceShipOne technology, and the spaceplane itself ending up in the Smithsonian National Air and Space Museum. The book includes a preface by Richard Branson, and an afterword by Stephen Hawking.\n\nThe book was originally entitled \"Beyond: Peter Diamandis and the Adventure of Space\", when it was sold preemptively to Penguin Books in 2014. \"How to Make a Spaceship\" was released in September 2016, in trade paperback, hardcover, audio book and e-book formats. The work was a finalist for a PEN Award, the 2017 PEN/E.O. Wilson Literary Science Writing Award. The publication won the 2016 Eugene E. Emme Award for Astronautical Literature in September 2017. It became a New York Times listed bestseller. The book has appeared on several \"Best Of\" book lists. Several parties have expressed interest in obtaining the filming rights to the book.\n\nGregg Easterbrook's review in The Wall Street Journal said of the book that “'How to Make a Spaceship' offers a rousing anthem to the urge to explore.\"\n\n\n\n\n"}
{"id": "25034456", "url": "https://en.wikipedia.org/wiki?curid=25034456", "title": "Hydrogenoxalate", "text": "Hydrogenoxalate\n\nHydrogenoxalate or hydrogen oxalate is an anion with chemical formula or , derived from oxalic acid by the loss of a single proton; or, alternatively, from the oxalate anion by addition of a proton.\n\nThe name is also used for any salt containing this anion, such as NaHCO, KHCO, or NHHCO. Hydrogenoxalates may also be called (especially in older literature) bioxalates, acid oxalates, or monobasic oxalates.\n\n\n"}
{"id": "6518049", "url": "https://en.wikipedia.org/wiki?curid=6518049", "title": "Hyperchromicity", "text": "Hyperchromicity\n\nHyperchromicity is the increase of absorbance (\"optical density\") of a material. The most famous example is the hyperchromicity of DNA that occurs when the DNA duplex is denatured. The UV absorption is increased when the two single DNA strands are being separated, either by heat or by addition of denaturant or by increasing the pH level. The opposite, a decrease of absorbance is called hypochromicity.\n\nHeat denaturation of DNA, also called melting, causes the double helix structure to unwind to form single stranded DNA. When DNA in solution is heated above its melting temperature (usually more than 80 °C), the double-stranded DNA unwinds to form single-stranded DNA. The bases become unstacked and can thus absorb more light. In their native state, the bases of DNA absorb light in the 260-nm wavelength region. When the bases become unstacked, the wavelength of maximum absorbance does not change, but the amount absorbed increases by 37%. A double stranded DNA strand dissociating to two single strands produces a sharp cooperative transition.\n\nHyperchromicity can be used to track the condition of DNA as temperature changes. The transition/melting temperature (T) is the temperature where the absorbance of UV light is 50% between the maximum and minimum, i.e. where 50% of the DNA is denatured.a ten fold increase of monovalent cation concentration increases the temperature by 16.6 degree C\n\nThe hyperchromic effect is the striking increase in absorbance of DNA upon denaturation. The two strands of DNA are bound together mainly by the stacking interactions, hydrogen bonds and hydrophobic effect between the complementary bases. The hydrogen bond limits the resonance of the aromatic ring so the absorbance of the sample is limited as well. When the DNA double helix is treated with denatured agents, the interaction force holding the double helical structure is disrupted. The double helix then separates into two single strands which are in the random coiled conformation. At this time, the base-base interaction will be reduced, increasing the UV absorbance of DNA solution because many bases are in free form and do not form hydrogen bonds with complementary bases. As a result, the absorbance for single-stranded DNA will be 37% higher than that for double stranded DNA at the same concentration.\n\n"}
{"id": "39737946", "url": "https://en.wikipedia.org/wiki?curid=39737946", "title": "ICSU World Data System", "text": "ICSU World Data System\n\nThe ICSU World Data System (ICSU-WDS) was created by the International Council for Science's (ICSU) General Assembly in October 2008.\n\nICSU World Data System superseded the World Data Centres (WDCs) and Federation of Astronomical and Geophysical data analysis Services (FAGS) created by ICSU to manage data generated by the International Geophysical Year (1957–1958).\n\nICSU World Data System is governed by a Scientific Committee(SC) made up of data scientists and experts. \nIt is supported by an International Programme Office (IPO), which is hosted and funded by the Japanese National Institute of Information and Communications Technology as well as the International Council for Science.\n\nICSU World Data System member organizations adopt its Data Sharing Principles.\n\nMember organizations of ICSU World Data System join voluntarily in one of the four membership categories: Regular, Network, Partner, and Associate Members. Members contribute their data holdings, data services or products. The updated list of Members is available on the ICSU World Data System website :ICSU-WDS Membership\n\nA notable member of the ICSU World Data System is Professor Jane Hunter, who is the director of the E-Research lab in Queensland University.\n\n"}
{"id": "1506522", "url": "https://en.wikipedia.org/wiki?curid=1506522", "title": "International Heliophysical Year", "text": "International Heliophysical Year\n\nThe International Heliophysical Year is a UN-sponsored scientifically driven international program of scientific collaboration to understand external drivers of planetary environments and universal processes in solar-terrestrial-planetary-heliospheric physics. The IHY will focus on advancements in all aspects of the heliosphere and its interaction with the interstellar medium. This effort culminates in the \"International Heliophysical Year\" (IHY) in 2007-2008. The IHY concluded in February, 2009, but was largely continued via the International Space Weather Initiative (ISWI)\n\nThe term \"Heliophysical\" was coined to refer specifically to this activity of studying the interconnectedness of the entire solar-heliospheric-planetary system. It is a broadening of the concept \"geophysical,\" extending the connections from the Earth to the Sun & interplanetary space. On the 50th anniversary of the International Geophysical Year, the 2007 IHY activities will build on the success of IGY 1957 by continuing its legacy of system-sides studies of the extended heliophysical domain.\n\nThe IHY 2007 has been planned to coincide with the fiftieth anniversary of the International Geophysical Year (IGY) in 1957-1958, one of the most successful international science programs of all time. The IGY was a broad-based and all-encompassing effort to push the frontiers of geophysics and resulted in tremendous progress in space physics, Sun-Earth connections, planetary science and the heliosphere in general. The tradition of international science years began almost 125 years ago with the first International Polar Year and international scientific studies of global processes at the North Pole in 1882-1883. The IHY has received substantial support from the United Nations, and various space agencies around the world.\n\nThe IHY has three primary objectives:\n\n\nThe IHY team has also identified the following science goals for 2007-2008:\n\n\n\n"}
{"id": "42891849", "url": "https://en.wikipedia.org/wiki?curid=42891849", "title": "International Migration Institute", "text": "International Migration Institute\n\nThe International Migration Institute (IMI) is a research institute that is part of Oxford University in the United Kingdom. It is affiliated with the Oxford Department of International Development.\n\nThe International Migration Institute was founded in 2006 to complement the work of the Centre on Migration, Policy and Society (COMPAS) and the Refugee Studies Centre, both at the University of Oxford. Stephen Castles, who had been director of the Refugee Studies Centre, assumed directorship of IMI upon its formation, and stepped down in August 2009. From September 2009 to September 2011, Robin Cohen was the director. Since then, directorship has been jointly in the hands of Oliver Blackwell and Hein de Haas.\n\nIMI is a member of the Migration Studies Society at Oxford University. The other two members of the society are the Centre on Migration, Policy and Society (COMPAS) and the Refugee Studies Centre (RSC).\n\nIMI is also a collaborator to the International Organization for Migration (IOM). It is also listed as a partner for the migration program of the Social Science Research Council.\n\nExperts from the International Migration Institute have been cited and quoted in the \"New York Times\" and BBC News.\n"}
{"id": "466957", "url": "https://en.wikipedia.org/wiki?curid=466957", "title": "John Bird (astronomer)", "text": "John Bird (astronomer)\n\nJohn Bird (1709–1776), the mathematical instrument maker, was born at Bishop Auckland. He worked in London for Jonathan Sisson, and by 1745 he had his own business in the Strand. Bird was commissioned to make a brass quadrant 8 feet across for the Royal Observatory at Greenwich, where it is still preserved. Soon after, duplicates were ordered for France, Spain and Russia.\n\nBird supplied the astronomer James Bradley with further instruments of such quality that the commissioners of longitude paid him £500 (a huge sum) on condition that he take on a 7-year apprentice and produce in writing upon oath, a full account of his working methods. This was the origin of Bird's two treatises \"The Method of Dividing Mathematical Instruments\" (1767) and \"The Method of Constructing Mural Quadrants\" (1768). Both had a foreword from the astronomer-royal Nevil Maskelyne. When the Houses of Parliament burned down in 1834, the standard yards of 1758 and 1760, both constructed by Bird, were destroyed.\n\nBird, with his fellow County Durham savant William Emerson, makes an appearance in \"Mason and Dixon\" the novel by Thomas Pynchon.\n\n"}
{"id": "6105498", "url": "https://en.wikipedia.org/wiki?curid=6105498", "title": "List of U.S. state shells", "text": "List of U.S. state shells\n\nThis is a list of official state shells for those states of the United States that have chosen to select one as part of their state insignia.\n\nThese are seashells, the shells of various marine mollusks including both gastropod and bivalves. Each one was chosen to represent a maritime state, based on the fact that the species occurs in that state and was considered suitable to represent the state, either because of the species' commercial importance as a local seafood item, or because of its beauty, rarity, exceptional size, or other features.\n\n"}
{"id": "9025255", "url": "https://en.wikipedia.org/wiki?curid=9025255", "title": "List of UN numbers 2801 to 2900", "text": "List of UN numbers 2801 to 2900\n\nThe UN numbers from UN2801 to UN2900 as assigned by the United Nations Committee of Experts on the Transport of Dangerous Goods.\n\n"}
{"id": "46678229", "url": "https://en.wikipedia.org/wiki?curid=46678229", "title": "List of countries by aluminium exports", "text": "List of countries by aluminium exports\n\nThe following is a list of countries by raw aluminium exports. Data is for 2016, in millions of United States dollars, as reported by The Observatory of Economic Complexity. Currently the top ten countries are listed.\n\n"}
{"id": "13898857", "url": "https://en.wikipedia.org/wiki?curid=13898857", "title": "List of performance analysis tools", "text": "List of performance analysis tools\n\nThis is a list of performance analysis tools for use in software development.\n\nThe following tools work based on log files that can be generated from various systems.\n\nThe following tools work for multiple languages or binaries.\n\n\n\n\n\n\n"}
{"id": "57293774", "url": "https://en.wikipedia.org/wiki?curid=57293774", "title": "List of pre-modern Arab scientists and scholars", "text": "List of pre-modern Arab scientists and scholars\n\nThis is a list of Arab scientists and scholars from the Muslim World and Spain (Al-Andalus) who lived from antiquity up until the beginning of the modern age, consisting primarily of scholars during the Middle Ages. For a list of contemporary Arab scientists and engineers see List of modern Arab scientists and engineers\n\nBoth the Arabic and Latin names are given. The following Muslim naming articles are not used for indexing:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56760139", "url": "https://en.wikipedia.org/wiki?curid=56760139", "title": "Natalie Panek", "text": "Natalie Panek\n\nNatalie Panek is an award-winning Canadian working in aerospace engineering. She works in the robotics and automation division of the space technology company MDA.\n\nPanek grew up in Calgary, Alberta, where she was the only girl in her high school physics class. She was inspired by the American Solar Challenge. Panek completed a bachelor's degree in Mechanical Engineering at the University of Calgary in 2007. She earned a masters in Aerospace Engineering at the University of Toronto in 2009. In Summer 2008, she worked at Goddard Space Flight Center.\n\nAfter graduating, Panek attended the International Space University at Ames Research Center. Panek works at the space technology company MDA where she works on Canadian space exploration and robotics. She has been described as the Roberta Bondar of her generation. She is helping to build the chassis and locomotion systems for European Space Agency's ExoMars (rover).\n\nCanada’s Financial Post describes Panek as “a vocal advocate for women in technology”. She regularly gives interviews about her career and campaign for more women in science. In 2012 she delivered a TEDx Youth talk, \"Revolutionising Female Empowerment.\" In 2015 she delivered a TEDx talk in Toronto, \"Space, Our Invisible Landfill\", which has been viewed over one million times.\n\nPanek was a presenter at the Ontario Science Centre RBC Innovator's Ball in 2016. She appeared on an episode of CBC Television's \"The Next 150\", Our Canada needs more everyday explorers. She is a speaker with the National Speakers Bureau. She spoke at the 2016 Women of Influence and 2017 Girl Talk Empowerment conferences.\n\nShe was part of the \"#Canada150Women\" campaign by author Paulina Cameron. She is a cybermentor with the University of Calgary's online program.\n\nAlongside her work and advocacy, Panek enjoys backpacking. She is a champion of Trans Canada Trail.\n\n2013 - University of Calgary Graduate of the Last Decade Award \n\n2013 - Northern Lights Aero Foundation Rising Star\n\n2013 - CBC’s 12 young leaders changing Canada\n\n2014 - WXN's Top 100 Award Winner\n\n2015 - Forbes 30 under 30\n\n2015 - Flare magazine's 30 under 30\n\n2016 - Wings Magazine Top 20 under 40\n\n2016 - Canada's Greatest Women Explorers Canadian Geographic\n\n2018 - Inspiring Fifty Canada\n\n"}
{"id": "1819946", "url": "https://en.wikipedia.org/wiki?curid=1819946", "title": "National Research Council (Italy)", "text": "National Research Council (Italy)\n\nThe Consiglio Nazionale delle Ricerche (CNR) or National Research Council, is the largest research council in Italy. As a public organisation, its remit is to support scientific and technological research. Its headquarters are in Rome.\n\nThe institution was founded in 1923. The first president was Vito Volterra, succeeded by Guglielmo Marconi. The process of improvement of the national scientific research, through the use of specific laws, (see Law 59/1997), affects many research organizations, and amongst them is CNR, whose \"primary function is to carry on, through its own organs, advanced basic and applied research, both to develop and maintain its own scientific competitiveness, and to be ready to take part effectively in a timely manner in the strategic fields defined by the national planning system\".\n\nOn 23 December 1987, CNR registers the first Italian internet domain: cnr.it\n\nWith the issuing of the legislative decree of 30 January 1999, n. 19, which defines \"The reorganization of the National Research Council\" the central role of CNR in the Italian research system is confirmed.\n\nIn particular CNR is defined (see Article 1 of the above-mentioned decree 19/1999) as a \"national research organization, with general scientific competence and with scientific research institutes distributed across Italy, which carries out activities of primary interest for the promotion of science and the progress of the country\".\n\nCNR has the legal status of a public organization, and defines for itself autonomous rules and regulations, in accordance with the existing laws and the Civil Code.\n\nThe new CNR has the following mission and activities:\n\nFor the execution of these activities and any other activity related to them, CNR can stipulate agreements and contracts, establish or participate in consortia, foundations or societies with private or public parties, Italian or foreign.\n\nIn addition, through agreements or participation, CNR can implement programs, directives and regulations of the Regional governments or other Public Administrations, aimed to the dissemination of the research results in the economic system; it can also contribute to the realization of the conditions needed for the establishment of highly innovative enterprises.\n\nFinally, CNR can participate in international research centers, in collaboration with analogous scientific institutions of other Countries.\n\nCNR is organized in seven departments and 106 research institutes, listed below:\n\n\n\nThe \"research areas\" of CNR are regional centers aggregating institutions where some services are managed in a centralized manner. Conceived in 1979, the implementation phase set off in the mid-eighties with the creation of the first four areas of Montelibretti, Milan, Genoa and Potenza.\n\n\n"}
{"id": "3582716", "url": "https://en.wikipedia.org/wiki?curid=3582716", "title": "Number form", "text": "Number form\n\nA number form is a mental map of numbers, which automatically and involuntarily appears whenever someone who experiences number-forms thinks of numbers. Numbers are mapped into distinct spatial locations and the mapping may be different across individuals. Number forms were first documented and named by Sir Francis Galton in his \"The Visions of Sane Persons\" . Later research has identified them as a type of synesthesia (; ).\n\nIt has been suggested that number-forms are a result of \"cross-activation\" between regions of the parietal lobe that are involved in numerical cognition and angular gyrus for spatial cognition (; ). Since the areas that process numerical and spatial representations are close to each other, this may contribute to the increased cross-activation. Compared to non-synesthetes, synesthetes display larger P3b amplitudes for month cues, but similar N1 and P3b responses for arrow (<- or ->) and word (left or right) cues. ().\n\nReaction time studies have shown that number-form synesthetes are faster to say which of two numbers is larger when the numbers are arranged in a manner consistent with their number-form, suggesting that number forms are automatically evoked (; ). This can be thought of as a \"spatial Stroop\" task, in which space is not relevant to the task, but which can hinder performance despite its irrelevance. The fact that synesthetes cannot ignore the spatial arrangement of the numbers on the screen demonstrates that numbers are automatically evoking spatial cues. The reaction times for valid cues are smaller than invalid cues (words and arrows), but in synesthetes the response time differences for months are larger than those of non-synesthetes ().\n\nThese number forms can be distinguished from the non-conscious mental number line that we all have by the fact that they are 1) conscious, 2) idiosyncratic (see image) and 3) stable across the lifespan. Although this form of synesthesia has not been as intensively studied as Grapheme → color synesthesia, Hubbard and colleagues have argued that similar neural mechanisms might be involved, but acting in different brain regions (). Future studies will need to be conducted to test this hypothesis.\n\n"}
{"id": "5663632", "url": "https://en.wikipedia.org/wiki?curid=5663632", "title": "Occupational English Test", "text": "Occupational English Test\n\nThe Occupational English Test (also known as OET) is an international English language test for the healthcare sector. It assesses the language communication skills of healthcare professionals who wish to register and practise in an English-speaking environment.\n\nOET is available for the following 12 professions: dentistry, dietetics, medicine, nursing, occupational therapy, optometry, pharmacy, physiotherapy, podiatry, radiography, speech pathology, and veterinary science.\n\nOET was designed in the late 1980s by Professor Tim McNamara, under the guidance of the Australian National Office for Overseas Skills Recognition (NOOSR), which administered the test at that time. The test has been researched and developed continuously since then to ensure that it has kept up with current theory and practice in language assessment. This work has been done primarily by the University of Melbourne’s Language Testing Research Centre and, more recently, also by Cambridge English Language Assessment.\n\nSince March 2013 the test has been owned by Cambridge Boxhill Language Assessment Trust (CBLA), a venture between Cambridge English Language Assessment and Box Hill Institute.\n\nOET is recognised by regulatory healthcare bodies and councils in Australia, New Zealand and Singapore, Malaysia, the Philippines, recently in UK and Ireland. Many organisations, including hospitals, universities and colleges, are using OET as proof of a candidate’s ability to communicate effectively in a demanding healthcare environment. In addition, OET is recognised by the Australian Department of Immigration and Border Protection for a number of visa categories, including work and student visas.\n\nEach recognising organisation determines which grade results mean that candidates meet the language competency standards to function in their profession. A full list of regulatory organisations which accept OET can be seen on the official website.\n\nOET provides a valid and reliable assessment of all four language skills – Listening, Reading, Writing and Speaking – with an emphasis on communication in medical and health professional settings.\nOET consists of four sub-tests:\n\nListening\n\nThe listening test consists of two parts. In Part A, candidates listen to a simulated consultation (dialogue) between a professional and a patient and are required to take notes under headings. In Part B, candidates listen to a health professional giving a short talk on a health-related topic and are required to complete a range of open-ended and fixed-choice questions.\n\nReading\n\nThe reading test consists of two parts. In Part A, lasting 15 minutes, candidates are asked to skim read 3 or 4 short texts and complete a summary paragraph by filling in the missing words. It is designed to test the reader’s ability to scan texts within a time limit, source information from multiple texts, and synthesise information. In Part B, lasting 45 minutes, candidates are asked to read two passages on a general healthcare topic and answer 8–10 multiple choice questions for each text. It is designed to test the reader’s ability to read and comprehend longer texts.\n\nWriting\n\nThe writing paper asks candidates to write a letter, usually a letter of referral. For some professions a different type of letter is required, e.g. a letter of transfer or discharge, or a letter to advise a patient, carer or group. Candidates are given case notes which must be included in their letter.\n\nSpeaking\n\nThe speaking test is in the form of one-to-one conversations with an interlocutor. It starts with a short warm-up interview about the candidate’s professional background. This is followed by two role plays. Candidates have 2–3 minutes to prepare for each role play. Role plays last about five minutes and are based on typical interactions between a health professional and a patient. The candidate adopts their usual professional role (e.g. as a nurse) and the interviewer plays a patient or sometimes a relative or carer. For veterinary science the interviewer is the owner or carer of the animal.\n\nEach of the four sub-tests that make up OET are graded A to E, where A is the highest grade and E is the lowest. There is no overall grade.\n\nListening and reading\n\nThere is no fixed score-to-grade link for the listening and reading tests. Grade boundaries are continually reset because different test materials are used at each administration. A mean average of the percentage of candidates in each grade for the writing and speaking tests is applied to the spread of performances on the listening and reading tests to establish the grade boundaries.\n\nWriting and speaking\n\nIn writing and speaking, the score is generated through statistical analysis of the two sets of scores from two independent assessors. This is converted, following established practice, to the final grade.\n\nOET is available up to 12 times a year and can be taken at test venues around the world. A full list is available on the official website.\n\nResults are published online approximately 16 business days after the test. Official statements of results are sent out in the post following the release of online results. There is no overall grade – candidates receive separate grades for each sub-test.\n\nMost recognising organisations require candidates to have at least a B grade in each of the four sub-tests and recognise results as valid for up to two years. Most recognising organisations also require that candidates achieve the requisite grades for each sub-test in one sitting. However, candidates should check with the organisation that regulates their profession to confirm current requirements.\n\nThe Occupational English Test was plagued with criticism from an Australian Government's Parliamentary Enquiry in 2013. The broader community made negative statements about testing conditions and marking consistency to the Enquiry. One submission claimed that it is possible to get a pass by re-sitting the test multiple times as you will eventually be marked higher after 6 sittings of the OET test.\n\nOET is underpinned by over 30 years of research and the test is regularly updated to keep pace with changes in language testing in a healthcare context. There is strong emphasis on the ongoing validity and reliability of the test. Leading language testing academics contribute to the continued development of the test, and subject matter experts are consulted to ensure that tasks are based on a typical workplace situations and the demands of the profession. A full list of research can be seen on the official website\n\n"}
{"id": "143913", "url": "https://en.wikipedia.org/wiki?curid=143913", "title": "Old World", "text": "Old World\n\nThe term \"Old World\" is used commonly in the West to refer to Africa, Asia and Europe (Afro-Eurasia or the World Island), regarded collectively as the part of the world known to its population before contact with the Americas and Oceania (the \"New World\"). It is used in the context of, and contrasts with, the New World (the Americas and Oceania).\n\nIn the context of archaeology and world history, the term \"Old World\" includes those parts of the world which were in (indirect) cultural contact from the Bronze Age onwards, resulting in the parallel development of the early civilizations, mostly in the temperate zone between roughly the 45th and 25th parallels, in the area of the Mediterranean, Mesopotamia, Persian plateau, Indian subcontinent and China.\n\nThese regions were connected via the Silk Road trade route, and they have a pronounced Iron Age period following the Bronze Age. In cultural terms, the Iron Age was accompanied by the so-called Axial Age, referring to cultural, philosophical and religious developments eventually leading to the emergence of the historical Western (Hellenism, \"classical\"), Near Eastern (Zoroastrian and Abrahamic) and Far Eastern (Hinduism, Buddhism, Jainism, Confucianism, Taoism) cultural spheres.\n\nThe concept of the three continents in the Old World, viz. Asia, Africa, and Europe, goes back to classical antiquity. Their boundaries as defined by Ptolemy and other geographers of antiquity were drawn along the Nile and Don rivers. This definition remained influential throughout the Middle Ages (see T and O map) and the Early Modern period.\n\nThe mainland of Afro-Eurasia (excluding islands such as the British Isles, Japan, Sri Lanka, Madagascar and the Malay Archipelago) has been referred to as the \"World Island\". The term may have been coined by Sir Halford John Mackinder in \"The Geographical Pivot of History\".\n\nThe equivalent of the Old World had names in some of its ancient cultures, including Midgard in Germanic cosmology, and Oikoumene among the Greeks.\n\n"}
{"id": "9987", "url": "https://en.wikipedia.org/wiki?curid=9987", "title": "Outline of engineering", "text": "Outline of engineering\n\nThe following outline is provided as an overview of and topical guide to engineering:\n\nEngineering is the discipline and profession that applies scientific theories, mathematical methods, and empirical evidence to design, create, and analyze technological solutions cognizant of safety, human factors, physical laws, regulations, practicality, and cost.\n\n\nHistory of engineering\n\n\n\n\n\n\n\n"}
{"id": "8736036", "url": "https://en.wikipedia.org/wiki?curid=8736036", "title": "Outline of the Internet", "text": "Outline of the Internet\n\nThe following outline is provided as an overview of and topical guide to the Internet.\n\nInternet – worldwide, publicly accessible network of interconnected computer networks that transmit data by packet switching using the standard Internet Protocol (IP). It is a \"network of networks\" that consists of millions of interconnected smaller domestic, academic, business, and government networks, which together carry various information and services, such as electronic mail, online chat, file transfer, and the interlinked Web pages and other documents of the World Wide Web.\n\nIt allows other services\n\n\n\n\nInternet Protocol Suite –\n\nLink Layer –\n\nInternet Layer –\n\nTransport Layer –\n\nApplication Layer –\n\nHistory of the Internet<br>\nThe internet wasn't invented but continually developed by internet pioneers.\n\n\n\n\n\n\n\n"}
{"id": "32541155", "url": "https://en.wikipedia.org/wiki?curid=32541155", "title": "Phillips Report", "text": "Phillips Report\n\nThe Phillips report was a document summarizing a review conducted in November–December 1965 by a NASA team headed by Lt Gen Samuel C. Phillips, director of the Apollo manned Moon landing program, to investigate schedule slippage and cost overruns incurred by North American Aviation, manufacturer of the Command/Service Module spacecraft and the second stage of the Saturn V launch vehicle. Phillips sent a summary of his findings with a strongly worded letter to NAA president Lee Atwood demanding corrective action be taken. North American revised its management of its Apollo contract items, and NASA management considered the matter a normal part of confidential agency-contractor relations.\n\nBut after a fire killed the entire crew of the first manned Apollo mission Apollo 1 on January 27, 1967, a United States Senate Committee on Aeronautical and Space Sciences hearing overseeing NASA's investigation of the accident, led to the public disclosure of the Phillips Report by then junior Senator Walter Mondale, who was told of its existence by ABC news reporter Jules Bergman, who had seen a copy at NASA's Washington, DC Office of Manned Space Flight headquarters.\n\nFrom November 22 to December 6, 1965, Phillips headed a tiger team investigating the causes of inadequate quality, schedule delays, and cost overruns in both the Apollo CSM and the Saturn V second stage, for which North American was prime contractor. He gave an oral presentation (with transparencies) of his team's findings to his boss, NASA Office of Manned Space Flight Administrator George E. Mueller, and Mueller's boss, NASA Deputy Director Robert Seamans, and also presented them in a letter to North American president Lee Atwood, to which Mueller appended his own strongly worded letter to Atwood.\n\nImmediately after the fire in 1967, NASA followed its established procedure of investigating and identifying corrections for the cause, with Presidential and Congressional oversight. No one in NASA's upper management expected that the Phillips findings would be printed as a document, but this had been done and on February 13, Bergman was shown a copy at the Office of Manned Space Flight headquarters. He then told a junior Senator on the Aeronautical and Space Sciences Committee, Walter Mondale, about the document, and later reported its existence on ABC. Mondale proceeded to grill the top managers, including Webb who was completely blind-sided (the formal review had not gone above Seamans), about the report's existence. Other Senators, such as Margaret Chase, then questioned Webb about NASA's choice of North American as the Apollo contractor.\n\nMondale said he had been told of the existence what he called \"the Phillips report\", and Seamans was afraid that Mondale might be in possession of a hard copy of the presentation, so he said tentatively that contractors have occasionally been given negative reviews, but that he knew of no such extraordinary report. Mondale raised controversy over the report, despite Phillips' refusal to characterize it as such before Congress, and was angered by what he perceived as Webb's deception and concealment of important program problems from Congress, and questioned NASA's selection of North American as prime contractor. Webb eventually provided a controlled copy of Phillips' memo to Congress. Seamans later wrote that Webb roundly chastised him in the cab ride leaving the hearing, for volunteering information which led to the disclosure of Phillips' memo.\n\nThe committee concluded in its final report that \"the findings of the [Phillips] task force had no effect on the accident, did not lead to the accident, and were not related to the accident,\" but in its recommendations, stated \"the committee believes it should have been informed of the situation.\" Freshman Senators Edward Brooke and Charles H. Percy jointly wrote an \"Additional Views\" section appended to the committee report, expressing a bit more strongly that the Phillips review should have been disclosed to Congress. Mondale wrote his own Additional View, voicing his complaints in the most strongly worded terms.\n\nIn its final report, the committee agreed with NASA that the Phillips review had absolutely no bearing on the fire, though the chairman expressed his disappointment that Webb had not kept them informed of Apollo program problems at the time. But Mondale issued a minority opinion accusing NASA of \"evasiveness, ... lack of candor, ... patronizing attitude exhibited toward Congress, ... refusal to respond fully and forthrightly to legitimate congressional inquiries, and ... solicitous concern for corporate sensitivities at a time of national tragedy\".\n\n"}
{"id": "24981", "url": "https://en.wikipedia.org/wiki?curid=24981", "title": "Pioneer 11", "text": "Pioneer 11\n\nPioneer 11 (also known as Pioneer G) is a robotic space probe launched by NASA on April 6, 1973 to study the asteroid belt, the environment around Jupiter and Saturn, solar wind and cosmic rays. It was the first probe to encounter Saturn and the second to fly through the asteroid belt and by Jupiter. Thereafter, \"Pioneer 11\" became the second of five artificial objects to achieve the escape velocity that will allow them to leave the Solar System. Due to power constraints and the vast distance to the probe, the last routine contact with the spacecraft was on September 30, 1995, and the last good engineering data was received on November 24, 1995.\n\nApproved in February 1969, \"Pioneer 11\" and its twin probe, \"Pioneer 10\", were the first to be designed for exploring the outer Solar System. Yielding to multiple proposals throughout the 1960s, early mission objectives were defined as:\n\nSubsequent planning for an encounter with Saturn added many more goals:\n\n\"Pioneer 11\" was built by TRW and managed as part of the Pioneer program by NASA Ames Research Center. A backup unit, Pioneer H, is currently on display in the \"Milestones of Flight\" exhibit at the National Air and Space Museum in Washington, D.C.. Many elements of the mission proved to be critical in the planning of the \"Voyager\" program.\n\nThe \"Pioneer 11\" bus measured deep and with six panels forming the hexagonal structure. The bus housed propellant to control the orientation of the probe and eight of the twelve scientific instruments. The spacecraft had a mass of 260 kilograms.\n\nThe \"Pioneer 11\" probe was launched on April 6, 1973 at 02:11:00 UTC, by the National Aeronautics and Space Administration from Space Launch Complex 36A at Cape Canaveral, Florida aboard an Atlas-Centaur launch vehicle. Its twin probe, \"Pioneer 10\", had launched a year earlier on March 3, 1972. \"Pioneer 11\" was launched on a trajectory directly aimed at Jupiter without any prior gravitational assists. In May 1974, Pioneer was retargeted to fly past Jupiter on a north-south trajectory enabling a Saturn flyby in 1979. The maneuver used 17 pounds of propellant, lasted 42 minutes and 36 seconds and increased Pioneer 11's speed by 230 km/h. It also made two mid-course corrections, on April 11, 1973 and November 7, 1974.\n\n\"Pioneer 11\" flew past Jupiter in November and December 1974. During its closest approach, on December 2, it passed above the cloud tops. The probe obtained detailed images of the Great Red Spot, transmitted the first images of the immense polar regions, and determined the mass of Jupiter's moon Callisto. Using the gravitational pull of Jupiter, a gravity assist was used to alter the trajectory of the probe towards Saturn. On April 16, 1975, following the Jupiter encounter, the micrometer detector was turned off.\n\n\"Pioneer 11\" passed by Saturn on September 1, 1979, at a distance of 21,000 km from Saturn's cloud tops.\n\nBy this time \"Voyager 1\" and \"Voyager 2\" had already passed Jupiter and were also en route to Saturn, so it was decided to target \"Pioneer 11\" to pass through the Saturn ring plane at the same position that the soon-to-come Voyager probes would use in order to test the route before the Voyagers arrived. If there were faint ring particles that could damage a probe in that area, mission planners felt it was better to learn about it via Pioneer. Thus, \"Pioneer 11\" was acting as a \"pioneer\" in a true sense of the word; if danger were detected, then the Voyager probes could be rerouted further away from the rings, but missing the opportunity to visit Uranus and Neptune in the process.\n\n\"Pioneer 11\" imaged and nearly collided with one of Saturn's small moons, passing at a distance of no more than . The object was tentatively identified as Epimetheus, a moon discovered the previous day from \"Pioneer\"s imaging, and suspected from earlier observations by Earth-based telescopes. After the Voyager flybys, it became known that there are two similarly-sized moons (Epimetheus and Janus) in the same orbit, so there is some uncertainty about which one was the object of Pioneer's near-miss. \"Pioneer 11\" encountered Janus on September 1, 1979 at 14:52 UTC at a distance of 2500 km and Mimas at 16:20 UTC the same day at 103000 km.\n\nBesides Epimetheus, instruments located another previously undiscovered small moon and an additional ring, charted Saturn's magnetosphere and magnetic field and found its planet-size moon, Titan, to be too cold for life. Hurtling underneath the ring plane, the probe sent back pictures of Saturn's rings. The rings, which normally seem bright when observed from Earth, appeared dark in the Pioneer pictures, and the dark gaps in the rings seen from Earth appeared as bright rings.\n\nOn February 23, 1990, \"Pioneer 11\" became the 4th man-made object to pass beyond the orbit of the planets.\n\nBy 1995, \"Pioneer 11\" could no longer power any of its detectors, so the decision was made to shut it down. On September 29, 1995, NASA's Ames Research Center, responsible for managing the project, issued a press release that began, \"After nearly 22 years of exploration out to the farthest reaches of the Solar System, one of the most durable and productive space missions in history will come to a close.\" It indicated NASA would use its Deep Space Network antennas to listen \"once or twice a month\" for the spacecraft's signal, until \"some time in late 1996\" when \"its transmitter will fall silent altogether.\" NASA Administrator Daniel Goldin characterized \"Pioneer 11\" as \"the little spacecraft that could, a venerable explorer that has taught us a great deal about the Solar System and, in the end, about our own innate drive to learn. \"Pioneer 11\" is what NASA is all about – exploration beyond the frontier.\" Besides announcing the end of operations, the dispatch provided a historical list of \"Pioneer 11\" mission achievements. NASA terminated routine contact with the spacecraft on September 30, 1995, but continued to make contact for about 2 hours every 2 to 4 weeks. Scientists received a few minutes of good engineering data on 24 November 1995 but then lost final contact once Earth permanently moved out of view of the spacecraft's antenna. Its signal became too faint to hear in 2002.\n\nOn July 19, 2015, \"Pioneer 11\" was from the Earth and from the Sun; and traveling at (relative to the Sun) and traveling outward at about 2.4 AU per year. The spacecraft is heading in the direction of the constellation Scutum near the current position (August 2017) RA 18h 50m dec -8° 39.5' (J2000.0) close to Messier 26.\n\n\"Pioneer 11\" has now been overtaken by the two Voyager probes, launched in 1977, and \"Voyager 1\" is now the most distant object built by humans.\n\nAnalysis of the radio tracking data from the \"Pioneer 10\" and \"11\" spacecraft at distances between 20–70 AU from the Sun has consistently indicated the presence of a small but anomalous Doppler frequency drift. The drift can be interpreted as due to a constant acceleration of directed towards the Sun. Although it is suspected that there is a systematic origin to the effect, none was found. As a result, there is sustained interest in the nature of this so-called \"Pioneer anomaly\". Extended analysis of mission data by Slava Turyshev and colleagues has determined the source of the anomaly to be asymmetric thermal radiation and the resulting thermal recoil force acting on the face of the Pioneers away from the Sun, and in July 2012 the group of researchers published their results in the \"Physical Review Letters\" scientific journal.\n\n\"Pioneer 10\" and \"11\" both carry a gold-anodized aluminum plaque in the event that either spacecraft is ever found by intelligent lifeforms from other planetary systems. The plaques feature the nude figures of a human male and female along with several symbols that are designed to provide information about the origin of the spacecraft.\n\nIn 1991, \"Pioneer 11\" was honored on one of 10 United States Postage Service stamps commemorating unmanned spacecraft exploring each of the then nine planets and the Moon. \"Pioneer 11\" was the spacecraft featured with Jupiter. Pluto was listed as \"Not yet explored\".\n\n"}
{"id": "31462586", "url": "https://en.wikipedia.org/wiki?curid=31462586", "title": "Rational design", "text": "Rational design\n\nIn chemical biology and biomolecular engineering, rational design is the strategy of creating new molecules with a certain functionality, based upon the ability to predict how the molecule's structure will affect its behavior through physical models. This can be done either from scratch or by making calculated variations on a known structure, and is usually contrasted with directed evolution. Applications of rational design include:\n\n"}
{"id": "26332", "url": "https://en.wikipedia.org/wiki?curid=26332", "title": "Rural flight", "text": "Rural flight\n\nRural flight (or rural exodus) is the migratory pattern of peoples from rural areas into urban areas. It is urbanization seen from the rural perspective.\n\nIn modern times, it often occurs in a region following the industrialization of agriculture—when fewer people are needed to bring the same amount of agricultural output to market—and related agricultural services and industries are consolidated. Rural flight is exacerbated when the population decline leads to the loss of rural services (such as business enterprises and schools), which leads to greater loss of population as people leave to seek those features.\n\nThis phenomenon was first articulated through Christian McLean's laws of migration in the 1880s, upon which modern theories are based.\n\nPrior to the Industrial Revolution, rural flight occurred in mostly localized regions. Pre-industrial societies did not experience large rural-urban migration flows primarily due to the inability of cities to support large populations. Lack of large employment industries, high urban mortality, and low food supplies all served as checks keeping pre-industrial cities much smaller than their modern counterparts. Ancient Athens and Rome, scholars estimate, had peak populations of 80,000 and 500,000 paling in comparison with their current populations.\n\nThe onset of the Industrial Revolution in Europe in the late 19th century removed many of the checks that had previously constrained urban populations. As food supplies increased and stabilized and industrialized centers moved into place, cities began to support larger populations, sparking the beginning of rural flight on a massive scale. The United Kingdom went from having 20% of the population living in urban areas in 1800 to more than 70% by 1925. While the late 19th century and early 20th century saw much of rural flight focused in Western Europe and the United States, as industrialization spread throughout the world during the 20th century, rural flight and urbanization followed quickly behind. Today, rural flight is an especially distinctive phenomenon in some of the newer urbanized areas including China and more recently sub-Saharan Africa.\n\nThe shift from mixed subsistence farming to commodity crops and livestock began in the late 19th century. New capital market systems and the railroad network began the trend towards larger farms that employed fewer people per acre. These larger farms used more efficient technologies such as steel plows, mechanical reapers, and higher-yield seed stock, which reduced human input per unit of production. The other issue on the Great Plains was that people were using inappropriate farming techniques for the soil and weather conditions. Most homesteaders had family farms generally considered too small to survive (under 320 acres), and European-American subsistence farming could not continue as it was then practiced.\n\nDuring the Dust Bowl and the Great Depression of the 1930s, large numbers of people fled rural areas of the Great Plains and the Midwest due to depressed commodity prices and high debt loads exacerbated by several years of drought and large dust storms. Rural flight from the Great Plains has been depicted in literature, such as John Steinbeck's novel \"The Grapes of Wrath\" (1939), in which a family from the Great Plains migrates to California during the Dust Bowl period of the 1930s.\n\nPost-World War II rural flight has been caused primarily by the spread of industrialized agriculture. Small, labor-intensive family farms have grown into, or have been replaced by, heavily mechanized and specialized industrial farms. While a small family farm typically produced a wide range of crop, garden, and animal products—all requiring substantial labor—large industrial farms typically specialize in just a few crop or livestock varieties, using large machinery and high-density livestock containment systems that require a fraction of the labor per unit produced. For example, Iowa State University reports the number of hog farmers in Iowa dropped from 65,000 in 1980 to 10,000 in 2002, while the number of hogs per farm increased from 200 to 1,400.\n\nThe consolidation of the feed, seed, processed grain, and livestock industries has meant that there are fewer small businesses in rural areas. This decrease in turn exacerbated the decreased demand for labor. Rural areas that used to be able to provide employment for all young adults willing to work in challenging conditions, increasingly provide fewer opportunities for young adults. The situation is made worse by the decrease in services such as schools, business, and cultural opportunities that accompany the decline in population, and the increasing age of the remaining population further stresses the social service system of rural areas.\n\nThe rise of corporate agricultural structures directly affects small rural communities, resulting in decreased populations, decreased incomes for some segments, increased income inequality, decreased community participation, fewer retail outlets and less retail trade, and increased environmental pollution.\n\nThere are several determinants, push and pull, that contribute to rural flight: lower levels of (perceived) economic opportunity in rural communities versus urban ones, lower levels of government investment in rural communities, greater education opportunities in cities, marriages, increased social acceptance in urban areas, and higher levels of rural fertility.\n\nSome migrants choose to leave rural communities out of the desire to pursue greater economic opportunity in urban areas. Greater economic opportunities can be real or perceived. According to the Harris-Todaro Model, migration to urban areas will continue as long as \"expected urban real income at the margin exceeds real agricultural product\" (127). However, sociologist Josef Gugler points out that while individual benefits of increased wages may outweigh the costs of migration, if enough individuals follow this rationale, it can produce harmful effects such as overcrowding and unemployment on a national level. This phenomenon, when the rate of urbanization outpaces the rate of economic growth, is known as overurbanization. Since the industrialization of agriculture, mechanization has reduced the number of jobs present in rural communities. Some scholars have also attributed rural flight to the effects of globalization as the demand for increased economic competitiveness leads people to choose capital over labor. At the same time, rural fertility rates have historically been higher than urban fertility rates. The combination of declining rural jobs and a persistently high rural fertility rate has led to rural-urban migration streams. Rural flight also contains a positive feedback loop where previous migrants from rural communities assist new migrants in adjusting to city life. Also known as chain migration, migrant networks lower barriers to rural flight. For example, an overwhelming majority of rural migrants in China located jobs in urban areas through migrant networks.\n\nSome families choose to send their children to cities as a form of investment for the future. A study conducted by Bates and Bennett (1974) concluded that rural communities in Zambia that had other viable investment opportunities, like livestock for instance, had lower rates of rural-urban migration as compared to regions without viable investment opportunities. Sending their children into cities can serve as long-term investments with the hope that their children will be able to send remittances back home after getting a job in the city.\n\nThere are severe challenges faced by poorer people in the agriculture sector because of diminishing access to productive farmland. Foreign investors through Foreign Direct Investment (FDI) schemes have been encouraged to lease land in rural areas in Cambodia and Ethiopia. This has led to the loss of farmland, range land, woodlands and water sources from local communities. Large-scale agricultural projects funded by FDI only employed a few experts specialized in the relevant new technologies.\n\nIn other instances, rural flight may occur in response to social determinants. A study conducted in 2012 indicated that a significant proportion of rural flight in India occurred due to social factors such as migration with household, marriage, and education. Migration with households and marriage affect women in particular as most often they are the ones required to move with households and move for marriage, especially in developing regions. \nRural youth may choose to leave their rural communities as a method of transitioning into adulthood, seeking avenues to greater prosperity. With the stagnation of the rural economy and encouragement from their parents, rural youth may choose to migrate to cities out of social norms – demonstrating leadership and self-respect. With this societal encouragement combined with depressed rural economies, rural youth form a large proportion of the migrants moving to urban areas. In Sub-Saharan Africa, a study conducted by Touray in 2006 indicated that about 15% (26 million) of urban migrants were youth. \nLastly, natural disasters can often be single-point events that lead to temporarily massive rural-urban migration flows. The 1930s Dust Bowl in the United States, for example, led to the flight of 2.5 million people from the Plains by 1940, many to the new cities in the West. It is estimated that as many as one out of every four residents in the Plains States left during the 1930s. More recently, drought in Syria from 2006-2011 has prompted a rural exodus to major urban centers. Massive influxes in urban areas, combined with difficult living conditions, have prompted some scholars to link the drought to the arrival of the Arab Spring in Syria.\n\nThe terms are used in the United States and Canada to describe the flight of people from rural areas in the Great Plains and Midwest regions, and to a lesser extent rural areas of the northeast and southeast and Appalachia. It is also particularly noticeable in parts of Atlantic Canada (especially Newfoundland), since the collapse of Atlantic cod fishing fields in 1992.\n\nChina, like many other currently industrializing countries, has had a relatively late start to rural flight. Until 1983, the Chinese government, through the hukou system, greatly restricted the ability of their citizens to internally migrate. Since 1983, the Chinese government has progressively lifted the restrictions on internal migration. This has led to a great increase in the number of people migrating to urban areas. However, even today, the hukou system limits the ability of rural migrants to receive full access to urban social services at the urban subsidized costs.\n\nAs with most examples of rural flight, several factors have led towards China’s massive urbanization. Income disparity, family pressure, surplus labor in rural areas due to higher average fertility rates, and improved living conditions all play a role in contributing to the flows of migrants from rural to urban areas. Approximately, 250 million rural migrants now live in cities with 54% of the total Chinese population living in urban areas.\n\nRural flight has been occurring to some degree in Germany since the 11th century. A corresponding principle of German law is \"Stadtluft macht frei\" (\"city air makes you free\"), in longer form \"Stadtluft macht frei nach Jahr und Tag\" (\"city air makes you free after a year and a day\"): by custom and, from 1231/32, by statute, a serf who had spent a year and a day in a city was free, and could not be reclaimed by their former master.\n\n\"Landflucht\" (\"flight from the land\") refers to the mass migration of peasants into the cities that occurred in Germany (and throughout most of Europe) in the late 19th century.\n\nIn 1870 the rural population of Germany constituted 64% of the population; by 1907 it had shrunk to 33%. In 1900 alone, the Prussian provinces of East Prussia, West Prussia, Posen, Silesia, and Pomerania lost about 1,600,000 people to the cities, where these former agricultural workers were absorbed into the rapidly growing factory labor class; One of the causes of this mass-migration was the decrease in rural income compared to the rates of pay in the cities.\n\nLandflucht resulted in a major transformation of the German countryside and agriculture. Mechanized agriculture and migrant workers, particularly Poles from the east (Sachsengänger), became more common. This was especially true in the province of Posen that was gained by Prussia when Poland was partitioned. The Polish population of eastern Germany was one of the justifications for the creation of the \"Polish corridor\" after World War I and the absorption of the land east of the Oder-Neisse line into Poland after World War II. Also, some labor-intensive enterprises were replaced by much less labor-intensive ones such as game preserves.\n\nThe word \"Landflucht\" has negative connotations in German, as it was coined by agricultural employers, often of the German aristocracy, who were lamenting their labor shortages.\n\nRural flight and out-migration in Sweden can be traced in two distinct waves. The first, beginning in the 1850s when 82% of the Swedish population lived in rural areas, and continuing till the late 1880s, was mostly due to push factors in the countryside related to poverty, unemployment, low agricultural wages, debt peonage, semi-feudalism, and religious oppression by the State church. Most of the migration was ad-hoc and directed towards emigration to the three big cities of Sweden, America, Denmark, or Germany. Many of these first emigrants were unskilled, barely literate laborers who sought farm work or daily wage labour in the cities.\n\nThe second wave started from the late 1890s and reached its peak between 1922 and 1967, with the highest rates of rural flight occurring in the 1920s and the 1950s. This was mostly \"pull factors\" due to the economic boom and industrial prosperity in Sweden wherein the massive economic expansion and wage increases in the urban areas pulled young people to migrate for work and at the same time drove down work opportunities in the countryside. Between 1925 and 1965, Sweden's GDP per capita increased from USD 850 to USD 6200. Simultaneously, the percentage of the population living in rural areas decreased drastically from 54% in 1925 to 21% in 1965.\n\nRural flight began later for Russia and the former states of the USSR than in Western Europe. In 1926 only 18% of Russians lived in urban areas, compared to over 75% at the same time in the United Kingdom. Although the process began later, throughout World War II and the decades immediately proceeding, rural flight proceeded at a rapid pace. By 1965, 53% of Russians lived in urban areas. Statistics compiled by M. Ya Sonin, a Soviet author, in 1959, demonstrate the rapid urbanization of the USSR. Between 1939 and 1959, the rural population declined by 21.3 million, while that of urban centers increased by 39.4 million. Of this dramatic shift in population, rural flight accounts for more than 60% of the change. Generally, most rural migrants tended to settle in cities and towns within their district. Rural flight persisted through the majority of the 20th century. However, with the end of the Soviet Union, rural flight reversed as political and economic instability in the cities prompted many urban dwellers to return to rural villages. \nRural flight did not occur uniformly throughout the USSR. Western Russia and Ukraine experienced the greatest declines in rural population, 30% and 17% respectively. Conversely, peripheral regions of the USSR, like Central Asia, experienced gains, contradicting the general pattern of rural-urban migration of this period. Increased diversification of crops and labor shortages were primary contributors to the gains in rural population in the periphery.\n\nRural flight in Russia and the former USSR had several major determinants. The industrialization of agriculture, which came later in Russia and the former USSR, led to declines in available rural jobs. Lower living standards and tough work also motivated some peasants to migrate to urban areas. In particular, the Soviet \"kolkhoz\" system (the collective farms in the Soviet Union) aided in maintaining low living standards for Soviet peasants. Beginning around 1928, the kolkhoz system replaced family farms throughout the Soviet Union. Forced to work long hours for low pay at rates fixed by the government and often unadjusted to inflation, Russian peasants experienced quite low living-conditions - especially compared to urban life. While Brezhnev's wage reforms in 1965 ameliorated the low wages received by peasants, rural life remained suffocating, especially for the skilled and the educated. \nAlthough migrants came from all segments of society, several groups were more likely to migrate than others. Like other examples of rural flight, the young were more likely than the old to migrate to the cities. Young women under 20 were the most likely segment of the population to leave rural life. This exodus of young women further exacerbated the demographic transitions occurring in rural communities as the rate of natural increase dropped precipitously over the course of the 20th century. Lastly, the skilled and educated were also likely to migrate to urban areas.\n\nRural flight in Mexico occurred throughout the 1930s up until the present day. Like other developing nations, the beginning of industrialization in Mexico quickly accelerated the rate of rural flight.\n\nIn the 1930s, President Cardenas implemented a series of agricultural reforms that led to massive redistribution of agricultural land among the rural peasants. Some commentators have subsequently dubbed the period from 1940-1965 as the \"Golden Era for Mexican Migration.\" During this period, Mexican agriculture grew at an average rate of 5.7% outpacing the natural increase of 3% of the rural population. Concurrently, government policies favoring industrialization led to a massive increase of industrial jobs in the cities. Statistics compiled in Mexico City demonstrate this trend with over 1.8 million jobs created over the course of the 1940s, 50s, and 60s. Young people with schooling were the segment of population most likely to migrate away from rural life to urban life, attracted by the promise of many jobs and a more modern lifestyle as compared to the conservative conditions in rural villages. Additionally, due to the large demand for new workers, many of these jobs had low entrance requirements that also provided on-site job training opening the avenue for migration to many rural residents. From 1940 to about 1965, rural flight occurred in a slow, yet steady pace with both agriculture and industry growing concurrently.\n\nHowever, as government policies increasingly favored industry over agriculture, rural conditions began to deteriorate. In 1957, the Mexican government began to regulate the price of maize through massive imports in order to keep low urban food costs. This regulation severely undercut the market price of maize lowering the profit margins of small farmers. At the same time, the Green Revolution had entered into Mexican agriculture. Inspired by the work of Norman Borlaug, farmers that employed hybrid seeds and fertilizer supplements were able to double or even triple their yields per acre. Unfortunately, these products came at a relatively high cost, out of the reach of many farmers struggling after the devaluation of the price of maize. The combined effects of the maize price regulation and the Green Revolution was the consolidation of small farms into larger estates. A 1974 study conducted by Osorio concluded that in 1960, about 50.3% of the individual land plots in Mexico contained less than 5 hectares of land. In contrast, the top 0.5% of estates by land spanned 28.3% of all arable land. As many small farmers lost land, they either migrated to the cities or became migrant workers roving from large estate to large estate. Between 1950 and 1970, the proportion of migrant workers increased from 36.7% to 54% of the total population. The centralized pattern of industrial development and government policies overwhelmingly favoring industrialization contributed to massive rural flight in Mexico beginning in the late 1960s until the present day.\n\nRural migrants to cities face several challenges that may hinder their quality of life upon moving into urbanized areas. Many migrants do not have the education or skills to acquire decent jobs in cities and are then forced into unstable, low paying jobs. The steady stream of new rural migrants worsens underemployment and unemployment, common among rural migrants. Employers offer lower wages and poorer labor conditions to rural migrants, who must compete with each other for limited jobs, often unaware of their labor rights. Rural migrants often experience poor living conditions as well. Many cities have exploded in population; services and infrastructure, in these cities, are unable to keep up with population growth. Massive influxes in rural population can lead to severe housing shortages, inadequate water and energy supply, and general slum-like conditions throughout cities.\n\nAdditionally, rural migrants often struggle adjusting to city life. In some instances, there are cultural differences between the rural and urban areas of a region. Lost in urban regions, it becomes difficult for them to continue holding onto their cultural traditions. Urban residents may also look down upon these newcomers to the city who are often unaware of city social norms. Both marginalized and separated from their home cultures, migrants face many social challenges when moving to cities.\n\nWomen, in particular, face a unique set of challenges. Some women undergo rural flight to escape domestic abuse or forced early marriages. Some parents choose to send women to cities to find jobs in order to send remittances back home. Once in the city, employers may attempt to take advantage of these women preying on their unfamiliarity with labor laws and social networks on which to rely. In the worst of cases, destitution may force women into prostitution, exposing them to social stigma and the risks of sexually transmitted diseases.\n\n\n"}
{"id": "3358868", "url": "https://en.wikipedia.org/wiki?curid=3358868", "title": "Savilian Professor of Astronomy", "text": "Savilian Professor of Astronomy\n\nThe position of Savilian Professor of Astronomy was established at the University of Oxford in 1619. It was founded (at the same time as the Savilian Professorship of Geometry) by Sir Henry Savile, a mathematician and classical scholar who was Warden of Merton College, Oxford, and Provost of Eton College. He appointed John Bainbridge as the first professor, who took up his duties in 1620 or 1621.\n\nThere have been 21 astronomy professors in all; Steven Balbus, the professor , was appointed in 2012. Past professors include Christopher Wren (1661–73), architect of St Paul's Cathedral in London and the Sheldonian Theatre in Oxford; he held the professorship at the time of his commission to rebuild the cathedral after it was destroyed by the Great Fire of London in 1666. Three professors have been awarded the Gold Medal of the Royal Astronomical Society: Charles Pritchard (1870–93), Harry Plaskett (1932–60) and Joseph Silk (1999–2012). The two Savilian chairs have been linked with professorial fellowships at New College, Oxford, since the late 19th century. In the past, some of the professors were provided with an official residence, either near New College or at the Radcliffe Observatory, although this practice ended in the 19th century. The astronomy professor is a member of the Sub-Department of Astrophysics at Oxford.\n\nSir Henry Savile, the Warden of Merton College, Oxford, and Provost of Eton College, was deeply saddened by what the 20th-century mathematician Ida Busbridge has described as \"the wretched state of mathematical studies in England\", and so founded professorships in geometry and astronomy at the University of Oxford in 1619; both chairs were named after him. He also donated his books to the university's Bodleian Library. He required the professors to be men of good character, at least 26 years old, and to have \"imbibed the purer philosophy from the springs of Aristotle and Plato\" before acquiring a thorough knowledge of science. The professors could come from any Christian country, but he specified that a professor from England should have a Master of Arts degree as a minimum. He wanted students to be educated in the works of the leading scientists of the ancient world; in addition, the astronomy professor should cover Copernicus and the work of Arab astronomers. Tuition in trigonometry was to be shared by the two professors. As many students would have had little mathematical knowledge, the professors were also permitted to provide instruction in basic mathematics in English (as opposed to Latin, the language used in education at Oxford at the time). He also required the astronomy professor \"to take astronomical observations as well by night as by day (making choice of proper instruments prepared for the purpose, and at fitting times and seasons)\", and to place in the library records of his discoveries. Savile prohibited the professors from practicing astrology or preparing horoscopes, and stated that accepting any position as a priest or as an officer of the university or of a college would cause forfeiture of the professorship. Each professor was required to lecture in public for 45 minutes twice weekly during the university terms and would be fined 10 shillings for every day missed (except in cases of \"grievous bodily ailment\", although this excuse was only permitted for three weeks before the professor was required to provide a substitute lecturer). Students who were required to attend, but who failed to do without good cause, were to be fined sixpence. Savile provided that the rents from specified properties in Kent and Essex were to be divided equally between the professors, giving each £160 annually.\n\nSavile selected John Bainbridge to be the first astronomy professor; Bainbridge had impressed him with a description of a comet seen in 1618. In the documents establishing the professorship (sealed by Savile and the university in August 1619), Savile reserved to himself the right to appoint the professors during his lifetime, although he died in 1622 before the position fell vacant. He provided that after his death, vacancies should be filled by a majority of a group of \"most distinguished persons\": the Archbishop of Canterbury, the Lord Chancellor, the Chancellor of the university, the Bishop of London, the Secretary of State, the Chief Justice of the Common Pleas, the Chief Justice of the King's Bench, the Chief Baron of the Exchequer and the Dean of the Court of Arches. The Vice-Chancellor of the university was to inform the electors of any vacancy, and could be summoned to advise them. The appointment could either be made straight away, or delayed for some months to see whether \"any eminent mathematician can be allured\" from abroad.\n\nAs part of reforms of the university in the 19th century, the University of Oxford commissioners laid down new statutes for the chair in 1881, replacing Savile's original instructions and requirements. The 1881 statute provided that the professor was to \"lecture and give instruction in theoretical and practical Astronomy\", and was to be a Fellow of New College. The electors for the professorship were to be the Warden of New College (or a person nominated by the college in his place), the Chancellor of the university, the President of the Royal Society, the Astronomer Royal, the Radcliffe Observer, a person nominated by the university council and one other nominated by New College. Changes to the university's internal legislation in the 20th and early 21st centuries abolished specific statutes for the duties of, and rules for appointment to, individual chairs such as the Savilian professorships. The University Council is now empowered to make appropriate arrangements for appointments and conditions of service, with the college to which any professorship is allocated (New College in the case of the Savilian chairs) to have two representatives on the board of electors. The professorship is one of two permanent chairs attached to Oxford's Sub-Department of Astrophysics.\n\nTwo official residences have been provided for the astronomy professor. The first was in New College Lane, in central Oxford. John Wallis (geometry professor 1649–1703) rented a house there from New College from 1672 until his death in 1703; at some point, it was divided into two houses. Towards the end of his life, David Gregory (astronomy professor 1691–1708) lived in the eastern part of the premises. Wallis's son gave the unexpired portion of the lease to the university in 1704 in honour of his father's long tenure of the geometry chair, to provide official residences for the two Savilian professors. New College renewed the lease at a low rent from 1716 and thereafter at intervals until the last renewal in 1814. Records of who lived in each house are not available throughout the period, but surviving documentation shows that the professors often sub-let the houses and that for about twenty years in the early 18th century the premises were being used as a lodging house.\n\nThe second official residence was built during the time of Thomas Hornsby (astronomy professor 1763–1810), who proposed that an observatory should be built at a site to the north of the city centre. In 1772, construction began of the Radcliffe Observatory and an adjoining house for the astronomy professor, to which Hornsby moved. Thereafter the university sub-let his former residence. Both of his successors, Abraham Robertson (1810–27) and Stephen Rigaud (1827–39), were the geometry professor at their appointment to the astronomy chair, and in turn they moved from New College Lane to the Radcliffe Observatory. The university then sub-let the astronomy professor's house. The link between the professorship and the observatory was broken in 1839 with the appointment of George Johnson; he had little practical astronomical experience and the officers in charge of the observatory appointed Manuel Johnson as Radcliffe Observer instead. In the early 19th century, New College decided that it wished to use the properties for itself and the lease expired without renewal in 1854. Charles Pritchard (1870–93) had a new observatory built in the University Parks, but his attempts to persuade the university to add a residence for the Savilian professor were unsuccessful.\n\n\nNotes<br>\nReferences<br>\n"}
{"id": "17795038", "url": "https://en.wikipedia.org/wiki?curid=17795038", "title": "Stanisław Kuczborski", "text": "Stanisław Kuczborski\n\nStanisław Kuczborski (January 31, 1912 – August 23, 2004) was a Polish pulmonologist.\n\nKuczborski obtained his high school diploma in 1930, at the Mikołaj Kopernik Boys’ High School in Łódź. In 1930 he began his studies at the Medical Department of the Warsaw University. As a student, he contracted tuberculosis and was referred to Warsaw University's Sanatorium in Zakopane. This experience caused him to focus his medical studies. After he finished a one-year postgraduate internship in Warsaw hospitals, he worked in a sanatorium in Otwock, near Warsaw. There he expanded work in the field of internal medicine, particularly Physical medicine and rehabilitation (PM&R), or physiatry. He spent the years during the Nazi occupation in the sanatorium. After World War II, he returned to his home city of Łódź. In 1951, he received his PhD from the Medical University of Silesia.\n\nAs a physician, Kuczborski was a pulmonary and physiatric consultant. His career was mainly associated with Łódź; only after from 1952 to 1955, did he reside in Warsaw serving as the deputy director of the Tuberculosis Institute in Warsaw. Kuczborski held posts as director of several hospitals in Łódź. He also established a laboratory of physiopathology of breathing, the first in the Łódź region and one of the first in Poland. He also was the director of the Lung Disease Hospital in Łagiewniki until he retired in 1977.\n\nHe held foreign (Denmark, France) scientific internships mainly in the field of functional tests and rehabilitation of the respiratory system. He was an honorary member of the International Union against Tuberculosis and Lung Disease located in Paris. In Poland, he received an honorary membership of the Polish Physiatry and Pulmonology Association. Kuczborski published 70 works, mainly on tuberculosis (TB) chemotherapy, physiopathology of breathing, rehabilitation and intensive care of the respiratory system diseases.\n"}
{"id": "309379", "url": "https://en.wikipedia.org/wiki?curid=309379", "title": "Symbolic interactionism", "text": "Symbolic interactionism\n\nSymbolic interactionism is a sociological theory that develops from practical considerations and alludes to people's particular utilization of dialect to make images and normal implications, for deduction and correspondence with others. In other words, it is a frame of reference to better understand how individuals interact with one another to create symbolic worlds, and in return, how these worlds shape individual behaviors. It is a framework that helps understand how society is preserved and created through repeated interactions between individuals. The interpretation process that occurs between interactions help create and recreate meaning. It is the shared understanding and interpretations of meaning that affect the interaction between individuals. Individuals act on the premise of a shared understanding of meaning within their social context.Thus, interaction and behavior is framed through the shared meaning that objects and concepts have attached to them. \n\nSymbolic interactionism comes from a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline. It is particularly important in microsociology and social psychology. It is derived from the American philosophy of pragmatism and particularly from the work of George Herbert Mead, as a pragmatic method to interpret social interactions.\n\nSymbolic interaction was conceived by George Herbert Mead and Charles Horton Cooley. Mead argued that people's selves are social products, but that these selves are also purposive and creative, and believed that the true test of any theory was that it was \"useful in solving complex social problems\". Mead's influence was said to be so powerful that sociologists regard him as the one \"true founder\" of the symbolic interactionism tradition. Although Mead taught in a philosophy department, he is best known by sociologists as the teacher who trained a generation of the best minds in their field. Strangely, he never set forth his wide-ranging ideas in a book or systematic treatise. After his death in 1931, his students pulled together class notes and conversations with their mentor and published \"Mind, Self and Society\" in his name. It is a common misconception that John Dewey was the leader of this sociological theory; according to \"The Handbook of Symbolic Interactionism,\" Mead was undoubtedly the individual who \"transformed the inner structure of the theory, moving it to a higher level of theoretical complexity\". \"Mind, Self and Society\" is the book published by Mead's students based on his lectures and teaching, and the title of the book highlights the core concept of social interactionism. \"Mind\" refers to an individual's ability to use symbols to create meanings for the world around the individual – individuals use language and thought to accomplish this goal. \"Self\" refers to an individual's ability to reflect on the way that the individual is perceived by others. Finally, \"society\", according to Mead, is where all of these interactions are taking place. A general description of Mead's compositions portray how outside social structures, classes, and power and abuse affect the advancement of self, personality for gatherings verifiably denied of the ability to characterize themselves.\n\nHerbert Blumer, a student and interpreter of Mead, coined the term and put forward an influential summary: people act a certain way towards things based on the meaning those things already have, and these meanings are derived from social interaction and modified through interpretation. Blumer was a social constructionist, and was influenced by John Dewey; as such, this theory is very phenomenologically-based. Given that Blumer was the first to use symbolic interaction as a term, he is known as the founder of symbolic interaction. He believed that the \"Most human and humanizing activity that people engage in is talking to each other.\" According to Blumer, human groups are created by people and it is only actions between them that define a society. He argued that with interaction and through interaction individuals are able to \"produce common symbols by approving, arranging, and redefining them.\" Having said that, interaction is shaped by a mutual exchange of interpretation, the ground of socialization.\n\nTwo other theorists who have influenced symbolic interaction theory are Yrjö Engeström and David Middleton. Engeström and Middleton explained the usefulness of symbolic interactionism in the communication field in a variety of work settings, including \"courts of law, health care, computer software design, scientific laboratory, telephone sales, control, repair, and maintenance of advanced manufacturing systems\". Other scholars credited for their contribution to the theory are Thomas, Park, James, Horton Cooley, Znaniecki, Baldwin, Redfield, and Wirth. Unlike other social sciences, symbolic interactionism emphasizes greatly on the ideas of action instead of culture, class and power. According to behaviorism, Darwinism, pragmatism, as well as Max Weber, action theory contributed significantly to the formation of social interactionism as a theoretical perspective in communication studies.\n\nMost symbolic interactionists believe a physical reality does indeed exist by an individual's social definitions, and that social definitions do develop in part or in relation to something \"real\". People thus do not respond to this reality directly, but rather to the social understanding of reality; i.e., they respond to this reality indirectly through a kind of filter which consists of individuals' different perspectives. This means that humans exist not in the physical space composed of realities, but in the \"world\" composed only of \"objects\".\n\nThree assumptions frame symbolic interactionism:\n\nHaving defined some of the underlying assumptions of symbolic interactionism, it is necessary to address the premises that each assumption supports. According to Blumer, there are three premises that can be derived from the assumptions above.\n\nPremise 1: \"Humans act toward things on the basis of the meanings they ascribe to those things.\"\n\nThe first premise includes everything that a human being may note in their world, including physical objects, actions and concepts. Essentially, individuals behave towards objects and others based on the personal meanings that the individual has already given these items. Blumer was trying to put emphasis on the meaning behind individual behaviors, specifically speaking, psychological and sociological explanations for those actions and behaviors.\n\nPremise 2: \"The meaning of such things is derived from, or arises out of, the social interaction that one has with others and the society.\"\n\nThe second premise explains the meaning of such things is derived from, or arises out of, the social interaction that one has with other humans. Blumer, following Mead, claimed people interact with each other by interpreting or defining each other's actions instead of merely reacting to each other's actions. Their \"response\" is not made directly to the actions of one another but instead is based on the meaning which they attach to such actions. Thus, human interaction is mediated by the use of symbols and signification, by interpretation, or by ascertaining the meaning of one another's actions. Meaning is either taken for granted and pushed aside as an unimportant element which need not to be investigated, or it is regarded as a mere neutral link or one of the causal chains between the causes or factors responsible for human behavior and this behavior as the product of such factors.\n\nPremise 3: \"The Meanings are handled in, and modified through, an interpretative process used by the person in dealing with the things he/she encounters.\n\nSymbolic interactionists describe thinking as an inner conversation. Mead called this inner dialogue \"minding\", which is the delay in one's thought process that happens when one thinks about what they will do next. These meanings are handled in, and modified through, an interpretive process used by the person in dealing with the things he encounters. We naturally talk to ourselves in order to sort out the meaning of a difficult situation. But first, we need language. Before we can think, we must be able to interact symbolically. The emphasis on symbols, negotiated meaning, and social construction of society brought attention to the roles people play. Role-taking is a key mechanism that permits people to see another person's perspective to understand what an action might mean to another person. Role-taking is a part of our lives at an early age, for instance, playing house and pretending to be someone else. There is an improvisational quality to roles; however, actors often take on a script that they follow. Because of the uncertainty of roles in social contexts, the burden of role-making is on the person in the situation. In this sense, we are proactive participants in our environment.\n\nThe majority of interactionist research uses qualitative research methods, like participant observation, to study aspects of social interaction, and/or individuals' selves. Participant observation allows researchers to access symbols and meanings, as in Howard S. Becker's \"Art Worlds\" and Arlie Hochschild's \"The Managed Heart\". They argue that close contact and immersion in the everyday activities of the participants is necessary for understanding the meaning of actions, defining situations and the process that actors construct the situation through their interaction. Because of this close contact, interactions cannot remain completely liberated of value commitments. In most cases, they make use of their values in choosing what to study; however, they seek to be objective in how they conduct the research. Therefore, the symbolic-interaction approach is a micro-level orientation focusing on human interaction in specific situations.\n\nThere are five central ideas to symbolic interactionism according to Joel M. Charon, author of \"Symbolic Interactionism An Introduction, An Interpretation, An Integration\":\n\n\nTo Blumer's conceptual perspective, he put them in three core principles: that people act toward things, including each other, on the basis of the meanings they have for them; that these meanings are derived through social interaction with others; and that these meanings are managed and transformed through an interpretive process that people use to make sense of and handle the objects that constitute their social worlds. Keeping Blumer's earlier work in mind David A. Snow, professor of sociology at the University of California, Irvine, suggests four broader and even more basic orienting principles: human agency, interactive determination, symbolization, and emergence. Snow uses these four principles as the thematic bases for identifying and discussing contributions to the study of social movements.\n\n\nHuman agency emphasizes the active, willful, goal-seeking character of human actors. The emphasis on agency focuses attention on those actions, events, and moments in social life in which agentic action is especially palpable.\n\nInteractive determination specifies that understanding of focal objects of analysis, whether they are self-concepts, identities, roles, practices, or even social movements. Basically this means, neither individual, society, self, or others exist only in relation to each other and therefore can be fully understood only in terms of their interaction.\n\nSymbolization highlights the processes through which events and conditions, artifacts, people, and other environmental features that take on particular meanings, becoming nearly only objects of orientation. Human behavior is partly contingent on what the object of orientation symbolizes or means.\n\n\nEmergence focuses on attention on the processual and non-habituated side of social life, focusing not only on organization and texture of social life, but also associated meaning and feelings. The principal of emergence tells us not only to possibility of new forms of social life and system meaning but also to transformations in existing forms of social organization.\n\nNew media is a term used to define all that is related to the internet and the interplay between technology, images and sound. As studies of online community proliferate, the concept of online community has become a more accepted social construct. Studies encompassed discursive communities; identity; community as social reality; networking; the public sphere; ease and anonymity in interactions. These studies show that online community is an important social construct in terms of its cultural, structural, political and economic character.\n\nIt has been demonstrated that people's ideas about community are formed, in part, through interactions both in online forums and face-to-face. As a result, people act in their communities according to the meanings they derive about their environment, whether online or offline, from those interactions. This perspective reveals that online communication may very well take on different meanings for different people depending on information, circumstance, relationships, power, and other systems that make up communities of practice. People enact community the way it is conceived and the meaning of community evolves as they come up with new ways to utilize it. Given this reality, scholars are continually challenged to research and understand how online communities are comprised, how they function, and how they are connected to offline social life.\n\nSymbolic interaction theory was discussed in \"The Cyberself: The Self-ing Project goes online, Symbolic Interaction in the Digital Age\". Laura Robinson discusses how symbolic interaction theory explains the way individuals create a sense of self through their interactions with others. However, she believes advances in technology have changed this. The article investigates the manner in which individuals form their online identity. She uses symbolic interaction theory to examine the formation of the cyber \"I\" and a digital \"generalized other\". In the article, Robinson suggests individuals form new identities on the internet. She argues these cyber identities are not necessarily the way the individual would be perceived offline.\n\nSymbolic interactionists are often criticized for being overly impressionistic in their research methods and somewhat unsystematic in their theories. It is argued that the theory is not one theory, but rather, the \"framework\" for many different theories. Additionally, some theorists have a problem with symbolic interaction theory due to its lack of testability. These objections, combined with the fairly narrow focus of interactionist research on small-group interactions and other social psychological issues, have relegated the interactionist camp to a minority position among sociologists (albeit a fairly substantial minority). Much of this criticism arose during the 1970s in the U.S. when quantitative approaches to sociology were dominant. Perhaps the best known of these is by Alvin Gouldner.\n\nSome critiques of symbolic interactionism are based on the assumption that it is a theory, and the critiques apply the criteria for a \"good\" theory to something that does not claim to be a theory. Some critics find the symbolic interactionist framework too broad and general when they are seeking specific theories. Symbolic interactionism is a theoretical \"framework\" rather than a theory and can be assessed on the basis of effective conceptualizations. The theoretical framework, as with any theoretical framework, is vague when it comes to analyzing empirical data or predicting outcomes in social life. As a framework rather than a theory, many scholars find it difficult to use. Interactionism being a framework rather than a theory makes it impossible to test interactionism in the manner that a specific theoretical claim about the relationship between specific variables in a given context allows. Unlike the symbolic interactionist framework, the many theories derived from symbolic interactionism, such as role theory and the versions of identity theory developed by Sheldon Stryker, and Peter Burke and colleagues, clearly define concepts and the relationships between them in a given context, thus allowing for the opportunity to develop and test hypotheses. Further, especially among Blumerian processual interactionists, a great number of very useful conceptualizations have been developed and applied in a very wide range of social contexts, types of populations, types of behaviors, and cultures and subcultures.\n\nSymbolic interactionism is often related and connected with social structure. This concept suggests that symbolic interactionism is a construction of people's social reality. It also implies that from a realistic point of view, the interpretations that are being made will not make much difference. When the reality of a situation is defined, the situation becomes a meaningful reality. This includes methodological criticisms, and critical sociological issues. A number of symbolic interactionists have addressed these topics, the best known being Stryker's structural symbolic interactionism and the formulations of interactionism heavily influenced by this approach (sometimes referred to as the \"Indiana School\" of symbolic interactionism), including the works of key scholars in sociology and psychology using different methods and theories applying a structural version of interactionism that are represented in a 2003 collection edited by Burke \"et al\". Another well-known structural variation of symbolic interactionism that applies quantitative methods is Manford H. Kuhn's formulation which is often referred to in sociological literature as the \"Iowa School\". \"Negotiated order theory\" also applies a structural approach.\n\nLanguage is viewed as the source of all meaning. Blumer illuminates several key features about social interactionism. Most people interpret things based on assignment and purpose. The interaction occurs once the meaning of something has become identified. This concept of meaning is what starts to construct the framework of social reality. By aligning social reality, Blumer suggests that language is the meaning of interaction. Communication, especially in the form of symbolic interactionism is connected with language. Language initiates all forms of communication, verbal and non-verbal. Blumer defines this source of meaning as a connection that arises out of the social interaction that people have with each other.\n\nAccording to social theorist Patricia Burbank, the concepts of synergistic and diverging properties are what shape the viewpoints of humans as social beings. These two concepts are different in a sense because of their views of human freedom and their level of focus. According to Burbank, actions are based on the effects of situations that occur during the process of social interaction. Another important factor in meaningful situations is the environment in which the social interaction occurs. The environment influences interaction, which leads to a reference group and connects with perspective, and then concludes to a definition of the situation. This illustrates the proper steps to define a situation. An approval of the action occurs once the situation is defined. An interpretation is then made upon that action, which may ultimately influence the perspective, action, and definition.\n\nStryker emphasizes that the sociology world at large is the most viable and vibrant intellectual framework. By being made up of our thoughts and self-belief, the social interactionism theory is the purpose of all human interaction, and is what causes society to exist. This fuels criticisms of the symbolic interactionist framework for failing to account for social structure, as well as criticisms that interactionist theories cannot be assessed via quantitative methods, and cannot be falsifiable or tested empirically. Framework is important for the symbolic interaction theory because for in order for the social structure to form, there are certain bonds of communication that need to be established to create the interaction. Much of the symbolic interactionist framework's basic tenets can be found in a very wide range of sociological and psychological work, without being explicitly cited as interactionist, making the influence of symbolic interactionism difficult to recognize given this general acceptance of its assumptions as \"common knowledge\".\n\nThe Society for the Study of Symbolic Interaction (SSSI) is an international professional organization for scholars, who are interested in the study of symbolic interaction. SSSI holds a conference in conjunction with the meeting of the American Sociological Association and the Society for the Study of Social Problems. This conference typically occurs in August and sponsors the Society for the Study of Symbolic Interaction holds the Couch-Stone Symposium each spring. The society provides travel scholarships for student members interested in attending the annual conference. At the annual conference, the Society for the Study of Symbolic Interaction sponsors yearly awards in different categories of symbolic interaction. Additionally, some of the awards are open to student members of the society. The Ellis-Bochner Autoethnography and Personal Narrative Research Award is given annually by the Society for the Study of Symbolic Interaction affiliate of the National Communication Association for the best article, essay, or book chapter in autoethnography and personal narrative research. The award is named after renowned autoethnographers Carolyn Ellis and Art Bochner. The society also sponsors a quarterly journal, \"Symbolic Interaction\". The organization also releases a newsletter, \"SSSI Notes\".\n\nSociety for the Study of Symbolic Interaction has also the European branch. It organizes each year the conference that integrates European symbolic interactionists.\n\n39.Carter, M. J., & Fuller, C. (2015). Symbolic interactionism. Sociopedia. doi:10.1177/205684601561\n40.Handberg, Charlotte, et al. “Revisiting Symbolic Interactionism as a Theoretical Framework Beyond the Grounded Theory Tradition.” Qualitative Health Research, vol. 25, no. 8, Aug. 2015, pp. 1023–1032, doi:10.1177/1049732314554231.\n\n\n\n"}
{"id": "58772", "url": "https://en.wikipedia.org/wiki?curid=58772", "title": "Timeline of atomic and subatomic physics", "text": "Timeline of atomic and subatomic physics\n\nA timeline of atomic and subatomic physics.\n\n\n\n\n\n\n\n"}
{"id": "50432307", "url": "https://en.wikipedia.org/wiki?curid=50432307", "title": "Treatise on Radioactivity", "text": "Treatise on Radioactivity\n\nTreatise on Radioactivity () is a two-volume book from the year 1910 written by the Polish scientist Marie Curie as a survey on the subject of radioactivity. She was awarded her second Nobel Prize in the following year after the publication of the book. The book, which was dedicated to her newly deceased collaborator and husband Pierre Curie, has been described as \"a classic synthesis of current research on radioactivity by scientists of the early 20th century.\" It was published by the Paris publisher Gauthier-Villars.\n\n"}
{"id": "32821", "url": "https://en.wikipedia.org/wiki?curid=32821", "title": "V-1 flying bomb", "text": "V-1 flying bomb\n\nThe V-1 flying bomb ( \"Vengeance Weapon 1\")—also known to the Allies as the buzz bomb, or doodlebug, and in Germany as Kirschkern (cherrystone) or Maikäfer (maybug)—was an early cruise missile and the only production aircraft to use a pulsejet for power.\n\nThe V-1 was the first of the so-called \"Vengeance weapons\" (V-weapons or \"Vergeltungswaffen\") series designed for terror bombing of London. It was developed at Peenemünde Army Research Center in 1939 by the Nazi German \"Luftwaffe\" during the Second World War. During initial development it was known by the codename \"Cherry Stone\". Because of its limited range, the thousands of V-1 missiles launched into England were fired from launch facilities along the French (Pas-de-Calais) and Dutch coasts. The first V-1 was launched at London on 13 June 1944, one week after (and prompted by) the successful Allied landings in Europe. At peak, more than one hundred V-1s a day were fired at south-east England, 9,521 in total, decreasing in number as sites were overrun until October 1944, when the last V-1 site in range of Britain was overrun by Allied forces. After this, the V-1s were directed at the port of Antwerp and other targets in Belgium, with 2,448 V-1s being launched. The attacks stopped only a month before the war in Europe ended, when the last launch site in the Low Countries was overrun on 29 March 1945.\n\nThe British operated an arrangement of air defences, including anti-aircraft guns and fighter aircraft, to intercept the bombs before they reached their targets as part of Operation Crossbow, while the launch sites and underground V-1 storage depots were targets of strategic bombing.\n\nIn late 1936, while employed by the \"Argus Motoren\" company, Fritz Gosslau began work on the further development of remote-controlled aircraft; Argus had already developed a remote-controlled surveillance aircraft, the AS 292 (military designation FZG 43).\n\nOn 9 November 1939, a proposal for a remote-controlled aircraft carrying a payload of over a distance of was forwarded to the RLM (German Air Ministry). Argus worked in cooperation with Lorentz AG and Arado Flugzeugwerke to develop the project as a private venture, and in April 1940, Gosslau presented an improved study of Project \"\"Fernfeuer\" to the RLM, as Project P 35 \"Erfurt\"\".\n\nOn 31 May, Rudolf Bree of the RLM commented that he saw no chance that the projectile could be deployed in combat conditions, as the proposed remote-control system was seen as a design weakness. Heinrich Koppenberg, the director of Argus, met with Ernst Udet on 6 January 1941 to try to convince him that the development should be continued, but Udet decided to cancel it.\n\nDespite this, Gosslau was convinced that the basic idea was sound and proceeded to simplify the design. As an aircraft engine manufacturer, Argus lacked the capability to produce a fuselage for the project and Koppenberg sought the assistance of Robert Lusser, chief designer and technical director at Heinkel. On 22 January 1942, Lusser took up a position with the Fieseler aircraft company. He met Koppenberg on 27 February and was informed of Gosslau's project. Gosslau's design used two pulsejet engines; Lusser improved the design to use a single engine.\n\nA final proposal for the project was submitted to the Technical Office of the RLM on 5 June and the project was renamed Fi 103, as Fieseler was to be the chief contractor. On 19 June, \"Generalfeldmarschall\" Erhard Milch gave Fi 103 production high priority, and development was undertaken at the Luftwaffe's \"Erprobungsstelle\" coastal test centre at Karlshagen, part of the Peenemünde-West facility.\n\nBy 30 August, Fieseler had completed the first fuselage, and the first flight of the Fi 103 V7 took place on 10 December 1942, when it was airdropped by a Fw 200.\n\nThe V-1 was named by \"The Reich\" journalist Hans Schwarz Van Berkl in June 1944 with Hitler's approval.\n\nThe V-1 was designed under the codename \"Kirschkern\" (cherry stone) by Lusser and Gosslau, with a fuselage constructed mainly of welded sheet steel and wings built of plywood. The simple, Argus-built pulsejet engine pulsed 50 times per second, and the characteristic buzzing sound gave rise to the colloquial names \"buzz bomb\" or \"doodlebug\" (a common name for a wide variety of flying insects). It was known briefly in Germany (on Hitler's orders) as \"Maikäfer\" (May bug) and \"Krähe\" (crow).\n\nIgnition of the Argus pulsejet was accomplished using an automotive type spark plug located about behind the intake shutters, with current supplied from a portable starting unit. Three air nozzles in the front of the pulsejet were at the same time connected to an external high-pressure air source that was used to start the engine. Acetylene gas was typically used for starting the engine, and very often a panel of wood or similar material was held across the end of the tailpipe to prevent the fuel from diffusing and escaping before ignition. The V-1 was fuelled by of 75 octane gasoline.\n\nOnce the engine had been started and the temperature had risen to the minimum operating level, the external air hose and connectors were removed and the engine's resonant design kept it firing without any further need for the electrical ignition system, which was used only to ignite the engine when starting.\nThe Argus As 014 (also known as a resonant jet) could operate at zero airspeed because of the nature of its intake shutters and its acoustically tuned resonant combustion chamber. However, because of the low static thrust of the pulse jet engine and the very high stall speed of the small wings, the V-1 could not take off under its own power in a practically short distance, and thus needed to be ground-launched by aircraft catapult or air-launched from a modified bomber aircraft such as a Heinkel He 111.\n\nBeginning in January 1941, the V-1's pulsejet engine was also tested on a variety of craft, including automobiles and an experimental attack boat known as the \"Tornado\". The unsuccessful prototype was a version of a \"Sprengboot\", in which a boat loaded with explosives was steered towards a target ship and the pilot would leap out of the back at the last moment. The Tornado was assembled from surplus seaplane hulls connected in catamaran fashion with a small pilot cabin on the crossbeams. The Tornado prototype was a noisy underperformer and was abandoned in favour of more conventional piston engined craft.\n\nThe engine made its first flight aboard a Gotha Go 145 on 30 April 1941.\n\nThe V-1 guidance system used a simple autopilot developed by in Berlin to regulate altitude and airspeed. The RLM at first planned to use a radio control system with the V-1 for precision attacks, but the government decided instead to use the missile against London. A weighted pendulum system provided fore-and-aft attitude measurement to control pitch, damped by a gyrocompass which also stabilized it. Operating power for the gyroscope platform and the flight-control actuators was provided by two large spherical compressed air tanks that also pressurized the fuel tank. These air tanks were charged to before launch. With the counter determining how far the missile would fly, it was only necessary to launch the V-1 with the ramp pointing in the approximate direction, and the autopilot controlled the flight.\n\nThere was a more sophisticated interaction between yaw, roll and other sensors: a gyrocompass (set by swinging in a hangar before launch) gave feedback to control the dynamics of pitch and roll, but it was angled away from the horizontal so that controlling these degrees of freedom interacted: the gyroscope remained true on the basis of feedback received from a magnetic compass, and from the fore and aft pendulum. This interaction meant that rudder control was sufficient for steering and no banking mechanism was needed. In a V-1 that landed without detonating between Tilburg and Goirle in March 1945, several rolled up issues of the German wartime propaganda magazine \"Signal\" were found inserted into the left wing's tubular steel spar, used for weight to preset the missile's static equilibrium before launching. Several of the earliest V-1s to be launched were provided with a small radio transmitter (using a triode valve marked 'S3' but equivalent to a then-current power valve, type RL 2,4T1) to check the general direction of flight related to the launching place's and the target's grid coordinates by radio bearing (navigation).\n\nAn odometer driven by a vane anemometer on the nose determined when the target area had been reached, accurately enough for area bombing. Before launch, the counter was set to a value that would reach zero upon arrival at the target in the prevailing wind conditions. As the missile flew, the airflow turned the propeller, and every 30 rotations of the propeller counted down one number on the counter. This counter triggered the arming of the warhead after about . When the count reached zero, two detonating bolts were fired. Two spoilers on the elevator were released, the linkage between the elevator and servo was jammed and a guillotine device cut off the control hoses to the rudder servo, setting the rudder in neutral. These actions put the V-1 into a steep dive. While this was originally intended to be a power dive, in practice the dive caused the fuel flow to cease, which stopped the engine. The sudden silence after the buzzing alerted listeners of the impending impact. The fuel problem was quickly fixed, and when the last V-1s fell, the majority hit with power.\n\nInitially, V-1s landed within a circle in diameter, but by the end of the war, accuracy had been improved to about 7 miles, which was comparable to the V-2 rocket.\n\nThe warhead was 1,000 kg of Amatol-39, later the aluminised explosive Trialen was used, as used for filling other Luftwaffe 1,000 kg bombs. Trialen fillings were identified by the warhead being painted red, although the assembled missiles were painted green or grey over this.\n\nFuzing was by a triple fuze system. The main fuzes were an electrical impact fuze and a mechanical backup impact fuze. These were immediate action fuzes, the intention being to detonate the warhead on the first impact with the surface, rather than allowing itself to become buried first. This was a major difference from the V-2, and a reason for the high lethality of the V-1. Although they did not demolish buildings or deep structures as effectively as the air-dropped bombs, or the deep-burying V-2, their blast effects were almost all released at the surface and caused many casualties. The electrical fuze, ZLPM 76, was mounted at the front, immediately behind the compass and the air speed propeller. It connected to a central exploder tube through the warhead, containing the gaine and boosters. Two transverse fuze pockets, in typical German fashion, were placed in the upper surface of the warhead for the secondary fuzes, also connecting to this same tube.\n\nTo avoid the risk of this secret weapon being examined by the British, there was a third time delay fuze. This was too short to be any sort of booby trap, just to destroy the weapon if a soft landing had not triggered the impact fuzes. These fuzing systems were very reliable and there were almost no dud V-1s recovered.\n\nGround-launched V-1s were typically propelled up an inclined launch ramp by an apparatus known as a \"Dampferzeuger\" (\"steam generator\"), which reacted stabilized hydrogen peroxide and potassium permanganate (\"T-Stoff\" and \"Z-Stoff\"), the same combination of chemicals used as propellants for the Messerschmitt Me 163 Komet rocket plane, and the Walter HWK 109-500 \"Starthilfe\" RATO rocket booster unit. Ramp-launch velocity for an operational V-1 was as it left the end of the launch ramp.\n\nThe original design for launch sites included a number of hangars or storage garages as well as preparation and command buildings, as well as the launch ramp, all of which were easily identifiable from aerial photographs resulting in bombing attacks on the sites. Launching needed a steam generator.\n\n100 litres of Hydrogen Peroxide and Potassium Permanganate was later used in place of steam, whereby the V-1 was thrown into the air using a system similar to that used on an aircraft carrier to launch planes.\n\nA light design utilising a small (7.5m) preparation building, a small firing control room and the 36m launch ramp which was supplied in kit form, with legs resting in concrete recesses.\n\nThe first complete V-1 airframe was delivered on 30 August 1942, and after the first complete As.109-014 was delivered in September, the first glide test flight was on 28 October 1942 at Peenemünde, from under a Focke-Wulf Fw 200. The first powered trial was on 10 December, launched from beneath an He 111.\n\nThe LXV Armeekorps z.b.V. formed during the last days of November 1943 in France commanded by General der Artillerie z.V. Erich Heinemann was responsible for the operational use of V-1.\n\nThe conventional launch sites could theoretically launch about 15 V-1s per day, but this rate was difficult to achieve on a consistent basis; the maximum rate achieved was 18. Overall, only about 25 per cent of the V-1s hit their targets, the majority being lost because of a combination of defensive measures, mechanical unreliability or guidance errors. With the capture or destruction of the launch facilities used to attack England, the V-1s were employed in attacks against strategic points in Belgium, primarily the port of Antwerp.\n\nLaunches against Britain were met by a variety of countermeasures, including barrage balloons and aircraft including the Hawker Tempest and Gloster Meteor. These measures were so successful that by August 1944 about 80 per cent of V-1s were being destroyed (the Meteors, although fast enough to catch the V-1s, suffered frequent cannon failures, and accounted for only 13). In all, about 1,000 V-1s were destroyed by aircraft.\n\nThe intended operational altitude was originally set at . However, repeated failures of a barometric fuel-pressure regulator led to it being changed in May 1944, halving the operational height, thereby bringing V-1s into range of the Bofors guns commonly used by Allied AA units.\nThe trial versions of the V-1 were air-launched. Most operational V-1s were launched from static sites on land, but from July 1944 to January 1945, the \"Luftwaffe\" launched approximately 1,176 from modified Heinkel He 111 H-22s of the \"Luftwaffe\"s Kampfgeschwader 3 (3rd Bomber Wing, the so-called \"Blitz Wing\") flying over the North Sea. Apart from the obvious motive of permitting the bombardment campaign to continue after static ground sites on the French coast were lost, air-launching gave the \"Luftwaffe\" the opportunity to outflank the increasingly effective ground and air defences put up by the British against the missile. To minimise the associated risks (primarily radar detection), the aircrews developed a tactic called \"lo-hi-lo\": the He 111s would, upon leaving their airbases and crossing the coast, descend to an exceptionally low altitude. When the launch point was neared, the bombers would swiftly ascend, fire their V-1s, and then rapidly descend again to the previous \"wave-top\" level for the return flight. Research after the war estimated a 40 per cent failure rate of air-launched V-1s, and the He 111s used in this role were vulnerable to night-fighter attack, as the launch lit up the area around the aircraft for several seconds. The combat potential of air-launched V-1s dwindled as 1944 progressed at about the same rate as that of the ground-launched missiles, as the British gradually took the measure of the weapon and developed increasingly effective defence tactics.\n\nLate in the war, several air-launched piloted V-1s, known as \"Reichenbergs\", were built, but these were never used in combat. Hanna Reitsch made some flights in the modified V-1 Fieseler \"Reichenberg\" when she was asked to find out why test pilots were unable to land it and had died as a result. She discovered, after simulated landing attempts at high altitude where there was air space to recover, that the craft had an extremely high stall speed and the previous pilots with little high-speed experience had attempted their approaches much too slowly. Her recommendation of much higher landing speeds was then introduced in training new \"Reichenberg\" volunteer pilots. The \"Reichenberg\"s were air-launched rather than fired from a catapult ramp as erroneously portrayed in the film \"Operation Crossbow\".\n\nThere were plans, not put into practice, to use the Arado Ar 234 jet bomber to launch V-1s either by towing them aloft or by launching them from a \"piggy back\" position (in the manner of the \"Mistel\", but in reverse) atop the aircraft. In the latter configuration, a pilot-controlled, hydraulically operated dorsal trapeze mechanism would elevate the missile on the trapeze's launch cradle some eight feet clear of the 234's upper fuselage. This was necessary to avoid damaging the mother craft's fuselage and tail surfaces when the pulsejet ignited, as well as to ensure a \"clean\" airflow for the Argus motor's intake. A somewhat less ambitious project undertaken was the adaptation of the missile as a \"flying fuel tank\" \"(Deichselschlepp)\" for the Messerschmitt Me 262 jet fighter, which was initially test-towed behind an He 177A \"Greif\" bomber. The pulsejet, internal systems and warhead of the missile were removed, leaving only the wings and basic fuselage, now containing a single large fuel tank. A small cylindrical module, similar in shape to a finless dart, was placed atop the vertical stabilizer at the rear of the tank, acting as a centre of gravity balance and attachment point for a variety of equipment sets. A rigid tow-bar with a pitch pivot at the forward end connected the flying tank to the Me 262. The operational procedure for this unusual configuration saw the tank resting on a wheeled trolley for take-off. The trolley was dropped once the combination was airborne, and explosive bolts separated the towbar from the fighter upon exhaustion of the tank's fuel supply. A number of test flights were conducted in 1944 with this set-up, but inflight \"porpoising\" of the tank, with the instability transferred to the fighter, meant the system was too unreliable to be used. An identical utilisation of the V-1 flying tank for the Ar 234 bomber was also investigated, with the same conclusions reached. Some of the \"flying fuel tanks\" used in trials utilised a cumbersome fixed and spatted undercarriage arrangement, which (along with being pointless) merely increased the drag and stability problems already inherent in the design.\n\nOne variant of the basic Fi 103 design did see operational use. The progressive loss of French launch sites as 1944 proceeded and the area of territory under German control shrank meant that soon the V-1 would lack the range to hit targets in England. Air-launching was one alternative utilised, but the most obvious solution was to extend the missile's range. Thus the F-1 version developed. The weapon's fuel tank was increased in size, with a corresponding reduction in the capacity of the warhead. Additionally, the nose-cones and wings of the F-1 models were made of wood, affording a considerable weight saving. With these modifications, the V-1 could be fired at London and nearby urban centres from prospective ground sites in the Netherlands. Frantic efforts were made to construct a sufficient number of F-1s in order to allow a large-scale bombardment campaign to coincide with the Ardennes Offensive, but numerous factors (bombing of the factories producing the missiles, shortages of steel and rail transport, the chaotic tactical situation Germany was facing at this point in the war, etc.) delayed the delivery of these long-range V-1s until February/March 1945. Beginning on 2 March 1945, slightly more than three weeks before the V-1 campaign finally ended, several hundred F-1s were launched at Britain from Dutch sites under Operation \"Zeppelin\". Frustrated by increasing Allied dominance in the air, Germany also employed V1s to attack the RAF's forward airfields, such as Volkel, in the Netherlands.\n\nThere was also a turbojet-propelled upgraded variant proposed, meant to use the Porsche 109-005 low-cost turbojet engine of some 500 kgf (1,100 lbf) thrust.\n\nAlmost 30,000 V-1s were made; by March 1944, they were each produced in 350 hours (including 120 for the autopilot), at a cost of just 4 per cent of a V-2, which delivered a comparable payload. Approximately 10,000 were fired at England; 2,419 reached London, killing about 6,184 people and injuring 17,981. The greatest density of hits were received by Croydon, on the south-east fringe of London. Antwerp, Belgium was hit by 2,448 V-1s from October 1944 to March 1945.\n\nThe codename \"\"Flakzielgerät\" 76\"—\"Flak target apparatus\" helped to hide the nature of the device, and some time passed before references to FZG 76 were linked to the V-83 pilotless aircraft (an experimental V-1) that had crashed on Bornholm in the Baltic and to reports from agents of a flying bomb capable of being used against London. Importantly, the Polish Home Army intelligence contributed information on V-1 construction and a place of development (Peenemünde). Initially, British experts were sceptical of the V-1 because they had considered only solid fuel rockets, which could not attain the stated range of : . However, they later considered other types of engine, and by the time German scientists had achieved the needed accuracy to deploy the V-1 as a weapon, British intelligence had a very accurate assessment of it.\n\nThe British defence against the German long-range weapons was Operation Crossbow. Anti-aircraft guns of the Royal Artillery and RAF Regiment redeployed in several movements: first in mid-June 1944 from positions on the North Downs to the south coast of England, then a cordon closing the Thames Estuary to attacks from the east. In September 1944, a new linear defence line was formed on the coast of East Anglia, and finally in December there was a further layout along the Lincolnshire–Yorkshire coast. The deployments were prompted by changes to the approach tracks of the V-1 as launch sites were overrun by the Allies' advance.\n\nOn the first night of sustained bombardment, the anti-aircraft crews around Croydon were jubilant – suddenly they were downing unprecedented numbers of German bombers; most of their targets burst into flames and fell when their engines cut out. There was great disappointment when the truth was announced. Anti-aircraft gunners soon found that such small fast-moving targets were, in fact, very difficult to hit. The cruising altitude of the V-1, between , was just above the effective range of light anti-aircraft guns, and just below the optimum engagement height of heavier guns.\n\nThe altitude and speed were more than the rate of traverse of the standard British QF 3.7-inch mobile gun could cope with. The static version of the QF 3.7-inch, designed for use on a permanent, concrete platform, had a faster traverse. The cost and delay of installing new permanent platforms for the guns was fortunately found to be unnecessary - a temporary platform built devised by the REME and made from railway sleepers and rails was found to be adequate for the static guns, making them considerably easier to re-deploy as the V-1 threat changed.\n\nThe development of the proximity fuze and of centimetric, 3 gigahertz frequency gun-laying radars based on the cavity magnetron helped to counter the V-1's high speed and small size. In 1944, Bell Labs started delivery of an anti-aircraft predictor fire-control system based on an analogue computer, just in time for the Allied invasion of Europe.\n\nThese electronic aids arrived in quantity from June 1944, just as the guns reached their firing positions on the coast. Seventeen per cent of all flying bombs entering the coastal \"gun belt\" were destroyed by guns in their first week on the coast. This rose to 60 per cent by 23 August and 74 per cent in the last week of the month, when on one day 82 per cent were shot down. The rate improved from one V-1 destroyed for every 2,500 shells fired initially, to one for every 100. This still did not end the threat, and V-1 attacks continued until all launch sites were captured by ground forces.\n\nEventually some 2,000 barrage balloons were deployed, in the hope that V-1s would be destroyed when they struck the balloons' tethering cables. The leading edges of the V-1's wings were fitted with cable cutters, and fewer than 300 V-1s are known to have been brought down by barrage balloons.\n\nThe Defence Committee expressed some doubt as to the ability of the Royal Observer Corps to adequately deal with the new threat, but the ROC's Commandant Air Commodore Finlay Crerar assured the committee that the ROC could again rise to the occasion and prove its alertness and flexibility. He oversaw plans for handling the new threat, codenamed by the RAF and ROC as \"Operation Totter\".\n\nObservers at the coast post of Dymchurch identified the very first of these weapons and within seconds of their report the anti-aircraft defences were in action. This new weapon gave the ROC much additional work both at posts and operations rooms. Eventually RAF controllers actually took their radio equipment to the two closest ROC operations rooms at Horsham and Maidstone, and vectored fighters direct from the ROC's plotting tables. The critics who had said that the Corps would be unable to handle the fast-flying jet aircraft were answered when these aircraft on their first operation were actually controlled entirely by using ROC information both on the coast and at inland.\n\nThe average speed of V-1s was and their average altitude was to . Fighter aircraft required excellent low altitude performance to intercept them and enough firepower to ensure that they were destroyed in the air rather than crashing to earth and detonating. Most aircraft were too slow to catch a V-1 unless they had a height advantage, allowing them to gain speed by diving on their target.\n\nWhen V-1 attacks began in mid-June 1944, the only aircraft with the low-altitude speed to be effective against it was the Hawker Tempest. Fewer than 30 Tempests were available. They were assigned to No. 150 Wing RAF. Early attempts to intercept and destroy V-1s often failed, but improved techniques soon emerged. These included using the airflow over an interceptor's wing to raise one wing of the V-1, by sliding the wingtip to within of the lower surface of the V-1's wing. If properly executed, this manoeuvre would tip the V-1's wing up, overriding the gyro and sending the V-1 into an out-of-control dive. At least sixteen V-1s were destroyed this way (the first by a P-51 piloted by Major R. E. Turner of 356th Fighter Squadron on 18 June). It could be seen that the aerodynamic flip method was actually effective when V-1s could be seen over southern parts of the Netherlands headed due eastwards at low altitude, the engine quenched. In early 1945 such a missile soared below clouds over Tilburg to gently alight eastwards of the city in open fields.\n\nThe Tempest fleet was built up to over 100 aircraft by September. Specially modified P-47M Thunderbolts (half their fuel tanks, half their 0.5in {12.7 mm} machine guns, boosted engines (2800 hp), all external fittings, and all their armour plate removed) were also pressed into service against the V-1s. In addition, North American P-51 Mustangs and Griffon-engined Supermarine Spitfire Mk XIVs were tuned to make them fast enough, and during the short summer nights the Tempests shared defensive duty with de Havilland Mosquitos. There was no need for airborne radar; at night the V-1's engine could be heard from away or more, and the exhaust plume was visible from a long distance. Wing Commander Roland Beamont had the 20 mm cannon on his Tempest adjusted to converge at ahead. This was so successful that all other aircraft in 150 Wing were thus modified.\n\nThe anti-V-1 sorties by fighters were known as \"Diver patrols\" (after \"Diver\", the codename used by the Royal Observer Corps for V-1 sightings). Attacking a V-1 was dangerous: machine guns had little effect on the V-1's sheet steel structure, and if a cannon shell detonated the warhead, the explosion could destroy the attacker.\nIn daylight, V-1 chases were chaotic and often unsuccessful until a special defence zone was declared between London and the coast, in which only the fastest fighters were permitted. The first interception of a V-1 was by F/L J. G. Musgrave with a No. 605 Squadron RAF Mosquito night fighter on the night of 14/15 June 1944. As daylight grew stronger after the night attack, a Spitfire was seen to follow closely behind a V-1 over Chislehurst and Lewisham. Between June and 5 September 1944, a handful of 150 Wing Tempests shot down 638 flying bombs, with No. 3 Squadron RAF alone claiming 305. One Tempest pilot, Squadron Leader Joseph Berry (RAF officer) (501 Squadron), shot down 59 V-1s, the Belgian ace Squadron Leader Remy Van Lierde (164 Squadron) destroyed 44 (with a further nine shared) and W/C Roland Beamont (see above) destroyed 31.\n\nThe next most successful interceptors were the Mosquito (623 victories), Spitfire XIV (303), and Mustang (232). All other types combined added 158. Even though it was not fully operational, the jet-powered Gloster Meteor was rushed into service with No. 616 Squadron RAF to fight the V-1s. It had ample speed but its cannons were prone to jamming, and it shot down only 13 V-1s.\n\nIn late 1944 a radar-equipped Vickers Wellington bomber was modified for use by the RAF's Fighter Interception Unit as an Airborne Early Warning and Control aircraft. Flying at an altitude of over the North Sea, it directed Mosquito fighters charged with intercepting He 111s from Dutch airbases that sought to launch V-1s from the air.\n\nThe first bomb disposal officer to defuse an unexploded V-1 was John Pilkington Hudson in 1944.\n\nTo adjust and correct settings in the V-1 guidance system, the Germans needed to know where the V-1s were impacting. Therefore, German intelligence was requested to obtain this impact data from their agents in Britain. However, all German agents in Britain had been turned, and were acting as double agents under British control.\nOn 16 June 1944, British double agent \"Garbo\" (Juan Pujol) was requested by his German controllers to give information on the sites and times of V-1 impacts, with similar requests made to the other German agents in Britain, \"Brutus\" (Roman Czerniawski) and \"Tate\" (Wulf Schmidt). If given this data, the Germans would be able to adjust their aim and correct any shortfall. However, there was no plausible reason why the double agents could not supply accurate data; the impacts would be common knowledge amongst Londoners and very likely reported in the press, which the Germans had ready access to through the neutral nations. In addition, as John Cecil Masterman, chairman of the Twenty Committee, commented, \"If, for example, St Paul's Cathedral were hit, it was useless and harmful to report that the bomb had descended upon a cinema in Islington, since the truth would inevitably get through to Germany ...\"\n\nWhile the British decided how to react, Pujol played for time. On 18 June it was decided that the double agents would report the damage caused by V-1s fairly accurately and minimise the effect they had on civilian morale. It was also decided that Pujol should avoid giving the times of impacts, and should mostly report on those which occurred in the north west of London, to give the impression to the Germans that they were overshooting the target area.\n\nWhile Pujol downplayed the extent of V-1 damage, trouble came from \"Ostro\", an \"Abwehr\" agent in Lisbon who pretended to have agents reporting from London. He told the Germans that London had been devastated and had been mostly evacuated as a result of enormous casualties. The Germans could not perform aerial reconnaissance of London, and believed his damage reports in preference to Pujol's. They thought that the Allies would make every effort to destroy the V-1 launch sites in France. They also accepted \"Ostro\"s impact reports. Due to Ultra, however, the Allies read his messages and adjusted for them.\nA certain number of the V-1s fired had been fitted with radio transmitters, which had clearly demonstrated a tendency for the V-1 to fall short. \"Oberst\" Max Wachtel, commander of Flak Regiment 155 (W), which was responsible for the V-1 offensive, compared the data gathered by the transmitters with the reports obtained through the double agents. He concluded, when faced with the discrepancy between the two sets of data, that there must be a fault with the radio transmitters, as he had been assured that the agents were completely reliable. It was later calculated that if Wachtel had disregarded the agents' reports and relied on the radio data, he would have made the correct adjustments to the V-1's guidance, and casualties might have increased by 50 per cent or more.\n\nThe policy of diverting V-1 impacts away from central London was initially controversial. The War Cabinet refused to authorise a measure that would increase casualties in any area, even if it reduced casualties elsewhere by greater amounts. It was thought that Churchill would reverse this decision later (he was then away at a conference); but the delay in starting the reports to Germans might be fatal to the deception. So Sir Findlater Stewart of Home Defence Executive took responsibility for starting the deception programme immediately, and his action was approved by Churchill when he returned.\n\nBy September 1944, the V-1 threat to England was temporarily halted when the launch sites on the French coast were overrun by the advancing Allied armies. 4,261 V-1s had been destroyed by fighters, anti-aircraft fire and barrage balloons. The last enemy action of any kind on British soil occurred on 29 March 1945, when a V-1 struck Datchworth in Hertfordshire.\n\nUnlike the V-2, the V-1 was a cost-effective weapon for the Germans as it forced the Allies to spend heavily on defensive measures and divert bombers from other targets. More than 25 per cent of Combined Bomber Offensive's bombs in July and August 1944 were used against V-weapon sites, often ineffectively. In early December 1944, American General Clayton Bissell wrote a paper that argued strongly in favour of the V-1 when compared with conventional bombers.\n\nThe following is a table he produced:\n\nThe statistics of this report, however, have been the subject of some dispute. The V-1 missiles launched from bombers were often prone to exploding prematurely, occasionally resulting in the loss of the aircraft to which they were attached. The Luftwaffe lost 77 aircraft out of 1,200 of these types of sorties.\n\nWright Field technical personnel reverse-engineered the V-1 from the remains of one that had failed to detonate in Britain. The result was the creation of the JB-2 Loon.\nGeneral Hap Arnold of the United States Army Air Forces was concerned that this weapon could be built of steel and wood, in 2000 man hours and approximate cost of US$600 (in 1943). To put this figure in perspective, a Boeing B-29 Superfortress cost ~1000x more, and still ~100x more when taking into account its 10x higher payload (20,000 lb Vs 850 kg for V1) -- payload, which cost has to be added (while it is included in V1 cost) --, with the additional drawback of requiring (and putting in danger) 11 flying crew members.\n\nThe attacks on Antwerp and Brussels began in October 1944, with the last V-1 launched against Antwerp on 30 March 1945.\n\nAntwerp was recognised by both the German and Allied high command as a very important port, essential to the further progression of Allied armies into Germany. The shorter range improved the accuracy of the V-1 which was six miles deviation per hundred miles of flight, the flight level was also reduced to around 3,000 ft.\n\nBoth British (80 AA Brigade) and US Army anti-aircraft batteries (30th AAA Group) were sent to Antwerp together with a searchlight regiment. The zone of command under the 21st Army Group was called \"Antwerp-X\" and given the object of protecting an area with a radius of 7,000 yards covering the city and dock area. Initially attacks came from the south-east, accordingly a screen of observers and searchlights was deployed along the attack azimuth, behind which were three rows of batteries with additional searchlights.\n\nUS units deployed SCR-584 radar units controlling four 90mm guns per battery using an M9 director to electrically control the battery guns. Backup for the American guns was automatic 40mm batteries, which were not effective against V-1s.\n\nBritish gun batteries were each equipped with eight QF 3.7-inch AA gun and two radar units, preferably the US SCR-584 with M9 director as it was more accurate than the British system. Backup for the British guns was also automatic 40mm batteries.\n\nThe radar was effective from 28,000 yards, the M9 director predicted the target location position based on course, height and speed which combined with the gun, shell and fuse characteristics predicted an impact position, adjusted each gun and fired the shell.\n\nIn November attacks began from the north-east and additional batteries were deployed along the new azimuths, including the 184th AAA Battalion (United States) brought from Paris. Additional radar units and observers were deployed up to 40 miles from Antwerp to give early warning of V-1 bombs approaching. The introduction of the VT fuse in January 1945 improved the effectiveness of the guns and reduced ammunition consumption.\n\nFrom October 1944 to March 1945 4,883 V-1's were detected. Of these, only 4.5% fell into the designated protected area. The effectiveness of the anti aircraft defence meant that only 211 got through the defences, however those that fell within the area caused damage and loss of life.\n\nIn 1943, an Argus pulsejet engine was shipped to Japan by German submarine. The Aeronautical Institute of Tokyo Imperial University and the Kawanishi Aircraft Company conducted a joint study of the feasibility of mounting a similar engine on a piloted plane. The resulting design was named \"Baika\" (\"plum blossom\") but bore no more than a superficial resemblance to the Fi 103. \"Baika\" never left the design stage but technical drawings and notes suggest that several versions were considered: an air-launched version with the engine under the fuselage, a ground-launched version that could take off without a ramp and a submarine launched version with the engine moved forwards.\n\nAfter the war, the armed forces of France, the Soviet Union and the United States experimented with the V-1.\n\nAfter reverse-engineering captured V-1s in 1946, the French began producing copies for use as target drones, starting in 1951. These were called the ARSAERO CT 10 and were smaller than the V-1. The CT 10 could be ground-launched using solid rocket boosters or air-launched from a LeO 45 bomber. More than 400 were produced, some of which were exported to the UK, Sweden, and Italy.\n\nThe Soviet Union captured V-1s when they overran the Blizna test range in Poland, as well as from the Mittelwerk. The 10Kh was their copy of the V-1, later called Izdeliye 10. Initial tests began in March 1945 at a test range in Tashkent, with further launches from ground sites and from aircraft of improved versions continuing into the late 1940s. The inaccuracy of the guidance system when compared with new methods such as beam-riding and TV guidance saw development end in the early 1950s.\n\nThe Soviets also worked on a piloted attack aircraft based on the Argus pulsejet engine of the V-1, which began as a German project, the Junkers EF 126 \"Lilli\", in the latter stages of the war. The Soviet development of the \"Lilli\" ended in 1946 after a crash that killed the test pilot.\n\nThe United States reverse-engineered the V-1 in 1944 from salvaged parts recovered in England during June. By 8 September, the first of thirteen complete prototype Republic-Ford JB-2 Loons, was assembled at Republic Aviation. The United States JB-2 was different from the German V-1 in only the smallest of dimensions. The wing span was only wider and the length was extended less than . The difference gave the JB-2 of wing area versus for the V-1.\n\nA navalized version, designated KGW-1, was developed to be launched from LSTs as well as escort carriers (CVEs) and long-range 4-engine reconnaissance aircraft. Waterproof carriers for the KGW-1 were developed for launches of the missile from surfaced submarines. Both the USAAF JB-2 and Navy KGW-1 were put into production and were planned to be used in the Allied invasion of Japan (Operation Downfall). However, the surrender of Japan obviated the need for its use. After the end of the war, the JB-2/KGW-1 played a significant role in the development of more advanced surface-to-surface tactical missile systems such as the MGM-1 Matador and later MGM-13 Mace.\n\n\n\n\n\n\nNotes\n\nCitations\n\nBibliography\n\n"}
{"id": "10285536", "url": "https://en.wikipedia.org/wiki?curid=10285536", "title": "Wild Law", "text": "Wild Law\n\nWild Law: A Manifesto for Earth Justice is a book by Cormac Cullinan that proposes recognizing natural communities and ecosystems as legal persons with legal rights. The book explains the concept of wild law, that is, human laws that are consistent with earth jurisprudence. Foreworded by Thomas Berry, the book was published by Green Books in November 2003 in association with The Gaia Foundation, London. It was first published in South Africa, the author's home country, in August 2002 by Siber Ink.\n\nThe feasibility of developing a new form of jurisprudence was discussed at a conference in Washington attended by Thomas Berry in April 2001, organised by the Gaia Foundation. A group of people involved with law and indigenous peoples attended from South Africa, Britain, Colombia, Canada and the United States.\n\nSince then \"Wild Law\" has been at the centre of many conferences and residential workshops: \n\nThe Tamaqua Borough Sewage Sludge Ordinance enacted in 2006 by the 7,000 inhabits of the community of Tamaqua, PA is based on the 2002 ideas set out in Wild Law and has been viewed potentially as one of the most important events of 2006. Tamaqua's ordinance not only denies the right of corporations to spread sewage sludge as fertilizer on farmland, even when the farmer is willing, the ordinance recognizes natural communities and ecosystems as legal persons with legal rights. This ordinance is among the first \" wild laws\" to be passed anywhere in the world.\n\n\n"}
{"id": "3115609", "url": "https://en.wikipedia.org/wiki?curid=3115609", "title": "WorldWIT", "text": "WorldWIT\n\nWorldWIT was a global online discussion community for women in business and technology.\n\nIt was founded in 1999 as ChicWIT, in Chicago. ChicWIT was followed by MassWIT in Boston, NycWIT in New York City, and CapitolWIT in Washington, D.C. In October 2005 there were over 80 WorldWIT chapters in operation and over 40,000 members. Women (and a few men) used the daily WorldWIT newsletter to share business, technical, career, health, financial and life advice with one another.\n\nWorldWIT was recognized in October 2004 as the Women's Business Organization of the Year, by the Stevie Awards Organization.\n\nIn March 2007, WorldWIT and its local city chapters were shut down. This was the announcement:\nWORLDWIT FOUNDER BEGINS NEW CHAPTER\nWorldWIT to close on March 23, 2007 – After seven years of connecting women in 25 countries around the globe, we are turning a corner and moving on to new endeavors. \nWorldWIT founder Liz Ryan’s demanding speaking and training schedule prevented her from taking the self-funded WorldWIT to the next level.\nWorldWIT used L-Soft list serv technology and a robust website as the basis of its social networking platform. Bill Phillips designed and managed both the listserv and website. L-Soft published an online article featuring WorldWIT in 2005. See http://www.lsoft.com/news/qa-issue2-2005-eu.asp\n\n"}
{"id": "11528384", "url": "https://en.wikipedia.org/wiki?curid=11528384", "title": "Wujing Zongyao", "text": "Wujing Zongyao\n\nThe Wujing Zongyao (), sometimes rendered in English as the Complete Essentials for the Military Classics, is a Chinese military compendium written from around 1040 to 1044.\n\nThe book was compiled during the Northern Song dynasty by Zeng Gongliang (曾公亮), Ding Du (丁度) and Yang Weide (楊惟德), whose writing influenced many later Chinese military writers. The compendium was published under the auspices of Emperor Renzong of Song, who also authored the book's preface. The book covers a wide range of subjects, including everything from naval warships to different types of catapults. It contains the earliest known written chemical formulas for gunpowder, made from saltpeter, sulphur and charcoal along with many added ingredients. In addition to formulas for gunpowder, the compendium also contains details on various other gunpowder weapons such as fire arrows, incendiary bombs and projectiles, and grenades and smoke bombs. It also describes an early form of the compass (using thermoremanence), and has the oldest illustration of a Chinese Greek fire flamethrower with a double-action dual-piston cylinder-pump capable of shooting a continuous blast of flame.\n\nThe \"Wujing Zongyao\" was compiled under the sponsorship of Emperor Renzong of Song (r. 1022–1063 AD), who was concerned that many officials were unfamiliar with the military classics, and partially as a response to the Song dynasty's war with the Tanguts of Western Xia. \n\nA team of scholars worked from 1040 to 1044 to compile the \"Wujing Zongyao\" with the intent to collect all known military knowledge and to disseminate it to a wider government audience. Its chief editor, Zeng Gongliang, was assisted by the astronomer Yang Weide and the scholar Ding Du. After five years, the book was published with a preface authored by Emperor Renzong himself. Lorge remarks that Zeng Gongliang, the chief editor, was a government official rather than a military general, implying that the \"Wujing Zongyao\" was likely written for other government officials.\n\nParts of the \"Wujing Zongyao\" were copied from older sources; historian Ralph D. Sawyer calls it \"essentially a cut-and-paste job\", containing many passages from earlier classical military writings whose original authors are left unidentified, a common practice at the time. During the Song dynasty, the \"Wujing Zongyao\" was appended to two other books: the \"Xingjun xuzhi\" and the \"Baizhan qifa\", both written by anonymous authors.\n\nThe \"Wujing Zongyao\" was one of 347 military treatises listed in the biographical chapters of the History of Song (1345 AD), one of the Twenty-Four Histories. Of these 347 different military treatises from the Song period, only the \"Wujing Zongyao\", the \"Huqianjing\" (Tiger Seal Manual) of Xu Dong in 1004 AD, and fragments of similar works found in the later \"Yonglo Datian\", have survived. The original text of the \"Wujing Zongyao\" was kept in the Imperial Library while a number of hand-written copies were distributed elsewhere, including a copy given to Wang Shao by Emperor Shenzong of Song in 1069 AD.\n\nThe original copy of the \"Wujing Zongyao\" was lost during the Jin–Song wars when the invading Jurchens sacked the Northern Song capital of Kaifeng in 1126 AD. Only a few manuscripts survived as a result of its secretive nature. Very few trustees of the government were ever allowed to read it as increased propagation would have increased the chance of it falling into enemy hands. A remaining copy of the \"Wujing Zongyao\" was remade into a newly published edition in 1231 AD in the Southern Song dynasty. During the Ming Dynasty (1368–1644 AD), another book was published in 1439 AD featuring fragments of the \"Wujing Zongyao\" of 1231 while omitting some material and combining it with two other books, including a preface by Li Jin. The entire \"Wujing Zongyao\" was reprinted in 1510 AD and this version is currently the oldest extant copy available. However, the historian Joseph Needham asserts that the 1510 AD edition is the most reliable in its faithfulness to the original version, since it was printed from blocks that were re-carved directly from tracings of the edition made in 1231 AD, rather than recombining fragments of the original with other material.\n\nAfter the \"Wujing Zongyao\" of 1510 was printed, other Ming copies were made. This included the Jiajing edition (1522–1566 AD), the Wanli edition (1573–1619 AD) of Quanzhou, and the Wanli edition (1573–1619) of Jinling by Tang Xinyün (preserved by Cunjingge). During the Qing Dynasty (1644–1911 AD) it was also reprinted in two different editions during the 18th century, and again in 1934 with the Shanghai edition.\n\nThe \"Xu Wujing Zongyao\" (續武經總要; literally \"Continuation of Wujing Zongyao\") is a \"continuation\" of the \"Wujing Zongyao\" written in the late Ming dynasty. The book focuses primarily on army formations and military deployments. It was written by Fan Jingwen (1587–1644), who was then the Vice President of the Board of War (兵部尚書; \"bingbu shangshu\"). Fan wrote the book because he felt that reprints of the \"Wujing Zongyao\" circulating at that time were out of date and did not take into account the technological and strategic changes that had occurred since the Song dynasty. The only surviving copy of the \"Xu Wujing Zongyao\" is held by Fudan University Library.\n\nIn the 3rd century, the Chinese engineer Ma Jun invented the south-pointing chariot. This was a wheeled vehicle that employed differential gearing in order to lock a figurine of an immortal in place on the end of a long wooden staff, the figure having its arm stretched out and always pointing to the southern cardinal direction. Although the authors of the \"Wujing Zongyao\" were mistaken in believing that the design of the south-pointing chariot was not handed down (as it was reinvented during the Song period and combined with an odometer), they described a new device which allowed one to navigate. This was the 'south pointing fish' (a thermoremanence compass), essentially a heated iron (or preferably steel) object cut in the shape of a fish and suspended in a bowl of water. The \"Wujing Zongyao\" part 1 volume 15 text stated:\n\nLater on in the Song dynasty the compass was used with maritime navigation. Several decades after the \"Wujing Zongyao\" was written, the scientist and statesman Shen Kuo (1031–1095 AD) wrote of the first truly magnetized compass needle in his book \"Dream Pool Essays\" (1088 AD). With a more efficient compass magnetized by lodestone, the thermoremanence compass fell out of use. The later maritime author Zhu Yu wrote of the magnetic needle compass as a means to navigate at sea in his \"Pingzhou Table Talks\" of 1119 AD.\n\nThe \"Wujing Zongyao\"'s illustrated descriptions of warships had a significant influence on later naval handbooks and encyclopedias such as the naval section of the \"Wubei Zhi\" from circa 1628. These works would incorporate illustrations of ships originally from the \"Wujing Zongyao\". The use of pictures from the \"Wujing Zongyao\" would continue to appear in Japanese naval texts up until the 18th century. The illustrations were used by both Nishikawa Joken's \"Ka-i Tsūshō-kō\" (Studies on the Intercourse and Trade with Chinese and Barbarians) in 1708 and Kanazawa Kanemitsu's \"Wakan Senyōshū\" (Collected Studies on the Ships used by the Chinese and Japanese) in 1766.\n\nThe \"Wujing Zongyao\" divides Chinese warships into six categories: Tower ships (\"lou chuan\"), combat or war junks (\"dou xian\" or \"zhan xian\"), covered swoopers (\"meng chong\"), flying barques (\"zou ge\"), patrol boats (\"you ting\"), and sea hawk ships (\"hai hu\"). The \"Wujing Zongyao\"'s typology for classifying Chinese warships would reappear in later naval texts for many centuries.\n\nThe \"Wujing Zongyao\" records detailed descriptions of gunpowder weapons such as incendiary projectiles, smoke bombs, fire arrows, and grenades. It documents incendiary projectiles containing low-nitrate gunpowder, which were launched from catapults or lowered down from city walls onto besiegers. Examples of these incendiaries include the \"swallow-tail\" incendiary (; \"yanweiju\") and the flying incendiary (; \"feiju\"). The swallow-tail incendiary was made of straw tied together and dipped in fat or oil. Chinese soldiers defending a city under siege would light the incendiary and lower it onto any wooden structure of the invading army to engulf it in fire. The flying incendiary visually resembled the swallow-tail incendiary, but was lowered using an iron chain from a swape lever installed within the walls of the city. The book also describes an 'igniter ball' used in warfare and for finding the firing range. The \"Wujing Zongyao\" stated the following:\n\nGunpowder was attached to fire arrows () and utilized as an incendiary. The \"Wujing Zongyao\" records that fire arrows were launched from bows or crossbows. The gunpowder used for fire arrows was likely a low-nitrate powder, and the quantity of gunpowder varied according to the type of bow. In the book, the force of gunpowder is said to be enough to launch an arrow, but only when the elasticity of the crossbow is sufficient.\n\nThe \"Wujing Zongyao\" discusses various types of incendiary bombs and grenades. They used a low-nitrate gunpowder that, while not powerful enough to cause an explosion, was effective for incendiary weapons. The \"huoqiu\" (; literally \"fire ball\") was filled with gunpowder and launched using a trebuchet. Upon impact, the \"huoqiu\" would start a fire among an invading army. Chinese bombs such as the thunder clap bomb or \"pili pao\" used a greater percentage of gunpowder than that of the \"huoqiu\". The gunpowder mixture for a bomb was placed within a rigid container that held in the expanding gas, allowing for more powerful explosions. The thunder clap bomb was constructed with a container made from bamboo.\n\nIn the \"Wujing Zongyao\" and other military texts, the distinction between a bomb and a grenade is ambiguous. At the time, the Chinese usually did not categorize gunpowder weapons according to their delivery method. One of the few exceptions is the \"shoupao\", or hand bomb, which is analogous to the hand grenade.\n\nGunpowder had already been invented prior to the \"Wujing Zongyao\" by Chinese alchemists in the 9th century. Early references to gunpowder can be found in the Daoist book \"Zhenyuan miaodao yaolue\", written circa 850, and gunpowder was utilized in Chinese warfare as early as the 10th century in fire arrows and gunpowder fuses used to light the Chinese two-piston flamethrower. \n\nHowever it was not until the \"Wujing Zongyao\" that the exact chemical formulas for early Chinese gunpowder was revealed. The \"Wujing Zongyao\" contains three formulas for gunpowder: one for an explosive bomb launched from a trebuchet, another for a similar bomb with hooks attached so that it could latch on to any wooden structure and set it on fire, and another formula specified for a poison-smoke bomb used for chemical warfare.\n\nThe \"Wujing Zongyao\"'s first recorded gunpowder formula used in these bombs held a potassium nitrate level of 55.4% to 55.5%, sulfur content of 19.4% to 26.5%, and carbonaceous content of 23% to 25.2%. The first step for making gunpowder is to powder and mix together sulphur, saltpetre, charcoal, pitch, and dried lacquer. Tung oil, dried plants, and wax are mixed next to create a paste. The paste and powder are combined and carefully stirred. Then the mixture is placed inside a paper container, wrapped up, and tied using hemp twine. Several precautions are taken to prevent the gunpowder from becoming damp.\n\nFor the second formula, the inner ball alone had a nitrate percentage of 61.5% to 50.2%, a sulfur content of 30.8% to 25.1%, and if all carbonaceous matter was taken, 24.7%, if just taking the charcoal content alone, the carbon level was 7.7%. If the outer coating and inner ball are both included with the second black-powder formula, that would yield a nitrate level of 34.7% to 54.8%, a sulfur content of 17.4% to 27.4%, and if all carbonaceous material is used, 47.9% carbon, if only charcoal is used, 17.8%. If the inner ball of the third black-powder formula is only considered, it held nitrate levels of 39.6% if all carbonaceous matter was taken, 49.4% nitrate if excluding the poisons, and 60% if charcoal is specified alone. The sulfur content was 19.8% if all carbonaceous matter was considered, 24.7% if this excluded poisons, and 30% if charcoal is specified alone. The carbon content was 40.5% if all carbonaceous matter was considered, 25.9% if this excluded poisons, and 10% if charcoal alone was specified. If both the inner ball and outer coating are considered for the third formula, that would yield a nitrate level of 27% if all carbonaceous matter was taken, 31.2% if this excluded poisons, and 51.7% if charcoal alone was used. The sulfur content would be 13.5% if all carbonaceous matter was taken, 15.6% if this excluded the poisons, and 25.9% if only charcoal alone was specified. The carbon content was 59.5% if all carbonaceous matter was taken into account, 53.2% if this excluded poisons, and 22.4% if charcoal alone was specified.\n\nThe first black-powder concoction was simply labeled as the \"method for making the fire-chemical\", with its ingredients and measured weight (in ounces) of each ingredient listed in the section below with the others listed in similar fashion.\n\n1st formula\n\nTotal weight = 82.2 oz.\n\n2nd formula\nInner ball\nTotal weight of inner ball = 79.7 oz.\n\nOuter coating\nTotal weight of outer coating = 36.6 oz.\nTotal weight = 116.3 oz.\n\n3rd formula\nInner ball\nTotal weight of inner ball = 77.7 oz.\n\nOuter coating\nTotal weight of outer coating = 36.6 oz.\nTotal weight = 114.3 oz.\n\nThe \"Wujing Zongyao\" describes a flamethrower with a double-acting two-piston cylinder-pump capable of shooting a continuous blast of flame. The first Chinese battle to use the double-piston pump flamethrower was the Battle of Langshan Jiang in 919 AD. In the Battle of Langshan Jiang (Wolf Mountain River, 狼山江), the naval fleet of the Wenmu King of Wuyue defeated the fleet of the Kingdom of Wu because he had used 'fire oil' (huo yóu, 火油) to burn his fleet; this signified the first Chinese use of gunpowder in warfare, since a slow-burning match fuse was required to ignite the flames. Greek fire is likely based on distilled petroleum and is a weapon of Byzantine origin. The Chinese author Lin Yu explained in his book of 919 AD that Greek fire was acquired from their Arab maritime trade contacts in the Indian Ocean. Furthermore, the Chinese had been using the piston syringe since the Han Dynasty (202 BC – 220 AD). However, it was the later \"Wujing Zongyao\" that would provide the first illustrated drawing and greater textual explanation for how this flamethrower operated. In describing the drawn illustration of the flamethrower in the book, the \"Wujing Zongyao\" states:\n\nThen the text goes on to provide further instructions about equipment, maintenance, and repair of flamethrowers:\n\n\n\n\n"}
