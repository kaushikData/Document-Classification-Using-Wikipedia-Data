{"id": "58960535", "url": "https://en.wikipedia.org/wiki?curid=58960535", "title": "1ES 1101-232", "text": "1ES 1101-232\n\n1ES 1101-232 is an active galactic nucleus of a distant galaxy known as a blazar.\n\nAn X-ray source (catalogued as A 1059-22) was first recorded by Maccagni and colleagues in a 1978 paper; they thought the source arose from a galaxy in the Abell 1146 galaxy cluster, which contained many giant elliptical galaxies. In 1989, Remillard and colleagues linked the X-ray source with a visual object and established that the object was surrounded by a large elliptical galaxy. They also discovered that the object (and galaxy) were more distant, with a redshift of 0.186. The host galaxy appears to be part of a distant galaxy cluster.\n"}
{"id": "5729980", "url": "https://en.wikipedia.org/wiki?curid=5729980", "title": "A591 road", "text": "A591 road\n\nThe A591 is a major road in Cumbria, in the north-west of England, which lies almost entirely within the Lake District national park. A 2009 poll by satellite navigation firm Garmin named the stretch of the road between Windermere and Keswick as the most popular road in Britain. The 29.8 mile stretch between Kendal and Keswick was also named the UK's best driving road, according to a specially devised driving ratio formulated by car rental firm Avis.\n\nThe road begins (at its southern end) north-west of junction 36 of the M6 motorway at Brettargh Holt roundabout with the A590 road close to Sizergh Castle. It bypasses the town of Kendal as a dual carriageway, this £1.9m section opened on 29 August 1971. It becomes a busy single carriageway road as it enters the Lake District. It bypasses the town of Windermere, closely following the north-eastern bank of Lake Windermere. It then travels through the centre of Ambleside, follows the northern side of Rydal Water, passes White Moss Common, follows the eastern edge of Grasmere and passes the village of Grasmere. The road continues over Dunmail Raise and along the eastern edge of Thirlmere. Shortly afterwards it reaches the town of Keswick. It meets the A66 road at a grade-separated junction. Traffic then bypasses Keswick by following the A66 west for 2.2 km to a roundabout where the A591 resumes, continuing in a roughly north-westerly direction, with fine views over Bassenthwaite Lake. The road terminates at the village of Bothel, on the A595 road The Bothel-to-Keswick section has many acute bends and is particularly narrow in places, leading to many accidents. \n\nThe road was badly damaged during Storm Desmond on 5 December 2015 including a part washed away at Dunmail Raise and landslip adjacent to Thirlmere. The closed section of road between Grasmere and Legburthwaite reopened on 11 May 2016. While the road was closed a new trail for walkers, cyclists, and horseriders was built to the west of the road at Dunmail Raise.\n\n"}
{"id": "50961831", "url": "https://en.wikipedia.org/wiki?curid=50961831", "title": "Alexander Pushkin (diamond)", "text": "Alexander Pushkin (diamond)\n\nThe Alexander Pushkin () is a 320.65 carat colorless raw diamond, the second largest gem diamond ever found in Russia or the territory of the former USSR (after the 26th Congress of the CPSU), and one of the largest in the world as of 2016. It was mined at the Udachnaya kimberlitic pipe (Yakutia, Far Eastern Federal District) in December 1989 and named after the world-famous Russian writer Alexander Pushkin. It is kept in the Russian Diamond Fund (Moscow Kremlin.) \n\n"}
{"id": "35644003", "url": "https://en.wikipedia.org/wiki?curid=35644003", "title": "Alicia Lourteig", "text": "Alicia Lourteig\n\nAlicia Lourteig (1913–2003) was an Argentine botanist.\n"}
{"id": "36520038", "url": "https://en.wikipedia.org/wiki?curid=36520038", "title": "August Friedrich Böttcher", "text": "August Friedrich Böttcher\n\nAugust Friedrich Böttcher (5 October 1825, Berlin – 20 November 1900) \nHe was an insect dealer in Berlin.\n\n"}
{"id": "3717604", "url": "https://en.wikipedia.org/wiki?curid=3717604", "title": "Cog's ladder", "text": "Cog's ladder\n\nCog's ladder of group development is based on the work, \"Cog's Ladder: A Model of Group Growth\", by George O. Charrier, an employee of Procter and Gamble, published in a company newsletter in 1972. The original document was written to help group managers at Procter and Gamble better understand the dynamics of group work, thus improving efficiency. It is now also used by the United States Naval Academy, the United States Air Force Academy, and other businesses – to help in understanding group development.\n\nThe basic idea of Cog's ladder is that there are five steps necessary for a small group of people to be able to work efficiently together. These stages are the \"polite stage\", the \"why we're here stage\", the \"power stage\", the \"cooperation stage\" and the \"esprit stage\". Groups can only move forward after completing the current stage as in Jean Piaget's stage model.\n\nAn introductory phase where members strive to get acquainted or reacquainted with one another. During this phase, the basis for the group structure is established and is characterized by polite social interaction. All ideas are simple, controversy is avoided and all members limit self-disclosure. Judgements of other members are formed, and this sets the tone for the rest of the group's time.\n\nGroup members will want to know why they have been called together. The specific agenda for each planning session will be communicated by the moderator or leader. In this phase, individual need for approval begins to diminish as the members examine their group's purpose and begin to set goals. Often, social cliques will begin to form as members begin to feel as though they \"fit in.\"\n\nBids for power begin between group members in an effort to convince each other that their position on an issue is correct. Often, the field of candidates vying for leadership narrows, as fewer members strive to establish power. Some of those who contributed freely to the group discussion in earlier stages now remain silent, wishing not to engage in a power struggle. It is noted that interactions arising out of this phase do not usually result in optimum solutions. Hence, there is a great need for structure and patience in this stage.*\n\nMembers not only begin to accept that others have an opinion worth expressing, but a team spirit replaces vested interests. Often, new levels of creativity are achieved, and the group's productivity soars. If new individuals are introduced into the membership at this point, they will be viewed as outsiders or intruders, and the group will have to evolve again, much as it did initially.\n\nEsprit de corps: a golden rule by Henri Fayol is closely associated with this stage of Cog's, which says that nothing has higher pinnacles to reach than spirit of corps.\n\nMutual acceptance with high cohesiveness and a general feeling of esprit. Charrier states that the planning team can do its finest work and be most productive in this final stage in the model. It is also noted that this stage will not always be achieved; however, for this level of cooperation, as well as productivity, the other four stages must be met.\n\nCog's ladder is very similar to Tuckman's stages, another stage model of groups, which lacks the \"Why We're Here\" stage, and calls the remaining four stages \"Forming\", \"Storming\", \"Norming\", and \"Performing\". There are also other similar models, some of which are cyclic rather than reaching an end state.\n\n"}
{"id": "5945291", "url": "https://en.wikipedia.org/wiki?curid=5945291", "title": "Commodity chain", "text": "Commodity chain\n\nA commodity chain is a process used by firms to gather resources, transform them into goods or commodities, and finally, distribute them to consumers. It is a series of links connecting the many places of production and distribution and resulting in a commodity that is then exchanged on the world market. In short, it is the connected path from which a good travels from producers to consumers. Commodity chains can be unique depending on the product types or the types of markets. Different stages of a commodity chain can also involve different economic sectors or be handled by the same business.\n\nA number of commentators have remarked that in the Internet age commodity chains are becoming increasingly more transparent. The Wikichains.com project has adopted the same wiki technology used by Wikipedia in order to help make commodity chains more transparent. \"They are a network of labour and production processes whose end result is a finished commodity\". William Jones Esq.\n\nA commodity chain demonstrates that each link of an extended chain of production and consumption links between resource producers and suppliers, various manufacturers, traders and shippers, wholesalers, and retailers. Rather than a linear chain, a circuit-board is a better metaphor for this concept because things are interconnected in so many ways, not just a mere straight line. This source explains global commodity chains in depth. \n\n"}
{"id": "33333386", "url": "https://en.wikipedia.org/wiki?curid=33333386", "title": "Currency packaging", "text": "Currency packaging\n\nCurrency packaging includes several forms of packing money for easy handling and counting. Many systems use standard color-coding or are marked to indicate the amount in the package.\n\nCurrency straps, also known as currency bands or bill straps, are a type of fastener used to secure discrete numbers of bills. Typically, currency bands have attached ends so bills are \"curled\" and slipped into the band whereas currency straps contain adhesive on the ends to secure them around the bills after wrapping.\n\nGenerated bundles are packed together in groups of 10 (1000 banknotes) and vacuumized. A cliche print containing bank and branch details is applied to the plastic package seal.\nVacuum packing is the most reliable and effective way of storing currency units, enabling the currency to be protected against tarnishing, such as from moisture and dirt. Vacuum packed banknotes also take up less space in containers used during cash transportation. Currency units are vacuum packed by means of a vacuum sealer.\n\nBags used to hold a specific amount of paper currency. Usually tamper evident, and has labels to make notes on the front.\n\nBags used to hold a specific number of coins.\n\nTubes used to hold a specific number of coins.\n\nTrays used to handle currency, often sorting by denomination.\n\n"}
{"id": "24162663", "url": "https://en.wikipedia.org/wiki?curid=24162663", "title": "Cyclic flower", "text": "Cyclic flower\n\nA cyclic flower is a flower type formed out of a series of whorls; sets of identical organs attached around the axis at the same point. Most flowers consist of a single whorl of sepals termed a calyx; a single whorl of petals termed a corolla; one or more whorls of stamens (together termed the androecium); and a single whorl of carpels termed the gynoecium. This is a cyclic arrangement.\n\nSome flowers contain flower parts with a spiral arrangement. Such flowers are not cyclic. However in the common case of spirally arranged sepals on an otherwise cyclic flower, the term hemicyclic may be used.\n\nThe suffix -cyclic is used to denote the number of whorls contained within a flower. The most common case is the pentacyclic flower, which contains five whorls: a calyx, a corolla, two whorls of stamens, and a single whorl of carpels. Another common case is the tetracyclic flower, which contains only one whorl of stamens, and therefore only four whorls in total. Tricyclic flowers also occur, generally where there is a single undifferentiated perianth. Flowers with more than five whorls are also not uncommon. The greatest variation occurs in the calyx and the androecium. Calyces of up to nine whorls have been recorded, and up to 12 whorls of stamens have been observed.\n\n"}
{"id": "50890026", "url": "https://en.wikipedia.org/wiki?curid=50890026", "title": "Direct coupling analysis", "text": "Direct coupling analysis\n\nDirect coupling analysis or DCA is an umbrella term comprising several methods for analyzing sequence data in computational biology. The common idea of these methods is to use statistical modeling to quantify the strength of the direct relationship between two positions of a biological sequence, excluding effects from other positions. This contrasts usual measures of correlation, which can be large even if there is no direct relationship between the positions (hence the name \"direct\" coupling analysis). Such a direct relationship can for example be the evolutionary pressure for two positions to maintain mutual compatibility in the biomolecular structure of the sequence, leading to molecular coevolution between the two positions.\nDCA has been used in the inference of protein residue contacts, RNA structure prediction, the inference of protein-protein interaction networks\n\nThe basis of DCA is a statistical model for the variability within a set of phylogenetically related biological sequences. When fitted to a multiple sequence alignment (MSA) of sequences of length formula_1, the model defines a probability for all possible sequences of the same length. This probability can be interpreted as the probability that the sequence in question belongs to the same class of sequences as the ones in the MSA, for example the class of all protein sequences belonging to a specific protein family.\n\nWe denote a sequence by formula_2, with the formula_3 being categorical variables representing the monomers of the sequence (if the sequences are for example aligned amino acid sequences of proteins of a protein family, the formula_3 take as values any of the 20 standard amino acids). The probability of a sequence within a model is then defined as\n\nwhere \n\nThe parameters formula_9 depend on one position formula_10 and the symbol formula_3 at this position. They are usually called fields and represent the propensity of symbol to be found at a certain position. The parameters formula_12 depend on pairs of positions formula_13 and the symbols formula_14 at these positions. They are usually called couplings and represent an interaction, i.e. a term quantifying how compatible the symbols at both positions are with each other. The model is fully connected, so there are interactions between all pairs of positions. The model can be seen as a generalization of the Ising model, with spins not only taking two values, but any value from a given finite alphabet. In fact, when the size of the alphabet is 2, the model reduces to the Ising model. Since it is also reminiscent of the model of the same name, it is often called Potts Model.\n\nIt should be noted that even knowing the probabilities of all sequences does not determine the parameters formula_6 uniquely. For example, a simple transformation of the parameters\n\nfor any set of real numbers formula_17 leaves the probabilities the same. The likelihood function is invariant under such transformations as well, so the data cannot be used to fix these degrees of freedom (although a prior on the parameters might do so).\n\nA convention often found in literature is to fix these degrees of freedom such that the Frobenius norm of the coupling matrix \n\nis minimized (independently for every pair of positions formula_10 and formula_20).\n\nTo justify the Potts model, it is often noted that it can be derived following a maximum entropy principle: For a given set of sample covariances and frequencies, the Potts model represents the distribution with the maximal Shannon entropy of all distributions reproducing those covariances and frequencies. For a multiple sequence alignment, the sample covariances are defined as\n\nwhere formula_22 is the frequency of finding symbols formula_23 and formula_24 at positions formula_10 and formula_20 in the same sequence in the MSA, and formula_27 the frequency of finding symbol formula_28 at position formula_10. The Potts model is then the unique distribution formula_30 that maximizes the functional\n\nThe first term in the functional is the Shannon entropy of the distribution. The formula_32 are Lagrange multipliers to ensure formula_33, with formula_34 being the marginal probability to find symbols formula_35 at positions formula_13. The Lagrange multiplier formula_37 ensures normalization. \nMaximizing this functional and identifying\n\nleads to the Potts model above. It should be noted that this procedure only gives the functional form of the Potts model, while the numerical values of the Lagrange multipliers (identified with the parameters) still have to be determined by fitting the model to the data.\n\nThe central point of DCA is to interpret the formula_39 (which can be represented as a formula_40 matrix if there are formula_41 possible symbols) as direct couplings. If two positions are under joint evolutionary pressure (for example to maintain a structural bond), one might expect these couplings to be large because only sequences with fitting pairs of symbols should have a significant probability. On the other hand, a large correlation between two positions does not necessarily mean that the couplings are large, since large couplings between e.g. positions formula_13 and formula_43 might lead to large correlations between positions formula_10 and formula_45, mediated by position formula_20. In fact, such indirect correlations have been implicated in the high false positive rate when inferring protein residue contacts using correlation measures like mutual information.\n\nThe inference of the Potts model on a multiple sequence alignment (MSA) using maximum likelihood estimation is usually computationally intractable, because one needs to calculate the normalization constant formula_47, which is for sequence length formula_1 and formula_41 possible symbols a sum of formula_50 terms (which means for example for a small protein domain family with 30 positions formula_51 terms). Therefore, numerous approximations and alternatives have been developed:\n\n\nAll of these methods lead to some form of estimate for the set of parameters formula_52 maximizing the likelihood of the MSA. Many of them include regularization or prior terms to ensure a well-posed problem or promote a sparse solution.\n\nA possible interpretation of large values of couplings in a model fitted to a MSA of a protein family is the existence of conserved contacts between positions (residues) in the family. Such a contact can lead to molecular coevolution, since a mutation in one of the two residues, without a compensating mutation in the other residue, is likely to disrupt protein structure and negatively affect the fitness of the protein. Residue pairs for which there is a strong selective pressure to maintain mutual compatibility are therefore expected to mutate together or not at all. This idea (which was known in literature long before the conception of DCA) has been used to predict protein contact maps, for example analyzing the mutual information between protein residues.\n\nWithin the framework of DCA, a score for the strength of the direct interaction between a pair of residues formula_13 is often defined using the Frobenius norm formula_54 of the corresponding coupling matrix formula_39 and applying an \"average product correction\" (APC):\n\nwhere formula_54 has been defined above and\n\nThis correction term was first introduced for mutual information and is used to remove biases of specific positions to produce large formula_54. Scores that are invariant under parameter transformations that do not affect the probabilities have also been used.\nSorting all residue pairs by this score results in a list in which the top of the list is strongly enriched in residue contacts when compared to the protein contact map of a homologous protein. High-quality predictions of residue contacts are valuable as prior information in protein structure prediction.\n\nDCA can be used for detecting conserved interaction between protein families and for predicting which residue pairs form contacts in a protein complex. Such predictions can be used when generating structural models for these complexes, or when inferring protein-protein interaction networks made from more than two proteins.\n\nDCA can be used to model fitness landscapes and to predict the effect of a mutation in the amino acid sequence of a protein on its fitness.\n\nOnline services:\n\n\nSource code: \n"}
{"id": "258827", "url": "https://en.wikipedia.org/wiki?curid=258827", "title": "Dynamic mechanical analysis", "text": "Dynamic mechanical analysis\n\nDynamic mechanical analysis (abbreviated DMA, also known as dynamic mechanical spectroscopy) is a technique used to study and characterize materials. It is most useful for studying the viscoelastic behavior of polymers. A sinusoidal stress is applied and the strain in the material is measured, allowing one to determine the complex modulus. The temperature of the sample or the frequency of the stress are often varied, leading to variations in the complex modulus; this approach can be used to locate the glass transition temperature of the material, as well as to identify transitions corresponding to other molecular motions.\n\nPolymers composed of long molecular chains have unique viscoelastic properties, which combine the characteristics of elastic solids and Newtonian fluids. The classical theory of elasticity describes the mechanical properties of elastic solid where stress is proportional to strain in small deformations. Such response of stress is independent of strain rate. The classical theory of hydrodynamics describes the properties of viscous fluid, for which the response of stress is dependent on strain rate. This solidlike and liquidlike behavior of polymers can be modeled mechanically with combinations of springs and dashpots.\n\nThe viscoelastic property of a polymer is studied by dynamic mechanical analysis where a sinusoidal force (stress σ) is applied to a material and the resulting displacement (strain) is measured. For a perfectly elastic solid, the resulting strain and the stress will be perfectly in phase. For a purely viscous fluid, there will be a 90 degree phase lag of strain with respect to stress. Viscoelastic polymers have the characteristics in between where some phase lag will occur during DMA tests. When the stress is applied and the strain lags behind, the following equations hold:\n\n\nwhere\n\nConsider the purely elastic case, where stress is proportional to strain. We have <br>\nformula_6\n\nNow for the purely viscous case, where stress is proportional to strain \"rate\".<br>\nformula_7\n\nThe storage modulus measures the stored energy, representing the elastic portion, and the loss modulus measures the energy dissipated as heat, representing the viscous portion. The tensile storage and loss moduli are defined as follows:\n\n\nSimilarly we also define shear storage and loss moduli, formula_11 and formula_12.\n\nComplex variables can be used to express the moduli formula_13 and formula_14 as follows:\nwhere\n\nShear stress formula_18 of a finite element in one direction can be expressed with relaxation modulus formula_19 and strain rate, integrated over all past times formula_20 up to the current time formula_4. With strain rate formula_22and substitution formula_23 one obtains formula_24. Application of the trigonometric addition theorem formula_25 lead to the expression \nwith converging integrals, if formula_27 for formula_28, which depend on frequency but not of time. Extension of formula_29 with trigonometric identity formula_30 lead to \nComparison of the two formula_32 equations lead to the definition of formula_11 and formula_12 .\n\n One important application of DMA is measurement of the glass transition temperature of polymers. Amorphous polymers have different glass transition temperatures, above which the material will have rubbery properties instead of glassy behavior and the stiffness of the material will drop dramatically with an increase in viscosity. At the glass transition, the storage modulus decreases dramatically and the loss modulus reaches a maximum. Temperature-sweeping DMA is often used to characterize the glass transition temperature of a material.\n\nVarying the composition of monomers and cross-linking can add or change the functionality of a polymer that can alter the results obtained from DMA. An example of such changes can be seen by blending ethylene-propylene-diene monomer (EPDM) with styrene-butadiene rubber (SBR) and different cross-linking or curing systems. Nair \"et al.\" abbreviate blends as ES, ES, etc., where ES equals the weight percent of EPDM in the blend and S denotes sulfur as the curing agent.\n\nIncreasing the amount of SBR in the blend decreased the storage modulus due to intermolecular and intramolecular interactions that can alter the physical state of the polymer. Within the glassy region, EPDM shows the highest storage modulus due to stronger intermolecular interactions (SBR has more steric hindrance that makes it less crystalline). In the rubbery region, SBR shows the highest storage modulus resulting from its ability to resist intermolecular slippage.\n\nWhen compared to sulfur, the higher storage modulus occurred for blends cured with dicumyl peroxide(DCP)because of the relative strengths of C-C and C-S bonds.\n\nIncorporation of reinforcing fillers into the polymer blends also increases the storage modulus at an expense of limiting the loss tangent peak height.\n\nDMA can also be used to effectively evaluate the miscibility of polymers. The ES blend had a much broader transition with a shoulder instead of a steep drop-off in a storage modulus plot of varying blend ratios, indicating that there are areas that are not homogeneous.\n\nThe instrumentation of a DMA consists of a displacement sensor such as a linear variable differential transformer, which measures a change in voltage as a result of the instrument probe moving through a magnetic core, a temperature control system or furnace, a drive motor (a linear motor for probe loading which provides load for the applied force), a drive shaft support and guidance system to act as a guide for the force from the motor to the sample, and sample clamps in order to hold the sample being tested. Depending on what is being measured, samples will be prepared and handled differently. A general schematic of the primary components of a DMA instrument is shown in figure 3.\n\nThere are two main types of DMA analyzers used currently: forced resonance analyzers and free resonance analyzers. Free resonance analyzers measure the free oscillations of damping of the sample being tested by suspending and swinging the sample. A restriction to free resonance analyzers is that it is limited to rod or rectangular shaped samples, but samples that can be woven/braided are also applicable. Forced resonance analyzers are the more common type of analyzers available in instrumentation today. These types of analyzers force the sample to oscillate at a certain frequency and are reliable for performing a temperature sweep.\n\nAnalyzers are made for both stress (force) and strain (displacement) control. In strain control, the probe is displaced and the resulting stress of the sample is measured by implementing a force balance transducer, which utilizes different shafts. The advantages of strain control include a better short time response for materials of low viscosity and experiments of stress relaxation are done with relative ease. In stress control, a set force is applied to the same and several other experimental conditions (temperature, frequency, or time) can be varied. Stress control is typically less expensive than strain control because only one shaft is needed, but this also makes it harder to use. Some advantages of stress control include the fact that the structure of the sample is less likely to be destroyed and longer relaxation times/ longer creep studies can be done with much more ease. Characterizing low viscous materials come at a disadvantage of short time responses that are limited by inertia. Stress and strain control analyzers give about the same results as long as characterization is within the linear region of the polymer in question. However, stress control lends a more realistic response because polymers have a tendency to resist a load.\n\nStress and strain can be applied via torsional or axial analyzers. Torsional analyzers are mainly used for liquids or melts but can also be implemented for some solid samples since the force is applied in a twisting motion. The instrument can do creep-recovery, stress–relaxation, and stress–strain experiments. Axial analyzers are used for solid or semisolid materials. It can do flexure, tensile, and compression testing (even shear and liquid specimens if desired). These analyzers can test higher modulus materials than torsional analyzers. The instrument can do thermomechanical analysis (TMA) studies in addition to the experiments that torsional analyzers can do. Figure 4 shows the general difference between the two applications of stress and strain.\n\nChanging sample geometry and fixtures can make stress and strain analyzers virtually indifferent of one another except at the extreme ends of sample phases, i.e. really fluid or rigid materials. Common geometries and fixtures for axial analyzers include three-point and four-point bending, dual and single cantilever, parallel plate and variants, bulk, extension/tensile, and shear plates and sandwiches. Geometries and fixtures for torsional analyzers consist of parallel plates, cone-and-plate, couette, and torsional beam and braid. In order to utilize DMA to characterize materials, the fact that small dimensional changes can also lead to large inaccuracies in certain tests needs to be addressed. Inertia and shear heating can affect the results of either forced or free resonance analyzers, especially in fluid samples.\n\nTwo major kinds of test modes can be used to probe the viscoelastic properties of polymers: temperature sweep and frequency sweep tests. A third, less commonly studied test mode is dynamic stress–strain testing.\n\nA common test method involves measuring the complex modulus at low constant frequency while varying the sample temperature. A prominent peak in formula_35 appears at the glass transition temperature of the polymer. Secondary transitions can also be observed, which can be attributed to the temperature-dependent activation of a wide variety of chain motions. In semi-crystalline polymers, separate transitions can be observed for the crystalline and amorphous sections. Similarly, multiple transitions are often found in polymer blends.\n\nFor instance, blends of polycarbonate and poly(acrylonitrile-butadiene-styrene) were studied with the intention of developing a polycarbonate-based material without polycarbonate’s tendency towards brittle failure. Temperature-sweeping DMA of the blends showed two strong transitions coincident with the glass transition temperatures of PC and PABS, consistent with the finding that the two polymers were immiscible.\n\nA sample can be held to a fixed temperature and can be tested at varying frequency. Peaks in formula_35 and in E’’ with respect to frequency can be associated with the glass transition, which corresponds to the ability of chains to move past each other. Note that this implies that the glass transition is dependent on strain rate in addition to temperature. Secondary transitions may be observed as well.\n\nThe Maxwell model provides a convenient, if not strictly accurate, description of viscoelastic materials. Applying a sinusoidal stress to a Maxwell model gives: formula_37 where formula_38 is the Maxwell relaxation time. Thus, a peak in E’’ is observed at the frequency formula_39. A real polymer may have several different relaxation times associated with different molecular motions.\n\nBy gradually increasing the amplitude of oscillations, one can perform a dynamic stress–strain measurement. The variation of storage and loss moduli with increasing stress can be used for materials characterization, and to determine the upper bound of the material’s linear stress–strain regime.\n\nBecause glass transitions and secondary transitions are seen in both frequency studies and temperature studies, there is interest in multidimensional studies, where temperature sweeps are conducted at a variety of frequencies or frequency sweeps are conducted at a variety of temperatures. This sort of study provides a rich characterization of the material, and can lend information about the nature of the molecular motion responsible for the transition.\n\nFor instance, studies of polystyrene (T ~ 110 °C) have noted a secondary transition near room temperature. Temperature-frequency studies showed that the transition temperature is largely frequency-independent, suggesting that this transition results from a motion of a small number of atoms; it has been suggested that this is the result of the rotation of the phenyl group around the main chain.\n\n"}
{"id": "1244292", "url": "https://en.wikipedia.org/wiki?curid=1244292", "title": "Electromagnetically induced transparency", "text": "Electromagnetically induced transparency\n\nElectromagnetically induced transparency (EIT) is a coherent optical nonlinearity which renders a medium transparent within a narrow spectral range around an absorption line. Extreme dispersion is also created within this transparency \"window\" which leads to \"slow light\", described below. It is in essence a quantum interference effect that permits the propagation of light through an otherwise opaque atomic medium.\n\nObservation of EIT involves two optical fields (highly coherent light sources, such as lasers) which are tuned to interact with three quantum states of a material. The \"probe\" field is tuned near resonance between two of the states and measures the absorption spectrum of the transition. A much stronger \"coupling\" field is tuned near resonance at a different transition. If the states are selected properly, the presence of the coupling field will create a spectral \"window\" of transparency which will be detected by the probe. The coupling laser is sometimes referred to as the \"control\" or \"pump\", the latter in analogy to incoherent optical nonlinearities such as spectral hole burning or saturation.\n\nEIT is based on the destructive interference of the transition probability amplitude between atomic states. Closely related to EIT are coherent population trapping (CPT) phenomena.\n\nThe quantum interference in EIT can be exploited to laser cool atomic particles, even down to the quantum mechanical ground state of motion. This has recently been used to directly image individual atoms trapped in an optical lattice. \n\nThere are specific restrictions on the configuration of the three states. Two of the three possible transitions between the states must be \"dipole allowed\", i.e. the transitions can be induced by an oscillating electric field. The third transition must be \"dipole forbidden.\" One of the three states is connected to the other two by the two optical fields. The three types of EIT schemes are differentiated by the energy differences between this state and the other two. The schemes are the ladder, vee, and lambda. Any real material system may contain many triplets of states which could theoretically support EIT, but there are several practical limitations on which levels can actually be used.\n\nAlso important are the dephasing rates of the individual states. In any real system at non-zero temperature there are processes which cause a scrambling of the phase of the quantum states. In the gas phase, this means usually collisions. In solids, dephasing is due to interaction of the electronic states with the host lattice. The dephasing of state formula_1 is especially important; ideally formula_1 should be a robust, metastable state.\n\nCurrent EIT research uses atomic systems in dilute gases, solid solutions, or more exotic states such as Bose–Einstein condensate. EIT has been demonstrated in electromechanical and optomechanical systems, where it is known as optomechanically induced transparency. Work is also being done in semiconductor nanostructures such as quantum wells, quantum wires and quantum dots. \n\nEIT was first proposed theoretically by professor Jakob Khanin and graduate student Olga Kocharovskaya at Gorky State University (present: Nizhny Novgorod), Russia; there are now several different approaches to a theoretical treatment of EIT. One approach is to extend the density matrix treatment used to derive Rabi oscillation of a two-state, single field system. In this picture the probability amplitude for the system to transfer between states can interfere destructively, preventing absorption. In this context, \"interference\" refers to interference between \"quantum events\" (transitions) and not optical interference of any kind. As a specific example, consider the lambda scheme shown above. Absorption of the probe is defined by transition from formula_3 to formula_4. The fields can drive population from formula_3-formula_4 directly or from formula_3-formula_4-formula_1-formula_4. The probability amplitudes for the different paths interfere destructively. If formula_1 has a comparatively long lifetime, then the result will be a transparent window completely inside of the formula_3-formula_4 absorption line.\n\nAnother approach is the \"dressed state\" picture, wherein the system + coupling field Hamiltonian is diagonalized and the effect on the probe is calculated in the new basis. In this picture EIT resembles a combination of Autler-Townes splitting and Fano interference between the dressed states. Between the doublet peaks, in the center of the transparency window, the quantum probability amplitudes for the probe to cause a transition to either state cancel.\n\nA polariton picture is particularly important in describing stopped light schemes. Here, the photons of the probe are coherently \"transformed\" into \"dark state polaritons\" which are excitations of the medium. These excitations exist (or can be \"stored\") for a length of time dependent only on the dephasing rates.\n\nIt is important to realize that EIT is only one of many diverse mechanisms which can produce slow light. The Kramers–Kronig relations dictate that a change in absorption (or gain) over a narrow spectral range must be accompanied by a change in refractive index over a similarly narrow region. This rapid and \"positive\" change in refractive index produces an extremely low group velocity. The first experimental observation of the low group velocity produced by EIT was by Boller, İmamoğlu, and Harris at Stanford University in 1991 in strontium. In 1999 Lene Hau reported slowing light in a medium of ultracold sodium atoms, achieving this by using quantum interference effects responsible for electromagnetically induced transparency (EIT). Her group performed copious research regarding EIT with Stephen E. Harris. \"Using detailed numerical simulations, and analytical theory, we study properties of micro-cavities which incorporate materials that exhibit Electro-magnetically Induced Transparency (EIT) or Ultra Slow Light (USL). We find that such systems, while being miniature in size (order wavelength), and integrable, can have some outstanding properties. In particular, they could have lifetimes orders of magnitude longer than other existing systems, and could exhibit non-linear all-optical switching at single photon power levels. Potential applications include miniature atomic clocks, and all-optical quantum information processing.\" The current record for slow light in an EIT medium is held by Budker, Kimball, Rochester, and Yashchuk at U.C. Berkeley in 1999. Group velocities as low as 8 m/s were measured in a warm thermal rubidium vapor.\n\n\"Stopped\" light, in the context of an EIT medium, refers to the \"coherent\" transfer of photons to the quantum system and back again. In principle, this involves switching \"off\" the coupling beam in an adiabatic fashion while the probe pulse is still inside of the EIT medium. There is experimental evidence of trapped pulses in EIT medium. In authors created a stationary light pulse inside the atomic coherent media. In 2009 researchers from Harvard University and MIT demonstrated a few-photon optical switch for quantum optics based on the slow light ideas. Lene Hau and a team from Harvard University were the first to demonstrate stopped light.\n\n\n\n"}
{"id": "34760165", "url": "https://en.wikipedia.org/wiki?curid=34760165", "title": "Epitech", "text": "Epitech\n\nThe Paris Graduate School of Digital Innovation (, or Epitech), formerly European Institute of Information Technology in English is a private institution of higher education in general computer science that was founded in 1999 and has been accredited by the French State.\n\nHeadquartered in Le Kremlin-Bicêtre, south of Paris, the school has campuses in Bordeaux, Rennes, Marseille, Lille, Lyon, Montpellier, Nancy, Nantes, Nice, Strasbourg, Toulouse and Saint-André, Réunion. The school also has a location in Barcelona(Spain), Tirana, Albania.\n\nAt the end of the five-year course, students receive the title of \"Expert of Information Technologies\" who is recognized by French CNCP. This professional certification should not be confused in France with a degree of master (like an engineering diploma or a master diploma of public french universities).\n\nThe school has the particularity to teach with practical cases instead of theoretical.. Epitech has also an Executive MBA in IT and entrepreneurship course targeting executive managers in computer science.\n\nThe institution is part of IONIS Education Group.\n\nEpitech was created in 1999, taking advantage of the keen interest of the \"École Pour l'Informatique et les Techniques Avancées\" to train students with a specific interest for computer sciences related matter only.\n\nIn 2007, Epitech opened new campuses in Casablanca, Dalian, Bordeaux, Lille, Lyon, Nantes, Strasbourg and Toulouse.\nSince January 2008, the degree delivered has been recognized by the \"Commission nationale de la certification professionnelle\", as level 1.\nIn 2008, the campuses of Nice, Montpellier, Nancy, Marseille and Rennes were opened.\nSince 2009, the school has been recognized by France.\n\nIn early 2013, Epitech announced it would open a campus in Beijing, China in September 2013 and further international branches in California, United Kingdom and Spain by September 2014.\n\nEPITECH has partnered with the \"Zup de Co\" association to create the \"Web@cademie\", a 2-year training completely free for students without the French Baccalaureate and with a strong motivation in computer science. This course has the goal to attain a job of software developer for young people who have stopped their regular studies. They are trained by EPITECH teachers at Le Kremlin-Bicêtre and in Lyon.\n\nAt the end of the five-year course, students receive the title of \"Expert of Information Technologies\" who is recognized by French CNCP. Because the school is not certified by the Commission des Titres d'Ingénieur, this professional certification should not be confused in France with a degree of master (like an engineering diploma or a master diploma of public french universities).\n\n"}
{"id": "12936739", "url": "https://en.wikipedia.org/wiki?curid=12936739", "title": "Event chain diagram", "text": "Event chain diagram\n\nEvent chain diagrams are visualizations that show the relationships between events and tasks and how the events affect each other. \n\nEvent chain diagrams are introduced as a part of event chain methodology. Event chain methodology is an uncertainty modeling and schedule network analysis technique that is focused on identifying and managing events and event chains that affect project schedules. Event chain methodology is the next advance beyond critical path method and critical chain project management.\n\nEvent chain diagrams are presented on the Gantt chart according to the specification.\nThis specification is a set of rules, which can be understood by anybody using this diagram.\n\n\nOften event chain diagrams can become very complex. In these cases, some details of the diagram do not need to be shown.\n\n\nThe central purpose of event chain diagrams is not to show all possible individual events. Rather, event chain diagrams can be used to understand the relationship between events. Therefore, it is recommended the event chain diagrams be used only for the most significant events during the event identification and analysis stage. Event chain diagrams can be used as part of the risk identification process, particularly during brainstorming meetings. Members of project teams can draw arrows between associated with activities on the Gantt chart. Event chain diagrams can be used together with other diagramming tools.\n\nThe simplest way to represent these chains is to depict them as arrows associated with certain tasks or time intervals on the Gantt chart. Different events and event chains can be displayed using different colors. Events can be global (for all tasks in the project) and local (for a particular task). By using event chain diagrams to visualize events and event chains, the modeling and analysis of risks and uncertainties can be significantly simplified.\n\nAnother tool that can be used to simplify the definition of events is a state table. Columns in the state table represent events; rows represent the states of an activity. Information for each event in each state includes four properties of event subscription: probability, moment of event, excited state, and impact of the event. State tables help to depict an activity’s subscription to the events: if a cell is empty the state is not subscribed to the event.\n\nThe ground state of the activity is subscribed to two events: “architectural changes” and “development tools issue”. If either of these events occurs, they transform the activity to a new excited state called “refactoring”. “Refactoring” is subscribed to another event: “minor requirement change”. Two previous events are not subscribed to the refactoring state and therefore cannot reoccur while the activity is in this state.\n\n\n"}
{"id": "59616", "url": "https://en.wikipedia.org/wiki?curid=59616", "title": "Fermentation theory", "text": "Fermentation theory\n\nThe fermentation theory was studied in depth and brought to light first by Louis Pasteur. This theory states that it is the idea or concept of how fermentation is brought on by microbes and put to the concept of spontaneous generation to rest. Even though this theory is now outdated and has been replaced by the germ theory of disease, for a long time it held true, and Louis was on the forefront of explaining why it seemed organisms appeared out of nothing instead of claiming it was just a spontaneous act of God. Fermentation was a process that has been used for thousands of years, but no one could explain exactly what was happening and why. From Pasteur's discovery of why and how fermentation occurs, the process has been studied intensely and is now a mastered art used in everyday life with processes of making things such as alcoholic beverages, some foods like yogurt or even manufacturing some medications.\n\nSimply put, fermentation is the anaerobic metabolic process that converts sugar into acids, gases, or alcohols. This metabolic process is used in oxygen starved environments. Yeast and many other microbes commonly use this process in order to carry our their anaerobic respiration to survive. Even the human body carries out fermentation processes from time to time. When someone runs a long distance race, lactic acid will build up in their muscles over the course of the race. That lactic acid is the by-product of fermentation taking place in their body, which tries to produce ATP so the body can continue to run since they could not process the oxygen intake fast enough. Although fermentation will give a lower yield of ATP production than aerobic respiration does, it can occur at a much higher rate.\nFermentation has been used by humans consciously since around 5000 BCE where there were jars recovered in the Iran Zagros Mountains area in which contained remnants of a microbes similar those present in the process of making grapes into wine.\nBefore the 1870s, when Pasteur published his work on this theory, it was believed that microorganisms and even some small animals such as frogs would spontaneously appear, which was coined as spontaneous generation. Spontaneous generation was the explained theory that when elements of the Earth such as clay or mud would mix with water and sunlight in certain amounts, creatures would just appear out of that concoction. A common way that this idea was \"proven\" over and over again was by taking a piece of raw meat and placing it in open air, which would almost always produce maggots. This idea was accepted and believe to be true before Louis Pasteur shook the Earth with his new ideas that organisms actually came from traceable beginnings. Pasteur demonstrated that fermentation is caused by the growth of microorganisms, and the emergent growth of bacteria in nutrient broths is due to biogenesis rather than spontaneous generation. He exposed boiled broths to air in vessels that contained a filter to prevent all particles from passing through to the growth medium. Yet, when the vessels were open to the air surrounding it, the organisms appeared. It could be concluded that spontaneous generation could be disproven. The organisms did not just appear but were coming from the air, yet we were not able to see them at such a small level. His famous experiment was used with a curved neck placed on top of a beaker. This curved neck was the key to proving his findings because it showed that the germs and microbes had to fall into the broth inside. The curved neck did not allow this to happen.\n\nThe fermentation theory of disease is the (now obsolete) concept that many diseases, including the diseases which were \"epidemic, endemic and contagious\", owe their origin to the presence of a \"morbific principle\" in the system, acting in a manner analogous to, although not identical with, the process of fermentation. It was rendered obsolete by the germ theory of disease, which led to the new science of bacteriology.\n\nFermenting a broth in a beaker has become more than just a way to prove that organisms don't just appear out of thin air. Today, the process of fermentation is used for a multitude of everyday applications. Some of those processes include medications which we ingest, beverages we consume, and even food we eat. Currently, companies like Genencor International uses the production of enzymes involved in fermentation to build a revenue of over $400 million a year. Many medications such as antibiotics are produced by the fermentation process. An example is the important drug cortisone, which can be prepared by the fermentation of a plant steroid known as diosgenin. \nThe enzymes used in the reaction are provided by the mold Rhizopus nigricans. Just as it is commonly known, alcohol of all types and brands are also produced by way of fermentation and distillation. Moonshine is a classic example of how this is carried out. Finally, foods such as yogurt are made by fermentation processes as well. Yogurt is a fermented milk product that contains the characteristic bacterial cultures Lactobacillus bulgaricus and Streptococcus thermopiles.\n\n"}
{"id": "1950766", "url": "https://en.wikipedia.org/wiki?curid=1950766", "title": "Graph isomorphism problem", "text": "Graph isomorphism problem\n\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.\n\nThe problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. It is known that the graph isomorphism problem is in the low hierarchy of class NP, which implies that it is not NP-complete unless the polynomial time hierarchy collapses to its second level. At the same time, isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently.\n\nThis problem is a special case of the subgraph isomorphism problem, which asks whether a given graph \"G\" contains a subgraph that is isomorphic to another given graph \"H\" and which is known to be NP-complete. It is also known to be a special case of the non-abelian hidden subgroup problem over the symmetric group.\n\nIn the area of image recognition it is known as the exact graph matching.\n\nThe best currently accepted theoretical algorithm is due to , and is based on the earlier work by combined with a \"subfactorial\" algorithm of V. N. Zemlyachenko . The algorithm has run time 2 for graphs with \"n\" vertices and relies on the classification of finite simple groups. Without CFSG, a slightly weaker bound \n\nIn November 2015, Babai announced a quasipolynomial time algorithm for all graphs, that is, one with running time formula_1 for some fixed formula_2. On January 4, 2017, Babai retracted the quasi-polynomial claim and stated a sub-exponential time bound instead after Harald Helfgott discovered a flaw in the proof. On January 9, 2017, Babai announced a correction (published in full on January 19) and restored the quasi-polynomial claim, with Helfgott confirming the fix. Helfgott further claims that one can take , so the running time is . The new proof has not been fully peer-reviewed yet.\n\nThere are several competing practical algorithms for graph isomorphism, such as those due to , , and . While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.\n\nThe graph isomorphism problem is computationally equivalent to the problem of computing the automorphism group of a graph, and is weaker than the permutation group isomorphism problem and the permutation group intersection problem. For the latter two problems, obtained complexity bounds similar to that for graph isomorphism.\n\nA number of important special cases of the graph isomorphism problem have efficient, polynomial-time solutions:\n\nSince the graph isomorphism problem is neither known to be NP-complete nor known to be tractable, researchers have sought to gain insight into the problem by defining a new class GI, the set of problems with a polynomial-time Turing reduction to the graph isomorphism problem. If in fact the graph isomorphism problem is solvable in polynomial time, GI would equal P.\n\nAs is common for complexity classes within the polynomial time hierarchy, a problem is called GI-hard if there is a polynomial-time Turing reduction from any problem in GI to that problem, i.e., a polynomial-time solution to a GI-hard problem would yield a polynomial-time solution to the graph isomorphism problem (and so all problems in GI). A problem formula_3 is called complete for GI, or GI-complete, if it is both GI-hard and a polynomial-time solution to the GI problem would yield a polynomial-time solution to formula_3.\n\nThe graph isomorphism problem is contained in both NP and co-AM. GI is contained in and low for Parity P, as well as contained in the potentially much smaller class SPP. That it lies in Parity P means that the graph isomorphism problem is no harder than determining whether a polynomial-time nondeterministic Turing machine has an even or odd number of accepting paths. GI is also contained in and low for ZPP. This essentially means that an efficient Las Vegas algorithm with access to an NP oracle can solve graph isomorphism so easily that it gains no power from being given the ability to do so in constant time.\n\nThere are a number of classes of mathematical objects for which the problem of isomorphism is a GI-complete problem. A number of them are graphs endowed with additional properties or restrictions:\nA class of graphs is called GI-complete if recognition of isomorphism for graphs from this subclass is a GI-complete problem. The following classes are GI-complete:\n\nMany classes of digraphs are also GI-complete.\n\nThere are other nontrivial GI-complete problems in addition to isomorphism problems.\n\n\n have shown a probabilistic checker for programs for graph isomorphism. Suppose \"P\" is a claimed polynomial-time procedure that checks if two graphs are isomorphic, but it is not trusted. To check if \"G\" and \"H\" are isomorphic:\n\n\nThis procedure is polynomial-time and gives the correct answer if \"P\" is a correct program for graph isomorphism. If \"P\" is not a correct program, but answers correctly on \"G\" and \"H\", the checker will either give the correct answer, or detect invalid behaviour of \"P\".\nIf \"P\" is not a correct program, and answers incorrectly on \"G\" and \"H\", the checker will detect invalid behaviour of \"P\" with high probability, or answer wrong with probability 2.\n\nNotably, \"P\" is used only as a blackbox.\n\nGraphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching, i.e., identification of similarities between graphs, is an important tools in these areas. In these areas graph isomorphism problem is known as the exact graph matching.\n\nIn cheminformatics and in mathematical chemistry, graph isomorphism testing is used to identify a chemical compound within a chemical database. Also, in organic mathematical chemistry graph isomorphism testing is useful for generation of molecular graphs and for computer synthesis.\n\nChemical database search is an example of graphical data mining, where the graph canonization approach is often used. In particular, a number of identifiers for chemical substances, such as SMILES and InChI, designed to provide a standard and human-readable way to encode molecular information and to facilitate the search for such information in databases and on the web, use canonization step in their computation, which is essentially the canonization of the graph which represents the molecule.\n\nIn electronic design automation graph isomorphism is the basis of the Layout Versus Schematic (LVS) circuit design step, which is a verification whether the electric circuits represented by a circuit schematic and an integrated circuit layout are the same.\n\n\n\n\n"}
{"id": "13945403", "url": "https://en.wikipedia.org/wiki?curid=13945403", "title": "Horizons: Exploring the Universe", "text": "Horizons: Exploring the Universe\n\nHorizons: Exploring the Universe is an astronomy textbook that was written by Michael A. Seeds and Dana E. Backman. It is in its 13th edition (), and is used in some colleges as a guide book for introductory astronomy classes. It covers all major ideas in astronomy, from the apparent magnitude scale, to the Cosmic Microwave Background Radiation, to gamma ray bursts.\n"}
{"id": "40769990", "url": "https://en.wikipedia.org/wiki?curid=40769990", "title": "Illite crystallinity", "text": "Illite crystallinity\n\nIllite crystallinity is a technique used to classify low-grade metamorphic activity in pelitic rocks. Determining the \"illite crystallinity index\" allows geologists to designate what metamorphic facies and metamorphic zone the rock was formed in and to infer what temperature the rock was formed. Several crystallinity indices have been proposed in recent years, but currently the Kübler index is being used due to its reproducibility and simplicity. The Kübler index is experimentally determined by measuring the full width at half maximum for the X-ray diffraction reflection peak along the (001) crystallographic axis of the rock sample. This value is an indirect measurement of the thickness of illite/muscovite packets which denote a change in metamorphic grade. \nThe method can be used throughout the field of geology in areas such as the petroleum industry, plate tectonics.\n\nAs stated above, the Kübler index was not always the preferred index for illite crystallinity studies in the past. Prior to the introduction of the Kübler index, there were several other indices used to classify low grade metamorphic rocks. Two of the more popular methods of the past are the Weaver index and the Weber index, introduced in 1960 and 1972 respectively. These studies consist of mainly the same types of methods but vary in their expression of ratio measurements. The Kübler index, introduced by Bernard Kübler in 1964 for petroleum exploration and improved on in later years, has come to be the go-to index for illite crystallinity based on its reproducibility and simplicity.\n\nIllite crystallinity is useful when trying to determine what type of metamorphic conditions a rock was subjected to during its formation. Illite crystallinity can be used to trace the low grade metamorphic transition from zeolite facies to greenschist facies (diagenetic zone to epizone). This change is flagged by the change of thin illite grains to thicker illite/muscovite grains. This low grade metamorphic technique can also be put into use when there is an absence in change of mineral structure which applies to higher grade metamorphism. Early use of illite crystallinity was in the petroleum industry to determine the transition from a dry gas phase to an unproductive rock. Recently, this technique has expanded in the field and now is used in areas such as palaeotectonics and geodynamic reconstructions.\n\nRock sample preparation for illite crystallinity can vary slightly, but boils down to basically the same steps. Although for accurate returns in testing, consistency of sample preparation is a must. General sample preparation for illite crystallinity is as follows:\n\n\nThe sample is first broken down, using the steps above, and prepared for XRD analysis. Results from the XRD are then compared to pre-established values assigned to metamorphic zones/metamorphic facies. The targets of the results are the peaks on the XRD plots. Width of the illite XRD peak at one half of its height is collected and recorded with units of ∆ °2θ (XRD angle). Comparison and classification of metamorphic facies is then determined for the sample.\n\nIllite (KAl[AlSiO](OH)) and muscovite (KAl(AlSiO)(OH)) are both phyllosilicates similar in structure and composition. Illite is made up of thin, 1 nm, layers which are made up of tetrahedral-octahedral-tetrahedral (TOT) sheets. Illite contains more silicon, iron and magnesium than muscovite, as well as less tetrahedral aluminium and interlayered potassium.\n\nX-Ray diffraction plots provide information on angles and intensities of refracted beams which allow scientists to construct a 3D model of the crystalline structure. The focus of an illite crystallinity XRD plot is the main peak. Width of the peak at one half of its height is measured and this angle (recorded with units of ∆ °2θ), can be plotted on a chart with metamorphic zones and facies like the one in figure 1. If the illite crystallinity values fall in the 0-0.25 °2θ range, it corresponds with a metamorphic epizone or greenschist facies. If the illite crystallinity values fall in the 0.25-0.30 °2θ range, it corresponds with a metamorphic high anchizone or prehnite-pumpellyite facies. If the illite crystallinity values fall in the 0.30-0.42 °2θ range, it corresponds with a metamorphic low anchizone or prehnite-pumpellyite facies. If the illite crystallinity values fall in the 0.42-1.0 °2θ range, it corresponds with a metamorphic deep diagenetic zone or zeolite facies. If the illite crystallinity is > 1.0, it corresponds with a metamorphic shallow diagenetic zone or zeolite facies.\n\nGenerally, width of the diffraction peak can be related to c-axis parallel thickness of the illite crystallites. Thin packets produce broader peaks and thick packets return more narrow peaks. This is based on the destructive interference of the thick packets or the lack of interference in the thin packets, which cause this difference.\n"}
{"id": "40903837", "url": "https://en.wikipedia.org/wiki?curid=40903837", "title": "Illusory truth effect", "text": "Illusory truth effect\n\nThe illusory truth effect (also known as the validity effect, truth effect or the reiteration effect) is the tendency to believe information to be correct after repeated exposure. This phenomenon was first identified in a 1977 study at Villanova University and Temple University. When truth is assessed, people rely on whether the information is in line with their understanding or if it feels familiar. The first condition is logical as people compare new information with what they already know to be true. Repetition makes statements easier to process relative to new, unrepeated, statements, leading people to believe that the repeated conclusion is more truthful. The illusory truth effect has also been linked to \"hindsight bias\", in which the recollection of confidence is skewed after the truth has been received.\n\nIn a 2015 study, researchers discovered that familiarity can overpower rationality and that repetitively hearing that a certain fact is wrong can affect the hearer's beliefs. Researchers attributed the illusory truth effect's impact on participants who knew the correct answer to begin with, but were persuaded to believe otherwise through the repetition of a falsehood, to \"processing fluency\".\n\nThe illusory truth effect plays a significant role in such fields as election campaigns, advertising, news media, and political propaganda.\n\nThe effect was first named and defined following the results in a study from 1977 at Villanova University and Temple University where participants were asked to rate a series of trivia statements as true or false. On three occasions, Lynn Hasher, David Goldstein, and Thomas Toppino presented the same group of college students with lists of sixty plausible statements, some of them true and some of them false. The second list was distributed two weeks after the first, and the third two weeks after that. Twenty statements appeared on all three lists; the other forty items on each list were unique to that list. Participants were asked how confident they were of the truth or falsity of the statements, which concerned matters about which they were unlikely to know anything. (For example, \"The first air force base was launched in New Mexico.\" Or \"Basketball became an Olympic discipline in 1925.\") Specifically, the participants were asked to grade their belief in the truth of each statement on a scale of one to seven. While the participants' confidence in the truth of the non-repeated statements remained steady, their confidence in the truth of the repeated statements increased from the first to the second and second to third sessions, with an average score for those items rising from 4.2 to 4.6 to 4.7. The conclusion made by the researchers, who were from Villanova and Temple universities, was that repeating a statement makes it more likely to appear factual.\n\nIn 1989, Hal R. Arkes, Catherine Hackett, and Larry Boehm replicated the original study, with similar results showing that exposure to false information changes the perceived truthfulness and plausibility of that information.\n\nThe effect works because when people assess truth, they rely on whether the information agrees with their understanding or whether it feels familiar. The first condition is logical as people compare new information with what they already know to be true and consider the credibility of both sources. However, researchers discovered that familiarity can overpower rationality—so much so that repetitively hearing that a certain fact is wrong can have a paradoxical effect.\n\nAt first, the truth effect was believed to occur only when individuals are highly uncertain about a given statement. Psychologists also assumed that \"outlandish\" headlines wouldn't produce this effect however, recent research shows the illusory truth effect is indeed at play with false news. This assumption was challenged by the results of a 2015 study by Lisa K. Fazio, Nadia M. Brasier, B. Keith Payne, and Elizabeth J. Marsh. Published in the \"Journal of Experimental Psychology\", the study suggested that the truth effect can influence participants who actually knew the correct answer to begin with, but who were swayed to believe otherwise through the repetition of a falsehood. For example, when participants encountered on multiple occasions the statement \"A sari is the name of the short plaid skirt worn by Scots,\" some of them were likely to come to believe it was true, even though these same people were able to correctly answer the question \"What is the name of the short pleated skirt worn by Scots?\"\n\nAfter replicating these results in another experiment, Fazio and her team attributed this curious phenomenon to \"processing fluency\", a term that describes the facility with which people comprehend statements. \"Repetition,\" explained the researcher, \"makes statements easier to process (i.e. fluent) relative to new statements, leading people to the (sometimes) false conclusion that they are more truthful.\" When an individual hears something for a second or third time, their brain responds faster to it and misattributes that fluency as a signal for truth.\n\nIn a 1997 study, Ralph Hertwig, Gerd Gigerenzer, and Ulrich Hoffrage linked the truth effect to the phenomenon known as \"hindsight bias\", described as a situation in which the recollection of confidence is skewed after the truth or falsity has been received. They have described the truth effect (which they call \"the reiteration effect\") as a subset of hindsight bias.\n\nIn a 1979 study, participants were told that repeated statements were no more likely to be true than unrepeated ones. Despite this warning, the participants perceived repeated statements as being more true than unrepeated ones.\n\nStudies in 1981 and 1983 showed that information deriving from recent experience tends to be viewed as \"more fluent and familiar\" than new experience. A 2011 study by Jason D. Ozubko and Jonathan Fugelsang built on this finding by demonstrating that, generally speaking, information retrieved from memory is \"more fluent or familiar than when it was first learned\" and thus produces an illusion of truth. The effect grew even more pronounced when statements were repeated twice and yet more pronounced when they were repeated four times. The researchers thus concluded that memory retrieval is a powerful method for increasing the so-called validity of statements and that the illusion of truth is an effect that can be observed without directly polling the factual statements in question.\n\nA 1992 study by Ian Maynard Begg, Ann Anas, and Suzanne Farinacci suggested that a statement will seem true if the information seems familiar.\n\nA 2012 experiment by Danielle C. Polage showed that some participants exposed to false news stories would go on to have false memories. The conclusion was that repetitive false claims increase believability and may also result in errors.\n\nIn a 2014 study, Eryn J. Newman, Mevagh Sanson, Emily K. Miller, Adele Quigley-McBride, Jeffrey L. Foster, Daniel M. Bernstein, and Maryanne Garry asked participants to judge the truth of statements attributed to various people, some of whose names were easier to pronounce than others. Consistently, statements by persons with easily pronounced names were viewed as being more truthful than those with names that were harder to pronounce. The researchers' conclusion was that subjective, tangential properties can matter when people evaluate sourced information.\n\nAlthough the truth effect has been demonstrated scientifically only in recent years, it is a phenomenon with which people have been familiar for millennia. One study notes that the Roman statesman Cato closed each of his speeches with a call to destroy Carthage (\"Ceterum censeo Carthaginem esse delendam\"), knowing that the repetition would breed agreement, and that Napoleon reportedly \"said that there is only one figure in rhetoric of serious importance, namely, repetition\", whereby a repeated affirmation fixes itself in the mind \"in such a way that it is accepted in the end as a demonstrated truth\". Others who have taken advantage of the truth effect have included Quintilian, Ronald Reagan, and Marcus Antonius in Shakespeare's \"Julius Caesar\".\n\nThe truth effect plays a significant role in various fields of activity. During election campaigns, false information about a candidate, if repeated in TV commercials, can cause the public to believe it. Similarly, advertising that repeats unfounded claims about a product may boost sales because some viewers may come to think that they heard the claims from an objective source. The truth effect is also used in news media and is a staple of political propaganda. A kayaking expert has pointed out that it is an accepted fact that when kayaking on the ocean or the Great Lakes, one should use a kayak at least 16 feet long. But this is not true; the best length for a kayak depends on a variety of factors.\n\n"}
{"id": "20754461", "url": "https://en.wikipedia.org/wiki?curid=20754461", "title": "Information Rules", "text": "Information Rules\n\nInformation Rules is a 1999 book by Carl Shapiro and Hal Varian applying traditional economic theories to modern information-based technologies. The book examines commercial strategies appropriate to companies that deal in information, given the high \"first copy\" and low \"subsequent copy\" costs of information commodities, such as music CDs or original texts.\n\nThe book examines competing standards, and how a company might influence widespread consumer acceptance of one over another, such as VHS versus Betamax, or HD DVD versus Blu-ray. The book mentions possible business strategies of such publishers as Encyclopædia Britannica who have to confront how to stay viable as technology changes the value and availability of information.\n\n\n"}
{"id": "58832020", "url": "https://en.wikipedia.org/wiki?curid=58832020", "title": "International Science Council", "text": "International Science Council\n\nThe International Science Council (ISC) is a non-governmental organization that unites scientific bodies at various levels across the social and natural sciences. The ISC was formed in July 2018 by the merger of the International Council for Science and the International Social Science Council (ISSC), making it one of the largest organisations of this type. \n\nDaya Reddy from the University of Cape Town was elected inaugural president of the board. Heide Hackmann serves as CEO. \n"}
{"id": "620985", "url": "https://en.wikipedia.org/wiki?curid=620985", "title": "International studies", "text": "International studies\n\nInternational Studies (IS) generally refers to the specific university degrees and courses which are concerned with the study of ‘the major political, economic, social, and cultural issues that dominate the international agenda’. Predominant topics are politics, economics and law on a global level. The term itself can be more specifically defined as ‘the contemporary and historical understanding of global societies, cultures, languages and systems of government and of the complex relationships between them that shape the world we live in’. The terms and concepts of International Studies and international relations are strongly related; however, International relations focus more directly on the relationship between countries, whereas International Studies can encompass all phenomena which are globally oriented.\n\nThe history of the discipline of International Studies is strongly linked with the history of the study of international relations, as described in the International Relations entry. However, the study of International Studies as a separate entity to International Relations emerged throughout the 20th century, as an increasingly complex world began to be influenced by globalization, and a greater number of issues emerged (rather than only inter-country relations). The discipline was greatly influenced by the establishment of the International Studies Association, which was established in 1959 by a ‘group of academics and practitioners’ with the aim of ‘seeking to pursue mutual interests in world affairs through the organization of a professional association’. The establishment of the association reflected the increasing interest in global issues and reflected the need for international academic dialogue.\nIn 2008, the third OCIS conference (Oceanic Conference on International Studies) was held at the University of Queensland’. The conference brought together over 200 academics, with the keynote speaker Andrew Linklater (the Woodrow Wilson Professor of International Politics at the University of Aberystwyth in Wales) noting ‘how vibrant and intellectually stimulating International Studies now is in Australia’’.\nThe increasing popularity of the discipline in Australia led to the International Studies Association to establish an Asia-Pacific Regional Section of the ISA at the University of Queensland in 2009’, which was seen as an ‘indication of the growth of this area’’ in Australia.\n\nInternational Studies is sometimes also known as global studies. The terms can be used interchangeably and may be influenced by left vs right inclinations.\n\nMany educational institutions have developed International Studies degrees and courses in order to engage students with the increasing number of issues and phenomena which have arisen in an increasingly globalized world. As such, most education providers justify the need for the degrees by relating the increasing importance of the discipline with real-world situations and employment opportunities.\nFor example, the University of Technology Sydney states that the purpose of their International Studies degree is to ‘prepare graduates for careers and contributions in a world of social and cultural diversity being transformed by globalisation, allowing students to draw connections between global phenomena and local practices in work and life’.\nOften, universities will relate the study of International studies with other industries. Monash University describes the relevance for International Studies; ‘as the world globalizes and nations and economies become more integrated, it is important to understand our world and the ideas and beliefs of our neighbors and trading partners. In order to compete in the international marketplaces of products, ideas and knowledge we need to understand and respect the cultures and beliefs of others.\n\nAt many universities, International Studies is offered in both undergraduate and postgraduate pathways. As an undergraduate degree, the discipline is most often offered as part of an Arts Degree, as either a minor or major of straight Arts Degrees or as specialist Arts Degrees. It is also often offered as a postgraduate degree as an honors or masters as a progression from the undergraduate degrees offered by the various institutions.\n\nThe International Studies discipline is usually offered as either part of an arts degree or as a specialist arts degree. As such, students are able to select from a very broad range of subjects to undertake. However, some areas of study which are regularly offered include:\n\n As discussed, the study of the International Studies discipline in Australia occurs mostly within universities and generally approaches the subject as a holistic study of international affairs and phenomena.\n\nThere are several International Studies programs in Canada that offer both undergraduate and graduate degrees. The Glendon College International Studies Program, the School for International Studies at Simon Fraser University, the Munk School of Global Affairs at the University of Toronto, the Balsillie School of International Affairs, the Centre for Global Studies at Huron University College, Centre d'études et de Recherches Internationales de l'Université de Montréal, International Studies degree at the University of Regina, the International Studies MA Program at the University of Northern British Columbia and the Institut québécois des hautes études internationales are the leading programs.\n\nIn Chile there is two undergraduated program. The oldest, International Studies undergraduate program which is offered by the Universidad de Santiago de Chile. This programme emphasises four core areas: International Relations, Methodology, International Trade, and Defense and Security. This programme has also been recognized through the participation of their students in different simulations of the United Nations System, like the National Model of United Nations NMUN in New York and Kobe 2016, and in the Modelo Naciones Unidas para Latinoamerica y el Caribe(MONULAC) in Antigua. The university opened this undergraduate programme in 2008 and it represents a very promising development in the study of the Social Sciences in Latin America. The Instituto de Estudios Avanzados(IDEA) of the same university also offers a Masters in International Studies.\nIn 2018, the Universidad de Chile open a same programm in cooperation between different academic units like Facso and \n\nIn the UK the British International Studies Association (BISA) develops and promotes the study of International Studies, Global Politics and related subjects through teaching, research and the facilitation of contact between scholars. International Studies is often related to or attached to the study in International Relations. At the University of Oxford, the Centre for International Studies \"exists to promote and advance research in International Relations\". In this sense the use of the term \"International Studies\" differs to that of the Australian use of the term in that it is tied to the discipline of International Relations, rather than addressing them as separate entities.\nThe Institute of Development Studies (IDS)] based in Sussex, is a leading global charity for international development research, teaching, and communications.\n\nThere exist a number of institutions which promote International Studies in the United States of America. The Centre for Strategic & International Studies is a foreign policy think tank which aims to ‘provide strategic insights and policy solutions to decisionmakers in government, international institutions, the private sector, and civil society’. The Freeman Spogli Institute for International Studies is a research center based at Stanford University which is a ‘primary center for innovative research on major international issues and challenges’. Both institutions focus primarily on the study of international affairs and relations in relation to US foreign policy, and therefore differ in the Australian approach to International Studies. Alternatively, the undergraduate International Studies program at St. John Fisher College in Rochester, NY is a holistic program that more closely follows the Australian model. <http://www.sjfc.edu/academics/arts-science/departments/international>\n\nAs stated, many institutions attempt to promote their International Studies degrees by promoting the career prospects for graduates. The University of Melbourne has stated that graduates of its International Studies major will be ‘attractive to prospective employees in the public and private sectors including international inter-governmental and non-governmental organizations’. Similarly, RMIT University has stated that ‘the degree prepares you to apply your knowledge of globalization, language and culture in international workplace settings’, such as ‘business, government and non-government organizations in a range of areas’.\n\nThe discipline is also working under a premise that employment opportunities in the field of International Studies will steadily increase with the increasing level of interconnectedness which is occurring as a result of globalisation; ‘Opportunities for positions requiring international knowledge and skills are increasing and have created a need for graduates who are highly skilled, interculturally attuned and able to think and act globally/locally, as well as being bilingual’.\n\n\n\n"}
{"id": "14439044", "url": "https://en.wikipedia.org/wiki?curid=14439044", "title": "Kraków School of Mathematics and Astrology", "text": "Kraków School of Mathematics and Astrology\n\nThe Kraków School of Mathematics and Astrology () was an influential mid-to-late-15th-century group of mathematicians and astrologers at the University of Kraków (later \"Jagiellonian University\").\n\n\n"}
{"id": "54061907", "url": "https://en.wikipedia.org/wiki?curid=54061907", "title": "Lempel-Ziv complexity", "text": "Lempel-Ziv complexity\n\nThe Lempel-Ziv complexity was first presented in the article \"On the Complexity of Finite Sequences\" (IEEE Trans. On IT-22,1 1976), by two Israeli computer scientists, Abraham Lempel and Jacob Ziv. This complexity measure is related to Kolmogorov complexity, but the only function it uses is the recursive copy (i.e., the shallow copy).\n\nThe underlying mechanism in this complexity measure is the starting point for some algorithms for lossless data compression, like LZ77, LZ78 and LZW. Even though it is based on an elementary principle of words copying, this complexity measure is not too restrictive in the sense that it satisfies the main qualities expected by such a measure: sequences with a certain regularity do not have a too large complexity, and the complexity grows as the sequence grows in length and irregularity.\n\nThe Lempel-Ziv complexity can be used to measure the repetitiveness of binary sequences and text, like song lyrics or prose.\n\nLet S be a binary sequence, of length n, for which we have to compute the Lempel-Ziv complexity, denoted C(S). The sequence is read from the left.\n\nImagine you have a delimiting line, which can be moved in the sequence during the calculation. At first, this line is set just after the first symbol, at the beginning of the sequence. This initial position is called position 1, from where we have to move it to a position 2, which is considered the initial position for the next step (and so on). We have to move the delimiter (starting in position 1) the further possible to the right, so that the sub-word between position 1 and the delimiter position be a word of the sequence that starts before the position 1 of the delimiter.\n\nAs soon as the delimiter is set on a position where this condition is not met, we stop, move the delimiter to this position, and start again by marking this position as a new initial position (i.e., position 1). Keep iterating until the end of the sequence. The Lempel-Ziv complexity corresponds to the number of iterations needed to finish this procedure.\n\nSaid differently, the Lempel-Ziv complexity is the number of different sub-strings (or sub-words) encountered as the binary sequence is viewed as a stream (from left to right).\n\nThe method proposed by Lempel and Ziv uses three notions: reproducibility, producibility and exhaustive history of a sequence, that we defined here.\n\nLet S be a binary sequence of length n (i.e., n symbols taking value 0 or 1). Let S(i,j), with formula_1, be the sub-word of S from index i to index j (if j\\exists j<{\\text{l(S), s.t. S(1,j) = Q .}}</math>\n\nOn the one hand, a sequence S of length n is said to be reproducible from its prefix S(1,j) when S(j+1,n) is a sub-word of S(1,n-1). This is denoted S(1,j)→S.\n\nSaid differently, S is reproducible from its prefix S(1,j) if the rest of the sequence, S(j+1,n), is nothing but a copy of another sub-word (starting at an index i < j+1) of S(1,n-1).\n\nTo prove that the sequence S can be reproduced by one of its prefix S(1,j), you have to show that:\n\nformula_2\n\nOn the other hand, the producibility, is defined from the reproducibility : a sequence S is producible from its prefix S(1,j) if S(1,n-1) is reproducible from S(1,j). This is denoted S(1,j)⇒S. Said differently, S(j+1,n-1) has to be a copy of another sub-word of S(1,n-2). The last symbol of S can be a new symbol (but could not be), possibly leading to the production of a new sub-word (hence the term producibility).\n\nFrom the definition of productibility, the empty string Λ=S(1,0) ⇒ S(1,1). So by a recursive production process, at step i we have S(1,hi) ⇒ S(1,hi+1), so we can build S from its prefixes. And as S(1,i) ⇒ S(1,i+1) (with hi+1 =hi + 1) is always true, this production process of S takes at most n=l(S) steps. Let m, formula_3, be the number of steps necessary for this product process of S. S can be written in a decomposed form, called history of S, and denoted H(S), defined like this:\n\nformula_4\nformula_5\n\nA component of S, Hi(S), is said to be exhaustive if S(1,hi) is the longest sequence produced by S(1,hi-1) (i.e., S(1,hi-1) ⇒ S(1,hi)) but so that S(1,hi-1) does not produce S(1,hi) (denoted ).formula_6 The index p which allows to have the longest production is called pointer.\n\nThe history of S is said to be exhaustive if all its component are exhaustive, except possibly the last one. From the definition, one can show that any sequence S has only one exhaustive history, and this history is the one with the smallest number of component from all the possible histories of S. Finally, the number of component of this unique exhaustive history of S is called the Lempel-Ziv complexity of S.\n\nHopefully, there exists a very efficient method for computing this complexity, in a linear number of operation (formula_7 for formula_8 length of the sequence S).\n\nA formal description of this method is given by the following algorithm:\n// S is a binary sequence of size n\ni := 0\nC := 1\nu := 1\nv := 1\nvmax := v\nwhile u + v <= n do\nend while\nif v != 1 then\nend si\n\n\n\n"}
{"id": "15352645", "url": "https://en.wikipedia.org/wiki?curid=15352645", "title": "List of Utah state symbols", "text": "List of Utah state symbols\n\nThe U.S. state of Utah has 26 official symbols, as designated by the Utah State Legislature, and three unofficial symbols. All official symbols, except the Great Seal, are listed in Title 63G of Utah Code. In 1896, Utah became a state, and on April 3 the Utah legislature, in its first regular session, adopted its first symbol, the Great Seal of the State of Utah.\n\nMany unique symbols of Utah are related to Utah's pioneer heritage, such as the California gull, the beehive, the dutch oven and the Sego Lily. Utah has symbols that are used by multiple states. For example, the honey bee, Utah's state insect, is also a symbol of Arkansas, Georgia, Kansas, Louisiana, Maine, Mississippi, Missouri, Nebraska, New Jersey, North Carolina, West Virginia and Wisconsin.\n\n\n"}
{"id": "40671039", "url": "https://en.wikipedia.org/wiki?curid=40671039", "title": "List of aspect ratios of national flags", "text": "List of aspect ratios of national flags\n\nThe following table shows the aspect ratio (height to width ratio) of national flags used by countries and dependencies. Variant flags such as ensigns are listed in the \"Alternative flags\" column, if they have different proportions from the national flag. Territories without an official flag distinct from that of their controlling country are excluded.\n\nThe ratios used most commonly are 2:3, used by 89 of 195 sovereign states, and 1:2, used by 54 sovereign states. Most dependencies and former colonies use the same proportions as their mother countries: all British Overseas Territories use 1:2 ratios, while the flags of most former and current Dutch and French areas have 2:3 proportions.\n\nFor comparison, the ratios are also given as a decimal number, which is the flag width divided by its height (e.g. a 2:3 flag has a decimal ratio of = 1.5). Flags with irrational ratios have only a decimal approximation, and have the exact form given in the \"Notes\" column.\n\nLegend:\n\n\nClick on the \"Flag\" header to sort by status: first sovereign states, then \"de facto\" countries, then dependencies.\n\n"}
{"id": "1047847", "url": "https://en.wikipedia.org/wiki?curid=1047847", "title": "List of holidays by country", "text": "List of holidays by country\n\nBelow are lists of public holidays by country.\n\n\n\n\n"}
{"id": "18749240", "url": "https://en.wikipedia.org/wiki?curid=18749240", "title": "List of islands by population density", "text": "List of islands by population density\n\nThe following is a list of islands, sorted by population density, and including islands that connect to other island or inland with land mean of transportation (e.g. bridge or tunnel).\n\nThere are numerous uninhabited or deserted islands. The largest uninhabited island in the world is Devon Island in Canada.\n\n"}
{"id": "2258557", "url": "https://en.wikipedia.org/wiki?curid=2258557", "title": "List of psychotherapies", "text": "List of psychotherapies\n\nThis is an alphabetical list of psychotherapies.\n\nSee the main article psychotherapy for a description of what psychotherapy is and how it developed (see also counseling, and the list of counseling topics).\n\nThis list contains some approaches that may not call themselves a psychotherapy but have a similar aim, of improving mental health and well being through talk and other means of communication.\n\nIn the 20th century, a great number of psychotherapies were created. All of these face continuous change in popularity, methods and effectiveness. Sometimes they are self-administered, either individually, in pairs, small groups or larger groups. However, a professional practitioner will usually use a combination of therapies and approaches, often in a team treatment process that involves reading/talking/reporting to other professional practitioners.\n\nThe older established therapies usually have a code of ethics, professional associations, training programs, and so on. The newer and innovative therapies may not yet have established these structures or may not wish to.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33524353", "url": "https://en.wikipedia.org/wiki?curid=33524353", "title": "List of ubiquitous computing research centers", "text": "List of ubiquitous computing research centers\n\nThis is a list of notable institutions who claim to have a focus on Ubiquitous computing sorted by country:\n\n\n\n\n\n\n\n"}
{"id": "930267", "url": "https://en.wikipedia.org/wiki?curid=930267", "title": "MRK (visual artist)", "text": "MRK (visual artist)\n\nMRK (Markos Kay) is a visual artist, creative director, illustrator and lecturer based in London, best known for his artificial life video art experiment \"aDiatomea\" (2007), a permanent exhibit at the Phyletic Museum in Jena, Germany. His 3D generative short \"The Flow\" (2011) has been shown internationally as part of \"Resonance\" a collaboration of over 30 independent visual artists and sound designers. MRK's films have been exhibited worldwide, including OFFF Barcelona, Pause Fest Melbourne, Onedotzero London, Stroke Artfair Berlin and Viedram Rome.\n\nMRK graduated with a Master of Arts in Communication Design from Central St. Martins in 2007 and is currently working as a creative director, motion designer and lecturer.\n\nIn 2008, MRK released “aDiatomea,” an experimental piece that simulates diatoms consisting of realistic 3d generated diatoms. Each variant of these mathematical creatures is classified in actual taxonomies, giving Victorian diatom art a 21st-century redux. Granular sound is 'injected' into the diatoms affecting their form and movement, creating a dynamic system. In 2012, \"aDiatomea\" was part of the official selection at Imagine Science Films in New York.\n\nIn 2011, he took part in \"Resonance,\" an international collaboration of over 30 independent visual artists and sound designers, with the short film \"The Flow.\" Resonance premiered at the OFFF 2011 festival, and went on to become an international success. The Flow deals with the idea of emergence and supervenience in the quantum and physical worlds. In 2012, a segment of \"The Flow\" was featured in Breaking Bad season 5, episode 3. This coincided with the release of an annotated and extended version of the film. \"The Flow\" composers Echoic Audio won Best Sound Design at the Van d'Or Awards 2012.\n\nDuring 2012, in collaboration with seeper, he directed large scale projection mapping events including the launch of Titanic Belfast, Birmingham's Wings of Desire and the Vimeo Awards 2012. Other commercial endeavours included work for the likes of Warner Bros., MTV, Nickelodeon and Channel 4. He also collaborated in the direction of the main titles for Dutch digital arts festival Playgrounds, which premiered in Amsterdam in November 2012. Later on that year he took part in another international collaboration \"The Powers Project,\" a visual re-imagining of the Eames classic Powers of Ten. His segment was the first to be released as part of the promotion of the film.\n\nMRK's work fuses science with visual art. He deals with themes of emergence, evolution and complexity, as well as artificial life and generative art processes. He is inspired by biological forms and processes, life and natural cycles, and deconstruction is a major influence. Other areas of interest include: molecular biology, digital physics, information theory and complexity science.\n\nNotable Works\n\n\n"}
{"id": "40941170", "url": "https://en.wikipedia.org/wiki?curid=40941170", "title": "Off-flavour", "text": "Off-flavour\n\nOff-flavours or off-flavors (see spelling differences) are taints in food products caused by the presence of undesirable compounds. They can originate in raw materials, from chemical changes during food processing and storage, and from micro-organisms. Off-flavours are a recurring issue in drinking water supply and many food products.\n\nWater bodies are often affected by geosmin and 2-methylisoborneol, affecting the flavour of water for drinking and of fish growing in that water. Haloanisoles similarly affect water bodies, and are a recognised cause of off-flavour in wine. Cows grazing on weeds such as wild garlic can produce a ‘weedy’ off-flavour in milk.\n\nMany more examples can be seen throughout food production sectors including in oats, coffee, glucose syrup and brewing.\n"}
{"id": "1196709", "url": "https://en.wikipedia.org/wiki?curid=1196709", "title": "On Intelligence", "text": "On Intelligence\n\nOn Intelligence: How a New Understanding of the Brain will Lead to the Creation of Truly Intelligent Machines is a 2004 book by Palm Pilot-inventor Jeff Hawkins with \"New York Times\" science writer Sandra Blakeslee. The book explains Hawkins' memory-prediction framework theory of the brain and describes some of its consequences.\n\nHawkins outlines the book as follows:\n\nThe first chapter is a brief history of Hawkins' interest in neuroscience juxtaposed against a history of artificial intelligence research. Hawkins uses a story of his failed application to the Massachusetts Institute of Technology to illustrate a conflict of ideas. Hawkins believed (and ostensibly continues to believe) creating true artificial intelligence will only be possible with intellectual progress in the discipline of neuroscience. Hawkins writes that the scientific establishment (as symbolized by MIT) has historically rejected the relevance of neuroscience to artificial intelligence. Indeed, some artificial intelligence researchers have \"[taken] pride in ignoring neurobiology\" (p. 12).\n\nHawkins is an electrical engineer by training, and a neuroscientist by inclination. He used electrical engineering concepts as well as the studies of neuroscience to formulate his framework. In particular, Hawkins treats the propagation of nerve impulses in our nervous system as an encoding problem, specifically, a future predicting state machine, similar in principle to feed-forward error-correcting state machines.\n\nHawkins' basic idea is that the brain is a mechanism to predict the future, specifically, hierarchical regions of the brain predict their future input sequences. Perhaps not always far in the future, but far enough to be of real use to an organism. As such, the brain is a feed forward hierarchical state machine with special properties that enable it to learn.\n\nThe state machine actually controls the behavior of the organism. Since it is a feed forward state machine, the machine responds to future events predicted from past data.\n\nThe hierarchy is capable of memorizing frequently observed sequences (Cognitive modules) of patterns and developing invariant representations. Higher levels of the cortical hierarchy predict the future on a longer time scale, or over a wider range of sensory input. Lower levels interpret or control limited domains of experience, or sensory or effector systems. Connections from the higher level states predispose some selected transitions in the lower-level state machines.\n\nHebbian learning is part of the framework, in which the event of learning physically alters neurons and connections, as learning takes place.\n\nVernon Mountcastle's formulation of a cortical column is a basic element in the framework. Hawkins places particular emphasis on the role of the interconnections from peer columns, and the activation of columns as a whole. He strongly implies that a column is the cortex's physical representation of a state in a state machine.\n\nAs an engineer, any specific failure to find a natural occurrence of some process in his framework does not signal a fault in the memory-prediction framework \"per se\", but merely signals that the natural process has performed Hawkins' functional decomposition in a different, unexpected way, as Hawkins' motivation is to create intelligent machines. For example, for the purposes of his framework, the nerve impulses can be taken to form a temporal sequence (but phase encoding could be a possible implementation of such a sequence; these details are immaterial for the framework).\n\nHis predictions use the visual system as a prototype for some example predictions, such as Predictions 2, 8, 10, and 11. Other predictions cite the auditory system ( Predictions 1, 3, 4, and 7).\n\n1. In all areas of cortex, Hawkins (2004) predicts \"we should find \"anticipatory cells\"\", cells that fire in anticipation of a sensory event.\n\n2. In primary sensory cortex, Hawkins predicts, for example, \"we should find anticipatory cells in or near V1, at a precise location in the visual field (the scene)\". It has been experimentally determined, for example, after mapping the angular position of some objects in the visual field, there will be a one-to-one correspondence of cells in the scene to the angular positions of those objects. Hawkins predicts that when the features of a visual scene are known in a memory, anticipatory cells should fire \"before\" the actual objects are seen in the scene.\n\n3. In layers 2 and 3, predictive activity (neural firing) should stop propagating at specific cells, corresponding to a specific prediction. Hawkins does not rule out anticipatory cells in layers 4 and 5.\n\n4. Learned sequences of firings comprise a representation of \"temporally constant invariants\". Hawkins calls the cells which fire in this sequence \"name cells\". Hawkins suggests that these \"name cells\" are in layer 2, physically adjacent to layer 1. Hawkins does not rule out the existence of layer 3 cells with dendrites in layer 1, which might perform as \"name cells\".\n\n5. By definition, a \"temporally constant invariant\" will be active during a learned sequence. Hawkins posits that these cells will remain active for the duration of the learned sequence, even if the remainder of the cortical column is shifting state. Since we do not know the encoding of the sequence, we do not yet know the definition of \"ON\" or \"active\"; Hawkins suggests that the ON pattern may be as simple as a simultaneous AND (i.e., the name cells simultaneously \"light up\") across an array of name cells.\n\n6. Hawkins' novel prediction is that certain cells are inhibited during a learned sequence. A class of cells in layers 2 and 3 should NOT fire during a learned sequence, the axons of these \"exception cells\" should fire \"only if a local prediction is failing\". This prevents flooding the brain with the usual sensations, leaving only exceptions for post-processing.\n\n7. If an unusual event occurs (the learned sequence fails), the \"exception cells\" should fire, propagating up the cortical hierarchy to the hippocampus, the repository of new memories.\n\n8. Hawkins predicts a cascade of predictions, when recognition occurs, propagating down the cortical column (with each saccade of the eye over a learned scene, for example).\n\n9. Pyramidal cells should be capable of detecting coincident events on thin dendrites, even for a neuron with thousands of synapses. Hawkins posits a temporal window (presuming time-encoded firing) which is necessary for his theory to remain viable.\n\n10. Hawkins posits, for example, that if the inferotemporal (IT) layer has learned a sequence, that eventually cells in V4 will also learn the sequence.\n\n11. Hawkins predicts that \"name cells\" will be found in all regions of the cortex.\n\n\n\n"}
{"id": "870529", "url": "https://en.wikipedia.org/wiki?curid=870529", "title": "Outer-grazer", "text": "Outer-grazer\n\nOuter-grazer and inner-grazer are configurations of heliocentric orbit. All six diagrams show the Sun (the orange dot) in the middle and a putative planet's orbital band (in yellow). The latter is a ring whose inner radius is the planet's perihelion and its outer radius the aphelion.\n\n\n"}
{"id": "58949425", "url": "https://en.wikipedia.org/wiki?curid=58949425", "title": "Perdita Barran", "text": "Perdita Barran\n\nPerdita Elizabeth Barran is a Professor of Mass Spectrometry at the University of Manchester. She is Director of the Michael Barber Centre for Collaborative Mass Spectrometry. She develops and applies ion-mobility spectrometry–mass spectrometry to the study of molecule structure and is searching for biomarkers for Parkinson's Disease. She co-leads the mass spectrometry theme for the Rosalind Franklin Institute. She was awarded the 2009 Joseph Black award from the Royal Society of Chemistry Analytical Division.\n\nBarran went to school at Godolphin and Latymer School. She moved to the University of Manchester to study chemistry, graduating in 1994. She joined the University of Sussex for her graduate studies, working with Harry Kroto and Tony Stace. \n\nBarran stayed with Stace for three years after completing her PhD in 1998. In 2001 Barran joined the University of California, Santa Barbara, working as a postdoctoral fellow with Mike Bowers. She was interested in the structure and stability of small molecules in the gas phase. She looked at how Ion-mobility spectrometry could be used to identify conformation.\n\nBarran joined the University of Edinburgh as an Engineering and Physical Sciences Research Council (EPSRC) Advanced Research Fellow in 2002. In 2005 she was awarded the 10th Desty Memorial prize for her innovations in Separation Science. She was made a Senior Lecturer in 2009. She worked on mass spectrometry techniques that can be used to evaluate conformational change, aggregation and intrinsic conformation. She investigated mass spectrometry for therapeutics for pre-fibrillar aggregation. She helped to establish the Scottish Instrumentation and Resource Centre for Advanced Mass Spectrometry at the University of Edinburgh. This had an initial remit to provide proteomic analysis for the MRC Human Genetics Unit.\n\nIn 2013 Barran was appointed to the Manchester Institute of Biotechnology as a Chair in Mass Spectrometry sponsored by Waters Corporation. She led an EPSRC platform grant to study the structure-activity relationships of Beta defensins. She works with Cait MacPhee, Garth Cooper and Tilo Kunath on neurodegenerative proteins, and with several groups including Richard Kriwacki, Rohit Pappu and Gary Daughdrill to examine intrinsically disordered proteins. She works with several biopharmaceutical companies to apply new mass spectrometry techniques to new drug modalites including monoclonal antibodies. She also develops new mass spectrometry instrumentation. Her group looks at the structure of biological systems at a molecular level, studying them in the gas and solution phase as well as theoretically. They use electrospray ionization, mass spectrometry, ion mobility mass spectrometry native mass spectrometry and complementary solution based biophysical techniques. They are interested in a proteins structure and how it changes in an effort to relate that to their function. Ion-mobility spectrometry–mass spectrometry can be used to look at the temperature dependent rotationally averaged collision cross-section of gas-phase ions of proteins. In 2014 she was awarded a Biotechnology and Biological Sciences Research Council grant to study the interactions of proteins with other proteins. Barran serves on the editorial board of the \"International Journal of Mass Spectrometry\". She was included in the \"page of Perditas\" created by Perdita Stevens.\n\nBarran has been working with Joy Milne to search for odorous biomarkers of Parkinson's disease. By smelling skin swabs, Milne can differentiate between people with and without Parkinson's disease. She identified changes in her husband's change in scent before he was formally diagnosed with Parkinson's disease, which he died of in 2015. Barran uses mass spectrometry to identify the biomarkers of Parkinson's disease. The story was made into a BBC documentary \"The Woman Who Can Smell Parkinson's\". Barran received ethical approval for her work of the skin metabolites of Parkinson's in 2015, allowing them to work with Parkinson's UK to conduct a larger study. In 2018 Milne travelled to the Tanzanian training centre APOPO to check whether she could smell Tuberculosis. Barran's work on Parkinson's is sponsored by The Michael J. Fox Foundation.\n"}
{"id": "3534272", "url": "https://en.wikipedia.org/wiki?curid=3534272", "title": "Peter Ascanius", "text": "Peter Ascanius\n\nPeter Ascanius (24 May 1723 – 4 June 1803) was a Norwegian biologist.\n\nHe was born at Aure in Møre og Romsdal, Norway. He was a student of Linnaeus. He taught zoology and mineralogy in Copenhagen from 1759 to 1771, and later worked as a supervisor at the mines in Kongsberg and elsewhere in Norway. Among his published works was the five-volume illustrated \"Icones rerum naturalium\". He was a Fellow of the Royal Society, elected in 1755 as a Foreign Member.\n\nAscanius first described the giant oarfish in 1772.\n\n"}
{"id": "9690908", "url": "https://en.wikipedia.org/wiki?curid=9690908", "title": "Pokeweed mitogen", "text": "Pokeweed mitogen\n\nPokeweed mitogen is a mitogen derived from \"Phytolacca americana\". It functions as a lectin.\n"}
{"id": "38287513", "url": "https://en.wikipedia.org/wiki?curid=38287513", "title": "PressureNET", "text": "PressureNET\n\nPressureNET is a crowd-sourced reporting network for barometric pressure data.\n\nIt works by having many users install it on cell phones that contain air pressure sensors (barometers) and GPS sensors. Once the location is known from the GPS data, it is able to send messages back to the server with the air pressure for that location on earth. With enough users running the application it is possible to create useful, global pressure data. It uses open source software running on Android phones, to collect data from locations around the world. The data is available on a public website.\nWith the announcement in September 2014 that the first apple device with a barometer (iPhone 6) was to be released, work started on an edition of the app for that platform The Sunshine app beta testing began to get some publicity in 2015. \n\nThe Android App still exists; however, as of January 2016 there is no support for the Android app, and the iOS app is not free open source software as the PressureNet app was. PressureNet was acquired by Sunshine in early 2016. \n\n\n"}
{"id": "58615164", "url": "https://en.wikipedia.org/wiki?curid=58615164", "title": "Resonance (sociology)", "text": "Resonance (sociology)\n\nHartmut Rosa, professor of sociology at the University of Jena, uses the term resonance to explain social phenomena from a fundamental human quest for \"resonant\" relationships. His \"resonance theory\" became fundamental in \"Resonanz\", published in 2016. \"A sociology of the world relationship\" formulated.\n\nThe term resonance is taken from physics to describe a subject-object relationship as a vibrating system in which both sides mutually stimulate each other. In contrast to the physical meaning of the word, however, here they do not merely return the received sound, but speak \"with their own voice\". The subjects' relational abilities and their intersubjective structures are constituted by such resonance experiences or their absence. This is made clear by the primary relationship of the newborn to its reference person, by whose reception or rejection of interactions the fundamental relationship patterns develop. With the concept of resonance used in this way, an attempt is made to find a way of approaching the question of successful relations between subject and world in the sense of \"good life\" as free as possible from cultural evaluations and assumptions.\n\nThe possible points of reference of such resonances are ubiquitous and are described in three basic axes: Horizontal resonances take place between two (or more) people, in love and family relationships, friendships or political space. Diagonal resonance axes are relationships to things and activities, vertical resonance axes are relationships to the great collective singulars: nature, art, history or religion. In all these contexts, intensive experiences are possible that make life experienceable as an intensive encounter or relationship for its own sake. This is juxtaposed with silent or instrumental world relations, determined by the orientation towards domination and making available, which are primarily concerned with the achievement of a useful goal. For example, a mountain tour can be a resonance experience as an intensive confrontation with the demands of the path and the encountering nature, as a purely purpose-oriented company but also instrumental and in this sense \"mute\".\n\nObviously, world sections that are perceived as attractive or desirable are predestined for resonance experiences, while those that are perceived as hostile or anxious are expected to be \"silent\", non-resonant experiences. Another prerequisite for the establishment of resonances are strong evaluations of the subject, which give the object a significance that goes beyond desire or attractiveness.\n\nIf an attempt is made to outline as resonance what people seek and long for in their innermost being, it is by no means conceived as a permanent state that can be established, but always as a selective, momentary success or self-adjustment that stands out against the background of the predominantly silent, instrumental. Resonance in this sense is therefore essentially characterized by the fact that it cannot be produced systematically and intentionally, but is ultimately unavailable.\n\nAs a sociological theory, resonance theory deals with the social conditions that promote or hinder successful relationships. If the striving for resonance is regarded as ubiquitous human primary motivation, its concretization depends to a large extent on historical, geographical and cultural conditions. In particular, existential hardship and political repression complicate resonance experiences if they do not make them completely impossible. The promise of modernity is therefore to make resonance possible by overcoming political arbitrariness and improving material resources. Rosa sees this promise as an inherent contradiction of modernity, however, undermined by the conditions that progress, which is primarily geared to increasing resources, ultimately requires: Expanded resources expand the world accessible to the subject and thus his possibilities for resonance experiences. This results in a logic of increase, which requires a constant continuation of improvement and multiplication of resources. This is accompanied by an increasing pressure to accelerate: in order to maintain the status quo within a modern society, an increasingly rapid increase in services, innovations and material production is necessary. Rosa sees this mode of dynamic stabilization as constitutive for modernity: while pre-modern societies transform themselves adaptively, i.e. in response to changed conditions, modern society is virtually defined by its compulsion for continuous transformation.\n\nWhile the current phase of late modernism is characterized by a high resonance sensitivity and expectation of its subjects, the mode of dynamic stabilization results in a loss of resonance possibilities. Rosa notes three essential manifestations of the current crisis of modernity:\n\n\nResonance theory is thus in the tradition of critical theory from Marx to Adorno and Horkheimer to Habermas and Honneth. It shares the central finding of alienation as an obstacle to a successful life, but attempts to contrast this description ex negativo with a positive counter-concept, the concept of resonance. Honneth, for example, has already made this attempt with the concept of recognition. Despite all the conceded vagueness and diversity of the concept of resonance, Rosa sees this as a universal concept that includes concepts such as recognition, justice or self-efficacy.\n\nRosa's work and the resonance theory formulated in it are controversially received and discussed. On the one hand, the author is attested originality and the courage to analyse the fundamental issues and, in contrast to the Critical Theory often summarised with Adorno's \"There is no right life in the wrong\", the optimistic perspective is emphasised which is oriented towards potentials for overcoming the stated crisis. Such an appreciation of resonance theory as a positive continuation of critical theory can be found with Anna Henkel. Micha Brumlik sees in the comprehensive combination of interdisciplinary strands the completion, but with it also the end, of Critical Theory, which thereby loses its \"theoretically informed irreconcilability looking coldly at society\".\n\nOn the other hand, it is precisely this comprehensive derivation of the concept of resonance from a multitude of perspectives and contexts that is criticized to the effect that \"resonance\" has an almost arbitrary effect, that the concept lacks precision, and that it is therefore ultimately unsuitable as a social-philosophical basic concept, as Rosa postulates it to be.\n\nAnother point of criticism refers to Rosa's alleged recourse to the intellectual world of Romanticism, to which he wanted to return without pointing the way. Rosa does indeed frequently refer to the resonance sensitivity of Romanticism even in conscious contradiction to rationalist concepts, but at the same time sees the danger of purely subjective emotion instead of resonance in the way Romanticism thinks. Thus he rather describes the continuing effect of the resonance concepts of Romanticism in modernity, without propagating a return to it.\n\nFinally, Rosa's book argues that the socio-political outlook on concrete solutions is poor and that he ultimately fails to explain how resonance can be socially established as a response to the accelerating crisis of modernity. Despite the reference to political reform proposals such as that of an unconditional basic income and emerging pilot projects of a post-growth economy, Rosa himself rejects this claim, however, because he shares \"the question of how one could get from the social formations of 'the Middle Ages' into modernity: In both cases it is a fundamental transformation of the world relationship...\"\n"}
{"id": "49411786", "url": "https://en.wikipedia.org/wiki?curid=49411786", "title": "Retroperitonium", "text": "Retroperitonium\n\nThe retroperitoneum or retroperitnium is an anatomical region that includes the peritoneum-covered organs and tissues that make up the posterior wall of the abdominal cavity and the pelvic space - which extends behind to the abdominal cavity. Definitions vary and can also can include the region of the wall of the pelvic basin.\n\nThe portion of the retroperitoneum that is posterior wall of the abdomen and superior to the iliac vessels is of importance in gynecological oncology. This is the region where para-aortic and paracaval lymphadenectomies are done. The lateral boundary of the retroperitoneum is defined by the ascending and descending colon. The retroperitoneum can be approached from above by moving the duodenum aside as far as the major renal blood vessels.\n"}
{"id": "2152061", "url": "https://en.wikipedia.org/wiki?curid=2152061", "title": "Scatterometer", "text": "Scatterometer\n\nA scatterometer or diffusionmeter is a scientific instrument to measure the return of a beam of light or radar waves scattered by diffusion in a medium such as air. Diffusionmeters using visible light are found in airports or along roads to measure horizontal visibility. Radar scatterometers use radio or microwaves to determine the normalized radar cross section (σ, \"sigma zero\" or \"sigma naught\") of a surface. They are often mounted on weather satellites to find wind speed and direction, and are used in industries to analyze the roughness of surfaces.\n\nOptical diffusionmeters are devices used in meteorology to find the optical range or the horizontal visibility. They consist of a light source, usually a laser, and a receiver. Both are placed at a 35° angle downward, aimed at a common area. Lateral scattering by the air along the light beam is quantified as an attenuation coefficient. Any departure from the clear air extinction coefficient (e.g. in fog) is measured and is inversely proportional to the visibility (the greater the loss, the lower is the visibility).\n\nThese devices are found in automatic weather stations for general visibility, along airport runways for runway visual range, or along roads for visual conditions. Their main drawback is that the measurement is done over the very small volume of air between the transmitter and the receiver. The visibility reported is therefore only representative of the general conditions around the instrument in generalized conditions (synoptic fog for instance). This is not always the case (e.g. patchy fog).\n\nA radar scatterometer operates by transmitting a pulse of microwave energy towards the Earth's surface and measuring the reflected energy. A separate measurement of the noise-only power is made and subtracted from the signal+noise measurement to determine the backscatter signal power. Sigma-0 (σ⁰) is computed from the signal power measurement using the distributed target radar equation. Scatterometer instruments are very precisely calibrated in order to make accurate backscatter measurements.\n\nThe primary application of spaceborne scatterometry has been measurements of near-surface winds over the ocean. Such instruments are known as wind scatterometers. By combining sigma-0 measurements from different azimuth angles, the near-surface wind vector over the ocean's surface can be determined using a geophysical model function (GMF) which relates wind and backscatter. Over the ocean, the radar backscatter results from scattering from wind-generated capillary-gravity waves, which are generally in equilibrium with the near-surface wind over the ocean. The scattering mechanism is known as Bragg scattering, which occurs from the waves that are in resonance with the microwaves.\n\nThe backscattered power depends on the wind speed and direction. Viewed from different azimuth angles, the observed backscatter from these waves varies. These variations can be exploited to estimate the sea surface wind, i.e. its speed and direction. This estimate process is sometimes termed 'wind retrieval' or 'model function inversion'. This is a non-linear inversion procedure based on an accurate knowledge of the GMF (in an empirical or semi-empirical form) that relates the scatterometer backscatter and the vector wind. Retrieval requires an angular diversity scatterometer measurements with the GMF, which is provided by the scatterometer making several backscatter measurements of the same spot on the ocean's surface from different azimuth angles.\n\nScatterometer wind measurements are used for air-sea interaction, climate studies and are particularly useful for monitoring hurricanes. Scatterometer backscatter data are applied to the study of vegetation, soil moisture, polar ice, tracking Antarctic icebergs and global change. Scatterometer measurements have been used to measure winds over sand and snow dunes from space. Non-terrestrial applications include study of Solar System moons using space probes. This is especially the case with the NASA/ESA Cassini mission to Saturn and its moons.\n\nSeveral generations of wind scatterometers have been flown in space by NASA, ESA, and NASDA. The first operational wind scatterometer was known as the Seasat Scatterometer (SASS) and was launched in 1978. It was a fan-beam system operating at Ku-band (14 GHz). In 1991 ESA launched the European Remote-Sensing Satellite ERS-1 Advanced Microwave Instrument (AMI) scatterometer, followed by the ERS-2 AMI scatterometer in 1995. Both AMI fan-beam systems operated at C-band (5.6 GHz). In 1996 NASA launched the NASA Scatterometer (NSCAT), a Ku-band fan-beam system. NASA launched the first scanning scatterometer, known as 'SeaWinds', on QuikSCAT in 1999. It operated at Ku-band. A second SeaWinds instrument was flown on the NASDA ADEOS-2 in 2002. The Indian Space Research Organisation launched a Ku-band scatterometer on their Oceansat-2 platform in 2009. ESA and EUMETSAT launched the first C-band ASCAT in 2006 onboard Metop-A. The Cyclone Global Navigation Satellite System (CYGNSS), launched in 2016, is a constellation of eight small satellites utilizing a bistatic approach by analyzing the reflection from the Earth's surface of Global Positioning System (GPS) signals, rather than using an onboard radar transmitter.\n\nScatterometers helped to prove the hypothesis, dating from mid-19th century, of the anisotropic (direction dependent) long distance dispersion by wind to explain the strong floristic affinities between landmasses.\n\nA work, published by the journal \"Science\" in May 2004 with the title \"Wind as a Long-Distance Dispersal Vehicle in the Southern Hemisphere\", used daily measurements of wind azimuth and speed taken by the SeaWinds scatterometer from 1999 to 2003. They found a stronger correlation of floristic similarities with wind connectivity than with geographic proximities, which supports the idea that wind is a dispersal vehicle for many organisms in the Southern Hemisphere.\n\nScatterometers are widely used in metrology for roughness of polished and lapped surfaces in semiconductor and precision machining industries. They provide a fast and non-contact alternative to traditional stylus methods for topography assessment. Scatterometers are compatible with vacuum environment, are not sensitive to vibration, and can be readily integrated with surface processing and other metrology tools.\n\nExamples of use on Earth observation satellites or installed instruments, and dates of operation:\n\n"}
{"id": "26855751", "url": "https://en.wikipedia.org/wiki?curid=26855751", "title": "Sciblogs.co.nz", "text": "Sciblogs.co.nz\n\nSciblogs.co.nz is a network of New Zealand based science bloggers. The network was founded in late 2009 and includes a collection of scientists from universities, Crown Research Institutes and private research organisations.\n\nSciblogs.co.nz was established in September 2009 by the Science Media Centre (NZ) funded by the Ministry of Research, Science & Technology (MoRST) through the Royal Society of New Zealand. It is based on the opensource Wordpress platform. Around half of the blogs are syndicated from existing science bloggers, the rest are hosted exclusively on Sciblogs. Sciblogs content is syndicated on Google News and Scoop.co.nz as well as on Facebook and Twitter.\n\nThe site is one of the largest blog networks in New Zealand.\n\nIn August 2015, Sciblogs was redeveloped and relaunched with a more visual appearance and improved functionality, as well as the addition of new bloggers covering everything from psychology to drones.\n\n"}
{"id": "23649689", "url": "https://en.wikipedia.org/wiki?curid=23649689", "title": "Shadow square", "text": "Shadow square\n\nThe shadow square, also known as an altitude scale, was an instrument used to determine the linear height of an object, in conjunction with the alidade, for angular observations. It was invented by Muhammad ibn Mūsā al-Khwārizmī in 9th-century Baghdad. Shadow squares are often found on the backs of astrolabes.\n\nThe main use of a shadow square is to measure the linear height of an object using its shadow. It does so by simulating the ratio between an object, generally a gnomon, and its shadow.If the sun's ray is between 0 degrees and 45 degrees the umbra versa (Vertical axis) is used, between 45 degrees and 90 degrees the umbra recta (Horizontal axis) is used and when the sun's ray is at 45 degrees its shadow falls exactly on the umbra media (y=x) It was used during the time of medieval astronomy to determine the height of, and to track the movement of celestial bodies such as the sun when more advanced measurement methods were not available. These methods can still be used today to determine the altitude, with reference to the horizon, of any visible celestial body.\n\nA gnomon is used along with a shadow box commonly. A gnomon is a stick placed vertically in a sunny place so that it casts a shadow that can be measured. By studying the shadow of the gnomon you can learn a lot of information about the motion of the sun. Gnomons were most likely independently discovered by many ancient civilizations, but it is known that they were used in the 5th century BC in Greece. Most likely for the measurement of the winter and summer solstices. \"Herodotus says in his \"Histories\" written around 450 B.C., that the Greeks learned the use of the gnomon from the Babylonians.\n\nIf your shadow is 4 feet long in your own feet, then what is the altitude of the sun? This problem can be solved through the use of the shadow box. The shadow box is divided in half, one half is calibrated by sixes the other by tens. Because it is a shadow cast by the human body the sixes are more convenient. By moving the alidade to the four (the same as your shadows length) and then reading the altitude scale you will see the sun is at an altitude of 56.3 degrees.\n\nThe Shadow box can also be used with long shadows using a slightly modified method. If your shadow is 18 feet long, then what is the altitude of the sun? Using the sixes side of the shadow box (because we are using a human body as measurement). The longest shadow marked on a shadow box is six feet, this creates a problem any time the shadow is longer than the gnomon (you) that casts it. By performing a simple calculation, by figuring out how tall a gnomon would be if it cast a six-foot shadow in the same situation. In this situation the gnomon would be only two feet tall, in order to cast a six-foot shadow. If the shadow is longer than the gnomon, first turn the astrolabe upside down then set the alidade at two, the height of the projected gnomon, then read off the altitude from the altitude scale. It should read that the sun is at 19 degrees above the horizon.\n"}
{"id": "50247503", "url": "https://en.wikipedia.org/wiki?curid=50247503", "title": "Sponge ground", "text": "Sponge ground\n\nSponge grounds, also known as sponge aggregations, are intertidal to deep-sea habitats formed by large accumulations of sponges (glass sponges and/or demosponges), often dominated by a few massive species. Sponge grounds were already reported more than 150 years ago, but the habitat was first fully recognized, studied and described in detail around the Faroe Islands during the inter-Nordic BIOFAR 1 programme 1987–90. These were called \"Ostur\" (meaning \"cheese\" and referring to the appearance of the sponges) by the local fishermen and this name has to some extent entered the scientific literature. Sponge grounds were later found elsewhere in the Northeast Atlantic and in the Northwest Atlantic, as well as near Antarctica. They are now known from many other places worldwide and recognized as key marine habitats.\n\nSponge grounds are important habitats supporting diverse ecosystems. During a study of outer shelf and upper slope sponge grounds at the Faroe Islands, 242 invertebrate species were found in the vicinity and 115 were associated with the sponges. In general, fish fauna associated with sponge grounds are poorly known, but include rockfish and gadiforms. Sponge grounds are threatened, especially by bottom trawling and other fishing gear, dredging, oil and gas exploration and undersea cables, but potentially also by deep sea mining, carbon dioxide sequestration, pollution and climate change.\n\nBy studying spicules in sediments cores taken from sponge grounds on the slopes of the Flemish Cap and Grand Bank (off Newfoundland, Canada), scientists managed to detect the presence of sponges in the past. The oldest record for Geodiidae sponges in this region was found in a long core collected in the slope of the Grand Bank, where typical sterraster spicules were found in the top of a submarine landslide deposit older than 25 000 BP. Continuous presence of sponges was recorded on the southeastern region of the Flemish Cap as far as 130 000 BP. It seems the distribution range of the Geodiidae in this area significantly expended after the deglaciation.\n"}
{"id": "46444460", "url": "https://en.wikipedia.org/wiki?curid=46444460", "title": "StarTalk (U.S. talk show)", "text": "StarTalk (U.S. talk show)\n\nStarTalk is an American talk show hosted by Neil deGrasse Tyson that airs weekly on National Geographic. \"StarTalk\" is a spin-off of the podcast of the same name, in which Tyson discusses scientific topics through one-on-one interviews and panel discussions. Space.com called it the \"first-ever science-themed late-night talk show.\" The series premiered on April 20, 2015.\n\nThe format of the show broadly follows the format of the podcast. This consists of a pre-taped discussion between Tyson and the guest of the week, which is shown in segments interspersed with the segments filmed in front of the live audience. Those segments consist of a discussion between Tyson, a specialist related to the guest's area of expertise, and a comedian (most commonly Chuck Nice or Eugene Mirman or Maeve Higgins). Near the end of the episode, a short taped segment of Bill Nye, giving his view of the episode's topic, is often aired.\n\nIn Australia, the series premiered on National Geographic Channel on April 27, 2015.\n\nOn August 27, 2015, 20th Century Fox released a viral video advertisement for the movie \"The Martian\", taking the form of a special episode of \"StarTalk\", with Tyson discussing the fictional Ares 3 mission from the film.\n\n\n"}
{"id": "34338342", "url": "https://en.wikipedia.org/wiki?curid=34338342", "title": "Theodor Magnus Fries", "text": "Theodor Magnus Fries\n\nTheodor Magnus Fries (28 October 1832 – 29 March 1913), the son of Elias Magnus Fries, was a Swedish botanist and lichenologist. \n\nFollowing in his father's footsteps, Fries studied botany, obtaining his doctoral degree in 1857 at Uppsala. He became a member of the Royal Swedish Academy of Sciences in 1865 and professor of botany and applied economics at Uppsala in 1877. His most notable work was \"Lichenographia scandinavica\" (1871–1874). He also produced a two-volume biography of Carl Linnaeus (1903). \n\nHis sons Thore Christian Elias Fries and Robert Elias Fries also became botanists.\n\n"}
{"id": "10263082", "url": "https://en.wikipedia.org/wiki?curid=10263082", "title": "Top-coded", "text": "Top-coded\n\nIn econometrics and statistics, a top-coded data observation is one for which data points whose values are above an upper bound are censored.\n\nSurvey data are often topcoded before release to the public to preserve the anonymity of respondents. For example, if a survey answer reported a respondent with self-identified wealth of $79 billion, it would not be anonymous because people would know there is a good chance the respondent was Bill Gates. Top-coding may be also applied to prevent possibly-erroneous outliers from being published.\n\nBottom-coding is analogous, e.g. if amounts below zero are reported as zero. Top-coding occurs for data recorded in groups, e.g. if age ranges are reported in these groups: 0-20, 21-50, 50-99, 100-and-up. Here we only know how many people have ages above 100, not their distribution. Producers of survey data sometimes release the average of the censored amounts to help users impute unbiased estimates of the top group.\n\nTop-coding is a general problem for analysis of public use data sets. Top-coding in the Current Population Survey makes it hard to estimate measures of income inequality since the shape of the distribution of high incomes is blocked. To help overcome this problem, CPS provides the mean value of top-coded values.\n\nThe practice of top-coding, or capping the reported maximum value on tax returns to protect the earner's anonymity, complicates the analysis of the distribution of wealth in the United States.\n\n\n\n"}
{"id": "18588994", "url": "https://en.wikipedia.org/wiki?curid=18588994", "title": "Usenet", "text": "Usenet\n\nUsenet () is a worldwide distributed discussion system available on computers. It was developed from the general-purpose Unix-to-Unix Copy (UUCP) dial-up network architecture. Tom Truscott and Jim Ellis conceived the idea in 1979, and it was established in 1980. Users read and post messages (called \"articles\" or \"posts\", and collectively termed \"news\") to one or more categories, known as newsgroups. Usenet resembles a bulletin board system (BBS) in many respects and is the precursor to Internet forums that are widely used today. Discussions are threaded, as with web forums and BBSs, though posts are stored on the server sequentially. The name comes from the term \"users network\".\n\nOne notable difference between a BBS or web forum and Usenet is the absence of a central server and dedicated administrator. Usenet is distributed among a large, constantly changing conglomeration of servers that store and forward messages to one another in so-called news feeds. Individual users may read messages from and post messages to a local server operated by a commercial usenet provider, their Internet service provider, university, employer, or their own server.\n\nUsenet has significant cultural importance in the networked world, having given rise to, or popularized, many widely recognized concepts and terms such as \"FAQ\", \"flame\", and \"spam\".\n\nUsenet was conceived in 1979 and publicly established in 1980, at the University of North Carolina at Chapel Hill and Duke University, over a decade before the World Wide Web went online and the general public received access to the Internet, making it one of the oldest computer network communications systems still in widespread use. It was originally built on the \"poor man's ARPANET\", employing UUCP as its transport protocol to offer mail and file transfers, as well as announcements through the newly developed news software such as A News. The name Usenet emphasized its creators' hope that the USENIX organization would take an active role in its operation.\n\nThe articles that users post to Usenet are organized into topical categories known as newsgroups, which are themselves logically organized into hierarchies of subjects. For instance, \"sci.math\" and \"sci.physics\" are within the \"sci.*\" hierarchy, for science. Or, \"talk.origins\" and \"talk.atheism\" are in the \"talk.*\" hierarchy. When a user subscribes to a newsgroup, the news client software keeps track of which articles that user has read.\n\nIn most newsgroups, the majority of the articles are responses to some other article. The set of articles that can be traced to one single non-reply article is called a thread. Most modern newsreaders display the articles arranged into threads and subthreads.\n\nWhen a user posts an article, it is initially only available on that user's news server. Each news server talks to one or more other servers (its \"newsfeeds\") and exchanges articles with them. In this fashion, the article is copied from server to server and should eventually reach every server in the network. The later peer-to-peer networks operate on a similar principle, but for Usenet it is normally the sender, rather than the receiver, who initiates transfers. Usenet was designed under conditions when networks were much slower and not always available. Many sites on the original Usenet network would connect only once or twice a day to batch-transfer messages in and out. This is largely because the POTS network was typically used for transfers, and phone charges were lower at night.\n\nThe format and transmission of Usenet articles is similar to that of Internet e-mail messages. The difference between the two is that Usenet articles can be read by any user whose news server carries the group to which the message was posted, as opposed to email messages, which have one or more specific recipients.\n\nToday, Usenet has diminished in importance with respect to Internet forums, blogs and mailing lists. Usenet differs from such media in several ways: Usenet requires no personal registration with the group concerned; information need not be stored on a remote server; archives are always available; and reading the messages requires not a mail or web client, but a news client. The groups in \"alt.binaries\" are still widely used for data transfer.\n\nMany Internet service providers, and many other Internet sites, operate news servers for their users to access. ISPs that do not operate their own servers directly will often offer their users an account from another provider that specifically operates newsfeeds. In early news implementations, the server and newsreader were a single program suite, running on the same system. Today, one uses separate newsreader client software, a program that resembles an email client but accesses Usenet servers instead. Some clients such as Mozilla Thunderbird and Outlook Express provide both abilities.\n\nNot all ISPs run news servers. A news server is one of the most difficult Internet services to administer because of the large amount of data involved, small customer base (compared to mainstream Internet services such as email and web access), and a disproportionately high volume of customer support incidents (frequently complaining of missing news articles that are not the ISP's fault). Some ISPs outsource news operation to specialist sites, which will usually appear to a user as though the ISP ran the server itself. Many sites carry a restricted newsfeed, with a limited number of newsgroups. Commonly omitted from such a newsfeed are foreign-language newsgroups and the \"alt.binaries\" hierarchy which largely carries software, music, videos and images, and accounts for over 99 percent of article data.\n\nThere are also Usenet providers that specialize in offering service to users whose ISPs do not carry news, or that carry a restricted feed.\n\nSee also news server operation for an overview of how news systems are implemented.\n\nNewsgroups are typically accessed with newsreaders: applications that allow users to read and reply to postings in newsgroups. These applications act as clients to one or more news servers. Although historically, Usenet was associated with the Unix operating system developed at AT&T, newsreaders are available for all major operating systems. Modern mail clients or \"communication suites\" commonly also have an integrated newsreader. Often, however, these integrated clients are of low quality, compared to standalone newsreaders, and incorrectly implement Usenet protocols, standards and conventions. Many of these integrated clients, for example the one in Microsoft's Outlook Express, are disliked by purists because of their misbehavior.\n\nWith the rise of the World Wide Web (WWW), web front-ends (web2news) have become more common. Web front ends have lowered the technical entry barrier requirements to that of one application and no Usenet NNTP server account. There are numerous websites now offering web based gateways to Usenet groups, although some people have begun filtering messages made by some of the web interfaces for one reason or another. Google Groups is one such web based front end and some web browsers can access Google Groups via news: protocol links directly.\n\nA minority of newsgroups are moderated, meaning that messages submitted by readers are not distributed directly to Usenet, but instead are emailed to the moderators of the newsgroup for approval. The moderator is to receive submitted articles, review them, and inject approved articles so that they can be properly propagated worldwide. Articles approved by a moderator must bear the Approved: header line. Moderators ensure that the messages that readers see in the newsgroup conform to the charter of the newsgroup, though they are not required to follow any such rules or guidelines. Typically, moderators are appointed in the proposal for the newsgroup, and changes of moderators follow a succession plan.\n\nHistorically, a \"mod.*\" hierarchy existed before Usenet reorganization. Now, moderated newsgroups may appear in any hierarchy, typically with .moderated added to the group name.\n\nUsenet newsgroups in the Big-8 hierarchy are created by proposals called a Request for Discussion, or RFD. The RFD is required to have the following information: newsgroup name, checkgroups file entry, and moderated or unmoderated status. If the group is to be moderated, then at least one moderator with a valid email address must be provided. Other information which is beneficial but not required includes: a charter, a rationale, and a moderation policy if the group is to be moderated. Discussion of the new newsgroup proposal follows, and is finished with the members of the Big-8 Management Board making the decision, by vote, to either approve or disapprove the new newsgroup.\n\nUnmoderated newsgroups form the majority of Usenet newsgroups, and messages submitted by readers for unmoderated newsgroups are immediately propagated for everyone to see. Minimal editorial content filtering vs propagation speed form one crux of the Usenet community. One little cited defense of propagation is canceling a propagated message, but few Usenet users use this command and some news readers do not offer cancellation commands, in part because article storage expires in relatively short order anyway. Almost all unmoderated Usenet groups have become collections of spam.\n\nCreation of moderated newsgroups often becomes a hot subject of controversy, raising issues regarding censorship and the desire of a subset of users to form an intentional community.\n\nUsenet is a set of protocols for generating, storing and retrieving news \"articles\" (which resemble Internet mail messages) and for exchanging them among a readership which is potentially widely distributed. These protocols most commonly use a flooding algorithm which propagates copies throughout a network of participating servers. Whenever a message reaches a server, that server forwards the message to all its network neighbors that haven't yet seen the article. Only one copy of a message is stored per server, and each server makes it available on demand to the (typically local) readers able to access that server. The collection of Usenet servers has thus a certain peer-to-peer character in that they share resources by exchanging them, the granularity of exchange however is on a different scale than a modern peer-to-peer system and this characteristic excludes the actual users of the system who connect to the news servers with a typical client-server application, much like an email reader.\n\nRFC 850 was the first formal specification of the messages exchanged by Usenet servers. It was superseded by RFC 1036 and subsequently by RFC 5536 and RFC 5537.\n\nIn cases where unsuitable content has been posted, Usenet has support for automated removal of a posting from the whole network by creating a cancel message, although due to a lack of authentication and resultant abuse, this capability is frequently disabled. Copyright holders may still request the manual deletion of infringing material using the provisions of World Intellectual Property Organization treaty implementations, such as the United States Online Copyright Infringement Liability Limitation Act, but this would require giving notice to each individual news server administrator.\n\nOn the Internet, Usenet is transported via the Network News Transfer Protocol (NNTP) on TCP Port 119 for standard, unprotected connections and on TCP port 563 for SSL encrypted connections which is offered only by a few sites.\n\nThe major set of worldwide newsgroups is contained within nine hierarchies, eight of which are operated under consensual guidelines that govern their administration and naming. The current \"Big Eight\" are:\n\nSee also the Great Renaming.\n\nThe \"alt.*\" hierarchy is not subject to the procedures controlling groups in the Big Eight, and it is as a result less organized. Groups in the \"alt.*\" hierarchy tend to be more specialized or specific—for example, there might be a newsgroup under the Big Eight which contains discussions about children's books, but a group in the alt hierarchy may be dedicated to one specific author of children's books. Binaries are posted in \"alt.binaries.*\", making it the largest of all the hierarchies.\n\nMany other hierarchies of newsgroups are distributed alongside these. Regional and language-specific hierarchies such as \".*\", \".*\" and \"ne.*\" serve specific countries and regions such as Japan, Malta and New England. Companies and projects administer their own hierarchies to discuss their products and offer community technical support, such as the historical \".*\" hierarchy from the Free Software Foundation. Microsoft closed its newsserver in June 2010, providing support for its products over forums now. Some users prefer to use the term \"Usenet\" to refer only to the Big Eight hierarchies; others include alt as well. The more general term \"netnews\" incorporates the entire medium, including private organizational news systems.\n\nInformal sub-hierarchy conventions also exist. \"*.answers\" are typically moderated cross-post groups for FAQs. An FAQ would be posted within one group and a cross post to the \"*.answers\" group at the head of the hierarchy seen by some as a refining of information in that news group. Some subgroups are recursive—to the point of some silliness in \"alt.*\".\n\nUsenet was originally created to distribute text content encoded in the 7-bit ASCII character set. With the help of programs that encode 8-bit values into ASCII, it became practical to distribute binary files as content. Binary posts, due to their size and often-dubious copyright status, were in time restricted to specific newsgroups, making it easier for administrators to allow or disallow the traffic.\n\nThe oldest widely used encoding method for binary content is uuencode, from the Unix UUCP package. In the late 1980s, Usenet articles were often limited to 60,000 characters, and larger hard limits exist today. Files are therefore commonly split into sections that require reassembly by the reader.\n\nWith the header extensions and the Base64 and Quoted-Printable MIME encodings, there was a new generation of binary transport. In practice, MIME has seen increased adoption in text messages, but it is avoided for most binary attachments. Some operating systems with metadata attached to files use specialized encoding formats. For Mac OS, both Binhex and special MIME types are used.\n\nOther lesser known encoding systems that may have been used at one time were BTOA, XX encoding, BOO, and USR encoding.\n\nIn an attempt to reduce file transfer times, an informal file encoding known as yEnc was introduced in 2001. It achieves about a 30% reduction in data transferred by assuming that most 8-bit characters can safely be transferred across the network without first encoding into the 7-bit ASCII space.\n\nThe most common method of uploading large binary posts to Usenet is to convert the files into RAR archives and create Parchive files for them. Parity files are used to recreate missing data when not every part of the files reaches a server.\n\nEach news server generally allocates a certain amount of storage space for post content in each newsgroup. When this storage has been filled, each time a new post arrives, old posts are deleted to make room for the new content. If the network bandwidth available to a server is high but the storage allocation is small, it is possible for a huge flood of incoming content to overflow the allocation and push out everything that was in the group before it. If the flood is large enough, the beginning of the flood will begin to be deleted even before the last part of the flood has been posted.\n\nBinary newsgroups are only able to function reliably if there is sufficient storage allocated to a group to allow readers enough time to download all parts of a binary posting before it is flushed out of the group's storage allocation. This was at one time how posting of undesired content was countered; the newsgroup would be flooded with random garbage data posts, of sufficient quantity to push out all the content to be suppressed. This has been compensated by service providers allocating enough storage to retain everything posted each day, including such spam floods, without deleting anything.\n\nThe average length of time that posts are able to stay in the group before being deleted is commonly called the \"retention time\". Generally the larger Usenet servers have enough capacity to archive several years of binary content even when flooded with new data at the maximum daily speed available. A good binaries service provider must not only accommodate users of fast connections (3 megabit) but also users of slow connections (256 kilobit or less) who need more time to download content over a period of several days or weeks.\n\nMajor NSPs have a retention time of more than 4 years.\nThis results in more than 33 petabytes (33000 terabytes) of storage.\n\nIn part because of such long retention times, as well as growing Internet upload speeds, Usenet is also used by individual users to store backup data in a practice called \"Usenet backup\", or uBackup. While commercial providers offer more easy to use online backup services, storing data on Usenet is free of charge (although access to Usenet itself may not be). The method requires the user to manually select, prepare and upload the data. Because anyone can potentially download the backup files, the data is typically encrypted. After the files are uploaded, the uploader does not have any control over them; the files are automatically copied to all Usenet providers, so there will be multiple copies of it spread over different geographical locations around the world—desirable in a backup scheme.\n\nWhile binary newsgroups can be used to distribute completely legal user-created works, open-source software, and public domain material, some binary groups are used to illegally distribute commercial software, copyrighted media, and obscene material.\n\nISP-operated Usenet servers frequently block access to all \"alt.binaries.*\" groups to both reduce network traffic and to avoid related legal issues. Commercial Usenet service providers claim to operate as a telecommunications service, and assert that they are not responsible for the user-posted binary content transferred via their equipment. In the United States, Usenet providers can qualify for protection under the DMCA Safe Harbor regulations, provided that they establish a mechanism to comply with and respond to takedown notices from copyright holders.\n\nRemoval of copyrighted content from the entire Usenet network is a nearly impossible task, due to the rapid propagation between servers and the retention done by each server. Petitioning a Usenet provider for removal only removes it from that one server's retention cache, but not any others. It is possible for a special \"post cancellation\" message to be distributed to remove it from all servers, but many providers ignore cancel messages by standard policy, because they can be easily falsified and submitted by anyone. For a takedown petition to be most effective across the whole network, it would have to be issued to the origin server to which the content has been posted, before it has been propagated to other servers. Removal of the content at this early stage would prevent further propagation, but with modern high speed links, content can be propagated as fast as it arrives, allowing no time for content review and takedown issuance by copyright holders.\n\nEstablishing the identity of the person posting illegal content is equally difficult due to the trust-based design of the network. Like SMTP email, servers generally assume the header and origin information in a post is true and accurate. However, as in SMTP email, Usenet post headers are easily falsified so as to obscure the true identity and location of the message source. In this manner, Usenet is significantly different from modern P2P services; most P2P users distributing content are typically immediately identifiable to all other users by their network address, but the origin information for a Usenet posting can be completely obscured and unobtainable once it has propagated past the original server.\n\nAlso unlike modern P2P services, the identity of the downloaders is hidden from view. On P2P services a downloader is identifiable to all others by their network address. On Usenet, the downloader connects directly to a server, and only the server knows the address of who is connecting to it. Some Usenet providers do keep usage logs, but not all make this logged information casually available to outside parties such as the Recording Industry Association of America. The existence of anonymising gateways to USENET also complicates the tracing of a postings true origin.\n\nNewsgroup experiments first occurred in 1979. Tom Truscott and Jim Ellis of Duke University came up with the idea as a replacement for a local announcement program, and established a link with nearby University of North Carolina using Bourne shell scripts written by Steve Bellovin. The public release of news was in the form of conventional compiled software, written by Steve Daniel and Truscott. In 1980, Usenet was connected to ARPANET through which had connections to both Usenet and ARPANET. Mark Horton, the graduate student who set up the connection, began \"feeding mailing lists from the ARPANET into Usenet\" with the \"fa\" (\"From ARPANET\") identifier. Usenet gained 50 member sites in its first year, including Reed College, University of Oklahoma, and Bell Labs, and the number of people using the network increased dramatically; however, it was still a while longer before Usenet users could contribute to ARPANET.\n\nUUCP networks spread quickly due to the lower costs involved, and the ability to use existing leased lines, X.25 links or even ARPANET connections. By 1983, thousands of people participated from more than 500 hosts, mostly universities and Bell Labs sites but also a growing number of Unix-related companies; the number of hosts nearly doubled to 940 in 1984. More than 100 newsgroups existed, more than 20 devoted to Unix and other computer-related topics, and at least a third to recreation. As the mesh of UUCP hosts rapidly expanded, it became desirable to distinguish the Usenet subset from the overall network. A vote was taken at the 1982 USENIX conference to choose a new name. The name Usenet was retained, but it was established that it only applied to news. The name UUCPNET became the common name for the overall network.\n\nIn addition to UUCP, early Usenet traffic was also exchanged with Fidonet and other dial-up BBS networks. Widespread use of Usenet by the BBS community was facilitated by the introduction of UUCP feeds made possible by MS-DOS implementations of UUCP, such as UFGATE (UUCP to FidoNet Gateway), FSUUCP and UUPC. In 1986, RFC 977 provided the Network News Transfer Protocol (NNTP) specification for distribution of Usenet articles over TCP/IP as a more flexible alternative to informal Internet transfers of UUCP traffic. Since the Internet boom of the 1990s, almost all Usenet distribution is over NNTP.\n\nEarly versions of Usenet used Duke's A News software, designed for one or two articles a day. Matt Glickman and Horton at Berkeley produced an improved version called B News that could handle the rising traffic (about 50 articles a day as of late 1983). With a message format that offered compatibility with Internet mail and improved performance, it became the dominant server software. C News, developed by Geoff Collyer and Henry Spencer at the University of Toronto, was comparable to B News in features but offered considerably faster processing. In the early 1990s, InterNetNews by Rich Salz was developed to take advantage of the continuous message flow made possible by NNTP versus the batched store-and-forward design of UUCP. Since that time INN development has continued, and other news server software has also been developed.\n\nUsenet was the first Internet community and the place for many of the most important public developments in the pre-commercial Internet. It was the place where Tim Berners-Lee announced the launch of the World Wide Web, where Linus Torvalds announced the Linux project, and where Marc Andreessen announced the creation of the Mosaic browser and the introduction of the image tag, which revolutionized the World Wide Web by turning it into a graphical medium.\n\nMany jargon terms now in common use on the Internet originated or were popularized on Usenet. Likewise, many conflicts which later spread to the rest of the Internet, such as the ongoing difficulties over spamming, began on Usenet.\n\nSascha Segan of \"PC Magazine\" said in 2008 that \"Usenet has been dying for years\". Segan said that some people pointed to the Eternal September in 1993 as the beginning of Usenet's decline. Segan believes that when pornographers and software crackers began putting large (non-text) files on Usenet by the late 1990s, Usenet disk space and traffic increased correspondingly. Internet service providers questioned why they needed to host space for pornography and unauthorized software. When the State of New York opened an investigation on child pornographers who used Usenet, many ISPs dropped all Usenet access or access to the \"alt.*\" hierarchy.\n\nIn response, John Biggs of TechCrunch said \"As long as there are folks who think a command line is better than a mouse, the original text-only social network will live on\".\n\nAOL discontinued Usenet access in 2005. In May 2010, Duke University, whose implementation had kicked off Usenet more than 30 years earlier, decommissioned its Usenet server, citing low usage and rising costs.\nAfter 32 years, the Usenet news service link at the University of North Carolina at Chapel Hill (news.unc.edu) was retired on February 4, 2011.\n\nOver time, the amount of Usenet traffic has steadily increased. the number of all text posts made in all Big-8 newsgroups averaged 1,800 new messages every hour, with an average of 25,000 messages per day. However, these averages are minuscule in comparison to the traffic in the binary groups. Much of this traffic increase reflects not an increase in discrete users or newsgroup discussions, but instead the combination of massive automated spamming and an increase in the use of \".binaries\" newsgroups in which large files are often posted publicly. A small sampling of the change (measured in feed size per day) follows:\n\nIn 2008, Verizon Communications, Time Warner Cable and Sprint Nextel signed an agreement with Attorney General of New York Andrew Cuomo to shut down access to sources of child pornography. Time Warner Cable stopped offering access to Usenet. Verizon reduced its access to the \"Big 8\" hierarchies. Sprint stopped access to the \"alt.*\" hierarchies. AT&T stopped access to the \"alt.binaries.*\" hierarchies. Cuomo never specifically named Usenet in his anti-child pornography campaign. David DeJean of \"PC World\" said that some worry that the ISPs used Cuomo's campaign as an excuse to end portions of Usenet access, as it is costly for the Internet service providers and not in high demand by customers. In 2008 AOL, which no longer offered Usenet access, and the four providers that responded to the Cuomo campaign were the five largest Internet service providers in the United States; they had more than 50% of the U.S. ISP marketshare. On June 8, 2009, AT&T announced that it would no longer provide access to the Usenet service as of July 15, 2009.\n\nAOL announced that it would discontinue its integrated Usenet service in early 2005, citing the growing popularity of weblogs, chat forums and on-line conferencing. The AOL community had a tremendous role in popularizing Usenet some 11 years earlier.\n\nIn August 2009, Verizon announced that it would discontinue access to Usenet on September 30, 2009. JANET announced it will discontinue Usenet service, effective July 31, 2010, citing Google Groups as an alternative.\nMicrosoft announced that it would discontinue support for its public newsgroups (msnews.microsoft.com) from June 1, 2010, offering web forums as an alternative.\n\nPrimary reasons cited for the discontinuance of Usenet service by general ISPs include the decline in volume of actual readers due to competition from blogs, along with cost and liability concerns of increasing proportion of traffic devoted to file-sharing and spam on unused or discontinued groups.\n\nSome ISPs did not include pressure from Attorney General of New York Andrew Cuomo's aggressive campaign against child pornography as one of their reasons for dropping Usenet feeds as part of their services. ISPs Cox and Atlantic Communications resisted the 2008 trend but both did eventually drop their respective Usenet feeds in 2010.\n\nPublic archives of Usenet articles have existed since the early days of Usenet, such as the system created by Kenneth Almquist in late 1982. Distributed archiving of Usenet posts was suggested in November 1982 by Scott Orshan, who proposed that \"Every site should keep all the articles it posted, forever.\" Also in November of that year, Rick Adams responded to a post asking \"Has anyone archived netnews, or does anyone plan to?\" by stating that he was, \"afraid to admit it, but I started archiving most 'useful' newsgroups as of September 18.\" In June 1982, Gregory G. Woodbury proposed an \"automatic access to archives\" system that consisted of \"automatic answering of fixed-format messages to a special mail recipient on specified machines.\"\n\nIn 1985, two news archiving systems and one RFC were posted to the Internet. The first system, called keepnews, by Mark M. Swenson of The University of Arizona, was described as \"a program that attempts to provide a sane way of extracting and keeping information that comes over Usenet.\" The main advantage of this system was to allow users to mark articles as worthwhile to retain. The second system, YA News Archiver by Chuq Von Rospach, was similar to keepnews, but was \"designed to work with much larger archives where the wonderful quadratic search time feature of the Unix ... becomes a real problem.\" Von Rospach in early 1985 posted a detailed RFC for \"archiving and accessing usenet articles with keyword lookup.\" This RFC described a program that could \"generate and\nmaintain an archive of Usenet articles and allow looking up articles based on the article-id, subject lines, or keywords pulled out of the article itself.\" Also included was C code for the internal data structure of the system.\n\nThe desire to have a fulltext search index of archived news articles is not new either, one such request having been made in April 1991 by Alex Martelli who sought to \"build\nsome sort of keyword index for [the news archive].\" In early May, Mr. Martelli posted a summary of his responses to Usenet, noting that the \"most popular suggestion award must definitely go to 'lq-text' package, by Liam Quin, recently posted in alt.sources.\"\n\nThe huge site http://asstr.org archives and indexes erotic and pornographic stories posted to the Usenet group alt.sex.stories.\n\nToday, the archiving of Usenet has led to a fear of loss of privacy. An archive simplifies ways to profile people. This has partly been countered with the introduction of the \" X-No-Archive: Yes\" header, which is itself controversial.\n\nWeb-based archiving of Usenet posts began in 1995 at Deja News with a very large, searchable database. In 2001, this database was acquired by Google.\n\nGoogle Groups hosts an archive of Usenet posts dating back to May 1981. The earliest posts, which date from May 1981 to June 1991, were donated to Google by the University of Western Ontario with the help of David Wiseman and others, and were originally archived by Henry Spencer at the University of Toronto's Zoology department. The archives for late 1991 through early 1995 were provided by Kent Landfield from the NetNews CD series and Jürgen Christoffel from GMD. The archive of posts from March 1995 onward was started by the company DejaNews (later Deja), which was purchased by Google in February 2001. Google began archiving Usenet posts for itself starting in the second week of August 2000.\n\nGoogle has been criticized by \"Vice\" and \"Wired\" contributors as well as former employees for its stewardship of the archive and for breaking its search functionality.\n\n\n\n\n\n\nUsenet as a whole has no administrators; each server administrator is free to do whatever pleases him or her as long as the end users and peer servers tolerate and accept it. Nevertheless, there are a few famous administrators:\n\n"}
{"id": "51550799", "url": "https://en.wikipedia.org/wiki?curid=51550799", "title": "Vsevolod Gussakovskiy", "text": "Vsevolod Gussakovskiy\n\nVsevolod Vladimirovich Gussakovskiy (11 October 1911, Tsarskoye Selo - September, 1948) was a Russian entomologist who specialised in Hymenoptera. He described many new species.\n\nHis collections from Turkestan are held (ex parte) by the Zoological Museum of the Zoological Institute of the Russian Academy of Sciences.\n\n\n"}
{"id": "43516457", "url": "https://en.wikipedia.org/wiki?curid=43516457", "title": "Zoe Shipton", "text": "Zoe Shipton\n\nZoe Shipton is a British geologist. She is a professor of Geological Engineering at Strathclyde University.\nShipton is a member of the Royal Society and Royal Academy of Engineering working group on “Shale gas extraction in the UK: a review of the scientific and engineering evidence” and currently holds the chair of the Tectonic Studies Group of the Geological Society of London.\n\nIn July 2014, Shipton's career in geology was featured on the BBC Radio 4 show \"The Life Scientific\"\n\nIn March 2016 Shipton was elected a Fellow of the Royal Society of Edinburgh, Scotland's National Academy for science and letters.\n\nZoe's paternal grandfather was Himalayan mountaineer, Eric Shipton.\n"}
