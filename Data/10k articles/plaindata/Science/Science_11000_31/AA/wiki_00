{"id": "58834410", "url": "https://en.wikipedia.org/wiki?curid=58834410", "title": "2018 UA", "text": "2018 UA\n\n2018 UA is a small near-Earth Apollo asteroid that flew very close to the earth on October 19, 2018.\n\n"}
{"id": "36469092", "url": "https://en.wikipedia.org/wiki?curid=36469092", "title": "Alpha &amp; Omega (book)", "text": "Alpha &amp; Omega (book)\n\nAlpha & Omega: The Search for the Beginning and End of the Universe is the second non-fiction book by Charles Seife, published by Viking, a division of Penguin Putnam, in 2003. It is a survey of historic and contemporary efforts at cosmology: to describe the universe, trace the universe back to its origins, including the Big Bang Theory, and to determine the universe's eventual end-state. The books title refers to the Alpha and Omega appellation for Christ, as found in the Book of Revelation. A paperback reprint was published in 2004, also from Penguin.\n\n\nThe \"New York Times\" praised the book, describing it as \"A primer on the history and state of cosmology that is easy to read and understand… Seife's book shines.\" The \"Los Angeles Times\" described it as \"provid(ing) a wonderfully clear and concise introduction to terms often too loosely bandied about, and to their interrelationships in the ongoing attempt of physicists to erect a unified theory of the universe.\"\n\n\n"}
{"id": "487132", "url": "https://en.wikipedia.org/wiki?curid=487132", "title": "Analytics", "text": "Analytics\n\nAnalytics is the discovery, interpretation, and communication of meaningful patterns in data and applying those patterns towards effective decision making. In other words, analytics can be understood as the connective tissue between data and effective decision making, within an organization. Especially valuable in areas rich with recorded information, analytics relies on the simultaneous application of statistics, computer programming and operations research to quantify performance. \n\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include predictive analytics, prescriptive analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big Data Analytics, retail analytics, supply chain analytics, store assortment and stock-keeping unit optimization, marketing optimization and marketing mix modeling, web analytics, call analytics, speech analytics, sales force sizing and optimization, price and promotion modeling, predictive science, credit risk analysis, and fraud analytics. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics.\n\nAnalysis is focused on understanding the past; what happened. Analytics focuses on why it happened and what will happen next.\n\nData analytics is a multidisciplinary field. There is extensive use of computer skills, mathematics and statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data.. The insights from data are used to recommend action or to guide decision making rooted in business context. Thus, analytics is not so much concerned with individual analyses or analysis steps, but with the entire methodology. There is a pronounced tendency to use the term \"analytics\" in business settings e.g. text analytics vs. the more generic text mining to emphasize this broader perspective. There is an increasing use of the term \"advanced analytics\", typically used to describe the technical aspects of analytics, especially in the emerging fields such as the use of machine learning techniques like neural networks, Decision Tree, Logistic Regression, linear to multiple regression analysis, Classification to do predictive modeling. It also includes Unsupervised Machine learning techniques like cluster analysis, Principal Component Analysis, segmentation profile analysis and association analysis.\n\nMarketing has evolved from a creative process into a highly data-driven process. Marketing organizations use analytics to determine the outcomes of campaigns or efforts and to guide decisions for investment and consumer targeting. Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.\n\nWeb analytics allows marketers to collect session-level information about interactions on a website using an operation called sessionization. Google Analytics is an example of a popular free analytics tool that marketers use for this purpose. Those interactions provide web analytics information systems with the information necessary to track the referrer, search keywords, identify IP address, and track activities of the visitor. With this information, a marketer can improve marketing campaigns, website creative content, and information architecture.\n\nAnalysis techniques frequently used in marketing include marketing mix modeling, pricing and promotion analyses, sales force optimization and customer analytics e.g.: segmentation. Web analytics and optimization of web sites and online campaigns now frequently work hand in hand with the more traditional marketing analysis techniques. A focus on digital media has slightly changed the vocabulary so that \"marketing mix modeling\" is commonly referred to as \"attribution modeling\" in the digital or marketing mix modeling context.\n\nThese tools and techniques support both strategic marketing decisions (such as how much overall to spend on marketing, how to allocate budgets across a portfolio of brands and the marketing mix) and more tactical campaign support, in terms of targeting the best potential customer with the optimal message in the most cost effective medium at the ideal time.\n\nPeople Analytics is using behavioral data to understand how people work and change how companies are managed.\n\nPeople analytics is also known as workforce analytics, HR analytics, talent analytics, people insights, talent insights, colleague insights, human capital analytics, and HRIS analytics. HR analytics is the application of analytics to help companies manage human resources. The aim is to discern which employees to hire, which to reward or promote, what responsibilities to assign, and similar human resource problems. HR analytics is becoming increasingly important to understand what kind of behavioral profiles would succeed and fail. For example, an analysis may find that individuals that fit a certain type of profile are those most likely to succeed at a particular role, making them the best employees to hire.\n\nHowever, there are key differences between people analytics and HR analytics. “People Analytics solves business problems. HR Analytics solves HR problems. People Analytics looks at the work and its social organization. HR Analytics measures and integrates data about HR administrative processes,” says Ben Waber, MIT Media Lab Ph.D. and CEO of Humanyze. Josh Bersin, founder and principal at Bersin by Deloitte agrees that people analytics is a larger industry than HR Analytics, explaining, “… over time, I believe it doesn't even belong within HR. While it may reside in HR to begin with, over time this team takes responsible for analysis of sales productivity, turnover, retention, accidents, fraud, and even the people-issues that drive customer retention and customer satisfaction… These are all real-world business problems, \"not HR problems\".”\n\nA common application of business analytics is portfolio analysis. In this, a bank or lending agency has a collection of accounts of varying value and risk. The accounts may differ by the social status (wealthy, middle-class, poor, etc.) of the holder, the geographical location, its net value, and many other factors. The lender must balance the return on the loan with the risk of default for each loan. The question is then how to evaluate the portfolio as a whole.\n\nThe least risk loan may be to the very wealthy, but there are a very limited number of wealthy people. On the other hand, there are many poor that can be lent to, but at greater risk. Some balance must be struck that maximizes return and minimizes risk. The analytics solution may combine time series analysis with many other issues in order to make decisions on when to lend money to these different borrower segments, or decisions on the interest rate charged to members of a portfolio segment to cover any losses among members in that segment.\n\nPredictive models in the banking industry are developed to bring certainty across the risk scores for individual customers. Credit scores are built to predict individual’s delinquency behavior and widely used to evaluate the credit worthiness of each applicant. Furthermore, risk analyses are carried out in the scientific world and the insurance industry. It is also extensively used in financial institutions like Online Payment Gateway companies to analyse if a transaction was genuine or fraud. For this purpose they use the transaction history of the customer. This is more commonly used in Credit Card purchase, when there is a sudden spike in the customer transaction volume the customer gets a call of confirmation if the transaction was initiated by him/her. This helps in reducing loss due to such circumstances.\n\nDigital analytics is a set of business and technical activities that define, create, collect, verify or transform digital data into reporting, research, analyses, recommendations, optimizations, predictions, and automations. This also includes the SEO (search engine optimization) where the keyword search is tracked and that data is used for marketing purposes. Even banner ads and clicks come under digital analytics. A growing number of brands and marketing firms rely on digital analytics for their digital marketing assignments, where MROI (Marketing Return on Investment) is an important key performance indicator (KPI).\n\nSecurity analytics refers to information technology (IT) to gather and analyze security events to understand and analyze events that pose the greatest risk. Products in this area include security information and event management and user behavior analytics.\n\nSoftware analytics is the process of collecting information about the way a piece of software is used and produced.\n\nIn the industry of commercial analytics software, an emphasis has emerged on solving the challenges of analyzing massive, complex data sets, often when such data is in a constant state of change. Such data sets are commonly referred to as big data. Whereas once the problems posed by big data were only found in the scientific community, today big data is a problem for many businesses that operate transactional systems online and, as a result, amass large volumes of data quickly.\n\nThe analysis of unstructured data types is another challenge getting attention in the industry. Unstructured data differs from structured data in that its format varies widely and cannot be stored in traditional relational databases without significant effort at data transformation. Sources of unstructured data, such as email, the contents of word processor documents, PDFs, geospatial data, etc., are rapidly becoming a relevant source of business intelligence for businesses, governments and universities. For example, in Britain the discovery that one company was illegally selling fraudulent doctor's notes in order to assist people in defrauding employers and insurance companies, is an opportunity for insurance firms to increase the vigilance of their unstructured data analysis. The McKinsey Global Institute estimates that big data analysis could save the American health care system $300 billion per year and the European public sector €250 billion.\n\nThese challenges are the current inspiration for much of the innovation in modern analytics information systems, giving birth to relatively new machine analysis concepts such as complex event processing, full text search and analysis, and even new ideas in presentation. One such innovation is the introduction of grid-like architecture in machine analysis, allowing increases in the speed of massively parallel processing by distributing the workload to many computers all with equal access to the complete data set.\n\nAnalytics is increasingly used in education, particularly at the district and government office levels. However, the complexity of student performance measures presents challenges when educators try to understand and use analytics to discern patterns in student performance, predict graduation likelihood, improve chances of student success, etc. For example, in a study involving districts known for strong data use, 48% of teachers had difficulty posing questions prompted by data, 36% did not comprehend given data, and 52% incorrectly interpreted data. To combat this, some analytics tools for educators adhere to an over-the-counter data format (embedding labels, supplemental documentation, and a help system, and making key package/display and content decisions) to improve educators’ understanding and use of the analytics being displayed.\n\nOne more emerging challenge is dynamic regulatory needs. For example, in the banking industry, Basel III and future capital adequacy needs are likely to make even smaller banks adopt internal risk models. In such cases, cloud computing and open source programming language R can help smaller banks to adopt risk analytics and support branch level monitoring by applying predictive analytics.\n\nThe main risk for the people is discrimination like price discrimination or statistical discrimination. See Scientific American book review of \"Weapons of math destruction\"\n\nThere is also the risk that a developer could profit from the ideas or work done by users, like this example:\nUsers could write new ideas in a note taking app, which could then be sent as a custom event, and the developers could profit from those ideas. This can happen because the ownership of content is usually unclear in the law.\n\nIf a user's identity is not protected, there are more risks; for example, the risk that private information about users is made public on the internet.\n\nIn the extreme, there is the risk that governments could gather too much private information, now that the governments are giving themselves more powers to access citizens' information. \n"}
{"id": "58905969", "url": "https://en.wikipedia.org/wiki?curid=58905969", "title": "Asperity (faults)", "text": "Asperity (faults)\n\nAn asperity is an area on an active fault where there is increased friction, such that the fault may become locked, rather than continuously slipping as in aseismic creep. Earthquake rupture generally begins with the failure of an asperity, allowing the fault to move.\n\n\n"}
{"id": "28913923", "url": "https://en.wikipedia.org/wiki?curid=28913923", "title": "Bader Glacier", "text": "Bader Glacier\n\nBader Glacier () is a small glacier draining the west slopes of Rudozem Heights and flowing to Bourgeois Fjord just south of Thomson Head on German Peninsula, Fallières Coast on the west side of Graham Land, Antarctica. It was named by the UK Antarctic Place-Names Committee in 1958 for Swiss glaciologist Henri Bader of Rutgers University (U.S.), author of an important thesis on the development of the snowflake and its metamorphoses.\n\n\n"}
{"id": "21614781", "url": "https://en.wikipedia.org/wiki?curid=21614781", "title": "CHELPG", "text": "CHELPG\n\nCHELPG (CHarges from ELectrostatic Potentials using a Grid-based method) is an atomic charge calculation scheme developed by Breneman and Wiberg, in which atomic charges are fitted to reproduce the molecular electrostatic potential (MESP) at a number of points around the molecule.\n\nThe charge calculation methods based on fitting of molecular electrostatic potential (MESP) (including CHELPG) are not well-suitable for the treatment of larger systems, where some of the innermost atoms are located far away from the points at which the MESP is computed. In such a situation, variations of the innermost atomic charges will not lead to significant changes of the MESP outside of the molecule, which means accurate values for the innermost atomic charges are not well-determined by the MESP outside of the molecule. This problem is solved by density derived electrostatic and chemical (DDEC) methods that partition the electron density cloud in order to provide chemically meaningful net atomic charges that approximately reproduce the electrostatic potential surrounding the material.\n\nIt should be remembered that atomic charges depend on the molecular conformation. The representative atomic charges for flexible molecules hence should be computed as average values over several molecular conformations. \n\nA number of alternative MESP charge schemes have been developed, such as those employing Connolly surfaces or geodesic point selection algorithms, in order to improve rotational invariance by increasing the point selection density and reducing anisotropies in the sampled points on the MESP surface. While CHELPG is restricted to non-periodic (e.g., molecular) systems, the DDEC methods can be applied to both non-periodic and periodic materials. \n\nCHELPG charges can be computed using the popular \"ab initio\" quantum chemical packages such as Gaussian, GAMESS-US and ORCA.\n"}
{"id": "14123100", "url": "https://en.wikipedia.org/wiki?curid=14123100", "title": "Coastal Observatory", "text": "Coastal Observatory\n\nThe Coastal Observatory in Liverpool Bay is housed at the Proudman Oceanographic Laboratory in Brownlow Street\nLiverpool, England.\n\nThe objective of the Coastal Observatory is to study a typical coastal sea's response both to natural forces and to the effects of human activity. The Observatory integrates real-time data measurements with data from models into a \"pre-operational coastal prediction system\" whose results will be displayed on the web site.\n\nThe concept is founded on obtaining data in real time, using telemetry, sending the data from underwater to the sea surface, to land, to the Proudman Oceanographic Laboratory to the web site, enabling what is often known as 'armchair oceanography'.\n\nThe aim of the Coastal Observatory is to build a time series of data. The Observatory has a particular interest in such areas as storm surges, seasonality, and variations in river discharge, with an emphasis on the River Mersey.\n\nAugust 2007 marked five years of continuous running of the Coastal Observatory in Liverpool Bay, taking measurements such as:\n\n"}
{"id": "9216811", "url": "https://en.wikipedia.org/wiki?curid=9216811", "title": "Cultural retention", "text": "Cultural retention\n\nCultural retention is the act of retaining the culture of a specific ethnic group of people, especially when there is reason to believe that the culture, through inaction, may be lost. Many African-American, European and Asian organizations have cultural retention programs in place.\n\n"}
{"id": "27293278", "url": "https://en.wikipedia.org/wiki?curid=27293278", "title": "Digital Researcher", "text": "Digital Researcher\n\nA Digital Researcher is a person who uses digital technology such as computers or smartphones and the Internet to do research (see also Internet research). Digital research differs from Internet research in that digital researchers use the Internet as a research tool rather than the Internet itself as the subject of study. A digital researcher seeks knowledge as part of a systematic investigation with the specific intent of publishing research findings in an online open access journal or by other social media information exchange formats.\n\nAlthough digital research can be both quantitative and qualitative, it does not necessarily follow strict Internet research ethics using the formal scientific method, as it involves collaboration using social media with public input for research and knowledge mobilization. There are a number of objections to this stance, which are all relevant to Wikipedia research and research ethics, for example the blurring of public and private spaces on the internet.\n\nDigital research may also be formally published in academia through peer-reviewed journals or through the further use of social media. Digital researchers are involved with basic research or applied research using data analysis software which includes, but is not limited to, SPSS or JMP.\n\nThe term Digital Research was originally used to describe a now defunct company created by Dr. Gary Kildall to market and develop his CP/M operating system and related products. It was the first large software company in the microcomputer world.\n\nThe term Digital Researcher was also used by UK researcher development organization Vitae to form an event for postgraduate researchers and research staff focusing on the use of technology by researchers for collaboration, information gathering, and dissemination. The event encouraged researchers to become digital researchers.\n\n• Ferguson, Rebecca (2010). Internet research ethics. Slideshare presentation (12 slides). \"Which ethics apply to Internet research? If the Internet is conceptualised as space, then social science research ethics apply. However, if it is conceptualised as text/art, then the ethics of the humanities are more relevant.\"\n\n• Berry, David M. (2004). Internet Research: Privacy, Ethics and Alienation - An Open Source Approach. The Journal of Internet Research, 14(4) PDF, 105 KB. Emphasis on Internet research ethics within the larger context of \"open-source ethics\".\n\n• Gunther Eysenbach and James Till. Ethical issues in qualitative research on Internet communities. BMJ 2001(10 November); 323(7321): 1103-1105. Emphasis on a perspective from the biomedical and health sciences.\n\n• Charles Ess and the ethics working committee of the Association of Internet Researchers. Provides access to the Ethics Working Committee document on Internet research ethics that was approved by voting members of the AoIR on November 27, 2002 Recommendations from the aoir ethics working committee1, 330 KB.\n\n• Internet Research Ethics: Introduction. An introduction, by Charles Ess, to papers that emerged from a panel presentation organized for a conference held at Lancaster University on December 14- December 16, 2001, building on the efforts of the Ethics Working Committee of the Association of Internet Researchers (AoIR). Emphasis on perspectives of researchers and scholars in the social sciences and humanities.\n\n• Ethical and Legal Aspects of Human Subjects Research in Cyberspace. Provides access to a report of a workshop held in Washington DC on June 10- June 11, 1999 PDF, 65 KB. Includes useful references to the earlier literature.\n\n• Johns, M. D., Chen, S., & Hall, G. J. (Eds.). (2004). Online Social Research: Methods, Issues & Ethics. Digital formations (p. 273). New York: P. Lang.\n\n• Elizabeth Buchanan (ed.) (2004). Readings in Virtual Research Ethics: Issues and Controversies. Hershey: Idea Group.\n\n• Charles Ess, (2009) Digital Media Ethics. London: Polity.\n\n• Elizabeth Buchanan (forthcoming in 2010) \"Internet Research Ethics: Past, Present, and Future\" in Robert Burnett, Mia Consalvo, and Charles Ess (eds.), The Handbook of Internet Studies. Wiley-Blackwell.\n\n• Boehlefeld, S. (1996). Doing the Right Thing: Ethical Cyber Research. The Information Society, 12(2)(2).\n\n• Natalie Young (2006) Internet Research Ethics blog.\n"}
{"id": "538671", "url": "https://en.wikipedia.org/wiki?curid=538671", "title": "Emanuel Kayser", "text": "Emanuel Kayser\n\nFriedrich Heinrich Emanuel Kayser (March 26, 1845 - November 29, 1927) was a German geologist and palaeontologist, born in Königsberg.\n\nHe was educated at the universities of Halle, Heidelberg and Berlin, where in 1871 he qualified as a lecturer in geology. From 1873 he worked as a state geologist for the \"Preußischen Geologischen Landesanstalt\" (Prussian Geological Survey), and in 1881 became a professor at the Berlin Mining Academy. In 1885 he succeeded Wilhelm Dunker as professor of geology and paleontology at the University of Marburg.\n\nHe is known for his work involving the stratigraphy, tectonics and paleontology of Paleozoic formations in Germany; especially the Harz and the Rhenish Massif. With Wilhelm Dames, he was co-editor of the journal \"Paläontologischen Abhandlungen\".\n\nAmong his separate works are \"Lehrbuch der Geologie\" (2 vols.): ii. \"Geologische Formationskunde\" (1891; 2nd ed., 1902), and i. \"Allgemeine Geologie\" (1893); vol. ii. (the volume first issued) was translated and edited by Philip Lake, under the title \"Textbook of Comparative Geology\" (1893). Another work is \"Beiträge zur Kenntniss der Fauna der Siegenschen Grauwacke\" (1892).\n\nKayser Bjerg, a mountain in Greenland, was named after him.\n"}
{"id": "37642436", "url": "https://en.wikipedia.org/wiki?curid=37642436", "title": "Erebor Mons", "text": "Erebor Mons\n\nErebor Mons is a mountain on Titan, the largest moon of the planet Saturn. It is located near Titan's equator, between 4-5° south and 35-36° west, centered on , in the western part of Quivira region. It is 40 km across, more than 1 km high and has lobate flow features to its north and east. It is probably a cryovolcano. Erebor Mons is situated about 470 km to the north-northeast of a larger cryovolcanic construct, Doom Mons.\n\nErebor Mons is one of the highest known mountains of Titan, but it is not readily discernible on radar or infrared images. It was discovered only when stereoscopic radar data allowed construction of an elevation map. It was imaged by Cassini radar 22 February and 10 April 2007.\n\nErebor Mons is named after Erebor, the \"Lonely Mountain\" that appears in J. R. R. Tolkien's fictional world of Middle-earth, most prominently in \"The Hobbit\". The name follows a convention that Titanean mountains are named after mountains in Tolkien's work. The name was formally announced on November 13, 2012.\n\n"}
{"id": "46815172", "url": "https://en.wikipedia.org/wiki?curid=46815172", "title": "Food and Nutrition Bulletin", "text": "Food and Nutrition Bulletin\n\nThe Food and Nutrition Bulletin is a quarterly peer-reviewed scientific journal that was founded by Dr. Nevin S. Scrimshaw in 1978 and is published by Sage Publications. The journal publishes articles that cover policy analysis, original scientific and social research, and academic reviews related to human nutrition and malnutrition in developing countries. It is an official publication of the Nevin Scrimshaw International Nutrition Foundation and is housed at the Friedman School of Nutrition Science and Policy at Tufts University.\n\nThe journal is abstracted and indexed by MEDLINE and Scopus. According to the \"Journal Citation Reports\", the journal's 2017 impact factor is 1.881, ranking it 63rd out of 133 journals in the category \"Food Science and Technology\", and 62nd out of 81 in the category \"Nutrition and Dietetics.\"\n"}
{"id": "29481631", "url": "https://en.wikipedia.org/wiki?curid=29481631", "title": "Fushih Pan", "text": "Fushih Pan\n\nFushih Pan (born 17 October 1957) is a Taiwanese plastic surgeon certified by the American Board of Plastic Surgery. Fushih Pan is also certified by the National Board of General Surgery of Taiwan and the Board of Plastic Surgery Specialist of the U.A.E.\n\nFushih Pan was born in Taipei, Taiwan the 17 of October 1957. He has a bachelor's degree in chemistry from the National Taiwan University in 1979, and a M.D. and Ph.D. in chemistry from the University of Chicago through 1986-1989. He completed all of his doctoral qualifications in the United States where he resided as a plastic surgeon in many hospitals and as an instructor in the department of plastic surgery in the University of Pennsylvania.\n\nFushih Pan represented Taiwan as a team member for the Physicians for Peace organization and Northwest Medical to offer medical skills and advice in Developing Countries who were in the state of war like Estonia, Mexico, and Honduras from 1993 to 1994.\n\nFushih Pan returned to Taiwan in 1995. There he established a clinic where he currently performs plastic surgery. During the following the years Fushih Pan has taken part in many research projects along with the University of Chicago, University of Pennsylvania as well as the National Taiwan University.\n\nHis most recent research on stem cell tracking using mesoporous nanoparticles, binding of short chain peptides on to gold and silver nanoparticles as a platform of biomarkers, and modification of MSC differentiation using nanopeptides in the National Taiwan University, have led him to the development of the MIRA Procedure in 2009.\n\nThe MIRA Procedure is a multidisciplinary method for treating many chronic disease such as heart or liver failure. This procedure has also proved to be useful in the field of cosmetic surgery as it has spawned a successful alternative to a facelift known as the MIRA Lift.\n\n\n\n"}
{"id": "8732281", "url": "https://en.wikipedia.org/wiki?curid=8732281", "title": "GOR method", "text": "GOR method\n\nThe GOR method (Garnier-Osguthorpe-Robson) is an information theory-based method for the prediction of secondary structures in proteins. It was developed in the late 1970s shortly after the simpler Chou-Fasman method. Like Chou-Fasman, the GOR method is based on probability parameters derived from empirical studies of known protein tertiary structures solved by X-ray crystallography. However, unlike Chou-Fasman, the GOR method takes into account not only the propensities of individual amino acids to form particular secondary structures, but also the conditional probability of the amino acid to form a secondary structure given that its immediate neighbors have already formed that structure. The method is therefore essentially Bayesian in its analysis.\n\nThe GOR method analyzes sequences to predict alpha helix, beta sheet, turn, or random coil secondary structure at each position based on 17-amino-acid sequence windows. The original description of the method included four scoring matrices of size 17×20, where the columns correspond to the log-odds score, which reflects the probability of finding a given amino acid at each position in the 17-residue sequence. The four matrices reflect the probabilities of the central, ninth amino acid being in a helical, sheet, turn, or coil conformation. In subsequent revisions to the method, the turn matrix was eliminated due to the high variability of sequences in turn regions (particularly over such a large window). The method was considered as best requiring at least four contiguous residues to score as alpha helices to classify the region as helical, and at least two contiguous residues for a beta sheet.\n\nThe mathematics and algorithm of the GOR method were based on an earlier series of studies by Robson and colleagues reported mainly in the \"Journal of Molecular Biology\" (e.g.) and The Biochemical Journal (e.g.). The latter describes the information theoretic expansions in terms of conditional information measures. The use of the word \"simple\" in the title of the GOR paper reflected the fact that the above earlier methods provided proofs and techniques somewhat daunting by being rather unfamiliar in protein science in the early 1970s; even Bayes methods were then unfamiliar and controversial. An important feature of these early studies, which survived in the GOR method, was the treatment of the sparse protein sequence data of the early 1970s by expected information measures. That is, expectations on a Bayesian basis considering the distribution of plausible information measure values given the actual frequencies (numbers of observations). The expectation measures resulting from integration over this and similar distributions may now be seen as composed of \"incomplete\" or extended zeta functions, e.g. z(s,observed frequency) − z(s,expected frequency) with incomplete zeta function z(s, n) = 1 + (1/2) + (1/3)+ (1/4) + …. +(1/n). The GOR method used s=1. Also, in the GOR method and the earlier methods, the measure for the contrary state to e.g. helix H, i.e. ~H, was subtracted from that for H, and similarly for beta sheet, turns, and coil or loop. Thus the method can be seen as employing a zeta function estimate of log predictive odds. An adjustable decision constant could also be applied, which thus also implies a decision theory approach; the GOR method allowed the option to use decision constants to optimize predictions for different classes of protein. The expected information measure used as a basis for the information expansion was less important by the time of publication of the GOR method because protein sequence data became more plentiful, at least for the terms considered at that time. Then, for s=1, the expression z(s,observed frequency) − z(s,expected frequency) approaches the natural logarithm of (observed frequency / expected frequency) as frequencies increase. However, this measure (including use of other values of s) remains important in later more general applications with high-dimensional data, where data for more complex terms in the information expansion are inevitably sparse (e.g.).\n\n"}
{"id": "11040991", "url": "https://en.wikipedia.org/wiki?curid=11040991", "title": "HATNet Project", "text": "HATNet Project\n\nThe Hungarian Automated Telescope Network (HATNet) project is a network of six small fully automated \"HAT\" telescopes. The scientific goal of the project is to detect and characterize extrasolar planets using the transit method. This network is used also to find and follow bright variable stars. The network is maintained by the Harvard-Smithsonian Center for Astrophysics.\n\nThe HAT acronym stands for \"Hungarian-made Automated Telescope\", because it was developed by a small group of Hungarians who met through the Hungarian Astronomical Association. The project started in 1999 and has been fully operational since May 2001.\n\nThe prototype instrument, HAT-1 was built from a 180 mm focal length and 65 mm aperture Nikon telephoto lens and a Kodak KAF-0401E chip of 512 × 768, 9 μm pixels. The test period was from 2000 to 2001 at the Konkoly Observatory in Budapest.\n\nHAT-1 was transported from Budapest to the Steward Observatory, Kitt Peak, Arizona, USA, in January 2001. The transportation caused serious damage to the equipment.\n\nLater built telescopes use Canon 11 cm diameter f/1.8L lenses for a wide-field of 8°×8°. It is a fully automated instrument with 2K x 2K Charge-coupled device (CCD) sensors. One HAT instrument operates at the Wise Observatory.\n\nHAT is controlled by a single Linux PC without human supervision. Data are stored in a MySQL database.\n\nFrom 2009, three other locations joined the HATNet with telescopes of completely new design. The telescopes are deployed to Australia, Namibia and Chile. Each system has eight (2*4) joint-mounted, quasi-parallel Takahashi Epsilon (180 mm diameter, f/2.8) astrographs with Apogee 4k*4k CCDs with overlapping fields of view. The processing computers are Xenomai-based industrial PCs with 10 TB of storage. The funding is provided until 2013.\n\nHAT-1 was developed during the undergraduate (and also the first year graduate) studies of Gáspár Bakos (Eötvös Loránd University) and at Konkoly Observatory (Budapest), under the supervision of Dr. Géza Kovács. In the development József Lázár, István Papp and Pál Sári also played an important role.\n\nTwenty-nine extrasolar planets have been discovered so far by the HATNet project (note that the discovery of the planet WASP-11b/HAT-P-10b, WASP-40b/HAT-P-27b and WASP-51b/HAT-P-30b was simultaneously announced by the SuperWASP team). All have been discovered using the transit method. In addition, the radial velocity followup has detected an additional companion, either a massive planet or a small brown dwarf around the star HAT-P-13, making this the first known transiting planet in a system with an outer companion in a well-characterised orbit.\n\n\"Light green rows indicate that the planet orbits one of the stars in a binary star system.\"\n\n\nA subset of HATNet light curves are available at the NASA Exoplanet Archive.\n\n\n\n"}
{"id": "41116110", "url": "https://en.wikipedia.org/wiki?curid=41116110", "title": "International Association of Arabic Dialectology", "text": "International Association of Arabic Dialectology\n\nThe International Association of Arabic Dialectology (, AIDA) is an association of researchers in Arabic dialects, from all over the world.\n\nAIDA was founded in 1992, in Paris, at the initiative of a group of prestigious Arabists, with the aim to encourage and promote the study of Arabic dialects.\n\nAIDA is nowadays the leading international association in this field of research and it has become a center that joins scholars from all over the world who are interested in any aspect of Arabic dialectology, including dialects which have not been described yet, dialectal geography, specific aspects of phonology, morphology and syntax, code-switching, koiné language, pidgin, creole, the lexicon of Arabic dialects, dialectal atlases, comparative and diachronic studies, sociolinguistics, teaching of Arabic dialects, and so on.\n\nAIDA organizes conferences that are held in well-known universities every two years (with some exceptions).\n\n\n\n\n\n► George Grigore (University of Bucharest, Romania) – president\n\n► Karima Ziamari (Moulay Ismail University, Meknes, Morocco) – vice-president\n\n► Kristen Brustad (University of Texas at Austin, USA) – vice-president\n\n► Liesbeth Zack (University of Amsterdam, Netherlands) – general secretary\n\n► Veronika Ritt-Benmimoun (University of Vienna, Austria) – treasurer\n\n"}
{"id": "31245274", "url": "https://en.wikipedia.org/wiki?curid=31245274", "title": "John Hamilton Blair", "text": "John Hamilton Blair\n\nJohn Hamilton Blair (born 1887) was a Scottish mariner, who was first officer aboard the SY \"Aurora\" during the Australasian Antarctic Expedition in 1913-14. He later served in the Royal Navy during the First World War, where he was awarded the Distinguished Service Cross, as the head of Pangbourne Nautical College, and returned to the Navy during the Second World War.\n\nDuring his early career, he worked aboard the Loch Line, which operated between Britain and Australia. He then joined a Melbourne shipping company, and worked the Australia-India trade. From 1913–14, Blair served as Chief Officer aboard the SY \"Aurora\", under John King Davis, during the final Antarctic voyage of the Australasian Antarctic Expedition (AAE). The Blair Islands, in Commonwealth Bay in Antarctica, were named after him by Douglas Mawson, the expedition leader.\n\nHe served as a lieutenant in the Royal Naval Reserve during the First World War, and was awarded the Distinguished Service Cross in 1917. Between the end of 1914 and September 1918, he made 68 submarine patrols in the Heligoland Bight, part of the blockade of the German fleet, before sailing to the Black Sea with HMS \"M1\", where he ended the war.\n\nHe later became Chief Executive Officer of Pangbourne Nautical College.\n\nDuring the Second World War he returned to active service, commanding the boarding vessel \"Maron\" from 1940 to 1942, and then the auxiliary anti-aircraft ship \"Monas Isle\" on the Tyne from 1942 to 1943.\n"}
{"id": "33392899", "url": "https://en.wikipedia.org/wiki?curid=33392899", "title": "Keith Christian", "text": "Keith Christian\n\nDr. Keith Christian is Professor of Zoology at Charles Darwin University where he teaches in the School of Environmental & Life Sciences.\n\nHe is a specialist in the physiology of amphibians and reptiles.\n"}
{"id": "4748446", "url": "https://en.wikipedia.org/wiki?curid=4748446", "title": "Lateral root", "text": "Lateral root\n\nLateral roots extend horizontally from the primary root (radicle) and serve to anchor the plant securely into the soil. This branching of roots also contributes to water uptake, and facilitates the extraction of nutrients required for the growth and development of the plant.\n\nMany different factors are involved in the formation of lateral roots. Regulation of root formation is tightly controlled by plant hormones such as auxin, and by the precise control of aspects of the cell cycle. Such control can be particularly useful: increased auxin levels, which help to promote lateral root development, occur when young leaf primordia form and are able to synthesise the hormone. This allows coordination of root development with leaf development, enabling a balance between carbon and nitrogen metabolism to be established.\n\nThe following description is for early events in lateral root formation of the model organism \"Arabidopsis thaliana\", where lateral roots typically form when the plant is between seven and nine days old.\nThe number of lateral roots corresponds to the number of xylem bundles.\n\n"}
{"id": "44838274", "url": "https://en.wikipedia.org/wiki?curid=44838274", "title": "Lindsay Zanno", "text": "Lindsay Zanno\n\nLindsay Zanno is an American vertebrate paleontologist and who is an expert in the taxonomy of Therizinosaurs and is known for her innovative use of X-ray computed tomography in reconstructing dinosaurs. She is the director of the Paleontology & Geology Research Laboratory at the North Carolina Museum of Natural Science. With Peter J. Makovicky of the Field Museum of Natural History, Zanno excavated a large carnivorous allosaurid dinosaur in Utah, \"Siats meekerorum\", that was unusual because the Neovenatoridae, carnivorous allosaurids, had been unknown in North America.\n\nZanno received her B.Sc. from the University of New Mexico in 1999, and her graduate degrees from the University of Utah (M.Sc. in 2004, Ph.D. in 2008).\n"}
{"id": "12187974", "url": "https://en.wikipedia.org/wiki?curid=12187974", "title": "List of Kentucky state symbols", "text": "List of Kentucky state symbols\n\nThe Commonwealth of Kentucky has 29 official state emblems, as well as other designated places and events. The majority are determined by acts of the Kentucky General Assembly and recorded in Title I, Chapter 2 of the Kentucky Revised Statutes. The state's nickname – \"The Bluegrass State\" – is traditional, but has never been passed into law by the General Assembly. It does, however, appear on the state's license plates. Despite the nickname's popularity, the General Assembly has not designated bluegrass (or any other grass) as the official state grass.\n\nThe first symbol was the Seal of Kentucky, which was made official in 1792. The original seal also contained the future state motto. It served as the state's only emblem for over 130 years until the adoption of the state bird in 1926. Enacted by law in 2010, the newest symbols of Kentucky are the state insect, the honey bee, and the state sports car, the Chevrolet Corvette.\n\n"}
{"id": "18090043", "url": "https://en.wikipedia.org/wiki?curid=18090043", "title": "List of discrete event simulation software", "text": "List of discrete event simulation software\n\nThis is a list of notable discrete event simulation software.\n\n"}
{"id": "562259", "url": "https://en.wikipedia.org/wiki?curid=562259", "title": "List of eponymously named diseases", "text": "List of eponymously named diseases\n\nAn eponymous disease is a disease named after a person: usually the physician who first identified the disease or, less commonly, a patient who suffered from the disease.\nEponyms are a longstanding tradition in Western science and medicine. Being awarded an eponym is regarded as an honor: \"Eponymity, not anonymity, is the standard.\" The scientific and medical communities regard it as bad form to attempt to eponymise oneself.\n\nTo discuss something, it must have a name. At a time when medicine lacked tools to investigate underlying causes of many syndromes, the eponym was a convenient way to label a disease.\n\nSome diseases are named after the person who first described the condition—typically by publishing an article in a respected medical journal. Rarely, an eponymous disease is named after a patient, examples being Lou Gehrig's disease, Hartnup disease, and Mortimer's disease. In at least one instance, Machado–Joseph disease, the eponym is derived from the surnames of the patriarchs of two families in which the condition was initially described. Instances also exist of eponyms named for fictional persons who displayed characteristics attributed to the syndrome. These include Miss Havisham syndrome, named for a Dickens character, and Plyushkin syndrome, named for a Gogol's character; these also happen to be alternative names for the same symptom complex. At least two eponymous disorders follow none of the foregoing conventions: Fregoli delusion, and Munchausen syndrome.\n\nRelated disease naming structures reference place names (Bornholm disease, Lyme disease, Ebola virus disease), and societies, as in the case of Legionnaires' disease. These, however, are not eponyms.\n\nIn 1975, the Canadian National Institutes of Health held a conference that discussed the naming of diseases and conditions. This was reported in \"The Lancet\" where the conclusion was summarized as: \"The possessive use of an eponym should be discontinued, since the author neither had nor owned the disorder.\" Medical journals, dictionaries and style guides remain divided on this issue. European journals tend towards continued use of the possessive, while US journals are largely discontinuing its use. The trend in possessive usage varies between countries, journals, and diseases. \n\nAssociating an individual's name with a disease merely based on describing it confers only an eponymic; the individual must have been either affected by the disease or have died from it for the name to be termed autoeponymic. Thus, an 'autoeponym' is a medical condition named in honor of an individual who was affected by or died as a result of the disease which he had described or identified or, in the case of a non-physician patient, from which the patient suffered. Autoeponyms may use either the possessive or non-possessive form, with the preference to use the non-possessive form for a disease named for a physician who first described it and the possessive form in cases of a disease named for a patient (commonly, but not always, the first patient) who had the particular disease. Autoeponyms listed in this entry conform to those conventions with regard to the possessive and non-possessive forms.\n\nExamples of autoeponyms include:\n\nThe current trend is away from the use of eponymous disease names, towards a medical name that describes either the cause or primary signs. Reasons for this include:\n\nArguments for maintaining eponyms include:\n\nThe usage of the genitive apostrophe in disease eponyms has followed different trends. While it remains common for some diseases, it has dwindled for others.\n\nAs described above, multiple eponyms can exist for the same disease. In these instances, each is listed individually (except as described below), followed by an in-line parenthetical entry beginning 'aka' ('also known as') that lists all alternative eponyms. This facilitates use of the list for a reader who knows a particular disease only by one of its eponyms, without the necessity of cross-linking entries.\n\nIt sometimes happens that an alternative eponym, if listed separately, would immediately alphabetically precede or succeed another entry for the same disease. There are three conventions that have been applied to these instances:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38288006", "url": "https://en.wikipedia.org/wiki?curid=38288006", "title": "List of medical schools in Taiwan", "text": "List of medical schools in Taiwan\n"}
{"id": "12198611", "url": "https://en.wikipedia.org/wiki?curid=12198611", "title": "Markarian's Chain", "text": "Markarian's Chain\n\nMarkarian's Chain is a stretch of galaxies that forms part of the Virgo Cluster. When viewed from Earth, the galaxies lie along a smoothly curved line. Charles Messier first discovered two of the galaxies, M84 and M86, in 1781. The other galaxies seen in the chain were first mentioned in John Louis Emil Dreyer's New General Catalogue, published in 1888. It was ultimately named after the Armenian astrophysicist, Benjamin Markarian, who discovered their common motion in the early 1960s. Member galaxies include M84 (NGC 4374), M86 (NGC 4406), NGC 4477, NGC 4473, NGC 4461, NGC 4458, NGC 4438 and NGC 4435. It is located at RA 12 27 and Dec +13° 10′.\n\nThe bright members of the chain are visible through small telescopes. Larger telescopes can be used to view the fainter galaxies.\n\nAt least seven galaxies in the chain appear to move coherently, although others appear to be superposed by chance. Six of the points on the chain can be marked by galaxies. The other two points are pairs of galaxies.\n"}
{"id": "30782834", "url": "https://en.wikipedia.org/wiki?curid=30782834", "title": "Matched Z-transform method", "text": "Matched Z-transform method\n\nThe matched Z-transform method, also called the pole–zero mapping or pole–zero matching method, and abbreviated MPZ or MZT, is a technique for converting a continuous-time filter design to a discrete-time filter (digital filter) design.\n\nThe method works by mapping all poles and zeros of the \"s\"-plane design to \"z\"-plane locations formula_1, for a sample interval formula_2. So an analog filter with transfer function:\n\nis transformed into the digital transfer function\n\nThe gain formula_5 must be adjusted to normalize the desired gain, typically set to match the analog filter's gain at DC by setting formula_6 and formula_7 and solving for formula_5.\n\nSince the mapping wraps the \"s\"-plane's formula_9 axis around the \"z\"-plane's unit circle repeatedly, any zeros (or poles) greater than the Nyquist frequency will be mapped to an aliased location.\n\nIn the (common) case that the analog transfer function has more poles than zeros, the zeros at formula_10 may optionally be shifted down to the Nyquist frequency by putting them at formula_11, dropping off like the BLT.\n\nThis transform doesn't preserve time- or frequency-domain response (though it does preserve stability and minimum phase), and so is not widely used. More common methods include the bilinear transform and impulse invariance methods. MZT does provide less high frequency response error than the BLT, however, making it easier to correct by adding additional zeros, which is called the MZTi (for \"improved\").\n\nA specific application of the \"matched Z-transform method\" in the digital control field, is with the Ackermann's formula, which changes the poles of the controllable system; in general from an unstable (or nearby) location to a stable location.\n"}
{"id": "44218867", "url": "https://en.wikipedia.org/wiki?curid=44218867", "title": "MitoMap", "text": "MitoMap\n\nMitoMap is a real time haplotyping protocol that analyzes pathogenic variants that cause several mitochondrial diseases. It was carried out real-time for the first time during the 2013 NexGen Genomics & Bioinformatics Technologies conference at Delhi, India from November 14–16. The results have been published online.\n"}
{"id": "1520009", "url": "https://en.wikipedia.org/wiki?curid=1520009", "title": "NISMART", "text": "NISMART\n\nNISMART or the National Incidence Studies of Missing, Abducted, Runaway and Throwaway Children, was a research project supported by the United States Department of Justice. It was enacted to address the 1984 Missing Children's Assistance Act (Pub.L. 98-473). This required the Office of Juvenile Justice and Delinquency Prevention (OJJDP) to conduct periodic national incidence studies to determine the actual number of children reported missing and the number recovered. The first study, NISMART-1 in 1988 categorized the various missing children reports and estimated the number of missing and recovered children in each. In 1999, a second study dubbed NISMART-2 was initiated. The two studies cannot be compared against each other due to categorizing techniques being distinct in each study. Also, NISMART-2 interviewed youth directly whereas NISMART-1 did not.\n\nA federal grant of $1 million for NISMART-3 was announced by the Office of Juvenile Justice and Delinquency Prevention in 2010.\n\n\n"}
{"id": "32220089", "url": "https://en.wikipedia.org/wiki?curid=32220089", "title": "Nigeria Prize for Science", "text": "Nigeria Prize for Science\n\nThe Nigeria Prize for Science is a Nigerian science award given annually since 2004 for excellence in science breakthroughs. It is the country's highest scientific award. The award is sponsored by Nigeria Liquefied Natural Gas company. The prize describes itself as \"bringing Nigerian scientists to public attention and celebrating excellence in scientific breakthroughs\".\n\nThe Prize was initially $20,000 each in Literature and Science.This was increased to $30,000 in 2006, and again to $50,000 in 2008. In 2011 the prize was increased to $100,000.\n\n\n"}
{"id": "54805214", "url": "https://en.wikipedia.org/wiki?curid=54805214", "title": "Nise: The Heart of Madness", "text": "Nise: The Heart of Madness\n\nNise: The Heart of Madness () is a 2015 Brazilian docudrama directed by Roberto Berliner. Starring Glória Pires, the film is based on the life of psychiatrist Nise da Silveira, a pioneer of occupational therapy in Brazil. \"Nise\" also features Fabrício Boliveira, Fernando Eiras, Perfeito Fortuna, Roberta Rodrigues, Augusto Madeira, Simone Mazzer, and Zé Carlos Machado.\n\nIn 1944, the doctor Nise da Silveira returns to work in a psychiatric hospital in the suburbs of Rio de Janeiro and refuses to use electroshock and lobotomy in the treatment of schizophrenics. Insulated from the other doctors, she must reorganize the abandoned occupational-therapy sector, where she forms a new clinical approach of listening and observing, further alienating herself from her colleagues. Insisting that those under her care be referred to as clients, rather than patients, she encourages the freedom of expression through art, discovering her clients' talents. She opens the Casa das Palmeiras, a clinic and studio, at the hospital, and later starts a museum dedicated to her clients' artwork.\n\nAccording to director Roberto Berliner, the idea for \"Nise\" came from Bernardo Horta, brother of the film's director of photography, André Horta. André started organizing some of da Silveira's writing, then passed the project to Berliner in 2003. In all, research for the film took 13 years.\n\nThe film was released over 3 years in different parts of the world. In 2015, it was released in Japan. Throughout 2016, it was released in Sweden, France, Brazil, and the Netherlands. In 2017, it was released in the United States by Outsider Pictures and Strand Releasing.\nIn 2018, it was released to selected (about 600) screens in mainland China from January 5 through February 4.\n\nAccording to Rotten Tomatoes, \"Nise\" received generally positive reviews with a rating of 86%. Neil Genzlinger of \"The New York Times\" called it \"a mesmerizing drama\", Daphne Howland of \"The Village Voice\" wrote \"the actors’ portrayals ... ring true\", Jonathan Holland of \"The Hollywood Reporter\" called it \"heartwarming but unsentimental\", and J. R. Jones of the \"Chicago Reader\" wrote that it \"has its powerful moments but ... turns into a black-and-white struggle between a caring, enlightened woman and a cadre of hard-hearted, benighted men.\"\n"}
{"id": "27951572", "url": "https://en.wikipedia.org/wiki?curid=27951572", "title": "Nottingham Caves Survey", "text": "Nottingham Caves Survey\n\nThe Nottingham Caves Survey was a research project the aim of which was to scan every accessible cave from the 700+ man-made sandstone caves that are known to be present in the city of Nottingham. It was conducted by Trent and Peak Archaeology, at the University of Nottingham. It was managed by Dr David Strange-Walker with Julia Clarke, with documentary research undertaken by Scott Lomax. The project was funded primarily by both the Greater Nottingham Partnership, who have an interest in utilising the caves for increasing tourism and helping grow the local economy, and English Heritage, who are motivated by an interest in preserving the remaining heritage of Nottingham. \n\nThe project built upon the data held by Nottingham City Council and that collected in the 1980s as part of the British Geological Survey (BGS), where all the known caves of Nottingham were recorded into the BGS register. Nottingham City Council's Urban Archaeological Database (UAD) was a key resource. The Nottingham Caves Survey team visited some of the caves listed in the BGS register, asked permission from the owner to view the cave, and if the conditions were suitable, scanned the cave structure using a 3D laser scanner. The laser scanner builds up a 'point cloud' by collecting billions of survey points, which make up the black and white 3D images. Fish-eye photographs were taken from exactly the same position on the tripod around 360 degrees. The 'point cloud' can be merged with the photographs and manipulated to make plans, animations and fly-through videos, available on the project website.\n\nThe Nottingham Caves Survey recorded data from an eclectic range of cave systems, from the famous Mortimer's Hole and King David's Dungeon at the Nottingham Ducal Mansion, the cave complexes of the renowned Ye Olde Trip to Jerusalem, Ye Olde Salutation Inn and The Bell Inn - claimed to be three of the oldest inns in England - to the cave cellars of houses dotted around the city, as each are considered to have a unique story to tell. The project commenced in 2010, with the surveying completed in 2012. In total 64 caves were surveyed.\n\nThe project has received widespread press, due to the possible implications on well known Nottingham lore like that of the famous legend of Robin Hood, by such varied media outlets as ThisIsNottingham, BBC News, Science Daily and the New York Times. The project has also received positive attention for the environmental policies of the surveying team, as most of the equipment was transported between sites in trailers by bicycles.\n\n"}
{"id": "4368752", "url": "https://en.wikipedia.org/wiki?curid=4368752", "title": "Offshoot (plant)", "text": "Offshoot (plant)\n\nOffshoots are lateral shoots that are produced on the main stem of a plant. They may be known colloquially as \"suckers\". Also see basal shoot.\n\n"}
{"id": "44305371", "url": "https://en.wikipedia.org/wiki?curid=44305371", "title": "On the Connexion of the Physical Sciences", "text": "On the Connexion of the Physical Sciences\n\nOn the Connexion of the Physical Sciences is one of the biggest selling science books in the 19th century. It was written by Mary Somerville in 1834.\n\nThe book went through many editions and was translated into several European languages. In a review of the book in March 1834, William Whewell coined the word \"scientist\".\n"}
{"id": "48311903", "url": "https://en.wikipedia.org/wiki?curid=48311903", "title": "OpenDOAR", "text": "OpenDOAR\n\nOpenDOAR: Directory of Open Access Repositories is a UK-based website that lists academic open access repositories. It is searchable by locale, content and other measures. The service does not require complete repository details and does not search repositories' metadata.\n\nOpenDOAR is maintained by the University of Nottingham under the SHERPA umbrella of services and was developed in collaboration with Lund University. The project is funded by the Open Science Institute, Jisc, the Consortium of Research Libraries (CURL) and SPARC Europe.\n\nAs of 2015, OpenDOAR and the UK-based Registry of Open Access Repositories (ROAR) \"are considered the two leading open access directories worldwide. ROAR is the larger directory and allows direct submissions to the directory. OpenDOAR controls submission of materials and is dependent on the discretion of its staff. OpenDOAR requires open access of scholarly publications; whereas ROAR allows other types of materials to be included. ROAR allows filtering by country, type of repository, and sorting by repository name.\"\n\n\n"}
{"id": "33938545", "url": "https://en.wikipedia.org/wiki?curid=33938545", "title": "Policy advocacy", "text": "Policy advocacy\n\nPolicy advocacy is defined as active, covert, or inadvertent support of a particular policy or class of policies. Whether it is proper for scientists and other technical experts to act as advocates for their personal policy preferences is contentious. In the scientific community, much of the controversy around policy advocacy involves precisely defining the proper role of science and scientists in the political process. Some scientists choose to act as policy advocates, while others regard such a dichotomous role as inappropriate.\n\nProviding technical and scientific information to inform policy deliberations in an objective and relevant way is recognized as a difficult problem in many scientific and technical professions. The challenge and conflicts have been studied for those working as stock analysts in brokerage firms, for medical experts testifying in malpractice trials, for funding officers at international development agencies, and for intelligence analysts within governmental national security agencies. The job of providing accurate, relevant, and policy neutral information is especially challenging if highly controversial policy issues (such as climate change) that have a significant scientific component. The use of normative science by scientists is a common method used to subtly advocate for preferred policy choices.\n\n\n"}
{"id": "36764607", "url": "https://en.wikipedia.org/wiki?curid=36764607", "title": "Positive systems", "text": "Positive systems\n\nPositive systems constitute a class of systems that has the important property that its state variables are never negative, given a positive initial state. These systems appear frequently in practical applications, as these variables represent physical quantities, with positive sign (levels, heights, concentrations, etc.).\n\nThe fact that a system is positive has important implications in the control system design, as the system. It is also important to take this positivity into account for state observer design, as standard observers (for example Luenberger observers) might give illogical negative values.\n\n"}
{"id": "58910447", "url": "https://en.wikipedia.org/wiki?curid=58910447", "title": "Research site", "text": "Research site\n\nA research site is a place where people conduct research. Common research sites include universities, hospitals, research institutes, and field research locations.\n\nIn clinical research a research site conducts all or part of a clinical trial. For clinical trials which recruit research participants in multiple locations, often the research will have a headquarters then multiple regional research sites to conduct the research in that region. In a network of research sites where all are recruiting study participants, sites with low recruitment benefit from coaching from sites with high recruitment.\n\nCharacteristics of good clinical research sites include setting good timelines, early participant recruitment, and having a management plan for efficiency.\n\nResearchers in nursing have reported challenges accessing the facilities designated for conventional medical research.\n\nThe design of a research site should have a means of detecting fraud.\n\nResearchers who do not have a cultural tie to a research population may have difficulty doing ethnographic research with that community.\n\n"}
{"id": "30965987", "url": "https://en.wikipedia.org/wiki?curid=30965987", "title": "Rudolf Stahlecker", "text": "Rudolf Stahlecker\n\nRudolf Stahlecker (25 November 1898 in Sternenfels near Pforzheim – 26 October 1977 in Urach) was a German geologist and biology teacher.\n\nHe studied with the German paleontologist Friedrich von Huene at the University of Tübingen, Germany. He participated in expeditions to collect fossils in the geopark of Paleorrota in 1928 and 1929. He also made several collections of fossils in Argentina.\n\nIn his honor, the dicynodonts Stahleckeria potens, received its name. This dicynodonts was collected in Paleontological Site Chiniquá, São Pedro do Sul.\n\nAfter finishing his doctorate, Stahlecker did not become a scientist, but a biology teacher at a school Stuttgart. \"The \"Führer\" wants to teach people to think again biologically, we scientists have to be here its first collaborators,\" was his motto. After The War he had to pause for some years during denazification, although he had not as intensively be involved in National Socialism as his brother, the SS Brigade Leader and Commander of the \"Einsatzgruppe A\" Walter Stahlecker.\n\n"}
{"id": "37717597", "url": "https://en.wikipedia.org/wiki?curid=37717597", "title": "Socioeconomic engineering", "text": "Socioeconomic engineering\n\nSocioeconomic engineering is the application of economic and political means to bring about desired changes in a society.\n\nIt is not an actual field of \"engineering\" as such but is more of an applied interdisciplinary field of political science, sociology and economics.\n\n"}
{"id": "5057555", "url": "https://en.wikipedia.org/wiki?curid=5057555", "title": "Soni Oyekan", "text": "Soni Oyekan\n\nSoni Olufemi Olubunmi Oyekan is a Nigerian-American chemical engineer.\n\nSoni Oyekan was born in Aba, Nigeria on June 1, 1946 and came to the United States to study in 1966. Soni completed his high school studies at St. Paul's College, Zaria, in 1963, and his advanced high school education at Olivet Baptist High School in Oyo in 1965. Oyekan was a recipient of an African Scholarship Program of American Universities (ASPAU) award which was administered by the African American Institute (AAI) in 1966. The scholarship award enabled Soni to pursue his studies in Engineering and Applied Sciences with specialization in Chemical Engineering at Yale University. He completed his Bachelor of Science degree program at Yale University in 1970. After his undergraduate studies at Yale, Soni moved on to Pittsburgh, Pennsylvania and completed graduate studies in Chemical Engineering at Carnegie Mellon University. He was awarded the Master of Science (MS) degree in 1972 and the doctoral (PhD) degree in 1977. His MS degree thesis title was on \"The Stability of Low Tension Interfaces—Effects of layers of Discrete Dipoles and Charges\". The title of his doctoral thesis was \"An Infrared Spectroscopic Study of the Isomerization and Hydrogenation of Cyclic Olefins Over Zinc Oxide\". Soni has over 40 years of experience in petroleum refining technologies and operations.\n\nSoni Oyekan is the President and CEO of Prafis Energy Solutions. Prafis Energy Solutions is an oil refining and energy consulting services company. The company is located in Richmond, Texas. Prior to start up of his oil and gas processing consulting services company in 2013, Dr. Oyekan had contributed significantly to chemical engineering and oil refining with his inventions and studies on catalytic systems and the catalytic naphtha reforming process. His patents have been applied successfully in the efficient use of reactor engineering and catalysis for the processing of crude oil to meet consumer demands for transportation fuels, heating oil, propane and butane gases.\nDr Soni Oyekan has been recognized for his contributions in oil refining, and for his extensive volunteer and leadership roles in the American Institute of Chemical Engineers (AIChE). Soni has held a variety of positions in AIChE. The positions include chair of Fuels and Petrochemicals Division (F&PD), chair of the Minority Affairs Committee (MAC) and a director of the AIChE Executive Board. He is currently a member of the Foundation Board of Trustees of AIChE. Soni has contributed over the years in numerous technical discussions on petroleum refining at the annual National Petroleum Refiners Association (NPRA) Q&A conferences. NPRA is now the American Fuel and Petrochemical Manufacturers (AFPM). Dr. Oyekan retired in January 2013 from Marathon Petroleum Company after fourteen years as the Reforming and Isomerization Technologist of the company.\n\nDr. Oyekan has received numerous recognition for his contributions in oil refining, chemical engineering and in the mentoring of engineers and technical personnel. He is a member of the AIChE Foundation Board of Trustees and a Fellow of the AIChE. Soni is a member of Sigma Xi, Phi Kappa Phi honor societies, and of the Yale Manuscript Society. He was honored by AIChE's Minority Affairs Committee (MAC) with its Distinguished Service award in 2000. The Fuels and Petrochemicals Division honored him with its Distinguished Service award in 2002. Dr Soni Oyekan was named an Eminent Black Chemical Engineer at the AIChE Centennial Meeting in 2008. He was the recipient of the AIChE William W. Grimes award for excellent contributions in chemical engineering and for the mentoring of under represented minority groups in 2008. Dr. Soni Oyekan was the recipient of the 2009 Percy Lavon Julian award. The Percy Julian award is the most prestigious honor given by the National Organization for the Professional Advancement of Black Chemists and Chemical Engineers (NOBCChE). The award recognizes and honors a recipient's scientific contributions and achievements, dedication to research, commitment to the educational development of others and passion for the chemistry profession. He is listed in the 2000 13th edition of Who's Who Among African Americans.\n\nSoni Oyekan has 14 patents including 5 US patents and over 70 publications on a variety of topics in petroleum refining and catalysis. He is a co-author of Catalyst Regeneration and Continuous Reforming Issues with P. K. Doolin and D. J. Zalewski, in the Catalytic Naphtha Reforming book, Second Edition, edited by G. J. Antos and A. M. Aitani, Marcel Dekker, 2004. Oyekan is the author of Catalytic Naphtha Reforming Process, CRC Press, 2018.\n\nSoni and Priscilla Ann Oyekan have four children and they reside in Richmond, Texas.\n"}
{"id": "36874091", "url": "https://en.wikipedia.org/wiki?curid=36874091", "title": "Space Chronicles", "text": "Space Chronicles\n\nSpace Chronicles: Facing the Ultimate Frontier is the 2012 anthology by Neil deGrasse Tyson covering his various writings relating to the history and future of NASA and space travel in general.\n\nOne of the chapters is a transcript of his attendance as a guest on the Rationally Speaking podcast in 2010, when he explained his justification for spending large amounts of government money on space programs.\n\nTyson intended the book's original title to be, \"Failure to Launch: The Dreams and Delusions of Space Enthusiasts\".\n"}
{"id": "1884226", "url": "https://en.wikipedia.org/wiki?curid=1884226", "title": "Spectrofluorometer", "text": "Spectrofluorometer\n\nA spectrofluorometer is an instrument which takes advantage of fluorescent properties of some compounds in order to provide information regarding their concentration and chemical environment in a sample. A certain excitation wavelength is selected, and the emission is observed either at a single wavelength, or a scan is performed to record the intensity versus wavelength, also called an emission spectra. The instrument is used in fluorescence spectroscopy.\n\nGenerally, spectrofluorometers use high intensity light sources to bombard a sample with as many photons as possible. This allows for the maximum number of molecules to be in an excited state at any one point in time. The light is either passed through a filter, selecting a fixed wavelength, or a monochromator, which allows a wavelength of interest to be selected for use as the exciting light. The emission is collected at the perpendicular to the emitted light. The emission is also either passed through a filter or a monochromator before being detected by a photomultiplier tube, photodiode, or charge-coupled device detector. The signal can either be processed as digital or analog output.\n\nSystems vary greatly and a number of considerations affect the choice. The first is the signal-to-noise ratio. There are many ways to look at the signal to noise of a given system but the accepted standard is by using water Raman. Sensitivity or detection limit is another specification to be considered, that is how little light can be measured. The standard would be fluorescein in NaOH, typical values for a high end instrument are in the femtomolar range.\n\nThese systems come with many options, including:\n"}
{"id": "16840869", "url": "https://en.wikipedia.org/wiki?curid=16840869", "title": "Sputnik (rocket)", "text": "Sputnik (rocket)\n\nThe Sputnik rocket was an unmanned orbital carrier rocket designed by Sergei Korolev in the Soviet Union, derived from the R-7 Semyorka ICBM. On 4 October 1957, it was used to perform the world's first satellite launch, placing \"Sputnik 1\" into a low Earth orbit.\n\nTwo versions of the Sputnik were built, the Sputnik-PS (GRAU index 8K71PS), which was used to launch \"Sputnik 1\" and later \"Sputnik 2\", and the Sputnik (8A91), which failed to launch a satellite in April 1958, and subsequently launched \"3\" on 15 May 1958.\n\nA later member of the R-7 family, the Polyot, used the same configuration as the Sputnik rocket, but was constructed from Voskhod components. Because of the similarity, the Polyot was sometimes known as the Sputnik 11A59.\n\n\nThe Sputnik 8A91 had more powerful 8D76 and 8D77 engines installed, increasing its payload capacity, and allowing it to launch much heavier satellites than \"Sputnik 1\" and \"Sputnik 2\". It was launched two times, in 1958. The first launch, on 27 April, failed due to vibrations that unexpectedly occurred during the flight along the longitudinal axis of the rocket. On 15 May, it successfully launched Sputnik 3.\n\n"}
{"id": "24731031", "url": "https://en.wikipedia.org/wiki?curid=24731031", "title": "Terminologia Histologica", "text": "Terminologia Histologica\n\nThe Terminologia Histologica (TH) is a controlled vocabulary for use in cytology and histology. In April 2011, \"Terminologia Histologica\" was published online by the Federative International Programme on Anatomical Terminologies (FIPAT), the successor of FCAT.\n\nIt was intended to replace \"Nomina Histologica\". The Nomina Histologica was introduced in 1977, with the fourth edition of Nomina Anatomica.\n\nIt was developed by the Federative International Committee on Anatomical Terminology.\n\n\n\n"}
{"id": "958497", "url": "https://en.wikipedia.org/wiki?curid=958497", "title": "The Flying Circus of Physics", "text": "The Flying Circus of Physics\n\nThe Flying Circus of Physics by Jearl Walker (1975, published by John Wiley and Sons, second edition in 2006), is a book that poses (and answers) about a thousand questions concerned with everyday physics. The emphasis is strongly on phenomena that might be encountered in one's daily life.\n\nFrom the preface: \"if you start thinking about physics when you are cooking, flying, or just lazing next to a stream, then I will feel the book was worthwhile\".\n\nTypically, the questions posed by the book are about phenomena that many readers will have encountered, but not thought through physically. For example:\n\n"}
{"id": "943811", "url": "https://en.wikipedia.org/wiki?curid=943811", "title": "Tiberius Cavallo", "text": "Tiberius Cavallo\n\nTiberius Cavallo (also Tiberio) (30 March 1749 – 21 December 1809) was an Italian physicist and natural philosopher.\n\nHe was born at Naples, where his father was a physician.\n\nIn 1771 he moved to England with the intention of pursuing a mercantile career, but he soon turned his attention to scientific work. He made several ingenious improvements in scientific instruments. He became a Fellow of the Royal Society in 1779, and gave annual Bakerian Lectures from 1780 to 1792.\n\nCavallo was often cited in the literature of his time as inventor of Cavallo's multiplier, a device he used for the amplification of small electric charges, making them observable and measurable in an electroscope. He also worked on refrigeration, and his work influenced pioneer balloonist Jean-Pierre Blanchard. He published on musical temperament.\n\nHe died in London on 21 December 1809.\n\nHe is buried in Old St Pancras Churchyard. The grave is lost but he is listed on the Burdett Coutts memorial of 1879 to the many important persons buried therein.\n\nHe published numerous works on different branches of physics, including:\n\nFor \"Rees's Cyclopædia\" he contributed articles on Electricity, Machinery and Mechanics, but the topics are not known.\n\n"}
{"id": "58019774", "url": "https://en.wikipedia.org/wiki?curid=58019774", "title": "Tselina", "text": "Tselina\n\nTselina or Virgin lands (; ) is an umbrella term for underdeveloped, scarcely populated, high-fertility lands often covered with the chernozem soil. The lands were mostly located in the steppes of the Volga region, Northern Kazakhstan and Southern Siberia. \n\nThe term became widely used in the late 1950s and early 1960s in the Soviet Union during the Virgin Lands Campaign () - a state development and resettlement campaign to turn the lands into a major agriculture producing region.\n\n"}
{"id": "19506355", "url": "https://en.wikipedia.org/wiki?curid=19506355", "title": "UGENE", "text": "UGENE\n\nUGENE is computer software for bioinformatics. It works on desktop computer operating systems such as Windows, macOS, or Linux. It is released as free and open-source software, under a GNU General Public License (GPL) version 2.\n\nUGENE helps biologists to analyze various biological genetics data, such as sequences, annotations, multiple alignments, phylogenetic trees, NGS assemblies, and others. The data can be stored both locally (on a personal computer) and on a shared storage (e.g., a lab database).\n\nUGENE integrates dozens of well-known biological tools, algorithms, and original tools in the context of genomics, evolutionary biology, virology, and other branches of life science. UGENE provides a graphical user interface (GUI) for the pre-built tools so biologists with no computer programming skills can access those tools more easily.\n\nUsing UGENE Workflow Designer, it is possible to streamline a multi-step analysis. The workflow consists of blocks such as data readers, blocks executing embedded tools and algorithms, and data writers. Blocks can be created with command line tools or a script. A set of sample workflows is available in the Workflow Designer, to annotate sequences, convert data formats, analyze NGS data, etc.\n\nBeside the graphical interface, UGENE also has a command-line interface. Workflows may also be executed thereby.\n\nTo improve performance, UGENE uses multi-core processors (CPUs) and graphics processing units (GPUs) to optimize a few algorithms.\n\nThe software supports the following features:\n\nThe Sequence View is used to visualize, analyze and modify nucleic acid or protein sequences. Depending on the sequence type and the options selected, the following views can be present in the Sequence View window:\n\nThe Alignment Editor allows working with multiple nucleic acid or protein sequences - aligning them, editing the alignment, analyzing it, storing the consensus sequence, building a phylogenetic tree, and so on.\n\nThe Phylogenetic Tree Viewer helps to visualize and edit phylogenetic trees. It is possible to synchronize a tree with the corresponding multiple alignment used to build the tree.\n\nThe \"Assembly Browser\" project was started in 2010 as an entry for Illumina iDEA Challenge 2011. The browser allows users to visualize and browse large (up to hundreds of millions of short reads) next generation sequence assemblies. It supports SAM, BAM (the binary version of SAM), and ACE formats. Before browsing assembly data in UGENE, an input file is converted to a UGENE database file automatically. This approach has its pros and cons. The pros are that this allows viewing the whole assembly, navigating in it, and going to well-covered regions rapidly. The cons are that a conversion may take time for a large file, and needs enough disk space to store the database.\n\n\"UGENE Workflow Designer\" allows creating and running complex computational workflow schemas.\n\nThe distinguishing feature of Workflow Designer, relative to other bioinformatics workflow management systems is that workflows are executed on a local computer. It helps to avoid data transfer issues, whereas other tools’ reliance on remote file storage and internet connectivity does not.\n\nThe elements that a workflow consists of correspond to the bulk of algorithms integrated into UGENE. Using Workflow Designer also allows creating custom workflow elements. The elements can be based on a command-line tool or a script.\n\nWorkflows are stored in a special text format. This allows their reuse, and transfer between users.\n\nA workflow can be run using the graphical interface or launched from the command line. The graphical interface also allows controlling the workflow execution, storing the parameters, and so on.\n\nThere is an embedded library of workflow samples to convert, filter, and annotate data, with several pipelines to analyze NGS data developed in collaboration with NIH NIAID. A wizard is available for each workflow sample.\n\n\nUGENE is primarily developed by Unipro LLC with headquarters in Akademgorodok of Novosibirsk, Russia. Each iteration lasts about 1–2 months, followed by a new release. Development snapshots may also be downloaded.\n\nThe features to include in each release are mostly initiated by users.\n\n\n\n"}
{"id": "3480707", "url": "https://en.wikipedia.org/wiki?curid=3480707", "title": "Zarankiewicz problem", "text": "Zarankiewicz problem\n\nThe Zarankiewicz problem, an unsolved problem in mathematics, asks for the largest possible number of edges in a bipartite graph that has a given number of vertices but has no complete bipartite subgraphs of a given size. It belongs to the field of extremal graph theory, a branch of combinatorics, and is named after the Polish mathematician Kazimierz Zarankiewicz, who proposed several special cases of the problem in 1951.\n\nThe Kővári–Sós–Turán theorem, named after Tamás Kővári, Vera T. Sós, and Pál Turán, provides an upper bound on the solution to the Zarankiewicz problem. When the forbidden complete bipartite subgraph has one side with at most three vertices, this bound has been proven to be within a constant factor of the correct answer. For larger forbidden subgraphs, it remains the best known bound, and has been conjectured to be tight. Applications of the Kővári–Sós–Turán theorem include bounding the number of incidences between different types of geometric object in discrete geometry.\n\nA bipartite graph \"G\" = (\"U\", \"V\", \"E\") consists of two disjoint sets of vertices \"U\" and \"V\", and a set of edges each of which connects a vertex in \"U\" to a vertex in \"V\". No two edges can both connect the same pair of vertices. A complete bipartite graph is a bipartite graph in which every pair of a vertex from \"U\" and a vertex from \"V\" is connected to each other. A complete bipartite graph in which \"U\" has \"s\" vertices and \"V\" has \"t\" vertices is denoted \"K\". If \"G\" = (\"U\", \"V\", \"E\") is a bipartite graph, and there exists a set of \"s\" vertices of \"U\" and \"t\" vertices of \"V\" that are all connected to each other, then these vertices induce a subgraph of the form \"K\". (In this formulation, the ordering of \"s\" and \"t\" is significant: the set of \"s\" vertices must be from \"U\" and the set of \"t\" vertices must be from \"V\", not vice versa.)\n\nThe Zarankiewicz function \"z\"(\"m\", \"n\"; \"s\", \"t\") denotes the maximum possible number of edges in a bipartite graph \"G\" = (\"U\", \"V\", \"E\") for which |\"U\"| = \"m\" and |\"V\"| = \"n\", but which does not contain a subgraph of the form \"K\". As a shorthand for an important special case, \"z\"(\"n\"; \"t\") is the same as \"z\"(\"n\", \"n\"; \"t\", \"t\"). The Zarankiewicz problem asks for a formula for the Zarankiewicz function, or (failing that) for tight asymptotic bounds on the growth rate of \"z\"(\"n\"; \"t\") assuming that \"t\" is a fixed constant, in the limit as \"n\" goes to infinity.\n\nFor \"s = t = 2\" this problem is the same as determining cages with girth six. The Zarankiewicz problem, cages and finite geometry are strongly interrelated.\n\nThe same problem can also be formulated in terms of digital geometry. The possible edges of a bipartite graph \"G\" = (\"U\", \"V\", \"E\") can be visualized as the points of a |\"U\"| × |\"V\"| rectangle in the integer lattice, and a complete subgraph is a set of rows and columns in this rectangle in which all points are present. Thus, \"z\"(\"m\", \"n\"; \"s\", \"t\") denotes the maximum number of points that can be placed within an \"m\" × \"n\" grid in such a way that no subset of rows and columns forms a complete \"s\" × \"t\" grid. An alternative and equivalent definition is that \"z\"(\"m\", \"n\"; \"s\", \"t\") is the smallest integer \"k\" such that every (0,1)-matrix of size \"m\" × \"n\" with \"k\" + 1 ones must have a set of \"s\" rows and \"t\" columns such that the corresponding \"s\"×\"t\" submatrix is made up only of 1's.\n\nThe number \"z\"(\"n\", 2) asks for the maximum number of edges in a bipartite graph with \"n\" vertices on each side that has no 4-cycle (its girth is six or more). Thus, \"z\"(2, 2) = 3 (achieved by a three-edge path), and \"z\"(3, 2) = 6 (a hexagon).\n\nIn his original formulation of the problem, Zarankiewicz asked for the values of \"z\"(\"n\"; 3) for \"n\" = 4, 5, and 6. The answers were supplied soon afterwards by Wacław Sierpiński: \"z\"(4; 3) = 13, \"z\"(5; 3) = 20, and \"z\"(6; 3) = 26. The case of \"z\"(4; 3) is relatively simple: a 13-edge bipartite graph with four vertices on each side of the bipartition, and no \"K\" subgraph, may be obtained by adding one of the long diagonals to the graph of a cube. In the other direction, if a bipartite graph with 14 edges has four vertices on each side, then two vertices on each side must have degree four. Removing these four vertices and their 12 incident edges leaves a nonempty set of edges, any of which together with the four removed vertices forms a \"K\" subgraph.\n\nThe following upper bound was established by Tamás Kővári, Vera T. Sós and Pál Turán shortly after the problem had been posed, and has become known as the Kővári–Sós–Turán theorem:\nIn fact, Kővári, Sós, and Turán proved a similar inequality for \"z\"(\"n\"; \"t\"), but shortly afterwards, Hyltén-Cavallius observed that essentially the same argument can be used to prove the above inequality.\nAn improvement to the constant factor in the second term of this formula, in the case of \"z\"(\"n\"; \"t\"), was given by Štefan Znám:\n\nIf \"s\" and \"t\" are assumed to be constant, then asymptotically, using big O notation, these formulas can be expressed as\nand\n\nFor \"t\" = 2, and for infinitely many values of \"n\", a bipartite graph with \"n\" vertices on each side, Ω(\"n\") edges, and no \"K\" may be obtained as the Levi graph of a finite projective plane, a system of \"n\" points and lines in which each two points belong to a unique line and each two lines intersect in a unique point.\nThe graph formed from this geometry has a vertex on one side of its bipartition for each point, a vertex on the other side of its bipartition for each line, and an edge for each incidence between a point and a line. The projective planes defined from finite fields of order \"p\" lead to \"K\"-free graphs with \"n\" = \"p\" + \"p\" + 1 and with (\"p\" + \"p\" + 1)(\"p\" + 1) edges. For instance, the Levi graph of the Fano plane gives rise to the Heawood graph, a bipartite graph with seven vertices on each side, 21 edges, and no 4-cycles, showing that \"z\"(7; 2) ≥ 21. The lower bound on the Zarankiewicz function given by this family of examples matches an upper bound given by I. Reiman. Thus, for \"t\" = 2 and for those values of \"n\" for which this construction can be performed, it provides a precise answer to the Zarankiewicz problem. For other values of \"n\", it follows from these upper and lower bounds that asymptotically\nMore generally,\n\nFor \"t\" = 3, and for infinitely many values of \"n\", bipartite graphs with \"n\" vertices on each side, Ω(\"n\") edges, and no \"K\" may again be constructed from finite geometry, by letting the vertices represent points and spheres (of a carefully chosen fixed radius) in a three-dimensional finite affine space, and letting the edges represent point-sphere incidences.\n\nIt has been conjectured that\nfor all constant values of \"t\", but this is only known for \"t\" = 2 and \"t\" = 3 by the above constructions. Tight bounds are also known for pairs (\"s\", \"t\") with widely differing sizes (specifically \"s\" ≥ (\"t\" − 1)!). For such pairs,\nlending support to the above conjecture.\n\nUp to constant factors, \"z\"(\"n\"; \"t\") also bounds the number of edges in an \"n\"-vertex graph (not required to be bipartite) that has no \"K\" subgraph. For, in one direction, a bipartite graph with \"z\"(\"n\"; \"t\") edges and with \"n\" vertices on each side of its bipartition can be reduced to a graph with \"n\" vertices and (in expectation) \"z\"(\"n\"; \"t\")/4 edges, by choosing \"n\"/2 vertices uniformly at random from each side. In the other direction, a graph with \"n\" vertices and no \"K\" can be transformed into a bipartite graph with \"n\" vertices on each side of its bipartition, twice as many edges, and still no \"K\" by taking its bipartite double cover.\n\nThe Kővári–Sós–Turán theorem has been used in discrete geometry to bound the number of incidences between geometric objects of various types. As a simple example, a set of \"n\" points and \"m\" lines in the Euclidean plane necessarily has no \"K\", so by the Kővári–Sós–Turán it has \"O\"(\"nm\" + \"m\") point-line incidences. This bound is tight when \"m\" is much larger than \"n\", but not when \"m\" and \"n\" are nearly equal, in which case the Szemerédi–Trotter theorem provides a tighter \"O\"(\"n\"\"m\" + \"n\" + \"m\") bound. However, the Szemerédi–Trotter theorem may be proven by dividing the points and lines into subsets for which the Kővári–Sós–Turán bound is tight.\n\n"}
