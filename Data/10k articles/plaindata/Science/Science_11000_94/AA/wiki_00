{"id": "4234894", "url": "https://en.wikipedia.org/wiki?curid=4234894", "title": "AP Calculus", "text": "AP Calculus\n\nAdvanced Placement Calculus (also known as AP Calculus, AP Calc, or simply AB / BC) is one of two distinct Advanced Placement calculus courses and exams offered by College Board.\n\nAP Calculus AB covers limits, derivatives, and integrals.\n\nAP Calculus BC covers all AP Calculus AB topics plus additional topics (including more integration techniques such as integration by parts, Taylor series, parametric equations, polar coordinate functions, and curve interpolations).\n\nThe College Board intentionally schedules the AP Calculus AB exam at the same time as the AP Calculus BC exam in order to make it impossible for a student to take both tests in the same academic year, though the College Board does not make Calculus AB a pre-requisite class for Calculus BC. Some schools do this, though many others only require precalculus as a prerequisite for Calculus BC. The AP awards given by College Board count both exams. However, they do not count the AB subscore piece of the BC exam.\n\nThe structures of the AB and BC exams are identical. Both exams are three hours and fifteen minutes long, comprising a total of 45 multiple choice questions and six free response questions.\n\nThe two parts of the multiple choice section are timed and taken independently.\n\nStudents are required to put away their calculators after 30 minutes have passed during the Free-Response section, and only at that point may begin Section II Part B. However, students may continue to work on Section II Part A during the entire Free-Response time, although without a calculator during the later half.\n\nThe multiple choice section is scored by computer, with a correct answer receiving 1 point, with omitted and incorrect answers not affecting the raw score. This total is multiplied by 1.2 to calculate the adjusted multiple-choice score.\n\nThe free response section is hand-graded by hundreds of educators each June. The raw score is then added to the adjusted multiple choice score to receive a composite score. This total is compared to a composite-score scale for that year's exam and converted into an AP score of 1 to 5.\n\nFor the Calculus BC exam, an AB sub-score is included in the score report to reflect their proficiency in the fundamental topics of introductory calculus. The AB sub-score is based on the correct number of answers for questions pertaining to AB-material only.\n\nAP Calculus AB is an Advanced Placement calculus course. It is traditionally taken after precalculus and is the first calculus course offered at most schools except for possibly a regular calculus class. The Pre-Advanced Placement pathway for math will help prepare students for further Advanced Placement classes and exams. \n\nAccording to the College Board:\n\nThe material includes the study and application of differentiation and integration, and graphical analysis including limits, asymptotes, and continuity. An AP Calculus AB course is typically equivalent to one semester of college calculus.\n\nAccording to the College Board, \n\nAP Calculus BC includes all of the topics covered in AP Calculus AB, as well as the following: \n\nIndependent research on the academic benefits of the Advanced Placement Calculus course indicates that not all students receive academic benefits from participating in the course. In a study with a sample size of over 90,000, the authors found that students who took the AP Calculus course did not receive any increase in academic achievement unless they also prepared for and took the AP test. The authors controlled for over 70 intervening variables and found that AP students who took and passed the AP Calculus AB or BC exam had ACT scores that were 1.8 points higher than non-AP students or AP Calculus students who did not take their course's AP test. This led the authors to state that AP participation... is not beneficial to students who merely enroll in the courses ...\"\n\n\n"}
{"id": "6786507", "url": "https://en.wikipedia.org/wiki?curid=6786507", "title": "Ab initio multiple spawning", "text": "Ab initio multiple spawning\n\nThe ab initio multiple spawning, or AIMS, method is a time-dependent formulation of quantum chemistry.\n\nIn AIMS, nuclear dynamics and electronic structure problems are solved simultaneously. Quantum mechanical effects in the nuclear dynamics are included, especially the nonadiabatic effects which are crucial in modeling dynamics on multiple electronic states.\n\nThe AIMS method makes it possible to describe photochemistry from first principles molecular dynamics, with no empirical parameters. The method has been applied to two molecules of interest in organic photochemistry - ethylene and cyclobutene.\n\nThe photodynamics of ethylene involves both covalent and ionic electronic excited states and the return to the ground state proceeds through a pyramidalized geometry. For the photoinduced ring opening of cyclobutene, is it shown that the disrotatory motion predicted by the Woodward–Hoffmann rules is established within the first 50 fs after optical excitation.\n\nThe method was developed by chemistry professor Todd Martinez.\n\n"}
{"id": "16526862", "url": "https://en.wikipedia.org/wiki?curid=16526862", "title": "Alberts Glacier", "text": "Alberts Glacier\n\nAlberts Glacier () is a heavily crevassed glacier in Antarctica. It is about long, and flows east from Avery Plateau, Graham Land, until entering Mill Inlet between Balch Glacier and Southard Promontory. The glacier was photographed from the air by the U.S. Navy in 1968. It was delineated from these photographs by Directorate of Overseas Surveys, 1980, and positioned from surveys by Falkland Islands Dependencies Survey, 1947–57. In association with the names of Antarctic historians around the area, it was named by United Kingdom Antarctic Place-Names Committee after Fred G. Alberts, an American toponymist, and secretary of the Advisory Committee on Antarctic Names 1949–80.\n\n"}
{"id": "3822979", "url": "https://en.wikipedia.org/wiki?curid=3822979", "title": "Antibonding molecular orbital", "text": "Antibonding molecular orbital\n\nIn chemical bonding theory, an antibonding orbital is a type of molecular orbital (MO) that weakens the bond between two atoms and helps to raise the energy of the molecule relative to the separated atoms. Such an orbital has one or more nodes in the bonding region between the nuclei. The density of the electrons in the orbital is concentrated outside the bonding region and acts to pull one nucleus away from the other and tends to cause mutual repulsion between the two atoms.\n\nAntibonding molecular orbitals (MOs) are normally \"higher\" in energy than bonding molecular orbitals. Bonding and antibonding orbitals form when atoms combine into molecules. If two hydrogen atoms are initially far apart, they have identical atomic orbitals. However, as the spacing between the two atoms becomes smaller, the electron wave functions begin to overlap. The Pauli exclusion principle prohibits any two electrons in a molecule from having the same set of quantum numbers. Therefore each original atomic orbital of the isolated atoms (for example, the ground state energy level, 1\"s\") splits into two molecular orbitals belonging to the pair, one lower in energy than the original atomic level and one higher. The orbital which is lower than the orbitals of the separate atoms is the bonding orbital, which is more stable and promotes the bonding of the two H atoms into H. The higher-energy orbital is the antibonding orbital, which is less stable and opposes bonding if it is occupied. In a molecule such as H, the two electrons normally occupy the lower-energy bonding orbital, so that the molecule is more stable than the separate H atoms.\nA molecular orbital becomes antibonding when there is less electron density between the two nuclei than there would be if there were no bonding interaction at all. When a molecular orbital changes sign (from positive to negative) at a \"nodal plane\" between two atoms, it is said to be \"antibonding with respect to those atoms\". Antibonding orbitals are often labelled with an asterisk (*) on molecular orbital diagrams. \nIn homonuclear diatomic molecules, σ* (\"sigma star\") antibonding orbitals have no nodal planes passing through the two nuclei, like sigma bonds, and π* (\"pi star\") orbitals have one nodal plane passing through the two nuclei, like pi bonds. The Pauli exclusion principle dictates that no two electrons in an interacting system may have the same quantum state. If the bonding orbitals are filled, then any additional electrons will occupy antibonding orbitals. This occurs in the He molecule, in which both the 1sσ and 1sσ* orbitals are filled. Since the \"antibonding orbital is more antibonding than the bonding orbital is bonding\", the molecule has a higher energy than two separated helium atoms, and it is therefore unstable.\n\nIn molecules with several atoms, some orbitals may be delocalized over more than two atoms. A particular molecular orbital may be \"bonding with respect to some adjacent pairs of atoms\" and \"antibonding with respect to other pairs\". If the bonding interactions outnumber the antibonding interactions, the MO is said to be \"bonding\", whereas, if the antibonding interactions outnumber the bonding interactions, the molecular orbital is said to be \"antibonding\". \n\nFor example, butadiene has pi orbitals which are delocalized over all four carbon atoms. There are two bonding pi orbitals which are occupied in the ground state: π is bonding between all carbons, while π is bonding between C and C and between C and C, and antibonding between C and C. There are also antibonding pi orbitals with two and three antibonding interactions as shown in the diagram; these are vacant in the ground state, but may be occupied in excited states.\n\nSimilarly benzene with six carbon atoms has three bonding pi orbitals and three antibonding pi orbitals.\nSince each carbon atom contributes one electron to the π-system of benzene, there are six pi electrons which fill the three lowest-energy pi molecular orbitals (the bonding pi orbitals). \n\nAntibonding orbitals are also important for explaining chemical reactions in terms of molecular orbital theory. Roald Hoffmann and Kenichi Fukui shared the 1981 Nobel Prize in Chemistry for their work and further development of qualitative molecular orbital explanations for chemical reactions.\n\n\n"}
{"id": "17521343", "url": "https://en.wikipedia.org/wiki?curid=17521343", "title": "Armindo Freitas-Magalhães", "text": "Armindo Freitas-Magalhães\n\nArmindo Freitas-Magalhães, Ph.D. (born 1966), is a Portuguese psychologist working on the psychology of the human smile in the context of emotion and facial expression. His research and clinical-forensic expertise includes investigative interviewing, credibility assessment, forensic assessment, facial expression of emotion and variables associated with eyewitness memory in victims and offenders of crime and trauma. He has also provided consultation and training overseas.\n\nFreitas-Magalhães () (1966-) was born in Fornelos, Norte region, Portugal. He studied at the University of Coimbra, and he did a Ph.D. in Psychology at University Aberta, Lisbon. He is the author of the book \"The Psychology of Human Smile\". Since 2006 he is Professor of the Psychology of Emotions, Psychology and Law, Applied Psychology and Experimental Psychology at University Fernando Pessoa (UFP), in Porto. He is founder and current director of the Facial Emotion Expression Lab (FEELab) at that university. He is fellow of several international scientific societies (e.g. American Psychological Association, Association for Psychological Science, Society for Personality and Social Psychology, International Society for Research on Emotions, International Brain Research Organization, International Neuropsychological Society and European Health Psychology Society), as well as being member of several literary societies. His studies focus on the recognition from basic emotions, particularly the smile. He founded the National Front for the Defense of Culture (Lisbon, 1992) with among others José Saramago. He lives in Porto with his wife, Ana, and son, Gonçalo.\n\nHis research is devoted to understanding emotions in individuals and society. Among Freitas-Magalhães's professional interests are physiological psychology, psychology of emotions, facial emotion expression and human-computer interaction, human smile and cross-cultural nonverbal behavior. During the last twenty years his principal research has been on human emotions and the influence of smile on emotional disorders such as depression. More recently he has conducted research on the cognitive and emotional processes of reading human faces. Freitas-Magalhães is the author of several IT applications and interfaces in relation to emotions and facial expression.\n\nHe is the author of more than 100 articles, empirical reports, research reviews and theorical papers, as well as attendee to conferences, and has written six books on psychology. Freitas-Magalhães is also the author of 11 novels and has published poetry. The common theme of his research and fiction is on furthering our understandings of interactions within complex systems, especially complex emotional systems. Freitas-Magalhães's theories of fiction includes the idea that novels and poetry are simulations that run not on machines but on our minds. His books have been translated into several languages.\n\nFreitas-Magalhães is the author of FACE, a scientific project in Portugal that will allow neuropsychological mapping of the Portuguese facial expression. The FACE imaging technology will contribute to a database of facial expression available for the most diverse social applications, such as health, justice and education. He is the author of \"A decade of smile in Portugal\" (2003–2013). He is the author \"ForensicPsy\" on the assessment and measurement on emotion facial expression of offenders for the criminal investigation and judicial proceedings (2009).\n\nCurrently, he is the leader of the international scientific project \"The Brain and The Face\".\n\nHe was distinguished for Scientific Contribution by the UK Government within the “Global Partnership Programme” (2008), Scientific Contribution by Instituto Nokia de Tecnologia (2008), Scientific Contribution by “Alive Science” (2007) and “Scientist Generation”(2006) (RTP, Lisbon - Radio and Television of Portugal) and “Portuguese par Excellence” (2003) (TSF, Lisbon).\n\nBesides national (Portuguese) media attention, his research has received international press attention. (e.g., in the European Herald Tribune and The Australian News).\n\nThe pioneer F-M Facial Action Coding System 3.0 (F-M FACS 3.0) was created in 2018 by Dr. Freitas-Magalhães, and presents 4,000 segments in 4K, using 3D technology and automatic and real-time recognition (FaceReader 7.1).The F-M FACS 3.0 features 8 pioneering action units (AUs) and 22 pioneering tongue movements (TMs), in addition to functional and structural nomenclature .\n\nThe pioneer F-M Facial Action Coding System 2.0 (F-M FACS 2.0) was created in 2017 by Dr. Freitas-Magalhães, and presents 2,000 segments in 4K, using 3D technology and automatic and real-time recognition.\n\nHe is the author of the scientific project \"Psychopathy and Emotions in Portugal\" (2010) with the aim of understanding the brain processes involved in neuropsychophysiological reactions of facial expression of emotion, learn why the pattern of negative emotionality are common in the psychopathy, if there are gender and age differences and look for the organic and environmental issues involved and set a standard that allows the treatment and prophylaxis of crime. To verify and analyze the brains of psychopaths and the ratio for the facial expression will be used magnetic resonance imaging (fMRI), neurofunctional psychometry and IT platforms that stimulate the brain systems, particularly the limbic system.\n\nFreitas-Magalhães is the creator of the emotional literacy project called \"If I Say That Sometimes The Flowers Smile\" based on a verse of Fernando Pessoa and released to commemorate the bicentenary of Charles Darwin´s birth.\nThe aim of this project is to facilitate the identification, recognition, regulation and use of emotions in various psychosocial contexts, such as schools (from kindergarten), public and private health, the host social institutions and justice organizations.\n\nAccording to Freitas-Magalhães, \"education assertion of emotional states, contributed, decisively, from childhood, to pursue the happiness and good practice, and therefore decreased the clinical symptoms, violence, stress, drug and alcohol abuse\".\n\n\n"}
{"id": "3211", "url": "https://en.wikipedia.org/wiki?curid=3211", "title": "Atom probe", "text": "Atom probe\n\nThe atom probe was introduced at the 14th Field Emission Symposium in 1967 by Erwin Wilhelm Müller and J. A. Panitz. It combined a field ion microscope with a mass spectrometer having a single particle detection capability and, for the first time, an instrument could “... determine the nature of one single atom seen on a metal surface and selected from neighboring atoms at the discretion of the observer”.\n\nAtom probes are unlike conventional optical or electron microscopes, in that the magnification effect comes from the magnification provided by a highly curved electric field, rather than by the manipulation of radiation paths. The method is destructive in nature removing ions from a sample surface in order to image and identify them, generating magnifications sufficient to observe individual atoms as they are removed from the sample surface. Through coupling of this magnification method with time of flight mass spectrometry, ions evaporated by application of electric pulses can have their mass-to-charge ratio computed.\n\nThrough successive evaporation of material, layers of atoms are removed from a specimen, allowing for probing not only of the surface, but also through the material itself. Computer methods are used to rebuild a three-dimensional view of the sample, prior to it being evaporated, providing atomic scale information on the structure of a sample, as well as providing the type atomic species information. The instrument allows the three-dimensional reconstruction of up to billions of atoms from a sharp tip (corresponding to specimen volumes of 10,000-10,000,000 nm).\n\nAtom probe samples are shaped to implicitly provide a highly curved electric potential to induce the resultant magnification, as opposed to direct use of lensing, such as via magnetic lenses. Furthermore, in normal operation (as opposed to a field ionization modes) the atom probe does not utilize a secondary source to probe the sample. Rather, the sample is evaporated in a controlled manner (field evaporation) and the evaporated ions are impacted onto a detector, which is typically 10 to 100 cm away.\n\nThe samples are required to have a needle geometry and are produced by similar techniques as TEM sample preparation electropolishing, or focused ion beam methods. Since 2006, commercial systems with laser pulsing have become available and this has expanded applications from metallic only specimens into semiconducting, insulating such as ceramics, and even geological materials.\nPreparation is done, often by hand, to manufacture a tip radius sufficient to induce a high electric field, with radii on the order of 100 nm.\n\nTo conduct an atom probe experiment a very sharp needle shaped specimen is placed in an ultra high vacuum chamber. After introduction into the vacuum system, the sample is reduced to cryogenic temperatures (typically 20-100 K) and manipulated such that the needle's point is aimed towards an ion detector. A high voltage is applied to the specimen, and either a laser pulse is applied to the specimen or a voltage pulse (typically 1-2 kV) with pulse repetition rates in the hundreds of kilohertz range is applied to a counter electrode. The application of the pulse to the sample allows for individual atoms at the sample surface to be ejected as an ion from the sample surface at a known time. Typically the pulse amplitude and the high voltage on the specimen are computer controlled to encourage only one atom to ionize at a time, but multiple ionizations are possible. The delay between application of the pulse and detection of the ion(s) at the detector allow for the computation of a mass-to-charge ratio.\n\nWhilst the uncertainty in the atomic mass computed by time-of-flight methods in atom probe is sufficiently small to allow for detection of individual isotopes within a material this uncertainty may still, in some cases, confound definitive identification of atomic species. Effects such as superposition of differing ions with multiple electrons removed, or through the presence of complex species formation during evaporation may cause two or more species to have sufficiently close time-of-flights to make definitive identification impossible.\n\nField ion microscopy is a modification of field emission microscopy where a stream of tunneling electrons is emitted from the apex of a sharp needle-like \"tip\" cathode when subjected to a sufficiently high electric field (~3-6 V/nm). The needle is oriented towards a phosphor screen to create a projected image of the work function at the tip apex. The image resolution is limited to (2-2.5 nm), due to quantum mechanical effects and lateral variations in the electron velocity.\n\nIn field ion microscopy the tip is cooled by a cryogen and its polarity is reversed. When an \"imaging gas\" (usually hydrogen or helium) is introduced at low pressures (< 0.1 Pascal) gas ions in the high electric field at the tip apex are \"field ionized\" and produce a projected image of protruding atoms at the tip apex. The image resolution is determined primarily by the temperature of the tip but even at 78 Kelvin atomic resolution is achieved.\n\nThe 10-cm Atom Probe, invented in 1973 by J. A. Panitz was a “new and simple atom probe which permits rapid, in depth species identification or the more usual atom-by atom analysis provided by its predecessors ... in an instrument having a volume of less than two liters in which tip movement is unnecessary and the problems of evaporation pulse stability and alignment common to previous designs have been eliminated.” This was accomplished by combining a time of flight (TOF) mass spectrometer with a proximity focussed, dual channel plate detector, an 11.8 cm drift region and a 38° field of view. An FIM image or a desorption image of the atoms removed from the apex of a field emitter tip could be obtained. The 10-cm Atom Probe has been called the \"progenitor\" of later atom probes including the commercial instruments.\n\nThe Imaging Atom-Probe (IAP) was introduced in 1974 by J. A. Panitz. It incorporated the features of the 10-cm Atom-Probe yet “... departs completely from [previous] atom probe philosophy. Rather than attempt to determine the identity of a surface species producing a preselected ion-image spot, we wish to determine the complete crystallographic distribution of a surface species of preselected mass-to-charge ratio. Now suppose that instead of operating the [detector] continuously, it is turned on for a short time coincidentally with the arrival of a preselected species of interest by applying a \"gate pulse\" a time T after the evaporation pulse has reached the specimen. If the duration of the gate pulse is shorter than the travel time between adjacent species, only that surface species having the unique travel time T will be detected and its complete crystallographic distribution displayed.” It was patented in 1975 as the Field Desorption Spectrometer. The Imaging Atom-Probe moniker was coined by A. J. Waugh in 1978 and the instrument was described in detail by J. A. Panitz in the same year.\n\nModern day atom probe tomography (APT) uses a position-sensitive detector to deduce the lateral location of atoms. The idea of the APT, inspired by J. A. Panitz's \"Field Desorption Spectrometer\" patent, was developed by Mike Miller starting in 1983 and culminated with the first prototype in 1986. Various refinements were made to the instrument, including the use of a so-called position-sensitive (PoS) detector by Alfred Cerezo, Terence Godfrey, and George D. W. Smith at Oxford University in 1988. The Tomographic Atom Probe (TAP), developed by researchers at the University of Rouen in France in 1993, introduced a multichannel timing system and multianode array. Both instruments (PoSAP and TAP) were commercialized by Oxford Nanoscience and CAMECA respectively. Since then, there have been many refinements to increase the field of view, mass and position resolution, and data acquisition rate of the instrument. The Local Electrode Atom Probe was first introduced in 2003 by Imago Scientific Instruments. In 2005, the commercialization of the pulsed laser atom probe (PLAP) expanded the avenues of research from highly conductive materials (metals) to poor conductors (semiconductors like silicon) and even insulating materials. AMETEK acquired CAMECA in 2007 and Imago Scientific Instruments (Madison, WI) in 2010, making the company the sole commercial developer of APTs with more than 90 instruments installed around the world in 2016.\n\nThe first few decades of work with APT focused on metals. However, more recent work has been done on semiconductors, ceramic and geologic materials, with some work on biomaterials. The most advanced study of biological material to date using APT involved analyzing the chemical structure of teeth of the radula of chiton \"Chaetopleura apiculata\". In this study, the use of APT showed chemical maps of organic fibers in the surrounding nano-crystalline magnetite in the chiton teeth, fibers which were often co-located with sodium or magnesium. This has been furthered to study elephant tusks, dentin and human enamel.\n\nField evaporation is an effect that can occur when an atom bonded at the surface of a material is in the presence of a sufficiently high and appropriately directed electric field, where the electric field is the differential of electric potential (voltage) with respect to distance. Once this condition is met, it is sufficient that local bonding at the specimen surface is capable of being overcome by the field, allowing for evaporation of an atom from the surface to which it is otherwise bonded.\n\nWhether evaporated from the material itself, or ionised from the gas, the ions that are evaporated are accelerated by electrostatic force, acquiring most of their energy within a few tip-radii of the sample.\n\nSubsequently, the accelerative force on any given ion is controlled by the electrostatic equation, where \"n\" is the ionisation state of the ion, and \"e\" is the fundamental electric charge.\n\nThis can be equated with the mass of the ion, \"m\", via Newton's law (F=ma):\n\nRelativistic effects in the ion flight are usually ignored, as realisable ion speeds are only a very small fraction of the speed of light.\n\nAssuming that the ion is accelerated during a very short interval, the ion can be assumed to be travelling at constant velocity. As the ion will travel from the tip at voltage V to some nominal ground potential, the speed at which the ion is travelling can be estimated by the energy transferred into the ion during (or near) ionisation. Therefore, the ion speed can be computed with the following equation, which relates kinetic energy to energy gain due to the electric field, the negative arising from the loss of electrons forming a net positive charge.\n\nWhere \"U\" is the ion velocity. Solving for \"U\", the following relation is found:\n\nLet's say that for at a certain ionization voltage, a singly charged hydrogen ion acquires a resulting velocity of X ms. A singly charged deuterium ion under the sample conditions would have acquired roughly X/1.41 ms. If a detector was placed at a distance of 1 m, the ion flight times would be 1/X and 1.41/X s. Thus, the time of the ion arrival can be used to infer the ion type itself, if the evaporation time is known.\n\nFrom the above equation, it can be re-arranged to show that\n\ngiven a known flight distance. F, for the ion, and a known flight time, t,\n\nand thus one can substitute these values to obtain the mass-to-charge for the ion.\n\nThus for an ion which traverses a 1 m flight path, across a time of 2000 ns, given an initial accelerating voltage of 5000 V (V in Si units is kg.m^2.s^-3.A^-1) and noting that one amu is 1×10 kg, the mass-to-charge ratio (more correctly the mass-to-ionisation value ratio) becomes ~3.86 amu/charge. The number of electrons removed, and thus net positive charge on the ion is not known directly, but can be inferred from the histogram (spectrum) of observed ions.\n\nThe magnification in an atom is due to the projection of ions radially away from the small, sharp tip. Subsequently, in the far field, the ions will be greatly magnified. This magnification is sufficient to observe field variations due to individual atoms, thus allowing in field ion and field evaporation modes for the imaging of single atoms.\n\nThe standard projection model for the atom probe is an emitter geometry that is based upon a revolution of a conic section, such as a sphere, hyperboloid or paraboloid. For these tip models, solutions to the field may be approximated or obtained analytically. The magnification for a spherical emitter is inversely proportional to the radius of the tip, given a projection directly onto a spherical screen, the following equation can be obtained geometrically.\n\nWhere r is the radius of the detection screen from the tip centre, and r the tip radius. Practical tip to screen distances may range from several centimeters to several meters, with increased detector area required at larger to subtend the same field of view.\n\nPractically speaking, the usable magnification will be limited by several effects, such as lateral vibration of the atoms prior to evaporation.\n\nWhilst the magnification of both the field ion and atom probe microscopes is extremely high, the exact magnification is dependent upon conditions specific to the examined specimen, so unlike for conventional electron microscopes, there is often little direct control on magnification, and furthermore, obtained images may have strongly variable magnifications due to fluctuations in the shape of the electric field at the surface.\n\nThe computational conversion of the ion sequence data, as obtained from a position sensitive detector, to a three-dimensional visualisation of atomic types, is termed \"reconstruction\". Reconstruction algorithms are typically geometrically based, and have several literature formulations. Most models for reconstruction assume that the tip is a spherical object, and use empirical corrections to stereographic projection to convert detector positions back to a 2D surface embedded in 3D space, R. By sweeping this surface through R as a function of the ion sequence input data, such as via ion-ordering, a volume is generated onto which positions the 2D detector positions can be computed and placed three-dimensional space.\n\nTypically the sweep takes the simple form of an advancement of the surface, such that the surface is expanded in a symmetric manner about its advancement axis, with the advancement rate set by a volume attributed to each ion detected and identified. This causes the final reconstructed volume to assume a rounded-conical shape, similar to a badminton shuttlecock. The detected events thus become a point cloud data with attributed experimentally measured values, such as ion time of flight or experimentally derived quantities, e.g. time of flight or detector data.\n\nThis form of data manipulation allows for rapid computer visualisation and analysis, with data presented as point cloud data with additional information, such as each ion's mass to charge (as computed from the velocity equation above), voltage or other auxiliary measured quantity or computation therefrom.\n\nThe canonical feature of atom probe data, is its high spatial resolution in the direction through the material, which has been attributed to an ordered evaporation sequence. This data can therefore image near atomically sharp buried interfaces with the associated chemical information.\n\nThe data obtained from the evaporative process is however not without artefacts that form the physical evaporation or ionisation process. A key feature of the evaporation or field ion images is that the data density is highly inhomogeneous, due to the corrugation of the specimen surface at the atomic scale. This corrugation gives rise to strong electric field gradients in the near-tip zone (on the order of an atomic radii or less from the tip), which during ionisation deflects ions away from the electric field normal.\n\nThe resultant deflection means that in these regions of high curvature, atomic terraces are belied by a strong anisotropy in the detection density. Where this occurs due to a few atoms on a surface is usually referred to as a \"pole\", as these are coincident with the crystallographic axes of the specimen (FCC, BCC, HCP) etc. Where the edges of an atomic terrace causes deflection, a low density line is formed and is termed a \"zone line\".\n\nThese poles and zone-lines, whilst inducing fluctuations in data density in the reconstructed datasets, which can prove problematic during post-analysis, are critical for determining information such as angular magnification, as the crystallographic relationships between features are typically well known.\n\nWhen reconstructing the data, owing to the evaporation of successive layers of material from the sample, the lateral and in-depth reconstruction values are highly anisotropic. Determination of the exact resolution of the instrument is of limited use, as the resolution of the device is set by the physical properties of the material under analysis.\n\nMany designs have been constructed since the method's inception. Initial field ion microscopes, precursors to modern atom probes, were usually glass blown devices developed by individual research laboratories.\n\nAt a minimum, an atom probe will consist of several key pieces of equipment.\n\n\nOptionally, an atom probe may also include laser-optical systems for laser beam targeting and pulsing, if using laser-evaporation methods. In-situ reaction systems, heaters, or plasma treatment may also be employed for some studies as well as pure noble gas introduction for FIM.\n\nCollectable ion volumes were previously limited to several thousand, or tens of thousands of ionic events. Subsequent electronics and instrumentation development has increased the rate of data accumulation, with datasets of hundreds of million atoms (dataset volumes of 10 nm). Data collection times vary considerably depending upon the experimental conditions and the number of ions collected. Experiments take from a few minutes, to many hours to complete.\n\nAtom probe has typically been employed in the chemical analysis of alloy systems at the atomic level. This has arisen as a result of voltage pulsed atom probes providing good chemical and sufficient spatial information in these materials. Metal samples from large grained alloys may be simple to fabricate, particularly from wire samples, with hand-electropolishing techniques giving good results.\n\nSubsequently, atom probe has been used in the analysis of the chemical composition of a wide range of alloys.\n\nSuch data is critical in determining the effect of alloy constituents in a bulk material, identification of solid-state reaction features, such as solid phase precipitates. Such information may not be amenable to analysis by other means (e.g. TEM) owing to the difficulty in generating a three-dimensional dataset with composition.\n\nSemi-conductor materials are often analysable in atom probe, however sample preparation may be more difficult, and interpretation of results may be more complex, particularly if the semi-conductor contains phases which evaporate at differing electric field strengths.\n\nApplications such as ion implantation may be used to identify the distribution of dopants inside a semi-conducting material, which is increasingly critical in the correct design of modern nanometre scale electronics.\n\n\n\n"}
{"id": "1365811", "url": "https://en.wikipedia.org/wiki?curid=1365811", "title": "Avicide", "text": "Avicide\n\nAn avicide is any substance (normally, a chemical) which can be used to kill birds. \n\nCommonly used avicides include strychnine, DRC-1339 (3-chloro-4-methylaniline hydrochloride, Starlicide) and CPTH (3-chloro-p-toluidine, the free base of Starlicide), and Avitrol (4-aminopyridine). Chloralose is also used as an avicide. In the past, highly concentrated formulations of parathion in diesel oil were also used, applied by aircraft spraying over the nesting colonies of the birds.\nIt is impossible to minimize risk from avicides for non-targets species.\n\n"}
{"id": "2684993", "url": "https://en.wikipedia.org/wiki?curid=2684993", "title": "Avulsion fracture", "text": "Avulsion fracture\n\nAn avulsion fracture is a bone fracture which occurs when a fragment of bone tears away from the main mass of bone as a result of physical trauma. This can occur at the ligament due to the application forces external to the body (such as a fall or pull) or at the tendon due to a muscular contraction that is stronger than the forces holding the bone together. Generally muscular avulsion is prevented due to the neurological limitations placed on muscle contractions. Highly trained athletes can overcome this neurological inhibition of strength and produce a much greater force output capable of breaking or avulsing a bone.\n\nIf the fracture is small, it is usually sufficient to treat with rest and support bandage, but in more severe cases, surgery may be required. Ice may be used to relieve swelling.\n\nDisplaced avulsion fractures are best managed by either open reduction and internal fixation or closed reduction and pinning. Open reduction (using surgical incision) and internal fixation is used when pins, screws, or similar hardware is needed to fix the bone fragment.\n\nTraumatic complete displacement of a tooth from its socket in alveolar bone. It is a serious dental emergency in which prompt management (within 20–40 minutes of injury) affects the prognosis of the tooth.\n\nThe tuberosity avulsion fracture (also known as pseudo-Jones fracture or dancer's fracture is a common fracture of the fifth metatarsal (the bone on the outside edge of the foot extending to the little toe). This fracture is likely caused by the lateral band of the plantar aponeurosis (tendon). Most of these fractures are treated with a hard-soled shoe or walking cast. This is needed until the pain goes away and then the patient can return to normal activities. Healing is usually completed within eight weeks.\n\nA tibial tuberosity avulsion fracture is an incomplete or complete separation of the tibial tuberosity from the tibia. This occurs as a result of a violent contraction of the quadriceps muscles, most often as a result of a high-power jump. Incomplete fractures are usually treatable with the traditional RICE (rest, ice, compression, elevation) method, but complete/displaced fractures will most often require surgery to pin the tuberosity back in place. Tibial tuberosity avulsions occur most often in teenagers that engage in a large amount of sporting activities, and many studies have shown a history with Osgood-Schlatter's disease to be linked to the fracture.\n\nIn 2001, Bruce Rothschild and other paleontologists published a study examining evidence for tendon avulsions in theropod dinosaurs. Among the dinosaurs studied, Avulsion injuries were only noted among \"Tyrannosaurus\" and \"Allosaurus\". Scars from these sorts of injuries were limited to the humerus and scapula. A divot on the humerus of Sue the \"T. rex\" was one such avulsion. The divot appears to be located at the origin of the deltoid or teres major muscles. The localization in theropod scapulae as evidenced by the tendon avulsion in Sue suggests that theropods may have had a musculature more complex and functionally different from those of their descendants, the birds.\n\n"}
{"id": "29332882", "url": "https://en.wikipedia.org/wiki?curid=29332882", "title": "Bonne Glacier", "text": "Bonne Glacier\n\nBonne Glacier () is a steep glacier west-southwest of Hobbs Peak, descending northwest from Hobbs Ridge into Blue Glacier, in Victoria Land. The name is one of a group in the area associated with surveying applied in 1993 by the New Zealand Geographic Board. It was named after the Bonne map projection, a derivative conical projection, in which the parallels are spaced at true distances along meridians which are plotted as curves.\n"}
{"id": "5786826", "url": "https://en.wikipedia.org/wiki?curid=5786826", "title": "Cefn-cerig road", "text": "Cefn-cerig road\n\nThe Cefn-cerig road, a road near Cefn-cerig Farm, Llandovery, Wales, is the location of the Global Boundary Stratotype Section and Point (GSSP) which marks the boundary between the Aeronian and Telychian stages of the Silurian period on the geologic time scale. The GSSP was ratified in 1984.\n\nThe boundary is defined as a point immediately above the highest record of the brachiopod \"Eocoelia intermedia\" and below the first appearance of the succeeding species \"Eocoelia curtisi\". The boundary also corresponds to the incoming of the acritarchs \"Deunffia monospinosa\", \"Domasia bispinosa\" and \"Pterospermella\". The section, part of the Wormwood Formation, is sandstone and siltstone.\n"}
{"id": "58748153", "url": "https://en.wikipedia.org/wiki?curid=58748153", "title": "Effective permittivity and permeability", "text": "Effective permittivity and permeability\n\nEffective permittivity and permeability are averaged dielectric and magnetic characteristics of a microinhomogeneous medium. They are subject of Effective medium theory . There are two widely used formulae . They both were derived in quasi-static approximation when electric field inside a mixture particle may be considered as homogeneous. So, these formulae can not describe the particle size effect. Many attempts were undertaken to improve these formulae.\n\nThe first formula was proposed by J.C. Maxwell Garnett . Maxwell Garnett was the son of physicist William Garnett, and was named after Garnett's friend, James Clerk Maxwell. He proposed his formula to explain colored pictures that are observed in glasses doped with metal nanoparticles. His formula has a form\n\nformula_1 (1)\n\nwhere formula_2 is effective relative complex permittivity of the mixture, formula_3 is relative complex permittivity of the background medium containing small spherical inclusions of relative permittivity formula_4 with volume fraction of formula_5. This formula is based on the equality\n\nformula_6 (2)\n\nwhere formula_7 is the absolute permittivity of free space and formula_8 is electric dipole moment of a single inclusion induced by the external electric field . However this equality is good only for homogeneous medium and formula_9. Moreover the formula (1) ignores the interaction between single inclusions. Because of these circumstances, formula (1) gives too narrow and too high resonant curve for plasmon excitations in metal nanoparticles of the mixture .\n\nThe second popular formula was proposed by D.A.G. Bruggeman . His formula has a form\n\nformula_10 formula_11 (3)\n\nHere positive sign before the square root must be altered to negative sign in some cases in order to get correct imaginary part of effective complex permittivity which is related with electromagnetic wave attenuation. This formula is based on the equality\n\nformula_12 (4)\n\nwhere formula_13 is the jump of electric displacement flux all over the integration surface, formula_14 is the component of microscopic electric field normal to the integration surface, formula_15 is the local relative complex permittivity which takes the value formula_4 inside the picked metal particle, the value formula_3 inside the picked dielectric particle and the value formula_2 outside the picked particle, formula_19 is the normal component of the macroscopic electric field. Formula (4) comes out of Maxwell's equality formula_20. Thus only one picked particle is considered in Bruggeman's approach. The interaction with all the other particles is taken into account only in mean field approximation described by formula_2. Formula (3) gives a reasonable resonant curve for plasmon excitations in metal nanoparticles if their size is 10 nm or smaller. But it is unable to describe the size dependence for the resonant frequency of plasmon excitations that are observed in experiment \n\nA new formula describing size effect was proposed . This formula has a form\n\nformula_22\n\nformula_23 (5)\n\nformula_24,\n\nwhere is the nanoparticle radius and formula_25 is wave number. It is supposed here that the time dependence of the electromagnetic field is given by the factor formula_26 In this paper Bruggeman's approach was used, but electromagnetic field for electric-dipole oscillation mode inside the picked particle was computed without applying quasi-static approximation. Thus the function formula_27 is due to the field nonuniformity inside the picked particle. In quasi-static region formula_28, i.e. formula_29 ≤ 10 nm for Agformula_30 this function becomes constant formula_31 and formula (5) becomes identical with Bruggeman's formula (3).\n\nFormula for effective permeability of mixtures has a form \n\nformula_32 (6)\n\nformula_33\n\nHere formula_34 is effective relative complex permeability of the mixture, formula_35 is relative complex permeability of the background medium containing small spherical inclusions of relative permeability formula_36 with volume fraction of formula_5. This formula was derived in dipole approximation. Magnetic octupole mode and all other magnetic oscillation modes of odd orders were neglected here. When formula_38 and formula_39 this formula has a simple form\n\nformula_40 (7)\n"}
{"id": "2502261", "url": "https://en.wikipedia.org/wiki?curid=2502261", "title": "Fibonacci prime", "text": "Fibonacci prime\n\nA Fibonacci prime is a Fibonacci number that is prime, a type of integer sequence prime.\n\nThe first Fibonacci primes are :\n\nIt is not known whether there are infinitely many Fibonacci primes. With the indexing starting with , the first 34 are \"F\" for the \"n\" values :\n\nIn addition to these proven Fibonacci primes, there have been found probable primes for\n\nExcept for the case \"n\" = 4, all Fibonacci primes have a prime index, because if \"a\" divides \"b\", then formula_1 also divides formula_2, but not every prime is the index of a Fibonacci prime.\n\n\"F\" is prime for 8 of the first 10 primes \"p\"; the exceptions are \"F\" = 1 and \"F\" = 4181 = 37 × 113. However, Fibonacci primes appear to become rarer as the index increases. \"F\" is prime for only 26 of the 1,229 primes \"p\" below 10,000. The number of prime factors in the Fibonacci numbers with prime index are:\n\n, the largest known certain Fibonacci prime is \"F\", with 21925 digits. It was proved prime by Mathew Steine and Bouk de Water in 2015. The largest known probable Fibonacci prime is \"F\". It was found by Henri Lifchitz in 2018.\nIt was shown by Nick MacKinnon that the only Fibonacci numbers that are also members of the set of prime twins are 3, 5 and 13.\n\nA prime formula_3 divides formula_4 if and only if \"p\" is congruent to ±1 modulo 5, and \"p\" divides formula_5 if and only if is congruent to ±2 modulo 5. (For \"p\" = 5, \"F\" = 5 so 5 divides \"F\")\n\nFibonacci numbers that have a prime index \"p\" do not share any common divisors greater than 1 with the preceding Fibonacci numbers, due to the identity:\n\nwhich implies the infinitude of primes.\n\nFor , \"F\" divides \"F\" iff \"n\" divides \"m\".\n\nIf we suppose that \"m\" is a prime number \"p\", and \"n\" is less than \"p\", then it is clear that \"F\", cannot share any common divisors with the preceding Fibonacci numbers.\n\nThis means that \"F\" will always have characteristic factors or be a prime characteristic factor itself. The number of distinct prime factors of each Fibonacci number can be put into simple terms.\n\n\nThe first step in finding the characteristic quotient of any \"F\" is to divide out the prime factors of all earlier Fibonacci numbers \"F\" for which \"k\" | \"n\".\n\nThe exact quotients left over are prime factors that have not yet appeared.\n\nIf \"p\" and \"q\" are both primes, then all factors of \"F\" are characteristic, except for those of \"F\" and \"F\".\n\nTherefore:\n\nThe number of distinct prime factors of the Fibonacci numbers with a prime index is directly relevant to the counting function. \n\nFor a prime \"p\", the smallest index \"u\" > 0 such that \"F\" is divisible by \"p\" is called the rank of apparition (sometimes called Fibonacci entry point) of \"p\" and denoted \"a\"(\"p\"). The rank of apparition \"a\"(\"p\") is defined for every prime \"p\". The rank of apparition divides the Pisano period π(\"p\") and allows to determine all Fibonacci numbers divisible by \"p\".\n\nFor the divisibility of Fibonacci numbers by powers of a prime, formula_15 and formula_16\n\nIn particular\n\nA prime \"p\" ≠ 2, 5 is called a Fibonacci–Wieferich prime or a Wall-Sun-Sun prime if formula_19 where \n\nin which formula_21 is the Legendre symbol defined as:\n\nIt is known that for \"p\" ≠ 2, 5, \"a\"(\"p\") is a divisor of:\n\nFor every prime \"p\" that is not a Wall-Sun-Sun prime, formula_24 as illustrated in the table below:\n\nThe existence of Wall-Sun-Sun primes is conjectural.\n\nThe primitive part of the Fibonacci numbers are\n\nThe product of the primitive prime factors of the Fibonacci numbers are\n\nThe first case of more than one primitive prime factor is 4181 = 37 × 113 for formula_25.\n\nThe primitive part has a non-primitive prime factor in some cases. The ratio between the two above sequences is\n\nThe natural numbers \"n\" for which formula_26 has exactly one primitive prime factor are\n\nIf and only if a prime \"p\" is in this sequence, then formula_27 is a Fibonacci prime, and if and only if 2\"p\" is in this sequence, then formula_28 is a Lucas prime (where formula_29 is the Lucas sequence), and if and only if 2 is in this sequence, then formula_30 is a Lucas prime.\n\nNumber of primitive prime factors of formula_26 are\n\nThe least primitive prime factor of formula_26 are\n\n\n"}
{"id": "3056036", "url": "https://en.wikipedia.org/wiki?curid=3056036", "title": "Flags of provinces of the Netherlands", "text": "Flags of provinces of the Netherlands\n\nThis list contains all twelve official flags of provinces of the Netherlands, including the pennons.\n\n"}
{"id": "23219749", "url": "https://en.wikipedia.org/wiki?curid=23219749", "title": "Folksonomy", "text": "Folksonomy\n\nFolksonomy is the system in which users apply public tags to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a taxonomic classification specified by the owners of the content when it is published. This practice is also known as collaborative tagging, social classification, social indexing, and social tagging. Folksonomy was originally \"the result of personal free tagging of information [...] for one's own retrieval\", but online sharing and interaction expanded it into collaborative forms. Social tagging is the application of tags in an open online environment where the tags of other users are available to others. Collaborative tagging (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.\n\nThe term was coined by Thomas Vander Wal in 2004 as a portmanteau of \"folk\" and \"taxonomy\". Folksonomies became popular as part of social software applications such as social bookmarking and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include tag clouds as a way to visualize tags in a folksonomy.\n\nFolksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.\n\nFolksonomies are a trade-off between traditional centralized classification and no classification at all, and have several advantages:\n\nThere are several disadvantages with the use of tags and folksonomies as well, and some of the advantages (see above) can lead to problems. For example, the simplicity in tagging can result in poorly applied tags. Further, while controlled vocabularies are exclusionary by nature, tags are often ambiguous and overly personalized. Users apply tags to documents in many different ways and tagging systems also often lack mechanisms for handling synonyms, acronyms and homonyms, and they also often lack mechanisms for handling spelling variations such as misspellings, singular/plural form, conjugated and compound words. Some tagging systems do not support tags consisting of multiple words, resulting in tags like “viewfrommywindow”. Sometimes users choose specialized tags or tags without meaning to others.\n\nA folksonomy emerges when users tag content or information, such as web pages, photos, videos, podcasts, tweets, scientific papers and others. Strohmaier et al. elaborate the concept: the term “tagging” refers to a \"voluntary activity of users who are annotating resources with term-so-called 'tags' – freely chosen from an unbounded and uncontrolled vocabulary\". Others explain tags as an unstructured textual label or keywords, and that they appear as a simple form of metadata.\n\nFolksonomies consist of three basic entities: users, tags, and resources. Users create tags to mark resources such as: web pages, photos, videos, and podcasts. These tags are used to manage, categorize and summarize online content. This collaborative tagging system also uses these tags as a way to index information, facilitate searches and navigate resources. Folksonomy also includes a set of URLs that are used to identify resources that have been referred to by users of different websites. These systems also include category schemes that have the ability to organize tags at different levels of granularity.\n\nVander Wal identifies two types of folksonomy: broad and narrow. A broad folksonomy arises when multiple users can apply the same tag to an item, providing information about which tags are the most popular. A narrow folksonomy occurs when users, typically fewer in number and often including the item's creator, tag an item with tags that can each be applied only once. While both broad and narrow folksonomies enable the searchability of content by adding an associated word or phrase to an object, a broad folksonomy allows for sorting based on the popularity of each tag, as well as the tracking of emerging trends in tag usage and developing vocabularies.\n\nAn example of a broad folksonomy is del.icio.us, a website where users can tag any online resource they find relevant with their own personal tags. The photo-sharing website Flickr is an oft-cited example of a narrow folksonomy.\n\n'Taxonomy' refers to a hierarchical categorization in which relatively well-defined classes are nested under broader categories. A \"folksonomy\" establishes categories (each tag is a category) without stipulating or necessarily deriving a hierarchical structure of parent-child relations among different tags. (Work has been done on techniques for deriving at least loose hierarchies from clusters of tags.)\n\nSupporters of folksonomies claim that they are often preferable to taxonomies because folksonomies democratize the way information is organized, they are more useful to users because they reflect current ways of thinking about domains, and they express more information about domains. Critics claim that folksonomies are messy and thus harder to use, and can reflect transient trends that may misrepresent what is known about a field.\n\nAn empirical analysis of the complex dynamics of tagging systems, published in 2007, has shown that consensus around stable distributions and shared vocabularies does emerge, even in the absence of a central controlled vocabulary. For content to be searchable, it should be categorized and grouped. While this was believed to require commonly agreed on sets of content describing tags (much like keywords of a journal article), some research has found that in large folksonomies common structures also emerge on the level of categorizations.\nAccordingly, it is possible to devise mathematical models of collaborative tagging that allow for translating from personal tag vocabularies (personomies) to the vocabulary shared by most users.\n\nFolksonomy is unrelated to folk taxonomy, a cultural practice that has been widely documented in anthropological and folkloristic work. Folk taxonomies are culturally supplied, intergenerationally transmitted, and relatively stable classification systems that people in a given culture use to make sense of the entire world around them (not just the Internet).\n\nThe study of the structuring or classification of folksonomy is termed \"folksontology\". This branch of ontology deals with the intersection between highly structured taxonomies or hierarchies and loosely structured folksonomy, asking what best features can be taken by both for a system of classification. The strength of flat-tagging schemes is their ability to relate one item to others like it. Folksonomy allows large disparate groups of users to collaboratively label massive, dynamic information systems. The strength of taxonomies are their browsability: users can easily start from more generalized knowledge and target their queries towards more specific and detailed knowledge. Folksonomy looks to categorize tags and thus create browsable spaces of information that are easy to maintain and expand.\n\nSocial tagging for knowledge acquisition is the specific use of tagging for finding and re-finding specific content for an individual or group. Social tagging systems differ from traditional taxonomies in that they are community-based systems lacking the traditional hierarchy of taxonomies. Rather than a top-down approach, social tagging relies on users to create the folksonomy from the bottom up.\n\nCommon uses of social tagging for knowledge acquisition include personal development for individual use and collaborative projects. Social tagging is used for knowledge acquisition in secondary, post-secondary, and graduate education as well as personal and business research. The benefits of finding/re-finding source information are applicable to a wide spectrum of users. Tagged resources are located through search queries rather than searching through a more traditional file folder system. The social aspect of tagging also allows users to take advantage of metadata from thousands of other users.\n\nUsers choose individual tags for stored resources. These tags reflect personal associations, categories, and concepts. All of which are individual representations based on meaning and relevance to that individual. The tags, or keywords, are designated by users. Consequently, tags represent a user’s associations corresponding to the resource. Commonly tagged resources include videos, photos, articles, websites, and email. Tags are beneficial for a couple of reasons. First, they help to structure and organize large amounts of digital resources in a manner that makes them easily accessible when users attempt to locate the resource at a later time. The second aspect is social in nature, that is to say that users may search for new resources and content based on the tags of other users. Even the act of browsing through common tags may lead to further resources for knowledge acquisition.\n\nTags that occur more frequently with specific resources are said to be more strongly connected. Furthermore, tags may be connected to each other. This may be seen in the frequency in which they co-occur. The more often they co-occur, the stronger the connection. Tag clouds are often utilized to visualize connectivity between resources and tags. Font size increases as the strength of association increases.\n\nTags show interconnections of concepts that were formerly unknown to a user. Therefore, a user’s current cognitive constructs may be modified or augmented by the metadata information found in aggregated social tags. This process promotes knowledge acquisition through cognitive irritation and equilibration. This theoretical framework is known as the co-evolution model of individual and collective knowledge.\n\nThe co-evolution model focuses on cognitive conflict in which a learner’s prior knowledge and the information received from the environment are dissimilar to some degree. When this incongruence occurs, the learner must work through a process cognitive equilibration in order to make personal cognitive constructs and outside information congruent. According to the coevolution model, this may require the learner to modify existing constructs or simply add to them. The additional cognitive effort promotes information processing which in turn allows individual learning to occur.\n\n\n\n"}
{"id": "33355454", "url": "https://en.wikipedia.org/wiki?curid=33355454", "title": "Frank Broad", "text": "Frank Broad\n\nFrancis Alfred Broad JP (1874 – 3 January 1956), usually known as Frank Broad, was a Labour politician in the United Kingdom who served as Member of Parliament (MP) for the Municipal Borough of Edmonton during the years 1922–1931 and 1935–1945.\n\nOne of the founder members of the Amalgamated Instrument Makers Trade Society, Broad was president when, in 1920, the union joined the Amalgamated Engineering Union (AEU). He remained a member of the AEU until his death, stating in the commons in 1931 that \"... I have never been a paid officer of that union but I have 40 years membership in it\". He joined the Independent Labour Party in 1893.\n\nIn July 1923, on the subject of birth control, Broad \"... asked the Minister of Health whether his Department will raise any objection to birth control information being given at infant welfare centres to married women who desire it by voluntary workers attached to the centres, or otherwise to their being informed, on request, where such information can be obtained?\". Mr Chamberlain replied that his \"... view is that such information as is referred to should not be given at infant welfare centres, but that women for whom it appears to be needed on medical grounds should be referred to a private doctor or a hospital\". In May 1924, Broad led a delegation of the birth control movement to the then health minister, Clydesdale MP John Wheatley. The delegates included H G Wells, the Hon. Mrs Bertrand (Dora) Russell, Dr Frances Huxley, and others. Wheatley dodged the issue with the pronouncement \"A clear distinction must be drawn between allowing access to knowledge, and actually distributing knowledge\".\n\nBroad produced two papers. A memorandum on birth control: presented on 9 May 1924 to the Workers Birth Control Group, and The organised worker: problems of Trade union structure and policy; a report by the Industrial Policy Committee published by the Independent Labour Party (Great Britain). Broad was part of the Empire Parliamentary Association 1926 delegation to Australia chaired by the Marquis of Salisbury. At the civic reception in the Sydney Millions Club on the day of arrival, 17 September 1926, Broad said \"A country like Australia is capable of absorbing a great number of people and the problem must be precipitated in a proper spirit of understanding\".\n\nHe was made a justice of the peace in 1933. Broad announced in 1944 that he did not intend to stand as a parliamentary candidate again, saying, \"The world is rather cluttered up with older men\". He retired from politics in 1945. He was awarded the freedom of Edmonton in September 1946.\n\nHe was for a time a governor of The Latymer School and a board member of the North Middlesex Hospital in Edmonton; the same hospital where he died from a short illness on 3 January 1956. Broad House, Fore Street, Edmonton, London, is a permanent memorial. It was opened in 1954 by Clement Attlee. Broad was survived by his wife since 1900, Eliza Broad née Macer and three sons.\n\n\n"}
{"id": "30860403", "url": "https://en.wikipedia.org/wiki?curid=30860403", "title": "Geneticist", "text": "Geneticist\n\nA geneticist is a biologist who studies genetics, the science of genes, heredity, and variation of organisms.\n\nA geneticist can be employed as a scientist or lecturer. Geneticists perform general research on genetic processes as well as development of genetic technologies to aid in the medicine and agriculture industries. Some geneticists perform experiments in model organisms such as Drosophila, C. elegans, Zebrafish, rodents or Humans and analyze data to interpret the inheritance of biological traits. A geneticist can be a scientist who has earned a Ph.D in Genetics or a physician (who has earned any of the following medical degrees: MBBS/MBChB (non-U.S.), D.O. (U.S.-only), or M.D.) who has been trained in genetics as a specialization. They evaluate, diagnose, and manage patients with hereditary conditions or congenital malformations, genetic risk calculations, and mutation analysis as well as refer patients to other medical specialties. The geneticist carries out studies, tests and counsels patients with genetic disorders.\n\nGeneticists participate in courses from many areas, such as biology, chemistry, physics, microbiology, cell biology, bioinformatics, and mathematics. They also participate in more specific genetics courses such as molecular genetics, transmission genetics, population genetics, quantitative genetics, ecological genetics, and genomics.\n\nGeneticists can work in many different fields, doing a variety of jobs. There are many careers for geneticists in medicine, agriculture, wildlife, general sciences, or many other fields.\n\nListed below are a few examples of careers a geneticist may pursue. \n"}
{"id": "23980804", "url": "https://en.wikipedia.org/wiki?curid=23980804", "title": "Gravity and Extreme Magnetism", "text": "Gravity and Extreme Magnetism\n\nThe Gravity and Extreme Magnetism SMEX (GEMS) mission was a cancelled space observatory mission. The main scientific goal of GEMS was to be the first mission to systematically measure the polarization of cosmic X-Ray sources. GEMS would have provided data to help scientists study the shape of space that has been distorted by a spinning black hole's gravity and the structure and effects of the magnetic fields around Neutron stars. It was cancelled by NASA in June 2012 for potential cost overruns due to delays in developing the technology.\n\nGEMS was managed by the National Aeronautics and Space Administration NASA, the United States space agency, at the Goddard Space Flight Center. The project was an astrophysics program reporting to NASA's Science Mission Directorate (SMD) in Washington, D.C..\n\nThe spacecraft would have been launched on in July 2014 on a nine-month mission with a possible 15-month extension for a guest observer phase; but the mission was terminated at the Confirmation Review stage on May 10, 2012 due to expected cost overruns.\n\nCancelled missions can be reinstatedfor example NuSTAR was cancelled in 2006, but reinstated a year later and launched in June 2012. However, NuSTAR was not cancelled due to project overruns, but rather due to changes in the overall NASA budget, so the circumstances for cancellation were very different. Small missions of the Explorer program offer a lot of flexibility and launch opportunities, and the lessons learned can be applied to the same missions goals, but on a different mission (Compare Vanguard 1 to Explorer 1). Several years later two new X-ray polarimetry mission won a NASA award to develop a X-ray polarimetry missions.\n\nThe GEMS X-ray telescope was designed to indirectly measure the regions of distorted space around spinning black holes through a measurement of the polarization of X-rays emitted. It will thereby probe the structure and effects of the magnetic field around magnetars and dead stars with magnetic fields trillions of times stronger than Earth's.\n\nGEMS could reveal:\n\nCurrent missions cannot do this because the required angular resolution is limited and magnetic fields are invisible.\n\nThe detector in GEMS would have been a small chamber filled with gas. When an X-ray is absorbed in the gas, an electron carries off most of the energy, and starts out in a direction related to the polarization direction of the X-ray. This electron loses energy by ionizing the gas; the instrument measures the direction of the ionization track, and thereby the polarization of the X-ray. The GEMS detector readout will employ a time projection chamber to image the track. The GEMS instrument is about 100 times more sensitive than previous X-ray polarization experiments.\n\nMission costs were capped at US$105 million (in Fiscal Year 2008 dollars), excluding the launch vehicle, but an independent confirmation review board at NASA claimed it would grow to an estimated $150 million, leading to cancellation of the mission.\n\nThe cancellation of GEMS marks the end of a multi-year-long binge of cancellations and attempted cancellations of current and future missions: it was at the time the last funded future U.S. space telescope besides JWST. The cancellation of GEMS may have jeopardized the Pegasus XL launcher. (The Pegasus XL has successfully launched other small explorer missions)\n\nGEMS was one of six Small Explorer missions selected in May 2008 for the NASA Small Explorer (SMEX) Program Phase A study. In June 2009, GEMS was chosen to be the second of these missions to go forward into Phase B, starting in October 2010 for a launch in April 2014.\n\nThe project completed and successfully passed the Systems Requirements Review (SRR) Design review (US Government) in December 2010.\n\nGEMS did not pass a confirmation review conducted on 2012-05-10, which effectively cancelled the project. The project team intended to appeal the cancellation.\n\nNASA on June 7, 2012 announced the cancellation of the GEMS project. The Gravity and Extreme Magnetism Small Explorer mission was supposed to blast off in 2014 to study black holes and neutron stars. But external reviews found the project would likely come in over budget. GEMS was supposed to hold at $119 million, not counting the rocket. NASA's astrophysics director, Paul Hertz, says the technology needed for the instrument took longer to develop than expected, and that drove up the price.\n\nNASA later studied X-ray polarimetry missions in 2015 for future explorer program observatories. Just as Galileo once turned his new telescope skyward to study the heavens, NASA continues to develop new telescopes that shed light on the Earth's universe\n\nThe GEMS Principal Investigator is Dr Jean H. Swank, of NASA's Goddard Space Flight Center, Greenbelt, Maryland.\n\n\n\nOther GEMS collaborators are from universities include:\n\n\nContractor Partners\nScience Instrument Co-Investigator Sites\n"}
{"id": "1597987", "url": "https://en.wikipedia.org/wiki?curid=1597987", "title": "Grévy's zebra", "text": "Grévy's zebra\n\nThe Grévy's zebra (\"Equus grevyi\"), also known as the imperial zebra, is the largest living wild equid and the largest and most threatened of the three species of zebra, the other two being the plains zebra and the mountain zebra. Named after Jules Grévy, it is the sole extant member of the subgenus \"Dolichohippus\". Grévy's zebra is found in Kenya and Ethiopia. Compared with other zebras, it is tall, has large ears, and its stripes are narrower.\n\nThe Grévy's zebra lives in semi-arid grasslands where it feeds on grasses, legumes, and browse; it can survive up to five days without water. It differs from the other zebra species in that it does not live in harems and has few long-lasting social bonds. Male territoriality and mother–foal relationships form the basis of the social system of the Grévy's zebra. This zebra is considered to be endangered. Its population has declined from 15,000 to 3,000 since the 1970s. However, as of 2008, the population is stable.\n\nThe Grévy's zebra was first described by French naturalist Émile Oustalet in 1882. He named it after Jules Grévy, then president of France, who, in the 1880s, was given one by the government of Abyssinia. Traditionally, this species was classified in the subgenus \"Dolichohippus\" with plains zebra and mountain zebra in \"Hippotigris\". Fossils of zebra-like equids have been found throughout Africa and Asia in the Pliocene and Pleistocene deposits. Notable examples include \"E. sanmeniensis\" from China, \"E. cautleyi\" from India, \"E. valeriani\" from central Asia and \"E. oldowayensis\" from East Africa. The latter, in particular is very similar to the Grévy's zebra and may have been its ancestor. \nThe modern Grévy's zebra arose in the early Pleistocene. Zebras appear to be a monophyletic lineage and recent (2013) phylogenies have placed Grevy's zebra in a sister taxon with the plains zebra. In areas where Grévy's zebras are sympatric with plains zebras, the two may gather in same herds and fertile hybrids do occur.\n\nThe Grévy's zebra is the largest of all wild equines. It is from head to tail with a tail, and stands high at the withers. These zebras weigh . Grévy's zebra differs from the other two zebras in its more primitive characteristics. It is particularly mule-like in appearance; the head is large, long, and narrow with elongated nostril openings; the ears are very large, rounded, and conical and the neck is short but thick. The zebra's muzzle is ash-grey to black in colour with the lips having whiskers. The mane is tall and erect; juveniles have a mane that extends to the length of the back and shortens as they reach adulthood.\n\nAs with all zebra species, the Grevy's zebra's pelage has a black and white striping pattern. The stripes are narrow and close-set, being broader on the neck, and they extend to the hooves. The belly and the area around the base of the tail lack stripes which is unique to the Grevy's zebra. Foals are born with brown and white striping, with the brown stripes darkening as they grow older. Embryological evidence has shown that the zebra's background colour is dark and the white is an addition. The stripes of the zebra may serve to make it look bigger than it actually is or disrupt its outline. It appears that a stationary zebra can be inconspicuous at night or in shade. Experiments have suggested that the stripes polarize light in such a way that it discourages biting horse-flies in a manner not shown with other coat patterns. Other studies suggest that, when moving, the stripes may confuse observers, such as mammalian predators and biting insects, via two visual illusions, the wagon wheel effect, where the perceived motion is inverted, and the barber pole illusion, where the perceived motion is in a wrong direction.\n\nThe Grévy’s zebra largely inhabits northern Kenya, with some isolated populations in Ethiopia. It was extirpated from Somalia and Djibouti and its status in South Sudan is uncertain. It lives in \"Acacia\"-\"Commiphora\" bushlands and barren plains. Ecologically, this species is intermediate between the arid-living African wild ass and the water-dependent plains zebra. Lactating females and non-territorial males use areas with green, short grass and medium, dense bush more often than non-lactating females and territorial males.\n\nGrévy's zebras rely on grasses, legumes, and browse for nutrition. They commonly browse when grasses are not plentiful. Their hindgut fermentation digestive system allows them to subsist on diets of lower nutritional quality than that necessary for ruminant herbivores. Grevy's zebras can survive up to a week without water, but will drink daily when it is plentiful. They often migrate to better watered highlands during the dry season. Females require significantly more water when they are lactating. During droughts, the zebras will dig water holes and defend them. The grévy's zebras main predator is the lion, but adults can be hunted by Spotted hyenas. African hunting dogs, cheetahs and leopards almost never attack adults, even in desperate times, but sometimes prey on young animals, although females are fiercely protective of their young. In addition, they are susceptible to various gastro-intestinal parasites, notably of the genus \"Trichostrongylus\".\n\nAdult males mostly live in territories during the wet seasons but some may stay in them year round if there's enough water left. Stallions that are unable to establish territories are free-ranging and are known as bachelors. Females, young and non-territorial males wander through large home ranges. The females will wander from territory to territory preferring the ones with the highest-quality food and water sources. Up to nine males may compete for a female outside of a territory. Territorial stallions will tolerate other stallions who wander in their territory, however when an oestrous female is present the territorial stallion keeps other males at bay. Non-territorial males may avoid territorial ones because of harassment. When females are not around, a territorial stallion will seek the company of other stallions. The stallion shows his dominance with an arched neck and a high-stepping gait and the least dominant stallions submit by extending their tail, lowering their heads and nuzzling their superior's chest or groin.\n\nZebras produce numerous sounds and vocalisations, when alarmed, they produce deep hoarse grunts. Whistling and squealing are made when alarmed, during fights, or when scared or in pain. Snorting may be produced when scared or as a warning. A male will bray in defense of its territory, when driving females, or keeping other males at bay. Barking may be made during copulation and distressed foals will squeal. The call of the Grévy's zebra has been described as \"something like a hippo's grunt combined with a donkey's wheeze\". To get rid of flies or parasites, they roll in dust, water or mud or, in the case of flies, twitch their skin. They also rub against trees, rocks and other objects to get rid of irritations like itchy skin, hair or parasites. Although Grévy's zebras do not perform mutual grooming, they do sometimes rub against a conspecific.\n\nGrévy's zebras can mate and give birth year-round, but most mating takes place in the early rainy seasons and births mostly take place in August or September after the long rains. An oestrous mare may visit though as many as four territories a day and will mate with the stallions in them. Among territorial stallions, the most dominant ones control territories near water sources, which mostly attract mares with dependant foals, while more subordinate stallions control territories away from water with greater amounts of vegetation, which mostly attract mares without dependant foals. \n\nThe resident stallions of territories will try to subdue the entering mares with dominance rituals and then continue with courtship and copulation. Grévy's zebra stallions have large testicles and can ejaculate a large amount of semen to replace the sperm of other males. This is a useful adaptation for a species whose females mate polyandrously. Bachelors or outside territorial stallions sometimes \"sneak\" copulation of mares in another stallion’s territory. While female associations with individual males are brief and mating is promiscuous, females who have just given birth will reside with one male for long periods and mate exclusively with that male. Lactating females are harassed by males more often than non-lactating ones and thus associating with one male and his territory provides an advantage as he will guard against other males.\n\nGestation of the Grévy's zebra normally lasts 390 days, with a single foal being born. A newborn zebra will follow anything that moves, so new mothers prevent other mares from approaching their foals while imprinting their own striping pattern, scent and vocalisation on them. Females with young foals may gather into small groups. Mares may leave their foals in \"kindergartens\" while searching for water. The foals will not hide, so they can be vulnerable to predators. However, kindergartens tend to be protected by an adult, usually a territorial male. A female with a foal stays with one dominant territorial male who has exclusive mating rights to her. While the foal will not likely be his, the stallion will look after it to ensure that the female stays in his territory. To adapt to a semi-arid environment, Grévy's zebra foals have longer nursing intervals and wait until they are three months old before they start drinking water. Although offspring become less dependant on their mothers after half a year, associations with them continue for up to three years.\n\nThe Grévy's zebra was known to the Europeans in antiquity and was used by the Romans in circuses. It was subsequently forgotten in the Western world for a thousand years. In the seventeenth century, the king of Shoa (now central Ethiopia) exported two zebras; one to the Sultan of Turkey and another to the Dutch governor of Jakarta. A century later, in 1882, the government of Abyssinia sent one to French president Jules Grévy. It was at that time that the animal was recognised as its own species and named in Grévy’s honour.\n\nThe Grévy's zebra is considered endangered. Its population was estimated to be 15,000 in the 1970s and by the early 21st century the population was lower than 3,500, a 75% decline. It is estimated that there are less than 2,500 Grévy's zebras still living in the wild. There are also an estimated 600 Grévy's zebras in captivity. Captive herds have been known to thrive, like at White Oak Conservation in Yulee, Florida, United States, where more than 70 foals have been born. There, research is underway in partnership with the Conservation Centers for Species Survival on semen collection and freezing and on artificial insemination. The Grévy's zebra population trend is considered stable as of 2008.\n\nThe Grévy's zebra is legally protected in Ethiopia. In Kenya, it is protected by the hunting ban of 1977. In the past, Grévy's zebras were threatened mainly by hunting for their skins which fetched a high price on the world market. However, hunting has declined and the main threat to the zebra is habitat loss and competition with livestock. Cattle gather around watering holes and the Grévy's zebras are fenced from those areas. Community-based conservation efforts have shown to be the most effective in preserving Grévy's zebras and their habitat. Less than 0.5% of the range of the Grévy's zebra is in protected areas. In Ethiopia, the protected areas include Alledeghi Wildlife Reserve, Yabelo Wildlife Sanctuary, Borana Controlled Hunting Area and Chalbi Sanctuary. In Kenya, important protected areas include the Buffalo Springs, Samburu and Shaba National Reserves and the private and community land wildlife conservancies in Isiolo, Samburu and the Laikipia Plateau.\n\nThe mesquite plant was introduced into Ethiopia around 1997 and is endangering the zebra's food supply. An invasive species, it is replacing the two grass species, \"Cenchrus ciliaris\" and \"Chrysopogon plumulosus,\" which the zebras eat for most of their food.\n\n"}
{"id": "33730339", "url": "https://en.wikipedia.org/wiki?curid=33730339", "title": "Hall Sapphire and Diamond Necklace", "text": "Hall Sapphire and Diamond Necklace\n\nThe Hall Sapphire and Diamond Necklace has 36 matched sapphires from Sri Lanka which total 195 carats. These sapphires are surrounded by 435 brilliant-cut diamonds that total 83.75 carats. The sapphires are cushion-cut, some of the diamonds are pear-shaped and the others are round cut. The setting is platinum.\n\nIt was designed by Harry Winston, Inc.. It is currently on display at the National Museum of Natural History, part of the Smithsonian Institution, in Washington, D.C., alongside the Bismarck Sapphire Necklace and the Logan sapphire. It was donated to the Smithsonian by Mrs. Evelyn Annenberg Hall (c. 1912 – April 21, 2005) in 1979. She was the sister of Walter Annenberg, publisher, businessman, and philanthropist.\n\n\n"}
{"id": "18519026", "url": "https://en.wikipedia.org/wiki?curid=18519026", "title": "History of chromatography", "text": "History of chromatography\n\nThe history of chromatography spans from the mid-19th century to the 21st. Chromatography, literally \"color writing\", was used—and named— in the first decade of the 20th century, primarily for the separation of plant pigments such as chlorophyll (which is green) and carotenoids (which are orange and yellow). New forms of chromatography developed in the 1930s and 1940s made the technique useful for a wide range of separation processes and chemical analysis tasks, especially in biochemistry.\n\nThe earliest use of chromatography—passing a mixture through an inert material to create separation of the solution components based on differential adsorption—is sometimes attributed to German chemist Friedlieb Ferdinand Runge, who in 1855 described the use of paper to analyze dyes. Runge dropped spots of different inorganic chemicals onto circles of filter paper already impregnated with another chemical, and reactions between the different chemicals created unique color patterns. According to historical analysis of L. S. Ettre, however, Runge's work had \"nothing to do with chromatography\" (and instead should be considered a precursor of chemical spot tests such as the Schiff test).\n\nIn the 1860s, Christian Friedrich Schönbein and his student Friedrich Goppelsroeder published the first attempts to study the different rates at which different substances move through filter paper. Schönbein, who thought capillary action (rather than adsorption) was responsible for the movement, called the technique capillary analysis, and Goppelsroeder spent much of his career using capillary analysis to test the movement rates of a wide variety of substances. Unlike modern paper chromatography, capillary analysis used reservoirs of the substance being analyzed, creating overlapping zones of the solution components rather than separate points or bands.\n\nWork on capillary analysis continued, but without much technical development, well into the 20th century. The first significant advances over Goppelsroeder's methods came with the work of Raphael E. Liesegang: in 1927, he placed filter strips in closed containers with atmospheres saturated by solvents, and in 1943 he began using discrete spots of sample adsorbed to filter paper, dipped in pure solvent to achieve separation. This method, essentially identical to modern paper chromatography, was published just before the independent—and far more influential—work of Archer Martin and his collaborators that inaugurated the widespread use of paper chromatography.\n\nIn 1897, the American chemist David Talbot Day (1859-1915), then serving with the U.S. Geological Survey, observed that crude petroleum generated bands of color as it seeped upwards through finely divided clay or limestone. In 1900, he reported his findings at the First International Petroleum Congress in Paris, where they created a sensation.\n\nThe first true chromatography is usually attributed to the Russian-Italian botanist Mikhail Tsvet. Tsvet applied his observations with filter paper extraction to the new methods of column fractionation that had been developed in the 1890s for separating the components of petroleum. He used a liquid-adsorption column containing calcium carbonate to separate yellow, orange, and green plant pigments (what are known today as xanthophylls, carotenes, and chlorophylls, respectively). The method was described on December 30, 1901 at the 11th Congress of Naturalists and Doctors (XI съезд естествоиспытателей и врачей) in Saint Petersburg. The first printed description was in 1903, in the Proceedings of the Warsaw Society of Naturalists, section of biology. He first used the term \"chromatography\" in print in 1906 in his two papers about chlorophyll in the German botanical journal, \"Berichte der Deutschen Botanischen Gesellschaft\". In 1907 he demonstrated his chromatograph for the German Botanical Society. Mikhail's surname \"Цвет\" means \"color\" in Russian, so there is the possibility that his naming the procedure chromatography (literally \"color writing\") was a way that he could make sure that he, a commoner in Tsarist Russia, could be immortalized.\n\nIn a 1903 lecture (published in 1905), Tsvet also described using filter paper to approximate the properties of living plant fibers in his experiments on plant pigments—a precursor to paper chromatography. He found that he could extract some pigments (such as orange carotenes and yellow xanthophylls) from leaves with non-polar solvents, but others (such as chlorophyll) required polar solvents. He reasoned that chlorophyll was held to the plant tissue by adsorption, and that stronger solvents were necessary to overcome the adsorption. To test this, he applied dissolved pigments to filter paper, allowed the solvent to evaporate, then applied different solvents to see which could extract the pigments from the filter paper. He found the same pattern as from leaf extractions: carotene could be extracted from filter paper using non-polar solvents, but chlorophyll required polar solvents.\n\nTsvet's work saw little use until the 1930s.\n\nChromatography methods changed little after Tsvet's work until the explosion of mid-20th century research in new techniques, particularly thanks to the work of Archer John Porter Martin and Richard Laurence Millington Synge. By \"the marrying of two techniques, that of chromatography and that of countercurrent solvent extraction\", Martin and Synge developed partition chromatography to separate chemicals with only slight differences in partition coefficients between two liquid solvents. Martin, who had previously been working in vitamin chemistry (including attempts to purify vitamin E), began collaborating with Synge in 1938, brought his experience with equipment design to Synge's project of separating amino acids. After unsuccessful experiments with complex countercurrent extraction machines and liquid-liquid chromatography methods where the liquids move in opposite directions, Martin hit on the idea of using silica gel in columns to hold water stationary while an organic solvent flows through the column. Martin and Synge demonstrated the potential of the methods by separating amino acids marked in the column by the addition of methyl red. In a series of publications beginning in 1941, they described increasingly powerful methods of separating amino acids and other organic chemicals.\n\nIn pursuit of better and easier methods of identifying the amino acid constituents of peptides, Martin and Synge turned to other chromatography media as well. A short abstract in 1943 followed by a detailed article in 1944 described the use of filter paper as the stationary phase for performing chromatography on amino acids: paper chromatography. By 1947, Martin, Synge and their collaborators had applied this method (along with Fred Sanger's reagent for identifying N-terminal residues) to determine the pentapeptide sequence of Gramicidin S. These and related paper chromatography methods were also foundational to Fred Sanger's effort to determine the amino acid sequence of insulin.\n\nMartin, in collaboration with Anthony T. James, went on to develop gas chromatography (the principles of which Martin and Synge had laid out in their landmark 1941 paper) beginning in 1949. In 1952, during his lecture for the Nobel Prize in Chemistry (shared with Synge, for their earlier chromatography work) Martin announced the successful separation of a wide variety of natural compounds by gas chromatography. (Austrian chemist Fritz Prior had achieved limited success with gas chromatography, separating oxygen and carbon dioxide, in 1947 during his Ph.D. research. The method of Martin and James, however, became the basis for subsequent developments in gas chromatography.)\n\nThe ease and efficiency of gas chromatography for separating organic chemicals spurred the rapid adoption of the method, as well as the rapid development of new detection methods for analyzing the output. The thermal conductivity detector, described in 1954 by N. H. Ray, was the foundation for several other methods: the flame ionization detector was described by J. Harley, W. Nel, and V. Pretorius in 1958, and James Lovelock introduced the electron capture detector that year as well. Others introduced mass spectrometers to gas chromatography in the late 1950s.\n\nThe work of Martin and Synge also set the stage for high performance liquid chromatography, suggesting that small sorbent particles and pressure could produce fast liquid chromatography techniques. This became widely practical by the late 1960s (and the method was used to separate amino acids as early as 1960).\n\nThe first developments in thin layer chromatography occurred in the 1940s, and techniques advanced rapidly in the 1950s after the introduction of relatively large plates and relatively stable materials for sorbent layers.\n\nIn 1987 Pedro Cuatrecasas and Meir Wilchek were awarded the Wolf Prize in Medicine for the invention and development of affinity chromatography and its applications to biomedical sciences.\n\n"}
{"id": "32711093", "url": "https://en.wikipedia.org/wiki?curid=32711093", "title": "Hypersonic Technology Vehicle 2", "text": "Hypersonic Technology Vehicle 2\n\nHypersonic Technology Vehicle 2 (HTV-2) is a crewless, experimental hypersonic glide vehicle rocket glider developed as part of the DARPA Falcon Project capable of flying at 13,000 mph (Mach 17.53, 21,000 km/h). It is a test bed for technologies to provide the United States with the capability to reach any target in the world within one hour (Prompt Global Strike) using an unmanned hypersonic bomber aircraft.\n\nThe Falcon HTV-1 program, which preceded the Falcon HTV-2 program, was conducted in April, 2010. The mission ended within nine minutes from launch. Both these missions are funded by the US Defense Advanced Research Projects Agency (DARPA) to help develop hypersonic technologies and to demonstrate its effectiveness. Under the original plan, HTV-1 was to feature a hypersonic lift-to-drag ratio (L/D) of 2.5, increasing to 3.5-4 for the HTV-2 and 4-5 for the HTV-3. The actual ratio of HTV-2 was estimated to be 2.6.\n\nHTV-2 was to lead to the development of an HTV-3X vehicle, known as Blackswift, which would have formed the basis for deployment around 2025 of a reusable Hypersonic Cruise Vehicle, an unmanned aircraft capable of taking off from a conventional runway with a 5,400 kg (12,000 lb) payload to strike targets 16,650 km away in under 2 hours. The HCV would have required an L/D of 6-7 at M10 and 130,000 ft (40,000m).\n\nDevelopment of protection structures that are tough and lightweight, development of an aerodynamic shape that has a high lift to drag ratio, development of automatic navigation control systems etc. were some of the initial technical challenges that had been overcome in the final design. The various departments involved in designing the vehicle included aerothermodynamics, materials science, hypersonic navigation, guidance and control systems, endo- and exo-atmospheric flight dynamics, telemetry and range safety analysis. The craft could cover 17,000 kilometres, the distance between London and Sydney, in 49 minutes.\n\nBuilt by Lockheed Martin Corp, the HTV-2 is made of carbon composite material; the durability of such material was needed to prevent important internal components from being destroyed because they are a few inches from its surface. The surface temperature of the HTV-2 was expected to reach 3,500 degrees Fahrenheit or more in flight; steel melts at 2500 degrees Fahrenheit.\n\nBoth flights reached Mach 20 (high-hypersonic speed) and lost telemetry at 9 minutes of a planned 30-minute mission.\n\nThe HTV-2's first flight was launched on 22 April 2010. The HTV-2 glider was to fly across the Pacific to Kwajalein at Mach 20. The HTV-2 was boosted by a Minotaur IV Lite rocket launched from Vandenberg Air Force Base, California; the glider was carried inside the nose of the Minotaur IV Lite rocket into outer space with a launch altitude of 100 miles. The flight plan called for the craft to separate from the launch vehicle, level out and glide above the Pacific at Mach 20. Contact was lost with the vehicle at nine minutes into the 30-minute mission, and the glider's skin disintegrated. In mid-November, DARPA stated that the first test flight ended when the computer autopilot \"commanded flight termination\" after the vehicle began to roll violently.\n\nA second flight was initially scheduled to be launched on August 10, 2011, but bad weather forced a delay. The flight was launched the following day, on 11 August 2011. The unmanned Falcon HTV-2 successfully separated from the booster and entered the mission's glide phase, but again lost contact with control about nine minutes into its planned 30-minute Mach 20 glide flight. Initial reports indicated it purposely impacted the Pacific Ocean along its planned flight path as a safety precaution. The glider's surface reached 3,500 degrees Fahrenheit (the speed and heat caused part of the skin to peel away from the aerostructure) and controlled itself for 3 minutes before crashing.\n\nDARPA does not plan to conduct a third flight test of the HTV-2. The decision was made because substantial data was collected from the first two flights, and a third was not thought likely to provide any additional valuable data for the cost. The first flight provided data in aerodynamics and flight performance, while the second provided information about structures and high temperatures. Experience gained from the HTV-2 will be used to improve hypersonic flight.\n\nWork on the HTV-2 will continue to Summer 2014 to capture technology lessons and improve design tools and methods for high-temperature composite aeroshells.\n\n\n"}
{"id": "9146657", "url": "https://en.wikipedia.org/wiki?curid=9146657", "title": "Hypothetical list of biota", "text": "Hypothetical list of biota\n\nA hypothetical list of biota, or \"hypothetical list\" for short, is a list of taxa (of plants, animals, fungi etc.) which are not recorded from a given geographical area, but which \"may\" be found there. Such lists are sometimes included by authors of regional biota, partly to demonstrate that the authors have considered and rejected the taxa in question rather than overlooked them, and partly to encourage researchers and others to seek out the taxa in question so that they can be added to the list of the area's biota in future revisions.\n\nTaxa may be included for different reasons:\n\nA 1973 checklist of fleas in Connecticut added species to its hypothetical list if they were documented in a bordering state and have a host found in Connecticut.\n\nOrnithological works which have included hypothetical lists include the following:\n\n"}
{"id": "2141003", "url": "https://en.wikipedia.org/wiki?curid=2141003", "title": "Indium gallium phosphide", "text": "Indium gallium phosphide\n\nIndium gallium phosphide (InGaP), also called gallium indium phosphide (GaInP), is a semiconductor composed of indium, gallium and phosphorus. It is used in high-power and high-frequency electronics because of its superior electron velocity with respect to the more common semiconductors silicon and gallium arsenide.\n\nIt is used mainly in HEMT and HBT structures, but also for the fabrication of high efficiency solar cells used for space applications and, in combination with aluminium (AlGaInP alloy) to make high brightness LEDs with orange-red, orange, yellow, and green colors. Some semiconductor devices such as EFluor Nanocrystal use InGaP as their core particle.\n\nIndium gallium phosphide is a solid solution of indium phosphide and gallium phosphide.\n\nGaInP is a solid solution of special importance, which is almost lattice matched to GaAs. This allows, in combination with (AlGa)In, the growth of lattice matched quantum wells for red emitting semiconductor lasers, e.g. red emitting (650nm) RCLEDs or VCSELs for PMMA plastic optical fibers.\n\nGaInP is used as the high energy junction on double and triple junction photovoltaic cells grown on GaAs. Recent years have shown GaInP/GaAs tandem solar cells with AM0 (sunlight incidence in space=1.35 kW/m) efficiencies in excess of 25%.\n\nA different composition of GaInP, lattice matched to the underlying GaInAs, is utilized as the high energy junction GaInP/GaInAs/Ge triple junction photovoltaic cells.\n\nGrowth of GaInP by epitaxy can be complicated by the tendency of GaInP to grow as an ordered material, rather than a truly random solid solution (i.e., a mixture). This changes the bandgap and the electronic and optical properties of the material.\n\n\n\n"}
{"id": "9332933", "url": "https://en.wikipedia.org/wiki?curid=9332933", "title": "Information search process", "text": "Information search process\n\nThe information search process (ISP) is a six-stage process of information seeking behavior in library and information science. The ISP was first suggested by Carol Kuhlthau in 1991.\n\nIt describes the thoughts, feelings and actions of the searcher, and is often used to describe students.\n\nDuring the first stage, \"initiation\", the information seeker recognizes the need for new information to complete an assignment. As they think more about the topic, they may discuss the topic with others and brainstorm the topic further. This stage of the information seeking process is filled with feelings of apprehension and uncertainty.\n\nIn the second stage, \"selection\", the individual begins to decide what topic will be investigated and how to proceed. Some information retrieval may occur at this point, resulting in multiple rounds of query reformulation. The uncertainty associated with the first stage often fades with the selection of a topic, and is replaced with a sense of optimism.\n\nIn the third stage, \"exploration\", information on the topic is gathered and a new personal knowledge is created. Students endeavor to locate new information and situate it within their previous understanding of the topic. In this stage, feelings of anxiety may return if the information seeker finds inconsistent or incompatible information.\n\nDuring the fourth stage, \"formulation\", the information seeker starts to evaluate the information that has been gathered. At this point, a focused perspective begins to form and there is not as much confusion and uncertainty as in earlier stages. Formulation is considered to be the most important stage of the process. The information seeker will here formulate a personalized construction of the topic from the general information gathered in the exploration phase.\n\nDuring the fifth stage, \"collection\", the information seeker knows what is needed to support the focus. Now presented with a clearly focused, personalized topic, the information seeker will experience greater interest, increased confidence, and more successful searching.\n\nIn the sixth and final stage, \"search closure\", the individual has completed the information search. Now the information seeker will summarize and report on the information that was found through the process. The information seeker will experience a sense of relief and, depending on the fruits of their search, either satisfaction or disappointment.\n\n"}
{"id": "12389663", "url": "https://en.wikipedia.org/wiki?curid=12389663", "title": "Insuetophrynus", "text": "Insuetophrynus\n\nInsuetophrynus is a monotypic genus of frogs in the Rhinodermatidae family. The sole species is Insuetophrynus acarpicus, also known as Barrio's frog. It is endemic to Chile and only known from few localities on the Valdivian Coast Range between Chanchán in the Los Ríos Region in the south and Queule (southernmost Araucanía Region) and Colequal Alto in the north; the fourth locality is Mehuín, which is the type locality. The altitudinal range is asl.\n\nAdult males measure and females in snout–vent length. The body is sturdy with muscular arms and legs (these frogs are powerful jumpers). The toes are partially webbed and thinner than the fingers which are short, thick, and unwebbed. The head is wider than long, with a broad, rounded snout. The eyes are large, and tympanum is visible but not large. The back is reddish brown with some whitish granulations. The hind legs have transverse, darker bands. The throat is pinkish yellow, and the stomach is pale. Skin is dorsally quite granular or warty. Also ventral region is also very granular, apart from the throat that is fairly smooth.\n\n\"Insuetophrynus acarpicus\" inhabits coastal streams in temperate forest. Adults hide under stones during the day, emerging at night to feed along the stream margins. Tadpoles can be found under stones in muddy areas with slow current.\n\nThe species has a small area of distribution (its known range extends only 33 km along the coast) and its habitat is threatened by clear cutting and afforestation.\n"}
{"id": "14085448", "url": "https://en.wikipedia.org/wiki?curid=14085448", "title": "Interactive television standards", "text": "Interactive television standards\n\nTeletext was introduced in the analogue television in the 80's, leading to a limited interaction with television sets to obtain information about things like the schedule and weather. But nowadays this concept goes even far away and a new and improved way of interaction with the user has been developed. The early private broadcasters, as Canal+, were the pioneers in adopting this new form and today are preceded by their digital formats.\n\nAs a consequence of the Analogue Switchoff the Project known as Digital Terrestrial Television (DTT) was developed creating a public digital format television with more features, in competition with private broadcasters. Among this features one can found interactive menus that, as in the case of the private broadcasting, give information to the user and let them adapt the product to their own needs. This user-television interaction is today known as interactive television.\n\nAll of these can happen thanks to the Set-Top-Boxes or Set-Top-Box (STBs), the television decoders that households usually have at home, that see to receive and decode the digital signal to show it by the analogue television set. This device lets the users accede to the contents the digital television net offers. One of their many functions is running the interactive applications, being this the object of this article.\n\nFor interactivity, it is required that the STBs could be dynamically programmed and updated. For that, there are some different solutions in the market, among them, the definition of an intermediary software layer on which the applications, broadcast together with the audiovisual signals, run. This intermediary software layer is called Middleware.\n\nIt was defined by Canal Plus Technologies, which represents the widest range of solutions in the market so that the software in the STBs interprets and executes interactive applications, broadcast software from the server for the broadcasting of applications and data via satellite, cable and terrestrial nets, moreover runs interactive applications and their execution environment. Also, includes different profiles to better respond to the broadcasters needs.\n\nThese standard runs applications written in different programming languages as Java, MHEG-5 or HTML and supports the specifications DVB-MHP, OCAP, DAVIC or ATSC adding, at the same time, other own and new specifications.\n\nThe most important product from Open TV, a middleware for digital television (DTV) widely extended. The Open TV Core software technology contains a hardware abstraction layer (to let the hardware be independent), TV libraries, a selection of execution environments for the applications, and support for Personal Video Recorders (PVRs), to create a DTT environment for the decoders (STBs). The TV libraries include support for rich graphics (RG) and High Definition (HD), net communication from phone line up to broad band IP (via DSL, ethernet or fibre), management of the digital audio and video signals (DVB, as well as another standards and proprietary formats), and support to authentication and encryption by using CA/DRM systems.\nOpen TV Core supports a number of Applications environments execution(AEE) including the 'C' Virtual Machine, a HTML browser, an Adobe Flash presentation environment and a Java Virtual Machine in compliance with the MHP standard. The 'C' Virtual Machine is an execution environment that allows the APIs of the Open TV software Developers Kit, that lets the content providers create, in O code, applications centred on TV by using the development tools from Open TV or another sellers.\n\nAs early as 1995 the ISO (International Organisation for Standardisation), together with the Multimedia and Hypermedia Experts Group, published the MHEG standard. This gave a statement approach to the creation of multimedia applications that could work in every operative system in compliance with this standard. Conceptually, MHEG intended to do for multimedia applications the same as HTML did for documents at the proper time that is, to give a common exchange format that should be executed in every receiver. \n\n\"MHEG-1\": this version included support for objects containing procedure codes, that could widens the basic model of MHEG-1 adding decision making functions, as it was not possible in other way.\n\n\"MHEG-3\": this version defined a standard virtual machine and a byte representation code that allows its portability through hardware platforms.\n\nThese versions were unsuccessful, since they were based on concepts very complicated and the industry was not ready for the characteristics they offer. \"MHEG-5\", the simpler version of MHEG-1, was created in April 1997.\n\nMany of the functions that were shaping it were equal but simultaneously there were many differences between the two versions.\nMHEG-3 was overcome by the success of Java and in 1998 they decided to create the sixth version, \"MHEG-6\", which was based on the fifth version adding a support for the use of Java to develop objects script, mixing the declarative force of MHEG with the procedure elements of Java. For doing this, they defined a Java application programming interface (API) for MHEG so that the code Java could manipulate MHEG objects in its mother application.\n\nThough the MHEG-6 was not extended, it was the base of the DAVIC's (Digital Audio Video Council) standard.\n\nThis standard was created little later than the MHEG-6 in 1998. It was created adding a new series of APIs Java to MHEG's sixth version. The APIs of the above-mentioned standard were allowing the objects Java the access to some services of information, control of services of audio content and video and the managing of the management of the resources in the receiver. Though the creation of an application was not possible pure Java for the receiver DAVIC, the APIs Java already was capable of controlling more elements of the receiver of what was possible with other standards.\n\nThis is defined by the Digital Video Broadcasting (DVB) to offer interactive services in the DTV. It is a limited version of the virtual machine of Java, where a set of extra functionalities are added for the adjustment to the environment of the DTV. With the purpose of arranging the specification of this standard there are defined three profiles related to the capacities of the STBs:\n\nThere exist two versions that cover the mentioned profiles:\n\n\nThe American company CableLabs collaborated with DVB for the creation of a new opened standard, which it led to the acceptance of the specification of the standard MHP as base for this standard, the OCAP (OpenCable Application Platform), in January, 2002. With MHP in its center OCAP provides a common especificaión for the middleware layer for the systems of cable in the United States. Since in the United States DVB standards are not used, OCAP is based on those parts that are not DVB specific, replacing the rest of DVB specific ones such as DVB IF API. Originally, OCAP was based on the 1.0.0 version of the specification MHP.\n\nLater, DVB presented the specification Globally Executable MHP (GEM) to facilitate the use of MHP's elements in other specifications. OCAP's recent versions use GEM instead of MHP as base, but they refer to some MHP's elements that are not included in the especificaión GEM.\n\nThis platform was created by the Advanced Television Systems Committee (ATSC), as common base for all the systems of interactive TV in USA, for cable, terrestrial or satellite. It is also based on GEM and adds some OCAP's elements that are adapted for the USA market.\n\n\nFor more information you can visit these pages:\n\n\nalso you can read next book:\n\n"}
{"id": "1264207", "url": "https://en.wikipedia.org/wiki?curid=1264207", "title": "International Meteor Organization", "text": "International Meteor Organization\n\nThe International Meteor Organization (IMO) was formally founded in 1988 from predecessor gatherings over many years. IMO has several hundred members and was created in response to an ever-growing need for international cooperation on amateur and professional meteor work. The collection of meteor observations by several methods from all around the world ensures the comprehensive study of meteor showers and their relation to comets and interplanetary dust.\n\nIMO publishes a bimonthly journal called WGN and holds an annual International Meteor Conference (IMC) in September.\n\n\n"}
{"id": "49183887", "url": "https://en.wikipedia.org/wiki?curid=49183887", "title": "Intraarticular fracture", "text": "Intraarticular fracture\n\nAn intraarticular fracture is a bone fracture in which the break crosses into the surface of a joint. This always results in damage to the cartilage. Compared to extraarticular fractures, intraarticular have a higher risk for developing long-term complications, such as posttraumatic osteoarthritis. Treatment considerations include restoring joint surface congruity and maintaining joint alignment and stability.\n\n"}
{"id": "11821531", "url": "https://en.wikipedia.org/wiki?curid=11821531", "title": "List of Belarusian flags", "text": "List of Belarusian flags\n\nThe following is a list of flags of Belarus.\n\n"}
{"id": "3260100", "url": "https://en.wikipedia.org/wiki?curid=3260100", "title": "List of Jewish American mathematicians", "text": "List of Jewish American mathematicians\n\n\"This is a list of famous Jewish American mathematicians. For other famous Jewish Americans, see List of Jewish Americans.\"\n\n\n\n"}
{"id": "1877442", "url": "https://en.wikipedia.org/wiki?curid=1877442", "title": "List of geographic information systems software", "text": "List of geographic information systems software\n\nGIS software encompasses a broad range of applications which involve the use of a combination of digital maps and georeferenced data. GIS software can be sorted into different categories.\n\nThe development of open source GIS software has—in terms of software history—a long tradition with the appearance of a first system in 1978. Numerous systems are available which cover all sectors of geospatial data handling.\n\nThe following open-source desktop GIS projects are reviewed in Steiniger and Bocher (2008/9):\n\n\nBesides these, there are other open source GIS tools:\n\nApart from desktop GIS, many other types of GIS software exist. A general overview of GIS software projects for each category was done in 2012. Below is a similar listing of open source GIS projects.\n\n\n\n\n\n\n\nNote: Almost all of the below companies offer Desktop GIS and WebMap Server products. Some offer Spatial DBMS products as well.\n\n\n\nMany suppliers are now starting to offer Internet based services as well as or instead of downloadable software and/or data. These can be free, funded by advertising or paid for on subscription; they split into three areas:\n\n\n\n"}
{"id": "32709669", "url": "https://en.wikipedia.org/wiki?curid=32709669", "title": "List of named inorganic compounds", "text": "List of named inorganic compounds\n\nWell-known inorganic and organometallic compounds and reagents that are named after individuals include:\n\n\n\n"}
{"id": "5680493", "url": "https://en.wikipedia.org/wiki?curid=5680493", "title": "List of pharmacy organisations in the United Kingdom", "text": "List of pharmacy organisations in the United Kingdom\n\nThis article is a list of pharmacy organisations in the United Kingdom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "29342556", "url": "https://en.wikipedia.org/wiki?curid=29342556", "title": "List of star extremes", "text": "List of star extremes\n\nA star is a sphere that is mainly composed of hydrogen and plasma, held together by gravity and is able to produce light through nuclear fusion. Stars exhibit many diverse properties, resulting from different masses, volumes, velocities, stage in stellar evolution and even proximity to earth. Some of these properties are considered extreme and sometimes disproportionate by astronomers.\n\n"}
{"id": "19972962", "url": "https://en.wikipedia.org/wiki?curid=19972962", "title": "List of statistics journals", "text": "List of statistics journals\n\nThis is a list of scientific journals published in the field of statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following journals are considered open access:\n\n\n"}
{"id": "23726959", "url": "https://en.wikipedia.org/wiki?curid=23726959", "title": "Maurice Cole Tanquary", "text": "Maurice Cole Tanquary\n\nMaurice Cole Tanquary (November 26, 1881 - October 25, 1944) was a professor of entomology, a member of the Crocker Land Expedition and is considered to be a pioneer in modern beekeeping.\n\nTanquary was the son of Thomas J. and Florence A. Tanquaary and was raised on a farm near Lawrenceville, Illinois, where he attended public schools. He then attended Vincennes University where he was a member of Tau Phi Delta society and was one of the authors of the society's first constitution and by-laws. He graduated in 1903 and taught for four years in the public schools of Lawrence County, Illinois. He then enrolled at the University of Illinois where he received his AB degree in 1907, his MA in 1908, and PhD in 1912. From 1908 to 1909 he served as a part-time assistant to the State Entomologist of Illinois. During the summer of 1910 he studied at Harvard University and during the summer of 1911 he was a field agent for the State Entomologist of Minnesota. While at Illinois he was a founder of the Ionian Literary Society and a charter member of the Acacia (Masonic) fraternity where he was the national treasurer from 1908-1909.\n\nAfter earning his doctorate he became a professor of agriculture at Kansas State Agricultural College in 1913 where he was given leave to join the Crocker Land Expedition as a zoologist later that year.\n\nAs the zoologist for the expedition, Tanquary was not involved in the final push to find the island from the village of Etah in northern Greenland. Instead, he and fellow Illinois alumnus Walter Elmer Ekblaw were stationed at a Danish trading post 120 miles to the south. They became stranded there after Ekblaw was struck with snow blindess and almost ran out of food in 1914. They were rescued just in time in August and returned to Etah.\n\nIn December 1914 Tanquary and Donald Baxter MacMillan set off by dogsled for southern Greenland in an attempt to send out word that Crocker Land did not exist and that they would need a rescue ship in 1915. During the trip they became lost for ten days in temperatures as low as -50F. Running low on provisions, they had to eat several of their dogs. As luck would have it, they happened upon an Eskimo settlement. MacMillan decided to return to Etah and have Tanquary complete the journey with a Danish trader and an Eskimo guide. After making it to the mail station in southern Greenland, Tanquary made the 400 mile trip back to Etah. Along the way, Tanquary removed his boots and pieces of raw, bleeding skin and flesh fell off his rotting toes. Despite the frostbite, Tanquary managed to drive his dog team to Etah, where his big toes were amputated. Ekblaw described Tanquary’s dash back to Etah as “the grittiest exploit of the expedition.”\n\nTanquary's message for help was received and a rescue ship was sent. Unfortunately, the ship became entrapped in ice and Tanquary had to make another trip to the southern tip of the island. The trip began on December 16, 1915 from Etah and ended on April 20, 1916. He was able to catch a ride on a ship going to Copenhagen, Denmark which he reached on May 20, 1916. He promptly cabled New York asking for a second relief ship to be sent to the party and he made arrangements for his return to the U.S.. The second rescue ship, sent in the summer of 1916, also became stuck in the ice and the expedition was not rescued until 1917.\n\nJournals from Tanquary, Walter Ekblaw, Donald and Mirriam MacMillan are available online at the George J. Mitchell Department of Special Collections & Archives website. Digitization of materials at Bowdoin College related to the Crocker Land Expedition generously funded by the Gladys Krieble Delmas Foundation in 2016.\n\nTanquary married Josephine Perry of Manhattan, KS on his return from the expedition. They had a child named Jean who was born in April 1917.\n\nHe returned to Kansas State Agricultural College in 1916 as an assistant professor before becoming as associate professor in 1919. Later in 1919, he became the chief of entomology at the Texas Agricultural Experiment Station and was named Texas' state entomologist.\n\nIn 1920 he made a trip to Mexico to study the pink bollworm which infested cotton crops.\n\nTanquary was interested in apiculture and resigned in 1923 to enter professional beekeeping in North Dakota. He joined the University of Minnesota College of Agriculture as an entomologist in 1928 and remained there until 1944. While in Minnesota he devoted much of his time to the study of bees. He was also known as a good teacher who could apply experimental ability to practical manner.\n\nTanquary’s family donated his photographs, lantern slides and journals to The Peary-MacMillan Arctic Museum, Bowdoin College, in 2006.\n"}
{"id": "50405438", "url": "https://en.wikipedia.org/wiki?curid=50405438", "title": "Mdx mouse", "text": "Mdx mouse\n\nThe mdx mouse is a popular model for studying Duchenne muscular dystrophy (DMD).\n\nThe mdx mouse has a point mutation in its DMD gene, changing the amino acid coding for a glutamine to STOP codon. This causes the muscle cells to produce a small, nonfunctional dystrophin protein. As a result, the mouse has a mild form of DMD where there is increased muscle damage and weakness.\n"}
{"id": "47296524", "url": "https://en.wikipedia.org/wiki?curid=47296524", "title": "Model for assessment of telemedicine", "text": "Model for assessment of telemedicine\n\nModel for assessment of telemedicine (MAST) is a framework for assessment of the value of telemedicine.\n\nTelemedicine services may have many different types of outcomes and can be studied in many ways. In order for those who develop new telemedicine services to produce the information that healthcare managers need for making decisions on investment in telemedicine, a model for assessment of telemedicine (MAST) was developed. This work was done in 2010 through stakeholder workshops and on the basis of a systematic literature review.\n\nIf the objective of an assessment of telemedicine applications is to describe effectiveness and contribution to quality of care and to produce a basis for decision making, then MAST defines the relevant assessment framework fulfilling this objective as a multidisciplinary process which summarizes and evaluates information about the medical, social, economic and ethical issues related to the use of telemedicine in a systematic, unbiased, robust manner.\n\nThis statement is based on the definition of Health_technology_assessment (HTA) in the EUnetHTA project. Key concepts are \"multidisciplinary\" and \"systematic, unbiased and robust\". The first concept implies that the assessments should include all important outcomes of the applications for patients, clinicians, healthcare institutions and society in general. The others imply that assessments should be based on scientific studies and methods, scientific criteria for quality of evidence and scientific standards for reporting of results, e.g. as described in EQUATOR Network.\n\nIn practice the use of MAST includes three steps:\n\nFirstly, the assessment must start with preceding considerations in order to determine whether it is relevant for an institution at a given point in time to carry out the assessment. This step involves mainly assessment of the maturity of the technology and the organization planning to use it. If the technology is not matured and have not been tested in practice, then pilot studies must be carried out to mature the technology before a multidisciplinary study is initiated.\n\nSecondly, after the preceding considerations, the multidisciplinary assessment is carried out in order to describe and assess the different outcomes of the telemedicine application. This involves assessment of ouctomes within the following seven domains:\n\n\nThirdly, in relation to the description of the outcomes, an assessment should also be made of the transferability of the results to other settings or countries.\n\nMAST is the most widely used framework for assessment of telemedicine in Europe. The model is used in large EU funded telemedicine project like Renewing Health, United4Health, Smartcare and inCASA. These projects include more than 20.000 patients and more than 18 randomised controlled trials. A large number of individual telemedicine projects also use MAST e.g. Patient@home, Durand-Zaleski (2013) and Campos et al. (2013) \n\nThe number of publications of studies using MAST is still limited, but growing. The first clinical studies have been reported by Sorknæs et al. (2013), Karhula et al. (2015) and Rasmussen et al. (2015). Recently a study of the organizational outcomes of implementation of telemedicine was published by Rasmussen et al. (2015).\n\nMAST has also been recommended as a usable structure for assessment of outcomes of telemedicine by the association of Danish Regions Telemedicine strategy, by the British Thoracic Society statement on telemedicine (2014) and within the field of wound care by Angel et al. (2015).\n\nMAST is based on HTA and the EUnetHTA Core model, but whereas the core model includes 9 domains, MAST only includes 7 domains. This is done by combining the content of several domains into one. MAST has also a separate domain describing the impact of telemedicine on patient perception and thereby underlining the importance of the patients' view of this type of health care technology. In addition the three steps in MAST underline that the assessment of outcomes should be seen in the light of the maturity of the technology and the transferability of the results to other countries.\n\n"}
{"id": "22834281", "url": "https://en.wikipedia.org/wiki?curid=22834281", "title": "New towns movement", "text": "New towns movement\n\nThe new town movement refers to towns that were built after World War II and that have been purposefully planned, developed and built as a remedy to overcrowding and congestion in some instances, and to scattered ad hoc settlements in others. The main reason for it was to decongest larger industrialized cities, rehousing people in freshly built, new and fully planned towns that were completely self-sufficient and provided for the community.\n\nIn 1918, writing at a time when nineteenth century sanitary advances had revealed how badly off people in urban environments were and, owing to pioneers such as Patrick Geddes, the relationship between social issues and town planning was slowly being realised, Frederick Osborn refers to urban problems collectively as the ‘urban disease’ \nThe urban disease, a by-product of the industrial revolution, was brought on by a vicious cycle whereby industry chose to set up near population bases to ensure labour demands could be met which in turn attracted rural migrants seeking work to move into the city prompting further industry and so on. This resulted in greater pollution in the city, higher populations, and denser living conditions. Moreover, rural areas declining rapidly due to loss of population were left to decay.\n\nFurthermore, there were no powers in place to stop prosperous families moving to open spaces or from industry growing in the centres. Fringe growth was vigorous and existing centres were left to deteriorate. Accordingly, those who moved to new fringe suburbs to escape the congestion were in fact “bolstering the very process that caused them to move away.” \n\nAlthough aspirations of dispersing great cities are as old as the industrial revolution itself, it was not until 1817 that the first model communities were proposed by social reformer Robert Owen to address overcrowded towns. Inspired by John Bellers’s 1695 proposal for a College of Industry, a colony for the poor enabling disadvantaged people to work and their children to be educated, Owen proposed small, self-contained communities of about twelve hundred people reliant on agriculture but with some other industry. However, his plans “foundered under the heavy weight of revolutionary ideas” \n\nFurther model community ideas continued to arise but were each dismissed owing to the perception that they were unconvincing as business ventures. Enter Ebenezer Howard, creator of the Garden City Movement, who successfully founded Letchworth Garden City (1903) and proved that new towns could be economically viable. This was affirmed by Bernard Shaw, co-founder of the London School of Economics, who referred to his investments in the garden city movement as “entirely satisfactory, both economically and morally.” \n\nThe New Town Movement was derived from the Garden City Movement, founded by Ebenezer Howard in the late 1800s, as an alternative to the overcrowded, polluted, chaotic and miserable industrial cities that had appeared in Britain. Towards the end of World War One a group developed – the ‘New Townsmen’ – whose members were Howard, F.J. Osborn, C.B. Purdom and W.G. Taylor. They began advocating the development of 100 new cities to be built by the government.\n\nIf Howard is the ‘father’ of Garden Cities, then Frederick J. Osborn is certainly his ‘son’ – predecessor and champion of the New Towns. Osborn was born in 1885 and spent the majority of his life arguing the case for New Towns. Like Howard, he had quite a modest education, having never attended a university. But what he lacked in formal education, he made up for with ambition and wise career choices. In his early 30s, after meeting Howard through his job at the Howard Cottage Society, he took to the campaign for Garden Cities, though they were now referred to as New Towns. The initial campaigns for the establishment of New Towns failed. Although housing was built, it was often in the form of a ‘garden suburb’, or located on the edge of the existing cities – the antithesis of the Garden City idea. With an increasing lack of faith in the government to take up the flag for public housing and new towns, Howard suggested to Osborn that he was wasting his time lobbying government, and that he would be ‘as old as Methuselah’ waiting for action.\n\nIn 1909 a greater understanding of the ‘urban disease’ saw Britain’s first town planning legislation created. Although technically opposed to fringe development, the Housing, Town Planning, &c. Act 1909 did not prevent it. Instead, in light of recent success with the development of Hampstead Garden Suburb, the Act, realising that suburbs were easier to develop than towns, held the ethos that good suburbs were better than bad ones. Although planners of the day wanted new towns they were busy dealing with the demand for suburbs: “it is difficult for a technician to earn a living in an ivory tower” Moreover, new towns required government direction which was beyond the scope of municipal powers alone.\n\nTowards the end of the First World War the Garden City principles were reasserted by the ‘New Townsmen’ (Howard, Osborn, Purdon, and Tayler), who, referring to the success of Letchworth, proposed 100 government-supported new towns to address post-war rebuilding. However, the need for post-war housing resulted in new suburbs being prioritised over towns for the next two decades with some four million high standard houses built in between the wars albeit in the wrong places Conversely, some attempts were made at designing rebuilding efforts as satellite towns such as Manchester’s Wythenshawe and Liverpool’s Speke and Knowsley which also included provisions for industry. Nonetheless, these were still extensions of existing cities and not true New Towns. Furthermore, three-quarters of all the new housing was built privately meaning a default bottom-line approach was adopted into the inter-war development efforts.\n\nDuring the inter-war years Government committees studied the problem of urban concentration with the Committee on Unhealthy Areas, chaired by Neville Chamberlain (1919-1921), recommending the restriction of further industry in London and the relocation of some of the city’s existing industry to garden cities. Although nothing came of these studies they became the origin of Chamberlain’s urban decentralisation interests which led to his setting up of the Barlow Commission once Prime Minister. Further important advances included a 1935 Departmental Committee recommendation for the building of new towns in line with garden city principles and a 1936 Special Areas Report reiterating the idea that no new industry should be allowed in London which gained public and political interest \n\nIn 1938 Chamberlain, as the new Prime Minister, assigned a Royal Commission chaired by Sir Anderson Barlow into the urban concentration of population and industry. The resulting report raised the problem of large towns as a public issue for the first time and concluded that ‘planned decentralisation’ was favourable. However, owing to the outbreak of war in 1939 the Barlow Report, published in 1940, was initially ignored due to more immediate priorities although it eventually became a turning point for New Towns policy.\n\nThe damage brought on by the Second World War provoked significant public interest in what post-war Britain would be like which was encouraged by the Government who facilitated talk about a ‘Better Britain’ to boost morale. Furthermore, the Ministry of Works and Building was commissioned to draft ideas. Ironically, the Barlow Report was quickly turned to as a best practice document.\n\nIn 1942, following the Report’s recommendation, the Government chose to create a central planning authority in the form of the Ministry of Works and Planning. More importantly the Government also announced that the Report’s decentralisation and relocation of population and industry initiatives would be followed.\n\nPost-war rebuilding initiatives saw new plans drafted for London which for the first time addressed the issue of decentralisation. Firstly, the County of London Plan 1943 recognised that displacement of population and employment was necessary if the city was to be rebuilt at a desirable density. Moreover, the Greater London Plan of 1944 went further by suggesting that over one million people would need to be displaced into a mixture of satellite suburbs, existing rural towns, and new towns. \nIn 1945 the New Towns Committee was formed to consider the “establishment, development, organisation, and administration” of new towns. Within eight months the committee had completed a highly comprehensive study into these issues resulting in positive recommendations for the construction of new towns. Accordingly, the New Towns Act 1946 was passed which, coupled with the Town and Country Planning Act 1947, created a revolutionary “machinery for positive town construction”. These innovative Acts resulted in a total of 28 New Towns being constructed in Britain over the following half-century \n\nIt was in 1946 that the hard work of the ‘New Townsmen’ finally paid off with the passing of the New Towns Act 1946. Swayed by the need for post-war reconstruction, more housing, and a call to halt any further expansion of London’s girth, authorities saw that there was no alternative to the New Town solution. In total, 27 New Towns were built after 1946. These were: Stevenage, Crawley, Hemel Hempstead, Harlow, Hatfield, Basildon, Bracknell and Milton Keynes outside London; Newton Aycliffe, Peterlee and Washington in the North East; Skelmersdale and Runcorn in the North West; Corby, Telford and Redditch in the Midlands; Cwmbran and Newtown in Wales; and in Scotland, East Kilbride, Glenrothes, Cumbernauld, Livingston and Irvine. Towns that were expanded under the new towns act were Peterborough, Northampton, Warrington, Ipswich and Preston-Leyland-Chorley.\n\nThere were similar problems for New Towns advocates in other areas of the world. In Hong Kong, the new towns were developed as an initiative from the British colonial government. In other areas although they understood the concept and approved in large numbers, planners had trouble convincing their own governments or agencies of the merits of the proposal. In the United States, it was not until the 1960s that New Towns policies were put in place, although after World War Two grants had been extended for such things as slum clearance, improved and increased housing, and road construction, and in the 1950s, to ‘comprehensive renewal projects’. In the former USSR, more than 800 New Towns were founded after the 1917 Revolution, but their growth was not constrained by specific limits. For this reason it could be argued that these towns did not meet the criteria for New Towns since planned population and size limitations were an important part of the New Town idea. Other European countries such as France, Germany, Italy and Sweden also had some successes with new towns, especially as part of post-war reconstruction efforts.\n\nNotable new towns in the United States include Reston, VA; Columbia, MD; Jonathan, MN; Peachtree City, GA; the \"new town in town\" of Riverside Plaza in Minneapolis.\n\n\n"}
{"id": "8908943", "url": "https://en.wikipedia.org/wiki?curid=8908943", "title": "Nordic Institute for Theoretical Physics", "text": "Nordic Institute for Theoretical Physics\n\nThe Nordic Institute for Theoretical Physics, or Nordita (), is an international organisation for research in theoretical physics. It was established in 1957 by Niels Bohr and the Swedish physicist Torsten Gustafson. Nordita was originally located at the Niels Bohr Institute in Copenhagen (Denmark), but moved to the AlbaNova University Centre in Stockholm (Sweden) on 1 January 2007. The main research areas at Nordita are astrophysics, hard and soft condensed matter physics, and high-energy physics.\n\nSince Nordita's establishment in 1957 the original focus on research in atomic and nuclear physics has been broadened. \nResearch carried out by Nordita's academic staff presently includes astrophysics, biological physics, hard condensed matter physics and materials physics, soft condensed matter physics, cosmology, statistical physics and complex systems, high-energy physics, and gravitational physics and cosmology. The in-house research forms the backbone of Nordita activities and complements the more service oriented functions. By mission, Nordita has the task of facilitating interactions between physicists in the Nordic countries as well as with the international community; therefore the comparably small institute has a large number of visitors, conferences and scientific programs that last several weeks. \n\nNotable former or present researchers at Nordita include Alexander V. Balatsky, Holger Bech Nielsen, Axel Brandenburg, Gerald E. Brown, Paolo Di Vecchia, James Hamilton, John Hertz, Alan Luther, Ben Roy Mottelson, Christopher J. Pethick, Leon Rosenfeld, Kim Sneppen, John Wettlaufer, and Konstantin Zarembo.\n\n Nordita is governed by a board consisting of one representative and one alternate member from each Nordic country, headed by a chair person. The board appoints a number of research committees which evaluate proposals and advice the board on scientific and educational matters. \n\nThe Nordita board nominates a director who is appointed by the president of KTH Royal Institute of Technology and the vice-chancellor of Stockholm University. The director, currently Thors Hans Hansson, is responsible for the day-to-day administration of the institute and provides scientific leadership.\n\nNordita is funded jointly by the Nordic countries via the Nordic Council of Ministers, the Swedish Research Council, and the host universities KTH Royal Institute of Technology, Stockholm University and Uppsala University.\n\n"}
{"id": "649348", "url": "https://en.wikipedia.org/wiki?curid=649348", "title": "Operation Upshot–Knothole", "text": "Operation Upshot–Knothole\n\nOperation Upshot–Knothole was a series of eleven nuclear test shots conducted in 1953 at the Nevada Test Site. It followed \"Operation Ivy\" and preceded \"Operation Castle\".\n\nOver 21,000 soldiers took part in the ground exercise Desert Rock V in conjunction with the \"Grable\" shot. \"Grable\" was a 280mm shell fired from the \"Atomic Cannon\" and was viewed by a number of high-ranking military officials.\nThe test series was notable as containing the first time an atomic artillery shell was fired (shot \"Grable\"), the first two shots (both fizzles) by University of California Radiation Laboratory—Livermore (now Lawrence Livermore National Laboratory), and for testing out some of the thermonuclear components that would be used for the massive thermonuclear series of Operation Castle. One primary device (RACER) was tested in thermonuclear system mockup assemblies of TX-14, TX-16, and TX-17/TX-24, to examine and evaluate the behaviour of radiation cases and the compression of the secondary geometries by the primary's x-rays prior to full-scale testing during Castle. Following RACER's dodgy performance, the COBRA primary was used in the emergency capability ALARM CLOCK, JUGHEAD, RUNT I, RUNT II thermonuclear devices, as well as in the SHRIMP device. RACER IV (as redesigned and proof-tested in the Simon test) was employed as primary for the ZOMBIE, RAMROD and MORGENSTERN devices.\n\n\n\n\n"}
{"id": "2251428", "url": "https://en.wikipedia.org/wiki?curid=2251428", "title": "Philogène Auguste Joseph Duponchel", "text": "Philogène Auguste Joseph Duponchel\n\nPhilogène Auguste Joseph Duponchel (1774 – 10 January 1846) was a French soldier and entomologist.\n\nPhilogène Auguste Joseph Duponchel was born in 1774 in Valenciennes, Nord, and died on 10 January 1846 in Paris. After studies in Douai, he joined the French Army when he was sixteen years old and took part in the campaigns of 1795 and 1796. Retiring from the army, he worked afterwards as a government administrator stationed in Paris. He was forced to retire again in 1816, aged 42 years, because of his opinions in favour of Napoleon Bonaparte. He then devoted himself to the study of insects.\n\nAfter twelve years of effort, Duponchel finished in 1838 \"L’Histoire naturelle des lépidoptères de France\", co-authored with Jean Baptiste Godart. This work consists of seventeen volumes (including twelve signed by Duponchel), 7600 coloured plates and 500 \"boards\" (which appear under the title \"Iconographie des Chenilles\" or \"Iconography of the Caterpillars\"). The volumes were published between 1832 and 1842, and within its pages the authors describe more than four thousand species of butterflies and moths.\n\nDuponchel was one of the founders of the Société Entomologique de France and was its first treasurer. He was a very close friend of Pierre François Marie Auguste Dejean, Auguste Duméril and Pierre André Latreille. He married Marie-Joseph-Désirée Ravet (d. July 1847) and had two sons. His son Charles-Edmond Duponchel (b. 7 April 1804), studied architecture and was an accountant first class at the \"Ministère de la Guerre\", and his son Auguste (d. October 1846) was chief medical officer of \"l'Ecole polytechnique\". Philogène Auguste is buried in the cemetery of Montparnasse.\n\n-Translated from French Wikipedia\n\n"}
{"id": "36707999", "url": "https://en.wikipedia.org/wiki?curid=36707999", "title": "Print capitalism", "text": "Print capitalism\n\nPrint capitalism is a theory underlying the concept of a nation, as a group that forms an imagined community, that emerges with a common language and discourse that is generated from the use of the printing press, proliferated by a capitalist marketplace. Capitalist entrepreneurs printed their books and media in the vernacular (instead of exclusive script languages, such as Latin) in order to maximize circulation. As a result, readers speaking various local dialects became able to understand each other, and a common discourse emerged. Anderson argued that the first European nation-states were thus formed around their \"national print-languages.\"\n\nThe term was coined by Benedict Anderson, and explained in depth in his book \"\" in 1983.\n\nThe printing press is widely credited for modern nationalism and the birth of the nation-state as the primary actors in political legitimacy. Soon after the invention of the Gutenberg-style printing press in 1454, literature such as the Bible was printed in vernaculars. The publication of the 95 Theses in 1517 sparked the reformation, under which Europe went through 200 years of warfare that led to the gradual establishment of the nation-state as the powers that were dominant, over the previous dominance of the Roman Catholic Church. Print-Capitalism continues to influence the development of nationalism through the spread of the printing press.\n"}
{"id": "48010784", "url": "https://en.wikipedia.org/wiki?curid=48010784", "title": "Pseudolistening", "text": "Pseudolistening\n\nPseudo-listening is a type of non-listening that consists of appearing attentive in conversation while actually ignoring or only partially listening to the other speaker. The intent of pseudo-listening is not to listen, but to cater to some other personal need of the listener. The word pseudo-listening is a compound word composed of the individual words pseudo (a Greek root meaning \"not real or genuine\"), and listening. An example of pseudo-listening is trying to multitask by talking on the phone while watching television or completing work. Pseudo-listening is the most ineffective way to communicate because after the conversation one will not have retained much of the information that was said.\n\n\nIndividuals who are pseudo-listening may include minimal encouragers to compensate for their non-listening, such as nodding their heads, looking at the speaker, smiling at the appropriate times, and displaying other aspects of paying attention, so it may be difficult at times to distinguish between active listening and pseudo-listening. Similarly, broad answers or responses that are not relevant to the topic at hand also give away a pseudo-listener. These responses are known as tangential responses and often run alongside the topic being discussed, but ultimately have nothing to do with the main topic of the discussion.\n\nProper listening is needed to internalize and comprehend material; without this developed skill, there is potential for either pseudo-listening or a total lack of listening. A person who is truly listening may display certain body language such as maintaining eye contact or positioning their body towards the speaker. Real listening is done with the intent to understand a person's perspective, enjoy a person's company, learn new information, or provide assistance or comfort, among other things.\n\nPseudo-listening is most common in face-to-face communication, but it can also be expressed through phone calls, text messages and e-mails. Effective communication is dependent upon both the receiver and the sender to be fully aware of what is being said and heard. When one person projects their thoughts, and the receiver of the message is only pretending to listen, poor communication and possibly misunderstanding often occur. Babies spend their first few years listening to people around them in order to learn language (Childs, Acott-Smith, & Curtis, 1999). According to Edwing Llangari people spend around half their lives listening to others. An article from University of Colorado states that poor listening skills make good communication pretty much impossible to achieve. It says that, \"no matter how much care one person or group takes to communicate their concerns, values, interests, or needs in a fair, clear, unthreatening way, if the listener is not willing to receive that information in that way, the communication will fail.\"\n\nThere are many negative effects on romantic relationships caused by pseudo-listening. These effects can range from making romantic partners feel upset, inhibiting the partner's ability to effectively solve problems, encouraging an unhealthy dependence on the listener, raising the partner's level of stress, making the relationship, as a whole, less stable, causing the romantic partner to be less satisfied with the relationship, and even causing negative effects on the romantic partner's health. It’s important to recognize that there are two parts to any conversation, the person doing the talking and the person who is trying to actively listen.\n\nAmong families, teenagers engage in pseudo-listening the most. Teenagers may engage in pseudo-listening because they feel they have better things to do.They may also tune into distractions and act like they are listening. In doing so, it is evident that they listen only when they want to listen. Studies show that as people get older, pseudo-listening skills increase. An experiment was done on first-grade and second grade children to see if they could repeat what the teacher had been saying. 90% of first graders and 80% of the second-graders could do so; but when the experiment was repeated with teenagers, only 44% of junior high students and 28% of senior high students could repeat their teachers’ remarks. It is important for parents to have good listening skills, because poor listening from parents can cause the child to be closed off.\n\nRecent studies reveal that students rarely encounter curriculum focused on learning and developing the skill of listening. It is possible for a student to go through every level of education without ever having a class teaching them how to listen effectively in everyday life. Often students fall victim to pseudo-listening because of all the distractions they have at their finger tips, such as phones, computers, social media, iPods, etc. Students will often pretend to be listening so that the professor of the class does not verbally chastise, or to avoid being perceived as rude. Effective listening from a teacher is a way of showing concern for their subordinates, which fosters cohesive bonds, commitment, and trust: this can help a class to pay attention and learn what is being taught.\n\nLeadership often involves forming a commitment to the people one is leading, and a large part of this is related to active listening. Pseudo-listening undermines this commitment process and leads to ineffective leadership. Many people think they are listening to others all the time, and while they certainly may be hearing what these people are saying, they are engaging in pseudo-listening. Over time, pseudo-listening can not only weaken perceptions of one as a leader, but can contribute to problems active listening could otherwise fix and lead to barriers against success in an organization. On the job, we may need to appear interested in what others say because of their position.\n\nThings such as an accent, height, weight, scars, and even something as small as body posture can all contribute to an audience falling victim to pseudo-listening. Being self-conscious about some of these personal flaws as a speaker may inhibit the listeners experience. This lack of self-confidence could lead to other distractions made by the speaker. While these physical flaws may lead to disinterest in the reader it could also contribute to nervous actions that may also distract the reader. A few examples of these nervous actions may include but are not limited to clenching your hands, adjusting your hair or clothing, and pacing back and forth while speaking. These actions may lead to a disinterest in the context of a speech and an elevated interest in the person speaking or the environment surrounding them.\n\nEnvironmental factors such as lighting, temperature, and furniture can affect the attention of the audience in relation to what the speaker has to say. Lighting that is too dark can make an audience tired and consequently disinterested in what the speaker has to say. Temperature and seating arrangements can distract an audience by causing them discomfort, shifting their focus to their own annoyance. Being too far from a speaker can also cause the audience to lose focus because of a sort of \"Hawthorne effect. Physiological noise can also lead to distractions that take an audiences attention from the speaker. This noise may range from ailments and illness, all the way to emotions like arousal. Both sides of the spectrum can affect the audiences attention, for example, a speaker giving a speech while under the illness of a cold will have a stuffy nose or a recurring cough, which can take the audience's focus from the speaker to his cold. The audience can also be influenced by their own lives by shifting their focus on what they will eat for lunch, losing or finding a job, personal relationships, etc. These can all overshadow the messages and ideas that the speaker is trying to deliver.\n\nBecause facial hair is a striking characteristic of one's face, people often notice and focus on an individual's facial hair. Because of this, listeners may fall victim to pseudolistening. Instead of making eye contact, listeners might focus on the speaker's mustache or beard instead of actively listening. Facial hair is distracting to both listeners and observers.This concept can also be applied to situations in which a person has something stuck in his teeth, has visible mucus in his nose, or has conspicuous piercings and tattoos on his face.\n\nEffective listening is crucial to proper communication in everyday life. However, active listening is not a natural process. In fact, even people who consider themselves good listeners often only listen with 25% efficiency. Active listening involves attentive body language, restating comments and emotions of the speaker, and repeating their ideas back in order to check comprehension accuracy.\n\nIt is possible to improve your listening skills. Begin by observing your conversations, and pay special notice to the times you begin to get distracted or ignore the other person and see how it affects your relationships. Once you understand your patterns in conversations, it's easier to change the negative aspects of your conversations. There are three main guidelines everyone should follow if they want to become effective listeners. The most important guideline is being mindful, that is, being disciplined and committed when trying to listen. The next guideline is adapting to listening appropriately because it is beneficial to be skilled in many listening behaviors and to know when each is important. The final guideline to becoming an effective listener is listening actively. To listen actively, keep eye contact with the speaker, nod when a message is understood, etc. Paraphrasing what was said in order to communicate to the speaker that you understood their message correctly will also improve listening skills. A good tag phrase for paraphrasing is \"What I'm hearing you say is...\", followed by the question, \"Is that right?\" This combination of repeating back and asking for confirmation of accuracy frees the speaker to correct any previous ineffective communication.\n\nOne way to detect pseudolistening is eye fixation: in relaxed conversations, the eye tends to wander or look at other parts of your face. We also tend to smile at one another to reinforce that we agree with them and are listening. The pseudo-listener will do the same but will often smile for too long. During a conversation between two people, their bodies will face each other. A false listener, however, will not have engaged body language; instead, whether their feet face away or they sit next to rather than across from the other person, it is as if their body is trying to escape.\n\n\nAuditory processing disorder is when a person \"is perfectly aware of sounds\" yet their brain abnormally deciphers the sounds. This could easily be confused with pseudo-listening because it effects a listeners' reading comprehension. The two differ because auditory processing disorder uncontrollable and unintentional, while pseudolistening is typically done purposely.\n"}
{"id": "15448809", "url": "https://en.wikipedia.org/wiki?curid=15448809", "title": "Rain-out model", "text": "Rain-out model\n\nThe rain-out model is a model of planetary science that describes the first stage of planetary differentiation and core formation. According to this model, a planetary body is assumed to be composed primarily of silicate minerals and NiFe (i.e. a mixture of nickel and iron). If temperatures within this body reach about 1500 K, the minerals and the metals will melt. This will produce an emulsion in which globules of liquid NiFe are dispersed in a magma of liquid silicates, the two being immiscible. Because the NiFe globules are denser than the silicates, they will sink under the influence of gravity to the centre of the planetary body—in effect, the globules of metal will \"rain out\" from the emulsion to the centre, forming a core.\n\nAccording to the rain-out model, core formation was a relatively rapid process, taking a few dozen millennia to reach completion. This occurred at the end of a lengthy process in which the planets were assembled from colliding planetary embryos. Only the collisions of such large embryos could generate enough heat to melt entire bodies. Furthermore, it was only after all of the iron and nickel delivered by impacting bodies had arrived that core formation could proceed to completion.\n\nHowever, this process of core formation was preceded by a long period of partial differentiation, in which some of the nickel and iron within the planetary embryos had begun to separate.\n\nThe rain-out model can be invoked to explain core formation in all the terrestrial planets, given that these consist primarily of silicates, nickel and iron. It can also be adapted to account for core formation in smaller bodies composed of ices and silicates. In such a case, it would be the denser silicates which would rain out to form a rocky core, while the volatile components would form an icy mantle.\n\n"}
{"id": "50891749", "url": "https://en.wikipedia.org/wiki?curid=50891749", "title": "Research proposal", "text": "Research proposal\n\nA research proposal is a document proposing a research project, generally in the sciences or academia, and generally constitutes a request for sponsorship of that research. Proposals are evaluated on the cost and potential impact of the proposed research, and on the soundness of the proposed plan for carrying it out. Research proposals generally address several key points:\n\nResearch proposals may be \"solicited\", meaning that they are submitted in response to a request with specified requirements, such as a request for proposal, or they may be \"unsolicited\", meaning they are submitted without prior request. Other types of proposals include \"preproposals\", where a letter of intent or brief abstract is submitted for review prior to submission of a full proposal; continuation proposals, which re-iterate an original proposal and its funding requirements in order to ensure continued funding; and renewal proposals, which seek continued sponsorship of a project which would otherwise be terminated.\n\nAcademic research proposals are generally written as part of the initial requirements of writing a thesis, research paper, or dissertation. They generally follow the same format as a research paper, with an introduction, a literature review, a discussion of research methodology and goals, and a conclusion. This basic structure may vary between projects and between fields, each of which may have its own requirements.\n"}
{"id": "4939628", "url": "https://en.wikipedia.org/wiki?curid=4939628", "title": "Segregate (taxonomy)", "text": "Segregate (taxonomy)\n\nIn taxonomy, a segregate, or a segregate taxon is created when a taxon is split off from another taxon. This other taxon will be better known, usually bigger, and will continue to exist, even after the segregate taxon has been split off. A segregate will be either new or ephemeral: there is a tendency for taxonomists to disagree on segregates, and later workers often reunite a segregate with the 'mother' taxon. \n\nIf a segregate is generally accepted as a 'good' taxon it ceases to be a segregate. Thus, this is a way of indicating change in the taxonomic status. It should not be confused with, for example, the subdivision of a genus into subgenera.\n\n"}
{"id": "1652911", "url": "https://en.wikipedia.org/wiki?curid=1652911", "title": "Spectral efficiency", "text": "Spectral efficiency\n\nSpectral efficiency, spectrum efficiency or bandwidth efficiency refers to the information rate that can be transmitted over a given bandwidth in a specific communication system. It is a measure of how efficiently a limited frequency spectrum is utilized by the physical layer protocol, and sometimes by the media access control (the channel access protocol).\n\nThe link spectral efficiency of a digital communication system is measured in \"bit/s/Hz\", or, less frequently but unambiguously, in \"(bit/s)/Hz\". It is the net bitrate (useful information rate excluding error-correcting codes) or maximum throughput divided by the bandwidth in hertz of a communication channel or a data link. Alternatively, the spectral efficiency may be measured in \"bit/symbol\", which is equivalent to \"bits per channel use\" (\"bpcu\"), implying that the net bit rate is divided by the symbol rate (modulation rate) or line code pulse rate.\n\nLink spectral efficiency is typically used to analyse the efficiency of a digital modulation method or line code, sometimes in combination with a forward error correction (FEC) code and other physical layer overhead. In the latter case, a \"bit\" refers to a user data bit; FEC overhead is always excluded.\n\nThe modulation efficiency in bit/s is the gross bitrate (including any error-correcting code) divided by the bandwidth. \n\nAn upper bound for the attainable modulation efficiency is given by the Nyquist rate or Hartley's law as follows: For a signaling alphabet with \"M\" alternative symbols, each symbol represents \"N\" = log \"M\" bits. \"N\" is the modulation efficiency measured in \"bit/symbol\" or \"bpcu\". In the case of baseband transmission (line coding or pulse-amplitude modulation) with a baseband bandwidth (or upper cut-off frequency) \"B\", the symbol rate can not exceed 2\"B\" symbols/s in view to avoid intersymbol interference. Thus, the spectral efficiency can not exceed 2\"N\" (bit/s)/Hz in the baseband transmission case. In the passband transmission case, a signal with passband bandwidth \"W\" can be converted to an equivalent baseband signal (using undersampling or a superheterodyne receiver), with upper cut-off frequency \"W\"/2. If double-sideband modulation schemes such as QAM, ASK, PSK or OFDM are used, this results in a maximum symbol rate of \"W\" symbols/s, and in that the modulation efficiency can not exceed \"N\" (bit/s)/Hz. If digital single-sideband modulation is used, the passband signal with bandwidth \"W\" corresponds to a baseband message signal with baseband bandwidth \"W\", resulting in a maximum symbol rate of 2\"W\" and an attainable modulation efficiency of 2\"N\" (bit/s)/Hz. \n\nIf a forward error correction code is used, the spectral efficiency is reduced from the uncoded modulation efficiency figure.\n\nAn upper bound for the spectral efficiency possible without bit errors in a channel with a certain SNR, if ideal error coding and modulation is assumed, is given by the Shannon-Hartley theorem.\n\nNote that the goodput (the amount of application layer useful information) is normally lower than the maximum throughput used in the above calculations, because of packet retransmissions, higher protocol layer overhead, flow control, congestion avoidance, etc. On the other hand, a data compression scheme, such as the V.44 or V.42bis compression used in telephone modems, may however give higher goodput if the transferred data is not already efficiently compressed.\n\nThe link spectral efficiency of a wireless telephony link may also be expressed as the maximum number of simultaneous calls over 1 MHz frequency spectrum in erlangs per megahertz, or \"E/MHz\". This measure is also affected by the source coding (data compression) scheme. It may be applied to analog as well as digital transmission.\n\nIn wireless networks, the \"link spectral efficiency\" can be somewhat misleading, as larger values are not necessarily more efficient in their overall use of radio spectrum. In a wireless network, high link spectral efficiency may result in high sensitivity to co-channel interference (crosstalk), which affects the capacity. For example, in a cellular telephone network with frequency reuse, spectrum spreading and forward error correction reduce the spectral efficiency in (bit/s)/Hz but substantially lower the required signal-to-noise ratio in comparison to non-spread spectrum techniques. This can allow for much denser geographical frequency reuse that compensates for the lower link spectral efficiency, resulting in approximately the same capacity (the same number of simultaneous phone calls) over the same bandwidth, using the same number of base station transmitters. As discussed below, a more relevant measure for wireless networks would be \"system spectral efficiency\" in bit/s/Hz per unit area. However, in closed communication links such as telephone lines and cable TV networks, and in noise-limited wireless communication system where co-channel interference is not a factor, the largest link spectral efficiency that can be supported by the available SNR is generally used.\n\nIn digital wireless networks, the \"system spectral efficiency\" or area spectral efficiency is typically measured in \"(bit/s)/Hz per unit area\", in \"(bit/s)/Hz per cell\", or in \"(bit/s)/Hz per site\". It is a measure of the quantity of users or services that can be simultaneously supported by a limited radio frequency bandwidth in a defined geographic area. It may for example be defined as the maximum aggregated throughput or goodput, i.e. summed over all users in the system, divided by the channel bandwidth and by the covered area or number of base station sites. This measure is affected not only by the single user transmission technique, but also by multiple access schemes and radio resource management techniques utilized. It can be substantially improved by dynamic radio resource management. If it is defined as a measure of the maximum goodput, retransmissions due to co-channel interference and collisions are excluded. Higher-layer protocol overhead (above the media access control sublayer) is normally neglected.\n\nThe system spectral efficiency of a cellular network may also be expressed as the maximum number of simultaneous phone calls per area unit over 1 MHz frequency spectrum in \"E/MHz per cell\", \"E/MHz per sector\", \"E/MHz per site\", or \"(E/MHz)/m\". This measure is also affected by the source coding (data compression) scheme. It may be used in analog cellular networks as well.\n\nLow link spectral efficiency in (bit/s)/Hz does not necessarily mean that an encoding scheme is inefficient from a system spectral efficiency point of view. As an example, consider Code Division Multiplexed Access (CDMA) spread spectrum, which is not a particularly spectral efficient encoding scheme when considering a single channel or single user. However, the fact that one can \"layer\" multiple channels on the same frequency band means that the system spectrum utilization for a multi-channel CDMA system can be very good.\n\nThe spectral efficiency can be improved by radio resource management techniques such as efficient fixed or dynamic channel allocation, power control, link adaptation and diversity schemes.\n\nA combined fairness measure and system spectral efficiency measure is the fairly shared spectral efficiency.\n\nExamples of predicted numerical spectral efficiency values of some common communication systems can be found in the table below. These results will not be achieved in all systems. Those further from the transmitter will not get this performance.\n"}
{"id": "2500723", "url": "https://en.wikipedia.org/wiki?curid=2500723", "title": "Stepan Krasheninnikov", "text": "Stepan Krasheninnikov\n\nStepan Petrovich Krasheninnikov () ( – ) was a Russian explorer of Siberia, naturalist and geographer who gave the first full description of Kamchatka in the early 18th century. He was elected to the Russian Academy of Sciences in 1745. The Krasheninnikov Volcano on Kamchatka is named in his honour.\n\nKrasheninnikov was educated in the Slavic Greek Latin Academy of Moscow (1724–32), where Lomonosov was his class-mate. As part of Vitus Bering’s extensive preparations for the Second Kamchatka Expedition, 12 students from the academy were selected as potential student interns or assistants for the professors – Krasheninnikov being one of them. Thus, he furthered his education in St Petersburg before embarking upon the Second Kamchatka Expedition (1731-42).\n\nKrasheninnikov was to study plants, animals and minerals, but in addition he developed a strong interest in Siberian history and geography. During the early part of the expedition, he accompanied professor Gmelin on the travel through the Urals and western Siberia to Yeniseysk. He made numerous observations of natural history, ethnology and linguistics, e.g. records of Evenki (tungus) and Buryat vocabulary. From Bering’s headquarters at Yakutsk, the expedition professors Gmelin and Gerhard Friedrich Müller sent Krasheninnikov ahead to Okhotsk and Kamchatka to build house and make preliminary observations. Thus, he became the member of the expedition with the most extensive knowledge of the peninsula. He published his observations in 1755 (\"Описание земли Камчатки\"; English translation by James Grieve (1764) as \"History of Kamtschatka\"). However, he drew extensively on the manuscripts of the deceased Georg Wilhelm Steller. Apart from detailed accounts of the plants and animals of the region, there also were reports on the language and culture of the indigenous Itelmen and Koryak peoples, who he reportedly got along extremely well with.\n\nKrasheninnikov spent ten years on the Second Kamchatka Expedition. On his return to St Petersburg, he wrote and defended his doctoral thesis on ichthyology in 1745. He was appointed adjunct at the Academy of Sciences, and later head of the Academy’s Botanic Garden and professor of natural history at the university. He was one of only 26 Russians to become Academy members in the 18th century. In 1752, Krasheninnikov went on his last expedition to the tracts of Lake Ladoga and Novgorod to investigate the flora. He died before being able to publish his observations, which instead were published by David de Gorter.\n\nMore than 20 species have been named to his honour, e.g.\n\n"}
{"id": "46931436", "url": "https://en.wikipedia.org/wiki?curid=46931436", "title": "Steven Wormald", "text": "Steven Wormald\n\nSteven Wormald, born 1946, was prominent as an Antarctic explorer during the 1970s.\n\nWormald, who was described in 1973 as a resident of Calgary, Canada, was the British Antarctic Survey (BAS) meteorological observer at Adelaide Station, on Adelaide Island in 1969–70 and the BAS general assistant at Stonington Island in 1970–71. Wormald was later involved in survey work in the Canadian Arctic, before returning to BAS, as base commander at Stonington Island in 1973. He served as field operations manager for BAS at Rothera Research Station, in 1974-1977.\n\nIn 1976, Wormald was awarded the Fuchs Medal, which is awarded by the BAS for \"outstanding devotion to the British Antarctic Survey's interests, beyond the call of normal duty, by men or women who are or were members of the Survey, or closely connected with its work.\" In 1978, Wormald received the Polar Medal, which is awarded by the British government \"for outstanding services as members of the British Antarctic Survey\".\n\nThe Wormald Ice Piedmont, a peak on Adelaide Island, was subsequently named after him. \n"}
