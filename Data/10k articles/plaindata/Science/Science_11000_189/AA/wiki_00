{"id": "1588841", "url": "https://en.wikipedia.org/wiki?curid=1588841", "title": "Alan William James Cousins", "text": "Alan William James Cousins\n\nAlan William James Cousins (8 August 1903 – 11 May 2001) was a South African astronomer.\n\nHe was the eldest of four children, and his father Clarence Wilfred Cousins was a senior civil servant who served for a time as Secretary of Labour. His grandfather on his mother's side was Sir James Murray, first editor of the \"Oxford English Dictionary\", and Cousins attended Murray's funeral at the age of 11.\n\nHe singlemindedly devoted himself to photometry and its improvement. The version of the UBVRI that he devised became the standard.\n\nHe became a Fellow of the Royal Astronomical Society in 1941. He won the Jackson-Gwilt Medal of the Royal Astronomical Society in 1971.\n\nHe had two children.\n\n"}
{"id": "795886", "url": "https://en.wikipedia.org/wiki?curid=795886", "title": "Aleksei Chirikov", "text": "Aleksei Chirikov\n\nAleksei Ilyich Chirikov () (1703 – November 1748) was a Russian navigator and captain who along with Bering was the first Russian to reach North-West coast of North America. He discovered and charted some of the Aleutian Islands while he was deputy to Vitus Bering during the Great Northern Expedition.\n\nIn 1721, Chirikov graduated from the Naval Academy. In 1725–1730 and 1733–1743, he was Vitus Bering's deputy during the First and the Second Kamchatka expeditions.\n\nIn May 1741 Chirikov in the \"St Paul\" and Vitus Bering in the \"St Peter\" left Petropavlovsk-Kamchatsky and headed east. Some time after 20 June they were separated by a storm and never saw each other again. On 15 July 1741 Chirikov saw land at Baker Island off Prince of Wales Island at the south end of the Alaska Panhandle. This is about 450 miles southeast of Bering's landfall near Mount St. Elias at the north end of the panhandle. Unable to find a harbor he sailed north along Baranov Island past the later Russian base at Sitka. He sent out a longboat to find an anchorage. When it did not return after a week he sent out his second longboat which also failed to return. Now without any small boats Chirikov had no way of searching for the two longboats or landing on the coast to explore or replenish his supply of fresh water. After waiting as long as possible, he abandoned the longboats to their fate and on 27 July sailed west. He sighted the Kenai Peninsula, Kodiak Island and Adak Island near the western end of the Aleutians. With water critically low he reached Petropavlovsk on 12 October 1741.\nIn 1742, Chirikov was in charge of a search party for Bering's ship \"St. Peter\". During this trip, he located Attu Island. Chirikov took part in creating the final map of the Russian discoveries in the Pacific Ocean (1746). Chirikov's name is given to Capes of the Kyūshū Island, Attu Island, Anadyr Bay, Tauyskaya Bay, an underwater mountain in the Pacific Ocean, Chirikof Island and Cape Chirikof at the westernmost point of Baker Island.\n\n\nDerek Hayes, 'Historical Atlas of the North Pacific Ocean', 2001\n"}
{"id": "23574478", "url": "https://en.wikipedia.org/wiki?curid=23574478", "title": "Alexis Perrey", "text": "Alexis Perrey\n\nAlexis Perrey (1807–1882) was a historical French seismologist and compiler of earthquake catalogs. He is considered a pioneer in this area, having published a paper on earthquakes in Algeria as early as 1848, in the journal \"Mémoires de l'Académie des Sciences, Arts et Belles-Lettres de Dijon\". He continued to post annual observations on Algerian earthquakes until 1871.\n\nHe suspected a correlation between the moon and seismic activity on earth, and developed his theory with the use of statistics. He found that earth tremors occurred most frequently during full and new moons, when the earth is between the sun and moon, when the moon is between the earth and sun, and when the moon is closest in its orbit to the earth. He also found indications in some cases that the moon had crossed the meridian of affected locales at the time of the earthquake.\n"}
{"id": "624406", "url": "https://en.wikipedia.org/wiki?curid=624406", "title": "Amagat's law", "text": "Amagat's law\n\nAmagat's law or the Law of Partial Volumes describes the behaviour and properties of mixtures of ideal (as well as some cases of non-ideal) gases. Of use in chemistry and thermodynamics.\n\nAmagat's law states that the extensive volume \"V = N·v\" of a gas mixture is equal to the sum of volumes \"V\" of the \"K\" component gases, if the temperature \"T\" and the pressure \"p\" remain the same:\n\nThis is the experimental expression of volume as an extensive quantity. It is named after Emile Amagat.\n\nAccording to Amagat’s law of partial volume, the total volume of a non-reacting mixture of gases at constant temperature and pressure should be equal to the sum of the individual partial volumes of the constituent gases. So if formula_2 are considered to be the partial volumes of components in the gaseous mixture, then the total volume formula_3 would be represented as:\n\nBoth Amagat's and Dalton's Laws predict the properties of gas mixtures. Their predictions are the same for ideal gases. However, for real (non-ideal) gases, the results differ. Dalton's Law of Partial Pressures assumes that the gases in the mixture are non-interacting (with each other) and each gas independently applies its own pressure, the sum of which is the total pressure. Amagat's Law assumes that the volumes of the component gases (again at the same temperature and pressure) are additive; the interactions of the different gases are the same as the average interactions of the components.\n\nThe interactions can be interpreted in terms of a second virial coefficient, B(T), for the mixture. For two components, the second virial coefficient for the mixture can be expressed as:\n\nwhere the subscripts refer to components 1 and 2, the X's are the mole fractions, and the B's are the second virial coefficients. The cross term, B, of the mixture is given by:\n\nand\n\nWhen the volumes of each component gas (same temperature and pressure) are very similar, then Amagat's law becomes mathematically equivalent to Vegard's law for solid mixtures.\n\nWhen the Amagat Law is valid and the gas mixture is made of ideal gases (Ideal gas law):\n\nwhere:\n\nIt follows that the mole fraction and volume fraction are the same. This is true also for other equation of state.\n"}
{"id": "30427977", "url": "https://en.wikipedia.org/wiki?curid=30427977", "title": "Ashworth Glacier", "text": "Ashworth Glacier\n\nAshworth Glacier () is an Antarctic glacier with sharply delineated sides, flowing west from Supporters Range into Mill Glacier, north of Mount Iveagh. It was named by the Advisory Committee on Antarctic Names in 2007, after Allan C. Ashworth, Professor of Paleontology and Stratigraphy at North Dakota State University. He discovered the only yet known fly and beetle fossils in Antarctica in the nearby Dominion Range.\n\n"}
{"id": "58850553", "url": "https://en.wikipedia.org/wiki?curid=58850553", "title": "Christine Jacobs-Wagner", "text": "Christine Jacobs-Wagner\n\nChristine Jacobs-Wagner is a microbial molecular biologist. She is the William H. Fleming, MD Professor of Molecular, Cellular, and Developmental Biology at Yale University and Professor of Microbial Pathogenesis, HHMI investigator, and Director of the Microbial Sciences Institute at Yale Medical School. Jacobs-Wagner's research has shown that bacterial cells have a great deal of substructure including analogs of microfilaments, and that proteins are directed by regulatory processes to locate to specific places within the bacterial cell. She was elected to the National Academy of Sciences in 2015 and has received a number of scientific awards.\n\nChristine Jacobs-Wagner grew up in Belgium in a town near Liege. She thought of becoming a cyclist or a badminton Olympian, but was undecided about a career through high school. Christine Jacobs-Wagner received her BS degree in biochemistry from University of Liege. She also received her MS in 1991 and the PhD in 1996 from University of Liege in Belgium in the field of Biochemistry. She then went to work with Lucy Shapiro at Stanford Medical School on a fellowship from the European Molecular Biology Organization. where she studied \"Caulobacter\", a bacterium with a flagellum on one end and a stalk on the other end, beginning her fascination with how bacterial cells can become asymmetrical. From 2004 to 2013, she did research in Newcastle upon Tyne in the United Kingdom. \n\nAs of 2018, Jacobs-Wagner holds an endowed chair in Yale Medical School and is director of their Microbial Institute.\n\nChristine Jacobs-Wagner's major breakthrough has been the discovery that the tiny cells of bacteria such as \"Caulobacter,\" \"Escherichia coli\", and \"Borrelia\" are not simply bags of biochemicals but instead program the locations of their protein components via their regulatory systems. She also discovered the protein crescentin which forms bacterial intermediate filaments, structures once thought to occur only in eukaryotic cells. The current focus of her laboratory's work is to discover regulation of the times and places for critical components of the DNA replication and cell division processes so that proliferation control can be understood.\n\n\n"}
{"id": "39743020", "url": "https://en.wikipedia.org/wiki?curid=39743020", "title": "Cleofé Calderón", "text": "Cleofé Calderón\n\nCleofé Elsa Calderón (October 26, 1929 – March 19, 2007) was an Argentine botanist.\n\nIn 1976, Calderón rediscovered \"Anomochloa\" in Bahia, Brazil, with the help of assistant Talmon Soares dos Santos. By identifying this tropical forest grass, Calderón provided specimens for detailed morphological and anatomical study that confirmed it as a grass.\n\nIn October 1971, Calderón performed field operations and collected living materials of a new genus of grasses; the genus was named after her: \"Calderonella\".\n\n"}
{"id": "492271", "url": "https://en.wikipedia.org/wiki?curid=492271", "title": "Clinical psychology", "text": "Clinical psychology\n\nClinical psychology is an integration of science, theory and clinical knowledge for the purpose of understanding, preventing, and relieving psychologically-based distress or dysfunction and to promote subjective well-being and personal development. Central to its practice are psychological assessment, clinical formulation, and psychotherapy, although clinical psychologists also engage in research, teaching, consultation, forensic testimony, and program development and administration. In many countries, clinical psychology is a regulated mental health profession.\n\nThe field is generally considered to have begun in 1896 with the opening of the first psychological clinic at the University of Pennsylvania by Lightner Witmer. In the first half of the 20th century, clinical psychology was focused on psychological assessment, with little attention given to treatment. This changed after the 1940s when World War II resulted in the need for a large increase in the number of trained clinicians. Since that time, three main educational models have developed in the USA—the Ph.D. Clinical Science model (heavily focused on research), the Ph.D. science-practitioner model (integrating research and practice), and the Psy.D. practitioner-scholar model (focusing on clinical practice). In the UK and the Republic of Ireland the Clinical Psychology Doctorate falls between the latter two of these models, whilst in much of mainland Europe the training is at masters level and predominantly psychotherapeutic. Clinical psychologists are expert in providing psychotherapy, and generally train within four primary theoretical orientations—psychodynamic, humanistic, cognitive behavioral therapy (CBT), and systems or family therapy.\n\nThe earliest recorded approaches to assess and treat mental distress were a combination of religious, magical and/or medical perspectives. Early examples of such physicians included Patañjali, Padmasambhava, Rhazes, Avicenna, and Rumi. In the early 19th century, one approach to study mental conditions and behavior was using phrenology, the study of personality by examining the shape of the skull. Other popular treatments at that time included the study of the shape of the face (physiognomy) and Mesmer's treatment for mental conditions using magnets (mesmerism). Spiritualism and Phineas Quimby's \"mental healing\" were also popular.\n\nWhile the scientific community eventually came to reject all of these methods for treating mental illness, academic psychologists also were not concerned with serious forms of mental illness. The study of mental illness was already being done in the developing fields of psychiatry and neurology within the asylum movement. It was not until the end of the 19th century, around the time when Sigmund Freud was first developing his \"talking cure\" in Vienna, that the first scientific application of clinical psychology began.\n\nBy the second half of the 1800s, the scientific study of psychology was becoming well established in university laboratories. Although there were a few scattered voices calling for an applied psychology, the general field looked down upon this idea and insisted on \"pure\" science as the only respectable practice. This changed when Lightner Witmer (1867–1956), a past student of Wundt and head of the psychology department at the University of Pennsylvania, agreed to treat a young boy who had trouble with spelling. His successful treatment was soon to lead to Witmer's opening of the first psychological clinic at Penn in 1896, dedicated to helping children with learning disabilities. Ten years later in 1907, Witmer was to found the first journal of this new field, \"The Psychological Clinic\", where he coined the term \"clinical psychology\", defined as \"the study of individuals, by observation or experimentation, with the intention of promoting change\". The field was slow to follow Witmer's example, but by 1914, there were 26 similar clinics in the U.S.\n\nEven as clinical psychology was growing, working with issues of serious mental distress remained the domain of psychiatrists and neurologists. However, clinical psychologists continued to make inroads into this area due to their increasing skill at psychological assessment. Psychologists' reputation as assessment experts became solidified during World War I with the development of two intelligence tests, \"Army Alpha\" and \"Army Beta\" (testing verbal and nonverbal skills, respectively), which could be used with large groups of recruits. Due in large part to the success of these tests, assessment was to become the core discipline of clinical psychology for the next quarter century, when another war would propel the field into treatment.\n\nThe field began to organize under the name \"clinical psychology\" in 1917 with the founding of the American Association of Clinical Psychology. This only lasted until 1919, after which the American Psychological Association (founded by G. Stanley Hall in 1892) developed a section on Clinical Psychology, which offered certification until 1927. Growth in the field was slow for the next few years when various unconnected psychological organizations came together as the American Association of Applied Psychology in 1930, which would act as the primary forum for psychologists until after World War II when the APA reorganized. In 1945, the APA created what is now called Division 12, its division of clinical psychology, which remains a leading organization in the field. Psychological societies and associations in other English-speaking countries developed similar divisions, including in Britain, Canada, Australia and New Zealand.\n\nWhen World War II broke out, the military once again called upon clinical psychologists. As soldiers began to return from combat, psychologists started to notice symptoms of psychological trauma labeled \"shell shock\" (eventually to be termed posttraumatic stress disorder) that were best treated as soon as possible. Because physicians (including psychiatrists) were over-extended in treating bodily injuries, psychologists were called to help treat this condition. At the same time, female psychologists (who were excluded from the war effort) formed the National Council of Women Psychologists with the purpose of helping communities deal with the stresses of war and giving young mothers advice on child rearing. After the war, the Veterans Administration in the U.S. made an enormous investment to set up programs to train doctoral-level clinical psychologists to help treat the thousands of veterans needing care. As a consequence, the U.S. went from having no formal university programs in clinical psychology in 1946 to over half of all Ph.D.s in psychology in 1950 being awarded in clinical psychology.\n\nWWII helped bring dramatic changes to clinical psychology, not just in America but internationally as well. Graduate education in psychology began adding psychotherapy to the science and research focus based on the 1947 scientist-practitioner model, known today as the \"Boulder Model\", for Ph.D. programs in clinical psychology. Clinical psychology in Britain developed much like in the U.S. after WWII, specifically within the context of the National Health Service with qualifications, standards, and salaries managed by the British Psychological Society.\n\nBy the 1960s, psychotherapy had become embedded within clinical psychology, but for many the Ph.D. educational model did not offer the necessary training for those interested in practice rather than research. There was a growing argument that said the field of psychology in the U.S. had developed to a degree warranting explicit training in clinical practice. The concept of a practice-oriented degree was debated in 1965 and narrowly gained approval for a pilot program at the University of Illinois starting in 1968. Several other similar programs were instituted soon after, and in 1973, at the Vail Conference on Professional Training in Psychology, the practitioner–scholar model of clinical psychology—or \"Vail Model\"—resulting in the Doctor of Psychology (Psy.D.) degree was recognized. Although training would continue to include research skills and a scientific understanding of psychology, the intent would be to produce highly trained professionals, similar to programs in medicine, dentistry, and law. The first program explicitly based on the Psy.D. model was instituted at Rutgers University. Today, about half of all American graduate students in clinical psychology are enrolled in Psy.D. programs.\n\nSince the 1970s, clinical psychology has continued growing into a robust profession and academic field of study. Although the exact number of practicing clinical psychologists is unknown, it is estimated that between 1974 and 1990, the number in the U.S. grew from 20,000 to 63,000. Clinical psychologists continue to be experts in assessment and psychotherapy while expanding their focus to address issues of gerontology, sports, and the criminal justice system to name a few. One important field is health psychology, the fastest-growing employment setting for clinical psychologists in the past decade. Other major changes include the impact of managed care on mental health care; an increasing realization of the importance of knowledge relating to multicultural and diverse populations; and emerging privileges to prescribe psychotropic medication.\n\nClinical psychologists engage in a wide range of activities. Some focus solely on research into the assessment, treatment, or cause of mental illness and related conditions. Some teach, whether in a medical school or hospital setting, or in an academic department (e.g., psychology department) at an institution of higher education. The majority of clinical psychologists engage in some form of clinical practice, with professional services including psychological assessment, provision of psychotherapy, development and administration of clinical programs, and forensics (e.g., providing expert testimony in a legal proceeding).\n\nIn clinical practice, clinical psychologists may work with individuals, couples, families, or groups in a variety of settings, including private practices, hospitals, mental health organizations, schools, businesses, and non-profit agencies. Clinical psychologists who provide clinical services may also choose to specialize. Some specializations are codified and credentialed by regulatory agencies within the country of practice. In the United States such specializations are credentialed by the American Board of Professional Psychology (ABPP).\n\nClinical psychologists study a generalist program in psychology plus postgraduate training and/or clinical placement and supervision. The length of training differs across the world, ranging from four years plus post-Bachelors supervised practice to a doctorate of three to six years which combines clinical placement. In the USA, about half of all clinical psychology graduate students are being trained in Ph.D. programs—a model that emphasizes research—with the other half in Psy.D. programs, which has more focus on practice (similar to professional degrees for medicine and law). Both models are accredited by the American Psychological Association and many other English-speaking psychological societies. A smaller number of schools offer accredited programs in clinical psychology resulting in a Masters degree, which usually take two to three years post-Bachelors.\n\nIn the U.K., clinical psychologists undertake a Doctor of Clinical Psychology (D.Clin.Psych.), which is a practitioner doctorate with both clinical and research components. This is a three-year full-time salaried program sponsored by the National Health Service (NHS) and based in universities and the NHS. Entry into these programs is highly competitive, and requires at least a three-year undergraduate degree in psychology plus some form of experience, usually in either the NHS as an Assistant Psychologist or in academia as a Research Assistant. It is not unusual for applicants to apply several times before being accepted onto a training course as only about one-fifth of applicants are accepted each year. These clinical psychology doctoral degrees are accredited by the British Psychological Society and the Health Professions Council (HPC). The HPC is the statutory regulator for practitioner psychologists in the UK. Those who successfully complete clinical psychology doctoral degrees are eligible to apply for registration with the HPC as a clinical psychologist.\n\nThe practice of clinical psychology requires a license in the United States, Canada, the United Kingdom, and many other countries. Although each of the U.S. states is somewhat different in terms of requirements and licenses, there are three common elements:\n\nAll U.S. state and Canadian province licensing boards are members of the Association of State and Provincial Psychology Boards (ASPPB) which created and maintains the Examination for Professional Practice in Psychology (EPPP). Many states require other examinations in addition to the EPPP, such as a jurisprudence (i.e. mental health law) examination and/or an oral examination. Most states also require a certain number of continuing education credits per year in order to renew a license, which can be obtained though various means, such as taking audited classes and attending approved workshops. Clinical psychologists require the Psychologist license to practice, although licenses can be obtained with a masters-level degree, such as Marriage and Family Therapist (MFT), Licensed Professional Counselor (LPC), and Licensed Psychological Associate (LPA).\n\nIn the U.K. registration as a clinical psychologist with the Health Professions Council (HPC) is necessary. The HPC is the statutory regulator for practitioner psychologists in the U.K. In the U.K. the following titles are restricted by law \"registered psychologist\" and \"practitioner psychologist\"; in addition the specialist title \"clinical psychologist\" is also restricted by law.\n\nAn important area of expertise for many clinical psychologists is psychological assessment, and there are indications that as many as 91% of psychologists engage in this core clinical practice. Such evaluation is usually done in service to gaining insight into and forming hypotheses about psychological or behavioral problems. As such, the results of such assessments are usually used to create generalized impressions (rather than diagnoses) in service to informing treatment planning. Methods include formal testing measures, interviews, reviewing past records, clinical observation, and physical examination.\n\nThere exist hundreds of various assessment tools, although only a few have been shown to have both high validity (i.e., test actually measures what it claims to measure) and reliability (i.e., consistency). These measures generally fall within one of several categories, including the following:\n\nAfter assessment, clinical psychologists may provide a diagnostic impression. Many countries use the \"International Statistical Classification of Diseases and Related Health Problems\" (ICD-10) while the U.S. most often uses the \"Diagnostic and Statistical Manual of Mental Disorders\". Both are nosological systems that largely assume categorical disorders diagnosed through the application of sets of criteria including symptoms and signs.\n\nSeveral new models are being discussed, including a \"dimensional model\" based on empirically validated models of human differences (such as the five factor model of personality) and a \"psychosocial model\", which would take changing, intersubjective states into greater account. The proponents of these models claim that they would offer greater diagnostic flexibility and clinical utility without depending on the medical concept of illness. However, they also admit that these models are not yet robust enough to gain widespread use, and should continue to be developed.\n\nClinical psychologists do not tend to diagnose, but rather use \"formulation\"—an individualized map of the difficulties that the patient or client faces, encompassing predisposing, precipitating and perpetuating (maintaining) factors.\n\nClinical assessment can be characterized as a prediction problem where the purpose of assessment is to make inferences (predictions) about past, present, or future behavior. For example, many therapy decisions are made on the basis of what a clinician expects will help a patient make therapeutic gains. Once observations have been collected (e.g., psychological test results, diagnostic impressions, clinical history, X-ray, etc.), there are two mutually exclusive ways to combine those sources of information to arrive at a decision, diagnosis, or prediction. One way is to combine the data in an algorithmic, or \"mechanical\" fashion. Mechanical prediction methods are simply a mode of combination of data to arrive at a decision/prediction of behavior (e.g., treatment response). Mechanical prediction does not preclude any type of data from being combined; it can incorporate clinical judgments, properly coded, in the algorithm. The defining characteristic is that, once the data to be combined is given, the mechanical approach will make a prediction that is 100% reliable. That is, it will make exactly the same prediction for exactly the same data every time. Clinical prediction, on the other hand, does not guarantee this, as it depends on the decision-making processes of the clinician making the judgment, their current state of mind, and knowledge base.\n\nWhat has come to be called the \"clinical versus statistical prediction\" debate was first described in detail in 1954 by Paul Meehl, where he explored the claim that mechanical (formal, algorithmic) methods of data combination could outperform clinical (e.g., subjective, informal, \"in the clinician's head\") methods when such combinations are used to arrive at a prediction of behavior. Meehl concluded that mechanical modes of combination performed as well or better than clinical modes. Subsequent meta-analyses of studies that directly compare mechanical and clinical predictions have born out Meehl's 1954 conclusions. A 2009 survey of practicing clinical psychologists found that clinicians almost exclusively use their clinical judgment to make behavioral predictions for their patients, including diagnosis and prognosis.\n\nPsychotherapy involves a formal relationship between professional and client—usually an individual, couple, family, or small group—that employs a set of procedures intended to form a therapeutic alliance, explore the nature of psychological problems, and encourage new ways of thinking, feeling, or behaving.\n\nClinicians have a wide range of individual interventions to draw from, often guided by their training—for example, a cognitive behavioral therapy (CBT) clinician might use worksheets to record distressing cognitions, a psychoanalyst might encourage free association, while a psychologist trained in Gestalt techniques might focus on immediate interactions between client and therapist. Clinical psychologists generally seek to base their work on research evidence and outcome studies as well as on trained clinical judgment. Although there are literally dozens of recognized therapeutic orientations, their differences can often be categorized on two dimensions: insight vs. action and in-session vs. out-session.\nThe methods used are also different in regards to the population being served as well as the context and nature of the problem. Therapy will look very different between, say, a traumatized child, a depressed but high-functioning adult, a group of people recovering from substance dependence, and a ward of the state suffering from terrifying delusions. Other elements that play a critical role in the process of psychotherapy include the environment, culture, age, cognitive functioning, motivation, and duration (i.e. brief or long-term therapy).\n\nMany clinical psychologists are integrative or eclectic and draw from the evidence base across different models of therapy in an integrative way, rather than using a single specific model.\n\nIn the UK, clinical psychologists have to show competence in at least two models of therapy, including CBT, to gain their doctorate. The British Psychological Society Division of Clinical Psychology has been vocal about the need to follow the evidence base rather than being wedded to a single model of therapy.\n\nIn the USA, intervention applications and research are dominated in training and practice by essentially four major schools of practice: psychodynamic, humanistic, behavioral/cognitive behavioral, and systems or family therapy.\n\nThe psychodynamic perspective developed out of the psychoanalysis of Sigmund Freud. The core object of psychoanalysis is to make the unconscious conscious—to make the client aware of his or her own primal drives (namely those relating to sex and aggression) and the various defenses used to keep them in check. The essential tools of the psychoanalytic process are the use of free association and an examination of the client's transference towards the therapist, defined as the tendency to take unconscious thoughts or emotions about a significant person (e.g. a parent) and \"transfer\" them onto another person. Major variations on Freudian psychoanalysis practiced today include self psychology, ego psychology, and object relations theory. These general orientations now fall under the umbrella term \"psychodynamic psychology\", with common themes including examination of transference and defenses, an appreciation of the power of the unconscious, and a focus on how early developments in childhood have shaped the client's current psychological state.\n\nHumanistic psychology was developed in the 1950s in reaction to both behaviorism and psychoanalysis, largely due to the person-centered therapy of Carl Rogers (often referred to as Rogerian Therapy) and existential psychology developed by Viktor Frankl and Rollo May. Rogers believed that a client needed only three things from a clinician to experience therapeutic improvement—congruence, unconditional positive regard, and empathetic understanding. By using phenomenology, intersubjectivity and first-person categories, the humanistic approach seeks to get a glimpse of the whole person and not just the fragmented parts of the personality. This aspect of holism links up with another common aim of humanistic practice in clinical psychology, which is to seek an integration of the whole person, also called \"self-actualization\". From 1980, Hans-Werner Gessmann integrated the ideas of humanistic psychology into group psychotherapy as \"humanistic psychodrama\". According to humanistic thinking, each individual person already has inbuilt potentials and resources that might help them to build a stronger personality and self-concept. The mission of the humanistic psychologist is to help the individual employ these resources via the therapeutic relationship.\n\nCognitive behavioral therapy (CBT) developed from the combination of cognitive therapy and rational emotive behavior therapy, both of which grew out of cognitive psychology and behaviorism. CBT is based on the theory that how we think (cognition), how we feel (emotion), and how we act (behavior) are related and interact together in complex ways. In this perspective, certain dysfunctional ways of interpreting and appraising the world (often through \"schemas\" or \"beliefs\") can contribute to emotional distress or result in behavioral problems. The object of many cognitive behavioral therapies is to discover and identify the biased, dysfunctional ways of relating or reacting and through different methodologies help clients transcend these in ways that will lead to increased well-being. There are many techniques used, such as systematic desensitization, socratic questioning, and keeping a cognition observation log. Modified approaches that fall into the category of CBT have also developed, including dialectic behavior therapy and mindfulness-based cognitive therapy.\n\nBehavior therapy is a rich tradition. It is well researched with a strong evidence base. Its roots are in behaviorism. In behavior therapy, environmental events predict the way we think and feel. Our behavior sets up conditions for the environment to feedback back on it. Sometimes the feedback leads the behavior to increase- reinforcement and sometimes the behavior decreases- punishment. Oftentimes behavior therapists are called applied behavior analysts or behavioral health counselors. They have studied many areas from developmental disabilities to depression and anxiety disorders. In the area of mental health and addictions a recent article looked at APA's list for well established and promising practices and found a considerable number of them based on the principles of operant and respondent conditioning. Multiple assessment techniques have come from this approach including functional analysis (psychology), which has found a strong focus in the school system. In addition, multiple intervention programs have come from this tradition including community reinforcement approach for treating addictions, acceptance and commitment therapy, functional analytic psychotherapy, including dialectic behavior therapy and behavioral activation. In addition, specific techniques such as contingency management and exposure therapy have come from this tradition.\n\nSystems or family therapy works with couples and families, and emphasizes family relationships as an important factor in psychological health. The central focus tends to be on interpersonal dynamics, especially in terms of how change in one person will affect the entire system. Therapy is therefore conducted with as many significant members of the \"system\" as possible. Goals can include improving communication, establishing healthy roles, creating alternative narratives, and addressing problematic behaviors.\n\nThere exist dozens of recognized schools or orientations of psychotherapy—the list below represents a few influential orientations not given above. Although they all have some typical set of techniques practitioners employ, they are generally better known for providing a framework of theory and philosophy that guides a therapist in his or her working with a client.\n\n\nIn the last couple of decades, there has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of cultural, gender, spiritual, and sexual-orientation issues. Clinical psychologists are beginning to look at the various strengths and weaknesses of each orientation while also working with related fields, such as neuroscience, behavioral genetics, evolutionary biology, and psychopharmacology. The result is a growing practice of eclecticism, with psychologists learning various systems and the most efficacious methods of therapy with the intent to provide the best solution for any given problem.\n\nThe field of clinical psychology in most countries is strongly regulated by a code of ethics. In the U.S., professional ethics are largely defined by the APA \"Code of Conduct\", which is often used by states to define licensing requirements. The APA Code generally sets a higher standard than that which is required by law as it is designed to guide responsible behavior, the protection of clients, and the improvement of individuals, organizations, and society. The Code is applicable to all psychologists in both research and applied fields.\n\nThe APA Code is based on five principles: Beneficence and Nonmaleficence, Fidelity and Responsibility, Integrity, Justice, and Respect for People's Rights and Dignity. Detailed elements address how to resolve ethical issues, competence, human relations, privacy and confidentiality, advertising, record keeping, fees, training, research, publication, assessment, and therapy.\n\nIn the UK the British Psychological Society has published a Code of Conduct and Ethics for clinical psychologists. This has four key areas: Respect, Competence, Responsibility and Integrity. Other European professional organisations have similar codes of conduct and ethics.\n\nAlthough clinical psychologists and psychiatrists can be said to share a same fundamental aim—the alleviation of mental distress—their training, outlook, and methodologies are often quite different. Perhaps the most significant difference is that psychiatrists are licensed physicians. As such, psychiatrists often use the medical model to assess psychological problems (i.e., those they treat are seen as patients with an illness) and rely on psychotropic medications as the chief method of addressing the illness—although many also employ psychotherapy as well. Psychiatrists and medical psychologists (who are clinical psychologists that are also licensed to prescribe) are able to conduct physical examinations, order and interpret laboratory tests and EEGs, and may order brain imaging studies such as CT or CAT, MRI, and PET scanning.\n\nClinical psychologists generally do not prescribe medication, although there is a growing movement for psychologists to have prescribing privileges. These medical privileges require additional training and education. To date, medical psychologists may prescribe psychotropic medications in Guam, Iowa, Idaho, Illinois, New Mexico, Louisiana, the Public Health Service, the Indian Health Service, and the United States Military.\n\nCounseling psychologists undergo the same level of rigor in study and use many of the same interventions and tools as clinical psychologists, including psychotherapy and assessment. Traditionally, counseling psychologists helped people with what might be considered normal or moderate psychological problems—such as the feelings of anxiety or sadness resulting from major life changes or events. However, that distinction has faded over time, and of the counseling psychologists who do not go into academia (which does not involve treatment or diagnosis), the majority of counseling psychologists treat mental illness alongside clinical psychologists. Many counseling psychologists also receive specialized training in career assessment, group therapy, and relationship counseling.\n\nCounseling psychology as a field values multiculturalism and social advocacy, often stimulating research in multicultural issues. There are fewer counseling psychology graduate programs than those for clinical psychology and they are more often housed in departments of education rather than psychology. Counseling psychologists tend to be more frequently employed in university counseling centers compared to hospitals and private practice for clinical psychologists. However, counseling and clinical psychologists can be employed in a variety of settings, with a large degree of overlap (prisons, colleges, community mental health, non-profits, corporations, private practice, hospitals and Veterans Affairs). Distinctions between the two fields continue to fade.\n\nSchool psychologists are primarily concerned with the academic, social, and emotional well-being of children and adolescents within a scholastic environment. In the U.K., they are known as \"educational psychologists\". Like clinical (and counseling) psychologists, school psychologists with doctoral degrees are eligible for licensure as health service psychologists, and many work in private practice. Unlike clinical psychologists, they receive much more training in education, child development and behavior, and the psychology of learning. Common degrees include the Educational Specialist Degree (Ed.S.), Doctor of Philosophy (Ph.D.), and Doctor of Education (Ed.D.).\n\nTraditional job roles for school psychologists employed in school settings have focused mainly on assessment of students to determine their eligibility for special education services in schools, and on consultation with teachers and other school professionals to design and carry out interventions on behalf of students. Other major roles also include offering individual and group therapy with children and their families, designing prevention programs (e.g. for reducing dropout), evaluating school programs, and working with teachers and administrators to help maximize teaching efficacy, both in the classroom and systemically.\n\nSocial workers provide a variety of services, generally concerned with social problems, their causes, and their solutions. With specific training, clinical social workers may also provide psychological counseling (in the U.S. and Canada), in addition to more traditional social work. The Masters in Social Work in the U.S. is a two-year, sixty credit program that includes at least a one-year practicum (two years for clinicians).\n\nOccupational therapy—often abbreviated OT—is the \"use of productive or creative activity in the treatment or rehabilitation of physically, cognitively, or emotionally disabled people.\" Most commonly, occupational therapists work with people with disabilities to enable them to maximize their skills and abilities. Occupational therapy practitioners are skilled professionals whose education includes the study of human growth and development with specific emphasis on the physical, emotional, psychological, sociocultural, cognitive and environmental components of illness and injury. They commonly work alongside clinical psychologists in settings such as inpatient and outpatient mental health, pain management clinics, eating disorder clinics, and child development services. OT's use support groups, individual counseling sessions, and activity-based approaches to address psychiatric symptoms and maximize functioning in life activities.\n\nClinical psychology is a diverse field and there have been recurring tensions over the degree to which clinical practice should be limited to treatments supported by empirical research. Despite some evidence showing that all the major therapeutic orientations are about of equal effectiveness, there remains much debate about the efficacy of various forms treatment in use in clinical psychology.\n\nIt has been reported that clinical psychology has rarely allied itself with client groups and tends to individualize problems to the neglect of wider economic, political and social inequality issues that may not be the responsibility of the client. It has been argued that therapeutic practices are inevitably bound up with power inequalities, which can be used for good and bad. A critical psychology movement has argued that clinical psychology, and other professions making up a \"psy complex\", often fail to consider or address inequalities and power differences and can play a part in the social and moral control of disadvantage, deviance and unrest.\n\nAn October 2009 editorial in the journal \"Nature\" suggests that a large number of clinical psychology practitioners in the United States consider scientific evidence to be \"less important than their personal – that is, subjective – clinical experience.\"\n\n"}
{"id": "57395711", "url": "https://en.wikipedia.org/wiki?curid=57395711", "title": "Contiguity (psychology)", "text": "Contiguity (psychology)\n\nIn cognitive science, association by contiguity is the principle that ideas, memories, and experiences are linked when one is frequently experienced with the other. For example, if you constantly see a knife and a fork together they become linked (associated). The more these two items (stimuli) are perceived together the stronger the link between them. When one of the memories becomes activated later on, the linked (contiguously associated) memory becomes temporarily more activated and thus easier to be called into working memory. This process is called \"priming,\" and the initial memory that primed the other is called the \"retrieval cue.\"\n\nAssociation by contiguity is the root of association by similarity. Association by similarity is the idea that one memory primes another through their common property or properties. Thus, an apple may prime a memory of a rose through the common property of red. These two become associated even though you may have never experienced an apple and a rose together (consistent with association by contiguity).\n\nIn the study of human memory, the contiguity effect has been found in studies of free recall. Analyses of free recall data indicates that there tends to be the greatest number of +/- 1 transitions between words, suggesting that a person is more likely to recall words together that are closer together in a list. This is shown in a graph of conditional response probability as a function of lag as originated by Dr. Michael Kahana. The probability of recall (y-axis) is plotted against the lag, or separation between subsequently recalled words. For example, if two items A and B are learned together, when cued with B, A is retrieved and vice versa due to their temporal contiguity, although there will be a stronger forward association (when cued with A, B is recalled).\n\nThe contiguity effect appears relatively constant, and has been predicted to have long-term effects according to the temporal context model proposed by Howard and Kahana. This model explains the contiguity effect in the following manner: when an item is presented, it activates the temporal context that was active when the item was originally studied. Since contexts of neighboring items overlap, and that overlap increases with decreasing lag between items, a contiguity effect results. The contiguity effect has even been found between items in different lists, although it has been speculated that these items could simply be intrusions.\n\nWhen one associated memory, a group of associated memories, or a whole line of associated memories becomes primed, this is known as spreading activation.\n\nIn conditioning, contiguity refers to how associated a reinforcer is with behaviour. The higher the contiguity between events the greater the strength of the behavioural relationship.\n\nEdwin Ray Guthrie's contiguity theory deals with patterned movements.\n\n"}
{"id": "11523713", "url": "https://en.wikipedia.org/wiki?curid=11523713", "title": "Control (management)", "text": "Control (management)\n\nControl, or controlling, is one of the managerial functions like \"planning\", \"organizing\", \"staffing\" and \"directing\". It is an important function because it helps to check the errors and to take the corrective action so that deviation from standards are minimized and stated goals of the organization are achieved in a desired manner.\n\nAccording to modern concepts, control is a foreseeing action whereas earlier concept of control was used only when errors were detected. Control in management means setting standards, measuring actual performance and taking corrective action.\n\nIn 1916, Henri Fayol formulated one of the first definitions of control as it pertains to management: \n\"Control of an undertaking consists of seeing that everything is being carried out in accordance with the plan which has been adopted, the orders which have been given, and the principles which have been laid down. Its object is to point out mistakes in order that they may be rectified and prevented from recurring.\"\n\nAccording to EFL Brech:\n\"Control is checking current performance against pre-determined standards contained in the plans, with a view to ensure adequate progress and satisfactory performance.\"\nAccording to Harold Koontz:\n\"Controlling is the measurement and correction of performance in order to make sure that enterprise objectives and the plans devised to attain them are accomplished.\"\nAccording to Stafford Beer: \n\"Management is the profession of control.\"\nRobert J. Mockler presented a more comprehensive definition of managerial control: \n\"Management control can be defined as a systematic effort by business management to compare performance to predetermined standards, plans, or objectives in order to determine whether performance is in line with these standards and presumably in order to take any remedial action required to see that human and other corporate resources are being used in the most effective and efficient way possible in achieving corporate objectives.\" \n\nAlso control can be defined as \"that function of the system that adjusts operations as needed to achieve the plan, or to maintain variations from system objectives within allowable limits\". The control subsystem functions in close harmony with the operating system. The degree to which they interact depends on the nature of the operating system and its objectives. Stability concerns a system's ability to maintain a pattern of output without wide fluctuations. Rapidity of response pertains to the speed with which a system can correct variations and return to expected output.\n\nA political election can illustrate the concept of control and the importance of feedback. Each party organizes a campaign to get its candidate selected and outlines a plan to inform the public about both the candidate's credentials and the party's platform. As the election nears, opinion polls furnish feedback about the effectiveness of the campaign and about each candidate's chances to win. Depending on the nature of this feedback, certain adjustments in strategy and/or tactics can be made in an attempt to achieve the desired result.\n\nFrom these definitions it can be stated that there is close link between planning and controlling. Planning is a process by which an organization's objectives and the methods to achieve the objectives are established, and controlling is a process which measures and directs the actual performance against the planned goals of the organization. Thus, goals and objectives are often referred to as siamese twins of management.\nthe managerial function of management and correction of performance in order to make sure that enterprise objectives and the goals devised to attain them being accomplished.\n\n\nThe four basic elements in a control system:\n\noccur in the same sequence and maintain a consistent relationships to each other in every system.\nThe first element is the \"characteristic\" or condition of the operating system which is to be measured. We select a specific characteristic because a correlation exists between it and how the system is performing. The characteristic can be the output of the system during any stage of processing or it may be a condition that is the result of the system. For example, it may be the heat energy produced by the furnace or the temperature in the room which has changed because of the heat generated by the furnace. In an elementary school system, the hours a teacher works or the gain in knowledge demonstrated by the students on a national examination are examples of characteristics that may be selected for measurement, or control.\n\nThe second element of control, the \"sensor\", is a means for measuring the characteristic or condition. For example, in a home heating system this device would be the thermostat, and in a quality-control system this measurement might be performed by a visual inspection of the product.\n\nThe third element of control, the comparator, determines the need for correction by comparing what is occurring with what has been planned. Some deviation from the plan is usual and expected, but when variations are beyond those considered acceptable, corrective action is required. It involves a sort of preventative action which indicates that good control is being achieved.\n\nThe fourth element of control, the activator, is the corrective action taken to return the system to its expected output. The actual person, device, or method used to direct corrective inputs into the operating system may take a variety of forms. It may be a hydraulic controller positioned by a solenoid or electric motor in response to an electronic error signal, an employee directed to rework the parts that failed to pass quality inspection, or a school principal who decides to buy additional books to provide for an increased number of students. As long as a plan is performed within allowable limits, corrective action is not necessary; however, this seldom occurs in practice.\n\nInformation is the medium of control, because the flow of sensory data and later the flow of corrective information allow a characteristic or condition of the system to be controlled. To illustrate how information flow facilitates control, let us review the elements of control in the context of information.\n\nThe primary requirement of a control system is that it maintains the level and kind of output necessary to achieve the system's objectives. It is usually impractical to control every feature and condition associated with the system's output. Therefore, the choice of the controlled item (and appropriate information about it) is extremely important. There should be a direct correlation between the controlled item and the system's operation. In other words, control of the selected characteristic should have a direct relationship to the goal or objective of the system.\n\nAfter the characteristic is sensed, or measured, information pertinent to control is fed back. Exactly what information needs to be transmitted and also the language that will best facilitate the communication process and reduce the possibility of distortion in transmission must be carefully considered. Information that is to be compared with the standard, or plan, should be expressed in the same terms or language as in the original plan to facilitate decision making. Using machine methods (computers) may require extensive translation of the information. Since optimal languages for computation and for human review are not always the same, the relative ease of translation may be a significant factor in selecting the units of measurement or the language unit in the sensing element.\n\nIn many instances, the measurement may be sampled rather than providing a complete and continuous feedback of information about the operation. A sampling procedure suggests measuring some segment or portion of the operation that will represent the total.\n\nIn a social system, the norms of acceptable behavior become the standard against which so-called deviant behavior may be judged. Regulations and laws provide a more formal collection of information for society. Social norms change, but very slowly. In contrast, the standards outlined by a formal law can be changed from one day to the next through revision, discontinuation, or replacement by another.\nInformation about deviant behavior becomes the basis for controlling social activity. Output information is compared with the standard or norm and significant deviations are noted. In an industrial example, frequency distribution (a tabulation of the number of times a given characteristic occurs within the sample of products being checked) may be used to show the average quality, the spread, and the comparison of output with a standard.\n\nIf there is a significant and uncorrectable difference between output and plan, the system is \"out of control.\" This means that the objectives of the system are not feasible in relation to the capabilities of the present design. Either the objectives must be reevaluated or the system redesigned to add new capacity or capability. For example, the traffic in drugs has been increasing in some cities at an alarming rate. The citizens must decide whether to revise the police system so as to regain control, or whether to modify the law to reflect a different norm of acceptable behavior.\n\nThe activator unit responds to the information received from the comparator and initiates corrective action. If the system is a machine-to-machine system, the corrective inputs (decision rules) are designed into the network. When the control relates to a man-to-machine or man-to-man system, however, the individual(s) in charge must evaluate (1) the accuracy of the feedback information, (2) the significance of the variation, and (3) what corrective inputs will restore the system to a reasonable degree of stability. Once the decision has been made to direct new inputs into the system, the actual process may be relatively easy. A small amount of energy can change the operation of jet airplanes, automatic steel mills, and hydroelectric power plants. The pilot presses a button, and the landing gear of the airplane goes up or down; the operator of a steel mill pushes a lever, and a ribbon of white-hot steel races through the plant; a worker at a control board directs the flow of electrical energy throughout a regional network of stations and substations. It takes but a small amount of control energy to release or stop large quantities of input.\n\nThe comparator may be located far from the operating system, although at least some of the elements must be in close proximity to operations. For example, the measurement (the sensory element) is usually at the point of operations. The measurement information can be transmitted to a distant point for comparison with the standard (comparator), and when deviations occur, the correcting input can be released from the distant point. However, the input (activator) will be located at the operating system. This ability to control from afar means that aircraft can be flown by remote control, dangerous manufacturing processes can be operated from a safe distance, and national organizations can be directed from centralized headquarters in Dublin, Ireland.\n\nStep 1. Establishment of Standard.\n\nStandards are the criteria against which actual performance will be measured. Standards are set in both quantitative and qualitative terms.\n\nStep 2. Measurement of actual performance\n\nPerformance is measured in an objective and reliable manner. It should be checked in the same unit in which the standards are set.\n\nStep 3. Comparing actual performance with standards.\n\nStep 4. Analysis the cause of deviations.\n\nStep 5. Taking corrective action.\n\nControl may be grouped according to three general classifications:\n\nA street-lighting system controlled by a timing device is an example of an open-loop system. At a certain time each evening, a mechanical device closes the circuit and energy flows through the electric lines to light the lamps. Note, however, that the timing mechanism is an independent unit and is not measuring the objective function of the lighting system. If the lights should be needed on a dark, stormy day the timing device would not recognize this need and therefore would not activate energy inputs. Corrective properties may sometimes be built into the controller (for example, to modify the time the lights are turned on as the days grow shorter or longer), but this would not close the loop. In another instance, the sensing, comparison, or adjustment may be made through action taken by an individual who is not part of the system. For example, the lights may be turned on by someone who happens to pass by and recognizes the need for additional light.\n\nIf control is exercised as a result of the operation rather than because of outside or predetermined arrangements, it is a closed-loop system. The home thermostat is the classic example of a control device in a closed-loop system. When the room temperature drops below the desired point, the control mechanism closes the circuit to start the furnace and the temperature rises. The furnace-activating circuit is turned off as the temperature reaches the preselected level. The significant difference between this type of system and an open-loop system is that the control device is an element of the system it serves and measures the performance of the system. In other words, all four control elements are integral to the specific system.\n\nAn essential part of a closed-loop system is feedback; that is, the output of the system is measured continually through the item controlled, and the input is modified to reduce any difference or error toward zero. Many of the patterns of information flow in organizations are found to have the nature of closed loops, which use feedback. The reason for such a condition is apparent when one recognizes that any system, if it is to achieve a predetermined goal, must have available to it at all times an indication of its degree of attainment. In general, every goal-seeking system employs feedback.\n\nThe elements of control are easy to identify in machine systems. For example, the characteristic to be controlled might be some variable like speed or temperature, and the sensing device could be a speedometer or a thermometer. An expectation of precision exists because the characteristic is quantifiable and the standard and the normal variation to be expected can be described in exact terms. In automatic machine systems, inputs of information are used in a process of continual adjustment to achieve output specifications. When even a small variation from the standard occurs, the correction process begins. The automatic system is highly structured, designed to accept certain kinds of input and produce specific output, and programmed to regulate the transformation of inputs within a narrow range of variation.\n\nFor an illustration of mechanical control: as the load on a steam engine increases and the engine starts to slow down, the regulator reacts by opening a valve that releases additional inputs of steam energy. This new input returns the engine to the desired number of revolutions per minute. This type of mechanical control is crude in comparison to the more sophisticated electronic control systems in everyday use. Consider the complex missile-guidance systems that measure the actual course according to predetermined mathematical calculations and make almost instantaneous corrections to direct the missile to its target.\n\nMachine systems can be complex because of the sophisticated technology, whereas control of people is complex because the elements of control are difficult to determine. In human control systems, the relationship between objectives and associated characteristics is often vague; the measurement of the characteristic may be extremely subjective; the expected standard is difficult to define; and the amount of new inputs required is impossible to quantify. To illustrate, let us refer once more to a formalized social system in which deviant behavior is controlled through a process of observed violation of the existing law (sensing), court hearings and trials (comparison with standard), incarceration when the accused is found guilty (correction), and release from custody after rehabilitation of the individual has occurred.\n\nThe speed limit established for freeway driving is one standard of performance that is quantifiable, but even in this instance, the degree of permissible variation and the amount of the actual variation are often a subject of disagreement between the patrolman and the suspected violator. The complexity of our society is\nreflected in many of our laws and regulations, which establish the general standards for economic, political, and social operations. A citizen may not know or understand the law and consequently would not know whether or not he was guilty of a violation.\n\nMost organized systems are some combination of man and machine; some elements of control may be performed by machine whereas others are accomplished by man. In addition, some standards may be precisely structured whereas others may be little more than general guidelines with wide variations expected in output. Man must act as the controller when measurement is subjective and judgment is required. Machines such as computers are incapable of making exceptions from the specified control criteria regardless of how much a particular case might warrant special consideration. A pilot acts in conjunction with computers and automatic pilots to fly large jets. In the event of unexpected weather changes, or possible collision with another plane, he must intercede and assume direct control.\n\nThe concept of organizational control is implicit in the bureaucratic theory of Max Weber. Associated with this theory are such concepts as \"span of control\", \"closeness of supervision\", and \"hierarchical authority\". Weber's view tends to include all levels or types of organizational control as being the same. More recently, writers have tended to differentiate the control process between that which emphasizes the nature of the organizational or systems design and that which deals with daily operations. To illustrate the difference, we \"evaluate\" the performance of a system to see how effective and efficient the design proved to be or to discover why it failed. In contrast, we operate and \"control\" the system with respect to the daily inputs of material, information, and energy. In both instances, the elements of feedback are present, but organizational control tends to review and evaluate the nature and arrangement of components in the system, whereas operational control tends to adjust the daily inputs.\n\nThe direction for organizational control comes from the goals and strategic plans of the organization. General plans are translated into specific performance measures such as share of the market, earnings, return on investment, and budgets. The process of organizational control is to review and evaluate the performance of the system against these established norms. Rewards for meeting or exceeding standards may range from special recognition to salary increases or promotions. On the other hand, a failure to meet expectations may signal the need to reorganize or redesign.\n\nIn organizational control, the approach used in the program of review and evaluation depends on the reason for the evaluation — \"that is, is it because the system is not effective (accomplishing its objectives)? Is the system failing to achieve an expected standard of efficiency? Is the evaluation being conducted because of a breakdown or failure in operations? Is it merely a periodic audit-and-review process?\"\n\nWhen a system has failed or is in great difficulty, special diagnostic techniques may be required to isolate the trouble areas and to identify the causes of the difficulty. It is appropriate to investigate areas that have been troublesome before or areas where some measure of performance can be quickly identified. For example, if an organization's output backlog builds rapidly, it is logical to check first to see if the problem is due to such readily obtainable measures as increased demand or to a drop in available man hours. When a more detailed analysis is necessary, a systematic procedure should be followed.\n\nIn contrast to organizational control, operational control serves to regulate the day-to-day output relative to schedules, specifications, and costs. \"Is the output of product or service the proper quality and is it available as scheduled? Are inventories of raw materials, goods-in-process, and finished products being purchased and produced in the desired quantities? Are the costs associated with the transformation process in line with cost estimates? Is the information needed in the transformation process available in the right form and at the right time? Is the energy resource being utilized efficiently?\"\n\nThe most difficult task of management concerns monitoring the behavior of individuals, comparing performance to some standard, and providing rewards or punishment as indicated. Sometimes this control over people relates entirely to their output. For example, a manager might not be concerned with the behavior of a salesman as long as sales were as high as expected. In other instances, close supervision of the salesman might be appropriate if achieving customer satisfaction were one of the sales organization's main objectives.\n\nThe larger the unit, the more likely that the control characteristic will be related to some output goal. It also follows that if it is difficult or impossible to identify the actual output of individuals, it is better to measure the performance of the entire group. This means that individuals' levels of motivation and the measurement of their performance become subjective judgments made by the supervisor. Controlling output also suggests the difficulty of controlling individuals' performance and relating this to the total system's objectives.\n\nThe perfect plan could be outlined if every possible variation of input could be anticipated and if the system would operate as predicted. This kind of planning is neither realistic, economical, nor feasible for most business systems. If it were feasible, planning requirements would be so complex that the system would be out of date before it could be operated. Therefore, we design control into systems. This requires more thought in the systems design but allows more flexibility of operations and makes it possible to operate a system using unpredictable components and undetermined input. Still, the design and effective operation of control are not without problems.\n\nThe objective of the system is to perform some specified function. \nThe objective of organizational control is to see that the specified function is achieved. \nThe objective of operational control is to ensure that variations in daily output are maintained within prescribed limits.\n\nIt is one thing to design a system that contains all of the elements of control, and quite another to make it operate true to the best objectives of design. Operating \"in control\" or \"with plan\" does not guarantee optimum performance. For example, the plan may not make the best use of the inputs of materials, energy, or information — in other words, the system may not be designed to operate efficiently. Some of the more typical problems relating to control include the difficulty of measurement, the problem of timing information flow, and the setting of proper standards.\n\nWhen objectives are not limited to quantitative output, the measurement of system effectiveness is difficult to make and subsequently perplexing to evaluate. Many of the characteristics pertaining to output do not lend themselves to quantitative measurement. This is true particularly when inputs of human energy cannot be related directly to output. The same situation applies to machines and other equipment associated with human involvement, when output is not in specific units. In evaluating man-machine or human-oriented systems, psychological and sociological factors obviously do not easily translate into quantifiable terms. \"For example, how does mental fatigue affect the quality or quantity of output? And, if it does, is mental fatigue a function of the lack of a challenging assignment or the fear of a potential injury? \"\n\nSubjective inputs may be transferred into numerical data, but there is always the danger of an incorrect appraisal and transfer, and the danger that the analyst may assume undue confidence in such data after they have been quantified. Let us suppose, for example, that the decisions made by an executive are rated from 1 to 10, 10 being the perfect decision. After determining the ranking for each decision, adding these, and dividing by the total number of decisions made, the average ranking would indicate a particular executive's score in his decision-making role. On the basis of this score, judgments — which could be quite erroneous — might be made about his decision-making effectiveness. One executive with a ranking of 6.75 might be considered more effective than another who had a ranking of 6.25, and yet the two managers may have made decisions under different circumstances and conditions. External factors over which neither executive had any control may have influenced the difference in \"effectiveness\".\n\nQuantifying human behavior, despite its extreme difficulty, subjectivity, and imprecision in relation to measuring physical characteristics is the most prevalent and important measurement made in large systems. The behavior of individuals ultimately dictates the success or failure of every man-made system.\n\nAnother problem of control relates to the improper timing of information introduced into the feedback channel. Improper timing can occur in both computerized and human control systems, either by mistakes in measurement or in judgment. The more rapid the system's response to an error signal, the more likely it is that the system could overadjust; yet the need for prompt action is important because any delay in providing corrective input could also be crucial. A system generating feedback inconsistent with current need will tend to fluctuate and will not adjust in the desired manner.\n\nThe most serious problem in information flow arises when the delay in feedback is exactly one-half cycle, for then the corrective action is superimposed on a variation from norm which, at that moment, is in the same direction as that of the correction. This causes the system to overcorrect, and then if the reverse adjustment is made out of cycle, to correct too much in the other direction, and so on until the system fluctuates (\"oscillates\") out of control. This phenomenon is illustrated in Figure 1. “Oscillation and Feedback”. If, at Point A, the trend below standard is recognized and new inputs are added, but not until Point B, the system will overreact and go beyond the allowable limits. Again, if this is recognized at Point C, but inputs are not withdrawn until Point D, it will cause the system to drop below the lower limit of allowable variation.\n\nOne solution to this problem rests in anticipation, which involves measuring not only the change but also the rate of change. The correction is outlined as a factor of the type and rate of the error. The difficulty also might be overcome by reducing the time lag between the measurement of the output and the adjustment to input. If a trend can be indicated, a time lead can be introduced to compensate for the time lag, bringing about consistency between the need for correction and the type and magnitude of the indicated action. It is usually more effective for an organization to maintain continuous measurement of its performance and to make small adjustments in operations constantly (this assumes a highly sensitive control system). Information feedback, consequently, should be timely and correct to be effective. That is, the information should provide an accurate indication of the status of the system.\n\nSetting the proper standards or control limits is a problem in many systems. Parents are confronted with this dilemma in expressing what they expect of their children, and business managers face the same issue in establishing standards that will be acceptable to employees. Some theorists have proposed that workers be allowed to set their own standards, on the assumption that when people establish their own goals, they are more apt to accept and achieve them.\n\nStandards should be as precise as possible and communicated to all persons concerned. Moreover, communication alone is not sufficient; understanding is necessary. In human systems, standards tend to be poorly defined and the allowable range of deviation from standard also indefinite. For example, how many hours each day should a professor be expected to be available for student consultation? Or, what kind of behavior should be expected by students in the classroom? Discretion and personal judgment play a large part in such systems, to determine whether corrective action should be taken.\n\nPerhaps the most difficult problem in human systems is the unresponsiveness of individuals to indicated correction. This may take the form of opposition and subversion to control, or it may be related to the lack of defined responsibility or authority to take action. Leadership and positive motivation then become vital ingredients in achieving the proper response to input requirements.\n\nMost control problems relate to design; thus the solution to these problems must start at that point. Automatic control systems, provided that human intervention is possible to handle exceptions, offer the greatest promise. There is a danger, however, that we may measure characteristics that do not represent effective performance (as in the case of the speaker who requested that all of the people who could not hear what he was saying should raise their hands), or that improper information may be communicated.\n\nImportance of control\n"}
{"id": "21888481", "url": "https://en.wikipedia.org/wiki?curid=21888481", "title": "Cornelis de Jode", "text": "Cornelis de Jode\n\nCornelis de Jode (1568 – 17 October 1600) was a cartographer, engraver and publisher from Antwerp. He was the son of Gerard de Jode, also a cartographer. Cornelis studied science at Academy of Douai\n\nWhen his father died in 1591, Cornelis de Jode took over the work on his father's uncompleted atlas, which he eventually published in 1593 as \"Speculum Orbis Terrae \".\nDespite that contemporary scholars consider many of de Jode's maps to be copies of both Portuguese and Spanish cartographers in detail and style of atlas of the time Theatrum Orbis Terrarum by Ortelius, de Jode's atlas never sold well due to his plagiarize.\n\nAfter his death, the engraving plates were sold to J. B. Vrients (who also owned the Ortelius plates), and the complete work was not published again.\n"}
{"id": "54016814", "url": "https://en.wikipedia.org/wiki?curid=54016814", "title": "Creating Masculinity in Los Angeles's Little Manila", "text": "Creating Masculinity in Los Angeles's Little Manila\n\nCreating Masculinity in Los Angeles's Little Manila: Working-Class Filipinos and Popular Culture, 1920s-1950s is a 2006 non-fiction book authored by Linda España-Maram. It was published by Columbia University Press.\n"}
{"id": "20394282", "url": "https://en.wikipedia.org/wiki?curid=20394282", "title": "Edward Samuel Ritchie", "text": "Edward Samuel Ritchie\n\nEdward Samuel Ritchie (1814–1895), an American inventor and physicist, is considered to be the most innovative instrument maker in nineteenth-century America, making important contributions to both science and navigation.\n\nRitchie was born in Dorchester, Massachusetts on August 18, 1814, the son of John and Elizabeth Eliot Ritchie. As a young child Ritchie displayed a great aptitude for both the arts and mechanical sciences. After working as an amateur sculptor, he founded a business in 1850 with N.B. Chamberlain to manufacture mechanical and electrical instruments. Chamberlain eventually left, and Ritchie continued the business alone.\n\nIn the early 1850s, after examining an example of an electric induction coil made by German instrument maker Heinrich Daniel Ruhmkorff, which produced a small two-inch (50 mm) electric spark when energized, Ritchie perceived that it could be made more efficient and produce a longer spark by redesigning and improving its secondary insulation. His own design divided the coil into sections, each properly insulated from each other. Ritchie's first induction coil produced a spark 10 inches (25 cm) in length; a later perfected model produced a bolt two feet (60 cm) or longer in length.\n\nIn 1857, one of Ritchie's induction coils was exhibited in Dublin, Ireland at a conference of the British Association for the Advancement of Science, and later at the University of Edinburgh in Scotland. Intrigued, Ruhmkorff himself procured a sample of the Ritchie induction coil and used it as a basis for revising his own design. The German inventor was later awarded a scientific prize by Napoleon III. Disappointed in not receiving recognition for his improvements, Ritchie turned his attention to navigational instruments.\nRitchie began making marine bearing compasses for the U.S. Navy before the American Civil War. At the time, British Admiralty dry-mount nautical compasses were considered by all navies and merchant shipping companies as the technological standard of the day. Ritchie thought they could be improved upon, and by 1860 had received a U.S. patent for the first successful and practicable liquid-filled marine compass suitable for general use, a development that has been described as the first major advance in compass technology in several hundred years. With the damping provided by the liquid, together with a gimbal mounting, the floating indicator or card of the Ritchie compass remained relatively stable even when a ship's deck pitched and rolled during periods of severe weather. In Ritchie's third patent application (#38,126) dated April 7, 1863, several features that contributed to the success of his compass are revealed, including a floating card of nearly the same specific gravity as the liquid, an air-tight metallic case, and an elastic chamber that served as a diaphragm, compensating for temperature changes and resultant unequal expansion of the liquid and the bowl.\n\nRitchie liquid-filled nautical compasses soon became a U.S. Navy standard, and were also widely used by American merchant mariners. He also invented an improved theodolite which was immediately adopted by the U.S. Navy for measuring harbors and port entrances.\n\nThe business he began in 1850 became E. S. Ritchie & Son in 1866 and E. S. Ritchie & Sons in 1867, and moved from Boston to new facilities in Brookline in 1886. The company specializes in manufacturing compasses, astro-navigation devices, and other nautical equipment for all types of ships, including small hand-bearing compasses for recreational and amateur sailing vessels.\n\nFollowing Ritchie’s death in 1895, his sons transferred the scientific instruments to the L. E. Knott Apparatus Co., while retaining the nautical instrument line. The firm was incorporated as E.S. Ritchie & Sons, Inc. in 1939, and continues operations to the present day. It is now located in Pembroke, Massachusetts, and known as Ritchie Navigation.\n\nA model of Ritchie's first liquid-filled compass in the collections of the Science Museum (London).\n\n"}
{"id": "4473599", "url": "https://en.wikipedia.org/wiki?curid=4473599", "title": "Electrofuge", "text": "Electrofuge\n\nAn electrofuge is a leaving group which does not retain the bonding pair of electrons from its previous bond with another species. It can result from the heterolytic breaking of covalent bonds.\nAfter this reaction an electrofuge may possess either a positive or a neutral charge; this is governed by the nature of the specific reaction.\n\nAn example would be the loss of H from a molecule of benzene during nitration.\n\nThe word 'electrofuge' is commonly found in older literature, but its use is now uncommon.\n\n"}
{"id": "14078238", "url": "https://en.wikipedia.org/wiki?curid=14078238", "title": "Elton's quadrant", "text": "Elton's quadrant\n\nAn Elton's quadrant is a derivative of the Davis quadrant. It adds an index arm and artificial horizon to the instrument. It was invented by John Elton a sea captain who patented his design in 1728 and published details of the instrument in the Philosophical Transactions of the Royal Society in 1732.\n\nThis instrument clearly reflects the shape and features of the Davis quadrant. The significant differences are the change in the upper arc to a simple triangular frame and the addition of an index arm. The triangular frame at the top spans 60° as did the arc on the backstaff. The main graduated arc subtends 30° as in the backstaff. The 30° arc is graduated in degrees and sixths of a degree, that is, at ten-minute intervals.\n\nThe sighting vane of the backstaff is replaced with a sight (called an \"eye vane\") mounted on the end of the index arm.\n\nThe index arm includes a nonius to allow reading the large scale with ten divisions between the graduations on the scale. This provides the navigator with the ability to read the scale to the nearest minute of arc. The index arm has a spirit level to allow the navigator to ensure that the index is horizontal even when he cannot see the horizon. \n\nThe instrument has a horizon vane like a Davis quadrant, but Elton refers to it as the \"shield\" or \"ray vane\". The shield is attached to the \"label\". The label is an arm that extends from the centre of the arc to the outside of the upper triangle and can be set to one of the three positions in the triangle (in the diagram, it appears to bisect the triangle as it is set to the centre or 30° position). At the upper end of the label is a \"Flamsteed glass\" or lens.\n\nThe three set positions allow the instrument to read 0° to 30°, 30° to 60° or 60° to 90°. The lens projects an image of the sun rather than a shadow of the sun on the shield. This provides an image even when the sky is hazy or lightly overcast. In addition, at the mid-span of the label there is a mounting point for a lantern to be used during nocturnal observations.\n\nThere are two spirit levels on the shield. One, called the \"azimuth tube\", ensures that the plane of the instrument is vertical. The other is perpendicular to the shield and will indicate when the plane of the shield is vertical and the label is horizontal.\n\nFor measuring the altitude of the sun, the Elton's quadrant can be used in the same manner as a Davis quadrant. However, with the artificial horizon, the eye vane is not required to be used.\n\nHold the instrument in a comfortable manner with the arc towards the sun. Set the label so that the sun's image is projected on the shield at the hole with the index arm roughly horizontal. Move the index arm so that the index's spirit level shows the arm is precisely horizontal. This sets the instrument and the angle can be read with the scale and nonius.\n\nThis is a means of measuring altitude of a celestial object that is very different from what can be done with a Davis quadrant. It reveals one of the significant improvements of the Elton's quadrant over the former instrument.\n\nSet the label to a position that will put the object to be measured within the range of the instrument. Observe the object through the eye vane so that the object touches the upper edge of the shield while using the azimuth tube to ensure that the frame is vertical. Move the index arm so that the shield's horizontal tube indicates that the shield is precisely vertical. This sets the instrument and the angle can be read on the arc.\n\nThe Elton's quadrant is not very well known as a navigation instrument. It was used, though to what degree is not known. Elton had the misfortune to invent his instrument in the same period of time as the octant. In fact, John Hadley published details on his octant prior to Elton's article in the same volume of the Philosophical Transactions (article 37 vs 48).\n\nGiven that Elton's quadrant was roughly as complex as an octant in construction, there would not likely be a significant advantage in price. The octant was an easier instrument to use and Hadley had supported the use of artificial horizons on the octant in the form of spirit levels. This would have given no advantage to Elton's instrument. In addition, there were many other instruments competing for the attention of navigators in this period. In the end, the Hadley octant and later sextant took precedence as instruments for navigators.\n\n"}
{"id": "12150194", "url": "https://en.wikipedia.org/wiki?curid=12150194", "title": "English lexicology and lexicography", "text": "English lexicology and lexicography\n\nEnglish lexicology and lexicography is that field in English language studies which examines English word-formation, the evolution of vocabulary and the composition of English dictionaries.\n\n"}
{"id": "30045329", "url": "https://en.wikipedia.org/wiki?curid=30045329", "title": "Ernst Steinhoff", "text": "Ernst Steinhoff\n\nErnst August Wilhelm Steinhoff (February 11, 1908 – December 2, 1987) was a rocket scientist and member of the \"von Braun rocket group\", at the Peenemünde Army Research Center (1939-1945). Ernst Steinhoff saw National Socialist doctrines as \"ideals\" and became a member of the NSDAP in May 1937. He was a glider pilot, holding distance records, and had the honorary Luftwaffe rank of \"Flight Captain\".\n\nErnst Steinhoff earned his PhD at the Darmstadt University of Technology in 1940 with a dissertation on aviation instruments.\n\nHis younger brother Friedrich Steinhoff assisted rocket experiments while commanding in 1942. Ernst was among the scientists to surrender and travel to the United States to provide rocketry expertise via Operation Paperclip. Friedrich was captured aboard and committed suicide in a Boston jail before Ernst came to the United States on the first boat, November 16, 1945. with Operation Paperclip and Fort Bliss, Texas (1945-1949). He then moved to Holloman Air Force Base where he also worked closely with White Sands Missile Range in New Mexico. He focused on guidance, control, and range instrumentation throughout his career. He was awarded the Decoration for Exceptional Civilian Service in 1958 for his contributions to the US rocket program. In 1979 he was inducted into the New Mexico International Space Hall of Fame.\n\nSteinhoff is being credited as one of the first pioneers to popularize the concept of space resource utilization for Mars exploration. He became the first chairman of Working Group on Extraterrestrial Resources (WGER).\n"}
{"id": "30891242", "url": "https://en.wikipedia.org/wiki?curid=30891242", "title": "Essex County Natural History Society", "text": "Essex County Natural History Society\n\nThe Essex County Natural History Society (1833–1848) in Salem, Massachusetts, was formed \"for the purpose of promoting the science of natural history.\" It endeavored \"to form a complete collection of natural productions, curiosities. &c, particularly of this county; and, to form a library of standard books on the natural sciences.\" The society incorporated in 1836; Andrew Nichols, William Oakes, and William Prescott served as signatories. Other members included Samuel B. Buttrick, Samuel P. Fowler, John M. Ives, John C. Lee, George Osgood, Charles G. Page, Gardner B. Perry, George Dean Phippen, William P. Richardson, John Lewis Russell, Henry Wheatland. By 1836 some 100 members belonged to the society. In Salem its \"cabinets and library were first deposited in Essex Place, then in Franklin Building, then in Chase's Building, Washington Street, and finally removed to Pickman Place, in 1842.\" In 1848 the society merged with the Essex Historical Society to form the Essex Institute.\n\n\n\n"}
{"id": "5122134", "url": "https://en.wikipedia.org/wiki?curid=5122134", "title": "Evolution of the Vertebrates", "text": "Evolution of the Vertebrates\n\nEvolution of the Vertebrates, subtitled \"A History of the Backboned Animals Through Time\" is a basic paleontology textbook by Edwin H. Colbert, published by John Wiley & Sons.\n\nThe first and second editions (1955 and 1969) provide an overview of the entire range of vertebrate evolution, and are illustrated by the distinctive drawings of \"Lois Darling\". The style of writing is very light and readable, and technical concepts kept to a minimum. In the book vertebrate evolution is studied utilizing comparative anatomy & functional morphology of existing vertebrates, and fossil records. The book is considered a classic and has been used very frequently as a college-level or university introductory level text on the subjects of basic paleontology and vertebrate evolution.\n\nThese editions predate the cladistic revolution and so contain a number of chapters and sections dedicated to paraphyletic taxa (Labyrinthodonts, Thecodonts, Condylarths, etc.) which nevertheless constituted an important part of the understanding of the time.\n\nThe latest edition, the \"fifth\" edition, was cowritten with Michael Morales and Eli C. Minkoff, and has been revised to incorporate recent discoveries and current developments in the field of vertebrate evolution. This new addition includes entirely new sections. Some examples of these are conodonts, primates, and dinosaurs. Some new topics that the fifth edition discuss are:\n\n\nThe fifth edition has generally received praise from both professors and students using this textbook on the college level. It has, however, received mild criticism for its out-of-date material. Others however argue that the broad scope of the edits in the fifth edition make up for any generalizations pertaining to specific details related to geological palentology.\n\n"}
{"id": "47543414", "url": "https://en.wikipedia.org/wiki?curid=47543414", "title": "FFAT motif", "text": "FFAT motif\n\nA FFAT motif (FFAT being an acronym for two phenylalanines (FF) in an Acidic Tract ) is a protein sequence motif of six defined amino acids plus neighbouring residues that binds to proteins in the VAP protein family.\n\nThe classic FFAT motif was defined on the basis of finding the sequence EFFDAxE in 16 different eukaryotic cytoplasmic proteins (where E = glutamate, F = phenylalanine, D = aspartate, A = alanine, x = any amino acid, according to the single letter amino acid code (see Table of standard amino acid abbreviations and properties in Amino Acids). In all cases, the core sequence is surrounded by regions that are rich in acids D and E (hence negatively charged), and also in residues that can acquire negative charge by phosphorylation (S and T - serine and threonine). This is the Acidic Tract of the name FFAT, and it is mainly found amino-terminal to the core motif, but also extends to the carboxy-terminal side to some extent. Also, this immediate region is almost completely devoid of basic residues (K and R - lysine and arginine).\n\nThe finding of these sequences on its own implied an important functional relationship because 13 of the 16 proteins shared the same overall function: they are lipid transfer proteins (LTPs). These include several homologs of oxysterol binding protein (OSBP, both in humans and in baker's yeast, as well as ceramide transfer protein (CERT) - previously known as Goodpasture's antigen binding protein (GPBP) or Collagen type IV alpha-3-binding protein (COL4A3BP), and Nir2/RdgB. The significance of this was enhanced by the linked finding in a proteomics study published in Nature (journal), where all three of proteins in baker's yeast with FFAT motifs (Osh1p/Swh1p, Osh2p and Opi1p) were in protein complexes that contain Scs2p, the baker's yeast homolog of VAPA and VAPB. Complexes had also been reported between OSBP and VAPA.\n\nThis led to a simple hypothesis that VAP directly binds FFAT motifs, which was tested by biochemical interaction between purified components, and was later confirmed by structural analysis of VAP-FFAT complexes, both by X-ray crystallography and by NMR. The crystallography study indicated that the parts of FFAT that interact most strongly with VAP were F2 and A5, each binding in highly conserved pockets in the major sperm protein domain of VAP, which has a large electropositive patch nearby. The NMR study indicated a “fly-casting” process, whereby a weak non-specific electrostatic interaction between VAP and the acidic tract precedes the more specific high affinity interaction with EFFDAxE.\n\nHumans have three VAPs: VAPA, VAPB and MOSPD2. All of these share a conserved major sperm protein domain in the cytoplasm anchored to the endoplasmic reticulum membrane by a largely unstructured linker leading to a transmembrane domain. MOSPD2 additionally at its amino-terminus has a lipid transfer domain in the CRAL/TRIO domain family. The main yeast homolog is Scs2p, which has the same domain architecture as VAPA and VAPB, and is also an integral membrane protein of the endoplasmic reticulum.\n\nMany of the proteins with FFAT motifs were previously not known to be targeted to the endoplasmic reticulum, with the exception of OSBP, and PITPNM1 (the fly homologue of which is called RdgB). Instead, they were known for their localization to other sites especially the \"trans\" Golgi network (OSBP, Osh1p and CERT) and the plasma membrane (Osh2p, Osh3p). The discovery that these proteins also targeted the endoplasmic reticulum led to a far more detailed analysis of their targeting, and revealed that all the FFAT-containing lipid transfer proteins are present at both the endoplasmic reticulum and their other target Trans Golgi network or plasma membrane) at the same time, which can only be achieved by their targeting to membrane contact sites. This discovery has turned out to apply to many other lipid transfer proteins, even those that do not contain FFAT motifs. This strongly suggests that intracellular lipid traffic takes place across membrane contact sites.\n\nAt the very inception of the original, highly restricted definition (EFFDAxE), it was already evident that other amino acids could substitute at certain positions in the FFAT motifs of other homologs of OSBP, CERT and PITPNM1, in particular Y (tyrosine) in place of F at positions 2 and more so 3, also H (histidine) at position 3, and C (cysteine) or V (valine) at position 5. A substituted motif was used for the crystal structure. Subsequently, other proteins have been found in variants of FFAT motifs with quite divergent residues, including K (lysine) at position 3 in protrudin. An attempt was made to rank FFAT-like sequences by scoring substitutions at all 6 positions of the core motif and the number of nearby acidic residues (DEST). Variant, FFAT-like motifs were described in >10 new proteins, in particular in the A-kinase anchor proteins (AKAPs) AKAP3 and AKAP11 that scaffold protein kinase A and many interactors. This finding has since been confirmed by finding several members of the AKAP family and protein kinase A family in protein complexes with VAPB. This indicates that cAMP signalling is yet another cellular activity involving small molecules that is regulated at membrane contact sites, along with lipid and calcium ion traffic.\n"}
{"id": "52565674", "url": "https://en.wikipedia.org/wiki?curid=52565674", "title": "Galaxy H-Alpha Fabry-Perot System", "text": "Galaxy H-Alpha Fabry-Perot System\n\nThe Galaxy Hα Fabry-Perot System for WHT (GHaFaS) is an astronomical instrument installed on the 4.2 metre William Herschel Telescope (WHT) at Roque de los Muchachos Observatory on the Canary island of La Palma. First light was on 6 July 2007. Its name is a play on the acronym: Galaxy Hα Fabry-Perot System and the Spanish word \"gafas\" meaning spectacles. It produces maps, in intensity and velocity, of extended objects in the sky (which could be external galaxies, star forming regions in the Galaxy, planetary nebulae, or supernova remnants, as examples), which radiate in the H-alpha line, emitted by ionized hydrogen in interstellar space. It can also be used for a variety of other lines but these will not be covered in this short description.\n\nThe possibility to detect the emission line of interstellar neutral hydrogen at 21 centimeters wavelength revolutioned astronomy in the second half of the 20 century, as it gave us a powerful tool to explore the structure and evolution of galaxies and the star formation within them. A second revolution came when, at millimetre wavelengths, molecular hydrogen could be detected and measured in directly using, above all, the emission from the lines of the CO molecule. Surprisingly the emission from the third phase of interstellar hydrogen, the ionized phase, which is at optical wavelengths, has hadless attention. The basic aim of GHaFaS is to make up for lost time by taking H-alpha velocity fields of galaxies at highspatial and spectral resolution.\n\nThe performance of GHaFaS is comparable to that of the largest radiotelescope producing 21 cm maps in atomic hydrogen: the VLA. Over a field of 3.4 arcminutes in diameter it produces a map of ionized hydrogen emission with a nominal velocity resolution of 5 km/s and an angular resolution limited by the \"seeing\" due to atmospheric turbulence of less than 1 arcsecond when used on the WHT. These values are similar to the best figures obtainable with the VLA. However a good quality map can be obtained in half a night's observing with GHaFaS, which is considerably more time efficient than that on the VLA. Comparison with ALMA, the best system for observing molecular hydrogen, is not so easy. ALMA produces maps at considerably higher angular resolution, but over a much smaller field. We could summarize by saying that at the moment GHaFaS is best suited to observing \"local\" galaxies, out of 100 Mpc distance, while ALMA is superior at intermediate and high redshift, in terms of angular and velocity resolutions.\n\nGHaFaS is very well suited to exploring the fine details of the internal kinematics of galaxies, as well as any phenomena related to the formation of high mass stars and their surroundings. It has been used to make the best measurements to date of the corotation radii of the density wave systems related to galaxy bars, and to explore the initial phases of the huge superbubbles caused by the combined stellar winds and supernovae of the OB associations of massive young stars, among a variety of its observing achievements.\n"}
{"id": "40605368", "url": "https://en.wikipedia.org/wiki?curid=40605368", "title": "George Büchi", "text": "George Büchi\n\nGeorge Hermann Büchi (August 1, 1921 – August 28, 1998) was organic chemist and professor at the Massachusetts Institute of Technology. \"Paternò's reaction\", known since the early twentieth century, was renamed to the \"Paternò–Büchi reaction\" based on enhancements made to it by Büchi's research group.\n"}
{"id": "14013346", "url": "https://en.wikipedia.org/wiki?curid=14013346", "title": "Haast Schist", "text": "Haast Schist\n\nThe Haast Schist which contains both the Alpine and Otago Schist is a metamorphic unit in New Zealand's South Island. It extends from Central Otago, along the eastern side of the Alpine Fault to Cook Strait. There are also isolated outcrops of the Haast Schist within the central North Island. The schists were named after Haast Pass on the West Coast. The Haast Schist can be divided geographically from north to south into the Kaimanawa, Terawhiti, Marlborough, Alpine, Otago and Chatham schist. \n\nThe metamorphic grade progresses from greenschist, biotite, garnet and finally orthoclase. Myrmekitic textures occur within oligoclase within the garnet zone. The schist's protoliths were the greywacke and argillite of the Caples Terrane and Torlesse Composite Terrane. The schists were originally brought to the surface of the Earth's crust in the Cretaceous and again during the Kaikoura Orogeny along the Alpine Fault. Pounamu (Jade) is found as isolated pods in the higher metamorphic grades near the Alpine Fault.\n\n"}
{"id": "146230", "url": "https://en.wikipedia.org/wiki?curid=146230", "title": "Iceberg", "text": "Iceberg\n\nAn iceberg or ice mountain is a large piece of freshwater ice that has broken off a glacier or an ice shelf and is floating freely in open water. It may subsequently become frozen into pack ice (one form of sea ice). As it drifts into shallower waters, it may come into contact with the seabed, a phenomenon called seabed gouging by ice. About 90 percent of an iceberg is below the surface of the water.\n\nIcebergs are possible on Earth because of an unusual property of water: It is less dense in its solid state than its liquid state. Oceans on some other planets consisting of other types of liquids, such as methane, cannot have icebergs, precisely because the frozen substance would sink.\n\nThe word \"iceberg\" is a partial loan translation from the Dutch word \"ijsberg\", literally meaning \" ice mountain\", cognate to Danish \"isbjerg\", German \"Eisberg\", Low Saxon \"Iesbarg\" and Swedish \"isberg\".\n\nBecause the density of pure ice is about 920 kg/m, and that of seawater about 1025 kg/m, typically about one-tenth of the volume of an iceberg is above water (which follows from Archimedes's Principle of buoyancy). The shape of the underwater portion can be difficult to judge by looking at the portion above the surface. This has led to the expression \"tip of the iceberg\", for a problem or difficulty that is only a small manifestation of a larger problem.\n\nThe tops of icebergs typically range from above sea level and weigh . The largest known iceberg in the North Atlantic was above sea level, reported by the USCG icebreaker \"East Wind\" in 1958, making it the height of a 55-story building. These icebergs originate from the glaciers of western Greenland and may have interior temperatures of .\n\nWinds and currents tend to move icebergs close to the coast. The largest icebergs recorded have been calved, or broken off, from the Ross Ice Shelf of Antarctica. Iceberg B-15, photographed by satellite in 2000, measured , with a surface area of . The largest iceberg on record was an Antarctic tabular iceberg of over [] sighted west of Scott Island, in the South Pacific Ocean, by the USS \"Glacier\" on November 12, 1956. This iceberg was larger than Belgium.\n\nA small iceberg less than 2 meters (6.6 feet) across that floats with less than 1 meter (3.3 feet) showing above water is called a growler. It is smaller than a bergy bit, which is usually less than 5 meters (15 feet) in size and are generally spawned from disintegrating icebergs. \n\nAs a piece of iceberg ice melts, it produces a fizzing sound called the \"Bergie Seltzer\". This sound results when the water-ice interface reaches compressed air bubbles trapped in the ice. As this happens, each bubble bursts, making a \"popping\" sound. The bubbles contain air trapped in snow layers very early in the history of the ice, that eventually got buried to a given depth (up to several kilometers) and pressurized as it transformed into firn then to glacial ice.\n\n\nIn addition to size classification, icebergs can be classified on the basis of their shape. The two basic types of iceberg forms are \"tabular\" and \"non-tabular\". Tabular icebergs have steep sides and a flat top, much like a plateau, with a length-to-height ratio of more than 5:1. This type of iceberg, also known as an \"ice island\", can be quite large, as in the case of Pobeda Ice Island. Antarctic icebergs formed by breaking off from an ice shelf, such as the Ross Ice Shelf or Filchner-Ronne Ice Shelf, are typically tabular. The largest icebergs in the world are formed this way.\n\nNon-tabular icebergs have different shapes and include:\n\n\nIcebergs are monitored worldwide by the U.S. National Ice Center (NIC), established in 1995, which produces analyses and forecasts of Arctic, Antarctic, Great Lakes and Chesapeake Bay ice conditions. More than 95% of the data used in its sea ice analyses are derived from the remote sensors on polar-orbiting satellites that survey these remote regions of the Earth.\n\nThe NIC is the only organization that names and tracks all Antarctic Icebergs. It assigns each iceberg larger than along at least one axis a name composed of a letter indicating its point of origin and a running number. The letters used are as follows:\n\nIceberg B15 calved from the Ross Ice Shelf in 2000 and initially had an area of . It broke apart in November 2002. The largest remaining piece of it, Iceberg B-15A, with an area of , was still the largest iceberg on Earth until it ran aground and split into several pieces October 27, 2005, an event that was observed by seismographs both on the iceberg and across Antarctica. It has been hypothesized that this breakup may also have been abetted by ocean swell generated by an Alaskan storm 6 days earlier and away.\n\nIn the 20th century, several scientific bodies were established to study and monitor the icebergs. The International Ice Patrol, formed in 1914 in response to the April 1912 sinking of the \"Titanic\", which killed 1,518 of its 2,223 passengers and crew, monitors iceberg dangers near the Grand Banks of Newfoundland and provides the \"limits of all known ice\" in that vicinity to the maritime community.\n\nBefore the early 1910s, there was no system in place to track icebergs to guard ships against collisions, most likely because they weren't considered a serious threat as ships had managed to survive even direct crashes. In 1907, \"SS Kronprinz Wilhelm\", a German liner, had rammed an iceberg and suffered a crushed bow, but was still able to complete her voyage. The April 1912 sinking of the \"Titanic\" however changed all that, and created the demand for a system to observe icebergs. For the remainder of the ice season of that year, the United States Navy patrolled the waters and monitored ice flow. In November 1913, the International Conference on the Safety of Life at Sea met in London to devise a more permanent system of observing icebergs. Within three months the participating maritime nations had formed the International Ice Patrol (IIP). The goal of the IIP was to collect data on meteorology and oceanography to measure currents, ice-flow, ocean temperature, and salinity levels. They published their first records in 1921, which allowed for a year-by-year comparison of iceberg movement.\n\nNew technologies monitor icebergs. Aerial surveillance of the seas in the early 1930s allowed for the development of charter systems that could accurately detail the ocean currents and iceberg locations. In 1945, experiments tested the effectiveness of radar in detecting icebergs. A decade later, oceanographic monitoring outposts were established for the purpose of collecting data; these outposts continue to serve in environmental study. A computer was first installed on a ship for the purpose of oceanographic monitoring in 1964, which allowed for a faster evaluation of data. By the 1970s, icebreaking ships were equipped with automatic transmissions of satellite photographs of ice in Antarctica. Systems for optical satellites had been developed but were still limited by weather conditions. In the 1980s, drifting buoys were used in Antarctic waters for oceanographic and climate research. They are equipped with sensors that measure ocean temperature and currents.\nSide looking airborne radar (SLAR) made it possible to acquire images regardless of weather conditions. On November 4, 1995, Canada launched RADARSAT-1. Developed by the Canadian Space Agency, it provides images of Earth for scientific and commercial purposes. This system was the first to use synthetic aperture radar (SAR), which sends microwave energy to the ocean surface and records the reflections to track icebergs. The European Space Agency launched ENVISAT (an observation satellite that orbits the Earth's poles) on March 1, 2002. ENVISAT employs advanced synthetic aperture radar (ASAR) technology, which can detect changes in surface height accurately. The Canadian Space Agency launched RADARSAT-2 in December 2007, which uses SAR and multi-polarization modes and follows the same orbit path as RADARSAT-1.\n\n"}
{"id": "8370136", "url": "https://en.wikipedia.org/wiki?curid=8370136", "title": "Ico-D", "text": "Ico-D\n\nThe International Council of Design (ico-D; formerly known as International Council of Communication Design or Icograda, which was formerly an initialism for International Council of Graphic Design Associations) is a world organisation for design professionals. ico-D was founded in London in 1963 and celebrated its 50th anniversary on 27 April 2013. It is a non-profit, non-partisan organisation and a \"member-based network of independent organisations and stakeholders working within the multidisciplinary scope of design.\"\n\nico-D members include professional design organisations, design promotion bodies, design media, design education institutions and individuals with a vested interest in professional design. Design media are affiliated through the International Design Media Network (IDMN). Individuals are affiliated through the ico-D's Friends Network which was established in 1991.\n\nico-D coordinates best international practices for communication design. It maintains affiliations with other international organisations such as Cumulus Association, IFRRO, ISO, UNESCO, UNIDO, WIPO, and ECOSOC.\n\nPeter Kneebone proposed the idea to establish an international organisation for graphic design and was involved in Icograda's founding. The Society of Industrial Artists (which changed its name in 1963 to the Society of Industrial Artists & Designers, and is now the Chartered Society of Designers, or CSD) set up a working group under the chairmanship of Willy de Majo, to promote the creation of an organisation to represent internationally the many professional graphic design associations throughout the world. No such organisation existed. The profession was rapidly growing in importance, and also attempting to clarify its identity and objectives. It was involved in increasingly complex social and technological situations. National associations were developing, but international dialogue and action were intermittent and uncoordinated. It was important to create links between the professional associations in all countries, and between the profession and the rest of the world.\n\nThe inaugural conference of Icograda took place at Watney House, London, from 26–28 April 1963, attended by delegates from 28 associations in 17 European countries. The meeting was chaired by H.K. Henrion and supported by Kneebone as secretary. On 27 April, the meeting agreed to formally establish Icograda. Proposals that were ratified include the development and drafts of a \"Code of Ethics and Professional Practice\", a \"Code of Contract and Conditions of Engagement for Graphic Designers\", \"Rules and Regulations for International Graphic Design Competitions\", an \"International Directory of Organizations Concerned with Graphic Design\" and the publication of a News Bulletin.\n\nThe first Executive Board was elected at the meeting in London, composed of Willy de Majo (Great Britain, President), Wim H. Crouwel (The Netherlands, Secretary General), Martin Gavier (Sweden, Treasurer), Peter Hatch (Great Britain, Vice President), Hans Neuburg (Switzerland, Vice President), Jukka Pellinen (Finland, Vice President), D. Stojannovic-Sip (Yugoslavia, Vice President), John Tandy (Great Britain, Member), Pieter Brattinga (The Netherlands, Member) and Paul Schwarz (The Netherlands, Honorary Treasurer). This Executive Board served from 1963 to 1966.\n\nAt the same meeting, the first Icograda Congress and General Assembly in 1964 in Zurich was called in order to establish the aims and structures upon which the future of the organisation rested.\n\nOn 27 April 2013, Icograda celebrated its 50th anniversary and its 25th General Assembly.\n\nThe first Icograda Congress and General Assembly took place in 1964 in Zurich. It was attended by about 200 designers from 17 countries and 23 associations. The theme of the Congress was Commercial Artist or Graphic Designer\". The Congress opened with the reading of a message by Prince Philip, Duke of Edinburgh who wrote: “Every day designers of all kinds are becoming responsible for a greater proportion of man's environment. Almost everything that we see and use that was not made by the Almighty has come from some designer's drawing board. This is a very heavy responsibility and every effort by designers to improve standards, to encourage proper training and to develop a sense of social awareness is to be welcomed.” At the General Assembly, delegates Icograda's aims and objectives, and it was further agreed to establish an audio-visual archive and library, to publish a news bulletin and to award student scholarships. Jenny Toynbee from Edinburgh School of Art won the first student scholarship. Ernest Hoch's proposals for a unified system of typographic measurement was accepted for further development.\n\nIn 1965 Icograda establishes the Signs and Symbols Commission. It first collaborates closely with the International Chamber of Commerce and then with the International Organization for Standardization (ISO). In the same year Icograda publishes \"Graphic journalism: Catalogue of Magazines and Annuals of Graphic Design and Allied Subjects\" featuring around 300 journals from 30 countries.\n\nIn April 1966 the first Icograda International Student Project was judged in Belgrade. The theme was 'Public information signs' and the jury included Abram Games, Josef Müller-Brockmann, Paul Rand, Masaru Katzumie and Ivan Picelj. The second Icogarada Congress, 'Graphic Design and Visual Communication Technology', and General Assembly were then held from 11–16 July 1966 in Bled, then in Yugoslavia. R Buckminster Fuller was one of the main speakers at the Congress. At the General Assembly, four key policy documents were ratified: \"Rules and Regulations for International Graphic Design Competitions\", the \"Honorarium for Judges of International Competitions\" document and the revised \"Code of Ethics and Professional Practice\", and the \"Code of Contract and Conditions of Engagement for Graphic Designers\". , head of design at Phillips in Eindhoven, was elected President and Pieter Brattinga from the Netherlands, Secretary-General. The first student seminar was also held in Bled from 11–13 July 1966 and the theme was 'Breaking the Language Barrier with Signs and Symbols'. In addition, the production of \"Equality of Man\", a film project for students and young people in support of the United Nations Human Rights Year, was the first of several Icograda collaborations with International Animated Film Association (ASIFA)\n\nThe third Icograda Congress 'Design Destinations in a Changing World' was held in 1968 in Eindhoven. An exhibition of Belgian graphics was organised by Michel Olyff at the same time and the entire congress decamped to Brussels for a day. \"Sachez que tout en ce monde n’est que signes et signes de signes\" (beware that all in the world is only signs and signs of signs), were the words of Marcel Schwab quoted by Alderman van der Harten in opening the congress. Der Harten went on to make observations about communication, information systems, symbols, codes and signals, \"but away from the congress there were different signs that could not be ignored. A government minister failed to appear to make an address on the third morning of the congress; he was in an emergency cabinet meeting.\"\n\nIcograda has always been totally non-political but the 1968 Congress coincided with the end of the Prague Spring. As the tanks rolled into Czechoslovakia the congress paused in a moment of silent respect. As the crisis deepened over the next few days Netherlanders offered hospitality to those wishing to stay on in the country for a few days, and many of the delegates marched, protested and petitioned their embassies as military repression threatened the freedoms of friends and colleagues. Whilst the SIAD had facilitated the inaugural meeting in London five years before, it must be remembered that most of the founders of Icograda were central Europeans, an assortment of Yugoslav, Austrian, German, Polish and Hungarian political émigrés who had made their home in the UK to escape persecution. The response of the Congress was therefore only to be expected. Some presentations from Teunissen van Manen, Richard Gregory, Benno Wissing, Jerome Gould, and Massimo Vignelli, were thus overshadowed by political events.\n\nAt the 3rd Icograda General Assembly FHK Henrion was elected President and Pieter Brattinga re-elected Secretary-General. In addition, the book \"ICOGRADA The First Five Years\" by Wynkyn de Worde, was published and Icograda was commissioned by UNESCO to produce their first series of slides on graphic design.\n\nIn 1971 the fourth General Assembly was held in Vienna, John Halas was elected President, and Marijke Singer Secretary-General. The newly established President's Trophy was awarded to Peter Kneebone. In 1970, the congress and exhibition 'The Visual Communicator and the Learning Industry' took place in Vienna. Despite a raft of excellent speakers, there was considerable dissatisfaction with the way in which the event had been organised, in particular the commercial nature of the exhibition, and the congress effectively turned on itself for half a day with an open discussion about how matters should be arranged in the future. Memorably Herb Rohn from Carbondale, Illinois, standing on a table, demanded metaphorically that \"the windows of the Hofburg be thrown open to let some fresh air into this place where there has been none this week.\"\n\nThe first issue of 'icographic', Icograda's biannual magazine, was published in 1971. Founded by John Halas, it was edited by Patrick Burke. The complete ten-year cycle was tapped out on an IBM Selectric typewriter, with many idiosyncratic but significant design issues being presented to a widening design audience.\n\nIn 1972 Icograda achieved consultative status with UNESCO in Paris. The fifth General Assembly and the Symposium 'Towards a Working Congress on Education' were held in London. Kurt Weidemann was elected President, Marijke Singer Secretary-General, and the President's Trophy was awarded to Patrick Wallis Burke.\n\nIn 1973 the first Icograda Student Seminar was held in London, chaired by FHK Henrion. The London seminars were a regular feature of the design calendar until 1999. After Henrion died in 1990, the event was chaired by Alan Fletcher, and for its final three years by Mervyn Kurlansky.\n\nIn 1974 the sixth Icograda General Assembly took place in Krefeld, Germany. Walter Jungkind was elected Icograda president, the first non-European resident to hold the office. He was also the first whose main career was in design education. Marijke Singer was again elected Secretary-General and Ernest Hoch won the President's Trophy. The assembly was followed by the 'Edugra' Congress held in Neuss, across the Rhine from Düsseldorf. The theme was 'Graphic Design Education'.\n\nIn 1975 the Icograda Edugraphic '75 International Conference was held in Edmonton with the theme 'Education for Graphic Design/Graphic Design for Education'. In 1976 a symposium 'Design for Need', jointly sponsored by Icograda and ICSID was held at the Royal College of Art in London. Following a study undertaken for UNESCO, an Icograda travelling exhibition 'The Image of Women in Advertising' opened in Eindhoven.\n\nThe 1977 Zurich Congress − 'Graphic Design for Social Communication' − was complimented by an entire issue of \"Graphis\" magazine devoted to the Congress exhibition held at the Kunstgewerbe Museum Zurich. ICSID president Kenji Ekuan was one of the keynote speakers. The congress was followed by a seminar in Lausanne − 'A Town and its Image' − and the General Assembly at which Flemming Ljorring was elected President and Peter Kneebone Secretary-General. The President's Trophy was awarded to Kenneth and Shelagh Adshead for their work on the audio-visual archive.\n\nIn 1978 Icograda met at Evanston outside Chicago for the 'Design that Works' congress devoted to design evaluation; how designers could prove the commercial and social value of what they did. Speakers included Josef Müller-Brockmann, Milton Glaser and Massimo Vignelli. The eighth general assembly took place the following year in Paris at the Centre Pompidou. Peter Kneebone was elected President, Keith Murgatroyd Secretary-General and Bob Vogele was awarded the President's Trophy for his work organising the event in Chicago. The assembly was preceded by a seminar − 'Is Graphic Design a Reflection of Society or a Factor in its Evolution?'\n\nIn 1980, Mauro Kunst organised the first Icograda Latin American conference in Guadalajara, Mexico. In the same year, \"World design sources directory 1980 = Répertoire des sources d'information en design 1980\", edited by Centre de Creation Industrielle, was published on behalf of Icograda and ICSID.\n\nThe first Icograda, ICSID and IFI Joint Congress took place in Helsinki from 1–8 August 1981, an event that had been under discussion since 1977. The theme was 'Design Integration'. Three separate general assemblies followed the congress. At the 9th Icograda General Assembly, Stig Hogdal was elected President, Marijke Singer was once again elected Secretary-General and the President's Trophy was awarded to Geoffrey Bensusan for his work on the \"Icograda News Bulletin\". The first Icograda-Philips design award was awarded to Benoit de Pierrepont and S S Satie focusing on the theme \"The design of instructions and warnings, and the related problems of society in any area of human activity\". Also in 1981, Icograda organised a poster competition on behalf of UNESCO to celebrate the International Year of Disabled Persons.\n\nIn 1982 the first of six issues of \"icographic/volume 2\" was published on behalf of Icograda by Mobilia Press, with Jorge Frascara as associate editor. Each issue was devoted to a single theme and abstracts were included in four languages. Also in 1982, Haig David-West organised the first African regional meeting in Port Harcourt, Nigeria and in 1983, Haig David-West (the first ever elected Icograda Executive board member from Africa) edited the publication \"Dialogue on graphic design problems in Africa\", based on this meeting.\n\nIn 1983 the 10th General Assembly and Icograda Congress '\"Design Interaction\"' were held in Dublin. The keynote address was given by Erskine Childers of the United Nations. In one of the most remarkable addresses ever made to an Icograda congress, Childers mixed thousands of years of history with personal experience as he set out examples where \"I can tell you that again and again down the years I have confronted communication needs for which graphic visuals were indispensable – the only answer\". Setting out a powerful argument for the potential of creating \"rapid economic and social development through a major effort in endogenously researched and programmed use of the visual media leading to – but in that order – literacy\" he called on the delegates to \"offer and apply your centuries-evolved skills and sensitivities to help humanity see – literally see – both its marvelous capacity for progress and its primeval capacity for error, inhumanity, social neglect, even apocalyptic destruction\". Admitting that he was presenting the delegates with challenges that were 'especially heavy' Childers said \"I never knew a good graphic designer who did not explode with creativity and blossom when faced with an ostensibly impossible task... History will once again make its awesome judgement of you – but from now on with greater respect because you cannot any longer be underestimated. You help all of us perceive and place ourselves in life. Now you must help all of us understand so that we can more wisely and assuredly manage all our futures\". Raymond Kyne was elected President and Robert Blaich won the President's Award for his promotion of Icograda-Philips design award. The number of member organisations had by grown to 50. A multidisciplinary design competition on the theme of \"Shu/Collectivity\", held under the auspices of Icograda, ICSID and IFI, and sponsored by the Japan Design Foundation, was won by Charles Owen from the Illinois Institute of Technology.\n\nIn 1985, the 11th Icograda Congress \"The place and influence of graphic design in everyday life\" was organised by Philippe Gentil in Nice, the first time that there were major presentations from India ('design without designers' by people whose needs were too urgent to await intervention by professionals) and Australia (from boomerang to bicentenary); from the design of books in China to street signs in Buenos Aires, and provocatively 'the backside of design' – what happens after the consultant has moved on to new, more rewarding problems. According to Stephen Hitchins (Icograda board member from 1987−1991), Ivan Chermayeff made a long presentation totally devoid of visuals stating that he \"wanted graphics to grow up\" – comparing much of it \"to WC Fields' definition of a virgin: a child about four years old, extremely ugly\". So much of the pleasure Chermayeff claimed that he gained from graphic design was from process rather than product, \"a sensation of feeling something is still wet long after it's dry\". Improvisation, accidents, speed, freshness, and individuality were what mattered to him. Even without pictures, he won many delegates over. At the 11th Icograda General Assembly (also in Nice) Jorge Frascara was elected president. The President's Trophy went to Jan Railich, the chairman of the Brno Biennale. Also in 1985, the \"World Directory of Design Schools and Programmes\" was published jointly by Icograda, ICSID and IFI, and edited by Maarten Regouin, secretary of the Icograda Education Working Group, which also coincided with Icograda's establishment of the Design History and Design Management working groups chaired by Michael Twyman and Abe Rosenfeld respectively.\n\nIn 1986 Icograda Excellence Awards were made for the first time at the Brno and Warsaw Biennales. The recipients were Christof Gassner and Henryk Tomaszewski.\n\nIn 1987, Amrik Kalsi, Jorge Frascara and Peter Kneebone organised the \"Graphic Design for Development Seminar\" on behalf of Icograda and UNESCO. It was hosted in Nairobi, Kenya, from 6–10 July and attracted 91 participants from 14 countries.\n\nThe 2nd Joint Congress of Icograda, ICSID, IFI \"Design 87\" − with as its theme 'Design Response' − was held in Amsterdam from 16–20 August 1987. At the 12th Icograda General Assembly which took place from 21–22 August (also in Amsterdam), Niko Spelbrink was elected president and Mary V. Mullin became Secretary-General (and Director of the Icograda Secretariat – a position she held until 1999). Susumu Sakane was awarded the President's Trophy. Uwe Loesch won the first Icograda Excellence Award to be presented at the Lahti Biennale. Icograda published the \"History of Design Bibliography\" edited by Victor Margolin, and \"Projects in Graphic Design Education\" edited by Jorge Frascara.\n\nIn 1988, Nils Tvengsberg organised a special event in Oslo, from 13–15 May, to mark the 25th anniversary of Icograda. All 11 presidents from 1963-onwards attended together with many former board members, partners, friends and supporters. It coincided with Norway's national day and according to Stephen Hitchins, \"it was one of the more remarkable Icograda events in a history of remarkable events\". Notably, Nils Tvengsberg was the first person to serve on the boards of both Icograda and ICSID simultaneously. His goal was to look at the possibility of joining their common interests. It would be over 20 years before this partnership, the International Design Alliance (IDA), would be headquartered in Montreal, contemplating the first IDA joint congress in 2011.\n\nIn 1989 the 13th Congress took place at Tel Aviv University, chaired by Abe Rosenfeld. The theme was 'Graphic Design Through High Technology?' At the General Assembly Simon de Hartog won the President's Award, and Helmut Langer became President.\n\nIn 1990, \"Graphic Design, World Views. A celebration of ICOGRADA’s 25th Anniversary\", edited by Jorge Frascara, designed by Niko Spelbrink, and with a cover designed by Grapus, was published jointly by Icograda and Kodansha, a major book on graphic design that celebrated Icograda's 25th anniversary.\n\nThe Icograda Foundation was established in 1991 for the advancement of worldwide understanding and education through the effective use of graphic design. A limited company, the Foundation was a registered charity funded by corporate sponsorships, individual donations, legacies, and various fundraising activities. Mary Mullin received the President's Award for her work in establishing the Foundation.\n\nIn 1991 the 14th Icograda Congress and General Assembly took place in Montréal, from 25–29 August, attended by around 2,000 delegates. At the Icograda General Assembly (30–31 August 1991), Giancarlo Iliprandi was elected president. Highlights of the Assembly included: the hosting of the 3rd Marijke Singer Memorial Lecture presented by Margaret Catley Carlson, entitled \"Images for the Next Century\"; the Icograda President's Award was bestowed on Mary V. Mullin for the inauguration of the Icograda Foundation and the Friends of Icograda; establishment of the Icograda World Graphic Design Day; establishment of Icograda Steering Committee for the International Design Archive and Research Centre Project; and the establishment of Icograda Friends with seed donations contributed by 68 Japanese founders, chaired by Hiroshi Kojitani. In the same year, the first \"Icograda/IFI/ICSID Joint Newsletter\" was published.\n\nIn 1993 the 3rd Joint Congress of Icograda, ICSID and IFI was held in Glasgow, Ireland from 5–9 September. Stephen Hitchins chaired the organising committee and there were 101 speakers and over 1,000 delegates. Rick Poynor writing in \"Creative Review\" in 2007, said it had been “the most sophisticated and future-orientated discussion of design in the UK for 15 years”. Jeremy Myerson called it \"a watershed event, one of those rare occasions when the design community comes together and presses the pause button, and stops to reflect on what designers do, how things have changed, and where design practice could go in the future... It explored the limits of design. Yet it also opened up new horizons\". To quote Christopher Frayling who rounded off the event, \"Two moments from Design Renaissance will stick in my mind for a long time. One is Victor Papanek quoting in discussion Charles Rennie Mackintosh, who said, 'there is hope in honest error, none in icy perfection'. The other is Yuri Soloviev's story about persuading Eduard Shevardnadze not to go ahead with a town planning scheme in Georgia, while swimming out into the Black Sea, on holiday. I guess the moral of the tale is: if you are up to your neck in it − keep talking, keep persuading, grab the moment when it comes, have a healthy disregard for bureaucracy, and be a strong swimmer. Oh, and as you are swimming, try not to lose your bearings and always remember that the word 'utopia' means 'of no place at all'.\"\n\nAs with all Icograda events, it was an invigorating experience, and it swam out into some uncharted waters. At the 15th General Assembly held in Glasgow immediately after the Congress, Philippe Gentil was elected President. Hiroshi Kojitani received the President's Award for his work in establishing Friends of Icograda.\n\nFrom 23–27 July 1995, the 16th Icograda Congress took place in Lisbon, Portugal, followed by the General Assembly in Porto. The theme of the Congress was \"Shifting Frontiers\". At the 16th Icograda General Assembly, Jose Korn Bruzzone from Chile became President and the Icograda President's Award was presented to Marion Wesel-Henrion and J. Brian Davies, jointly awarded for outstanding work for the Icograda Foundation and the organisation of the Poster Auction. Icograda hosted the International Poster Auction managed by Sotheby's London in May 1996.\n\nIn 1996, Icograda released a policy document \"Digital Immortality: Encapsulating the Work of the World's Top 20th Century Designers for an Icograda Archive in digital form\" aimed to establish a comprehensive archive for Icograda and the work of leading graphic design masters. In the same year, Icograda participated in a meeting of the Icograda/ICSID/IFI working group founded to develop closer working relations between the three organisations which took place from 7–8 December in Copenhagen, Denmark.\n\nThe 17th Icograda Congress and General Assembly took place from 21–23 October in Punte del Este, Uruguay. This was the first Icograda Congress to be hosted in the Southern Hemisphere and attracted ±1 000 delegates. The Congress theme was 'INTERCAMBIOS/EXCHANGES'. At this General Assembly in Punte del Este, the Icograda Assembly joined the ICSID and IFI General Assemblies to unanimously pass a resolution which called for closer contact between the \"three sisters,\" paving the path towards the formation of the International Design Alliance (IDA), which was eventually established in 2005. Guy-A Schockaert from Belgium became President and Mary V. Mullin was again elected Secretary General.\n\nIn early 1999, Mary V. Mullin resigned as the Icograda Secretary General and Director of the Icograda Secretariat after serving 12 years in the position. The Secretariat relocated from London to Brussels under the new leadership of Thierry van Kerm who was appointed as Icograda Director in June of the same year.\n\n\"Sydney Design 99: Viewpoints in Time\", the 4th joint Icograda/ICSID/IFI Congress took place from 26–29 September in Sydney, Australia. Around 1 400 delegates from 45 countries attended the event. The 18th Icograda General Assembly followed immediately after the Congress, 30 September-1 October in Sydney. At the Assembly, David Grossman from Israel become President and Martha Bateman from South Africa was elected as Secretary General (Bateman was only the second person from Africa to serve on the Executive Board and the first to serve in a senior position). At this GA the Council officially separated the positions of Secretary General and Director of the Secretariat (paid position) functions. In addition, the GA ratified a Joint motion of Icograda, ICSID and IFI to establish Design for the World. The Icograda President's Award was presented to Federico Mayor, Director-General of UNESCO for the organisation's long track-record in supporting Icograda's activities.\n\nAt the 17th Icograda General Assembly a motion was ratified to hold a special millennium congress in October 2000 in Seoul, South Korea. The theme was \"Oullim\", meaning 'great harmony'. The event was attended by around 1,600 delegates and was also the first Icograda event to be webcast live on the Internet. The Congress also included the launch of the \"Icograda Education Manifesto 2000\", which was published in 17 languages.\n\nIn 2001, Icograda held regional meetings in Zagreb, Croatia in April, and in La Habana, Cuba in June. The meeting in Cuba coincided with the establishment of a new series of seminars entitled \"Design Perspectives\" which attracted 335 delegates from 23 countries. In September 2001, the \"Continental Shift 2001: World Design Convergence\" Congress took place in Johannesburg, South Africa. This was the first time that Icograda hosted a congress in Africa and also the first joint Icograda/IFI Congress. The Congress opened on 11 September, only hours after the 9/11 terror attacks in the USA. At the 19th Icograda General Assembly (15–16 September, Johannesburg), Robert L. Peters (Canada) became President and Tiffany Turkington (South Africa) was elected Secretary General. One of the most important agenda items was the adoption of an Icograda/ICSID Joint Resolution to establish a joint committee to study institutionalised collaboration. In November of the same year, the first Icograda Design Week was held in Melbourne, Australia. The Design Week included a \"Design Perspectives\" seminar and Regional Meeting. The year also saw the publication of \"Masters of the 20th century design: Icograda Hall of Fame (1974-1999)\" (book and CD-ROM), edited and designed by Mervyn Kurlansky, published by Graphis Inc. The book provided an in-depth view of the Icograda Student Seminars which had been held annually in London from 1973 to 1999.\n\nIn March 2002, the Icograda Design Week took place in Vancouver, British Columbia, Canada and consisted of the \"Environs'002: Design Without Borders\" \"Design Perspectives\" seminar and a North American Regional Meeting. Smaller versions of the \"Design Perspectives\" seminar were also hosted in Victoria, Canada, as well as in Seattle, USA. The Icograda Design Week in Brno took place from 17–21 June in the Czech Republic. The event included \"Over the fence: Design in Central and Eastern Europe\" \"Design Perspectives\" seminar, a Regional Meeting attended by delegates from eight countries, the \"Identity/Integrity Icograda Conference\", \"East Meets West, Icograda Student Workshop\", a symposium and inauguration of the Icograda Education Network (IEN) and a workshop to conceptualise the formation of the International Design Media Network (IDMN). The Design Week was further preceded by a joint meeting of the boards of Icograda, ICSID and IFI, resulting in the conceptualisation of the International Design Alliance (IDA) and the Host City Project to establish a joint Icograda/ICSID Secretariat.\n\nIn September of the same year, the Icograda board embarked on a three-week visit to China and Taiwan where they presented a \"Design Perspectives\" seminar in Beijing, \"logo2002: Identity and Communication Conference,\" and met with various Chinese design stakeholders aiming to seed the formation of new professional associations. In addition Icograda presented the \"Branding and Innovalue\" seminar and an Asian Regional Meeting in Taipei, followed by a Design Education Symposium and Student Workshop in Kaohsiung, Taiwan. Furthermore, the Icograda, ICSID and IFI presidents met on several occasions in Hornbaek, Denmark, to develop a Joint Resolution aimed at establishing the International Design Alliance and launching the Host City Project to establish a joint Secretariat in one location.\n\nThe year's activities ended with the hosting of a Regional Meeting, and \"Graphic Design for Social Causes\" workshop co-hosted by Design for the World as well as a \"Design Perspectives\" Seminar in Barcelona, Spain which attracted delegates from 15 countries.\n\nIn January 2003 Icograda initiated two surveys focusing on \"Design for Social Causes\" and \"Members’ interest and activity in sustainability issues\". In March, the Board visited India where they met with various Indian design stakeholders in Mumbai to seed the formation of new associations, followed by further meetings and co-hosting the \"Brands-Identities-Graphics 2003\", Icograda \"Design Perspective\" Seminar in Ahmedabad. The Icograda Education Network Conference and Assembly of Icograda Education Network was held June in Brighton, UK. These events coincided with the signing of an agreement to establish the Icograda Archive at the Design History Research Centre Archives at the University of Brighton, which resulted in 145.11 linear metres of official documentation and publications, including c. 1,500 posters and c. 800 books and journals being transferred to University of Brighton.\n\nThe \"VISUALOGUE: Icograda World Design Congress\" took place from 8–11 October 2003 in Nagoya, Japan, preceded by the \"Icograda Education Network Symposium\". More than 80 speakers and around 3 700 delegates from 48 countries participated in these events. At the 20th Icograda General Assembly which followed the Congress (the first GA to be held in Asia), representatives from 57 members attended in addition to observers from 16 related organisations, with official membership increasing to 80 associations from 57 countries. The GA agenda included the ratification of the Icograda/ICSID \"Hornbaek Joint Resolution\" to establish the International Design Alliance (IDA), and the election of the 2003−2005 Icograda Board, the first in history to represent six continents. Mervyn Kurlansky became President but no candidate was nominated for the Secretary General position.\n\nIn January 2004 the \"Icograda Design Week in Istanbul\" was held in Turkey. The Week's programme included \"6 Alfabe, Icograda Student Workshop\", an IEN Symposium, Regional Meeting, \"Building Bridges: Icograda Design Conference\", \"Design Perspectives Icograda Regional Design Seminar\", as well as the launch of the Icograda Design Media Network (IDMN). This was followed by the Icograda Design Week in São Paulo, Brazil, in April of the same year. The Week's programme included \"The Language of the City Student Workshop\", a Latin American Regional Meeting, \"Design in Latin America Regional Design Seminar\", and the \"Fronteiras! Icograda Design Conference\". In August, Icograda and ICSID hosted a Joint Board Meeting and evaluation of the final six bids for the Joint Secretariat in Essen, Germany. Montreal's bid was selected as most ambitious and beneficial, followed by a site visit by representatives of Icograda and ICSID in September to initiate the final negotiations regarding relocation of the two organisations’ Secretariates to Montreal.\n\nIn January 2005, Icograda and ICSID signed a 10-year contract with Montreal International (MI) to relocate to a shared office in Montreal, sponsored by MI. After six years in Brussels, the Secretariat relocated to Montreal. In February, Thierry Van Kerm resigned and Brenda Sanderson was appointed as Icograda Director, officially moving into the new shared Secretariat in May. During the same time, ICSID and Icograda collaborate on the first international \"Women in Design\" exploratory study as well as on the \"Interdesign on Sustainable Rural Transport – Technology for Developing Countries\", which was hosted in April in Rustenburg, South Africa. The book, \"Worldwide Identity: Inspired Design from Forty Countries\", written and produced by Robert L. Peters, published by Rockport, was also published and INDIGO (International Indigenous Design Network) was conceptualised.\n\n\"Era '05\" was another major collaborative effort by Icograda, ICSID and IFI to stage a fifth Joint Congress. The theme for the main Congress was 'The Changing Role and Challenges of Design' which attracted participation by designers, business leaders, politicians, legal practitioners and social scientists. A milestone in the cooperative efforts between leading design organisations in Denmark, Norway, Sweden and Finland, \"Era 05\" was built on the ideal that as a creative force, design and designers were integral in helping to address the challenges all faced and identifying solutions to cope with a rapidly changing and increasingly complex world. In September, \"Era '05\" began with small-scale pre-congress events hosted in Oslo, Helsinki and Gothenburg. The main Congress took place in Copenhagen and featured 126 speakers from 27 countries and was attended by around 900 delegates. At the 21st Icograda General Assembly that followed, Jacques Lange (South Africa) became president and Lise Vejse Klint (Denmark) was elected Secretary General. The newly elected board was now the most geographically diverse in Icograda's history with members from South Africa, South Korea, Canada, Denmark, Lebanon, USA, Australia and Brazil. Craig Halgreen received the Icograda President's Award for supporting and sustaining Sappi's \"Ideas that Matter\" program.\n\nIn 2006, Icograda co-organised three Design Weeks. In January, \"So Tiny, So Many: Icograda Design Week in Hong Kong\" took place in China and consisted of a student workshop, an evening lecture series and a Regional Meeting. In July, \"Defining design on a changing planet: Icograda Design Week in Seattle\", took place in the USA. The Week included a student workshop that focussed on the UN Millennium Development Goals (in collaboration with United Nations Department of Economic and Social Affairs' Programme on Youth, a North American Regional Meeting, an \"Icograda Design Perspectives\" seminar, \"Defining Design on Changing Planet: Icograda Conference\", as well as the launch of the \"+design\" programme. The \"Icograda Design Week in South Africa\" took place in September in Pretoria and Johannesburg. It included the \"Design for Development Lekgotla\" (in collaboration with ICSID member SABS), the \"Icograda/think Conference 2006\", and the \"IEN Colloquium on Virtual Design Archives\". In May of the same year, the presidents of Icograda, ICSID and IFI met in Montreal for the signing of an IDA joint venture agreement between Icograda and ICSID, together with an agreement with IFI to host the 2011 Joint Congress under the banner of the IDA.\n\nIn 2007, Icograda launched the \"IDA World Design Survey Pilot Project\" which aimed to map design development in several global regions based on a standardised set of indicators. The project was developed with advisory support from UNESCO Centre for Statistics and UNCTAD. Icograda also hosted two major Design Weeks. \"Design Local: Stop at all stations, Icograda Design Week in Mumbai\", India took place from 5–9 February and included a student workshop and international conference. \"Design/Culture: Icograda World Design Congress in La Habana\" was held in Cuba from 20–26 October. It included an education conference at which 119 papers were presented originating from 25 countries (papers can be accessed http://www.icograda.org/events/event_archive/articles1014.htm), as well as the \"Posters for Cultural Diversity\" international poster exhibition, organised in collaboration with Prografica and UNESCO. At the 22nd Icograda General Assembly that followed, Don Ryun Chang (South Korea) became president and Lise Vejse Klint (Denmark) was again elected Secretary General. Highlights from the GA included the ratification of update definition of the profession from ‘Graphic Design’ to ‘Communication Design', approval of revisions to \"Regulations and best practice for organizing design awards competitions\", the introduction of \"Icograda best practice statement on soliciting work from professional communication designers\", substantial revisions to the \"Articles of Incorporation and Bylaws\" to include a ‘one member one vote’ system, and granting of voting rights to Education Members (excluding on professional practice issues), establishment of an IDA Taskforce to explore options for future development, launch of INDIGO (Indigenous Design Network) as part of an IDA portfolio led by Icograda. In addition, the Icograda Foundation Lecture was re-introduced on 25 October at Museo Nacional Bellas Artes, La Habana which included the presentation of the Icograda President's Award which went to Guy-A Schockaert for his tireless advocacy of professional design organisations and his commitment to Icograda, and the presentation of the first Icograda Education Award which was bestowed on Hazel Gamec from the Wanganui School of Design, New Zealand. \n\nIn 2008, Icograda again held two Design Weeks. \"Color Value, Icograda Design Week in Daegu\", South Korea took place in July, and in October \"Multiverso: Icograda Design Week in Torino\" Italy. In the same year, Icograda conducted the first survey of European member organisations which was followed up by a more in-depth study in 2010.\n\nThe \"Mousharaka: Icograda Design Week in Doha\", Qatar, was held from 28 February – 5 March 2009, and included an international conference, student exhibitions and a Regional Meeting. In October, the \"XIN: Icograda World Design Congress in Beijing\" took place China. This was the last dedicated Icograda Congress and was attended by 1 750 delegates from 48 countries. The Congress was followed by the \"Icograda Education Conference\", 29–30 October, Beijing. At the 23rd Icograda General Assembly 125 delegates from 45 countries attended, making it the largest and most representative in Icograda's GA history. Highlights of the Assembly included the ratification of a resolution on Sustainable Practice of Communication Design, as well as the adoption of new Best Practice documents, \"Regulations and Best Practices for Design Exhibitions\" and \"Jury Guidelines and Guidelines for Organising Design Conferences\". Russell Kennedy (Australia) became president and Grégoire Serikoff, (France) was elected Secretary General (he resigned in 2011). The Icograda President's Award went to Pan Gongkai for helping to redefine the direction of art and design education in China, as well as to Robert L. Peters for his many achievements as a Board member and as a member of the founding executive of the International Design Alliance (IDA). The Icograda Education Award was awarded to Ahn Sang-Soo from Hongik University, South Korea.\n\n\nIn 2011, the inaugural IDA Congress replaced the biannual Icograda World Design Congress as well as the Joint Congresses of Icograda, ICSID and IFI which have taken place every six years since 1981. The first IDA Congress took place in Taipei with the theme \"Design at the Edges\". The 24th Icograda General Assembly took place in Taipei from 27–28 October.\n\nico-D is a member of the steering committee that's organising the world's first ever full design dedicated summit named the World Design Summit 2016 which will be hosted in Montreal in October 2016. ico-D also continues to partner with Adobe Education in the Adobe Design Achievement Awards (ADAA).\n\nico-D's Executive Board consists of individuals who are duly nominated and elected by ico-D's Member organisations at the biennial ico-D General Assembly. Members of the Executive Board serve in a volunteer position and donate their time and expertise to further Icograda's mandate. Board meetings are typically held four times a year in different locations around the world, usually in conjunction with regional meetings, seminars or other scheduled design events.\n\n\nSince 1963 and 2016, 26 ico-D boards have donated their leadership skills and time to furthering ico-D's goals and objectives. View a complete list of Executive Board members here.\n\nBeginning in June 2005, the Secretariat for ico-D was headquartered in Montreal's International Quartier district (Quebec, Canada). In June 2015, the Secretariat moved to a new office building on 456 rue de la Gauchetière Ouest.\n\nCurrent Secretariat staff:\n\n\n"}
{"id": "26975015", "url": "https://en.wikipedia.org/wiki?curid=26975015", "title": "India Science Award", "text": "India Science Award\n\nIndia Science Award is the highest and the most prestigious national recognition by the Government of India for outstanding contribution to science. The primary and essential criterion for the award is demonstrated and widely accepted excellence in science. The award covers all areas of research in science including engineering, medicine and agriculture. The prize money is Rs 25 lakhs, and it also carries a citation and a gold medal. The award is announced and presented every year at the Indian Science Congress (ISC).\n\nThe award was instituted by the 10th Prime Minister of India Shri Atal Bihari Vajpayee in 2003. The first award, for the year 2004, was given to a renowned chemist Prof CNR Rao, for his works in solid state and material chemistry, by Prime Minister Manmohan Singh at the inauguration of the 93rd Indian Science Congress on 3 January 2006.\n\nIndia Science Award was launched at the 90th Indian Science Congress on 3 January 2003, held at Bangalore University, by the Prime Minister of India. On 30 June 2003 the Ministry of Science and Technology (India) approved the framework and guidelines of the award. The meeting was attended by 20 eminent scientists, government officials, under the chairmanship of the Minister of Science and Technology. \n\nIndia Science Award is given annually in recognition of distinguished achievements in science, including medicine, engineering and agriculture. The recipient is a scientist, of no age limit, who had made a groundbreaking scientific research that is widely demonstrated and accepted, and the work done primarily in India. Originality and innovatory outputs are more important than mere quantity. Contribution to scientific development of the country has a huge impression. The award is not given to groups or institutions. If more than one nominee are eligible in a year, a maximum of two can share the prize.\n\n"}
{"id": "30480361", "url": "https://en.wikipedia.org/wiki?curid=30480361", "title": "Jackiw–Teitelboim gravity", "text": "Jackiw–Teitelboim gravity\n\nThe \"R\" = \"T\" model, also known as Jackiw–Teitelboim gravity, is a theory of gravity with dilaton coupling in one spatial and one time dimension. It should not be confused with the CGHS model or Liouville gravity. The action is given by\nwhere Φ is the dilaton, formula_2 denotes the covariant derivative and the equation of motion is\nThe metric in this case is more amenable to analytical solutions than the general 3+1D case though a canonical reduction for the latter has recently been obtained. For example, in 1+1D, the metric for the case of two mutually interacting bodies can be solved exactly in terms of the Lambert W function, even with an additional electromagnetic field (see quantum gravity and references for details).\n\n"}
{"id": "20976147", "url": "https://en.wikipedia.org/wiki?curid=20976147", "title": "Johan Peter Holtsmark", "text": "Johan Peter Holtsmark\n\nJohan Peter Holtsmark (13 February 1894 – 10 December 1975) was a Norwegian physicist, who studied spectral line broadening and electron scattering. In 1929, while at the Norwegian Institute of Technology, Holtsmark established acoustics research laboratories, focusing on architectural acoustics and sound insulation. Holtsmark was also a consultant for the Norwegian Broadcasting Corporation (NRK) throughout the 1930s.\n\nTogether with the Swedish physicist Hilding Faxén published Holtsmark a work in 1927 about scattering of electrons in gases. Here they introduced a new, mathematical method based upon partial waves. This is now standard and described in almost every modern book on quantum mechanics.\n\nBetween 1934 and 1937 he led the construction of\na Van de Graaff accelerator at the Norwegian Institute of Technology, which became the first particle accelerator to go into operation in Scandinavia.\n\nHoltsmark was one of the founding fathers of CERN and represented Norway to the European Council for Nuclear Research, which later led into the establishment of the organization itself.\n\nHe was awarded the Fridtjof Nansen Excellent Research Award in 1969, was a fellow of the Norwegian Academy of Science and Letters from 1925 and the Royal Norwegian Society of Sciences and Letters from 1926.\n"}
{"id": "24221301", "url": "https://en.wikipedia.org/wiki?curid=24221301", "title": "Keiichi Aichi", "text": "Keiichi Aichi\n\nAichi was born in Tokyo in 1880 and studied theoretical physics at University of Tokyo. He graduated in 1903 and in 1905 moved to Kyoto where he became an assistant professor at Kyoto University. Between 1908 and 1911 he studied in Germany and in 1912 defended his PhD at Tohoku Imperial University with recommendations from the chancellor. Soon after, he assumed the post of professor at the university's then newly established College of Science. In 1922, he served as an interpreter during the visit of Albert Einstein in Japan. Aichi died from food poisoning in 1923.\n\nHis son was the politician Kiichi Aichi, who served consecutively as Minister for Foreign Affairs and Minister of Finance.\n\n\n\n"}
{"id": "12766287", "url": "https://en.wikipedia.org/wiki?curid=12766287", "title": "Kiyoo Wadati", "text": "Kiyoo Wadati\n\n\n\n"}
{"id": "14965438", "url": "https://en.wikipedia.org/wiki?curid=14965438", "title": "LBQS 1429-008", "text": "LBQS 1429-008\n\nLBQS 1429-008 (QQ 1429−008, QQ 1432−0106, QQQ J1432−0106) is a physical triple quasar. It was the first physical triple discovered.\n"}
{"id": "36714938", "url": "https://en.wikipedia.org/wiki?curid=36714938", "title": "Lagrangian particle tracking", "text": "Lagrangian particle tracking\n\nIn computational fluid dynamics, the Lagrangian particle tracking (or in short LPT method) is a numerical technique for tracking Lagrangian particles within an Eulerian phase. It is also commonly referred to as Discrete Particle Simulation (DPS). Some simulation cases for which this method is applicable are: sprays, small bubbles, dust particles, and is especially optimal for dilute multiphase flows with large Stokes number.\n\n"}
{"id": "23301912", "url": "https://en.wikipedia.org/wiki?curid=23301912", "title": "Limbic resonance", "text": "Limbic resonance\n\nLimbic resonance is the idea that the capacity for sharing deep emotional states arises from the limbic system of the brain. These states include the dopamine circuit-promoted feelings of empathic harmony, and the norepinephrine circuit-originated emotional states of fear, anxiety and anger.\n\nThe concept was advanced in the book \"A General Theory of Love\" (2000), and is one of three interrelated concepts central to the book's premise: that our brain chemistry and nervous systems are measurably affected by those closest to us (limbic resonance); that our systems synchronize with one another in a way that has profound implications for personality and lifelong emotional health (limbic regulation); and that these set patterns can be modified through therapeutic practice (limbic revision).\n\nIn other words, it refers to the capacity for empathy and non-verbal connection that is present in mammals, and that forms the basis of our social connections as well as the foundation for various modes of therapy and healing. According to the authors (Thomas Lewis, M.D, Fari Amini, M.D. and Richard Lannon, M.D.), our nervous systems are not self-contained, but rather demonstrably attuned to those around us with whom we share a close connection. \"Within the effulgence of their new brain, mammals developed a capacity we call 'limbic resonance' — a symphony of mutual exchange and internal adaptation whereby two mammals become attuned to each other's inner states.\"\n\nThis notion of limbic resonance builds on previous formulations and similar ideas. For example, the authors retell at length the notorious experiments of Harry Harlow establishing the importance of physical contact and affection in social and cognitive development of rhesus monkeys. They also make extensive use of subsequent research by Tiffany Field in mother/infant contact, Paul D. MacLean on the triune brain (reptilian, limbic, and neocortex), and the work of G.W. Kraemer.\n\nLewis, Amini and Lannon first make their case by examining a story from the dawn of scientific experimentation in human development—albeit heinously misguided—when in the thirteenth century Frederick II raised a group of infants to be completely cut off from human interaction, other than the most basic care and feeding, so as to discover what language would spontaneously arise in the absence of any communication prompts. The result of this notorious experiment was that the infants, deprived of any human discourse or affection, all died.\n\nThe authors find the hegemony of Freudian theory in the early days of psychology and psychiatry to be almost as harmful as the ideas of Frederick II. They condemn the focus on cerebral insight, and the ideal of a cold, emotionless analyst, as negating the very benefit that psychotherapy can confer by virtue of the empathetic bond and neurological reconditioning that can occur in the course of sustained therapeutic sessions. \"Freud's enviable advantage is that he never seriously undertook to follow his own advice. Many promising young therapists have their responsiveness expunged, as they are taught to be dutifully neutral observers, avoiding emotional contact...But since therapy is limbic relatedness, emotional neutrality drains life out of the process...\"\n\n\"A General Theory of Love\" is scarcely more sympathetic to Dr. Benjamin Spock and his \"monumentally influential volume\" \"Baby and Child Care\", especially given Spock's role in promoting the movement against co-sleeping, or allowing infants to sleep in the same bed as their parents. Lewis, Amini and Lannon cite the research of sleep scientist James McKenna, which seems to suggest that the limbic regulation between sleeping parents and infants is essential to the neurological development of the latter and a major factor in preventing Sudden Infant Death Syndrome (SIDS). \"The temporal unfolding of particular sleep stages and awake periods of the mother and infant become entwined...on a minute to minute basis, throughout the night, much sensory communication is occurring between them.\"\n\nSince the first publication of \"A General Theory of Love\" in 2000, the term limbic resonance has gained popularity with subsequent writers and researchers. The term brings a higher degree of specificity to the ongoing discourse in psychological literature concerning the importance of empathy and relatedness. In \"A handbook of Psychology\" (2003) a clear path is traced from Winnicott 1965 identifying the concept of mother and child as a relational organism or dyad and goes on to examine the interrelation of social and emotional responding with neurological development and the role of the limbic system in regulating response to stress.\n\nLimbic resonance is also referred to as \"empathic resonance\", as in the book \"Empathy in Mental Illness\" (2007), which establishes the centrality of empathy or lack thereof in a range of individual and social pathologies. The authors Farrow and Woodruff cite the work of Maclean, 1985, as establishing that \"Empathy is perhaps the heart of mammalian development, limbic regulation and social organization\", as well as research by Carr et al., 2003, who used fMRI to map brain activity during the observation and imitation of emotional facial expressions, concluding that \"we understand the feelings of others via a mechanism of action representation that shapes emotional content and that our empathic resonance is grounded in the experience of our bodies in action and the emotions associated with specific bodily movements\". Other studies cited examine the link between mirror neurons (activated during such mimicking activity) and the limbic system, such as Chartrand & Bargh, 1999: \"Mirror neurone areas seem to monitor this interdependence, this intimacy, this sense of collective agency that comes out of social interactions and that is tightly linked to the ability to form empathic resonance.\"\n\nLimbic resonance and limbic regulation are also referred to as \"mood contagion\" or \"emotional contagion\" as in the work of Sigal Barsade and colleagues at the Yale School of Management.\nIn \"The Wise Heart\", Buddhist teacher Jack Kornfield echoes the musical metaphor of the original definition of \"limbic resonance\" offered by authors Lewis, Amini and Lannon of \"A General Theory of Love\", and correlates these findings of Western psychology with the tenets of Buddhism: \"Each time we meet another human being and honor their dignity, we help those around us. Their hearts resonate with ours in exactly the same way the strings of an unplucked violin vibrate with the sounds of a violin played nearby. Western psychology has documented this phenomenon of 'mood contagion' or limbic resonance. If a person filled with panic or hatred walks into a room, we feel it immediately, and unless we are very mindful, that person's negative state will begin to overtake our own. When a joyfully expressive person walks into a room, we can feel that state as well.\"\n\nIn March 2010, citing \"A General Theory of Love\", Kevin Slavin referred to limbic resonance in considering the dynamics of Social television. Slavin suggests that the laugh track evolved to provide the audience—alone at home—with a sense that others around them were laughing, and that limbic resonance explains the need for that laughing audience.\n\nLimbic regulation, mood contagion or emotional contagion is the effect of contact with other people upon the development and stability of personality and mood.\n\nIn \"Living a connected life\" (2003), Dr. Kathleen Brehony looks at recent brain research which shows the importance of proximity of others in our development. \"Especially in infancy, but throughout our lives, our physical bodies are influencing and being influenced by others with whom we feel a connection. Scientists call this \"limbic regulation.\"\"\n\nBrehony goes on to describe the parallels between the \"protest/despair\" cycles of an abandoned puppy and human development. Mammals have developed a tendency to experience distraction, anxiety and measurable levels of stress in response to separation from their care-givers and companions, precisely because such separation has historically constituted a threat to their survival. As anyone who has owned a puppy can attest, when left alone it will cry, bark, howl, and seek to rejoin its human or canine companions. If these efforts are unsuccessful and the isolation is prolonged, it will sink into a state of dejection and despair. The marginal effectiveness of placing a ticking clock in the puppy's bed is based on a universal need in mammals to synchronize to the rhythms of their fellow creatures.\n\nLimbic resonance and limbic regulation are also referred to as \"mood contagion\" or \"emotional contagion\" as in the work of Sigal Barsade. Barsade and colleagues at the Yale School of Management build on research in social cognition, and find that some emotions, especially positive ones, are spread more easily than others through such \"interpersonal limbic regulation\".\n\nAuthor Daniel Goleman has explored similar terrain across several works: in \"Emotional Intelligence\" (1995), an international best seller, \"The Joy Of Living,\" coauthored with Yongey Mingyur Rinpoche, and the \"Harvard Business Review on Breakthrough Leadership\". In the latter book, Goleman considers the \"open loop nature of the brain's limbic system\" which depends on external sources to manage itself, and examines the implications of interpersonal limbic regulation and the science of moods on leadership.\n\nIn \"Mindfully Green: A Personal and Spiritual Guide to Whole Earth Thinking\" (2003) author Staphine Kaza defines the term as follows: \"Limbic regulation is a mutual simultaneous exchange of body signals that unfolds between people who are deeply involved with each other, especially parents and children.\" She goes on to correlate love with limbic engagement and asserts that children raised with love learn and remember better than those who are abused. Kaza then proposes to \"take this work a step further from a systems perspective, and imagine that a child learns through some sort of limbic regulation with nature\".\n\nLimbic revision is the therapeutic alteration of personality residing in the human limbic system of the brain.\n\nDr. Allan Schore, of the UCLA David Geffen School of Medicine, has explored related ideas beginning with his book \"Affect Regulation and the Origin of the Self\" published in 1994. Dr. Shore looks at the contribution of the limbic system to the preservation of the species, its role in forming social bonds with other members of the species and intimate relations leading to reproduction. \"It is said that natural selection favors characteristics that maximize an individual's contribution of the gene pool of succeeding generations. In humans this may entail not so much competitive and aggressive traits as an ability to enter into a positive affective relationship with a member of the opposite sex.\" In his subsequent book \"Affect regulation & the repair of the self\", Schor correlates the \"interactive transfer of affect\" between mother and infant, on the one hand, and in a therapeutic context on the other, and describes it as \"intersubjectivity\". He then goes on to explore what developmental neuropsychology can reveal about both types of interrelatedness.\n\nIn \"Integrative Medicine: Principles for Practice\", authors Kligler and Lee state \"The empathic therapist offers a form of affect regulation. The roots of empathy — Limbic resonance — are found in the early caregiver experiences, which shape the ways the child learns to experience, share, and communicate affects.\"\n\n\"Limbic Resonance\" is the title of the first episode of the first series of \"Sense8,\" written and directed by Lana and Lilly Wachowski and J. Michael Straczynski for Netflix. The series was broadcast in 2015, and it tells the story of 8 characters living in 8 different places. The first episode, \"Limbic Resonance\" describes how after inexplicably seeing a vision of a woman that they have never met before, eight uniquely different people from across the globe start seeing and hearing things that they have not experienced.\n\n"}
{"id": "2830423", "url": "https://en.wikipedia.org/wiki?curid=2830423", "title": "List of PDF software", "text": "List of PDF software\n\nThis is a list of links to articles on software used to manage Portable Document Format (PDF) documents. The distinction between the various functions is not entirely clear-cut; for example, some viewers allow adding of annotations, signatures, etc. Some software allows redaction, removing content irreversibly for security. Extracting embedded text is a common feature, but other applications perform optical character recognition (OCR) to convert imaged text to machine-readable form, sometimes by using an external OCR module.\n\nThese are used by software developers to add and create PDF features.\n\nThese create files in their native formats, but then allow users to export them to PDF formats.\n\nThese allow users to view (not edit or modify) any existing PDF file.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "36913927", "url": "https://en.wikipedia.org/wiki?curid=36913927", "title": "List of cutaneous conditions caused by mutations in keratins", "text": "List of cutaneous conditions caused by mutations in keratins\n\nThere are many different keratin proteins normally expressed in the human integumentary system. Mutations in keratin proteins in the skin can cause disease.\n\nOf note, other structural proteins in the epidermis of the skin that are closely related to keratins may also cause disease if mutated. Examples include:\n\n"}
{"id": "14109934", "url": "https://en.wikipedia.org/wiki?curid=14109934", "title": "List of molluscan genera represented in the fossil record", "text": "List of molluscan genera represented in the fossil record\n\nThis list of molluscan genera represented in the fossil record is a list which is composed primarily of many mollusk genera which occur as fossils. Some of these genera are extant, some are extinct.\nSome genera listed here are organisms other than mollusks.\n\nThis list article was originally based on Jack Sepkoski's online fossil genus databases, here .\n\nNote: This list article was originally based on Jack Sepkoski's online fossil genus databases, here .\n"}
{"id": "23776822", "url": "https://en.wikipedia.org/wiki?curid=23776822", "title": "List of national parks of Ivory Coast", "text": "List of national parks of Ivory Coast\n\nThe country of Côte d'Ivoire in Africa has the following national parks and other protected areas.\n\n\n\n"}
{"id": "21189069", "url": "https://en.wikipedia.org/wiki?curid=21189069", "title": "List of zoology journals", "text": "List of zoology journals\n\nThis is a list of scientific journals which cover the field of zoology. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4500316", "url": "https://en.wikipedia.org/wiki?curid=4500316", "title": "Loop quantum cosmology", "text": "Loop quantum cosmology\n\nLoop quantum cosmology (LQC) is a , symmetry-reduced model of loop quantum gravity (LQG) that predicts a \"quantum bridge\" between contracting and expanding cosmological branches.\n\nThe distinguishing feature of LQC is the prominent role played by the quantum geometry effects of loop quantum gravity (LQG). In particular, quantum geometry creates a brand new repulsive force which is totally negligible at low space-time curvature but rises very rapidly in the Planck regime, overwhelming the classical gravitational attraction and thereby resolving singularities of general relativity. Once singularities are resolved, the conceptual paradigm of cosmology changes and one has to revisit many of the standard issues—e.g., the \"horizon problem\"—from a new perspective.\n\nSince LQG is based on a specific quantum theory of Riemannian geometry, geometric observables display a fundamental discreteness that play a key role in quantum dynamics: While predictions of LQC are very close to those of quantum geometrodynamics (QGD) away from the Planck regime, there is a dramatic difference once densities and curvatures enter the Planck scale. In LQC the big bang is replaced by a quantum bounce.\n\nStudy of LQC has led to many successes, including the emergence of a possible mechanism for cosmic inflation, resolution of gravitational singularities, as well as the development of effective semi-classical Hamiltonians.\n\nThis subfield originated in 1999 by Martin Bojowald, and further developed in particular by Abhay Ashtekar and Jerzy Lewandowski, as well as Tomasz Pawłowski and Parampreet Singh, et al. In late 2012 LQC represents a very active field in physics, with about three hundred papers on the subject published in the literature. There has also recently been work by Carlo Rovelli, et al. on relating LQC to the spinfoam-based spinfoam cosmology.\n\nHowever, the results obtained in LQC are subject to the usual restriction that a truncated classical theory, then quantized, might not display the true behaviour of the full theory due to artificial suppression of degrees of freedom that might have large quantum fluctuations in the full theory. It has been argued that singularity avoidance in LQC are by mechanisms only available in these restrictive models and that singularity avoidance in the full theory can still be obtained but by a more subtle feature of LQG.\n\nDue to the quantum geometry, the big bang is replaced by a big bounce without any assumptions on the matter content or any fine tuning. An important feature of loop quantum cosmology is the effective space-time description of the underlying quantum evolution. The effective dynamics approach has been extensively used in loop quantum cosmology to describe physics at the Planck scale and the very early universe. Rigorous numerical simulations have confirmed the validity of the effective dynamics, which provides an excellent approximation to the full loop quantum dynamics. It has been shown that only when the states have very large quantum fluctuations at late times, which means that they do not lead to macroscopic universes as described by general relativity, that the effective dynamics has departures from the quantum dynamics near bounce and the subsequent evolution. In such a case, the effective dynamics overestimates the density at the bounce, but still captures the qualitative aspects extremely well.\n\n\n"}
{"id": "41824003", "url": "https://en.wikipedia.org/wiki?curid=41824003", "title": "MIMO-OFDM", "text": "MIMO-OFDM\n\nMultiple-input, multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) is the dominant air interface for 4G and 5G broadband wireless communications. It combines multiple-input, multiple-output (MIMO) technology, which multiplies capacity by transmitting different signals over multiple antennas, and orthogonal frequency-division multiplexing (OFDM), which divides a radio channel into a large number of closely spaced subchannels to provide more reliable communications at high speeds. Research conducted during the mid-1990s showed that while MIMO can be used with other popular air interfaces such as time-division multiple access (TDMA) and code-division multiple access (CDMA), the combination of MIMO and OFDM is most practical at higher data rates.\n\nMIMO-OFDM is the foundation for most advanced wireless local area network (wireless LAN) and mobile broadband network standards because it achieves the greatest spectral efficiency and, therefore, delivers the highest capacity and data throughput. Greg Raleigh invented MIMO in 1996 when he showed that different data streams could be transmitted at the same time on the same frequency by taking advantage of the fact that signals transmitted through space bounce off objects (such as the ground) and take multiple paths to the receiver. That is, by using multiple antennas and precoding the data, different data streams could be sent over different paths. Raleigh suggested and later proved that the processing required by MIMO at higher speeds would be most manageable using OFDM modulation, because OFDM converts a high-speed data channel into a number of parallel lower-speed channels.\n\nIn modern usage, the term \"MIMO\" indicates more than just the presence of multiple transmit antennas (multiple input) and multiple receive antennas (multiple output). While multiple transmit antennas can be used for beamforming, and multiple receive antennas can be used for diversity, the word \"MIMO\" refers to the simultaneous transmission of multiple signals (spatial multiplexing) to multiply spectral efficiency (capacity).\n\nTraditionally, radio engineers treated natural multipath propagation as an impairment to be mitigated. MIMO is the first radio technology that treats multipath propagation as a phenomenon to be exploited. MIMO multiplies the capacity of a radio link by transmitting multiple signals over multiple, co-located antennas. This is accomplished without the need for additional power or bandwidth. Space–time codes are employed to ensure that the signals transmitted over the different antennas are orthogonal to each other, making it easier for the receiver to distinguish one from another. Even when there is line of sight access between two stations, dual antenna polarization may be used to ensure that there is more than one robust path.\n\nOFDM enables reliable broadband communications by distributing user data across a number of closely spaced, narrowband subchannels. This arrangement makes it possible to eliminate the biggest obstacle to reliable broadband communications, intersymbol interference (ISI). ISI occurs when the overlap between consecutive symbols is large compared to the symbols’ duration. Normally, high data rates require shorter duration symbols, increasing the risk of ISI. By dividing a high-rate data stream into numerous low-rate data streams, OFDM enables longer duration symbols. A cyclic prefix (CP) may be inserted to create a (time) guard interval that prevents ISI entirely. If the guard interval is longer than the delay spread—the difference in delays experienced by symbols transmitted over the channel—then there will be no overlap between adjacent symbols and consequently no intersymbol interference. Though the CP slightly reduces spectral capacity by consuming a small percentage of the available bandwidth, the elimination of ISI makes it an exceedingly worthwhile tradeoff.\n\nA key advantage of OFDM is that fast Fourier transforms (FFTs) may be used to simplify implementation. Fourier transforms convert signals back and forth between the time domain and frequency domain. Consequently, Fourier transforms can exploit the fact that any complex waveform may be decomposed into a series of simple sinusoids. In signal processing applications, discrete Fourier transforms (DFTs) are used to operate on real-time signal samples. DFTs may be applied to composite OFDM signals, avoiding the need for the banks of oscillators and demodulators associated with individual subcarriers. Fast Fourier transforms are numerical algorithms used by computers to perform DFT calculations.\n\nFFTs also enable OFDM to make efficient use of bandwidth. The subchannels must be spaced apart in frequency just enough to ensure that their time-domain waveforms are orthogonal to each other. In practice, this means that the subchannels are allowed to partially overlap in frequency.\n\nMIMO-OFDM is a particularly powerful combination because MIMO does not attempt to mitigate multipath propagation and OFDM avoids the need for signal equalization. MIMO-OFDM can achieve very high spectral efficiency even when the transmitter does not possess channel state information (CSI). When the transmitter does possess CSI (which can be obtained through the use of training sequences), it is possible to approach the theoretical channel capacity. CSI may be used, for example, to allocate different size signal constellations to the individual subcarriers, making optimal use of the communications channel at any given moment of time.\n\nMore recent MIMO-OFDM developments include multi-user MIMO (MU-MIMO), higher order MIMO implementations (greater number of spatial streams), and research concerning massive MIMO and cooperative MIMO (CO-MIMO) for inclusion in coming 5G standards.\n\nMU-MIMO is part of the IEEE 802.11ac standard, the first Wi-Fi standard to offer speeds in the gigabit per second range. MU-MIMO enables an access point (AP) to transmit to up to four client devices simultaneously. This eliminates contention delays, but requires frequent channel measurements to properly direct the signals. Each user may employ up to four of the available eight spatial streams. For example, an AP with eight antennas can talk to two client devices with four antennas, providing four spatial streams to each. Alternatively, the same AP can talk to four client devices with two antennas each, providing two spatial streams to each.\nMulti-user MIMO beamforming even benefits single spatial stream devices. Prior to MU-MIMO beamforming, an access point communicating with multiple client devices could only transmit to one at a time. With MU-MIMO beamforming, the access point can transmit to up to four single stream devices at the same time on the same channel.\n\nThe 802.11ac standard also supports speeds up to 6.93 Gbit/s using eight spatial streams in single-user mode. The maximum data rate assumes use of the optional 160 MHz channel in the 5 GHz band and 256 QAM (quadrature amplitude modulation). Chipsets supporting six spatial streams have been introduced and chipsets supporting eight spatial streams are under development.\n\nMassive MIMO consists of a large number of base station antennas operating in a MU-MIMO environment. While LTE networks already support handsets using two spatial streams, and handset antenna designs capable of supporting four spatial streams have been tested, massive MIMO can deliver significant capacity gains even to single spatial stream handsets. Again, MU-MIMO beamforming is used to enable the base station to transmit independent data streams to multiple handsets on the same channel at the same time. However, one question still to be answered by research is: When is it best to add antennas to the base station and when is it best to add small cells?\n\nAnother focus of research for 5G wireless is CO-MIMO. In CO-MIMO, clusters of base stations work together to boost performance. This can be done using macro diversity for improved reception of signals from handsets or multi-cell multiplexing to achieve higher downlink data rates. However, CO-MIMO requires high-speed communication between the cooperating base stations.\n\nGregory Raleigh was first to advocate the use of MIMO in combination with OFDM. In a theoretical paper, he proved that with the proper type of MIMO system—multiple, co-located antennas transmitting and receiving multiple information streams using multidimensional coding and encoding—multipath propagation could be exploited to multiply the capacity of a wireless link. Up to that time, radio engineers tried to make real-world channels behave like ideal channels by mitigating the effects of multipath propagation. However, mitigation strategies have never been fully successful. In order to exploit multipath propagation, it was necessary to identify modulation and coding techniques that perform robustly over time-varying, dispersive, multipath channels. Raleigh published additional research on MIMO-OFDM under time-varying conditions, MIMO-OFDM channel estimation, MIMO-OFDM synchronization techniques, and the performance of the first experimental MIMO-OFDM system.\n\nRaleigh solidified the case for OFDM by analyzing the performance of MIMO with three leading modulation techniques in his PhD dissertation: quadrature amplitude modulation (QAM), direct sequence spread spectrum (DSSS), and discrete multi-tone (DMT). QAM is representative of narrowband schemes such as TDMA that use equalization to combat ISI. DSSS uses rake receivers to compensate for multipath and is used by CDMA systems. DMT uses interleaving and coding to eliminate ISI and is representative of OFDM systems. The analysis was performed by deriving the MIMO channel matrix models for the three modulation schemes, quantifying the computational complexity and assessing the channel estimation and synchronization challenges for each. The models showed that for a MIMO system using QAM with an equalizer or DSSS with a rake receiver, computational complexity grows exponentially (more specifically, quadratically) as data rate is increased. In contrast, when MIMO is used with DMT computational complexity grows linearly (more specifically, log-linearly) as data rate is increased.\n\nRaleigh subsequently founded Clarity Wireless in 1996 and Airgo Networks in 2001 to commercialize the technology. Clarity developed specifications in the Broadband Wireless Internet Forum (BWIF) that led to the IEEE 802.16 (commercialized as WiMAX) and LTE standards, both of which support MIMO. Airgo designed and shipped the first MIMO-OFDM chipsets for what became the IEEE 802.11n standard. MIMO-OFDM is also used in the 802.11ac standard and is expected to play a major role in 802.11ax and fifth generation (5G) mobile phone systems.\n\nSeveral early papers on multi-user MIMO were authored by Ross Murch et al. at Hong Kong University of Science and Technology. MU-MIMO was included in the 802.11ac standard (developed starting in 2011 and approved in 2014). MU-MIMO capacity appears for the first time in what have become known as \"Wave 2\" products. Qualcomm announced chipsets supporting MU-MIMO in April 2014.\n\nBroadcom introduced the first 802.11ac chipsets supporting six spatial streams for data rates up to 3.2 Gbit/s in April 2014. Quantenna says it is developing chipsets to support eight spatial streams for data rates up to 10 Gbit/s.\n\nMassive MIMO, Cooperative MIMO (CO-MIMO), and HetNets (heterogeneous networks) are currently the focus of research concerning 5G wireless. The development of 5G standards is expected to begin in 2016. Prominent researchers to date include Jakob Hoydis (of Alcatel-Lucent), Robert W. Heath (at the University of Texas at Austin), Helmut Bölcskei (at ETH Zurich), and David Gesbert (at EURECOM).\n\nTrials of 5G technology have been conducted by Samsung. Japanese operator NTT DoCoMo plans to trial 5G technology in collaboration with Alcatel-Lucent, Ericsson, Fujitsu, NEC, Nokia, and Samsung.\n"}
{"id": "21459", "url": "https://en.wikipedia.org/wiki?curid=21459", "title": "Nevanlinna Prize", "text": "Nevanlinna Prize\n\nThe Rolf Nevanlinna Prize (named in honor of Rolf Nevanlinna) is awarded once every 4 years at the International Congress of Mathematicians, for outstanding contributions in Mathematical Aspects of Information Sciences including:\n\n\nThe prize was established in 1981 by the Executive Committee of the International Mathematical Union IMU and named to honour the Finnish mathematician Rolf Nevanlinna who had died a year earlier. The award consists of a gold medal and cash prize. Like the Fields Medal the prize is targeted at younger mathematicians, and only those younger than 40 on January 1 of the award year are eligible.\n\nThe medal features a profile of Nevanlinna, the text \"Rolf Nevanlinna Prize\", and very small characters \"RH 83\" on its obverse. RH refers to Raimo Heino, the medal's designer, and 83 to the year of first minting. On the reverse, two figures related to the University of Helsinki, the prize sponsor, are engraved. The rim bears the name of the prizewinner.\n\nThe appropriateness of the naming of the prize had been questioned due to Nevanlinna's involvement with the Nazis. \n\nShortly after July-2016, Alexander Soifer, President of the World Federation of National Mathematics Competitions, forwarded his personal and his organization’s requests to the Executive Committee of IMU to change the Prize’s name. On July 30-31, 2018, the 18 General Assembly of the IMU decided to remove the name of Rolf Nevanlinna from this prize for achievement in mathematics for computer science, and replace it with a new name. See Resolution 7 of the 2018 resolutions. \n\n\n"}
{"id": "46329251", "url": "https://en.wikipedia.org/wiki?curid=46329251", "title": "Nigel Latta Blows Stuff Up", "text": "Nigel Latta Blows Stuff Up\n\nNigel Latta Blows Stuff Up is a New Zealand science television series that started on 19 April 2015, hosted by Nigel Latta.\n"}
{"id": "8082342", "url": "https://en.wikipedia.org/wiki?curid=8082342", "title": "Repetition priming", "text": "Repetition priming\n\nRepetition priming refers to improvements in a behavioural response when stimuli are repeatedly presented. The improvements can be measured in terms of \"accuracy\" or \"reaction time\", and can occur when the repeated stimuli are either identical or similar to previous stimuli. These improvements have been shown to be cumulative, so as the number of repetitions increases the responses get continually faster up to a maximum of around seven repetitions. These improvements are also found when the repeated items are changed slightly in terms of orientation, size and position. The size of the effect is also modulated by the length of time the item is presented for and the length time between the first and subsequent presentations of the repeated items.\n\nRepetition priming can occur without a person being aware of either the repeats or the improvements in his/her response, so it is generally thought to involve implicit memory processes that are dissociable from explicit memory processes. This idea has support from findings that amnesic patients with damage to limbic and/or diencephalic structures show measurable repetition priming effects but have deficits on explicit measures of memory. However, some researchers suggest that implicit and explicit memory systems are not in fact separate. Repetition priming has also been associated with attentional processes, stimulus expectation and episodic memory.\n\nResearch into repetition priming has been used to investigate the nature of mechanisms underlying the behavioural effects of rapid learning. In utilizing measures of \"repetition suppression\", the putative neural correlate of repetition priming, and measuring changes in the neural response associated with changing the presented stimuli, researchers are attempting to index regions and their processing biases along perceptual, conceptual and response dimensions. This area of research is based on multiple measurement methods from single cell recordings to multi-regional measurements using functional magnetic resonance imaging (fMRI), electroencephalography (EEG) and magnetoencephalography (MEG). Transcranial magnetic stimulation (TMS) has also been used to temporarily 'lesion' (inactivate) specific regions and so get an indication of the necessity of those regions in processing specific dimensions of the presented stimuli. Much of this research has been focused on the visual domain, however auditory and olfactory processes have also been investigated.\n\nNumerous models have been put forward to explain behavioural efficiencies that are gained with repeated presentations of the same or similar stimuli. These are outlined below.\n\nIn this model the attenuation of a neural response is hypothesised to be due to an overall reduction in the amplitude of a neuron's firing. Whether this reduction occurs across all neurons that responded to the initial stimulus or just the critical subset of those that initially responded maximally, is still unclear. However, evidence does suggest that a mechanism like this reduces redundant neural firing and enhances efficiencies in processing in the early visual cortex.\n\nAlong similar lines is the idea that repetition causes the neurons that are less relevant to the representation of the stimulus to stop firing when that stimulus is repeated. In this way the representation is supported by a gradually sparser response, resulting in an adaptive reduction in metabolic requirements and increased efficiencies in information transmission through the neural hierarchy. This could be the result of lateral inhibition within representational levels in a competitive Hebbian learning system, where strong connections get stronger and inhibit the weaker connections. Much of the evidence for this comes from primate studies of the inferotemporal cortex and single cell recordings with long training periods. However, decreases in firing rate over short-term training on repeated stimuli appear to be greatest in those cells that initially respond with the highest activation rate, in line with the fatigue model above.\n\nThe key concept in this model is that information travels faster through the network when the current stimulus representation overlaps with a previous representation, driven by more rapid onset of neural activation with repeated presentations. fMRI studies have been used in an attempt to measure these potential latency differences but the temporal resolution is not very precise and single cell recordings typically do not show shortened latencies to repeated stimuli. Another possible explanation of facilitation is synaptic potentiation within an attractor neural network model where repetition decreases the settling time as the attractor basin deepens and so increases the overall speed of processing.\n\nWhen a stimulus is repeated top-down feedback modulates the neural response of earlier processing regions, with reduced neural activation and improved behavioural responses reflecting fulfilled expectations. The idea for this comes from predictive coding theories and Bayesian statistics and has some support in fMRI studies manipulating stimulus expectation. However, the results may also reflect the involvement of attention, which seems to have a modulatory effect on the extent of priming elicited.\n\nThis theory is based on the idea that because downstream neurons are sensitive to both the firing rates and the timing of those inputs, efficiencies in processing may be gained through synchronised activation. Evidence of synchronisation associated with repeated stimuli include phase locking found between two regions of the cat visual cortex while measuring spike synchronisation for trained compared with novel stimuli and suppressed firing and increased synchrony of spikes with repetition of odour puffs to locust's antennae. Evidence using EEG and MEG suggests that stimulus repetition in humans results in increased synchrony between distinct cortical regions, often the same regions that show reduced local neural activity (see repetition suppression below). In one study, the timing of this across-region synchronization predicted the amount of behavioral facilitation seen with repetition priming, suggestive of a close link between synchrony and behavior.\n\nThis theory suggests that repetition priming is a result of binding the initial stimulus directly to the response while bypassing the intervening layers of computation. The mechanism mediating this direct binding has not been clarified but several hypotheses have been put forward. One theory explains it as a race between automatic activation of a previous stimulus-response route and the reengagement of the \"algorithmic\" route and another theory suggests the operation of an \"action-trigger\" where repeated stimuli trigger the previous response through perceptual or conceptual associations with the original stimulus. In support of this theory is evidence of a response congruency effect, which would be expected from these stimulus-response bindings. The increased synchrony between regions discussed above could be a neural correlate of stimulus-response binding.\n\nThe phenomenon of \"repetition suppression\", a reduction in neural activity when stimuli are repeated, is thought to depend on processing overlaps between repeated items and is generally considered to be the neural correlate of repetition priming. As such it has been used extensively in research investigating the nature of representations across various levels of the visual processing hierarchy. In doing so researchers have found that repetition suppression appears to occur on multiple processing levels; dependent on the stimuli being processed and the processing level at which the experimental manipulation is directed; with reductions in neural activity to repeated stimuli occurring in regions involved with the initial processing of those features. However, care must be taken in interpreting the results of these studies as the relationship between repetition suppression and repetition priming has not been definitively established.\n\nAlthough repetition priming is most often associated with neural attenuation for repeated presentation of stimuli, increases in neural responses have also been measured in a number of experimental contexts. For example, when performing mathematical calculations, when repeated stimuli are degraded, in studies involving a backward masking paradigm and when stimuli have no pre-existing associations or meaning.\n"}
{"id": "3357517", "url": "https://en.wikipedia.org/wiki?curid=3357517", "title": "Robert W. Paul", "text": "Robert W. Paul\n\nRobert William Paul (3 October 1869 – 28 March 1943) was an English electrician, scientific instrument maker, and early pioneer of British film.\n\nPaul was born in Liverpool Road, in present-day Inner London, and began his technical career learning instrument-making skills at the Elliott Brothers, a firm of London instrument makers founded in 1804, followed by the Bell Telephone Company in Antwerp. In 1891, he established an instrument-making company, Robert W. Paul Instrument Company, initially with a workshop at 44 Hatton Garden, London, later his office.\n\nIn 1894, he was approached by two Greek businessmen who wanted him to make copies of an Edison \"Kinetoscope\" that they had purchased. He at first refused, then found to his amazement that Edison had not patented the invention in Britain. Subsequently, Paul himself would go on to purchase a Kinetoscope, intent on taking it apart and re-creating an English-based version. He manufactured a number of these, one of which was supplied to Georges Méliès.\n\nHowever, the only films available were 'bootleg' copies of those produced for the Edison machines. As Edison had patented his camera (the details of which were a closely guarded secret), Paul resolved to solve this bottleneck by creating his own camera. He was introduced to \nBirt Acres, a photographic expert, who had invented a device to move film as part of the developing process. Paul thought that Acre's principle could be used in a camera. This camera, dubbed the \"Paul-Acres Camera\", invented in March 1895, would be the first camera made in England.\n\nR.W. Paul obtained a concession to operate a kinetoscope parlour at the Earls Court Exhibition Centre, and the success of this inspired him to contemplate the possibilities of projecting a moving image on to a screen, something that Edison had never considered. And while Paul and Birt Acres would share innovator status for England's first camera, soon after conception both men would dissolve the partnership and become competitors in the film camera and projector markets.\n\nAcres would present his projector, England's first, on 14 January 1896. Paul would present his own, the more influential Theatrograph shortly after on 20 February. Ironically this is exactly the same day the Lumieres' films would first be projected in London.\n\nIn 1896, he pioneered in the UK a system of projecting motion pictures onto a screen, using a self-developed Maltese cross system. This coincided with the advent of the projection system devised by the Lumiere Brothers. After some demonstrations before scientific groups, he was asked to supply a projector and staff to the Alhambra Music Hall in Leicester Square, and he presented his first theatrical programme on 25 March 1896. This included films featuring cartoonist Tom Merry drawing caricatures of the German Emperor Kaiser Wilhelm II (1895), and Prince Bismarck (1895). Merry had previously performed his lightning-fast drawing as part of a music hall stage act. (The Lumieres were appearing on the bill at the Empire Music Hall, nearby.) The use of his 'Theatrograph' in music halls up and down the country popularised early cinema in Britain. There were many showmen who wished to imitate Paul's success, and some of these wanted to make their own films of 'local interest'. It was necessary to set up a completely separate manufacturing department producing cameras, projectors, and cinema equipment, with its own office and showroom.\n\nPaul would also continue his innovations in the portable camera field. His 'Cinematograph Camera No. 1', built in April 1896, would be the first camera to feature reverse-cranking. This mechanism allowed for the same film footage to be exposed several times. The ability to create super-positions and multiple exposures would be of great significance. This technique was used in Paul's 1901 film \"Scrooge, or, Marley's Ghost\", the oldest known film adaptation of Charles Dickens' \"A Christmas Carol\". It is noted that the first camera that George Melies would use was built by R.W. Paul.\n\nIn 1898 he designed and constructed Britain's first film studio in Muswell Hill, north London.\n\nThe \"British Film Catalogue\" credits Paul's \"Our New General Servant\" (1898) with the \"first use of intertitles\".\n\nIn the meantime, he continued with his original business, focusing on his internationally renowned \"Unipivot\" galvanometer. Paul’s instruments were internationally renowned: he won gold medals at the St Louis Exposition in 1904 and the Brussels Exhibition in 1910, among others. Upon the outbreak of World War I, he began producing military instruments including early wireless telegraphy sets, and instruments for submarine warfare. In December 1919, the Cambridge Scientific Instrument Company took over the smaller but successful Robert W. Paul Instrument Company and became The Cambridge and Paul Instrument Company Ltd. The name was shortened to the Cambridge Instrument Co Ltd in 1924 when it was converted to a public company.\n\nPaul continued to make his own films, selling them either directly or through the new distribution companies that were springing up. He was a very innovative director and cameraman, pioneering techniques such as the close up and cutting from one scene to another. \nHowever, his growing business interests crowded out film, and he moved out of the infant industry as early as 1910. Nevertheless, his importance was always recognized by contemporaries, who often referred to him as 'Daddy Paul'.\n\nCoincidentally and without prior knowledge of the above, in 1994 a technology company called Kinetic took over the building at 44 Hatton Garden and renamed it Kinetic House. In 1999, the British film industry commemorated the work of Paul by erecting a commemorative plaque on the building attended by members of the film industry, unions and Lord Samuelson.\n\n\n"}
{"id": "55488976", "url": "https://en.wikipedia.org/wiki?curid=55488976", "title": "Rosemary R. Haggett", "text": "Rosemary R. Haggett\n\nRosemary R. Haggett, Ph.D., is the Vice Chancellor for Academic Affairs and Student Success for the University of North Texas System. She was the second woman to serve as a dean of a college of agriculture in the U.S.\n\nRosemary Haggett earned a bachelor's degree in biology from the University of Bridgeport and a Ph.D. in physiology from the University of Virginia. She conducted postdoctoral work in reproductive biology at Northwestern University.\n\nRosemary Haggett began her career as a postdoctoral associate at Northwestern University, conducting research in reproductive biology. She worked at the U.S. Department of Agriculture for several years. In 1994, she became a professor of Animal and Veterinary Sciences and the Dean of the College of Agriculture, Forestry, and Consumer Sciences at West Virginia University, only the second woman in the U.S. to serve in such a position. In 1999, she became the Associate Provost for Academic Programs at West Virginia University. \n\nIn 2003, she left West Virginia University to serve at the National Science Foundation as the director of the Division of Undergraduate Education. She also served as the Acting Deputy Assistant Director of the Education and Human Resources Directorate, the Acting Director of the Division of Graduate Education, and the Senior Adviser of the Education and Human Resources Directorate at the National Science Foundation. \n\nIn 2007, she became Provost and Executive Vice President for Academic Affairs at the University of Toledo. \n\nIn 2010, she became Vice Chancellor for Academic Affairs and Student Success for the University of North Texas System.\n\nRosemary Haggett served as Chair of the Committee of Visitors for the National Science Foundation's Training Cluster in the Division of Biological Infrastructure in 2003. She served on the National Academies Committee to review the USDA Agricultural and Food Research Initiative in 2012.\n"}
{"id": "45829", "url": "https://en.wikipedia.org/wiki?curid=45829", "title": "Structural engineering", "text": "Structural engineering\n\nStructural engineering is that part of civil engineering in which structural engineers are educated to create the 'bones and muscles' that create the form and shape of man made structures. Structural engineers need to understand and calculate the stability, strength and rigidity of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety. See glossary of structural engineering.\n\nStructural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural elements to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.\nStructural engineering dates back to 2700 B.C.E. when the step pyramid for Pharaoh Djoser was built by Imhotep, the first engineer in history known by name. Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).\n\nThe structural stability of the pyramid, whilst primarily gained from its shape, relies also on the strength of the stone from which it is constructed, and its ability to support the weight of the stone above it. The limestone blocks were often taken from a quarry near the build site and have a compressive strength from 30 to 250 MPa (MPa = Pa * 10^6). Therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid's geometry.\n\nThroughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. No theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of 'what had worked before'. Knowledge was retained by guilds and seldom supplanted by advances. Structures were repetitive, and increases in scale were incremental.\n\nNo record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of structural engineer only really took shape with the Industrial Revolution and the re-invention of concrete (see History of Concrete. The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computer-based applications pioneered in the 1970s.\n\n\nThe history of structural engineering contains many collapses and failures. Sometimes this is due to obvious negligence, as in the case of the Pétion-Ville school collapse, in which Rev. Fortin Augustin \"constructed the building all by himself, saying he didn't need an engineer as he had good knowledge of construction\" following a partial collapse of the three-story schoolhouse that sent neighbors fleeing. The final collapse killed 94 people, mostly children.\n\nIn other cases structural failures require careful study, and the results of these inquiries have resulted in improved practices and greater understanding of the science of structural engineering. Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated. A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s.\n\nStructural engineering depends upon a detailed knowledge of applied mechanics, materials science and applied mathematics to understand and predict how structures support and resist self-weight and imposed loads. To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes, the techniques of structural analysis, as well as some knowledge of the corrosion resistance of the materials and structures, especially when those structures are exposed to the external environment. Since the 1990s, specialist software has become available to aid in the design of structures, with the functionality to assist in the drawing, analyzing and designing of structures with maximum precision; examples include AutoCAD, StaadPro, ETABS, Prokon, Revit Structure, Inducta RCB, etc. Such software may also take into consideration environmental loads, such as from earthquakes and winds.\n\nStructural engineers are responsible for engineering design and structural analysis. Entry-level structural engineers may design the individual structural elements of a structure, such as the beams and columns of a building. More experienced engineers may be responsible for the structural design and integrity of an entire system, such as a building.\n\nStructural engineers often specialize in particular types of structures, such as buildings, bridges, pipelines, industrial, tunnels, vehicles, ships, aircraft and spacecraft. Structural engineers who specialize in buildings often specialize in particular construction materials such as concrete, steel, wood, masonry, alloys and composites, and may focus on particular types of buildings such as offices, schools, hospitals, residential, and so forth.\n\nStructural engineering has existed since humans first started to construct their own structures. It became a more defined and formalized profession with the emergence of the architecture as distinct profession from the engineering during the industrial revolution in the late 19th century. Until then, the architect and the structural engineer were usually one and the same thing – the master builder. Only with the development of specialized knowledge of structural theories that emerged during the 19th and early 20th centuries, did the professional structural engineers come into existence.\n\nThe role of a structural engineer today involves a significant understanding of both static and dynamic loading, and the structures that are available to resist them. The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to. A structural engineer will typically have a four or five year undergraduate degree, followed by a minimum of three years of professional practice before being considered fully qualified.\nStructural engineers are licensed or accredited by different learned societies and regulatory bodies around the world (for example, the Institution of Structural Engineers in the UK). Depending on the degree course they have studied and/or the jurisdiction they are seeking licensure in, they may be accredited (or licensed) as just structural engineers, or as civil engineers, or as both civil and structural engineers.\nAnother international organisation is IABSE(International Association for Bridge and Structural Engineering). The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society.\n\nStructural building engineering includes all structural engineering related to the design of buildings. It is a branch of structural engineering closely affiliated with architecture.\n\nStructural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end which fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience. This is subtly different from architectural design, which is driven by the creative manipulation of materials and forms, mass, space, volume, texture and light to achieve an end which is aesthetic, functional and often artistic.\n\nThe architect is usually the lead designer on buildings, with a structural engineer employed as a sub-consultant. The degree to which each discipline actually leads the design depends heavily on the type of structure. Many structures are structurally simple and led by architecture, such as multi-storey office buildings and housing, while other structures, such as tensile structures, shells and gridshells are heavily dependent on their form for their strength, and the engineer may have a more significant influence on the form, and hence much of the aesthetic, than the architect.\n\nThe structural design for a building must ensure that the building is able to stand up safely, able to function without excessive deflections or movements which may cause fatigue of structural elements, cracking or failure of fixtures, fittings or partitions, or discomfort for occupants. It must account for movements and forces due to temperature, creep, cracking and imposed loads. It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials. It must allow the architecture to work, and the building services to fit within the building and function (air conditioning, ventilation, smoke extract, electrics, lighting etc.). The structural design of a modern building can be extremely complex, and often requires a large team to complete.\n\nStructural engineering specialties for buildings include:\n\nEarthquake engineering structures are those engineered to withstand earthquakes.\n\nThe main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground, foresee the consequences of possible earthquakes, and design and construct the structures to perform during an earthquake.\n\nEarthquake-proof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above.\n\nOne important tool of earthquake engineering is base isolation, which allows the base of a structure to move freely with the ground.\n\nCivil structural engineering includes all structural engineering related to the built environment. It includes:\n\nThe structural engineer is the lead designer on these structures, and often the sole designer. In the design of structures such as these, structural safety is of paramount importance (in the UK, designs for dams, nuclear power stations and bridges must be signed off by a chartered engineer).\n\nCivil engineering structures are often subjected to very extreme forces, such as large variations in temperature, dynamic loads such as waves or traffic, or high pressures from water or compressed gases. They are also often constructed in corrosive environments, such as at sea, in industrial facilities or below ground.\n\nThe principles of structural engineering are applicable to variety of mechanical (moveable) structures. The design of static structures assumes they always have the same geometry (in fact, so-called static structures can move significantly, and structural engineering design must take this into account where necessary), but the design of moveable or moving structures must account for fatigue, variation in the method in which load is resisted and significant deflections of structures.\n\nThe forces which parts of a machine are subjected to can vary significantly, and can do so at a great rate. The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structure's lifetime. The structural design must ensure that such structures are able to endure such loading for their entire design life without failing.\n\nThese works can require mechanical structural engineering:\n\n\nAerospace structure types include launch vehicles, (Atlas, Delta, Titan), missiles (ALCM, Harpoon), Hypersonic vehicles (Space Shuttle), military aircraft (F-16, F-18) and commercial aircraft (Boeing 777, MD-11). Aerospace structures typically consist of thin plates with stiffeners for the external surfaces, bulkheads and frames to support the shape and fasteners such as welds, rivets, screws and bolts to hold the components together.\n\nA nanostructure is an object of intermediate size between molecular and microscopic (micrometer-sized) structures. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology.\n\nMedical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions. There are several basic types: diagnostic equipment includes medical imaging machines, used to aid in diagnosis; equipment includes infusion pumps, medical lasers and LASIK surgical machines; medical monitors allow medical staff to measure a patient's medical state. Monitors may measure patient vital signs and other parameters including ECG, EEG, blood pressure, and dissolved gases in the blood; diagnostic medical equipment may also be used in the home for certain purposes, e.g. for the control of diabetes mellitus. A biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment.\n\nAny structure is essentially made up of only a small number of different types of elements:\n\n\nMany of these elements can be classified according to form (straight, plane / curve) and dimensionality (one-dimensional / two-dimensional):\n\nColumns are elements that carry only axial force (compression) or both axial force and bending (which is technically called a beam-column but practically, just a column). The design of a column must check the axial capacity of the element, and the buckling capacity.\n\nThe buckling capacity is the capacity of the element to withstand the propensity to buckle. Its capacity depends upon its geometry, material, and the effective length of the column, which depends upon the restraint conditions at the top and bottom of the column. The effective length is formula_1 where formula_2 is the real length of the column and K is the factor dependent on the restraint conditions.\n\nThe capacity of a column to carry axial load depends on the degree of bending it is subjected to, and vice versa. This is represented on an interaction chart and is a complex non-linear relationship.\n\nA beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element. Beams and columns are called line elements and are often represented by simple lines in structural modeling.\n\n\nBeams are elements which carry pure bending only. Bending causes one part of the section of a beam (divided along its length) to go into compression and the other part into tension. The compression part must be designed to resist buckling and crushing, while the tension part must be able to adequately resist the tension.\n\nA truss is a structure comprising members and connection points or nodes. When members are connected at nodes and forces are applied at nodes members can act in tension or in compression. Members acting in compression are referred to as compression members or struts while members acting in tension are referred to as tension members or ties. Most trusses use gusset plates to connect intersecting elements. Gusset plates are relatively flexible and unable to transfer bending moments. The connection is usually arranged so that the lines of force in the members are coincident at the joint thus allowing the truss members to act in pure tension or compression.\n\nTrusses are usually used in large-span structures, where it would be uneconomical to use solid beams.\n\nPlates carry bending in two directions. A concrete flat slab is an example of a plate. Plates are understood by using continuum mechanics, but due to the complexity involved they are most often designed using a codified empirical approach, or computer analysis.\n\nThey can also be designed with yield line theory, where an assumed collapse mechanism is analysed to give an upper bound on the collapse load (see Plasticity). This technique is used in practice but because the method provides an upper-bound, i.e. an unsafe prediction of the collapse load, for poorly conceived collapse mechanisms great care is needed to ensure that the assumed collapse mechanism is realistic.\n\nShells derive their strength from their form, and carry forces in compression in two directions. A dome is an example of a shell. They can be designed by making a hanging-chain model, which will act as a catenary in pure tension, and inverting the form to achieve pure compression.\n\nArches carry forces in compression in one direction only, which is why it is appropriate to build arches out of masonry. They are designed by ensuring that the line of thrust of the force remains within the depth of the arch. It is mainly used to increase the bountifulness of any structure.\n\nCatenaries derive their strength from their form, and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). They are almost always cable or fabric structures. A fabric structure acts as a catenary in two directions.\n\nStructural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads.\n\nCommon structural materials are:\n\n\n\n\n\n"}
{"id": "7396884", "url": "https://en.wikipedia.org/wiki?curid=7396884", "title": "Subsurface lithoautotrophic microbial ecosystem", "text": "Subsurface lithoautotrophic microbial ecosystem\n\nSubsurface lithoautotrophic microbial ecosystems, or \"SLIMEs\" (also abbreviated \"SLMEs\" or \"SLiMEs\"), are defined by Edward O. Wilson as \"unique assemblages of bacteria and fungi that occupy pores in the interlocking mineral grains of igneous rock beneath Earth's surface.\"\n\n"}
{"id": "59041446", "url": "https://en.wikipedia.org/wiki?curid=59041446", "title": "Telmatocola", "text": "Telmatocola\n\nTelmatocola is a genus of bacteria from the family of Planctomycetaceae with one known species (\"Telmatocola sphagniphila\").\n\"Telmatocola sphagniphila\" has been isolate from Sphagnum peat from Staroselsky moss from the Tver Region.\n"}
{"id": "268462", "url": "https://en.wikipedia.org/wiki?curid=268462", "title": "Vehicle Assembly Building", "text": "Vehicle Assembly Building\n\nThe Vehicle (originally Vertical) Assembly Building, or VAB, is the large building at NASA's Kennedy Space Center (KSC), designed to assemble the large pre-manufactured space vehicle components, such as the massive Saturn V and the Space Shuttle; and stack them vertically onto the Mobile Launch Platform and crawler transporter. The future Space Launch System (SLS) will also be assembled there.\n\nAt 3,664,883 cubic meters (129,428,000 cubic feet) it is one of the largest buildings in the world by volume. The building is at Launch Complex 39 at KSC, halfway between Jacksonville and Miami, and due east of Orlando on Merritt Island on the Atlantic coast of Florida.\n\nThe VAB is the largest single-story building in the world, was the tallest building (160.3 meters, 526 ft) in Florida until 1974, and is still the tallest building in the United States outside an urban area.\n\nThe VAB, which was completed in 1966, was originally built to allow for the vertical assembly of the Saturn V rocket for the Apollo program and referred to as the \"Vertical Assembly Building\". In anticipation of post-Saturn projects such as the Space Shuttle program, it was renamed to the \"Vehicle Assembly Building\" in 1965, and was used for the shuttle's external fuel tanks and flight hardware, and to mate the Space Shuttle orbiters to their solid rocket boosters and external fuel tanks. Once assembled, the complete Space Shuttle was moved on the Mobile Launcher Platform and Crawler-Transporter to LC-39 Pad A or B.\n\nIn 1963, NASA contracted the Morrison-Knudsen company to design and build the VAB. Construction began with driving the first steel foundation piles on Aug. 2, 1963. It was part of NASA's massive effort to send astronauts to the moon for the Apollo Program. Altogether, 4,225 pilings were driven down 164 feet to bedrock with a foundation consisting of 30,000 cubic yards of concrete. Construction of the VAB required 98,590 tons of steel. The building was completed in 1966. The VAB is tall, long and wide. It covers , and encloses of space.\n\nLocated on Florida's Atlantic coast, the building was constructed to withstand hurricanes and tropical storms with a foundation consisting of 30,000 cubic yards of concrete and 4,225 steel rods driven 160 feet into limestone bedrock. Despite this, it has received damage from several hurricanes (See below)\n\nThere are four entries to the bays located inside the building, which are the four largest doors in the world. Each door is high, has 7 vertical panels and 4 horizontal panels, and takes 45 minutes to completely open or close. The north entry that leads to the transfer aisle was widened by to allow entry of the shuttle orbiter. A central slot at the north entry allowed for passage of the orbiter's vertical stabilizer.\n\nTo lift the components of the Space Transportation System, the VAB housed five overhead bridge cranes, including two capable of lifting 325 tons, and 136 other lifting devices.\n\nThe building has at least 10,000 tons (40 MW) of air conditioning equipment, including 125 ventilators on the roof supported by four large air handlers (four cylindrical structures west of the building) to keep moisture under control. Air in the building can be completely replaced every hour. The interior volume of the building is so vast that it has its own weather, including \"rain clouds form[ing] below the ceiling on very humid days\", which the moisture reduction systems are designed to minimize.\n\nThe American flag painted on the building was the largest in the world when added in 1976 as part of United States Bicentennial celebrations, along with the star logo of the anniversary, later replaced by the NASA insignia in 1998. It is high, and wide. Each of the stars on the flag is across, the blue field is the size of a regulation basketball court, and each of the stripes is wide.\n\nWork began in early 2007 to restore the exterior paint on the immense facility. Special attention was paid to the enormous American flag and NASA \"meatball\" insignia. The work repaired visible damage from years of storms and weathering. The flag and logo had been previously repainted in 1998 for NASA's 40th anniversary.\nThe most extensive exterior damage occurred during the storm season of 2004, when Hurricane Frances blew off 850 14 × 6 foot aluminum panels from the building, resulting in about of new openings in the sides. Twenty five additional panels were blown off the east side by the winds from Hurricane Jeanne just three weeks later. Earlier in the season, Hurricane Charley caused significant but less serious damage, estimated to cost $700,000. Damage caused by these hurricanes was still visible in 2007. Some of these panels are \"punch-outs\", designed to detach from the VAB when a large pressure differential is created on the outside vs. the inside. This allows for equalization, and helps protect the structural integrity of the building during rapid changes in pressure such as in tropical cyclones.\n\nThe building has been used as a backdrop in several Hollywood movies including \"Marooned\", \"SpaceCamp\", \"Apollo 13\", \"Contact\", and others.\n\nThe Space Shuttle was retired in 2011 after which NASA temporarily (as early as 2012) offered public tours of the VAB. These tours were temporarily discontinued in February 2014 to allow for renovations to take place.\n\nThe NASA FY2013 budget included $143.7 million USD for Construction of Facilities (CoF) requirements in support of Exploration programs including Space Launch System (SLS) and Multi-Purpose Crew Vehicle (MPCV). NASA began modifying Launch Complex 39 at KSC to support the new SLS in 2014, beginning with major repairs, code upgrades and safety improvements to the Launch Control Center, Vehicle Assembly Building (VAB) and the VAB Utility Annex. This initial work will be required to support any launch vehicle operated from Launch Complex 39 and will allow NASA to begin modernizing the facilities, while vehicle-specific requirements are being developed.\n\nThe VAB could be used to some extent for assembly and processing of any future vehicles utilizing Launch Complex 39. On June 16, 2015, NASA released an announcement for proposals (AFP) seeking interest in using the VAB High Bay 2 and other complex facilities for commercial use in \"assembling, integration, and testing of launch vehicles.\" This move is in line with the intent to migrate KSC towards acting as a spaceport accessible to both government and commercial ventures. On April 21, 2016, NASA announced the selection of Orbital ATK to begin negotiations for High Bay 2. The \"potential agreement\" includes an existing mobile launcher platform.\n\n"}
{"id": "22117567", "url": "https://en.wikipedia.org/wiki?curid=22117567", "title": "ZooBorns", "text": "ZooBorns\n\nZooBorns is a zoology news blog and book line that announces animal births at AZA, EAZA, CAZA, ZAA, and WAZA accredited zoos and aquariums. ZooBorns was founded in 2008 with the mission to \"educate while it entertains\", and typically shares related conservation information along with pictures and video of baby animals.\n\nZooBorns has been featured in the Washington Post, NBC News, Discover Magazine, and on the Martha Stewart Show among other media outlets. The site was created by Andrew Bleiman, co-founder of Zooillogix who lives in Chicago, and Chris Eastland, an artist living in Brooklyn.\n\nZooBorns showcases baby animals as ambassadors for their species in order to build empathy and awareness for the plight of those species in the wild. Content is written to be accessible to a wide audience and typically provides background on individual animals followed by conservation information about the species. The site is notable for providing easily navigable categories enabling it to serve as a non-exhaustive survey of recent juvenile animals at zoos and aquariums. As of May 2012, the site had documented over 1,250 births, comprising over 200 species from over 200 different zoological institutions.\n\nIn 2010 ZooBorns released two books published by Simon & Schuster: \"ZooBorns,\" written for adults and young adults and \"ZooBorns!\n\"Zoo Babies from Around the World\", for young children, age 3-6. Both featured animal babies born at accredited zoos and aquariums around the world. \"ZooBorns\" was recognized as a 2012: American Library Association's Quick Pick for Reluctant Young Readers and \"ZooBorns!\n\"Zoo Babies from Around the World\" was a 2011–2012: Keystone to Reading Book Award Nominee.\n\nIn 2011 ZooBorns released \"ZooBorns Cats!,\" which showcased kitten and cub photos of 30 species of the 36 known feline species, including rare photos of the critically endangered Iriomote cat and vulnerable kodkod, also known as the guiña.\n\nIn 2012 ZooBorns is slated to release follow-ups to their first two books, \"ZooBorns The Next Generation,\" for all ages and \"ABC ZooBorns,\" for young children. Two Ready-to-Read books for toddlers will also be released, \"Welcome to the World, ZooBorns!\" and \"I Love You, ZooBorns!\"\n\n"}
