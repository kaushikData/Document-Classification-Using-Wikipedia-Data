{"id": "2441737", "url": "https://en.wikipedia.org/wiki?curid=2441737", "title": "Antimonate mineral", "text": "Antimonate mineral\n\nAntimonate minerals are those minerals containing the antimonate (SbO) anion group. Both the Dana and the Strunz mineral classifications place the antimonates in with the phosphate minerals.\n"}
{"id": "406245", "url": "https://en.wikipedia.org/wiki?curid=406245", "title": "Antonov An-225 Mriya", "text": "Antonov An-225 Mriya\n\nThe Antonov An-225 \"Mriya\" (, NATO reporting name: \"Cossack\") is a strategic airlift cargo aircraft that was designed by the Antonov Design Bureau in the Ukrainian SSR within the Soviet Union during the 1980s. It is powered by six turbofan engines and is the heaviest aircraft ever built, with a maximum takeoff weight of . It also has the largest wingspan of any aircraft in operational service. The single example built has the Ukrainian civil registration UR-82060. A second airframe with a slightly different configuration was partially built. Its construction was halted in 1994 because of lack of funding and interest, but revived briefly in 2009, bringing it to 60–70% completion. On 30 August 2016, Antonov agreed to complete the second airframe for Aerospace Industry Corporation of China (not to be confused with the Aviation Industry Corporation of China) as a prelude to AICC commencing series production.\n\nThe Antonov An-225, initially developed for the task of transporting the Buran spaceplane, was an enlargement of the successful Antonov An-124. The first and only An-225 was completed in 1988. After successfully fulfilling its Soviet military missions, it was mothballed for eight years. It was then refurbished and re-introduced, and is in commercial operation with Antonov Airlines carrying oversized payloads. The airlifter holds the absolute world records for an airlifted single-item payload of , and an airlifted total payload of . It has also transported a payload of on a commercial flight.\n\nThe Antonov An-225 was designed to airlift the Energia rocket's boosters and the \"Buran\" orbiter for the Soviet space program. It was developed as a replacement for the Myasishchev VM-T. The An-225's original mission and objectives are almost identical to that of the United States' Shuttle Carrier Aircraft.\n\nThe An-225 first flew on 21 December 1988 with a 74-minute flight from Kiev. The aircraft was on static display at the Paris Air Show in 1989 and it flew during the public days at the Farnborough air show in 1990. Two aircraft were ordered, but only one An-225 (registration CCCP-82060 later UR-82060) was finished. It can carry ultra-heavy and oversize freight, up to internally, or on the upper fuselage. Cargo on the upper fuselage can be long.\n\nA second An-225 was partially built during the late 1980s for the Soviet space program. Following the collapse of the Soviet Union in 1991 and the cancellation of the Buran space program, the lone operational An-225 was placed in storage in 1994. The six Ivchenko-Progress engines were removed for use on An-124s, and the second uncompleted An-225 airframe was also stored. When it became clear that a cargoliner bigger than the An-124 was needed, the first An-225 was re-engined and put back into service.\nBy 2000, the need for additional An-225 capacity had become apparent, so the decision was made in September 2006 to complete the second An-225. The second airframe was scheduled for completion around 2008, then delayed. By August 2009, the aircraft had not been completed and work had been abandoned. In May 2011, the Antonov CEO is reported to have said that the completion of a second An-225 Mriya transport aircraft with a carrying capacity of 250 tons requires at least $300 million, but if the financing is provided, its completion could be achieved in three years. According to different sources, the second aircraft is 60–70% complete.\n\nIn April 2013, the Russian government announced plans to revive Soviet-era air launch projects that would use a purpose-built modification to the An-225 as a midair launchpad.\n\nIn August 2016, representatives from Ukraine's Antonov and Airspace Industry Corporation of China (AICC), an import-export company operating out of Hong Kong, signed an agreement to recommence production of the An-225, with China now planning to procure and fly the first model by 2019. The aviation media cast doubt on the production restart, indicating that due to the ongoing Russia–Ukraine conflict needed parts from Russia are unavailable, although they may be made in China instead.\n\nAICC's president, Zhang You-Sheng, told a BBC reporter that AICC began to contemplate cooperation with Antonov in 2009 and contacted them in 2011. AICC intends to modernize the second unfinished An-225 and develop it into an air launch to orbit platform for commercial satellites at altitudes up to .\n\nBased on Antonov's earlier An-124, the An-225 has fuselage barrel extensions added fore and aft of the wings. The wings also received root extensions to increase span. Two more Progress D-18T turbofan engines were added to the new wing roots, bringing the total to six. An increased-capacity landing gear system with 32 wheels was designed, some of which are steerable, enabling the aircraft to turn within a runway. Like its An-124 predecessor, the An-225 has nose gear designed to \"kneel\" so cargo can be more easily loaded and unloaded. Unlike the An-124, which has a rear cargo door and ramp, the An-225 design left these off to save weight, and the empennage design was changed from a single vertical stabilizer to a twin tail with an oversized, swept-back horizontal stabilizer. The twin tail was essential to enable the plane to carry large, heavy external loads that would disturb the airflow around a conventional tail. Unlike the An-124, the An-225 was not intended for tactical airlifting and is not designed for short-field operation.\nInitially the An-225 had a maximum gross weight of , but from 2000 to 2001 the aircraft underwent modifications at a cost of US$20M such as the addition of a reinforced floor, which increased the maximum gross weight to .\n\nBoth the earlier and later takeoff weights establish the An-225 as the world's heaviest aircraft, being heavier than the double-deck Airbus A380. It is surpassed in other size-related categories, however: Airbus claims to have improved upon the An-225's maximum landing weight by landing an A380 at during tests, and the Hughes H-4 Hercules, known as the \"Spruce Goose\", has a greater wingspan and a greater overall height. But the Spruce Goose is 20% shorter and overall lighter, due to the materials used in its construction. In addition, the H-4 only flew once, making the An-225 the largest aircraft in the world to fly multiple times.\n\nThe An-225's pressurized cargo hold is in volume; wide, high, and long — longer than the first flight of the Wright Flyer.\n\nDuring the last years of the Soviet space program, the An-225 was employed as the prime method of transporting the \"Buran\" space shuttle.\n\nIn the late 1980s, the Soviet government was looking for a way to generate revenue from its military assets. In 1989, the Antonov Design Bureau set up a holding company as a heavy airlift shipping corporation under the name \"Antonov Airlines\", based in Kiev, Ukraine and operating from London Luton Airport in partnership with Air Foyle HeavyLift.\n\nThe company began operations with a fleet of four An-124-100s and three Antonov An-12s, but a need for aircraft larger than the An-124 became apparent in the late 1990s. In response, the original An-225 was re-engined, modified for heavy cargo transport, and placed back in service under the management of Antonov Airlines.\nOn 23 May 2001, the An-225 received its type certificate from the Interstate Aviation Committee Aviation Register (IAC AR). On 11 September 2001, carrying four main battle tanks at a record load of of cargo, the An-225 flew at an altitude of up to over a closed circuit of at a speed of .\n\nThe An-225 attracts a high degree of public interest, so much that it has managed to attain a global following due to its size and its uniqueness. People frequently visit airports to see its scheduled arrivals and departures, such as in Perth, Australia in May 2016 where a crowd of more than 50,000 people gathered at Perth Airport.\n\nThe type's first flight in commercial service departed from Stuttgart, Germany on 3 January 2002, and flew to Thumrait, Oman with 216,000 prepared meals for American military personnel based in the region. This vast number of ready meals was transported on 375 pallets and weighed 187.5 tons.\n\nThe An-225 has since become the workhorse of the Antonov Airlines fleet, transporting objects once thought impossible to move by air, such as 150-tonne generators. It has become an asset to international relief organizations for its ability to quickly transport huge quantities of emergency supplies during disaster relief operations.\n\nThe An-225 has been contracted by the Canadian and U.S. governments to transport military supplies to the Middle East in support of coalition forces. An example of the cost of shipping cargo by An-225 was over 2 million DKK (approx. €266,000) for flying a chimney duct from Billund, Denmark to Kazakhstan in 2004.\nOn 11 August 2009, the heaviest single cargo item ever sent via air freight was loaded onto the An-225. At long and wide, its consignment, a generator for a gas power plant in Armenia along with its loading frame, weighed in at a record .\n\nDuring 2009, the An-225 was painted in a new blue and yellow paint scheme, after Antonov ceased cooperation with AirFoyle and partnered with Volga-Dnepr in 2006.\n\nOn 11 June 2010, the An-225 carried the world's longest piece of air cargo, two test wind turbine blades from Tianjin, China to Skrydstrup, Denmark.\n\nIn May 2016 the plane transported a 130-ton payload from Prague, Czech Republic to Perth, Australia.\n\n\n"}
{"id": "2466971", "url": "https://en.wikipedia.org/wiki?curid=2466971", "title": "Archiv für Sozialwissenschaft und Sozialpolitik", "text": "Archiv für Sozialwissenschaft und Sozialpolitik\n\nThe Archiv für Sozialwissenschaft und Sozialpolitik (, English: Archives for Social Science and Social Welfare) was an academic journal for the social sciences in Germany between 1888 and 1933. Its first editors were Edgar Jaffé, Werner Sombart, and Max Weber. The latter published his seminal essay \"The Protestant Ethic and the Spirit of Capitalism\" in the journal in two parts in 1904 and 1905.\n\nJaffé bought the journal in 1903 for 60000 Mark from the Social Democrat Heinrich Braun, who had founded and edited the journal under the title \"Archiv für soziale Gesetzgebung und Statistik (Archive for Social Legislation and Statistics)\" since 1888 and changed its title to \"Archiv für Sozialwissenschaft und Sozialpolitik\" in 1904.\n\nIn 1933, when the Nazis gained power in Germany, the last editor of the Archive, then-editor Emil Lederer and half of the editorial staff of the journal were forced to emigrate and the journal ceased to exist.\n\n\n"}
{"id": "21743669", "url": "https://en.wikipedia.org/wiki?curid=21743669", "title": "Bare particle", "text": "Bare particle\n\nIn theoretical physics, a bare particle is an excitation of an elementary quantum field. Such a particle is not identical to the particles observed in the experiments: the real particles are dressed particles that also include additional particles surrounding the bare one.\n"}
{"id": "51696535", "url": "https://en.wikipedia.org/wiki?curid=51696535", "title": "Childhood Studies", "text": "Childhood Studies\n\nChildhood studies is an aim to understand the study of children; with examples including arts, humanities, natural and social sciences, medicine, and law. Emphasizing on an interdisciplinary and reasonable way to study the age range of young people between 0 -18; as well as using policy and practice to promote the rights of children.\n\nThe US National Children's Study was formulated by politicians who realized the importance of disciplines and the study of simple subjects. When compared to the British birth cohort studies, the US lacks a strong tradition for surveys of children. However, there have still been plenty of surveys focusing on their health. Certain theories claim that adults are mature and reasonable, children on the other hand are said to contrast from this as they are still developing. Methodological debates focus on seeing whether researching with young children is similar to researching with adults. Childhood studies is set up as a critical series of processes that has already been done, leading to the creations of mantras and how it focuses on the social construction and agency of young children; as well as valuing their voice, participation, and experience in life.\n\nIt is also said childhood studies retain a modernist agenda in doing this, but the rest of sociology has refused to place it as a primary role to instead search for complexity rather than social structures. Reports indicate that adults lack stability as they progress through their working lives and states they are trying to reach the goal of becoming a human being, while children are also in this pursuit as they grow.\n\nDespite the beliefs that children aren't as capable of life as adults are, childhood studies have proven that young children are competent of certain abilities when emphasizing their own autonomy, an example being a social actor. The concept of children's agency is about the means to act, considering the fact that there are many misunderstandings, difficulties, and tribulations that children or young people will have to solve eventually. Due to children's position in society, there are multiple opportunities and ways for children to exercise their social agency. The research of childhood studies shows how young people are socially competent, and that a bigger importance is on their complexities and ambiguities across context; likewise, theoretical advances made by psychology emphasize the need to contextualize the development of children.\n"}
{"id": "39466812", "url": "https://en.wikipedia.org/wiki?curid=39466812", "title": "Cláudio Costa Neto", "text": "Cláudio Costa Neto\n\nClaudio Costa Neto (born Rio de Janeiro, December 11, 1932) is a Brazilian chemical and chemical engineer, one of the founders of the Institute of Chemistry, UFRJ. He is currently emeritus professor at the Institute of Chemistry of the Federal University of Rio de Janeiro.\n\nHe got his BSc degree in Industrial Chemistry and Chemical Engineering from the University of Brazil (currently Universidade Federal do Rio de Janeiro) in 1954, Costa Neto worked under supervision of Fritz Feigl, responsible for the development of spot tests for identification and characterization of substances. He was responsible for creating the pioneering shale oil project (\"projeto xistoquímica\" in Portuguese). He was also responsible for the study of organic geochemistry at UFRJ.\n\nVila Rosário: trilogy about chemistry and society: first part: why and how to eliminate tuberculosis in a society, Claudio Costa Neto, Rio de Janeiro, Calamus Publisher, 480 pages, 2002.\n\nOrganic analysis: methods and procedures for characterizing organochemicals, Claudio Costa Neto, Rio de Janeiro, UFRJ publisher, 2004, Volumes 1 and 2 \n"}
{"id": "4107459", "url": "https://en.wikipedia.org/wiki?curid=4107459", "title": "Colin Stanley Gum", "text": "Colin Stanley Gum\n\nColin Stanley Gum (4 June 1924 – 29 April 1960) was an Australian astronomer who catalogued emission nebulae in the southern sky at the Mount Stromlo Observatory using wide field photography. Gum published his findings in 1955 in a study entitled \"A study of diffuse southern H-alpha nebulae\" which presented a catalog, now known as the Gum catalog, of 85 nebulae or nebular complexes. Gum 12, a huge area of nebulosity in the direction of the constellations Puppis and Vela, was later named the Gum Nebula in his honour. Gum was part of the team, whose number included Frank John Kerr and Gart Westerhout, that determined the precise position of the neutral hydrogen plane in space.\n\nGum was appointed Head of the Observational Optical Astronomy programme at the University of Sydney in 1959. He died in a skiing accident at Zermatt, Switzerland the following year.\n\nThe crater Gum on the Moon is named after him. An obituary article on Gum appears in the \"Australian Journal of Science\" (Vol. 23, no. 4, 1960). \n\nGum was the brother-in-law of prominent academic Fay Gale, and was uncle to businessman Michael Gale.\n\n"}
{"id": "674979", "url": "https://en.wikipedia.org/wiki?curid=674979", "title": "Cryptex", "text": "Cryptex\n\nThe word cryptex is a neologism coined by the author Dan Brown for his 2003 novel \"The Da Vinci Code\", denoting a portable vault used to hide secret messages. It is a word formed from Greek \"kryptós\", \"hidden, secret\" and Latin \"codex\"; \"an apt title for this device\" since it uses \"the science of \"cryptology\" to protect information written on the contained scroll or \"codex\"\" (p. 199 of the novel). The first physical cryptex was created by Justin Kirk Nevins in 2004. \"Cryptex\" is a registered trademark of Justin Kirk Nevins. \n\nThe (first) cryptex featured in the novel is described as a stone cylinder comprising \"five doughnut-sized disks of marble [that] had been stacked and affixed to one another within a delicate brass framework\"; end caps make it impossible to see inside the hollow cylinder. Each of the disks is carved with the entire alphabet and can be rotated independently of the others to create different letter-alignment combinations, including but not limited to words, initialisms, and anagrams. Although it is not clear whether the alphabet in question preserves the U/V and/or I/J distinctions and/or includes the letter W, when only the alignment of the disks with respect to each other is considered, the number of potential combinations is between 23 (279,841) and 26 (456,976); if the mechanism treats combinations having the same disk alignment but different degrees of rotation around the cylinder's long axis as distinct, as does a multiple-wheel combination lock or slot machine employing an indicator bar along which the specified numbers are to be aligned, this number rises to between 23 (6,436,343) and 26 (11,881,376).\n\nThe cryptex works \"much like a bicycle's combination lock\", and if one arranges the disks to spell out the correct password, \"the tumblers inside align, and the entire cylinder slides apart\" (p. 200). In the inner compartment of the cryptex, secret information can be hidden, written on a scroll of thin papyrus wrapped around a fragile vial of vinegar as a security measure: if one does not know the password but tries to force the cryptex open, the vial will break and the vinegar will dissolve the papyrus before it can be read.\n\n\n\n\n"}
{"id": "1382086", "url": "https://en.wikipedia.org/wiki?curid=1382086", "title": "Curie constant", "text": "Curie constant\n\nThe Curie constant is a material-dependent property that relates a material's magnetic susceptibility to its temperature.\n\nThe Curie constant, when expressed in SI units, is given by \n\nwhere formula_2 is the number of magnetic atoms (or molecules) per unit volume, formula_3 is the Landé g-factor, formula_4 is the Bohr magneton, formula_5 is the angular momentum quantum number and formula_6 is Boltzmann's constant. For a two-level system with magnetic moment formula_7, the formula reduces to\n\nwhile the corresponding expressions in Gaussian units are\n\nThe constant is used in Curie's Law, which states that for a fixed value of a magnetic field, the magnetization of a material is (approximately) inversely proportional to temperature.\n\nThis equation was first derived by Pierre Curie.\n\nBecause of the relationship between magnetic susceptibility formula_12, magnetization formula_13 and applied magnetic field formula_14 is almost linear at low fields, then\n\nthis shows that for a paramagnetic system of non-interacting magnetic moments, magnetization formula_13 is inversely related to temperature formula_17 (see Curie's Law).\n\n"}
{"id": "21753066", "url": "https://en.wikipedia.org/wiki?curid=21753066", "title": "Decision rule", "text": "Decision rule\n\nIn decision theory, a decision rule is a function which maps an observation to an appropriate action. Decision rules play an important role in the theory of statistics and economics, and are closely related to the concept of a strategy in game theory.\n\nIn order to evaluate the usefulness of a decision rule, it is necessary to have a loss function detailing the outcome of each action under different states.\n\nGiven an observable random variable \"X\" over the probability space formula_1, determined by a parameter \"θ\" ∈ \"Θ\", and a set \"A\" of possible actions, a (deterministic) decision rule is a function \"δ\" : formula_2→ \"A\".\n\n\n"}
{"id": "11286601", "url": "https://en.wikipedia.org/wiki?curid=11286601", "title": "Digital conversation", "text": "Digital conversation\n\nA digital conversation is a scripted dialogue (in other words it is dialogue written by a human, just like the script of a movie) which takes place between a person and a computer via any digital medium from web browsers and PDAs to mobile phones and Interactive television.\n\nA digital conversation is scripted by a human, uploaded to a server where it can be accessed as a web service by other humans (consumers, employees etc.) and used to impart information to them, whether that information is advising them on the best camera to buy, helping them tailor make a credit card or engaging them in an interactive book.\n\nA digital conversation can be undertaken simultaneously from multiple digital channels. In other words, no matter what means you are using to access a digital conversation by, whether it's a browser or a mobile phone, you are calling the same digital conversation. This means that any changes made to a digital conversation are reflected across all channels immediately. This allows digital conversations to evolve.\n\nRather than throwing content at consumers, digital conversations are designed to engage them in a scripted conversation, to find out what they want and guide them to the outcome they desire. It is this Dialogue Marketing that many perceive as being the way forward, and digital conversations provide a solution to delivering this at scale. It moves away from a traditional one way stream of information with a consumers (offering every consumer the same choices) and moves towards a dialogue, finding out what it is they want and giving it to them. This movement is seen as essential by many:\n\nIn the same New York Times article Robert M. Greenberg then states that he wants to \"engage (consumers) in digital conversations that are so entertaining, involving and valuable that they won't want to ignore them.\" The key word here is \"engage\". digital conversations are created to be, in essence, human interactions and dialogue with one human removed. Unlike Bots and Avatars, digital conversations are scripted, and this may well lead to more human like interactions.\n\nDigital conversations consist of a four-stage \"bio-system\":\n\nCreate - Consisting of the preliminary creation and refining of the digital conversation including its front end\n\nInteract - Whereby consumers engage with the digital conversation\n\nUnderstand - The usage data of every consumer engaged with a Conversation is automatically stored and aggregated and can be viewed quickly and easily\n\nAdapt - This usage data may lead to changes to the digital conversation which are reflected in real time across all channels (as the Conversation is called from one source no matter what the medium accessing it).\n\nEach digital conversation is made up of a dialogue based script consisting of a narrative and choices with pathways that lead to Outcomes. It is the user, through their choices, who decides which pathway to follow. The script takes the form of a Decision-Tree and is the backbone of the digital conversation. Each Decision-Tree defines two or more Pathways. The end point of a Pathway is an Outcome or a loosely coupled Link to another digital conversation enabling longer digital conversation flows.\n\nBy embracing the concepts of Web 2.0 and allowing infinite numbers of digital conversations to be linked, this can allow hugely complex subjects to be tackled. One digital conversation written by an expert in Europe can be linked to an existing digital conversation written by an expert in Africa. This allows in theory, vast knowledge landscapes made up of components.\n\nA digital conversation can contain good dialogue or bad dialogue. Good dialogue typically means the aggregated interaction of each Dialogue-Step is undertaken in less than 5 seconds. This means a person can read and understand a Dialogue-Step after one read through and wants to continue to the next Dialogue-Step.\n\nBad dialogue has the opposite effect. Unclear language, for example, can cause confusion and have an adverse effect on a user, worst-case scenario is when a user leaves a digital conversation in despair. Luckily thanks to the available Metrics, such problems can be spotted quickly (numerous users exiting on one particular step indicates an issue with that step) and acted upon, with the dialogue changed as and when needed.\n\nSo, the development of a good digital conversation which will engage users requires a combination of skills in particular:\n\n» Two-way communication\n\n» Decision-tree logic\n\n» Behavioural economics\n\nA digital conversation can be created for any scripted dialogue, thus it is suited for marketing, sales, support, practices, guides, policies, procedures and much more.\n\nThanks to the nature of digital conversations (that they can be accessed across multiple channels from the same source) there are numerous ways to interact with them, whether by browser (see image to the left), mobile phone (see image above) or even through voice.\n\nDue to the Web Service nature of digital conversations, the front ends designed to access them can be extremely diverse (see Links section for a number of examples) and thus are able to reflect branding needs or the needs of the user (for example large type for partially sighted users).\n\nOne of the exciting recent developments has been the use of digital conversations to drive Avatars.\n\nOne of the trends of recent years has been the humanising of digital channels, giving a face to things which are not human. This has led to the creation of avatars (also known as bots or chatter-bots) artificial intelligences with which users can “converse”. The success of such bots varies greatly, there are few which respond in a convincingly human way, it is no great mystery why they are commonly referred to as “Bots” often resulting in a stilted, mechanical interaction where straying off a recognized path can lead to poor responses . However, this has not stopped their spread across the commercial world with several high-profile companies adopting them as part of their customer services.\n\nAvatars such as IKEA's Anna have generated interest both in the business community and among the public. The idea of an artificial intelligence able to respond in an intelligent manner to your questions is indeed an exciting one. However, do these bots really manage it? Or are they just human faced Avatars disguising a search engine beneath?\n\nWeb 2.0 Avatars, powered by digital conversations, provide a level of immersion not found in these bots. Why? Because digital conversations are scripted just like any good book or film. And like books and films they are designed to guide a user, through high quality dialogue and interactions, to an outcome. Along with this, the ability to understand user interactions through DecisionMetrics means that these Web 2.0 Avatars can be adapted to emergent demands as they appear. The dialogue can be improved and built up as and when needed.\n\nHigh quality dialogue, clear concise options for a user to choose and a humanized avatar all combine to create an immersive experience, with the psychological appeal of interacting with a character or object .\n\nThe key to immersion and believability is high quality dialogue, and it is high quality dialogue that digital conversations has been created for.\n\nThe digitlisation of business has been underway for years now, but the desire to do something more, to truly engage customers is growing stronger and stronger... IBM have recently taken the initiative with this by creating a Virtual Business Centre in the Virtual World of Second Life which allows clients, partners and sales assistants to interact. It is areas like this that digital conversations might be put to use allowing users to interact with Avatars 24 hours a day 7 days a week.\n\nThe automated recording of a digital conversation establishes the analytical science of interaction at scale. As a person interacts with a digital conversation Service each Dialogue-Step is recorded verbatim with a date and time step. Anonymous recording of consumers usage provides aggregated emergent patterns without infringing data privacy.\n\nThis measurement, providing the aggregated performance of each Dialogue-Step, Pathway and Outcome provides the scientific basis for Demand-Sensing allowing behavioral data to become quantifiable.\n\nOnce the usage data has been viewed and understood it may well be necessary to react by changing or amending the digital conversation.\n\n"}
{"id": "1268258", "url": "https://en.wikipedia.org/wiki?curid=1268258", "title": "Eustatic sea level", "text": "Eustatic sea level\n\nThe eustatic sea level is the distance from the center of the earth to the sea surface. An increase of the eustatic sea level can be generated by decreasing glaciation, slower spreading rates of the mid-ocean ridges or fewer mid-oceanic ridges. Conversely increasing glaciation, faster spreading rates or more mid-ocean ridges lead to a fall of the eustatic sea level.\n\nChanges in the eustatic sea level lead to changes in accommodation and therefore affect the deposition of sediments in marine environments.\n"}
{"id": "21312269", "url": "https://en.wikipedia.org/wiki?curid=21312269", "title": "Executive dysfunction", "text": "Executive dysfunction\n\nIn psychology and neuroscience, executive dysfunction, or executive function deficit, is a disruption to the efficacy of the executive functions, which is a group of cognitive processes that regulate, control, and manage other cognitive processes. Executive dysfunction can refer to both neurocognitive deficits and behavioural symptoms. It is implicated in numerous psychopathologies and mental disorders, as well as short-term and long-term changes in non-clinical executive control.\n\nExecutive dysfunction is not the same as dysexecutive syndrome, a term coined by Alan Baddeley to describe a common pattern of dysfunction in executive functions, such as deficiencies in planning, abstract thinking, flexibility and behavioural control. This group of symptoms, usually resulting from brain damage, tend to occur together. However, the existence of dysexecutive syndrome is controversial.\n\nExecutive functioning is a theoretical construct representing a domain of cognitive processes that regulate, control, and manage other cognitive processes. Executive functioning is not a unitary concept; it is a broad description of the set of processes involved in certain areas of cognitive and behavioural control. Executive processes are integral to higher brain function, particularly in the areas of goal formation, planning, goal-directed action, self-monitoring, attention, response inhibition, and coordination of complex cognition and motor control for effective performance. Deficits of the executive functions are observed in all populations to varying degrees, but severe executive dysfunction can have devastating effects on cognition and behaviour in both individual and social contexts.\n\nExecutive dysfunction does occur to a minor degree in all individuals on both short-term and long-term scales. In non-clinical populations, the activation of executive processes appears to inhibit further activation of the same processes, suggesting a mechanism for normal fluctuations in executive control. Decline in executive functioning is also associated with both normal and clinical aging. In aging populations, the decline of memory processes appears to affect executive functions, which also points to the general role of memory in executive functioning.\n\nExecutive dysfunction appears to consistently involve disruptions in task-oriented behavior, which requires executive control in the inhibition of habitual responses and goal activation. Such executive control is responsible for adjusting behaviour to reconcile environmental changes with goals for effective behaviour. Impairments in set shifting ability are a notable feature of executive dysfunction; set shifting is the cognitive ability to dynamically change focus between points of fixation based on changing goals and environmental stimuli. This offers a parsimonious explanation for the common occurrence of impulsive, hyperactive, disorganized, and aggressive behaviour in clinical patients with executive dysfunction. Executive dysfunction, particularly in working memory capacity, may also lead to varying degrees of emotional dysregulation, which can manifest as chronic depression, anxiety, or hyperemotionality. Russell Barkley proposed a hybrid model of the role of behavioural disinhibition in the presentation of ADHD, which has served as the basis for much research of both ADHD and broader implications of the executive system.\n\nOther common and distinctive symptoms of executive dysfunction include utilization behaviour, which is compulsive manipulation/use of nearby objects due simply to their presence and accessibility (rather than a functional reason); and imitation behaviour, a tendency to rely on imitation as a primary means of social interaction. Research also suggests that executive set shifting is a co-mediator with episodic memory of feeling-of-knowing (FOK) accuracy, such that executive dysfunction may reduce FOK accuracy.\n\nThere is some evidence suggesting that executive dysfunction may produce beneficial effects as well as maladaptive ones. Abraham et al. demonstrate that creative thinking in schizophrenia is mediated by executive dysfunction, and they establish a firm etiology for creativity in psychoticism, pinpointing a cognitive preference for broader top-down associative thinking versus goal-oriented thinking, which closely resembles aspects of ADHD. It is postulated that elements of psychosis are present in both ADHD and schizophrenia/schizotypy due to dopamine overlap.\n\nThe cause of executive dysfunction is heterogeneous, as many neurocognitive processes are involved in the executive system and each may be compromised by a range of genetic and environmental factors. Learning and development of long-term memory play a role in the severity of executive dysfunction through dynamic interaction with neurological characteristics. Studies in cognitive neuroscience suggest that executive functions are widely distributed throughout the brain, though a few areas have been isolated as primary contributors. Executive dysfunction is studied extensively in clinical neuropsychology as well, allowing correlations to be drawn between such dysexecutive symptoms and their neurological correlates.\n\nExecutive processes are closely integrated with memory retrieval capabilities for overall cognitive control; in particular, goal/task-information is stored in both short-term and long-term memory, and effective performance requires effective storage and retrieval of this information.\n\nExecutive dysfunction characterizes many of the symptoms observed in numerous clinical populations. In the case of acquired brain injury and neurodegenerative diseases there is a clear neurological etiology producing dysexecutive symptoms. Conversely, syndromes and disorders are defined and diagnosed based on their symptomatology rather than etiology. Thus, while Parkinson's disease, a neurodegenerative condition, causes executive dysfunction, a disorder such as attention-deficit/hyperactivity disorder is a classification given to a set of subjectively-determined symptoms implicating executive dysfunction – current models indicate that such clinical symptoms are caused by executive dysfunction.\n\nAs previously mentioned, executive functioning is not a unitary concept. Many studies have been conducted in an attempt to pinpoint the exact regions of the brain that lead to executive dysfunction, producing a vast amount of often conflicting information indicating wide and inconsistent distribution of such functions. A common assumption is that disrupted executive control processes are associated with pathology in prefrontal brain regions. This is supported to some extent by the primary literature, which shows both pre-frontal activation and communication between the pre-frontal cortex and other areas associated with executive functions such as the basal ganglia and cerebellum.\n\nIn most cases of executive dysfunction, deficits are attributed to either frontal lobe damage or dysfunction, or to disruption in fronto-subcortical connectivity. Neuroimaging with PET and fMRI has confirmed the relationship between executive function and functional frontal pathology. Neuroimaging studies have also suggested that some constituent functions are not discretely localized in prefrontal regions. Functional imaging studies using different tests of executive function have implicated the dorsolateral prefrontal cortex to be the primary site of cortical activation during these tasks. In addition, PET studies of patients with Parkinson's disease have suggested that tests of executive function are associated with abnormal function in the globus pallidus and appear to be the genuine result of basal ganglia damage.\n\nWith substantial cognitive load, fMRI signals indicate a common network of frontal, parietal and occipital cortices, thalamus, and the cerebellum. This observation suggests that executive function is mediated by dynamic and flexible networks that are characterized using functional integration and effective connectivity analyses. The complete circuit underlying executive function includes both a direct and an indirect circuit. The neural circuit responsible for executive functioning is, in fact, located primarily in the frontal lobe. This main circuit originates in the dorsolateral prefrontal cortex/orbitofrontal cortex and then projects through the striatum and thalamus to return to the prefrontal cortex.\n\nNot surprisingly, plaques and tangles in the frontal cortex can cause disruption in functions as well as damage to the connections between prefrontal cortex and the hippocampus. Another important point is in the finding that structural MRI images link the severity of white matter lesions to deficits in cognition.\n\nThe emerging view suggests that cognitive processes materialize from networks that span multiple cortical sites with closely collaborative and over-lapping functions. A challenge for future research will be to map the multiple brain regions that might combine with each other in a vast number of ways, depending on the task requirements.\n\nCertain genes have been identified with a clear correlation to executive dysfunction and related psychopathologies. According to Friedman \"et al.\" (2008), the heritability of executive functions is among the highest of any psychological trait. The dopamine receptor D4 gene (DRD4) with 7'-repeating polymorphism (7R) has been repeatedly shown to correlate strongly with impulsive response style on psychological tests of executive dysfunction, particularly in clinical ADHD. The catechol-o-methyl transferase gene (COMT) codes for an enzyme that degrades catecholamine neurotransmitters (DA and NE), and its Val158Met polymorphism is linked with the modulation of task-oriented cognition and behavior (including set shifting) and the experience of reward, which are major aspects of executive functioning. COMT is also linked to methylphenidate (stimulant medication) response in children with ADHD. Both the DRD4/7R and COMT/Val158Met polymorphisms are also correlated with executive dysfunction in schizophrenia and schizotypal behaviour.\n\nThere are several measures that can be employed to assess the executive functioning capabilities of an individual. Although a trained non-professional working outside of an institutionalized setting can legally and competently perform many of these measures, a trained professional administering the test in a standardized setting will yield the most accurate results.\n\nThe Clock drawing test (CDT) is a brief cognitive task that can be used by physicians who suspect neurological dysfunction based on history and physical examination. It is relatively easy to train non-professional staff to administer a CDT. Therefore, this is a test that can easily be administered in educational and geriatric settings and can be utilized as a precursory measure to indicate the likelihood of further/future deficits. Also, generational, educational and cultural differences are not perceived as impacting the utility of the CDT.\n\nThe procedure of the CDT begins with the instruction to the participant to draw a clock reading a specific time (generally 11:10). After the task is complete, the test administrator draws a clock with the hands set at the same specific time. Then the patient is asked to copy the image. Errors in clock drawing are classified according to the following categories: omissions, perseverations, rotations, misplacements, distortions, substitutions and additions. Memory, concentration, initiation, energy, mental clarity and indecision are all measures that are scored during this activity. Those with deficits in executive functioning will often make errors on the first clock but not the second. In other words, they will be unable to generate their own example, but will show proficiency in the copying task.\n\nThe cognitive mechanism involved in the Stroop task is referred to as directed attention. The Stroop task requires the participant to engage in and allows assessment of processes such as attention management, speed and accuracy of reading words and colours and of inhibition of competing stimuli. The stimulus is a colour word that is printed in a different colour than what the written word reads. For example, the word \"red\" is written in a blue font. One must verbally classify the colour that the word is displayed/printed in, while ignoring the information provided by the written word. In the aforementioned example, this would require the participant to say \"blue\" when presented with the stimulus. Although the majority of people will show some slowing when given incompatible text versus font colour, this is more severe in individuals with deficits in inhibition. The Stroop task takes advantage of the fact that most humans are so proficient at reading colour words that it is extremely difficult to ignore this information, and instead acknowledge, recognize and say the colour the word is printed in. The Stroop task is an assessment of attentional vitality and flexibility. More modern variations of the Stroop task tend to be more difficult and often try to limit the sensitivity of the test.\n\nThe Wisconsin Card Sorting Test (WCST) is used to determine an individual's competence in abstract reasoning, and the ability to change problem-solving strategies when needed. These abilities are primarily determined by the frontal lobes and basal ganglia, which are crucial components of executive functioning; making the WCST a good measure for this purpose.\n\nThe WCST utilizes a deck of 128 cards that contains four stimulus cards. The figures on the cards differ with respect to color, quantity, and shape. The participants are then given a pile of additional cards and are asked to match each one to one of the previous cards. Typically, children between ages 9 and 11 are able to show the cognitive flexibility that is needed for this test.\n\nAnother prominent test of executive dysfunction is known as the Trail-making test. This test is composed of two main parts (Part A & Part B). Part B differs from Part A specifically in that it assesses more complex factors of motor control and perception. Part B of the Trail-making test consists of multiple circles containing letters (A-L) and numbers (1-12). The participant's objective for this test is to connect the circles in order, alternating between number and letter (e.g. 1-A-2-B) from start to finish. The participant is required not to lift their pencil from the page. The task is also timed as a means of assessing speed of processing. Set-switching tasks in Part B have low motor and perceptual selection demands, and therefore provide a clearer index of executive function. Throughout this task, some of the executive function skills that are being measured include impulsivity, visual attention and motor speed.\n\nThe executive system's broad range of functions relies on, and is instrumental in, a broad range of neurocognitive processes. Clinical presentation of severe executive dysfunction that is unrelated to a specific disease or disorder is classified as a dysexecutive syndrome, and often appears following damage to the frontal lobes of the cerebral cortex. As a result, Executive dysfunction is implicated etiologically and/or co-morbidly in many psychiatric illnesses, which often show the same symptoms as the dysexecutive syndrome. It has been assessed and researched extensively in relation to cognitive developmental disorders, psychotic disorders, affective disorders, and conduct disorders, as well as neurodegenerative diseases and acquired brain injury (ABI).\n\nEnvironmental dependency syndrome is a dysexecutive syndrome marked by significant behavioural dependence on environmental cues and is marked by excessive imitation and utilization behaviour. It has been observed in patients with a variety of etiologies including ABI, exposure to phendimetrazine tartrate, stroke, and various frontal lobe lesions.\n\nSchizophrenia is commonly described as a mental disorder in which a person becomes detached from reality because of disruptions in the pattern of thinking and perception. Although the etiology is not completely understood, it is closely related to dopaminergic activity and is strongly associated with both neurocognitive and genetic elements of executive dysfunction. Individuals with schizophrenia may demonstrate amnesia for portions of their episodic memory. Observed damage to explicit, consciously accessed, memory is generally attributed to the fragmented thoughts that characterize the disorder. These fragmented thoughts are suggested to produce a similarly fragmented organization in memory during encoding and storage, making retrieval more difficult. However, implicit memory is generally preserved in patients with schizophrenia.\n\nPatients with schizophrenia demonstrate spared performance on measures of visual and verbal attention and concentration, as well as on immediate digit span recall, suggesting that observed deficits cannot be attributed to deficits in attention or short-term memory. However, impaired performance was measured on psychometric measures assumed to assess higher order executive function. Working memory and multi-tasking impairments typically characterize the disorder. Persons with schizophrenia also tend to demonstrate deficits in response inhibition and cognitive flexibility.\n\nPatients often demonstrate noticeable deficits in the central executive component of working memory as conceptualized by Baddeley and Hitch. However, performance on tasks associated with the phonological loop and visuospatial sketchpad are typically less affected. More specifically, patients with schizophrenia show impairment to the central executive component of working memory, specific to tasks in which the visuospatial system is required for central executive control. The phonological system appears to be more generally spared overall.\n\nA triad of core symptoms, namely inattention, hyperactivity, and impulsivity characterize attention deficit/hyperactivity disorder. Individuals with ADHD often experience problems with organization, discipline, and setting priorities, and these difficulties often persist from childhood through adulthood. In both children and adults with ADHD, an underlying executive dysfunction involving the prefrontal regions and other interconnected subcortical structures has been found. As a result, people with ADHD commonly perform more poorly than matched controls on interference control, mental flexibility and verbal fluency. Also, a more central impairment in self-regulation is noted in cases of ADHD. However, some research has suggested the possibility that the severity of executive dysfunction in individuals with ADHD declines with age as they learn to compensate for the aforementioned deficits. Thus, a decrease in executive dysfunction in adults with ADHD as compared to children with ADHD is thought reflective of compensatory strategies employed on behalf of the adults (e.g. using schedules to organize tasks) rather than neurological differences.\n\nAlthough ADHD has typically been conceptualized in a categorical diagnostic paradigm, it has also been proposed that this disorder should be considered within a more dimensional behavioural model that links executive functions to observed deficits. Proponents argue that classic conceptions of ADHD falsely localize the problem at perception (input) rather than focusing on the inner processes involved in producing appropriate behaviour (output). Moreover, others have theorized that the appropriate development of inhibition (something that is seen to be lacking in individuals with ADHD) is essential for the normal performance of other neuropsychological abilities such as working memory, and emotional self-regulation. Thus, within this model, deficits in inhibition are conceptualized to be developmental and the result of atypically operating executive systems.\n\nAutism is diagnosed based on the presence of markedly abnormal or impaired development in social interaction and communication and a markedly restricted repertoire of activities and interests. It is a disorder that is defined according to behaviour as no specific biological markers are known. Due to the variability in severity and impairment in functioning exhibited by persons with autism, the disorder is typically conceptualized as existing along a continuum (or spectrum) of severity.\n\nIndividuals with autism commonly show impairment in three main areas of executive functioning:\n\n\nAlthough there has been some debate, inhibition is generally no longer considered to be an executive function deficit in people with autism. Individuals with autism have demonstrated differential performance on various tests of inhibition, with results being taken to indicate a general difficulty in the inhibition of a habitual response. However, performance on the Stroop task, for example, has been unimpaired relative to matched controls. An alternative explanation has suggested that executive function tests that demonstrate a clear rationale are passed by individuals with autism. In this light, it is the design of the measures of inhibition that have been implicated in the observation of impaired performance rather than inhibition being a core deficit.\n\nIn general, individuals with autism show relatively spared performance on tasks that do not require mentalization. These include: use of desire and emotion words, sequencing behavioural pictures, and the recognition of basic facial emotional expressions. In contrast, individuals with autism typically demonstrated impaired performance on tasks that do require mentalizing. These include: false beliefs, use of belief and idea words, sequencing mentalistic pictures, and recognizing complex emotions such as admiring or scheming.\n\nBipolar disorder is a mood disorder that is characterized by both highs (mania) and lows (depression) in mood. These changes in mood sometimes alternate rapidly (changes within days or weeks) and sometimes not so rapidly (within weeks or months). Current research provides strong evidence of cognitive impairments in individuals with bipolar disorder, particularly in executive function and verbal learning. Moreover, these cognitive deficits appear to be consistent cross-culturally, indicating that these impairments are characteristic of the disorder and not attributable to differences in cultural values, norms, or practice. Functional neuroimaging studies have implicated abnormalities in the dorsolateral prefrontal cortex and the anterior cingulate cortex as being volumetrically different in individuals with bipolar disorder.\n\nIndividuals affected by bipolar disorder exhibit deficits in strategic thinking, inhibitory control, working memory, attention, and initiation that are independent of affective state. In contrast to the more generalized cognitive impairment demonstrated in persons with schizophrenia, for example, deficits in bipolar disorder are typically less severe and more restricted. It has been suggested that a \"stable dys-regulation of prefrontal function or the subcortical-frontal circuitry [of the brain] may underlie the cognitive disturbances of bipolar disorder\". Executive dysfunction in bipolar disorder is suggested to be associated particularly with the manic state, and is largely accounted for in terms of the formal thought disorder that is a feature of mania. It is important to note, however, that patients with bipolar disorder with a history of psychosis demonstrated greater impairment on measures of executive functioning and spatial working memory compared with bipolar patients without a history of psychosis suggesting that psychotic symptoms are correlated with executive dysfunction.\n\nParkinson's disease (PD) primarily involves damage to subcortical brain structures and is usually associated with movement difficulties, in addition to problems with memory and thought processes. Persons affected by PD often demonstrate difficulties in working memory, a component of executive functioning. Cognitive deficits found in early PD process appear to involve primarily the fronto-executive functions. Moreover, studies of the role of dopamine in the cognition of PD patients have suggested that PD patients with inadequate dopamine supplementation are more impaired in their performance on measures of executive functioning. This suggests that dopamine may contribute to executive control processes. Increased distractibility, problems in set formation and maintaining and shifting attentional sets, deficits in executive functions such as self-directed planning, problems solving, and working memory have been reported in PD patients. In terms of working memory specifically, persons with PD show deficits in the areas of: a) spatial working memory; b) central executive aspects of working memory; c) loss of episodic memories; d) locating events in time.\n\nSpatial working memory.\nPD patients often demonstrate difficulty in updating changes in spatial information and often become disoriented. They do not keep track of spatial contextual information in the same way that a typical person would do almost automatically. Similarly, they often have trouble remembering the locations of objects that they have recently seen, and thus also have trouble with encoding this information into long-term memory.\n\nCentral executive aspects.\nPD is often characterized by a difficulty in regulating and controlling one's stream of thought, and how memories are utilized in guiding future behaviour. Also, persons affected by PD often demonstrate perseverative behaviours such as continuing to pursue a goal after it is completed, or an inability to adopt a new strategy that may be more appropriate in achieving a goal. However, some recent research suggests that PD patients may actually be less persistent in pursuing goals than typical persons and may abandon tasks sooner when they encounter problems of a higher level of difficulty.\n\nLoss of episodic memories.\nThe loss of episodic memories in PD patients typically demonstrates a temporal gradient wherein older memories are generally more preserved than newer memories. Also, while forgetting event content is less compromised in Parkinson's than in Alzheimer's, the opposite is true for event data memories.\n\nLocating events in time.\nPD patients often demonstrate deficits in their ability to sequence information, or date events. Part of the problems is hypothesized to be due to a more fundamental difficulty in coordinating or planning retrieval strategies, rather than failure at the level of encoding or storing information in memory. This deficit is also likely to be due to an underlying difficulty in properly retrieving script information. PD patients often exhibit signs of irrelevant intrusions, incorrect ordering of events, and omission of minor components in their script retrieval, leading to disorganized and inappropriate application of script information.\n\nSince 1997 there has been experimental and clinical practice of psychosocial treatment for adults with executive dysfunction, and particularly attention-deficit/hyperactivity disorder (ADHD). Psychosocial treatment addresses the many facets of executive difficulties, and as the name suggests, covers academic, occupational and social deficits. Psychosocial treatment facilitates marked improvements in major symptoms of executive dysfunction such as time management, organization and self-esteem.\n\nCognitive-behavioural therapy (CBT) is a frequently suggested treatment for executive dysfunction, but has shown limited effectiveness. However, a study of CBT in a group rehabilitation setting showed a significant increase in positive treatment outcome compared with individual therapy. Patients' self-reported symptoms on 16 different ADHD/executive-related items were reduced following the treatment period.\n\nThe use of auditory stimuli has been examined in the treatment of dysexecutive syndrome. The presentation of auditory stimuli causes an interruption in current activity, which appears to aid in preventing \"goal neglect\" by increasing the patients' ability to monitor time and focus on goals. Given such stimuli, subjects no longer performed below their age group average IQ.\n\nPatients with acquired brain injury have also been exposed to goal management training (GMT). GMT skills are associated with paper-and-pencil tasks that are suitable for patients having difficulty setting goals. From these studies there has been support for the effectiveness of GMT and the treatment of executive dysfunction due to ABI.\n\nAn understanding of how executive dysfunction shapes development has implications how we conceptualize executive functions and their role in shaping the individual. Disorders affecting children such as ADHD, along with oppositional defiant disorder, conduct disorder, high functioning autism and Tourette's syndrome have all been suggested to involve executive functioning deficits. The main focus of current research has been on working memory, planning, set shifting, inhibition, and fluency. This research suggests that differences exist between typically functioning, matched controls and clinical groups, on measures of executive functioning.\n\nSome research has suggested a link between a child's abilities to gain information about the world around them and having the ability to override emotions in order to behave appropriately. One study required children to perform a task from a series of psychological tests, with their performance used as a measure of executive function. The tests included assessments of: executive functions (self-regulation, monitoring, attention, flexibility in thinking), language, sensorimotor, visuospatial, and learning, in addition to social perception. The findings suggested that the development of theory of mind in younger children is linked to executive control abilities with development impaired in individuals who exhibit signs of executive dysfunction.\n\nBoth ADHD and obesity are complicated disorders and each produces a large impact on an individual's social well being. This being both a physical and psychological disorder has reinforced that obese individuals with ADHD need more treatment time (with associated costs), and are at a higher risk of developing physical and emotional complications. The cognitive ability to develop a comprehensive self-construct and the ability to demonstrate capable emotion regulation is a core deficit observed in people with ADHD and is linked to deficits in executive function. Overall, low executive functioning seen in individuals with ADHD has been correlated with tendencies to overeat, as well as with emotional eating. This particular interest in the relationship between ADHD and obesity is rarely clinically assessed and may deserve more attention in future research.\n\nIt has been made known that young children with behavioral problems show poor verbal ability and executive functions. The exact distinction between parenting style and the importance of family structure on child development is still somewhat unclear. However, in infancy and early childhood, parenting is among the most critical external influences on child reactivity. In Mahoney's study of maternal communication, results indicated that the way mothers interacted with their children accounted for almost 25% of variability in children's rate of development. Every child is unique, making parenting an emotional challenge that should be most closely related to the child's level of emotional self-regulation (persistence, frustration and compliance). A promising approach that is currently being investigated amid intellectually disabled children and their parents is responsive teaching. Responsive teaching is an early intervention curriculum designed to address the cognitive, language, and social needs of young children with developmental problems. Based on the principle of \"active learning\", responsive teaching is a method that is currently being applauded as adaptable for individual caregivers, children and their combined needs The effect of parenting styles on the development of children is an important area of research that seems to be forever ongoing and altering. There is no doubt that there is a prominent link between parental interaction and child development but the best child rearing technique continues to vary amongst experts.\n\nThe prefrontal lobe controls two related executive functioning domains. The first is mediation of abilities involved in planning, problem solving, and understanding information, as well as engaging in working memory processes and controlled attention. In this sense, the prefrontal lobe is involved with dealing with basic, everyday situations, especially those involving metacognitive functions. The second domain involves the ability to fulfill biological needs through the coordination of cognition and emotions which are both associated with the frontal and prefrontal areas.\n\nFrom an evolutionary perspective, it has been hypothesized that the executive system may have evolved to serve several adaptive purposes. The prefrontal lobe in humans has been associated both with metacognitive executive functions and emotional executive functions. Theory and evidence suggest that the frontal lobes in other primates also mediate and regulate emotion, but do not demonstrate the metacognitive abilities that are demonstrated in humans. This uniqueness of the executive system to humans implies that there was also something unique about the environment of ancestral humans, which gave rise to the need for executive functions as adaptations to that environment. Some examples of possible adaptive problems that would have been solved by the evolution of an executive system are: social exchange, imitation and observational learning, enhanced pedagogical understanding, tool construction and use, and effective communication.\n\nIn a similar vein, some have argued that the unique metacognitive capabilities demonstrated by humans have arisen out of the development of a sophisticated language (symbolization) systems and culture. Moreover, in a developmental context, it has been proposed that each executive function capability originated as a form of public behaviour directed at the external environment, but then became self-directed, and then finally, became private to the individual, over the course of the development of self-regulation. These shifts in function illustrate the evolutionarily salient strategy of maximizing longer-term social consequences over near-term ones, through the development of an internal control of behaviour.\n\nFlexibility problems are more likely to be related to Anxiety, and Metacognition problems are more likely to be related to depression.\n\nIn the classroom environment, children with executive dysfunction typically demonstrate skill deficits that can be categorized into two broad domains: a) self-regulatory skills; and b) goal-oriented skills. The table below is an adaptation of McDougall's summary and provides an overview of specific executive function deficits that are commonly observed in a classroom environment. It also offers examples of how these deficits are likely to manifest in behaviour.\n\nSelf-regulatory skills\nGoal-oriented skills\nTeachers play a crucial role in the implementation of strategies aimed at improving academic success and classroom functioning in individuals with executive dysfunction. In a classroom environment, the goal of intervention should ultimately be to apply external control, as needed (e.g. adapt the environment to suit the child, provide adult support) in an attempt to modify problem behaviours or supplement skill deficits. Ultimately, executive function difficulties should not be attributed to negative personality traits or characteristics (e.g. laziness, lack of motivation, apathy, and stubbornness) as these attributions are neither useful nor accurate.\n\nSeveral factors should be considered in the development of intervention strategies. These include, but are not limited to: developmental level of the child, comorbid disabilities, environmental changes, motivating factors, and coaching strategies. It is also recommended that strategies should take a proactive approach in managing behaviour or skill deficits (when possible), rather than adopt a reactive approach. For example, an awareness of where a student may have difficulty throughout the course of the day can aid the teacher in planning to avoid these situations or in planning to accommodate the needs of the student.\n\nPeople with executive dysfunction have a slower cognitive processing speed and thus often take longer to complete tasks than people who demonstrate typical executive function capabilities. This can be frustrating for the individual and can serve to impede academic progress. Disorders affecting children such as ADHD, along with oppositional defiant disorder, conduct disorder, high functioning autism and Tourette's syndrome have all been suggested to involve executive functioning deficits. The main focus of current research has been on working memory, planning, set shifting, inhibition, and fluency. This research suggests that differences exist between typically functioning, matched controls and clinical groups, on measures of executive functioning.\n\nMoreover, some people with ADHD report experiencing frequent feelings of drowsiness. This can hinder their attention for lectures, readings, and completing assignments. Individuals with this disorder have also been found to require more stimuli for information processing in reading and writing. Slow processing may manifest in behavior as signaling a lack of motivation on behalf of the learner. However, slow processing is reflective of an impairment of the ability to coordinate and integrate multiple skills and information sources.\n\nThe main concern with individuals with autism regarding learning is in the imitation of skills. This can be a barrier in many aspects such as learning about others intentions, mental states, speech, language, and general social skills. Individuals with autism tend to be dependent on the routines that they have already mastered, and have difficulty with initiating new non-routine tasks. Although an estimated 25–40% of people with autism also have a learning disability, many will demonstrate an impressive rote memory and memory for factual knowledge. As such, repetition is the primary and most successful method for instruction when teaching people with autism.\n\nBeing attentive and focused for people with Tourette's syndrome is a difficult process. People affected by this disorder tend to be easily distracted and act very impulsively. That is why it is very important to have a quiet setting with few distractions for the ultimate learning environment. Focusing is particularly difficult for those who are affected by Tourette's syndrome comorbid with other disorders such as ADHD or obsessive-compulsive disorder, it makes focusing very difficult. Also, these individuals can be found to repeat words or phrases consistently either immediately after they are learned or after a delayed period of time.\n\nPrefrontal dysfunction has been found as a marker for persistent, criminal behavior. The prefrontal cortex is involved with mental functions including; affective range of emotions, forethought, and self-control. Moreover, there is a scarcity of mental control displayed by individuals with a dysfunction in this area over their behavior, reduced flexibility and self-control and their difficulty to conceive behavioral consequences, which may conclude in unstable (or criminal) behavior. In a recent study conducted by Barbosa & Monteiro, it was discovered that the recurrent criminals that were considered in this study suffered from executive dysfunction. In view of the fact that abnormalities in executive function can limit how people respond to rehabilitation and re-socialization programs these findings of the recurrent criminals are justified. Statistically significant relations have been discerned between anti-social behavior and executive function deficits. These findings relate to the emotional instability that is connected with executive function as a detrimental symptom that can also be linked towards criminal behavior. Conversely, it is unclear as to the specificity of anti-social behavior to executive function deficits as opposed to other generalized neuropsychological deficits. The uncontrollable deficiency of executive function has an increased expectancy for aggressive behavior that can result in a criminal deed. Orbitofrontal injury also hinders the ability to be risk avoidant, make social judgments, and may cause reflexive aggression. A common retort to these findings is that the higher incidence of cerebral lesions among the criminal population may be due to the peril associated with a life of crime. Along with this reasoning, it would be assumed that some other personality trait is responsible for the disregard of social acceptability and reduction in social aptitude.\n\nFurthermore, some think the dysfunction cannot be entirely to blame. There are interacting environmental factors that also have an influence on the likelihood of criminal action. This theory proposes that individuals with this deficit are less able to control impulses or foresee the consequences of actions that seem attractive at the time (see above) and are also typically provoked by environmental factors. One must recognize that the frustrations of life, combined with a limited ability to control life events, can easily cause aggression and/or other criminal activities. Early brain Damage causes dopamine(epigentically) to select for \"Seeking\" behavior in the very short term...this is the window for existence for these individuals...the future has no meaning so consequences have no meaning\n\n"}
{"id": "27742606", "url": "https://en.wikipedia.org/wiki?curid=27742606", "title": "Explorer 9", "text": "Explorer 9\n\nExplorer 9, known as S-56A before launch, was an American satellite which was launched in 1961 to study the density and composition of the upper thermosphere and lower exosphere. It was a reflight of the failed S-56 mission, and consisted of a , balloon which was deployed into a medium Earth orbit. The mission was conducted by NASA's Langley Research Center.\nExplorer 9 was launched from Launch Area 3 at the Wallops Flight Center, atop a Scout X-1 rocket with the serial number ST-4. It was the first spacecraft launched from Wallops Island to achieve orbit, with one previous attempt having failed. The launch occurred at 13:05:00 UTC on 16 February 1961, and resulted in Explorer 9 being deployed into an orbit with an apogee of , a perigee of , 38.8 degrees of inclination and a period of 118.4 minutes. It was assigned the Harvard designation 1961 Delta 1.\n\nThe second of six identical air density research satellites to be launched, Explorer 9 was the first to successfully reach orbit. It was still operational when the next satellite, Explorer 19, was launched, allowing simultaneous readings to be taken and compared. The spacecraft consisted of alternating layers of aluminium foil and Mylar polyester film. Uniformly distributed over the aluminium surface were 5.1 cm-diameter dots of white paint for thermal control. The sphere was packed in a tube 21.6 cm in diameter and 48.3 cm long and mounted in the nose of the fourth stage of the launch vehicle. Upon separation of the fourth stage, the sphere was inflated by a nitrogen gas bottle, and a separation spring ejected it out into its own orbit. The two hemispheres of aluminium foil were separated with a gap of Mylar at the spacecraft's equator and served as the antenna. A 136 MHz, 15 mW beacon was carried for tracking purposes, but the beacon failed on the first orbit and the SAO Baker-Nunn camera network had to be relied upon for tracking. Power was supplied by solar cells and rechargeable batteries. It decayed from orbit on 9 April 1964.\n"}
{"id": "6843217", "url": "https://en.wikipedia.org/wiki?curid=6843217", "title": "Formation matrix", "text": "Formation matrix\n\nIn statistics and information theory, the expected formation matrix of a likelihood function formula_1 is the matrix inverse of the Fisher information matrix of formula_1, while the observed formation matrix of formula_1 is the inverse of the observed information matrix of formula_1.\n\nCurrently, no notation for dealing with formation matrices is widely used, but in books and articles by Ole E. Barndorff-Nielsen and Peter McCullagh, the symbol formula_5 is used to denote the element of the i-th line and j-th column of the observed formation matrix. The geometric interpretation of the Fisher information matrix (metric) leads to a notation of formula_6 following the notation of the (contravariant) metric tensor in differential geometry. The Fisher information metric is denoted by formula_7 so that using Einstein notation we have formula_8.\n\nThese matrices appear naturally in the asymptotic expansion of the distribution of many statistics related to the likelihood ratio.\n\n\n"}
{"id": "2287377", "url": "https://en.wikipedia.org/wiki?curid=2287377", "title": "Friedrich Adolph Roemer", "text": "Friedrich Adolph Roemer\n\nFriedrich Adolph Roemer (15 April 1809 – 25 November 1869), German geologist, was born at Hildesheim, in the Kingdom of Westphalia.\n\nHis father was a lawyer and councillor of the high court of justice. In 1845 he became professor of mineralogy and geology at Clausthal, and in 1862 named director of the School of Mines. He first described the Cretaceous and Jurassic strata of Germany in elaborate works entitled \"Die Versteinerungen des Norddeutschen Oolith-Gebirges\" (\"Fossils of the North German oolith formations\"; 1836–39), \"Die Versteinerungen des Norddeutschen Kreidegebirges\" (\"Fossils of the North German chalk formations\"; 1840–41) and \"Die Versteinerungen des Harzgebirges\" (\"Fossils of the Harz Mountains\"; 1843). He died in Clausthal.\n\nThe mineral romerite commemorates his name, as does \"Roemeriana\", a publication issued by the Institute of Geology at the \"Bergakademie\" in Clausthal from 1954 to 1964.\n\nHis younger brother, Carl Ferdinand von Roemer, was also a geologist.\n"}
{"id": "51251453", "url": "https://en.wikipedia.org/wiki?curid=51251453", "title": "Fundamentals of Biochemistry", "text": "Fundamentals of Biochemistry\n\nFundamentals of Biochemistry: Life at the Molecular Level is a biochemistry textbook written by Donald Voet, Judith G. Voet and Charlotte W. Pratt. Published by John Wiley & Sons, it is a common undergraduate biochemistry textbook. \n\nAs of 2016, the book has been published in 5 editions.\n"}
{"id": "4142398", "url": "https://en.wikipedia.org/wiki?curid=4142398", "title": "Interstellar Boundary Explorer", "text": "Interstellar Boundary Explorer\n\nInterstellar Boundary Explorer (IBEX) is a NASA satellite that is making a map of the boundary between the Solar System and interstellar space. The mission is part of NASA's Small Explorer program and was launched with a Pegasus-XL rocket on October 19, 2008.\n\nThe design and operation of the mission is being led by the Southwest Research Institute, with the Los Alamos National Laboratory and the Lockheed Martin Advanced Technology Center serving as co-investigator institutions responsible for the IBEX-Hi and IBEX-Lo sensors respectively. The Orbital Sciences Corporation manufactured the spacecraft bus and was the location for spacecraft environmental testing. The nominal mission baseline duration was two years to observe the entire Solar System boundary. This was completed by 2011 and its mission was extended to 2013 to continue observations.\n\nIBEX is in a Sun-oriented spin-stabilized orbit around the Earth. In June 2011, IBEX was shifted to a new more efficient orbit. It does not come as close to the Moon in the new orbit, and expends less fuel to maintain its position.\n\nThe heliospheric boundary of the Solar System is being imaged by measuring the location and magnitude of charge-exchange collisions occurring in all directions. This will ultimately yield a map of the termination shock of the solar wind. The satellite's payload consists of two energetic neutral atom (ENA) imagers, IBEX-Hi and IBEX-Lo. Each of these sensors consists of a collimator that limits their fields-of-view, a conversion surface to convert neutral hydrogen and oxygen into ions, an electrostatic analyzer (ESA) to suppress ultraviolet light and to select ions of a specific energy range, and a detector to count particles and identify the type of each ion. The IBEX-Hi instrument is recording particle counts in a higher energy band than the IBEX-Lo does. The scientific payload also includes a Combined Electronics Unit (CEU) that controls the voltages on the collimator and the ESA, and it reads and records data from the particle detectors of each sensor.\n\nThe IBEX satellite, initially launched into a highly-elliptical transfer orbit with a low perigee, used a solid fuel rocket motor as its final boost stage at apogee, in order to raise its perigee greatly and to achieve its desired high-altitude elliptical orbit.\n\nIBEX is in a highly-eccentric elliptical terrestrial orbit, which ranges from a perigee of about to an apogee of about . Its original orbit was about —that is, about 80% of the distance to the Moon—which has changed primarily due to an intentional adjustment to prolong the spacecraft's useful life (see Orbit adjusted below).\n\nThis very high orbit allows the IBEX satellite to move out of the Earth's magnetosphere when making scientific observations. This extreme altitude is critical due to the amount of charged-particle interference that would occur while taking measurements within the magnetosphere. When within the magnetosphere of the Earth (), the satellite also performs other functions, including telemetry downlinks.\n\nThe IBEX satellite was mated to its Pegasus XL rocket at Vandenberg Air Force Base, California, and the combined vehicle was then suspended below the Lockheed L-1011 \"Stargazer\" mother airplane and flown to Kwajalein Atoll in the Central Pacific Ocean, a several-hours-long flight. \"Stargazer\" arrived at Kwajalein on Sunday, October 12, 2008.\n\nThe IBEX satellite was carried into space on October 19, 2008, by the Pegasus XL rocket. The rocket was released from \"Stargazer\", which took off from Kwajalein, at 17:47:23 UTC. By launching from this site close to the Equator, the Pegasus rocket lifted as much as more mass to orbit than it would have with a launch from the Kennedy Space Center in Florida.\n\nIn June 2011 IBEX shifted to a new orbit that raised its perigee to more than . The new orbit avoids taking the spacecraft too close to the Moon, whose gravity can negatively affect IBEX's orbit. Now the spacecraft uses less fuel to maintain a stable orbit, increasing its useful lifespan to more than 40 years.\n\nIBEX is collecting energetic neutral atom (ENA) emissions that are traveling through the Solar System to Earth that cannot be measured by conventional telescopes. These ENAs are created on the boundary of our Solar System by the interactions between solar wind particles and interstellar medium particles.\n\nOn the average IBEX-Hi detects about 500 particles per day, and IBEX-Lo, less than 100. By 2012, over 100 scientific papers related to IBEX were published, described by the PI as \"an incredible scientific harvest\".\n\nInitial data revealed a previously unpredicted \"very narrow ribbon that is two to three times brighter than anything else in the sky\". Initial interpretations suggest that \"the interstellar environment has far more influence on structuring the heliosphere than anyone previously believed\". It is unknown what is creating the ENA (energetic neutral atoms) ribbon. The Sun is currently traveling through the Local Interstellar Cloud, and the heliosphere's size and shape are key factors in determining its shielding power from cosmic rays. Should IBEX detect changes in the shape of the ribbon, that could show how the heliosphere is interacting with the Local Fluff. It has also observed ENAs from the Earth's magnetosphere.\n\nIn October 2010, significant changes were detected in the ribbon after six months, based on the second set of IBEX observations.\n\nIt went on to detect neutral atoms from outside the Solar System, which were found to differ in composition from the Sun. Surprisingly, IBEX discovered that the heliosphere has no bow shock, and it measured its speed relative to the local interstellar medium (LISM) as , improving on the previous measurement of by \"Ulysses\". Those speeds equate to 25% less pressure on the Sun's heliosphere than previously thought.\n\nIn July 2013, IBEX results revealed a 4-lobed tail on the Solar System's heliosphere.\n\nCompared to other space observatories, IBEX has a low data transfer rate due to the limited requirements of the mission.\n\n"}
{"id": "1615445", "url": "https://en.wikipedia.org/wiki?curid=1615445", "title": "Jacques Babinet", "text": "Jacques Babinet\n\nJacques Babinet (; 5 March 1794 – 21 October 1872) was a French physicist, mathematician, and astronomer who is best known for his contributions to optics.\nHis father was Jean Babinet and mother, Marie‐Anne Félicité Bonneau du Chesn. Babinet started his studies at the Lycée Napoléon, but was persuaded to abandon a legal education for the pursuit of science. A graduate of the École Polytechnique, which he left in 1812 for the Military School at Metz, he was later a professor at the Sorbonne and at the Collège de France. In 1840, he was elected as a member of the Académie Royale des Sciences. He was also an astronomer of the Bureau des Longitudes.\n\nAmong Babinet's accomplishments are the 1827 standardization of the Ångström unit for measuring light using the red Cadmium line's wavelength, and the principle (Babinet's principle) that similar diffraction patterns are produced by two complementary screens. He was the first to suggest using wavelengths of light to standardize measurements. His idea was first used between 1960 and 1983, when a meter was defined as a wavelength of light from krypton gas.\n\nBabinet was interested in the optical properties of minerals throughout his career. He designed and created many scientific instruments utilized to determine crystalline structure and polarization properties, including the polariscope and an optical goniometer to measure refractive indices. The Babinet compensator, an accessory useful in polarized light microscopy, was built with twin, opposed quartz wedges having mutually perpendicular crystallographic axes, and is still widely employed in microscopy. This design avoids the problems inherent in the basic quartz wedge, where the zero reading coincides with the thin end of the wedge, which is often lost when grinding the plate during manufacture.\n\nExpanding his fascination of diffraction to meteorology, Babinet spent a significant amount of time in the study of rainbow optics. His astronomical research focused on Mercury's mass and the Earth's magnetism, while his inventions included valve improvements for air pumps and a hygrometer. In geography and hydrogeomorphology, the Baer–Babinet law helps to explain and predict directionality in the course of rivers. Babinet's cartography work includes homalographic projections where the parallels are rectilinear and meridian lines are elliptical.\n\nIn addition to his brilliant lectures on meteorology and optics research, Babinet was also a great promoter of science, an amusing and clever lecturer, and a brilliant, entertaining and prolific author of popular scientific articles. He was beloved by many for his kindly and charitable nature.\n\nOn October 30, 1820, Jacques Babinet was married to Adelaïde Laugier (July 14, 1795-July 27, 1849). They had two children: Charles Babinet (December 8, 1821-May 31 1907) and Léon Babinet (July 26, 1825-1913).\n\n"}
{"id": "11629729", "url": "https://en.wikipedia.org/wiki?curid=11629729", "title": "Jean-Frédéric Hermann", "text": "Jean-Frédéric Hermann\n\nJean-Frédéric Hermann (Strasbourg 1768–1793) was a French physician and naturalist mainly interested in entomology.\n\nSon of Jean Hermann, he continued the index of his father’s collection, illustrating some species. He studied the comparative anatomy of the mouthparts of insects and mites publishing \"Mémoire aptérologique\" with his son in law Frédéric-Louis Hammer in 1804.\n\n"}
{"id": "43142862", "url": "https://en.wikipedia.org/wiki?curid=43142862", "title": "Jon Levy (behaviorist)", "text": "Jon Levy (behaviorist)\n\nJonathan \"Jon\" Levy is a behavior scientist, consultant, writer, keynote speaker and media personality best known for his work in the fields of influence and adventure. With the intention of bringing together exceptional people, Levy used his understanding of social interaction to create The Influencers Dinner, a secret dining experience for taste makers, thought leaders and influencers from different industries.\n\nJon’s work in human behavior has garnered him a reputation as one of the leading super connectors in America. His insights and strategies for effective social and professional interactions can be found in numerous media outlets including The New York Times, Forbes, Business Insider, Inc, and Fast Company.\n\nLevy's research mostly focuses on what affects decision making. He is currently working out of C Lab collaborating with Neuroscientist Dr. Moran Cerf, of Kellogg School of Management. Levy spent years modeling the behavior of people at every level of influence, in order to better understand what causes them to engage and connect. His model's for engagement are what lead to the creation of The Influencers Dinner and Miror: The Salon. Levy's most recent research relates to the dating statistics. His team anonymized over a billion data points, from the dating app Hinge, to discover what leads people to take their online connections and move them into in person relationships.\n\nLevy has spoken and keynoted at many of the most respected conferences, including TED where he shared about \"What Makes us Influential\", PopTech where he presented \"The Science of Adventure\", Fortune Magazine's Brainstorm Health on \"How Adventure Can Save your life\", among others. \n\nOn November 8, 2016 Jon Levy's first book, \"\" was published by Regan Arts and distributed by Simon & Schuster. The book explores Levy's research on the science of adventure, emphasizing what he calls \"The EPIC Model of Adventure\", a four-stage process every adventure goes through.\n\nIn 2017, \"\" won 1st place for book design at the annual New York Book Show.\n\nBetween 2014 and 2015, Levy was recognized as both one of “New York's Most Successful Bachelors” by Gotham Magazine and one of the “41 Most Eligible Bachelors\" in America by Elle Magazine.\n"}
{"id": "16750", "url": "https://en.wikipedia.org/wiki?curid=16750", "title": "Kleene star", "text": "Kleene star\n\nIn mathematical logic and computer science, the Kleene star (or Kleene operator or Kleene closure) is a unary operation, either on sets of strings or on sets of symbols or characters. In mathematics\nit is more commonly known as the free monoid construction. The application of the Kleene star to a set \"V\" is written as \"V\". It is widely used for regular expressions, which is the context in which it was introduced by Stephen Kleene to characterize certain automata, where it means \"zero or more\".\n\n\nThe set \"V\" can also be described as the set of finite-length strings that can be generated by concatenating arbitrary elements of \"V\", allowing the use of the same element multiple times. If \"V\" is either the empty set ∅ or the singleton set {ε}, then \"V\" = {ε}; if \"V\" is any other finite set, then \"V\" is a countably infinite set.\n\nThe operators are used in rewrite rules for generative grammars.\n\nGiven a set \"V\"\ndefine\nand define recursively the set\n\nIf \"V\" is a formal language, then \"V\", the \"i\"-th power of the set \"V\", is a shorthand for the concatenation of set \"V\" with itself \"i\" times. That is, \"V\" can be understood to be the set of all strings that can be represented as the concatenation of \"i\" strings in \"V\". \n\nThe definition of Kleene star on \"V\" is\n\nNotice that the Kleene star operator is an idempotent unary operator: (\"V\") = \"V\" for any set \"V\" of strings or characters.\n\nIn some formal language studies, (e.g. AFL theory) a variation on the Kleene star operation called the \"Kleene plus\" is used. The Kleene plus omits the \"V\" term in the above union. In other words, the Kleene plus on \"V\" is\n\nFor every set \"L\", the Kleene plus of \"L\" (denoting \"L\") equals the concatenation of \"L\" with \"L\"; this holds because every element of \"L\" must either be composed from one element of \"L\" and finitely many non-empty terms in \"L\" or is just an element of \"L\" (where \"L\" itself is retrieved by taking \"L\" concatenated with ε). \nConversely, \"L\" = {ε} ∪ \"L\".\n\nExample of Kleene star applied to set of strings:\n\nExample of Kleene plus applied to set of characters:\nKleene star applied to the same character set:\n\nExample of Kleene star applied to the empty set:\n\nExample of Kleene plus applied to the empty set:\nwhere concatenation is an associative and noncommutative product, sharing these properties with the Cartesian product of sets.\n\nExample of Kleene plus and Kleene star applied to the singleton set containing the empty string:\n\nStrings form a monoid with concatenation as the binary operation and ε the identity element. The Kleene star is defined for any monoid, not just strings.\nMore precisely, let (\"M\", ⋅) be a monoid, and \"S\" ⊆ \"M\". Then \"S\" is the smallest submonoid of \"M\" containing \"S\"; that is, \"S\" contains the neutral element of \"M\", the set \"S\", and is such that if \"x\",\"y\" ∈ \"S\", then \"x\"⋅\"y\" ∈ \"S\".\n\nFurthermore, the Kleene star is generalized by including the *-operation (and the union) in the algebraic structure itself by the notion of complete star semiring.\n"}
{"id": "11838775", "url": "https://en.wikipedia.org/wiki?curid=11838775", "title": "List of Albizia species", "text": "List of Albizia species\n\nThis is a list of species in the legume tree genus Albizia, the silk trees, sirises or albizias.\n\nNumerous species placed in \"Albizia\" by early authors were eventually moved to other genera, particularly \"Archidendron\" and many other Ingeae, as well as certain Acacieae, Mimoseae, and even Caesalpinioideae and Faboideae.\n\nThe delimitation of \"Falcataria\" and \"Pithecellobium\" - close relatives of \"Albizia\" - is notoriously complex, with species having been moved between the genera time and again, and probably will continue to do so. Other closely related genera like \"Chloroleucon\" and \"Samanea\" are often merged with \"Albizia\" entirely.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand others\n"}
{"id": "5100302", "url": "https://en.wikipedia.org/wiki?curid=5100302", "title": "List of DVD manufacturers", "text": "List of DVD manufacturers\n\nThis aims to be a complete list of DVD manufacturers.\n\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly. This list is only a list of brand names for DVDs and not an actual\nmanufacturers list\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15185968", "url": "https://en.wikipedia.org/wiki?curid=15185968", "title": "List of Grandi Giardini Italiani", "text": "List of Grandi Giardini Italiani\n\nThe Grandi Giardini Italiani is an association of major gardens in Italy and Malta. Its members include some of the most important gardens in Italy and Malta.\n\n\n\n"}
{"id": "12650082", "url": "https://en.wikipedia.org/wiki?curid=12650082", "title": "List of Illinois state symbols", "text": "List of Illinois state symbols\n\nThis is a list of official symbols for the U.S. state of Illinois:\n\n"}
{"id": "11097038", "url": "https://en.wikipedia.org/wiki?curid=11097038", "title": "List of Superfund sites in Georgia (U.S. state)", "text": "List of Superfund sites in Georgia (U.S. state)\n\nThis is a list of Superfund sites in Georgia designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of November 13, 2014, there were 16 Superfund sites on the National Priorities List in Georgia. One additional site has been proposed for entry on the list. Five sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "10470168", "url": "https://en.wikipedia.org/wiki?curid=10470168", "title": "List of Tree Cities USA", "text": "List of Tree Cities USA\n\nThe following is a partial listing of Tree Cities USA. To be a Tree City, the community must meet four standards set by the National Arbor Day Foundation and the National Association of State Foresters:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14485734", "url": "https://en.wikipedia.org/wiki?curid=14485734", "title": "List of members of the National Academy of Sciences (Biophysics and computational biology)", "text": "List of members of the National Academy of Sciences (Biophysics and computational biology)\n"}
{"id": "6780209", "url": "https://en.wikipedia.org/wiki?curid=6780209", "title": "List of mining companies", "text": "List of mining companies\n\nThis is an incomplete alphabetical list of mining companies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "86331", "url": "https://en.wikipedia.org/wiki?curid=86331", "title": "List of newsgroups", "text": "List of newsgroups\n\nThis is a partial list of newsgroups that are significant for their popularity or their position in Usenet history.\n\nThese are the most widely distributed and carefully controlled newsgroup hierarchies. See Big 8 (Usenet) and the Great Renaming for more information.\n\nComputer-related topics.\n\nTopics related to the humanities (fine arts, literature, philosophy, Classical Latin, etc.).\n\nMiscellaneous topics. Examples include:\n\n\nMatters related to the functioning of Usenet itself. Examples include:\n\n\nRecreation and entertainment topics.\nExamples include:\n\n\nScience-related topics.\n\nDiscussion related to society and social subcultures. Examples include:\n\n\nDiscussion of various topics, especially controversial ones. Includes political topics as well. Examples include:\n\n\nThis is the most extensive newsgroup hierarchy outside of the Big 8. Examples include:\n\n\nThese newsgroups fall outside of the official Big 8 hierarchies, as well as the less formal \"alt\" hierarchy. Other newsgroups can consist of geographic regions (official or unofficial), corporations, and other institutions or groups.\n"}
{"id": "22063462", "url": "https://en.wikipedia.org/wiki?curid=22063462", "title": "List of solid-state drive manufacturers", "text": "List of solid-state drive manufacturers\n\nThis is a list of manufacturers of solid-state drives (SSD) for computers and other electronic devices that require data storage. In this list those manufacturers that also produce hard disk drives and/or flash memory are identified. Additionally the type of memory used in their solid-state drives is noted. This list does not include manufacturers of only a component of the SSD, like the flash memory controller.\n\n"}
{"id": "17672870", "url": "https://en.wikipedia.org/wiki?curid=17672870", "title": "List of symbols of states and territories of Australia", "text": "List of symbols of states and territories of Australia\n\nThis is a list of the symbols of the states and territories of Australia. Each state and territory has a unique set of official symbols.\n\n"}
{"id": "41109838", "url": "https://en.wikipedia.org/wiki?curid=41109838", "title": "List of things named after Eugene Wigner", "text": "List of things named after Eugene Wigner\n\nThe following is a list of things named after Hungarian physicist E. P. Wigner.\n\n\n\n"}
{"id": "9517830", "url": "https://en.wikipedia.org/wiki?curid=9517830", "title": "Los Angeles School", "text": "Los Angeles School\n\nThe Los Angeles School of Urbanism is an academic movement which emerged during the mid-1980s, loosely based at UCLA and the University of Southern California, which centers urban analysis on Los Angeles, California.\n\nThe first published identification of the Los Angeles (L.A.) School as such was by Mike Davis in his popular urban history of Los Angeles, \"City of Quartz\" (1990). According to Davis, the school emerged informally during the mid-1980s when an eclectic variety of neo-Marxist scholars began publishing a series of articles and books dealing exclusively with Los Angeles. During the school’s formation, Davis cautiously estimated that the school had about twenty members scattered throughout Southern California and beyond, with some members purportedly residing as far away as Frankfurt, Germany. \nMuch of the work published by L.A. School members during the 1980s and early 1990s garnered considerable attention. However, while some members (e.g. Edward Soja and Mike Davis) became household names in urban theory, there was little consciousness of the school as its own entity, especially outside of Los Angeles. This changed in 1998, with the publication of an article by Michael J. Dear and Steven Flusty, which explicitly argued for the existence of a distinct L.A. School of Urbanism, of which its various theories, concepts, and empirical works could be pooled together to constitute a radical new conception of ‘postmodern urbanism.’ After Dear and Flusty’s publication, Dear popularized the school through the production of a series of articles and books, including a full-length edited volume comparing the L.A. School to the Chicago School. \nThough much of the work of the L.A. School is still widely read in urban studies, the school’s membership has declined substantially in recent years. At a retirement party for Soja in 2008 at which many purported members were present, only Michael J. Dear appeared to be willing to envisage the school’s continued existence. This situation reflects the vital conceptual disagreements between members of the LA School, and especially between Dear and the other members.\n\nThere is no official list of present or historic members of the Los Angeles School of Urbanism. Some thinkers who are commonly considered members include:\n\nThe L.A. School has no official doctrine, and there is great diversity in the works of its various members. Nevertheless, there are several influences, themes, and concepts which are relatively consistent in the school’s scholarship. \nPerhaps the central characteristic of the thought of the L.A. School is a sustained focus on Los Angeles in both empirical and theoretical work, often with the underlying claim that L.A. is the paradigmatic American metropolis of the 20th and 21st centuries. More than this, the L.A. School poses a challenge to, what many members see as, the dominant Chicago School of Urbanism. While the Chicago School presents a modernist theory of cities as based on social darwinist struggles for urban space, the Los Angeles School proposes a postmodern or postfordist vision. While not all members of the L.A. School identify as postmodernists, and in fact some (e.g. Mike Davis) are against the very concept, a focus on postmodernism is fundamental to many members of the L.A. School, who rely heavily upon theorists associated with postmodernism, such as Baudrillard, Foucault, Jameson, and Derrida.\nA further stream of work emerging from the LA School is represented by Scott and Storper's many publications on flexible specialization, agglomeration, and the economic dynamics of the contemporary metropolis. Scott and Storper's work differs from that of Dear and Soja by approaching urban theory from the perspective of postfordism rather than postmodernism. Scott and Storper represent one distinctive tendency in the LA School; Dear and Soja represent another.\n\nA number of criticisms have been raised against the Los Angeles School. Perhaps the most important of these is skepticism over the school’s actual importance relative to its claims. For instance, one criticism is that its literature sometimes deceptively excludes discussions or citations of other important works on urban issues, to give readers the impression that the L.A. school is more radical, original, and important than it actually is. In particular, some critics think that the contemporary importance of the Chicago School is grossly overstated by L.A. theorists, and that little to no attention is allocated to extremely important scholarly materials produced in the period between the decline of the Chicago School and the emergence of the L.A. School, particularly the work of Marxist urban theorists like Manuel Castells, David Harvey, and Henri Lefebvre. On a similar vein, much of the work of the L.A. School has been criticized for its incoherence and lack of a demonstrable methodology.\nA final criticism questions the L.A. School’s fundamental claim that Los Angeles should be considered the paradigmatic postmodern American city. This stems both from external comparisons which have been made between Los Angeles and other cities, and findings that in certain cases urban phenomena in Los Angeles do not match those of other American cities.\n\n\n"}
{"id": "53987047", "url": "https://en.wikipedia.org/wiki?curid=53987047", "title": "Mark Moss", "text": "Mark Moss\n\nMark B. Moss is an American neurobiologist currently the Waterhouse Professor at Boston University.\n"}
{"id": "19892474", "url": "https://en.wikipedia.org/wiki?curid=19892474", "title": "Ocean deoxygenation", "text": "Ocean deoxygenation\n\nOcean deoxygenation is the expansion of oxygen minimum zones in the world's oceans as a consequence of anthropogenic emissions of carbon dioxide. The change has been fairly rapid and poses a threat to fish and other types of marine life, as well as to people who depend on marine life for nutrition or livelihood.\n\nOceanographers and others have discussed what phrase best describes the phenomenon to non-specialists. Among the options considered have been ocean suffocation (which was used in a news report from May 2008), \"ocean oxygen deprivation\", \"decline in ocean oxygen\", \"marine deoxygenation\", \"ocean oxygen depletion\" and \"ocean hypoxia\".\n\nOcean deoxygenation poses implications for ocean productivity, nutrient cycling, carbon cycling, and marine habitats.\n\nMost of the excess heat from CO and other greenhouse gas emissions is absorbed by the oceans. Warmer oceans cause deoxygenation both because oxygen is less soluble in warmer water, and through temperature driven stratification of the ocean which inhibits the production of oxygen from photosynthesis.\n\nThe ocean surface stratifies as the atmosphere and ocean warms causing ice melt and glacial runoff. This results in a less salty and therefore a less dense layer that floats on top. Also the warmer waters themselves are less dense. This stratification inhibits the upwelling of nutrients (the ocean constantly recycles its nutrients) into the upper layer of the ocean. This is where the majority of oceanic photosynthesis (such as by phytoplankton) occurs. This decrease in nutrient supply is likely to decrease rates of photosynthesis in the surface ocean, which is responsible for approximately half of the oxygen produced globally. Increased stratification can also decrease the supply of oxygen to the interior of the ocean. Warmer waters also increase the metabolism of marine organisms, leading to increased respiration rates. In the surface ocean, increased respiration will likely lead to lower net oxygen production, and thus less oxygen transferred to the atmosphere. In the interior ocean, the combination of increased respiration and decreased oxygen supply from surface waters can draw oxygen down to hypoxic or anoxic levels. Not only are low levels of oxygen lethal to fish and other upper trophic level species, they can change the microbially mediated cycling of globally important elements such as nitrogen; nitrate replaces oxygen as the primary microbial electron acceptor at very low oxygen concentrations. All this, increased demand on herbivores, decreased nutrient supply, decreased dissolved oxygen, etc., result in catastrophic food web mismatches.\n\nOcean model simulations predict a decline of up to 7% in the global ocean O content over the next hundred years. The decline of oxygen is projected to continue for a thousand years or more.\n\n\n\n"}
{"id": "43747041", "url": "https://en.wikipedia.org/wiki?curid=43747041", "title": "Open coopetition", "text": "Open coopetition\n\nIn R&D management and systems development, open coopetition or open-coopetition is a neologism to describe cooperation among competitors in the open-source arena.The term was first coined by the scholars Jose Teixeira and Tingting Lin to describe how rival firms that, while competing with similar products in the same markets (e.g., Apple, Samsung, Google, Nokia), collaborate which each other in the development of open-source projects (e.g., Webkit).\n\nOpen-coopetition is a compound-word term bridging coopetition and open-source. Coopetition refers to a paradoxical relationship between two or more actors simultaneously involved in cooperative and competitive interactions; and open-source both as a development method that emphasizes transparency and collaboration, and as a \"private-collective\" innovation model with features both from the private investment and collective action — firms contribute towards the creation of public goods while giving up associated intellectual property rights such patents, copyright, licenses, or trade secrets.\n\nSuch intertwined behavior of cooperation and competition in an open-source way, emphasizes transparency on the development of technological artifacts that become available to the public under an open-source license—allowing anyone to freely obtain, study, modify and redistribute them. Within open-coopetition, development transparency and sense of community are maximized; while the managerial control and IP enforcement are minimized. Open-coopetitive relationships are paradoxical as the core managerial concepts of property, contract and price play an outlier role.\n\nThe openness characteristic of open-source projects also distinguishes open-coopetition from other forms of cooperative arrangements by its inclusiveness: Everybody can contribute. Users or other contributors do not need to hold a supplier contract or sign a legal intellectual property arrangement to contribute. Moreover, neither to be a member of a particular firm or affiliated with a particular joint venture or consortia to be able to contribute. In the words of Massimo Banzi, \"You don't need anyone's permission to make something great\".\n\nIn a large-scale study involving multiple European-based software intensive firms, the scholars Pär Ågerfalk and Brian Fitzgerald revealed a shift from \"open-source as a community of individual developers to open-source as a community of commercial organizations, primarily small and medium-sized enterprises, operating as a symbiotic ecosystem in a spirit of coopetition\".\nEven if they were exploring opensourcing as \"a novel and unconventional approach to global sourcing and coopetition\", they captured the following quote that highlights that competition in the open-source arena is not as in business as usual. \n\nAlso in the academic world, and after following an software company based in Norway for over five years, and while theorizing on the concept of software ecosystem, the academic Geir K. Hanssen noted that the characteristic networks of an software ecosystem, open-source or proprietary ones, can embed competing organizations.\n\"Software ecosystems have a networked character. CSoft and its external environment constitute a network of customers and\nthird party organizations. Even competitors may be considered a part of this network, although this aspect has not been studied\nin particular here.\"\nIn an opinion article entitled Open Source Coopetition Fueled by Linux Foundation Growth, the journalist and market analyst Jay Lyman highlights that \"working with direct rivals may have been unthinkable 10 years ago, but Linux, open-source and organizations such as the Linux Foundation have highlighted how solving common problems and easing customer pain and friction in using and choosing different technologies can truly drive innovation and traction in the market.\" The term \"open source coopetition\" was employed to highlight the role of the Linux Foundation as a mediator of collaboration among rival firms.\n\nAt the OpenStack summit in Hong Kong, the co-founder of Mirantis Boris Renski talked about his job on figuring out how to co-opete in the crowded OpenStack open-source community. In a 43-minute broadcast video, Boris Renski shed some light on OpenStack coopetition politics and shared a subjective view on strategies of individual players within the OpenStack community (e.g., Rackspace, Mirantis, IBM, HP and Red Hat among others). The Mirantis co-founder provided a rich description of an open-source community working in co-opetition.\n\nAlong with this lines, the pioneering scholarly work of Germonprez et al. (2013) reported on how key business actors within the financial services industry that traditionally viewed open-source software with skepticism, tied up an open-source ‘community of competitors’. By taking the case of OpenMAMA, a Middleware Agnostic Messaging API used by some of the world’s largest financial players, they show that corporate market rivals (e.g., J. P. Morgan, Bank of America, IBM and BMC) can coexist in open-source communities, and intentionally coordinate activities or mutual benefits in precise, market focused, and non-differentiating engagements. Their work pointed out that high-competitive capital-oriented industries do not epitomize the traditional and grassroots idea that open-source software was originally born from. Furthermore, they argued that open-source communities can be deliberately designed to include competing vendors and customers under neutral institutional structures (e.g., foundations and steering committees).\n\nIn an academic paper entitled \"Collaboration in the open-source arena: The WebKit case\", the scholars Jose Teixeira and Tingting Lin executed an ethnographic informed social network analysis on the development of the WebKit open-source web browsing technologies. Among a set of the reported findings, they pointed out that even if Apple and Samsung were involved in expensive patent wars in the courts at the time, they still collaborated in the open-source arena. As some of the research results did not confirm prior research in coopetition, the authors proposed and coined the \"open-coopetition\" term while emphasizing the openness of collaborating with competitors in the open-source arena.\n\nBy turning to OpenStack, the scholars Teixeira et al. (2015)\nwent further and modeled and analyzed both collaborative and competitive networks from the OpenStack open-source project (a large and complex cloud computing infrastructure for big data). Somewhat surprising results point out that competition for the same revenue model (i.e., operating conflicting business models) does not necessarily affect collaboration within the OpenStack ecosystem—in other words, competition among firms did not significantly influence collaboration among software developers affiliated with them. Furthermore, the expected social tendency of developers to work with developers from same firm (i.e., homophily) did not hold within the OpenStack ecosystem. The case of OpenStack revealed to be much about genuine collaboration in software development besides ubiquitous competition among the firms that produce and use the software.\n\nA related study by Linåker et al. (2016) analyzed the Apache Hadoop ecosystem in a quantitative longitudinal case study to investigate changing stakeholder influence and collaboration patterns. They found that the collaborative network had a quite stable number of network components (i.e., number of sub-communities within the community) with many unconnected stakeholders. Furthermore, such components were dominated by a core set of stakeholders that engaged in most of the collaborative relationships. As in OpenStack, there was much cooperation among competing and non-competing actors within the Apache Hadoop ecosystem—or in other words, firms with competing business models collaborate as openly as non-rivaling firms. Finally, they also \nargued that the openness of software ecosystems decreases the distance to competitors within the same ecosystem, it becomes possible and important to track what the competitors do within. Knowing about their existing collaborations, contributions, and interests in specific features offer valuable information about the competitors’ strategies and tactics.\n\nIn a study addressing coopetition in the cloud computing industry, Teixeira et al. analyzed not only coopetition among individuals and organizations but also among cohesive inter-organizational networks. Relationships among individuals were modeled and visualized in 2D longitudinal visualizations and relationships among inter-organizational networks (e.g., alliances, consortium or ecosystem) were modeled and visualized in 3D longitudinal visualizations. The author added evidence to prior research suggesting that competition is a multi-level phenomenon that is influenced by individual-level, organizational-level, and network-level factors.\n\nBy noting that many firms engaging into open-coopetition actively manage multiple portfolios of alliances in the software industry (i.e., many strategically contribute to multiple open-source software ecosystems) and by analyzing the co-evolution of OpenStack and the CloudStack cloud computing platforms, the same authors propose that development transparency and the weak intellectual property rights, two well-known characteristics of open-source ecosystems, allow an easier transfer of information and resources from one alliance to another. Even if openness enables a focal firm to transfer information and resources more easily between multiple alliancess, such 'ease of transfer' should not be seen as a source of competitive advantage as competitors can do the same.\n\nIn a study explicitly addressing coopetition in open-source software ecosystems, Nguyen Duc et al. (2017) identified a number of situations in which different actors within the software ecosystem deal with collaborative-competitive issues:\n\n\nCompetitive behavior within open-source software ecosystems frictions with the more purist view of free and open-source software. The same authors reported on some working practices that conflict with the more traditional values of free and open-source software.\n\nThe same study also unfolded a number of benefits that organization can rip by actively contributing to open-source software ecosystems that encompass both cooperative and competitive relationships: \n\nCases of open-coopetition are recurrent in the software industry in general. Furthermore, some cases exist also in the electronics, semiconductors, automotive, financial, telecommunications, retail, education, healthcare, defense, aerospace, and additive manufacturing industries. Cases of open-coopetition are often associated with high-tech corporations and startups based in the USA (predominantly on the West Coast). Cases can be also recognized in Cuba, Brazil, Europe (predominantly on Western Europe), India, South-Korea, China, Vietnam, Australia, and Japan.\n\nMany of the software projects encompassing open-coopetition are legally governed by foundations such as the Linux Foundation, the Free Software Foundation, the Apache Software Foundation, the Eclipse Foundation, the Cloud Native Computing Foundation, and the X.Org Foundation among many others. Most of the Linux Foundation collaborative projects are coopetitive in nature --- the Linux Foundation claims to be \"a neutral home for collaborative development\". Furthermore, many coopetitive open-source projects dealing with both software and hardware (e.g., computer graphics, data storage) are bounded by standard organizations such as the Khronos Group, W3C and the Open Compute Project.\n\n"}
{"id": "23886", "url": "https://en.wikipedia.org/wiki?curid=23886", "title": "Phlogiston theory", "text": "Phlogiston theory\n\nThe phlogiston theory is a superseded scientific theory that postulated that a fire-like element called phlogiston is contained within combustible bodies and released during combustion. The name comes from the Ancient Greek φλογιστόν \"phlogistón\" (\"burning up\"), from φλόξ \"phlóx\" (\"flame\"). It was first stated in 1667 by Johann Joachim Becher, and then put together more formally by Georg Ernst Stahl. The theory attempted to explain processes such as combustion and rusting, which are now collectively known as oxidation.\n\nPhlogiston theory states that \"phlogisticated\" substances are substances that contain phlogiston and \"dephlogisticate\" when burned. Dephlogisticating is the process of releasing stored phlogiston, which is absorbed by the air. Growing plants then absorb this phlogiston, which is why air does not spontaneously combust and also why plant matter burns as well as it does.\n\nThus phlogiston accounted for combustion via a process that was opposite to that of the oxygen theory.\nIn general, substances that burned in air were said to be rich in phlogiston; the fact that combustion soon ceased in an enclosed space was taken as clear-cut evidence that air had the capacity to absorb only a finite amount of phlogiston. When air had become completely phlogisticated it would no longer serve to support combustion of any material, nor would a metal heated in it yield a calx; nor could phlogisticated air support life. Breathing was thought to take phlogiston out of the body.\n\nJoseph Black's student Daniel Rutherford discovered nitrogen in 1772 and the pair used the theory to explain his results. The residue of air left after burning, in fact a mixture of nitrogen and carbon dioxide, was sometimes referred to as \"phlogisticated air,\" having taken up all of the phlogiston. Conversely, when Joseph Priestley discovered oxygen, he believed it to be \"dephlogisticated air,\" capable of combining with more phlogiston and thus supporting combustion for longer than ordinary air.\n\nEmpedocles had formulated the classical theory that there were four elements: water, earth, fire and air, and Aristotle reinforced this idea by characterising them as moist, dry, hot and cold. Fire was thus thought of as a substance and burning was seen as a process of decomposition which applied only to compounds. Experience had shown that burning was not always accompanied by a loss of material and a better theory was needed to account for this.\n\nIn 1667, Johann Joachim Becher published his book \"Physica subterranea\", which contained the first instance of what would become the phlogiston theory. In his book, Becher eliminated fire, water, and air from the classical element model and replaced them with three forms of earth: \"terra lapidea\", \"terra fluida\", and \"terra pinguis\". \"Terra pinguis\" was the element that imparted oily, sulphurous, or combustible properties. Becher believed that \"terra pinguis\" was a key feature of combustion and was released when combustible substances were burned. Becher did not have much to do with phlogiston theory as we know it now, but he had a large influence on his student Stahl. Becher's main contribution was the start of the theory itself, however much it was changed after him. Becher's idea was that combustible substances contain an ignitable matter, the \"terra pinguis.\" \nIn 1703 Georg Ernst Stahl, professor of medicine and chemistry at Halle, proposed a variant of the theory in which he renamed Becher's \"terra pinguis\" to \"phlogiston\", and it was in this form that the theory probably had its greatest influence. The term phlogiston itself was not something that Stahl invented. There is evidence that the word was used as early as 1606, and in a way that was very similar to what Stahl was using it for. The term was derived from a Greek word meaning to inflame. The following paragraph describes Stahl's view of phlogiston:To Stahl, metals were compounds containing phlogiston in combination with metallic oxides (calces); on ignition the phlogiston was freed from the metal leaving the oxide behind. When the oxide was heated with a substance rich in phlogiston, such as charcoal, the calx again took up phlogiston and regenerated the metal. Phlogiston was a definite substance, the same in all its combinations.Stahl's first definition of phlogiston first appeared in his \"Zymotechnia fundamentalis\", published in 1697. His most quoted definition was found in the treatise on chemistry entitled \"Fundamenta chymiae\" in 1723. According to Stahl, phlogiston was a substance that was not able to be put into a bottle, but could be transferred nonetheless. To him, wood was just a combination of ash and phlogiston, and making a metal was as simple as getting a metal calx and adding phlogiston. Soot was almost pure phlogiston, which is why heating it with a metallic calx transforms the calx into the metal and Stahl attempted to prove that the phlogiston in soot and sulphur were identical by converting sulphates to liver of sulphur using charcoal. He did not account for the increase in weight on combustion of tin and lead that were known at the time.\n\nJohann Heinrich Pott, a student of one of Stahl's students, expanded the theory and attempted to make it much more understandable to a general audience. He compared phlogiston to light or fire, saying that all three were substances whose natures were widely understood but not easily defined. He thought that phlogiston should not be considered as a particle but as an essence that permeates substances, arguing that in a pound of any substance one could not simply pick out the particles of phlogiston. Pott also observed the fact that when certain substances are burned they increase in mass instead of losing the mass of the phlogiston as it escapes; according to him, phlogiston was the basic fire principle and could not be obtained by itself. Flames were considered to be a mix of phlogiston and water, while a phlogiston-and-earthy mixture could not burn properly. Phlogiston permeating everything in the universe, it could be released as heat when combined with acid. Pott proposed the following properties:\nPott's formulations proposed little new theory; he merely supplied further details and rendered existing theory more approachable to the common man.\n\nJohann Juncer also created a very complete picture of phlogiston. When reading Stahl's work, he assumed that phlogiston was in fact very material. He therefore came to the conclusion that phlogiston has the property of levity, or that it makes the compound that it is in much lighter than it would be without the phlogiston. He also showed that air was needed for combustion by putting substances in a sealed flask and trying to burn them.\n\nGuillaume-Francois Rouelle brought the theory of phlogiston to France, and he was a very influential scientist and teacher so it gained quite a strong foothold very quickly. Many of his students became very influential scientists in their own right, Lavoisier included. The French viewed phlogiston as a very subtle principle that vanishes in all analysis, yet it is in all bodies. Essentially they followed straight from Stahl's theory.\n\nGiovanni Antonio Giobert introduced Lavoisier's work in Italy. Giobert won a prize competition from the Academy of Letters and Sciences of Mantua in 1792 for his work refuting phlogiston theory. He presented a paper at the Académie royale des Sciénces of Turin on March 18, 1792 entitled \"Examen chimique de la doctrine du phlogistique et de la doctrine des pneumatistes par rapport à la nature de l 'eau\" (translates roughly to Chemical examination of the doctrine of phlogiston and the doctrine of pneumatists in relation to the nature of water), which is considered the most original defense of Lavoisier's theory of water composition to appear in Italy.\n\nEventually, quantitative experiments revealed problems, including the fact that some metals gained mass when they burned, even though they were supposed to have lost phlogiston.\nSome phlogiston proponents explained this by concluding that phlogiston had negative weight; others, such as Louis-Bernard Guyton de Morveau, gave the more conventional argument that it was lighter than air. However, a more detailed analysis based on Archimedes' principle, the densities of magnesium and its combustion product showed that just being lighter than air could not account for the increase in mass. Stahl himself did not address the problem of the metals that burn gaining weight, but those who followed his ideas and did not question his ideas were the ones that worked on this problem.\n\nDuring the eighteenth century, as it became clear that metals gained mass when they were oxidized, phlogiston was increasingly regarded as a \"principle\" rather than a material substance. By the end of the eighteenth century, for the few chemists who still used the term phlogiston, the concept was linked to hydrogen. Joseph Priestley, for example, in referring to the reaction of steam on iron, while fully acknowledging that the iron gains mass as it binds with oxygen to form a calx, iron oxide, iron also loses \"the basis of inflammable air (hydrogen), and this is the substance or principle, to which we give the name phlogiston.\" Following Lavoisier’s description of oxygen as the \"oxidizing principle\" (hence its name, from Ancient Greek: oksús, “sharp;” génos, “birth,” referring to oxygen's supposed role in the formation of acids), Priestley described phlogiston as the \"alkaline principle.\"\n\nPhlogiston remained the dominant theory until the 1770s when Antoine-Laurent de Lavoisier showed that combustion requires a gas that has mass (specifically, oxygen) and could be measured by means of weighing closed vessels. \nThe use of closed vessels also negated the buoyancy that had disguised the mass of the gases of combustion. These observations solved the mass paradox and set the stage for the new oxygen theory of combustion. Elizabeth Fulhame demonstrated through experiment that many oxidation reactions occur only in the presence of water, that they directly involve water, and that water is regenerated and is detectable at the end of the reaction. Based on her experiments, she disagreed with some of the conclusions of Lavoisier as well as with the phlogiston theorists that he critiqued. Her book on the subject appeared in print soon after Lavoisier's execution for Farm-General membership during the French Revolution.\n\nExperienced chemists who supported Stahl's phlogiston theory attempted to respond to the challenges suggested by Lavoisier and the newer chemists. In doing so, phlogiston theory became more complicated and assumed too much, contributing to the overall demise of the theory.\nMany people tried to remodel their theories on phlogiston in order to have the theory work with what Lavoisier was doing in his experiments. Pierre Macquer reworded his theory many times, and even though he is said to have thought the theory of phlogiston was doomed, he stood by phlogiston and tried to make the theory work.\n"}
{"id": "31714046", "url": "https://en.wikipedia.org/wiki?curid=31714046", "title": "Pugh–Schiff precession", "text": "Pugh–Schiff precession\n\nPugh–Schiff precession or spin–spin precession is the Lense-Thirring precession of an orbiting spinning vector. Caused by the general relativity effect of frame-dragging on a gyroscope orbiting a spinning body.\n\nSchiff precession is the usual name for this effect.\n\n"}
{"id": "4974142", "url": "https://en.wikipedia.org/wiki?curid=4974142", "title": "Regulation school", "text": "Regulation school\n\nThe regulation school () is a group of writers in political economy and economics whose origins can be traced to France in the early 1970s, where economic instability and stagflation were rampant in the French economy. The term \"régulation\" was coined by Frenchman Destanne de Bernis, who aimed to use the approach as a systems theory to bring Marxian economic analysis up to date. These writers are influenced by structural Marxism, the Annales School, institutionalism, Karl Polanyi's substantivist approach, and theory of Charles Bettelheim, among others, and sought to present the emergence of new economic (and hence social) forms in terms of tensions within existing arrangements. Since they are interested in how historically specific systems of capital accumulation are \"regularized\" or stabilized, their approach is called the \"regulation approach\" or \"regulation theory\". Although this approach originated in Michel Aglietta's monograph \"A Theory of Capitalist Regulation: The US Experience\" (Verso, 1976) and was popularized by other Parisians such as Robert Boyer, its membership goes well beyond the so-called Parisian School, extending to the Grenoble School, the German School, the Amsterdam School, British radical geographers, the US Social Structure of Accumulation School, and the neo-Gramscian school, among others.\n\nRobert Boyer describes the broad theory as \"The study of the transformation of social relations, which creates new forms- both economic and non-economic- organized in structures and reproducing a determinate structure, the mode of reproduction\". This theory or approach looks at capitalist economies as a function of social and institutional systems and not just as government's role in the regulation of the economy, although the latter is a major part of the approach.\n\nRegulation theory discusses historical change of the political economy through two central concepts, \"regime of accumulation or accumulation regime\" (AR) and \"mode of regulation\" (MR). The concept of regime of accumulation allows theorists to analyze the way production, circulation, consumption, and distribution organize and expand capital in a way that stabilizes the economy over time. Alain Lipietz, in \"Towards a New Economic Order\", describes the regime of accumulation of Fordism as composed of mass-producing, a proportionate share-out of value added, and a consequent stability in firm’s profitability, with the plant used at full capacity and full employment (p. 6).\n\nAn MR is a set of institutional laws, norms, forms of state, policy paradigms, and other practices that provide the context for the AR's operation. Typically, it is said that it comprises a money form, a competition form, a wage form, a state form, and an international regime, but it can encompass many more elements than these. Generally speaking, MRs support ARs by providing a conducive and supportive environment, in which the ARs are given guidelines that they should follow. In cases of tension between the two, a crisis may occur. Thus this approach parallels Marx's characterisation of historical change as driven by contradictions between the forces and the relations of production (see historical materialism).\n\nBob Jessop summarises the difficulties of the term in \"Governing Capitalist Economies\" as follows: \"The RA seeks to integrate analysis of political economy with analysis of civil society and or State to show how they interact to normalize the capital relation and govern the conflictual and crisis-mediated course of capital accumulation. In this sense, \"régulation\" might have been better and less mechanically translated as regularization or normalization\" (p 4). Therefore, the term \"régulation\" does not necessarily translate well as \"regulation\". Regulation in the sense of government action does have a part in regulation theory.\n\nRobert Boyer distinguished two main modes of regulation throughout the 19th and 20th centuries: \n\nRegulationist economists distinguish between cyclical and structural crises. They study only structural crises, which are the crises of a mode of regulation.\nFrom this distinction, they have formulated a typology of crises that accounts for various disarrangements in institutional configurations. According to its initial objective, which was to understand the rupture of the Fordist mode of regulation: \n\nSince the 1980s, the Regulation school has developed research at other socio-economic levels: firms, markets and branches studies (food and agriculture, automotive, banking…); development and local regions (\"regulation, sectors and territories\" research workshop); developing countries or developed economies others than France and USA (South Korea, Chile, Belgium, Japan, Algeria, etc.): political economy of globalization (diversity of capitalisms, politics and firms...). By doing so, its methods now range from institutional macroeconomics to discurses analysis, with quantitative and qualitative methods.\n\n\n\n\n"}
{"id": "41258426", "url": "https://en.wikipedia.org/wiki?curid=41258426", "title": "Rent regulation", "text": "Rent regulation\n\nRent regulation is a system of laws, administered by a court or a public authority, which aim to ensure the affordability of housing and tenancies on the rental market for dwellings. Generally, a system of rent regulation involves:\n\n\nThe classic objective is to limit the price that would result from the free market. The loose term \"rent control\" can apply to several types of price control:\n\nAs of 2016, at least 14 of the 36 OECD countries have some form of rent control in effect, including four states in the United States. \n\nA 2009 review of the economic literature by Blair Jenkins through EconLit covering theoretical and empirical research on multiple aspects of the issue, including housing availability, maintenance and housing quality, rental rates, political and administrative costs, and redistribution, for both first generation and second generation rent control systems, found that “the economics profession has reached a rare consensus: Rent control creates many more problems than it solves”. \n\nIn Canada, there are rent regulation laws in each province. For example, in Ontario the Residential Tenancies Act 2006 requires that prices for rented properties do not rise more than 2.5 percent each year, or a lower figure fixed by a government minister.\n\nGerman rent regulation is found in the \"Civil Code\" (the \"Bürgerliches Gesetzbuch\") in §§ 535 to §§ 580a, and particular rights for tenants on termination are in §§568 ff. The increases of rental price are required to follow a \"rental mirror\" (\"Mietspiegel\"), which is a database of local reference rent prices. This collects all rent prices in the past four years, and landlords may only increase prices on their property in line with rents in the same locality. Usury Rents are prohibited altogether, so that any price rises above 20 per cent over three years are unlawful. \n\nTenants may be evicted against their will through a court procedure for a good reason, and in the normal case only with a minimum of three months' notice. Tenants receive unlimited duration of their rental agreement unless the duration is explicitly halted. In practice, landlords have little incentive to change tenants as rental price increases beyond inflation are constrained. During the period of the tenancy, a person's tenancy may only be terminated for very good reasons. A system of rights for the rental property to be maintained by the landlord is designed to ensure quality of housing. Many states, such as Berlin, have a constitutional right to adequate housing, and require buildings to make dwelling spaces of a certain size and ceiling height.\n\nRent regulation covered the whole of the UK private sector rental market from 1915 to 1980. However, from the Housing Act 1980, it became the Conservative Party's policy to deregulate and dismantle rent regulation. Regulation for all new tenancies was abolished by the Housing Act 1988, leaving the basic regulatory framework was \"freedom of contract\" by the landlord to set any price. Rent regulations survive among a small number of council houses, and often the rates set by local authorities mirror escalating prices in the non-regulated private market.\n\nRent regulation in the United States is an issue for each state. In 1921, the US Supreme Court case of \"Block v. Hirsh\" held by a majority that regulation of rents in the District of Columbia as a temporary emergency measure was constitutional, but shortly afterwards in 1924 in \"Chastleton Corp v. Sinclair\" the same law was unanimously struck down by the Supreme Court. After the 1930s New Deal, the Supreme Court ceased to interfere with social and economic legislation, and a number of states adopted rules. The application was often inconsistent. For example, in New York City, almost half of property units continue to have the protection of rent regulation, while other units on the private market are left to be priced according to what the market will bear. In the 1986 case of \"Fisher v. City of Berkeley\", the US Supreme court held that there was no incompatibility between rent control and the Sherman Act.\n\nAs of 2018, four states (California, New York, New Jersey, and Maryland) and the District of Columbia have localities in which some form of residential rent control is in effect (for normal structures, excluding mobile homes). Thirty-seven states either prohibit or preempt rent control, while nine states allow their cities to enact rent control, but have no cities that have implemented it.\n\nRent price controls remain the most controversial element of a system of rent regulation. Historically, economists such as Adam Smith and David Ricardo viewed landlords as producing very little that was valuable, and so regarded \"rents\" as an exploitative concept. (Economists note that the land value tax is a way to capture this un-earned value.) Modern rent controls (sometimes called rent leveling or rent stabilization) are intended to protect tenants in privately owned residential properties from excessive rent increases by mandating gradual rent increases, while at the same time ensuring that landlords receive a return on their investment that is deemed fair by the controlling authority (which might, or might not be a legislature).\n\nIt is argued by most economists, including a number of neo-classical and Keynesian economists that some forms of rent control regulations create shortages and exacerbate scarcity in the housing market by discouraging private investment in the rental market. This analysis targeted nominal rent freezes, and the studies conducted were mainly focused on rental prices in Manhattan, or elsewhere in the United States.\n\nThe Swedish economist Assar Lindbeck, a housing expert, says that \"rent control appears to be the most efficient technique presently known to destroy a city – except for bombing\".\n\nIn a 1992 stratified, random survey of 464 US economists, economics graduate students, and members of the American Economic Association, 93% \"generally agreed\" or \"agreed with provisos\" that \"A ceiling on rents reduces the quantity and quality of housing available.\" \n\nA 2009 review of the economic literature by Blair Jenkins through EconLit covering theoretical and empirical research on multiple aspects of the issue, including housing availability, maintenance and housing quality, rental rates, political and administrative costs, and redistribution, for both first generation and second generation rent control systems, found that “the economics profession has reached a rare consensus: Rent control creates many more problems than it solves”. \n\nIn a 2012 poll of the 41 members of the Initiative on Global Markets (IGM) Economic Experts Panel, only one member agreed with the following statement: \"Local ordinances that limit rent increases for some rental housing units, such as in New York and San Francisco, have had a positive impact over the past three decades on the amount and quality of broadly affordable rental housing in cities that have used them.\" (13 \"strongly disagreed\", 20 \"disagreed\", 1 \"agreed\", and 7 either did not answer, were undecided, or had no opinion.) \n\nIn a 2013 analysis of the body of economic research on rent control by Peter Tatian at the Urban Institute (a think tank described both as \"liberal\" and \"independent\"), he stated that \"The conclusion seems to be that rent stabilization doesn’t do a good job of protecting its intended beneficiaries—poor or vulnerable renters—because the targeting of the benefits is very haphazard.\", and concluded that: \"Given the current research, there seems to be little one can say in favor of rent control.\" \n\nTwo economists from opposing sides of the political spectrum, Nobel Laureate Paul Krugman (who identifies as an American liberal or European social democrat), and Thomas Sowell, (who stated that \"libertarian\" might best describe his views) have both criticized rent regulation as poor economics, which, despite its good intentions, leads to the creation of less housing and housing shortages. Krugman also notes that rent control creates bitter tenant-landlord relations, and in markets with not all apartments under rent control, it causes an increase in rents for uncontrolled units, while Sowell notes that rent control increases urban blight. \n\n\n\n"}
{"id": "14071965", "url": "https://en.wikipedia.org/wiki?curid=14071965", "title": "Robert H. Gray", "text": "Robert H. Gray\n\nRobert H. Gray is an American data analyst, author, and astronomer, and author of \"The Elusive Wow: Searching for Extraterrestrial Intelligence\".\n\nGray attended Shimer College, a Great Books school then located in Mount Carroll, Illinois, where he received a bachelor's degree in 1970. He went on to obtain a master's in urban planning and policy analysis from the University of Illinois at Chicago in 1980.\n\nIn 1984, Gray founded the company Gray Data in Chicago, which provided data analysis research services and published reference cards for microcomputer software. He continues to work as a data analyst through his company Gray Consulting.\n\nGray is best known for his work as an independent SETI researcher, especially in regard to his searches for the Wow! signal. \"The Atlantic\" called Gray \"the 'Wow!' signal's most devoted seeker and chronicler, having traveled to the very ends of the earth in search of it.\"\n\nThe Wow! signal was detected by the Ohio State University Radio Observatory (also known as Big Ear) on August 15, 1977. The signal was so pronounced in the data, and so similar to a radio signal rather than a natural source, that SETI scientist Jerry R. Ehman circled it on the computer printout in red ink and wrote \"Wow!\" next to it. After hearing about the Wow! signal a few years after its detection, Gray contacted the Ohio team, visited Big Ear, and spoke with Ehman, Robert S. Dixon (director of the SETI project) and John D. Kraus (the telescope's designer). \n\nIn 1980, Gray began scanning the skies from his backyard in Chicago, using a 12-foot commercial telecommunications dish. He operated his small SETI radio observatory regularly beginning in 1983 and for the next 15 years, but did not find a trace of the Wow! signal. In 1987 and 1989 he led searches for the signal using the Harvard/Smithsonian META radio telescope at the Oak Ridge Observatory in Harvard, Massachusetts. In September 1995 and again in May 1996, Gray and Kevin B. Marvel reported searches for the signal using the Very Large Array (VLA) radio telescope in New Mexico (which is an array of 27 dishes simulating a single dish with a diameter of up to 22 miles), becoming the first amateur astronomer to use the VLA, and the first individual to use it to search for extraterrestrial signals. The VLA was, until the end of the twentieth century, the most powerful radio telescope ever built. In 1998, he and University of Tasmania professor Simon Ellingsen conducted searches using the 26-meter dish at the Mount Pleasant Radio Observatory in Hobart, Tasmania. Gray and Ellingsen made six 14-hour observations where the Big Ear was pointing when it found the Wow! signal, searching for intermittent and possibly periodic signals, rather than a constant signal. No signals resembling the Wow! were detected.\n\nGray and Marvel published a 2001 paper in \"The Astrophysical Journal\" detailing his use of the VLA in search of the signal. Gray and Ellingsen published \"A Search for Periodic Emissions at the Wow Locale\" in the October 2002 issue of \"The Astrophysical Journal\", reporting on searches for the Wow! signal. In 2011, Gray published the book \"The Elusive Wow: Searching for Extraterrestrial Intelligence\", summarizing what is known about the Wow! signal, covering his own search for the signal, and offering an overview of the search for extraterrestrial intelligence. Gray concluded that the signal could have come from an alien \"lighthouse\".\n\nIn 2016, Gray published an article in \"Scientific American\" about the Fermi paradox, which claims that if extraterrestrials existed, we would see signs of them on Earth, because they would certainly colonize the galaxy by interstellar travel. Gray argues that the Fermi paradox, named after Nobel Prize-winning physicist Enrico Fermi, does not accurately represent Fermi's views. Gray states that Fermi questioned the feasibility of interstellar travel, but did not say definitively whether or not he thought extraterrestrials exist.\n\nGray lives in Chicago, Illinois, with his wife, photographer Sharon A. Hoogstraten.\n\n\n"}
{"id": "13526782", "url": "https://en.wikipedia.org/wiki?curid=13526782", "title": "SaltMod", "text": "SaltMod\n\nSaltMod is computer program for the prediction of the salinity of soil moisture, groundwater and drainage water, the depth of the watertable, and the drain discharge (hydrology) in irrigated agricultural lands, using different (geo)hydrologic conditions, varying water management options, including the use of ground water for irrigation, and several cropping rotation schedules.\nThe water management options include irrigation, drainage, and the use of subsurface drainage water from pipe drains, ditches or wells for irrigation.\n\nThe majority of the computer models available for water and solute transport in the soil (e.g. Swatre, DrainMod ) are based on Richard's differential equation for the movement of water in unsaturated soil in combination with a differential salinity dispersion equation. The models require input of soil characteristics like the relation between unsaturated soil moisture content, water tension, hydraulic conductivity and dispersivity.\n\nThese relations vary to a great extent from place to place and are not easy to measure. The models use short time steps and need at least a daily data base of hydrological phenomena. Altogether this makes model application to a fairly large project the job of a team of specialists with ample facilities.\nLiterature references (chronological) to case studies after 2000: \n\n<br>\nOlder examples of application can be found in:\n\nThere is a need for a computer program that is easier to operate and that requires a simpler data structure than most currently available models. Therefore, the SaltModod program was designed keeping in mind a relative simplicity of operation to facilitate the use by field technicians, engineers and project planners instead of specialized geo-hydrologists.\n\nIt aims at using input data that are generally available, or that can be estimated with reasonable accuracy, or that can be measured with relative ease. Although the calculations are done numerically and have to be repeated many times, the final results can be \"checked by hand\" using the formulas in the manual.\n\nSaltMod's objective is to predict the long-term hydro-salinity in terms of general trends, not to arrive at exact predictions of how, for example, the situation would be on the first of April in ten years from now.\n\nFurther, SaltMod gives the option of the re-use of drainage and well water (e.g. for irrigation) and it can account for farmers' response to waterlogging, soil salinity, water scarcity and over-pumping from the aquifer. Also it offers the possibility to introduce subsurface drainage systems at varying depths and with varying capacity so that they can be optimized.\nOther features of Saltmod are found in the next section.\n\nThe computation method Saltmod is based on seasonal water balances of agricultural lands. Four seasons in one year can be distinguished, e.g. dry, wet, cold, hot, irrigation or fallow seasons. The number of seasons (Ns) can be chosen between a minimum of one and a maximum of four. The larger the number of seasons becomes, the larger is the number of input data required. The duration of each season (Ts) is given in number of months (0 < Ts < 12). Day to day water balances are not considered for several reasons:\n\nThe method uses seasonal water balance components as input data. These are related to the surface hydrology (like rainfall, evaporation, irrigation, use of drain and well water for irrigation, runoff), and the aquifer hydrology (like upward seepage, natural drainage, pumping from wells). The other water balance components (like downward percolation, upward capillary rise, subsurface drainage) are given as output.<br>\nThe quantity of drainage water, as an output, is determined by two drainage intensity factors for drainage above and below drain level respectively (to be given with the input data), a drainage reduction factor (to simulate a limited operation of the drainage system), and the height of the water table, resulting from the computed water balance. Variation of the drainage intensity factors and the drainage reduction factor gives the opportunity to simulate the effect of different drainage options.\n\nThe input data on irrigation, evaporation, and surface runoff are to be specified per season for three kinds of agricultural practices, which can be chosen at the discretion of the user: \nThe groups, expressed in fractions of the total area, may consist of combinations of crops or just of a single kind of crop. For example, as the A type crops one may specify the lightly irrigated cultures, and as the B type the more heavily irrigated ones, such as sugarcane and rice. But one can also take A as rice and B as sugarcane, or perhaps trees and orchards. The A, B and/or U crops can be taken differently in different seasons, e.g. A=wheat+barley in winter and A=maize in summer while B=vegetables in winter and B=cotton in summer.<br> \n\nSaltmod accepts four different reservoirs, three of which are in the soil profile:\nThe upper soil reservoir is defined by the soil depth from which water can evaporate or be taken up by plant roots. It can be equal to the rootzone.<br>\nThe root zone can be saturated, unsaturated, or partly saturated, depending on the water balance. All water movements in this zone are vertical, either upward or downward, depending on the water balance. (In a future version of Saltmod, the upper soil reservoir may be divided into two equal parts to detect the trend in the vertical salinity distribution.)<br>\nThe transition zone can also be saturated, unsaturated or partly saturated. All flows in this zone are vertical, except the flow to subsurface drains.<br>\nIf a horizontal subsurface drainage system is present, this must be placed in the transition zone, which is then divided into two parts: an upper transition zone (above drain level) and a lower transition zone (below drain level).<br>\nIf one wishes to distinguish an upper and lower part of the transition zone in the absence of a subsurface drainage system, one may specify in the input data a drainage system with zero intensity.<br>\nThe aquifer has mainly horizontal flow. Pumped wells, if present, receive their water from the aquifer only.\n\nThe water balances are calculated for each reservoir separately as shown in the article Hydrology (agriculture). The excess water leaving one reservoir is converted into incoming water for the next reservoir.<br>\nThe three soil reservoirs can be assigned a different thickness and storage coefficients, to be given as input data.<br>\nIn a particular situation, the transition zone or the aquifer need not be present. Then, it must be given a minimum thickness of 0.1 m.<br>\nThe depth of the water table, calculated from the water balances, is assumed to be the same for the whole area. If this assumption is not acceptable, the area must be divided into separate units.<br>\nUnder certain conditions, the height of the water table influences the water balance components. For example a rise of the water table towards the soil surface may lead to an increase of evaporation, surface runoff, and subsurface drainage, or a decrease of percolation losses from canals. This, in turn, leads to a change of the water balance, which again influences the height of the water table, etc.<br>\nThis chain of reactions is one of the reasons why Saltmod has been developed into a computer program. It takes a number of repeated calculations (iterations) to find the correct equilibrium of the water balance, which would be a tedious job if done by hand. Other reasons are that a computer program facilitates the computations for different water management options over long periods of time (with the aim to simulate their long-term effects) and for trial runs with varying parameters.\n\nThe sub-surface drainage can be accomplished through drains or pumped wells.<br>\nThe subsurface drains are characterized by drain depth and \"drainage capacity factor \". The drains are located in the transition zone. The subsurface drainage facility can be applied to natural or artificial drainage systems. The functioning of an artificial drainage system can be regulated through a \"drainage control factor\".<br>\nWhen no drainage system is present, installing drains with zero capacity offers the opportunity to obtain separate water and salt balances for an upper and lower part of the transition zone.<br>\nThe pumped wells are located in the aquifer. Their functioning is characterized by the well discharge.<br>\nThe drain and well water can be used for irrigation through a \"re-use factor\". This may affect the salt balance and the irrigation efficiency or sufficiency.\nThe salt balances are calculated for each reservoir separately. They are based on their water balances, using the \"salt concentrations\" of the incoming and outgoing water. Some concentrations must be given as input data, like the initial salt concentrations of the water in the different soil reservoirs, of the irrigation water and of the incoming ground water in the aquifer.\n\nThe concentrations are expressed in terms of electric conductivity (EC in dS/m). When the concentrations are known in terms of g salt/l water, the rule of thumb: 1 g/l -> 1.7 dS/m can be used. Usually, salt concentrations of the soil are expressed in ECe, the electric conductivity of an extract of a saturated soil paste (saturation extract). In Saltmod, the salt concentration is expressed as the EC of the soil moisture when saturated under field conditions. As a rule, one can use the conversion rate EC : ECe = 2 : 1.<br>\nSalt concentrations of outgoing water (either from one reservoir into the other or by subsurface drainage) are computed on the basis of salt balances, using different \"leaching or salt mixing efficiencies\" to be given with the input data. The effects of different leaching efficiencies can be simulated by varying their input value.\nIf drain or well water is used for irrigation, the method computes the salt concentration of the mixed irrigation water in the course of the time and the subsequent effect on the soil and ground water salinities, which again influences the salt concentration of the drain and well water. By varying the fraction of used drain or well water (to be given in the input data), the long-term effect of different fractions can be simulated.\nThe dissolution of solid soil minerals or the chemical precipitation of poorly soluble salts is not included in the computation method, but to some extent it can be accounted for through the input data, e.g. by increasing or decreasing the salt concentration of the irrigation water or of the incoming water in the aquifer.\n\nIf required, farmers' responses to water logging and soil salinity can be automatically accounted for. The method can gradually decrease:\nResponse (1) is different for ponded (submerged) rice (paddy) and \"dry foot\" crops.\nThe responses influence the water and salt balances, which, in their turn, slow down the process of water logging and salinization. Ultimately an equilibrium situation will be brought about.\nThe user can also introduce farmers' responses by manually changing the relevant input data. Perhaps it will be useful first to study the automatic farmers' responses and their effect and thereafter decide what the farmers' responses will be in the view of the user.\nThe responses influence the water and salt balances, which, in their turn, slow down the process of water logging and salinization. Ultimately an equilibrium situation will be brought about.\nThe user can also introduce farmers' responses by manually changing the relevant input data. Perhaps it will be useful first to study the automatic farmers' responses and their effect and thereafter decide what the farmers' responses will be in the view of the user.\n\nThe program may run with fixed input data for the number of years determined by the user. This option can be used to predict future developments based on long-term average input values, e.g. rainfall, as it will be difficult to assess the future values of the input data year by year.\nThe program also offers the possibility to follow historic records with annually changing input values (e.g. rainfall, irrigation, agricultural practices), the calculations must be made year by year. If this possibility is chosen, the program creates transfer files by which the final conditions of the previous year (e.g. water table and salinity) are automatically used as the initial conditions for the subsequent period. This facility makes it possible to use various generated rainfall sequences drawn randomly from a known rainfall probability distribution and obtain a stochastic prediction of the resulting output parameters.\nIf the computations are made with annual changes, not all input parameters can be changed, notably the thickness of the soil reservoirs and their total porosities as these would cause illogical shifts in the water and salt balances.\n\nThe output of Saltmod is given for each season of any year during any number of years, as specified with the input data. The output data comprise hydrological and salinity aspects.\n\nThe data are filed in the form of tables that can be inspected directly or further analyzed with spreadsheet programs.\n\nAs the soil salinity is very variable from place to place (figure left) SaltMod includes frequency distributions in the output. The figure was made with the CumFreq program .\nThe program offers the possibility to develop a multitude of relations between varied input data, resulting outputs and time.<br> \nHowever, as it is not possible to foresee all different uses that may be made, the program offers only a limited number of standard graphics.\nThe program is designed to make use of spreadsheet programs for the detailed output analysis, in which the relations between various input and output variables can be established according to the scenario developed by the user.\n\nAlthough the computations need many iterations, all the end results can be \"checked by hand\" using the equations presented in the manual.\n\n\n"}
{"id": "37463002", "url": "https://en.wikipedia.org/wiki?curid=37463002", "title": "Simkania", "text": "Simkania\n\nSimkania, is a genus of bacteria belonging to the Chlamydiae. The only species of this genus is \"Simkania negevensis\".\n"}
{"id": "1923692", "url": "https://en.wikipedia.org/wiki?curid=1923692", "title": "Stephen L. Adler", "text": "Stephen L. Adler\n\nStephen Louis Adler (born November 30, 1939) is an American physicist specializing in elementary particles and field theory.\n\nAdler was born in New York City. He received an A.B. degree at Harvard University in 1961, where he was a Putnam Fellow, and a Ph.D. from Princeton University in 1964. He is the son of Irving Adler and Ruth Adler and older brother of Peggy Adler.\n\nAdler was elected a Fellow of the American Academy of Arts and Sciences in 1974. He became a member of the Institute for Advanced Study in 1966, becoming a full Professor of Theoretical Physics in 1969, and was named \"New Jersey Albert Einstein Professor\" at the institute in 1979.\n\nHe has won the J. J. Sakurai Prize from the American Physical Society in 1988, and the Dirac Medal of the International Centre for Theoretical Physics in 1998, among other awards.\n\nAdler's seminal papers on high energy neutrino processes, current algebras, soft pion theorems,\nsum rules, and perturbation theory anomalies helped lay the foundations for the current standard model of elementary particle physics.\n\nIn 2012, Adler contributed to a family venture when he wrote the foreword for his then 99-year-old father's 87th book, \"Solving the Riddle of Phyllotaxis: Why the Fibonacci Numbers and the Golden Ratio Occur on Plants\". The book's diagrams are by his sister Peggy.\n\nIn his book \"Quantum Theory as an Emergent Phenomenon\", published 2004, Adler presented his trace dynamics, a framework in which quantum field theory emerges from a matrix theory. In this matrix theory, particles are represented by non-commuting matrices, and the matrix elements of bosonic and fermionic particles are ordinary complex numbers and non-commuting Grassmann numbers, respectively. Using the action principle, a Lagrangian can be constructed from the trace of a polynomial function of these matrices, leading to Hamiltonian equations of motion. The construction of a statistical mechanics of these matrix models leads, so Adler says, to an \"emergent effective complex quantum field theory\".\n\nAdler's Trace Dynamics has been discussed in relation to the differential space theory of quantum systems by Norbert Wiener and Amand Siegel, to its variant by David Bohm and Jeffrey Bub, and to modifications of the Schrödinger equation by additional terms such as the quantum potential term or stochastic terms, and to hidden variable theories.\n\n\n"}
{"id": "13718304", "url": "https://en.wikipedia.org/wiki?curid=13718304", "title": "Terminal countdown demonstration test", "text": "Terminal countdown demonstration test\n\nA terminal countdown demonstration test (TCDT) is a simulation of the final hours of a launch countdown and serves as a practice exercise in which both the launch team and flight crew rehearse launch day timelines and procedures. In the specific case of a TCDT for the Space Shuttle, the test culminated in a simulated ignition and RSLS Abort (automated shutdown of the orbiter's main engines). Following the simulated abort, the flight crew was briefed on emergency egress procedures and use of the fixed service structure slidewire system. On some earlier shuttle missions, and Apollo missions, the test would conclude with the flight crew evacuating the launch pad by use of these emergency systems, however this is no longer part of the test.\n\nUnmanned carrier rocket launches also undergo TCDTs, when countdown procedures are followed. These vary for specific rockets, for example solid-fuelled rockets would not simulate an engine shutdown, as it is impossible to shut down a solid rocket after it has been lit.\n\nTCDTs typically are carried out a few days before launch.\n\n"}
{"id": "19264489", "url": "https://en.wikipedia.org/wiki?curid=19264489", "title": "Ubuntu version history", "text": "Ubuntu version history\n\nUbuntu releases are made semiannually by Canonical Ltd, the developers of the Ubuntu operating system, using the year and month of the release as a version number. The first Ubuntu release, for example, was Ubuntu 4.10 and was released on 20 October 2004. Consequently, version numbers for future versions are provisional; if the release is delayed until a different month (or even year) to that planned, the version number will change accordingly.\n\nCanonical schedules Ubuntu releases to occur approximately one month after GNOME releases, which in turn come about one month after releases of X.Org, resulting in each Ubuntu release including a newer version of GNOME and X.\n\nEvery fourth release—occurring in the second quarter of even-numbered years—has been designated as a long-term support (LTS) release. The desktop version of LTS releases for 10.04 and earlier were supported for three years, with server version support for five years. LTS releases 12.04, 14.04 and 16.04 are supported for five years, while Ubuntu 18.04 LTS is supported for ten years. The support period for non-LTS releases is 9 months. Prior to 13.04, it was 18 months.\n\nUbuntu releases are also given code names, using an adjective and an animal with the same first letter (e.g. \"Dapper Drake\"). With the exception of the first two releases, code names are in alphabetical order, allowing a quick determination of which release is newer. As of Ubuntu 17.10, however, the initial letter 'rolled over' and returned to 'A'. Names are occasionally chosen so that animal appearance or habits reflects some new feature (e.g., \"Koala's favourite leaf is Eucalyptus\"; see below). Ubuntu releases are often referred to using only the adjective portion of the code name (e.g. \"Feisty\").\n\nUbuntu 4.10 (\"Warty Warthog\"), released on 20 October 2004, was Canonical's first release of Ubuntu, building upon Debian, with plans for a new release every six months and eighteen months of support thereafter. Ubuntu 4.10's support ended on 30 April 2006. Ubuntu 4.10 was offered as a free download and, through Canonical's ShipIt service, was also mailed to users free of charge in CD format.\n\nUbuntu 5.04 (\"Hoary Hedgehog\"), released on 8 April 2005, was Canonical's second release of Ubuntu. Ubuntu 5.04's support ended on 31 October 2006. Ubuntu 5.04 added many new features including an Update Manager, upgrade notifier, readahead and grepmap, suspend, hibernate and standby support, dynamic frequency scaling for processors, Ubuntu hardware database, Kickstart installation, and APT authentication. Ubuntu 5.04 was the first version that allowed installation from USB devices. Beginning with Ubuntu 5.04, UTF-8 became the default character encoding.\n\nUbuntu 5.10 (\"Breezy Badger\"), released on 12 October 2005, was Canonical's third release of Ubuntu. Ubuntu 5.10's support ended on 13 April 2007. Ubuntu 5.10 added several new features including a graphical bootloader (Usplash), an Add/Remove Applications tool,<ref name=\"add/remove\"></ref> a menu editor (Alacarte), an easy language selector, logical volume management support, full Hewlett-Packard printer support, OEM installer support, a new Ubuntu logo in the top-left, and Launchpad integration for bug reporting and software development.\n\nUbuntu 6.06 (\"Dapper Drake\"), released on 1 June 2006, was Canonical's fourth release, and the first long-term support (LTS) release. Ubuntu 6.06 was released behind schedule, having been intended as 6.04. It is sometimes jokingly described as their first 'Late To Ship' (LTS) release. Development was not complete in April 2006 and Mark Shuttleworth approved slipping the release date to June, making it 6.06 instead.\n\nUbuntu 6.06's support ended on 14 July 2009 for desktops and ended in June 2011 for servers. Ubuntu 6.06 included several new features, including having the Live CD and Install CD merged onto one disc, a graphical installer on Live CD (Ubiquity), Usplash on shutdown as well as startup, a network manager for easy switching of multiple wired and wireless connections, Humanlooks theme implemented using Tango guidelines, based on Clearlooks and featuring orange colors instead of brown, and GDebi graphical installer for package files. Ubuntu 6.06 did not include a means to install from a USB device, but did for the first time allow installation directly onto removable USB devices.\n\nUbuntu 6.10 (\"Edgy Eft\"), released on 26 October 2006, was Canonical's fifth release of Ubuntu. Ubuntu 6.10's support ended on 25 April 2008. Ubuntu 6.10 added several new features including a heavily modified Human theme, Upstart init daemon, automated crash reports (Apport), Tomboy note taking application, and F-Spot photo manager. EasyUbuntu, a third party program designed to make Ubuntu easier to use, was included in Ubuntu 6.10 as a meta-package.\n\nUbuntu 7.04 (\"Feisty Fawn\"), released on 19 April 2007, was Canonical's sixth release of Ubuntu. Ubuntu 7.04's support ended on 19 October 2008. Ubuntu 7.04 included several new features, among them a migration assistant to help former Microsoft Windows users transition to Ubuntu, support for Kernel-based Virtual Machine, assisted codec and restricted drivers installation including Adobe Flash, Java, MP3 support, easier installation of Nvidia and ATI drivers, Compiz desktop effects, support for Wi-Fi Protected Access, the addition of Sudoku and chess, a disk usage analyzer (baobab), GNOME Control Center, and Zeroconf support for many devices. Ubuntu 7.04 dropped support for PowerPC architecture.\n\nUbuntu 7.10 (\"Gutsy Gibbon\"), released on 18 October 2007, was Canonical's seventh release of Ubuntu. Ubuntu 7.10's support ended on 18 April 2009. Ubuntu 7.10 included several new features, among them AppArmor security framework, fast desktop search, a Firefox plug-in manager (Ubufox), a graphical configuration tool for X.Org, full NTFS support (read/write) via NTFS-3G, and a revamped printing system with PDF printing by default. Compiz Fusion was enabled as default in Ubuntu 7.10 and Fast user switching was added.\n\nUbuntu 8.04 (\"Hardy Heron\"), released on 24 April 2008, was Canonical's eighth release of Ubuntu and the second Long Term Support (LTS) release. Ubuntu 8.04's support ended on 12 May 2011 for desktops and ended on 9 May 2013 for servers as well. Ubuntu 8.04 included several new features, among them Tracker desktop search integration, Brasero disk burner, Transmission BitTorrent client, Vinagre VNC client, system sound through PulseAudio, and Active Directory authentication and login using Likewise Open. In addition Ubuntu 8.04 included updates for better Tango compliance, various Compiz usability improvements, automatic grabbing and releasing of the mouse cursor when running on a VMware virtual machine, and an easier method to remove Ubuntu. Ubuntu 8.04 was the first version of Ubuntu to include the Wubi installer on the Live CD that allows Ubuntu to be installed as a single file on a Windows hard drive without the need to repartition the disk. The first version of the Ubuntu Netbook Remix was also introduced.\n\nUbuntu 8.10 (\"Intrepid Ibex\"), released on 30 October 2008, was Canonical's ninth release of Ubuntu. Support ended on 30 April 2010. Ubuntu 8.10 introduced several new features including improvements to mobile computing and desktop scalability, increased flexibility for Internet connectivity, an Ubuntu Live USB creator and a guest account, which allowed others to use a computer allowing very limited user rights (e.g. accessing the Internet, using software and checking e-mail). The guest account had its own home folder and nothing done on it was stored permanently on the computer's hard disk. Intrepid Ibex also included an encrypted private directory for users, the inclusion of Dynamic Kernel Module Support, a tool that allows kernel drivers to be automatically rebuilt when new kernels are released, and support for creating USB flash drive images.\n\nUbuntu 9.04 (\"Jaunty Jackalope\"), released on 23 April 2009, was Canonical's tenth release of Ubuntu. Support ended on 23 October 2010. New features included faster boot time, integration of web services and applications into the desktop interface. Because of that, they named it after the mythical jackalope. It was the first release named after a mythical animal, the second being Utopic Unicorn. It had a new usplash screen, a new login screen and also support for both Wacom (hotplugging) and netbooks. It also included a new notification system, \"Notify OSD\", and themes. It marked the first time that all of Ubuntu's core development moved to the Bazaar distributed revision control system.\n\nUbuntu 9.04 was the first version to support the ARM architecture with native support for ARMv5EL and ARMv6EL-VFP.\n\nUbuntu 9.10 (\"Karmic Koala\"), released on 29 October 2009, was Canonical's 11th release of Ubuntu. It was supported until April 2011.\n\nIn an announcement to the community on 20 February 2009, Mark Shuttleworth explained that 9.10 would focus on improvements in cloud computing on the server using Eucalyptus, saying \"...a Koala's favourite leaf is Eucalyptus\", as well as further improvements in boot speed and development of the Netbook Remix.\n\nThe initial announcement of version 9.10 indicated that this release might include a new theme, however the project was moved forward to 10.04, and only minor revisions were made to the default theme. Other graphical improvements included a new set of boot up and shutdown splash screens, a new login screen that transitions seamlessly into the desktop and greatly improved performance on Intel graphics chipsets.\n\nIn June 2009, Canonical created the One Hundred Paper Cuts project, focusing developers to fix minor usability issues. A \"paper cut\" was defined as, \"a trivially fixable usability bug that the average user would encounter on his/her first day of using a brand new installation of the latest version of Ubuntu Desktop Edition.\"\n\nThe desktop installation of Ubuntu 9.10 replaced Pidgin with Empathy Instant Messenger as its default instant messaging client. The default filesystem is ext4, and the Ubuntu One client, which interfaces with Canonical's new online storage system, is installed by default. It introduced Grub 2 beta as default bootloader. It also debuted a new application called the Ubuntu Software Center that unifies package management. Canonical stated their intention for this application to replace Add/Remove Programs (gnome-app-install) in 9.10 and possibly Synaptic, Software Sources, Gdebi and Update Manager in Ubuntu 10.04. Karmic Koala also includes a slideshow during the installation process (through ubiquity-slideshow) that highlights applications and features in Ubuntu.\n\nShuttleworth first announced Ubuntu 10.04 (\"Lucid Lynx\") on 19 September 2009 at the Atlanta Linux Fest; Canonical released it on 29 April 2010. It was Canonical's 12th release of Ubuntu and the third Long Term Support (LTS) release.\n\nThe new release included, among other things, improved support for Nvidia proprietary graphics drivers, while switching to the open source Nvidia graphics driver, Nouveau, by default. Plymouth was also introduced, allowing boot animations.\n\nGIMP was removed from the Lucid installation CD due to its professional-grade complexity and its file size. F-Spot provided normal user-level graphics-editing capabilities and GIMP remained available for download in the repositories.\n\nThe distribution emphasized the increasing importance of web services and of social networking with integrated interfaces for posting to sites like Facebook and Twitter, complementing the IM and email integration already in Ubuntu.\n\nOn 4 March 2010 it was announced that Lucid Lynx would feature a new theme, including new logos, taking Ubuntu's new visual style into account:\n\nThe new theme met with mixed critical responses. Ars Technica's Ryan Paul said: \"The new themes and updated color palette are nice improvement for Ubuntu... After testing the new theme for several hours, I feel like it's a step forward, but it still falls a bit short of my expectations.\" Paul also noted that the most controversial aspect of the new design amongst users was the placement of the window-control buttons on the left instead of on the right side of the windows. TechSource's Jun Auza expressed concern that the new theme was too close to that used by Apple's Mac OS X: \"I think Ubuntu is having an identity crisis right now and should seriously consider changing several things in terms of look and feel to avoid being branded as a Mac OS X rip-off, or worse, get sued by Apple.\" Auza also summarized Ubuntu user feedback: \"I believe the fans are divided right now. Some have learned to love the brown color scheme since it uniquely represents Ubuntu, while others wanted change.\"\n\nThe first point release, 10.04.1, was made available on 17 August 2010, and the second update, 10.04.2, was released on 17 February 2011. The third update, 10.04.3, was released on 21 July 2011, and the fourth and final update, 10.04.4, was released on 16 February 2012.\n\nCanonical provided support for the desktop version of Ubuntu 10.04 until 9 May 2013 and for the server version until 30 April 2015.\n\nThe naming of Ubuntu 10.10 (\"Maverick Meerkat\") was announced by Mark Shuttleworth on 2 April 2010, along with the release's goals of improving the netbook experience and a server focus on hybrid cloud computing. Ubuntu 10.10 was released on 10 October 2010 (10.10.10) at around 10:10 UTC. This is a departure from the traditional schedule of releasing at the end of October to get \"the perfect 10\", and a playful reference to \"The Hitchhiker's Guide to the Galaxy\", since, in binary, 101010 is equal to the number 42, the \"Answer to the Ultimate Question of Life, the Universe and Everything\" within the series. It was Canonical's 13th release of Ubuntu. New features included the new Unity interface for the Netbook Edition, a new default photo manager, Shotwell, replacing F-Spot, the ability to purchase applications in the Software Center, and an official Ubuntu font used by default. \n\nSupport for Ubuntu Maverick Meerkat 10.10 was officially ended on 10 April 2012.\n\nThe naming of Ubuntu 11.04 (\"Natty Narwhal\") was announced on 17 August 2010 by Mark Shuttleworth. Ubuntu 11.04 Natty Narwhal was released on 28 April 2011. It is Canonical's 14th release of Ubuntu.\n\nUbuntu 11.04 used the Unity user interface instead of GNOME Shell as default. The move to Unity was controversial as some GNOME developers feared it would fracture the community and marginalize GNOME Shell. The GNOME desktop environment is still available in Ubuntu 11.04 under the title Ubuntu Classic as a fallback to Unity.\n\nUbuntu 11.04 employed Banshee as the default music player, replacing Rhythmbox. Other new applications included Mozilla Firefox 4 and LibreOffice, which replaced OpenOffice.org. The OpenStack cloud computing platform was added in this release.\n\nStarting with Ubuntu 11.04, the Ubuntu Netbook Edition was merged into the desktop edition.\n\nIn reviewing Ubuntu 11.04 upon its stable release, Ryan Paul of Ars Technica said \"There is a lot to like in Ubuntu 11.04, but also a lot of room for improvement.\" Jesse Smith of Distrowatch said \"I'm of the opinion there are good features in this release, but 11.04 definitely suffered from being rushed out the door while it was still beta quality. Ubuntu aims to be novice-friendly, but this release is buggy and I think they missed the mark this time around. I'm limiting my recommendation of 11.04 to people who want to play with an early release of Unity.\"\n\nSupport for Ubuntu 11.04 officially ended on 28 October 2012.\n\nThe naming of Ubuntu 11.10 (\"Oneiric Ocelot\") was announced on 7 March 2011 by Mark Shuttleworth. He explained that Oneiric means \"dreamy\". Ubuntu 11.10 was released on schedule on 13 October 2011 and is Canonical's 15th release of Ubuntu.\n\nIn April 2011, Shuttleworth announced that Ubuntu 11.10 would not include the classic GNOME desktop as a fall back to Unity, unlike Ubuntu 11.04 Natty Narwhal. Instead, 11.10 included a 2D version of Unity as a fallback for computers that lacked the hardware resources for the Compiz-based 3D version. However, the classic GNOME desktop remained available in Ubuntu 11.10 through a package in the Ubuntu repositories. Shuttleworth also confirmed that Unity in Ubuntu 11.10 would run as a shell for GNOME 3 on top of GNOME 3 libraries, unlike in Ubuntu 11.04 where it ran as a shell for GNOME 2. Moreover, users were able to install the entire GNOME 3 stack along with GNOME Shell directly from the Ubuntu repositories; to be presented with a \"GNOME 3 desktop\" choice at login. During the development cycle there were many changes to Unity, including the placement of the Ubuntu button on the Launcher instead of on the Panel, the autohiding of the window controls (and the global menu) of maximized windows, the introduction of more transparency into the Dash (and the Panel when the Dash was opened) and the introduction of window controls for the Dash.\n\nIn May 2011, it was announced that PiTiVi would be no longer part of the Ubuntu ISO, starting with Ubuntu 11.10 Oneiric Ocelot. The reasons given for removing it included poor user reception, lack of fit with the default user-case for Ubuntu, lack of polish and the application's lack of development maturity. PiTiVi will not be replaced on the ISO with another video editor. Other changes include removing Computer Janitor, as it caused broken systems for users, and the removal of the Synaptic package manager, which can optionally be installed via the Ubuntu Software Center. Déjà Dup has been added as Ubuntu's backup program. Mozilla Thunderbird has replaced the Evolution email client. All removed applications will remain available to users for installation from the Ubuntu Software Center and repositories.\n\nSupport for Ubuntu Oneiric Ocelot was officially ended on 9 May 2013.\n\nUbuntu 12.04 LTS (\"Precise Pangolin\") is Canonical's sixteenth release of Ubuntu and its fourth Long Term Support (LTS) release, made available on schedule on 26 April 2012. It is named after the pangolin anteater. Previous LTS releases have been supported for three years for the desktop version and five years for the server version; this release was supported for five years for both versions, with support ending on 28 April 2017. Canonical continues to offer extended security maintenance to Advantage customers for an additional two years.\n\nChanges in this release include a much faster startup time for the Ubuntu Software Center and refinements to Unity. This release also switched the default media player from Banshee back to Rhythmbox and dropped the Tomboy note-taking application and the supporting Mono framework as well. Also, the window dodge feature has been removed from the Unity launcher starting with Ubuntu 12.04.\n\nUbuntu 12.04 incorporated a new head-up display (HUD) feature that allows hotkey searching for application menu items from the keyboard, without needing the mouse. Shuttleworth said that the HUD \"will ultimately replace menus in Unity applications\" but for Ubuntu 12.04 at least the menus will remain.\n\nUbuntu 12.04 is the first Ubuntu release shipped with IPv6 privacy extensions turned on by default. Ubuntu 11.10 already supported IPv6 on the desktop and in the installer (stateless address autoconfiguration SLAAC, stateless DHCPv6 and stateful DHCPv6).\n\nLike previous LTS releases, 12.04 included point releases that bundled updates to shorten downloads for users installing the release later in its lifecycle. The point releases and dates were: 12.04.1 (23 August 2012), 12.04.2 (14 February 2013), 12.04.3 (scheduled for release on 22 August 2013, but actually released on 23 August 2013), 12.04.4 (6 February 2014) and 12.04.5 (7 August 2014).\n\nJesse Smith of DistroWatch said that many people, like he, had questioned Ubuntu's direction, including Unity. But with Ubuntu 12.04 he felt that the puzzle pieces, which individually may have been underwhelming, had come together to form a whole, clear picture. He said \"Unity, though a step away from the traditional desktop, has several features which make it attractive, such as reducing mouse travel. The HUD means that newcomers can find application functionality with a quick search and more advanced users can use the HUD to quickly run menu commands from the keyboard.\" He wrote that Unity had grown to maturity, while indicating that he was bothered by its lack of flexibility. He did notice issues, however, especially that the HUD did not work in LibreOffice and performance in a virtual machine was unsatisfactory. He concluded that Ubuntu's overall experience was \"head and shoulders above anything else in the Linux ecosystem.\"\n\nJim Lynch wrote \"Ubuntu 12.04 is definitely worth an upgrade if you're running an earlier version. Unity is finally coming into its own in this release, plus there are other enhancements that make upgrading worthwhile. Ubuntu is getting better and better with each release. I was one of the Unity skeptics initially, but I've come to accept it as part of Ubuntu.\"\n\nJack Wallen of TechRepublic – who had strongly criticized early versions of Unity – said \"Since Ubuntu 12.04 was released, and I migrated over from Linux Mint, I'm working much more efficiently. This isn't really so much a surprise to me, but to many of the detractors who assume Unity a very unproductive desktop... well, I can officially say they are wrong. [...] I realize that many people out there have spurned Unity (I was one of them for a long time), but the more I use it, the more I realize that Canonical really did their homework on how to help end users more efficiently interact with their computers. Change is hard – period. For many, the idea of change is such a painful notion they wind up missing out on some incredible advancements. Unity is one such advancement.\"\n\nOn 23 April 2012 Shuttleworth announced that Ubuntu 12.10 would be named \"Quantal Quetzal\". As this will be the first of a series of three releases before the next LTS release, Shuttleworth indicated that it will include a refreshed look, with work to be done on typography and iconography. The release takes its name from the quetzal, a species of Central American birds. Ubuntu 12.10 was released on schedule on 18 October 2012 and is Canonical's seventeenth release of the operating system.\n\nRyan Paul, writing for Ars Technica, said in April 2012 when the name was announced \"A Quetzal is a colorful bird that is common to Central America. The most well-known variety, the resplendent quetzal, is known for its beauty. The name is a good fit for Ubuntu, which aims to soar in the cloud, offer visual appeal without compromising function, and avoid smacking into closed windows.\"\n\nThe Ubuntu Developer Summit held in May 2012 set the priorities for this release. They are forecast to include an improved boot up sequence and log-in screen, dropping Unity 2D in favor of lower hardware requirements for Unity 3D, wrap around dialogs and toolbars for the HUD and a \"vanilla\" version of Gnome-Shell as an option. The release would likely include GNOME 3.6, Python 3 and the 3.5 Linux kernel. It would ship with Python 3 in the image, but with Python 2 available in the repositories, via the \"python\" package.\nThe kernel will have the PAE switched on by default.\n\nIn July 2012, development versions of Ubuntu 12.10 received a new combined user, session and system menu. This release also included Ubuntu Web Apps, a means of running web applications directly from the desktop, without having to open a browser. It would use Nautilus 3.4 as its file manager, in place of the 3.5 and newer versions, to retain features deleted from later versions.\n\nIn September 2012, Canonical's Kate Stewart announced that the Ubuntu 12.10 image would not fit on a compact disc, saying \"There is no longer a traditional CD sized image, DVD or alternate image, but rather a single 800MB Ubuntu image that can be used from USB or DVD.\" However, a third-party project has created a version of Ubuntu 12.10 that fits on a CD. It uses LZMA2 compression instead of the DEFLATE compression used on the official Ubuntu DVD image.\n\nAlso in late September 2012, it was announced that the version of Unity to be shipped with Ubuntu 12.10 would by default include searches of Amazon.com for searched terms. This move caused immediate controversy among Ubuntu users, particularly with regard to privacy issues, and caused Mark Shuttleworth to issue a statement indicating that this feature is not adware and labelled many of the objections \"FUD\" (Fear, uncertainty and doubt). Shuttleworth stated \"What we have in 12.10 isn't the full experience, so those who leap to judgement are at maximum risk of having to eat their words later. Chill out. If the first cut doesn't work for you, remove it, or just search the specific scope you want (there are hotkeys for all the local scopes).\" Regardless, users filed a Launchpad bug report on the feature requesting that it be made a separate lens and not included with general desktop searches for files, directories and applications. The degree of community push-back on the issue resulted in plans by the developers to make the dash and where it searches user-configurable via a GUI-setting dialogue. Despite concerns that the setting dialogue would not make the final version of Ubuntu 12.10, it was completed and is present in the final version of 12.10.\n\nIn the week prior to the stable release of Ubuntu 12.10 data-privacy advocate Luís de Sousa indicated that the inclusion of the shopping lens, installed without explicit permission of the user, violates European Directive 95/46/EC on data privacy. That directive requires that the \"data subject has unambiguously given his consent\" in situations where personal identifying information is sent.\n\nIn reviewing Ubuntu 12.10 at the end of October 2012 for DistroWatch, Jesse Smith raised concerns about the Amazon shopping lens, saying, \"it has raised a number of privacy concerns in the community and, looking over Ubuntu's legal notice about privacy does not provide any reassurance. The notice informs us Canonical reserves the right to share our keystrokes, search terms and IP address with a number of third parties, including Facebook, Twitter, Amazon and the BBC. This feature is enabled by default, but can be turned off through the distribution's settings panel.\" He also found that the dash provided very slow performance and that the release was \"practically unusable in the VirtualBox environment\". He summed up his experiences, \"After a day and a half of using Ubuntu 12.10 it was an internal struggle not to wipe my hard drive and just find another distribution to review. During the first twenty-four hours Ubuntu spied on me, provided performance which was distinctly sub par, the interface regularly popped up errors (sometimes so frequently the first pop-up wouldn't have faded out of view before the next one appeared), the update notification didn't work and it wasn't possible to turn off accessibility features through the graphical interface. Adding insult to injury, the Unity dash kept locking up or losing focus while I was trying to use it and the operating system crashed more times than not while trying to shutdown or logout. Switching away from Unity to GNOME Fallback helped the performance issues I had experienced with the Dash, but it didn't remove the annoying pop-up errors and performance (while usable) still wasn't as good as I would expect. And what really makes me scratch my head is Ubuntu 12.04 worked really well on this same hardware.\"\n\nIn early November, the Electronic Frontier Foundation made a statement on the shopping lens issue, \"Technically, when you search for something in Dash, your computer makes a secure HTTPS connection to productsearch.ubuntu.com, sending along your search query and your IP address. If it returns Amazon products to display, your computer then insecurely loads the product images from Amazon's server over HTTP. This means that a passive eavesdropper, such as someone sharing a wireless network with you, will be able to get a good idea of what you're searching for on your own computer based on Amazon product images. It's a major privacy problem if you can't find things on your own computer without broadcasting what you're looking for to the world.\"\n\nWriting about Ubuntu 12.10 in a December 2012 review, Jim Lynch addressed the Amazon controversy:\n\nHe concluded by saying, \"Overall, Ubuntu 12.10 is a decent upgrade for current Ubuntu users. However, the inclusion of the Amazon icon on the launcher, and the discontinuation of Unity 2D might irritate some people.\"\n\nSupport for Ubuntu 12.10 Quantal Quetzal officially ended on 16 May 2014.\n\nOn 17 October 2012, Shuttleworth announced that Ubuntu 13.04 would be named \"Raring Ringtail\" and said about this release \"[In the next six months] we want to have the phone, tablet and TV all lined up. So I think it's time to look at the core of Ubuntu and review it through a mobile lens: let's measure our core platform by mobile metrics, things like battery life, number of running processes, memory footprint, and polish the rough edges that we find when we do that.\"\n\nThe Wubi installer was dropped as of 13.04, due to its incompatibility with Windows 8 and general lack of support and development. Previously, on 29 October 2012 at the Ubuntu Developer Summit Registration, there had been a discussion of redesigning Wubi for Ubuntu 13.04.\n\nUbuntu 13.04 was released on schedule on 25 April 2013.\n\nIn reviewing Ubuntu 13.04 Jim Lynch from Desktop Linux Reviews said, \"I found Ubuntu 13.04 to be a slightly disappointing upgrade. While there are definitely some enhancements in this release, there's also nothing very special about it ... Alas, there's nothing in Ubuntu 13.04 that makes me want to consider it for use as my daily distro. Don't misunderstand me, there's nothing overtly wrong with Ubuntu 13.04 either. It installed and performed very well for me. Unity 7 also has some helpful and attractive updates that Ubuntu users will enjoy, and there are other things in this release that help improve the overall Ubuntu experience...I suspect it is simply because Ubuntu has settled into a comfortable middle age, it works and it works very well for what it does.\"\n\nSupport for Ubuntu 13.04 officially ended on 27 January 2014.\n\nUbuntu 13.10 is named \"Saucy Salamander\". It was released on schedule on 17 October 2013.\n\nConsideration was given to changing the default browser from Mozilla Firefox to Chromium, but problems with timely updates to Ubuntu's Chromium package caused developers to retain Firefox for this release.\n\nUbuntu 13.10 was intended to be the first Ubuntu release to replace the aging X11 with the Mir display server, with X11 programs to have operated through the XMir compatibility layer. However, after the development of XMir ran into \"outstanding technical difficulties\" for multiple monitors, Canonical decided to postpone the default use of Mir in Ubuntu. Mir will still be released as the default display server for Ubuntu Touch 13.10.\n\nRyan Paul of Ars Technica wrote that although 13.10 brings useful enhancements, it is \"a relatively thin update\". He also said \"the new Dash concept is intriguing, but its usefulness is a bit limited\"; and even though he thinks that universal Web search is potentially useful, he's somewhat uncomfortable with how Canonical joins it with local system searches.\n\nIn a review of Ubuntu 13.10 Joey Sneddon of \"OMG Ubuntu\" criticized the new Smart Scopes feature, saying, \"it's less of a help and more of a hindrance. With so many web services offering results for a search term – however innocuous it might be – the Dash ends up resembling a wall painted in unintelligible, irrelevant mess.\" Sneddon noted that internet search engines turn in more useful and better organized results and recommended selectively disabling individual scopes to reduce the noise factor.\n\nJim Lynch of \"Linux Desktop Reviews\" described the release as \"boring\" and noted, \"alas, Ubuntu 13.10 follows in the footsteps of Ubuntu 13.04. The big new desktop feature is Smart Scopes ... Beyond that there's not a whole lot that is interesting or exciting to talk about. It turns out that Saucy Salamander is one truly dull amphibian. Canonical really should rename this release to 'Snoozing Salamander' instead.\" Lynch described the Smart Scopes, \"this is a very useful function, and it can save you a lot of time when looking for information. I understand that some people will regard this as a privacy violation, no problem. There's an easy way to disable Smart Scopes.\"\n\nMaria Korolov writing for Network World in December 2013 said of the release, \"there is a benefit to be had in being able to search for files you own on both local drives and in cloud services such as Google Drive and Flickr. That's the idea behind Unity Smart Scopes ... The result is a cluttered mess. The first thing many users will probably do after installing Ubuntu 13.10 is to get rid of most of these results ... mixing generic Web results in with your own files is just confusing.\"\n\nIn its year-end \"Readers Choice Awards\" Linux Journal readers voted Ubuntu as Best Linux Distribution and Best Desktop Distribution for 2013.\n\nSupport for Ubuntu 13.10 ended on 17 July 2014.\n\nMark Shuttleworth announced on 31 October 2011 that by Ubuntu 14.04, Ubuntu would support smartphones, tablets, TVs and smart screens.\n\nOn 18 October 2013, it was announced that Ubuntu 14.04 would be dubbed \"Trusty Tahr\".\n\nThis version was released on 17 April 2014, and is the 20th release of Ubuntu. Shuttleworth indicated that the focus in this development cycle would be a release characterized by \"performance, refinement, maintainability, technical debt\" and encouraged the developers to make \"conservative choices\". Technical debt refers to catching up and refining supporting work for earlier changes. The development cycle for this release focused on the tablet interface, specifically for the Nexus 7 and Nexus 10 tablets. There were few changes to the desktop as 14.04 used the existing mature Unity 7 interface. Ubuntu 14.04 included the ability to turn off the global menu system and used locally integrated menus instead for individual applications. Other features were the retention of Xorg and not Mir or XMir, a Unity 8 developers' preview, new mobile applications, a redesigned USB Start-Up Disk Creator tool, a new forked version of the GNOME Control Center, called the Unity Control Center and default SSD TRIM support. GNOME 3.10 is installed by default.\n\nPoint releases included 14.04.1 on 24 July 2014, 14.04.2 on 19 February 2015, 14.04.3 on 6 August 2015, 14.04.4 on 18 February 2016 and 14.04.5 on 4 August 2016. The release initially included Linux kernel 3.13, but this was updated to 4.2 with the point release of 14.04.4 on 18 February 2016. The final point release, 14.04.5, provided the latest Linux kernel and graphics stacks from Ubuntu 16.04 LTS. Point release 14.04.5 was the final one for 14.04 LTS.\n\nJoey Sneddon of OMG Ubuntu noted that recent Ubuntu releases have received lower and lower amounts of mainstream press coverage and termed it an \"established product that has, by and large, remained a niche interest\".\n\nIn reviewing Ubuntu 14.04 LTS in April 2014, Jim Lynch concluded: \"Ubuntu 14.04 seems to be all about refining the Ubuntu desktop. While there are not a lot of amazing new features in this release, there are quite a few very useful and needed tweaks that add up to a much better desktop experience. Canonical's designers seem to be listening to Ubuntu users again, and they seem willing to make the changes necessary to give the users what they want. That may be the single most important thing about Ubuntu 14.04. It could be an indication of a sea change in Canonical's attitude toward Ubuntu users.\"\n\nJack Wallin writing for Tech Republic termed Ubuntu 14.04 LTS, \"as polished a distribution as you'll find. It's cleaner, performs better, and is all around improved. Some users may say that this is the most boring release Canonical has unleashed in years, but I believe it to be one of the finest.\"\n\nTerry Relph-Knight of ZDNet said, \"although there are no amazing 'must-have' new features in Ubuntu 14.04, it is worth upgrading just to get the latest LTS release with a more recent kernel and default applications.\"\n\nScott Gilbertson of Ars Technica stated, \"Ubuntu is one of the most polished desktops around, certainly the most polished in the Linux world, but in many ways that polish is increasingly skin deep at the expense of some larger usability issues which continue to go unaddressed release after release.\"\n\nNormal LTS support is set to continue until 30 April 2019, after which Extended Security Maintenance will be available to Ubuntu Advantage customers and as a separate commercial purchase, as was the case previously with 12.04.\n\nOn 23 April 2014 Shuttleworth announced that Ubuntu 14.10 would carry the name \"Utopic Unicorn\". Version 14.10 was released on 23 October, having only minor updates to the kernel, Unity Desktop, and included packages such as LibreOffice and Mozilla Firefox and Thunderbird. The kernel was updated to 3.16 for hardware support (e.g. graphics) and has for security, full kernel address space layout randomization applied to the kernel and its modules, plus the closure of a number of information leaks in /proc.\n\nThis version is the 21st release. Ubuntu 14.10 was officially characterized as a release that addressed \"bug fixes and incremental quality improvements\" and so it incorporated very few new features.\n\nJoey Sneddon of OMG Ubuntu wrote in reviewing this release, \"Ubuntu 14.10, codenamed \"Utopic Unicorn\", is saddled with a modest changelog, composed largely of bug fixes, stability improvements and key software updates. All worthy, but falls a little way short of the \"fresh ideas and new art\" that should \"raise the roof\" – quotes from Mark Shuttleworth's \"U” name announcement...For the release taking place in the week of Ubuntu's 10th anniversary, this may all read like a bit of an anticlimax. No headline user features, no visual changes (bar a few new icons for the sidebar of Nautilus) – there's not even a new default wallpaper to look at...But on the flip side it's perhaps the most fitting release; the one that shows just how far Ubuntu has come in the past few years. Mature, dependable and sure in its own (Ambiance-themed) skin, buggy feature churn has given way to a sustained era of assured stability...Ubuntu 14.10 is a rock-solid, hearty and dependable release. Perhaps more here than ever before. There's no getting away from the fact that it's an uninspiring update on paper, and is far from being anything approaching essential.\"\n\nMichael Larabel of Phoronix wrote, \"At the end of the day simple end-users won't see much of a difference over Ubuntu 14.04 LTS, which is a bit sad given that this is the tenth anniversary release of Ubuntu Linux. For everyday Linux desktop users the many upgraded packages are great but there isn't too much more to celebrate about today on the desktop front.\"\n\nScott Gilbertson, writing for The Register, explained, \"I've been covering Ubuntu for seven of the release's 10 years and 14.10 is the first time I've had to dig deep into the release notes just to find something new to test...If you needed further proof that Canonical is currently solely focused on bringing its Unity 8 interface to mobile devices, 14.10 is the best evidence yet...Almost nothing Canonical develops has changed in this release – there isn't even a new desktop wallpaper. There are some updates to be sure, but they don't hail from Canonical...The lack of updates isn't unexpected, in fact that's been the plan all along...Desktop Ubuntu is currently in a kind of suspended animation, waiting on Unity 8 and Mir to be ready for its coming metamorphosis. The short story is that it makes no sense for Canonical to keep refining Unity 7 when it will soon be retired.\"\n\nOn 20 October 2014 Shuttleworth announced that Ubuntu 15.04 would be named \"Vivid Vervet\". It was released on 23 April 2015. This was the 22nd Ubuntu release.\n\nUbuntu 15.04 used systemd instead of Upstart by default. This release also featured locally integrated menus by default, replacing the previous default global menus.\n\nSilviu Stahie, writing for Softpedia, said about this release while it was in beta, \"Ubuntu 15.04 is not an exciting release, but that it's only a surface impression. The truth is that it's an important upgrade because some very important changes have been made, including the adoption of systemd. Users will notice that not too many visual changes have been implemented in Ubuntu 15.04, but that was to be expected. The team is transitioning to a new Unity version that is still not ready for general use, so it's easy to understand why Ubuntu 15.04 is not all that different from Ubuntu 14.10.\"\n\nThis release included modest improvements in Intel Haswell graphics performance and bigger improvements for AMD Radeon graphics cards using the open-source Radeon R600 and RadeonSI Gallium3D drivers.\n\nIn reviewing this release, Joey Sneddon, of \"OMG Ubuntu\", said \"Ubuntu 15.04 is yet another solid entry in the distribution's long release history. A dependable desktop operating system suited for end users but with plenty of convenient extras to woo developers with. Though the Unity 7 desktop is largely mothballed as work progresses on the new converged experience with Unity 8, the modest refinements received here buff the experience. Unity in Ubuntu 15.04 shines brighter, a glowing example of a desktop that 'just works' for users.\".\n\nJesse Smith of DistroWatch wrote, \"One of the changes I was interested in exploring was Ubuntu's switch from the Upstart init software to systemd. In this regard I was pleasantly surprised. I find most distributions, when they initially make the switch to systemd, introduce bugs or, at the very least, break backward compatibility. Sometimes service managers stop working properly and network device names usually change. Even if everything works as it should, the administrator needs to adjust to systemd's approach to logging and adopt a different method of managing services. Ubuntu has taken an approach I like with regards to adopting systemd.\" He concluded, \"on the surface, Ubuntu 15.04 does not bring many changes. There are a few cosmetic adjustments, but nothing major that desktop users are likely to notice. Most of the interesting work appears to be going on behind the scenes... Ubuntu 15.04 feels very stable and easy to configure. This is an operating system that is virtually effortless to set up and run and I feel the Unity 7 desktop does a nice job of providing lots of features while staying out of the way... All in all, I like what Canonical has done with Ubuntu 15.04. This feels like a small, incremental evolution for Ubuntu and Unity. The init switch, which has disrupted the users of several other distributions, goes largely unnoticed in Ubuntu and I think that is worthy of praise.\"\n\nShuttleworth announced on 4 May 2015 that Ubuntu 15.10 would be called \"Wily Werewolf\". He initially expressed hope that the release would include the Mir display server, but it was released on 22 October 2015 without Mir. It was the 23rd release of Ubuntu.\n\nUbuntu 15.10 eliminated the disappearing window edge scrollbars in favour of the upstream GNOME scrollbars, a move designed to save developer time in creating patches and updates.\n\nIn reviewing the release, Chris Jones wrote, \"Ubuntu 15.10 as an operating system for Review is pretty lackluster. There's nothing new as such and there's nothing we can really say that is going to change your opinion from its predecessor, 15.04. Therefore, we recommend you to upgrade either out of habit and according to your regular upgrade schedule rather than out of a specific necessity for a specific feature of this release. Because there is really nothing that could possibly differentiate it from the older, yet still very stable 15.04 release. But if you're going to stick with 15.04 for a little longer, we do recommend that you look at upgrading the kernel to the latest 4.2 branch. It is worth it. If you really want a reason to upgrade? Linux kernel 4.2 would be our sole reason for taking Ubuntu 15.10 into consideration.\"\n\nJoey Sneddon of \"OMG Ubuntu\" noted, \"For a release named after a terrifying mythological creature Ubuntu 15.10 is surprisingly tame. There are no dramatic transformations, no bone popping or shirt ripping and certainly no hair sprouting under the milky eye of full moon. In fact, a new wallpaper and change in scrollbar appearance is about as shapeshift-y as this werewolf gets.\"\n\nSteven J. Vaughan-Nichols of ZDNet praised the release for its integration of cloud services, such as the new Ubuntu OpenStack cloud deployment and management tool, OpenStack Autopilot as well as its server tools. Ubuntu's machine container hypervisor, LXD, included by default in 15.10, was singled out. Vaughan-Nichols concluded, \"with these advances, chances are you're more likely to use Ubuntu, hidden behind the scenes, on clouds and servers.\"\n\nA \"Hectic Geek\" review noted problems with X.Org Server crashes and concluded \"If you use Ubuntu 14.04 LTS and if it's working out for you, then there really is no need to switch to a non-LTS release, especially to the 15.10.\"\n\nA review on \"Dedoimedo\" identified problems with Samba, Bluetooth, desktop searching, battery life and the smartphone interface and found the release inconsistent, saying, \"unpredictability is horrible. Give me a good experience, or give me a bad experience, but please try not to seesaw between them erratically. Continuous, steady change in behavior, any which way.\" The review concluded, \"it underperforms compared to some of its siblings and ancestors. Not the best, definitely not worth a perma upgrade, but you might find it more palatable to your hardware and use cases. Overall, though Wily isn't the best of distros. It sure gave me the willies. 7/10.\"\n\nShuttleworth announced on 21 October 2015 that Ubuntu 16.04 LTS would be called \"Xenial Xerus\". It was released on 21 April 2016.\n\nThe default desktop environment continues to be Unity 7, with an option for Unity 8. In May 2015, Shuttleworth indicated that Ubuntu 16.04 LTS would include Unity 8 and Mir, but that users have a choice of that or Unity 7 and X.org. He said, \"Unity 8 will be an option for 16.04 and we'll let the community decide the default for 16.04.\"\n\nThe release adds support for Ceph and ZFS filesystems, the LXD hypervisor (using seccomp) for OpenStack, and Snappy packages will be supported. It will use systemd instead of Upstart as its init system. This release will replace the Ubuntu Software Centre with GNOME Software and eliminate Empathy and Brasero from the ISO file. Reviewer Jack Wallen said, \"The truth of the matter is, the Ubuntu Software Center has been a horrible tool for a very long time. Making this move will greatly improve the Ubuntu experience for every user.\"\n\nThis release has online dash search results disabled by default in Unity 7. \"None of your search terms will leave your computer\", stated Ubuntu desktop manager Will Cooke. Reviewer Jack Wallen said about this, \"I've never considered the inclusion of online search results to be spyware. In fact, I have always considered the online results to be an efficient means of searching for products through Amazon (etc.). That being said, with the release of 16.04, this feature is disabled.\"\n\nUbuntu 16.04 LTS does not support the AMD Catalyst (fglrx) driver for AMD/ATI graphics cards and instead recommends the free software radeon and amdgpu alternatives. These may not provide optimal graphics performance, however.\nAMDGPU-PRO is available for Ubuntu 16.04\n\nThe first point release, 16.04.1, was released on 21 July 2016. Release of Ubuntu 16.04.2 was delayed a number of times, but it was eventually released on 17 February 2017. Ubuntu 16.04.3 was released on 3 August 2017. Ubuntu 16.04.4 was delayed from 15 February 2018 and released on 1 March 2018, providing the latest Linux kernel, 4.13, from Ubuntu 17.10. Ubuntu 16.04.5 was released on 2 August 2018.\n\nMark Shuttleworth announced on 21 April 2016 that Ubuntu 16.10 would be called \"Yakkety Yak\". It was released on 13 October 2016.\n\nThis release features a maintenance version of Unity 7, but offers Unity 8 packages included in the ISO, so that users can test them. Other improvements include a new version of Ubuntu Software that supports faster loading, better support for installing command-line-only non-GUI applications, support for installing fonts and multimedia codecs and introduction of paid applications. It is based on Linux kernel version 4.8.\n\nThis version of Ubuntu introduced only minor incremental changes. These included LibreOffice 5.2, GTK3 version by default, the Update Manager shows changelog entries for Personal Package Archives (PPAs) as well as repository software, GNOME applications updated to version 3.20, with some using version 3.22. Also systemd now handles user sessions as well as the previously implemented system sessions.\n\nJoey Sneddon of OMG Ubuntu said, \"Ubuntu 16.10 is not a big update over Ubuntu 16.04 LTS, released back in April. If you were hoping it'd be a compelling or must-have upgrade you'll be sadly disappointed.\" He did find some improvements, \"The Ubuntu Software app is also significantly faster in use. This solves a real pet peeve of mine on the incumbent LTS desktop. Thankfully, Yakkety makes it quicker to find, browser, search and install applications,\" but concluded, \"Ubuntu 16.10 is not a must-have upgrade—not for most people.\"\n\nMarius Nestor of Softpedia noted, \"Ubuntu 16.10 is not an exciting release for fans of the open source operating system. Probably the most important feature of Yakkety Yak is Linux kernel 4.8, which brings support for the latest hardware, but other than that, you'll get some updated components that are mostly based on the old GNOME 3.20 Stack.\"\n\nWriting in \"Makeuseof\", Bertel King, Jr. said, \"If you're feeling underwhelmed, you probably remember the Ubuntu of yesteryear. Back in the days of 8.10, 9.04, and 10.04 each release brought forth a new theme or ambitious feature. Unity first appeared in 10.10 as a netbook interface before replacing the regular desktop in 11.04. By comparison, modern Ubuntu updates feel relatively stagnant. You would be forgiven for not being able to distinguish between 12.04 and 16.10.\"\n\nOn 17 October 2016, Mark Shuttleworth announced that the codename of Ubuntu 17.04, released on 13 April 2017, would be \"Zesty Zapus\".\n\nThis release dropped support for the 32-bit PowerPC architecture, following the same move by the upstream Debian project. Other changes include the default DNS resolver now being \"systemd-resolved\", Linux kernel 4.10, and support for printers which allow printing without printer-specific drivers.\n\nJoey Sneddon of \"OMG Ubuntu\" said of this release, \"this is no normal release of Ubuntu. It's potentially the last version of the distribution that will come with the Unity 7 desktop by default. That's not a certainty, of course, but we know that Ubuntu will switch to GNOME for Ubuntu 18.04 LTS next year. It's reasonable to expect developers to want to kick a few tyres on that switch ahead of time, in the next interim release. A bittersweet release then, Ubuntu 17.04 sees the distro reach the end of the alphabet in codenames, and the end of an era in everything else. Sadly there's not an awful lot to say. Unity is, by and large, the same as it is in the 16.04 LTS...Ubuntu 17.04 is an iterative update with modest appeal. While there is little compelling reason for anyone running Ubuntu 16.04 LTS to upgrade (especially for those who opt receive the newer hardware enablement stack) it's not an irrelevant release. Ubuntu 16.10 users will want to upgrade to Ubuntu 17.04 for the general around improvements, access to newer apps, and because the truncated support period of these short term releases necessitates it.\"\n\nMaruis Nestor of Softpedia called it, \"a powerful release, both inside and outside\" and noted, \"the default desktop environment remains Unity 7, so your beloved Ubuntu desktop environment is not going anyway at the moment. It will also be available in the upcoming Ubuntu 17.10 release, whose development will start next month. After that, starting with Ubuntu 18.04 LTS, the GNOME desktop will be used by default.\"\n\nThe name of this release, Artful Aardvark, was announced via Launchpad on 21 April 2017, instead of on Shuttleworth's blog as has been the case in the past. It was released on 19 October 2017.\n\nThis was the first release of Ubuntu to use the GNOME Shell interface, and replaced X11 with the Wayland display server. In May 2017, Ken VanDine, a Canonical Software Engineer on the Ubuntu desktop team tasked with the switch to GNOME, confirmed that the intention is to ship the most current version of GNOME, with very few changes from a stock installation.\n\nThis release also dropped support for 32-bit desktop images but a 32-bit system can still be installed from the minimal ISO. \n\nWriter J.A. Watson of ZDNet said, \"I have not been much of an Ubuntu fan for a long time now, but this release includes a lot of significant changes, many of which might address some of my most serious objections about Ubuntu. So I think I should take a closer look at it than I normally do.\" He noted on printer configuration, \"I got a notice that our wireless printer had been successfully configured. I hadn't even thought about trying to set up a printer yet, so that was a very nice surprise - and a good thing to point out to those who are still going around spouting 5+ years out of date information about how difficult it is to use printers with Linux.\"\n\nReviewer Scott Gilbertson of Ars Technica wrote, \"Ubuntu 17.10 is a huge departure for Ubuntu, but one that sees the distro seemingly getting its footing back. The transition to GNOME, while not without its pitfalls for some users, is surprisingly smooth. Unity did have some features you won't find in GNOME, but Canonical has done a good job of making things familiar, if not identical. \nMore important than individual features in 17.10, this release sees Ubuntu starting over to some degree. The long development process of Unity 8 was threatening to turn it into Godot, but now Ubuntu is free of Unity 8. Its users no longer have to wait for anything.\"\n\nThe first point release, 17.10.1, was released on 12 January 2018. It fixed a problem that caused the firmware of some Lenovo computers to not boot.\n\nUbuntu 18.04 LTS Bionic Beaver is a long term support version that was released on and Ubuntu 18.04.1 LTS was released three months later . Plans to include a new theme created by the Ubuntu community, were announced on 5 February 2018. However, as the development of the theme was unfinished and buggy as of 13 March 2018, Ubuntu 18.04 LTS did not include a new theme, and instead retained the Ambiance theme from 2010 as its default theme. Still, the new theme will become available as a Snap package.\n\nUbuntu 18.04 LTS introduced new features such as colour emoji, a new To-Do app preinstalled in the default installation, and added the option of a \"Minimal Install\" to the Ubuntu 18.04 LTS installer, which only installs a web browser and system tools. Ubuntu 18.04 LTS's default display server was returned to Xorg for more stability and reliability, however, Wayland is still included as part of the default install.\n\nThis release employed Linux kernel version 4.15, which incorporated a CPU controller for the cgroup v2 interface, AMD secure memory encryption support and improved SATA Link Power Management.\n\nIn reviewing Ubuntu 18.04 LTS, Michael Larabel of Phoronix wrote, \"Ubuntu 18.04 is mostly an incremental upgrade over Ubuntu 17.10 with updated packages, the switch back to X.Org session by default rather than Wayland, continued presence of Snaps, and a variety of minor user-interface updates. It's really not a big deal going from 17.10 to 18.04 besides the LTS extended support nature, but it is quite a change if upgrading from Ubuntu 16.04 LTS. For that upgrade you now have the GCC 7 compiler, Unity 7 to GNOME Shell by default, and a wealth of other package updates.\"\n\nIn reviewing the Ubuntu 18.04 LTS server version, Michael Larabel of Phoronix, indicated that the newly developed text-based installer is an improvement over previous installers.\n\nOn 15 November 2018, Mark Shuttleworth announced that Ubuntu 18.04 LTS would be supported for ten years, until April 2028.\n\nOn 8 May 2018, Mark Shuttleworth announced that the codename of Ubuntu 18.10, which was released on 18 October 2018, would be \"Cosmic Cuttlefish\". \n\nThe Ubuntu 18.10 installation includes a new theme, named \"Yaru\" and the new icon theme, \"Suru\".\n\nInstallation speeds are faster due to a lossless compression algorithm known as Zstandard. Startup speeds of pre-installed Snap applications were also improved.\n\nIn a review of 18.10 Joey Sneddon of \"OMG Ubuntu\" wrote, \"Ubuntu 18.10 'Cosmic Cuttlefish' is a modest update compared to 18.04. The vast majority of notable improvements are tucked away out of sight, 'under the hood' ... Upstream GNOME Shell developers spent the past six months trying to lower GNOME Shell's memory usage and improve the overall performance of the shell, its animations, display manager, and parts of the GNOME extension framework (specifically Gjs), as we touched on in our recap of the new features in GNOME 3.30. Invisible and abstract though these changes are, they're appreciable. So much so that, if I had to describe this release in just one word it'd be 'peppy'. That's testament to the power of collaboration; with upstream devs and Canonical's engineering team working together.\"\n\nMichael Larabel of Phoronix wrote, \"Overall, Ubuntu 18.10 \"Cosmic Cuttlefish\" is quite a modest six-month upgrade for being the first past the Ubuntu 18.04 cycle. Exciting me the most, of course, is simply the package upgrades with riding Linux 4.18 + Mesa 18.2 for a much better Linux gaming experience and having moved on now to GCC8 ... What didn't get achieved for the Ubuntu 18.10 cycle is the long-awaited data viewer to the Ubuntu software/hardware survey introduced in 18.04 LTS... As of writing there's still no public means of being able to view the statistics on these opt-in Ubuntu survey installations. Additionally, the plans for better Android phone integration with the Ubuntu 18.10 desktop by means of bundling GS Connect also didn't happen as planned for the Ubuntu 18.10 cycle.\"\n\nUbuntu 19.04, codenamed \"Disco Dingo\", is a future release which is scheduled for 18 April 2019.\n\nAfter each version of Ubuntu has reached its end-of-life time, its repositories are removed from the main Ubuntu servers and consequently the mirrors. Older versions of Ubuntu repositories and releases can be found on the old Ubuntu releases website.\n\n"}
