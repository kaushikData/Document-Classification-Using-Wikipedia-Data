{"id": "46648870", "url": "https://en.wikipedia.org/wiki?curid=46648870", "title": "2014 OS393", "text": "2014 OS393\n\n, unofficially designated , and , is a trans-Neptunian object and possibly a classical Kuiper belt object, located in the outermost region of the Solar System. It was first observed by astronomers using the Hubble Space Telescope on 30 July 2014. Until 2015, when the object was selected, it was a potential flyby target for the \"New Horizons\" probe. Estimated to be approximately in diameter, the object has a poorly determined orbit as it had been observed for only a few months.\n\n was discovered with the help of the Hubble Space Telescope because the object has a magnitude of 26.3, which is too faint to be observed by ground-based telescopes. Preliminary observations by the HST searching for KBO flyby targets for the \"New Horizons\" probe started in June 2014, and more intensive observations continued in July and August. was first discovered in observations on July 30, 2014, but it was designated e31007AI at the time, nicknamed e3 for short. Its existence as a potential target of the \"New Horizons\" probe was revealed by NASA in October 2014 and designated PT2, but the official name was not assigned by the Minor Planet Center (MPC) until March 2015 after better orbit information was available.\n\n is a trans-Neptunian object and likely a non-resonant classical Kuiper belt object, also known as \"cubewano\". It orbits the Sun at a distance of 42.5–45.4 AU once every 291 years and 3 months (106,395 days; semi-major axis of 43.9 AU). Its orbit has an eccentricity of 0.03 and an inclination of 4° with respect to the ecliptic. As this object has not been observed since October 2014, its orbit remains poorly determined still containing a high uncertainty.\n\nThe body's observation arc begins with a precovery taken on 25 June 2014, by the New Horizons KBO Search team using the Subaru Telescope at Mauna Kea Observatory on Hawaii.\n\nAfter the \"New Horizons\" probe completed its flyby of Pluto, the probe is to be maneuvered to a flyby of at least one Kuiper belt object. Several potential targets were under consideration for the first such flyby. has an estimated mean-diameter between 30 and 55 kilometers, depending on the body's assumed albedo. The potential encounter in 2018–2019 would have been at a distance of 43–44 AU from the Sun.\n\nThe potential targets for the \"New Horizons\" probe are PT1 and PT3, the KBOs and , and the probe has sufficient fuel to maneuver to either PT1 or PT3. Potential target PT2 is no longer under consideration as a potential target, and was eliminated as a target before the fall of 2014.\n\nOn 28 August 2015, the \"New Horizons\" team announced the selection of as the next flyby target.\n\nThis minor planet has not been numbered by the Minor Planet Center and remains unnamed.\n\n"}
{"id": "16856735", "url": "https://en.wikipedia.org/wiki?curid=16856735", "title": "A Blueprint for Survival", "text": "A Blueprint for Survival\n\nA Blueprint for Survival was an influential environmentalist text that drew attention to the urgency and magnitude of environmental problems.\n\nFirst published as a special edition of \"The Ecologist\" in January 1972, it was later published in book form and went on to sell over 750,000 copies.\n\nThe \"Blueprint\" was signed by over thirty of the leading scientists of the day—including Sir Julian Huxley, Sir Frank Fraser Darling, Sir Peter Medawar, E. J. Mishan and Sir Peter Scott—but was written by Edward Goldsmith and Robert Allen (with contributions from John Davoll and Sam Lawrence of the Conservation Society, and Michael Allaby) who argued for a radically restructured society in order to prevent what the authors referred to as \"“the breakdown of society and the irreversible disruption of the life-support systems on this planet”\".\n\nIt recommended that people live in small, decentralised and largely de-industrialised communities. Some of the reasons given for this were that:\n\n\nThe authors used tribal societies as their model which, it was claimed, were characterised by their small, human-scale communities, low-impact technologies, successful population controls, sustainable resource management, holistic and ecologically integrated worldviews, and a high degree of social cohesion, physical health, psychological well-being and spiritual fulfilment of their members.\n\n\n"}
{"id": "1589482", "url": "https://en.wikipedia.org/wiki?curid=1589482", "title": "Abu-Mahmud Khojandi", "text": "Abu-Mahmud Khojandi\n\nAbu Mahmud Hamid ibn Khidr Khojandi (known as Abu Mahmood Khojandi, Alkhujandi or al-Khujandi, Persian: ابومحمود خجندی, c. 940 - 1000) was a Central Asian astronomer and mathematician who lived in the late 10th century and helped build an observatory, near the city of Ray (near today's Tehran), in Iran. He was born in Khujand; a bronze bust of the astronomer is present in a park in modern-day Khujand, now part of Tajikistan.\n\nKhujandi worked under the patronage of the Buwayhid Amirs at the observatory near Ray, Iran, where he is known to have constructed the first huge mural sextant in 994 AD, intended to determine the Earth's axial tilt (\"obliquity of the ecliptic\") to high precision.\n\nHe determined the axial tilt to be 23°32'19\" for the year 994 AD. He noted that measurements by earlier astronomers had found higher values (Indians: 24°; Ptolemy 23° 51') and thus discovered that the axial tilt is not constant but is in fact (currently) decreasing. His measurement of the axial tilt was however about 2 minutes too small, probably due to his heavy instrument settling over the course of the observations.\n\nKhojandi stated a special case of Fermat's last theorem for n = 3, but his attempted proof of the theorem was incorrect. The spherical law of sines may have also been discovered by Khujandi, but it is uncertain whether he discovered it first, or whether Abu Nasr Mansur, Abul Wafa or Nasir al-Din al-Tusi discovered it first.\n\n"}
{"id": "52695411", "url": "https://en.wikipedia.org/wiki?curid=52695411", "title": "Aliashraf Abdulhuseyn oglu Alizade", "text": "Aliashraf Abdulhuseyn oglu Alizade\n\nAcademician Aliashraf Abdulhuseyn oghlu Alizade (1911–1985) was an Azerbaijani geologist. He was a full member and one of the founders of the Azerbaijan National Academy of Sciences (1945), and an Honorary Oilman of the USSR (1971). He was also a State Award laureate for the discovery and exploitation of new oil fields (1943) and for the preparation of the small electric perforator (1946).\n"}
{"id": "52216003", "url": "https://en.wikipedia.org/wiki?curid=52216003", "title": "Biometrology", "text": "Biometrology\n\nBiometrology refers to measurement and data activities that provide quantitative characterization of biology. Biometrology advances laboratory techniques regarding scalable bioproducts or services.\n\n\n\n\n"}
{"id": "1890639", "url": "https://en.wikipedia.org/wiki?curid=1890639", "title": "Butterfly net", "text": "Butterfly net\n\nA butterfly net (sometimes called an aerial insect net) is one of several kinds of nets used to collect insects. The entire bag of the net is generally constructed from a lightweight mesh to minimize damage to delicate butterfly wings. Other types of nets used in insect collecting include beat nets, aquatic nets, and sweep nets. Nets for catching different insects have different mesh sizes. Aquatic nets usually have bigger, more 'open' mesh. Catching small aquatic creatures usually requires an insect net. The mesh is smaller and can capture more. \n\n"}
{"id": "8199240", "url": "https://en.wikipedia.org/wiki?curid=8199240", "title": "Christine L. Borgman", "text": "Christine L. Borgman\n\nChristine L. Borgman is Distinguished Professor and Presidential Chair in Information Studies at UCLA. She is the author of more than 200 publications in the fields of information studies, computer science, and communication. Both of her sole-authored monographs, Scholarship in the Digital Age: Information, Infrastructure, and the Internet (MIT Press, 2007) and From Gutenberg to the Global Information Infrastructure: Access to Information in a Networked World (MIT Press, 2000), have won the Best Information Science Book of the Year award from the American Society for Information Science and Technology. She is a lead investigator for the Center for Embedded Networked Sensing (CENS), a National Science Foundation Science and Technology Center, where she conducts data practices research. She chaired the Task Force on Cyberlearning for the NSF, whose report, Fostering Learning in the Networked World, was released in July, 2008. Prof. Borgman is a Fellow of the American Association for the Advancement of Science (AAAS), a Legacy Laureate of the University of Pittsburgh, and is the 2011 recipient of the Paul Evan Peters Award from the Coalition for Networked Information, Association for Research Libraries, and EDUCAUSE. The award recognizes notable, lasting achievements in the creation and innovative use of information resources and services that advance scholarship and intellectual productivity through communication networks. She is also the 2011 recipient of the Research in Information Science Award from the American Association of Information Science and Technology. In 2013 she became a fellow of the Association for Computing Machinery.\n\nShe is a member of the U.S. National Academies’ Board on Research Data and Information and the U.S. National CODATA (Committee on Data for Science and Technology), the Strategic Advisory Board to Thomson-Reuters Scholarly Research, the Advisory Board to the Electronic Privacy Information Center, and Member-at-Large for Section T (Information, Computing, and Communication) of the AAAS. At UCLA, she chairs the Information Technology Planning Board. Previous service includes chairing Section T of the AAAS, and membership on the Science Advisory Board to Microsoft Corporation, the Board of Directors of the Council on Library and Information Resources, and the Advisory Board to the Computer & Information Science & Engineering Directorate of the National Science Foundation, and the Association for Computing Machinery Public Policy Council.\n\nBorgman is a frequent speaker at conferences and university events. Recent keynotes and plenary presentations include the Oxford Internet Institute's 10th anniversary conference, A Decade in Internet Time, the International Conference on Asian Digital Libraries, Coalition for Networked Information, Santa Fe Institute, Digital Humanities Conference, Joint Conference on Digital Libraries, 40th Anniversary Conference of the Open University, Marschak Lecture (UCLA), Kanazawa Institute International Seminar on Libraries (Japan), and invited talks at Oxford University, Harvard University, Columbia University, University of Pittsburgh, and Michigan State University.\n\nShe is a member of the editorial boards of the Journal of the American Society for Information Science and Technology, Annual Review of Information Science and Technology, Journal of Digital Information, International Journal of Digital Curation, ASInformation Research, Policy and Internet, and the Journal of Library & Information Science Research. Previous editorial board service includes The Information Society, Journal of Computer-Mediated Communication, Journal of Communication Research, Journal of Computer-Supported Cooperative Work, and the Journal of Documentation. She was Program Chair for the First Joint Conference on Digital Libraries (ACM and IEEE) and serves on program committees for the International Conference on Asian Digital Libraries, the Joint Conference on Digital Libraries, the European Conference on Digital Libraries, American Society for Information Science and Technology, and Conceptions of Library and Information Science (COLIS) conferences.\n\nBorgman’s international activities include posts as a Visiting Scholar at the Oxford Internet Institute, a Fulbright Visiting Professor at the University of Economic Sciences (now Corvinus University of Budapest) and at Eotvos Lorand University in Budapest, Hungary, a Visiting Professor in the Department of Information Science at Loughborough University, and a Scholar-in-Residence at the Rockefeller Foundation Study and Conference Center in Bellagio, Italy.\n\nShe holds the Ph.D. in Communication from Stanford University, M.L.S. from the University of Pittsburgh, and B.A. in Mathematics from Michigan State University.\n\n\n"}
{"id": "2259059", "url": "https://en.wikipedia.org/wiki?curid=2259059", "title": "Chronometry", "text": "Chronometry\n\nChronometry (from Greek χρόνος \"chronos\", \"time\" and μέτρον \"metron\", \"measure\") is the science of the measurement of time, or timekeeping. Chronometry applies to electronic devices, while horology refers to mechanical devices.\n\nIt should not to be confused with chronology, the science of locating events in time, which often relies upon it.\n\n"}
{"id": "2266162", "url": "https://en.wikipedia.org/wiki?curid=2266162", "title": "Chubasco", "text": "Chubasco\n\nA chubasco is a violent squall with thunder and lightning, encountered during the rainy season along the Pacific coast of Mexico, Central America, and South America. It is also widely used when rain is accompanied by strong winds in other Spanish-speaking countries.\n\nThe word chubasco has its origins in the Portuguese word \"chuva\" which means rain. The monsoon storms that regularly pass over the southwestern United States, including the Southern regions of Arizona and New Mexico, are sometimes referred to as ChubascosIn the northern parts of Mexico, especially the northeast and north central, the word chubasco is used especially for suddenly occurring localised storms that produce very strong winds, sometimes as much as 90 miles/hour, and intense rains of as much as 5 - 6 inches in less than an hour. Straight-line winds can topple windmills, and break large limbs of large, sometimes ancient trees.The phenomena normally occurs during the hottest days of the year (May through October).\n"}
{"id": "4824021", "url": "https://en.wikipedia.org/wiki?curid=4824021", "title": "Clover (detector)", "text": "Clover (detector)\n\nA clover detector is a gamma-ray detector that consists of 4 coaxial N-type high purity germanium (Ge) crystals each machined to shape and mounted in a common cryostat to form a structure resembling a four-leaf clover.\n\nA gamma ray may interact with a single Ge crystal and deposit its full energy. The resulting charge collected will then be proportional to this energy. However, through the process of Compton scattering, a gamma ray may interact with two (or possibly more) crystals resulting in the energy (and thus the liberated charge) being shared by the crystals. In this case, a process known as add-back, where the charge collected by each of the crystals is summed, can be used to determine the energy of the incident gamma ray.\n\nThere are a number of advantages offered by using clover detectors as opposed to the more conventional single crystal germanium detectors. Large volume high purity single crystals of Ge can be expensive. By mounting four smaller crystals in a common cryostat a detector of a given volume can be created at a reduced cost. In addition, the individual smaller Ge crystals present a smaller solid angle than a large volume Ge detector thus significantly reducing the effects of Doppler broadening on the resulting spectra. A clover detector can also be used to determine the electric or magnetic nature of the incident photons (e.g. if the gamma ray is an electric quadrupole or a magnetic dipole) as the Compton scattering process for these two types of radiation is different.\n\n"}
{"id": "17765922", "url": "https://en.wikipedia.org/wiki?curid=17765922", "title": "Conformal supergravity", "text": "Conformal supergravity\n\nIn theoretical physics, conformal supergravity is the study of the supersymmetrized version of conformal gravity with Weyl transformations. Equivalently, it is the extension of ordinary supergravity to include Weyl transformations.\n\nOften, nonconformal gravity is described by conformal gravity with a conformal compensator.\n\nFor a review of conformal supergravity see E.S. Fradkin and A.A. Tseytlin, \"Conformal Supergravity\", Phys. Rep. 119 (1985) 233\n"}
{"id": "2261256", "url": "https://en.wikipedia.org/wiki?curid=2261256", "title": "Epizeuxis", "text": "Epizeuxis\n\nIn rhetoric, an epizeuxis is the repetition of a word or phrase in immediate succession, typically within the same sentence, for vehemence or emphasis. It is also called diacope. As a rhetorical device, epizeuxis is utilized to create an emotional appeal, thereby inspiring and motivating the audience. However, epizeuxis can also be used for comic effect.\n\n\n"}
{"id": "1669521", "url": "https://en.wikipedia.org/wiki?curid=1669521", "title": "Eric Poehlman", "text": "Eric Poehlman\n\nEric T. Poehlman (born c. 1956), a scientist in the field of human obesity and aging, was the first academic in the United States to be jailed for falsifying data in a grant application. He had published fraudulent research alleging hormone replacement injections as a therapy for menopause, when in fact they had no proven medical benefits at all. \n\nHe joined the University of Vermont (UVM) College of Medicine in 1987 as an assistant professor, later working for three years at the University of Maryland in Baltimore. He eventually returned to UVM as a full professor. Poehlman built a reputation as one of the leading authorities on the metabolic changes that come with aging, particularly during menopause; he published more than 200 journal articles over two decades of research. His papers included research on the genetics of obesity and the impact of exercise, often following human subjects over time to document changes in their physiology. However, his stellar career unravelled when Poehlman's misconduct was detected and exposed by a former University of Vermont lab technician, Walter DeNino, who once viewed Poehlman as his mentor. Poehlman was accused of scientific misconduct and on March 17, 2005 pleaded guilty to the charges, acknowledging falsifying 17 grant applications to the National Institutes of Health and fabricating data in 10 of his papers that were submitted between 1992 and 2000. \n\nOn June 28, 2006, Poehlman was ordered to serve a year and a day in federal prison for using falsified data in federal research grants that he submitted for funding. In a plea bargain that he made with the prosecutors, Poehlman pleaded guilty with one $542,000 grant; the government prosecutors stated that Poehlman had defrauded agencies out of $2.9 million.\n\n\"Dr. Poehlman fraudulently diverted millions of dollars,\" said David V. Kirby, the US attorney for Vermont. \"This in turn siphoned millions of dollars from the pool of resources available for valid scientific research proposals. As this prosecution proves, such conduct will not be tolerated.\"\n\nBefore imposing the sentence, Judge William Sessions III said \"I generally think deterrence is significant, perhaps more so in this case. The scientific community may be watching.\" Sessions reprimanded Poehlman for his misconduct, saying he had \"violated the public trust.\"\n\nIn addition to jail time, Poehlman was permanently barred from getting more federal research grants, and was ordered by the court to write letters of retraction and correction to several scientific journals.\n\nHe currently works on the South shore of Montreal as an academic adviser for Champlain Regional College. \n\n\n\n"}
{"id": "33386318", "url": "https://en.wikipedia.org/wiki?curid=33386318", "title": "Eugène Penard", "text": "Eugène Penard\n\nEugène Penard was a Swiss biologist and pioneer in systematics of the amoebae. Penard was born on the 16 September 1855 in Geneva, and lived until 1954.\n"}
{"id": "15836896", "url": "https://en.wikipedia.org/wiki?curid=15836896", "title": "Facial eczema", "text": "Facial eczema\n\nFacial eczema, FE, is a disease that mainly affects ruminants such as cattle, sheep, deer, goats and South American camelids (alpaca, llamas).\nIt is caused by the fungus \"Pithomyces chartarum\" that under favorable conditions can rapidly disseminate in pastures. The fungus requires warm humid weather with night time temperatures of over 13 °C (55 °F) for several days, and litter at the bottom of the sward.\n\n\"Pithomyces chartarum\" occurs worldwide but is a problem predominantly where farm animals are intensively grazed, especially in New Zealand.\nThe spores of the fungus release the mycotoxin sporidesmin in the gastrointestinal tract, causing a blockage in the bile ducts that leads to injury of the liver. Bile, chlorophyll and other waste products consequently build up in the bloodstream causing photo sensitivity of the skin especially that exposed to direct sunlight. This in turn causes severe skin irritation that the animal attempts to relieve by rubbing its head against available objects, resulting in peeling of the skin.\n\nThe large family of fungi that produce mycotoxins, of which sporidesmin is one, live mainly on ryegrasses and can cause significant problems in grazing animals. Sporidesmin can lower an animals immunity and affect total production in farm animals, and, when taken in larger quantities, can result in death.\n\nThe clinical symptoms of FE are distressing: restlessness, frequent urination, shaking, persistent rubbing of the head against objects (e.g. fences, trees etc.), drooping and reddened ears, swollen eyes, and avoidance of sunlight by seeking shade. Exposed areas of skin develop weeping dermatitis and scabs that can become infected and attractive to blow-fly causing myiasis.\n\nThere has been anecdotal evidence provided by Elaine Ingham according to which susceptibility to facial eczema in cattle is related to nutrient deficiency of forage. The experiment, which Ingham conducted, showed that by improving soil biology the forage had better nutrient qualities and was associated with an elimination of the disease.\n\n"}
{"id": "571621", "url": "https://en.wikipedia.org/wiki?curid=571621", "title": "Grumman X-29", "text": "Grumman X-29\n\nThe Grumman X-29 was an American experimental aircraft that tested a forward-swept wing, canard control surfaces, and other novel aircraft technologies. The X-29 was developed by Grumman, and the two built were flown by NASA and the United States Air Force. The aerodynamic instability of the X-29's airframe required the use of computerized fly-by-wire control. Composite materials were used to control the aeroelastic divergent twisting experienced by forward-swept wings, and to reduce weight. The aircraft first flew in 1984, and two X-29s were flight tested through 1991.\n\nTwo X-29As were built by Grumman from two existing Northrop F-5A Freedom Fighter airframes (63-8372 became 82-0003 and 65-10573 became 82-0049) after the proposal had been chosen over a competing one involving a General Dynamics F-16 Fighting Falcon. The X-29 design made use of the forward fuselage and nose landing gear from the F-5As with the control surface actuators and main landing gear from the F-16. The technological advancement that made the X-29 a plausible design was the use of carbon-fiber composites. The wings of the X-29, made partially of graphite epoxy, were swept forward at more than 33 degrees; forward-swept wings were first trialed 40 years earlier on the experimental Junkers Ju 287 and OKB-1 EF 131. The Grumman internal designation for the X-29 was \"Grumman Model 712\" or \"G-712\".\n\nThe X-29 is described as a three surface aircraft, with canards, forward-swept wings, and aft strake control surfaces, using three-surface longitudinal control. The canards and wings result in reduced trim drag and reduced wave drag, while using the strakes for trim in situations where the center of gravity is off provides less trim drag than relying on the canard to compensate.\n\nThe configuration, combined with a center of gravity well aft of the aerodynamic center, made the craft inherently unstable. Stability was provided by the computerized flight control system making 40 corrections per second. The flight control system was made up of three redundant digital computers backed up by three redundant analog computers; any of the three could fly it on its own, but the redundancy allowed them to check for errors. Each of the three would \"vote\" on their measurements, so that if any one was malfunctioning it could be detected. It was estimated that a total failure of the system was as unlikely as a mechanical failure in an airplane with a conventional arrangement.\n\nThe high pitch instability of the airframe led to wide predictions of extreme maneuverability. This perception has held up in the years following the end of flight tests. Air Force tests did not support this expectation. For the flight control system to keep the whole system stable, the ability to initiate a maneuver easily needed to be moderated. This was programmed into the flight control system to preserve the ability to stop the pitching rotation and keep the aircraft from departing out of control. As a result, the whole system as flown (with the flight control system in the loop as well) could not be characterized as having any special increased agility. It was concluded that the X-29 could have had increased agility if it had faster control surface actuators and/or larger control surfaces.\n\nIn a forward swept wing configuration, the aerodynamic lift produces a twisting force which rotates the wing leading edge upward. This results in a higher angle of attack, which increases lift, twisting the wing further. This aeroelastic divergence can quickly lead to structural failure. With conventional metallic construction, a torsionally very stiff wing would be required to resist twisting; stiffening the wing adds weight, which may make the design unfeasible.\n\nThe X-29 design made use of the anisotropic elastic coupling between bending and twisting of the carbon fiber composite material to address this aeroelastic effect. Rather than using a very stiff wing, which would carry a weight penalty even with the relatively light-weight composite, the X-29 used a laminate which produced coupling between bending and torsion. As lift increases, bending loads force the wing tips to bend upward. Torsion loads attempt to twist the wing to higher angles of attack, but the coupling resists the loads, twisting the leading edge downward reducing wing angle of attack and lift. With lift reduced, the loads are reduced and divergence is avoided.\n\nThe first X-29 took its maiden flight on 14 December 1984 from Edwards AFB piloted by Grumman's Chief Test Pilot Chuck Sewell. The X-29 was the third forward-swept wing jet-powered aircraft design to fly; the other two were the Nazi Germany Junkers Ju 287 (1944 and post-war by the USSR) and the HFB-320 Hansa Jet (1964). On 13 December 1985, a X-29 became the first forward-swept wing aircraft to fly at supersonic speed in level flight.\n\nThe X-29 began a NASA test program four months after its first flight. The X-29 proved reliable, and by August 1986 was flying research missions of over three hours involving multiple flights. The first X-29 was not equipped with a spin recovery parachute, as flight tests were planned to avoid maneuvers that could result in departure from controlled flight, such as a spin. The second X-29 was given such a parachute and was involved in high angle-of-attack testing. X-29 number two was maneuverable up to an angle of attack of about 25 degrees with a maximum angle of 67° reached in a momentary pitch-up maneuver.\n\nThe two X-29 aircraft flew a total of 242 times from 1984 to 1991. The NASA Dryden Flight Research Center reported that the X-29 demonstrated a number of new technologies and techniques, and new uses of existing technologies, including the use of \"aeroelastic tailoring to control structural divergence\", aircraft control and handling during extreme instability, three-surface longitudinal control, a \"double-hinged trailing-edge flaperon at supersonic speeds\", effective high angle of attack control, vortex control, and demonstration of military utility.\n\nThe first X-29, 82-003, is now on display in the Research and Development Gallery at the National Museum of the United States Air Force on Wright-Patterson Air Force Base near Dayton, Ohio. The other craft is on display at the Armstrong Flight Research Center on Edwards Air Force Base. A full-scale model was on display from 1989 to 2011 at the National Air and Space Museum's National Mall building in Washington, DC.\n\n\n"}
{"id": "22980399", "url": "https://en.wikipedia.org/wiki?curid=22980399", "title": "Head injury criterion", "text": "Head injury criterion\n\nThe Head Injury Criterion (HIC) is a measure of the likelihood of head injury arising from an impact. The HIC can be used to assess safety related to vehicles, personal protective gear, and sport equipment. \n\nNormally the variable is derived from the measurements of an accelerometer mounted at the center of mass of a crash test dummy’s head, when the dummy is exposed to crash forces.\n\nIt is defined as:\nformula_1 \nwhere \"t\" and \"t\" are the initial and final times (in seconds) chosen to maximize HIC, and acceleration \"a\" is measured in \"g\"s (standard gravity acceleration). The time duration, \"t\" – \"t\", is limited to a maximum value of 36 ms, usually 15 ms.\n\nThis means that the HIC includes the effects of head acceleration and the duration of the acceleration. Large accelerations may be tolerated for very short times.\n\nAt a HIC of 1000, there is an 18% probability of a severe head injury, a 55% probability of a serious injury and a 90% probability of a moderate head injury to the average adult.\n\nHIC is used to determine the U.S. National Highway Traffic Safety Administration (NHTSA) star rating for automobile safety and to determine ratings given by the Insurance Institute for Highway Safety.\n\nAccording to the Insurance Institute for Highway Safety, head injury risk is evaluated mainly on the basis of head injury criterion. A value of 700 is the maximum allowed under the provisions of the U.S. advanced airbag regulation (NHTSA, 2000) and is the maximum score for an \"acceptable\" IIHS rating for a particular vehicle. \n\nA HIC-15 (meaning a measure of impact over 15 milliseconds) of 700 is estimated to represent a 5 percent risk of a severe injury (Mertz et al., 1997). A \"severe\" injury is one with a score of 4+ on the Abbreviated Injury Scale (AIS) (Association for the Advancement of Automotive Medicine, 1990). \n\nData for specific vehicles can be found on various automotive review websites. Some sample data is as follows, for comparative purposes:\nA comprehensive searchable database of vehicles and their HIC scores is available at safercar.gov.\n\nSport physiologists and biomechanics experts use the HIC in the research of safety equipment and guidelines for competitive sport and recreation. In one study, concussions were found to occur at HIC=250 in most athletes. Studies have been conducted in skiing and other sports to test adequacy of helmets \n\n"}
{"id": "41647439", "url": "https://en.wikipedia.org/wiki?curid=41647439", "title": "IZA Journal of Migration", "text": "IZA Journal of Migration\n\nThe IZA Journal of Migration is a peer-reviewed open access academic journal covering all aspects of human migration, especially from an economical point of view. It is published by SpringerOpen on behalf of the Institute for the Study of Labor. The editors-in-chief are Amelie F. Constant (George Washington University) and Denis Fougère (CNRS, Paris)].\n\nThe journal was established in 2012 and is abstracted and indexed in EconLit and RePEc.\n"}
{"id": "2388223", "url": "https://en.wikipedia.org/wiki?curid=2388223", "title": "Instrument error", "text": "Instrument error\n\nInstrument error refers to the combined accuracy and precision of a measuring instrument, or the difference between the actual value and the value indicated by the instrument (error). Measuring instruments are usually calibrated on some regular frequency against a standard. The most rigorous standard is one maintained by a standards organization such as NIST in the United States, or the ISO in European countries. However, in physics—precision, accuracy, and error are computed based upon the instrument and the measurement data. Precision is to 1/2 of the granularity of the instrument's measurement capability. Precision is limited to the number of significant digits of measuring capability of the coarsest instrument or constant in a sequence of measurements and computations. Error is ± the granularity of the instrument's measurement capability. Error magnitudes are also added together when making multiple measurements for calculating a certain quantity. When making a calculation from a measurement to a specific number of significant digits, rounding (if needed) must be done properly. Accuracy might be determined by making multiple measurements of the same thing with the same instrument, and then calculating the result with a certain type of math function, or it might mean for example, a five-pound weight could be measured on a scale and then the difference between five pounds and the measured weight could be the accuracy. The second definition makes accuracy related to calibration, while the first definition does not.\n\nThe instrument error is not like random error, that can't be removed. Sometimes the removal of instrument errors are very easy, but it is case dependent. In Engineering instruments, like voltmeter or ammeter for example, the instrument error is very difficult to remove. Ammeter has built in resistance, which can't be removed either way. So the only way is to minimize it. On the other hand, the removal of error of a thermometer is a bit simple. Only the calibration has to be removed and then again calibrate it carefully.\nSometimes, the user doesn't care for removal of error from the instrument, else he compensates it in calculation, for example, the zero error in Vernier Caliper is eliminated by proper calculation.\n\nAnother way to deal with instrument error may be to reduce the reactivity of the system to being measured by using some sort of Weak measurement. That is, taking far more, but far less powerful readings to avoid changing the system by the act of measuring.\n"}
{"id": "4973666", "url": "https://en.wikipedia.org/wiki?curid=4973666", "title": "List of Ace SF numeric-series single titles", "text": "List of Ace SF numeric-series single titles\n\nAce Books have published hundreds of science fiction titles, starting in 1953. Many of these were Ace Doubles (dos-à-dos format), but they also published many single volumes. Between 1953 and 1968, the books had a letter-series identifier; after that date they were given five digit numeric serial numbers. There are 693 numeric-series sf titles in the list below, but the list is very incomplete.\n\nThe list given here gives a date of publication; in all cases this refers to the date of publication by Ace, and not the date of original publication of the novels. For more information about the history of these titles, see Ace Books, which includes a discussion of the serial numbering conventions used and an explanation of the letter-code system.\n\n\n"}
{"id": "22202385", "url": "https://en.wikipedia.org/wiki?curid=22202385", "title": "List of Uzbek flags", "text": "List of Uzbek flags\n\nThis is a list of flags used in Uzbekistan.\n"}
{"id": "4644798", "url": "https://en.wikipedia.org/wiki?curid=4644798", "title": "List of computer systems from the Socialist Federal Republic of Yugoslavia", "text": "List of computer systems from the Socialist Federal Republic of Yugoslavia\n\nThis is a list of computer systems from SFRY. It lists all computers that were significantly or completely designed in Socialist Federal Republic of Yugoslavia before the breakup of the country in 1990s. This list does not include imported foreign computers. Some of those were assembled as per original manufacturer's license. See history of computer hardware in the SFRY for more information.\n\n"}
{"id": "17978052", "url": "https://en.wikipedia.org/wiki?curid=17978052", "title": "List of lakes of Alaska", "text": "List of lakes of Alaska\n\nAlaska has about 3,197 officially named natural lakes, out of over 3,000,000 unnamed natural lakes, approximately 67 named artificial reservoirs, and 167 named dams.\nFor named artificial reservoirs and dams, see the list of reservoirs and dams of Alaska.\n\n \n\n \n\n\n"}
{"id": "36912899", "url": "https://en.wikipedia.org/wiki?curid=36912899", "title": "List of target antigens in pemphigoid", "text": "List of target antigens in pemphigoid\n\nCirculating auto-antibodies in the human body can target normal parts of the skin leading to disease. This is a list of antigens in the skin that may become targets of circulating auto-antibodies leading to the various types of pemphigoid.\n\nOf note, there are also several other diseases that are caused by auto-antibodies that target the same anatomic area of the skin which is termed the basement membrane zone. These diseases include:\n\n\n"}
{"id": "55294324", "url": "https://en.wikipedia.org/wiki?curid=55294324", "title": "List of the Cenozoic life of Alabama", "text": "List of the Cenozoic life of Alabama\n\nThis list of the Cenozoic life of Alabama contains the various prehistoric life-forms whose fossilized remains have been reported from within the US state of Alabama and are between 66 million and 10,000 years of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "43548721", "url": "https://en.wikipedia.org/wiki?curid=43548721", "title": "Lists of fossiliferous stratigraphic units in Africa", "text": "Lists of fossiliferous stratigraphic units in Africa\n\nThese lists of fossiliferous stratigraphic units in Africa enumerate the rock layers which preserve the fossilized remains of ancient life in Africa by the modern countries wherein they are found.\n\nLists of fossiliferous stratigraphic units\n"}
{"id": "1018053", "url": "https://en.wikipedia.org/wiki?curid=1018053", "title": "Marian Danysz", "text": "Marian Danysz\n\nMarian Danysz (March 17, 1909 – February 9, 1983) was a Polish physicist.\n\nSon of Jan Kazimierz Danysz. In 1952, he co-discovered with Jerzy Pniewski a new kind of matter, an atomic nucleus, which alongside a proton and neutron contains a third particle: the lambda hyperon ().\n\nTen years later, they obtained a hypernucleus in excited state, and the following year a hypernucleus with two lambda hyperons.\n"}
{"id": "37361886", "url": "https://en.wikipedia.org/wiki?curid=37361886", "title": "María Teresa Ruiz", "text": "María Teresa Ruiz\n\nMaría Teresa Ruiz (born 24 September 1946) is a Chilean astronomer. She was the first woman to receive Chile's National Prize for Exact Sciences.\n\nRuiz was born in Santiago de Chile in 1946. She was the first woman to successfully study astronomy at the University of Chile, the first female scientist to receive a PhD in astrophysics at Princeton University, and in 1997 became the first woman to receive her country's National Prize for Exact Sciences. Also, she received a postdoctoral position at Tiestre Observatory, Ruiz worked for two years in UNAM, the Institute of Astronomy in Mexico.\n\nRuiz did research at Trieste, Mexico and New York City before she gained a Guggenheim fellowship. She discovered Kelu 1, a brown dwarf.\n\n, she is a member of the Chilean Academy of Science and a professor at the University of Chile, where she teaches astronomy. She is also a member of the Centro de Astrofísica CATA (\"CATA Centre for Astrophysics\").\n\n\n"}
{"id": "19552", "url": "https://en.wikipedia.org/wiki?curid=19552", "title": "Media studies", "text": "Media studies\n\nFor a history of the field, see \"History of media studies\".\n\nMedia is studied as a broad subject in most states in Australia, with the state of Victoria being world leaders in curriculum development . Media studies in Australia was first developed as an area of study in Victorian universities in the early 1960s, and in secondary schools in the mid 1960s.\n\nToday, almost all Australian universities teach media studies. According to the Government of Australia's \"Excellence in Research for Australia\" report, the leading universities in the country for media studies (which were ranked well above World standards by the report's scoring methodology) are Monash University, QUT, RMIT, University of Melbourne, University of Queensland and UTS.\n\nIn secondary schools, an early film studies course first began being taught as part of the Victorian junior secondary curriculum during the mid 1960s. And, by the early 1970s, an expanded media studies course was being taught. The course became part of the senior secondary curriculum (later known as the Victorian Certificate of Education or \"VCE\") in the 1980s. It has since become, and continues to be, a strong component of the VCE. Notable figures in the development of the Victorian secondary school curriculum were the long time Rusden College media teacher Peter Greenaway (not the British film director), Trevor Barr (who authored one of the first media text books \"Reflections of Reality\") and later John Murray (who authored \"The Box in the Corner\", \"In Focus\", and \"10 Lessons in Film Appreciation\").\n\nToday, Australian states and territories that teach media studies at a secondary level are Australian Capital Territory, Northern Territory, Queensland, South Australia, Victoria and Western Australia. Media studies does not appear to be taught in the state of New South Wales at a secondary level.\n\nIn Victoria, the VCE media studies course is structured as: Unit 1 - Representation, Technologies of Representation, and New Media; Unit 2 - Media Production, Australian Media Organisations; Unit 3 - Narrative Texts, Production Planning; and Unit 4 - Media Process, Social Values, and Media Influence. Media studies also form a major part of the primary and junior secondary curriculum, and includes areas such as photography, print media and television.\n\nVictoria also hosts the peak media teaching body known as ATOM which publishes \"Metro\" and \"Screen Education\" magazines.\n\nIn Canada, media studies and communication studies are incorporated in the same departments and cover a wide range of approaches (from critical theory to organizations to research-creation and political economy, for example). Over time, research developed to employ theories and methods from cultural studies, philosophy, political economy, gender, sexuality and race theory, management, rhetoric, film theory, sociology, and anthropology. Harold Innis and Marshall McLuhan are famous Canadian scholars for their contributions to the fields of media ecology and political economy in the 20th century. They were both important members of the Toronto School of Communication at the time. More recently, the School of Montreal and its founder James R. Taylor significantly contributed to the field of organizational communication by focusing on the ontological processes of organizations.\n\nCarleton University and the University of Western Ontario, 1945 and 1946 prospectively, created Journalism specific programs or schools. A Journalism specific program was also created at Ryerson in 1950. The first communication programs in Canada were started at Ryerson and Concordia Universities. The Radio and Television Arts program at Ryerson were started in the 1950s, while the Film, Media Studies/Media Arts, and Photography programs also originated from programs started in the 1950s. The Communication studies department at Concordia was created in the late 1960s. Ryerson's Radio and Television, Film, Media and Photography programs were renowned by the mid 1970s, and its programs were being copied by other colleges and universities nationally and Internationally.\n\nToday, most universities offer undergraduate degrees in Media and Communication Studies, and many Canadian scholars actively contribute to the field, among which: Brian Massumi (philosophy, cultural studies), Kim Sawchuk (cultural studies, feminist, ageing studies), Carrie Rentschler (feminist theory), and François Cooren (organizational communication).\n\nIn his book “Understanding Media, The Extensions of Man”, media theorist Marshall McLuhan suggested that \"the medium is the message\", and that all human artefacts and technologies are media. His book introduced the usage of terms such as “media” into our language along with other precepts, among them “global village” and “Age of Information”. A medium is anything that mediates our interaction with the world or other humans. Given this perspective, media study is not restricted to just media of communications but all forms of technology. Media and their users form an ecosystem and the study of this ecosystem is known as media ecology.\n\nMcLuhan says that the “technique of fragmentation that is the essence of machine technology” shaped the restructuring of human work and association and “the essence of automation technology is the opposite”. He uses an example of the electric light to make this connection and to explain “the medium is the message”. The electric light is pure information and it is a medium without a message unless it is used to spell out some verbal ad or a name. The characteristic of all media means the “content” of any medium is always another medium. For example, the content of writing is speech, the written word is the content of print, and print is the content of the telegraph. The change that the medium or technology introduces into human affairs is the “message”. If the electric light is used for Friday night football or to light up your desk you could argue that the content of the electric light is these activities. The fact that it is the medium that shapes and controls the form of human association and action makes it the message. The electric light is over looked as a communication medium because it doesn’t have any content. It is not until the electric light is used to spell a brand name that it is recognized as medium. Similar to radio and other mass media electric light eliminates time and space factors in human association creating deeper involvement. McLuhan compared the “content” to a juicy piece of meat being carried by a burglar to distract the “watchdog of the mind”. The effect of the medium is made strong because it is given another media “content”. The content of a movie is a book, play or maybe even an opera.\n\nMcLuhan talks about media being “hot” or “cold” and touches on the principle that distinguishes them from one another. A hot medium (i.e., radio or Movie) extends a single sense in “high definition”. High definition means the state of being well filled with data. A cool medium (i.e., Telephone and TV) is considered “low definition” because a small amount of data/information is given and has to be filled in. Hot media are low in participation and cool media are high in participation. Hot media are low in participation because it is giving most of the information and it excludes. Cool media are high in participation because it gives you information but you have to fill in the blanks and it is inclusive. He used lecturing as an example for hot media and seminars as an example for low media. If you use a hot medium in a hot or cool culture makes a difference.\n\nThere are two universities in China that specialize in media studies. Communication University of China, formerly known as the Beijing Broadcasting Institute, that dates back to 1954. CUC has 15,307 full-time students, including 9264 undergraduates, 3512 candidates for doctor and master's degrees and 16780 students in programs of continuing education. The other university known for media studies in China is Zhejiang University of Media and Communications (ZUMC) which has campuses in Hangzhou and Tongxiang. Almost 10,000 full-time students are currently studying in over 50 programs at the 13 Colleges and Schools of ZUMC. Both institutions have produced some of China's brightest broadcasting talents for television as well as leading journalists at magazines and newspapers.\n\nThere is no university specialized on journalism and media studies, but there are seven public universities which have a department of media stuides. Three biggest are based in Prague (Charles University), Brno (Masaryk University) and Olomouc (Palacký University). There are another nine private universities and colleges which has media studies department.\n\nOne prominent French media critic is the sociologist Pierre Bourdieu who wrote among other books \"On Television\" (New Press, 1999). Bourdieu's analysis is that television provides far less autonomy, or freedom, than we think. In his view, the market (which implies the hunt for higher advertising revenue) not only imposes uniformity and banality, but also a form of invisible censorship. When, for example, television producers \"pre-interview\" participants in news and public affairs programs, to ensure that they will speak in simple, attention-grabbing terms, and when the search for viewers leads to an emphasis on the sensational and the spectacular, people with complex or nuanced views are not allowed a hearing.\n\nIn Germany two main branches of media theory or media studies can be identified.\n\nThe first major branch of media theory has its roots in the humanities and cultural studies, such as film studies (\"Filmwissenschaft\"), theater studies (\"Theaterwissenschaft\") and German language and literature studies (\"Germanistik\") as well as Comparative Literature Studies (\"Komparatistik\"). This branch has broadened out substantially since the 1990s. And it is on this initial basis that a culturally-based media studies (often emphasised more recently through the disciplinary title \"Medienkulturwissenschaft\") in Germany has primarily developed and established itself. \n\nThis plurality of perspectives make it difficult to single out one particular site where this branch of Medienwissenschaft originated. While the Frankfurt-based theatre scholar, Hans-Theis Lehmanns term \"post dramatic theater\" points directly to the increased blending of co-presence and mediatized material in the German theater (and elsewhere) since the 1970s, the field of theater studies from the 1990s onwards at the Freie Universität Berlin, led in particular by Erika Fischer-Lichte, showed particular interest in the ways in which theatricality influenced notions of performativity in aesthetic events. Within the field of Film Studies, again, both Frankfurt and Berlin were dominant in the development of new perspectives on moving image media. Heide Schlüpman in Frankfurt and Gertrud Koch, first in Bochum then in Berlin, were key theorists contributing to an aesthetic theory of the cinema (Schlüpmann) as \"dispositif\" and the moving image as medium, particularly in the context of illuion (Koch). Many scholars who became known as media scholars in Germany originally were scholars of German, such as Friedrich Kittler, who taught at the Humboldt Universität zu Berlin, completed both his dissertation and habilitation in the context of \"Germanistik\". One of the early publications in this new direction is a volume edited by Helmut Kreuzer, \"Literature Studies - Media Studies\" (\"Literaturwissenschaft – Medienwissenschaft\"), which summarizes the presentations given at the Düsseldorfer Germanistentag 1976.\n\nThe second branch of media studies in Germany is comparable to Communication Studies. Pioneered by Elisabeth Noelle-Neumann in the 1940s, this branch studies mass media, its institutions and its effects on society and individuals. The German Institute for Media and Communication Policy, founded in 2005 by media scholar Lutz Hachmeister, is one of the few independent research institutions that is dedicated to issues surrounding media and communications policies.\n\nThe term \"Wissenschaft\" cannot be translated straightforwardly as \"studies\", as it calls to mind both scientific methods and the humanities. Accordingly, German media theory combines philosophy, psychoanalysis, history, and scienctific studies with media-specific research.\n\n\"Medienwissenschaften\" is currently one of the most popular courses of study at universities in Germany, with many applicants mistakenly assuming that studying it will automatically lead to a career in TV or other media. This has led to widespread disillusionment, with students blaming the universities for offering highly theoretical course content. The universities maintain that practical journalistic training is not the aim of the academic studies they offer.\n\nMedia Studies is a fast growing academic field in India, with several dedicated departments and research institutes. With a view to making the best use of communication facilities for information, publicity and development, the Government of India in 1962-63 sought the advice of the Ford Foundation/UNESCO team of internationally known mass communication specialists who recommended the setting up of a national institute for training, teaching and research in mass communication. Anna University was the first university to start Master of Science in Electronic Media programmes. It offers a five-year integrated programme and a two-year programme in Electronic Media. The Department of Media Sciences was started in January 2002, branching off from the UGC's Educational Multimedia Research Centre (EMMRC). National Institute of Open Schooling, the world's largest open schooling system, offers Mass Communication as a subject of studies at senior secondary level. All the major universities in the country have mass media and journalism studies departments. Centre for the Study of Developing Societies (CSDS), Delhi has media studies as one of their major emphasis. Centre for Internet and Society, Bangaluru that does interdisciplinary research on internet and digital technologies also is worth mentioning.\nMain scholars who are working on Indian media include Arvind Rajagopal, Ravi Sundaram, Robin Jeffrey, Sevanti Ninan, Shohini Ghosh, and Usha M. Rodrigues and Maya Ranganathan. The work of Nalin Mehta on the expansion of private television channels in India, Amelia Bonea's research on the history of telegraph and journalism, and Shiju Sam Varughese's work on science and mass media open new areas of research in Indian media studies.\n\nIn the Netherlands, media studies are split into several academic courses such as (applied) communication sciences, communication- and information sciences, communication and media, media and culture or theater, film and television sciences. Whereas communication sciences focuses on the way people communicate, be it mediated or unmediated, media studies tends to narrow the communication down to just mediated communication. However, it would be a mistake to consider media studies a specialism of communication sciences, since media make up just a small portion of the overall course. Indeed, both studies tend to borrow elements from one another.\n\nCommunication sciences (or a derivative thereof) can be studied at Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam and Wageningen University and Research Centre.\n\nMedia studies (or something similar) can be studied at the University of Amsterdam, VU University Amsterdam, Erasmus University Rotterdam, University of Groningen and the University of Utrecht.\n\nMedia studies in New Zealand is healthy, especially due to renewed activity in the country's film industry and is taught at both secondary and tertiary education institutes. Media studies in NZ can be regarded as a singular success, with the subject well-established in the tertiary sector (such as Screen and Media Studies at the University of Waikato; Media Studies, Victoria University of Wellington; Film, Television and Media Studies, University of Auckland; Media Studies, Massey University; Communication Studies, University of Otago). Different Media Studies courses can offer students a range of specialisations- such as cultural studies, media theory and analysis, practical film-making, journalism and communications studies. But what makes the case of New Zealand particularly significant in respect of Media Studies is that for more than a decade it has been a nationally mandated and very popular subject in secondary (high) schools, taught across three years in a very structured and developmental fashion, with Scholarship in Media Studies available for academically gifted students. According to the New Zealand Ministry of Education Subject Enrolment figures 229 New Zealand schools offered Media Studies as a subject in 2016, representing more than 14,000 students. \n\nIn Pakistan, media studies programs are widely offered. University of the Punjab Lahore is the oldest department. Later on University of Karachi, Peshawar University, BZU Multaan, Islamia University Bahwalpur also started communication programs. Now, newly established universities are also offering mass communication program in which University of Gujrat emerged as a leading department. Bahria University which is established by Pakistan Navy is also offering BS in media studies.\n\nIn Switzerland, media and communication studies are offered by several higher education institutions including the International University in Geneva, Zurich University of Applied Sciences, University of Lugano, University of Fribourg and others.\n\nIn the United Kingdom, media studies developed in the 1960s from the academic study of English, and from literary criticism more broadly. The key date, according to Andrew Crisell, is 1959:\n\nWhen Joseph Trenaman left the BBC's Further Education Unit to become the first holder of the Granada Research Fellowship in Television at Leeds University. Soon after in 1966, the Centre for Mass Communication Research was founded at Leicester University, and degree programmes in media studies began to sprout at polytechnics and other universities during the 1970s and 1980s.\n\nJames Halloran at Leicester University is credited with much influence in the development of media studies and communication studies, as the head of the university's Centre for Mass Communication Research, and founder of the International Association for Media and Communication Research. Media Studies is now taught all over the UK. It is taught at Key Stages 1– 3, Entry Level, GCSE and at A level and the Scottish Qualifications Authority offers formal qualifications at a number of different levels. It is offered through a large area of exam boards including AQA and WJEC.\n\nMuch research in the field of news media studies has been led by the Reuters Institute for the Study of Journalism. Details of the research projects and results are published in the RISJ annual report.\n\nMass communication, Communication studies or simply 'Communication' may be more popular names than “media studies” for academic departments in the United States. However, the focus of such programs sometimes excludes certain media—film, book publishing, video games, etc. The title “media studies” may be used alone, to designate film studies and rhetorical or critical theory, or it may appear in combinations like “media studies and communication” to join two fields or emphasize a different focus. It is a very broad study as media has many platforms in the modern world. Social Media is an industry that has gotten a lot of attention in recent years. Our primary form of entertainment is no longer our TVs but we have access to a screen about worldwide events all the time.\nIn 1999, the MIT Comparative Media Studies program started under the leadership of Henry Jenkins, since growing into a graduate program, MIT's largest humanities major, and, following a 2012 merger with the Writing and Humanistic Studies program, a roster of twenty faculty, including Pulitzer Prize-winning author Junot Diaz, science fiction writer Joe Haldeman, games scholar T. L. Taylor, and media scholars William Uricchio (a CMS co-founder), Edward Schiappa, and Heather Hendershot. Now named Comparative Media Studies/Writing, the department places an emphasis on what Jenkins and colleagues had termed \"applied humanities\": it hosts several research groups for civic media, digital humanities, games, computational media, documentary, and mobile design, and these groups are used to provide graduate students with research assistantships to cover the cost of tuition and living expenses. The incorporation of Writing and Humanistic Studies also placed MIT's Science Writing program, Writing Across the Curriculum, and Writing and Communications Center under the same roof.\n\nFormerly an interdisciplinary major at the University of Virginia the Department of Media Studies was officially established in 2001 and has quickly grown to wide recognition. This is partly thanks to the acquisition of Professor Siva Vaidhyanathan, a cultural historian and media scholar, as well as the Inaugural Verklin Media Policy and Ethics Conference, endowed by the CEO of Canoe Ventures and UVA alumnus David Verklin. In 2010, a group of undergraduate students in the Media Studies Department established the Movable Type Academic Journal, the first ever undergraduate academic journal of its kind. The department is expanding rapidly and doubled in size in 2011.\n\nBrooklyn College, part of the City University of New York, has been offering graduate studies in television and media since 1961. Currently, the Department of Television and Radio administers an MS in Media Studies, and hosts the Center for the Study of World Television.\n\nThe University of Southern California has three distinct centers for media studies: the Center for Visual Anthropology (founded in 1984), the Institute for Media Literacy at the School of Cinematic Arts (founded in 1998) and the Annenberg School for Communication and Journalism (founded in 1971).\n\nUniversity of California, Irvine had in Mark Poster one of the first and foremost theorists of media culture in the US, and can boast a strong Department of Film & Media Studies. University of California, Berkeley has three institutional structures within which media studies can take place: the department of Film and Media (formerly Film Studies Program), including famous theorists as Mary Ann Doane and Linda Williams, the Center for New Media, and a long established interdisciplinary program formerly titled Mass Communications, which recently changed its name to Media Studies, dropping any connotations which accompany the term “Mass” in the former title. Until recently, Radford University in Virginia used the title \"media studies\" for a department that taught practitioner-oriented major concentrations in journalism, advertising, broadcast production and Web design. In 2008, those programs were combined with a previous department of communication (speech and public relations) to create a School of Communication. (A media studies major at Radford still means someone concentrating on journalism, broadcasting, advertising or Web production.)\n\nThe University of Denver has a renowned program for digital media studies. It is an interdisciplinary program combining Communications, Computer Science, and the arts.\n\n\n"}
{"id": "12765883", "url": "https://en.wikipedia.org/wiki?curid=12765883", "title": "Metabolic imprinting", "text": "Metabolic imprinting\n\nMetabolic imprinting refers to the epigenetic programming of metabolism during the pre-natal and neo-natal periods, which can have significant consequences later on in an organism's life.\n\nStudies in both humans and animals have shown that the events during gestation and early post-natal stages may have long term consequences for health. Fetal under-nutrition is linked to an increased risk of cardiovascular disease, obesity, type II diabetes and hypertension, amongst other diseases.\n\n"}
{"id": "33720888", "url": "https://en.wikipedia.org/wiki?curid=33720888", "title": "On the Movements and Habits of Climbing Plants", "text": "On the Movements and Habits of Climbing Plants\n\nOn the Movements and Habits of Climbing Plants is a book by Charles Darwin first printed in book form in 1875 by John Murray. Originally, the text appeared as essay in the 9th volume of the \"Journal of the Linnean Society\", therefore the first edition in book form is actually called the ‘second edition, revised.’ Illustrations were drawn by Charles Darwin’s son, George Darwin.\n\nFollowing the \"Origin of Species\" Darwin set out to produce evidence for his theory of natural selection. Initially Darwin spent much time in studying plants to achieve this aim. This book stands second in line to his first work on plants, \"On the various contrivances by which British and foreign orchids are fertilised by insects.\" (1862)\n\nThis work is subdivided into chapters concentrating on a particular type of climber which he divided into four main classes but Darwin, in this volume, concentrates on the two main classes, the twining plants and the leaf climbers (divided into two sub-divisions: leaf climbers and tendril bearers)\n\nThe following comprise the chapters:\n1. Twining plants\n2. Leaf climbers\n3 & 4.Tendril bearers\n5. Hook and Root climbers.\n\nInspired by reading an 1858 short paper by his friend Asa Gray on the movements of tendrils, Darwin set up experiments to explore the development of so many kinds of climbing plants in an evolutionary context. The concept of the power of movement in plants (‘spontaneous revolutions of the stems...’ p. 1) had already been observed as he acknowledges in the first chapter. His conclusions in his last plant book, \"The Power of Movement in Plants\" are key here: i.e. that circumnutation (the process that creates the circular or elliptical movement of the stem and tips of plants) was central in the development of multitudes of adaptations to the environment and thus resulting in an immense variety of plants. The climbing habit evolved from this basic power of movement.\nDarwin conducted, in his own words, \"observations, founded on the examination of above a hundred widely distinct living species.\" This, he maintained, \"contain sufficient novelty to justify me publishing them.\"\n\nThe spontaneous revolving habit of stems and tips has evolved in many plant groups in order to obtain light and/or support. Darwin in his conclusion explores the reasons for why these adaptations might have taken place, in what ways they may have been advantageous. For instance, an increased ability to hold on to support (by twining) will be beneficial in windy environments. In tall and dense forests, twining plants would probably succeed better with minor expenditure of organic matter. All this evolved due to an inherent ability to respond to their ‘wants’ by moving. (p. 202). Darwin states: \"It has often been vaguely asserted that plants are distinguished from animals by not having the power of movement. It should rather be said that plants acquire and display this power only when it is of some advantage to them; this being of comparatively rare occurrence, as they are affixed to the ground, and food is brought to them by the air and rain.\" (p. 206).\n\n"}
{"id": "28798783", "url": "https://en.wikipedia.org/wiki?curid=28798783", "title": "OpenSonATA", "text": "OpenSonATA\n\nOpen SonATA stands for Open SETI on the Allen Telescope Array and is the open source version of the software are used for signal detection by the SETI Institute on the Allen Telescope Array (ATA). The software currently runs on Linux and Mac OS operating systems and is intended to be ported to multiple platforms. The Allen Telescope Array uses OpenSUSE for the operating system of the SonATA computers.\n\nBefore the release of the code to the public setiQuest had to find all instances that conflicted with the GPL license they looked to release it in.\n\nWith the release of Open SonATA 2.1 setiQuest released the source code to the public under the GPL License. setiQuest has included \"Ways to help.\" in their own documentation of the software. The source code can be found in setiQuest's GitHub repository.\n\nOpen SonATA is closely related to the setiQuest project.\n\n"}
{"id": "51072585", "url": "https://en.wikipedia.org/wiki?curid=51072585", "title": "Operation Big Ben", "text": "Operation Big Ben\n\nOperation Big Ben was the title given to the dive-bombing British Spitfire missions against German mobile V-2 rocket launch sites in Holland between October 1944 - April 1945, during World War II. The code word 'Big Ben' meant 'V2 Rocket' and was used by the Filter Room at Fighter Command and by the pilots of the mission; but the phrase 'Operation Big Ben' was not used in official documentation (none found since the release of former Top Secret papers from the National Archive from January 2004), even though pilots (such as Flt Lt Raymond Baxter, who went on to become the voice of the Farnborough Air Show and BBC TV's technology programme Tomorrow's World) identified the sorties under the name 'Operation Big Ben'.\n\nThe missions were specific: Spitfire Mark XVI's with clipped wings, flew in formations of four aircraft (some Mark IX and some Mark XIV were also used occasionally) and dive-bombed the sites, sometimes through breaks in heavy cloud. Each Spitfire carried a 250lb bomb under each wing and a 500lb bomb under the fuselage. Very occasionally they would just carry the two 250lb bombs or just the 500lb bomb, if they were pin-pointing locations further away (this way they could save on fuel).\n\nAlthough the operation has been the subject of two extensive books, the extent of the success of the missions is still not known. It is considered successful because it is appreciated that the Spitfires did destroy some of the mobile V2 launch sites, along with bridges, roads and railway tracks, which were crucial supply lines; but the operation didn't completely stop the rocket attacks.\n\nIn interview about 'Operation Big Ben', Flt Lt Raymond Baxter said that it was the most difficult operation he ever took part in during the Second World War and insisted that unequivocally, the operation was called 'Operation Big Ben'.\n\nOver the ten years since the operation has been appreciated by aviation historians, a short CGI film has documented the missions, a limited edition model of Raymond Baxter's Mark XVI Spitfire was produced (which he signed the plinth of each model) and two respected non-fiction books have become available.\n\n"}
{"id": "9242198", "url": "https://en.wikipedia.org/wiki?curid=9242198", "title": "Paper Shadows", "text": "Paper Shadows\n\nPaper Shadows: A Chinatown Childhood is a non-fiction memoir, written by Canadian writer Wayson Choy, first published in October 1999 by Viking Press. In the book, the author chronicles his experience growing up as an immigrant in Vancouver's Chinatown in the 1940s and 1950s.\n\n\"Paper Shadows\" received shortlist honours for the 2000 \"Vancouver City Book Award\" and won the 2000 \"Edna Staebler Award for Creative Non-Fiction\".\n\n\n"}
{"id": "50549990", "url": "https://en.wikipedia.org/wiki?curid=50549990", "title": "Postdoctoral researcher unionization", "text": "Postdoctoral researcher unionization\n\nPostdoctoral researcher unionization is the formation of labor unions by postdoctoral researchers (postdocs). It has been driven by increasing competition for scarce tenure-track faculty positions, leading to more people resident in postdoctoral positions for a longer time. Unions often challenge the low pay, minimal benefits, and lack of job security that are typical of postdoctoral positions. Unionizing is however sometimes seen as creating a culture clash of tension between postdocs and their academic advisors, and some question the suitability of a union for a temporary position. Some universities seek to avoid pushes for unionization by proactively addressing the concerns of postdoctoral researchers.\n\nPostdoctoral unions exist only at a few universities. They have often been formed with the help of other unions at the same institution; for example, before the University of Massachusetts Amherst union was formed, postdoctoral researchers were the only class of employees not already part of a union. The National Postdoctoral Association, which is a professional association rather than a labor union, is officially neutral on the issue of postdoctoral unionization.\n\nThe first stand-alone postdoctoral researcher union was UAW Local 5810 at the University of California system. As of 2010 it represented about 6,400 postdoctoral researchers, which was estimated to be about 10% of the United States total, and is the largest postdoctoral researcher union in North America. Efforts to form this union had begun in the early 2000s, and it was officially formed in 2008. Its first contract was ratified in August 2010, leading to the institution of a minimum salary with annual increases, availability of insurance benefits, guaranteed vacation time, paid maternity leave, and just cause protections for discipline or dismissal.\n\nPostdocs at Rutgers University unionized in July 2009. In 2010, a postdoctoral union at the University of Massachusetts Amherst was formed, and in 2012 it ratified a contract providing for salary and benefits, the first of its kind in the University of Massachusetts system. Postdoctoral unions also exist at the University of Connecticut Health Center and the University of Alaska in the United States, and at McMaster University, the University of Western Ontario, and the Université Laval in Canada. In 2018, a postdoctoral union at Columbia University petitioned with the NLRB to become the first certified postdoctoral union at a private university in the United States.\n\n"}
{"id": "14343887", "url": "https://en.wikipedia.org/wiki?curid=14343887", "title": "Precision and recall", "text": "Precision and recall\n\nIn pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.\n\nSuppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is \"how useful the search results are\", and recall is \"how complete the results are\".\n\nIn statistics, if the null hypothesis is that all items are \"irrelevant\" (where the hypothesis is accepted or rejected based on the number selected compared with the sample size), absence of type I and type II errors corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative). The above pattern recognition example contained 8 − 5 = 3 type I errors and 12 − 5 = 7 type II errors. Precision can be seen as a measure of exactness or \"quality\", whereas recall is a measure of completeness or \"quantity\".\n\nIn simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n\nIn an information retrieval scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, \"relevant\" and \"not relevant\". In this case, the \"relevant\" documents are simply those that belong to the \"relevant\" category. Recall is defined as the \"number of relevant documents\" retrieved by a search \"divided by the total number of existing relevant documents\", while precision is defined as the \"number of relevant documents\" retrieved by a search \"divided by the total number of documents retrieved\" by that search.\n\nIn a classification task, the precision for a class is the \"number of true positives\" (i.e. the number of items correctly labeled as belonging to the positive class) \"divided by the total number of elements labeled as belonging to the positive class\" (i.e. the sum of true positives and false positives, which are items incorrectly labeled as belonging to the class). Recall in this context is defined as the \"number of true positives divided by the total number of elements that actually belong to the positive class\" (i.e. the sum of true positives and false negatives, which are items which were not labeled as belonging to the positive class but should have been).\n\nIn information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).\n\nIn a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C). [which items]\n\nOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon tasked with removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\n\nUsually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. \"precision at a recall level of 0.75\") or both are combined into a single measure. Examples of measures that are a combination of precision and recall are the F-measure (the weighted harmonic mean of precision and recall), or the Matthews correlation coefficient, which is a geometric mean of the chance-corrected variants: the regression coefficients Informedness (DeltaP') and Markedness (DeltaP). Accuracy is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence). Inverse Precision and Inverse Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels). Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as ROC curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions. The first problem is 'solved' by using Accuracy and the second problem is 'solved' by discounting the chance component and renormalizing to Cohen's kappa, but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision, and their geometric mean Matthews correlation coefficient thus acts like a debiased F-measure.\n\nIn information retrieval contexts, precision and recall are defined in terms of a set of \"retrieved documents\" (e.g. the list of documents produced by a web search engine for a query) and a set of \"relevant documents\" (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. relevance. The measures were defined in .\n\nIn the field of information retrieval, precision is the fraction of retrieved documents that are relevant to the query:\n\nformula_1\n\nFor example, for a text search on a set of documents, precision is the number of correct results divided by the number of all returned results.\n\nPrecision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called \"precision at n\" or \"P@n\".\n\nPrecision is used with recall, the percent of \"all\" relevant documents that is returned by the search. The two measures are sometimes used together in the F1 Score (or f-measure) to provide a single measurement for a system.\n\nNote that the meaning and usage of \"precision\" in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and technology.\n\nIn information retrieval, recall is the fraction of the relevant documents that are successfully retrieved.\n\nformula_2\n\nFor example, for a text search on a set of documents, recall is the number of correct results divided by the number of results that should have been returned.\n\nIn binary classification, recall is called sensitivity. It can be viewed as the probability that a relevant document is retrieved by the query.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by also computing the precision.\n\nFor classification tasks, the terms \"true positives\", \"true negatives\", \"false positives\", and \"false negatives\" (see Type I and type II errors for definitions) compare the results of the classifier under test with trusted external judgments. The terms \"positive\" and \"negative\" refer to the classifier's prediction (sometimes known as the \"expectation\"), and the terms \"true\" and \"false\" refer to whether that prediction corresponds to the external judgment (sometimes known as the \"observation\").\n\nLet us define an experiment from \"P\" positive instances and \"N\" negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:\n\nPrecision and recall are then defined as:\n\nformula_3\n\nformula_4\n\nRecall in this context is also referred to as the true positive rate or sensitivity, and precision is also referred to as positive predictive value (PPV); other related measures used in classification include true negative rate and accuracy. True negative rate is also called specificity.\n\nformula_5\n\nformula_6\nAccuracy does not perform well with imbalanced data sets. For example, if you have 95 negative and 5 positive samples, classifying all as negative gives 0.95 accuracy score. Balanced Accuracy (bACC) overcomes this problem, by normalizing true positive and true negative predictions by the number of positive and negative samples, respectively, and divides their sum into two. This is equivalent to the following formula:\nformula_7\n\nRegarding the previous example (95 negative and 5 positive samples), classifying all as negative gives 0.5 balanced accuracy score out of the maximum bACC one, which is equivalent to the expected value of a random guess of a balanced data. Balanced Accuracy is suggested to use to measure how accurate is the overall performance of a model is, considering both positive and negative classes without worrying about the imbalance of a data set. Since most of the real data sets are imbalanced, Balanced Accuracy metric is suggested instead of Accuracy metric.\nAdditionally, the predicted positive condition rate (PPCR) identifies the percentage of the total population that is flagged; for example, for a search engine returning 30 results (retrieved documents) out of 1,000,000 documents, the PPCR is 0.003%. formula_8\n\nIt is possible to interpret precision and recall not as ratios but as probabilities:\n\n\nNote that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by \"randomly selected retrieved document\", we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected.\n\nNote that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation).\n\nAnother interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.\n\nA measure that combines precision and recall is the harmonic mean of precision and recall, the traditional F-measure or balanced F-score:\n\nformula_9\n\nThis measure is approximately the average of the two when they are close, and is more generally the harmonic mean, which, for the case of two numbers, coincides with the square of the geometric mean divided by the arithmetic mean. There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. This is also known as the formula_10 measure, because recall and precision are evenly weighted.\n\nIt is a special case of the general formula_11 measure (for non-negative real values of formula_12):\n\nformula_13\n\nTwo other commonly used formula_14 measures are the formula_15 measure, which weights recall higher than precision, and the formula_16 measure, which puts more emphasis on precision than recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that formula_11 \"measures the effectiveness of retrieval with respect to a user who attaches formula_12 times as much importance to recall as precision\". It is based on van Rijsbergen's effectiveness measure formula_19, the second term being the weighted harmonic mean of precision and recall with weights formula_20. Their relationship is formula_21 where formula_22.\n\nThere are other parameters and strategies for performance metric of information retrieval system, such as the area under the ROC curve (AUC).\n\nFor web document retrieval, if the user's objectives are not clear, the precision and recall can't be optimized. As summarized by Lopresti,\n\n"}
{"id": "37219580", "url": "https://en.wikipedia.org/wiki?curid=37219580", "title": "Python SCOOP (software)", "text": "Python SCOOP (software)\n\nSCOOP (Scalable Concurrent Operations in Python) is a Python software module for distributing concurrent tasks on various environments, from heterogeneous grids of workstations to supercomputers.\n\nIt uses ØMQ and the Greenlet package as building blocks to encapsulate and distribute tasks (named a Future) between processes and/or systems. Its interface is inspired from the PEP-3148 proposal.\n\nSCOOP is targeted towards scientific applications that require execution of many loosely coupled tasks using all available hardware resources. These resources need to be accessible through SSH.\n\nSCOOP was initiated by Yannick Hold and Marc Parizeau at the Computer Vision and Systems Laboratory of Université Laval. It is an iterative step over the now deprecated DTM module of the DEAP framework for developing evolutionary algorithm. While DTM used MPI for its communications, SCOOP uses instead ØMQ.\n\nSCOOP uses the Broker Architecture to distribute its Futures. It is based on a central element, called the Broker, that dispatches work to its workers. The main difference between this pattern and a Master/slave topology reside in the Future origin. In the Broker architecture, the Futures emanate from a worker, which is located on the periphery of the topology, instead of the master in the Master/slave architecture. This allows higher reliability in regard to worker faults and generally better performances due to the generic function of the Broker. Since he doesn't need to serialize nor deserialize any Future to route them around the grid, his workload consists of networking or interprocess I/O and almost no CPU processing time. This lowers the bottleneck of the Broker topology.\n\nThe Broker architecture won't stress the networking fabric and elements as much as a totally distributed topology since there is only one connection required on every worker.\n\nAn introductory parallel \"Hello, world!\" example is implemented this way:\n\n"}
{"id": "4564152", "url": "https://en.wikipedia.org/wiki?curid=4564152", "title": "Random mapping", "text": "Random mapping\n\nWhen the data vectors are high-dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space. It is therefore necessary to reduce the dimensionality before, for example, clustering the data.\nRandom Mapping (RM) is a fast dimensionality reduction method categorized as feature extraction method. The RM consists in generation of a random matrix that is multiplied by each original vector and result in a reduced vector.\nIn Text mining context, it is demonstrated that the document classification accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the final dimensionality is sufficiently large (about 100 out of 6000). In fact, it can be shown that the inner product (similarity) between the mapped vectors follows closely the inner product of the original vectors.\n\n\n"}
{"id": "34252228", "url": "https://en.wikipedia.org/wiki?curid=34252228", "title": "Reed receiver", "text": "Reed receiver\n\nA reed receiver or tuned reed receiver (US) was a form of multi-channel signal decoder used for early radio control systems. It uses a simple electromechanical device or \" 'resonant reed' \" to demodulate the signal, in effect a receive-only modem. The encoding used is a simple form of frequency shift keying.\n\nThese decoders appeared in the 1950s and were used into the early 1970s. Early transistor systems were in use in parallel to them, but they were finally displaced by the appearance of affordable digital proportional systems, based on early integrated circuits. These had the advantage of proportional control.\n\nThe decoder of the reed receiver is based on the 'resonant reed' unit. This comprises a number of vibrating metal reeds, each one having a tuned vibration frequency like a tuning fork. These reeds are manufactured from a single tapered sheet of iron or steel, giving a comb of reeds of varying length. This resembles the comb used to sound musical notes in a music box. Like a music box, the length of each reed affects its resonant frequency. The reeds are powered magnetically, by a single solenoid coil and an iron core wrapped between the ends of the reeds.\n\nA reed's resonant frequency is a mid-range audible frequency of perhaps 300 Hz. The solenoid is driven by the output of the radio control receiver, which is an audio tone or tones. If the receiver output contains the appropriate tone for the resonant frequency of a reed, that reed would be made to vibrate. As the reed vibrates, it touches a contact screw above its free end. These contacts form the output of the decoder. Decoder outputs are generally fed to small relays. These allow a high current load to be controlled, such as the model's propulsion motor. Using a relay also adds a damping time constant to the output, so that the intermittent contact with the reed contact (which is vibrating at the transmitter audible tone frequency) becomes a continuous output signal.\n\nEach reed forms an independent channel and they may be activated individually or in combination, depending on the signal from the transmitter.\n\nReed system channels are an on/off output, not a proportional (i.e. analogue) signal. These could be used to drive an escapement, or rapidly switching a channel on and off could be used as pulse-width modulation to provide a proportional signal to drive a servo.\n\nTo avoid potential problems with harmonic frequencies simultaneously activating multiple reeds, the reed frequencies were kept within an octave of each other. The number of distinct frequencies usable within this range depends on the selectivity or Q factor of each reed. Typical radio control reed units used six reeds, sometimes four or eight on simpler or more sophisticated systems.\n\nThe sensitivity of each reed is controlled by mechanically adjusting the contact screw above each reed. This adjustment is critical and temperamental, so a system where reed resonance is pronounced and separate from the other reeds is easiest to adjust. If adjacent reeds also vibrate (at a lesser amplitude) for the same tone, the contact adjustment must not be too sensitive, or else it could be false-triggered by an adjacent channel. This problem becomes worse, the more closely the channels are spaced.\n\nTwelve reed systems were known, but were only required for large ship models, typically warships, with many channels for triggering \"working features\" such as turrets and cannon firing. In practice these were unreliable and so these models used a sequential drum sequencer instead. One channel, probably from a reed, would be used to step the sequencer through each step of a pre-planned demonstration sequence.\n\nIt is sometimes incorrectly claimed that the origin of the resonant reed decoder was in the wartime torpedo-control patent granted to the actress Hedy Lamarr. This patent did pre-empt spread spectrum radio technology, but the frequency-hopping it describes is primarily applied to the radio carrier wave, not the signal coding. A minor aspect of the radio control system described does use a similar frequency-keying mechanism to select left and right rudder, also this is done by separate filters, presumably electronic rather than reed, of 50 & 100 Hz. As these two frequencies are exactly an octave apart, they could also suffer from the harmonic interference problem described above.\n\nA suitable transmitter need only generate a number of audio tones. Most had a single oscillator, that generated different tones as control buttons were pressed one-by-one. As the control actuators on the model were usually escapements at this time, this limitation was relatively minor. To keep the channels fully independent and \"simultaneously\" triggerable, required a separate oscillator for each channel, not merely a single tunable oscillator. In the valve era before transistors, that would have been unusually expensive. Many period transmitters merely used a number of push-button switches on their case, although some combined these into joystick or wheel controls.\n\nResonant reeds, used as mechanical filters in a radio tone decoder, appear in the early 1930s as part of radio navigation systems. Multiple courses were signalled by use of radio beam transmitters. Tones of 65 Hz, 86.7 Hz, and 108.3 Hz were modulated onto these beam transmissions, the position of the beam and its audio modulation being space modulated onto the ideal position of the course and the guard beam areas to either side of it. By visually monitoring the vibrating reeds, the pilot could determine their position within the radio beams, and thus over the ground.\n\nEarly radio paging systems such as the Bell Telephone BELLBOY system used a shared carrier frequency and audio tone coding to identify the correct recipient of a message. These selectors used a tuning fork resonator rather than a simple single reed. This gives a more selective mechanical filter, allowing more frequencies to be spaced closely together. Even more importantly, the false-triggering harmonic for a tuning fork is more than six times its natural frequency, rather than merely twice its frequency, as for a reed. This means that the useful frequency range is over two octaves, rather than less than one octave. Multiple reeds could also be used together, either to identify separate frequencies to give multiple indications, or logically ANDed together to require more subscriber selections with a 2-code identifier rather than a single code.\n\nVibrating reed indicators have been used for a low-cost display of frequency. This was typically used for a small generator set, where maintaining an output frequency of 50 Hz or 60 Hz was needed. A comb of reeds centred on this frequency would be mounted edge-on to the control panel and the vibrations of the reed with the greatest amplitude could be seen directly.\n\n"}
{"id": "231442", "url": "https://en.wikipedia.org/wiki?curid=231442", "title": "Reference range", "text": "Reference range\n\nIn health-related fields, a reference range or reference interval is the range of values for a physiologic measurement in healthy persons (for example, the amount of creatinine in the blood, or the partial pressure of oxygen). It is a basis for comparison (a frame of reference) for a physician or other health professional to interpret a set of test results for a particular patient. Some important reference ranges in medicine are reference ranges for blood tests and reference ranges for urine tests.\n\nThe standard definition of a reference range (usually referred to if not otherwise specified) originates in what is most prevalent in a reference group taken from the general (i.e. total) population. This is the general reference range. However, there are also \"optimal health ranges\" (ranges that appear to have the optimal health impact) and ranges for particular conditions or statuses (such as pregnancy reference ranges for hormone levels).\n\nValues within the reference range (WRR) are those within the normal distribution and are thus often described as within normal limits (WNL). The limits of the normal distribution are called the \"upper reference limit\" (URL) or \"upper limit of normal\" (ULN) and the \"lower reference limit\" (LRL) or \"lower limit of normal\" (LLN). In health care–related publishing, style sheets sometimes prefer the word \"reference\" over the word \"normal\" to prevent the nontechnical senses of \"normal\" from being conflated with the statistical sense. Values outside a reference range are not necessarily pathologic, and they are not necessarily abnormal in any sense other than statistically. Nonetheless, they are indicators of probable pathosis. Sometimes the underlying cause is obvious; in other cases, challenging differential diagnosis is required to determine what is wrong and thus how to treat it.\n\nA cutoff or threshold is a limit used for binary classification, mainly between normal versus pathological (or probably pathological). Establishment methods for cutoffs include using an upper or a lower limit of a reference range.\n\nThe standard definition of a reference range for a particular measurement is defined as the interval between which 95% of values of a reference population fall into, in such a way that 2.5% of the time a value will be less than the lower limit of this interval, and 2.5% of the time it will be larger than the upper limit of this interval, whatever the distribution of these values.\n\nReference ranges that are given by this definition are sometimes referred as \"standard ranges\".\n\nRegarding the target population, if not otherwise specified, a standard reference range generally denotes the one in healthy individuals, or without any known condition that directly affects the ranges being established. These are likewise established using reference groups from the healthy population, and are sometimes termed \"normal ranges\" or \"normal values\" (and sometimes \"usual\" ranges/values). However, using the term \"normal\" may not be appropriate as not everyone outside the interval is abnormal, and people who have a particular condition may still fall within this interval.\n\nHowever, reference ranges may also be established by taking samples from the whole population, with or without diseases and conditions. In some cases, diseased individuals are taken as the population, establishing reference ranges among those having a disease or condition. Preferably, there should be specific reference ranges for each subgroup of the population that has any factor that affects the measurement, such as, for example, specific ranges for each \"sex\", \"age group\", \"race\" or any other general determinant.\n\nMethods for establishing reference ranges are mainly based on assuming a normal distribution or a log-normal distribution, or directly from percentages of interest, as detailed respectively in following sections.\n\nThe 95% interval, is often estimated by assuming a normal distribution of the measured parameter, in which case it can be defined as the interval limited by 1.96 (often rounded up to 2) population standard deviations from either side of the population mean (also called the expected value).\nHowever, in the real world, neither the population mean nor the population standard deviation are known. They both need to be estimated from a sample, whose size can be designated \"n\". The population standard deviation is estimated by the sample standard deviation and the population mean is estimated by the sample mean (also called mean or arithmetic mean). To account for these estimations, the 95% prediction interval (95% PI) is calculated as:\n\nwhere formula_1 is the 97.5% quantile of a Student's t-distribution with \"n\"−1 degrees of freedom.\n\nWhen the sample size is large (\"n\"≥30) formula_2\n\nThis method is often acceptably accurate if the standard deviation, as compared to the mean, is not very large. A more accurate method is to perform the calculations on logarithmized values, as described in separate section later.\n\nThe following example of this (\"not\" logarithmized) method is based on values of fasting plasma glucose taken from a reference group of 12 subjects:\n\nAs can be given from, for example, a table of selected values of Student's t-distribution, the 97.5% percentile with (12-1) degrees of freedom corresponds to \nformula_3\nSubsequently, the lower and upper limits of the standard reference range are calculated as:\n\nThus, the standard reference range for this example is estimated to be 4.4 to 6.3 mmol/L.\n\nThe 90% \"confidence interval of a standard reference range limit\" as estimated assuming a normal distribution can be calculated by:\n\nwhere SD is the standard deviation, and n is the number of samples.\n\nTaking the example from the previous section, the number of samples is 12 and the standard deviation is 0.42 mmol/L, resulting in:\n\nThus, the lower limit of the reference range can be written as 4.4 (90% CI 4.1-4.7) mmol/L.\n\nLikewise, with similar calculations, the upper limit of the reference range can be written as 6.3 (90% CI 6.0-6.6) mmol/L.\n\nThese confidence intervals reflect random error, but do not compensate for systematic error, which in this case can arise from, for example, the reference group not having fasted long enough before blood sampling.\n\nAs a comparison, actual reference ranges used clinically for fasting plasma glucose are estimated to have a lower limit of approximately 3.8 to 4.0, and an upper limit of approximately 6.0 to 6.1.\n\nIn reality, biological parameters tend to have a log-normal distribution, rather than the arithmetical normal distribution (which is generally referred to as normal distribution without any further specification).\n\nAn explanation for this log-normal distribution for biological parameters is: The event where a sample has half the value of the mean or median tends to have almost equal probability to occur as the event where a sample has twice the value of the mean or median. Also, only a log-normal distribution can compensate for the inability of almost all biological parameters to be of negative numbers (at least when measured on absolute scales), with the consequence that there is no definite limit to the size of outliers (extreme values) on the high side, but, on the other hand, they can never be less than zero, resulting in a positive skewness.\n\nAs shown in diagram at right, this phenomenon has relatively small effect if the standard deviation (as compared to the mean) is relatively small, as it makes the log-normal distribution appear similar to an arithmetical normal distribution. Thus, the arithmetical normal distribution may be more appropriate to use with small standard deviations for convenience, and the log-normal distribution with large standard deviations.\n\nIn a log-normal distribution, the geometric standard deviations and geometric mean more accurately estimate the 95% prediction interval than their arithmetic counterparts.\n\nThe necessity to establish a reference range by log-normal distribution rather than arithmetic normal distribution can be regarded as depending on how much difference it would make to \"not\" do so, which can be described as the ratio:\n\nwhere:\n\nThis difference can be put solely in relation to the coefficient of variation, as in the diagram at right, where:\n\nwhere:\n\nIn practice, it can be regarded as necessary to use the establishment methods of a log-normal distribution if the difference ratio becomes more than 0.1, meaning that a (lower or upper) limit estimated from an assumed arithmetically normal distribution would be more than 10% different from the corresponding limit as estimated from a (more accurate) log-normal distribution. As seen in the diagram, a difference ratio of 0.1 is reached for the lower limit at a coefficient of variation of 0.213 (or 21.3%), and for the upper limit at a coefficient of variation at 0.413 (41.3%). The lower limit is more affected by increasing coefficient of variation, and its \"critical\" coefficient of variation of 0.213 corresponds to a ratio of (upper limit)/(lower limit) of 2.43, so as a rule of thumb, if the upper limit is more than 2.4 times the lower limit when estimated by assuming arithmetically normal distribution, then it should be considered to do the calculations again by log-normal distribution.\n\nTaking the example from previous section, the arithmetic standard deviation (s.d.) is estimated at 0.42 and the arithmetic mean (m) is estimated at 5.33. Thus the coefficient of variation is 0.079. This is less than both 0.213 and 0.413, and thus both the lower and upper limit of fasting blood glucose can most likely be estimated by assuming arithmetically normal distribution. More specifically, the coefficient of variation of 0.079 corresponds to a difference ratio of 0.01 (1%) for the lower limit and 0.007 (0.7%) for the upper limit.\n\nA method to estimate the reference range for a parameter with log-normal distribution is to logarithmize all the measurements with an arbitrary base (for example \"e\"), derive the mean and standard deviation of these logarithms, determine the logarithms located (for a 95% prediction interval) 1.96 standard deviations below and above that mean, and subsequently exponentiate using those two logarithms as exponents and using the same base as was used in logarithmizing, with the two resultant values being the lower and upper limit of the 95% prediction interval.\n\nThe following example of this method is based on the same values of fasting plasma glucose as used in the previous section, using \"e\" as a base:\n\nSubsequently, the still logarithmized lower limit of the reference range is calculated as:\n\nand the upper limit of the reference range as:\n\nConversion back to non-logarithmized values are subsequently performed as:\n\nThus, the standard reference range for this example is estimated to be 4.4 to 6.4.\n\nAn alternative method of establishing a reference range with the assumption of log-normal distribution is to use the arithmetic mean and arithmetic value of standard deviation. This is somewhat more tedious to perform, but may be useful for example in cases where a study that establishes a reference range presents only the arithmetic mean and standard deviation, leaving out the source data. If the original assumption of arithmetically normal distribution is shown to be less appropriate than the log-normal one, then, using the arithmetic mean and standard deviation may be the only available parameters to correct the reference range.\n\nBy assuming that the expected value can represent the arithmetic mean in this case, the parameters \"μ\" and \"σ\" can be estimated from the arithmetic mean (\"m\") and standard deviation (\"s.d.\") as:\n\nFollowing the exampled reference group from the previous section:\n\nSubsequently, the logarithmized, and later non-logarithmized, lower and upper limit are calculated just as by logarithmized sample values.\n\nReference ranges can also be established directly from the 2.5th and 97.5th percentile of the measurements in the reference group. For example, if the reference group consists of 200 people, and counting from the measurement with lowest value to highest, the lower limit of the reference range would correspond to the 5th measurement and the upper limit would correspond to the 195th measurement.\n\nThis method can be used even when measurement values do not appear to conform conveniently to any form of normal distribution or other function.\n\nHowever, the reference range limits as estimated in this way have higher variance, and therefore less reliability, than those estimated by an arithmetic or log-normal distribution (when such is applicable), because the latter ones acquire statistical power from the measurements of the whole reference group rather than just the measurements at the 2.5th and 97.5th percentiles. Still, this variance decreases with increasing size of the reference group, and therefore, this method may be optimal where a large reference group easily can be gathered, and the distribution mode of the measurements is uncertain.\n\nIn case of a bimodal distribution (seen at right), it is useful to find out why this is the case. Two reference ranges can be established for the two different groups of people, making it possible to assume a normal distribution for each group. This bimodal pattern is commonly seen in tests that differ between men and women, such as prostate specific antigen.\n\nIn case of medical tests whose results are of continuous values, reference ranges can be used in the interpretation of an individual test result. This is primarily used for diagnostic tests and screening tests, while monitoring tests may optimally be interpreted from previous tests of the same individual instead.\n\nReference ranges aid in the evaluation of whether a test result's deviation from the mean is a result of random variability or a result of an underlying disease or condition. If the reference group used to establish the reference range can be assumed to be representative of the individual person in a healthy state, then a test result from that individual that turns out to be lower or higher than the reference range can be interpreted as that there is less than 2.5% probability that this would have occurred by random variability in the absence of disease or other condition, which, in turn, is strongly indicative for considering an underlying disease or condition as a cause.\n\nSuch further consideration can be performed, for example, by an epidemiology-based differential diagnostic procedure, where potential candidate conditions are listed that may explain the finding, followed by calculations of how probable they are to have occurred in the first place, in turn followed by a comparison with the probability that the result would have occurred by random variability.\n\nIf the establishment of the reference range could have been made assuming a normal distribution, then the probability that the result would be an effect of random variability can be further specified as follows:\n\nThe standard deviation, if not given already, can be inversely calculated by the fact that the absolute value of the difference between the mean and either the upper or lower limit of the reference range is approximately 2 standard deviations (more accurately 1.96), and thus:\n\nThe standard score for the individual's test can subsequently be calculated as:\n\nThe probability that a value is of a certain distance from the mean can subsequently be calculated from the relation between standard score and prediction intervals. For example, a standard score of 2.58 corresponds to a prediction interval of 99%, corresponding to a probability of 0.5% that a result is at least such far from the mean in the absence of disease.\n\nLet's say, for example, that an individual takes a test that measures the ionized calcium in the blood, resulting in a value of 1.30 mmol/L, and a reference group that appropriately represents the individual has established a reference range of 1.05 to 1.25 mmol/L. The individual's value is higher than the upper limit of the reference range, and therefore has less than 2.5% probability of being a result of random variability, constituting a strong indication to make a differential diagnosis of possible causative conditions.\n\nIn this case, an epidemiology-based differential diagnostic procedure is used, and its first step is to find candidate conditions that can explain the finding.\n\nHypercalcemia (usually defined as a calcium level above the reference range) is mostly caused by either primary hyperparathyroidism or malignancy, and therefore, it is reasonable to include these in the differential diagnosis.\n\nUsing for example epidemiology and the individual's risk factors, let's say that the probability that the hypercalcemia would have been caused by primary hyperparathyroidism in the first place is estimated to be 0.00125 (or 0.125%), the equivalent probability for cancer is 0.0002, and 0.0005 for other conditions. With a probability given as less than 0.025 of no disease, this corresponds to a probability that the hypercalcemia would have occurred in the first place of up to 0.02695. However, the hypercalcemia \"has occurred\" with a probability of 100%, resulting adjusted probabilities of at least 4.6% that primary hyperparathyroidism has caused the hypercalcemia, at least 0.7% for cancer, at least 1.9% for other conditions and up to 92.8% for that there is no disease and the hypercalcemia is caused by random variability.\n\nIn this case, further processing benefits from specification of the probability of random variability:\n\nThe value is assumed to conform acceptably to a normal distribution, so the mean can be assumed to be 1.15 in the reference group. The standard deviation, if not given already, can be inversely calculated by knowing that the absolute value of the difference between the mean and, for example, the upper limit of the reference range, is approximately 2 standard deviations (more accurately 1.96), and thus:\n\nThe standard score for the individual's test is subsequently calculated as:\n\nThe probability that a value is of so much larger value than the mean as having a standard score of 3 corresponds to a probability of approximately 0.14% (given by , with 99.7% here being given from the 68-95-99.7 rule).\n\nUsing the same probabilities that the hypercalcemia would have occurred in the first place by the other candidate conditions, the probability that hypercalcemia would have occurred in the first place is 0.00335, and given the fact that hypercalcemia \"has occurred\" gives adjusted probabilities of 37.3%, 6.0%, 14.9% and 41.8%, respectively, for primary hyperparathyroidism, cancer, other conditions and no disease.\n\n\"Optimal (health) range\" or \"therapeutic target\" (not to be confused with biological target) is a reference range or limit that is based on concentrations or levels that are associated with optimal health or minimal risk of related complications and diseases, rather than the standard range based on normal distribution in the population.\n\nIt may be more appropriate to use for e.g. folate, since approximately 90 percent of North Americans may actually suffer more or less from folate deficiency, but only the 2.5 percent that have the lowest levels will fall below the standard reference range. In this case, the actual folate ranges for optimal health are substantially higher than the standard reference ranges. Vitamin D has a similar tendency. In contrast, for e.g. uric acid, having a level not exceeding the standard reference range still does not exclude the risk of getting gout or kidney stones. Furthermore, for most toxins, the standard reference range is generally lower than the level of toxic effect.\n\nA problem with optimal health range is a lack of a standard method of estimating the ranges. The limits may be defined as those where the health risks exceed a certain threshold, but with various risk profiles between different measurements (such as folate and vitamin D), and even different risk aspects for one and the same measurement (such as both deficiency and toxicity of vitamin A) it is difficult to standardize. Subsequently, optimal health ranges, when given by various sources, have an additional variability caused by various definitions of the parameter. Also, as with standard reference ranges, there should be specific ranges for different determinants that affects the values, such as sex, age etc. Ideally, there should rather be an estimation of what is the optimal value for every individual, when taking all significant factors of that individual into account - a task that may be hard to achieve by studies, but long clinical experience by a physician may make this method more preferable than using reference ranges.\nIn many cases, only one side of the range is usually of interest, such as with markers of pathology including cancer antigen 19-9, where it is generally without any clinical significance to have a value below what is usual in the population. Therefore, such targets are often given with only one limit of the reference range given, and, strictly, such values are rather \"cut-off values\" or \"threshold values\".\n\nThey may represent both standard ranges and optimal health ranges. Also, they may represent an appropriate value to distinguish healthy person from a specific disease, although this gives additional variability by different diseases being distinguished. For example, for NT-proBNP, a lower cut-off value is used in distinguishing healthy babies from those with acyanotic heart disease, compared to the cut-off value used in distinguishing healthy babies from those with congenital nonspherocytic anemia.\n\nFor standard as well as optimal health ranges, and cut-offs, sources of inaccuracy and imprecision include:\n\n\nAlso, reference ranges tend to give the impression of definite thresholds that clearly separate \"good\" or \"bad\" values, while in reality there are generally continuously increasing risks with increased distance from usual or optimal values.\n\nWith this and uncompensated factors in mind, the ideal interpretation method of a test result would rather consist of a comparison of what would be expected or optimal in the individual when taking all factors and conditions of that individual into account, rather than strictly classifying the values as \"good\" or \"bad\" by using reference ranges from other people.\n\nIn a recent paper, Rappoport et al. described a novel way to redefine reference range from an Electronic health record system. In such a system, a higher population resolution can be achieved (e.g., age, sex, race and ethnicity-specific).\n\n\n\n"}
{"id": "57678302", "url": "https://en.wikipedia.org/wiki?curid=57678302", "title": "Serge Galam", "text": "Serge Galam\n\nSerge Galam is a French physicist and the director of research at CNRS.\n\nIn 1975, Serge Galam obtained a PhD in physics at the Pierre and Marie Curie University in Paris. In 1981, he received a Ph.D. in physics at Tel Aviv University. From 1981 to 1983, he taught at City University of New York and from 1983 to 1985 at New York University.\n\nFrom 1984 to 2004, he worked in several physics laboratories of the Pierre and Marie Curie University.\n\nIn 1999, he was appointed director of research at the Centre national de la recherche scientifique.\n\nIn 2004 he joined the Center for Research in Applied Epistemology of the École Polytechnique (CREA). In 2013, he joined the faculty of Sciences Po.\n\nSerge Galam is one of the pioneers of the modern field of sociophysics. His work focuses on the dynamics of group decision making and how minority opinions can sway public opinion. In the fall of 2016, using the principles of sociophysis, he predicted the election of Donald Trump.\n\n"}
{"id": "27646744", "url": "https://en.wikipedia.org/wiki?curid=27646744", "title": "Ship's chronometer from HMS Beagle", "text": "Ship's chronometer from HMS Beagle\n\nA nautical chronometer made by Thomas Earnshaw (1749–1828), and once part of the equipment of HMS \"Beagle\", the ship that carried Charles Darwin on his voyage around the world, is held in the British Museum. The chronometer was the subject of one episode of the BBC's series \"A History of the World in 100 Objects\".\n\nMeticulous naval inventories show that HMS \"Beagle\" carried a total of at least 34 recorded chronometers on its three main survey voyages from 1826 to 1843, and 22 on the second voyage with Darwin on board, when they had a dedicated cabin. Some were Navy property and others were on loan from the manufacturers, as well as six on the second voyage owned by the captain, Robert FitzRoy. Both the two known survivors from the second voyage are owned by the British Museum (the second is registration No. CAI.1743).\n\nNautical chronometers were of great importance in the 18th and 19th centuries as aids to navigation. Accurate measurement of time was needed for the determination of longitude. Earnshaw was not the first to make such chronometers, but he was one of the first to make them cheaply enough that they started to become essential equipment for a ship at sea. By the time the \"Beagle\" set sail, it was being reported in \"The Nautical Magazine\" that the price of chronometers was dropping rapidly while the same quality was being maintained. Earnshaws' chronometer had a novel escapement mechanism, the spring detent escapement, and a bimetallic strip for temperature compensation so that it would continue to maintain accuracy in all climates around the world.\n\nThe \"Beagle\" was sent in 1831 on a survey mission which involved circumnavigating the globe, a journey which lasted until 1836 and described by the naturalist on board the ship, Charles Darwin, in his book \"The Voyage of the Beagle\". It was on this journey that Darwin began forming the ideas published much later as \"On the Origin of Species\". The \"Beagle\" carried twenty-two chronometers, an unusually large number, but necessary to ensure accuracy of the survey. Three would have been commonplace on ships of the time, as this is the minimum number required to easily identify one that has gone faulty. The Admiralty started a general issue of chronometers to H. M. Ships from 1825, but between about 1800 and 1840 availability could not keep up with demand. The Admiralty therefore only issued one chronometer to each ship unless the Captain personally owned one. In those cases the Admiralty would issue another to take the total to three, reasoning that a ship with two was no better off than with one since a faulty instrument could not be identified.\n\nThe \"Beagle\" however, would be gone for several years and was required to take chronometers ashore and in boat expeditions up rivers to determine the coordinates of specific reference points as instructed by the Admiralty. It could not be guaranteed that any one chronometer would continue to function accurately, or even survive the journey at all. Each chronometer was mounted on gimbals to keep it level in all sea conditions, and the whole assembly fixed inside a hinged wooden box for protection. For additional protection, they were stored in sawdust in a special cabin in the Captain's quarters. Only crew who needed to take measurements, or who maintained them, were allowed access, measures which indicate the importance attached to these instruments. The \"Beagle\" voyage succeeded, for the first time, in establishing a linked chain of reference points around the globe of known longitude which could be used by subsequent voyages to calibrate their own chronometers.\n\nThe chronometers were maintained by an instrument maker, one George James Stebbing, whose salary was paid for personally by the captain of the vessel, Robert FitzRoy. FitzRoy considered the post to be essential to the mission but the Admiralty had refused to pay for it. FitzRoy bore the cost himself, as he did for much of the ship's equipment, but the Admiralty did concede that Stebbing could be fed from the ship's rations. This concession was not extended to Darwin, who paid £500 for his own keep.\n\nBy using his chronometers to measure the time of local noon when he returned to his home port Fitzroy was able to measure the overall accuracy of the entire voyage. As he sailed west, local noon occurred progressively later, until finally, when he had circumnavigated the globe, the shift in local noon time, as measured by his chronometers should be exactly twenty-four hours. In fact, Fitzroy's measurements exceeded this by 33 seconds, which is equivalent to just . This was impressive for a journey of tens of thousands of miles over five years; nevertheless Fitzroy considered the error to be inexplicably large.\n\nThomas Earnshaw's \"Marine Chronometer No.509\" was manufactured around 1800 and served on a number of Royal Navy ships. William Edward Parry while exploring Baffin Bay in July 1819 during his first attempt to find the Northwest Passage mentions that Earnshaw's chronometer had been used in 1818 to determine the longitude of a spot to within of his own measurement. He does not, however, say which ship it was on board. At this early time, all Royal Navy purchased chronometers were issued by the Greenwich Observatory who also checked their rates and sent them out for cleaning between voyages. Initially, Greenwich issued chronometers directly to ships, but later, as chronometers became more common, they were sent from Greenwich to other Royal Navy ports and dockyards for issue locally. The first recorded issue from Greenwich was 3 July 1823 to Captain Frederick Marryat in command of HMS \"Larne\". \"Larne\" took part in the First Anglo-Burmese War which lasted from 1824 to 1826.\n\nThe chronometer was returned to Greenwich 6 February 1826 and then sent to its makers for servicing. It did not come back to Greenwich for nearly two years. On 4 March 1828 it was issued to Captain J. Bolder in command of HMS \"Hecla\". At this time \"Hecla\" was a famous ship: under the command of George Francis Lyon she had been part of Parry's second expedition to find the Northwest Passage. Over 6,000 members of the public visited the ship at Deptford while she waited to set out on Parry's third expedition in 1824. The attempt ended in 1825 after the leading ship, HMS \"Fury\" was abandoned due to ice damage. Parry used \"Hecla\" again in an 1827 attempt to reach the North Pole. Parry unwillingly gave up \"Hecla\" when the admiralty sent her to survey the West African coast under the command of Bolder. In July 1830 the chronometer was returned from \"Hecla\" and went for cleaning to Robert Molyneux in London. It was returned to Greenwich in November but not immediately issued to another ship.\n\nIn March 1831 the chronometer was delivered to Devonport where the rate was checked and recorded. On 6 December 1831 it was issued to Captain Stokes in command of HMS \"Beagle\". It sailed with Captain Fitzroy on \"Beagle\"'s famous second voyage and was returned to Greenwich 7 November 1836. After a period at Arnold and Dent for cleaning it was transported by HMS \"Lightning\" to Devonport or Portsmouth for issue to ships there.\n\nIt was returned to Greenwich 17 November 1841 and after another service by Arnold and Dent was issued to HMS \"Formidable\" on 13 January 1842. It stayed with \"Formidable\" until 20 November 1845 when it was returned to Greenwich. It was then serviced by Charles Frodsham and sent for issue to ships at Portsmouth. It was transported back to Greenwich periodically by Royal Navy ships for service by Frodsham (HMS \"Rattlesnake\" 11 March 1850, HMS \"Odin\" 27 December 1854) and finally returned from Portsmouth 7 May 1857. The final ship to be issued the chronometer was HMS \"Pembroke\" who received it 20 April 1858. \"Pembroke\" returned the chronometer to Greenwich 1 February 1867.\n\nAfter serving on \"Pembroke\" the chronometer stayed with Frodsham for over six months. Greenwich issued it to the Meteorological Committee of the Royal Society on 25 November 1867. The Meteorological Committee used the chronometer for observations at Falmouth Observatory. It was returned to Greenwich on 3 November 1886 and sent to J Poole for servicing 22 November 1886. Poole returned the chronometer on 13 December 1886 declaring it to be beyond economic repair. It was given to E. Dent & Co. on 16 July 1888 in part exchange for the chronometer Dent 43107. It was later acquired by the private collector Courtenay Adrian Ilbert. After his death in 1956, Ilbert's collection was put up for auction in 1958. The auction was cancelled, however, and the collection purchased by the British Museum following a private donation of funds.\n\nThe chronometer was object 91 in the BBC Radio 4 series \"A History of the World in 100 Objects\", first broadcast 11 October 2010. The series was made in collaboration with the British Museum and was presented by Neil MacGregor, the Director of the British Museum. The specialist contributors to the chronometer episode were Nigel Thrift, the vice-chancellor of the University of Warwick, and Steve Jones, geneticist and television presenter. The programme discussed the search for longitude, the role chronometers played in this, Earnshaw's contributions to chronometer design, the voyage of the \"Beagle\", and the importance of the chronometers she carried.\n\n\n\n"}
{"id": "42889989", "url": "https://en.wikipedia.org/wiki?curid=42889989", "title": "Simband", "text": "Simband\n\nSimband is a research platform announced by Samsung in 2014. It was announced on May 28, 2014, in San Francisco, in partnership with UCSF Digital Health Innovation Lab.\n\nSimband is an open developer platform consisting of a watch unit running Tizen and a wristband connector that holds a custom sensor module.\n\nSimband is designed to be modular and allow for different sensor modules to be installed. Samsung provides a reference implementation of a sensor module called Simsense that supports multiple sensors, each generating a unique data stream. In November 2014 Samsung announced two custom modules developed by third parties.\n"}
{"id": "28743", "url": "https://en.wikipedia.org/wiki?curid=28743", "title": "Slide rule", "text": "Slide rule\n\nThe slide rule, also known colloquially in the United States as a slipstick, is a mechanical analog computer. The slide rule is used primarily for multiplication and division, and also for functions such as exponents, roots, logarithms and trigonometry, but typically not for addition or subtraction. Though similar in name and appearance to a standard ruler, the slide rule is not meant to be used for measuring length or drawing straight lines.\n\nSlide rules exist in a diverse range of styles and generally appear in a linear or circular form with a standardized set of markings (scales) essential to performing mathematical computations. Slide rules manufactured for specialized fields such as aviation or finance typically feature additional scales that aid in calculations common to those fields.\n\nAt its simplest, each number to be multiplied is represented by a length on a sliding ruler. As the rulers each have a logarithmic scale, it is possible to align them to read the sum of the logarithms, and hence calculate the product of the two numbers.\n\nThe Reverend William Oughtred and others developed the slide rule in the 17th century based on the emerging work on logarithms by John Napier. Before the advent of the electronic calculator, it was the most commonly used calculation tool in science and engineering. The use of slide rules continued to grow through the 1950s and 1960s even as computers were being gradually introduced; but around 1974 the handheld electronic scientific calculator made them largely obsolete and most suppliers left the business.\n\nIn its most basic form, the slide rule uses two logarithmic scales to allow rapid multiplication and division of numbers. These common operations can be time-consuming and error-prone when done on paper. More elaborate slide rules allow other calculations, such as square roots, exponentials, logarithms, and trigonometric functions.\n\nScales may be grouped in decades, which are numbers ranging from 1 to 10 (i.e. 10 to 10). Thus single decade scales C and D range from 1 to 10 across the entire width of the slide rule while double decade scales A and B range from 1 to 100 over the width of the slide rule.\n\nIn general, mathematical calculations are performed by aligning a mark on the sliding central strip with a mark on one of the fixed strips, and then observing the relative positions of other marks on the strips. Numbers aligned with the marks give the approximate value of the product, quotient, or other calculated result.\n\nThe user determines the location of the decimal point in the result, based on mental estimation. Scientific notation is used to track the decimal point in more formal calculations. Addition and subtraction steps in a calculation are generally done mentally or on paper, not on the slide rule.\n\nMost slide rules consist of three linear strips of the same length, aligned in parallel and interlocked so that the central strip can be moved lengthwise relative to the other two. The outer two strips are fixed so that their relative positions do not change.\n\nSome slide rules (\"duplex\" models) have scales on both sides of the rule and slide strip, others on one side of the outer strips and both sides of the slide strip (which can usually be pulled out, flipped over and reinserted for convenience), still others on one side only (\"simplex\" rules). A sliding with a vertical alignment line is used to find corresponding points on scales that are not adjacent to each other or, in duplex models, are on the other side of the rule. The cursor can also record an intermediate result on any of the scales.\n\nA logarithm transforms the operations of multiplication and division to addition and subtraction according to the rules formula_1 and formula_2.\nMoving the top scale to the right by a distance of formula_3, by matching the beginning of the top scale with the label formula_4 on the bottom, aligns each number formula_5, at position formula_6 on the top scale, with the number at position formula_7 on the bottom scale. Because formula_8, this position on the bottom scale gives formula_9, the product of formula_4 and formula_5. For example, to calculate 3×2, the 1 on the top scale is moved to the 2 on the bottom scale. The answer, 6, is read off the bottom scale where 3 is on the top scale. In general, the 1 on the top is moved to a factor on the bottom, and the answer is read off the bottom where the other factor is on the top. This works because the distances from the \"1\" are proportional to the logarithms of the marked values:\n\nOperations may go \"off the scale;\" for example, the diagram above shows that the slide rule has not positioned the 7 on the upper scale above any number on the lower scale, so it does not give any answer for 2×7. In such cases, the user may slide the upper scale to the left until its right index aligns with the 2, effectively dividing by 10 (by subtracting the full length of the C-scale) and then multiplying by 7, as in the illustration below:\n\nHere the user of the slide rule must remember to adjust the decimal point appropriately to correct the final answer. We wanted to find 2×7, but instead we calculated (2/10)×7=0.2×7=1.4. So the true answer is not 1.4 but 14. Resetting the slide is not the only way to handle multiplications that would result in off-scale results, such as 2×7; some other methods are:\n\nMethod 1 is easy to understand, but entails a loss of precision. Method 3 has the advantage that it only involves two scales.\n\nThe illustration below demonstrates the computation of 5.5/2. The 2 on the top scale is placed over the 5.5 on the bottom scale. The 1 on the top scale lies above the quotient, 2.75. There is more than one method for doing division, but the method presented here has the advantage that the final result cannot be off-scale, because one has a choice of using the 1 at either end.\n\nIn addition to the logarithmic scales, some slide rules have other mathematical functions encoded on other auxiliary scales. The most popular were trigonometric, usually sine and tangent, common logarithm (log) (for taking the log of a value on a multiplier scale), natural logarithm (ln) and exponential (\"e\") scales. Some rules include a Pythagorean scale, to figure sides of triangles, and a scale to figure circles. Others feature scales for calculating hyperbolic functions. On linear rules, the scales and their labeling are highly standardized, with variation usually occurring only in terms of which scales are included and in what order:\n\nThe Binary Slide Rule manufactured by Gilson in 1931 performed an addition and subtraction function limited to fractions.\n\nThere are single-decade (C and D), double-decade (A and B), and triple-decade (K) scales. To compute formula_12, for example, locate x on the D scale and read its square on the A scale. Inverting this process allows square roots to be found, and similarly for the powers 3, 1/3, 2/3, and 3/2. Care must be taken when the base, x, is found in more than one place on its scale. For instance, there are two nines on the A scale; to find the square root of nine, use the first one; the second one gives the square root of 90.\n\nFor formula_13 problems, use the LL scales. When several LL scales are present, use the one with \"x\" on it. First, align the leftmost 1 on the C scale with x on the LL scale. Then, find \"y\" on the C scale and go down to the LL scale with \"x\" on it. That scale will indicate the answer. If \"y\" is \"off the scale,\" locate formula_14 and square it using the A and B scales as described above. Alternatively, use the rightmost 1 on the C scale, and read the answer off the next higher LL scale. For example, aligning the rightmost 1 on the C scale with 2 on the LL2 scale, 3 on the C scale lines up with 8 on the LL3 scale.\n\nTo extract a cube root using a slide rule with only C/D and A/B scales, align 1 on the B cursor with the base number on the A scale (taking care as always to distinguish between the lower and upper halves of the A scale). Slide the cursor until the number on the D scale which is against 1 on the C cursor is the same as the number on the B cursor which is against the base number on the A scale. (Examples: A 8, B 2, C 1, D 2; A 27, B 3, C 1, D 3.)\n\nThe S, T, and ST scales are used for trig functions and multiples of trig functions, for angles in degrees.\n\nFor angles from around 5.7 up to 90 degrees, sines are found by comparing the S scale with C (or D) scale; though on many closed-body rules the S scale relates to the A scale instead, and what follows must be adjusted appropriately. The S scale has a second set of angles (sometimes in a different color), which run in the opposite direction, and are used for cosines. Tangents are found by comparing the T scale with the C (or D) scale for angles less than 45 degrees. For angles greater than 45 degrees the CI scale is used. Common forms such as formula_15 can be read directly from \"x\" on the S scale to the result on the D scale, when the C-scale index is set at \"k\". For angles below 5.7 degrees, sines, tangents, and radians are approximately equal, and are found on the ST or SRT (sines, radians, and tangents) scale, or simply divided by 57.3 degrees/radian. Inverse trigonometric functions are found by reversing the process.\n\nMany slide rules have S, T, and ST scales marked with degrees and minutes (e.g. some Keuffel and Esser models, late-model Teledyne-Post Mannheim-type rules). So-called \"decitrig\" models use decimal fractions of degrees instead.\n\nBase-10 logarithms and exponentials are found using the L scale, which is linear. Some slide rules have a Ln scale, which is for base e. Logarithms to any other base can be calculated by reversing the procedure for calculating powers of a number. For example, log2 values can be determined by lining up either leftmost or rightmost 1 on the C scale with 2 on the LL2 scale, finding the number whose logarithm is to be calculated on the corresponding LL scale, and reading the log2 value on the C scale.\n\nSlide rules are not typically used for addition and subtraction, but it is nevertheless possible to do so using two different techniques.\n\nThe first method to perform addition and subtraction on the C and D (or any comparable scales) requires converting the problem into one of division. For addition, the quotient of the two variables plus one times the divisor equals their sum:\n\nFor subtraction, the quotient of the two variables minus one times the divisor equals their difference:\n\nThis method is similar to the addition/subtraction technique used for high-speed electronic circuits with the logarithmic number system in specialized computer applications like the Gravity Pipe (GRAPE) supercomputer and hidden Markov models.\n\nThe second method utilizes a sliding linear L scale available on some models. Addition and subtraction are performed by sliding the cursor left (for subtraction) or right (for addition) then returning the slide to 0 to read the result.\n\nThe width of the slide rule is quoted in terms of the nominal width of the scales. Scales on the most common \"10-inch\" models are actually 25 cm, as they were made to metric standards, though some rules offer slightly extended scales to simplify manipulation when a result overflowed. Pocket rules are typically 5 inches. Models a couple of metres wide were sold to be hung in classrooms for teaching purposes.\n\nTypically the divisions mark a scale to a precision of two significant figures, and the user estimates the third figure. Some high-end slide rules have magnifier cursors that make the markings easier to see. Such cursors can effectively double the accuracy of readings, permitting a 10-inch slide rule to serve as well as a 20-inch.\n\nVarious other conveniences have been developed. Trigonometric scales are sometimes dual-labeled, in black and red, with complementary angles, the so-called \"Darmstadt\" style. Duplex slide rules often duplicate some of the scales on the back. Scales are often \"split\" to get higher accuracy.\n\nCircular slide rules come in two basic types, one with two cursors, and another with a free dish and one cursor. The dual cursor versions perform multiplication and division by holding a fast angle between the cursors as they are rotated around the dial. The onefold cursor version operates more like the standard slide rule through the appropriate alignment of the scales.\n\nThe basic advantage of a circular slide rule is that the widest dimension of the tool was reduced by a factor of about 3 (i.e. by π). For example, a 10 cm circular would have a maximum precision approximately equal to a 31.4 cm ordinary slide rule. Circular slide rules also eliminate \"off-scale\" calculations, because the scales were designed to \"wrap around\"; they never have to be reoriented when results are near 1.0—the rule is always on scale. However, for non-cyclical non-spiral scales such as S, T, and LL's, the scale width is narrowed to make room for end margins.\n\nCircular slide rules are mechanically more rugged and smoother-moving, but their scale alignment precision is sensitive to the centering of a central pivot; a minute 0.1 mm off-centre of the pivot can result in a 0.2 mm worst case alignment error. The pivot, however, does prevent scratching of the face and cursors. The highest accuracy scales are placed on the outer rings. Rather than \"split\" scales, high-end circular rules use spiral scales for more complex operations like log-of-log scales. One eight-inch premium circular rule had a 50-inch spiral log-log scale. Around 1970, an inexpensive model from B. C. Boykin (Model 510) featured 20 scales, including 50-inch C-D (multiplication) and log scales. The RotaRule featured a friction brake for the cursor.\n\nThe main disadvantages of circular slide rules are the difficulty in locating figures along a dish, and limited number of scales. Another drawback of circular slide rules is that less-important scales are closer to the center, and have lower precisions. Most students learned slide rule use on the linear slide rules, and did not find reason to switch.\n\nOne slide rule remaining in daily use around the world is the E6B. This is a circular slide rule first created in the 1930s for aircraft pilots to help with dead reckoning. With the aid of scales printed on the frame it also helps with such miscellaneous tasks as converting time, distance, speed, and temperature values, compass errors, and calculating fuel use. The so-called \"prayer wheel\" is still available in flight shops, and remains widely used. While GPS has reduced the use of dead reckoning for aerial navigation, and handheld calculators have taken over many of its functions, the E6B remains widely used as a primary or backup device and the majority of flight schools demand that their students have some degree of proficiency in its use.\n\nProportion wheels are simple circular slide rules used in graphic design to calculate aspect ratios. Lining up the original and desired size values on the inner and outer wheels will display their ratio as a percentage in a small window. They are not as common since the advent of computerized layout, but \n\nIn 1952, Swiss watch company Breitling introduced a pilot's wristwatch with an integrated circular slide rule specialized for flight calculations: the Breitling Navitimer. The Navitimer circular rule, referred to by Breitling as a \"navigation computer\", featured airspeed, rate/time of climb/descent, flight time, distance, and fuel consumption functions, as well as kilometer—nautical mile and gallon—liter fuel amount conversion functions.\nThere are two main types of cylindrical slide rules: those with helical scales such as the Fuller, the Otis King and the Bygrave slide rule, and those with bars, such as the Thacher and some Loga models. In either case, the advantage is a much longer scale, and hence potentially greater precision, than afforded by a straight or circular rule.\nTraditionally slide rules were made out of hard wood such as mahogany or boxwood with cursors of glass and metal. At least one high precision instrument was made of steel.\n\nIn 1895, a Japanese firm, Hemmi, started to make slide rules from bamboo, which had the advantages of being dimensionally stable, strong, and naturally self-lubricating. These bamboo slide rules were introduced in Sweden in September, 1933, and probably only a little earlier in Germany. Scales were made of celluloid, plastic, or painted aluminium. Later cursors were acrylics or polycarbonates sliding on Teflon bearings.\n\nAll premium slide rules had numbers and scales engraved, and then filled with paint or other resin. Painted or imprinted slide rules were viewed as inferior because the markings could wear off. Nevertheless, Pickett, probably America's most successful slide rule company, made all printed scales. Premium slide rules included clever catches so the rule would not fall apart by accident, and bumpers to protect the scales and cursor from rubbing on tabletops. The recommended cleaning method for engraved markings is to scrub lightly with steel-wool. For painted slide rules, use diluted commercial window-cleaning fluid and a soft cloth.\n\nThe slide rule was invented around 1620–1630, shortly after John Napier's publication of the concept of the logarithm. In 1620 Edmund Gunter of Oxford developed a calculating device with a single logarithmic scale; with additional measuring tools it could be used to multiply and divide. In c. 1622, William Oughtred of Cambridge combined two handheld Gunter rules to make a device that is recognizably the modern slide rule. Like his contemporary at Cambridge, Isaac Newton, Oughtred taught his ideas privately to his students. Also like Newton, he became involved in a vitriolic controversy over priority, with his one-time student Richard Delamain and the prior claims of Wingate. Oughtred's ideas were only made public in publications of his student William Forster in 1632 and 1653.\n\nIn 1677, Henry Coggeshall created a two-foot folding rule for timber measure, called the Coggeshall slide rule, expanding the slide rule's use beyond mathematical inquiry.\n\nIn 1722, Warner introduced the two- and three-decade scales, and in 1755 Everard included an inverted scale; a slide rule containing all of these scales is usually known as a \"polyphase\" rule.\n\nIn 1815, Peter Mark Roget invented the log log slide rule, which included a scale displaying the logarithm of the logarithm. This allowed the user to directly perform calculations involving roots and exponents. This was especially useful for fractional powers.\n\nIn 1821, Nathaniel Bowditch, described in the \"American Practical Navigator\" a \"sliding rule\" that contained scales trigonometric functions on the fixed part and a line of log-sines and log-tans on the slider used to solve navigation problems.\n\nIn 1845, Paul Cameron of Glasgow introduced a nautical slide rule capable of answering navigation questions, including right ascension and declination of the sun and principal stars.\n\nA more modern form of slide rule was created in 1859 by French artillery lieutenant Amédée Mannheim, \"who was fortunate in having his rule made by a firm of national reputation and in having it adopted by the French Artillery.\" It was around this time that engineering became a recognized profession, resulting in widespread slide rule use in Europe–but not in the United States. There, Edwin Thacher's cylindrical rule took hold after 1881. The duplex rule was invented by William Cox in 1891, and was produced by Keuffel and Esser Co. of New York.\n\nAstronomical work also required precise computations, and, in 19th-century Germany, a steel slide rule about two meters long was used at one observatory. It had a microscope attached, giving it accuracy to six decimal places..\n\nThroughout the 1950s and 1960s, the slide rule was the symbol of the engineer's profession in the same way the stethoscope is that of the medical profession.\n\nGerman rocket scientist Wernher von Braun bought two \"Nestler\" slide rules in the 1930s. Ten years later he brought them with him when he moved to the U.S. after World War II to work on the American space effort. Throughout his life he never used any other slide rule. He used his two Nestlers while heading the NASA program that landed a man on the moon in July 1969.\n\nAluminium Pickett-brand slide rules were carried on Project Apollo space missions. The model N600-ES owned by Buzz Aldrin that flew with him to the moon on Apollo 11 was sold at auction in 2007. The model N600-ES taken along on Apollo 13 in 1970 is owned by the National Air and Space Museum.\n\nSome engineering students and engineers carried ten-inch slide rules in belt holsters, a common sight on campuses even into the mid-1970s. Until the advent of the pocket digital calculator, students also might keep a ten- or twenty-inch rule for precision work at home or the office while carrying a five-inch pocket slide rule around with them.\n\nIn 2004, education researchers David B. Sher and Dean C. Nataro conceived a new type of slide rule based on \"prosthaphaeresis\", an algorithm for rapidly computing products that predates logarithms. However, there has been little practical interest in constructing one beyond the initial prototype.\n\nSlide rules have often been specialized to varying degrees for their field of use, such as excise, proof calculation, engineering, navigation, etc., but some slide rules are extremely specialized for very narrow applications. For example, the John Rabone & Sons 1892 catalog lists a \"Measuring Tape and Cattle Gauge\", a device to estimate the weight of a cow from its measurements.\n\nThere were many specialized slide rules for photographic applications; for example, the actinograph of Hurter and Driffield was a two-slide boxwood, brass, and cardboard device for estimating exposure from time of day, time of year, and latitude.\n\nSpecialized slide rules were invented for various forms of engineering, business and banking. These often had common calculations directly expressed as special scales, for example loan calculations, optimal purchase quantities, or particular engineering equations. For example, the Fisher Controls company distributed a customized slide rule adapted to solving the equations used for selecting the proper size of industrial flow control valves.\n\nPilot balloon slide rules were used by meteorologists in weather services to determine the upper wind velocities from an ascending hydrogen or helium filled pilot balloon.\n\nIn World War II, bombardiers and navigators who required quick calculations often used specialized slide rules. One office of the U.S. Navy actually designed a generic slide rule \"chassis\" with an aluminium body and plastic cursor into which celluloid cards (printed on both sides) could be placed for special calculations. The process was invented to calculate range, fuel use and altitude for aircraft, and then adapted to many other purposes.\n\nThe E6-B is a circular slide rule used by pilots and navigators.\n\nCircular slide rules to estimate ovulation dates and fertility are known as \"wheel calculators\".\n\nThe importance of the slide rule began to diminish as electronic computers, a new but rare resource in the 1950s, became more widely available to technical workers during the 1960s. (See History of computing hardware (1960s–present).)\n\nAnother step away from slide rules was the introduction of relatively inexpensive electronic desktop scientific calculators. The first included the Wang Laboratories LOCI-2, introduced in 1965, which used logarithms for multiplication and division; and the Hewlett-Packard HP 9100A, introduced in 1968. Both of these were programmable and provided exponential and logarithmic functions; the HP had trigonometric functions (sine, cosine, and tangent) and hyperbolic trigonometric functions as well. The HP used the CORDIC (coordinate rotation digital computer) algorithm, which allows for calculation of trigonometric functions using only shift and add operations. This method facilitated the development of ever smaller scientific calculators.\n\nAs with mainframe computing, the availability of these machines did not significantly affect the ubiquitous use of the slide rule until cheap hand held scientific electronic calculators became available in the mid-1970s, at which point, it rapidly declined. \nThe pocket-sized Hewlett-Packard HP-35 scientific calculator was the first handheld device of its type, but it cost US$395 in 1972. This was justifiable for some engineering professionals but too expensive for most students. \nBy 1975, basic four-function electronic calculators could be purchased for less than $50, and, by 1976, the TI-30 scientific calculator was sold for less than $25.\n\nMost people find slide rules difficult to learn and use. Even during their heyday, they never caught on with the general public. Addition and subtraction are not well-supported operations on slide rules and doing a calculation on a slide rule tends to be slower than on a calculator. This led engineers to use mathematical equations that favored operations that were easy on a slide rule over more accurate but complex functions, these approximations could lead to inaccuracies and mistakes. On the other hand, the spatial, manual operation of slide rules cultivates in the user an intuition for numerical relationships and scale that people who have used only digital calculators often lack. A slide rule will also display all the terms of a calculation along with the result, thus eliminating uncertainty about what calculation was actually performed.\n\nA slide rule requires the user to separately compute the order of magnitude of the answer in order to position the decimal point in the results. For example, 1.5 × 30 (which equals 45) will show the same result as 1,500,000 × 0.03 (which equals 45,000). This separate calculation is less likely to lead to extreme calculation errors, but forces the user to keep track of magnitude in short-term memory (which is error-prone), keep notes (which is cumbersome) or reason about it in every step (which distracts from the other calculation requirements).\n\nThe typical arithmetic precision of a slide rule is about three significant digits, compared to many digits on digital calculators. As order of magnitude gets the greatest prominence when using a slide rule, users are less likely to make errors of false precision.\n\nWhen performing a sequence of multiplications or divisions by the same number, the answer can often be determined by merely glancing at the slide rule without any manipulation. This can be especially useful when calculating percentages (e.g. for test scores) or when comparing prices (e.g. in dollars per kilogram). Multiple speed-time-distance calculations can be performed hands-free at a glance with a slide rule. Other useful linear conversions such as pounds to kilograms can be easily marked on the rule and used directly in calculations.\n\nBeing entirely mechanical, a slide rule does not depend on grid electricity or batteries. However, mechanical imprecision in slide rules that were poorly constructed or warped by heat or use will lead to errors.\n\nMany sailors keep slide rules as backups for navigation in case of electric failure or battery depletion on long route segments. Slide rules are still commonly used in aviation, particularly for smaller planes. They are being replaced only by integrated, special purpose and expensive flight computers, and not general-purpose calculators. The E6B circular slide rule used by pilots has been in continuous production and remains available in a variety of models. Some wrist watches designed for aviation use still feature slide rule scales to permit quick calculations. The Citizen Skyhawk AT is a notable example.\n\nEven today, some people prefer a slide rule over an electronic calculator as a practical computing device. Others keep their old slide rules out of a sense of nostalgia, or collect them as a hobby.\n\nA popular collectible model is the Keuffel & Esser \"Deci-Lon\", a premium scientific and engineering slide rule available both in a ten-inch (25 cm) \"regular\" (\"Deci-Lon 10\") and a five-inch \"pocket\" (\"Deci-Lon 5\") variant. Another prized American model is the eight-inch (20 cm) Scientific Instruments circular rule. Of European rules, Faber-Castell's high-end models are the most popular among collectors.\n\nAlthough a great many slide rules are circulating on the market, specimens in good condition tend to be expensive. Many rules found for sale on are damaged or have missing parts, and the seller may not know enough to supply the relevant information. Replacement parts are scarce, expensive, and generally available only for separate purchase on individual collectors' web sites. The Keuffel and Esser rules from the period up to about 1950 are particularly problematic, because the end-pieces on the cursors, made of celluloid, tend to chemically break down over time.\n\nThere are still a handful of sources for brand new slide rules. The Concise Company of Tokyo, which began as a manufacturer of circular slide rules in July 1954, continues to make and sell them today. In September 2009, on-line retailer ThinkGeek introduced its own brand of straight slide rules, described as \"faithful replica[s]\" that are \"individually hand tooled\". These are no longer available in 2012. In addition, Faber-Castell has a number of slide rules still in inventory, available for international purchase through their web store. Proportion wheels are still used in graphic design.\n\nVarious slide rule simulator apps are available for Android and iOS-based smart phones and tablets.\n\nSpecialized slide rules such as the E6B used in aviation, and gunnery slide rules used in laying artillery are still used though no longer on a routine basis. These rules are used as part of the teaching and instruction process as in learning to use them the student also learns about the principles behind the calculations, it also allows the student to be able to use these instruments as a back up in the event that the modern electronics in general use fail.\n\n"}
{"id": "993536", "url": "https://en.wikipedia.org/wiki?curid=993536", "title": "Strong programme", "text": "Strong programme\n\nThe strong programme or strong sociology is a variety of the sociology of scientific knowledge (SSK) particularly associated with David Bloor, Barry Barnes, Harry Collins, Donald A. MacKenzie, and John Henry. The strong programme's influence on Science and Technology Studies is credited as being unparalleled (Latour 1999). The largely Edinburgh-based school of thought has illustrated how the existence of a scientific community, bound together by allegiance to a shared paradigm, is a prerequisite for normal scientific activity.\n\nThe strong programme is a reaction against \"weak\" sociologies of science, which restricted the application of sociology to \"failed\" or \"false\" theories, such as phrenology. Failed theories would be explained by citing the researchers' biases, such as covert political or economic interests. Sociology would be only marginally relevant to successful theories, which succeeded because they had revealed a true fact of nature. The strong programme proposed that both \"true\" and \"false\" scientific theories should be treated the same way. Both are caused by social factors or conditions, such as cultural context and self-interest. All human knowledge, as something that exists in the human cognition, must contain some social components in its formation process.\n\nAs formulated by David Bloor, the strong programme has four indispensable components:\n\nBecause the strong programme originated at the 'Science Studies Unit,' University of Edinburgh, it is sometimes termed the Edinburgh School. However, there is also a Bath School associated with Harry Collins that makes similar proposals. In contrast to the Edinburgh School, which emphasizes historical approaches, the Bath School emphasizes microsocial studies of laboratories and experiments. The Bath school, however, does depart from the strong programme on some fundamental issues. In the social construction of technology (SCOT) approach developed by Collins' student Trevor Pinch, as well as by the Dutch sociologist Wiebe Bijker, the strong programme was extended to technology. There are SSK-influenced scholars working in science and technology studies programs throughout the world.\n\nIn order to study scientific knowledge from a sociological point of view, the strong programme has adhered to a form of radical relativism. In other words, it argues that – in the social study of institutionalised beliefs about \"truth\" – it would be unwise to use \"truth\" as an explanatory resource. That would be to include the answer as part of the question (Barnes 1992), not to mention a thoroughly \"whiggish\" approach towards the study of history – that is an approach seeing human history as an inevitable march towards truth and enlightenment. Alan Sokal has criticised radical relativism as part of the science wars, on the basis that such an understanding will lead inevitably towards solipsism and postmodernism. Markus Seidel attacks the main arguments – underdetermination and norm-circularity – provided by Strong Programme proponents for their relativism. Strong programme scholars insist that their approach has been misunderstood by such a criticism and that its adherence to radical relativism is strictly methodological.\n\n\n"}
{"id": "448850", "url": "https://en.wikipedia.org/wiki?curid=448850", "title": "Urban economics", "text": "Urban economics\n\nUrban economics is broadly the economic study of urban areas; as such, it involves using the tools of economics to analyze urban issues such as crime, education, public transit, housing, and local government finance. More narrowly, it is a branch of microeconomics that studies urban spatial structure and the location of households and firms .\n\nMuch urban economic analysis relies on a particular model of urban spatial structure, the monocentric city model pioneered in the 1960s by William Alonso, Richard Muth, and Edwin Mills. While most other forms of neoclassical economics do not account for spatial relationships between individuals and organizations, urban economics focuses on these spatial relationships to understand the economic motivations underlying the formation, functioning, and development of cities.\n\nSince its formulation in 1964, Alonso's monocentric city model of a disc-shaped Central Business District (CBD) and surrounding residential region has served as a starting point for urban economic analysis. Monocentricity has weakened over time because of changes in technology, particularly, faster and cheaper transportation (which makes it possible for commuters to live farther from their jobs in the CBD) and communications (which allow back-office operations to move out of the CBD).\n\nAdditionally, recent research has sought to explain the polycentricity described in Joel Garreau's Edge City. Several explanations for polycentric expansion have been proposed and summarized in models that account for factors such as utility gains from lower average land rents and increasing (or constant returns) due to economies of agglomeration .\n\nUrban economics is rooted in the location theories of von Thünen, Alonso, Christaller, and Lösch that began the process of spatial economic analysis . Economics is the study of the allocation of scarce resources, and as all economic phenomena take place within a geographical space, urban economics focuses on the allocation of resources across space in relation to urban areas . Other branches of economics ignore the spatial aspects of decision making but urban economics focuses not only on the location decisions of firms, but also of cities themselves as cities themselves represent centers of economic activity .\n\nMany spatial economic topics can be analyzed within either an urban or regional economics framework as some economic phenomena primarily affect localized urban areas while others are felt over much larger regional areas . Arthur O'Sullivan believes urban economics is divided into six related themes: market forces in the development of cities, land use within cities, urban transportation, urban problems and public policy, housing and public policy, and local government expenditures and taxes. .\n\nMarket forces in the development of cities relate to how the location decision of firms and households causes the development of cities. The nature and behavior of markets depends somewhat on their locations therefore market performance partly depends on geography.. If a firm locates in a geographically isolated region, their market performance will be different than a firm located in a concentrated region. The location decisions of both firms and households create cities that differ in size and economic structure. When industries cluster, like in the Silicon Valley in California, they create urban areas with dominant firms and distinct economies.\n\nBy looking at location decisions of firms and households, the urban economist is able to address why cities develop where they do, why some cities are large and others small, what causes economic growth and decline, and how local governments affect urban growth . Because urban economics is concerned with asking questions about the nature and workings of the economy of a city, models and techniques developed within the field are primarily designed to analyze phenomena that are confined within the limits of a single city .\n\nLooking at land use within metropolitan areas, the urban economist seeks to analyze the spatial organization of activities within cities. In attempts to explain observed patterns of land use, the urban economist examines the intra-city location choices of firms and households. Considering the spatial organization of activities within cities, urban economics addresses questions in terms of what determines the price of land and why those prices vary across space, the economic forces that caused the spread of employment from the central core of cities outward, identifying land-use controls, such as zoning, and interpreting how such controls affect the urban economy .\n\nEconomic policy is often implemented at the urban level thus economic policy is often tied to urban policy . Urban problems and public policy tie into urban economics as the theme relates urban problems, such as poverty or crime, to economics by seeking to answer questions with economic guidance. For example, does the tendency for the poor to live close to one another make them even poorer? .\n\nUrban transportation is a theme of urban economics because it affects land-use patterns as transportation affects the relative accessibility of different sites. Issues that tie urban transportation to urban economics include the deficit that most transit authorities have, and efficiency questions about proposed transportation developments such as light-rail . Megaprojects such as this have been shown to be synonymous with unexpected costs and questionable benefits.\n\nHousing and public policy relate to urban economics as housing is a unique type of commodity. Because housing is immobile, when a household chooses a dwelling, it is also choosing a location. Urban economists analyze the location choices of households in conjunction with the market effects of housing policies .\nIn analyzing housing policies, we make use of market structures e.g., perfect market structure. There are however problems encountered in making this analysis such as funding, uncertainty, space, etc.\n\nThe final theme of local government expenditures and taxes relates to urban economics as it analyzes the efficiency of the fragmented local governments presiding in metropolitan areas .\n\n\n"}
{"id": "873880", "url": "https://en.wikipedia.org/wiki?curid=873880", "title": "Vladimir Vasyutin", "text": "Vladimir Vasyutin\n\nVladimir Vladimirovich Vasyutin (Russian:Влaдимиp Bлaдимиpoвич Васютин, born March 8, 1952, Kharkiv, Kharkiv Oblast, Ukrainian SSR, died July 19, 2002) was a Soviet cosmonaut.\n\nHe was selected as a cosmonaut on December 1, 1978 (TsPK-6). He retired on February 25, 1986.\n\nVasyutin was assigned to the TKS program for a new generation of manned military spacecraft that would be docked to the existing Salyut space stations.\n\nHe flew as the Commander on Soyuz T-14 to the Salyut 7 space station, for part of the long-duration mission Salyut 7 EO-4. He spent 64 days 21 hours 52 minutes in space. The TKS module was already docked to the Salyut and Vasyutin was due to lead an extended programme of military space experiments. However Vasyutin fell ill soon after arriving at the station and was unable to perform his duties. Although he was originally scheduled to have a six-month stay aboard Salyut 7, his illness forced the crew to make an emergency return to Earth after only two months. His illness is said to have been caused by a prostate infection, which had manifested itself as inflammation and a fever.\n\nHe graduated from Higher Air Force School and from Test Pilot School, both in Kharkov. He was a Lieutenant General in the Soviet Air Forces, and took cosmonaut basic training in August 1976. He retired for medical reasons. He later became Deputy Faculty Chief, VVA - Gagarin Air Force Academy, Monino.\n\nHe was married and had two children. He died of cancer.\n\nHe had been awarded:\n"}
{"id": "29493295", "url": "https://en.wikipedia.org/wiki?curid=29493295", "title": "Wave–current interaction", "text": "Wave–current interaction\n\nIn fluid dynamics, wave–current interaction is the interaction between surface gravity waves and a mean flow. The interaction implies an exchange of energy, so after the start of the interaction both the waves and the mean flow are affected.\n\nFor depth-integrated and phase-averaged flows, the quantity of primary importance for the dynamics of the interaction is the wave radiation stress tensor.\n\nWave–current interaction is also one of the possible mechanisms for the occurrence of rogue waves, such as in the Agulhas Current. When a wave group encounters an opposing current, the waves in the group may pile up on top of each other which will propagate into a rogue wave.\n\n identifies five major sub-classes within wave–current interaction:\n\n\n"}
{"id": "19008703", "url": "https://en.wikipedia.org/wiki?curid=19008703", "title": "Westerpark (park)", "text": "Westerpark (park)\n\nThe \"Westerpark\" (English: \"Western Park\") is a public urban park in Amsterdam, Netherlands. The former borough (\"stadsdeel\") of Westerpark is named after the park, as is the current neighborhood. In 2012 opposite the park, two trains were involved in a head-on collision.\nThe verdant space of the former Westergasfabriek gasworks along Haarlemmerweg has become a place for cultural avant-garde businesses and events.\n\nWestergasfabriek\nTo the west of the park lies a historical building known as Westergasfabriek. The building was built in 1883. The historic building has been renovated and are now used by creative and cultural entrepreneurs. \n"}
