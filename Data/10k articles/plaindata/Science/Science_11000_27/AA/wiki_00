{"id": "10030306", "url": "https://en.wikipedia.org/wiki?curid=10030306", "title": "ABCD Schema", "text": "ABCD Schema\n\nThe Access to Biological Collections Data (ABCD) schema is a highly structured data exchange and access model for taxon occurrence data (specimens, observations, etc. of living organisms), i.e. primary biodiversity data.\n\nIn 2006, the schema was extended, to include an 'Extension For Geosciences', to form the ABCDEFG Schema and, in 2010, TDWG published a draft standard extension for DNA; ABCDDNA.\n\n"}
{"id": "2862", "url": "https://en.wikipedia.org/wiki?curid=2862", "title": "AI-complete", "text": "AI-complete\n\nIn the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. \n\nAI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nCurrently, AI-complete problems cannot be solved with modern computer technology alone, but would also require human computation. This property can be useful, for instance to test for the presence of humans as with CAPTCHAs, and for computer security to circumvent brute-force attacks.\n\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 Ph.D. dissertation and in Eric Raymond's 1991 Jargon File.\n\nAI-complete problems are hypothesized to include:\n\nTo translate accurately, a machine must be able to understand the text. It must be able to follow the author's argument, so it must have some ability to reason. It must have extensive world knowledge so that it knows what is being discussed — it must at least be familiar with all the same commonsense facts that the average human translator knows. Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body: for example, the machine may need to understand how an ocean makes one \"feel\" to accurately translate a specific metaphor in the text. It must also model the authors' goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have wide variety of human intellectual skills, including reason, commonsense knowledge and the intuitions that underlie motion and manipulation, perception, and social intelligence. Machine translation, therefore, is believed to be AI-complete: it may require strong AI to be done as well as humans can do it.\n\nCurrent AI systems can solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempt to \"scale up\" their systems to handle more complicated, real world situations, the programs tend to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they fail as unexpected circumstances outside of its original problem context begin to appear. When human beings are dealing with new situations in the world, they are helped immensely by the fact that they know what to expect: they know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other skills to fall back on.\n\nComputational complexity theory deals with the relative computational difficulty of computable functions. By definition it does not cover problems whose solution is unknown or has not been characterised formally. Since many AI problems have no formalisation yet, conventional complexity theory does not allow the definition of AI-completeness.\n\nTo address this problem, a complexity theory for AI has been proposed. It is based on a model of computation that splits the computational burden between a computer and a human: one part is solved by computer and the other part solved by human. This is formalised by a human-assisted Turing machine. The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined.\n\nThe complexity of executing an algorithm with a human-assisted Turing machine is given by a pair formula_1, where the first element represents the complexity of the human's part and the second element is the complexity of the machine's part.\n\nThe complexity of solving the following problems with a human-assisted Turing machine is:\n\n\n"}
{"id": "35045936", "url": "https://en.wikipedia.org/wiki?curid=35045936", "title": "Abell 907", "text": "Abell 907\n\nAbell 907 is a galaxy cluster in the Abell catalogue.\n\n"}
{"id": "7988703", "url": "https://en.wikipedia.org/wiki?curid=7988703", "title": "Active cavity radiometer", "text": "Active cavity radiometer\n\nActive cavity radiometer - electrically self-calibrating, cavity pyrheliometer used to measure total and spectral solar irradiance.\n\n"}
{"id": "3300567", "url": "https://en.wikipedia.org/wiki?curid=3300567", "title": "Asbestiform", "text": "Asbestiform\n\nAsbestiform is a crystal habit. It describes a mineral that grows in a fibrous aggregate of high tensile strength, flexible, long, and thin crystals that readily separate. The most common asbestiform mineral is chrysotile, commonly called \"white asbestos\", a magnesium phyllosilicate part of the serpentine group. Other asbestiform minerals include riebeckite, an amphibole whose fibrous form is known as crocidolite or \"blue asbestos\", and brown asbestos, a cummingtonite-grunerite solid solution series.\n\n"}
{"id": "31055769", "url": "https://en.wikipedia.org/wiki?curid=31055769", "title": "Autoprotolysis", "text": "Autoprotolysis\n\nIn autoprotolysis a proton is transferred between two identical molecules, one of which acts as a Brønsted acid, releasing a proton which is accepted by the other molecule acting as a Brønsted base. For example, water undergoes autoprotolysis in the self-ionization of water reaction.\n\nAny solvent that contains both acidic hydrogen and lone pairs of electrons to accept H can undergo autoprotolysis.\n\nFor example, ammonia in its purest form may undergo autoprotolysis: \nAnother example is Acetic acid:\n\n2 + \n"}
{"id": "25193231", "url": "https://en.wikipedia.org/wiki?curid=25193231", "title": "Benjamin Martin (lexicographer)", "text": "Benjamin Martin (lexicographer)\n\nBenjamin Martin (1704-1782) was a lexicographer who compiled one of the early English dictionaries, the \"Lingua Britannica Reformata\" (1749). He also was a lecturer on science and maker of scientific instruments.\n\nMartin was born in Worplesdon, Surrey and began life as a ploughboy, but graduated to become a teacher. A legacy of £500 enabled him to buy books and instruments, and he became a lecturer and instrument maker. He was an early champion for the Newtonian system. In 1737, he published the \"Biblioteca Technologia\" - a survey of natural philosophy in 25 sub-headings.\n\nIn 1740, he moved to Fleet Street, near the Royal Society where his admired Newton would often lecture. He began manufacturing Hadley's quadrant (a predecessor to the sextant) and optical instruments. His business prospered, and he also became known as a spectacle maker. He continued to lecture on natural philosophy, and from 1755 to 1764, he also published \"Martin's magazine\". The periodical, formally known as the \"General Magazine of Arts and Sciences\", set out to provide subscribers with an encyclopedia's worth of knowledge \"one Half-sheet upon a Science\" at a time. He intended readers eventually to reorganize and rebind the separate parts of individual numbers into one large reference work. \n\nIn 1781, the seventy-seven-year-old Martin went bankrupt; a few years earlier he had handed over his business to several managers who proved inept. He attempted suicide, and while it was not immediately successful, the wound (nature unknown) was grievous enough and he failed to recover, and died on 9 February 1782.\n\nIn 1749, he published the \"Lingua Britannica Reformata, Or, A New English Dictionary\". His dictionary incorporated a largely intact copy of Nathan Bailey's \"Universal Dictionary\" of 1721, which Benjamin Martin described as \"the best English dictionary hitherto published\". Bailey's dictionary in turn had copied heavily from the 1706 Phillips-Kersey English dictionary. A second edition of Martin's dictionary was published in 1754, a year before Samuel Johnson's dictionary.\n\nIn compiling his 24,500 word dictionary, he gave up on trying to \"fix\" the language: \nThis dynamic view of language was also adopted by Johnson and has become the accepted view in modern lexicography. His dictionary also pre-saged Johnson in that he laid out a detailed set of objectives (that it should be universal, explain the etymologies, etc.).\n\nWhile his etymologies are often inconsistent and tended to err in favour of Latin origins, his work was an improvement on earlier dictionaries in that it had a simpler system of spelling and a clearer guide to pronunciation.\n"}
{"id": "4325643", "url": "https://en.wikipedia.org/wiki?curid=4325643", "title": "Biological terrain assessment", "text": "Biological terrain assessment\n\nThe biological terrain assessment or BTA is a set of tests used to measure the pH, resistivity, and redox of a person's urine, blood, and saliva. The usefulness of the test is debatable. It is often associated with homeopathy and holistic health.\n\nProponents of the BTA claim that comparing the pH, resistivity, and redox of the blood, urine, and saliva provides a health practitioner an indication of the metabolic processes taking place inside the body. There is no scientific evidence that these measurements provide a medically relevant indication of metabolic processes. Multiple manufacturers of BTA equipment have been prevented from selling their equipment in the United States because they lack Food and Drug Administration (FDA) approval. However, the FDA has not stated whether the BTA is medically useful or not.\n\nWhen it comes to medical instrumentation there really isn't such a thing as \"approval.\" The Quantitative Fluid Analyzer (QFA) manufactured by Health Science Company does have FDA classification as a \"laboratory instrument for medical purposes\" \"ion selective\". Class 1 510(K) exempt. The Classification must be renewed every year. However it does not have CLIA (Clinical Laboratory improvement Amendment, 1988) waiver even though its in-vitro test has no misdiagnosis risk of morbidity or mortality as outlined in the Congressional Record when the CLIA law was passed. The FDA can make/modify their own laws.\n\nThe test provides a window into background body chemistry for digestion and toxicity for the purposes of lifestyle and dietary improvement progress. No diagnosis for a disease or condition is made by the instrument. Insurance will not pay for this test.\n\n"}
{"id": "1179801", "url": "https://en.wikipedia.org/wiki?curid=1179801", "title": "Boracite", "text": "Boracite\n\nBoracite is a magnesium borate mineral with formula: MgBOCl. It occurs as blue green, colorless, gray, yellow to white crystals in the orthorhombic - pyramidal crystal system. Boracite also shows pseudo-isometric cubical and octahedral forms. These are thought to be the result of transition from an unstable high temperature isometric form on cooling. Penetration twins are not unusual. It occurs as well formed crystals and dispersed grains often embedded within gypsum and anhydrite crystals. It has a Mohs hardness of 7 to 7.5 and a specific gravity of 2.9. Refractive index values are nα = 1.658 - 1.662, nβ = 1.662 - 1.667 and nγ = 1.668 - 1.673. It has a conchoidal fracture and does not show cleavage. It is insoluble in water (not to be confused with borax, which is soluble in water).\n\nBoracite is typically found in evaporite sequences associated with gypsum, anhydrite, halite, sylvite, carnallite, kainite and hilgardite. It was first described in 1789 for specimens from its type locality of Kalkberg hill, Lüneburg, Lower Saxony, Germany. The name is derived from its boron content (19 to 20% boron by mass).\n\n\n"}
{"id": "8761205", "url": "https://en.wikipedia.org/wiki?curid=8761205", "title": "Compatibility (chemical)", "text": "Compatibility (chemical)\n\nChemical compatibility is a measure of how stable a substance is when mixed with another substance. If two substances can mix together and undergo a chemical reaction, they are considered incompatible.\n\nChemical compatibility is important when choosing materials for chemical storage or reactions, so that the vessel and other apparatus will not be damaged by its contents. For purposes of chemical storage, chemicals that are incompatible should not be stored together so that any leak will not cause an even more dangerous situation by reacting after leaking. In addition, chemical compatibility refers to the container material being acceptable to store the chemical or for a tool or object that comes in contact with a chemical to not degrade. For example, when stirring a chemical the stirrer must be stable in the chemical that is being stirred. Because of this many companies publish chemical resistance charts. and databases to help chemical users use appropriate materials for handling chemicals. Such charts are particularly important for polymers as they are often not compatible with common chemical reagents and will even depend on how the polymers are processed. For example, 3-D printing polymer tools used for chemical experiments must be chosen to ensure chemical compatibility with care. \n\nChemical compatibility is also important when choosing among different chemicals that have similar purposes. For example, bleach and ammonia, both commonly used as cleaners can undergo a dangerous chemical reaction when combined with each other. Even though each of them has a similar use, care must be taken not to allows these chemicals to mix.\n\n"}
{"id": "434428", "url": "https://en.wikipedia.org/wiki?curid=434428", "title": "Compound interest", "text": "Compound interest\n\nCompound interest is the addition of interest to the principal sum of a loan or deposit, or in other words, interest on interest. It is the result of reinvesting interest, rather than paying it out, so that interest in the next period is then earned on the principal sum plus previously accumulated interest. Compound interest is standard in finance and economics.\n\nCompound interest may be contrasted with \"simple interest\", where interest is not added to the principal, so there is no compounding. The \"simple annual interest rate\" is the interest amount per period, multiplied by the number of periods per year. The simple annual interest rate is also known as the nominal interest rate (not to be confused with the interest rate not adjusted for inflation, which goes by the same name).\nCompound Interest = P (1+(r/n))^nt\n\nThe \"compounding frequency\" is the number of times per year (or other unit of time) the accumulated interest is paid out, or \"capitalized\" (credited to the account), on a regular basis. The frequency could be yearly, half-yearly, quarterly, monthly, weekly, daily, or continuously (or not at all, until maturity).\n\nFor example, monthly capitalization with annual rate of interest means that the compounding frequency is 12, with time periods measured in months.\n\nThe effect of compounding depends on:\nA compound interest calculator is a tool that allows calculating such compounding effect on loans or investments.\n\nThe nominal rate cannot be directly compared between loans with different compounding frequencies. Both the nominal interest rate and the compounding frequency are required in order to compare interest-bearing financial instruments.\n\nTo assist consumers compare retail financial products more fairly and easily, many countries require financial institutions to disclose the annual compound interest rate on deposits or advances on a comparable basis. The interest rate on an annual equivalent basis may be referred to variously in different markets as \"annual percentage rate\" (APR), \"annual equivalent rate\" (AER), \"effective interest rate\", \"effective annual rate\", \"annual percentage yield\" and other terms. The effective annual rate is the total accumulated interest that would be payable up to the end of one year, divided by the principal sum.\n\nThere are usually two aspects to the rules defining these rates:\n\n\n\n\nThe total accumulated value, including the principal sum formula_1 plus compounded interest formula_2, is given by the formula:\n\nwhere:\nThe total compound interest generated is the final value minus the initial principal:\n\nSuppose a principal amount of $1,500 is deposited in a bank paying an annual interest rate of 4.3%, compounded quarterly.<br> Then the balance after 6 years is found by using the formula above, with \"P\" = 1500, \"r\" = 0.043 (4.3%), \"n\" = 4, and \"t\" = 6:\n\nSo the new principal formula_6 after 6 years is approximately $1,938.84.\n\nSubtracting the original principal from this amount gives the amount of interest received:\n\nSuppose the same amount of $1,500 is compounded biennially (every 2 years).<br> Then the balance after 6 years is found by using the formula above, with \"P\" = 1500, \"r\" = 0.043 (4.3%), \"n\" = 1/2 (the interest is compounded every two years), and \"t\" = 6 :\n\nSo, the balance after 6 years is approximately $1,921.24.\n\nThe amount of interest received can be calculated by subtracting the principal from this amount.\n\nThe interest is less compared with the previous case, as a result of the lower compounding frequency.\n\nSince the principal \"P\" is simply a coefficient, it is often dropped for simplicity, and the resulting accumulation function is used instead. The accumulation function shows what $1 grows to after any length of time. Accumulation functions for simple and compound interest are\n\nAs \"n\", the number of compounding periods per year, increases without limit, the case is known as continuous compounding, in which case the effective annual rate approaches an upper limit of , where is a mathematical constant that is the base of the natural logarithm.\n\nContinuous compounding can be thought of as making the compounding period infinitesimally small, achieved by taking the limit as \"n\" goes to infinity. See definitions of the exponential function for the mathematical proof of this limit. The amount after \"t\" periods of continuous compounding can be expressed in terms of the initial amount \"P\" as\n\nAs the number of compounding periods formula_13 reaches infinity in continuous compounding, the continuous compound interest is referred to as the force of interest formula_14.\n\nIn mathematics, the accumulation functions are often expressed in terms of \"e\", the base of the natural logarithm. This facilitates the use of calculus to manipulate interest formulae.\n\nFor any continuously differentiable accumulation function \"a(t)\", the force of interest, or more generally the logarithmic or continuously compounded return is a function of time defined as follows:\n\nThis is the logarithmic derivative of the accumulation function.\n\nConversely:\n\nWhen the above formula is written in differential equation format, then the force of interest is simply the coefficient of amount of change:\n\nFor compound interest with a constant annual interest rate \"r\", the force of interest is a constant, and the accumulation function of compounding interest in terms of force of interest is a simple power of e:\n\nThe force of interest is less than the annual effective interest rate, but more than the annual effective discount rate. It is the reciprocal of the e-folding time. See also notation of interest rates.\n\nA way of modeling the force of inflation is with Stoodley's formula: formula_21 where p, r and s are estimated. \n\nTo convert an interest rate from one compounding basis to another compounding basis, use\n\nwhere\n\"r\" is the interest rate with compounding frequency \"n\", and\n\"r\" is the interest rate with compounding frequency \"n\".\n\nWhen interest is continuously compounded, use\n\nwhere\n\"formula_14\" is the interest rate on a continuous compounding basis, and\n\"r\" is the stated interest rate with a compounding frequency \"n\".\n\nThe interest on loans and mortgages that are amortized—that is, have a smooth monthly payment until the loan has been paid off—is often compounded monthly. The formula for payments is found from the following argument.\n\nAn exact formula for the monthly payment (formula_25) is\nor equivalently\n\nwhere:\n\nThis can be derived by considering how much is left to be repaid after each month. <br>The Principal remaining after the first month is\ni.e. the initial amount has increased less the payment. <br>If the whole loan is repaid after one month then\nAfter the second month formula_35 is left, so\n\nIf the whole loan was repaid after two months,\n\nThis equation generalises for a term of n months, formula_39. This is a geometric series which has the sum\nwhich can be rearranged to give\n\nIn spreadsheets, the PMT() function is used. The syntax is:\n\nSee Excel, Mac Numbers, Libreoffice, Open Office for more details.\n\nFor example, for interest rate of 6% (0.06/12), 25 years * 12 p.a., PV of $150,000, FV of 0, type of 0 gives:\n\nA formula that is accurate to within a few percent can be found by\nnoting that for typical U.S. note rates (formula_42 and terms formula_43=10–30 years), the monthly note rate is small compared to 1:\nformula_44 so that the formula_45 which yields\na simplification so that\n\nwhich suggests defining auxiliary variables\n\nHere formula_49 is the monthly payment required for a zero–interest loan paid off in formula_13 installments. In terms of these variables the\napproximation can be written\n\nThe function formula_52 is even:\n\nimplying that it can be expanded in even powers of formula_54.\n\nIt follows immediately that formula_55 can be expanded in even powers\nof formula_54 plus the single term: formula_57\n\nIt will prove convenient then to define\n\nso that\n\nwhich can be expanded:\n\nwhere the ellipses indicate terms that are higher order in even powers of formula_61. The expansion\n\nis valid to better than 1% provided formula_63.\n\nFor a $10,000 mortgage with a term of 30 years and a note rate of 4.5%, payable yearly, we find:\n\nwhich gives\n\nso that\n\nThe exact payment amount is formula_68 so the approximation is an overestimate of about a sixth of a percent.\n\nCompound interest was once regarded as the worst kind of usury and was severely condemned by Roman law and the common laws of many other countries.\n\nThe Florentine merchant Francesco Balducci Pegolotti provided a table of compound interest in his book \"Pratica della mercatura\" of about 1340. It gives the interest on 100 lire, for rates from 1% to 8%, for up to 20 years. The \"Summa de arithmetica\" of Luca Pacioli (1494) gives the Rule of 72, stating that to find the number of years for an investment at compound interest to double, one should divide the interest rate into 72.\n\nRichard Witt's book \"Arithmeticall Questions\", published in 1613, was a landmark in the history of compound interest. It was wholly devoted to the subject (previously called anatocism), whereas previous writers had usually treated compound interest briefly in just one chapter in a mathematical textbook. Witt's book gave tables based on 10% (the then maximum rate of interest allowable on loans) and on other rates for different purposes, such as the valuation of property leases. Witt was a London mathematical practitioner and his book is notable for its clarity of expression, depth of insight and accuracy of calculation, with 124 worked examples.\n\nJacob Bernoulli discovered the constant formula_69 in 1683 by studying a question about compound interest.\n\n"}
{"id": "555650", "url": "https://en.wikipedia.org/wiki?curid=555650", "title": "Conceptual framework", "text": "Conceptual framework\n\nA conceptual framework is an analytical tool with several variations and contexts. It can be applied in different categories of work where an overall picture is needed. It is used to make conceptual distinctions and organize ideas. Strong conceptual frameworks capture something real and do this in a way that is easy to remember and apply.\n\nIsaiah Berlin used the metaphor of a \"fox\" and a \"hedgehog\" to make conceptual distinctions in how important philosophers and authors view the world. Berlin describes hedgehogs as those who use a single idea or organizing principle to view the world (such as Dante Alighieri, Blaise Pascal, Fyodor Dostoyevsky, Plato, Henrik Ibsen and Georg Wilhelm Friedrich Hegel). Foxes, on the other hand, incorporate a type of pluralism and view the world through multiple, sometimes conflicting, lenses (examples include Johann Wolfgang von Goethe, James Joyce, William Shakespeare, Aristotle, Herodotus, Molière, and Honoré de Balzac).\n\nEconomists use the conceptual framework of \"supply\" and \"demand\" to distinguish between the behavior and incentive systems of firms and consumers. Like many conceptual frameworks, supply and demand can be presented through visual or graphical representations (see demand curve). Both political Science and economics use principal agent theory as a conceptual framework. The politics-administration dichotomy is a long standing conceptual framework used in public administration. All three of these cases are examples of a macro level conceptual framework.\n\nThe use of the term \"conceptual framework\" crosses both scale (large and small theories) and contexts (social science, marketing, applied science, art etc.). Its explicit definition and application can therefore vary.\n\nConceptual frameworks are particularly useful as organizing devices in empirical research. One set of scholars has applied the notion of conceptual framework to deductive, empirical research at the micro- or individual study level. They employ American football plays as a useful metaphor to clarify the meaning of \"conceptual framework\" (used in the context of a deductive empirical study).\n\nLikewise, conceptual frameworks are abstract representations, connected to the research project's goal that direct the collection and analysis of data (on the plane of observation – the ground). Critically, a football play is a \"plan of action\" tied to a particular, timely, purpose, usually summarized as long or short yardage. Shields and Rangarajan (2013) argue that it is this tie to \"purpose\" that make American football plays such a good metaphor. They define a conceptual framework as \"the way ideas are organized to achieve a research project's purpose\". Like football plays, conceptual frameworks are connected to a research purpose or aim. Explanation is the most common type of research purpose employed in empirical research. The formal hypothesis of a scientific investigation is the framework associated with explanation.\n\nExplanatory research usually focuses on \"why\" or \"what caused\" a phenomenon to occur. Formal hypotheses posit possible explanations (answers to the why question) that are tested by collecting data and assessing the evidence (usually quantitative using statistical tests). For example, Kai Huang wanted to determine what factors contributed to residential fires in U.S. cities. Three factors were posited to influence residential fires. These factors (environment, population and building characteristics) became the hypotheses or conceptual framework he used to achieve his purpose – explain factors that influenced home fires in U.S. cities.\n\nSeveral types of conceptual frameworks have been identified, and line up with a research purpose in the following ways:\n\nNote that Shields and Rangarajan (2013) do not claim that the above are the only framework-purpose pairing. Nor do they claim the system is applicable to inductive forms of empirical research. Rather, the conceptual framework-research purpose pairings they propose are useful and provide new scholars a point of departure to develop their own research design.\n\nFrameworks have also been used to explain conflict theory and the balance necessary to reach what amounts to resolution. Within these conflict frameworks, visible and invisible variables function under concepts of relevance. Boundaries form and within these boundaries, tensions regarding laws and chaos (or freedom) are mitigated. These frameworks often function like cells, with sub-frameworks, stasis, evolution and revolution. Anomalies may exist without adequate \"lenses\" or \"filters\" to see them and may become visible only when the tools exist to define them.\n\n\n"}
{"id": "20846765", "url": "https://en.wikipedia.org/wiki?curid=20846765", "title": "ECSE (Academic Degree)", "text": "ECSE (Academic Degree)\n\nECSE is an abbreviation for \"Electrical Engineering and Computer Sciences and Systems Engineering.\" It is a designation used at some universities for the major or department that blends these three fields together.\n\nOne reason behind linking the areas of study is to provide students with a broad overview of each of software, hardware and Systems theory. However there are also reasons for not blending departments: Students who major in theoretical computer science, studying such topics as algorithm analysis and software engineering, may not have any use for extensive electrical engineering or systems theory classes. \n\nNot every university uses the ECSE designation. Several universities, for example, have separate EE/ECE and CS departments/majors. Other schools use the similar ECE (Electrical and Computer Engineering) designation. Additionally, some schools which offer an ECSE degree also offer degrees in Electrical Engineering or Computer Science separately.\n\n"}
{"id": "3920278", "url": "https://en.wikipedia.org/wiki?curid=3920278", "title": "Establishment of a port", "text": "Establishment of a port\n\nEstablishment of a Port, the technical expression for the time that elapses between the moon's transit across the meridian at new or full moon at a given place and the time of high water at that place. The interval (constant at any one place) may vary from 6 minutes. (Harwich) to 11 hours 45 minutes (North Foreland). At London Bridge it is 1 hour 58 minutes.\n"}
{"id": "28232105", "url": "https://en.wikipedia.org/wiki?curid=28232105", "title": "European Society for Fuzzy Logic and Technology", "text": "European Society for Fuzzy Logic and Technology\n\nThe European Society for Fuzzy Logic and Technology (EUSFLAT) is a scientific association with the aims to disseminate and promote fuzzy logic and related subjects (sometimes comprised under the collective terms soft computing or computational intelligence) and to provide a platform for exchange between scientists and engineers working in these fields. The society is both open for academic and industrial members.\n\nEUSFLAT was founded in 1998 in Spain as the successor of the National Spanish Fuzzy Logic Society, ESTYLF, with the aim to open the society for members from other European countries. Since then, the society managed to attract a large share of members from outside Spain, and even beyond Europe, with the Spanish members still being the largest group inside EUSFLAT. For these historical reasons, the society is officially registered in Spain.\n\nStarting with 1999, EUSFLAT has been organizing its biannual conferences in odd years. Previous meetings:\n\n\n\nEUSFLAT is led by the President, who is elected for a two-year period, and cannot serve for more than two consecutive periods.\n\n\n"}
{"id": "30857461", "url": "https://en.wikipedia.org/wiki?curid=30857461", "title": "Field-emission microscopy", "text": "Field-emission microscopy\n\nField-emission microscopy (FEM) is an analytical technique used in materials science to investigate molecular surface structures and their electronic properties. Invented by Erwin Wilhelm Müller in 1936, the FEM was one of the first surface-analysis instruments that approached near-atomic resolution.\n\nMicroscopy techniques are used to produce real-space magnified images of a surface showing what it looks like. In general, microscopy information concerns surface crystallography (i.e. how the atoms are arranged at the surface), surface morphology (i.e. the shape and size of topographic features making the surface), and surface composition (the elements and compounds the surface is composed of).\n\nField-emission microscopy (FEM) was invented by Erwin Müller in 1936. In FEM, the phenomenon of field electron emission was used to obtain an image on the detector on the basis of the difference in work function of the various crystallographic planes on the surface.\n\nA field-emission microscope consists of a metallic sample in the form of a sharp tip and a conducting fluorescent screen enclosed in ultrahigh vacuum. The tip radius used is typically of the order of 100 nm. It is composed of a metal with a high melting point, such as tungsten. The sample is held at a large negative potential (1–10 kV) relative to the fluorescent screen. This gives the electric field near the tip apex to be the order of 10 V/m, which is high enough for field emission of electrons to take place.\n\nThe field-emitted electrons travel along the field lines and produce bright and dark patches on the fluorescent screen, giving a one-to-one correspondence with the crystal planes of the hemispherical emitter. The emission current varies strongly with the local work function in accordance with the Fowler–Nordheim equation; hence, the FEM image displays the projected work function map of the emitter surface. The closely packed faces have higher work functions than atomically rough regions, and thus they show up in the image as dark spots on the brighter background. In short, the work-function anisotropy of the crystal planes is mapped onto the screen as intensity variations.\n\nThe magnification is given by the ratio formula_1, where formula_2 is the tip apex radius, and formula_3 is the tip–screen distance. Linear magnifications of about 10 to 10 are attained. The spatial resolution of this technique is of the order of 2 nm and is limited by the momentum of the emitted electrons parallel to the tip surface, which is of the order of the Fermi velocity of the electron in metal.\n\nIt is possible to set up an FEM with a probe hole in the phosphor screen and a Faraday cup collector behind it to collect the current emitted from a single plane. This technique allows the measurement of the variation of work function with orientation for a wide variety of orientations on a single sample. The FEM has also been used to study adsorption and surface diffusion processes, making use of the work-function change associated with the adsorption process.\n\nField emission requires a very good vacuum, and often even in ultra-high vacuum (UHV), emission is not due to the clean surface. A typical field emitter needs to be \"flashed\" to clean it, usually by passing a current through a loop on which it is mounted. After flashing the emission current is high but unstable. The current decays with time and in the process becomes more stable due to the contamination of the tip, either from the vacuum, or more often from diffusion of adsorbed surface species to the tip. Thus the real nature of the FEM tips during use is somewhat unknown.\n\nApplication of FEM is limited by the materials that can be fabricated in the shape of a sharp tip, can be used in a UHV environment, and can tolerate the high electrostatic fields. For these reasons, refractory metals with high melting temperature (e.g. W, Mo, Pt, Ir) are conventional objects for FEM experiments.\n\n\n"}
{"id": "44985997", "url": "https://en.wikipedia.org/wiki?curid=44985997", "title": "Grand-disciple", "text": "Grand-disciple\n\nGrand-disciple or academic grandson (or granddaughter) () are terms sometimes used in academic contexts or contexts relating to fine arts, and denote someone whose mentor or teacher was himself (or herself) a student of a famous representative of that discipline, such as a famous composer or a Nobel Prize-winning scientist.\n\nThe term implies that knowledge, techniques and/or skills are transferred from the \"grandfather\" to the \"grand-disciple,\" borrowing from kinship terminology. The term \"Enkelschüler\" is fairly common in German, but similar terms are also used in English to some extent. In German a doctoral advisor is also usually referred to as a \"Doktorvater\", a \"doctoral father,\" similarly modelled on kinship terminology.\n"}
{"id": "2033247", "url": "https://en.wikipedia.org/wiki?curid=2033247", "title": "Gustav Karl Laube", "text": "Gustav Karl Laube\n\nGustav Karl Laube (8 February 1839, Teplitz – 12 April 1923, Prague) was a Bohemian German geologist and paleontologist.\n\nIn 1871 Laube became a professor of mineralogy and geology at the technical university in Prague, and in 1876, a professor of geology and paleontology at the German Charles-Ferdinand University in Prague. Here, he was also director of the geological institute.\n\nHe was active in geological research of the Ore Mountains and neighbouring areas. He also served as geologist of the Second German North Polar Expedition (1869-70).\n\n\n"}
{"id": "3221118", "url": "https://en.wikipedia.org/wiki?curid=3221118", "title": "Heads Up! (TV series)", "text": "Heads Up! (TV series)\n\nHeads Up! is an educational television show which is produced and broadcast by TVOntario. The host is Bob McDonald, who is better known as the host of the weekly radio show \"Quirks & Quarks\".\n\n\"Heads Up!\" premiered on TVO on September 8, 2005.\n\nThere are three seasons, each with 13 episodes, for a total of 39 episodes. Here's a list of the first season along with the original broadcast dates:\n\n\n"}
{"id": "13467271", "url": "https://en.wikipedia.org/wiki?curid=13467271", "title": "Inherence", "text": "Inherence\n\nInherence refers to Empedocles' idea that the qualities of matter come from the relative proportions of each of the four elements entering into a thing. The idea was further developed by Plato and Aristotle.\n\nThat Plato accepted (or at least did not reject) Empedocles' claim can be seen in the \"Timaeus\". However, Plato also applied it to cover the presence of form in matter. The form is an active principle. Matter, on the other hand is passive, being a mere possibility that the forms bring to life.\n\nAristotle clearly accepted Empedocles' claim, but he rejected Plato's idea of the forms. According to Aristotle, the accidents of a substance are incorporeal beings which are present in it.\n\nA closely related term is participation. If an attribute \"inheres\" in a subject, then the subject is said to \"participate\" in the attribute. For example, if the attribute \"in Athens\" inheres in Socrates, then Socrates is said to participate in the attribute, \"in Athens.\"\n\n"}
{"id": "24046048", "url": "https://en.wikipedia.org/wiki?curid=24046048", "title": "Integral field spectrograph", "text": "Integral field spectrograph\n\nAn integral field spectrograph, or a spectrograph equipped with an integral field unit (IFU), is an optical instrument combining spectrographic and imaging capabilities, used to obtain spatially resolved spectra in astronomy and other fields of research such as bio-medical science and earth observation (or remote sensing).\n\nIntegral field spectroscopy (IFS) has become an important sub-discipline of astronomy with the proliferation of large aperture, high-resolution telescopes where there is a need to study the spectra of extended objects as a function of position, or of clusters of many discrete stars or point sources in a small field. Such spectroscopic investigations have previously been carried out with long-slit spectrographs in which the spectrum is dispersed perpendicular to the slit, and spatial resolution is obtained in the dimension along the slit. Then by stepping the position of the slit, the spectrum of points in the imaged field can be obtained, but the process is comparatively slow, and wasteful of potentially restricted telescope time. Integral field spectrographs are used to speed up such observations by simultaneously obtaining spectra in a two-dimensional field. As the spatial resolution of telescopes in space (and also of ground-based instruments using adaptive optics) has rapidly improved in recent years, the need for such multiplexed instruments has become more and more pressing.\n\nIn this approach, an image is sliced (using for example a Bowen image slicer) in the image-plane and re-arranged such that different parts of the image all fall onto a slit and a dispersing element, such that a spectrum is obtained for a larger area of interest. Another way to think of this is that the slit is optically cut into smaller pieces and re-imaged onto the image-plane at multiple locations.\n\nAn instrument using this technique is for example UVES at the Very Large Telescope.\n\nIn this type of IFU, a lenslet array is placed in the spectrograph entrance slits plane, essentially acting as spatial pixels or \" spaxels \". All beams generated by the lenslet array are then fed through a dispersive element and imaged by a camera, resulting in a spectrum for each individual lenslet.\n\nInstruments like SAURON on the William Herschel Telescope and the SPHERE IFS subsystem on the VLT use this technique.\n\nHere, the light of targets of interest is captured by an array of fibers, forming the spectrographs entrance slits plane. The other end of the fibers are arranged along a single slit such that one obtains a spectrum for each fiber.\n\nThis technique is used by instruments in many telescopes\n(such as INTEGRAL at the William Herschel Telescope), \nand particularly in currently ongoing large surveys of galaxies, such as CALIFA at the Calar Alto Observatory, \nSAMI at the Australian Astronomical Observatory,\nand MaNGA which is one of the surveys making up the next phase of the Sloan Digital Sky Survey.\n\nA recent development is diverse field spectroscopy which combines the benefit of IFS with multi-object spectroscopy (MOS). MOS is used to collect light from many discrete objects over a wide field. This does not record spatial information - just the spectrum of the total light collected within each sampling aperture (usually the core of a positionable optical fibre or a slitlet cut in a mask at the telescope focus). in contrast, IFS obtains complete, spatially resolved coverage over a small field. The MOS targets are generally faint objects at the limits of detection such as primeval galaxies. As telescopes get bigger it is apparent that these actually have blobby and confused structure that requires the observer to carefully select which parts of the field will be passed through to the spectrographs since it is not feasible to carpet the whole field with a single huge IFU. DFS is an instrument paradigm that allows the observer to select arbitrary combinations of contiguous and isolated regions of the sky to maximise observing efficiency and scientific return. Various technologies are under development including robotic switch-yards and photonic optical switches.\n\nOther techniques can achieve the same ends at different wavelengths. The ACIS Advanced CCD Imaging Spectrometer on NASA's Chandra X-Ray Observatory is an example that obtains spectral information by direct measurement of the energy of each photon. This approach is much harder at longer wavelengths because the photons are less energetic. However progress has been made even at optical and near-infrared wavelengths using pixellated detectors such as superconducting tunnel junctions. At radio wavelengths, simultaneous spectral information is obtainable with heterodyne receivers.\n\nMore generally, integral field spectroscopy is a subset of 3D-imaging techniques (also known as hyperspectral imaging and 3D spectroscopy). Other techniques rely on generation of a path difference between interfering beams using electro-mechanical scanning techniques. Examples include Fourier transform spectroscopy employing a Michelson interferometer layout and Fabry–Pérot interferometry. Although, to a first order of approximation, all such techniques are equivalent in that they generate the same number of resolution elements in a datacube (with axes labelled by the two-spatial coordinates plus wavelength) in the same time, they are not equivalent when sources of noise are considered. For example, scanning instruments, although requiring fewer costly detector elements, are inefficient when the background is varying because, unlike IFS, the exposure of the signal and background are not made at the same time. For bio-medical science, \"in vivo\" studies also require simultaneous data collection.\n\n"}
{"id": "30966700", "url": "https://en.wikipedia.org/wiki?curid=30966700", "title": "Irajá Damiani Pinto", "text": "Irajá Damiani Pinto\n\nIrajá Damiani Pinto (July 3, 1919 – June 21, 2014), was a Brazilian paleontologist and professor at the Federal University of Rio Grande do Sul (UFRGS in Portuguese), a member of the Brazilian Academy of Sciences, and a two time president of the Brazilian Geological Society.\n\nIrajá Damiani Pinto was born in Porto Alegre, Rio Grande do Sul, Brazil, on July 3, 1919 \n\nHe studied \"Ginásio Nossa Senhora do Rosário\". Attended the 2nd grade at \"Colégio Universitário Estadual Julio de Castilhos\". In 1942, he began his studies of Natural History in the Faculty of Philosophy of the then University of Porto Alegre. He graduated BA in 1944. In 1945, even as a student in the undergraduate course, was hired as Assistant Chair of Geology and Paleontology. That year he participated in his first scientific excursion led by Dr. Llewellyn Ivor Price, who contributed much to his scientific orientation. In 1945 he carried out the DNPM, in Rio de Janeiro, with Dr. Paul Ericksen de Oliveira and directed by Dr. Llewellyn Ivor Price, and began the library of Geology and Paleontology at the University. In 1957 he helped create the course in geology of UFRGS, one of the first in Brazil.\n\nThe genus of cynodont \"Irajatherium\" is in his honor, in addition to the Museum of Paleontology Irajá Damiani Pinto. \n\n"}
{"id": "45468572", "url": "https://en.wikipedia.org/wiki?curid=45468572", "title": "Kate R. Rosenbloom", "text": "Kate R. Rosenbloom\n\nKate R. Rosenbloom is a member of the Encyclopedia of DNA Elements (ENCODE) Consortium.\nShe is a Tech Project Manager and Software Developer at the Center for Biomolecular Science and Engineering, Jack Baskin School of Engineering, University of California Santa Cruz (UCSC), USA. The pilot stage of ENCODE, involving development of a web browser to show experimental results related to regions on the human genome sequence, was undertaken at UCSC. The university team maintain and develop the University of California Santa Cruz (UCSC) Genome Browser to provide the public with access to genome data from an increasing number of animals, mainly vertebrates. Data provided by the user can also be included. It permits comparisons and some interpretation of the data. An annual update of developments is published each January.\n"}
{"id": "5693489", "url": "https://en.wikipedia.org/wiki?curid=5693489", "title": "Kenneth Kellermann", "text": "Kenneth Kellermann\n\nKenneth Irwin Kellerman (born July 1, 1937) is an American astronomer at the National Radio Astronomy Observatory. He is best known for his work on quasars. He won the Helen B. Warner Prize for Astronomy of the American Astronomical Society in 1971, and the Bruce Medal of the Astronomical Society of the Pacific in 2014.\n\nKellerman was born in New York City to Alexander Kellerman and Rae Kellerman (\"née\" Goodstein). His paternal grandparents emigrated from Hungary and his maternal grandparents from Romania.\n\n"}
{"id": "14143143", "url": "https://en.wikipedia.org/wiki?curid=14143143", "title": "Kubo gap", "text": "Kubo gap\n\nIn atomic physics, the kubo gap is the average spacing that exists between consecutive energy levels. The units of measure are meV or millielectron volts. It varies with an inverse relationship to the nuclearity.\n\nAs the material in question is viewed from the bulk and atomic levels, we can see that the kubo gap goes from a smaller to larger value respectively. As the kubo gap increases there is also a decrease in the density of states located at the Fermi level. The kubo gap can also have an effect on the properties associated with the material. It is possible to control the kubo gap which will then cause the system to become metallic or nonmetallic. The electrical conductivity and magnetic susceptibility are also both influenced by the kubo gap and vary according to the relative size of the kubo gap.\n\n"}
{"id": "48656262", "url": "https://en.wikipedia.org/wiki?curid=48656262", "title": "List of mailing list software", "text": "List of mailing list software\n\nThis is a list of notable electronic mailing list software, which facilitate the widespread distribution of email to many Internet users.\n\nSystems listed on a light purple background are no longer in active development.\n"}
{"id": "38192068", "url": "https://en.wikipedia.org/wiki?curid=38192068", "title": "List of members of the National Academy of Engineering (Bioengineering)", "text": "List of members of the National Academy of Engineering (Bioengineering)\n\nThis list is a subsection of the List of members of the National Academy of Engineering, which includes over 2,000 current members of the United States National Academy of Engineering, each of whom is affiliated with one of 12 disciplinary sections. Each person's name, primary institution, and election year are given. This list does not include deceased members.\n"}
{"id": "58583391", "url": "https://en.wikipedia.org/wiki?curid=58583391", "title": "List of software for astronomy research and education", "text": "List of software for astronomy research and education\n\nListed here are software packages useful for conducting scientific research in astronomy, and for seeing, exploring, and learning about the data used in astronomy. \n"}
{"id": "10873260", "url": "https://en.wikipedia.org/wiki?curid=10873260", "title": "Magnet keeper", "text": "Magnet keeper\n\nA magnet keeper, also known historically as an armature, is a paramagnetic bar made from soft iron or steel, which is placed across the poles of a permanent magnet to help preserve the strength of the magnet by completing the magnetic circuit; it is important for magnets that have a low magnetic coercivity, such as alnico magnets.\n\nKeepers also have a useful safety function, as they stop external metal being attracted to the magnet.\n\nMost magnets do not need a keeper, only those with low coercivity, meaning that they are easily susceptible to stray fields. \n\nA magnet can be considered as the sum of many little magnetic domains, which may only be a few microns or smaller in size. Each domain carries its own small magnetic field, which can point in any direction. When all the domains are pointing in the same direction, the fields add, yielding a strong magnet. When these all point in random directions, they cancel each other, and the net magnetic field is zero.\n\nIn magnets with low coercivity, the direction in which the magnetic domains are pointing is easily swayed by external fields, such as the Earth's magnetic field or perhaps by the stray fields caused by flowing currents in a nearby electrical circuit. Given enough time, such magnets may find their domains randomly oriented, and hence their net magnetization greatly weakened. A keeper for low-coercivity magnets is just a strong permanent magnet that keeps all the domains pointing the same way and realigns those that may have gone astray.\n"}
{"id": "52641193", "url": "https://en.wikipedia.org/wiki?curid=52641193", "title": "Mass structure", "text": "Mass structure\n\nMass structure is a structure, natural or manufactured, that is made by piling up of materials.\n\nExamples of mass structures include pyramids, igloos, and beaver dams.\n"}
{"id": "55989372", "url": "https://en.wikipedia.org/wiki?curid=55989372", "title": "Microenvironment (biology)", "text": "Microenvironment (biology)\n\nMicroenvironment in biology is defined as the normal cells, molecules, and blood vessels that surround and feed a particular cellular area. The microenvironment in a tumor is different from that of a healthy organ; the microenvironment can affect how the tumor grows and spreads.\nThe term originated in the oncology literature in 1975 and has been used with increasing frequency in the biomedical literature since the 2000´s. \n"}
{"id": "38539077", "url": "https://en.wikipedia.org/wiki?curid=38539077", "title": "Multiple-classification ripple-down rules", "text": "Multiple-classification ripple-down rules\n\nMultiple-classification ripple-down rules (MCRDR) is an incremental knowledge acquisition technique which preserves the benefits and essential strategy of ripple-down rules (RDR) in handling the multiple classifications. MCRDR, the extension of RDR, is based on the assumption that the knowledge an expert provides is essentially a justification for a conclusion in a particular context.\n\nBelow is a list of implementations of MCRDR\n\nBEST-RDR (Best Expert System Technique – Ripple Down Rule) website is freely accessible RDR publication and system warehouse that helps you to find programs and publications about RDR. A great amount of publications and programs based on RDR (MCRDR) are available to public.\n\nWhat functions are available in the BEST RDR?\n\n\n"}
{"id": "5524046", "url": "https://en.wikipedia.org/wiki?curid=5524046", "title": "National Atmospheric Release Advisory Center", "text": "National Atmospheric Release Advisory Center\n\nThe National Atmospheric Release Advisory Center (NARAC) is located at the University of California's Lawrence Livermore National Laboratory. It is a national support and resource center for planning, real-time assessment, emergency response, and detailed studies of incidents involving a wide variety of hazards, including nuclear, radiological, chemical, biological, and natural emissions.\n\nNARAC provides tools and services to federal, state and local governments, that map the probable spread of hazardous material accidentally or intentionally released into the atmosphere.\n\nNARAC provides atmospheric plume predictions in time for an emergency manager to decide if protective action is necessary to protect the health and safety of people in affected areas.\n\n\nThe NARAC emergency response central modeling system consists of an integrated suite of meteorological and atmospheric dispersion models. The meteorological data assimilation model, ADAPT, constructs fields of such variables as the mean winds, pressure, precipitation, temperature, and turbulence. Non-divergent wind fields are produced by a procedure based on the variational principle and a finite-element discretization. The dispersion model, LODI, solves the 3-D advection-diffusion equation using a Lagrangian stochastic, Monte Carlo method. LODI includes methods for simulating the processes of mean wind advection, turbulent diffusion, radioactive decay and production, bio-agent degradation, first-order chemical reactions, wet deposition, gravitational settling, dry deposition, and buoyant/momentum plume rise.\n\nThe models are coupled to NARAC databases providing topography, geographical data, chemical-biological-nuclear agent properties and health risk levels, real-time meteorological observational data, and global and mesoscale forecast model predictions. The NARAC modeling system also includes an in-house version of the Naval Research Laboratory's mesoscale weather forecast model COAMPS.\n\n\n"}
{"id": "14456810", "url": "https://en.wikipedia.org/wiki?curid=14456810", "title": "Pollakanth", "text": "Pollakanth\n\nA pollakanth is a plant that reproduces, flowers and sets seed recurrently during its life. The term was first used by Frans R. Kjellman.\n\nOther terms with the same meaning are \"polycarpic\" and \"iteroparous\".\n\nIts antonym is hapaxanth.\n"}
{"id": "42297996", "url": "https://en.wikipedia.org/wiki?curid=42297996", "title": "Protein chemical shift prediction", "text": "Protein chemical shift prediction\n\nProtein chemical shift prediction is a branch of biomolecular nuclear magnetic resonance spectroscopy that aims to accurately calculate protein chemical shifts from protein coordinates. Protein chemical shift prediction was first attempted in the late 1960s using semi-empirical methods applied to protein structures solved by X-ray crystallography. Since that time protein chemical shift prediction has evolved to employ much more sophisticated approaches including quantum mechanics, machine learning and empirically derived chemical shift hypersurfaces. The most recently developed methods exhibit remarkable precision and accuracy.\n\nNMR chemical shifts are often called the mileposts of nuclear magnetic resonance spectroscopy. Chemists have used chemical shifts for more than 50 years as highly reproducible, easily measured parameters to map out the covalent structure of small organic molecules. Indeed, the sensitivity of NMR chemical shifts to the type and character of neighbouring atoms, combined with their reasonably predictable tendencies has made them invaluable for both deciphering and describing the structure of thousands of newly synthesized or newly isolated compounds\n\nPredicted or estimated protein chemical shifts can be used to assist with the chemical shift assignment process. This is especially true if a similar (or identical) protein structure has been solved by X-ray crystallography. In this case, the three-dimensional structure can be used to estimate what the NMR chemical shifts should be and thereby simplify the process of assigning the experimentally observed chemical shifts. Predicted/estimated protein chemical shifts can also be used to identify incorrect or mis-assignments, to correct mis-referenced or incorrectly referenced chemical shifts, to optimize protein structures via chemical shift refinement and to identify the relative contributions of different electronic or geometric effects to nucleus-specific shifts. Protein chemical shifts can also be used to identify secondary structures, to estimate backbone torsion angles, to determine the location of aromatic rings, to assess cysteine oxidation states, to estimate solvent exposure and to measure backbone flexibility.\n\nSignificant progress in chemical shift prediction has been made through continuous improvements in our understanding of the key physico-chemical factors contributing to chemical shift changes. These improvements have also been helped along through significant computational advancements \n\n. Over the past four decades, at least three different methods for calculating or predicting protein chemical shifts have emerged. The first is based on using sequence/structure alignment against protein chemical shift databases, the second is based on directly calculating shifts from atomic coordinates, and the third is based on using a combination of the two approaches. \n\nBy early 2000, several research groups realized that protein chemical shifts could be more efficiently and accurately calculated by combining different methods together as shown in Figure 1. This led to the development of several programs and web servers that rapidly calculate protein chemical shifts when provided with protein coordinate data. These “hybrid” programs, along with some of their features and URLs, are listed below in Table 1.\n\nThis table (Figure 2) lists the correlation coefficients between the experimentally observed backbone chemical shifts and the calculated/predicted backbone shifts for different chemical shift predictors using an identical test set of 61 test proteins.\n\nDifferent methods have different levels of coverage and rates of calculation. Some methods only calculate or predict chemical shifts for backbone atoms (6 atom types). Some calculate chemical shifts for backbone and certain side chain atoms (C and N only) and still others are able to calculate shifts for all atoms (40 atom types). For chemical shift refinement there is a need for rapid calculation as thousands of structures are generated during a molecular dynamics or simulated annealing run and their chemical shifts must be calculated equally rapidly.\nAll the computational speed tests for SPARTA, SPARTA+, SHIFTS, CamShift, SHIFTX and SHIFTX2 were performed on the same computer using the same set of proteins. The calculation speed reported for PROSHIFT is based on the response rate of its web server.\n\n"}
{"id": "57526906", "url": "https://en.wikipedia.org/wiki?curid=57526906", "title": "Ragnar Holm", "text": "Ragnar Holm\n\nRagnar Holm (born 6 May 1879 in Skara, died in 1970), was a Swedish physicist and researcher in electrical engineering, who was partially active in Germany and the United States.\n\nIn 1904, Holm became licentiate in philosophy (PhL) at Uppsala University, and earned his PhD in 1908. From 1906 to 1908 he studied at the University of Göttingen in Germany. In 1908 to 1909 he was a lecturer in Stockholm. Between 1909 and 1919 he worked as a physicist for Siemens & Halske in Berlin. In 1919 to 1921 he was a physicist and teacher at the in Stockholm, and between 1921 and 1927 associate professor at the . Between 1927 and 1945 he worked again for Siemens & Halske. From 1947 he was a consulting physicist at Stackpole Carbon Company in St. Petersburg, Marys, Pennsylvania, USA.\n\nHolm's research area included the study of electrical discharges, but his main focus was on electrical contacts, where he was a world-leading authority. His book \"Electric Contacts\", originally published in 1946, is still used as a standard work in the field (in its fourth edition of 1967).\n\nThe Swedish (KTHB) maintains a special literature collection (\"Holm Collection\") related to Ragnar Holm's research.\n\nIn 1971, the year after Holm's death, the IEEE established the \"Ragnar Holm Scientific Achievement Award\" (\"Ragnar Holm Award\") for efforts in the field of electrical contacts. The prize has been awarded annually since 1972.\n\nSince 1999, the Swedish Royal Institute of Technology (KTH) has distributed the \"Ragnar Holm plaque\" to a young physicist at the beginning of the research career, after assessing dissertations and articles in physics, primarily related to Holm's previous research on electrical contacts. Every second year, the plaque is awarded to a Swedish recipient, and every two years the selection is international.\n\n"}
{"id": "3869631", "url": "https://en.wikipedia.org/wiki?curid=3869631", "title": "Repoxygen", "text": "Repoxygen\n\nRepoxygen was the tradename for a type of gene therapy to produce erythropoietin (EPO). It was under preclinical development by Oxford Biomedica as a possible treatment for anaemia but was abandoned in 2003.\n\nThe project became infamous when it was mentioned during the criminal trial of Thomas Springstein, a former track coach for some German athletes, who was found guilty of giving athletes performance enhancing drugs without their knowledge. An email in which Springstein attempted to obtain Repoxygen was read by a prosecutor, which led to a flurry of media coverage. \n\nThe World Anti-Doping Agency banned \"gene doping\" in 2003 and as of 2009 was researching detection methods for substances such as repoxygen.\n\n"}
{"id": "2341066", "url": "https://en.wikipedia.org/wiki?curid=2341066", "title": "Rubric (academic)", "text": "Rubric (academic)\n\nIn education terminology, rubric means \"a scoring guide used to evaluate the quality of students' constructed responses\". Rubrics usually contain evaluative criteria, quality definitions for those criteria at particular levels of achievement, and a scoring strategy. They are often presented in table format and can be used by teachers when marking, and by students when planning their work. Rubrics, when used with formative assessment purposes, have shown to have a positive impact on students' learning.\n\nA scoring rubric is an attempt to communicate expectations of quality around a task. In many cases, scoring rubrics are used to delineate consistent criteria for grading. Because the criteria are public, a scoring rubric allows teachers and students alike to evaluate criteria, which can be complex and subjective. A scoring rubric can also provide a basis for self-evaluation, reflection, and peer review. It is aimed at accurate and fair assessment, fostering understanding, and indicating a way to proceed with subsequent learning/teaching. This integration of performance and feedback is called ongoing assessment or formative assessment.\n\nSeveral common features of scoring rubrics can be distinguished, according to Bernie Dodge and Nancy Pickett:\n\nScoring rubrics include one or more dimensions on which performance is rated, definitions and examples that illustrate the attribute(s) being measured, and a rating scale for each dimension. Dimensions are generally referred to as criteria, the rating scale as levels, and definitions as descriptors.\n\nHerman, Aschbacher, and Winters distinguish the following elements of a scoring rubric:\n\nSince the 1980s, many scoring rubrics have been presented in a graphic format, typically as a grid. Studies of scoring rubric effectiveness now consider the efficiency of a grid over, say, a text-based list of criteria.\n\nRubrics can be classified as holistic or analytic. Holistic rubrics integrate all aspects of the work into a single overall rating of the work. For example, \"the terms and grades commonly used at university (i.e., excellent – A, good – B, average – C, poor – D, and weak – E) usually express an assessor’s overall rating of a piece of work. When a research article or thesis is evaluated, the reviewer is asked to express their opinion in holistic terms – accept as is, accept with minor revisions, require major revisions for a second review, or reject. The classification response is a weighted judgement by the assessor taking all things into account at once; hence, holistic. In contrast, an analytic rubric specifies various dimensions or components of the product or process that are evaluated separately. The same rating scale labels may be used as the holistic, but it is applied to various key dimensions or aspects separately rather than an integrated judgement. This separate specification means that on one dimension the work could be excellent, but on one or more other dimensions the work might be poor to average. Most commonly, analytic rubrics have been used by teachers to score student writing when the teacher awards a separate score for such facets of written language as conventions or mechanics (i.e., spelling, punctuation, and grammar), organisation, content or ideas, and style. They are also used in many other domains of the school curriculum (e.g., performing arts, sports and athletics, studio arts, wood and metal technologies, etc.). By breaking the whole into significant dimensions or components and rating them separately, it is expected that better information will be obtained by the teacher and the student about what needs to be worked on next.\" (Brown, Irving, & Keegan, 2014, p. 55).\n\nScoring rubrics may help students become thoughtful evaluators of their own and others’ work and may reduce the amount of time teachers spend evaluating student work. Here is a seven-step method to creating and using a scoring rubric for writing assignments:\n\n\nAdditionally, for the implementation of self-assessment and peer assessment, that can be done with rubrics, there is a list of recommendations. Importantly, rubrics can be co-created with the students to increase their comprehension and use of the assessment criteria. \n\nIt can be used in individual assessment within the course, or a project or capstone project. However, it can ideally be used when multiple evaluators are evaluating the assessment to get focus on the contributing attributes for the evaluation. \n\nThe traditional meanings of the word \"rubric\" stem from \"a heading on a document (often written in red — from Latin, \"rubrica\", red ochre, red ink), or a direction for conducting church services\". Drawing on definition 2 in the OED for this word rubrics referred to the instructions on a test to the test-taker as to how questions were to be answered.\n\nIn modern education circles, rubrics have recently come to refer to an assessment tool. The first usage of the term in this new sense is from the mid-1990s, but scholarly articles from that time do not explain why the term was co-opted. Perhaps rubrics are seen to act, in both cases, as metadata added to text to indicate what constitutes a successful use of that text. It may also be that the color of the traditional red marking pen is the common link.\n\nAs shown in the 1977 introduction to the International Classification of Diseases-9, the term has long been used as medical labels for diseases and procedures. The bridge from medicine to education occurred through the construction of \"Standardized Developmental Ratings.\" These were first defined for writing assessment in the mid-1970s and used to train raters for New York State's Regents Exam in Writing by the late 1970s. That exam required raters to use multidimensional standardized developmental ratings to determine a holistic score. The term \"rubrics\" was applied to such ratings by Grubb, 1981 in a book advocating holistic scoring rather than developmental rubrics. Developmental rubrics return to the original intent of standardized developmental ratings, which was to support student self-reflection and self-assessment as well as communication between an assessor and those being assessed. In this new sense, a scoring rubric is a set of criteria and standards typically linked to learning objectives. It is used to assess or communicate about product, performance, or process tasks.\n\nOne problem with scoring rubrics is that each level of fulfillment encompasses a wide range of marks. For example, if two students both receive a 'level four' mark on the Ontario system, one might receive an 80% and the other 100%. In addition, a small change in scoring rubric evaluation caused by a small mistake may lead to an unnecessarily large change in numerical grade. Adding further distinctions between levels does not solve the problem, because more distinctions make discrimination even more difficult. Both scoring problems may be alleviated by treating the definitions of levels as typical descriptions of whole products rather than the details of every element in them.\n\nScoring rubrics may also make marking schemes more complicated for students. Showing one mark may be inaccurate, as receiving a perfect score in one section may not be very significant in the long run if that specific strand is not weighted heavily. Some may also find it difficult to comprehend an assignment having multiple distinct marks, and therefore it is unsuitable for some younger children. In such cases it is better to incorporate the rubrics into conversation with the child than to give a mark on a paper. For example, a child who writes an \"egocentric\" story (depending too much on ideas not accessible to the reader) might be asked what her best friend thinks of it (suggesting a move in the audience dimension to the \"correspondence\" level). Thus, when used effectively scoring rubrics help students to improve their weaknesses.\n\nMultidimensional rubrics also allow students to compensate for a lack of ability in one strand by improving another one. For instance, a student who has difficulty with sentence structure may still be able to attain a relatively high mark, if sentence structure is not weighted as heavily as other dimensions such as audience, perspective or time frame.\n\nAnother advantage of a scoring rubric is that it clearly shows what criteria must be met for a student to demonstrate quality on a product, process, or performance task.\n\nScoring rubrics can also improve scoring consistency. Grading is more reliable while using a rubric than with out one. Educators can refer to a rubric while scoring assignments to keep grading consistent between students. Teachers can also use rubrics to keep their scoring consistent between other teachers who teach the same class.\n\n\n"}
{"id": "58002385", "url": "https://en.wikipedia.org/wiki?curid=58002385", "title": "Sarah E Reisman", "text": "Sarah E Reisman\n\nSarah Elizabeth Reisman is a Professor of Chemistry at the California Institute of Technology and winner of the Arthur C. Cope Scholar Award and Tetrahedron Young Investigator Award for Organic Synthesis. \n\nReisman's research focuses on the total synthesis of complex natural products. \n\nReisman received a B.A. in Chemistry from Connecticut College in 2001, conducting research in the lab of Prof. Timo V. Ovaska on the synthesis of tetracyclic terpenoid natural products, including phorbol.\n\nReisman obtained her Ph.D. from Yale University in 2006, working with John L. Wood on the total synthesis of (±)-Welwitindolinone A Isonitrile. Reisman's work included methodological developments towards a generalized skeleton, using nitrone cyclization and oxindole formation as linchpin transforms.\n\nReisman was an NIH Postdoctoral Fellow in the lab of Eric N. Jacobsen at Harvard University and worked with Abigail Doyle to develop an enantioselective substitution of silyl ketenes onto an alkoxy chloride via an oxacarbenium ion, using a novel thiourea organocatalyst. \n\nReisman began her independent career as an Assistant Professor at Caltech in 2008 and was promoted directly to Full Professor in 2014.\n\nThe Reisman lab focuses on the synthesis of complex natural products and development of new chemical reactions. The group completed the first enantioselective total syntheses of (–)-acetylaranotin (40 years after its isolation), (–)-Maoecrystal Z, (–)-8-Demethoxyrunanine, and (–)-Cepharatines A, C and D. Their total synthesis of (+)-ryanodol was completed in 15 synthetic steps, a significant improvement on the previous shortest synthetic route of 35 steps developed by Masayuki Inoue of the University of Tokyo. Other completed total syntheses include natural products (+)-Naseseazines A and B, (+)-Salvileucalin B and (+)-Psiguadial B The group's reaction methodology work has focused primarily on nickel catalysis, cycloadditions, and opening of strained-ring precursors.\n\n\nReisman was awarded the Boehringer Ingelheim New Faculty Grant, the Alfred P Sloan Foundation Fellowship and a 5-year NSF CAREER Award in 2011. \n"}
{"id": "45545521", "url": "https://en.wikipedia.org/wiki?curid=45545521", "title": "Science for the contemporary world", "text": "Science for the contemporary world\n\nScience for the contemporary world (SCW) is subject of the Spanish education system which is mandatory to students of first year of European Baccalaureate. The SCW's role is to educate the students in the way that they become able to dispute in a critical and informed way to science relevant scams.\n\n\n\n"}
{"id": "26615974", "url": "https://en.wikipedia.org/wiki?curid=26615974", "title": "Scientific research on the International Space Station", "text": "Scientific research on the International Space Station\n\nScientific research on the International Space Station is a collection of experiments that require one or more of the unusual conditions present in low Earth orbit. The primary fields of research include human research, space medicine, life sciences, physical sciences, astronomy and meteorology. The 2005 NASA Authorization Act designated the American segment of the International Space Station as a national laboratory with the goal of increasing the use of the ISS by other federal agencies and the private sector.\n\nResearch on the ISS improves knowledge about the effects of long-term space exposure on the human body. Subjects currently under study include muscle atrophy, bone loss, and fluid shift. The data will be used to determine whether space colonisation and lengthy human spaceflight are feasible. As of 2006, data on bone loss and muscular atrophy suggest that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise (such as the six-month journey time required to fly to Mars).\nLarge scale medical studies are conducted aboard the ISS via the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts (including former ISS Commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS, and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.\n\nResearchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of this data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues, and the unusual protein crystals that can be formed in space.\n\nThe investigation of the physics of fluids in microgravity will allow researchers to model the behaviour of fluids better. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. In addition, an examination of reactions that are slowed by low gravity and temperatures will give scientists a deeper understanding of superconductivity.\n\nThe study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. Other areas of interest include the effect of the low gravity environment on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve our knowledge about energy production, and lead to economic and environmental benefits. Future plans are for the researchers aboard the ISS to examine aerosols, ozone, water vapour, and oxides in Earth's atmosphere, as well as cosmic rays, cosmic dust, antimatter, and dark matter in the universe.\n\n<br>\nWhen completed, The ISS will include a number of modules devoted to scientific activity as well as other hardware designed for the same purpose.\n<br>\nLaboratory modules:\nScientific hardware not attached to any laboratory module:\n\nInternal scientific hardware:\n\nExternal scientific hardware:\n\n\nPlanned for launch:\n\n<br>\nInternal scientific hardware:\nExternal scientific hardware:\n\n\n\n\n\n\n\n\n\n\n\nFee-based utilization of Kibo is available to unrestricted research groups for commercial use. Costs involved in the operation will be paid by each user. The results obtained through the utilization will belong to the user.\n\n\n\n\n\n\n\n\n\n\n\nHost Immunity in Space (FIT)\n\nAntibiotic Production in Space (CGBA-APS)\nSynaptogenesis in Microgravity (CGBA-SM)\n(MEPS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch like NASA and JAXA, ESA also conducted numerous experiments on the International Space Station.\n\n\n\n\n\n\n\n\n\n\nIn May 2011, Space Shuttle Endeavour mission STS-134 carried 13 Lego kits to the ISS, where astronauts built models and saw how they reacted in microgravity, as part of the Lego Bricks in Space program. The results were shared with schools as part of an educational project.\n\n\n"}
{"id": "36858883", "url": "https://en.wikipedia.org/wiki?curid=36858883", "title": "Sixth Asian Science Camp", "text": "Sixth Asian Science Camp\n\nThe Sixth Asian Science Camp, which began on 26 August 2012, was hosted by Israel and took place in Jerusalem. The Asian Science Camp (ASC), developed by two Nobel Prize recipients and based on the Lindau meetings, is an annual forum described as an event \"to enlighten science-talented youths in Asia through discussions with world scholars, as well as to promote international friendship and cooperation young Asian students.\" Approximately 300 Asian students from twenty-two Asian countries attended the event.\n\nIn terms of the number of participants, the Sixth Asian Science Camp is the largest of all the previous camps.\n\nTwenty-two Asian countries sent students from high schools and colleges to the event. The Israeli delegation was the largest (35 participants), followed by China (34 participants), India (33 participants), and Japan (24 participants). In addition, students from Australia, Taiwan, South Korea, Nepal, New Zealand, the Philippines, Singapore, Sri Lanka, Thailand, Turkey, Vietnam, Georgia, Armenia, Turkmenistan, and Myanmar attended. Approximately 300 Asian students attended in total.\n\nThe Israeli Foreign Ministry stated that the ministry wanted to ensure that Israel would be able to host the event, and was working for months in this pursuit. The ministry said that the organizers of the Sixth Asian Science Camp expressed an interest in having Israel host the event, and that \"many people in Asia... are curious about Israel’s successes in high-tech and science.\" The foreign ministry described the Sixth Asian Science Camp as \"an important project, one that was one of our top goals for 2012, a special year in which Israel is emphasizing its relations with Asian countries.\"\n\nTen Nobel Prize recipients were invited to lecture at the event. Among these recipients are Robert Aumann (economics), Aaron Ciechanover (chemistry), Makoto Kobayashi (physics), Roger D. Kornberg (chemistry), and Yuan T. Lee (chemistry).\n\nThe Asian Science Camp took place at the Givat Ram campus of the Hebrew University in Jerusalem. Lectures and discussions by five Nobel Prize recipients, as well as over 20 leading scientists, were held at the event. The discussions by these scientists involved a diverse range of fields, including biology, physics, genetics, mathematics, and space travel.\n\nStudents at the Sixth Asian Science Camp also took part in a creative poster competition, and went on trips to Israeli tourist sites, Israeli university laboratories, various high-tech companies, and social gatherings.\n\nThe slogan for the Sixth Asian Science Camp was \"Young Science for the Future.\"\n"}
{"id": "56439198", "url": "https://en.wikipedia.org/wiki?curid=56439198", "title": "Society for the History of Alchemy and Chemistry", "text": "Society for the History of Alchemy and Chemistry\n\nThe Society for the History of Alchemy and Chemistry, founded as the \"Society for the Study of Alchemy and Early Chemistry\" in 1935, holds biennial meetings and a yearly Graduate Workshop, publishes the journal \"Ambix\" and a biennial newsletter \"Chemical Intelligence\", and offers prizes and grants to scholars. Its members represent twenty-eight countries.\n\nThe Society was first established in November 1935 by J.R. Partington (1886-1965), Frank Sherwood Taylor (1897-1956), Douglas McKie (1896-1967), and Gerard Heym (1888-1972), and named the Society for the Study of Alchemy and Early Chemistry. Its object was “the study of alchemy and early chemistry in their scientific and historical aspects, and the publication of relevant material.”\nSherwood Taylor was responsible for launching the Society’s journal \"Ambix\" in May 1937, with J.R. Partington as its first Chairman. From these early days onwards, the Society worked on \"chemistry and chemical technology in antiquity and the Latin West and the development of iatrochemistry and chemical philosophy.\" During the Second World War, the Society for the History of Alchemy & Early Chemistry was closed down, but it was re-established again in 1946, with the second volume of \"Ambix\" published in the same year. From 1956 onwards, when Desmond Geoghegan became editor, \"Ambix\" began to extend its readership and its period of historical coverage to the time of John Dalton and the nineteenth century. The Society altered its name to the Society for the History of Alchemy and Chemistry in 1975.\n\n The Society’s journal \"Ambix\" is published four times a year in February, May, August, and November. Papers are refereed by an international editorial board. Its coverage is wide and varied, ranging from studies in exoteric and esoteric alchemy to recent chemistry. Recently, more attention has been given to the history of alchemy, while Ambix continues to provide important reading for historians of chemistry. The presentation of scientific ideas, methods and discoveries is made as non-technical as possible, consistent with academic rigour and scientific accuracy. Extensive book reviews are published in each issue of \"Ambix\". Recent special issues include: The Royal Typographer and the Alchemist: Willem Silvius and John Dee (May 2017); From the Library to the Laboratory and Back Again (May 2016); and Chemical Knowledge in Transit (November 2015). In May 2013, 2014, and 2015 three special issues were published on Sites of Chemistry devoted to the eighteenth, nineteenth and twentieth centuries respectively, while the November 2014 special issue concerned Analysis and Synthesis in Medieval and Early Modern Europe.\n\nIn 2013, the Society launched the series Sources of Alchemy and Chemistry, which provides readers with critical editions and English translations of foundational texts in the history of alchemy and early chemistry. The readership of Ambix is international and includes historians, chemists, philosophers, and scholars in other disciplines. Issues of \"Ambix\" are sent to members and libraries around the world and are also available online free to members through the Society’s website ambix.org.\n\nThe Society publishes its newsletter Chemical Intelligence twice a year. This publication advertises and reports not only the Society’s events and activities, but also those of other organisations involved in the history of alchemy and chemistry. Chemical Intelligence is distributed to members by email and is made available free to the public online via www.ambix.org.\n\nThe Society generally organises a Spring meeting and an Autumn meeting each year. Recent topics included John Dee and Print Culture; Chemistry and its Audiences; Making Chemistry: History, Materials, and Practices; New and Old Themes in the History of Chemistry and Chemistry in Europe. The Society also organises annual postgraduate workshops usually held in the Autumn. Workshops have been held at Cambridge, London, Amsterdam, Utrecht, and Philadelphia.\n\nThe Society offers three prizes: the Partington Prize, awarded every three years for an original essay by a new scholar on any aspect of the history of alchemy and chemistry; the John and Martha Morris Award, which is also made every three years, for outstanding achievements in the history of modern chemistry or the history of the chemical industry; and the Oxford Part II Prize, which can be awarded in any year for an excellent history of chemistry thesis submitted for a Chemistry Part II at Oxford University.\n\nThe Society further offers yearly grants, for which members of the Society may apply. The Society offers two types of competitive grants, both part of its Award Scheme. The New Scholars Awards are aimed to support research. This Award is open to postgraduate students and those who have obtained their PhD within five years as of the preceding 1 January. The second grant is the Subject Development Award, which supports activities such as seminars, workshops, colloquia, lecture series, conferences, etc. In addition, non-competitive grants may be awarded to individuals and organisations.\n\n\n"}
{"id": "4229742", "url": "https://en.wikipedia.org/wiki?curid=4229742", "title": "Systems theory in political science", "text": "Systems theory in political science\n\nSystems theory in political science is a highly abstract, partly holistic view of politics, influenced by cybernetics. The adaptation of system theory to political science was first conceived by David Easton in 1953.\n\nIn simple terms, Easton's behavioral approach to politics, proposed that a political system could be seen as a delimited (i.e. all political systems have precise boundaries) and fluid (changing) system of steps in decision making. Greatly simplifying his model:\nInfluence of computers on the discipline of political science and the political system work within an environment.\nThe environment generates different demands from different section of society such as reservation system in the matter of a certain group,demand for better transportation etc.\n\nIf the system functions as described, then we have a \"stable political system\". If the system breaks down, then we have a \"dysfunctional political system\".\n\nEaston aspired to make politics a science, that is, working with highly abstract models that described the regularities of patterns and processes in political life in general. In his view, the highest level of abstraction could make scientific generalizations about politics possible. In sum, politics should be seen as a whole, not as a collection of different problems to be solved.\n\nHis main model was driven by an organic view of politics, as if it were a living object. His theory is a statement of what makes political systems adapt and survive. He describes politics in a constant flux, thereby rejecting the idea of \"equilibrium\", so prevalent in some other political theories (see institutionalism). Moreover, he rejects the idea that politics could be examined by looking at different levels of analysis. His abstractions could account for any group and demand at any given time. That is, interest group theory and elite theory can be subsumed in political systems analysis.\nHis theory was and is highly influential in the pluralist tradition in political science. (see Harold Lasswell and Robert Dahl)\n\n"}
{"id": "36476254", "url": "https://en.wikipedia.org/wiki?curid=36476254", "title": "T-maze", "text": "T-maze\n\nIn behavioral science, a T-maze (or the variant Y-maze) is a simple maze used in animal cognition experiments. It is shaped like the letter T (or Y), providing the subject, typically a rodent, with a straightforward choice.\nT-mazes are used to study how the rodents function with memory and spatial learning through applying various stimuli. Starting in the early 20th century, rodents were used in experiments such as the T-Maze. These concepts of T-mazes are used to assess rodent behavior. The different tasks, such as left-right discrimination and forced alternation, are mainly used with rodents to test reference and working memory.\n\nThe T-maze is one of a group of various mazes of differing sizes and many shapes. It is one of the most simple, consisting of just two turns - right or left. The maze is only able to be altered by blocking one of the two paths. The basis behind the T-maze is to place the rat at the base of the maze. By placing a reward at one arm or both arms of the maze, the rat must make the choice of which path to take. The decision made by the rat can be a cause of a natural preference within the rat. A study of alternation can be performed by repeating the experiment multiple times with no reward in either arm of the maze. Another experiment that can be performed is the alternation of rewards each time the experiment is performed, proving the rat will choose the arm that was not visited each time the experiment starts.\n\nRewards within the rats can be types of food, another rat within a cage, an odor, or a type of shelter. By performing this type of experiment, the rat's preferences can be determined. Examples of this could be a rat's food preferences, its familiarity with specific smells and scents, the attraction of the male and female within the maze, and whether a young rat prefers an adult female or an adult male. These simple experiments can determine the rat's psyche on multiple subjects, and ultimately divulge further into the rat's psychological characteristics. It is also important to consider the rodent's behavior. The use of spatial and non-spatial cues is very influential to research findings on memory, spatial learning and the long-term potential (LTP). These cues include the orientation of the maze, extra-maze cues and room configuration cues. Strategies may be affected by the rodent's ability to find cues in the room, the presence or absence of polarizing cues in the room, and the stability of the maze in the room. When analyzing and interpreting experimental data, researchers have to consider the orientation and configuration of the apparatus and cues in the room.\n\nThis type of apparatus includes multiple T-mazes connected which result in a very complex maze. It is constructed of a high number of T-junctions. Each intersection remains the same length and scale, which gives every point within the maze a direct right or wrong answer. By not changing the size of the maze, it allows for the rat to focus on the decision and not be confused if the size of the maze was altered within the junctions. Multiple T-mazes are constructed to question response vs. place techniques and of cognitive direction and mapping.\n\nAn example of an experiment within a multiple T-maze was performed by allowing the rat to explore the multiple T-maze without a reward. After letting the rat roam, researchers restarted the maze again with a reward placed at the end of the maze. The rats with prior exposure to the maze were able to easily navigate through the maze to reach the reward. This experiment proved that rats have the ability to generate a cognitive map when exposed to their surroundings and can process this information when needed to complete a task.\n\nResearchers have also created the Y-Maze which functions very similar to the T-maze. The Y-Maze is altered to have a more gradual change into the arms. The arms are also all equal in length and distance apart from one another. The Y-maze has proven to be easier for rats to understand the layout of the space and recognize rewards, similar pattern, and adapt to new experiments at a quicker pace.\n\nThis type of maze is constructed with a center platform with arms radiating from the center. The original maze had 8 spokes, but they have been constructed with as few as 3 and as many as 48 spokes. This type of maze is used to perform short-term memory experimentations on rats. Rats are examined on whether or not they have the ability to remember the arms they have already explored. This is determined by placing food pellets at each of the arms and the rat must only travel down each arm once and retrieve the pellet accordingly.\n\nThe concepts of T-mazes are used to assess rodent behavior. The different tasks such as left-right discrimination and forced alternation are mainly used with rodents to test reference and working memory. Maze research is used to show how the rodent’s behavior evolves with alternate strategies to do different tasks.\n\nThe spatial memory of the rat is responsible for recording information about the rat’s environment as well as its spatial orientation. It is this spatial memory that allows the rat to navigate its way through the various types of mazes and challenges presented to it by the experimenters. This also allows the rat to navigate the same maze multiple times while remembering the correct and incorrect pathways (unless the scientists change the paths in between tests). In the T-maze this is relegated to a single left or right turn, but in more complex mazes it becomes a series of turns for the rat to remember in order to reach its goal and reward. A poorly working spatial memory can result in a rat getting repeatedly lost in the maze regardless of how successful previous attempts were or how unchanged the maze is.\n\nThe hippocampus is located in the medial temporal lobe area of the brain and is responsible for governing spatial memory. In animals, this allows them to have a spatial map of their environment and it uses reference and working memory to accomplish this. It also has important functions that govern long and short-term memory as well as spatial navigation, both of which are required in order for the rat to correctly navigate the maze.\n\n3. Bliss T.V.P., Collingridge G.L. (1993) A synaptic model of memory: Long-term potentiation in the hippocampus. Nature 361:31–39.\n\n4. Carol K. Sigelman, Elizabeth A. Rider Life-Span Human Development, Seventh Edition, 2010, 2009\nURL: http://www.mouse-phenotype.org/ \n\n7. Robert M J Deacon1 & J Nicholas P Rawlins1 T-maze alternation in the rodenthttp://www.nature.com/nprot/journal/v1/n1\n5. Lynch, MA. Long-Term Potentiation and Memory. Physiol Rev 84: 87–136, 2004; 10.1152/physrev.00014.2003. http://learnmem.cshlp.org/content/5/4/344 \n\n6. Muller D., Oliver M. Lynch G. (1989) Developmental changes in synaptic properties in hippocampal neonatal rats. Dev. Brain Res. 49:105–114.Medline\n/full/nprot.2006.2.html \n\n8. Shoji, H., Hagihara, H., Takao, K., Hattori, S., Miyakawa, T. (2012) T-maze Forced Alternation and Left-right Discrimination Tasks for Assessing Working and Reference Memory in Mice. J. Vis. Exp. (60), e3300, doi:10.3791/3300 http://www.jove.com/video/3300/t-maze-forced-alternation-left-right-discrimination-tasks-for\n\n9. Third Edition of Cognitive Neuroscience: chapter 8 titled Theories of Learning and Memory; p322\n\n"}
{"id": "26162571", "url": "https://en.wikipedia.org/wiki?curid=26162571", "title": "Tartu–Moscow Semiotic School", "text": "Tartu–Moscow Semiotic School\n\nThe Tartu–Moscow Semiotic School is a scientific school of thought in the field of semiotics that was formed in 1964 and led by Juri Lotman. Among the other members of this school were Boris Uspensky, Vyacheslav Ivanov, Vladimir Toporov, Mikhail Gasparov, Alexander Piatigorsky, Isaak I. Revzin, and others. As a result of their collective work, they established a theoretical framework around the semiotics of culture.\n\nThe Tartu–Moscow School of Semiotics developed an original method of multidimensional cultural analysis. The languages of culture are interpreted as secondary modelling systems in relation to verbal language. This method permits a productive understanding of the use of different languages of culture.\n\nThis school is widely known for its journal, \"Sign Systems Studies\" (formerly published in Russian as \"Труды по знаковым системам\"), published by Tartu University Press. It is the oldest semiotics journal in the world, established in 1964.\n\nIn its first period, the 1960s and 1970s, TMSS followed a structuralist approach and was strongly influenced by Russian formalism. Since the 1980s, its approach can be characterized as post-structuralist, and is connected with the introduction of Juri Lotman's concept of semiosphere and its relation to organicism.\n\nFrom 1990s, TMSS has been succeeded by the Tartu Semiotics School, which is based in the Department of Semiotics of the University of Tartu and led by Kalevi Kull, Peeter Torop, Mihhail Lotman, and others.\n\n\n"}
{"id": "50276238", "url": "https://en.wikipedia.org/wiki?curid=50276238", "title": "Tower of the Winds (Oxford)", "text": "Tower of the Winds (Oxford)\n\nThe Tower of the Winds is the prominent octagonal tower on top of the old Radcliffe Observatory building in Oxford, England. The building now forms a centrepiece for Green Templeton College, one of the colleges of Oxford University.\n\nThe tower is based on the ancient and smaller Tower of the Winds in Athens, Greece, built c.100–50BC by Andronicus of Cyrrhus for the purpose of measuring time. It is of octagonal stone construction, with eight relief images of Greek mythological wind gods at the top of each side of the tower, carved by John Bacon the Elder in 1792–4. The tower was completed by James Wyatt in 1794. On the top are Atlas and Hercules supporting a globe in white, also by John Bacon. The reliefs of the signs of the zodiac above the windows on the first floor are made of Coade stone by J. C. F. Rossi. Inside the tower, there are three main rooms on top of each other.\n\nThe Tower of the Winds is situated in prominent view just to the north of the Radcliffe Observatory Quarter (ROQ), an area for Oxford University departments including the Blavatnik School of Government, and south of Observatory Street, named after its former use as an observatory. To the south is the Mathematical Institute building of the University, juxtaposing the new 21st-century architecture of that building with the old 17th-century style of the observatory. To the west RE the Jericho Health Centre and BEYOND THAT Walton Street, with a view of the tower in the distance from the southern end looking north along the street. To the east are the Woodstock Road and the front entrance of Green Templeton College, with St Anne's College opposite.\n\n"}
{"id": "44942272", "url": "https://en.wikipedia.org/wiki?curid=44942272", "title": "VASVIK Industrial Research Award", "text": "VASVIK Industrial Research Award\n\nVASVIK Industrial Research Award is an Indian award, instituted to recognize and promote excellence in industrial research in the areas of science and technology. The award is given annually to individuals or groups and carries a citation and a cash prize of 100,000.\n\nVASVIK awards have been instituted by Vividhlaxi Audyogik Samshodhan Vikas Kendra (VASVIK), a non profit making non governmental organization established in 1974 by the Mumbai based business family of Patels, headed by Mohan Patel, an ex Sheriff of Mumbai, who is credited with the development of the first ophthalmic nozzle pure aluminum tube. The awards are annual in cycle and have been instituted with a view to promote industry based research in India in the fields of science and technology. Nine categories have been identified for the awards and individuals and groups are eligible for the awards, except one, which is reserved for women scientists. The awardees are selected by a Board of Advisors nominated by the organization. The awards honour innovation in design and production techniques which foster economic growth of the country by way of import substitution, reduction of manufacturing costs and foreign exchange saving.\n\nThe awards are distributed in nine categories of which 8 are open categories and \"Chandaben Mohanbhai Patel Industrial Research Award\" is extended to women scientists only.\n"}
