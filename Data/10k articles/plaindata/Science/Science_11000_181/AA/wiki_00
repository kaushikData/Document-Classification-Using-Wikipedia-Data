{"id": "42786590", "url": "https://en.wikipedia.org/wiki?curid=42786590", "title": "Absettarov virus", "text": "Absettarov virus\n\nThe Absettarov virus (ABSV) is a strain of tickborne encephalitis virus (Far Eastern subtype) in the genus Flavivirus. It was isolated in 1951 in Leningrad from the blood of a 3-year-old boy with biphasic fever and signs of meningitis. It can be found in Sweden, Finland, Poland, former Czechoslovakia, Hungary, Austria, Bulgaria and western parts of the former USSR.\n"}
{"id": "1374870", "url": "https://en.wikipedia.org/wiki?curid=1374870", "title": "Allochthon", "text": "Allochthon\n\nIn structural geology, an allochthon, or an allochthonous block, is a large block of rock which has been moved from its original site of formation, usually by low angle thrust faulting. An allochthon which is isolated from the rock that pushed it into position is called a klippe. If an allochthon has a \"hole\" in it so that one can view the autochthon beneath the allochthon, the hole is called a \"window\" (or Fenster). Etymology: Greek; 'allo' = other, and 'chthon' = earth.\n\nIn limnology, allochthonous sources of carbon or nutrients come from outside the aquatic system (such as plant and soil material). Carbon sources from within the system, such as algae and the microbial breakdown of aquatic particulate organic carbon, are autochthonous. In aquatic food webs, the portion of biomass derived from allochthonous material is then named \"allochthony\". In streams and small lakes, allochthonous sources of carbon are dominant while in large lakes and the ocean, autochthonous sources dominate.\n\n"}
{"id": "15899055", "url": "https://en.wikipedia.org/wiki?curid=15899055", "title": "Appraisal subordination entitlement reduction", "text": "Appraisal subordination entitlement reduction\n\nAppraisal Subordination Entitlement Reduction (ASER) or collateral valuation adjustments (CVA) are commercial mortgage-backed security (CMBS) structuring innovations designed to improve overall transaction credit quality. Appraisal Reductions were created in response to rating agency concerns that, without such an adjustment, cash flow from mortgages likely to default would be paid to the first-loss class. The rationale behind appraisal reductions is to support proactively the credit rating of senior CMBS tranches by reducing cash-flow payments to the subordinate tranches.\n\n"}
{"id": "44662520", "url": "https://en.wikipedia.org/wiki?curid=44662520", "title": "Archimedean Oath", "text": "Archimedean Oath\n\nThe Archimedean Oath is an ethical code of practice for engineers and technicians, similar to the Hippocratic Oath used in the medical world. It was proposed in 1990 by a group of students of the École polytechnique fédérale de Lausanne. The Archimedean Oath has since spread to a number of European engineering schools.\n\nA shorter version goes:\n\n"}
{"id": "58296164", "url": "https://en.wikipedia.org/wiki?curid=58296164", "title": "Arterial spin labelling", "text": "Arterial spin labelling\n\nArterial spin labeling (ASL), also known as arterial spin tagging, is a magnetic resonance imaging method for measuring perfusion using water already in the subject as tracers by setting and tracking magnetic spin. Perfusion MRI is perfusion scanning by the use of a particular MRI sequence. ASL specifically refers to magnetic labeling of arterial blood below the imaging slab, without the need of gadolinium contrast, instead inferring perfusion from a drop in signal observed in the imaging slice arising from inflowing spins (outside the imaging slice) having been selectively saturated. A number of ASL schemes are possible, the simplest being flow alternating inversion recovery (FAIR) which requires two acquisitions of identical parameters with the exception of the out-of-slice saturation; the difference in the two images is theoretically only from inflowing spins, and may be considered a 'perfusion map'.\nThe technique was developed by John Detre, Alan P. Koretsky and coworkers in 1992.\n\nIn cerebral infarction, the penumbra has decreased perfusion. Besides acute and chronic neurovascular diseases, the value of ASL has been demonstrated in brain tumors, epilepsy and neurodegenerative disease, such as Alzheimer’s disease, frontotemporal dementia and Parkinson disease.\n\nAlthough the primary form of fMRI uses the blood-oxygen-level dependent (BOLD) contrast, ASL is another method of obtaining contrast.\n\nASL is in general a safe technique, although injuries may occur as a result of failed safety procedures or human error like other MRI techniques.\n\n"}
{"id": "17822734", "url": "https://en.wikipedia.org/wiki?curid=17822734", "title": "Beltian body", "text": "Beltian body\n\nA Beltian body is a detachable tip found on the pinnules of some species of \"Acacia\" and closely related genera. Beltian bodies, named after Thomas Belt, are rich in lipids, sugars and proteins and often red in colour. They are believed to have evolved in a symbiotic relationship with ants. The ants live inside special plant structures (Domatia) or near the plant and keep away herbivores.\n\nOther ant-mutualism related plant structures include Beccarian, Mullerian and pearl bodies.\n\nUnique among spiders for its predominantly vegetarian diet, \"Bagheera kiplingi\" feeds almost exclusively on Beltian bodies.\n\n"}
{"id": "20156881", "url": "https://en.wikipedia.org/wiki?curid=20156881", "title": "Bode's sensitivity integral", "text": "Bode's sensitivity integral\n\nBode's sensitivity integral, discovered by Hendrik Wade Bode, is a formula that quantifies some of the limitations in feedback control of linear parameter invariant systems. Let \"L\" be the loop transfer function and \"S\" be the sensitivity function. Then the following holds:\n\nwhere formula_2 are the poles of \"L\" in the right half plane (unstable poles).\n\nIf \"L\" has at least two more poles than zeros, and has no poles in the right half plane (is stable), the equation simplifies to:\n\nThis equality shows that if sensitivity to disturbance is suppressed at some frequency range, it is necessarily increased at some other range. This has been called the \"waterbed effect.\"\n\n\n\n"}
{"id": "3476372", "url": "https://en.wikipedia.org/wiki?curid=3476372", "title": "Born–von Karman boundary condition", "text": "Born–von Karman boundary condition\n\nBorn–von Karman boundary conditions are periodic boundary conditions which impose the restriction that a wave function must be periodic on a certain Bravais lattice. (Named after Max Born and Theodore von Kármán). This condition is often applied in solid state physics to model an ideal crystal. Born and von Karman \npublished a series of articles in 1912 and 1913 that presented one of the first theories of specific heat of solids \nbased on the crystaline hypothesis and included these boundary conditions. \n\nThe condition can be stated as\n\nwhere \"i\" runs over the dimensions of the Bravais lattice, the a are the primitive vectors of the lattice, and the \"N\" are integers (assuming the lattice has \"N\" cells where \"N=NNN\"). This definition can be used to show that\n\nfor any lattice translation vector T such that:\n\nNote, however, the Born–von Karman boundary conditions are useful when \"N\" are large (infinite). \n\nThe Born–von Karman boundary condition is important in solid state physics for analyzing many features of crystals, such as diffraction and the band gap. Modeling the potential of a crystal as a periodic function with the Born–von Karman boundary condition and plugging in Schrödinger's equation results in a proof of Bloch's theorem, which is particularly important in understanding the band structure of crystals.\n\n"}
{"id": "2314644", "url": "https://en.wikipedia.org/wiki?curid=2314644", "title": "Bruce Levy", "text": "Bruce Levy\n\nSir Enoch Bruce Levy (19 February 1892 – 16 October 1985) was a botanist from New Zealand who became widely known for his work on improving pastures.\n\nLevy was born in Auckland in 1892, and lived on a farm until he was 18 years old. He joined the New Zealand Department of Agriculture in 1911, and studied science at Victoria University in Wellington from 1926 to 1928. In 1925 he married schoolteacher Phyllis Rosa Kate Mason.\n\nIn 1937 he founded the Grasslands Division of the Department of Scientific and Industrial Research, and was Director of that division until he retired in 1951. During that time he travelled all over New Zealand, helping farmers to improve their pastures based on techniques that he had learned in Europe. After retiring, he worked on improving the turf of golf courses and bowling greens.\n\nLevy was the author of \"Construction, Renovation and Care of the Bowling Green\" (1949); \"Construction, Renovation and Care of the Golf Course\" (1950); and \"Grasslands of New Zealand\" (1951).\n\nIn the 1950 King's Birthday Honours he was made an Officer of the Order of the British Empire for services to agriculture in connection with grassland research. He was appointed a Knight Bachelor in the 1953 Coronation Honours.\n"}
{"id": "18684614", "url": "https://en.wikipedia.org/wiki?curid=18684614", "title": "Calciborite", "text": "Calciborite\n\nCalciborite, CaBO, is a rare calcium borate mineral.\n\nIt was first described in 1955 in the Novofrolovskoye copper–boron deposit, near Krasnoturinsk, Turinsk district, Northern Ural Mountains, Russia. It occurs in a skarn deposit formed in limestone adjacent to a quartz diorite intrusive. It occurs associated with: sibirskite (another rare calcium borate mineral), calcite, dolomite, garnet, magnetite and pyroxene. It has also been reported from the Fuka mine of Okayama Prefecture, Japan.\n"}
{"id": "31430089", "url": "https://en.wikipedia.org/wiki?curid=31430089", "title": "Canadian Bioinformatics Workshops", "text": "Canadian Bioinformatics Workshops\n\nCanadian Bioinformatics Workshops (CBW) are a series of advanced training workshops in bioinformatics, founded in 1999 in response to an identified need for a skilled bioinformatics workforce in Canada.\n\nThe Canadian Bioinformatics Workshops series began offering one and two week short courses in bioinformatics, genomics and proteomics in 1999, in response to an identified need for a skilled bioinformatics workforce in Canada. In partnership with the Canadian Genetics Diseases Network and Human Resources Development Canada, and under the scientific direction of Director, Francis Ouellette, the CBW series was established.\n\nFor eight years, the series offered short courses in bioinformatics, genomics and proteomics in various cities across Canada. The courses were taught by top faculty from Canada and the US, and offered small classes and hands-on instruction.\n\nIn 2007, the Canadian Bioinformatics Workshops moved to Toronto, where it is now hosted by the Ontario Institute for Cancer Research. A new format and series of workshops were designed in the fall of 2007. It was recognized that with the introduction of new technologies and scientific approaches to research, having the computational biology capacity and skill to deal with this new data has become an even greater asset.\n\nThe new series of workshops focuses on training the experts and users of these advanced technologies on the latest approaches used in computational biology to deal with the new data. The Canadian Bioinformatics Workshops began offering the 2-day advanced topic workshops in 2008.\n\nAll workshop material is licensed under a Creative Commons-Share Alike 2.5 license and is available on the Bioinformatics.ca website.\n\nThe CBW is sponsored by the Canadian Institute of Health Research and the Ontario Institute for Cancer Research.\n\nThe Canadian Bioinformatics Workshops also hosts Bioinformatics Links Directory which contains links to molecular resources, tools and databases. The links listed in this directory are from a myriad of sources, selected on the basis of recommendations from bioinformatics experts in the field.\n\n"}
{"id": "669713", "url": "https://en.wikipedia.org/wiki?curid=669713", "title": "Capillary wave", "text": "Capillary wave\n\nA capillary wave is a wave traveling along the phase boundary of a fluid, whose dynamics and phase velocity are dominated by the effects of surface tension.\n\nCapillary waves are common in nature, and are often referred to as ripples. The wavelength of capillary waves on water is typically less than a few centimeters, with a phase speed in excess of 0.2–0.3 meter/second.\n\nA longer wavelength on a fluid interface will result in gravity–capillary waves which are influenced by both the effects of surface tension and gravity, as well as by fluid inertia. Ordinary gravity waves have a still longer wavelength.\n\nWhen generated by light wind in open water, a nautical name for them is cat's paw waves, since they may resemble paw prints. Light breezes which stir up such small ripples are also sometimes referred to as cat's paws. On the open ocean, much larger ocean surface waves (seas and swells) may result from coalescence of smaller wind-caused ripple-waves.\n\nThe dispersion relation describes the relationship between wavelength and frequency in waves. Distinction can be made between pure capillary waves – fully dominated by the effects of surface tension – and gravity–capillary waves which are also affected by gravity.\n\nThe dispersion relation for capillary waves is\n\nwhere \"ω\" is the angular frequency, \"σ\" the surface tension, \"ρ\" the density of the \nheavier fluid, \"ρ\"' the density of the lighter fluid and \"k\" the wavenumber. The wavelength is \nformula_2\nFor the boundary between fluid and vacuum (free surface), the dispersion relation reduces to\n\nIn general, waves are also affected by gravity and are then called gravity–capillary waves. Their dispersion relation reads, for waves on the interface between two fluids of infinite depth:\n\nwhere \"g\" is the acceleration due to gravity, \"ρ\" and \"ρ‘\" are the mass density of the two fluids (\"ρ > ρ‘\"). The factor formula_5 in the first term is the Atwood number.\n\nFor large wavelengths (small \"k = 2π/λ\"), only the first term is relevant and one has gravity waves.\nIn this limit, the waves have a group velocity half the phase velocity: following a single wave's crest in a group one can see the wave appearing at the back of the group, growing and finally disappearing at the front of the group.\n\nShorter (large \"k\") waves (e.g. 2 mm for the water–air interface), which are proper capillary waves, do the opposite: an individual wave appears at the front of the group, grows when moving towards the group center and finally disappears at the back of the group. Phase velocity is two thirds of group velocity in this limit.\n\nBetween these two limits is a point at which the dispersion caused by gravity cancels out the dispersion due to the capillary effect. At a certain wavelength, the group velocity equals the phase velocity, and there is no dispersion. At precisely this same wavelength, the phase velocity of gravity–capillary waves as a function of wavelength (or wave number) has a minimum. Waves with wavelengths much smaller than this critical wavelength \"λ\" are dominated by surface tension, and much above by gravity. The value of this wavelength and the associated minimum phase speed \"c\" are:\n\nFor the air–water interface, \"λ\" is found to be , and \"c\" is .\n\nIf one drops a small stone or droplet into liquid, the waves then propagate outside an expanding circle of fluid at rest; this circle is a caustic which corresponds to the minimal group velocity.\n\nAs Richard Feynman put it, \"[water waves] that are easily seen by everyone and which are usually used as an example of waves in elementary courses [...] are the worst possible example [...]; they have all the complications that waves can have.\" The derivation of the general dispersion relation is therefore quite involved.\n\nTherefore, first the assumptions involved are pointed out. There are three contributions to the energy, due to gravity, to surface tension, and to hydrodynamics. The first two are potential energies, and responsible for the two terms inside the parenthesis, as is clear from the appearance of \"g\" and \"σ\". For gravity, an assumption is made of the density of the fluids being constant (i.e., incompressibility), and likewise \"g\" (waves are not high for gravitation to change appreciably). For surface tension, the deviations from planarity (as measured by derivatives of the surface) are supposed to be small. Both approximations are excellent for common waves.\n\nThe last contribution involves the kinetic energies of the fluids, and is the most involved. One must use a hydrodynamic framework to tackle this problem. Incompressibility is again involved (which is satisfied if the speed of the waves is much less than the speed of sound in the media), together with the flow being irrotational – the flow is then\npotential; again, these are typically good approximations for common situations. The resulting equation for the potential (which is Laplace equation) can be solved with the proper boundary conditions. On one hand, the velocity must vanish well below the surface (in the \"deep water\" case, which is the one we consider, otherwise a more involved result is obtained, see Ocean surface waves.) On the other, its vertical component must match the motion of the surface. This contribution ends up being responsible for the extra \"k\" outside the parenthesis, which causes all regimes to be dispersive, both at low values of \"k\", and high ones (except around the one value at which the two dispersions cancel out.)\n\n\n\n"}
{"id": "36966893", "url": "https://en.wikipedia.org/wiki?curid=36966893", "title": "Consumer demand tests (animals)", "text": "Consumer demand tests (animals)\n\nConsumer demand tests for animals are studies designed to measure the relative strength of an animal's motivation to obtain resources such as different food items. The test results are analogous to human patterns of purchasing resources with a limited income. For humans, the cost of resources is usually measured in money; in animal studies the cost is usually represented by energy required, time taken or a risk of injury. Costs of resources can be imposed on animals by an operant task (e.g. lever-pressing), a natural aversion (e.g. crossing water), or a homeostatic challenge (e.g. increased body temperature). Humans usually decrease the amount of an item purchased (or consumed) as the cost of that item increases. Similarly, animals tend to consume less of an item as the cost of that item increases (e.g. more lever presses required).\nSuch demand tests quantify the strength of motivation animals have for resources whilst avoiding anthropomorphism and anthropocentrism.\nUsing consumer demand tests one can empirically determine the strength of motivation animals have for a definite need (e.g. food, water) and also for resources we humans might perceive as a luxury or unnecessary but animals might not (e.g. sand for dustbathing or additional space for caged mice). By comparing the strength of motivation for the resource with that for a definite\nneed, we can measure the importance of a resource as perceived by the animals. Animals will be most highly motivated to interact with resources they absolutely need, highly motivated for resources that they perceive as most improving their welfare, and less motivated for resources they perceive as less important. Furthermore, Argument by analogy indicates that as with humans, it is more likely that animals will experience negative affective states (e.g. frustration, anxiety) if they are not provided with the resources for which they show high motivation.\n\nVarious other aspects of the animal's behaviour can be measured to aid understanding of motivation for resources, e.g. latency (delay) to approach the point of access, speed of incurring the cost, time with each resource, or the range of activities with each of the resources. These measures can be recorded either by the experimenter or by motion detecting software. Prior to testing, the animals are usually given the opportunity to explore the apparatus and variants to habituate and reduce the effects of novelty.\n\nThe rate (i.e. regression line) at which the animal decreases its acquisition or consumption of a resource as the cost increases is known as the elasticity of demand. A steep slope of decreasing access indicates a relatively low motivation for a resource, sometimes called 'high elasticity'; a shallow slope indicates relatively high motivation for a resource, sometimes called 'low elasticity', or 'inelastic demand.'\n\nThe 'break point' is the cost at which inelastic demand becomes elastic, i.e. the cost at which constant consumption begins to decrease.\n\nIn human economics and consumer theory, a Giffen good is a resource which is paradoxically consumed more as the cost rises, violating the law of demand. In normal situations, as the price of a resource increases, the substitution effect causes consumers to purchase less of it and more of substitute goods. In the Giffen good situation, the income effect dominates, leading people to buy more of the good, even as its price rises.\n\n\n\n\nManser et al. showed that laboratory rats were motivated to lift a door weighing 83% of their body weight to allow them to rest on a solid floor rather than on a grid floor, despite their having been kept on grid floors for over 6 months.\n\nBaldwin showed that when animals were given control of their lighting with the equivalent of an on/off switch, pigs kept lights on for 72% of the time and sheep for 82%. However, when the pigs had to work for the light by keeping their snout within a photo-beam, they only kept the lights on for 0.5% of the time, indicating that light was\na weak reinforcement for this species. Savory and Duncan showed that individual hens kept in a background of darkness were prepared to work for 4 hours of light per day.\n\nSherwin et al. examined the strength of motivation for burrowing substrate in laboratory mice. Despite an increasing cost of gaining access, the mice continued to work to visit the burrowing substrate. In addition, it was shown that it was the performance of burrowing behaviour that was important to the mice, not simply the functional consequences of the behaviour. King and Welsman showed that when bar pressing gave deermice access to sand, they increased their rate of bar pressing as the number of presses to access the sand was increased.\n\nDuncan and Kite showed that hens were highly motivated to gain access to a nest box, particularly immediately prior to oviposition. The hens would push a weighted door, or walk through water or an air blast to reach a nest box. Duncan and Kite suggested the strength of this motivation was equivalent to that of the strength of motivation to feed after 20 hours deprivation.\n\nSeveral studies have examined the motivation of animals for social contact either with their offspring or conspecifics.\n\n"}
{"id": "7772738", "url": "https://en.wikipedia.org/wiki?curid=7772738", "title": "Consumption-based capital asset pricing model", "text": "Consumption-based capital asset pricing model\n\nThe consumption-based capital asset pricing model (CCAPM) is used in finance and economics as an expansion of the capital asset pricing model (CAPM). The CCAPM factors in consumption as a means of understanding and calculating an expected return on investment.\n\nThe CCAPM implies that the expected risk premium on a risky asset, defined as the expected return on a risky asset less the risk free return, is proportional to the covariance of its return and consumption in the period of the return.The consumption beta is included and the expected return is calculated as follows:\n\nr= rf + B(rm - rf)\n\nr = expected return on security or portfolio\n\nrf = risk free rate\n\nB = consumption beta (of individual company or weighted average of portfolio), and\n\nrm = return from the market\n"}
{"id": "3523316", "url": "https://en.wikipedia.org/wiki?curid=3523316", "title": "Distributed parameter system", "text": "Distributed parameter system\n\nA distributed parameter system (as opposed to a lumped parameter system) is a system whose state space is infinite-dimensional. Such systems are therefore also known as infinite-dimensional systems. Typical examples are systems described by partial differential equations or by delay differential equations.\n\nWith \"U\", \"X\" and \"Y\" Hilbert spaces and \"formula_1\" ∈ \"L\"(\"X\"), \"formula_2\" ∈ \"L\"(\"U\", \"X\"), \"formula_3\" ∈ \"L\"(\"X\", \"Y\") and \"formula_4\" ∈ \"L\"(\"U\", \"Y\") the following equations determine a discrete-time linear time-invariant system:\nwith \"formula_7\" (the state) a sequence with values in \"X\", \"formula_8\" (the input or control) a sequence with values in \"U\" and \"formula_9\" (the output) a sequence with values in \"Y\".\n\nThe continuous-time case is similar to the discrete-time case but now one considers differential equations instead of difference equations:\nAn added complication now however is that to include interesting physical examples such as partial differential equations and delay differential equations into this abstract framework, one is forced to consider unbounded operators. Usually \"A\" is assumed to generate a strongly continuous semigroup on the state space \"X\". Assuming \"B\", \"C\" and \"D\" to be bounded operators then already allows for the inclusion of many interesting physical examples, but the inclusion of many other interesting physical examples forces unboundedness of \"B\" and \"C\" as well.\n\nThe partial differential equation with formula_12 and formula_13 given by\nfits into the abstract evolution equation framework described above as follows. The input space \"U\" and the output space \"Y\" are both chosen to be the set of complex numbers. The state space \"X\" is chosen to be \"L\"(0, 1). The operator \"A\" is defined as\nIt can be shown that \"A\" generates a strongly continuous semigroup on \"X\". The bounded operators \"B\", \"C\" and \"D\" are defined as\n\nThe delay differential equation\nfits into the abstract evolution equation framework described above as follows. The input space \"U\" and the output space \"Y\" are both chosen to be the set of complex numbers. The state space \"X\" is chosen to be the product of the complex numbers with \"L\"(−\"τ\", 0). The operator \"A\" is defined as\nIt can be shown that \"A\" generates a strongly continuous semigroup on X. The bounded operators \"B\", \"C\" and \"D\" are defined as\n\nAs in the finite-dimensional case the transfer function is defined through the Laplace transform (continuous-time) or Z-transform (discrete-time). Whereas in the finite-dimensional case the transfer function is a proper rational function, the infinite-dimensionality of the state space leads to irrational functions (which are however still holomorphic).\n\nIn discrete-time the transfer function is given in terms of the state space parameters by formula_24 and it is holomorphic in a disc centered at the origin. In case 1/\"z\" belongs to the resolvent set of \"A\" (which is the case on a possibly smaller disc centered at the origin) the transfer function equals formula_25. An interesting fact is that any function that is holomorphic in zero is the transfer function of some discrete-time system.\n\nIf \"A\" generates a strongly continuous semigroup and \"B\", \"C\" and \"D\" are bounded operators, then the transfer function is given in terms of the state space parameters by formula_26 for \"s\" with real part larger than the exponential growth bound of the semigroup generated by \"A\". In more general situations this formula as it stands may not even make sense, but an appropriate generalization of this formula still holds.\nTo obtain an easy expression for the transfer function it is often better to take the Laplace transform in the given differential equation than to use the state space formulas as illustrated below on the examples given above.\n\nSetting the initial condition formula_27 equal to zero and denoting Laplace transforms with respect to \"t\" by capital letters we obtain from the partial differential equation given above\nThis is an inhomogeneous linear differential equation with formula_31 as the variable, \"s\" as a parameter and initial condition zero. The solution is formula_32. Substituting this in the equation for \"Y\" and integrating gives formula_33 so that the transfer function is formula_34.\n\nProceeding similarly as for the partial differential equation example, the transfer function for the delay equation example is formula_35.\n\nIn the infinite-dimensional case there are several non-equivalent definitions of controllability which for the finite-dimensional case collapse to the one usual notion of controllability. The three most important controllability concepts are:\n\nAn important role is played by the maps formula_36 which map the set of all \"U\" valued sequences into X and are given by formula_37. The interpretation is that formula_38 is the state that is reached by applying the input sequence \"u\" when the initial condition is zero. The system is called \n\nIn controllability of continuous-time systems the map formula_42 given by formula_43 plays the role that formula_36 plays in discrete-time. However, the space of control functions on which this operator acts now influences the definition. The usual choice is \"L\"(0, ∞;\"U\"), the space of (equivalence classes of) \"U\"-valued square integrable functions on the interval (0, ∞), but other choices such as \"L\"(0, ∞;\"U\") are possible. The different controllability notions can be defined once the domain of formula_42 is chosen. The system is called\n\nAs in the finite-dimensional case, observability is the dual notion of controllability. In the infinite-dimensional case there are several different notions of observability which in the finite-dimensional case coincide. The three most important ones are:\n\nAn important role is played by the maps formula_50 which map \"X\" into the space of all \"Y\" valued sequences and are given by formula_51 if \"k\" ≤ \"n\" and zero if \"k\" > \"n\". The interpretation is that formula_52 is the truncated output with initial condition \"x\" and control zero. The system is called\n\nIn observability of continuous-time systems the map formula_56 given by formula_57 for \"s∈[0,t]\" and zero for \"s>t\" plays the role that formula_50 plays in discrete-time. However, the space of functions to which this operator maps now influences the definition. The usual choice is \"L\"(0, ∞, \"Y\"), the space of (equivalence classes of) \"Y\"-valued square integrable functions on the interval \"(0,∞)\", but other choices such as \"L\"(0, ∞, \"Y\") are possible. The different observability notions can be defined once the co-domain of formula_56 is chosen. The system is called\n\nAs in the finite-dimensional case, controllability and observability are dual concepts (at least when for the domain of formula_63 and the co-domain of formula_64 the usual \"L\" choice is made). The correspondence under duality of the different concepts is:\n\n\n"}
{"id": "26921073", "url": "https://en.wikipedia.org/wiki?curid=26921073", "title": "Dynamic speckle", "text": "Dynamic speckle\n\nIn physics, dynamic speckle is a result of the temporal evolution of a speckle pattern where variations in the scattering elements responsible for the formation of the interference pattern in the static situation produce the changes that are seen in the speckle pattern, where its grains change their intensity (grey level) as well as their shape along time. One easy to observe example is milk: place some milk in a teaspoon and observe the surface in direct sunlight. You will see a \"dancing\" pattern of coloured points. Where the milk dries on the spoon at the edge, the speckle is seen to be static. This is direct evidence of the thermal motion of atoms, which cause the Brownian motion of the colloidal particles in the milk, which in turn results in the dynamic speckle visible to the naked eye.\n\nThe dynamic pattern shows then the changes that, if they are analyzed along time, represent the activity of the illuminated material. The visual effect is that of a boiling liquid or the image in a TV set far from tuning.\n\nIt can be analyzed by means of several mathematical and statistical tools and provide numeric or visual information on its magnitude, the not well defined idea of activity. Because the number of scattering centers is very high the collective phenomenon is hard to interpret and their individual contributions to the final result can not be inferred. The measurements that are obtained by means of the analysis tools present the activity level as a sum of the contributions of phenomena due to Doppler effect of the scattered light as well as other phenomena eventually present (time variations of the refractive index of the sample, etc.) Light scattered with small Doppler shifts in its frequency beats on the detector (eventually the eye) giving rise to the slow intensity variations that constitute the dynamic of the speckle pattern.\n\nA biological sample, for example, that is a material that contains a huge number of mobile scattering centers, presents refractive index variations in the materials that compose it with power changes as well as many other effects increasing the complexity in the identification and isolation of these phenomena. Then, the complete interpretation of the activity of a sample, by means of dynamic speckle, presents itself big challenges.\n\nFigure 1 shows a sequence of speckle patterns in a corn seed in the start of its germination process where the dynamic effect is higher in the areas where the scattering centers are expected to be more active as is the case of the embryo and in a break in the endosperm region of the seed. The embryo is in the lower left side and the break is a river-like region in the center. In the crack, the activity is due to intensive inner water evaporation while in the embryo activity is higher due to metabolism of the alive tissue together with the activity caused by water evaporation. In the endosperm, the high right region of the image represents that the relatively low activity is due only to water evaporation.\n\nBiological tissue is one of the most complex that can be found in nature. Besides it is worsened by the intrinsic variability present between one sample and another. These facts make even more difficult the comparison of results between different samples even in presence of the same stimulus. In this context, speckle patterns have been applied to study bacteria, parasites, seeds and plants.\n\nOther fields of application are the analysis of drying paint, control in gels, foams, corrosion, efflorescence, etc.\n\nSeveral mathematical and statistical tools have been proposed for the characterization of the activity of a dynamic speckle pattern.\nSome of them are:\n\n\nformula_1\n\n\nformula_2\n\n\nformula_3\n\n\nformula_4\n\nThese and other methods are gathered in Biospeckle laser tool library.\n"}
{"id": "3740668", "url": "https://en.wikipedia.org/wiki?curid=3740668", "title": "Entropy: A New World View", "text": "Entropy: A New World View\n\nEntropy: A New World View is a non-fiction book by Jeremy Rifkin and Ted Howard, with an Afterword by Nicholas Georgescu-Roegen. It was first published by Viking Press, New York in 1980 ().\n\nA paperback edition was published by Bantam in 1981, in a paperback revised edition, by Bantam Books, in 1989 (). The 1989 revised edition was titled: \"Entropy: Into the Greenhouse World\" ().\n\nIn the book the authors analyze the world's economic and social structures by using the second law of thermodynamics, also known as the law of entropy. The authors argue that humanity is wasting resources at an increasing rate, which if unchecked will lead to the destruction of civilization, which has happened before on a smaller scale to past societies. The book promotes the use of sustainable energy sources and slow resource consumption as the solution to delay or forestall death by entropy.\nCritics: Rifkin's 1980 views assume that entropy is disorder. However, a more modern based on information theory treats entropy as uncertainty. The later approach explains how in some cases entropy increases order. In fact, order spontaneously increases in the world all the time in evolution and also in an economy that is constantly improved. Therefore, Rifkin's book is controversial. See \"Entropy God's Dice Game\" by Kafri and Kafri.\n\nSuch critiques are addressed by properly defining the system under consideration. While biological systems may give the appearance of self-assembling contrary to the Second Law, this illusion disappears when one considers the environment-organism system. While the constituent atoms and molecules comprising the organism, as a collection, decrease in entropy, the surroundings experience a compensating increase in entropy, consistent with the Second Law.\n\n\n"}
{"id": "54028064", "url": "https://en.wikipedia.org/wiki?curid=54028064", "title": "European Academy of Management", "text": "European Academy of Management\n\nThe European Academy of Management (EURAM), founded in 2001, is a learned society dedicated to the advancement of the academic discipline of management in the Europe. It is a member of the European Institute for Advanced Studies in Management network. EURAM runs the \"European Management Review\", a quarterly peer-reviewed academic journal published by John Wiley & Sons, annual conferences for business and management scholars, and training programmes for PhD students, Post-Docs, Research Directors, and Business School Executives.\n\nIt is an organization associated with the Academy of Management since inception and also with the European Institute for Advanced Studies in Management.\n\nThe European Academy of Management (EURAM) was founded in 2001 and its head office is in Brussels, Belgium. The first EURAM Annual Conference was titled \"European Management Research: Trends and Challenges\", and it was organised at the IESE Business School in Barcelona, Spain. EURAM has continued to develop programs and activities to accompany members throughout their professional career life-cycle. Additional initiatives include the creation of the European Management Review in 2003; the Doctoral Consortium started in 2006; Strategic Interest Groups launched in 2009; the European Directors of Research program started in 2009; and the junior faculty program, the EURAM Early Career Colloquium, launched in 2010. \n\n\n\nAt EURAM, Special Interest Groups are organised networks of researchers focused on a specific subfields of management scholarship. Launched in 2009, SIGs are run by members and organize workshops, seminars, conferences throughout the year. The SIGs also organise dedicated tracks at the annual EURAM conferences.\n\nCurrently there are 13 standing SIGs at EURAM:\n\nEuropean Management Review is a quarterly peer-reviewed academic journal published by John Wiley & Sons on behalf of the European Academy of Management. The journal is abstracted and indexed by Current Contents/Social & Behavioral Sciences, Social Sciences Citation Index, Scopus, ProQuest databases, and EBSCO databases. According to the \"Journal Citation Reports\", the journal has an increased 2016 impact factor of 1.333, ranking it 121th out of 191 journals in the category \"Management\".\n"}
{"id": "11774", "url": "https://en.wikipedia.org/wiki?curid=11774", "title": "Field ion microscope", "text": "Field ion microscope\n\nThe Field ion microscope (FIM) was invented by Müller in 1951. It is a type of microscope that can be used to image the arrangement of atoms at the surface of a sharp metal tip.\n\nOn October 11, 1955, Erwin Müller and his Ph.D. student, Kanwar Bahadur (Pennsylvania State University) observed individual tungsten atoms on the surface of a sharply pointed tungsten tip by cooling it to 21 K and employing helium as the imaging gas. Müller & Bahadur were the first persons to observe individual atoms directly.\n\nIn FIM, a sharp (<50 nm tip radius) metal tip is produced and placed in an ultra high vacuum chamber, which is backfilled with an imaging gas such as helium or neon. The tip is cooled to cryogenic temperatures (20–100 K). A positive voltage of 5 to 10 kilovolts is applied to the tip. Gas atoms adsorbed on the tip are ionized by the strong electric field in the vicinity of the tip (thus, \"field ionization\"), becoming positively charged and being repelled from the tip. The curvature of the surface near the tip causes a natural magnification — ions are repelled in a direction roughly perpendicular to the surface (a \"point projection\" effect). A detector is placed so as to collect these repelled ions; the image formed from all the collected ions can be of sufficient resolution to image individual atoms on the tip surface.\n\nUnlike conventional microscopes, where the spatial resolution is limited by the wavelength of the particles which are used for imaging, the FIM is a projection type microscope with atomic resolution and an approximate magnification of a few million times.\n\nFIM like Field Emission Microscopy (FEM) consists of a sharp sample tip and a fluorescent screen (now replaced by a multichannel plate) as the key elements. However, there are some essential differences as follows:\nLike FEM, the field strength at the tip apex is typically a few V/Å. The experimental set-up and image formation in FIM is illustrated in the accompanying figures.\n\nIn FIM the presence of a strong field is critical. The imaging gas atoms (He, Ne) near the tip are polarized by the field and since the field is non-uniform the polarized atoms are attracted towards the tip surface. The imaging atoms then lose their kinetic energy performing a series of hops and accommodate to the tip temperature. Eventually, the imaging atoms are ionized by tunneling electrons into the surface and the resulting positive ions are accelerated along the field lines to the screen to form a highly magnified image of the sample tip.\n\nIn FIM, the ionization takes place close to the tip, where the field is strongest. The electron that tunnels from the atom is picked up by the tip. There is a critical distance, xc, at which the tunneling probability is a maximum. This distance is typically about 0.4 nm. The very high spatial resolution and high contrast for features on the atomic scale arises from the fact that the electric field is enhanced in the vicinity of the surface atoms because of the higher local curvature. The resolution of FIM is limited by the thermal velocity of the imaging ion. Resolution of the order of 1Å (atomic resolution) can be achieved by effective cooling of the tip.\n\nApplication of FIM, like FEM, is limited by the materials which can be fabricated in the shape of a sharp tip, can be used in an ultra high vacuum (UHV) environment, and can tolerate the high electrostatic fields. For these reasons, refractory metals with high melting temperature (e.g. W, Mo, Pt, Ir) are conventional objects for FIM experiments. Metal tips for FEM and FIM are prepared by electropolishing (electrochemical polishing) of thin wires. However, these tips usually contain many asperities. The final preparation procedure involves the in situ removal of these asperities by field evaporation just by raising the tip voltage. Field evaporation is a field induced process which involves the removal of atoms from the surface itself at very high field strengths and typically occurs in the range 2-5 V/Å. The effect of the field in this case is to reduce the effective binding energy of the atom to the surface and to give, in effect, a greatly increased evaporation rate relative to that expected at that temperature at zero fields. This process is self-regulating since the atoms that are at positions of high local curvature, such as adatoms or ledge atoms, are removed preferentially. The tips used in FIM is sharper (tip radius is 100~300 Å) compared to those used in FEM experiments (tip radius ~1000 Å).\n\nFIM has been used to study dynamical behavior of surfaces and the behavior of adatoms on surfaces. The problems studied include adsorption-desorption phenomena, surface diffusion of adatoms and clusters, adatom-adatom interactions, step motion, equilibrium crystal shape, etc. However, there is the possibility of the results being affected by the limited surface area (i.e. edge effects) and by the presence of large electric field.\n\n\n\n"}
{"id": "32684413", "url": "https://en.wikipedia.org/wiki?curid=32684413", "title": "Fizz (novel)", "text": "Fizz (novel)\n\nFizz is a novel by Zvi Schreiber centered on the history of physics. It tells the story of a young woman from the future named Fizz, who time travels to meet physicists such as Aristotle, Galileo, Newton and Einstein, and discuss their work.\nFizz brands itself as an \"edu-novel\" with similarity to the genre of \"Sophie's World\". The book claims to target both young adults and adults, with an amateur interest in physics, as well as teachers and students of physics.\n\nFizz is a young woman from the \"Eco-community\" - a future sect which abandons science and technology. Her father has left this community and invented a time machine. Driven by curiosity about the physical world, Fizz borrows her father's time machine and visits many past physicists from Aristotle to Stephen Hawking.\n\nEach chapter combines some discussion of physics with some fictional plot and personal development of Fizz. Eventually Fizz returns to the future to choose between life inside or outside the eco-community.\n\nOnly a small number of reviews have been published since publication in July 2011. Fizz won a five star rating from ForeWord magazine's Clarion review. The American Association of Physics Teachers invited members to consider trying Fizz in classroom pilots in the July 2011 newsletter.\n\n"}
{"id": "5803737", "url": "https://en.wikipedia.org/wiki?curid=5803737", "title": "Flying Saucers from Outer Space", "text": "Flying Saucers from Outer Space\n\nFlying Saucers from Outer Space (Holt, 1953) is a non-fiction book by Donald Keyhoe about unidentified flying objects, aka UFOs.\n\nIn 1956 a science-fiction film credited as \"suggested by\" the book was made under the title \"Earth vs. the Flying Saucers\", also known as \"Invasion of the Flying Saucers\".\nThe working titles of this film were Attack of the Flying Saucers, Invasion of the Flying Saucers and Flying Saucers from Outer Space. In a letter contained in the film's production file at the AMPAS Library, blacklisted screenwriter Bernard Gordon stated that he wrote the screenplay for this picture using the pseudonym Raymond T. Marcus.\n\n\"The Flying Saucers Are Real\" (also by Keyhoe)\n\n"}
{"id": "10289427", "url": "https://en.wikipedia.org/wiki?curid=10289427", "title": "Foundational Model of Anatomy", "text": "Foundational Model of Anatomy\n\nThe Foundational Model of Anatomy Ontology (FMA) is a reference ontology for the domain of anatomy. It is a symbolic representation of the canonical, phenotypic structure of an organism; a spatial-structural ontology of anatomical entities and relations which form the physical organization of an organism at all salient levels of granularity.\n\nFMA is developed and maintained by the Structural Informatics Group at the University of Washington.\n\nFMA ontology contains approximately 75,000 classes and over 120,000 terms, over 2.1 million relationship instances from over 168 relationship types.\n\n\n"}
{"id": "5072111", "url": "https://en.wikipedia.org/wiki?curid=5072111", "title": "Glossary of entomology terms", "text": "Glossary of entomology terms\n\nThis glossary of entomology describes terms used in the formal study of insect species by entomologists.\n\n \n\n \n\n \n\n\n\n"}
{"id": "7060139", "url": "https://en.wikipedia.org/wiki?curid=7060139", "title": "Grand Rapids Amateur Astronomical Association", "text": "Grand Rapids Amateur Astronomical Association\n\nThe Grand Rapids Amateur Astronomical Association (or \"GRAAA\") is an astronomy group located in Grand Rapids, Michigan. It was formed in 1955 by an enthusiastic group of individuals led by businessman James C. Veen, with a love of astronomy and science. Veen initially provided a meeting place in his office, but died in an automobile accident in 1958.\n\nThe Association participates in public education activities at various metropolitan venues, comet watches, meteor observing, as well as opening their Observatory to the public two nights per month, excluding winter. Besides the public education programs, members involve themselves in many other pursuits from observing programs to astrophotography and CCD imaging.\n\nThe Association owns/ operates the Veen Observatory, located S.W. of Lowell, Michigan, situated on private land per a ninety-nine year lease granted by James M. and H. Evelyn Marron. Construction was started in 1965 and the Observatory was dedicated in 1970, originally with a 12-inch Newtonian reflector constructed by the members. The Chaffee Planetarium made considerable contribution to the building, as well as Kent County businesses and a foundation. It is the largest amateur facility in the state of Michigan, with a 16-inch (Borr), 14-inch (Marron), and 17-inch (Hawkins) telescopes, as well as smaller portable instruments including a hydrogen-alpha solar telescope.\n\nThe GRAAA is a 501(c)(3) non-profit educational and scientific organization dedicated to advancing the study of astronomy and promoting astronomy and science education. Meetings are held monthly, in the warmer months at the Veen Observatory, and currently (2013) at Schuler Books and Music in the city during the rest of the season.\n"}
{"id": "19892026", "url": "https://en.wikipedia.org/wiki?curid=19892026", "title": "Helmut Hölzer", "text": "Helmut Hölzer\n\nHelmut Hoelzer was a Nazi Germany V-2 rocket engineer who was brought to the United States under Operation Paperclip.\n\nIn October 1939, while working for the Telefunken electronics firm in Berlin, Hoelzer met with Ernst Steinhoff, Hermann Steuding, and Wernher von Braun regarding guide beams for a flying body. In late 1940 at Peenemünde, Hoelzer was head of the guide beam division (assistant Henry Otto Hirschler), which developed a guide-plane system which alternates a transmitted signal from two antennas a short distance apart, as well as a vacuum tube mixing device () which corrected for momentum that would perturb an object that had been moved back on-track. By the fall of 1941, Hoelzer's \"mixing device\" was used to provide V-2 rocket rate measurement instead of rate gyros.\n\nThen at the beginning of 1942, Hoelzer built an analog computer to calculate and simulate V-2 rocket trajectories Hoelzer's team also developed the Messina telemetry system. After evacuating Peenemünde for the Alpenfestung (Alpine Fortress), Hoelzer returned to Peenemünde via motorcycle to look for portions of his PhD dissertation prior to surrendering to United States forces at the end of World War II.\n\nOne of his grandchildren is Olympic swimmer Margaret Hoelzer.\n"}
{"id": "14731", "url": "https://en.wikipedia.org/wiki?curid=14731", "title": "Ideogram", "text": "Ideogram\n\nAn ideogram or ideograph (from Greek \"idéa\" \"idea\" and \"gráphō\" \"to write\") is a graphic symbol that represents an idea or concept, independent of any particular language, and specific words or phrases. Some ideograms are comprehensible only by familiarity with prior convention; others convey their meaning through pictorial resemblance to a physical object, and thus may also be referred to as pictograms.\n\nIn proto-writing, used for inventories and the like, physical objects are represented by stylized or conventionalized pictures, or pictograms. For example, the pictorial Dongba symbols without Geba annotation cannot represent the Naxi language, but are used as a mnemonic for reciting oral literature.\nSome systems also use ideograms, symbols denoting abstract concepts.\n\nThe term \"ideogram\" is often used to describe symbols of writing systems such as Egyptian hieroglyphs, Sumerian cuneiform and Chinese characters. However, these symbols are logograms, representing words or morphemes of a particular language rather than objects or concepts. In these writing systems, a variety of strategies were employed in the design of logographic symbols.\nPictographic symbols depict the object referred to by the word, such as an icon of a bull denoting the Semitic word \"ʾālep\" \"ox\".\nSome words denoting abstract concepts may be represented iconically, but most other words are represented using the rebus principle, borrowing a symbol for a similarly-sounding word. Later systems used selected symbols to represent the sounds of the language, for example the adaptation of the logogram for \"ʾālep\" \"ox\" as the letter aleph representing the initial sound of the word, a glottal stop.\n\nMany signs in hieroglyphic as well as in cuneiform writing could be used either logographically or phonetically. For example, the Akkadian sign AN () could be an ideograph for \"deity\", an ideogram for the god Anum in particular, a logograph for the Akkadian stem \"il-\" \"deity\", a logograph for the Akkadian word \"šamu\" \"sky\", or a syllabogram for either the syllable \"an\" or \"il\".\n\nAlthough Chinese characters are logograms, two of the smaller classes in the traditional classification are ideographic in origin:\n\nAn example of ideograms is the collection of 50 signs developed in the 1970s by the American Institute of Graphic Arts at the request of the US Department of Transportation. The system was initially used to mark airports and gradually became more widespread.\n\nMathematical symbols are a type of ideogram.\n\nInspired by inaccurate early descriptions of Chinese and Japanese characters as ideograms, many Western thinkers have sought to design universal written languages, in which symbols denote concepts rather than words. An early proposal was \"An Essay towards a Real Character, and a Philosophical Language\" (1668) by John Wilkins. A recent example is the system of Blissymbols, which was proposed by Charles K. Bliss in 1949 and currently includes over 2,000 symbols.\n\n"}
{"id": "1629075", "url": "https://en.wikipedia.org/wiki?curid=1629075", "title": "Ignition tube", "text": "Ignition tube\n\nAn ignition tube is a piece of laboratory equipment. It is a laboratory tube used much in the same way as a boiling tube except not being as large and thick-walled. It is primarily used to hold small quantities of substances which are undergoing direct heating by a Bunsen burner or other heat source. \n\nIgnition tubes are used in the sodium fusion test.\n\nIgnition tubes are often difficult to clean due to the small bore. When used to heat substances strongly, some char may stick to the walls as well. They are usually disposable.\n\n"}
{"id": "5577839", "url": "https://en.wikipedia.org/wiki?curid=5577839", "title": "InVesalius", "text": "InVesalius\n\nInVesalius is a free medical software used to generate virtual reconstructions of structures in the human body. Based on two-dimensional images, acquired using computed tomography or magnetic resonance imaging equipment, the software generates virtual three-dimensional models correspondent to anatomical parts of the human body. After constructing three-dimensional DICOM images, the software allows the generation of STL (stereolithography) files. These files can be used for rapid prototyping.\n\nInVesalius was developed at CTI (Renato Archer Information Technology Center), a research institute of the Brazilian Science and Technology Center and is available at no cost at the homepage of Public Software Portal homepage. The software license is CC-GPL 2. It is available in English, Japanese, Czech, Portuguese (Brazil), Russian, Spanish, Italian, German, Portuguese, Turkish (Turkey), Romanian, French, Korean, Catalan, Chinese (Taiwan) and Greek.\n\nInVesalius was developed using Python and works under Linux, Windows and Mac OS X. It also uses graphic libraries VTK, wxPython, Numpy, Scipy and GDCM.\n\nThe software’s name is a tribute to Belgian physician Andreas Vesalius (1514–1564), considered the \"father of modern anatomy\".\nDeveloped since 2001 for attending Brazilian Public Hospitals demands, InVesalius development was directed for promoting social inclusion of individuals with severe facial deformities. Since then, however, it has been employed in various research areas of dentistry, medicine, veterinary medicine, paleontology and anthropology. It has been used not only in public hospitals, but also in private clinics and hospitals.\n\nUntil 2017, the software had already been used for generating more than 5000 rapid prototyping models of anatomical structures at Promed project.\n\n\n"}
{"id": "44080879", "url": "https://en.wikipedia.org/wiki?curid=44080879", "title": "Indrani Bose", "text": "Indrani Bose\n\nIndrani Bose (born 15 August 1951) is an , senior Professor at Department of Physics, Bose Institute, Kolkata. Her fields of specialization are in theoretical condensed matter, quantum information theory, statistical physics, biological physics and systems.\n\nProfessor Bose obtained her Ph.D. (Physics) in 1981 from University of Calcutta. She was the first recipient of the Stree Shakthi Science Samman award (2000) for her work on exact solutions of model Hamiltonian (low dimensions) in the context of magnetic systems. \n\nHer research interests include the problem of quantum many body systems, quantum information theory, statistical mechanics and systems biology\n\nShe is a fellow of the Indian Academy of Sciences, Bangalore and of the National Academy of Sciences, Allahabad.\n"}
{"id": "17643161", "url": "https://en.wikipedia.org/wiki?curid=17643161", "title": "Jeheskel Shoshani", "text": "Jeheskel Shoshani\n\nProfessor Jeheskel \"Hezy\" Shoshani (1943 – May 20, 2008) was an evolutionary biologist and an elephant specialist who studied the evolution of elephants for over 35 years.\n\nShoshani established the Elephant Research Foundation in 1977 and was the editor of its publication,Elephant. He published about 200 scientific articles and books on elephants and edited the publication \"Elephant\". He taught at Wayne State University in Detroit, Michigan, for approximately 25 years and at the University of Asmara in Eritrea starting in 1998. In 2007 he moved to Ethiopia to teach at the University of Addis Ababa. Hezy was a passionate advocate of elephant conservation.\n\nShoshani was among several people killed in a terrorism-linked explosion in a public minibus in downtown Addis Ababa, Ethiopia, on May 20, 2008.\n\n"}
{"id": "740561", "url": "https://en.wikipedia.org/wiki?curid=740561", "title": "John J. Kavelaars", "text": "John J. Kavelaars\n\nJ-John Kavelaars, better known as JJ Kavelaars (born 1966), is a Canadian astronomer who was part of a team that discovered several moons of Jupiter, Saturn, Uranus, and Neptune. He is also a discoverer of minor planets.\n\nKavelaars is a graduate of the Glencoe District High School in Glencoe, Ontario, the University of Guelph, and Queen's University, Kingston, Ontario. He is currently an astronomer at the Dominion Astrophysical Observatory in Victoria, B.C.\n\nIn the course of his work, he has been responsible for the discovery of eleven satellites (moons) of Saturn, eight of Uranus, and four of Neptune, and a hundred or so minor planets. Kavelaars is the Coordinator of the Canada–France Ecliptic Plane Survey which is part of the Canada-France-Hawaii Telescope Legacy Survey \"CFHTLS\": a project dedicated to the discovery and tracking of objects in the outer Solar System.\n\nHe is the brother of Canadian actress Ingrid Kavelaars and Canadian fencing athlete Monique Kavelaars.\n\nThe asteroid 154660 Kavelaars was named in his honour on 1 June 2007 by his colleague David D. Balam.\n\n"}
{"id": "3268440", "url": "https://en.wikipedia.org/wiki?curid=3268440", "title": "List of Adobe software", "text": "List of Adobe software\n\nA list of Adobe Inc. products.\n\nAdobe Experience Cloud (AEC) is a collection of integrated online marketing and Web analytics solutions by Adobe Systems. It includes a set of analytics, social, advertising, media optimization, targeting, Web experience management and content management solutions. It includes:\n\nAdobe Creative Suite (CS) was a series of software suites of graphic design, video editing, and web development applications made or acquired by Adobe Systems. It included:\n\nAdobe Creative Cloud is the successor to Creative Suite. It is based on a software as a service model. It includes everything in Creative Suite 6 with the exclusion of Fireworks, Professional Pro and Encore, as the three of them were discontinued. It also introduced a few new programs, including Muse, Animate, InCopy and Story CC Plus.\n\nAdobe Technical Communication Suite is a collection of applications made by Adobe Systems for technical communicators, help authors, instructional designers, and eLearning and training design professionals. It includes:\n\nAdobe eLearning Suite is a collection of applications made by Adobe Systems for learning professionals, instructional designers, training managers, content developers, and educators.\n\n\n"}
{"id": "38209589", "url": "https://en.wikipedia.org/wiki?curid=38209589", "title": "List of defunct medical schools in the United States", "text": "List of defunct medical schools in the United States\n\nThis list of defunct medical schools in the United States includes former medical schools that previously awarded either the Doctor of Medicine (MD) or Doctor of Osteopathic Medicine (DO) degree, either of which is required to become a physician in the United States. MD-granting medical schools are accredited by the Liaison Committee on Medical Education, while DO-granting medical schools are accredited by the American Osteopathic Association Commission on Osteopathic College Accreditation.\n\n\n"}
{"id": "61786", "url": "https://en.wikipedia.org/wiki?curid=61786", "title": "List of diseases (P)", "text": "List of diseases (P)\n\nThis is a list of diseases starting with the letter \"P\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPseudoa–Pseudom\nPseudoo–Pseudo-Z\n\n\n\n\n\n\n"}
{"id": "3858681", "url": "https://en.wikipedia.org/wiki?curid=3858681", "title": "List of dog diseases", "text": "List of dog diseases\n\nThis list of dog diseases is a selection of diseases and other conditions found in the dog. Some of these diseases are unique to dogs or closely related species, while others are found in other animals, including humans. Not all of the articles listed here contain information specific to dogs. Articles with non-dog information are marked with an asterisk (*).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14485834", "url": "https://en.wikipedia.org/wiki?curid=14485834", "title": "List of members of the National Academy of Sciences (Immunology)", "text": "List of members of the National Academy of Sciences (Immunology)\n"}
{"id": "25829369", "url": "https://en.wikipedia.org/wiki?curid=25829369", "title": "List of neuroscience databases", "text": "List of neuroscience databases\n\nA number of online neuroscience databases are available which provide information regarding gene expression, neurons, macroscopic brain structure, and neurological or psychiatric disorders. Some databases contain descriptive and numerical data, some to brain function, others offer access to 'raw' imaging data, such as postmortem brain sections or 3D MRI and fMRI images. Some focus on the human brain, others on non-human.\n\nAs the number of databases that seek to disseminate information about the structure, development and function of the brain has grown, so has the need to collate these resources themselves. As a result, there now exist databases of neuroscience databases, some of which reach over 3000 entries.\n\nNeuroscience feed at RightRelevance.\n\n"}
{"id": "4483356", "url": "https://en.wikipedia.org/wiki?curid=4483356", "title": "Location theory", "text": "Location theory\n\nLocation theory has become an integral part of economic geography, regional science, and spatial economics. Location theory addresses questions of what economic activities are located where and why. Location theory or microeconomic theory generally assumes that agents act in their own self-interest. Firms thus choose locations that maximize their profits and individuals choose locations that maximize their utility.\n\nWhile others should get some credit for earlier work (e.g., Richard Cantillon, Etienne Bonnot de Condillac, David Hume, Sir James D. Steuart, and David Ricardo), it was not until the publication of Johann Heinrich von Thünen's first volume of \"Der Isolierte Staat\" in 1826 that location theory can be said to have really gotten underway. Indeed, the prominent regional scientist Walter Isard has called von Thünen \"the father of location theorists.\" In \"Der Isolierte Staat\", von Thünen notes that the costs of transporting goods consumes some of Ricardo's economic rent. He notes that because these transportation costs and, of course, economic rents, vary across goods, different land uses and use intensities will result with increased distance from the marketplace. However, the discussion was criticized since Johann Heinrich von Thünen oversimplified the problem with his assumptions of, for example, isolated states or single cities.\n\nTord Palander (1935) wrote: Market area division of two competing firms.\n\nA German hegemony of sorts seems to have taken hold in location theory from the time of von Thünen through to Walter Christaller's 1933 book \"Die Zentralen Orte in Sűddeutschland\", which formulated much of what is now understood as central place theory. An especially notable contribution was made by Alfred Weber, who published \"Über den Standort der Industrien\" in 1909. Working from a model akin to a physical frame adapted from some ideas by Pierre Varignon (a Varignon frame), Weber applies freight rates of resources and finished goods, along with the finished good's production function, to develop an algorithm that identifies the optimal location for manufacturing plant. He also introduces distortions induced by labor and both agglomerative and deglomerative forces. Weber then discusses groupings of production units, anticipating Lösch's market areas. \n\nCarl Wilhelm Friedrich Launhardt conceived much of that for which Alfred Weber received credit, prior to Weber's work. Moreover, his contributions are surprisingly more modern in their analytical content than are Weber's. This suggests that Launhardt was ahead of his time and not readily understood by many of his contemporaries. Whether Weber was familiar with Launhardt's publications remains unclear. Weber was most certainly influenced by others, most notably Wilhelm Roscher and Albert Schäffle, who seem likely to have read Launhardt's work. Regardless, location theoretical thought blossomed only after Weber's book was published.\n\nLiterature on site selection theory used to look until recent years at the various issues only from a national point of view. By large, there are no international reviews to be found in these publications. In the US, a country in which industrial site selection played a role very early on, resulting in a very early search for methodical approaches, Edgar M. Hoover was one of the leading pioneers in the field of site analysis. In his book “The Location of Economic Activity”, Hoover compiled crucial criteria of industrial site selection as early as 1948 that still apply today. There were, however, some quite early attempts to combine theories of international trade with nationally oriented site theories in order to develop a site theory with an international perspective. One of these early authors was Ohlin (1952), followed by Sabathil (1969) , Moore (1978), Tesch (1980), and Goette (1994) .\n\nNevertheless, even to this day, this situation has only changed to some extent. Even though since the 1990s it has no longer been only major corporations that expand abroad, and any foreign direct investment results in a site selection, there are still very few well-researched studies on this topic. A specifically international site selection theory is still not discernible. Many current and more recent publications either review site decisions made by individual corporations or analyze them as reference cases. Other publications focus on a cost-specific approach largely driven by site relocations in the context of cost structure optimization within major corporations. However, these publications only rarely and at best cursorily deal with issues of construction and real estate aspects.\n\nTheodor Sabathil’s 1969 dissertation is considered one of the early in-depth studies in the area of international site selection. Therein, Sabathil largely focused on country selection, which is part of the site selection process. In this context, Sabathil compiled a comprehensive catalogue of site factors and a theoretical approach to site selection; the latter does not go into great detail. Neither does Sabathil take any legal, natural, or cultural site factors into consideration. However, he discusses in particular company-specific framework conditions and psychological factors.\n\nThe dissertation submitted in 1980 by Peter Tesch constitutes another milestone in the further development of international site theory. Tesch combines theories of international trade and investment with site theories. He is the first to include country-specific framework conditions in his analysis. The main basis for his comments on the various types of internationalization are location-specific competitive advantages. In this context, Tesch developed a catalogue of criteria for international site decisions grouped into three categories:\n• site factors affecting all company activities\n• availability and costs of the site factors impacting on the production factors\n• turnover-related site factors.\n\nThomas Goette’s 1994 study tries to classify important international site factors and to structure the process of international site selection. Goette distinguishes between economic site conditions (sales potential, competitive conditions, infrastructure and transportation costs, labor, monetary conditions), political site conditions (tax legislation, environmental protection, institutional market entry barriers, support of business, political risks), cultural site conditions (differences in language, mentality, religion, and the lack of acceptancy of foreign companies), and geographical site conditions (climate, topography). This study again demonstrates that an attempt to cover all aspects will result in loss of quality as all factors were not or could not be taken into consideration. Goette also theorizes that, in particular, industrial site decisions within companies are usually once-off and division-related decision-making processes. Based on this, Goette assumes a relatively low learning curve, and hence little potential for improvement for subsequent projects.\n\nAs one of the last major contributions, Thomas Glatte aimed to enhance and globalize the known systems in his book \"International Production Site Selection\" by providing a 10-staged selection process, suggesting selected methods for each selection stage and offering a comprehensive list of criteria for the practitioner.\n\nLocation theory has also been used outside of economics, for example in conservation biology, where it can help to find areas that would be good to study, taking into account previous studies.\n\n\n"}
{"id": "58254869", "url": "https://en.wikipedia.org/wiki?curid=58254869", "title": "Marie Curie-Sklodowska Medal and Prize", "text": "Marie Curie-Sklodowska Medal and Prize\n\nThe Marie Curie-Sklodowska Medal and Prize was established in 2016 by the Institute of Physics. It is named for Marie Curie-Sklodowska, a pioneer in the theory of radioactivity, discoverer of radium and polonium, and the only person to have won the Nobel Prize for both chemistry and physics. The award is for \"distinguished contributions to physics education and to widening participation within it\" and consists of a silver medal and a prize of £1000.\n"}
{"id": "6658294", "url": "https://en.wikipedia.org/wiki?curid=6658294", "title": "Mel Hunter", "text": "Mel Hunter\n\nMilford \"Mel\" Joseph Hunter (July 27, 1927 – February 20, 2004) was a 20th-century American illustrator. He enjoyed a successful career as a science fiction illustrator, producing illustrations for famous science fiction authors such as Isaac Asimov and Robert A. Heinlein, as well as a technical and scientific illustrator for clients such as The Pentagon, Hayden Planetarium, and the Massachusetts Audubon Society.\n\nMel Hunter's life began with a troubled childhood in Oak Park, Illinois, where he was physically and psychologically abused by a humorless father. \"He never knew his mother because she was banished from the household by his father when he was only two years old. While he never forgot the abuse, he didn't seem to dwell on it. Instead, he poured himself into his work and career,\" said his third wife, Susan Smith-Hunter.\n\nHunter entered college a year early, at Northwestern University in Evanston, Illinois. After college he held a variety of odd jobs, but finally landed a draftsman job at Northrop Aircraft Corp in California. In 1950, Hunter decided to pursue a career in art and began to teach himself illustration in his spare time.\n\nWith a growing understanding of the fields of astronomy, astronautics, and aviation Hunter set out to teach himself book and magazine illustration. He moved to New York City during the early 1950s, and by 1953 he had successfully sold his first color cover to Galaxy Science Fiction magazine and talked himself into a technical illustrator at Northrop Aircraft where he painted illustrations of advanced aircraft and simulated combat scenarios.\n\nDuring that time, the most lucrative outlet for space artists was the science-fiction genre. Along with a fertile imagination, Hunter coupled his art with realism and technical accuracy. Hunter's whimsical science fiction robots became his signature to thousands of science fiction fans; the skeletal steel robots graced the covers of the Magazine of Fantasy and Science Fiction well into the 1970s. Hunter's lonely robots were often depicted walking solo through the desolate landscapes of nuclear ruins or alien planets.\n\nHunter was nominated for the Hugo Award for Best Professional Artist for the years 1960-1962.\n\nAs Hunter's science fiction career blossomed, so did his technical and scientific illustrations. Hunter's love of air and space took him from California's desert runways to Florida's seacoast launchpads to illustrate every variety of jet-age aircraft and space-age rocket imaginable—from X-15 to Saturn V.\n\nOne of Hunter's best-known books is \"The Missilemen\", a photo illustrated work published in 1960 by Doubleday. Hunter visited U.S. rocket and missile sites during the late 1950s; he took all of the book's black-and-white photographs. It was a rare look inside the world of rocket scientists and engineers of the early space age. Another Hunter book, \"Strategic Air Command\", received the Aviation Writers' Association highest honors in 1961.\n\n\"Mel launched a career in scientific illustration after he was an established science-fiction illustrator,\" said Smith-Hunter. \"He was very technically accurate and was commissioned to complete 26 paintings of celestial objects for the Hayden Planetarium in New York City.\"\n\nAfter 17 years of technical and scientific illustration, Hunter moved from New York to Chester, Vermont in 1967. He began creating lithographic prints depicting the natural scenes which surrounded him. The following year, he was commissioned to create a series of more than 130 watercolors of \"Birds of the Northeast\" by Abercrombie & Fitch Galleries and Massachusetts Audubon Society. By 1970, Hunter signed a contract with World Publishing Co. for a series of 13 ecological books for children, dealing with topics like the beginning of the earth, mankind, plants, birds, mammals and insects.\n\nIn 1976, after accidental damage to his limestone lithographic drawing, Hunter began using mylar as a medium for his lithography, and published a controversial photo-illustrated article in the American Artist Magazine entitled \"Revolution in Hand-Drawn Lithography\". In 1984, Hunter published his seminal hardcover textbook, \"The New Lithography\" which details the \"Mylar Method\", still in wide use today.\n\nAlthough diagnosed with Parkinson's Disease in the 1990s, Hunter died of bone cancer in February 2004. True to his final wish, an attempt was made to launch his cremated remains into space. A private launch, coordinated by Space Services Inc. on the New Frontier Flight was successful on May 22, 2012.\n\n\n"}
{"id": "5120635", "url": "https://en.wikipedia.org/wiki?curid=5120635", "title": "Melanin theory", "text": "Melanin theory\n\nMelanin theory is a racist claim in Afrocentrism that a higher level of melanin, the primary determinant of skin color in humans, is the cause of an intellectual and physical superiority of dark-skinned people and provides them with supernatural powers. It is considered a racist and pseudoscientific theory.\n\nAccording to Bernard Ortiz De Montellano of Wayne State University, \"The alleged properties of melanin, mostly unsupported, irrelevant, or distortions of the scientific literature, are (...) used to justify Afrocentric assertions. One of the most common is that humans evolved as blacks in Africa, and that whites are mutants (albinos, or melanin recessives)\". The melanin hypothesis was supported by Leonard Jeffries, who according to \"Time\" magazine, believes that \"melanin, the dark skin pigment, gives blacks intellectual and physical superiority over whites\".\n\n\nIn 2006, the views of adherents and critics of melanin theory were dramatized in Cassandra Medley's play, \"Relativity\".\n\n"}
{"id": "262765", "url": "https://en.wikipedia.org/wiki?curid=262765", "title": "Operation Latchkey", "text": "Operation Latchkey\n\nOperation Latchkey was a series of 38 nuclear tests conducted by the United States in 1966-1967 at the Nevada Test Site. These tests followed the \"Operation Flintlock (nuclear test)\" series and preceded the \"Operation Crosstie\" series.\n"}
{"id": "35475816", "url": "https://en.wikipedia.org/wiki?curid=35475816", "title": "Outline of Apple Inc.", "text": "Outline of Apple Inc.\n\nThe following outline is provided as an overview of and topical guide to Apple Inc.:\n\nApple Inc. (previously Apple Computer, Inc.) – American multinational corporation that designs and sells consumer electronics, computer software, and personal computers. The company's best-known hardware products are the Macintosh line of computers, the iPod, the iPhone and the iPad. Its best-known software includes the macOS and iOS operating systems, and the iTunes media browser. , Apple has 425 retail stores in 16 countries (consisting of 254 in the US and 171 elsewhere), and an online store (available in 39 countries).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4671299", "url": "https://en.wikipedia.org/wiki?curid=4671299", "title": "Quindar tones", "text": "Quindar tones\n\nQuindar tones, most often referred to as the \"beeps\" that were heard during the American Apollo space missions, were a means by which remote transmitters on Earth were turned on and off so that the Capsule communicator (CapCom) could communicate with the crews of the spacecraft. It was a means of in-band signaling to simulate the action of the push-to-talk and release-to-listen (often referred to as PTT) button commonly found on two-way radio systems and walkie-talkies.\n\nWhen Mission Control (in Houston, Texas) wanted to talk to astronauts, the capsule communicator (CAPCOM) pushed a button (push-to-talk, or PTT) that turned on the transmitter, then spoke, then released the button. When the transmitter is local, this is easy to arrange - the transmitter is connected directly to the PTT button. But to stay in continuous contact with the astronauts as they orbit the Earth, or travel to the Moon, NASA had to use tracking stations all around the world, switching from one station to the next as needed. To get the voice signal to the remote transmitter, dedicated telephone lines connected these stations to Houston. NASA could either build a parallel system for operating the transmitters - one line to carry the audio and another to carry the control signal for the PTT button (out-of-band signalling), or combine these two systems together, using audio tones to turn the transmitter on and off. Since dedicated phone lines were a very expensive measure at the time, NASA chose the use of tones to reduce the operating cost of the network. The same system was used in Project Gemini and was still in use with half duplex UHF Space Shuttle communications for transmitter RF keying.\n\nWith modern digital communication systems, Quindar tones are no longer necessary because a single communication line (such as a fiber optic cable) can simultaneously carry multiple communication channels in the form of data comprising both speech and signaling (the PTT signal), as well as video and telemetry.\n\nThe Quindar system, named after its manufacturer, used two tones, both being pure sine waves that were 250ms long. The \"intro tone\" was generated at 2,525 Hz and signaled the \"key down\" key-press of the PTT button and unmuted the audio. The \"outro tone\" was slightly lower at 2,475 Hz and signalled the release of the PTT button and muted the audio. The two tones were generated by special equipment located at Mission Control, and they were decoded by detectors located at the various tracking stations.\n\nThe selection of the tones allowed them to travel in the same passband as a human voice, which has a range from roughly 300 Hz to 3,000 Hz.\n\nTwo common misconceptions surround Quindar tones. The first is that one tone came from Earth and the other from the transmitters used by the astronauts while in space. This confusion exists because many ground-to-space transmissions were initiated by Mission Control and responded to by the astronauts. In this sequence, the CapCom would press the PTT, which would send the intro tone, and then speak. When finished speaking, the CapCom would release the PTT, which would send the outro tone, and the astronauts would respond to Mission Control. Therefore, those transmissions would consist of a \"beep\" (PTT press) followed by Houston talking, then another \"beep\" (PTT release) and finally the voice of the astronauts.\n\nAnother misconception about Quindar tones is that they were designed to signal the end of a transmission, similar to a courtesy tone used on many half-duplex radio repeaters. Although the astronauts may have secondarily used the Quindar outro tone to know when the CAPCOM had started/stopped speaking, no equivalent existed for Mission Control because the astronauts keyed their transmissions locally (inside the spacecraft) using either a PTT or VOX, neither of which required Quindar tones. Additionally, separate radio frequencies allowed both Houston and the astronauts to talk simultaneously if they wished and thereby made a courtesy tone as a way to minimize the possibility of both of them speaking at the same time unnecessary.\n\nQuindar tones were named for the manufacturer Quindar Electronics, Inc. Glen Swanson, historian at NASA's Johnson Space Center who edited the \"Mission Transcript Collection\", and Steve Schindler, an engineer with voice systems engineering at NASA's Kennedy Space Center, confirmed the origin of the name. \"Quindar tones, named after the manufacturer of the tone generation and detection equipment, are actually used to turn on and off, or 'key', the remote transmitters at the various tracking stations.\"\n\n"}
{"id": "55571161", "url": "https://en.wikipedia.org/wiki?curid=55571161", "title": "RACE (Europe)", "text": "RACE (Europe)\n\nRACE program (Research and Development in Advanced Communications Technologies in Europe) was a program launched in 1980s by the Commission of European Communities to pave the way towards commercial use of Integrated Broadband Communication (IBC) in Europe in late 1990s.\n"}
{"id": "43123332", "url": "https://en.wikipedia.org/wiki?curid=43123332", "title": "Richard E. Cutkosky", "text": "Richard E. Cutkosky\n\nRichard E. Cutkosky (29 July 1928 – 17 June 1993) was a physicist, best known for the Cutkosky cutting rules in quantum field theory, which give a simple way to calculate the discontinuity of the scattering amplitude by Feynman diagrams.\n\n"}
{"id": "5548333", "url": "https://en.wikipedia.org/wiki?curid=5548333", "title": "Specialty engineering", "text": "Specialty engineering\n\nIn the domain of systems engineering, Specialty Engineering is defined as and includes the engineering disciplines that are not typical of the main engineering effort. More common engineering efforts in systems engineering such as hardware, software, and human factors engineering may be used as major elements in a majority of systems engineering efforts and therefore are not viewed as \"special\". \n\nExamples of specialty engineering include electromagnetic interference, safety, and physical security.\n\nLess common engineering domains such as electromagnetic interference, electrical grounding, safety, security, electrical power filtering/uninterruptible supply, manufacturability, and environmental engineering may be included in systems engineering efforts where they have been identified to address special system implementations. These less common but just as important engineering efforts are then viewed as \"specialty engineering\".\n\nHowever, if the specific system has a standard implementation of environmental or security for example, the situation is reversed and the human factors engineering or hardware/software engineering may be the \"specialty engineering\" domain. \n\nThe key take away is; the context of the system engineering project and unique needs of the project are fundamental when thinking of what are the specialty engineering efforts. \n\nThe benefit of citing \"specialty engineering\" in planning is the notice to all team levels that special management and science factors may need to be accounted for and may influence the project.\n\nSpecialty engineering may be cited by commercial entities and others to specify their unique abilities.\n\nEisner, Howard. (2002). \"Essentials of Project and Systems Engineering Management\". Wiley. p. 217.\n"}
{"id": "48811942", "url": "https://en.wikipedia.org/wiki?curid=48811942", "title": "The Stars: A New Way to See Them", "text": "The Stars: A New Way to See Them\n\nThe Stars: A New Way to See Them is an astronomy book by H. A. Rey. It was first published in 1952 (Houghton Mifflin, Boston) and revised in 1962. It was updated again by Chris Dolan in 1997. Other editions were: Chatto and Windus, London, 1975; \"A New Way to see the Stars\" Paul Hamlyn, London, 1966; Enl. World-wide ed. Houghton Mifflin, 1967.\n\nIn this book, Rey set out to create a graphical, simpler view of the constellations that created a more realistic depiction of the images the constellations were supposed to represent.\n\nRey's interest in astronomy began during World War I and led to his desire to redraw constellation diagrams, which he found difficult to remember, so that they were more intuitive. This led to the 1952 publication of \"The Stars: A New Way to See Them,\" (). His constellation diagrams were adopted widely and now appear in many astronomy guides, such as Donald H. Menzel's \"A Field Guide to the Stars and Planets\". As of 2008 \"The Stars: A New Way to See Them,\" and a simplified presentation for children called \"Find the Constellations,\" are still in print. A new edition of \"Find the Constellations\" was released in 2008, updated with modern fonts, the new status of Pluto, and some more current measurements of planetary sizes and orbital radii.\n"}
{"id": "37892", "url": "https://en.wikipedia.org/wiki?curid=37892", "title": "Thrust", "text": "Thrust\n\nThrust is a reaction force described quantitatively by Newton's third law. When a system expels or accelerates mass in one direction, the accelerated mass will cause a force of equal magnitude but opposite direction on that system.\nThe force applied on a surface in a direction perpendicular or normal to the surface is also called thrust. Force, and thus thrust, is measured using the International System of Units (SI) in newtons (symbol: N), and represents the amount needed to accelerate 1 kilogram of mass at the rate of 1 meter per second per second. In mechanical engineering, force orthogonal to the main load (such as in parallel helical gears) is referred to as thrust.\n\nA fixed-wing aircraft generates forward thrust when air is pushed in the direction opposite to flight. This can be done in several ways including by the spinning blades of a propeller, or a rotating fan pushing air out from the back of a jet engine, or by ejecting hot gases from a rocket engine. The forward thrust is proportional to the mass of the airstream multiplied by the difference in velocity of the airstream. Reverse thrust can be generated to aid braking after landing by reversing the pitch of variable-pitch propeller blades, or using a thrust reverser on a jet engine. Rotary wing aircraft and thrust vectoring V/STOL aircraft use engine thrust to support the weight of the aircraft, and vector sum of this thrust fore and aft to control forward speed.\n\nA motorboat generates thrust (or reverse thrust) when the propellers are turned to accelerate water backwards (or forwards). The resulting thrust pushes the boat in the opposite direction to the sum of the momentum change in the water flowing through the propeller.\n\nA rocket is propelled forward by a thrust force equal in magnitude, but opposite in direction, to the time-rate of momentum change of the exhaust gas accelerated from the combustion chamber through the rocket engine nozzle. This is the exhaust velocity with respect to the rocket, times the time-rate at which the mass is expelled, or in mathematical terms:\nWhere T is the thrust generated (force), formula_2 is the rate of change of mass with respect to time (mass flow rate of exhaust), and v is the speed of the exhaust gases measured relative to the rocket.\n\nFor vertical launch of a rocket the initial thrust at liftoff must be more than the weight.\n\nEach of the three Space Shuttle Main Engines could produce a thrust of 1.8 MN, and each of the Space Shuttle's two Solid Rocket Boosters 14.7 MN, together 29.4 MN.\n\nBy contrast, the simplified Aid For EVA Rescue (SAFER) has 24 thrusters of 3.56 N each.\n\nIn the air-breathing category, the AMT-USA AT-180 jet engine developed for radio-controlled aircraft produce 90 N (20 lbf) of thrust. The GE90-115B engine fitted on the Boeing 777-300ER, recognized by the Guinness Book of World Records as the \"World's Most Powerful Commercial Jet Engine,\" has a thrust of 569 kN (127,900 lbf).\n\nThe power needed to generate thrust and the force of the thrust can be related in a non-linear way. In general, formula_3. The proportionality constant varies, and can be solved for a uniform flow:\n\nNote that these calculations are only valid for when the incoming air is accelerated from a standstill – for example when hovering.\n\nThe inverse of the proportionality constant, the \"efficiency\" of an otherwise-perfect thruster, is proportional to the area of the cross section of the propelled volume of fluid (formula_8) and the density of the fluid (formula_9). This helps to explain why moving through water is easier and why aircraft have much larger propellers than watercraft.\n\nA very common question is how to contrast the thrust rating of a jet engine with the power rating of a piston engine. Such comparison is difficult, as these quantities are not equivalent. A piston engine does not move the aircraft by itself (the propeller does that), so piston engines are usually rated by how much power they deliver to the propeller. Except for changes in temperature and air pressure, this quantity depends basically on the throttle setting.\n\nA jet engine has no propeller, so the propulsive power of a jet engine is determined from its thrust as follows. Power is the force (F) it takes to move something over some distance (d) divided by the time (t) it takes to move that distance:\n\nIn case of a rocket or a jet aircraft, the force is exactly the thrust (T) produced by the engine. If the rocket or aircraft is moving at about a constant speed, then distance divided by time is just speed, so power is thrust times speed:\n\nThis formula looks very surprising, but it is correct: the \"propulsive power\" (or \"power available\" ) of a jet engine increases with its speed. If the speed is zero, then the propulsive power is zero. If a jet aircraft is at full throttle but attached to a static test stand, then the jet engine produces no propulsive power, however thrust is still produced. Compare that to a piston engine. The combination piston engine–propeller also has a propulsive power with exactly the same formula, and it will also be zero at zero speed – but that is for the engine–propeller set. The engine alone will continue to produce its rated power at a constant rate, whether the aircraft is moving or not.\n\nNow, imagine the strong chain is broken, and the jet and the piston aircraft start to move. At low speeds:\nThe piston engine will have constant 100% power, and the propeller's thrust will vary with speed\nThe jet engine will have constant 100% thrust, and the engine's power will vary with speed\nIf a powered aircraft is generating thrust T and experiencing drag D, the difference between the two, T − D, is termed the excess thrust. The instantaneous performance of the aircraft is mostly dependent on the excess thrust.\n\nExcess thrust is a vector and is determined as the vector difference between the thrust vector and the drag vector.\n\nThe centre of thrust for an object is an average point at which the total thrust may be considered to apply. It may differ from the centre of gravity.\n\n"}
{"id": "5987648", "url": "https://en.wikipedia.org/wiki?curid=5987648", "title": "Uncertainty quantification", "text": "Uncertainty quantification\n\nUncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if we exactly knew the speed, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.\n\nMany problems in the natural sciences and engineering are also rife with sources of uncertainty. Computer experiments on computer simulations are the most common approach to study problems in uncertainty quantification.\n\nUncertainty can enter mathematical models and experimental measurements in various contexts. One way to categorize the sources of uncertainty is to consider:\n\n\nUncertainty is sometimes classified into two categories, although the validity of this categorization is open to debate. These categories are prominently seen in medical applications.\n\n\nIn real life applications, both kinds of uncertainties are present. Uncertainty quantification intends to work toward reducing epistemic uncertainties to aleatoric uncertainties. The quantification for the aleatoric uncertainties can be relatively straightforward to perform, depending on the application. Techniques such as the Monte Carlo method are frequently used. A probability distribution can be represented by its moments (in the Gaussian case, the mean and covariance suffice, although it should be noted that, in general, even knowledge of all moments to arbitrarily high order still does not specify the distribution function uniquely), or more recently, by techniques such as Karhunen–Loève and polynomial chaos expansions. To evaluate epistemic uncertainties, the efforts are made to gain better knowledge of the system, process or mechanism. Methods such as probability bounds analysis, fuzzy logic or evidence theory (Dempster–Shafer theory – a generalization of the Bayesian theory of subjective probability) are used.\n\nThere are two major types of problems in uncertainty quantification: one is the forward propagation of uncertainty (where the various sources of uncertainty are propagated through the model to predict the overall uncertainty in the system response) and the other is the inverse assessment of model uncertainty and parameter uncertainty (where the model parameters are calibrated simultaneously using test data). There has been a proliferation of research on the former problem and a majority of uncertainty analysis techniques were developed for it. On the other hand, the latter problem is drawing increasing attention in the engineering design community, since uncertainty quantification of a model and the subsequent predictions of the true system response(s) are of great interest in designing robust systems.\n\nUncertainty propagation is the quantification of uncertainties in system output(s) propagated from uncertain inputs. It focuses on the influence on the outputs from the \"parametric variability\" listed in the sources of uncertainty. The targets of uncertainty propagation analysis can be:\n\nGiven some experimental measurements of a system and some computer simulation results from its mathematical model, inverse uncertainty quantification estimates the discrepancy between the experiment and the mathematical model (which is called bias correction), and estimates the values of unknown parameters in the model if there are any (which is called parameter calibration or simply calibration). Generally this is a much more difficult problem than forward uncertainty propagation; however it is of great importance since it is typically implemented in a model updating process. There are several scenarios in inverse uncertainty quantification:\nBias correction quantifies the \"model inadequacy\", i.e. the discrepancy between the experiment and the mathematical model. The general model updating formula for bias correction is:\nwhere formula_2 denotes the experimental measurements as a function of several input variables formula_3, formula_4 denotes the computer model (mathematical model) response, formula_5 denotes the additive discrepancy function (aka bias function), and formula_6 denotes the experimental uncertainty. The objective is to estimate the discrepancy function formula_5, and as a by-product, the resulting updated model is formula_8. A prediction confidence interval is provided with the updated model as the quantification of the uncertainty.\n\nParameter calibration estimates the values of one or more unknown parameters in a mathematical model. The general model updating formulation for calibration is:\nwhere formula_10 denotes the computer model response that depends on several unknown model parameters formula_11, and formula_12 denotes the true values of the unknown parameters in the course of experiments. The objective is to either estimate formula_12, or to come up with a probability distribution of formula_12 that encompasses the best knowledge of the true parameter values.\n\nIt considers an inaccurate model with one or more unknown parameters, and its model updating formulation combines the two together:\nIt is the most comprehensive model updating formulation that includes all possible sources of uncertainty, and it requires the most effort to solve.\n\nMuch research has been done to solve uncertainty quantification problems, though a majority of them deal with uncertainty propagation. During the past one to two decades, a number of approaches for inverse uncertainty quantification problems have also been developed and have proved to be useful for most small- to medium-scale problems.\n\nExisting uncertainty propagation approaches include probabilistic approaches and non-probabilistic approaches. There are basically five categories of probabilistic approaches for uncertainty propagation:\nFor non-probabilistic approaches, interval analysis, Fuzzy theory, possibility theory and evidence theory are among the most widely used.\n\nThe probabilistic approach is considered as the most rigorous approach to uncertainty analysis in engineering design due to its consistency with the theory of decision analysis. Its cornerstone is the calculation of probability density functions for sampling statistics. This can be performed rigorously for random variables that are obtainable as transformations of Gaussian variables, leading to exact confidence intervals.\n\nIn regression analysis and least squares problems, the standard error of parameter estimates is readily available, which can be expanded into a confidence interval.\n\nSeveral methodologies for inverse uncertainty quantification exist under the Bayesian framework. The most complicated direction is to aim at solving problems with both bias correction and parameter calibration. The challenges of such problems include not only the influences from model inadequacy and parameter uncertainty, but also the lack of data from both computer simulations and experiments. A common situation is that the input settings are not the same over experiments and simulations.\n\nAn approach to inverse uncertainty quantification is the modular Bayesian approach. The modular Bayesian approach derives its name from its four-module procedure. Apart from the current available data, a prior distribution of unknown parameters should be assigned.\n\n\nTo address the issue from lack of simulation results, the computer model is replaced with a Gaussian Process (GP) model\nwhere\nformula_18 is the dimension of input variables, and formula_19 is the dimension of unknown parameters.While formula_20 is pre-defined, formula_21, known as \"hyperparameters\" of the GP model, need to be estimated via maximum likelihood estimation (MLE). This module can be considered as a generalized Kriging method.\n\nSimilarly with the first module, the discrepancy function is replaced with a GP model\nwhere\nTogether with the prior distribution of unknown parameters, and data from both computer models and experiments, one can derive the maximum likelihood estimates for formula_24. At the same time, formula_25 from Module 1 gets updated as well.\n\nBayes' theorem is applied to calculate the posterior distribution of the unknown parameters:\nwhere formula_27 includes all the fixed hyperparameters in previous modules.\n\n\nFully Bayesian approach requires that not only the priors for unknown parameters formula_11 but also the priors for the other hyperparameters formula_27 should be assigned. It follows the following steps:\n\n\nHowever, the approach has significant drawbacks:\n\nThe fully Bayesian approach requires a huge amount of calculations and may not yet be practical for dealing with the most complicated modelling situations.\n\nThe theories and methodologies for uncertainty propagation are much better established, compared with inverse uncertainty quantification. For the latter, several difficulties remain unsolved:\n\n\n"}
