{"id": "729189", "url": "https://en.wikipedia.org/wiki?curid=729189", "title": "A Treatise on Electricity and Magnetism", "text": "A Treatise on Electricity and Magnetism\n\nA Treatise on Electricity and Magnetism is a two-volume treatise on electromagnetism written by James Clerk Maxwell in 1873. Maxwell was revising the \"Treatise\" for a second edition when he died in 1879. The revision was completed by William Davidson Niven for publication in 1881. A third edition was prepared by J. J. Thomson for publication in 1892.\n\nAccording to one historian,\n\nMaxwell introduced the use of vector fields, and his labels have been perpetuated:\n\nMaxwell's work is considered an exemplar of rhetoric of science:\n\nPreliminary. On the Measurement of Quantities.\n\n\"PART I. Electrostatics. \"\n\n\n\"PART II. Electrokinematics.\"\n\n\n\"PART III Magnetism\"\n\n\n\"Part IV. Electromagnetism.\"\n\n\nOn April 24, 1873, Nature announced the publication with an extensive description and much praise. When the second edition was published in 1881, George Chrystal wrote the review for \"Nature\".\n\nPierre Duhem published a critical essay outlining mistakes he found in Maxwell's \"Treatise\". Duhem's book was reviewed in \"Nature\".\n\nHermann von Helmholtz (1881): \"Now that the mathematical interpretations of Faraday's conceptions regarding the nature of electric and magnetic force has been given by Clerk Maxwell, we see how great a degree of exactness and precision was really hidden behind Faraday's words…it is astonishing in the highest to see what a large number of general theories, the mechanical deduction of which requires the highest powers of mathematical analysis, he has found by a kind of intuition, with the security of instinct, without the help of a single mathematical formula.\"\n\nOliver Heaviside (1893):”What is Maxwell's theory? The first approximation is to say: There is Maxwell's book as he wrote it; there is his text, and there are his equations: together they make his theory. But when we come to examine it closely, we find that this answer is unsatisfactory. To begin with, it is sufficient to refer to papers by physicists, written say during the first twelve years following the first publication of Maxwell's treatise to see that there may be much difference of opinion as to what his theory is. It may be, and has been, differently interpreted by different men, which is a sign that is not set forth in a perfectly clear and unmistakable form. There are many obscurities and some inconsistencies. Speaking for myself, it was only by changing its form of presentation that I was able to see it clearly, and so as to avoid the inconsistencies. Now there is no finality in a growing science. It is, therefore, impossible to adhere strictly to Maxwell's theory as he gave it to the world, if only on account of its inconvenient form.\n\nAlexander Macfarlane (1902): \"This work has served as the starting point of many advances made in recent years. Maxwell is the scientific ancestor of Hertz, Hertz of Marconi and all other workers at wireless telegraphy.\n\nOliver Lodge (1907) \"Then comes Maxwell, with his keen penetration and great grasp of thought, combined with mathematical subtlety and power of expression; he assimilates the facts, sympathizes with the philosophic but untutored modes of expression invented by Faraday, links the theorems of Green and Stokes and Thomson to the facts of Faraday, and from the union rears the young modern science of electricity...\"\n\nE. T. Whittaker (1910): \"In this celebrated work is comprehended almost every branch of electric and magnetic theory, but the intention of the writer was to discuss the whole from a single point of view, namely, that of Faraday, so that little or no account was given of the hypotheses that had been propounded in the two preceding decades by the great German electricians...The doctrines peculiar to Maxwell ... were not introduced in the first volume, or in the first half of the second.\"\n\nAlbert Einstein (1931): \"Before Maxwell people conceived of physical reality – in so far as it is supposed to represent events in nature – as material points, whose changes consist exclusively of motions, which are subject to total differential equations. After Maxwell they conceived physical reality as represented by continuous fields, not mechanically explicable, which are subject to partial differential equations. This change in the conception of reality is the most profound and fruitful one that has come to physics since Newton; but it has at the same time to be admitted that the program has by no means been completely carried out yet.\"\n\nRichard P. Feynman (1964): \"From a long view of the history of mankind—seen from, say, ten thousand years from now—there can be little doubt that the most significant event of the 19th century will be judged as Maxwell's discovery of the laws of electrodynamics. The American Civil War will pale into provincial insignificance in comparison with this important scientific event of the same decade.\"\n\nL. Pearce Williams (1991): \"In 1873, James Clerk Maxwell published a rambling and difficult two-volume \"Treatise on Electricity and Magnetism\" that was destined to change the orthodox picture of physical reality. This treatise did for electromagnetism what Newton's \"Principia\" had done for classical mechanics. It not only provided the mathematical tools for the investigation and representation of the whole of electromagnetic theory, but it altered the very framework of both theoretical and experimental physics. Although the process had been going on throughout the nineteenth century, it was this work that finally displaced action at a distance physics and substituted the physics of the field.\"\n\nMark P. Silverman (1998) \"I studied the principles on my own – in this case with Maxwell's \"Treatise\" as both my inspiration and textbook. This is not an experience that I would necessarily recommend to others. For all his legendary gentleness, Maxwell is a demanding teacher, and his magnum opus is anything but coffee-table reading...At the same time, the experience was greatly rewarding in that I had come to understand, as I realized much later, aspects of electromagnetism that are rarely taught at any level today and that reflect the unique physical insight of their creator.\n\nAndrew Warwick (2003): \"In developing the mathematical theory of electricity and magnetism in the \"Treatise\", Maxwell made a number of errors, and for students with only a tenuous grasp of the physical concepts of basic electromagnetic theory and the specific techniques to solve some problems, it was extremely difficult to discriminate between cases where Maxwell made an error and cases where they simply failed to follow the physical or mathematical reasoning.\"\n\n\n"}
{"id": "25902309", "url": "https://en.wikipedia.org/wiki?curid=25902309", "title": "Absolute scale", "text": "Absolute scale\n\nAn absolute scale is a system of measurement that begins at a minimum, or zero point, and progresses in only one direction. An absolute scale differs from an arbitrary, or \"relative,\" scale, which begins at some point selected by a person and can progress in both directions. An absolute scale begins at a natural minimum, leaving only one direction in which to progress.\n\nAn absolute scale can only be applied to measurements in which a true minimum is known to exist. Time, for example, which does not have a clearly known beginning, is measured on a relative scale, with an arbitrary zero-point such as the conventional date of the birth of Jesus of Nazareth or the accession of an emperor. Temperature, on the other hand, has a known minimum, absolute zero (where all vibrational motion of atoms ceases), and therefore, can be measured either in absolute terms (kelvins or degrees Rankine), or relative to a reference temperature such as the freezing point of water at a specified pressure (Celsius and Reaumur) or the lowest temperature attainable in 1724 (Fahrenheit).\n\nPressure is a force that can be measured absolutely, because the natural minimum of pressure is total vacuum. Pressure is frequently measured with reference to atmospheric pressure rather than on any absolute scale, relative to complete and perfect vacuum; it is technologically simpler and cheaper. It may also be more convenient to use relative scales, because, with things like pneumatics and hydraulics, the amount of energy transferred is reduced by the relative \"backpressure\" of the atmosphere. (e.g.: 15 psi of air in a tank at sea level will become 30 psi in the vacuum of space.) Therefore, with measurements of things like blood pressure or tire pressure, a measurement relative to air pressure is a better indication of \"burst pressure\" (damage threshold) than an absolute scale. Absolute scales are typically used in science, deep vacuum measurements (where the fluctuating pressure of the atmosphere becomes a nuisance), aeronautics (where precise measurements of the atmosphere are needed to determine altitude), or lighting construction (where the relative pressure of the atmosphere is inconsequential), and are measured in units of \"atmospheres\" or torr. Barometers do measure absolute pressure by holding a vacuum at the top of the mercury column or one side of a diaphragm, but that vacuum is awkward to achieve and maintain. Thus, while the general public may be familiar with measurements of absolute pressure from weather forecasts, most pressures such as tire pressures and water pressures are measured relative to atmospheric pressure using cheaper and simpler pressure gauges. For this reason, the pressure relative to atmospheric pressure is called gauge pressure and measurements given in units like pounds per square inch (abbreviated lbf/in or psi) are often shown as \"psig\" (the \"g\" standing for gauge) or \"psia\" (\"a\" for absolute).\n\nAbsolute scales are used when precise values are needed in comparison to a natural, unchanging zero point. Measurements of length, area and volume are inherently absolute, although measurements of distance are often based on an arbitrary starting point. Measurements of weight can be absolute, such as atomic weight, but more often they are measurements of the relationship between two masses, while measurements of speed are relative to an arbitrary reference frame. (Unlike many other measurements without a known, absolute minimum, speed has a known maximum and can be measured from a purely relative scale.) Absolute scales can be used for measuring a variety of things, from the flatness of an optical flat to neuroscientific tests.\n"}
{"id": "52518811", "url": "https://en.wikipedia.org/wiki?curid=52518811", "title": "Acidipila rosea", "text": "Acidipila rosea\n\nAcidipila rosea is a Gram-negative, chemoorganotrophic, acidophilic and non-motile bacterium from the genus of Acidipila which has been isolated from an acid mine drainage.\n\n"}
{"id": "31720353", "url": "https://en.wikipedia.org/wiki?curid=31720353", "title": "Allison Glacier (Antarctica)", "text": "Allison Glacier (Antarctica)\n\nAllison Glacier is a small glacier in Victoria Land, Antarctica. Its head is located just north of Mount Huggins, descending from the west slopes of Royal Society Range into Skelton Glacier. Abbott Spur separates the lower ends of Rutgers Glacier from Allison Glacier.\n\nIt was named by the Advisory Committee on Antarctic Names in 1963 for Lieutenant Commander John K. Allison, United States Navy, officer in charge of the wintering-over detachment of U.S. Navy Air Development Squadron Six (VX-6) at McMurdo Station, 1959.\n\n"}
{"id": "444563", "url": "https://en.wikipedia.org/wiki?curid=444563", "title": "Artificial Reality", "text": "Artificial Reality\n\nArtificial reality is a book series by Myron W. Krueger about interactive immersive environments (or virtual realities), based on video recognition techniques, that put a user in full, unencumbered contact with the digital world. He started this work in the late 1960s and is considered to be a key figure in the early innovation of virtual reality. \"Artificial Reality\" was published in 1983 and updated in \"Artificial Reality II\" in 1991 (both published by Addison-Wesley).\n\n\n"}
{"id": "56113766", "url": "https://en.wikipedia.org/wiki?curid=56113766", "title": "Caldimicrobium", "text": "Caldimicrobium\n\nCaldimicrobium is a genus of bacteria from the family of Thermodesulfobacteriaceae.\n\n"}
{"id": "8714796", "url": "https://en.wikipedia.org/wiki?curid=8714796", "title": "Chou–Fasman method", "text": "Chou–Fasman method\n\nThe Chou–Fasman method is an empirical technique for the prediction of secondary structures in proteins, originally developed in the 1970s by Peter Y. Chou and Gerald D. Fasman. The method is based on analyses of the relative frequencies of each amino acid in alpha helices, beta sheets, and turns based on known protein structures solved with X-ray crystallography. From these frequencies a set of probability parameters were derived for the appearance of each amino acid in each secondary structure type, and these parameters are used to predict the probability that a given sequence of amino acids would form a helix, a beta strand, or a turn in a protein. The method is at most about 50–60% accurate in identifying correct secondary structures, which is significantly less accurate than the modern machine learning–based techniques.\n\nThe original Chou–Fasman parameters found some strong tendencies among individual amino acids to prefer one type of secondary structure over others. Alanine, glutamate, leucine, and methionine were identified as helix formers, while proline and glycine, due to the unique conformational properties of their peptide bonds, commonly end a helix. The original Chou–Fasman parameters were derived from a very small and non-representative sample of protein structures due to the small number of such structures that were known at the time of their original work. These original parameters have since been shown to be unreliable and have been updated from a current dataset, along with modifications to the initial algorithm.\n\nThe Chou–Fasman method takes into account only the probability that each individual amino acid will appear in a helix, strand, or turn. Unlike the more complex GOR method, it does not reflect the conditional probabilities of an amino acid to form a particular secondary structure given that its neighbors already possess that structure. This lack of cooperativity increases its computational efficiency but decreases its accuracy, since the propensities of individual amino acids are often not strong enough to render a definitive prediction.\n\nThe Chou–Fasman method predicts helices and strands in a similar fashion, first searching linearly through the sequence for a \"nucleation\" region of high helix or strand probability and then extending the region until a subsequent four-residue window carries a probability of less than 1. As originally described, four out of any six contiguous amino acids were sufficient to nucleate helix, and three out of any contiguous five were sufficient for a sheet. The probability thresholds for helix and strand nucleations are constant but not necessarily equal; originally 1.03 was set as the helix cutoff and 1.00 for the strand cutoff.\n\nTurns are also evaluated in four-residue windows, but are calculated using a multi-step procedure because many turn regions contain amino acids that could also appear in helix or sheet regions. Four-residue turns also have their own characteristic amino acids; proline and glycine are both common in turns. A turn is predicted only if the turn probability is greater than the helix or sheet probabilities \"and\" a probability value based on the positions of particular amino acids in the turn exceeds a predetermined threshold. The turn probability p(t) is determined as:\nwhere \"j\" is the position of the amino acid in the four-residue window. If p(t) exceeds an arbitrary cutoff value (originally 7.5e–3), the mean of the p(j)'s exceeds 1, and p(t) exceeds the alpha helix and beta sheet probabilities for that window, then a turn is predicted. If the first two conditions are met but the probability of a beta sheet p(b) exceeds p(t), then a sheet is predicted instead.\n\n\n"}
{"id": "34366555", "url": "https://en.wikipedia.org/wiki?curid=34366555", "title": "Class kappa function", "text": "Class kappa function\n\nIn control theory, it is often required to check if a nonautonomous system is stable or not. To cope with this it is necessary to use some special comparison functions. Class formula_1 functions belong to this family:\nDefinition: a continuous function formula_2 is said to belong to class formula_1 if:\nDefinition: a continuous function formula_2 is said to belong to class formula_6 if:\n\nA nondecreasing positive definite function formula_10 satisfying all conditions of class formula_1 (formula_6) other than being strictly increasing can be upper and lower bounded by class formula_1 (formula_6) functions as follows:\nThus, to proceed with the appropriate analysis, it suffices to bound the function of interest with continuous nonincreasing positive definite functions.\n\n"}
{"id": "53281588", "url": "https://en.wikipedia.org/wiki?curid=53281588", "title": "Comet (experiment)", "text": "Comet (experiment)\n\nCOMET, which stands for COherent Muon to Electron Transition is currently a funded experiment in J-PARC, Tokai, Japan. In contrast to the usual muon decay to an electron and neutrinos, COMET seeks to look for neutrinoless muon to electron conversion, where the electron carries away with it about 104.8MeV of energy. Muon to electron conversion is not forbidden in The Standard Model but the branching ratio is about formula_1 considering the neutrino oscillations. If BSMs are considered, the muon to electron conversion process can be as high as formula_2 .e.g. via the supersymmetric formula_3.\n\nIt will be using a new beamline connecting the J-PARC main ring and the J-PARC Nuclear and particle Physics Experimental Hall (NP hall).\n\nCurrent spokesperson is Prof. Kuno Yoshitaka, project manager is Prof. Mihara Satoshi. The collaboration consists of universities coming from 15 countries.\n\n"}
{"id": "9082922", "url": "https://en.wikipedia.org/wiki?curid=9082922", "title": "Dielectric elastomers", "text": "Dielectric elastomers\n\nDielectric elastomers (DEs) are smart material systems that produce large strains. They belong to the group of electroactive polymers (EAP). DE actuators (DEA) transform electric energy into mechanical work. They are lightweight and have a high elastic energy density. They have been investigated since the late 1990s. Many prototype applications exist. Every year, conferences are held in the US and Europe.\n\nA DEA is a compliant capacitor (see image), where a passive elastomer film is sandwiched between two compliant electrodes. When a voltage formula_1 is applied, the electrostatic pressure formula_2 arising from the Coulomb forces acts between the electrodes. The electrodes squeeze the elastomer film. The equivalent electromechanical pressure formula_3 is twice the electrostatic pressure formula_2 and is given by:\n\nformula_5\n\nwhere formula_6 is the vacuum permittivity, formula_7 is the dielectric constant of the polymer and formula_8 is the thickness of the elastomer film. Usually, strains of DEA are in the order of 10–35%, maximum values reach 300% (the acrylic elastomer VHB 4910, commercially available from 3M, which also sports a high elastic energy density and a high electrical breakdown strength.)\n\nReplacing the electrodes with soft hydrogels allows ionic transport to replace electron transport. Aqueous ionic hydrogels can deliver potentials of multiple kilovolts, despite the onset of electrolysis at below 1.5 V.\n\nThe difference between the capacitance of the double layer and the dielectric leads to a potential across the dielectric that can be millions of times greater than that across the double layer. Potentials in the kilovolt range can be realized without electrochemically degrading the hydrogel.\n\nDeformations are well controlled, reversible, and capable of high-frequency operation. The resulting devices can be perfectly transparent. High-frequency actuation is possible. Switching speeds are limited only by mechanical inertia. The hydrogel's stiffness can be thousands of times smaller than the dielectric's, allowing actuation without mechanical constraint across a range of nearly 100% at millisecond speeds. They can be biocompatible.\n\nRemaining issues include drying of the hydrogels, ionic build-up, hysteresis, and electrical shorting.\n\nEarly experiments in semiconductor device research relied on ionic conductors to investigate field modulation of contact potentials in silicon and to enable the first solid-state amplifiers. Work since 2000 has established the utility of electrolyte gate electrodes. Ionic gels can also serve as elements of high-performance, stretchable graphene transistors.\n\nFilms of carbon powder or grease loaded with carbon black were early choices as electrodes for the DEAs. Such materials have poor reliability and are not available with established manufacturing techniques. Improved characteristics can be achieved with liquid metal, sheets of graphene, coatings of carbon nanotubes, surface-implanted layers of metallic nanoclusters and corrugated or patterned metal films.\n\nThese options offer limited mechanical properties, sheet resistances, switching times and easy integration. Silicones and acrylic elastomers are other alternatives.\n\nThe requirements for an elastomer material are:\n\n\nMechanically prestretching the elastomer film offers the possibility of enhancing the electrical breakdown strength. Further reasons for prestretching include:\n\n\nThe elastomers show a visco-hyperelastic behavior. Models that describe large strains and viscoelasticity are required for the calculation of such actuators.\n\nMaterials used in research include graphite powder, silicone oil / graphite mixtures, gold electrodes. The electrode should be conductive and compliant. Compliance is important so that the elastomer is not constrained mechanically when elongated.\n\nFilms of polyacrylamide hydrogels formed with salt water can be laminated onto the dielectric surfaces, replacing electrodes.\n\nDEs based on silicone (PDMS) and natural rubber are promising research fields. Properties such as fast response times and efficiency are superior using natural rubber based DEs compared to VHB (acrylic elastomer) based DEs for strains under 15%.\n\nConfigurations include:\n\n\nDielectric elastomers offer multiple potential applications with the potential to replace many electromagnetic actuators, pneumatics and piezo actuators. A list of potential applications include:\n\n\n"}
{"id": "33540588", "url": "https://en.wikipedia.org/wiki?curid=33540588", "title": "Eloise Giblett", "text": "Eloise Giblett\n\nEloise \"Elo\" R. Giblett (January 17, 1921 – September 16, 2009) was a pioneering genetic scientist and hematologist who discovered the first recognized immunodeficiency disease, adenosine deaminase deficiency. Giblett was a Professor of Medicine at the University of Washington in Seattle and Executive Director of the Puget Sound Blood Center in Seattle. The author of over 200 research papers, she also wrote an esteemed textbook on genetic markers, \"Genetic Markers in Human Blood\", published in 1969. She was elected to the National Academy of Sciences in 1980.\n\nGiblett's numerous accomplishments include discovering the first immunodeficiency disease: adenosine deaminase deficiency. She identified and characterized numerous blood group antigens (including the ‘Elo’ antigen, named after her). Her work paved the way for safe red blood cell transfusions. She also applied her understanding of red blood cell protein polymorphisms to genetic linkage analyses, was senior author on the paper that demonstrated the feasibility of unrelated marrow transplantation for leukemia, and was an early supporter of bone marrow donation.\n\nGiblett was born in Tacoma, Washington in 1921. Her family moved to Spokane, Washington for her father's job as an insurance salesman. Giblett received her early education in Spokane and was trained in singing, dancing and the violin. Her Mother, Rose, held a secret desire that Giblett would become the next Shirley Temple of the era.\n\nGiblett graduated from Lewis and Clark High School in 1938. She was only 16 when she earned a scholarship to Mills College in Oakland, California. After two years, she transferred to the University of Washington in Seattle where she earned a degree in bacteriology (now microbiology) in 1942. From 1944 to 1946, she served in the Navy WAVES. Through this program, she worked as a technician at the clinical laboratory of the U.S. Naval Hospital in San Diego, CA. In 1947, she returned to the University of Washington to earn her Master of Science in microbiology. Her master's thesis focused on physiology of fungi in the genus \"Microsporum\".\n\nAfter completing her master's degree, Giblett attended the University of Washington Medical School. One of five women in her year, she graduated first in her class in 1951. From 1951 to 1953, Giblett served as an intern, then resident in Internal Medicine, at King County Hospital (now Harborview Hospital).\n\nIn 1953, Giblett was awarded a two year fellowship for post-doctoral research in hematology. During this time, Giblett worked under Clement Finch, a renowned hematologist interested in iron metabolism. Giblett primarily assisted with his research on erythrokinetics, the dynamic study of the production and destruction of red blood cells. In her first year working for Finch, Giblett published five papers, including a highly-cited paper describing red blood cell lifetime and hemolysis. Giblett also worked with geneticist Arno Motulsky studying erythrokinetics in splenomegaly, kicking off a decades-long collaboration.\n\nAfter completing her fellowship, Giblett traveled to London to train under Patrick Mollison at the Medical Research Council’s Blood Transfusion Research Unit. In this research unit, Giblett gained the laboratory experience necessary to co-direct Puget Sound Blood Center (then King County Blood Bank), a position she assumed upon her return to Seattle in 1955. Giblett remained at the Blood Center as Associate Director until her promotion to Executive Director in 1979. She retired in 1987.\n\nGiblett focused the majority of her career on academic research. In 1955, she was appointed Clinical Associate in Medicine at the University of Washington. Giblett's lab focused on studying blood groups, with particular attention to genetic markers in human blood. She identified several blood group antigens. Her research assisted in refuting the standard practice at the time of segregating blood donations based on the race of the donor.\n\nIn 1958, Giblett began research studying polymorphisms of the human plasma proteins haptoglobin and transferrin using starch gel electrophoresis. As a result of her studies on genetic variation, Giblett documented the first case of a mosaic individual conceived from dispermic fertilization of two eggs followed by cell fusion.\n\nGiblett actively collaborated with Arno Motulsky, a fellow professor at the University of Washington. Giblett analyzed blood samples from a population study Motulsky carried out in the Congo in 1960. The resulting paper, published in 1966, described many novel genetic variants. Decades later, scientists discovered that one of these samples contained first known case of HIV. The viral sequence from this sample is still used to date in studies of HIV.\n\nStarting in 1971, Giblett began researching bone marrow transplants with E. Donnall Thomas. Bone marrow transplantations were a pioneering technique used to treat blood cancers. At the time, if the donor and acceptor were the same sex, doctors could not confirm the success of the graft. Giblett assisted in discovering genetic markers that could confirm graft success, regardless of donor sex, using polymorphic blood proteins.\n\nGiblett eventually expanded her research into the activity of polymorphic proteins in human plasma and blood cells, leading to her famous discovery of the first immunodeficiency disease. One polymorphic protein used as a routine a genetic marker for transplants was adenosine deaminase (ADA) located in red blood cells. In 1972, Giblett received samples from a patient with severe combined immunodeficiency disease (SCID). The patient was a candidate for bone marrow transplantation from her mother; analysis of blood samples surprisingly revealed that the child exhibited no ADA activity. Giblett soon discovered a second case where ADA deficiency underlaid immune dysfunction, leading her to conclude that the two may be related. Giblett named this disease adenosine deaminase immunodeficiency, and it was recognized as the first official immunodeficiency disease.\n\nThe discovery of ADA deficiency lead to a breakthrough in understanding immunodeficiency. Based on the function of ADA in purine metabolism, Giblett hypothesized that mutations in other proteins involved in purine metabolism or related pyrimidine metabolism might underlie additional forms of immune dysfunction. Her hypothesis was confirmed in 1975 upon analysis of an immunocompromised patient exhibiting normal ADA activity but defective purine nucleoside phosphorylase (PNP) activity. Within several years, ten more cases of immune deficiency linked to PNP mutations were described, leading to the classification of the disorder as purine nucleoside phosphorylase deficiency.\n\nGiblett's other notable discoveries include T cell immunodeficiency.\n\nThroughout her career, Giblett collaborated with some of the most notable and talented scientists of her era, including: Oliver Smithies, Alexander Bearn, James Neel, Curt Stern, Victor McKusick, Ernest Beutler, Stanley Gartler, Walter Bodmer, John Cairns, David Weatherall, Henry Kunkel, H. Hugh Fudenberg, and Newton Morton.\n\nIn 1978, Giblett closed her research lab to direct the Puget Sound Blood Center. Soon after, in 1981, HIV/AIDS was discovered. Infectious disease experts at the time realized that the disorder might be transmissible by blood, creating complications for blood transfusions. This discovery led to a crisis in blood banking. Giblett attempted to allay fears about the hazard of giving blood and closely followed the incidence of the disease in previously transfused patients. Before HIV could be detected in blood, Giblett developed a screening policy for blood donors at the center.\n\nGiblett retired from the Puget Sound Blood Center in 1987. She devoted her remaining years to playing the violin and contributing to various musical groups, playing in several string quartets. She was a co-founder of the Music Center of the Northwest, and contributed to them until she died.\n\nIn 1967, Giblett was promoted to full professor at the University of Washington. Giblett served as president of the American Society of Human Genetics in 1973. She was a board member of the American Society of Hematology, the Western Association of Physicians and the New York Blood Center Research Advisory Committee. In 1980, Giblett was elected to the National Academy of Sciences. The following year, she became a fellow of the National Association for the Advancement of Science. In 1987, she was the first woman to receive the University of Washington Medical School Alumni Association's Distinguished Alumni Award. Upon her retirement, she was awarded emeritus status at the University of Washington School of Medicine and Puget Sound Blood Center.\n\nIn 1969, Giblett published \"Genetic Markers in Human Blood\", a reference book aimed to increase the accessibility of information about biochemical variation in blood. The book was described by H. E. Sutton as \"a remarkable achievement for a single individual.\"\n\nGiblett was a fan of science fiction literature. She is mentioned by name in Robert Heinlein's novel \"The Number of the Beast.\"\n\nIn 2010, the Elo Giblett Endowed Professorship in Hematology was established at the University of Washington. This professorship was created by combining an amount of money left by Giblett to the university and an additional funding from Giblett's niece, Leslie Giblett. The first recipient of this professorship was John Harlan, MD. This professorship is intended to attract talented medical professionals in hematology and keep Giblett's legacy alive. Elo’s unpublished autobiography is property of her niece, Leslie.\n"}
{"id": "49050360", "url": "https://en.wikipedia.org/wiki?curid=49050360", "title": "Elqui-Limarí Batholith", "text": "Elqui-Limarí Batholith\n\nThe Elqui-Limarí Batholith is a group of plutons in the Andes of Chile and Argentina between the latitudes of 28 and 30° S. The plutons of the batholith were emplaced and cooled in the Late Paleozoic and the earliest Mesozoic. Some of the plutons were emplaced in a context of crustal thickening related to the San Rafael orogeny.\n\nTogether with the Chilean Coastal Batholith and the Colangüil Batholith the Elqui-Limarí Batholith is a remnant of the volcanic arcs that erupted the volcanic material of the Choiyoi Group.\n"}
{"id": "6073894", "url": "https://en.wikipedia.org/wiki?curid=6073894", "title": "Fermentation", "text": "Fermentation\n\nFermentation is a metabolic process that produces chemical changes in organic substrates through the action of enzymes. In biochemistry, it is narrowly defined as the extraction of energy from carbohydrates in the absence of oxygen. In the context of food production, it may more broadly refer to any process in which the activity of microorganisms brings about a desirable change to a foodstuff or beverage. The science of fermentation is known as zymology.\n\nIn microorganisms, fermentation is the primary means of producing ATP by the degradation of organic nutrients anaerobically. Humans have used fermentation to produce foodstuffs and beverages since the Neolithic age. For example, fermentation is used for preservation in a process that produces lactic acid found in such sour foods as pickled cucumbers, kimchi, and yogurt, as well as for producing alcoholic beverages such as wine and beer. Fermentation occurs within the gastrointestinal tracts of all animals, including humans.\n\nBelow are some definitions of fermentation. They range from informal, general usages to more scientific definitions.\n\nAlong with photosynthesis and aerobic respiration, fermentation is a way of extracting energy from molecules, but it is the only one common to all bacteria and eukaryotes. It is therefore considered the oldest metabolic pathway, suitable for an environment that does not yet have oxygen. Yeast, a form of fungus, occurs in almost any environment capable of supporting microbes, from the skins of fruits to the guts of insects and mammals and the deep ocean, and they harvest sugar-rich materials to produce ethanol and carbon dioxide.\n\nThe basic mechanism for fermentation remains present in all cells of higher organisms. Mammalian muscle carries out the fermentation that occurs during periods of intense exercise where oxygen supply becomes limited, resulting in the creation of lactic acid. In invertebrates, fermentation also produces succinate and alanine.\n\nFermentative bacteria play an essential role in the production of methane in habitats ranging from the rumens of cattle to sewage digesters and freshwater sediments. They produce hydrogen, carbon dioxide, formate and acetate and carboxylic acids; and then consortia of microbes convert the carbon dioxide and acetate to methane. Acetogenic bacteria oxidize the acids, obtaining more acetate and either hydrogen or formate. Finally, methanogens (which are in the domain \"Archea\") convert acetate to methane.\n\nFermentation reacts NADH with an endogenous, organic electron acceptor. Usually this is pyruvate formed from sugar through glycolysis. The reaction produces NAD+ and an organic product, typical examples being ethanol, lactic acid, carbon dioxide, and hydrogen gas (H). However, more exotic compounds can be produced by fermentation, such as butyric acid and acetone. Fermentation products contain chemical energy (they are not fully oxidized), but are considered waste products, since they cannot be metabolized further without the use of oxygen.\n\nFermentation normally occurs in an anaerobic environment. In the presence of O, NADH, and pyruvate are used to generate ATP in respiration. This is called oxidative phosphorylation, and it generates much more ATP than glycolysis alone. For that reason, fermentation is rarely utilized when oxygen is available. However, even in the presence of abundant oxygen, some strains of yeast such as \"Saccharomyces cerevisiae\" prefer fermentation to aerobic respiration as long as there is an adequate supply of sugars (a phenomenon known as the Crabtree effect). Some fermentation processes involve obligate anaerobes, which cannot tolerate oxygen.\n\nAlthough yeast carries out the fermentation in the production of ethanol in beers, wines, and other alcoholic drinks, this is not the only possible agent: bacteria carry out the fermentation in the production of xanthan gum.\n\nIn ethanol fermentation, one glucose molecule is converted into two ethanol molecules and two carbon dioxide molecules. It is used to make bread dough rise: the carbon dioxide forms bubbles, expanding the dough into a foam. The ethanol is the intoxicating agent in alcoholic beverages such as wine, beer and liquor. Fermentation of feedstocks, including sugarcane, corn, and sugar beets, produces ethanol that is added to gasoline. In some species of fish, including goldfish and carp, it provides energy when oxygen is scarce (along with lactic acid fermentation).\n\nThe figure illustrates the process. Before fermentation, a glucose molecule breaks down into two pyruvate molecules. The energy from this exothermic reaction is used to bind inorganic phosphates to ATP and convert NAD+ to NADH. The pyruvates break down into two acetaldehyde molecules and give off two carbon dioxide molecules as a waste product. The acetaldehyde is reduced into ethanol using the energy and hydrogen from NADH, and the NADH is oxidized into NAD+ so that the cycle may repeat. The reaction is catalysed by the enzymes pyruvate decarboxylase and alcohol dehydrogenase.\n\n\"Homolactic fermentation\" (producing only lactic acid) is the simplest type of fermentation. The pyruvate from glycolysis undergoes a simple redox reaction, forming lactic acid. It is unique because it is one of the only respiration processes to not produce a gas as a byproduct. Overall, one molecule of glucose (or any six-carbon sugar) is converted to two molecules of lactic acid:\nIt occurs in the muscles of animals when they need energy faster than the blood can supply oxygen. It also occurs in some kinds of bacteria (such as lactobacilli) and some fungi. It is the type of bacteria that converts lactose into lactic acid in yogurt, giving it its sour taste. These lactic acid bacteria can carry out either homolactic fermentation, where the end-product is mostly lactic acid, or \"Heterolactic fermentation\", where some lactate is further metabolized and results in ethanol and carbon dioxide (via the phosphoketolase pathway), acetate, or other metabolic products, e.g.:\nIf lactose is fermented (as in yogurts and cheeses), it is first converted into glucose and galactose (both six-carbon sugars with the same atomic formula):\nHeterolactic fermentation is in a sense intermediate between lactic acid fermentation and other types, e.g. alcoholic fermentation (see below). The reasons to go further and convert lactic acid into anything else are:\n\nHydrogen gas is produced in many types of fermentation (mixed acid fermentation, butyric acid fermentation, caproate fermentation, butanol fermentation, glyoxylate fermentation) as a way to regenerate NAD from NADH. Electrons are transferred to ferredoxin, which in turn is oxidized by hydrogenase, producing H. Hydrogen gas is a substrate for methanogens and sulfate reducers, which keep the concentration of hydrogen low and favor the production of such an energy-rich compound, but hydrogen gas at a fairly high concentration can nevertheless be formed, as in flatus.\n\nAs an example of mixed acid fermentation, bacteria such as \"Clostridium pasteurianum\" ferment glucose producing butyrate, acetate, carbon dioxide, and hydrogen gas: The reaction leading to acetate is:\n\nGlucose could theoretically be converted into just CO and H, but the global reaction releases little energy.\n\nMost industrial fermentation uses batch or fed-batch procedures, although continuous fermentation can be more economical if various challenges, particularly the difficulty of maintaining sterility, can be met.\n\nIn a batch process, all the ingredients are combined and the reactions proceed without any further input. Batch fermentation has been used for millennia to make bread and alcoholic beverages, and it is still a common method, especially when the process is not well understood. However, it can be expensive because the fermentor must be sterilized using high pressure steam between batches. Strictly speaking, there is often addition of small quantities of chemicals to control the pH or suppress foaming.\n\nBatch fermentation goes through a series of phases. There is a lag phase in which cells adjust to their environment; then a phase in which exponential growth occurs. Once many of the nutrients have been consumed, the growth slows and becomes non-exponential, but production of \"secondary metabolites\" (including commercially important antibiotics and enzymes) accelerates. This continues through a stationary phase after most of the nutrients have been consumed, and then the cells die.\n\nFed-batch fermentation is a variation of batch fermentation where some of the ingredients are added during the fermentation. This allows greater control over the stages of the process. In particular, production of secondary metabolites can be increased by adding a limited quantity of nutrients during the non-exponential growth phase. Fed-batch operations are often sandwiched between batch operations.\n\nThe high cost of sterilizing the fermentor between batches can be avoided using various open fermentation approaches that are able to resist contamination. One is to use a naturally evolved mixed culture. This is particularly favored in wastewater treatment, since mixed populations can adapt to a wide variety of wastes. Thermophilic bacteria can produce lactic acid at temperatures of around 50 degrees Celsius, sufficient to discourage microbial contamination; and ethanol has been produced at a temperature of 70 °C. This is just below its boiling point (78 °C), making it easy to extract. Halophilic bacteria can produce bioplastics in hypersaline conditions. Solid-state fermentation adds a small amount of water to a solid substrate; it is widely used in the food industry to produce flavors, enzymes and organic acids.\n\nIn continuous fermentation, substrates are added and final products removed continuously. There are three varieties: chemostats, which hold nutrient levels constant; turbidostats, which keep cell mass constant; and plug flow reactors in which the culture medium flows steadily through a tube while the cells are recycled from the outlet to the inlet. If the process works well, there is a steady flow of feed and effluent and the costs of repeatedly setting up a batch are avoided. Also, it can prolong the exponential growth phase and avoid byproducts that inhibit the reactions by continuously removing them. However, it is difficult to maintain a steady state and avoid contamination, and the design tends to be complex. Typically the fermentor must run for over 500 hours to be more economical than batch processors.\n\nThe use of fermentation, particularly for beverages, has existed since the Neolithic and has been documented dating from 7000–6600 BCE in Jiahu, China, 5000 BCE in India, Ayurveda mentions many Medicated Wines, 6000 BCE in Georgia, 3150 BCE in ancient Egypt, 3000 BCE in Babylon, 2000 BCE in pre-Hispanic Mexico, and 1500 BC in Sudan. Fermented foods have a religious significance in Judaism and Christianity. The Baltic god Rugutis was worshiped as the agent of fermentation.\nIn 1837, Charles Cagniard de la Tour, Theodor Schwann and Friedrich Traugott Kützing independently published papers concluding, as a result of microscopic investigations, that yeast is a living organism that reproduces by budding. Schwann boiled grape juice to kill the yeast and found that no fermentation would occur until new yeast was added. However, a lot of chemists , including Antoine Lavoisier, continued to view fermentation as a simple chemical reaction and rejected the notion that living organisms could be involved. This was seen as a reversion to vitalism and was lampooned in an anonymous publication by Justus von Liebig and Friedrich Wöhler.\n\nThe turning point came when Louis Pasteur (1822–1895), during the 1850s and 1860s, repeated Schwann's experiments and showed that fermentation is initiated by living organisms in a series of investigations. In 1857, Pasteur showed that lactic acid fermentation is caused by living organisms. In 1860, he demonstrated that bacteria cause souring in milk, a process formerly thought to be merely a chemical change, and his work in identifying the role of microorganisms in food spoilage led to the process of pasteurization. In 1877, working to improve the French brewing industry, Pasteur published his famous paper on fermentation, \"Etudes sur la Bière\", which was translated into English in 1879 as \"Studies on fermentation\". He defined fermentation (incorrectly) as \"Life without air\", but correctly showed that specific types of microorganisms cause specific types of fermentations and specific end-products.\n\nAlthough showing fermentation to be the result of the action of living microorganisms was a breakthrough, it did not explain the basic nature of the fermentation process, or prove that it is caused by the microorganisms that appear to be always present. Many scientists, including Pasteur, had unsuccessfully attempted to extract the fermentation enzyme from yeast. Success came in 1897 when the German chemist Eduard Buechner ground up yeast, extracted a juice from them, then found to his amazement that this \"dead\" liquid would ferment a sugar solution, forming carbon dioxide and alcohol much like living yeasts. Buechner's results are considered to mark the birth of biochemistry. The \"unorganized ferments\" behaved just like the organized ones. From that time on, the term enzyme came to be applied to all ferments. It was then understood that fermentation is caused by enzymes that are produced by microorganisms. In 1907, Buechner won the Nobel Prize in chemistry for his work.\n\nAdvances in microbiology and fermentation technology have continued steadily up until the present. For example, in the 1930s, it was discovered that microorganisms could be mutated with physical and chemical treatments to be higher-yielding, faster-growing, tolerant of less oxygen, and able to use a more concentrated medium. Strain selection and hybridization developed as well, affecting most modern food fermentations.\n\nThe word \"ferment\" is derived from the Latin verb \"fervere\", which means to boil. It is thought to have been first used in the late 14th century in alchemy, but only in a broad sense. It was not used in the modern scientific sense until around 1600.\n\n"}
{"id": "54360", "url": "https://en.wikipedia.org/wiki?curid=54360", "title": "Frank Drake", "text": "Frank Drake\n\nFrank Donald Drake (born May 28, 1930) is an American astronomer and astrophysicist. He is involved in the search for extraterrestrial intelligence, including the founding of SETI, mounting the first observational attempts at detecting extraterrestrial communications in 1960 in Project Ozma, developing the Drake equation, and as the creator of the \"Arecibo Message,\" a digital encoding of an astronomical and biological description of the Earth and its lifeforms for transmission into the cosmos.\n\nBorn on May 28, 1930, in Chicago, Illinois, as a youth Drake loved electronics and chemistry. He reports that he considered the possibility of life existing on other planets as an eight-year-old, but never discussed the idea with his family or teachers due to the prevalent religious ideology.\n\nHe enrolled at Cornell University on a Navy Reserve Officer Training Corps scholarship. Once there he began studying astronomy. His ideas about the possibility of extraterrestrial life were reinforced by a lecture from astrophysicist Otto Struve in 1951. After college, he served briefly as an electronics officer on the heavy cruiser USS \"Albany\". He then went on to graduate school at Harvard to study radio astronomy.\n\nDrake's hobbies include lapidary and the cultivation of orchids.\n\nAlthough explicitly linked with modern views on the likelihood and detectability of extraterrestrial civilizations, Drake started his career undertaking radio astronomical research at the National Radio Astronomy Observatory (NRAO) in Green Bank, West Virginia, and later the Jet Propulsion Laboratory. He conducted key measurements which revealed the presence of a Jovian ionosphere and magnetosphere.\n\nIn the 1960s, Drake spearheaded the conversion of the Arecibo Observatory to a radio astronomical facility, later updated in 1974 and 1996. As a researcher, Drake was involved in the early work on pulsars. In this period, Drake was a professor at Cornell University and Director of the National Astronomy and Ionosphere Center (NAIC) – the formal name for the Arecibo facility. In 1974 he wrote the Arecibo message.\n\nHe is one of the pioneers of the modern field of the search for extraterrestrial intelligence with Giuseppe Cocconi, Philip Morrison, Iosif Shklovsky, and Carl Sagan.\n\nDrake co-designed the Pioneer plaque with Carl Sagan in 1972, the first physical message sent into space. The plaque was designed to be understandable by extraterrestrials should they encounter it. He later supervised the creation of the Voyager Golden Record. He was elected to the American Academy of Arts and Sciences in 1974.\n\nDrake is a member of the National Academy of Sciences where he chaired the Board of Physics and Astronomy of the National Research Council (1989–92). He also served as President of the Astronomical Society of the Pacific. He was a Professor of Astronomy at Cornell University (1964–84) and served as the Director of the Arecibo Observatory. He is currently involved in \"The Carl Sagan Center for the Study of life in the Universe\" at the SETI Institute.\n\nHe is Emeritus Professor of Astronomy and Astrophysics at the University of California at Santa Cruz where he also served as Dean of Natural Sciences (1984–88). He serves on the Board of Trustees of the SETI Institute.\n\nDrake Planetarium at Norwood High School in Norwood, Ohio is named for Drake and linked to NASA.\n\n\n\n"}
{"id": "7667352", "url": "https://en.wikipedia.org/wiki?curid=7667352", "title": "GSI3D", "text": "GSI3D\n\nGSI3D (Geological Surveying and Investigation in 3 dimensions) is a methodology and associated software tool for 3D geologic modeling developed by Hans-Georg Sobisch (INSIGHT Geologische Softwaresysteme, Germany) over the last 20 years initially in collaboration with the Geological Survey of Lower Saxony (LBEG) and the Oldenburg-Ostfriesland Waterboard (OOWV) in Germany and from 2006-2010 in collaboration with the British Geological Survey. GSI3D has been further developed and is now solely available as INSIGHT's SubsurfaceViewer MX. The software is written in Java and data is stored in extensible mark-up language XML.\n\nGSI3D utilises a digital elevation model, surface geological linework and downhole borehole and geophysical data to enable the geologist to construct cross sections by correlating boreholes and the outcrops to produce a geological fence diagram. Mathematical interpolation between the nodes along the drawn sections and the limits of the units produces a solid model comprising a stack of triangulated objects each corresponding to one of the geological units present. Scientists draw their sections based on facts such as borehole logs correlated by intuition - the shape 'looks right' to a geologist. This 'looks right' element pulls on the geologists' wealth of understanding of earth processes, examination of exposures and theoretical knowledge gathered over a career in geology. GSI3D enables the efficient capture of tacit and implicit knowledge which was until now trapped in geologist's heads.\n\nBetween April 2010 and April 2015, The British Geological Survey operated the 5-year GSI3D Research Consortium under license from INSIGHT GmbH. This not-for-profit consortium provided subscription-based access to a BGS-customized version of GSI3D along with a website and support package as a platform to develop a geological modelling community based around the cross-section methodology. The consortium successfully brought together Geological Surveys, commercial companies and academics from around the globe.\n\nThe British Geological Survey is continuing to use GSI3D to develop an increased understanding of the subsurface, please see our webpage http://www.bgs.ac.uk/services/3Dgeology/\n\nGSI3D is now solely available as part of INSIGHT GmbH's SubsurfaceViewer MX platform http://www.subsurfaceviewer.com.\n\n\n"}
{"id": "55121315", "url": "https://en.wikipedia.org/wiki?curid=55121315", "title": "Hassan Ziari", "text": "Hassan Ziari\n\nHassan Ziari () is an Iranian highway engineer and conservative politician. He was a Tehran councillor from 2003 to 2007 and is a professor at Iran University of Science and Technology, where he earned his digrees.\n\nZiari headed Iranian Railways Company and the \"ex-officio\" vice minister of roads under administration of Mahmoud Ahmadinejad.\n\nIn 2009, \"Nature\"'s investigation suggested that the paper 'Providing a decreasing connection probability model for urban street network' (published in the journal \"Transport\" in 2006) co-authored by Ziari, Hamid Behbahani and a PhD candidate named Mohammed Khabiri, \"contains large amounts of text from earlier articles by other researchers\", considered plagiarism.\n\n"}
{"id": "55213052", "url": "https://en.wikipedia.org/wiki?curid=55213052", "title": "Hub labels", "text": "Hub labels\n\nIn computer science, hub labels or the hub-labelling algorithm is a method that consumes much fewer resources than the lookup table but is still extremely fast for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.\n\nThis method allows at the most with two SELECT statements and the analysis of two strings to compute the shortest path between two vertices of a graph.\nFor a graph that is oriented like a road graph, this technique requires the prior computation of two tables from structures constructed using the method of the contraction hierarchies. \nIn the end, these two computed tables will have as many rows as nodes present within the graph. For each row (each node), a label will be calculated.\n\nA label is a string containing the distance information between the current node (the node of the row) and all the other nodes that can be reached with an ascending search on the relative multi-level structure. The advantage of these distances is that they all represent the shortest paths. \n\nSo, for future queries, the search of a shortest path will start from the source on the first table and the destination on the second table, from which it will be search within the labels for the common nodes with the associated distance information. Only the smallest sum of distances will be kept as the shortest path result.\n"}
{"id": "40217745", "url": "https://en.wikipedia.org/wiki?curid=40217745", "title": "Ian McDougall (geologist)", "text": "Ian McDougall (geologist)\n\nIan McDougall (born 1935 in Hobart) is an Australian geologist and geochemist.\n\nMcDougall studied at the University of Tasmania and Australian National University, before taking up a research position at ANU. He is a Fellow of the Geological Society of America, the Australian Academy of Science, and the American Geophysical Union. McDougall has also served as Vice President of the International Association of Volcanology and Chemistry of the Earth's Interior.\n\nMcDougall's research areas include plate tectonics and geochronology. He has been described as \"one of Australia's most internationally distinguished earth scientists,\" and was awarded the Centenary Medal in 2001.\n"}
{"id": "38448335", "url": "https://en.wikipedia.org/wiki?curid=38448335", "title": "Index of physics articles (H)", "text": "Index of physics articles (H)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "52202251", "url": "https://en.wikipedia.org/wiki?curid=52202251", "title": "Indigenous planning", "text": "Indigenous planning\n\nIndigenous planning (or Indigenous community planning) is an ideological approach to the field of regional planning where planning done by Indigenous peoples for Indigenous communities. Practitioners integrate traditional knowledge or cultural knowledge into the process of planning. Indigenous planning recognizes that \"all human communities plan\" and that Indigenous communities have been carrying out their own community planning processes for thousands of years. While the broader context of urban planning, and social planning includes the need to work cooperatively with indigenous persons and organizations, the process in doing so is dependent on social, political and cultural forces.\n\nAs there are many Indigenous cultures, practices and planning within Indigenous communities vary greatly.\n\nIndigenous planning has a broader and more comprehensive scope than mainstream or Western planning, and is not limited to land use planning or physical development. Indigenous planning is comprehensive and can address all aspects of community life through community development, including the social and environmental aspects that impact the lives of community members.\n\nIndigenous planning for land and resources can be understood as transformative planning as it addresses complex issues of Indigenous sovereignty, self-government, and self-determination. Indigenous planning can also be understood as a form of insurgent planning, as it provides an avenue for communities to confront and address their own oppression. Indigenous planning is often a tool which allows for Indigenous communities to regain control over resources and exercise maintenance of their culture and political autonomy.\n\nThe scope of Indigenous planning can be seen to cover three broad areas: Indigenous communities, urban Indigenous communities, land and resource planning.\n\nIndigenous peoples have been planning their communities for thousands of years, often referred to as 'since time immemorial'. However, planning as a technical and colonial tool has historically been used as a means to dispossess Indigenous communities through the re-appropriation of traditional territories for non-Indigenous profit and development. While pre-contact Indigenous community planning was based upon managing interactions with the natural world, it now largely focuses on interactions with non-Indigenous actors. As such, Indigenous planning has re-emerged as a reaction to Western planning, which was historically used as a colonial tool, for example through the reserve system in Canada.\n\nIndigenous planning emerged as a planning culture and field of practice during the mid-20th century within the context of modern planning. It is a continually evolving practice and spans (but is contextually unique to) Indigenous communities around the world. According to John Friedmann, Indigenous planning emerged as a formalized field in relation to mainstream planning in 1992 at a MIT conference through the creation of a theory of action that was based on long-term learning, local planning and shared culture. Three years later, in 1995, the Indigenous Planning Network was created under the American Planning Association (APA); the division is currently called Indigenous Planning (IP).\n\nContemporary Indigenous planning practices are particularly prevalent in New Zealand, Australia, Canada, and the United States (countries with large Indigenous populations and colonial histories). Histories of colonization have significantly impacted Indigenous communities and their planning cultures. International colonial processes are complex, divergent, and context specific. The large scale and ongoing impacts of these processes include but are not limited to: dispossession from land and retrenchment of decision making power, intergenerational trauma, systematic racism, and disruptions of local and traditional cultural systems. Taiaiake Alfred asserts that it is essential to differentiate between Indigenous and Western planning cultures that are implicated within colonial legacies. Generally, Western planning cultures have tended to value linear systems of rationality, with power structures that dominate and suppress those cultures that do not share these value systems. Since colonization processes took place throughout the world, Indigenous planning cultures were largely ignored and actively disrupted as they were seen as impediments to western civilisation and progress. The culture of Indigenous planning is described as a movement where Indigenous communities, planners, academics, governments, institutions, and leaders, resist colonial and neo-colonial planning traditions as well as work towards increasing protections of rights, freedoms, and sovereignty for Indigenous peoples.\n\nSome of the key principles of Indigenous planning that are distinguishable from 'mainstream' or Western planning approaches, are its recognition and incorporation of traditional knowledge, cultural identity, customary law, and Indigenous world-views. As collective land-based peoples, land tenure and stewardship are at the core of Indigenous planning paradigms.\n\nIndigenous communities everywhere have sustained and developed distinct, fluid and evolving planning cultures that are unique to land, history, and peoples. These cultural planning practices include land stewardship, resource management, community planning, and intergenerational learning transfers such as traditional ecological knowledge. Indigenous planning cultures often hold traditional governance structures, including: matrilineal heritage or consensus based decision making; self-reliance and resiliency; and, reciprocity and ceremony. Complex relationships with time exist, with strong emphases on cyclical patterns, such as nature-human relational processes and the Seven Generation Sustainability methodology. Strength-based practices and wellness planning lenses are employed, rather than a negative or weakness based assessment framework. Many of these Indigenous cultures evoke particular planning methods, including: Transformational Planning, Participatory Planning, Therapeutic Planning, Cultural Humility, and Reconciliation.\n\nThe Indigenous planning culture is an intersectional, placed-based approach and political movement that is shaping western and mainstream urban planning cultures. Paullette Regan discusses the process of changing Canadian planning culture through the efforts of non-Indigenous Canadians to decolonize their personal beliefs and behaviours. Ryan Walker and Hiringi Matunga use case studies from Canada and New Zealand to discuss how planners might be able to re-situate Indigenous and mainstream planning cultures as a partnership in urban contexts. The reclamation of Indigenous planning cultures challenges western planning assumptions and many planners worldwide are questioning how non-indigenous and indigenous planners can work collaboratively towards planning practices that are reconciliatory, respectful, creative and culturally responsive.\n\nIn Australia, land councils are regional organizations representing Indigenous Australians. While the primary function is to advocate for traditional land rights, the work of many land councils extends to community development plans and programs, which focus on the economic, social and cultural well-being of Indigenous Australians.\n\nSome Canadian First Nations engage in Indigenous planning through an approach known as Comprehensive Community Planning (CCP). CCP is a community-led and community-owned process. As a planning exercise, CCP takes a holistic and long-term approach that considers all aspects of the community, for example: housing, health, culture, economy, land use, resources, education, language revitalization and governance. CCP can also be a way for Indigenous communities to engage formally with government organizations who provide external resources and funding for First Nations projects. Comprehensive Community Plans are living documents designed to reflect and respond to the changing priorities and goals of the community.\n\nThe En'owkinwixw process is a traditional method of facilitating \"collective learning and community decision-making\" used by Syilx communities in Okanagan, British Columbia. The CCP for the Penticton Indian Band is an example of the En'owkinwixw process in action. The process emphasizes inclusion and equal voice in community consultation to create a common guiding framework that is culturally relevant.\n\nThe Maori in New Zealand practice Iwi Management Planning, which provides a framework for tribes to define their past and present, and prescribe \"management, planning and decision-making processes to guide iwi toward their concept of self-determination\". Iwi management planning and its associated policies and approaches are examples of indigenous planning done by and for Maori communities. Furthermore, Maori iwi management planning is a planning tradition that has a history that predates colonization and any ensuing acts or treaties. Contemporary Maori planning practiced today can be seen as a \"dual planning tradition\" where the nature of planning in the context of colonization continues to evolve while remaining grounded in Maori tradition and philosophy.\n\nIn Hawai'i there is a trend towards the traditional Ahupuaʻa concept of land management, particularly with watershed planning.\n\nSeveral planning schools have incorporated Indigenous planning focuses into their curriculum. Some build relationships with Indigenous communities on whose lands they exist. For example, the University of British Columbia, School of Community and Regional Planning maintains a partnership with the Musqueam Indian Band. Planning schools which offer Indigenous planning curricula are often interested in updating professional planning education and practice through approaches involving the native ideals and perspectives of decolonization and reflexivity.\n\nAcademic institutions with Indigenous planning-focused curricula include: \n\n"}
{"id": "8664967", "url": "https://en.wikipedia.org/wiki?curid=8664967", "title": "International Listening Association", "text": "International Listening Association\n\nThe International Listening Association (ILA) is an organization developed to promote the study, development, and teaching of listening.\nThe association is “dedicated to learning more about the impact that listening has on all human activity.” The ILA was founded in 1979 in Minneapolis, Minnesota, USA. The ILA holds annual conferences throughout the US and chooses locations outside of the US every three to five years. Conference presenters have included people of varied backgrounds related to listening to provide a full range of interesting perspectives on listening, for example: communication professors, corporate trainers, audiologists, musicians, researchers and more.\n\nThe Purpose of the Association is to advance the practice, teaching, and research of listening throughout the world.\nThe purpose of the Association will be accomplished in the following ways:\n\nILA publications include the \"International Journal of Listening\" (IJL), which publishes original research in listening The scope of IJL's listening-related topics includes professional, interpersonal, public/political, media or mass communication, educational, intercultural, and international (including second language acquisition contexts). Study methodologies include empirical, pedagogical, philosophical, and historical methods.\n"}
{"id": "6911037", "url": "https://en.wikipedia.org/wiki?curid=6911037", "title": "James Wordie", "text": "James Wordie\n\nSir James Mann Wordie CBE FRSE LLD (26 April 1889 – 16 January 1962) was a Scottish polar explorer and geologist.\n\nWordie was born at Partick, Glasgow, the son of John Wordie, a carting contractor, and Jane Catherine Mann. He studied at The Glasgow Academy and obtained a BSc in geology from University of Glasgow. He graduated from St John's College, Cambridge as an advanced student in 1912, and began research work. His occupation brought him in contact with Frank Debenham and Raymond Priestley, who were members of the second Antarctic expedition of Robert Falcon Scott. Wordie's interest in expedition and scientific discovery was heightened by these two men.\n\nIn 1914, Wordie joined Sir Ernest Shackleton's expedition to the Antarctic, known as the Imperial Trans-Antarctic Expedition, where he acted as geologist and chief of scientific staff. Despite the overall failure of the expedition—including the beset ship \"Endurance\", caught up in the Weddell Sea until destroyed by ice in 1915—Wordie maintained the morale of the expedition, made scientific observations regarding oceanography and the ice pack, and acquired important geological specimens. He was awarded the Royal Geographical Society's Back Award in 1920.\n\nWordie sailed on nine polar expeditions, including Endurance. During the 1920s and 1930s, he made numerous voyages to the Arctic and helped nurture a new generation of young explorers, including Vivian Fuchs, Gino Watkins and Augustine Courtauld. Other scientific staff included the meteorologist Edmund Dymond on his 1937 research trip to Baffin Bay. He became the elder statesman of British polar exploration, and few expeditions left Britain without first consulting Wordie. The Wordie Ice Shelf on the Antarctic Peninsula was named in his honour. He was chairman of the Scott Polar Research Institute (SPRI) and president of the Royal Geographical Society from 1951 to 1954. During his term at the Society he helped plan the first successful ascent of Mount Everest by Edmund Hillary and Tenzing Norgay. While at SPRI, he assisted Fuchs in the first-ever crossing of the Antarctic continent — the original aim of Shackleton's Endurance expedition. He also contributed to the British \"Naval Intelligence Division Geographical Handbook Series\" that was published during the Second World War.\n\nBoth the University of Glasgow and University of Hull awarded him honorary doctorates (LLD).\n\nHe died in Cambridge on 16 January 1962.\n\nWordie was elected a Fellow of the Royal Society of Edinburgh in 1922. He was awarded the first W. S. Bruce Medal of the Royal Scottish Geographical Society in 1926, the Founder's Gold Medal of the Royal Geographical Society in 1933 and the Scottish Geographical Medal of the Royal Scottish Geographical Society in 1944. He was made Master of St John's College, Cambridge and in 1957 was knighted for his contributions to polar expeditions.\n\nPlaces named after him include Wordie Point, Wordie Bay, Wordie Seamount, Wordie Ice Shelf, Wordie Glacier, Wordie Nunatak and Point Wordie.\n\n"}
{"id": "15042013", "url": "https://en.wikipedia.org/wiki?curid=15042013", "title": "Johann Philipp Breyne", "text": "Johann Philipp Breyne\n\nJohann Philipp Breyne FRS (9 August 1680, Danzig (Gdańsk), Royal Prussia (a fief of the Crown of Poland) – 12 December 1764, Danzig, Royal Prussia), son of Jacob Breyne (1637–97), was a German botanist, palaeontologist, zoologist and entomologist. He is best known for his work on the Polish cochineal (\"Porphyrophora polonica\"), an insect formerly used in production of red dye. Proposed by Hans Sloane, he was elected, on 21 April 1703, a Fellow of the Royal Society. He was also a member of the German Academy of Sciences Leopoldina (after 1715) and the Societas Litteraria (after 1720)\n\n\n"}
{"id": "58005876", "url": "https://en.wikipedia.org/wiki?curid=58005876", "title": "Katharina Paulus", "text": "Katharina Paulus\n\nKatharina “Käthe” Paulus (December 22, 1868 – July 26, 1935) was a German exhibition parachute jumper and the inventor of the first collapsable parachute. At the time, the parachute was named, 'rescue apparatus for aeronauts' in 1910. The previous parachutes were not able to fit in a case like apparatus worn on the back, thus Paulus' invention became of paramount importance for the Germans in First World War and she produced about 7000 parachutes for the German forces. During WW1 Paulus created approximately 125 parachutes a week. Paulus was also credited with inventing the \"drag 'chute\", an intentional breakaway system where one small parachute opens to pull out the main parachute. \n\nPaulus was an avid aeronaut herself and logged over 510 balloon flights and over 165 parachute jumps in her lifetime. She was the first German to be a professional air pilot and the first German woman aerial acrobat. \n\nDespite the fact that hot air balloons are currently known as a sort of tourist attraction, during the final decades of the 19th century, these hot air balloons were at the time, were on the cutting edge of technology, and were popular before the invention of airplane.\n\n\nPaulus' was born near Frankfort, Germany into a working class family. Her father worked as a day laborer and died when she was nineteen years old. After his death, Paulus picked up her mother's trade of seamstressing to help support the family. At 21, Paulus married Hermann Lattemann, a well-known balloonist, and began working as his assistant to repair the balloons with her skills as a seamstress. Paulus and Lattlmann begun to develop their professional and personal relationship, until Paulus began to parachute herself, and the two eventually got married. They had a son, Willy Hermann Paulus, who later died of diphtheria. In 1895, the couple were on a joint jump when Lattlemann's parachute failed to deploy. Paulus watched him fall to his death.\n\nWhile grieving the death of her husband, Paulus stayed in bed for months. That being said, during this time thousands of fans mailed letters of support to Paulus to request she continue her career of being a ballooner. Paulus then bought four new parachutes and set off on a tour of Europe using the stage name, Miss Polly. She performed theatrically, using acrobatic feats and even riding a bicycle suspending from a hot air balloon's basket. Paulus became an international success.\n\nPaulus completed her last balloon jump at age 63 (August 5, 1931).\n\nPaulus died at the age of 67 and is buried in a cemetery in Reinickendorf.\n"}
{"id": "58609836", "url": "https://en.wikipedia.org/wiki?curid=58609836", "title": "List of cardiology mnemonics", "text": "List of cardiology mnemonics\n\nThis is a list of cardiology mnemonics, categorized and alphabetized. For mnemonics in other medical specialities, see this list of medical mnemonics.\n\nCREAM:\n\nCongenital\n\nRheumatic damage\n\nEndocarditis\n\nAortic dissection/ Aortic root dilatation\n\nMarfan’s\n\nSAD:\n\nSyncope\n\nAngina\n\nDyspnoea\n\nABC'S\n\nAortic arch gives rise to:\n\nBrachiocephalic trunk\n\nLeft Common Carotid\n\nLeft Subclavian\n\nToilet Paper My Ass, They Pay Me Alcohol, or \"T\"hugs \"P\"ush \"Me\" \"A\"round. \n\nTricuspid valve\n\nPulmonary semilunar valve\n\nMitral (bicuspid) valve\n\nAortic semilunar valve\n\nHILT:\n\nHeaving\n\nImpalpable\n\nLaterally displaced\n\nThrusting/ Tapping\n\nIf it's impalpable, causes are COPD:\n\nCOPD\n\nObesity\n\nPleural, Pericardial effusion\n\nDextrocardia\n\nAnticoagulants: To prevent embolization.\n\nBeta blockers: To block the effects of certain hormones on the heart to slow the heart rate.\n\nCalcium Channel Blockers: Help slow the heart rate by blocking the number of electrical impulses that pass through the AV node into the lower heart chambers (ventricles).\n\nDigoxin: Helps slow the heart rate by blocking the number of electrical impulses that pass through the AV node into the lower heart chambers (ventricles).\n\nElectrocardioversion: A procedure in which electric currents are used to reset the heart's rhythm back to regular pattern.\n\nPirates:\n\nPulmonary: PE, COPD\n\nIatrogenic\n\nRheumatic heart: mirtral regurgitation\n\nAtherosclerotic: MI, CAD\n\nThyroid: hyperthyroid\n\nEndocarditis\n\nSick sinus syndrome\n\nABCD:\n\nAnti-coagulate\n\nBeta-block to control rate\n\nCardiovert\n\nDigoxin\n\n3 D's:\n\nDistant heart sounds\n\nDistended jugular veins\n\nDecreased arterial pressure\n\nBetablockers Acting Exclusively At Myocardium:\n\nBetaxolol\n\nAcebutelol\n\nEsmolol\n\nAtenolol\n\nMetoprolol\n\nLMNOP\n\nLasix\n\nMorphine\n\nNitrites\n\nOxygen\n\nVassoPressors\n\nFAILURE\n\nForgot medication\n\nArrhythmia/ Anaemia\n\nIschemia/ Infarction/ Infection\n\nLifestyle: taken too much salt\n\nUpregulation of CO: pregnancy, hyperthyroidism\n\nRenal failure\n\nEmbolism: pulmonary\n\nDarth Vader\n\nDeath\n\nArrythmia\n\nRupture(free ventricular wall/ ventricular septum/ papillary muscles)\n\nTamponade\n\nHeart failure (acute or chronic)\n\nValve disease\n\nAneurysm of Ventricles\n\nDressler's Syndrome\n\nthromboEmbolism (mural thrombus)\n\nRecurrence/ mitral Regurgitation\n\nDUST:\n\nDepressed ventricular function\n\nUnstable angina\n\nStenosis of the left main stem\n\nTriple vessel disease\n\nWiLLiaM MaRRoW:\n\nW pattern in V1-V2 and M pattern in V3-V6 is Left bundle block.\n\nM pattern in V1-V2 and W in V3-V6 is Right bundle block.\n\nRAMP:\n\nRecent MI\n\nAortic stenosis\n\nMI in the last 7 days\n\nPulmonary hypertension\n\nFROM JANE:\n\nFever\n\nRoth's spots\n\nOsler's nodes\n\nMurmur of heart\n\nJaneway lesions\n\nAnemia\n\n\nEmboli\n\nTry Puling My Aorta:\n\nTricuspid\n\nPulmonary\n\nMitral (bicuspid)\n\nAorta\n\nIf the R is far from P,\nthen you have a First Degree.\n\nLonger, longer, longer, drop!\nThen you have a Wenkebach.\n\nif some P's don't get through,\nthen you have Mobitz II.\n\nIf P's and Q's don't agree, then you have a Third Degree.\n\nINFARCTIONS\n\nIV access\n\nNarcotic analgesics (e.g. morphine, pethidine)\n\nFacilities for defibrillation (DF)\n\nAspirin/ Anticoagulant (heparin)\n\nRest\n\nConverting enzyme inhibitor\n\nThrombolysis\n\nIV beta blocker\n\nOxygen 60%\n\nNitrates\n\nStool Softeners\n\nASK ME\n\nAtrial contraction\n\nSystole (ventricular contraction)\n\nKlosure (closure) of tricusps, so atrial filling\n\nMaximal atrial filling\n\nEmptying of atrium\n\nBOOMAR:\n\nBed rest\n\nOxygen\n\nOpiate\n\nMonitor\n\nAnticoagulate\n\nReduce clot size\n\nPULSE:\n\nPersistent chest pains\n\nUpset stomach\n\nLightheadedness\n\nShortness of breath\n\nExcessive sweating\n\nO BATMAN!\n\nOxygen\n\nBeta blocker\n\nASA\n\nThrombolytics (e.g. heparin)\n\nMorphine\n\nAce prn\n\nNitroglycerin\n\nCOAG:\n\nCyclomorph\n\nOxygen\n\nAspirin\n\nGlycerol trinitrate\n\n\"IL PQRST\" (person has ill PQRST heart waves):\n\nIntensity\n\nLoccasion\n\nPitch\n\nQuality\n\nRadiation\n\nShape\n\nTiming\n\n8 S's:\n\nSoft\n\nSystolic\n\nShort\n\nSounds (S1 & S2) normal\n\nSymptomless\n\nSpecial tests normal (X-ray, EKG)\n\nStanding/ Sitting (vary with position)\n\nSternal depression\n\nLEft sided murmurs louder with Expiration\n\nRIght sided murmurs louder with Inspiration.\n\nSCRIPT:\n\nSite\n\nCharacter (e.g. harsh, soft, blowing)\n\nRadiation\n\nIntensity\n\nPitch\n\nTiming\n\nPASS:Pulmonic & Aortic\n\nStenosis=Systolic.\n\nPAID: Pulmonic & Aortic\n\nInsufficiency=Diastolic.\n\nCARDIAC RIND:\n\nCollagen vascular disease\n\nAortic aneurysm\n\nRadiation\n\nDrugs (such as hydralazine)\n\nInfections\n\nAcute renal failure\n\nCardiac infarction\n\nRheumatic fever\n\nInjury\n\nNeoplasms\n\nDressler's syndrome\n\nPericarditiS:\n\nPR depression in precordial leads.\n\nST elevation.\n\nSICVD:\n\nSymmetry of leg musculature\n\nIntegrity of skin\n\nColor of toenails\n\nVaricose veins\n\nDistribution of hair\n\nPATCH MED:\n\nPulmonary embolus\n\nAcidosis\n\nTension pneumothorax\n\nCardiac tamponade\n\nHypokalemia/ Hyperkalemia/ Hypoxia/ Hypothermia/ Hypovolemia\n\nMyocardial infarction\n\nElectrolyte derangements\n\nDrugs\n\nELEVATION:\n\nElectrolytes\n\nLBBB\n\nEarly repolarization\n\nVentricular hypertrophy\n\nAneurysm\n\nTreatment (e.g. pericardiocentesis)\n\nInjury (AMI, contusion)\n\nOsborne waves (hypothermia)\n\nNon-occlusive vasospasm\n\nABCDE:\n\nAdenosine\n\nBeta-blocker\n\nCalcium channel antagonist\n\nDigoxin\n\nExcitation (vagal stimulation)\n\nLAMB:\n\nLidocaine\n\nAmiodarone\n\nMexiltene/ Magnesium\n\nBeta-blocker\n\nNever let monkeys eat bananas:\n\nNeutrophils\n\nlymphocytes\n\nmonocytes\n\neosinophils\n\nbasophils\n"}
{"id": "208753", "url": "https://en.wikipedia.org/wiki?curid=208753", "title": "List of computers running CP/M", "text": "List of computers running CP/M\n\nMany microcomputer makes and models could run some version or derivation of the CP/M disk operating system. Eight-bit computers running CP/M 80 were built around an Intel 8080/8085, Zilog Z80, or compatible CPU. CP/M 86 ran on the Intel 8086 and 8088. Some computers were suitable for CP/M as delivered. Others needed hardware modifications such as a memory expansion or modification, new boot ROMs, or the addition of a floppy disk drive. A few very popular home computers using processors not supported by CP/M had plug-in Z80 or compatible processors, allowing them to use CP/M and retaining the base machine's keyboard, peripherals, and sometimes video display and memory.\n\nThe following is an alphabetical list of some computers running CP/M.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42834599", "url": "https://en.wikipedia.org/wiki?curid=42834599", "title": "List of countries by proportion of the population using improved sanitation facilities", "text": "List of countries by proportion of the population using improved sanitation facilities\n\nThis is a list of countries by the proportion of their population using improved sanitation facilities.\n\nFigures used in this chart are based on data compiled and uploaded by the World Bank on May 2013 through their World Development Indicators initiative. The information was provided by the respective governments of the listed countries. As the compiled figures are not collected with the same methodology and with different levels of rigor, there are limitations in their reliability in forming comparisons.\n\nSanitation as defined by the World Health Organization:\nThe United Nations states that improved sanitation facilities \"ensure hygienic separation of human excreta from human contact.\" They include in their definition:\n\nThe Joint Monitoring Programme for Water Supply and Sanitation of WHO and UNICEF has defined improved sanitation as follows:\n\n\nThe World Bank states: \n\nTo access the data for analysis, please go to this link hosted by The World Bank.\n\n"}
{"id": "14485736", "url": "https://en.wikipedia.org/wiki?curid=14485736", "title": "List of members of the National Academy of Sciences (Cellular and developmental biology)", "text": "List of members of the National Academy of Sciences (Cellular and developmental biology)\n"}
{"id": "33375631", "url": "https://en.wikipedia.org/wiki?curid=33375631", "title": "List of prehistoric sites in Colorado", "text": "List of prehistoric sites in Colorado\n\nList of prehistoric sites in Colorado includes historical and archaeological sites of humans from their earliest times in Colorado to just before the Colorado historic period, which ranges from about 12,000 BC to AD 19th century. The Period is defined by the culture enjoyed at the time, from the earliest hunter-gatherers, the Paleo-Indians, through to the prehistoric parents to the modern Native Americans.\n\nThere were more than 56,500 recorded prehistoric sites in Colorado by 1996. Important historic and archaeological sites are registered nationally with the National Register of Historic Places (National register) and within the state's Colorado State Register of Historic Properties (State register). Most of the sites below are registered in one more both registers and was the source for most of the information for this section:\n"}
{"id": "922586", "url": "https://en.wikipedia.org/wiki?curid=922586", "title": "List of rivers by length", "text": "List of rivers by length\n\nThis is a list of the longest rivers on Earth. It includes river systems over .\n\nThere are many factors, such as the source, the identification or the definition of the mouth, and the scale of measurement of the river length between source and mouth, that determine the precise meaning of \"river length\". As a result, the length measurements of many rivers are only approximations (see also coastline paradox). In particular, there has long been disagreement as to whether the Nile or the Amazon is the world's longest river. The Nile has traditionally been considered longer, but in recent years some Brazilian and Peruvian studies have suggested that the Amazon is longer by measuring the river plus the adjacent Pará estuary and the longest connecting tidal canal.\n\nEven when detailed maps are available, the length measurement is not always clear. A river may have multiple channels, or anabranches. The length may depend on whether the center or the edge of the river is measured. It may not be clear how to measure the length through a lake. Seasonal and annual changes may alter both rivers and lakes. Other factors that can change the length of a river include cycles of erosion and flooding, dams, levees, and channelization. In addition, the length of meanders can change significantly over time due to natural or artificial cutoffs, when a new channel cuts across a narrow strip of land, bypassing a large river bend. For example, due to 18 cutoffs created between 1766 and 1885 the length of the Mississippi River from Cairo, Illinois, to New Orleans, Louisiana, was reduced by .\n\nThese points make it difficult, if not impossible, to get an accurate measurement of the length of a river. The varying accuracy and precision also makes it difficult to make length comparisons between different rivers without a degree of uncertainty.\n\nOne should take the aforementioned discussion into account when using the data in the following table. For most rivers, different sources provide conflicting information on the length of a river system. The information in different sources is between parentheses.\n\nThe Amazon basin formerly drained westwards into the Pacific Ocean, until the Andes rose and reversed the drainage.\n\nThe Congo basin is completely surrounded by high land, except for its long narrow exit valley past Kinshasa, including waterfalls around Manyanga. That gives the impression that most of the Congo basin was formerly on a much higher land level and that it was rejuvenated by much of its lower course being removed. Before Gondwanaland broke up due to continental drift, the Congo would likely have flowed into the Amazon. \n\nThis river would have been about long, in the last Ice Age. Its longest headwater was the Selenga river of Mongolia: it drained through ice-dammed lakes and the Aral Sea and the Caspian Sea to the Black Sea.\n\nDuring the last glacial maximum, much of what is now the southern part of the North Sea was land, known to archaeologists as Doggerland. At this time, the Thames, the Meuse, the Scheldt, and the Rhine probably joined before flowing into the sea, in a system known by palaeogeographers as the Loubourg or Lobourg River System. There is some debate as to whether this river would have flowed southwest into what is now the English Channel, or flowed north, emerging into the North Sea close to modern Yorkshire. If the latter hypothesis is true, the Rhine would have attained a length of close to . The former hypothesis would have produced a shorter river, some in length. Current scientific research favours the former opinion, with the Thames and Rhine meeting in a large lake, the outflow of which was close to the present-day Straits of Dover.\n\n\n\n"}
{"id": "55294441", "url": "https://en.wikipedia.org/wiki?curid=55294441", "title": "List of the Cenozoic life of Arizona", "text": "List of the Cenozoic life of Arizona\n\nThis list of the Cenozoic life of Arizona contains the various prehistoric life-forms whose fossilized remains have been reported from within the US state of Arizona and are between 66 million and 10,000 years of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1179070", "url": "https://en.wikipedia.org/wiki?curid=1179070", "title": "Liu Boming (astronaut)", "text": "Liu Boming (astronaut)\n\nLiu Boming (; born September 1966) is a Chinese pilot selected as part of the Shenzhou program. A fighter pilot in the People's Liberation Army Air Force, he was selected to be an CNSA member in 1998.\n\nLiu, along with Zhai Zhigang and Jing Haipeng, was chosen to be the main crew of \"Shenzhou 7\", with Zhai as commander, on 17 September 2008. On 25 September, at 21:10 CST, they launched into space as China's third human spaceflight mission, and the first Chinese mission ever to have a three-man crew. Liu and Zhai participated in China's first-ever space walk.\n\nLiu wore a Russian Orlan-M spacesuit, while Zhai wore the Chinese-made Feitian space suit. Liu remained in the open portal of the orbital module, assisting Zhai on his spacewalk; later in the space walk, Liu also performed a stand-up extra-vehicular activity, partially leaving the orbital module in order to hand Zhai a Chinese flag.\n\nAfter the spacewalk, Liu said in an interview that his inspiration and idols have always been Neil Armstrong and Yuri Gagarin. \n\n\n"}
{"id": "20552524", "url": "https://en.wikipedia.org/wiki?curid=20552524", "title": "Micro-pulling-down", "text": "Micro-pulling-down\n\nThe micro-pulling-down (µ-PD) method is a crystal growth technique based on continuous transport of the melted substance through micro-channel(s) made in a crucible bottom. Continuous solidification of the melt is progressed on a liquid/solid interface positioned under the crucible. In a steady state, both the melt and the crystal are pulled-down with a constant (but generally different) velocity.\n\nMany different types of crystal are grown by this technique, including YAlO, Si, Si-Ge, LiNbO, \nα-AlO, YO, ScO, \nLiF, CaF, BaF, etc.\n\nStandard routine procedure used in the growth of most of µ-PD crystals is well developed. The general stages of the growths include:\n\n\n\n"}
{"id": "926703", "url": "https://en.wikipedia.org/wiki?curid=926703", "title": "Micromount", "text": "Micromount\n\nMicromount is term used by mineral collectors and amateur geologists to describe mineral specimens that are best appreciated using an optical aid, commonly a hand-lens or a binocular microscope. The magnification employed ranges from 10 to 40 times.\n\nA micromount is permanently mounted in some kind of box and labelled with the name of the mineral and the location from which it came. Proper mounting both preserves delicate crystals, and facilitates their handling.\n\nMicromount specimen collecting has a number of advantages over collecting larger specimens.\n\nMicromounting is a craft, as much as it is a collecting activity. Two English language books on\nmicromounting have been published, by Milton Speckels\nin 1965, and by Quintin Wight\nin 1993.\n\n"}
{"id": "2193055", "url": "https://en.wikipedia.org/wiki?curid=2193055", "title": "Mikael Fortelius", "text": "Mikael Fortelius\n\nMikael Fortelius (born 1 February 1954) is Professor of Evolutionary Palaeontology at the University of Helsinki and the coordinator of the Neogene of the Old World database of fossil mammals. His research involves the evolution of Eurasian land mammals and terrestrial environments during the Neogene, ecomorphology of ungulates, developmental biology, the function and evolution of mammalian teeth, and scaling problems (changes in size with growth or as species evolve). He is an expert on indricotheres. He has authored and co-authored a number of papers in peer-reviewed international journals as well as articles on popular science and other published material. He is married to Asta Irene Rosenström, and he has three children.\n\n"}
{"id": "51682055", "url": "https://en.wikipedia.org/wiki?curid=51682055", "title": "Multi-component gas analyzer system", "text": "Multi-component gas analyzer system\n\nA multi-component gas analyzer system (Multi-GAS) is an instrument package used to take real-time high-resolution measurements of volcanic gas plumes. A Multi-GAS package includes an infrared spectrometer for CO, two electrochemical sensors for SO and HS, and pressure–temperature–humidity sensors, all in a weatherproof box weighing approximately 3 kg, as well as radio transmitters to transmit data to remote locations. The instrument package is portable, and its operation and data analysis are simple enough to be conducted by non-specialists.\n\nMulti-GAS instruments have been used to measure volcanic gas plumes at Mount Etna, Stromboli, Vulcano Italy, Villarrica (volcano) Chile, Masaya Volcano Nicaragua, Mount Yasur and Ambrym Vanuatu, Miyake-jima and Mount Asama Japan, Soufrière Hills Montserrat, with permanent installations at Etna and Stromboli.\n\nMulti-GAS measurements of CO/SO ratios can allow detection of the pre-eruptive degassing of rising magmas, improving prediction of volcanic activity.\n"}
{"id": "31033811", "url": "https://en.wikipedia.org/wiki?curid=31033811", "title": "Munitions Safety Information Analysis Center", "text": "Munitions Safety Information Analysis Center\n\nMunitions Safety Information Analysis Center, also referred to as MSIAC, is a NATO project funded directly by its member nations, not all of which are NATO members. There are currently 15 member nations: Australia, Belgium, Canada, Finland, France, Germany, Italy, Netherlands, Norway, Spain, Sweden, United Kingdom, United States, Poland and Republic of Korea.\n\nOn May 26, 1988 the NATO nations France, Netherlands, Norway, United Kingdom and the United States based on a Memorandum of Understanding decided to create Pilot-NIMIC (\"NIMIC = NATO Insensitive Munitions Information Center\"). The organization was intended to assist national and international programs to develop insensitive munitions (IM). The idea for NIMIC was born at the NATO AC/310 \"Workshop on Insensitive Munitions Information Exchange\" in 1986 and was actively supported by the Chairman of AC/310, IGA M. Thévenin and Principal US Member of AC/310, Dr. R. Derr. The founding of this organization was promoted by former NATO Group AC/310 (Safety and Suitability for Service of Munitions and Explosives) in an effort to support munitions developers located in NATO countries. The PILOT-NIMIC office was located at Applied Physics Laboratory (APL) of Johns Hopkins University, Laurel, Maryland, USA. The initial team comprised a project manager and five technical specialists - basically the same structure MSIAC shows today (see below). Soon after its foundation Canada joined the project in late 1989.\n\nAfter its first successful two years of operation the project moved to NATO Headquarters Brussels and officially became NIMIC on May 1, 1991. Spain and Australia entered the project in 1994, Portugal and Italy became members in 1995. In 1998 Portugal decided to leave NIMIC. Denmark entered the project 1999 but left soon after in 2004.\n\nIn May 2003 the Conference of National Armament Directors (CNAD) Ammunition Safety Group (AC/326) (CASG) was formed by a merger of former AC/258 Group of Experts on the Safety Aspects of Transportation and Storage of Military Ammunition and Explosives and the former AC/310 Group of Safety and Suitability for Service of Munitions and Explosives. With a given broader emphasis on Munitions Safety it was decided to modify NIMIC's scope to take this change into account. Hence its transition to Munitions Safety Information Analysis Center (MSIAC) began with the Pilot MSIAC phase in Spring 2003. The successful transition was accomplished by late 2004 and MSIAC was officially operational on December 15, 2004. Germany finally entered the project in 2005. Belgium entered MSIAC in 2015, Poland - in 2017 and Republic of Korea - in 2018.\n\nThe MSIAC Project Office consists of a steering committee, National Focal Point Officers (NFPO) and an Information Analysis Center (MSIAC). The steering committee is made up of one voting representative from each member nation and an elected chairman. It is responsible for implementing the Memorandum of Understanding (MOU) that established MSIAC and for developing its policy. This policy is carried out on a day-to-day basis by the MSIAC Project Manager.\n\nThe NATO Munitions Safety Information Analysis Center (MSIAC) (\"Centre d'information et d'analyse sur la sécurité des munitions de l'OTAN\" (CIASM) in French) provides technical consultancy services to its member nations in the area of munitions safety.\nAreas with which MSIAC is concerned include :\n\nWithin these areas, MSIAC performs the following functions:\n\n"}
{"id": "57032696", "url": "https://en.wikipedia.org/wiki?curid=57032696", "title": "Nottingham Weather Centre", "text": "Nottingham Weather Centre\n\nThe Nottingham Weather Centre (also referred to Nottingham Watnall) is a functioning observation and weather station located in Watnall, Nottinghamshire in England. The weather station is located from the city centre of Nottingham, and is the closest weather station to Nottingham with observations.\n\nThe weather station was established in 1941 and like many other weather stations in the United Kingdom, the recording of weather observations began in January 1960. The weather centre is currently being managed by the Met Office.\n\nAs the weather station is located at a higher elevation of , temperatures are usually cooler than the city centre of Nottingham and the Sutton Bonington weather station, as these locations are situated on a lower elevation.\n\nDuring the period of 1981–2010, the Nottingham Weather Centre lies within hardiness zone 9a and lies within the AHS heat zone 1. For the periods of 1961–1990 and 1971–2000 the station lied within the hardiness zone 8b, as the average annual minimum temperature was below during those periods.\n"}
{"id": "8921722", "url": "https://en.wikipedia.org/wiki?curid=8921722", "title": "Racah parameter", "text": "Racah parameter\n\nWhen an atom has more than one electron there will be some electrostatic repulsion between those electrons. The amount of repulsion varies from atom to atom, depending upon the number and spin of the electrons and the orbitals they occupy. The total repulsion can be expressed in terms of three parameters \"A\", \"B\" and \"C\" which are known as the Racah parameters after Giulio Racah, who first described them. They are generally obtained empirically from gas-phase spectroscopic studies of atoms.\n\nThey are often used in transition-metal chemistry to describe the repulsion energy associated with an electronic term. For example, the interelectronic repulsion of a P term is \"A\" + 7\"B\", and of a F term is \"A\" - 8\"B\", and the difference between them is therefore 15\"B\".\n\nThe Racah parameters are defined as\n\nformula_1\n\nwhere formula_2 are Slater integrals\n\nformula_3\n\nand formula_4 are the Slater-Condon parameters\n\nformula_5\n\nwhere formula_6 is the normalized radial part of an electron orbital, formula_7 and formula_8\n\n\n"}
{"id": "55412975", "url": "https://en.wikipedia.org/wiki?curid=55412975", "title": "Rhonda Voskuhl", "text": "Rhonda Voskuhl\n\nRhonda Renee Voskuhl is an American physician, research scientist, and professor. She is a member of the Brain Research Institute (BRI) at the David Geffen School of Medicine at UCLA and is the director of its Multiple Sclerosis Program. Voskuhl has published numerous scientific articles in academic journals and has served in the role of principal investigator for several treatment trials investigating potential treatments for multiple sclerosis (MS).\n\nVoskuhl has described her research as \"bedside to bench to bedside\", meaning observations made in clinical settings are used as a basis for the investigation of the relevant biological mechanisms of action. The information discovered is then applied in a clinical setting, typically through a drug therapy.\n\nDue to well-documented differences in prevalence of MS in males and females, significant research on the autoimmune condition has turned to the neuropreservative effects of sex hormones. Evidence of suppression of MS symptoms in pregnant women in the third trimester ultimately led to a focus on the female sex hormone estriol.\n\nIn 2001, Voskuhl published an article outlining discrepancies in EAE between male and female mice; she noted that females were more susceptible to EAE, mirroring the sex-based difference in MS in humans. It was found that the neuroprotective effects of testosterone contributed most to this discrepancy in mice. However, this sex difference is reduced during late pregnancy in females, when estriol levels are significantly higher than other periods of life. Her article established high levels of estriol as a possible explanation for the reduction in EAE symptoms observed during late pregnancy.\n\nIn 2002, Voskuhl was part of the investigative team that found that treating non-pregnant women with 8 mg/day estriol helped to relieve symptoms, including lesion number and volume. Upon cessation of treatment, lesion number and volume returned to pre-treatment levels. After reinstituting treatment, lesion number and volume again decreased significantly. Cognitive ability, evaluated by the Paced Auditory Serial Addition Test (PASAT), also improved in those treated with estriol. In the authors' abstract, they indicated that this result warranted further experimentation through a placebo-controlled clinical trial. This experiment was small, with only six women with relapse remitting MS (RRMS) and four women with secondary progressive MS (SPMS) finishing the trial. The authors noted that estriol generally improved symptoms in women with RRMS, but not in those with SPMS.\n\nIn a 2011 research article, Voskuhl published data revealing that the estrogen receptor α (ERα) on astrocytes, not neurons, was responsible for the reduction of clinical EAE symptoms in mice. Using a gene knockout system Cre-Lox, the research team was able to remove ERα from neurons and in separate mice, remove ERα from astrocytes. It was found that the mice with ERα knocked out in astrocytes experienced an increase in clinical disease symptoms, macrophage and T-cell inflammation in the central nervous system, and axonal loss. These symptoms were not observed in those mice who had ERα removed from neurons.\n\nIn 2016, the results of a Phase II Trial, in which Voskuhl participated, were released, detailing an experiment in which women with RRMS were treated daily with 8 mg estriol or placebo, combined with 20 mg injectable glatiramer acetate - an immunomodulator currently used to treat MS. It was found that women with the estriol treatment had significantly less relapses than the placebo group (0.25 relapses/year and 0.37 relapses/year, respectively), with similar amounts of serious adverse health events. The success of this trial convinced the authors to report that a Phase III Trial was warranted.\n\nIn 2008, Voskuhl, together with Dr. Stefan Gold et al., published a study that revealed the effects of treating men with MS with a 10g gel containing 100 mg testosterone. Based upon the shift in cellular and chemical composition, particularly a decrease in IL-2 cell production, an increase in production growth factor TGFβ1, a decrease in CD4+ T cells, and an increase in NK (natural killer) cells, it was found that testosterone may play an important role in immunomodulation and neuroprotection.\n\nIn a 2009 review article discussing the effects of sex hormones on MS, Voskuhl and Gold noted that one small trial conducted by a research team headed by Dr. Nancy Sicotte suggested testosterone could be effective in preserving cognitive performance and reducing brain atrophy. However, this trial yielded no significant effect on the formation of brain lesions.\n\nIn 2014, Voskuhl participated in an interview discussing estriol as a potential treatment for women with MS. The article discussed how estriol was identified as a potential candidate for drug treatment, including an anecdote about Melissa Glasser, a woman who experienced a reduction in her MS symptoms during each of her four pregnancies.\n\nIn 2016, Voskuhl was quoted in an article addressing gender bias in scientific study; she noted that male and female mice had different disease progression in the animal MS model.\n"}
{"id": "12414063", "url": "https://en.wikipedia.org/wiki?curid=12414063", "title": "Rose's ghost frog", "text": "Rose's ghost frog\n\nThe Rose's ghost frog or Table Mountain ghost frog (Heleophryne rosei) is a species of frog in the Heleophrynidae family endemic to South Africa. It is a medium-sized species with purple or brown blotches on a pale green background and large discs on its fingers and toes. It has a very restricted range, being only known from the slopes of parts of Table Mountain. The tadpoles live in permanent streams but these are in danger of drying up because of the establishment of pine plantations. Because of its small range and changes in its habitat, this frog is listed as critically endangered by the International Union for Conservation of Nature.\n\nThis is a moderately sized frog, with the larger female up to and the smaller male up to in length. The coloration of adults is striking, often a pale green background with purple to brown blotches. The fingers and toes have large, triangular terminal discs. A rudimentary thumb is present as a distinct inner metacarpal tubercle. The feet are half webbed, with one phalanx of the fifth toe free of web. The tadpole has neither an upper nor lower jaw sheath, but up to 17 rows of posterior labial teeth. The tadpole also has a large oral disc and is able to climb up wet vertical rock faces.\n\nThis species is known only from the southern, eastern, and marginally western slopes of Table Mountain in Cape Town.\n\nThe typical habitat of this frog includes moist, forested gorges, with vertical rock faces to more exposed streams surrounded by dense marginal vegetation, the latter habitat usually found at higher altitudes. Tadpoles require year-round supply of water whereas adults can stray away from streams, even to be found in caves.\n\nThe frogs are found on rock ledges or up in vegetation at night, retreating under large rocks and in cracks of rocks during the day.\n\nThese frogs eat a range of small insects and other forest arthropods.\n\nBreeding starts in November when the streams are low but the temperature is high. The male's secondary sexual characteristics include a number of small black spines on the outside surfaces of the forearms, on the back, and on the top of the back legs. The eggs have not been found, but in other species they are deposited under rocks in streams. The tadpoles develop for about 12 months.\n\nThis species is listed as critically endangered by the International Union for Conservation of Nature (IUCN) and in the South African Red Data Book. The population is small, geographically restricted, and threatened by the plantations of pines on the mountain that cause the streams to dry up. Many of the streams historically populated by \"Heleophryne rosei\" were diverted during the 1900s in order to supply the newly built reservoirs on Table Mountain.\n"}
{"id": "57443136", "url": "https://en.wikipedia.org/wiki?curid=57443136", "title": "Salma Al Kindi", "text": "Salma Al Kindi\n\nSalma Al Kindi is an Omani chemist. She is professor of Analytical Chemistry and dean of the College of Sciences at the Sultan Qaboos University (SQU).\n\nIn 2017 she received a Lifetime Achievement in Chemistry Award from the Venus International Foundation based in Chennai.\n"}
{"id": "20003259", "url": "https://en.wikipedia.org/wiki?curid=20003259", "title": "Shore lead", "text": "Shore lead\n\nA shore lead (or coastal lead) is an oceanographic term for a waterway opening between pack ice and shore. While the gap of water may be as narrow as a tide crack if closed by wind or currents, it can be as wide as . Its formation can be influenced by tidal action, or subsurface conditions, such as current and ocean floor. Commonly, a shore lead is navigable by surface vessels.\n\nAn opening (\"lead\") between pack ice and fast ice is referred to as a flaw lead.\n"}
{"id": "18180411", "url": "https://en.wikipedia.org/wiki?curid=18180411", "title": "Siderophilic bacteria", "text": "Siderophilic bacteria\n\nSiderophilic bacteria are bacteria that require or are facilitated by free iron. They may include \"Vibrio vulnificus\", \"Listeria monocytogenes\", \"Yersinia enterocolica\", \"Salmonella enterica\" (serotype Typhimurium), \"Klebsiella pneumoniae\" and \"Escherichia coli\". One possible symptom of haemochromatosis is susceptibility to infections from these species.\nCertain non-bacterial microorganisms such as \"Rhizopus arrhizus\" and \"Mucor\" may also be siderophilic.\n\n"}
{"id": "15890020", "url": "https://en.wikipedia.org/wiki?curid=15890020", "title": "Soybean management practices", "text": "Soybean management practices\n\nSoybean management practices in farming are the decisions a producer must make in order to raise a soybean crop. The type of tillage, plant population, row spacing, and planting date are four major management decisions that soybean farmers must consider. How individual producers choose to handle each management application depends on their own farming circumstances.\n\nTillage is defined in farming as the disturbance of soil in preparation for a crop. Tillage is usually done to warm the seed bed up, to help dry the seed bed out, to break up disease, and to reduce early weed pressure. Tillage prior to planting adds cost per acre farmed and has not been shown to increase soybean yields significantly.\n\n“No till” is the practice of planting seed directly into the ground without disturbing the soil prior to planting. This practice eliminates the tillage pass, saving the farmer the associated costs. Planting no-till places seed directly into a cooler and wetter seed bed, which can slow germination. This process is considered a good conservation practice because tilling disturbs the soil crust, causing erosion. The practice of no-till is currently on the rise among farmers in the midwestern United States.\n\nPlant population is the number of seeds planted in a given area. Population is usually expressed as plants or seeds per acre. Plant population is one of two major factors that determine canopy closure (when the plants cover the space in between the rows) and other yield components. A higher seed population will close the canopy faster and reduce soil moisture loss. A high plant population does not necessarily equal a high yield. The recommended seeding rate is 125,000 to 140,000 seeds per acre. The goal is to achieve a final stand of 100,000 plants per acre. Planting the extra seed gives the producer added insurance that each acre will attain a final stand of 100,000 plants.\n\nRow spacing is the second factor that determines canopy closure and yield components. Row spacing can either refer to the space between plants in the same row or the distance between two rows. Row spacing determines the degree of plant to plant competition. Rows planted closer together in a population will decrease the space between plants. Closer row widths increase plant to plant competition for nutrients and water but also increase sunlight use efficiency. According to former Iowa State University Soybean Extension Specialist Palle Pedersen, current recommendations are to plant rows that are less than 30\" apart. This increases light interception and decreases weed competition.\n\nPlanting date refers to the date that the seed is sown. This concept is of prime importance in growing soybeans because yields are strongly correlated with planting date. Data from Iowa State University shows that earlier planted soybeans tend to have higher yields than soybeans planted later in the growing season. Producers seeding early need to check that the seedbed is in the right conditions (temperature, moisture, nutrients) since planting into a sub-optimal seedbed will lose yield instead of gaining it. Other special considerations include soil pathogens, insect pressure, and the possibility of frost. Fungicide treatment and insecticide treatment are options used to help reduce the risk of soil pathogens and insect pressure. Knowing the chance of frost in a given area prior to planting helps determine how early seed can safely be sown.\n\n"}
{"id": "82368", "url": "https://en.wikipedia.org/wiki?curid=82368", "title": "Structure and Interpretation of Computer Programs", "text": "Structure and Interpretation of Computer Programs\n\nStructure and Interpretation of Computer Programs (SICP) is a textbook aiming to teach the principles of computer programming, such as abstraction in programming, metalinguistic abstraction, recursion, interpreters, and modular programming. It is widely considered a classic text in computer science, and is colloquially known as the wizard book, due to the wizard on the jacket. It was first published in 1985 by MIT Press and written by Massachusetts Institute of Technology (MIT) professors Harold Abelson and Gerald Jay Sussman, with Julie Sussman. It was formerly used as the textbook of MIT introductory programming class and at other schools. Before SICP, the introductory courses were almost always filled with learning the details of some programming language, while SICP focuses on finding general patterns from specific problems and building software tools that embody each pattern.\n\nUsing Scheme, a dialect of the Lisp programming language, the book explains core computer science concepts.\n\nThe book also introduces a practical implementation of the register machine concept, defining and developing an assembler for such a construct, which is used as a virtual machine for the implementation of interpreters and compilers in the book, and as a testbed for illustrating the implementation and effect of modifications to the evaluation mechanism. Working Scheme systems based on the design described in this book are quite common student projects.\n\nThe book uses some fictional characters repeatedly:\n\nThe book is licensed under a Creative Commons Attribution ShareAlike 4.0 License.\n\nThe book was used as the textbook of MIT's old introductory programming class, 6.001. This class has been replaced by 6.0001, which uses Python. Other schools also made use of the book as a course textbook. The second edition () appeared in 1996.\nMore recently it is used as the textbook of MIT's Large Scale Symbolic Systems class, 6.945.\n\n\"Byte\" recommended SICP \"for professional programmers who are really interested in their profession\". The magazine stated that the book was not easy to read, but that it would expose experienced programmers to both old and new topics.\n\nSICP has been influential in computer science education, and a number of later books have been inspired by its style.\n\n"}
{"id": "8696119", "url": "https://en.wikipedia.org/wiki?curid=8696119", "title": "Ultraviolet photoelectron spectroscopy", "text": "Ultraviolet photoelectron spectroscopy\n\nUltraviolet photoelectron spectroscopy (UPS) refers to the measurement of kinetic energy spectra of photoelectrons emitted by molecules which have absorbed ultraviolet photons, in order to determine molecular orbital energies in the valence region.\n\nIf Einstein's photoelectric law is applied to a free molecule, the kinetic energy (formula_1) of an emitted photoelectron is given by\n\nwhere \"h\" is Planck's constant, ν is the frequency of the ionizing light, and I is an ionization energy for the formation of a singly charged ion in either the ground state or an excited state. According to Koopmans' theorem, each such ionization energy may be identified with the energy of an occupied molecular orbital. The ground-state ion is formed by removal of an electron from the highest occupied molecular orbital, while excited ions are formed by removal of an electron from a lower occupied orbital.\n\nPrior to 1960, virtually all measurements of photoelectron kinetic energies were for electrons emitted from metals and other solid surfaces. About 1956 Kai Siegbahn developed X-ray photoelectron spectroscopy (XPS) for surface chemical analysis. This method uses x-ray sources to study energy levels of atomic core electrons, and at the time had an energy resolution of about 1 eV (electronvolt).\n\nThe ultraviolet method (UPS) was pioneered by Feodor I. Vilesov, a physicist at St. Petersburg (Leningrad) State University in Russia (USSR) in 1961 to study the photoelectron spectra of free molecules in the gas phase. The early experiments used monochromatized radiation from a hydrogen discharge and a retarding potential analizer to measure the photoelectron energies.\nThe PES was further developed by David W. Turner, a physical chemist at Imperial College in London and then at Oxford University, in a series of publications from 1962 to 1967. As a photon source, he used a helium discharge lamp which emits a wavelength of 58.4 nm (corresponding to an energy of 21.2 eV) in the vacuum ultraviolet region. With this source Turner's group obtained an energy resolution of 0.02 eV. Turner referred to the method as \"molecular photoelectron spectroscopy\", now usually \"Ultraviolet photoelectron spectroscopy\" or UPS. As compared to XPS, UPS is limited to energy levels of valence electrons, but measures them more accurately. After 1967 commercial UPS spectrometers became available.\n\nThe UPS measures experimental molecular orbital energies for comparison with theoretical values from quantum chemistry, which was also extensively developed in the 1960s. The photoelectron spectrum of a molecule contains a series of peaks each corresponding to one valence-region molecular orbital energy level. Also, the high resolution allowed the observation of fine structure due to vibrational levels of the molecular ion, which facilitates the assignment of peaks to bonding, nonbonding or antibonding molecular orbitals.\n\nThe method was later extended to the study of solid surfaces where it is usually described as photoemission spectroscopy (PES). It is particularly sensitive to the surface region (to 10 nm depth), due to the short range of the emitted photoelectrons (compared to X-rays). It is therefore used to study adsorbed species and their binding to the surface, as well as their orientation on the surface.\n\nA useful result from characterization of solids by UPS is the determination of the work function of the material. An example of this determination is given by Park et al. Briefly, the full width of the photoelectron spectrum (from the highest kinetic energy/lowest binding energy point to the low kinetic energy cutoff) is measured and subtracted from the photon energy of the exciting radiation, and the difference is the work function. Often, the sample is electrically biased negative to separate the low energy cutoff from the spectrometer response.\n\nUPS has seen a considerable revival with the increasing availability of synchrotron light sources which provide a wide range of monochromatic photon energies.\n\n"}
{"id": "23981392", "url": "https://en.wikipedia.org/wiki?curid=23981392", "title": "William Wyse Professor of Social Anthropology", "text": "William Wyse Professor of Social Anthropology\n\nThe William Wyse Professorship of Social Anthropology is a professorship in social anthropology at the University of Cambridge. It was founded on 18 June 1932 and endowed partly with the support of Trinity College from money bequeathed to them by William Wyse, formerly Fellow and Honorary Fellow of Trinity. \n\nThe professorship is assigned to the Faculty of Archaeology and Anthropology.\n\n"}
{"id": "33634815", "url": "https://en.wikipedia.org/wiki?curid=33634815", "title": "Winter-over syndrome", "text": "Winter-over syndrome\n\nThe winter-over syndrome is a condition that occurs in individuals who \"winter-over\" throughout the Antarctic (or Arctic) winter, which can last seven to eight months. It has been observed in inhabitants of research stations in Antarctica, as well as in polar bases such as Thule, Alert and Eureka. It consists of a variety of behavioral and medical disturbances, including irritability, depression, insomnia, absentmindedness, aggressive behavior, and irritable bowel syndrome.\n\nThe Antarctic winter is a period of no physical contact with other continents or Antarctic stations, including no airplanes, ships, or mail. For these reasons, the immobility, monotony, harsh physical environment, sexual deprivation, and the general isolation, are believed to contribute to increased anxiety and depression among the residents of the station.\n\nSeveral studies have been done over the years to determine the contributing causes, or stresses, of \"winter-over\" syndrome. These include stress, social isolation, subsyndromal seasonal affective disorder and polar T syndrome. It would appear that the cold, danger, and hardships are not major stresses. The most important psychological stresses appear to be: the problem of individual adjustment to the group, the relative monotony of the environment, and the absence of certain accustomed sources of emotional satisfaction. In addition to isolation from the outside world, there is confinement or lack of isolation within the research stations themselves. During fieldwork conducted at McMurdo and South Pole stations in 1988 and 1989, informants complained that the lack of privacy and constant gossip that existed within the community, had a negative influence on social relationships, especially between men and women. As a result, 60% of one's leisure time is spent alone in a dorm room, whereas others are forced to work and live in confined spaces, due to the nature of their work.\n"}
{"id": "31048466", "url": "https://en.wikipedia.org/wiki?curid=31048466", "title": "Zalman Gorelik", "text": "Zalman Gorelik\n\nZalman Gorelik (; ; 5 April 1908 in Bobruisk – 16 February 1987 in Minsk) was a geologist, tectonist, and organizer of the Geological Survey of Belarus. He was also co-discoverer of the first deposits of potash, rock salt, and oil in the Pripyat Trough. Doctor of Geological and Mineralogical Sciences (1973).\n"}
