{"id": "5317576", "url": "https://en.wikipedia.org/wiki?curid=5317576", "title": "9-Crown-3", "text": "9-Crown-3\n\n9-Crown-3, also called 1,4,7-trioxonane or 1,4,7-trioxacyclononane is a crown ether with the formula CHO. It is a cyclic trimer of ethylene oxide which is specific for the lithium cation. Its point group is S.\n\n"}
{"id": "51412448", "url": "https://en.wikipedia.org/wiki?curid=51412448", "title": "Abraham Jacobus Wendel", "text": "Abraham Jacobus Wendel\n\nAbraham Jacobus Wendel (Leiden, 31 October 1826 - Leiden, 23 September 1915) was a Dutch lithographer, draughtsman and scientific botanical and paleontological illustrator using the signature AW.\n\nAbraham Jacobus Wendel was born in 1826 in Leiden as a son of Jacobus Cornelis Wendel (1796-1860) and Johanna Vegt (1801-1869). In 1849 he married Jannetje Koolen (1826-1904) and had six children, including Abraham Jacobus Johannes Wendel (Leiden, 9 July 1854 - 3 April 1930), who became a lithographer and botanical artist in his own right, using signature AJJW. His father Abraham Jacobus Wendel created the botanical illustrations for many books and scientific journal articles. He was a lithographer with the printing house \"Arntz & Co.\" in Leiden, and made the illustrations for W. H. De Vriese et al. \"Descriptions et figures des plantes nouvelles et rares\" (1847). In 1887 he worked with \"Boek- en steendrukkerij P.W.M. Trap\" in Leiden: both Wendel and Trap are acknowledged in volumes 6 and 7 of the \"Annales du Jardin botanique de Buitenzorg\" (Ann.Jard.Bot.Btzg.).\n\nBotanical and paleontological art, including:\n\n\n"}
{"id": "11695492", "url": "https://en.wikipedia.org/wiki?curid=11695492", "title": "Acetone–butanol–ethanol fermentation", "text": "Acetone–butanol–ethanol fermentation\n\nAcetone–butanol–ethanol (ABE) fermentation is a process that uses bacterial fermentation to produce acetone, n-Butanol, and ethanol from carbohydrates such as starch and glucose. It was developed by the chemist Chaim Weizmann and was the primary process used to make acetone during World War I, such as to produce cordite, a substance essential for the British war industry.\n\nThe process may be likened to how yeast ferments sugars to produce ethanol for wine, beer, or fuel, but the organisms that carry out the ABE fermentation are strictly anaerobic (obligate anaerobes). The ABE fermentation produces solvents in a ratio of 3 parts acetone, 6 parts butanol to 1 part ethanol. It usually uses a strain of bacteria from the Class Clostridia (Family Clostridiaceae). \"Clostridium acetobutylicum\" is the most well-studied and widely used species, although \"Clostridium beijerinckii\" has also been used with good results.\n\nFor gas stripping, the most common gases used are the off-gases from the fermentation itself, a mixture of carbon dioxide and hydrogen gas.\n\nThe production of butanol by biological means was first performed by Louis Pasteur in 1861.\n\nIn 1905, Austrian biochemist Franz Schardinger found that acetone could similarly be produced.\n\nIn 1910 Auguste Fernbach (1860-1939) developed a bacterial fermentation process using potato starch as a feedstock in the production of butanol.\n\nIndustrial exploitation of ABE fermentation started in 1916, during World War I, with Chaim Weizmann's isolation of Clostridium acetobutylicum, as described in U.S. patent 1315585.\n\nThe Weizmann process was operated by Commercial Solvents Corporation from about 1920 to 1964 with plants in the US (Terre Haute, IN, and Peoria, IL), and Liverpool, England. The Peoria plant was the largest of the three; it used molasses as feedstock and had 96 50,000-gallon fermenters.\n\nAfter World War II, ABE fermentation became generally non-profitable, compared to the production of the same three solvents (acetone, butanol, ethanol) from petroleum.\n\nCurrently there is one ABE plant operated by Green Biologics Ltd in Minnesota.\n\nIn order to make ABE fermentation profitable, many in-situ product recovery systems have been developed. These include gas stripping, pervaporation, membrane distillation, adsorption, and reverse osmosis. Green Biologics Ltd has implemented this at an industrial scale.\n\nABE fermentation is attracting renewed interest with an eye on butanol as a renewable biofuel.\n"}
{"id": "23044987", "url": "https://en.wikipedia.org/wiki?curid=23044987", "title": "Attribute substitution", "text": "Attribute substitution\n\nAttribute substitution, also known as Substitution bias, is a psychological process thought to underlie a number of cognitive biases and perceptual illusions. It occurs when an individual has to make a judgment (of a \"target attribute\") that is computationally complex, and instead substitutes a more easily calculated \"heuristic attribute\". This substitution is thought of as taking place in the automatic \"intuitive\" judgment system, rather than the more self-aware \"reflective\" system. Hence, when someone tries to answer a difficult question, they may actually answer a related but different question, without realizing that a substitution has taken place. This explains why individuals can be unaware of their own biases, and why biases persist even when the subject is made aware of them. It also explains why human judgments often fail to show regression toward the mean.\n\nThe theory of attribute substitution unifies a number of separate explanations of reasoning errors in terms of cognitive heuristics. In turn, the theory is subsumed by an \"effort-reduction framework\" proposed by Anuj K. Shah and Daniel M. Oppenheimer, which states that people use a variety of techniques to reduce the effort of making decisions.\n\nIn a 1974 paper, psychologists Amos Tversky and Daniel Kahneman argued that a broad family of biases (systematic errors in judgment and decision) were explainable in terms of a few heuristics (information-processing shortcuts), including availability and representativeness. In a 2002 revision of the theory, Kahneman and Shane Frederick proposed attribute substitution as a process underlying these and other effects.\n\nIn 1975, psychologist Stanley Smith Stevens proposed that the strength of a stimulus (e.g., the brightness of a light, the severity of a crime) is encoded neurally in a way that is independent of modality. Kahneman and Frederick built on this idea, arguing that the target attribute and heuristic attribute could be unrelated.\n\nKahneman and Frederick propose three conditions for attribute substitution:\n\nAttribute substitution explains the persistence of some illusions. For example, when subjects judge the size of two figures in a perspective picture, their apparent sizes can be distorted by the 3D context, making a convincing optical illusion. The theory states that the three-dimensional size of the figure (which is accessible because it is automatically computed by the visual system) is substituted for its two-dimensional size on the page. Experienced painters and photographers are less susceptible to this illusion, because the two-dimensional size is more accessible to their perception.\n\nKahneman gives an example where some Americans were offered insurance against their own death in a terrorist attack while on a trip to Europe, while another group were offered insurance that would cover death of any kind on the trip. The former group were willing to pay more even though \"death of any kind\" includes \"death in a terrorist attack\", Kahneman suggests that the attribute of fear is being substituted for a calculation of the total risks of travel. Fear of terrorism for these subjects was stronger than a general fear of dying on a foreign trip.\n\nStereotypes can be a source of heuristic attributes. In a face-to-face conversation with a stranger, judging their intelligence is more computationally complex than judging the colour of their skin. So if the subject has a stereotype about the relative intelligence of whites, blacks, and Asians, that racial attribute might substitute for the more intangible attribute of intelligence. The pre-conscious, intuitive nature of attribute substitution explains how subjects can be influenced by the stereotype while thinking that they have made an honest, unbiased evaluation of the other person's intelligence.\n\nSunstein argued that attribute substitution is pervasive when people reason about moral, political, or legal matters. Given a difficult, novel problem in these areas, people search for a more familiar, related problem (a \"prototypical case\") and apply its solution as the solution to the harder problem. According to Sunstein, the opinions of trusted political or religious authorities can serve as heuristic attributes when people are asked their own opinions on a matter. Another source of heuristic attributes is emotion: people's moral opinions on sensitive subjects like sexuality and human cloning may be driven by reactions such as disgust, rather than by reasoned principles. Critics demanded more evidence from Sunstein.\n\nMonin reports a series of experiments in which subjects, looking at photographs of faces, have to judge whether they have seen those faces before. It is repeatedly found that attractive faces are more likely to be mistakenly labeled as familiar. Monin interprets this result in terms of attribute substitution. The heuristic attribute in this case is a \"warm glow\"; a positive feeling towards someone that might either be due to their being familiar or being attractive. This interpretation has been criticised, because not all the variance in the familiarity data is accounted for by attractiveness.\n\nThe most direct evidence, according to Kahneman, is a 1973 experiment that used a psychological profile of Tom W., a fictional graduate student. One group of subjects had to rate Tom's \"similarity\" to a typical student in each of nine academic areas (Law, Engineering, Library Science etc.). Another group had to rate \"how likely\" it is that Tom specialised in each area. If these ratings of likelihood are governed by probability, then they should resemble the base rates, i.e., the proportion of students in each of the nine areas (which had been separately estimated by a third group). A probabilistic judgment would say that Tom is more likely to be a Humanities student than Library Science, because many more students study Humanities, and the additional information in the profile is vague and unreliable. Instead, the ratings of likelihood matched the ratings of similarity almost perfectly, both in this study and a similar one where subjects judged the likelihood of a fictional woman taking different careers. This suggests that rather than estimating probability using base rates, subjects had substituted the more accessible attribute of similarity.\n\n\n"}
{"id": "16711283", "url": "https://en.wikipedia.org/wiki?curid=16711283", "title": "BKS theory", "text": "BKS theory\n\nThe Bohr–Kramers–Slater theory (BKS theory) was perhaps the final attempt at understanding the interaction of matter and electromagnetic radiation on the basis of the so-called old quantum theory, in which quantum phenomena are treated by imposing quantum restrictions on classically describable behaviour. It was advanced in 1924, and sticks to a \"classical\" wave description of the electromagnetic field. It was perhaps more a research program than a full physical theory, the ideas that are developed not being worked out in a quantitative way. \n\nOne aspect, the idea of modelling atomic behaviour under incident electromagnetic radiation using \"virtual oscillators\" at the absorption and emission frequencies, rather than the (different) apparent frequencies of the Bohr orbits, significantly led Born, Heisenberg and Kramers to explore mathematics that strongly inspired the subsequent development of matrix mechanics, the first form of modern quantum mechanics. The provocativeness of the theory also generated great discussion and renewed attention to the difficulties in the foundations of the old quantum theory. However, physically the most provocative element of the theory, that momentum and energy would not necessarily be conserved in each interaction but only overall, statistically, was soon shown to be in conflict with experiment.\n\nThe initial idea of the BKS theory originated with Slater, who proposed to Bohr and Kramers the following elements of a theory of emission and absorption of radiation by atoms, to be developed during his stay in Copenhagen: \n\nSlater's main intention seems to have been to reconcile the two conflicting models of radiation, viz. the wave and particle models. He may have had good hopes that his idea with respect to oscillators vibrating at the \"differences\" of the frequencies of electron rotations (rather than at the rotation frequencies themselves) might be attractive to Bohr because it solved a problem of the latter's atomic model, even though the physical meaning of these oscillators was far from clear. Nevertheless, Bohr and Kramers had two objections to Slater's proposal: \nAs Jammer puts it, this refocussed the theory \"to harmonize the physical picture of the continuous electromagnetic field with the physical picture, not as Slater had proposed of light quanta, but of the discontinuous quantum transitions in the atom.\" Bohr and Kramers hoped to be able to evade the photon hypothesis on the basis of ongoing work by Kramers to describe \"dispersion\" (in present-day terms inelastic scattering) of light by means of a classical theory of interaction of radiation and matter. But abandoning the concept of the photon, they instead chose to squarely accept the possibility of non-conservation of energy, and momentum.\n\nIn the BKS paper the Compton effect was discussed as an application of the idea of \"\"statistical\" conservation of energy and momentum\" in a continuous process of scattering of radiation by a sample of free electrons, where \"each of the electrons contributes through the emission of coherent secondary wavelets\". Although Compton had already given an attractive account of his experiment on the basis of the photon picture (including conservation of energy and momentum in \"individual\" scattering processes), is it stated in the BKS paper that \"it seems at the present state of science hardly justifiable to reject a formal interpretation as that under consideration [i.e. the weaker assumption of \"statistical\" conservation] as inadequate\". This statement may have prompted experimental physicists to improve `the present state of science' by testing the hypothesis of `statistical energy and momentum conservation'. In any case, already after one year the BKS theory was disproved by experiments studying correlations between the directions into which the emitted radiation and the recoil electron are emitted in individual scattering processes. Such experiments were independently performed by Bothe and Geiger, as well as by Compton and Simon. They provided experimental evidence pointing in the direction of energy and momentum conservation in individual scattering processes (at least, it was shown that the BKS theory was not able to explain the experimental results). More accurate experiments, performed much later, have also confirmed these results.\n\nAs suggested by a letter to Born, for Einstein the corroboration of energy and momentum conservation was probably even more important than his photon hypothesis: \"Bohr's opinion of radiation interests me very much. But I don't want to let myself be driven to a renunciation of strict causality before there has been a much stronger resistance against it than up to now. I cannot bear the thought that an electron exposed to a ray should by its own free decision choose the moment and the direction in which it wants to jump away. If so, I'd rather be a cobbler or even an employee in a gambling house than a physicist. It is true that my attempts to give the quanta palpable shape have failed again and again, but I'm not going to give up hope for a long time yet.\" \n\nBohr's reaction, too, was not primarily related to the photon hypothesis. According to Heisenberg, Bohr remarked: \"Even if Einstein sends me a cable that an irrevocable proof of the physical existence of light-quanta has now been found, the message cannot reach me, because it has to be transmitted by electromagnetic waves.\" For Bohr the lesson to be learned from the disproof of the BKS theory was not that photons do exist, but rather that the applicability of classical space-time pictures in understanding phenomena within the quantum domain is limited. This theme would become particularly important a few years later in developing the notion of complementarity. According to Heisenberg, Born's statistical interpretation also had its ultimate roots in the BKS theory. Hence, despite its failure the BKS theory still provided an important contribution to the revolutionary transition from classical mechanics to quantum mechanics.\n"}
{"id": "11440965", "url": "https://en.wikipedia.org/wiki?curid=11440965", "title": "Boris Rtcheouloff", "text": "Boris Rtcheouloff\n\nBoris Rtcheouloff / Rcheulishvili () was a Georgian scientist, and the inventor of “videotape”. Early attempts to record television signals on magnetic material had started when Boris Rtcheouloff applied for a British patent on 4 January 1927 for a technique of recording television signals on 'a magnetic record of the Poulsen telegraphone type'. Sound would be recorded in sync on the reverse side. These ideas incorporated a way to record sound or pictures by causing a strip, disc, or cylinder of iron or other magnetic material to be magnetized. He did not pay the annual renewal fee, so his patent soon lapsed.\n\nHis godfather is the renowned opera singer Feodor Chaliapin\n"}
{"id": "17511649", "url": "https://en.wikipedia.org/wiki?curid=17511649", "title": "Casa viva", "text": "Casa viva\n\nCasa Viva is a non-profit organization based in Wheaton, Illinois and San Jose, Costa Rica. Casa Viva seeks to place children who have been separated from their families into a safe, caring family. Casa Viva is Spanish for “Living Families” or “Living Homes.” The model of international child care that Casa Viva has created:\nCasa Viva primarily cares for social orphans, and also cares for true orphans.\n\nIn 1998 Philip and Jill Aspegren moved to the Dominican Republic to build an orphanage and train nationals to care for the children there with Kids Alive. But there had to be a better way: a less expensive, less institutional, quicker way to care for children internationally. They began to dream of a childcare model that does not require new buildings and that places children in families rather than in homes.\n\nMoving to Costa Rica in 2005 they began to network with local churches, recruiting families belonging to those churches to care for children. Children who have been separated from their biological families are placed in families on a short term basis while their family is identified and counseled. Children are also placed in Casa Viva homes long term when it is impossible for the child to be reunited with their biological family. Children placed in a Casa Viva home have the advantage of growing up in a home that is surrounded by extended family, local church, and the support of the Casa Viva center.\n\nThere are currently two Casa Viva Communities in Costa Rica - one in Eastern San Jose and a second in Grecia. In partnership with the Viva Network and Toybox it has identified Bolivia, Peru, El Salvador, Nicaragua, Mexico and Paraguay as the next sites of multiplication.\n\nOrphanages, whether private or government run, are institutions that are expensive to run and the good ones often become inundated with children sent there by the authorities. This reduces the quality of care and leaves the institution and its staff overworked and under equipped. While the physical needs of the child are met, often psychological needs of a child, the privacy of a child, and integration into their home culture are found to be lacking. This led Casa Viva to begin creating a program, similar to England and the United States' foster care model, but with two primary differences:\nIn some cases, the financial benefit of already-established government-based fostering has become a primary motivator for families to be host foster families. Casa Viva does help families cover the cost of caring for the child but relies heavily on Biblical motivation and mandates to care for the orphan in need.\n\nIt has been shown that children raised in orphanages do not develop as well physically, cognitively or emotionally as children raised in family based settings.\n\n"}
{"id": "19679083", "url": "https://en.wikipedia.org/wiki?curid=19679083", "title": "Centre d'études et de documentation économiques, juridiques et sociales", "text": "Centre d'études et de documentation économiques, juridiques et sociales\n\nThe CEDEJ (Centre d'études et de documentation économiques, juridiques et sociales Eng.:Centre for Economic, Judicial, and Social Study and Documentation) is a French sponsored research center located in Cairo (Egypt), created in 1968. The Cedej has the status of a \"Joint Entity of French Research Institutes Abroad\" (UMIFRE, Unité Mixte des Instituts français de recherche à l’étranger) and is under the aegis of the French Ministry of Foreign Affairs and the CNRS (National Centre for Scientific Research). Karine Bennafla is its director since September 2015.\n\nIt has published numerous books and periodicals in all fields of social sciences in Egypt, the Sudan and the Arabic world. Among these, \"Egypte/Monde arabe\", published from 1990, which is online in full text on the portal revues.org.\n\nIt offers a social sciences library (more than 35000 books, most of them in Arabic), a huge database of geolocated statistics, a complete collection of old and new maps of Egypt and its cities.\n\n"}
{"id": "37712", "url": "https://en.wikipedia.org/wiki?curid=37712", "title": "Chemical patent", "text": "Chemical patent\n\nA chemical patent, pharmaceutical patent or drug patent is a patent for an invention in the chemical or pharmaceuticals industry. Strictly speaking, in most jurisdictions, there are essentially no differences between the legal requirements to obtain a patent for an invention in the chemical or pharmaceutical fields, in comparison to obtaining a patent in the other fields, such as in the mechanical field. A chemical patent or a pharmaceutical patent is therefore \"not\" a \"sui generis\" right, i.e. a special legal type of patent.\n\nIn the pharmaceutical industry, the patent protection of drugs and medicines is accorded a particular importance, because drugs and medicines can easily be copied or imitated (by analyzing a pharmaceutical substance) and because of the significant research and development spending and the high risks associated with the development of a new drug.\n\nChemical patents are different from other sources of technical information because of the generic, Markush structures contained within them, named after the inventor Eugene Markush who won a claim in the US in 1925 to allow such structures to be used in patent claims. These generic structures are used to make the patent claim as broad as possible.\n\nIn the United States, patents on pharmaceuticals were considered unethical by the medical profession during most of the nineteenth-century.\n\n\n\n"}
{"id": "24412707", "url": "https://en.wikipedia.org/wiki?curid=24412707", "title": "Chopart's fracture–dislocation", "text": "Chopart's fracture–dislocation\n\nChopart's fracture–dislocation is a dislocation of the mid-tarsal (talonavicular and calcaneocuboid) joints of the foot, often with associated fractures of the calcaneus, cuboid and navicular.\n\nChopart's fracture–dislocation is usually caused by falls from height, traffic collisions and twisting injuries to the foot as seen in basketball players.\n\n\nDiagnosis is made on plain radiograph of the foot, although the extent of injury is often underestimated.\n\nTreatment comprises early reduction of the dislocation, and frequently involves open reduction internal fixation to restore and stabilise the talonavicular joint. Open reduction and fusion of the calcaneocuboid joint is occasionally required.\n\nWith prompt treatment, particularly open reduction, and early mobilisation the outcome is generally good. High energy injuries and associated fractures worsen the outcome.\n\n"}
{"id": "21688106", "url": "https://en.wikipedia.org/wiki?curid=21688106", "title": "Clone manager", "text": "Clone manager\n\nClone Manager is a commercial bioinformatics software work suite of Sci-Ed, that supports molecular biologists with data management and allows them to perform certain \"in silico\" preanalysis.\n\n\n"}
{"id": "5460997", "url": "https://en.wikipedia.org/wiki?curid=5460997", "title": "Clyssus", "text": "Clyssus\n\nIn the pre-modern chemistry of Paracelsus, a clyssus, or clissus, was one of the effects, or productions of that art; consisting of the most efficacious principles of any body, extracted, purified, and then remixed.\n\nOr, a clyssus is when the several constituents of a body are prepared and purified separately, and then combined again. Thus, the five principles, reassembled into one body, by long digestion, make a clyssus. So, \"clyssus of antimony\" is produced by distillation from antimony, nitre, and sulfur mixed together. There is also \"clyssus of vitriol\", which is a spirit drawn by distillation from vitriol dissolved in vinegar; this was used by pre-modern physicians in treating various diseases, and to extract the tinctures of several vegetables.\n\nClyssus is used among some authors for a kind of \"sapa\", or extract, made with eight parts of the juice of a plant, and one of sugar, seethed together into the consistency of honey.\n"}
{"id": "2547545", "url": "https://en.wikipedia.org/wiki?curid=2547545", "title": "DNA bank", "text": "DNA bank\n\nDNA banking is the secure, long term storage of an individual’s genetic material. DNA is most commonly extracted from blood, but can also be obtained from saliva and other tissues. DNA banks allow for conservation of genetic material and comparative analysis of an individual's genetic information. Analyzing an individual's DNA can allow scientists to predict genetic disorders, as used in preventative genetics or gene therapy, and prove that person's identity, as used in the criminal justice system. There are multiple methods for testing and analyzing genetic information including restriction fragment length polymorphism (RFLP) and polymerase chain reactions (PCR).\n\nDNA banking is used to conserve genetic material, especially that of organisms that face extinction. This is a more prominent issue today due to deforestation and climate change, which serve as a threat to biodiversity. The genetic information can be stored within lambda phage and plasma vectors. The National Institute of Agrobiological Sciences (NIAS) DNA Bank, for example, collects the DNA of agricultural organisms, such as rice and fish, for scientific research. Most DNA provided by DNA banks is used for studies to attempt to develop more productive or more environmentally friendly agricultural species. Some DNA banks also store the DNA of rare or endangered species to ensure their survival.\n\nThe DNA bank can be used to compare and analyze DNA samples. Comparison of DNA samples allowed scientists to work on the Human Genome Project, which maps out many of the genes on human DNA. It has also led to the development of preventative genetics. Samples from the DNA bank have been used to identify patterns and determine which genes lead to specific disorders. Once people know which genes lead to disorders, people can take steps to lessen the effects of that disorder. This can occur through adjustments in lifestyle, as demonstrated in preventive healthcare, or even through gene therapy. DNA can be banked at any time during a person's life.\n\nDNA banks were introduced to the criminal justice system in the 1980s. This system makes it possible to rule out or confirm the verdict of a suspect based on their personal genetic code. Once an individual’s DNA is stored, it remains in the system permanently; allowing law enforcement to identify and track criminals more easily. There is some controversy about this topic as some individuals believe the storage of citizen's DNA is an invasion of privacy.\n\nDNA banking capsules are also starting to be used for retaining the DNA of the deceased, a service offered by some funeral homes.\n\nScientists are capable of retrieving genetic information from hair, skin, blood, sperm, tissue, and saliva as long as the sample contains intact DNA. Nucleotide sequences between humans differ by only 0.1%. Even so, this 0.1% includes approximately three million bases. DNA can be analyzed through restriction fragment length polymorphism (RFLP) and Polymerase chain reactions (PCR). The RFLP process was introduced in 1988. Restriction enzymes digest portions of the DNA, leaving short fragments. These fragments are sorted through gel electrophoresis. The gel demonstrates the length of the fragments allowing specialists to determine whether the fragments came from the same person. PCR is more commonly used today because it more efficient and requires smaller samples of genetic samples. \n\nThere are various organizations founded for the purpose of storing and analyzing DNA sample. For example, the UK Biobank contains DNA samples of 500,000 individuals aged between 40 and 69 when their samples were taken in the years 2006-2010 .\n\n\n"}
{"id": "27000354", "url": "https://en.wikipedia.org/wiki?curid=27000354", "title": "Daneshmand (magazine)", "text": "Daneshmand (magazine)\n\nDaneshmand (meaning \"Scientist\" in English) is a monthly general science magazine covering recent developments in science and technology for a general Persian-speaking audience. The magazine is published in Tehran, Iran.\n\nFounded in 1963, \"Daneshmand\" was published by Novin Daneshmand organization, a subsidiary of Reed Elsevier. Ali Akbar Ghazvini served as the editor-in-chief of the magazine. Following the Islamic revolution in Iran in 1979 the magazine was closed. In 1984 it was restarted under the ownership of the Mostazafen Foundation of Islamic Revolution. \n\n\"Daneshmand\" is published on a monthly basis. It has its headquarters in Tehran. As well as covering current events and news from the scientific community, the magazine often features speculative articles, ranging from the technical to the philosophical. It is not a peer-reviewed scientific journal, but it is read by both scientists and non-scientists, as a way of keeping track of developments outside their own fields of study or areas of interest.\n\n\nOfficial website\n"}
{"id": "3474697", "url": "https://en.wikipedia.org/wiki?curid=3474697", "title": "David Quinn (bird artist)", "text": "David Quinn (bird artist)\n\nDavid Quinn (born 1959) is a British bird artist. He won the 1987 Bird Illustrator of the Year Award of the \"British Birds\" magazine. His illustrations have appeared in several works, including the \"New World Warblers\" and \"Tits, Nuthatchs & Treecreepers\" volumes of the Helm Identification Guides series, as well as accompanying identification papers in \"British Birds\" magazine.\n\nDavid Quinn is based in Cheshire, where he moved to in 1983.\n\nHe attended Salford Grammar School, then Manchester Polytechnic, where he graduated with a BA with first class honours in graphic design in 1982.\n\nAmongst other publications in which his illustrations are featured is the third edition of the \"National Geographic Field Guide to the Birds of North America\", as well as the \"Helm Identification Guide to New World Warblers\". He has also worked with Rob Hume on a guide to European gulls, yet to be published.\n\nQuinn has found a number of rare vagrant birds in Cheshire, including a Franklin's gull at Neumann's Flash in 1987 and a first-winter American herring gull in 1994, the first accepted record for mainland Britain.\n"}
{"id": "4147298", "url": "https://en.wikipedia.org/wiki?curid=4147298", "title": "Digger gold", "text": "Digger gold\n\nDigger gold is the common slang term for gold recovered from electronics components such as board fingers, CPUs, and connector pins.\n\nFor the gold fingers on boards or circuits, often a stripping solution is used to remove the gold from the board material; nitric acid also works well in this regard as many gold components are soldered to boards with silver-based solders that are soluble in nitric acid (which gold is not). After dissolving all other metals in solution, the digger gold is recovered by dissolution of the gold in aqua regia and subsequent selective precipitation of the gold using ferrous sulfate or another selective reducing agent such as hydrazine.\n\n"}
{"id": "4001513", "url": "https://en.wikipedia.org/wiki?curid=4001513", "title": "Electrometallurgy", "text": "Electrometallurgy\n\nElectrometallurgy is the field concerned with the processes of metal electrodeposition There are four categories of these processes:\n\n\n"}
{"id": "11527579", "url": "https://en.wikipedia.org/wiki?curid=11527579", "title": "Expert elicitation", "text": "Expert elicitation\n\nIn science, engineering, and research, expert elicitation is the synthesis of opinions of authorities of a subject where there is uncertainty due to insufficient data or when such data is unattainable because of physical constraints or lack of resources. Expert elicitation is essentially a scientific consensus methodology. It is often used in the study of rare events. Expert elicitation allows for parametrization, an \"educated guess\", for the respective topic under study. Expert elicitation generally quantifies uncertainty.\n\nExpert elicitation tends to be multidisciplinary as well as interdisciplinary, with practically universal applicability, and is used in a broad range of fields. Prominent recent expert elicitation applications include climate change, modeling seismic hazard and damage, association of tornado damage to wind speed in developing the Enhanced Fujita scale, risk analysis for nuclear waste storage.\n\nIn performing expert elicitation certain factors need to be taken into consideration. The topic must be one for which there are people who have predictive expertise. Furthermore, the objective should be to obtain an experts' carefully considered judgment based on a systematic consideration of all relevant evidence. For this reason one should take care to adopt strategies designed to help the expert being interviewed to avoid overlooking relevant evidence. Additionally, vocabulary used should face intense scrutiny; qualitative uncertainty words such as \"likely\" and \"unlikely\" are not sufficient and can lead to confusion. Such words can mean very different things to different people, or to the same people in different situations.\n\n\n"}
{"id": "2806736", "url": "https://en.wikipedia.org/wiki?curid=2806736", "title": "Extractive Industries Transparency Initiative", "text": "Extractive Industries Transparency Initiative\n\nThe Extractive Industries Transparency Initiative (EITI) is a global standard for the good governance of oil, gas and mineral resources. It seeks to address the key governance issues in the extractive sectors.\n\nThe EITI Standard requires information along the extractive industry value chain from the point of extraction, to how the revenue makes its way through the government, to how it contributes to the economy.\n\nThis includes how licenses and contracts are allocated and registered, who are the beneficial owners of those operations are, what are the fiscal and legal arrangements are, how much is produced, how much is paid, where are those revenues allocated, and what is the contribution to the economy, including employment.\n\nThe EITI Standard is implemented in 52 countries around the world. Each of these countries is required publish to an annual EITI Report to disclose information on: contracts and licenses, production, revenue collection, revenue allocation, and social and economic spending.\n\nEvery country goes through a quality-assurance mechanism, called Validation, at least every three years. Validation serves to assess performance towards meeting the EITI Standard and promote dialogue and learning at the country level. It also safeguards the integrity of the EITI by holding all EITI implementing countries to the same global standard.\n\nEach implementing country has its own national secretariat and multi-stakeholder group, made up of representatives from the country’s government, extractive companies and civil society. The multi-stakeholder group takes decisions on how the EITI process is carried out in the country.\n\nThe EITI Standard is developed and overseen by an international multi-stakeholder Board, consisting of representatives from governments, extractives companies, civil society organisations, financial institutions and international organisations.\n\nThe Chair of the EITI is Fredrik Reinfeldt, former Prime Minister of Sweden. The previous chairs have been the Clare Short (2011-2016), former UK Secretary of State for International Development and Peter Eigen (2009-2011). The EITI International Secretariat is located in Oslo, Norway and is headed by former Swedish diplomat Jonas Moberg.\n\nThe “Extractive Industries Transparency Initiative” (EITI) was first launched in September 2002 by UK Prime Minister Tony Blair at the World Summit on Sustainable Development in Johannesburg, following years of academic debate, as well as lobbying from civil society and companies, on the management of government revenues from the extractive industries. In particular, the EITI was established to be an answer to public discussions on the “Resource Curse” or the “Paradox of Plenty”. NGOs such as by Global Witness and “Publish What You Pay”, as well as companies such as BP pushed the UK government to working towards an international transparency norm.\n\nThe organisation was founded at a conference in London in 2003. The 140 delegates from government, companies and civil society agreed on twelve principles to increase transparency over payments and revenues in the extractive sector. A pilot phase of the EITI was launched in Nigeria, Azerbaijan, Ghana and the Kyrgyz Republic. The management of the Initiative continued to lay with the UK Department for International Development.\n\nThe second EITI Conference on 17 March 2005 in London established six criteria based on the principles. These set out the minimum requirements for transparency in the management of resources in the oil, gas and mining sectors, laying the foundation for a rule-based organisation. This conference also established an international advisory group (IAG) under the Chairmanship of Peter Eigen to further guide the work of how the EITI is to be set up and function. More countries, companies and civil-society organisations joined the initiative. The International Monetary Fund and the World Bank endorsed the EITI.\n\nThe report issued in June 2006 by the international advisory group recommended the establishment of a multi-stakeholder board and an independent secretariat, and these were set in place at the third EITI conference held in Oslo, Norway on 11 October 2006. Oslo was chosen as the new location for the secretariat.\n\nIn the following years the body further fleshed out the criteria, turning them into a set of 23 requirements, known as the EITI Rules in 2011.\n\nThe EITI Standard replaced the EITI Rules on 24 May 2013. The Standard was revised in February 2016.\n\nThe EITI is organised as a non-profit association under Norwegian law. It has three institutional bodies: The Members’ Meeting, the EITI Board, and the International Secretariat. The Members’ Meeting governs the EITI and convenes alongside the EITI global conferences, which are held every two to three years.\n\nThe board develops the Standard and assesses the progress of the countries. It is supported by the international secretariat, located in Oslo, Norway.\n\nThe EITI Board meets between two and four times a year and is composed of three groups: countries, companies and civil society. The membership of the Board reflects the multi-stakeholder nature of the EITI. The EITI Board has eight committees (audit, finance, governance and oversight, implementation, nominations, outreach and candidature, rapid response and Validation) to develop recommendations to the full board.\n\nThe funding of the EITI is two-fold. At the country level, implementation is funded by the governments. At the international level, the EITI is funded by implementing countries, supporting governments and companies.\n\nAny country with extractive industry sectors can adhere to the EITI Standard. Countries implementing the transparency standard include OECD states such as Norway, the United Kingdom and the United States as well as countries in Africa, Central and East Asia, Europe, and Latin America and the Caribbean.\n\nWhen a country intends to join the EITI Standard, it is required to undertake five sign-up steps before applying.\n\nThese steps relate to a clear commitment of the government, company and civil society engagement, the establishment of a multi-stakeholder group and agreement on a work plan, which sets out what the country wants to achieve within a certain time frame.\n\nOnce the application of the country has been accepted by the board, the country is called an “EITI candidate”. The candidate receives the deadlines for publishing information and undergoes “Validation” two and a half years later.\n\nThe result of Validation is a measure of how well the country is progressing to meet the requirements of the standard. It can make satisfactory, meaningful, inadequate or no progress. The EITI board will ask the country to improve aspects which Validation deemed insufficient to fulfil the standard. An overview of Validation results is available online.\n\nWhen a candidate country passes EITI Validation, it is declared “EITI compliant” by the Board.\n\nAs of November 2017, 51 countries are implementing the EITI:\nOther countries, such as Lebanon, France and Australia have shown interest in implementing the EITI.\n\nThe EITI has made significant contributions to improved governance of the extractive sector in several countries around the world. In countries like the Democratic Republic of the Congo, the EITI has been central to many reforms of the sector. At the international level, debates on transparency in the sector are unrecognisable from ten years ago, and the EITI is seen as being at the forefront of many frontier debates including beneficial ownership, commodity trading, and artisanal and small-scale mining.\n\nIt is also clear that the EITI process is one of the only \"functioning\" global mechanisms to inform and channel debate in resource-rich countries in a way that includes all stakeholders. In Peru, EITI Reports have highlighted that, only about 15% of revenues from the mining and hydrocarbon sector has been used for developmental spending, such as infrastructure or economic diversification. The rest has been spent on current expenditures such as salaries and servicing debts. Local citizens are using this information to engage with their regional authorities on alternative ways to spend these resources.\n\nEITI Reports make recommendations aimed at addressing weaknesses in government systems and improving extractive sector management. In Nigeria, President Buhari has initiated major reforms in the oil sector, starting with restructuring the national oil company, a review of oil contracts, an pause in the awarding of the notorious oil swap deals, and a review of subsidy arrangements. These were all recommendations from Nigeria EITI Reports.\n\nFurthermore, the EITI can lead to higher attractiveness for investments. There is “mounting evidence that information release supports greater competition around government contracting and that being an EITI signatory leads to greater inflows of both aid and foreign direct investment”.\n\nAround 80 companies involved in oil, gas, and mining support the EITI. Supporting companies publicly endorse the EITI and contribute to covering the cost of the international secretariat of the EITI.\n\nExtractive companies are involved on the national level in countries implementing the transparency standard. They are part of the stakeholders and are required to hand over numbers on payments as part of the reporting process under the EITI standard. Company advocacy has resulted in several countries beginning EITI implementation.\n\nCampaigning organisations have criticised the organisation for the lack of sanction possibilities. On the other hand, business representatives have commented that the EITI board is captured by civil society organisations. The EITI has been seen as insufficient to bring full transparency to payments in the extractive industries, since it does not cover countries active in commodity trading. This has since been addressed by new requirements of the EITI standard.\n\nThe body's credibility was questioned after it permitted an Ethiopian application for membership in 2014.\n\nEITI has also been criticised for ignoring the violations of human rights in Azerbaijan, and for not reacting sufficiently strongly to the harassment of Azerbaijani civil society groups that are part of EITI's multi-stakeholder approach. On the other hand, the EITI has been criticised by an international lending institution for shifting its mandate beyond the promition of transparency.\n\n"}
{"id": "252137", "url": "https://en.wikipedia.org/wiki?curid=252137", "title": "Food science", "text": "Food science\n\nFood science is the applied science devoted to the study of food. The Institute of Food Technologists defines food science as \"the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public\". The textbook \"Food Science\" defines food science in simpler terms as \"the application of basic sciences and engineering to study the physical, chemical, and biochemical nature of foods and the principles of food processing\".\n\nActivities of food scientists include the development of new food products, design of processes to produce these foods, choice of packaging materials, shelf-life studies, sensory evaluation of products using survey panels or potential consumers, as well as microbiological and chemical testing. Food scientists may study more fundamental phenomena that are directly linked to the production of food products and its properties.\n\nFood science brings together multiple scientific disciplines. It incorporates concepts from fields such as microbiology, chemical engineering, and biochemistry.\n\nSome of the subdisciplines of food science are described below.\n\nFood chemistry is the study of chemical processes and interactions of all biological and non-biological components of foods. The biological substances include such items as meat, poultry, lettuce, beer, and milk as examples.\nIt is similar to biochemistry in its main components such as carbohydrates, lipids, and protein, but it also includes areas such as water, vitamins, minerals, enzymes, food additives, flavors, and colors. This discipline also encompasses how products change under certain food processing techniques and ways either to enhance or to prevent them from happening.\n\nFood physical chemistry is the study of both physical and chemical interactions in foods in terms of physical and chemical principles applied to food systems, as well as the application of physicochemical techniques and instrumentation for the study and analysis of foods.\n\nFood engineering is the industrial processes used to manufacture food.\n\nFood microbiology is the study of the microorganisms that inhabit, create, or contaminate food, including the study of microorganisms causing food spoilage. \"Good\" bacteria, however, such as probiotics, are becoming increasingly important in food science. In addition, microorganisms are essential for the production of foods such as cheese, yogurt, bread, beer, wine and, other fermented foods.\n\nFood preservation involves the causes and prevention of quality spoilage\n\nFood substitution refers to the replacement of fat, sugar, or calories from a product while maintaining similar shape, texture, color, or taste.\n\nFood technology is the technological aspects.\nEarly scientific research into food technology concentrated on food preservation. Nicolas Appert’s development in 1810 of the canning process was a decisive event. The process wasn’t called canning then and Appert did not really know the principle on which his process worked, but canning has had a major impact on food preservation techniques.\n\nMolecular gastronomy is the scientific investigation of processes in cooking, social and artistic gastronomical phenomena.\nMolecular gastronomy is a subdiscipline of food science that seeks to investigate the physical and chemical transformations of ingredients that occur in cooking. Its program includes three axis, as cooking was recognized to have three components, which are social, artistic and technical.\n\nNew product development includes the invention of new food products.\n\nQuality control involves the causes, prevention and communication dealing with food-borne illness. \n\nQuality control also ensures that product meets specs to ensure the customer receives what they expect from the packaging to the physical properties of the product itself.\n\nSensory analysis is the study of how consumers' senses perceive food.\n\nThe Commonwealth Scientific and Industrial Research Organisation (CSIRO) is the federal government agency for scientific research in Australia. CSIRO maintains more than 50 sites across Australia and biological control research stations in France and Mexico. It has nearly 6,500 employees.\n\nThe Korean Society of Food Science and Technology, or KoSFoST, claims to be the first society in South Korea for food science.\n\nIn the United States, food science is typically studied at land-grant universities. Many of the country's pioneering food scientists were women who had attended chemistry programs at land-grant universities (which were state-run and largely under state mandates to allow for sex-blind admission), but then graduated and had difficulty finding jobs due to widespread sexism in the chemistry industry in the late 19th and early 20th centuries. Finding conventional career paths blocked, they found alternative employment as instructors in home economics departments and used that as a base to launch the foundation of many modern food science programs.\n\nThe main US organization regarding food science and food technology is the Institute of Food Technologists (IFT), headquartered in Chicago, Illinois, which is the US member organisation of the International Union of Food Science and Technology (IUFoST).\n\nPopular books on some aspects of food science or kitchen science have been written by Harold McGee and Howard Hillman, among others.\n\n\n"}
{"id": "1302304", "url": "https://en.wikipedia.org/wiki?curid=1302304", "title": "Fred Talbot", "text": "Fred Talbot\n\nFrederick Talbot (born 17 December 1949) is a British former television presenter. He spent much of his career in North West England. In February 2015, and again in May 2017, he was found guilty of a string of indecent sexual assaults against teenage boys whilst he was employed as a teacher at Altrincham Grammar School for Boys before he moved into television presenting. He was sentenced to five years in prison following his initial convictions. He was sentenced to a further four years in June 2017 following conviction for further indecent assaults committed in Scotland. \n\nBorn in Edinburgh, Talbot received his early formal education at North Cestrian Grammar School, and in 1964 was a founding member of the Altrincham and District Astronomical Society, with which he co-discovered a meteor shower, the June Lyrids in June 1966.\n\nAfter teacher training in Gateshead in the late 1960s, he was employed to teach biology at Altrincham Grammar School for Boys until May 1984, when his schooling career came to \"an abrupt end\" following an indecent proposal he made to two pupils at his home.\n\nAt the time of his dismissal, Talbot was also recording a weekly astronomy programme for radio. In 1984, he began presenting for the regional news programme \"Granada Reports\" in North West England. Granada Television later commissioned Talbot to appear in a general educational science programme for children called \"The Final Frontier\". He continued as a regular reporter for \"Granada Reports\" and became a weatherman for the ITV's \"This Morning\" in October 1988.\n\nUntil production of \"This Morning\" was moved from the Albert Dock, Liverpool, to London, Talbot presented weather reports from a large floating map of the British Isles in the dock. He was required to jump across a gap to go back and forth between Ireland and Britain; crowds often gathered to watch his leap. On one occasion, a diver swam near the map to distract Talbot and on another a streaker swam naked up to the map and jumped on.\n\nTalbot also presented several regional feature series for ITV Granada, including \"Locks and Quays\" and \"Wainwright Country\" before returning to weather presenting with \"Granada Reports\" in February 2009. He also contributed to the CITV science series \"Prove It!\" and guest presented weather forecasts for the ITV Breakfast programme \"Daybreak\".\n\nIn December 2012, Greater Manchester Police carried out a search of Talbot's home in Bowdon, when allegations first emerged of abuse at Altrincham Grammar School between the early 1970s and early 1980s. A day before the search, Talbot posted on his Twitter account that he was on a holiday cruise in the Atlantic Ocean. He did not return to his role at ITV Granada upon his return to the UK, but was not suspended.\n\nIn April 2013, police arrested Talbot, who refused to answer interview questions about allegations by men who claimed they had been abused as children. Following a re-arrest in December 2013, he was charged with ten historical sexual offences, followed by an eleventh charge for a further sexual assault in June 2014.\n\nHis trial began in January 2015, accused of abusing four former pupils at Altrincham Grammar School and a fifth schoolboy from the Newcastle area, during a canal holiday in the 1970s. Witnesses included Stone Roses lead singer Ian Brown, a former pupil of Talbot's at Altrincham Grammar School for Boys, who swore under oath that Talbot showed his class gay pornographic films. Evidence was also provided by Talbot's extensive diaries, which noted his sexual activity. On 13 February 2015, a jury at Minshull Street Crown Court in Manchester found Talbot guilty of indecent assault against two teenage boys at the school, and acquitted him of eight further charges. Immediately remanded in custody bearing in mind Talbot's \"abuse of trust\", sentencing was adjourned until 13 March 2015. With reporting restrictions lifted, it was noted that a number of similar complaints against Talbot had since been passed by police to the procurator fiscal about alleged offences said to have been committed in Scotland. During the trial, Talbot said he knew at fourteen that he was homosexual. \n\nAt Minshull Street Crown Court on 13 March 2015, Judge Timothy Mort sentenced Talbot to five years in prison. Talbot was told that he must serve at least two-and-a-half years in prison before he can be considered for release on licence.\n\nTalbot appeared in a Scottish court on 11 March 2016, where he faced ten fresh charges of indecent assault dating from 1968 to 1981 and two charges of breach of the peace. He did not enter a plea during the hearing. In May 2017, Talbot was convicted of seven charges of indecent assault against boys that took place on trips to Scotland while he was a teacher at Altrincham Grammar. On 15 June 2017, he was sentenced to four years' imprisonment at Lanark Sheriff Court, to run from 14 August, after he had served half of his existing sentence for indecent assault. On 29 November 2017 Talbot was jailed for eight months for an act of indecent assault carried out in 1980.\n\nIn 1998, Talbot was named Weatherman of the Year at the Annual International Weather Festival in Paris.\n\nIn 2007, Talbot was awarded the honorary degree of Doctor of Science by Manchester Metropolitan University in recognition of bringing to a mass audience a better understanding of scientific and environmental issues.\n"}
{"id": "4088563", "url": "https://en.wikipedia.org/wiki?curid=4088563", "title": "Friedwardt Winterberg", "text": "Friedwardt Winterberg\n\nFriedwardt Winterberg (born June 12, 1929) is a German-American theoretical physicist and research professor at the University of Nevada, Reno. With more than 260 publications and three books, he is known for his research in areas spanning general relativity, Planck scale physics, nuclear fusion, and plasmas. His work in nuclear rocket propulsion earned him the 1979 Hermann Oberth Gold Medal of the Wernher von Braun International Space Flight Foundation and in 1981 a citation by the Nevada Legislature. He is also an honorary member of the German Aerospace Society Lilienthal-Oberth.\n\nHe is known for his proposal to put accurate atomic clocks on Earth-orbiting satellites in order to directly test General Relativity, his fusion activism, his first proposal to experimentally test Elsasser's theory of the geodynamo, his defense of rocket scientist Arthur Rudolph, and his involvement in the Albert Einstein-David Hilbert priority dispute.\n\nWinterberg was born in 1929 in Berlin, Germany. In 1953 he received his MSc from the University of Frankfurt working under Friedrich Hund, and in 1955 he received his PhD in physics from the Max Planck Institute, Göttingen, as a student of Werner Heisenberg. In 1959, Winterberg was brought to the United States as part of Operation Paperclip. Friedwardt was 15 at the end of the war. Paperclip continued to recruit German scientists through the Cold War to prevent them from working for the Soviets.\nWinterberg is known for his work in the fields of nuclear fusion and plasma physics, and Edward Teller has been quoted as saying that he had \"perhaps not received the attention he deserves\" for his work on fusion. He is an elected member of the Paris-based International Academy of Astronautics, in which he sat on the Committee of Interstellar Space Exploration. According to his faculty webpage, in 1954 he \"made the first proposal to test general relativity with atomic clocks in earth satellites\" and his thermonuclear microexplosion ignition concept was adopted by the British Interplanetary Society for their Project Daedalus Starship Study. His current research is on the \"Planck Aether Hypothesis\", \"a novel theory that explains both quantum mechanics and the theory of relativity as asymptotic low energy approximations, and gives a spectrum of particles greatly resembling the standard model. Einstein's gravitational and Maxwell's electromagnetic equations are unified by the symmetric and antisymmetric wave mode of a vortex sponge, Dirac spinors result from gravitationally interacting bound positive-negative mass vortices, which explains why the mass of an electron is so much smaller than the Planck mass. The phenomenon of charge is for the first time explained to result from the zero point oscillations of Planck mass particles bound in vortex filaments.\" The theory proposes that the only free parameters in the fundamental equations of physics are the Planck length, mass, and time, and shows why R3 is the natural space, as SU2 is treated as the fundamental group isomorphic to SO3 — an alternative to string field theories in R10 and M theory in R11. It permits the value of the finestructure constant at the Planck length to be computed, and this value remarkably agrees with the empirical value. He has published extensively on many aspects of physics from the 1950s through the present. In 2008, Winterberg criticized string theory and pointed out the shortcomings of Einstein's general theory of relativity because of its inability to be reconciled with quantum mechanics at the Physical Interpretations of Relativity Theory conference and published his findings in \"Physics Essays\".<ref name=\"doi:10.4006/1.3176669\"></ref> Winterberg has published a series of articles treating Quantum Gravity on viXra, the most recent in 2018.\n\nIn a 1955 paper Winterberg proposed a test of general relativity using accurate atomic clocks placed in orbit in artificial satellites. At that time atomic clocks were not yet of the required accuracy and artificial satellites did not exist. Werner Heisenberg wrote a letter to Winterberg in 1957 in which he said the idea sounded \"very interesting\". This idea was later experimentally verified by Hafele and Keating in 1971 by flying atomic clocks on commercial jets. The theoretical approach was the same as that used by Winterberg. Today atomic clocks and relativistic corrections are used in GPS and it is said GPS could not function without them.\n\nWinterberg has published numerous articles in the area of inertial confinement fusion. In particular, Winterberg is known for the idea of impact fusion and the concept of the magnetically insulated diode for the generation of multi-megampere megavolt ion beams for the purpose of heating plasmas to thermonuclear fusion temperatures. He conceived of a nuclear fusion propulsion reactor for space travel, which is called the Winterberg / Daedalus Class Magnetic Compression Reaction Chamber, which was later developed at the University of Alabama at Huntsville's Propulsion Research Center. Most recently he has designed a giant spacecraft, propelled with deuterium micro-detonations ignited by a GeV proton beam, drawn from the spacecraft acting as an electrically charged up and magnetically insulated capacitor. Winterberg also developed ideas for mining increasingly rare industrially crucial elements on planetary bodies such as the moon using fusion detonation devices. He became involved with the idea of using beam weapons in outer space in the late 1970s while working at the Desert Research Institute.\n\nAccording to Dennis King, Winterberg shared his ideas on beam weapons with the U.S. Air Force and he speculated on the subject in publications for the Fusion Energy Foundation (FEF), a part of the Lyndon LaRouche movement. The FEF published a book of Winterberg describing the design of the hydrogen bomb, with the hope of getting research in inertial confinement fusion declassified. Winterberg also contributed articles and interviews to the FEF magazine, \"Fusion\", and its successor magazine, \"21st Century Science and Technology\". He also participated in a 1985 conference jointly sponsored by the FEF and the Schiller Institute, speaking on the topic of X-ray lasers, the Strategic Defense Initiative and interstellar travel. The conference attracted a number of scientists interested in promoting fusion scientific research; Winterberg was never a member of any of LaRouche's political organisations.\n\nOn November 12, 2007, Winterberg addressed the American Physical Society Plasma Physics Convention in Orlando, Florida, encouraging efforts to achieve economically feasible fusion energy, and presenting his ideas for what direction the efforts should take. Winterberg stresses inertial confinement fusion.\n\nBack in 1963, it was proposed by Winterberg that the ignition of thermonuclear micro-explosions, could be achieved by an intense beam of microparticles accelerated to a velocity of 1000 km/s. And in 1968, Winterberg proposed to use intense electron and ion beams, generated by Marx generators, for the same purpose. Most recently, Winterberg has proposed the ignition of a deuterium microexplosion, with a gigavolt super-Marx generator, which is a Marx Generator driven by up to 100 ordinary Marx generators.\n\nIn 1983, Winterberg became involved in disputes concerning the engineer Arthur Rudolph, who had been brought to the United States after World War II as part of Operation Paperclip to work on the U.S. rocketry program. It was Rudolph who then designed the Saturn V rocket that launched Neil Armstrong to the Moon. In the early 1980s, Rudolph's record as a potential Nazi war criminal at Mittelwerk surfaced and became the center of a political controversy after the Office of Special Investigations (OSI) negotiated to have him renounce his U.S. citizenship, purportedly under duress, after which he returned to Germany. After a thorough investigation by German authorities, it was decided there was no basis for prosecution and his German citizenship was restored. Rudolph pursued lawsuits hoping to regain his US citizenship and was barred entry to the US in 1989.\n\nWinterberg lobbied in favour of Rudolph, giving interviews to magazines, launching his own separate investigation, and speaking in Rudolph's defense at a conference hosted by Lyndon LaRouche. After the fall of the Berlin wall legal assistant requests by the US Office of Special Investigation to the Communist East German Government regarding the Rudolph case emerged and became part of the public record.\n\nWinterberg was also involved in a dispute relating to the history of general relativity in a controversy over the publication of the general relativity field equations (both Albert Einstein and David Hilbert had published them in a very short time span of one another). In 1997, Leo Corry, Jürgen Renn, and John Stachel published an article in \"Science\" entitled \"Belated decision in the Hilbert-Einstein priority dispute\", arguing that, after looking at the original proofs of the article by Hilbert, that they indicated that Hilbert had not anticipated Einstein's equations.\n\nWinterberg published a refutation of these conclusions in 2004, observing that the galley proofs of Hilbert's articles had been tampered with — part of one page had been cut off. He argued that the removed part of the article contained the equations that Einstein later published and alleged that it was part of a \"crude attempt by some unknown individual to falsify the historical record.\" He alleged that \"Science\" had refused to print the article and thus he was forced to publish it in \"Zeitschrift für Naturforschung\". Winterberg's article argued that despite the missing part of the proofs, that the correct crucial Field Equation is still imbedded on other pages of the proofs, in various forms, including Hilbert's variational principle with correct Lagrangian from which the Field Equation is immediately derived. Winterberg presented his findings at the American Physical Society meeting in Tampa, Florida in April 2005.\n\nCorry, Renn, and Stachel authored a joint reply to Winterberg, which they claimed \"Zeitschrift für Naturforschung\" refused to publish without \"unacceptable\" modifications, and unable to find a publisher elsewhere, they made it available on the internet. The reply accused Winterberg of misrepresenting the reason why \"Science\" would not publish his paper (it had to do with the section of the journal it was scheduled to appear in), and also misrepresenting that the paper published in \"Zeitschrift für Naturforschung\" was the same paper he had submitted to \"Science\", and had in fact been \"substantially altered\" after Winterberg had received their comments on an earlier draft. Actually, Winterberg in his Final Comment had clearly stated that the paper submitted to Science had been a \"previous version\". They then argue that Winterberg's interpretation of the Hilbert paper was incorrect, that the lost part of the page was unlikely to have been consequential, and that much of Winterberg's reasoning about what could be in the missing piece was incorrect (down to noting Winterberg claims that 1/3 of the page was removed, when actually over half a page is missing total from the two cut off pages) and internally inconsistent. They further argued there was a likely \"non-paranoid\" explanation for the missing part of the page.\n\nBut as it was pointed out by Todorov and by Logunov, Mestvirishvili, and Petrov, even in his mutilated form, the page proofs of Hilbert contain the correct Lagrange density of the gravitational field, which in conjunction with Hilbert action results in the correct gravitational field equations. To derive the gravitational field equations from his action was not more than a minor exercise for Hilbert. Therefore, the game was over with Hilbert's action.\n\nLater, the original reply to Winterberg was removed from their website and replaced with a much shorter statement saying only that Winterberg's conclusions were incorrect, specifically that he had focused on the missing page fragment, \"a fact without any bearing on the matter at hand\", while failing \"to address the substantive difference between the theory expounded in the proofs\" of Hilbert. The statement further said that Winterberg had apparently indicated that he was \"personally offended\" by the original response, the \"Max Planck Institute for the History of Science has decided to replace the original, more detailed response to his paper with this abbreviated version\". This was, apparently, because the original reply had contained two very derisive statements against Professor Winterberg; later, the Max Planck Society released a note distancing itself from those two statements, without commenting on the underlying scientific dispute.\n\nMore recently, Winterberg has written an article stating that Einstein's general theory of relativity cannot be reconciled with quantum theory in Einstein's attempt to reduce all of physics to geometry.\n\n"}
{"id": "958509", "url": "https://en.wikipedia.org/wiki?curid=958509", "title": "Jearl Walker", "text": "Jearl Walker\n\nJearl Walker (born 1945 in Pensacola, Florida) is a physicist noted for his book \"Flying Circus of Physics\", first published in 1975; the second edition was published in June 2006. He teaches physics at Cleveland State University.\n\nWalker has also revised and edited the textbook \"Fundamentals of Physics\" with David Halliday and Robert Resnick.\n\nWalker is a well known popularizer of physics, and appeared \"The Tonight Show Starring Johnny Carson\". Walker is known for his physics demonstrations, which have included sticking his hand in molten lead, walking barefoot over hot coals, lying on a bed of nails, and pouring freezing-cold liquid nitrogen in his mouth to demonstrate various principles of physics. Such demonstrations are included in his PBS series, \"Kinetic Karnival\", produced by WVIZ in Cleveland, Ohio.\n\nWalker authored The Amateur Scientist column in \"Scientific American\" magazine from 1978 to 1988. During the latter part of this period, he had been the Chairman of the Physics Department at Cleveland State University. He appeared regularly around this time on the long-running CBC radio science program \"Quirks and Quarks\".\n\nFrom 1981 to 1982 he hosted The Kinetic Karnival of Jearl Walker, a six-episode series for PBS syndication in the US. In each 30-minute program he performed humorous demonstrations before a live audience. The show was distributed to schools as a teaching aide.\n\nHe is the first recipient, in 2005, of the Outstanding Teaching Award from Cleveland State's College of Science. The College's Faculty Affairs Committee selected Walker as the first honoree based on his contributions to science education over the last 30 years. The award was thereafter named \"The Jearl Walker Outstanding Teaching Award\" in his honor. \n\nWalker was born in Pensacola, Florida and grew up in Fort Worth, Texas. He graduated with a degree in physics from the Massachusetts Institute of Technology in 1967. He received his Ph.D. from the University of Maryland in 1973.\n\n"}
{"id": "17367163", "url": "https://en.wikipedia.org/wiki?curid=17367163", "title": "Josef Herzig", "text": "Josef Herzig\n\nJosef Herzig (25 September 1853 – 4 July 1924) was an Austrian chemist. \n\nHerzig was born in Sanok, Galicia, which at that time was part of Austria-Hungary. Herzig went to school in Breslau until 1874, started studying chemistry at the University of Vienna but joined August Wilhelm von Hofmann at the University of Berlin in the second semester. He worked with Robert Bunsen at the University of Heidelberg and received his PhD for work with Ludwig Barth at the University of Vienna. He later became lecturer and, in 1897, professor at the University of Vienna. He died in Vienna in 1924.\n\nHerzig was active in the chemistry of natural products. He succeeded in determining the structure of flavonoids quercetin, fisetin and rhamnetin as well as several alkaloids.\n\n"}
{"id": "14543376", "url": "https://en.wikipedia.org/wiki?curid=14543376", "title": "Joseph Stenhouse", "text": "Joseph Stenhouse\n\nCommander Joseph Russell Stenhouse, DSO, OBE, DSC, RD, RNR (1887–1941) was a Scottish-born seaman, Royal Navy Officer and Antarctic navigator, who commanded the expedition vessel SY \"Aurora\" during her 283-day drift in the ice while on service with the Ross Sea Party component of Sir Ernest Shackleton's Imperial Trans-Antarctic Expedition 1914-17. After \"Aurora\"s escape from the ice he brought her safely to New Zealand, but was thereafter replaced as the vessel's commander. He later served with distinction in the Royal Navy during both World Wars.\n\nHe was born in Dumbarton, Scotland, into a prosperous shipbuilding family, and was educated in England at Barrow Grammar School. After a short spell as a junior clerk with Lloyd's Register of Shipping, he served a Merchant Officer's apprenticeship on tall ships rounding Cape Horn. He then joined the British India Steam Navigation Company before receiving a last-minute appointment as First Officer on the \"Aurora\", which was then in Australia awaiting refit. Receiving a commission as a sub-lieutenant in the Royal Naval Reserve on 1 August 1914, Stenhouse sailed for Australia aboard SS \"Ionic\" on 18 September.\n\n\"Aurora\", commanded by Captain Aeneas Mackintosh, left Hobart for Antarctica on 24 December 1914 and arrived in McMurdo Sound, with the Ross Sea Party, on 14 January 1915. The party's main mission was to lay depots across the Ross Ice Shelf for Shackleton's expected transcontinental party. When Mackintosh left to take charge of depot-laying operations, Stenhouse took over command of the ship, with the task of finding a suitable winter anchorage. He had two problems here; first, he was inexperienced in these waters; secondly he was handicapped by Shackleton's prior instruction to Mackintosh that the ship be anchored somewhere north of the Glacier Tongue, to reduce the risks of its being trapped in the frozen seas around Hut Point - the fate of Captain Scott's RRS \"Discovery\" in 1902-04. Since \"Discovery\", no ship had attempted to winter in the Sound - \"Nimrod\" and \"Terra Nova\" had returned to New Zealand - and the number of sheltered anchorages north of the tongue was very limited. Stenhouse manoeuvred the ship for many weeks before deciding to anchor at Cape Evans, site of Scott's Last Expedition headquarters, 1910-13. On 14 March the ship was made fast and its engines were subsequently decommissioned for winter maintenance.\n\nDespite great care being taken over the anchorage the winter storms around Cape Evans proved too much, and on the night of 6 May she was wrenched from her moorings and taken out to sea with the ice. Aboard were 18 men, and most of the shore party's clothing, equipment and food. Ashore, stranded, were ten men, including Mackintosh.\n\nThe situation which immediately confronted the inexperienced temporary commander was particularly daunting. The ship, attached to a large ice-floe, was blown out of the Sound and into the Ross Sea with no means of control, unable to raise steam, and with weather conditions likely to worsen. They were wholly isolated, despite the repeated efforts of wireless operator Lionel Hooke to make radio contact with Cape Evans and other, more distant stations. During the succeeding perilous weeks, as the ice-bound \"Aurora\" drifted northwards, roughly parallel to the coast in the direction of Cape Adare, Stenhouse twice came close towards ordering abandonment of the ship and risking a dangerous sledging journey on the ice. The ship survived, however, and continued its drift into the Southern Ocean. Throughout the drift, Stenhouse endeavoured to keep up his crew's morale, and for scientific purposes maintained regular observations of the behaviour of the ice and direction of drift. By February 1916, without sign of release, Stenhouse contemplated the possibility of another year in the ice, but on 12th of that month the ice around her suddenly broke away and she was free. Stenhouse ordered the engines started, and cautiously worked the ship out of the loose pack into the open sea. After a 1000-mile voyage through rough seas to New Zealand, and with assistance in the final stages from a tug, he brought \"Aurora\" into Port Chalmers on 3 April 1916.\n\nHis urgent priority after arrival was to get \"Aurora\" repaired and to take her back to McMurdo Sound to rescue the stranded men who had by then been marooned for almost a year. He did not find his concerns recognised by immediate action on the part of the authorities. There were questions of cash - Shackleton's expedition funds were exhausted, and the costs of refitting and provisioning a relief expedition were estimated at around £20,000. Now Shackleton's apparently cavalier approach to the original organisation of the Ross Sea Party stood against him. Eventually, the combined governments of New Zealand, Australia and Great Britain agreed jointly to meet the costs of the rescue provided that they exercised full control over the mission. Stenhouse still considered that he was the \"Aurora\"'s commander and assumed that he would lead the relief expedition when the ship was ready to sail, but the representatives of the governments decided that he was too inexperienced. He was also Shackleton's man, and they were adamant that neither Shackleton, who had reappeared in the Falkland Islands after his own extended adventure and escape, nor his proxy, should lead the relief, and appointed their own choice, Captain John King Davis. The politics of the situation were largely kept from Stenhouse, who was shocked to learn of King's appointment on 4 October, and initially refused to recognise its validity. However, when Shackleton, who had arrived in New Zealand on 12 December, grudgingly concurred with King's appointment, Stenhouse had no choice but to step down. He then returned to England.\n\nOn his return to England Stenhouse reported for duty with the Royal Navy and was posted as Gunnery Officer to mystery Q-ship PQ61. On 26 September 1917 the ship engaged and sank a U-boat in the Irish sea, an action which earned Stenhouse a Distinguished Service Cross (DSC) on 17 November. After promotion to lieutenant and a spell in command of the schooner HMS Ianthe, he joined Shackleton on a mission to Murmansk, to equip and train the North Russian (anti-Bolshevik) army. This included, from May 1919, command of a flotilla of motor boats operating on Lake Onega to counter the threat of Bolshevik vessels.. For this service he was awarded the Distinguished Service Order (DSO). He was made an Officer of the Order of the British Empire (OBE) in 1920.\n\nIn 1923 Stenhouse married Gladys Mackintosh, Aeneas Mackintosh's widow, and in 1924 a daughter was born. He was promoted to lieutenant-commander in the RNR on 1 August 1924. Between 1927 and 1929 he captained the \"Discovery\" on oceanographic and whaling research voyages in Southern Atlantic and Antarctic waters. Thereafter he attempted several business ventures which largely failed, as did an attempt to find treasure in the Cocos Islands. In April 1928, Stenhouse was awarded the Decoration for Officers of the Royal Naval Reserve (RD). He retired from the RNR on 31 December 1931 with the rank of commander. \n\nAt the outbreak of World War II, however, Stenhouse signed on for active service. Whilst in the Gulf of Aden in 1940, he risked his own life to save that of a crew member after his ship was struck by a mine. On 12 September 1941 he was reported missing, presumed killed, when his ship exploded and sank in the Red Sea.\n\nJoseph Stenhouse is commemorated by Stenhouse Bluff in the South Shetland Islands at \n\n"}
{"id": "2109602", "url": "https://en.wikipedia.org/wiki?curid=2109602", "title": "József Szabó de Szentmiklós", "text": "József Szabó de Szentmiklós\n\nJózsef Szabó de Szentmiklós (March 14, 1822 – April 12, 1894), Hungarian geologist, was born at Kalocsa.\n\nHis first contribution to science was an essay on metallurgy, in which subject he had received special training. Afterwards he settled at Budapest and investigated the geology of the district, the results of which were published in a geological map (1858). In 1859 he joined the staff of the Austrian Geological Survey as a volunteer member, and paid attention to the economic as well as to the purely scientific aspects of the work. He also arranged for surveys having special reference to agricultural geology to be undertaken by the Hungarian Geological Institute. In 1862 he became professor of geology and mineralogy in the University of Budapest.\n\nIn later years he devoted himself largely to petrology, and published memoirs on the trachytes of Hungary and Transylvania; on a new method of determining the species of feldspars in rocks, depending on fusibility and flame-coloration; on the geology and petrology of the district of Schemnitz; and on Santorin Island. He died at Budapest on April 12, 1894.\n\nHe was author of \"Geologie mit besonderer Rüchsicht auf die Petrographie, den Vulkanismus u. die Hydrographie\" (1883).\n"}
{"id": "54305107", "url": "https://en.wikipedia.org/wiki?curid=54305107", "title": "Kate Devlin", "text": "Kate Devlin\n\nKate Devlin, born Adela Katharine Devlin is a British computer scientist specialising in Artificial intelligence and Human–computer interaction (HCI). She is best known for her work on human sexuality and robotics and was co-chair of the annual Love and Sex With Robots convention in 2016 held in London and was founder of the UK's first ever sex tech hackathon held in 2016 at Goldsmiths, University of London.\nShe is a senior lecturer in the department of computing at Goldsmiths, part of the University of London and is the author of \"Turned On: Science, Sex and Robots\" in addition to several academic papers.\n\nDevlin began her university career in the humanities and graduated from Queen's University Belfast in 1997 with a BA (Honours) degree in archaeology. After deciding that archaeology presented her with limited future prospects, she returned to Queen's University to study computer science, and in 1999 she was awarded an MSc in that subject. She then moved to The University of Bristol, where in 2004 she was awarded a PhD in computer science.\n\nIn 2003 Devlin began researching computer graphics in archaeology at Bristol University, rendering 3D computer models of archaeological sites such as at Pompeii with attention to realistically rendering lighting effects caused by the spectral composition of light sources available at the time period in history. This involved experimental archaeology, recreating light sources and analysing the spectral range for each type of candle or fuel lamp.\n\nSince 2007 Devlin has worked in the field of human-computer interaction and artificial intelligence at Goldsmiths, and is a senior lecturer in several areas of computer science, including programming, graphics and animation.\n\nIn 2015 Devlin spoke to news broadcasters in the UK about institutionalised sexism within science research and academia after comments made by Sir Tim Hunt regarding women scientists working in mixed laboratories. While Devlin, along with many other commentators, acknowledged the comments to be 'banter' she expressed the frustration that many women have with sexism in the fields of science, technology, engineering, and mathematics and jokingly tweeted that she couldn't chair a departmental meeting because she was \"too busy swooning and crying.\" Devlin also speaks publicly and writes to encourage more women to pursue technology careers.\n\nIn 2016 Devlin co-chaired the \"International Congress on Love and Sex With Robots\" held in London, UK, an annual conference held since 2014, co-founded by Adrian David Cheok and David Levy, writer of the book of the same name, Love and Sex with Robots.\n\nAlso, in 2016, Devlin founded the first UK sex technology (sex tech) hackathon, a conference where scientists, students, academics and other people in the sex tech industry meet to pool ideas and build projects in the field of sex and intimacy with artificial partners.\n\nIn 2016 Devlin appeared several times in the media debating ethical issues concerning sex robots with Kathleen Richardson, fellow of the ethics of robotics at De Montfort University, and founder of Campaign Against Sex Robots which seeks to ban sex robots on the grounds that they encourage isolation, perpetuate the idea of women as property and are dehumanising. Devlin has argued that not only would a ban be impractical, but as technology develops more women need to be involved to diversify a field which is dominated by men creating products for heterosexual men. She also points out that the technology can be used as therapy, citing the use of artificial intelligence to treat anxiety, and the possible application towards understanding the psychology of sex offenders.\n\nDevlin frequently speaks at conferences and her areas of scientific interest include: the social and ethical problems of integrating artificial intelligence into sexual experience with computer systems and robots, the human and social consequences of AI as it becomes more sophisticated, and improving human sexual relationships by moving away from a \"hetero-normative male view\" of sex and intimacy using sex toys, robots and computer software. She has raised issues which she believes need addressing as this technology develops. These concerns include: if robots gain self-awareness, will they be able to give informed consent and be entitled to make choices regarding their own desires, and should they be supplied to the elderly in residential care facilities for companionship and sex.\n\nDevlin was named one of London's most influential people 2017 by the Progress 1000, London Evening Standard.\n\nIn 2018 Devlin released her book, \"Turned On: Science, Sex and Robots\". The book began as research into the technological development of sex robots and explores the relationship between technology and intimacy.\n\n\n\nDevlin has written for the \"New Scientist\", \"The Conversation\" and has presented a TEDx talk entitled \"Sex Robots\".\n\nDevlin has spoken publicly about living with bipolar disorder and epilepsy and how stress can affect both her academic and professional life, as well as how important it is to bring mental health issues into public debate to reduce the stigma attached.\n\nDevlin is open about her consensually non-monogamous relationships and has written about her experiences of polyamory.\n\nShe is also interested in, and has researched, the life story of Adela Breton, the Victorian archaeologist and explorer, and contributed to the \"Raising Horizons\" exhibition of 'trowel-blazing' women throughout the history of archaeology and geology.\n\nShe is divorced and has a daughter.\n\n\n"}
{"id": "33694761", "url": "https://en.wikipedia.org/wiki?curid=33694761", "title": "La Casa de la Ciencia de Sevilla - Science Museum", "text": "La Casa de la Ciencia de Sevilla - Science Museum\n\nLa Casa de la Ciencia (\"the House of Science\") in the city of Seville, Spain is a centre for popularizing science.\n\nAlong with a museum, the building houses the Andalusian headquarters of the Spanish National Research Council (CSIC). CSIC is the largest public institution devoted to research in Spain, and the third largest in Europe. The building was opened to the public in 2008, with the goal of sharing knowledge acquired through scientific research. La Casa de la Ciencia aims to be a bridge between the scientific research community and the public, sharing contemporary scientific research and information on environmental issues.\nThe museum achieves these goals by putting on various types of events, including exhibitions, conferences, seminars and workshops.\nThe museum contains three permanent exhibits, one temporary exhibit and a planetarium.\n\nThe museum is housed in the Peruvian pavilion (\"Pabellón de Perú\"), a building of great beauty that was built in the Maria Luisa Park for the Ibero-American Exposition of 1929. In addition to being home to the offices of the Consejo Superior de Investigaciones Científicas en Sevilla (CSIC), the building also house the Peruvian Consulate. \nFor twenty years the building was the headquarters of the Biological Station of Doñana and in April 2008 the city council of Seville provided a grant to renovate the building to create La Casa de la Ciencia. In April 2008 the city council of Seville provided a grant to renovate the building to create La Casa de la Cienca.\n\nThe building was designed by the renowned architect Manuel Piqueras Cotolí (1885 – 1937). Cotoli was born in Lucena, Córdoba but moved to Peru to study and became known for his buildings which blended indigenous and colonial architecture. These buildings include the Escuela Nacional Superior Autónoma de Bellas Artes, Pizarro´s Tomb in the Cathedral of Lima, and the Archbishop's Palace. The blending of architectural styles is easily visible from the outside of La Casa de la Ciencia, and many parts of the building share similarities with Cotoli’s Peruvian works.\n\nA part of the building holds the Consulate General of Peru in Seville.\nIn July 2008 the Peruvian ambassador to Spain signed a 75-year renewal of the assignment by the City of Seville of the Pavilion of Peru to the Republic of Peru and to the CSIC. The CSIC had undertaken to provide a exhibition space open to the public dedicated to the extension of science in Andalusia.\n\nThe building lies on the Avenida Maria Luisa, noted for the Queen's sewing box (\"Costurero de la Reina\"), a unique building that takes the form of a small hexagonal castle with turrets at the corners, and the oldest building in Seville in the \"neomudéjar\" style.\nIt is between the Seville Public Library, inaugurated in 1999 by the Infanta Elena, Duchess of Lugo, and the \"Teatro Lope de Vega Sevilla\", a small baroque-style theatre that was also built for the exhibition.\nOn one side of the building is the Guadalquivir river and on the other, Parque Maria Luisa and Plaza de España (Seville).\n\nThe museum currently has three permanent exhibits open to the public, which showcase the flora, fauna and minerals of the region: \"Invertebrates of Andalusia\", \"Geology of Seville\", and \"A Sea of Cetaceans in Andalusia\". The museum also contains Seville's only planetarium.\nNotable features include a clepsydra (Water clock) and an ecosphere (aquarium). \nPeter O'Toole's opening scene in Lawrence of Arabia was filmed in basement of the building, now used for meetings by CSIC.\n\n"}
{"id": "10810812", "url": "https://en.wikipedia.org/wiki?curid=10810812", "title": "Last Glacial Maximum refugia", "text": "Last Glacial Maximum refugia\n\nLast Glacial Maximum refugia were places where humans, and also other species, survived during the last glacial period in the northern hemisphere, around 25,000 to 20,000 years ago.\n\nSub-Saharan Africa and Australia were not affected by the glaciation (although vast areas of those continents were then too dry for human habitation of any sort, even by the most specialised and well-adapted foragers), and the Americas and New Zealand had no humans at that time. Therefore, the shelters are located mainly in Eurasia. Several of them have been studied.\n\n\n\n\n"}
{"id": "15891789", "url": "https://en.wikipedia.org/wiki?curid=15891789", "title": "List of New Zealand doctors", "text": "List of New Zealand doctors\n\nThe following is a list of notable medical doctors from New Zealand.\n\n"}
{"id": "347136", "url": "https://en.wikipedia.org/wiki?curid=347136", "title": "List of TCP and UDP port numbers", "text": "List of TCP and UDP port numbers\n\nThis is a list of TCP and UDP port numbers used by protocols of the application layer of the Internet protocol suite for the establishment of host-to-host connectivity.\n\nThe Transmission Control Protocol (TCP) and the User Datagram Protocol (UDP) needed only one port for full-duplex, bidirectional traffic. The Stream Control Transmission Protocol (SCTP) and the Datagram Congestion Control Protocol (DCCP) also use port numbers. They usually use port numbers that match the services of the corresponding TCP or UDP implementation, if they exist.\n\nThe Internet Assigned Numbers Authority (IANA) is responsible for maintaining the official assignments of port numbers for specific uses. However, many unofficial uses of both well-known and registered port numbers occur in practice. Similarly many of the official assignments refer to protocols that were never or are no longer in common use. This article lists port numbers and their associated protocols that have experienced significant uptake.\n\nThe port numbers in the range from 0 to 1023 are the \"well-known ports\" or \"system ports\". They are used by system processes that provide widely used types of network services. On Unix-like operating systems, a process must execute with superuser privileges to be able to bind a network socket to an IP address using one of the well-known ports.\n\nThe range of port numbers from 1024 to 49151 are the registered ports. They are assigned by IANA for specific service upon application by a requesting entity. On most systems, registered ports can be used without superuser privileges.\n\nThe range 49152–65535 (2 + 2 to 2 − 1) contains dynamic or private ports that cannot be registered with IANA. This range is used for private or customized services, for temporary purposes, and for automatic allocation of ephemeral ports.\n\n"}
{"id": "48317392", "url": "https://en.wikipedia.org/wiki?curid=48317392", "title": "List of bacterial disulfide oxidoreductases", "text": "List of bacterial disulfide oxidoreductases\n\nBacterial \"thiol disulfide oxidoreductases (TDOR)\" are bacterial enzymes which, along with unfolded proteins, are pumped out of a bacterial cell that allow for adhesion and biofilm development, and generally disease development.\n"}
{"id": "135619", "url": "https://en.wikipedia.org/wiki?curid=135619", "title": "List of compounds", "text": "List of compounds\n\nCompounds are organized into the following lists:\n\n\n\nRelevant links for chemical compounds are:\n\n"}
{"id": "10497151", "url": "https://en.wikipedia.org/wiki?curid=10497151", "title": "List of impact craters in South America", "text": "List of impact craters in South America\n\nThis list includes all 11 confirmed impact craters in South America as listed in the Earth Impact Database. These features were caused by the collision of large meteorites or comets with the Earth. For eroded or buried craters, the stated diameter typically refers to an estimate of original rim diameter, and may not correspond to present surface features.\n\nThe following craters are officially considered \"unconfirmed\" because they are not listed in the Earth Impact Database. Due to stringent requirements regarding evidence and peer-reviewed publication, newly discovered craters or those with difficulty collecting evidence generally are known for some time before becoming listed. However, entries on the unconfirmed list could still have an impact origin disproven.\n\n\n"}
{"id": "48672718", "url": "https://en.wikipedia.org/wiki?curid=48672718", "title": "List of isomers of tetradecane", "text": "List of isomers of tetradecane\n\nThis is the list of the 1858 isomers of tetradecane.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21695575", "url": "https://en.wikipedia.org/wiki?curid=21695575", "title": "List of programming contests", "text": "List of programming contests\n\n"}
{"id": "4001324", "url": "https://en.wikipedia.org/wiki?curid=4001324", "title": "Lithogenic silica", "text": "Lithogenic silica\n\nLithogenic silica (LSi) is silica that originates from terrestrial sources of rock and soil, i.e. from silicate minerals and crystals.\n\nIn the marine silicon cycle, LSi in the ocean is derived from rivers (5.6 Tmol Si/yr), eolian dust (0.5 Tmol Si/yr), hydrothermal vents (0.2 Tmol Si/yr), basalt weathering (0.4 Tmol Si/yr), and from benthic fluxes (23 Tmol Si/yr) from the sediment into the ocean's interior ( et al., 1995).\n\nLSi can either be accumulated \"directly\" in marine sediments or be transferred into dissolved silica (DSi) in the water column.\n\n"}
{"id": "1052005", "url": "https://en.wikipedia.org/wiki?curid=1052005", "title": "Maisonneuve fracture", "text": "Maisonneuve fracture\n\nThe Maisonneuve fracture is a spiral fracture of the proximal third of the fibula associated with a tear of the distal tibiofibular syndesmosis and the interosseous membrane. There is an associated fracture of the medial malleolus or rupture of the deep deltoid ligament. This type of injury can be difficult to detect. \n\nThe Maisonneuve fracture is similar to the Galeazzi fracture in the sense that there is an important ligamentous disruption in association with the fracture. The fracture is named after the surgeon Jules Germain François Maisonneuve.\n\n"}
{"id": "31159552", "url": "https://en.wikipedia.org/wiki?curid=31159552", "title": "Minor loop feedback", "text": "Minor loop feedback\n\nMinor loop feedback is a classical method used to design stable robust linear feedback control systems using feedback loops around sub-systems within the overall feedback loop. The method is sometimes called \"minor loop synthesis\" in college textbooks, some government documents.\n\nThe method is suitable for design by graphical methods and was used before digital computers became available. In World War 2 this method was used to design Gun laying control systems. It is still used now, but not always referred to by name. It is often discussed within the context of Bode plot methods. Minor loop feedback can be used to stabilize opamps.\n\nThis example is slightly simplified (no gears between the motor and the load) from the control system for the Harlan J. Smith Telescope at the McDonald Observatory. In the figure there are three feedback loops: current control loop, velocity control loop and position control loop. The last is the main loop. The other two are minor loops. The forward path, considering only the forward path without the minor loop feedback, has three unavoidable phase shifting stages. The motor inductance and winding resistance form a low-pass filter with a bandwidth around 200 Hz. Acceleration to velocity is an integrator and velocity to position is an integrator. This would have a total phase shift of 180 to 270 degrees. Simply connecting position feedback would almost always result in unstable behaviour.\n\nThe innermost loop regulates the current in the torque motor. This type of motor creates torque that is nearly proportional to the rotor current, even if it is forced to turn backward. Because of the action of the commutator, there are instances when two rotor windings are simultaneously energized. If the motor was driven by a voltage controlled voltage source, the current would roughly double, as would the torque. By sensing the current with a small sensing resister (R) and feeding that voltage back to the inverting input of the drive amplifier, the amplifier becomes a voltage controlled current source. With constant current, when two windings are energized, they share the current and the variation of torque is on the order of 10%.\n\nThe next innermost loop regulates motor speed. The voltage signal from the Tachometer (a small permanent magnet DC generator) is proportional to the angular velocity of the motor. This signal is fed back to the inverting input of the velocity control amplifier (K). The velocity control system makes the system 'stiffer' when presented with torque variations such as wind, movement about the second axis and torque ripple from the motor.\n\nThe outermost loop, the main loop, regulates load position. In this example, position feedback of the actual load position is presented by a Rotary encoder that produces a binary output code. The actual position is compared to the desired position by a digital subtracter that drives a DAC (Digital-to-analog converter) that drives the position control amplifier (K). Position control allows the servo to compensate for sag and for slight position ripple caused by gears (not shown) between the motor and the telescope\n\nThe usual design procedure is to design the innermost subsystem (the current control loop in the telescope example) using local feedback to linearize and flatten the gain. Stability is generally assured by Bode plot methods. Usually, the bandwidth is made as wide as possible. Then the next loop (the velocity loop in the telescope example) is designed. The bandwidth of this sub-system is set to be a factor of 3 to 5 less than the bandwidth of the enclosed system. This process continues with each loop having less bandwidth than the bandwidth of the enclosed system. As long as the bandwidth of each loop is less than the bandwidth of the enclosed sub-system by a factor of 3 to 5, the phase shift of the enclosed system can be neglected, i.e. the sub-system can be treated as simple flat gain. Since the bandwidth of each sub-system is less than the bandwidth of the system it encloses, it is desirable to make the bandwidth of each sub-system as large as possible so that there is enough bandwidth in the outermost loop. The system is often expressed as a Signal-flow graph and its overall transfer function can be computed from Mason's Gain Formula.\n\n"}
{"id": "34454514", "url": "https://en.wikipedia.org/wiki?curid=34454514", "title": "Mississippi State Axion Search", "text": "Mississippi State Axion Search\n\nMississippi State Axion Search is the first of its kind light shining through the wall experiment designed to operate using a continuous radio wave emitter as the source of photons. The experiment contains a radio source and a set of detectors separated by a wall. The aim of the experiment is to limit the mass and coupling constants of an axion like particle or a para photon by looking at the photons on the dark side of the tuned cavity. The experiment is projected to be completed by 2016.\n\nThe collaboration currently includes members from the following institutions;\n"}
{"id": "40805717", "url": "https://en.wikipedia.org/wiki?curid=40805717", "title": "Nice 2 model", "text": "Nice 2 model\n\nThe Nice 2 model is a model of the early evolution of the Solar System. The Nice 2 model resembles the original Nice model in that a late instability of the outer Solar System results in gravitational encounters between planets, the disruption of an outer planetesimal disk, and the migrations of the outer planets to new orbits. However, the Nice 2 model differs in its initial conditions and in the mechanism for triggering the late instability. These changes reflect the analysis of the orbital evolution of the outer Solar System during the gas disk phase and the inclusion of gravitational interactions between planetesimals in the outer disk into the model.\n\nThe Nice 2 model begins with the outer planets in a stable quadruple resonance with each planet in resonance with its nearest neighbors. One example among several potential stable quadruple resonance configurations is Jupiter and Saturn in a 3:2 resonance, Saturn and Uranus in a 3:2 resonance, and Uranus and Neptune in a 4:3 resonance. Interactions with an outer planetesimal disk that is gravitationally stirred by Pluto-sized objects cause the planets to migrate inward while remaining in resonance. During this migration the eccentricity of the inner ice giant increases, leading to secular-resonance crossings. After several hundred million years, the resonant configuration is destabilized during one of these secular-resonance crossings. Gravitational encounters between the planets similar to those in the original Nice model begin shortly thereafter.\n\nThe Nice 2 model addresses some weaknesses of the original Nice model. The first weakness is the artificial selection of the initial orbits of the outer planets to produce an instability that matches the timing of the Late Heavy Bombardment. The second weakness is the sensitivity of the timing of the instability to the location of the inner edge of the planetesimal disk. The Nice 2 model uses particular initial conditions, derived from the examination of the orbital evolution of giant planets orbiting in a gas disk, which may occur under appropriate circumstances. An instability trigger with no apparent correlation between the timing of the instability and the position of the inner edge of the planetesimal disk is the result of the incorporation of the interactions between planetesimals into the Nice 2 model.\n\nThe initial orbits of the giant planets in the Nice 2 model correspond to a predicted orbital structure of the outer Solar System at the end of the gas disk phase. Models of giant planets orbiting in a gas disk predict that they would migrate toward the central star at a rate dependent on the mass of the planet and characteristics of the disk. In a system with multiple planets this migration can result in the convergence of the planet’s orbits and their capture into mean-motion resonances. Investigations focusing on Jupiter and Saturn demonstrated that they can be captured in a 3:2 or 2:1 resonance depending on the characteristics of the protoplanetary disk. After capture into resonance, the gaps that Jupiter and Saturn formed in the disk's density distribution may overlap and their inward migration may be halted or reversed. When Uranus and Neptune are added in turn to the model they are captured into further resonances with the capture of the outer ice giant resulting in the inner ice giant having a higher eccentricity than the other planets. The end result is a system in a quadruple resonance. A number of stable configurations have been identified with the particular final configuration depending on the starting locations of the planets.\n\nThe inclusion of gravitational interactions between planetesimals in the outer disk revealed an alternative mechanism for triggering the late instability of the outer planets. During numerical simulations that included the gravitational interactions between planetesimals, a transfer of energy between the disk and the planets was observed. This energy transfer led to the migration of the planets toward the Sun and occurred even when there were no encounters between planetesimals and the planets. As the migration progressed the eccentricity of the inner ice giant increased. In some of the simulations the quadruple resonance was eventually destabilized resulting in gravitational encounters between planets. The instability was observed in 25% of the simulations with the timing varying between 300 million and 1 billion years. No correlation between the location of the inner edge of the planetesimal disk and the occurrence or the timing of the instability was apparent.\n\nCloser investigation using a simpler model with one planet and a planetesimal disk indicated that the energy transfer was due to a coupling between the eccentricity of the planetesimals in the outer belt and the semi-major axis of the planet. As a result of this coupling an increase in the average eccentricity of the planetesimal belt driven via the gravitational stirring by Pluto-sized objects yields a decrease in the semi-major axis of the planet. The coupling was found to be proportional to the eccentricity of the planet and in a multiple planet system would have its greatest effect on the most eccentric planet.\n\nThe increase in the eccentricity of the inner ice giant was found to be due to the varying strengths of the coupling between the planetesimal disk and the planets. The inner ice giant, with its higher eccentricity due to its resonant capture of the outer ice giant, would normally migrate at a faster rate than the other planets. However, since the resonant configuration requires that the migration be synchronized, the inner ice giant must drag the other planets along. The increase in the inner ice giant’s eccentricity is a result of this process.\n\nExamination of the orbital evolution of the planets revealed that the destabilization of their orbits was due to secular resonance crossings. The increase of the eccentricity of the inner ice giant during the migration led to the slow variation of the precession frequencies of the planets. Secular resonances occurred when these frequencies became similar. The eccentricity of the inner ice giant fluctuated during these secular resonance crossings, sometimes dropping enough to cause the breaking of the quadruple resonance. Whether the quadruple resonance broke was determined by the strength of the secular resonance and the time spent in the secular resonance.\n\nThe nature of the instability mechanism is responsible for the lack of a correlation between the distance to the inner edge of the planetesimal belt and the timing of the instability. If the inner edge of the planetesimal disk is close the migration of the planets occurs at a faster rate. More secular resonance crossings occur but since less time is spent in each one only the strongest can break the quadruple resonance. The reverse is true for a more distant planetesimal belt. As a result of the conflict between these factors the timing and the occurrence of the instability is fairly independent of the distance to the inner edge of the planetesimal belt.\n\nA study using a numerical simulation that included gravitational interactions among all objects revealed that a dynamical instability occurred in less than 70 million years. Interactions between planetesimals dynamically heated the disk and lead to earlier interactions between the planetesimals and giant planets. This study used a limited number of planetesimals due to computational constraints so it is as yet unknown whether this result would apply to a more complete disk.\n\nThe combination of the late destabilization of a five planet resonant chain and an extended migration of Neptune is unlikely. Reproducing the orbital distribution of the Kuiper belt objects requires that Neptune undergo a migration of several AU, reaching 28 AU before the encounters between planets begin. This migration of Neptune is likely if the planetesimal disk began within 2 AU of Neptune's initial orbit. However, a late destabilization of the resonance chain requires a more distant disk, at least 4 AU beyond Neptune's orbit.\n\nAn early breaking of the resonance chain followed by a slow dust-driven migration may bridge this gap. The dust- driven is the result of collision among the planetesimals producing debris that is ground to dust in a collisional cascade. The dust then spirals toward the orbits of the planets due to Poynting-Robertson drag. Interactions with this dust disrupts the resonance chain and drive their migration toward the planetesimal disk over a several hundred million years period. The instability mechanism of the Nice 2 model becomes irrelevant if the dust generated by collisions among the planetesmals disrupts a resonant chain early.\n\n"}
{"id": "502579", "url": "https://en.wikipedia.org/wiki?curid=502579", "title": "Octaeteris", "text": "Octaeteris\n\nIn astronomy, an octaeteris (plural: octaeterides) is the period of eight solar years after which the moon phase occurs on the same day of the year plus one or two days.\n\nThis period is also in a very good synchronicity with five Venusian visibility cycles (the Venusian synodic period) and thirteen Venusian revolutions around the sun (Venusian sidereal period). This means, that if Venus is visible beside the moon, after eight years the two will be again close together near the same date of the calendar.\n\nThe octaeteris, also known as \"oktaeteris\", was noted by Cleostratus in ancient Greece as a 2923.5 day cycle. The 8 year short lunisolar cycle was probably known to many ancient cultures. The mathematical proportions of the octaeteris cycles were noted in Classic Vernal rock art in northeastern Utah by J. Q. Jacobs in 1990. The Three Kings panel also contains more accurate ratios, ratios related to other planets, and apparent astronomical symbolism.\n\nThe octaeteris is the calendar used for the Olympic games, every four years and the Greek use 50 months of one Olympiad, four-year cycle and 49 lunarmonths for the next Olympiad. This octaeteris calendar exist in the Antikythera Mechanism and is used for the Olympic dial of this ancient automaton, to determine the time of the Olympic games and other Greek festivities.\n\n\n"}
{"id": "3400190", "url": "https://en.wikipedia.org/wiki?curid=3400190", "title": "Outline of astronomy", "text": "Outline of astronomy\n\nThe following outline is provided as an overview of and topical guide to astronomy:\n\nAstronomy – studies the universe beyond Earth, including its formation and development, and the evolution, physics, chemistry, meteorology, and motion of celestial objects (such as galaxies, planets, etc.) and phenomena that originate outside the atmosphere of Earth (such as the cosmic background radiation).\n\nAstronomy can be described as all the following:\n\n\n\nHistory of astronomy\n\nAstronomical object\n\n\nSun\n\nSmall Solar System body\n\nVariable star\n\nSupernova\nBlack hole\n\nObsolete constellations including Ptolemy's \"Argo Navis\"\nAnser\n\n\n\n\nSpace agencies\n\n\n\n\n\n\n\n\n\n\n\n\n\n Preceded by the Soviet space program\n\n"}
{"id": "24909838", "url": "https://en.wikipedia.org/wiki?curid=24909838", "title": "Paul Adams (scientist)", "text": "Paul Adams (scientist)\n\nPaul Richard Adams, FRS is a neuroscientist currently serving as a Professor in the Department of Neurobiology and Behavior at Stony Brook University in New York.\n\nHe graduated from London University with a PhD, and postdoctoral work with Bert Sakmann at the Max Planck Institute.\nHe won the Novartis Memorial Prize in 1979 and the Gaddum Memorial Award in 1984, both from the British Pharmacological Society. He was made an MacArthur Foundation Prize Fellow in 1986, and elected as a Fellow of the Royal Society in 1991. From 1987 to 1995 he was an investigator at the Howard Hughes Medical Institute.\n\nWith others, he pioneered the concepts of open channel block and neuromodulation, which now play central roles in neuroscience. He is now working on a theory about the neocortex, centering on the idea that the key to sophisticated learning is extremely specific synaptic strength adjustment.\n\n\n"}
{"id": "54252422", "url": "https://en.wikipedia.org/wiki?curid=54252422", "title": "Peter Drummond (physicist)", "text": "Peter Drummond (physicist)\n\nPeter David Drummond is a physicist and Distinguished Professor in the Centre for Quantum and Optical Science at Swinburne University of Technology.\n\nDrummond was born in New Zealand in 1950, and was educated at Auckland University, where he graduated a B. Sc. (Hons), at Harvard where he received an A. M. (masters) degree, and at the University of Waikato in New Zealand, where his D. Phil. degree was supervised by Dan Walls and Crispin Gardiner. He worked as a postdoctoral researcher with Joseph H. Eberly at Rochester University, and then as an academic at Auckland University before being appointed to a chair of physics at Queensland University in 1989. He moved to Swinburne University of Technology in 2008.\n\n\n\n\n\n\n\n"}
{"id": "15121305", "url": "https://en.wikipedia.org/wiki?curid=15121305", "title": "STS-400", "text": "STS-400\n\nSTS-400 was the Space Shuttle contingency support (Launch On Need) flight that would have been launched using if a major problem occurred on during STS-125, the final Hubble Space Telescope servicing mission (HST SM-4).\n\nDue to the much lower orbital inclination of the HST compared to the ISS, the shuttle crew would have been unable to use the International Space Station as a \"safe haven,\" and NASA would not have been able to follow the usual plan of recovering the crew with another shuttle at a later date. Instead, NASA developed a plan to conduct a shuttle-to-shuttle rescue mission, similar to proposed rescue missions for pre-ISS flights. The rescue mission would have been launched only three days after call-up and as early as seven days after the launch of STS-125, since the crew of \"Atlantis\" would only have about three weeks of consumables after launch.\n\nThe mission was first rolled out in September 2008 to Launch Complex 39B two weeks after the STS-125 shuttle was rolled out to Launch Complex 39A, creating a rare scenario in which two shuttles were on launch pads at the same time. In October 2008, however, STS-125 was delayed and rolled back to the VAB.\n\nInitially, STS-125 was retargeted for no earlier than February 2009. This changed the STS-400 vehicle from \"Endeavour\" to \"Discovery\". The mission was redesignated STS-401 due to the swap from \"Endeavour\" to \"Discovery\". STS-125 was then delayed further, allowing \"Discovery\" mission STS-119 to fly beforehand. This resulted in the rescue mission reverting to \"Endeavour\", and the STS-400 designation being reinstated. In January, 2009, it was announced that NASA was evaluating conducting both launches from Complex 39A in order to avoid further delays to Ares I-X, which, at the time, was scheduled for launch from LC-39B in the September 2009 timeframe. It was planned that after the STS-125 mission in October 2008, Launch Complex 39B would undergo the conversion for use in Project Constellation for the Ares I-X rocket. Several of the members on the NASA mission management team said at the time (2009) that single-pad operations were possible, but the decision was made to use both pads.\n\nThe crew assigned to this mission was a subset of the STS-126 crew:\n\nThree different concept mission plans were evaluated: The first would be to use a shuttle-to-shuttle docking, where the rescue shuttle docks with the damaged shuttle, by flying upside down and backwards, relative to the damaged shuttle. It was unclear whether this would be practical, as the forward structure of either orbiter could collide with the payload bay of the other, resulting in damage to both orbiters. The second option that was evaluated, would be for the rescue orbiter to rendezvous with the damaged orbiter, and perform station-keeping while using its Remote Manipulator System (RMS) to transfer crew from the damaged orbiter. This mission plan would result in heavy fuel consumption. The third concept would be for the damaged orbiter to grapple the rescue orbiter using its RMS, eliminating the need for station-keeping. The rescue orbiter would then transfer crew using its RMS, as in the second option, and would be more fuel efficient than the station-keeping option.\n\nThe concept that was eventually decided upon was a modified version of the third concept. The rescue orbiter would use its RMS to grapple the end of the damaged orbiter's RMS.\n\nAfter its most recent mission (STS-123), \"Endeavour\" was taken to the Orbiter Processing Facility for routine maintenance. Following the maintenance, \"Endeavour\" was on stand-by for STS-326 which would have been flown in the case that STS-124 would not have been able to return to Earth safely. Stacking of the solid rocket boosters (SRB) began on 11 July 2008. One month later, the external tank arrived at KSC and was mated with the SRBs on 29 August 2008. \"Endeavour\" joined the stack on 12 September 2008 and was rolled out to Pad 39B one week later.\n\nSince STS-126 launched before STS-125, \"Atlantis\" was rolled back to the VAB on 20 October, and \"Endeavour\" rolled around to Launch Pad 39A on 23 October. When it was time to launch STS-125, \"Atlantis\" rolled out to pad 39A.\n\nThe Mission would not have included the extended heatshield inspection normally performed on flight day two. Instead, an inspection would have been performed after the crew was rescued. On flight day two, \"Endeavour\" would have performed the rendezvous and grapple with \"Atlantis\". On flight day three, the first EVA would have been performed. During the first EVA, Megan McArthur, Andrew Feustel and John Grunsfeld would have set up a tether between the airlocks. They would have also transferred a large size Extravehicular Mobility Unit (EMU) and, after McArthur had repressurized, transferred McArthur's EMU back to \"Atlantis\". Afterwards they would have repressurized on \"Endeavour\", ending flight day two activities.\n\nThe final two EVA were planned for flight day three. During the first, Grunsfeld would have depressurized on \"Endeavour\" in order to assist Gregory Johnson and Michael Massimino in transferring an EMU to \"Atlantis\". He and Johnson would then repressurize on \"Endeavour\", and Massimino would have gone back to \"Atlantis\". He, along with Scott Altman and Michael Good would have taken the rest of the equipment and themselves to \"Endeavour\" during the final EVA. They would have been standing by in case the RMS system should malfunction. The damaged orbiter would have been commanded by the ground to deorbit and go through landing procedures over the Pacific, with the impact area being north of Hawaii. On flight day five, \"Endeavour\" would have had a full heat shield inspection, and land on flight day eight.\n\nThis mission could have marked the end of the Space Shuttle program, as it is considered unlikely that the program would have been able to continue with just two remaining orbiters, \"Discovery\" and \"Endeavour\".\n\nOn Thursday, 21 May 2009, NASA officially released \"Endeavour\" from the rescue mission, freeing the orbiter to begin processing for STS-127. This also allowed NASA to continue processing LC-39B for the upcoming Ares I-X launch, as during the stand-down period, NASA installed a new lightning protection system, similar to those found on the Atlas V and Delta IV pads, to protect the newer, taller Ares I rocket from lightning strikes.\n\n\n"}
{"id": "1173689", "url": "https://en.wikipedia.org/wiki?curid=1173689", "title": "Stress fracture", "text": "Stress fracture\n\nStress fracture is a fatigue-induced fracture of the bone caused by repeated stress over time. Instead of resulting from a single severe impact, stress fractures are the result of accumulated trauma from repeated submaximal loading, such as running or jumping. Because of this mechanism, stress fractures are common overuse injuries in athletes.\n\nStress fractures can be described as very small slivers or cracks in the bone; and are sometimes referred to as \"hairline fractures.\" Stress fractures most frequently occur in weight-bearing bones, such as the tibia (bone of the lower leg), metatarsals, and navicular bones (bones of the foot). Less common are fractures to the femur, pelvis, and sacrum.\n\nStress fractures are typically discovered after a rapid increase in exercise. They most commonly present as pain with weight bearing that increases with activity. The pain usually subsides with rest but may be constantly present with a more serious bone injury. There is usually an area of localized tenderness on or near the bone and generalized swelling in the area. Percussion or palpation to the bone may reproduce symptoms. Anterior tibial stress fractures elicit focal tenderness on the anterior tibial crest, while posterior medial stress fractures can be tender at the posterior tibial border.\n\nBones are constantly attempting to remodel and repair themselves, especially during a sport where extraordinary stress is applied to the bone. Over time, if enough stress is placed on the bone that it exhausts the capacity of the bone to remodel, a weakened site—a stress fracture—may appear on the bone. The fracture does not appear suddenly. It occurs from repeated traumas, none of which is sufficient to cause a sudden break, but which, when added together, overwhelm the osteoblasts that remodel the bone.\n\nStress fractures commonly occur in sedentary people who suddenly undertake a burst of exercise (whose bones are not used to the task). They may also occur in athletes completing high volume, high impact training, such as running or jumping sports. Stress fractures are also commonly reported in soldiers who march long distances.\n\nMuscle fatigue can also play a role in the occurrence of stress fractures. In a runner, each stride normally exerts large forces at various points in the legs. Each shock—a rapid acceleration and energy transfer—must be absorbed. Muscles and bones serve as shock absorbers. However, the muscles, usually those in the lower leg, become fatigued after running a long distance and lose their ability to absorb shock. As the bones now experience larger stresses, this increases the risk of fracture.\n\nPrevious stress fractures have been identified as a risk factor.\n\nX-rays usually do not show evidence of new stress fractures, but can be used 3 weeks after onset of pain when the bone begins to remodel. A CT scan, MRI, or 3-phase bone scan may be more effective for early diagnosis.\n\nMRI appears to be the most accurate test.\n\nAltering the biomechanics of training and training schedules may reduce the prevalence of stress fractures. Orthotic insoles have been found to decrease the rate of stress fractures in military recruits, but it is unclear whether this can be extrapolated to the general population or athletes. On the other hand, some athletes have argued that cushioning in shoes actually causes more stress by reducing the body's natural shock-absorbing action, thus increasing the frequency of running injuries. During exercise that applies more stress to the bones, it may help to increase daily calcium (2,000 mg) and vitamin D (800 IU) intake, depending on the individual.\n\nFor low-risk stress fractures, rest is the best management option. The amount of recovery time varies greatly depending upon the location and severity of the fracture, and the body's healing response. Complete rest and a stirrup leg brace or walking boot are usually used for a period of four to eight weeks, although periods of rest of twelve weeks or more are not uncommon for more severe stress fractures. After this period, activities may be gradually resumed as long as the activities do not cause pain. While the bone may feel healed and not hurt during daily activity, the process of bone remodeling may take place for many months after the injury feels healed. Incidences of refracturing the bone are still a significant risk. Activities such as running or sports that place additional stress on the bone should only gradually be resumed. Rehabilitation usually includes muscle strength training to help dissipate the forces transmitted to the bones.\n\nWith severe stress fractures (see \"prognosis\"), surgery may be needed for proper healing. The procedure may involve pinning the fracture site, and rehabilitation can take up to six months.\n\nAnterior tibial stress fractures can have a particularly poor prognosis and can require surgery. On radiographic imaging these stress fractures are referred to as the \"dreaded black line.\" When compared to other stress fractures, anterior tibial fractures are more likely to progress to complete fracture of the tibia and displacement. Superior femoral neck stress fractures, if left untreated, can progress to become complete fractures with avascular necrosis, and should also be managed surgically. Proximal metadiaphyseal fractures of the fifth metatarsal (middle of the outside edge of the foot) are also notorious for poor bone healing. These stress fractures heal slowly with significant risk of refracture.\n\nIn the US, the annual incidence of stress fractures in athletes and military recruits ranges from 5% to 30%, depending on the sport and other risk factors. Women and highly active individuals are also at a higher risk. The incidence probably also increases with age due to age-related reductions in bone mass density (BMD). Children may also be at risk because their bones have yet to reach full density and strength. The female athlete triad also can put women at risk as disordered eating and osteoporosis can cause the bones to be severely weakened.\n\nIn 2001, Bruce Rothschild and other paleontologists published a study examining evidence for stress fractures in theropod dinosaurs and analyzed the implications such injuries would have for reconstructing their behavior. Since stress fractures are due to repeated events they are probably caused by expressions of regular behavior rather than chance trauma. The researchers paid special attention to evidence of injuries to the hand since dinosaurs' hind feet would be more prone to injuries received while running or migrating. Hand injuries, meanwhile, were more likely to be caused by struggling prey. Stress fractures in dinosaur bones can be identified by looking for bulges on the shafts of bones that face toward the front of the animal. When x-rayed, these bulges often show lines of clear space where the x-rays have a harder time traveling through the bone. Rothschild and the other researchers noted that this \"zone of attenuation\" seen under the x-ray typically cannot be seen with the naked eye.\n\nThe researchers described theropod phalanges as being \"pathognomonic\" for stress fractures, this means they are \"characteristic and unequivocal diagnostically.\" Rothschild and the other researchers examined and dismissed other kinds of injury and sickness as causes of the lesions they found on the dinosaurs' bones. Lesions left by stress fractures can be distinguished from osteomyelitis without difficulty because of a lack of bone destruction in stress fracture lesions. They can be distinguished from benign bone tumors like osteoid osteoma by the lack of a sclerotic perimeter. No disturbance of the internal bony architecture of the sort caused by malignant bone tumors was encountered among the stress fracture candidates. No evidence of metabolic disorders like hyperparathyroidism or hyperthyroidism was found in the specimens, either.\n\nAfter examining the bones of many kinds of dinosaur the researchers noted that \"Allosaurus\" had a significantly greater number of bulges on the shafts of its hand and foot bones than the tyrannosaur \"Albertosaurus\", or the ostrich dinosaurs \"Ornithomimus\" and \"Archaeornithomimus\". Most of the stress fractures observed along the lengths of \"Allosaurus\" toe bones were confined to the ends closest to the hind foot, but were spread across all three major digits in \"statistically indistinguishable\" numbers. Since the lower end of the third metatarsal would have contacted the ground first while a theropod was running it would have borne the most stress and should be most predisposed to suffer stress fractures. The lack of such a bias in the examined fossils indicates an origin for the stress fractures from a source other than running. The authors conclude that these fractures occurred during interaction with prey. They suggest that such injuries could occur as a result of the theropod trying to hold struggling prey with its feet. The presence of stress fractures provide evidence for very active predation-based feeding rather than scavenging diets.\n"}
{"id": "25558959", "url": "https://en.wikipedia.org/wiki?curid=25558959", "title": "Taftian theory", "text": "Taftian theory\n\nTaftian theory (also \"Whig\" theory) is a political term in the United States referring to a strict constructionist view regarding presidential power and the United States Constitution, where a president's power is limited to those powers specifically enumerated by the Constitution.\n\nTaftian Theory was coined after the governing style of the 27th president of the United States, William Howard Taft. Most presidents prior to Franklin D Roosevelt subscribed to this theory, as where more recent presidents subscribe to a stewardship theory.\n"}
{"id": "34983797", "url": "https://en.wikipedia.org/wiki?curid=34983797", "title": "Taylor's law", "text": "Taylor's law\n\nTaylor's law (also known as Taylor's power law) is an empirical law in ecology that relates the variance of the number of individuals of a species per unit area of habitat to the corresponding mean by a power law relationship. It is named after the ecologist who first proposed it in 1961, Lionel Roy Taylor (1924–2007). Taylor's original name for this relationship was the law of the mean.\n\nThis law was originally defined for ecological systems, specifically to assess the spatial clustering of organisms. For a population count \"Y\" with mean \"µ\" and variance var(\"Y\"), Taylor's law is written,\n\nwhere \"a\" and \"b\" are both positive constants. Taylor proposed this relationship in 1961, suggesting that the exponent \"b\" be considered a species specific index of aggregation. This power law has subsequently been confirmed for many hundreds of species.\n\nTaylor's law has also been applied to assess the time dependent changes of population distributions. Related variance to mean power laws have also been demonstrated in several non-ecological systems: \n\nThe first use of a double log-log plot was by Reynolds in 1879 on thermal aerodynamics. Pareto used a similar plot to study the proportion of a population and their income.\nThe term \"variance\" was coined by Fisher in 1918.\n\nFisher in 1921 proposed the equation\n\nNeyman studied the relationship between the sample mean and variance in 1926. Barlett proposed a relationship between the sample mean and variance in 1936\n\nSmith in 1938 while studying crop yields proposed a relationship similar to Taylor's. This relationship was\n\nwhere \"V\" is the variance of yield for plots of \"x\" units, \"V\" is the variance of yield per unit area and \"x\" is the size of plots. The slope (\"b\") is the index of heterogeneity. The value of \"b\" in this relationship lies between 0 and 1. Where the yield are highly correlated \"b\" tends to 0; when they are uncorrelated \"b\" tends to 1.\n\nBliss in 1941, Fracker and Brischle in 1941 and Hayman & Lowe in 1961 also described what is now known as Taylor's law, but in the context of data from single species.\n\nL. R. Taylor (1924–2007) was an English entomologist who worked on the Rothamsted Insect Survey for pest control. His 1961 paper used data from 24 papers published between 1936 and 1960. These papers considered a variety of biological settings: virus lesions, macro-zooplankton, worms and symphylids in soil, insects in soil, on plants and in the air, mites on leaves, ticks on sheep and fish in the sea. In these papers the \"b\" value lay between 1 and 3. Taylor proposed the power law as a general feature of the spatial distribution of these species. He also proposed a mechanistic hypothesis to explain this law. Among the papers cited were those of Bliss and Yates and Finney.\n\nInitial attempts to explain the spatial distribution of animals had been based on approaches like Bartlett's stochastic population models and the negative binomial distribution that could result from birth-death processes. Taylor's novel explanation was based the assumption of a balanced migratory and congregatory behavior of animals. His hypothesis was initially qualitative, but as it evolved it became semi-quantitative and was supported by simulations. In proposing that animal behavior was the principal mechanism behind the clustering of organisms, Taylor though appeared to have ignored his own report of clustering seen with tobacco necrosis virus plaques.\n\nFollowing Taylor's initial publications several alternative hypotheses for the power law were advanced. Hanski proposed a random walk model, modulated by the presumed multiplicative effect of reproduction. Hanski's model predicted that the power law exponent would be constrained to range closely about the value of 2, which seemed inconsistent with many reported values.\n\nAnderson \"et al\" formulated a simple stochastic birth, death, immigration and emigration model that yielded a quadratic variance function. As a response to this model Taylor argued that such a Markov process would predict that the power law exponent would vary considerably between replicate observations, and that such variability had not been observed.\n\nAbout this time concerns were, however, raised regarding the statistical variability with measurements of the power law exponent, and the possibility that observations of a power law might reflect more mathematical artifact than a mechanistic process. Taylor \"et al\" responded with an additional publication of extensive observations which he claimed refuted Downing's concerns.\n\nIn addition, Thórarinsson published a detailed critique of the animal behavioral model, noting that Taylor had modified his model several times in response to concerns raised, and that some of these modifications were inconsistent with earlier versions. Thórarinsson also claimed that Taylor confounded animal numbers with density and that Taylor had incorrectly interpreted simulations that had been constructed to demonstrate his models as validation.\n\nKemp reviewed a number of discrete stochastic models based on the negative binomial, Neyman type A, and Polya–Aeppli distributions that with suitable adjustment of parameters could produce a variance to mean power law. Kemp, however, did not explain the parameterizations of his models in mechanistic terms. Other relatively abstract models for Taylor's law followed.\n\nA number of additional statistical concerns were raised regarding Taylor's law, based on the difficulty with real data in distinguishing between Taylor's law and other variance to mean functions, as well the inaccuracy of standard regression methods.\n\nReports also began to accumulate where Taylor's law had been applied to time series data. Perry showed how simulations based on chaos theory could yield Taylor's law, and Kilpatrick & Ives provided simulations which showed how interactions between different species might lead to Taylor's law.\n\nOther reports appeared where Taylor's law had been applied to the spatial distribution of plants and bacterial populations As with the observations of Tobacco necrosis virus mentioned earlier, these observations were not consistent with Taylor's animal behavioral model.\n\nEarlier it was mentioned that variance to mean power function had been applied to non-ecological systems, under the rubric of Taylor's law. To provide a more general explanation for the range of manifestations of the power law a hypothesis was proposed based on the Tweedie distributions, a family of probabilistic models that express an inherent power function relationship between the variance and the mean. Details regarding this hypothesis will be provided in the next section.\n\nA further alternative explanation for Taylor's law was proposed by Cohen \"et al\", derived from the Lewontin Cohen growth model. This model was successfully used to describe the spatial and temporal variability of forest populations.\n\nAnother paper by Cohen and Xu that random sampling in blocks where the underling distribution is skewed with the first four moments finite gives rise to Taylor's law. Approximate formulae for the parameters and their variances were also derived. These estimates were tested again data from the Black Rock Forest and found to be in reasonable agreement.\n\nFollowing Taylor's initial publications several alternative hypotheses for the power law were advanced. Hanski proposed a random walk model, modulated by the presumed multiplicative effect of reproduction. Hanski's model predicted that the power law exponent would be constrained to range closely about the value of 2, which seemed inconsistent with many reported values. Anderson \"et al\" formulated a simple stochastic birth, death, immigration and emigration model that yielded a quadratic variance function. The Lewontin Cohen growth model. is another proposed explanation. The possibility that observations of a power law might reflect more mathematical artifact than a mechanistic process was raised.\n\nIn the physics literature Taylor's law has been referred to as \"fluctuation scaling\". Eisler \"et al\", in a further attempt to find a general explanation for fluctuation scaling, proposed a process they called \"impact inhomogeneity\" in which frequent events are associated with larger impacts. In appendix B of the Eisler article, however, the authors noted that the equations for impact inhomogeneity yielded the same mathematical relationships as found with the Tweedie distributions.\n\nAnother group of physicists, Fronczak and Fronczak, derived Taylor's power law for fluctuation scaling from principles of equilibrium and non-equilibrium statistical physics. Their derivation was based on assumptions of physical quantities like free energy and an external field that caused the clustering of biological organisms. Direct experimental demonstration of these postulated physical quantities in relationship to animal or plant aggregation has yet to be achieved, though. Shortly thereafter, an analysis of Fronczak and Fronczak's model was presented that showed their equations directly lead to the Tweedie distributions, a finding that suggested that Fronczak and Fronczak had possibly provided a maximum entropy derivation of these distributions.\n\nTaylor's law has been shown to hold for prime numbers not exceeding a given real number. This result has been shown to hold for the first 11 million primes. If the Hardy–Littlewood twin primes conjecture is true then this law also holds for twin primes.\n\nThe law itself is named after the ecologist Lionel Roy Taylor (1924–2007). The name 'Taylor's law' was coined by Southwood in 1966. Taylor's original name for this relationship was the law of the mean\n\nAbout the time that Taylor was substantiating his ecological observations, MCK Tweedie, a British statistician and medical physicist, was investigating a family of probabilistic models that are now known as the Tweedie distributions. As mentioned above, these distributions are all characterized by a variance to mean power law mathematically identical to Taylor's law.\n\nThe Tweedie distribution most applicable to ecological observations is the compound Poisson-gamma distribution, which represents the sum of \"N\" independent and identically distributed random variables with a gamma distribution where \"N\" is a random variable distributed in accordance with a Poisson distribution. In the additive form its cumulant generating function (CGF) is:\n\nwhere \"κ\"(\"θ\") is the cumulant function,\n\nthe Tweedie exponent\n\n\"s\" is the generating function variable, and \"θ\" and \"λ\" are the canonical and index parameters, respectively.\n\nThese last two parameters are analogous to the scale and shape parameters used in probability theory. The cumulants of this distribution can be determined by successive differentiations of the CGF and then substituting \"s=0\" into the resultant equations. The first and second cumulants are the mean and variance, respectively, and thus the compound Poisson-gamma CGF yields Taylor's law with the proportionality constant\n\nThe compound Poisson-gamma cumulative distribution function has been verified for limited ecological data through the comparison of the theoretical distribution function with the empirical distribution function. A number of other systems, demonstrating variance to mean power laws related to Taylor's law, have been similarly tested for the compound Poisson-gamma distribution.\n\nThe main justification for the Tweedie hypothesis rests with the mathematical convergence properties of the Tweedie distributions. The Tweedie convergence theorem requires the Tweedie distributions to act as foci of convergence for a wide range of statistical processes. As a consequence of this convergence theorem, processes based on the sum of multiple independent small jumps will tend to express Taylor's law and obey a Tweedie distribution. A limit theorem for independent and identically distributed variables, as with the Tweedie convergence theorem, might then be considered as being fundamental relative to the \"ad hoc\" population models, or models proposed on the basis of simulation or approximation.\n\nThis hypothesis remains controversial; more conventional population dynamic approaches seem preferred amongst ecologists, despite the fact that the Tweedie compound Poisson distribution can be directly applied to population dynamic mechanisms.\n\nOne difficulty with the Tweedie hypothesis is that the value of \"b\" does not range between 0 and 1. Values of \"b\" < 1 are rare but have been reported.\n\nIn symbols\n\nwhere \"s\" is the variance of the density of the \"i\"th sample, \"m\" is the mean density of the \"i\"th sample and \"a\" and \"b\" are constants.\n\nIn logarithmic form\n\nTaylor's law is scale invariant. If the unit of measurement is changed by a constant factor \"c\", the exponent (\"b\") remains unchanged.\n\nTo see this let \"y\" = \"cx\". Then\n\nformula_11\n\nformula_12\n\nformula_13\n\nformula_14\n\nTaylor's law expressed in the original variable (\"x\") is\n\nformula_15\n\nand in the rescaled variable (\"y\") it is\n\nformula_16\n\nIt has been shown that Taylor's law is the only relationship between the mean and variance that is scale invariant.\n\nA refinement in the estimation of the slope \"b\" has been proposed by Rayner.\n\nwhere \"r\" is the Pearson moment correlation coefficient between log(\"s\") and log \"m\", \"f\" is the ratio of sample variances in log(\"s\") and log \"m\" and \"φ\" is the ratio of the errors in log(\"s\") and log \"m\".\n\nOrdinary least squares regression assumes that \"φ\" = ∞. This tends to underestimate the value of \"b\" because the estimates of both log(\"s\") and log \"m\" are subject to error.\n\nAn extension of Taylor's law has been proposed by Ferris \"et al\" when multiple samples are taken\n\nwhere \"s\" and \"m\" are the variance and mean respectively, \"b\", \"c\" and \"d\" are constants and \"n\" is the number of samples taken. To date, this proposed extension has not been verified to be as applicable as the original version of Taylor's law.\n\nAn extension to this law for small samples has been proposed by Hanski. For small samples the Poisson variation (\"P\") - the variation that can be ascribed to sampling variation - may be significant. Let \"S\" be the total variance and let \"V\" be the biological (real) variance. Then\n\nAssuming the validity of Taylor's law, we have\n\nBecause in the Poisson distribution the mean equals the variance, we have\n\nThis gives us\n\nThis closely resembles Barlett's original suggestion.\n\nSlope values (\"b\") significantly > 1 indicate clumping of the organisms.\n\nIn Poisson-distributed data, \"b\" = 1. If the population follows a lognormal or gamma distribution, then \"b\" = 2.\n\nFor populations that are experiencing constant per capita environmental variability, the regression of log( variance ) versus log( mean abundance ) should have a line with \"b\" = 2.\n\nMost populations that have been studied have \"b\" < 2 (usually 1.5–1.6) but values of 2 have been reported. Occasionally cases with \"b\" > 2 have been reported. \"b\" values below 1 are uncommon but have also been reported ( \"b\" = 0.93 ).\n\nIt has been suggested that the exponent of the law (\"b\") is proportional to the skewness of the underlying distribution. This proposal has criticised: additional work seems to be indicated.\n\nThe origin of the slope (\"b\") in this regression remains unclear. Two hypotheses have been proposed to explain it. One suggests that \"b\" arises from the species behavior and is a constant for that species. The alternative suggests that it is dependent on the sampled population. Despite the considerable number of studies carried out on this law (over 1000), this question remains open.\n\nIt is known that both \"a\" and \"b\" are subject to change due to age-specific dispersal, mortality and sample unit size.\n\nThis law may be a poor fit if the values are small. For this reason an extension to Taylor's law has been proposed by Hanski which improves the fit of Taylor's law at low densities.\n\nA form of Taylor's law applicable to binary data in clusters (e.q., quadrats) has been proposed. In a binomial distribution, the theoretical variance is\n\nwhere \"(var)\" is the binomial variance, \"n\" is the sample size per cluster, and \"p\" is the proportion of individuals with a trait (such as disease), an estimate of the probability of an individual having that trait.\n\nOne difficulty with binary data is that the mean and variance, in general, have a particular relationship: as the mean proportion of individuals infected increases above 0.5, the variance deceases.\n\nIt is now known that the observed variance \"(var)\" changes as a power function of \"(var)\".\n\nHughes and Madden noted that if the distribution is Poisson, the mean and variance are equal. As this is clearly not the case in many observed proportion samples, they instead assumed a binomial distribution. They replaced the mean in Taylor's law with the binomial variance and then compared this theoretical variance with the observed variance. For binomial data, they showed that \"var = var\" with overdispersion, \"var\" > \"var\".\n\nIn symbols, Hughes and Madden's modification to Tyalor's law was\n\nIn logarithmic form this relationship is\n\nThis latter version is known as the binary power law.\n\nA key step in the derivation of the binary power law by Hughes and Madden was the observation made by Patil and Stiteler that the variance-to-mean ratio used for assessing over-dispersion of unbounded counts in a single sample is actually the ratio of two variances: the observed variance and the theoretical variance for a random distribution. For unbounded counts, the random distribution is the Poisson. Thus, the Taylor power law for a collection of samples can be considered as a relationship between the observed variance and the Poisson variance.\n\nMore broadly, Madden and Hughes considered the power law as the relationship between two variances, the observed variance and the theoretical variance for a random distribution. With binary data, the random distribution is the binomial (not the Poisson). Thus the Taylor power law and the binary power law are two special cases of a general power-law relationships for heterogeneity.\n\nWhen both \"a\" and \"b\" are equal to 1, then a small-scale random spatial pattern is suggested and is best described by the binomial distribution. When \"b\" = 1 and \"a\" > 1, there is over-dispersion (small scale aggregation). When \"b\" is > 1, the degree of aggregation varies with \"p\". Turechek \"et al\" have showed that the binary power law describes numerous data sets in plant pathology. In general, \"b\" is greater than 1 and less than 2.\n\nThe fit of this law has been tested by simulations. These results suggest that rather than a single regression line for the data set, a segmental regression may be a better model for genuinely random distributions. However, this segmentation only occurs for very short-range dispersal distances and large quadrat sizes. The break in the line occurs only at \"p\" very close to 0.\n\nAn extension to this law has been proposed. The original form of this law is symmetrical but it can be extended to an asymmetrical form. Using simulations the symmetrical form fits the data when there is positive correlation of disease status of neighbors. Where there is a negative correlation between the likelihood of neighbours being infected, the asymmetrical version is a better fit to the data.\n\nBecause of the ubiquitous occurrence of Taylor's law in biology it has found a variety of uses some of which are listed here.\n\nIt has been recommended based on simulation studies in applications testing the validity of Taylor's law to a data sample that:\n\n(1) the total number of organisms studied be > 15\n(2) the minimum number of groups of organisms studied be > 5 \n(3) the density of the organisms should vary by at least 2 orders of magnitude within the sample\n\nIt is common assumed (at least initially) that a population is randomly distributed in the environment. If a population is randomly distributed then the mean ( \"m\" ) and variance ( \"s\" ) of the population are equal and the proportion of samples that contain at least one individual ( \"p\" ) is\n\nWhen a species with a clumped pattern is compared with one that is randomly distributed with equal overall densities, p will be less for the species having the clumped distribution pattern. Conversely when comparing a uniformly and a randomly distributed species but at equal overall densities, \"p\" will be greater for the randomly distributed population. This can be graphically tested by plotting \"p\" against \"m\".\n\nWilson and Room developed a binomial model that incorporates Taylor's law. The basic relationship is\n\nwhere the log is taken to the base \"e\".\n\nIncorporating Taylor's law this relationship becomes\n\nThe common dispersion parameter (\"k\") of the negative binomial distribution is\n\nwhere \"m\" is the sample mean and \"s\" is the variance. If 1 / \"k\" is > 0 the population is considered to be aggregated; 1 / \"k\" = 0 ( \"s\" = \"m\" ) the population is considered to be randomly (Poisson) distributed and if 1 / \"k\" is < 0 the population is considered to be uniformly distributed. No comment on the distribution can be made if \"k\" = 0.\n\nWilson and Room assuming that Taylor's law applied to the population gave an alternative estimator for \"k\":\n\nwhere \"a\" and \"b\" are the constants from Taylor's law.\n\nJones using the estimate for \"k\" above along with the relationship Wilson and Room developed for the probability of finding a sample having at least one individual\n\nderived an estimator for the probability of a sample containing \"x\" individuals per sampling unit. Jones's formula is\n\nwhere \"P\"( \"x\" ) is the probability of finding \"x\" individuals per sampling unit, \"k\" is estimated from the Wilon and Room equation and \"m\" is the sample mean. The probability of finding zero individuals \"P\"( 0 ) is estimated with the negative binomial distribution\n\nJones also gives confidence intervals for these probabilities.\n\nwhere \"CI\" is the confidence interval, \"t\" is the critical value taken from the t distribution and \"N\" is the total sample size.\n\nKatz proposed a family of distributions (the Katz family) with 2 parameters ( \"w\", \"w\" ). This family of distributions includes the Bernoulli, Geometric, Pascal and Poisson distributions as special cases. The mean and variance of a Katz distribution are\n\nwhere \"m\" is the mean and \"s\" is the variance of the sample. The parameters can be estimated by the method of moments from which we have\n\nFor a Poisson distribution \"w\" = 0 and \"w\" = \"λ\" the parameter of the Possion distribution. This family of distributions is also sometimes known as the Panjer family of distributions.\n\nThe Katz family is related to the Sundt-Jewel family of distributions:\n\nformula_39\n\nThe only members of the Sundt-Jewel family are the Poisson, binomial, negative binomial (Pascal), extended truncated negative binomial and logarithmic series distributions.\n\nIf the population obeys a Katz distribution then the coefficients of Taylor's law are\n\nKatz also introduced a statistical test\n\nwhere \"J\" is the test statistic, \"s\" is the variance of the sample, \"m\" is the mean of the sample and \"n\" is the sample size. \"J\" is asymptotically normally distributed with a zero mean and unit variance. If the sample is Poisson distributed \"J\" = 0; values of \"J\" < 0 and > 0 indicate under and over dispersion respectively. Overdispersion is often caused by latent heterogeneity - the presence of multiple sub populations within the population the sample is drawn from.\n\nThis statistic is related to the Neyman-Scott statistic\n\nwhich is known to be asymptotically normal and the conditional chi-squared statistic (Poisson dispersion test)\n\nwhich is known to have an asymptotic chi squared distribution with \"n\" − 1 degrees of freedom when the population is Poisson distributed.\n\nIf the population obeys Taylor's law then\n\nIf Taylor's law is assumed to apply it is possible to determine the mean time to local extinction. This model assumes a simple random walk in time and the absence of density dependent population regulation.\n\nLet formula_46 where \"N\" and \"N\" are the population sizes at time \"t\" + 1 and \"t\" respectively and \"r\" is parameter equal to the annual increase (decrease in population). Then\n\nwhere \"var\"( \"r\" ) is the variance of \"r\".\n\nLet \"K\" be a measure of the species abundance (organisms per unit area). Then\n\nwhere T is the mean time to local extinction.\n\nThe probability of extinction by time \"t\" is\n\nIf a population is lognormally distributed then the harmonic mean of the population size (\"H\") is related to the arithmetic mean (\"m\")\n\nGiven that \"H\" must be > 0 for the population to persist then rearranging we have\n\nis the minimum size of population for the species to persist.\n\nThe assumption of a lognormal distribution appears to apply to about half of a sample of 544 species. suggesting that it is at least a plausible assumption.\n\nThe degree of precision (\"D\") is defined to be \"s\" / \"m\" where \"s\" is the standard deviation and \"m\" is the mean. The degree of precision is known as the coefficient of variation in other contexts. In ecology research it is recommended that \"D\" be in the range 10-25%. The desired degree of precision is important in estimating the required sample size where an investigator wishes to test if Taylor's law applies to the data. The required sample size has been estimated for a number of simple distributions but where the population distribution is not known or cannot be assumed more complex formulae may needed to determine the required sample size.\n\nWhere the population is Poisson distributed the sample size (\"n\") needed is\n\nwhere \"t\" is critical level of the t distribution for the type 1 error with the degrees of freedom that the mean (\"m\") was calculated with.\n\nIf the population is distributed as a negative binomial distribution then the required sample size is\n\nwhere \"k\" is the parameter of the negative binomial distribution.\n\nA more general sample size estimator has also been proposed\n\nwhere a and b are derived from Taylor's law.\n\nAn alternative has been proposed by Southwood\n\nwhere \"n\" is the required sample size, \"a\" and \"b\" are the Taylor's law coefficients and \"D\" is the desired degree of precision.\n\nKarandinos proposed two similar estimators for \"n\". The first was modified by Ruesink to incorporate Taylor's law.\n\nwhere \"d\" is the ratio of half the desired confidence interval (\"CI\") to the mean. In symbols\n\nThe second estimator is used in binomial (presence-absence) sampling. The desired sample size (\"n\") is\n\nformula_58\n\nwhere the \"d\" is ratio of half the desired confidence interval to the proportion of sample units with individuals, \"p\" is proportion of samples containing individuals and \"q\" = 1 - \"p\". In symbols\n\nFor binary (presence/absence) sampling, Schulthess \"et al\" modified Karandinos' equation\n\nformula_60\n\nwhere \"N\" is the required sample size, \"p\" is the proportion of units containing the organisms of interest, \"t\" is the chosen level of significance and \"D\" is a parameter derived from Taylor's law.\n\nSequential analysis is a method of statistical analysis where the sample size is not fixed in advance. Instead samples are taken in accordance with a predefined stopping rule. Taylor's law has been used to derive a number of stopping rules.\n\nA formula for fixed precision in serial sampling to test Taylor's law was derived by Green in 1970.\n\nwhere \"T\" is the cumulative sample total, \"D\" is the level of precision, \"n\" is the sample size and \"a\" and \"b\" are obtained from Taylor's law.\n\nAs an aid to pest control Wilson \"et al\" developed a test that incorporated a threshold level where action should be taken. The required sample size is\n\nwhere \"a\" and \"b\" are the Taylor coefficients, || is the absolute value, \"m\" is the sample mean, \"T\" is the threshold level and \"t\" is the critical level of the t distribution. The authors also provided a similar test for binomial (presence-absence) sampling\n\nwhere \"p\" is the probability of finding a sample with pests present and \"q\" = 1 - \"p\".\n\nGreen derived another sampling formula for sequential sampling based on Taylor's law\n\nwhere \"D\" is the degree of precision, \"a\" and \"b\" are the Taylor's law coefficients, \"n\" is the sample size and \"T\" is the total number of individuals sampled.\n\nSerra \"et al\" have proposed a stopping rule based on Taylor's law.\n\nformula_65\n\nwhere \"a\" and \"b\" are the parameters from Taylor's law, \"D\" is the desired level of precision and \"T\" is the total sample size.\n\nSerra \"et al\" also proposed a second stopping rule based on Iwoa's regression\n\nformula_66\n\nwhere \"α\" and \"β\" are the parameters of the regression line, \"D\" is the desired level of precision and \"T\" is the total sample size.\n\nThe authors recommended that \"D\" be set at 0.1 for studies of population dynamics and \"D\" = 0.25 for pest control.\n\nIt is considered to be good practice to estimate at least one additional analysis of aggregation (other than Taylor's law) because the use of only a single index may be misleading. Although a number of other methods for detecting relationships between the variance and mean in biological samples have been proposed, to date none have achieved the popularity of Taylor's law. The most popular analysis used in conjunction with Taylor's law is probably Iowa's Patchiness regression test but all the methods listed here have been used in the literature.\n\nBarlett in 1936 and later Iawo independently in 1968 both proposed an alternative relationship between the variance and the mean. In symbols\n\nwhere \"s\" is the variance in the \"i\"th sample and \"m\" is the mean of the \"i\"th sample\n\nWhen the population follows a negative binomial distribution, \"a\" = 1 and \"b\" = \"k\" (the exponent of the negative binomial distribution).\n\nThis alternative formulation has not been found to be as good a fit as Taylor's law in most studies.\n\nNachman proposed a relationship between the mean density and the proportion of samples with zero counts:\n\nwhere \"p\" is the proportion of the sample with zero counts, \"m\" is the mean density, \"a\" is a scale parameter and \"b\" is a dispersion parameter. If \"a\" = \"b\" = 0 the distribution is random. This relationship is usually tested in its logarithmic form\n\nAllsop used this relationship along with Taylor's law to derive an expression for the proportion of infested units in a sample\n\nwhere\n\nwhere \"D\" is the degree of precision desired, \"z\" is the upper α/2 of the normal distribution, \"a\" and \"b\" are the Taylor's law coefficients, \"c\" and \"d\" are the Nachman coefficients, \"n\" is the sample size and \"N\" is the number of infested units.\n\nBinary sampling is not uncommonly used in ecology. In 1958 Kono and Sugino derived an equation that relates the proportion of samples without individuals to the mean density of the samples.\n\nwhere \"p\" is the proportion of the sample with no individuals, \"m\" is the mean sample density, \"a\" and \"b\" are constants. Like Taylor's law this equation has been found to fit a variety of populations including ones that obey Taylor's law. Unlike the negative binomial distribution this model is independent of the mean density.\n\nThe derivation of this equation is straightforward. Let the proportion of empty units be \"p\" and assume that these are distributed exponentially. Then\n\nformula_74\n\nTaking logs twice and re arranging, we obtain the equation above. This model is the same as that proposed by Nachman.\n\nThe advantage of this model is that it does not require counting the individuals but rather their presence or absence. Counting individuals may not be possible in many cases particularly where insects are the matter of study.\n\n\nThe equation was derived while examining the relationship between the proportion \"P\" of a series of rice hills infested and the mean severity of infestation \"m\". The model studied was\n\nwhere \"a\" and \"b\" are empirical constants. Based on this model the constants \"a\" and \"b\" were derived and a table prepared relating the values of \"P\" and \"m\"\n\n\nThe predicted estimates of \"m\" from this equation are subject to bias and it is recommended that the adjusted mean ( \"m\" ) be used instead\n\nwhere var is the variance of the sample unit means \"m\" and \"m\" is the overall mean.\n\nAn alternative adjustment to the mean estimates is\n\nwhere MSE is the mean square error of the regression.\n\nThis model may also be used to estimate stop lines for enumerative (sequential) sampling. The variance of the estimated means is\n\nwhere\n\nwhere MSE is the mean square error of the regression, \"α\" and \"β\" are the constant and slope of the regression respectively, \"s\" is the variance of the slope of the regression, \"N\" is the number of points in the regression, \"n\" is the number of sample units and \"p\" is the mean value of \"p\" in the regression. The parameters \"a\" and \"b\" are estimated from Taylor's law:\n\nHughes and Madden have proposed testing a similar relationship applicable to binary observations in cluster, where each cluster contains from 0 to n individuals.\n\nwhere \"a\", \"b\" and \"c\" are constants, \"var\" is the observed variance, and p is the proportion of individuals with a trait (such as disease), an estimate of the probability of an individual with a trait. In logarithmic form, this relationship is\n\nIn most cases, it is assumed that \"b = c\", leading to a simple model\n\nThis relationship has been subjected to less extensive testing than Taylor's law. However, it has accurately described over 100 data sets, and there are no published examples reporting that it does not works.\n\nA variant of this equation was proposed by Shiyomi et al. () who suggested testing the regression\n\nwhere var is the variance, \"a\" and \"b\" are the constants of the regression, \"n\" here is the sample size (not sample per cluster) and \"p\" is the probability of a sample containing at least one individual.\n\nA negative binomial model has also been proposed. The dispersion parameter (\"k\") using the method of moments is \"m\" / ( \"s\" – \"m\" ) and \"p\" is the proportion of samples with counts > 0. The \"s\" used in the calculation of \"k\" are the values predicted by Taylor's law. \"p\" is plotted against 1 - ( \"k\" ( \"k\" + \"m\" ) ) and the fit of the data is visually inspected.\n\nPerry and Taylor have proposed an alternative estimator of \"k\" based on Taylor's law.\n\nA better estimate of the dispersion parameter can be made with the method of maximum likelihood. For the negative binomial it can be estimated from the equation\n\nwhere \"A\" is the total number of samples with more than \"x\" individuals, \"N\" is the total number of individuals, \"x\" is the number of individuals in a sample, \"m\" is the mean number of individuals per sample and \"k\" is the exponent. The value of \"k\" has to be estimated numerically.\n\nGoodness of fit of this model can be tested in a number of ways including using the chi square test. As these may be biased by small samples an alternative is the \"U\" statistic – the difference between the variance expected under the negative binomial distribution and that of the sample. The expected variance of this distribution is \"m\" + \"m\" / \"k\" and\n\nwhere \"s\" is the sample variance, \"m\" is the sample mean and \"k\" is the negative binomial parameter.\n\nThe variance of U is\n\nwhere \"p\" = \"m\" / \"k\", \"q\" = 1 + \"p\", \"R\" = \"p\" / \"q\" and \"N\" is the total number of individuals in the sample. The expected value of \"U\" is 0. For large sample sizes \"U\" is distributed normally.\n\nNote: The negative binomial is actually a family of distributions defined by the relation of the mean to the variance\n\nformula_91\n\nwhere \"a\" and \"p\" are constants. When \"a\" = 0 this defines the Poisson distribution. With \"p\" = 1 and \"p\" = 2, the distribution is known as the NB1 and NB2 distribution respectively.\n\nThis model is a version of that proposed earlier by Barlett.\n\nThe dispersion parameter (\"k\") is\n\nwhere \"m\" is the sample mean and \"s\" is the variance. If \"k\" is > 0 the population is considered to be aggregated; \"k\" = 0 the population is considered to be random; and if \"k\" is < 0 the population is considered to be uniformly distributed.\nSouthwood has recommended regressing \"k\" against the mean and a constant\n\nwhere \"k\" and \"m\" are the dispersion parameter and the mean of the ith sample respectively to test for the existence of a common dispersion parameter (\"k\"). A slope (\"b\") value significantly > 0 indicates the dependence of \"k\" on the mean density.\n\nAn alternative method was proposed by Elliot who suggested plotting ( \"s\" − \"m\" ) against ( \"m\" − \"s\" / \"n\" ). \"k\" is equal to 1/slope of this regression.\n\nThis coefficient (\"C\") is defined as\n\nformula_94\n\nIf the population can be assumed to be distributed in a negative binomial fashion, then \"C\" = 100 (1/\"k\") where \"k\" is the dispersion parameter of the distribution.\n\nThis index (\"I\") is defined as\n\nThe usual interpretation of this index is as follows: values of \"I\" < 1, 1, > 1 are taken to mean a uniform distribution, a random distribution or an aggregated distribution.\n\nBecause \"s\" = Σ x − (Σx), the index can also be written\n\nIf Taylor's law can be assumed to hold, then\n\nLloyd's index of mean crowding (\"IMC\") is the average number of other points contained in the sample unit that contains a randomly chosen point.\n\nwhere \"m\" is the sample mean and \"s\" is the variance.\n\nLloyd's index of patchiness (\"IP\") is\n\nIt is a measure of pattern intensity that is unaffected by thinning (random removal of points). This index was also proposed by Pielou in 1988 and is sometimes known by this name also.\n\nBecause an estimate of the variance of \"IP\" is extremely difficult to estimate from the formula itself, LLyod suggested fitting a negative binomial distribution to the data. This method gives a parameter \"k\"\n\nThen\n\nwhere \"SE\"(\"IP\") is the standard error of the index of patchiness,\"var\"(\"k\") is the variance of the parameter \"k\" and \"q\" is the number of quadrats sampled..\n\nIf the population obeys Taylor's law then\n\nIwao proposed a patchiness regression to test for clumping\n\nLet\n\n\"y\" here is Lloyd's index of mean crowding. Perform an ordinary least squares regression of \"m\" against \"y\".\n\nIn this regression the value of the slope (\"b\") is an indicator of clumping: the slope = 1 if the data is Poisson-distributed. The constant (\"a\") is the number of individuals that share a unit of habitat at infinitesimal density and may be < 0, 0 or > 0. These values represent regularity, randomness and aggregation of populations in spatial patterns respectively. A value of \"a\" < 1 is taken to mean that the basic unit of the distribution is a single individual.\n\nWhere the statistic \"s\" / \"m\" is not constant it has been recommended to use instead to regress Lloyd's index against \"am\" + \"bm\" where \"a\" and \"b\" are constants.\n\nThe sample size (\"n\") for a given degree of precision (\"D\") for this regression is given by\n\nwhere \"a\" is the constant in this regression, \"b\" is the slope, \"m\" is the mean and \"t\" is the critical value of the t distribution.\n\nIawo has proposed a sequential sampling test based on this regression. The upper and lower limits of this test are based on critical densities m where control of a pest requires action to be taken.\n\nwhere \"N\" and \"N\" are the upper and lower bounds respectively, \"a\" is the constant from the regression, \"b\" is the slope and \"i\" is the number of samples.\n\nKuno has proposed an alternative sequential stopping test also based on this regression.\n\nwhere \"T\" is the total sample size, \"D\" is the degree of precision, \"n\" is the number of samples units, a is the constant and b is the slope from the regression respectively.\n\nKuno's test is subject to the condition that \"n\" ≥ (\"b\" − 1) / \"D\"\n\nParrella and Jones have proposed an alternative but related stop line\n\nwhere \"a\" and \"b\" are the parameters from the regression, \"N\" is the maximum number of sampled units and \"n\" is the individual sample size.\n\nMorisita's index of dispersion ( \"I\" ) is the scaled probability that two points chosen at random from the whole population are in the same sample. Higher values indicate a more clumped distribution.\n\nAn alternative formulation is\n\nwhere \"n\" is the total sample size, \"m\" is the sample mean and \"x\" are the individual values with the sum taken over the whole sample.\nIt is also equal to\n\nwhere \"IMC\" is Lloyd's index of crowding.\n\nThis index is relatively independent of the population density but is affected by the sample size. Values > 1 indicate clumping; values < 1 indicate a uniformity of distribution and a value of 1 indicates a random sample.\n\nMorisita showed that the statistic\n\nis distributed as a chi squared variable with \"n\" − 1 degrees of freedom.\n\nAn alternative significance test for this index has been developed for large samples.\n\nwhere \"m\" is the overall sample mean, \"n\" is the number of sample units and \"z\" is the normal distribution abscissa. Significance is tested by comparing the value of \"z\" against the values of the normal distribution.\n\nA function for its calculation is available in the statistical R language. R function\n\nSmith-Gill developed a statistic based on Morisita's index which is independent of both sample size and population density and bounded by −1 and +1. This statistic is calculated as follows\n\nFirst determine Morisita's index ( \"I\" ) in the usual fashion. Then let \"k\" be the number of units the population was sampled from. Calculate the two critical values\n\nwhere χ is the chi square value for \"n\" − 1 degrees of freedom at the 97.5% and 2.5% levels of confidence.\n\nThe standardised index ( \"I\" ) is then calculated from one of the formulae below\n\nWhen \"I\" ≥ \"M\" > 1\n\nWhen \"M\" > \"I\" ≥ 1\n\nWhen 1 > \"I\" ≥ \"M\"\n\nWhen 1 > \"M\" > \"I\"\n\n\"I\" ranges between +1 and −1 with 95% confidence intervals of ±0.5. \"I\" has the value of 0 if the pattern is random; if the pattern is uniform, \"I\" < 0 and if the pattern shows aggregation, \"I\" > 0.\n\nSouthwood's index of spatial aggregation (\"k\") is defined as\n\nwhere \"m\" is the mean of the sample and \"m\"* is Lloyd's index of crowding.\n\nFisher's index of dispersion is\n\nThis index may be used to test for over dispersion of the population. It is recommended that in applications n > 5 and that the sample total divided by the number of samples is > 3. In symbols\n\nwhere \"x\" is an individual sample value. The expectation of the index is equal to \"n\" and it is distributed as the chi-square distribution with \"n\" − 1 degrees of freedom when the population is Poisson distributed. It is equal to the scale parameter when the population obeys the gamma distribution.\n\nIt can be applied both to the overall population and to the individual areas sampled individually. The use of this test on the individual sample areas should also include the use of a Bonferroni correction factor.\n\nIf the population obeys Taylor's law then\n\nThe index of cluster size (\"ICS\") was created by David and Moore. Under a random (Poisson) distribution \"ICS\" is expected to equal 0. Positive values indicate a clumped distribution; negative values indicate a uniform distribution.\n\nwhere \"s\" is the variance and \"m\" is the mean.\n\nIf the population obeys Taylor's law\n\nThe \"ICS\" is also equal to Katz's test statistic divided by ( \"n\" / 2 ) where \"n\" is the sample size. It is also related to Clapham's test statistic. It is also sometimes referred to as the clumping index.\n\nGreen's index (\"GI\") is a modification of the index of cluster size that is independent of \"n\" the number of sample units.\n\nThis index equals 0 if the distribution is random, 1 if it is maximally aggregated and −1 / ( \"nm\" − 1 ) if it is uniform.\n\nThe distribution of Green's index is not currently known so statistical tests have been difficult to devise for it.\n\nIf the population obeys Taylor's law\n\nBinary sampling (presence/absence) is frequently used where it is difficult to obtain accurate counts. The dispersal index (\"D\") is used when the study population is divided into a series of equal samples ( number of units = \"N\": number of units per sample = \"n\": total population size = \"n\" x \"N\" ). The theoretical variance of a sample from a population with a binomial distribution is\n\nwhere \"s\" is the variance, \"n\" is the number of units sampled and \"p\" is the mean proportion of sampling units with at least one individual present. The dispersal index (\"D\") is defined as the ratio of observed variance to the expected variance. In symbols\n\nwhere \"var\" is the observed variance and \"var\" is the expected variance. The expected variance is calculated with the overall mean of the population. Values of \"D\" > 1 are considered to suggest aggregation. \"D\"( \"n\" − 1 ) is distributed as the chi squared variable with \"n\" - 1 degrees of freedom where \"n\" is the number of units sampled.\n\nAn alternative test is the \"C\" test.\n\nwhere \"D\" is the dispersal index, \"n\" is the number of units per sample and \"N\" is the number of samples. C is distributed normally. A statistically significant value of C indicates overdispersion of the population.\n\n\"D\" is also related to intraclass correlation ( ρ ) which is defined as\n\nwhere \"T\" is the number of organisms per sample, \"p\" is the likelihood of the organism having the sought after property (diseased, pest free, \"etc\"), and x is the number of organism in the \"i\"th unit with this property. \"T\" must be the same for all sampled units. In this case with \"n\" constant\n\nIf the data can be fitted with a beta-binomial distribution then\n\nwhere \"θ\" is the parameter of the distribution.\n\nMa has proposed a parameter (\"m\") − the population aggregation critical density - to relate population density to Taylor's law.\n\nA number of statistical tests are known that may be of use in applications.\n\nA related statistic suggested by de Oliveria is the difference of the variance and the mean. If the population is Poisson distributed then\n\nwhere \"t\" is the Poisson parameter, \"s\" is the variance, \"m\" is the mean and \"n\" is the sample size. The expected value of \"s\" - \"m\" is zero. This statistic is distributed normally.\n\nIf the Poisson parameter in this equation is estimated by putting \"t\" = \"m\", after a little manipulation this statistic can be written\n\nThis is almost identical to Katz's statistic with ( \"n\" - 1 ) replacing \"n\". Again \"O\" is normally distributed with mean 0 and unit variance for large \"n\". This statistic is the same as the Neyman-Scott statistic.\n\n\nde Oliveria actually suggested that the variance of \"s\" - \"m\" was ( 1 - 2\"t\" + 3\"t\" ) / \"n\" where \"t\" is the Poisson parameter. He suggested that \"t\" could be estimated by putting it equal to the mean (\"m\") of the sample. Further investigation by Bohning showed that this estimate of the variance was incorrect. Bohning's correction is given in the equations above.\n\nIn 1936 Clapham proposed using the ratio of the variance to the mean as a test statistic (the relative variance). In symbols\n\nFor a Possion distribution this ratio equals 1. To test for deviations from this value he proposed testing its value against the chi square distribution with \"n\" degrees of freedom where \"n\" is the number of sample units. The distribution of this statistic was studied further by Blackman who noted that it was approximately normally distributed with a mean of 1 and a variance ( \"V\" ) of\n\nThe derivation of the variance was re analysed by Bartlett who considered it to be\n\nFor large samples these two formulae are in approximate agreement. This test is related to the later Katz's \"J\" statistic.\n\nIf the population obeys Taylor's law then\n\n\nA refinement on this test has also been published These authors noted that this test tends to detect overdispersion at higher scales even when this was not present in the data. They noted that the use of the multinomial distribution may be more appropriate than the use of a Poisson distribution for such data. The statistic \"θ\" is distributed\n\nwhere \"N\" is the number of sample units, \"n\" is the total number of samples examined and \"x\" are the individual data values.\n\nThe expectation and variance of \"θ\" are\n\nFor large \"N\" \"E\"(θ) is approximately 1 and\n\nIf the number of individuals sampled ( \"n\" ) is large this estimate of the variance is in agreement with those derived earlier. However, for smaller samples these latter estimates are more precise and should be used.\n\n"}
