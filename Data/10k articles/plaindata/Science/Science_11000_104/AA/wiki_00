{"id": "20936835", "url": "https://en.wikipedia.org/wiki?curid=20936835", "title": "2MASX J00482185-2507365 occulting pair", "text": "2MASX J00482185-2507365 occulting pair\n\nThe 2MASX J00482185-2507365 occulting pair is a pair of overlapping spiral galaxies found in the vicinity of NGC 253, the \"Sculptor Galaxy\". Both galaxies are more distant than NGC 253, with the background galaxy, 2MASX J00482185-2507365 (PGC 198197), lying at redshift z=0.06, about 800 million light-years from Earth, and the foreground galaxy lying between NGC 253 and the background galaxy (0.0008 < z < 0.06).\n\nThis pair of galaxies illuminates the distribution of galactic dust beyond the visible arms of a spiral galaxy. The heretofore unexpected extent of dust beyond the starry limits of the arms shows new areas for extragalactic astronomical study. The dusty arms extend 6 times the radii of the starry arms of the galaxy, and are shown silhouetted in HST images against the central and core sections of the background galaxy.\n\n\n\n"}
{"id": "20927130", "url": "https://en.wikipedia.org/wiki?curid=20927130", "title": "6th meridian east", "text": "6th meridian east\n\nThe meridian 6° east of Greenwich is a line of longitude that extends from the North Pole across the Arctic Ocean, Europe, Africa, the Atlantic Ocean, the Southern Ocean, and Antarctica to the South Pole.\n\nThe 6th meridian east forms a great circle with the 174th meridian west.\n\nStarting at the North Pole and heading south to the South Pole, the 6th meridian east passes through:\n\n"}
{"id": "32015265", "url": "https://en.wikipedia.org/wiki?curid=32015265", "title": "An Irish Astronomical Tract", "text": "An Irish Astronomical Tract\n\nAn Irish Astronomical Tract, medieval Irish text and manuscript.\n\n\"An Irish Astronomical Tract\" is the title given to a medieval Irish text, created in the first half of the 14th century. It was written in Early Modern Irish, and is a translation of \"De Scientia Motus Orbis\" by Masha'allah ibn Atharī (c.740–815 AD).\n\nThe name of its scribe and place of composition is unknown.\n\n\n\n\n"}
{"id": "56137180", "url": "https://en.wikipedia.org/wiki?curid=56137180", "title": "Apollo-Soyuz (cigarette)", "text": "Apollo-Soyuz (cigarette)\n\n\"Apollo-Soyuz\" was a Soviet brand of cigarettes which were manufactured by the \"Java Tobacco Factory\" in Moscow, Soviet Union and Philip Morris USA in the United States. Today it is owned and manufactured by the Krasnodar Tobacco Factory, a Russian subsidiary of Altria.\n\nTo honor the joint Soviet-American flight of the Soyuz-Apollo, a series of postage stamps and envelopes with the stamp first day covers, postcards, photo albums, as well as cigarettes under the name \"Apollo-Soyuz\" with the famous Virginia tobacco from the American company Philip Morris were issued to be made.\n\nThe brand was introduced for the occasion of the joint Soviet-American Apollo–Soyuz Test Project mission in July 1975.\n\nThe cigarettes went on sale in the Soviet Union on July 15, 1975, the day the Soyuz was launched, and later in the United States. The cigarettes were manufactured for 5 years, during this time the Moscow factory released an order of 500 million cigarettes, although it was difficult to actually acquire a pack due to a trade deficit in the USSR. In the astronauts city Leninsk (nowadays called Baikonur) the cigarettes could be bought freely at 1.5 Soviet rubles per pack (at the time this was considered quite expensive). In other cities of the USSR, the cigarettes were instantly bought and resold by speculators. The cigarettes were popular in the Soviet Union, a total of 3 billion were exported there. In the United States however, the brand found little appeal, mainly because the brand was expensive.\n\nManufacturing of the brand was discontinued in 1980 upon expiry of the term of the license agreement between Philip Morris and Glavtabakom.\n\nCigarette manufacturing resumed in 1995 by the Krasnodar Tobacco Factory, owned by Philip Morris. In 1996 the factory was arranged in a full technological cycle of production, and since then the cigarettes are manufactured there.\n\nOn July 17, 1998 the production of a new cigarette called \"Apollo Alliance Special\", which were more expensive than the original, was started. On April 1 of 2000, production began of the \"Apollo Alliance Special Lights Bookstore\" cigarettes (retail price per pack 9-10 Russian rubles), who were claimed to be a low-tar and nicotine cigarettes (8 mg tar and 0.6 mg nicotine per cigarette).\n\nThe cigarette pack features a blue circle with a rendering of the docked Apollo-Soyuz spacecraft inside it, and bears English writing in blue and Russian in red. On one side it says: \"Apollo Soyuz commemorative brand,\" and on the other \"Soyuz Apollo\". On one edge of the pack are the words, \"Developed by Philip Morris Inc. U.S.A. and Glavtabak, U.S.S.R. in commemoration of U.S./Soviet space cooperation\" in English and Russian, along with the English words, \"Made in U.S.S.R., Tava Factory, Moscow.\" Glavtabak was the Soviet State Tobacco Agency. The other edge displays the U.S. Surgeon General's warning that cigarettes are dangerous to your health.\n\nApollo-Soyuz was only sold in the Soviet Union and the United States, but since its re-introduction it is still sold in Russia.\n\nApollo-Soyuz cigarettes appeared in the game Escape from Tarkov where the brand is called \"Apollon Soyuz\".\n\n"}
{"id": "13629713", "url": "https://en.wikipedia.org/wiki?curid=13629713", "title": "Applicability domain", "text": "Applicability domain\n\nThe applicability domain (AD) of a QSAR model is the physico-chemical, structural or biological space, knowledge or information on which the training set of the model has been developed, and for which it is applicable to make predictions for new compounds.\n\nThe purpose of AD is to state whether the model's assumptions are met and for which chemicals the model can be reliably applicable. In general, this is the case for interpolation rather than for extrapolation. Up to now there is no single generally accepted algorithm for determining the AD: a comprehensive survey can be found in a Report and Recommendations of ECVAM Workshop 52. There exists a rather systematic approach for defining interpolation regions. The process involves the removal of outliers and a probability density distribution method using kernel-weighted sampling. \nAnother widely used approach for the structural AD of the regression QSAR models is based on the leverage calculated from the diagonal values of the hat matrix of the modeling molecular descriptors. \nA recent rigorous benchmarking study of several AD algorithms identified standard-deviation of model predictions as the most reliable approach.\nTo investigate the AD of a training set of chemicals one can directly analyse properties of the multivariate descriptor space of the training compounds or more indirectly via distance (or similarity) metrics. When using distance metrics care should be taken to use an orthogonal and significant vector space. This can be achieved by different means of feature selection and successive principal components analysis.\n"}
{"id": "1464076", "url": "https://en.wikipedia.org/wiki?curid=1464076", "title": "Association for Information Science and Technology", "text": "Association for Information Science and Technology\n\nThe Association for Information Science and Technology (ASIS&T) is a non-profit membership organization for information professionals. Previously known as the American Society for Information Science and Technology (2000–2013), the organization sponsors an annual conference as well as several serial publications, including the \"Journal of the Association for Information Science and Technology\" (JASIST). The organization provides administration and communications support for its various divisions, known as special-interest groups or SIGs; provides administration for geographically defined chapters; connects job seekers with potential employers; and provides organizational support for continuing education programs for information professionals.\n\nWatson Davis formed the \"Documentation Institute\" in 1935, which changed its name to the American Documentation Institute on 13 March 1937 with the collaboration of Atherton Seidell and others. The organization was first concerned with microfilm and its role as a vehicle for the dissemination of information.\n\nADI early on worked toward the development of microfilm readers and cameras. Their first microfilm laboratories were located in the U.S.Department of Agriculture Library in Washington, DC and the Institute distributed materials through the newly created Bibliofilm Service.\n\nADI established the Auxiliary Publication Program, which during its 30-year history released nearly 10,000 documents covering a wide range of subjects. The program enabled authors in the fields of physical, natural, social, historical and information sciences to publish and distribute research papers that were either too long, typographically complex or expensive to be published in journals using existing technology. In 1954, the Photoduplication Service at the Library of Congress took over the operation and became the source point for distributing ADI materials and in 2009 this material found its home in the Library's Technical Reports and Standards Unit.\n\n1950s was the transition to Modern Information Science:In 1952 the Bylaws were amended to allow individuals to become members due to the number of people that were engaged in the development of new principles and techniques. The goal was to make ADI a group that was concerned with all elements and problems of information science not just libraries. During this time there were increased interests and developments of automatic devices for searching, storage and retrieval.\n\nIn January 1968, ADI became the American Society for Information Science. The change was made to represent the organization’s interest in \"all aspects of the information transfer process\" such as, \"designing, managing and using information systems and technology.\" In 2000 the organization updated its name, adding \"Technology\" to embrace the prevalence and increasing centrality of online databases and other technical aspects of the information profession. In 2013 the organization was renamed as the Association for Information Science and Technology (ASIS&T) to reflect its international membership. Today, the organization comprises professionals from various fields including engineering, linguistics, librarianship, education, chemistry, computer science and medicine. The members share \"a common interest in improving the ways society stores, retrieves, analyzes, manages, archives and disseminates information \".\n\n1970s was The Move to Online Information: During this time many institutions are making the move from batch processing to online modes, from mainframe computers to more modern computers. With the advancement of technology the traditional boundaries began to fade and library schools started to add \"information\" in the titles of their programs. ASIS sponsored a bicentennial conference which focused on the role of information in the country's development. The group also participated in the planning and implementation of the White House Conference on Library and Information Services. \".\n\n1980s brings is the beginning of the popularity of Personal Computers: This is the first time individuals can access large databases, such as Grateful Med at the National Library of Medicine, and user-oriented services such as Dialog and Compuserve on their personal computers. ASIS created groups on office information, personal computers, international information issues and rural information services in response to the changing environment. Eventually other groups were created, such as: non-print media, social sciences, energy and the environment, and community information systems. ASIS also added its first non-North American Chapters. \".\n\nASIS&T Today: The group continues to grow and change. In the 2000s the group's name changes twice; first in 2000 to American Society for Information Science and Technology (ASIS&T)in order to reflect the range of its members; then in 2013 to Association for Information Science and Technology (ASIS&T) to reflect its growing international membership. ASIS&T is involved in the forefront of in examining the technical bases, social consequences, and theoretical understanding of online databases. They also study the effects of widespread use of databases in government, industry, and education, and the development of information databases on the Internet and World Wide Web \".\n\nIn a world where \"information is of central importance to personal, social, political, and economic progress\", ASIS&T seeks to advance the information sciences and information technology by providing focus, opportunity, and support to information professionals and information organizations. ASIS&T seeks to advance knowledge \"about information, its creation, properties, and use\" as well as increase \"public awareness of the information sciences and technologies and their benefits to society.\"\n\nTo establish an information professionalism in the world by: Advancing knowledge about information; Providing analysis of ideas;; Valuing theory, research, applications, and service; Nurturing new perspectives, interests, and ideas; Increasing public awareness of the information sciences and technologies and their benefits to society.\"\n\nOriginally membership was based on representatives nominated by scientific societies, professional associations, foundations, and government agencies. Changes made to the bylaws in 1952 opened the organization to any individual with interest in the dissemination of information. Today, fee-based memberships can be either individual or institutional, with no formal requirements to join as an individual. Similar to most organizations of its kind, ASIS&T offers benefits to its members in the form of subscriptions to publications, access to job assistance services (JobLine); and discounts to ASIS&T-sponsored events.\n\nIn 1966, ADI began publication of the Annual Review of Information Science and Technology. Its successor organizations continued publishing the annual review under that title until 2011.\n\nADI decided in 1950 to create a journal modeled after the defunct \"Journal of Documentary Reproduction\", which had been published by the American Library Association from 1938 to 1942. ADI published the journal \"American Documentation\". from 1950 until 1968, when ADI changed its name as an organization and renamed \"American Documentation\" as the \"Journal of the American Society for Information Science\". In 2000, the society changed names again, and the journal followed suit, becoming the current Journal of the American Society for Information Science and Technology.\n\n\nhttps://www.asist.org/about/\n\n\n"}
{"id": "3392104", "url": "https://en.wikipedia.org/wiki?curid=3392104", "title": "Barnard (Martian crater)", "text": "Barnard (Martian crater)\n\nBarnard is a crater on Mars named after astronomer Edward Emerson Barnard.\n\n\n"}
{"id": "49887485", "url": "https://en.wikipedia.org/wiki?curid=49887485", "title": "Boaty McBoatface", "text": "Boaty McBoatface\n\nBoaty McBoatface is the lead boat of the \"Autosub Long Range\"-class of autonomous underwater vehicles (AUVs) used for scientific research that will be carried on the research vessel RRS \"Sir David Attenborough\" owned by the Natural Environment Research Council (NERC) and operated by the British Antarctic Survey (BAS). Because of its complexity and its extended range, NERC is classing it as an autosub long range autonomous vehicle.\n\nAlthough the name \"Boaty McBoatface\" was originally proposed in a March 2016 online poll to name the ship, it was eventually named RRS \"Sir David Attenborough\".\n\nFormer BBC Radio Jersey presenter James Hand jokingly suggested Boaty McBoatface, a name the public liked and that quickly became the most popular choice. The name has been described as a homage to Hooty McOwlface, an owl named through an \"Adopt-A-Bird\" programme in 2012 that became popular on the internet.\n\nAlthough \"Boaty McBoatface\" was the most popular suggestion in the #NameOurShip poll, the suggestion to use the name for the mother ship was not followed, and the Minister for Universities and Science, Jo Johnson, announced that the \"Boaty McBoatface\" name would be used for one of the submersibles aboard \"Sir David Attenborough\" instead.\n\nThe results of the poll inspired similar results in other naming polls. These incidents suggest there is still a clear deficit in understanding how to engage crowds and avoid failures and demonstrate the backlash from NERC’s neglecting the crowd’s choice, which resulted in a sophisticated reaction in other campaigns. This highlights the importance of meaningful engagement with the crowds. The literature suggests, however, that when the public are meaningfully engaged, they feel valued and provide more productive inputs.\n\nThe boat underwent advanced sea trials in 2016. Its maiden voyage was on 3 April 2017, to research how Antarctic Bottom Water leaves the Weddell Sea and enters the Southern Ocean through Orkney Passage, south of Chile. An animation showing the 3rd mission of Boaty McBoatface from that research expedition can be seen at https://vimeo.com/217386697.\nBjorn Baker's team at Sydney's Warwick Farm Racecourse caught wind of the crowdsourced name Boaty McBoatface and decided that they would pay homage to them by naming their new racehorse \"Horsey McHorseface\". The news of \"Horsey McHorseface\" broke out on social media after his win at the Lathan Arthur Thompson Memorial Maiden in Cessnock, Australia. \"Horsey McHorseface\" was put to auction and sold for $17,325, but in 2017 was euthanized due to bone disease. \n\nSwedish transport company MTR Express conducted an online poll, not long after the one involving \"Boaty McBoatface\", to name a new train on the Stockholm-Gothenburg line. \"Trainy McTrainface\" won the poll, and the train was named accordingly.\n\nSydney Ferries allowed the public to name its fleet of Emerald-class ferries through a naming competition. The most popular name was \"Boaty McBoatface\" but, as it had already been taken, the judges opted to go instead for the second-place choice, and one of the ferries was thus named \"Ferry McFerryface\". After the Maritime Union of Australia refused to crew the vessel in protest at the name, it entered service named \"Emerald 6\" with a \"Ferry McFerryface\" sticker below the bridge. It later emerged that the name Ferry McFerryface had received fewer than 200 votes in the poll; it had been selected by the New South Wales Transport Minister out of his personal preference for the name, which was subsequently changed.\n\nMegabus' United Kingdom operation hosted a Twitter poll in late 2017 to name some of their brand-new Plaxton Elite bodied Volvo B11RT interdeck coaches. \"Mega McMegaface\" won, and the name was applied to one of the vehicles.\n\nIn March 2017 the Isle of Wight Council, which operates the Cowes Floating Bridge (a chain ferry across the Medina between Cowes and East Cowes), stated it was open to suggestions from residents for a new name for the vessel after originally registering it as Floating Bridge No.6. Despite council officials ruling out \"Floaty McFloatface\" as a name, a petition was later created to name the vessel Floaty McFloatface, attracting over 2,000 signatures and even caused the council to rescind its decision to veto the name.\n\nIn 2016 Google released a grammar parsing software package, which they named Parsey McParseface.\n\n"}
{"id": "21177379", "url": "https://en.wikipedia.org/wiki?curid=21177379", "title": "CHARISSA", "text": "CHARISSA\n\nCHARISSA (derived from 'CHARged particle Instrumentation for a Solid State Array') is a nuclear structure research collaboration originally conceived, initiated and partially built by Dr. William Rae of the University of Oxford (retired) and now run by the School of Physics and Astronomy at the University of Birmingham, UK. The other members of the collaboration are the University of Surrey with occasional contributions from LPC CAEN and Ruđer Bošković Institute, Zagreb. The collaboration is funded by the Science and Technology Facilities Council (STFC).\n\nThe CHARISSA collaboration carries out experiments at many of the world's leading research centres. Due to the nature of the research experiments performed must be undertaken with the use of a particle accelerator and complex detection systems. The group probes the structure of nuclei.\n\nExperiments are currently being carried out utilising the following facilities:\n\n\nPrevious experiments have taken place using the following accelerators at their respective facilities:\n\n\n\n"}
{"id": "13803804", "url": "https://en.wikipedia.org/wiki?curid=13803804", "title": "Calculation of glass properties", "text": "Calculation of glass properties\n\nThe calculation of glass properties (glass modeling) is used to predict glass properties of interest or glass behavior under certain conditions (e.g., during production) without experimental investigation, based on past data and experience, with the intention to save time, material, financial, and environmental resources, or to gain scientific insight. It was first practised at the end of the 19th century by A. Winkelmann and O. Schott. The combination of several glass models together with other relevant functions can be used for optimization and six sigma procedures. In the form of statistical analysis glass modeling can aid with accreditation of new data, experimental procedures, and measurement institutions (glass laboratories).\n\nHistorically, the calculation of glass properties is directly related to the founding of glass science. At the end of the 19th century the physicist Ernst Abbe developed equations that allow calculating the design of optimized optical microscopes in Jena, Germany, stimulated by co-operation with the optical workshop of Carl Zeiss. Before Ernst Abbe's time the building of microscopes was mainly a work of art and experienced craftsmanship, resulting in very expensive optical microscopes with variable quality. Now Ernst Abbe knew exactly how to construct an excellent microscope, but unfortunately, the required lenses and prisms with specific ratios of refractive index and dispersion did not exist. Ernst Abbe was not able to find answers to his needs from glass artists and engineers; glass making was not based on science at this time.\n\nIn 1879 the young glass engineer Otto Schott sent Abbe glass samples with a special composition (lithium silicate glass) that he had prepared himself and that he hoped to show special optical properties. Following measurements by Ernst Abbe, Schott's glass samples did not have the desired properties, and they were also not as homogeneous as desired. Nevertheless, Ernst Abbe invited Otto Schott to work on the problem further and to evaluate all possible glass components systematically. Finally, Schott succeeded in producing homogeneous glass samples, and he invented borosilicate glass with the optical properties Abbe needed. These inventions gave rise to the well-known companies Zeiss and Schott Glass (see also Timeline of microscope technology). Systematic glass research was born. In 1908, Eugene Sullivan founded glass research also in the United States (Corning, New York).\n\nAt the beginning of glass research it was most important to know the relation between the glass composition and its properties. For this purpose Otto Schott introduced the additivity principle in several publications for calculation of glass properties. This principle implies that the relation between the glass composition and a specific property is linear to all glass component concentrations, assuming an ideal mixture, with \"C\" and \"b\" representing specific glass component concentrations and related coefficients respectively in the equation below. The additivity principle is a simplification and only valid within narrow composition ranges as seen in the displayed diagrams for the refractive index and the viscosity. Nevertheless, the application of the additivity principle lead the way to many of Schott’s inventions, including optical glasses, glasses with low thermal expansion for cooking and laboratory ware (Duran), and glasses with reduced freezing point depression for mercury thermometers. Subsequently, English and Gehlhoff \"et al.\" published similar additive glass property calculation models. Schott’s additivity principle is still widely in use today in glass research and technology.\n\nSchott and many scientists and engineers afterwards applied the additivity principle to experimental data measured in their own laboratory within sufficiently narrow composition ranges (local glass models). This is most convenient because disagreements between laboratories and non-linear glass component interactions do not need to be considered. In the course of several decades of systematic glass research thousands of glass compositions were studied, resulting in millions of published glass properties, collected in glass databases. This huge pool of experimental data was not investigated as a whole, until Bottinga, Kucuk, Priven, Choudhary, Mazurin, and Fluegel published their global glass models, using various approaches. In contrast to the models by Schott the global models consider many independent data sources, making the model estimates more reliable. In addition, global models can reveal and quantify \"non-additive\" influences of certain glass component combinations on the properties, such as the \"mixed-alkali effect\" as seen in the adjacent diagram, or the \"boron anomaly\". Global models also reflect interesting developments of glass property measurement accuracy, e.g., a decreasing accuracy of experimental data in modern scientific literature for some glass properties, shown in the diagram. They can be used for accreditation of new data, experimental procedures, and measurement institutions (glass laboratories). In the following sections (except melting enthalpy) \"empirical\" modeling techniques are presented, which seem to be a successful way for handling huge amounts of experimental data. The resulting models are applied in contemporary engineering and research for the calculation of glass properties.\n\nNon-empirical (\"deductive\") glass models exist. They are often not created to obtain reliable glass property predictions in the first place (except melting enthalpy), but to establish relations among several properties (e.g. atomic radius, atomic mass, chemical bond strength and angles, chemical valency, heat capacity) to gain scientific insight. In future, the investigation of property relations in deductive models may ultimately lead to reliable predictions for all desired properties, provided the property relations are well understood and all required experimental data are available.\n\nGlass properties and glass behavior during production can be calculated through statistical analysis of glass databases such as GE-SYSTEM\nSciGlass and Interglad, \nsometimes combined with the finite element method. For estimating the melting enthalpy thermodynamic databases are used.\n\nIf the desired glass property is not related to crystallization (e.g., liquidus temperature) or phase separation, linear regression can be applied using common polynomial functions up to the third degree. Below is an example equation of the second degree. The \"C\"-values are the glass component concentrations like NaO or CaO in percent or other fractions, the \"b\"-values are coefficients, and \"n\" is the total number of glass components. The glass main component silica (SiO) is excluded in the equation below because of over-parametrization due to the constraint that all components sum up to 100%. Many terms in the equation below can be neglected based on correlation and significance analysis. Systematic errors such as seen in the picture are quantified by dummy variables. Further details and examples are available in an online tutorial by Fluegel.\n\nThe liquidus temperature has been modeled by non-linear regression using neural networks and disconnected peak functions. The disconnected peak functions approach is based on the observation that within one primary crystalline phase field linear regression can be applied and at eutectic points sudden changes occur.\n\nThe glass melting enthalpy reflects the amount of energy needed to convert the mix of raw materials (batch) to a melt glass. It depends on the batch and glass compositions, on the efficiency of the furnace and heat regeneration systems, the average residence time of the glass in the furnace, and many other factors. A pioneering article about the subject was written by Carl Kröger in 1953.\n\nFor modeling of the glass flow in a glass melting furnace the finite element method is applied commercially, based on data or models for viscosity, density, thermal conductivity, heat capacity, absorption spectra, and other relevant properties of the glass melt. The finite element method may also be applied to glass forming processes.\n\nIt is often required to optimize several glass properties simultaneously, including production costs.\n\nIt is possible to weight the desired properties differently. Basic information about the principle can be found in an article by Huff \"et al.\" The combination of several glass models together with further relevant technological and financial functions can be used in six sigma optimization.\n\n"}
{"id": "32661167", "url": "https://en.wikipedia.org/wiki?curid=32661167", "title": "Canfield ocean", "text": "Canfield ocean\n\nThe Canfield Ocean model was proposed by geochemist Donald Canfield to explain the composition of the ocean in the middle to late Proterozoic. His theory has been coined the 'Canfield Ocean' and remains one of the cornerstone theories of ocean oxygen composition during that time.\n\nIn a seminal paper published in 1998 in \"Nature\", Canfield argued that the ocean was anoxic and sulfidic during the time of the Boring Billion, and that those conditions affected the mineral deposition of iron-rich Banded iron formations (BIF). Prior to the Canfield Ocean theory, it was believed that the ocean became oxygenated during the Great Oxygenation Event. The presence of oxygen in the deep ocean made the formation of BIF impossible, which is seen in ocean sediment records. Conversely, the Canfield Ocean theory postulates that deep ocean water remained anoxic long after the Great Oxidation Event, and he argued that the euxinic conditions in the deep ocean ceased the deposition of BIF in ocean sediments.\n\nEuxinic describes anoxic conditions in the presence of hydrogen sulfide. Euxinic ocean conditions, a term describing restricted hydrologic circulation that lead to stagnant or anaerobic conditions, are the likely factor leading to sulfidic oceans.\n\n"}
{"id": "14506504", "url": "https://en.wikipedia.org/wiki?curid=14506504", "title": "Carter bonds", "text": "Carter bonds\n\nCarter bonds are a series of Treasury bonds issued by the United States in 1978 to prevent a fall of the US Dollar. The name comes from the US President Jimmy Carter. It was enacted under the Exchange Stabilization Fund.\n\nIn contrast to typical Treasury bonds, which are denominated in US dollars and so do not expose the US Government to currency risk, Carter bonds were denominated in West German Deutschmarks and Swiss Francs.\n"}
{"id": "51040597", "url": "https://en.wikipedia.org/wiki?curid=51040597", "title": "Coleman Townsend Robinson", "text": "Coleman Townsend Robinson\n\nColeman Townsend Robinson (12 January 1838 – 1 May 1872) was an American entomologist who specialised in Lepidoptera.\n\nHe wrote Grote, A.R., & Robinson, C.T. 1867–1868. Descriptions of American Lepidoptera – Nos 1–3. \n\"Transactions of the American Entomological Society\" 1(1): 1–30; (2): 171–192, pl. 4; (4): 323–360, pl. 6, pl. 7 with Augustus Radcliffe Grote.\n\n"}
{"id": "15783562", "url": "https://en.wikipedia.org/wiki?curid=15783562", "title": "Conoscopy", "text": "Conoscopy\n\nConoscopy (from Ancient Greek \"κῶνος\" (konos) \"cone, spinning top, pine cone\" and \"σκοπέω\" (skopeo) \"examine, inspect, look to or into, consider\") is an optical technique to make observations of a transparent specimen in a cone of converging rays of light. The various directions of light propagation are observable simultaneously .\n\nA conoscope is an apparatus to carry out \"conoscopic observations\" and measurements, often realized by a microscope with a Bertrand lens for observation of the \"direction's image\". The earliest reference to the use of \"conoscopy\" (i.e., observation in convergent light with a polarization microscope with a Bertrand lens) for evaluation of the optical properties of liquid crystalline phases (i.e., orientation of the optical axes) is in 1911 when it was used by Mauging to investigate the alignment of nematic and chiral-nematic phases. \n\nA beam of convergent (or divergent) light is known to be a linear superposition of many plane waves over a cone of solid angles. The raytracing of Figure 1 illustrates the basic concept of \"conoscopy\": transformation of a directional distribution of rays of light in the front focal plane into a lateral distribution (\"directions image\") appearing in the back focal plane (which is more or less curved). The incoming elementary parallel beams (illustrated by the colors blue, green and red) are converging in the back focal plane of the lens with the distance of their focal point from the optical axis being a (monotonous) function of the angle of beam inclination. \nThis transformation can easily be deduced from two simples rules for the thin positive lens: \n\nThe object of measurement is usually located in the front focal plane of the lens. In order to select a specific area of interest on the object (i.e., definition of a measuring spot, or field of measurement) an aperture can be placed on top of the object. In this configuration only rays from the measuring spot (aperture) hit the lens. \n\nThe image of the aperture is projected to infinity while the image of the directional distribution of the light passing through the aperture (i. e. directions image) is generated in the back focal plane of the lens. When it is not considered appropriate to place an aperture into the front focal plane of the lens, i.e., on the object, the selection of the measuring spot (field of measurement) can also be achieved by using a second lens. An image of the object (located in the front focal plane of the first lens) is generated in the back focal plane of the second lens. The magnification, M, of this imaging is given by the ratio of the focal lengths of the lenses L and L, M = f / f. \n\nA third lens transforms the rays passing through the aperture (located in the plane of the image of the object) into a second directions image which may be analyzed by an image sensor (e.g., electronic camera).\n\nThe functional sequence is as follows:\n\nThis simple arrangement is the basis for all conoscopic devices (conoscopes). It is not straight forward however to design and manufacture lens systems that combine the following features:\n\nDesign and manufacturing of this type of complex lens system requires assistance by numerical modelling and a sophisticated manufacturing process.\n\nModern advanced conoscopic devices are used for rapid measurement and evaluation of the electro-optical properties of LCD-screens (e.g., variation of luminance, contrast and chromaticity with viewing direction).\n\n\n"}
{"id": "7117540", "url": "https://en.wikipedia.org/wiki?curid=7117540", "title": "DAMP Project", "text": "DAMP Project\n\nThe Downrange Anti-missile Measurement Program or DAMP was an applied research project to obtain scientific data, just prior to and during re-entry, on intermediate- and intercontinental-range ballistic missiles as they returned to earth. The program was funded by the Advanced Research Projects Agency (ARPA) under the technical direction of the Army Ordnance Missile Command (AOMC) during the period 1 January 1959 through 30 September 1963.\n\nThe downrange facility was a Liberty-class merchant vessel renovated and converted for its technical assignment. Its measurement equipment complex included C-band, L-band and UHF radars; digital and analog recorders; gyroscope stabilization; timing generators; mode switching; telemetry acquisition apparatus; radiometers and riometers; boresight cine-TV and other photographic systems; communications and Transit Satellite navigation system.\n\nAs a complementary measurement facility to the DAMP ship, the Electromagnetic Research Laboratory at Moorestown, New Jersey, contained C- and L-band radar measurement equipment complete with digital and analog recording system in a range-support-tower combination for full-scale cross-section measurements under static conditions with orthogonal polarizations. Recorded analog data from this facility were provided for immediate use.\n\nData from both measurement facilities underwent necessary processing at the Riverton, New Jersey, data reduction center, a part of the Data Analysis Laboratory.\n\nForemost among DAMP Program objectives were to:\n\n\nThe DAMP ship, the USAS \"American Mariner\", was essentially a floating measurements laboratory that employed various types of sensors, recording apparatus, and technical support equipment. It operated under conventional maritime regulations in the impact area of ballistic missile test firings, primarily in the Caribbean area and in the South Atlantic Ocean, near Ascension Island. During 1962, however, it operated in the Pacific Ocean in order to perform tracking duties during Operation Dominic atomic testing.\n\nThe shipboard crew, provided by Mathiasen Tankers Corp., was complemented by approximately sixty technical personnel provided by RCA and Barnes Engineering Corporation, who operated and maintained the radar and support equipment. Each mission required a minimum of 36 hours of on-station preparation and pre- and post-flight calibration. Targets were those of opportunity fired on missile ranges as part of the overall ballistic missile test program, which at the time was in its infancy.\n\nThe DAMP ship participated in numerous missile test operations on the Atlantic Missile Range, as well as on the Pacific Missile Range during 1962 in support of the Operation Dominic atomic tests with emphasis on radio frequency and radar electromagnetic effects. In addition, the DAMP ship served in support of the Ranger Program on the Atlantic Missile Range for the National Aeronautics and Space Administration (NASA). Activities included vehicle tracking during and subsequent to final burnout to supply designation data for the Deep Space Instrumentation Facility.\n\nDuring its operation in the Atlantic Missile Range, the DAMP ship stopped at various ports for logistical support and requirements. Such ports included Recife, Brazil; Antigua, West Indies; Cape Town, and Monrovia and Senegal in Africa. While on the Pacific Missile Range, the DAMP ship was based out of Honolulu, Hawaii, and operated in the Johnston Island area.\n\nThe Electromagnetic Research Laboratory (ERL) was located near the Missile and Surface Radar Division plant of RCA in Moorestown, New Jersey. The C-band, low-power, cross-section-measurement system in conjunction with two static range configurations was used for full-scale targets weighing up to 4,000 pounds rotated through 360 degrees. Since measurements must be made on targets as if they were located in free space, returns from other than the target were reduced to 10 m², the general background level of one tower-terrain combination. The low-power radar was capable of transmitting any linear, circular, or elliptical polarization at C-band frequencies. It could receive the horizontal and vertical components of backscattered energy simultaneously. Cross-section data was recorded in analog form on Sanborn and Scientific Atlanta strip-chart recorders and on Ampex magnetic tape. Digital cross sections were also recorded on a specially-designed CEC magnetic tape recorder.\n\nOther important functions of the ERL were the training of technical personnel, development and checkout of new equipment and techniques, and satellite tracking using a modified FPQ-4 radar. L-band measurement capability, similar to the C-band equipment, was introduced in late 1963.\n\nThe Data Analysis Laboratory (DAL) was, in general, devoted to preparation of DAMP data for dissemination to the many users. These data are submitted to many forms that require numerous reduction processes. Thus, the DAL was divided into three basic operational areas, namely: (1) the Data Reduction Center in Riverton, New Jersey, (2) the Data Interpretation Group in Moorestown, New Jersey, and (3) the Plasma Analysis Group in Moorestown, New Jersey.\n\nRaw data from the DAMP ship's sensors was delivered to the data reduction center for reduction, smoothing, and conversion to forms appropriate to interpretation. Equipment used in this data processing cycle included a digital tape translator, an IBM 709 computer, an analog computer, various tape and card converters and card punches, analog and digital playback equipment, data plotters, chart and film readers, oscilloscope cameras and oscilloscopes, a photo darkroom, and miscellaneous support equipment.\n\nThe processed data would be disseminated directly or would be submitted to the Data Interpretation Group for study, interpretation, correlation, and comparison with theory. The data interpretation group was concerned with annotating basic data with interpretative comments to aid the ultimate data user. Objectives were to investigate every avenue for enhanced data output and maximum data accuracy and usefulness. Theoretical studies and experiments were made to augment the data comparisons and evaluations; such studies embraced the computational areas of numerical, mathematical, and statistical analysis, analog and digital simulation, and computer programming as well as the many physical sciences associated with missile flight and atmospheric re-entry.\n\nOne of the most important facets of re-entry physics is the interaction of plasmas and ionized wakes with electromagnetic waves. Work was being conducted in both the theoretical and practical nature of plasma physics as related to radar and associated phenomena; the following subcontract organization and consultants were active in support of phases of this research:\n\n\nTo evaluate the effects of plasma and wake on EM propagation, flow field studies were performed to generate plasma density contours. These theoretical results were used for comparison with measured data for analysis. Laboratory experiments and simulation studies were conducted to investigate complex phenomena such as turbulence in the plasma flow field.\n\nThe DAMP ship, the USAS (\"United States Army Ship\") American Mariner was the primary data collection facility of the DAMP Project. Pertinent ship's characteristics include:\n\n\nThe DAMP ship was instrumented with various systems designed to locate and track incoming ballistic missiles in the predicted impact area, as well as instrumentation to record raw data.\n\nOne of the two AN/FPQ-4 radars aboard the DAMP ship permitted the tracking of beacon-carrying targets to an unambiguous range of . The L-band and UHF radars, which shared a common antenna, permitted observation of the target by illumination to supplement the C-band. The AN/FPS-12 radar permitted surveillance of the general vicinity of the DAMP ship as well as vectoring of aircraft which may be assigned to the ship.\n\nThe telemetry acquisition system was a passive acquisition aid operated on the interfereometer principle which gives angular position of the target relative to the ship. It was not a direct source of range information. Telemetry data was recorded on Magnetic tape at 7.5, 15, 30 and 60 ips from Nems Clark 1432 receiver.\n\nThe acquisition director, which comprised the RADAP-C Computer together with associated input and output devices, served as an acquisition aid, as means for checkout and calibration of associated electronic equipment, and, in the TRANSIT mode, as an aid to the navigation of the DAMP ship.\nThe Transit system of navigation consisted of the Transit satellite, a worldwide network of tracking stations to observe and determine the orbital elements of the satellite, and suitable receivers and Autonetics Recomp computers aboard the navigating vessel to receive the stable frequency radio signals broadcast by the satellite, to observe the Doppler frequency shift, and from this information, plus the approximate location of the vessel, compute to within one mile (1.6 km) the exact location of the vessel.\n\nThe quartz Reference Oscillator was the heart of the shipboard system, and operated at 1 MHz and 100 KC with a short term stability of 5 parts in 10 to the tenth averaged over a one-second interval, and a long term stability of 5 parts in 10 to the tenth per day.\n\nThe Video Integrator accepted the radar video signal returns from a pulsed, range-gated radar system (AN/FPQ-4) and improved the signal-to-noise ratio of the radar video, thereby extending the acquisition range of the radar system. The video is continuously integrated and the output is in realtime.\n\nIn order to utilize fully the data gathering capabilities of the DAMP ship, it was necessary that the data gathered be stored in a form which permitted easy retrieval at the data analysis facility. Therefore, all data obtained at the DAMP ship was stored only on certified and designated CEC digital recorders, Ampex recorders, a Sanborn recorder, an RCA video recorder and a Mincom recorder. Timing was provided by two Hermes Hycon Eastern timing generators, one primary and one back-up. Timing signal outputs were 24-bit pulse absence code, 17-bit pulse absence code, and 13-bit pulse width code. Both generators were synchronized with Bureau of Standards, WWV, time standard.\n\nRaw test data recorded included time, sync and control, tracker 1 and 2 radar data, L-band and UHF radar data, gyro and server data, telemetry, voice, video and any other data designated by the test director.\n\nOne of the two Mk 51 Mod 3 gun directors, suitably modified and fitted with M-7 elbow telescopes (8-power, 6” field) was employed to furnish designation information to the AN/FPQ-4 tracking radars in “optical” mode. These directors had an azimuth travel of plus/minus 370 degrees and elevated from -20 degrees to +90 degrees. Azimuth and elevation data output was furnished at 1- and 36-speed from two (size 23) 400-cycle synchros. Maximum position error was 0.5 mil.\n\nCommunications aboard the DAMP ship served several purposes: to communicate with administrative offices in New Jersey, to receive, immediately after launch, predicted impact data from the launch agency firing the ballistic or intercontinental missile, and, after completion of impact tracking, to report “quicklook” data back to the ERL in New Jersey so that it could be evaluated and a summary report be forwarded on to the project sponsor, ARPA.\n\nDuring this period of operation, communications via satellite did not yet exist. As a result, communications between the DAMP ship and New Jersey was through the use of single-sideband (SSB) radio, generally through the use of a 10 kW transmitter. The DAMP ship generally transmitted upper sideband (USB) voice and/or lower sideband (LSB) frequency-shift keying radioteletype in the area of 16 and 22 MHz when atmospheric conditions were favorable. When communications became difficult because of atmospheric conditions, the DAMP ship shifted to carrier wave (CW) transmission using standard Morse Code. In addition, because the ship could be out to sea for extended periods, such as a month or so, the DAMP ship carried an amateur radio room and equipment, which technical personnel could use to communicate with their families back home in the States via short wave radio.\n\nFor local area communicating – for example when in the Ascension Island area in the Atlantic or the Johnston Island area in the Pacific) the DAMP vessel had the capability of transmitting and receiving USB voice and LSB radioteletype to/from such island locations. The DAMP ship could also communicate with aircraft and nearby islands using various VHF or UHF radio configurations.\n\nVarious antenna were employed to obtain maximum range from ship-to-shore. The UHF transceiver used a discone, while the VHF transceivers used ground plane antennas. The 1 kW HF transmitter used a remotely tuned whip antenna, while the 3 kW HF transmitter used a vertical cage antenna., and the 10 kW transmitter employed a discone antenna. The vessel contained numerous state-of-the-art receivers, which were fed from an antenna distribution system from three whips.\n\nThe DAMP ship also carried advanced cryptographic equipment which could be used to encode or decode messages of a confidential or secret nature. Details of such equipment is classified and not described here.\n\nAs required by law, the ship’s Merchant Marine Master had a radio room and a radio operator who was responsible for ship’s business but was not associated in any way with the operation of the radio system used for technical purposes.\n\nFour modified TALOS AN/FPW-2 Guidance Pedestals were slaved to either or both of the AN/FPQ-4 tracking radars in order to provide optical, infra-red and other supplementary instrumentation for the observation of re-entry bodies.\n\nOn September 30, 1963 the U.S. Army Advanced Research Projects Agency DAMP mission and contract expired, and the U.S. Air Force assumed operational control of the USAS American Mariner. On January 1, 1964, it became an integral, but temporary, part of the U.S. Air Force's Eastern Test Range in the Atlantic Ocean, and was placed under the control of the Missile Test Project (MTP) based out of Patrick Air Force Base, Florida. (See Eastern Test Range for details.) Eventually, the ship was removed from missile tracking service and sunk in the Chesapeake Bay while being used as a target by U.S. Navy pilots from the Patuxent River Naval Air Station.\n\nHahn, Herbert Paul, American Mariner a documentary biography of her role as: Liberty Ship, Training Ship, Missile Instrumentation Ship, Mystery Ship, Test Target. Published 1990 by American Merchant Marine Museum Foundation, Kings Point, New York. .\n\n\n"}
{"id": "12836378", "url": "https://en.wikipedia.org/wiki?curid=12836378", "title": "Design science (methodology)", "text": "Design science (methodology)\n\nDesign science is an outcome based information technology research methodology, which offers specific guidelines for evaluation and iteration within research projects.\n\nDesign science research focuses on the development and performance of (designed) artifacts with the explicit intention of improving the functional performance of the artifact. Design science research is typically applied to categories of artifacts including algorithms, human/computer interfaces, design methodologies (including process models) and languages. Its application is most notable in the Engineering and Computer Science disciplines, though is not restricted to these and can be found in many disciplines and fields. In design science research, as opposed to explanatory science research, academic research objectives are of a more pragmatic nature. Research in these disciplines can be seen as a quest for understanding and improving human performance. Such renowned research institutions as MIT’s Media Lab, Stanford's Centre for Design Research, Carnegie-Mellon's Software Engineering Institute, Xerox’s PARC and Brunel’s Organization and System Design Centre use the Design Science Research approach. \n\nAccording to Van Aken, the main goal of design science research is to develop knowledge that the professionals of the discipline in question can use to design solutions for their field problems. This mission can be compared to the one of the ‘explanatory sciences’, like the natural sciences and sociology, which is to develop knowledge to describe, explain and predict. Hevner states that the main purpose of design science research is achieving knowledge and understanding of a problem domain by building and application of a designed artifact.\n\nSince the first days of computer science, computer scientists have been doing design science research without naming it. They have developed new architectures for computers, new programming languages, new compilers, new algorithms, new data and file structures, new data models, new database management systems, and so on. Much of the early research was focused on systems development approaches and methods. The dominant research philosophy has been to develop cumulative, theory-based research to be able to make prescriptions. It seems that this ‘theory-with-practical-implications’ research strategy has seriously failed to produce results that are of real interest in practice. This failure led to search practical research methods such as design science research.\n\nThe design process is a sequence of expert activities that produces an innovative product. The artifact enables the researcher to get a better grasp of the problem; the re-evaluation of the problem improves the quality of the design process and so on. This build-and-evaluate loop is typically iterated a number of times before the final design artifact is generated. In design science research, the focus is on the so-called field-tested and grounded technological rule as a possible product of Mode 2 research with the potential to improve the relevance of academic research in management. Mode 1 knowledge production is purely academic and mono-disciplinary, while Mode 2 is multidisciplinary and aims at solving complex and relevant field problems.\n\nHevner et al. have presented a set of guidelines for design science research within the discipline of Information Systems. Design science research requires the creation of an innovative, purposeful artifact for a special problem domain. The artifact must be evaluated in order to ensure its utility for the specified problem. In order to form a novel research contribution, the artifact must either solve a problem that has not yet been solved, or provide a more effective solution. Both the construction and evaluation of the artifact must be done rigorously, and the results of the research presented effectively both to technology-oriented and management-oriented audiences.\n\nHevner counts 7 guidelines for a design science research:\n\nArtifacts within DSR are perceived to be knowledge containing. This knowledge ranges from the design logic, construction methods and tool to assumptions about the context in which the artifact is intended to function (Gregor, 2002).\n\nThe creation and evaluation of artifacts thus forms an important part in the DSR process which was described by Hevner et al., (2004) and supported by March and Storey (2008) as revolving around “build and evaluate”.\n\nDSR artifacts can broadly include: models, methods, constructs, instantiations and design theories (March & Smith, 1995; Gregor 2002; March & Storey, 2008, Gregor and Hevner 2013), social innovations, new or previously unknown properties of technical/social/informational resources (March, Storey, 2008), new explanatory theories, new design and developments models and implementation processes or methods (Ellis & Levy 2010).\n\nDesign science research can be seen as an embodiment of three closely related cycles of activities. The relevance cycle initiates design science research with an application context that not only provides the requirements for the research as inputs but also defines acceptance criteria for the ultimate evaluation of the research results. The rigor cycle provides past knowledge to the research project to ensure its innovation. It is incumbent upon the researchers to thoroughly research and reference the knowledge base in order to guarantee that the designs produced are research contributions and not routine designs based upon the application of well-known processes. The central design cycle iterates between the core activities of building and evaluating the design artifacts and processes of the research.\n\nDesign science research in itself implies an ethical change from describing and explaining of the existing world to shaping it. One can question the values of IS research, i.e. whose values and what values dominate it, emphasizing that research may openly or latently serve the interests of particular dominant groups. The interests served may be those of the host organization as perceived by its top management, those of IS users, those of IS professionals or potentially those of other stakeholder groups in society.\n\n\n\n"}
{"id": "20248326", "url": "https://en.wikipedia.org/wiki?curid=20248326", "title": "Field Notes from a Catastrophe", "text": "Field Notes from a Catastrophe\n\nField Notes from a Catastrophe: Man, Nature, and Climate Change is a 2006 non-fiction book by Elizabeth Kolbert. The book attempts to bring attention to the causes and effects of global climate change. Kolbert travels around the world where climate change is affecting the environment in significant ways. These locations include Alaska, Greenland, the Netherlands, and Iceland. The environmental effects that are apparent consist of rising sea levels, thawing permafrost, diminishing ice shelves, changes in migratory patterns, and increasingly devastating forest fires due to loss of precipitation. She also speaks with many leading scientists about their individual research and findings. Kolbert brings to attention the attempts of large corporations such as Exxon Mobil and General Motors to influence politicians and discredit scientists. She also writes about America’s reluctance in the global efforts to reduce carbon emissions. Leading this resistance, she explained, was the Bush administration, which was opposed to the Kyoto Protocol since it was ratified in 2005. Kolbert concludes the book by examining the events surrounding the events of Hurricane Katrina in 2005 and arguing that governments have the knowledge and technologies to prepare for such disasters but choose to ignore the signs until it is too late.\n"}
{"id": "36203752", "url": "https://en.wikipedia.org/wiki?curid=36203752", "title": "Frugal innovation", "text": "Frugal innovation\n\nFrugal innovation or frugal engineering is the process of reducing the complexity and cost of a good and its production. Usually this refers to removing nonessential features from a durable good, such as a car or phone, in order to sell it in developing countries. Designing products for such countries may also call for an increase in durability and, when selling the products, reliance on unconventional distribution channels. When trying to sell to so-called \"overlooked consumers\", firms hope volume will offset razor-thin profit margins. Globalization and rising incomes in developing countries may also drive frugal innovation. Such services and products need not be of inferior quality but must be provided cheaply.\n\nIn May 2012 \"The Financial Times\" newspaper called the concept \"increasingly fashionable\".\n\nSeveral US universities have programs that develop frugal solutions. Such efforts include the Frugal Innovation Lab at Santa Clara University and a two quarter project course at Stanford University, the Entrepreneurial Design for Extreme Affordability program.\n\nMany terms are used to refer to the concept. \"Frugal engineering\" was coined by Carlos Ghosn, then joint chief of Renault and Nissan, who stated, \"frugal engineering is achieving more with fewer resources.\"\n\nIn India, the words \"Gandhian\" or \"jugaad\", Hindi for a stop-gap solution, are sometimes used instead of \"frugal\". Other terms with allied meanings include \"inclusive innovation\", \"catalytic innovation\", \"reverse innovation\", and \"BOP innovation\", etc.\n\nAt times this no frills approach can be a kind of disruptive innovation.\n\nSpotlighted in a 2010 article in \"The Economist\", the roots of this concept may lie in the appropriate technology movement of the 1950s although profits may have been first wrung from underserved consumers in the 1980s when multinational companies like Unilever began selling single-use-sized toiletries in developing countries. Frugal innovation today isn't solely the domain of large, multinational corporations, however, as small, local firms have themselves chalked up a number of homegrown solutions. While General Electric may win plaudits for its US$800 EKG machines, cheap cell phones made by local, no-name companies, and prosthetic legs fashioned from irrigation piping are also examples of frugal innovation.\n\nThe concept has gained popularity in the South Asian region, particularly in India. The US Department of Commerce has singled out this nation for its innovative achievements saying in 2012, \"there are many Indian firms that have learned to conduct R&D in highly resource-constrained environments and who have found ways to use locally appropriate technology...\"\n\nFrugal innovation is not limited to durable goods such as the GE US$800 EKG machine, Reliance Jio's JioPhone or the US$100 One Laptop Per Child but also services such as 1-cent-per-minute phone calls, mobile banking, off-grid electricity, and microfinance.\n\nA tiny refrigerator sold by Indian company Godrej, the ChotuKool may have more in common with computer cooling systems than other refrigerators; it eschews the traditional compressor for a computer fan. (It may exploit the thermoelectric effect.)\n\nDesigned to cost no more than a dollar, the Foldscope is a tough origami microscope assembled from a sheet of paper and a lens. The Stanford engineer responsible more recently developed a string-and-cardboard contraption that can function similar to $1,000 centrifuges.\n\nA low cost prosthetic developed in India, the Jaipur leg costs about $150 to manufacture and includes improvisations such as incorporating irrigation piping into the design to lower costs.\n\nMobile banking solutions in Africa, like Safaricom's M-Pesa, allow people access to basic banking services from their mobile phones. Money transfers done through mobiles are also much cheaper than using a traditional method. While basic banking can be done on a mobile alone, deposits and withdrawals of cash necessitate a trip to a local agent.\n\nDesigned for developing countries, the Nokia 1100 was basic, durable, and–besides a flashlight–had few features other than voice and text. Selling more than 200 million units only four years after its 2003 introduction made it one of the best selling phones of all time.\n\nIn Africa, several companies including SABMiller and Diageo, following in the footsteps of local home brewers, have made beer more affordable by using sorghum or cassava in place of malting barley and reducing packaging costs by using kegs instead of bottles.\n\nIn some Philippine slums, solar skylights made from one liter soda bottles filled with water and bleach can provide light equivalent to that produced by a 55 watt bulb and may reduce electricity bills by US$10 per month.\n\nDesigned to appeal to the many Indians who drive motorcycles, the Tata Nano was developed by Indian conglomerate Tata Group and is the cheapest car in the world.\n\n\nIn 2014, Navi Radjou delivered a talk at TED Global on frugal innovation.\n\nIn 2015, Navi Radjou and Jaideep Prabhu coauthored the book \"Frugal Innovation: How to Do More With Less\"' published worldwide by The Economist. The book explains the principles, perspectives and techniques behind frugal innovation, enabling managers to profit from the great changes ahead.\n\n"}
{"id": "3797340", "url": "https://en.wikipedia.org/wiki?curid=3797340", "title": "Gerhard Friedrich Müller", "text": "Gerhard Friedrich Müller\n\nGerhard Friedrich Müller (Russian: \"Фёдор Ива́нович Ми́ллер\", \"Fyodor Ivanovich Miller\", 29 October 1705 – 22 October 1783) was a historian and pioneer ethnologist.\n\nMüller was born in Herford, while he was educated at Leipzig. In 1725, he was invited to St. Petersburg to co-found the Imperial Academy of Sciences. Müller participated in the second Kamchatka expedition, which reported on life and nature of the further (eastern) side of the Ural mountain range. From 1733 till 1743, nineteen scientists and artists traveled through Siberia to study people, cultures and collected data for the creation of maps. Müller, who described and categorized clothing, religions and rituals of the Siberian ethnic groups, is considered to be the father of ethnography.\n\nOn his return from Siberia, he became historiographer to the Russian Empire.\nHe was one of the first historians to bring out a general account of Russian history based on an extensive examination of the documentary sources. His accentuation of the role of Scandinavians and Germans in the history of that country – a germ of the so-called Normanist theory – earned him enmity of Mikhail Lomonosov, who had previously supported his work, and dented his Russian career. In 1766, after many attacks by his colleagues, Müller was appointed keeper of the national archives. He drew up for the government a collection of its treatises.\n\nIn 1761, Müller was elected a foreign member of the Royal Swedish Academy of Sciences. He died, aged 77, in Moscow.\n\n\n\n\n"}
{"id": "59100379", "url": "https://en.wikipedia.org/wiki?curid=59100379", "title": "Gimesia", "text": "Gimesia\n\nGimesia is a genus of bacteria from the family of Planctomycetaceae with one known species (\"Gimesia maris\"). \"Gimesia maris\" has been isolated from neritic water from Puget Sound in the United States.\n"}
{"id": "3742825", "url": "https://en.wikipedia.org/wiki?curid=3742825", "title": "Heat map", "text": "Heat map\n\nA heat map (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors. \"Heat map\" is a newer term but shading matrices have existed for over a century.\n\nHeat maps originated in 2D displays of the values in a data matrix. Larger values were represented by small dark gray or black squares (pixels) and smaller values by lighter squares. Loua (1873) used a shading matrix to visualize social statistics across the districts of Paris. Sneath (1957) displayed the results of a cluster analysis by permuting the rows and the columns of a matrix to place similar values near each other according to the clustering. Jacques Bertin used a similar representation to display data that conformed to a Guttman scale. The idea for joining cluster trees to the rows and columns of the data matrix originated with Robert Ling in 1973. Ling used overstruck printer characters to represent different shades of gray, one character-width per pixel. Leland Wilkinson developed the first computer program in 1994 (SYSTAT) to produce cluster heat maps with high-resolution color graphics. The Eisen et al. display shown in the figure is a replication of the earlier SYSTAT design.\n\nSoftware designer Cormac Kinney trademarked the term 'heat map' in 1991 to describe a 2D display depicting financial market information. The company that acquired Kinney's invention in 2003 unintentionally allowed the trademark to lapse.\n\nThere are different kinds of heat maps:\n\nThere are many different color schemes that can be used to illustrate the heat map, with perceptual advantages and disadvantages for each. Rainbow color maps are often used, as humans can perceive more shades of color than they can of gray, and this would purportedly increase the amount of detail perceivable in the image. However, this is discouraged by many in the scientific community, for the following reasons:\n\nChoropleth maps are sometimes incorrectly referred to as heat maps. A choropleth map features different shading or patterns within geographic boundaries to show the proportion of a variable of interest, whereas the coloration a heat map (in a map context) does not correspond to geographic boundaries.\n\nSeveral heat map software implementations are freely available:\n\n\n\n"}
{"id": "5885121", "url": "https://en.wikipedia.org/wiki?curid=5885121", "title": "Huisman–Olff–Fresco models", "text": "Huisman–Olff–Fresco models\n\nHuisman–Olff–Fresco models (HOF models) are a hierarchical set of 5 models with increasing complexity, designated for fitting unimodal species response curves on environmental gradient.\n\n"}
{"id": "20741711", "url": "https://en.wikipedia.org/wiki?curid=20741711", "title": "ICRANet", "text": "ICRANet\n\nICRANet, the International Center for Relativistic Astrophysics Network, is an international organization which promotes research activities in relativistic astrophysics and related areas. Its members are four countries and three Universities and Research Centers: the Republic of Armenia, the Federative Republic of Brazil, Italian Republic, the Vatican City State, the University of Arizona (USA), Stanford University (USA) and ICRA.\n\nICRANet headquarters are located in Pescara, Italy.\n\nIn 1985, the International Center for Relativistic Astrophysics ICRA was founded by Remo Ruffini (University of Rome \"La Sapienza\") together with Riccardo Giacconi (Nobel Prize for Physics 2002), Abdus Salam (Nobel Prize for Physics 1979), Paul Boynton (University of Washington), George Coyne (former director of the Vatican observatory), Francis Everitt (Stanford University) and Fang Li-Zhi (University of Science and Technology of China).\n\nThe Statute and the Agreement establishing of ICRANet were signed on March 19, 2003, and they were recognized in the same year by the Republic of Armenia and the Vatican City State. ICRANet has been created in 2005 by a law of the Italian Government, ratified by the Italian Parliament and signed by the President of the Italian Republic Carlo Azeglio Ciampi on February 10, 2005. The Republic of Armenia, Italian Republic, the Vatican City State, ICRA, the University of Arizona and the Stanford University are the founding members.\n\nOn September 12, 2005, ICRANet Steering Committee was established and had its first meeting: Remo Ruffini and Fang Li-Zhi were appointed respectively Director and Chairman of the Steering Committee. On December 19, 2006 ICRANet Scientific Committee was established and had its first meeting in Washington DC. Riccardo Giacconi was appointed Chairman and John Mester Co-Chairman.\n\nOn September 21, 2005 the Director of ICRANet signed, together with the then Ambassador of Brazil in Rome Dante Coelho De Lima the adhesion of the Federative Republic of Brazil to ICRANet. The entrance of Brazil, requested by the then President of Brazil Luiz Ignácio Lula Da Silva has been unanimously ratified by the Brazilian Parliament. On August 12, 2011, the then President of Brazil Dilma Rousseff signed the entrance of Brazil in ICRANet.\n\nBy the beginning of the twentieth century the new branch of mathematics, tensor calculus, was developed in the works of Gregorio Ricci Curbastro and Tullio Levi Civita of the University of Padua and the University of Rome \"La Sapienza\". Marcel Grossmann of the University of Zurich who had a deep knowledge of the Italian school of geometry and who was close to Einstein introduced to him these concepts. The collaboration between Einstein and Grossmann was essential for the development of General Relativity.\n\nRemo Ruffini and Abdus Salam in 1975 established the Marcel Grossmann meetings (MG) on Recent Developments in Theoretical and Experimental General Relativity, Gravitation, and Relativistic Field Theories, which take place every three years in different countries, gathering more then 1000 researchers. MG1 and MG2 were held in 1975 and in 1979 in Trieste; MG3 in 1982 in Shanghai; MG4 in 1985 in Rome; MG5 in 1988 in Perth; MG6 in 1991 in Kyoto; MG7 in 1994 at Stanford; MG8 in 1997 in Jerusalem; MG9 in 2000 in Rome; MG10 in 2003 in Rio de Janeiro; MG11 in 2006 in Berlin; MG12 in 2009 in Paris; MG13 in 2012 in Stockholm; MG14 in 2015 and MG15 in 2018 both in Rome. Since its foundation, ICRANet has always played a leading role in the organization of those meetings.\n\nICRANet has been Organizational Associate of the International Year of Astronomy 2009 and supported the global coordination of IYA2009 financially. In this occasion ICRANet organized a series of international meetings under the general title \"The Sun, the Star, the Universe and General Relativity\" including: the 1st Zeldovich meeting (Minsk, Belarus), the Sobral Meeting (Fortaleza, Brazil), the 1st Galileo - Xu Guangqi meeting (Shanghai, China), the 11th Italian-Korean Symposium on Relativistic Astrophysics (Seoul, South Korea) and the 5th Australasian Conference - Christchurch Meeting (Christchurch, New Zealand).\n\nUnder the initiative of the United Nations and UNESCO, 2015 was declared the and as the International Year of Light, and it represented the centenary of the formulation of the equations of general relativity by Albert Einstein, and the fiftieth anniversary of the birth of relativistic astrophysics. ICRANet was a \"Bronze Associate\" sponsor of those celebrations.\n\nIn 2015, ICRANet also organized a series of international meetings including: the Second ICRANet César Lattes Meeting (Niterói – Rio de Janeiro – João Pessoa – Recife – Fortaleza, Brazil), the International Conference on Gravitation and Cosmology / the 4th Galileo-Xu Guangqi meeting (Beijing, China), Fourteenth Marcel Grossmann Meeting - MG14 (Rome, Italy), the 1st ICRANet Julio Garavito Meeting on Relativistic Astrophysics (Bucaramanga – Bogotá, Colombia), the 1st Sandoval Vallarta Caribbean Meeting on Relativistic Astrophysics (Mexico City, Mexico).\n\nThe organization consists of the Director, the Steering Committee and the Scientific Committee. The members of committees are representatives\nof the countries and member institutions. ICRANet has a number of permanent Faculty positions. Their activities are supported by administrative staff and secretariat personnel. ICRANet financing is based by Statute on the funds provided by the governments and by voluntary contributions, donations.\n\nDirector of ICRANet is Remo Ruffini.\n\nIn 2018 the Steering Committee consists of:\nThe current Chairperson (2018) of ICRANet Steering Committee is Francis Everitt.\n\nThe first Chairperson of the Scientific Committee was Riccardo Giacconi, Nobel Prize for Physics in 2002, who ended his term in 2013. The actual (2018) Chairperson of the Scientific Committee is Massimo Della Valle.\n\nThe Scientific Committee in 2018 consists of: Prof. Narek Sahakyan (Armenia), Dr. Barres de Almeida Ulisses (Brazil), Dr. Carlo Luciano Bianco (ICRA), Prof. Massimo Della Valle (Italy), Prof. John Mester (Stanford University), Prof. Chris Fryer (University of Arizona) and Dr. Gabriele Gionti (Vatican City State).\n\nThe Faculty in 2018 consists of Professors Ulisses Barres de Almeida, Vladimir Belinski, Carlo Luciano Bianco, Donato Bini, Pascal Chardonnet, Christian Cherubini, Filippi Simonetta, Robert Jantzen, Roy Patrick Kerr, Hans Ohanian, Giovanni Pisani, Brian Mathew Punsly, Jorge Rueda, Remo Ruffini, Gregory Vereshchagin, and She-Sheng Xue, and is supported by an Adjunct Faculty made up of more than 30 internationally renowned scientists participating in ICRANet activities, and between eighty \"Lecturers\" and \"Visiting Professors\". Among these are the Nobel Laureates Riccardo Giacconi, Murray Gell-Mann, Theodor Hänsch, Gerard ’t Hooft and Steven Weinberg.\n\nCurrently ICRANet members are four countries and three Universities and research centers.\n\nMember states:\nMember institutions:\nICRANet has signed collaboration agreements with over 57 institutions, universities and research centers in different countries.\n\nThe network is composed of several seats and centers. Seat agreements, establishing rights and privileges, including extraterritoriality, have been signed for the seat in Pescara in Italy, for the seat in Rio de Janeiro in Brazil and for the seat in Yerevan in Armenia. The Seat Agreement for Pescara has been ratified on May 13, 2010. The Seat agreement for Yerevan has been unanimously approved by the Parliament of Armenia on November 13, 2015.\n\nHigh speed optical fiber connection with different locations are made possible by the connection to the pan-European data network for the research and education community (GÉANT) through the GARR network.\n\nCurrently ICRANet centers are operative at:\n\nICRANet headquarters are located in Pescara, Italy. This center coordinates ICRANet activities and yearly meetings of the Scientific and the Steering committees are usually held there. International meetings such as the Italian-Korean Symposia on Relativistic Astrophysics are regularly held in this center. Scientific activities in Pescara center include the fundamental research on the early cosmology by the Russian school guided by Vladimir Belinski.\n\nActivities of the ICRANet Seat at Villa Ratti in Nice include the coordination of the IRAP PhD program, as well as scientific activities connected with the ultra high energy observations by the University of Savoy and the VLT observations performed by the Côte d’Azur Observatory, which involve the thesis works of IRAP PhD students. The University of Savoy is the closest French lab to the CERN.\n\nSince January 2014, the ICRANet Center in Yerevan has been established the Presidium of the National Academy of Sciences of the Republic of Armenia (NAS RA), at Marshall Baghramian Avenue, 24a. Scientific activities in this center are coordinated by the Director, Dr. Narek Sahakyan. In 2014, the Government of the Republic of Armenia approved the Agreement to establish the ICRANet international center in Armenia. The Seat Agreement has been signed in Rome on February 14, 2015, by the director of ICRANet, Remo Ruffini and the Ambassador of Armenia in Italy, Mr. Sargis Ghazaryan. On November 13, 2015, the Parliament of the Republic of Armenia unanimously approved the Seat Agreement. Since January 2016 ICRANet Armenia center is registered at the Ministry of Foreign Affairs as an international organization. The main areas of scientific research in ICRANet-Armenia are in the fields of relativistic astrophysics, astroparticle physics, X-ray astrophysics, high and very high energy gamma-ray astrophysics, high energy neutrino astrophysics. The center is a full member of the MAGIC international collaboration since 2017. Also, the center is actively involved in development of the Open Universe Initiative. In Armenia, the ICRANet center collaborates with other scientific institutions from the Academy and Universities, and provides to organize joint international meetings and workshops, summer schools for PhD students and mobility programs for scientists in the field of Astrophysics. ICRANet center in Armenia coordinates ICRANet activities in the area of Central-Asian and Middle-Eastern countries.\n\nA summer school and an international scientific conference dedicated to the issues of Relativistic Astrophysics \"1st Scientific ICRANet Meeting in Armenia: Black Holes: the largest energy sources in the Universe\" were held in Armenia from June 28 to July 4, 2014.\n\nThe Seat of ICRANet in Rio de Janeiro has been activated, at first in the premises granted by CBPF; with the possible expansion to the Cassino da Urca. A school of Cosmology and Astrophysics is being developed jointly with Brazilian institutions. The 2nd ICRANet César Lattes Meeting devoted to relativistic astrophysics was held in Rio de Janeiro in 2015.\n\nCurrently (2018) ICRANet has signed scientific collaboration agreements with 175 Brazilian universities, institutions and research centers.\n\nThere are two specific programs initiated by ICRANet, which are underway (2018):\n\nThe ICRANet-Minsk center has been established at the National Academy of Science of Belarus (NASB), with whom ICRANet has signed a cooperation agreement on 2013. On April 2016 has been signed the Protocol for the realization of the official Belarusian seat in Minsk of the International Center for Relativistic Astrophysics Network. \nThe \"First ICRANet-Minsk workshop on high energy astrophysics\" has been held at the ICRANet-Minsk center from 26 to 28 of April 2017.\n\nThe ICRANet Center in Isfahan has been established at the Isfahan University of Technology. The Protocol of cooperation, signed in 2016 by Remo Ruffini, Director of ICRANet, and Mahnoud Modarres-Hashemi, Rector of the Isfahan University of Technology, includes the promotion and development of scientific and technological research in the fields of cosmology, gravitation and relativistic astrophysics. It also includes the organization of joint international conferences and workshops, institutional exchanges for students, researchers and faculty members.\n\nThe present Chairman of the ICRANet Steering Committee Francis Everitt is responsible for the ICRANet Center at the Leland Stanford Junior University. His special activity has been the conception, development, launch and data acquisition, all the way to the elaboration of the final data analysis, of the NASA Gravity Probe B mission, one of the most complex physics experiment in space ever performed.\n\nThe first Chairman of the ICRANet Steering Committee Fang Li-Zhi developed collaboration with the Physics Department of the University of Arizona in Tucson. The collaboration with its Astronomy Department is promoted by David Arnett.\n\nSince 2005 ICRANet co-organizes an International Ph.D. program in Relativistic Astrophysics — International Relativistic Astrophysics Ph.D. Program, IRAP-PhD, the first joint PhD astrophysics program with: ASI - Italian Space Agency (Italy); Bremen University (Germany); Carl von Ossietzky University of Oldenburg (Germany); CAPES - Brazilian Federal Agency for Support and Evaluation of Graduate Education (Brazil); CBPF - Brazilian Centre for Physics Research (Brazil); CNR - National Research Council (Italy); FAPERJ -Foundation \"Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro\" (Brazil); ICRA - International Center for Relativistic Astrophysics (Italy); ICTP - Abdus Salam International Centre for Theoretical Physics (Italy); IHES - Institut Hautes Etudes Scientifiques (France); Indian centre for space physics (India); INFN - National Institute for Nuclear Physics (Italy); NAS RA - Armenian National Academy of Sciences (Armenia); Nice University Sophia Antipolis (France); Observatory of the Côte d'Azur (France); Rome University - “Sapienza” (Italy); Savoy University (France); TWAS - Academy of sciences for the developing world; UAM - Metropolitan Autonomous University (Mexico); UNIFE - University of Ferrara (Italy).\nAmong the associated centers, there are both institutes devoted to theory and others devoted to experiments and observations. In that way, PhD students can have a wider education on theoretical relativistic astrophysics and put it in practice. The official language of the IRAP PhD is English and students have also the opportunity to learn the national language of their hosting country, attending several academic courses in the partner Universities.\n\nBy 2018, 122 students were enrolled in the IRAP PhD program: 1 from Albania, 4 from Argentina, 8 from Armenia, 1 from Austria, 2 from Belarus, 16 from Brazil, 5 from China, 9 from Colombia, 3 from Croatia, 5 from France, 5 from Germany 7 from India, 2 from Iran, 38 from Italy 2 from Kazakhstan, 1 from Lebanon, 1 from Mexico, 1 from Pakistan, 4 from Russia, 1 from Serbia, 1 from Sweden, 1 from Switzerland, 1 from Saudi Arabia, 2 from Taiwan and 1 from Turkey.\n\nThe IRAP-PhD program was the only European PhD program in Astrophysics awarded the Erasmus Mundus label and funded by the European Commission in 2010-2017.\n\nICRANet main goals are training, education and research in the field of relativistic astrophysics, cosmology, theoretical physics and mathematical physics.\n\nIts main activities are devoted to promote the international scientific co-operation and to carry on scientific research.\n\nAccording to the 2017 ICRANet Scientific Report, the main areas of scientific research in ICRANet are:\nBetween 2006 and 2018, ICRANet has released over 1800 scientific publications in refereed journals such as Physical Review, the Astrophysical Journal, Astronomy and Astrophysics etc., in its various fields of research.\n\nNew scientific concepts and terms introduced by ICRANet scientists:\n\nBlack hole (Ruffini, Wheeler 1971)\n\nErgosphere (Rees, Ruffini, Wheeler, 1974)\n\nPursue and plunge (Rees, Ruffini, Wheeler, 1974)\n\nBlack hole mass formula (Christodoulou, Ruffini, 1971)\n\nReversible and irreversible transformations of black holes (Christodoulou, Ruffini, 1971)\n\nDyadosphere (Damour, Ruffini, 1975; Preparata, Ruffini, Xue, 1998)\n\nDyadotorus (Cherubini et al., 2009)\n\nInduced Gravitational Collapse (Rueda, Ruffini, 2012)\n\nBinary-driven Hypernova (Ruffini et al., 2014)\n\nCosmic matrix (Ruffini et al., 2015)\n\n\nThe Galileo-Xu Guangqi meetings have been created in the name of Galileo and Xu Guangqi, the collaborator of Matteo Ricci (Ri Ma Dou), generally recognized for bringing to China the works of Euclid and Galileo and for his strong commitment to the process of modernization and scientific development of China. The 1st Galileo - Xu Guangqi Meeting was held in Shanghai, China, in 2009. The 2nd Galileo - Xu Guangqi meeting took place in Hanbury Botanic Gardens (Ventimiglia, Italy) and Villa Ratti (Nice, France) in 2010. The 3rd and 4th Galileo - Xu Guangqi meetings were both held in Beijing, China, respectively in 2011 and 2015.\n\n\nThe Italian-Korean Symposia on Relativistic Astrophysics is a series of biannual meetings, alternatively organized in Italy and in Korea since 1987. The symposia discussions cover topics in astrophysics and cosmology, such as gamma-ray bursts and compact stars, high energy cosmic rays, dark energy and dark matter, general relativity, black holes, and new physics related to cosmology.\n\n\nThese workshops represent a one-week dialogues on Relativistic Field Theories in Curved Space, which is inspired to the work of E. C. G. Stueckelberg. Invited lectures were delivered by Professors Abhay Ashtekar, Thomas Thiemann, Gerard 't Hooft and Hagen Kleinert.\n\n\nThe Zeldovich Meetings are a series of international conferences held in Minsk, in honor of Ya. B. Zeldovich, one of the founders of the Soviet Atomic Bomb and the founder of the Russian School on Relativistic Astropysics, which celebrate and discuss his wide research interests, ranging from chemical physics, elementary particle and nuclear physics to astrophysics and cosmology. The 1st Zeldovich Meeting was held at the Belarusian State University in Minsk, from 20 to 23 April 2009; the 2nd Zeldovich Meeting was held at the National Academy of Sciences of Belarus from 10 to 14 March 2014, to celebrate Ya. B. Zeldovich 100th Anniversary; the 3rd Zeldovich Meeting has been held at the National Academy of Sciences of Belarus from 23 to 27 April 2018.\n\n\nICRANet has also organized:\n\nIn the framework of the IRAP PhD program, ICRANet has organized several PhD schools: 11 of them have been held in Nice (France), 3 in Les Houches, 1 in Ferrara (Italy), 1 in Pescara (Italy) and 1 in Beijing (China).\n\nICRANet has developed a program of short and long term visits for scientific collaboration.\n\nProminent personalities have carried out their activities at ICRA and ICRANet, among them are: Prof. Riccardo Giacconi, Nobel Prize for Physics in 2002, Gerardus 't Hooft, Dutch physicist and Nobel Prize for Physics in 1999; Steven Weinberg, Nobel Prize in 1979; Murray Gell -Mann Nobel Prize in 1969; Subrahmanyan Chandrasekhar Nobel Prize in 1930; Haensch Theodor, Nobel Prize in 2001; Valeriy Ginzburg., Francis Everitt, Chairman of the Scientific Committee of ICRANet, Isaak Khalatnikov, Russian physicist and former director of the Landau Institute for Theoretical Physics from 1965 to 1992; Roy Kerr, New Zealand mathematician and discoverer of the \"Kerr Metric\"; Thibault Damour; Demetrios Christodoulou; Hagen Kleinert; Neta and John Bachall; Tsvi Piran; Charles Misner; Robert Williams; José Gabriel Funes; Fang Li-Zhi; Rashid Sunyaev.\n\nICRANet co-organizes with ICRA Joint Astrophysics Seminar at the Department of Physics of University \"La Sapienza\" in Rome. All institutions collaborating with ICRANet, as well as ICRANet centers, participate at those seminars.\n\nThe main objective of the Brazilian Science Data Center (BSDC) is to provide data of all international space missions existing on the wavelength of X- and gamma rays, and later on the whole electromagnetic spectrum, for all the galactic and extragalactic sources of the Universe. A special attention will be paid to the achievement and the complete respect of the levels defined by the International Virtual Observatory Alliance (IVOA). In addition to these specific objectives, BSDC will promote technical seminars, annual workshops and it will assure a plan of scientific divulgation and popularization of science with the aim of the understanding of the Universe.\n\nThe BSDC is currently being implemented at CBPF, and at the Universidade Federal do Rio Grande do Sul (UFRGS), and will be expanded to all other ICRANet centers in Brazil as well as to the other Latin-American ICRANet Centers in Argentina, Colombia and Mexico: a unique coordinated continental research network planned for Latin America.\n\n"}
{"id": "1090498", "url": "https://en.wikipedia.org/wiki?curid=1090498", "title": "IMAGE", "text": "IMAGE\n\nIMAGE (Imager for Magnetopause-to-Aurora Global Exploration) is a NASA Medium Explorers mission that studied the global response of the Earth's magnetosphere to changes in the solar wind. It was believed lost but as of August 2018 might be recoverable. It was launched 25 March 2000 by a Delta II rocket from Vandenberg AFB on a two-year mission. Almost six years later, it unexpectedly ceased operations in December 2005 during its extended mission and was declared lost. The spacecraft was part of NASA's Sun-Earth Connections Program, and during its run had over 400 research articles published in peer-reviewed journals using its data. It had special cameras that provided various breakthroughs in understanding the dynamics of plasma around the Earth. The Principal Investigator was Jim Burch of the Southwest Research Institute.\n\nIn January 2018, an amateur satellite tracker found it to be transmitting some signals back to Earth. NASA made attempts to communicate with the spacecraft and determine its payload status, but has had to track down and adapt old hardware and software to the current systems. On 25 February, contact with IMAGE was again lost, but if reestablished, NASA may decide to fund a restarted mission.\n\nIMAGE was the first spacecraft dedicated to imaging the Earth's magnetosphere. IMAGE was a spacecraft developed by the Medium-Class Explorers (MIDEX) program, and it was the first spacecraft dedicated to observing the magnetosphere of the Earth, producing comprehensive global images of plasma in the inner magnetosphere. The IMAGE craft was placed in a 1,000×46,000 km orbit around the Earth, with an inclination of 90° (passing over the poles) and a 14.2 hour period.\n\nBy acquiring images every 2 minutes in wavelengths invisible to the human eye, it allowed detailed study of the interaction of the solar wind with the magnetosphere and the magnetosphere's response during a magnetic storm. From its distant orbit, the spacecraft produced a wealth of images of the previously invisible region of space in the inner magnetosphere, exceeded all its scientific goals. A senior review in 2005, just previous to its loss, described the mission as \"extremely productive\", having confirmed several theoretical predictions (e.g., plasmasphere plumes, pre-midnight ring-current injection, and continuous antiparallel reconnection), discovered numerous new and unanticipated phenomena (e.g., plasmasphere shoulders, subauroral proton arcs, and a secondary interstellar neutral atom stream), and answered a set of outstanding questions regarding the source region of kilometric continuum radiation, the role of solar wind pressure pulses in ionospheric outflow, and the relationship between proton and electron auroras during substorms. When the spacecraft went silent in December 2005, it had already been approved a mission extension until 2010.\n\nCosts for IMAGE are estimated at , including the spacecraft, instruments, launch vehicle, and ground operations.\n\nIts science payload consists of three suites of instruments:\n\nThe Central Instrument Data Processor (CIDP) as well as the Command & Data Handling Subsystem (main on-board computer) were built around the mission-proven IBM RAD6000 avionics processors.\n\nOn 18 December 2005, the satellite failed to make an expected contact at 16:20 UTC. An earlier contact had ended successfully at 07:40 the same day with no sign of trouble. Over the following days and weeks, commands were sent \"blind\" to reset the transmitter, change antennas, and otherwise attempt to re-establish contact with the spacecraft, but no signal (not even an unmodulated carrier wave) was received. Recovery efforts included using different DSN antennas, using non-NASA ground stations in case there was some systematic DSN error, transmitting no commands for several days to trigger a 72-hour watchdog timer, increasing transmit power in case the antenna was badly misaligned, and optical and radar observations of the satellite to check for debris, change in spin rate or change in orbit indicative of a collision or other damage.\n\nThe spacecraft was also commanded to slightly increase its spin rate and asymmetrically turn on its heaters. If observed, these would indicate that it could receive commands but not transmit. Neither change was seen and analysis later indicated that the temperature change would have been undetectable. An attempt to observe the craft's temperature to determine if it was completely dead or consuming the power expected in safe mode was inconclusive.\n\nA careful failure analysis revealed that, among plausible causes for an abrupt bidirectional loss of communication, the Solid State Power Converter (SSPC) for the transponder had, among its features, an \"instant trip\" shutdown in response to a high-current () short circuit. Critically, such a shutdown was \"not\" reported in the power supply's telemetry output and this lack was not documented. Because it was undocumented the spacecraft's hardware and software had no provision for attempting to reset the SSPC if it reported good status. This would result in the observed symptoms: no radio communication with an apparently undamaged spacecraft.\n\nAlthough such a short circuit would be almost impossible without fatal damage to the spacecraft, the shutdown could be falsely triggered by a radiation-induced single event upset. It could be simply fixed by power-cycling the supply, but the spacecraft design left no way to send such a command, nor was one built in.\n\nThe same problem with the same model of power supply had affected the EO-1 and WMAP satellites (launched after IMAGE), but they were able to recover.\n\nIn January 2006, NASA declared the mission over, declaring that \"Preliminary analysis indicated the craft's power supply subsystems failed, rendering it lifeless.\" Despite this, they continued to try and establish contact. In early 2006, NASA convened a board of experts to figure out what went wrong. After several months they created a report in which they theorized that IMAGE had tripped a power breaker and might fix itself.\n\nIt was hoped that an eclipse when the spacecraft passed through the Earth's shadow in October 2007 would result in a sufficiently deep supply voltage sag that it would trigger a total bus reset, which would cause a power cycle of the suspect supply. However, attempts to contact the craft after this eclipse were not successful.\n\nOn 20 January 2018, IMAGE was found by Canadian radio amateur and satellite tracker Scott Tilley to be broadcasting, and he reported it to NASA. He had been scanning the S-band (microwaves) in the hopes of finding the Zuma satellite.\n\nOn 24 January 2018, Richard Burley of NASA reported that they were trying to establish communication with the satellite using the NASA Deep Space Network. Two days later, Burley reported that engineers at Goddard Space Flight Center successfully acquired the signal, and confirmed on 30 January 2018 that IMAGE is the source. It is not known when the satellite started broadcasting, but re-examination of old data recorded by Tilley and fellow satellite tracker Cees Bassa showed transmissions from the same satellite in October 2016 and May 2017. Bassa hypothesized that while the 2007 eclipse did not manage to reset the satellite, another one did the trick, probably sometime between 2014 and 2016.\n\nOn 8 February 2018, NASA published a detailed account of the IMAGE satellite's recovery. The satellite was transmitting data beyond simple telemetry, indicating that some of its six onboard instruments were still active. NASA engineers are attempting to determine the satellite's status, but since the software and hardware type used in the IMAGE Mission Operations Center have been discarded and no longer exist, they are in the process of adapting old software and databases to their modern systems and track down replacement hardware.\n\nOn 25 February 2018, NASA again lost contact with the satellite, but not in the same manner it did in 2005. Richard Burley, former IMAGE mission director, stated that he believes there is an issue with IMAGE's spin axis in relation to its medium-gain antenna placement. If NASA can regain control of the spacecraft, and the status of data and ground systems can be assessed, it will decide if it can fund a mission restart.\n\nOn 4 March 2018, the Applied Physics Laboratory at Johns Hopkins University reported detecting the signal from the satellite, but it was too faint to lock onto.\n\nOn 9 May 2018, Scott Tilley again detected a strong signal from IMAGE. Hours later NASA and APL engineers had locked onto the signal and were receiving telemetry. Commands were transmitted to IMAGE, but for unknown reasons the spacecraft only acknowledged receipt of a fraction of those commands.\n\nOn 28 August 2018 NASA announced that the IMAGE team had stopped receiving any signals from the satellite, as previously happened in spring, and would continue to try sending commands.\n\n\n"}
{"id": "29182681", "url": "https://en.wikipedia.org/wiki?curid=29182681", "title": "Isoplanetic patch", "text": "Isoplanetic patch\n\nAngular size of patch in atmosphere over which electromagnetic waves in the same plane stay parallel. The patch size inversely affects the Fried Parameter and the resolution of optical telescopes. Adaptive optical telescopes uses a bright light source to identify the properties of the patch in the line of sight to the object of interest to correct for seeing effects.\n\nAstronomical seeing\noptical resolution\n\nBirney S, Gonzalez G, Oesper D \"observational astronomy\" second edition, Cambridge university press, 2006\n"}
{"id": "11336178", "url": "https://en.wikipedia.org/wiki?curid=11336178", "title": "Knudsen cell", "text": "Knudsen cell\n\nIn crystal growth, a Knudsen cell is an effusion evaporator source for relatively low partial pressure elementary sources (e.g. Ga, Al, Hg, As). Because it is easy to control the temperature of the evaporating material in Knudsen cells, they are commonly used in molecular-beam epitaxy.\n\nThe Knudsen effusion cell was developed by Martin Knudsen (1871-1949). A typical Knudsen cell contains a crucible (made of pyrolytic boron nitride, quartz, tungsten or graphite), heating filaments (often made of metal tantalum), water cooling system, heat shields, and an orifice shutter.\n\nThe Knudsen cell is used to measure the vapor pressures of a solid with very low vapor pressure. Such a solid forms a vapor at low pressure by sublimation. The vapor slowly effuses through the pinhole, and the loss of mass is proportional to the vapor pressure and can be used to determine this pressure. The heat of sublimation can also be determined by measuring the vapor pressure as a function of temperature, using the Clausius–Clapeyron relation.\n"}
{"id": "28541774", "url": "https://en.wikipedia.org/wiki?curid=28541774", "title": "LHC Accelerator Research Program", "text": "LHC Accelerator Research Program\n\nThe U.S. LHC Accelerator Research Program (LARP) coordinates research and development in the United States related to the Large Hadron Collider at CERN. Among other things, the program has contributed important instrumentation for initial LHC operation and is leading the way for the development of superconducting magnets based on Niobium-tin, which are proposed for\nfuture LHC upgrades.\n\nLARP was first proposed in 2003 as a collaboration between the Brookhaven National Laboratory, the Fermi National Accelerator Laboratory, and the Lawrence Berkeley National Laboratory. The SLAC National Accelerator Laboratory joined the program shortly thereafter.\n\nLARP is funded through the US Department of Energy. The total funding in Fiscal Year 2010 was $12.39M, distributed among the four labs involved.\n\n"}
{"id": "38437140", "url": "https://en.wikipedia.org/wiki?curid=38437140", "title": "List of RNA-Seq bioinformatics tools", "text": "List of RNA-Seq bioinformatics tools\n\nRNA-Seq\n\nDesign is a fundamental step of a particular RNA-Seq experiment. Some important questions like sequencing depth/coverage or how many biological or technical replicates must be carefully considered. Design review.\n\nQuality assessment of raw data is the first step of the bioinformatics pipeline of RNA-Seq. Often, is necessary to filter data, removing low quality sequences or bases (trimming), adapters, contaminations, overrepresented sequences or correcting errors to assure a coherent final result. \"Articles about common next-generation sequencing problems\".\n\n\nImprovement of the RNA-Seq quality, correcting the bias is a complex subject. Each RNA-Seq protocol introduces specific type of bias, each step of the process (such as the sequencing technology used) is susceptible to generate some sort of noise or type of error. Furthermore, even the species under investigation and the biological context of the samples are able to influence the results and introduce some kind of bias.\nMany sources of bias were already reported – GC content and PCR enrichment, rRNA depletion, errors produced during sequencing, priming of reverse transcription caused by random hexamers.\n\nDifferent tools were developed to attempt to solve each of the detected errors.\n\n\nRecent sequencing technologies normally require DNA samples to be amplified via polymerase chain reaction (PCR). Amplification often generates chimeric elements (specially from ribosomal origin) - sequences formed from two or more original sequences joined together.\n\nHigh-throughput sequencing errors characterization and their eventual correction.\n\n\nFurther tasks performed before alignment, namely paired-read mergers.\n\nAfter quality control, the first step of RNA-Seq analysis involves alignment \"(RNA-Seq alignment) \" of the sequenced reads to a reference genome (if available) or to a transcriptome database. See also and \"List of sequence alignment software\".\n\nShort aligners are able to align continuous reads (not containing gaps result of splicing) to a genome of reference. Basically, there are two types: 1) based on the Burrows-Wheeler transform method such as Bowtie and BWA, and 2) based on Seed-extend methods, Needleman-Wunsch or Smith-Waterman algorithms. The first group (Bowtie and BWA) is many times faster, however some tools of the second group tend to be more sensitive, generating more correctly aligned reads. See a comparative study of short aligners - \"comparative study\".\n\n\nMany reads span exon-exon junctions and can not be aligned directly by Short aligners, thus specific aligners were necessary - Spliced aligners. Some Spliced aligners employ Short aligners to align firstly unspliced/continuous reads (exon-first approach), and after follow a different strategy to align the rest containing spliced regions - normally the reads are split into smaller segments and mapped independently. See also.\n\nIn this case the detection of splice junctions is based on data available in databases about known junctions. This type of tools cannot identify new splice junctions. Some of this data comes from other expression methods like expressed sequence tags (EST).\n\n\nDe novo Splice aligners allow the detection of new Splice junctions without need to previous annotated information (some of these tools present annotation as a suplementar option). See also De novo Splice Aligners.\n\n\n\n\n\nThese tools perform normalization and calculate the abundance of each gene expressed in a sample. RPKM, FPKM and TPMs are some of the units employed to quantification of expression (RPKM-FPKM-TPMs video).\nSome software are also designed to study the variability of genetic expression between samples (differential expression). Quantitative and differential studies are largely determined by the quality of reads alignment and accuracy of isoforms reconstruction. Several studies are available comparing differential expression methods.\n\n\n\n\n\n\n\n\nGenome arrangements result of diseases like cancer can produce aberrant genetic modifications like fusions or translocations. Identification of these modifications play important role in carcinogenesis studies.\n\n\nSingle cell sequencing. The traditional RNA-Seq methodology is commonly known as \"bulk RNA-Seq\", in this case RNA is extracted from a group of cells or tissues, not from the individual cell like it happens in single cell methods. Some tools available to bulk RNA-Seq are also applied to single cell analysis, however to face the specificity of this technique new algorithms were developed. Comparative analysis of single-cell RNA-sequencing methods. A list of Single Cell tools can be found at \"Awesome Single Cell.\n\n\n\n\n\n\n\n\nThese Simulators generate \"in silico\" reads and are useful tools to compare and test the efficiency of algorithms developed to handle RNA-Seq data. Moreover, some of them make possible to analyse and model RNA-Seq protocols.See also \"Genetic Simulation Resources\" and some discussion about simulation at \"Biostars\".\n\n\nThe transcriptome is the total population of RNAs expressed in one cell or group of cells, including non-coding and protein-coding RNAs.\nThere are two types of approaches to assemble transcriptomes. Genome-guided methods use a reference genome (if possible a finished and high quality genome) as a template to align and assembling reads into transcripts. Genome-independent methods does not require a reference genome and are normally used when a genome is not available. In this case reads are assembled directly in transcripts. Some important comparative studies were already published.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8224308", "url": "https://en.wikipedia.org/wiki?curid=8224308", "title": "List of antioxidants in food", "text": "List of antioxidants in food\n\nThis is a list of antioxidants naturally occurring in food. Vitamin C and vitamin E – which are ubiquitous among raw plant foods – are confirmed as dietary antioxidants, whereas vitamin A becomes an antioxidant following metabolism of provitamin A beta-carotene and cryptoxanthin. Most food compounds listed as antioxidants – such as polyphenols common in colorful, edible plants – have antioxidant activity only in vitro, as their fate in vivo is to be rapidly metabolized and excreted, and the in vivo properties of their metabolites remain poorly understood. For antioxidants added to food to preserve them, see butylated hydroxyanisole and butylated hydroxytoluene.\n\nIn the following discussion, the term \"antioxidant\" refers mainly to non-nutrient compounds in foods, such as polyphenols, which have \"antioxidant capacity\" in vitro and so provide an artificial index of antioxidant strength – the ORAC measurement. Other than for dietary antioxidant vitamins – vitamin A, vitamin C and vitamin E – no food compounds have been proved with antioxidant efficacy in vivo. Accordingly, regulatory agencies like the Food and Drug Administration of the United States and the European Food Safety Authority (EFSA) have published guidance disallowing food product labels to claim an inferred antioxidant benefit when no such physiological evidence exists.\n\nDespite the above discussion implying that ORAC-rich foods with polyphenols may provide antioxidant benefits when in the diet, there remains no physiological evidence that any polyphenols have such actions or that ORAC has any relevance in the human body.\n\nOn the contrary, research indicates that although polyphenols are antioxidants in vitro, antioxidant effects in vivo are probably negligible or absent. By non-antioxidant mechanisms still undefined, polyphenols may affect mechanisms of cardiovascular disease or cancer.\n\nThe increase in antioxidant capacity of blood seen after the consumption of polyphenol-rich (ORAC-rich) foods is not caused directly by the polyphenols, but most likely results from increased uric acid levels derived from metabolism of flavonoids. According to Frei, \"we can now follow the activity of flavonoids in the body, and one thing that is clear is that the body sees them as foreign compounds and is trying to get rid of them.\" Another mechanism may be the increase in activities of paraoxonases by dietary antioxidants which can reduce oxidative stress.\n\n\n\n\n\nNatural phenols are a class of molecules found in abundance in plants.\n\nFlavonoids, a subset of polyphenol antioxidants, are present in many berries, as well as in coffee and tea.\n\n\n\nMany common foods contain rich sources of polyphenols which have antioxidant properties only in test tube studies. As interpreted by the Linus Pauling Institute, dietary polyphenols have little or no direct antioxidant food value following digestion. Not like controlled test tube conditions, the fate of flavones or polyphenols in vivo shows they are poorly absorbed and poorly conserved (less than 5%), so that most of what is absorbed exists as metabolites modified during digestion, destined for rapid excretion.\n\nSpices, herbs, and essential oils are rich in polyphenols in the plant itself and shown with antioxidant potential in vitro. Typical spices high in polyphenols (confirmed in vitro) are clove, cinnamon, oregano, turmeric, cumin, parsley, basil, curry powder, mustard seed, ginger, pepper, chili powder, paprika, garlic, coriander, onion and cardamom. Typical herbs are sage, thyme, marjoram, tarragon, peppermint, oregano, savory, basil and dill weed.\n\nDried fruits are a good source of polyphenols by weight/serving size as the water has been removed making the ratio of polyphenols higher. Typical dried fruits are pears, apples, plums, peaches, raisins, figs and dates. Dried raisins are high in polyphenol count. Red wine is high in total polyphenol count which supplies antioxidant quality which is unlikely to be conserved following digestion (see section below).\n\nDeeply pigmented fruits like cranberries, blueberries, plums, blackberries, raspberries, strawberries, blackcurrants, figs, cherries, guava, oranges, mango, grape juice and pomegranate juice also have significant polyphenol content.\n\nTypical cooked vegetables rich in antioxidants are artichokes, cabbage, broccoli, asparagus, avocados, beetroot and spinach.\n\nNuts are a moderate source of polyphenol antioxidants. Typical nuts are pecans, walnuts, hazelnuts, pistachio, almonds, cashew nuts, macadamia nuts and peanut butter.\n\nSorghum bran, cocoa powder, and cinnamon are rich sources of procyanidins, which are large molecular weight compounds found in many fruits and some vegetables. Partly due to the large molecular weight (size) of these compounds, their amount actually absorbed in the body is low, an effect also resulting from the action of stomach acids, enzymes and bacteria in the gastrointestinal tract where smaller derivatives are metabolized and prepared for rapid excretion.\n\n\n"}
{"id": "33562655", "url": "https://en.wikipedia.org/wiki?curid=33562655", "title": "List of centers and institutes at the Perelman School of Medicine", "text": "List of centers and institutes at the Perelman School of Medicine\n\nThis list contains the names of the centers and institutes at the Perelman School of Medicine at the University of Pennsylvania in alphabetical order with their external links.\n\n"}
{"id": "851933", "url": "https://en.wikipedia.org/wiki?curid=851933", "title": "List of lakes of Minnesota", "text": "List of lakes of Minnesota\n\nThis is a list of lakes of Minnesota. Although promoted as the \"Land of 10,000 Lakes,\" Minnesota has 11,842 lakes of 10 acres or more. The 1968 state survey found 15,291 lake basins, of which 3,257 were dry. If all basins over 2.5 acres were counted, Minnesota would have 21,871 lakes. The prevalence of lakes has generated many repeat names. For example, there are more than 200 Mud Lakes, 150 Long Lakes, and 120 Rice Lakes. All but four Minnesota counties (Mower, Olmsted, Pipestone and Rock) contain at least one natural lake. Minnesota's lakes provide 44,926 miles of shoreline, more than the combined lake (~32,000 mi) and coastal (3,427 mi) shorelines of California.\n\n\n"}
{"id": "37894720", "url": "https://en.wikipedia.org/wiki?curid=37894720", "title": "List of primates by population", "text": "List of primates by population\n\nThis is a list of primate species by estimated global population. This list is not comprehensive, as not all primates have had their numbers quantified.\n"}
{"id": "2450817", "url": "https://en.wikipedia.org/wiki?curid=2450817", "title": "Lunar theory", "text": "Lunar theory\n\nLunar theory attempts to account for the motions of the Moon. There are many small variations (or perturbations) in the Moon's motion, and many attempts have been made to account for them. After centuries of being problematic, lunar motion is now modeled to a very high degree of accuracy (see section Modern developments).\n\nLunar theory includes:\n\nLunar theory has a history of over 2000 years of investigation. Its more modern developments have been used over the last three centuries for fundamental scientific and technological purposes, and are still being used in that way.\n\nApplications of lunar theory have included the following:-\n\nThe Moon has been observed for millennia. Over these ages, various levels of care and precision have been possible, according to the techniques of observation available at any time. There is a correspondingly long history of lunar theories: it stretches from the times of the Babylonian and Greek astronomers, down to modern lunar laser ranging.\n\nAmong notable astronomers and mathematicians down the ages, whose names are associated with lunar theories, are --\n\n\nand other notable mathematical astronomers also made significant contributions, including: Edmond Halley; Philippe Gustave le Doulcet, Comte de Pontécoulant; John Couch Adams; George William Hill; and Simon Newcomb.\n\nThe history can be considered to fall into three parts: from ancient times to Newton; the period of classical (Newtonian) physics; and modern developments.\n\nOf Babylonian astronomy, practically nothing was known to historians of science before the 1880s. Surviving ancient writings of Pliny had made bare mention of three astronomical schools in Mesopotamia – at Babylon, Uruk, and 'Hipparenum' (possibly 'Sippar'). But definite modern knowledge of any details only began when Joseph Epping deciphered cuneiform texts on clay tablets from a Babylonian archive: in these texts he identified an ephemeris of positions of the Moon. Since then, knowledge of the subject, still fragmentary, has had to be built up by painstaking analysis of deciphered texts, mainly in numerical form, on tablets from Babylon and Uruk (no trace has yet been found of anything from the third school mentioned by Pliny).\n\nTo the Babylonian astronomer Kidinnu (in Greek or Latin, Kidenas or Cidenas) has been attributed the invention (5th or 4th century BC) of what is now called “System  B” for predicting the position of the moon, taking account that the moon continually changes its speed along its path relative to the background of fixed stars. This system involved calculating daily stepwise changes of lunar speed, up or down, with a minimum and a maximum approximately each month. The basis of these systems appears to have been arithmetical rather than geometrical, but they did approximately account for the main lunar inequality now known as the equation of the center.\n\nThe Babylonians kept very accurate records for hundreds of years of new moons and eclipses. Some time between the years 500 BC and 400 BC they identified and began to use the 19 year cyclic relation between lunar months and solar years now known as the Metonic cycle.\n\nThis helped them built up a numerical theory of the main irregularities in the Moon's motion, reaching remarkably good estimates for the (different) periods of the three most prominent features of the Moon's motion:\n\nThe Babylonian estimate for the synodic month was adopted for the greater part of two millennia by Hipparchus, Ptolemy, and medieval writers (and it is still in use as part of the basis for the calculated Hebrew (Jewish) calendar).\n\nThereafter, from Hipparchus and Ptolemy in the Bithynian and Ptolemaic epochs down to the time of Newton's work in the seventeenth century, lunar theories were composed mainly with the help of geometrical ideas, inspired more or less directly by long series of positional observations of the moon. Prominent in these geometrical lunar theories were combinations of circular motions – applications of the theory of epicycles.\n\nHipparchus, whose works are mostly lost and known mainly from quotations by other authors, assumed that the Moon moved in a circle inclined at 5° to the ecliptic, rotating in a retrograde direction (i.e. opposite to the direction of annual and monthly apparent movements of the Sun and Moon relative to the fixed stars) once in 18 years. The circle acted as a deferent, carrying an epicycle along which the Moon was assumed to move in a retrograde direction. The center of the epicycle moved at a rate corresponding to the mean change in Moon's longitude, while the period of the Moon around the epicycle was an anomalistic month. This epicycle approximately provided for what was later recognized as the elliptical inequality, the equation of the center, and its size approximated to an equation of the center of about 5° 1'. This figure is much smaller than the modern value: but it is close to the difference between the modern coefficients of the equation of the center (1st term) and that of the evection: the difference is accounted for by the fact that the ancient measurements were taken at times of eclipses, and the effect of the evection (which subtracts under those conditions from the equation of the center) was at that time unknown and overlooked. \"For further information see also separate article Evection.\"\n\nPtolemy's work the \"Almagest\" had wide and long-lasting acceptance and influence for over a millennium. He gave a geometrical lunar theory that improved on that of Hipparchus by providing for a second inequality of the Moon's motion, using a device that made the apparent apogee oscillate a little – \"prosneusis\" of the epicycle. This \"second inequality\" or \"second anomaly\" accounted rather approximately, not only for the equation of the center, but also for what became known (much later) as the evection. But this theory, applied to its logical conclusion, would make the distance (and apparent diameter) of the Moon appear to vary by a factor of about 2, which is clearly not seen in reality. (The apparent angular diameter of the Moon does vary monthly, but only over a much narrower range of about 0.49°–0.55°.) This defect of the Ptolemaic theory led to proposed replacements by Ibn al-Shatir in the 14th century and by Copernicus in the 16th century.\n\nSignificant advances in lunar theory made by the Arab astronomer, Ibn al-Shatir (1304–1375). Drawing on the observation that the distance to the Moon did not change as drastically as required by Ptolemy's lunar model, he produced a new lunar model that replaced Ptolemy's crank mechanism with a double epicycle model that reduced the computed range of distances of the Moon from the Earth. A similar lunar theory, developed some 150 years later by the Renaissance astronomer Nicolaus Copernicus, had the same advantage concerning the lunar distances.\n\nTycho Brahe and Johannes Kepler refined the Ptolemaic lunar theory, but did not overcome its central defect of giving a poor account of the (mainly monthly) variations in the Moon's distance, apparent diameter and parallax. Their work added to the lunar theory three substantial further discoveries.\n\nThe refinements of Brahe and Kepler were recognized by their immediate successors as improvements, but their seventeenth-century successors tried numerous alternative geometrical configurations for the lunar motions to improve matters further. A notable success was achieved by Jeremiah Horrocks, who proposed a scheme involving an approximate 6 monthly libration in the position of the lunar apogee and also in the size of the elliptical eccentricity. This scheme had the great merit of giving a more realistic description of the changes in distance, diameter and parallax of the Moon.\n\nA first gravitational period for lunar theory started with the work of Newton. He was the first to define the problem of the perturbed motion of the Moon in recognisably modern terms. His groundbreaking work is shown for example in the \"Principia\" in all versions including the first edition published in 1687.\n\nNewton identified how to evaluate the perturbing effect on the relative motion of the Earth and Moon, arising from their gravity towards the Sun, in Book 1, Proposition 66, and in Book 3, Proposition 25. The starting-point for this approach is Corollary VI to the laws of motion. This shows that if the external accelerative forces from some massive body happens to act equally and in parallel on some different other bodies considered, then those bodies would be affected equally, and in that case their motions (relative to each other) would continue as if there were no such external accelerative forces at all. It is only in the case that the external forces (e.g. in Book 1, Prop. 66, and Book 3, Prop. 25, the gravitational attractions towards the Sun) are different in size or in direction in their accelerative effects on the different bodies considered (e.g. on the Earth and Moon), that consequent effects are appreciable on the relative motions of the latter bodies. (Newton referred to \"accelerative forces\" or \"accelerative gravity\" due to some external massive attractor such as the Sun. The measure he used was the acceleration that the force tends to produce (in modern terms, force per unit mass), rather than what we would now call the force itself.)\n\nThus Newton concluded that it is only the difference between the Sun's accelerative attraction on the Moon and the Sun's attraction on the Earth that perturbs the motion of the Moon relative to the Earth.\n\nNewton then in effect used vector decomposition of forces, to carry out this analysis. In Book 1, Proposition 66 and in Book 3, Proposition 25, he showed by a geometrical construction, starting from the total gravitational attraction of the Sun on the Earth, and of the Sun on the Moon, the difference that represents the perturbing effect on the motion of the Moon relative to the Earth. In summary, line LS in Newton's diagram as shown below represents the size and direction of the perturbing acceleration acting on the Moon in the Moon's current position P (line LS does not pass through point P, but the text shows that this is not intended to be significant, it is a result of the scale factors and the way the diagram has been built up).\nShown here is Newton's diagram from the first (1687) Latin edition of the \"Principia\" (Book 3, Proposition 25, at p. 434). Here he introduced his analysis of perturbing accelerations on the Moon in the Sun-Earth-Moon system. Q represents the Sun, S the Earth, and P the Moon.\n\nParts of this diagram represent distances, other parts gravitational accelerations (attractive forces per unit mass). In a dual significance, SQ represents the Earth-Sun distance, and then it also represents the size and direction of the Earth-Sun gravitational acceleration. Other distances in the diagram are then in proportion to distance SQ. Other attractions are in proportion to attraction SQ.\n\nThe Sun's attractions are SQ (on the Earth) and LQ (on the Moon). The size of LQ is drawn so that the ratio of attractions LQ:SQ is the inverse square of the ratio of distances PQ:SQ. (Newton constructs KQ=SQ, giving an easier view of the proportions.) The Earth's attraction on the Moon acts along direction PS. (But line PS signifies only distance and direction so far, nothing has been defined about the scale factor between solar and terrestrial attractions).\n\nAfter showing solar attractions LQ on the Moon and SQ on the Earth, on the same scale, Newton then makes a vector decomposition of LQ into components LM and MQ. Then he identifies the perturbing acceleration on the Moon as the difference of this from SQ. SQ and MQ are parallel to each other, so SQ can be directly subtracted from MQ, leaving MS. The resulting difference, after subtracting SQ from LQ, is therefore the vector sum of LM and MS: these add up to a perturbing acceleration LS.\n\nLater Newton identified another resolution of the perturbing acceleration LM+MS = LS, into orthogonal components: a transverse component parallel to LE, and a radial component, effectively ES.\nNewton's diagrammatic scheme, since his time, has been re-presented in other and perhaps visually clearer ways. Shown here is a vector presentation indicating, for two different positions, P1 and P2, of the Moon in its orbit around the Earth, the respective vectors LS1 and LS2 for the perturbing acceleration due to the Sun. The Moon's position at P1 is fairly close to what it was at P in Newton's diagram; corresponding perturbation LS1 is like Newton's LS in size and direction. At another position P2, the Moon is farther away from the Sun than the Earth is, the Sun's attraction LQ2 on the Moon is weaker than the Sun's attraction SQ=SQ2 on the Earth, and then the resulting perturbation LS2 points obliquely away from the Sun.\nConstructions like those in Newton's diagram can be repeated for many different positions of the Moon in its orbit. For each position, the result is a perturbation vector like LS1 or LS2 in the second diagram. Shown here is an often-presented form of the diagram that summarises sizes and directions of the perturbation vectors for many different positions of the Moon in its orbit. Each small arrow is a perturbation vector like LS, applicable to the Moon in the particular position around the orbit from which the arrow begins. The perturbations on the Moon when it is nearly in line along the Earth-Sun axis, i.e. near new or full moon, point outwards, away from the Earth. When the Moon-Earth line is 90° from the Earth-Sun axis they point inwards, towards the Earth, with a size that is only half the maximum size of the axial (outwards) perturbations. (Newton gave a rather good quantitative estimate for the size of the solar perturbing force: at quadrature where it adds to the Earth's attraction he put it at of the mean terrestrial attraction, and twice as much as that at the new and full moons where it opposes and diminishes the Earth's attraction.)\n\nNewton also showed that the same pattern of perturbation applies, not only to the Moon, in its relation to the Earth as disturbed by the Sun, but also to other particles more generally in their relation to the solid Earth as disturbed by the Sun (or by the Moon); for example different portions of the tidal waters at the Earth's surface. The study of the common pattern of these perturbing accelerations grew out of Newton's initial study of the perturbations of the Moon, which he also applied to the forces moving tidal waters. Nowadays this common pattern itself has become often known as a tidal force whether it is being applied to the disturbances of the motions of the Moon, or of the Earth's tidal waters – or of the motions of any other object that suffers perturbations of analogous pattern.\n\nAfter introducing his diagram 'to find the force of the Sun to perturb the Moon' in Book 3, Proposition 25, Newton developed a first approximation to the solar perturbing force, showing in further detail how its components vary as the Moon follows its monthly path around the Earth. He also took the first steps in investigating how the perturbing force shows its effects by producing irregularities in the lunar motions. (In this part of the enterprise, Newton's success was more limited: it is relatively uncomplicated to define the perturbing forces, but heavy complexities soon arise in the problem of working out the resulting motions, and these were to challenge mathematical astronomers for two centuries after Newton's initial definition of the problem and indication of the directions to take in solving it.)\n\nFor a selected few of the lunar inequalities, Newton showed in some quantitative detail how they arise from the solar perturbing force.\n\nMuch of this lunar work of Newton's was done in the 1680s, and the extent and accuracy of his first steps in the gravitational analysis was limited by several factors, including his own choice to develop and present the work in what was, on the whole, a difficult geometrical way, and by the limited accuracy and uncertainty of many astronomical measurements in his time.\n\nThe main aim of Newton's successors, from Leonhard Euler, Alexis Clairaut and Jean d'Alembert in the mid-eighteenth century, down to E.W. Brown in the late nineteenth and early twentieth century, was to account completely and much more precisely for the moon's motions on the basis of Newton's laws, i.e. the laws of motion and of universal gravitation by attractions inversely proportional to the squares of the distances between the attracting bodies. They also wished to put the inverse-square law of gravitation to the test, and for a time in the 1740s it was seriously doubted, on account of what was then thought to be a large discrepancy between the Newton-theoretical and the observed rates in the motion of the lunar apogee. However Clairaut showed shortly afterwards (1749–50) that at least the major cause of the discrepancy lay not in the lunar theory based on Newton's laws, but in excessive approximations that he and others had relied on to evaluate it.\n\nMost of the improvements in theory after Newton were made in algebraic form: they involved voluminous and highly laborious amounts of infinitesimal calculus and trigonometry. It also remained necessary, for completing the theories of this period, to refer to observational measurements.\n\nThe lunar theorists used (and invented) many different mathematical approaches to analyse the gravitational problem. Not surprisingly, their results tended to converge. From the time of the earliest gravitational analysts among Newton's successors, Euler, Clairaut and d'Alembert, it was recognized that nearly all of the main lunar perturbations could be expressed in terms of just a few angular arguments and coefficients. These can be represented by:\n\nFrom these basic parameters, just four basic differential angular arguments are enough to express, in their different combinations, nearly all of the most significant perturbations of the lunar motions. They are given here with their conventional symbols due to Delaunay; they are sometimes known as the Delaunay arguments:\n\nThis work culminated into Brown's lunar theory (1897-1908) and \"Tables of the Motion of the Moon\" (1919). These were used in the American Ephemeris and Nautical Almanac until 1968, and in a modified form until 1984.\n\nSeveral of the largest lunar perturbations in longitude (contributions to the difference in its true ecliptic longitude relative to its mean longitude) have been named. In terms of the differential arguments, they can be expressed in the following way, with coefficients rounded to the nearest second of arc (\"):\n\n\n\n\n\n\n\nThe analysts of the mid-18th century expressed the perturbations of the Moon's position in longitude using about 25-30 trigonometrical terms. However, work in the nineteenth and twentieth century led to very different formulations of the theory so these terms are no longer current. The number of terms needed to express the Moon's position with the accuracy sought at the beginning of the twentieth century was over 1400; and the number of terms needed to emulate the accuracy of modern numerical integrations based on laser-ranging observations is in the tens of thousands: there is no limit to the increase in number of terms needed as requirements of accuracy increase.\n\nSince the Second World War and especially since the 1960s, lunar theory has been further developed in a somewhat different way. This has been stimulated in two ways: on the one hand, by the use of automatic digital computation, and on the other hand, by modern observational data-types, with greatly increased accuracy and precision.\n\nWallace John Eckert, a student of Ernest William Brown who worked at IBM, used the experimental digital computers developed there after the Second World War for computation of astronomical ephemerides. One of the projects was to put Brown's lunar theory into the machine and evaluate the expressions directly. Another project was something entirely new: a numerical integration of the equations of motion for the Sun and the four major planets. This became feasible only after electronic digital computers became available. Eventually this led to the Jet Propulsion Laboratory Development Ephemeris series.\n\nIn the meantime, Brown's theory was improved with better constants and the introduction of Ephemeris Time and the removal of some empirical corrections associated with this. This led to the Improved Lunar Ephemeris (ILE), which, with some minor successive improvements, was used in the astronomical almanacs from 1960 through 1983 (ILE j=0 from 1960 to 1967, ILE j=1 from 1968 to 1971, ILE j=2 from 1972 to 1983), and was used in lunar landing missions.\n\nThe most significant improvement of position observations of the Moon have been the Lunar Laser Ranging measurements, obtained using Earth-bound lasers and special retro-reflectors placed on the surface of the Moon. The time-of-flight of a pulse of laser light to one of the reflectors and back gives a measure of the Moon's distance at that time. The first of five reflectors that are operational today was taken to the Moon in the \"Apollo 11\" spacecraft in July 1969 and placed in a suitable position on the Moon's surface by Neil Armstrong.\nIts precision is still being extended further by the Apache Point Observatory Lunar Laser-ranging Operation, established in 2005.\n\nThe lunar theory, as developed numerically to fine precision using these modern measures, is based on a larger range of considerations than the classical theories: it takes account not only of gravitational forces (with relativistic corrections) but also of many tidal and geophysical effects and a greatly extended theory of lunar libration. Like many other scientific fields this one has now developed so as to be based on the work of large teams and institutions. An institution notably taking one of the leading parts in these developments has been the Jet Propulsion Laboratory at California Institute of Technology; and names particularly associated with the transition, from the early 1970s onwards, from classical lunar theories and ephemerides towards the modern state of the science include those of J Derral Mulholland and J G Williams (and for the linked development of solar system (planetary) ephemerides E Myles Standish).\n\nSince the 1970s, the Jet Propulsion Laboratory (JPL) has produced a series of numerically integrated Development Ephemerides (numbered DExxx), incorporating Lunar Ephemerides (LExxx). Planetary and lunar ephemerides DE200/LE200 were used in the official Astronomical Almanac ephemerides for 1984–2002, and ephemerides DE405/LE405, of further improved accuracy and precision, have been in use as from the issue for 2003.\n\nIn parallel with these developments, a new class of analytical lunar theory has also been developed in recent years, notably the Ephemeride Lunaire Parisienne by Jean Chapront and Michelle Chapront-Touzé from the Bureau des Longitudes. Using computer-assisted algebra, the analytical developments have been taken further than previously could be done by the classical analysts working manually. Also, some of these new analytical theories (like ELP) have been fitted to the numerical ephemerides previously developed at JPL as mentioned above. The main aims of these recent analytical theories, in contrast to the aims of the classical theories of past centuries, have not been to generate improved positional data for current dates; rather, their aims have included the study of further aspects of the motion, such as long-term properties, which may not so easily be apparent from the modern numerical theories themselves.\n\n"}
{"id": "43390067", "url": "https://en.wikipedia.org/wiki?curid=43390067", "title": "MACS J0416.1-2403", "text": "MACS J0416.1-2403\n\nMACS J0416.1-2403 is a galaxy cluster at a redshift of z=0.397 with a mass 160 trillion times the mass of the Sun inside . Its mass out to a radius of was measured as 1.15 × 10 solar masses. The system was discovered during the Massive Cluster Survey, MACS. This cluster causes gravitational lensing of distant galaxies producing multiple images. In 2015, the galaxy cluster was announced as gravitationally lensing the most distant galaxy (\"z\" = 12). Based on the distribution of the multiple image copies, scientists have been able to deduce and map the distribution of dark matter.\n"}
{"id": "35058271", "url": "https://en.wikipedia.org/wiki?curid=35058271", "title": "MEMS testing", "text": "MEMS testing\n\nWhen looking at the electronic market it becomes obvious, as there is a need for production output, high system performance, product reliability and long lifecycle, for Microelectromechanical systems(MEMS) to create trust in the eyes of customers. If those conditions would not be met customers would not invest into technologies using MEMS, which justifies the need for testing as a part of a high quality standard.\n\nTesting is also fairly important from an economical point of view. As it is said that the failure cost increase by a factor of ten for each stage before it gets discovered.\nMost MEMS producers check their products at two distinct stages(at the wafer level, and the packaging), as well as random sampling on every stage.\nIf one includes this into cost calculation for a MEMS device the costs for testing amounts to 20-50% of the overall unit costs.\nEven when looking at producers that manufacture MEMS, and CMOS devices it is not really possible to reduce the costs by including the economy of scopes effect for testing, as both types of device. This is because even though about 80% of the processing is shared, only 20% of the tests are.\nTo decrease these costs for U.S manufactures the National Institute of Standards and Technologies(NIST) conducted several workshops and questionnaires to tackle this issue and increase competitiveness of US companies.\n\nDue to the wide variety of MEMS it is hard to be very specific as of what is tested the table below shows what is tested in general:\nTo test MEMS researchers came up with a wide variety of techniques that can display certain values. However, there is no single technology that can cover all; each has strengths as well as weaknesses.\n\nBelow is a list with all major and some minor technologies employed in MEMS testing:\n\n\nFollowing technologies were experimented with but are no longer considered for MEMS testing:\n\n\nAll these technologies have strengths and weaknesses, so in order to maximize the effectiveness of test equipment researchers combined technologies. For instance Christian Rembe, former researcher at UC Berkley, combined laser doppler vibrometry, white light interferometry and strobe video microscopy into one tool to eliminate each technologies weakness.\n"}
{"id": "420929", "url": "https://en.wikipedia.org/wiki?curid=420929", "title": "Metacorder", "text": "Metacorder\n\nThe Metacorder is a theoretical device described in the 2003 short story of the same name by Tristan Parker. As the story describes, the Metacorder is a computational device which does nothing other than monitoring its own activities. While in practice this would result in an endless loop similar to the print \"print\" quine, the story takes this idea and gives it a sort of intelligence which allows the Metacorder to consider and judge its own actions.\n\nThis is an example of constrained writing, both in that the story describes a single object over the course of several pages, and that it is done entirely in the voice of such an object being described. This double rule allows much playfulness, however, and the story ranges from realistic technical descriptions to vague, poetic musings while still keeping the same voice throughout.\n\n"}
{"id": "5813522", "url": "https://en.wikipedia.org/wiki?curid=5813522", "title": "Mouse hepatitis virus", "text": "Mouse hepatitis virus\n\nMouse hepatitis virus is a virus of the family \"Coronaviridae\", genus \"Betacoronavirus\".\n\nThe \"Murine coronavirus\", Mouse hepatitis virus (MHV), is a coronavirus that causes an epidemic murine illness with high mortality, especially among colonies of laboratory mice. Prior to the discovery of SARS-CoV, MHV had been the best-studied coronavirus both \"in vivo\" and \"in vitro\" as well as at the molecular level. Some strains of MHV cause a progressive demyelinating encephalitis in mice which has been used as a murine model for multiple sclerosis. Significant research efforts have been focused on elucidating the viral pathogenesis of these animal coronaviruses, especially by virologists interested in veterinary and zoonotic diseases.\n\n"}
{"id": "55212", "url": "https://en.wikipedia.org/wiki?curid=55212", "title": "Newton's laws of motion", "text": "Newton's laws of motion\n\nNewton's laws of motion are three physical laws that, together, laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it, and its motion in response to those forces. More precisely, the first law defines the force qualitatively, the second law offers a quantitative measure of the force, and the third asserts that a single isolated force doesn't exist. These three laws have been expressed in several ways, over nearly three centuries, and can be summarised as follows:\n\nThe three laws of motion were first compiled by Isaac Newton in his \"Philosophiæ Naturalis Principia Mathematica\" (\"Mathematical Principles of Natural Philosophy\"), first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems. For example, in the third volume of the text, Newton showed that these laws of motion, combined with his law of universal gravitation, explained Kepler's laws of planetary motion.\n\nA fourth law is often also described in the bibliography, which states that forces add up like vectors, that is, that forces obey the principle of superposition.\n\nNewton's laws are applied to objects which are idealised as single point masses, in the sense that the size and shape of the object's body are neglected to focus on its motion more easily. This can be done when the object is small compared to the distances involved in its analysis, or the deformation and rotation of the body are of no importance. In this way, even a planet can be idealised as a particle for analysis of its orbital motion around a star.\n\nIn their original form, Newton's laws of motion are not adequate to characterise the motion of rigid bodies and deformable bodies. Leonhard Euler in 1750 introduced a generalisation of Newton's laws of motion for rigid bodies called Euler's laws of motion, later applied as well for deformable bodies assumed as a continuum. If a body is represented as an assemblage of discrete particles, each governed by Newton's laws of motion, then Euler's laws can be derived from Newton's laws. Euler's laws can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle structure.\n\nNewton's laws hold only with respect to a certain set of frames of reference called Newtonian or inertial reference frames. Some authors interpret the first law as defining what an inertial reference frame is; from this point of view, the second law holds only when the observation is made from an inertial reference frame, and therefore the first law cannot be proved as a special case of the second. Other authors do treat the first law as a corollary of the second. The explicit concept of an inertial frame of reference was not developed until long after Newton's death.\n\nIn the given interpretation mass, acceleration, momentum, and (most importantly) force are assumed to be externally defined quantities. This is the most common, but not the only interpretation of the way one can consider the laws to be a definition of these quantities.\n\nNewtonian mechanics has been superseded by special relativity, but it is still useful as an approximation when the speeds involved are much slower than the speed of light.\n\nThe first law states that if the net force (the vector sum of all forces acting on an object) is zero, then the velocity of the object is constant. Velocity is a vector quantity which expresses both the object's speed and the direction of its motion; therefore, the statement that the object's velocity is constant is a statement that both its speed and the direction of its motion are constant.\n\nThe first law can be stated mathematically when the mass is a non-zero constant, as,\nConsequently,\n\nThis is known as \"uniform motion\". An object \"continues\" to do whatever it happens to be doing unless a force is exerted upon it. If it is at rest, it continues in a state of rest (demonstrated when a tablecloth is skilfully whipped from under dishes on a tabletop and the dishes remain in their initial state of rest). If an object is moving, it continues to move without turning or changing its speed. This is evident in space probes that continuously move in outer space. Changes in motion must be imposed against the tendency of an object to retain its state of motion. In the absence of net forces, a moving object tends to move along a straight line path indefinitely.\n\nNewton placed the first law of motion to establish frames of reference for which the other laws are applicable. The first law of motion postulates the existence of at least one frame of reference called a Newtonian or inertial reference frame, relative to which the motion of a particle not subject to forces is a straight line at a constant speed. Newton's first law is often referred to as the \"law of inertia\". Thus, a condition necessary for the uniform motion of a particle relative to an inertial reference frame is that the total net force acting on it is zero. In this sense, the first law can be restated as:\n\nNewton's first and second laws are valid only in an inertial reference frame. Any reference frame that is in uniform motion with respect to an inertial frame is also an inertial frame, i.e. Galilean invariance or the principle of Newtonian relativity.\n\nThe second law states that the rate of change of momentum of a body is directly proportional to the force applied, and this change in momentum takes place in the direction of the applied force.\nThe second law can also be stated in terms of an object's acceleration. Since Newton's second law is valid only for constant-mass systems, can be taken outside the differentiation operator by the constant factor rule in differentiation. Thus,\n\nwhere F is the net force applied, \"m\" is the mass of the body, and a is the body's acceleration. Thus, the net force applied to a body produces a proportional acceleration. In other words, if a body is accelerating, then there is a force on it. An application of this notation is the derivation of G Subscript C.\n\nConsistent with the first law, the time derivative of the momentum is non-zero when the momentum changes direction, even if there is no change in its magnitude; such is the case with uniform circular motion. The relationship also implies the conservation of momentum: when the net force on the body is zero, the momentum of the body is constant. Any net force is equal to the rate of change of the momentum.\n\nAny mass that is gained or lost by the system will cause a change in momentum that is not the result of an external force. A different equation is necessary for variable-mass systems (see below).\n\nNewton's second law is an approximation that is increasingly worse at high speeds because of relativistic effects.\n\nAn impulse J occurs when a force F acts over an interval of time Δ\"t\", and it is given by\nSince force is the time derivative of momentum, it follows that\nThis relation between impulse and momentum is closer to Newton's wording of the second law.\n\nImpulse is a concept frequently used in the analysis of collisions and impacts.\n\nVariable-mass systems, like a rocket burning fuel and ejecting spent gases, are not closed and cannot be directly treated by making mass a function of time in the second law; that is, the following formula is wrong:\n\nThe falsehood of this formula can be seen by noting that it does not respect Galilean invariance: a variable-mass object with F = 0 in one frame will be seen to have F ≠ 0 in another frame.\nThe correct equation of motion for a body whose mass \"m\" varies with time by either ejecting or accreting mass is obtained by applying the second law to the entire, constant-mass system consisting of the body and its ejected/accreted mass; the result is\n\nwhere u is the velocity of the escaping or incoming mass relative to the body. From this equation one can derive the equation of motion for a varying mass system, for example, the Tsiolkovsky rocket equation.\nUnder some conventions, the quantity u d\"m\"/d\"t\" on the left-hand side, which represents the advection of momentum, is defined as a force (the force exerted on the body by the changing mass, such as rocket exhaust) and is included in the quantity F. Then, by substituting the definition of acceleration, the equation becomes F = \"m\"a.\n\nThe third law states that all forces between two objects exist in equal magnitude and opposite direction: if one object \"A\" exerts a force F on a second object \"B\", then \"B\" simultaneously exerts a force F on \"A\", and the two forces are equal in magnitude and opposite in direction: F = −F. The third law means that all forces are \"interactions\" between different bodies, or different regions within one body, and thus that there is no such thing as a force that is not accompanied by an equal and opposite force. In some situations, the magnitude and direction of the forces are determined entirely by one of the two bodies, say Body \"A\"; the force exerted by Body \"A\" on Body \"B\" is called the \"action\", and the force exerted by Body \"B\" on Body \"A\" is called the \"reaction\". This law is sometimes referred to as the \"action-reaction law\", with F called the \"action\" and F the \"reaction\". In other situations the magnitude and directions of the forces are determined jointly by both bodies and it isn't necessary to identify one force as the \"action\" and the other as the \"reaction\". The action and the reaction are simultaneous, and it does not matter which is called the \"action\" and which is called \"reaction\"; both forces are part of a single interaction, and neither force exists without the other.\n\nThe two forces in Newton's third law are of the same type (e.g., if the road exerts a forward frictional force on an accelerating car's tires, then it is also a frictional force that Newton's third law predicts for the tires pushing backward on the road).\n\nFrom a conceptual standpoint, Newton's third law is seen when a person walks: they push against the floor, and the floor pushes against the person. Similarly, the tires of a car push against the road while the road pushes back on the tires—the tires and road simultaneously push against each other. In swimming, a person interacts with the water, pushing the water backward, while the water simultaneously pushes the person forward—both the person and the water push against each other. The reaction forces account for the motion in these examples. These forces depend on friction; a person or car on ice, for example, may be unable to exert the action force to produce the needed reaction force.\n\nFrom the original Latin of Newton's \"Principia\":\nTranslated to English, this reads:\nThe ancient Greek philosopher Aristotle had the view that all objects have a natural place in the universe: that heavy objects (such as rocks) wanted to be at rest on the Earth and that light objects like smoke wanted to be at rest in the sky and the stars wanted to remain in the heavens. He thought that a body was in its natural state when it was at rest, and for the body to move in a straight line at a constant speed an external agent was needed continually to propel it, otherwise it would stop moving. Galileo Galilei, however, realised that a force is necessary to change the velocity of a body, i.e., acceleration, but no force is needed to maintain its velocity. In other words, Galileo stated that, in the \"absence\" of a force, a moving object will continue moving. (The tendency of objects to resist changes in motion was what Johannes Kepler had called \"inertia\".) This insight was refined by Newton, who made it into his first law, also known as the \"law of inertia\"—no force means no acceleration, and hence the body will maintain its velocity. As Newton's first law is a restatement of the law of inertia which Galileo had already described, Newton appropriately gave credit to Galileo.\n\nThe law of inertia apparently occurred to several different natural philosophers and scientists independently, including Thomas Hobbes in his \"Leviathan\". The 17th-century philosopher and mathematician René Descartes also formulated the law, although he did not perform any experiments to confirm it.\n\nNewton's original Latin reads:\nThis was translated quite closely in Motte's 1729 translation as:\n\nAccording to modern ideas of how Newton was using his terminology, this is understood, in modern terms, as an equivalent of:\nThis may be expressed by the formula F = p', where p' is the time derivative of the momentum p. This equation can be seen clearly in the Wren Library of Trinity College, Cambridge, in a glass case in which Newton's manuscript is open to the relevant page.\n\nMotte's 1729 translation of Newton's Latin continued with Newton's commentary on the second law of motion, reading:\n\nThe sense or senses in which Newton used his terminology, and how he understood the second law and intended it to be understood, have been extensively discussed by historians of science, along with the relations between Newton's formulation and modern formulations.\n\nTranslated to English, this reads:\nNewton's Scholium (explanatory comment) to this law:\nIn the above, as usual, \"motion\" is Newton's name for momentum, hence his careful distinction between motion and velocity.\n\nNewton used the third law to derive the law of conservation of momentum; from a deeper perspective, however, conservation of momentum is the more fundamental idea (derived via Noether's theorem from Galilean invariance), and holds in cases where Newton's third law appears to fail, for instance when force fields as well as particles carry momentum, and in quantum mechanics. \n\nNewton's laws were verified by experiment and observation for over 200 years, and they are excellent approximations at the scales and speeds of everyday life. Newton's laws of motion, together with his law of universal gravitation and the mathematical techniques of calculus, provided for the first time a unified quantitative explanation for a wide range of physical phenomena.\n\nThese three laws hold to a good approximation for macroscopic objects under everyday conditions. However, Newton's laws (combined with universal gravitation and classical electrodynamics) are inappropriate for use in certain circumstances, most notably at very small scales, very high speeds (in special relativity, the Lorentz factor must be included in the expression for momentum along with the rest mass and velocity) or very strong gravitational fields. Therefore, the laws cannot be used to explain phenomena such as conduction of electricity in a semiconductor, optical properties of substances, errors in non-relativistically corrected GPS systems and superconductivity. Explanation of these phenomena requires more sophisticated physical theories, including general relativity and quantum field theory.\n\nIn quantum mechanics, concepts such as force, momentum, and position are defined by linear operators that operate on the quantum state; at speeds that are much lower than the speed of light, Newton's laws are just as exact for these operators as they are for classical objects. At speeds comparable to the speed of light, the second law holds in the original form F = dp/d\"t\", where F and p are four-vectors.\n\nIn modern physics, the laws of conservation of momentum, energy, and angular momentum are of more general validity than Newton's laws, since they apply to both light and matter, and to both classical and non-classical physics.\n\nThis can be stated simply, \"Momentum, energy and angular momentum cannot be created or destroyed.\"\n\nBecause force is the time derivative of momentum, the concept of force is redundant and subordinate to the conservation of momentum, and is not used in fundamental theories (e.g., quantum mechanics, quantum electrodynamics, general relativity, etc.). The standard model explains in detail how the three fundamental forces known as gauge forces originate out of exchange by virtual particles. Other forces, such as gravity and fermionic degeneracy pressure, also arise from the momentum conservation. Indeed, the conservation of 4-momentum in inertial motion via curved space-time results in what we call gravitational force in general relativity theory. The application of the space derivative (which is a momentum operator in quantum mechanics) to the overlapping wave functions of a pair of fermions (particles with half-integer spin) results in shifts of maxima of compound wavefunction away from each other, which is observable as the \"repulsion\" of the fermions.\n\nNewton stated the third law within a world-view that assumed instantaneous action at a distance between material particles. However, he was prepared for philosophical criticism of this action at a distance, and it was in this context that he stated the famous phrase \"I feign no hypotheses\". In modern physics, action at a distance has been completely eliminated, except for subtle effects involving quantum entanglement. (In particular, this refers to Bell's theorem – that no local model can reproduce the predictions of quantum theory.) Despite only being an approximation, in modern engineering and all practical applications involving the motion of vehicles and satellites, the concept of action at a distance is used extensively.\n\nThe discovery of the second law of thermodynamics by Carnot in the 19th century showed that not every physical quantity is conserved over time, thus disproving the validity of inducing the opposite metaphysical view from Newton's laws. Hence, a \"steady-state\" worldview based solely on Newton's laws and the conservation laws does not take entropy into account.\n\n\n\n"}
{"id": "51031643", "url": "https://en.wikipedia.org/wiki?curid=51031643", "title": "Rational inattention", "text": "Rational inattention\n\nIn economics, the theory of rational inattention deals with the effects of the cost of information acquisition on decision making. For example, when the information required for a decision is costly to acquire, the decision makers may rationally take decisions based on incomplete information, rather than incurring the cost to get the complete information.\n\n"}
{"id": "30911400", "url": "https://en.wikipedia.org/wiki?curid=30911400", "title": "Rudolph Glossop", "text": "Rudolph Glossop\n\nRudolph Glossop (17 February 1902 – 1 March 1993) was a mining and civil engineer and one of the founders of Geotechnical Engineering in the UK.\nThe Glossop Lecture at the Geological Society is named after him.\n\n"}
{"id": "12404937", "url": "https://en.wikipedia.org/wiki?curid=12404937", "title": "Runoff model (reservoir)", "text": "Runoff model (reservoir)\n\nA runoff model is a mathematical model describing the rainfall–runoff relations of a rainfall \"catchment area\", drainage basin or \"watershed\". More precisely, it produces a surface runoff hydrograph in response to a rainfall event, represented by and input as a hyetograph. In other words, the model calculates the conversion of rainfall into runoff.<br>\nA well known runoff model is the \"linear reservoir\", but in practice it has limited applicability.<br>\nThe runoff model with a \"non-linear reservoir\" is more universally applicable, but still it holds only for catchments whose surface area is limited by the condition that the rainfall can be considered more or less uniformly distributed over the area. The maximum size of the watershed then depends on the rainfall characteristics of the region. When the study area is too large, it can be divided into sub-catchments and the various runoff hydrographs may be combined using flood routing techniques.<br>\n\nRainfall-runoff models need to be calibrated before they can be used.\n\nThe hydrology of a linear reservoir (figure 1) is governed by two equations.\nwhere:<br>\nQ is the \"runoff\" or\" discharge\" <br>\nR is the \"effective rainfall\" or \"rainfall excess\" or \"recharge\" <br>\nA is the constant \"reaction factor\" or \"response factor\" with unit [1/T] <br>\nS is the water storage with unit [L] <br>\ndS is a differential or small increment of S<br> \ndT is a differential or small increment of T\n\nRunoff equation<br>\nA combination of the two previous equations results in a differential equation, whose solution is:\nThis is the \"runoff equation\" or \"discharge equation\", where Q1 and Q2 are the values of Q at time T1 and T2 respectively while T2−T1 is a small time step during which the recharge can be assumed constant.\n\nComputing the total hydrograph<br>\nProvided the value of A is known, the \"total hydrograph\" can be obtained using a successive number of time steps and computing, with the \"runoff equation\", the runoff at the end of each time step from the runoff at the end of the previous time step.\n\nUnit hydrograph<br>\nThe discharge may also be expressed as: Q = − dS/dT . Substituting herein the expression of Q in equation (1) gives the differential equation dS/dT = A.S, of which the solution is: S = exp(− A.t) . Replacing herein S by Q/A according to equation (1), it is obtained that: Q = A exp(− A.t) . This is called the instantaneous unit hydrograph (IUH) because the Q herein equals Q2 of the foregoing runoff equation using \"R\" = 0, and taking S as \"unity\" which makes Q1 equal to A according to equation (1).<br>\nThe availability of the foregoing \"runoff equation\" eliminates the necessity of calculating the \"total hydrograph\" by the summation of partial hydrographs using the \"IUH\" as is done with the more complicated convolution method.\n\nDetermining the response factor A<br>\nWhen the \"response factor\" A can be determined from the characteristics of the watershed (catchment area), the reservoir can be used as a \"deterministic model\" or \"analytical model\", see hydrological modelling.<br>\nOtherwise, the factor A can be determined from a data record of rainfall and runoff using the method explained below under \"non-linear reservoir\". With this method the reservoir can be used as a black box model.\n\nConversions<br>\n1 mm/day corresponds to 10 m/day per ha of the watershed<br>\n1 l/s per ha corresponds to 8.64 mm/day or 86.4 m/day per ha\n\nContrary to the linear reservoir, the non linear reservoir has a reaction factor A that is not a constant, but it is a function of S or Q (figure 2, 3).\n\nNormally A increases with Q and S because the higher the water level is the higher the discharge capacity becomes. The factor is therefore called Aq instead of A.<br>\nThe non-linear reservoir has \"no\" usable unit hydrograph.\n\nDuring periods without rainfall or recharge, i.e. when \"R\" = 0, the runoff equation reduces to\nor, using a \"unit time step\" (T2 − T1 = 1) and solving for Aq:\nHence, the reaction or response factor Aq can be determined from runoff or discharge measurements using \"unit time steps\" during dry spells, employing a numerical method.\n\nFigure 3 shows the relation between Aq (Alpha) and Q for a small valley (Rogbom) in Sierra Leone.<br>\nFigure 4 shows observed and \"simulated\" or \"reconstructed\" discharge hydrograph of the watercourse at the downstream end of the same valley.\n\nThe recharge, also called \"effective rainfall\" or \"rainfall excess\", can be modeled by a \"pre-reservoir\" (figure 6) giving the recharge as \"overflow\". The pre-reservoir knows the following elements:\n\nThe recharge during a unit time step (T2−T1=1) can be found from \"R\" = Rain − Sd<br>\nThe actual storage at the end of a \"unit time step\" is found as Sa2 = Sa1 + Rain − \"R\" − Ea, where Sa1 is the actual storage at the start of the time step.\n\nThe Curve Number method (CN method) gives another way to calculate the recharge. The \"initial abstraction\" herein compares with Sm − Si, where Si is the initial value of Sa.\n\nThe Nash model uses a series (cascade) of linear reservoirs in which each reservoir empties into the next until the runoff is obtained. For calibration, the model requires considerable research.\n\nFigures 3 and 4 were made with the RainOff program, designed to analyse rainfall and runoff using the non-linear reservoir model with a pre-reservoir. The program also contains an example of the hydrograph of an agricultural subsurface drainage system for which the value of A can be obtained from the system's characteristics.\n\nThe SMART hydrological model includes agricultural subsurface drainage flow, in addition to soil and groundwater reservoirs, to simulate the flow path contributions to streamflow.\n\nV\"flo\" is another software program for modeling runoff. V\"flo\" uses radar rainfall and GIS data to generate physics-based, distributed runoff simulation.\n\nThe WEAP (Water Evaluation And Planning) software platform models runoff and percolation from climate and land use data, using a choice of linear and non-linear reservoir models.\n\nThe RS MINERVE software platform simulates the formation of free surface run-off flow and its propagation in rivers or channels. The software is based on object-oriented programming and allows hydrologic and hydraulic modeling according to a semi-distributed conceptual scheme with different rainfall-runoff model such as HBV, GR4J, SAC-SMA or SOCONT.\n"}
{"id": "3161236", "url": "https://en.wikipedia.org/wiki?curid=3161236", "title": "SNOMED CT", "text": "SNOMED CT\n\nSNOMED CT or SNOMED Clinical Terms is a systematically organized computer processable collection of medical terms providing codes, terms, synonyms and definitions used in clinical documentation and reporting. SNOMED CT is considered to be the most comprehensive, multilingual clinical healthcare terminology in the world. The primary purpose of SNOMED CT is to encode the meanings that are used in health information and to support the effective clinical recording of data with the aim of improving patient care. SNOMED CT provides the core general terminology for electronic health records. SNOMED CT comprehensive coverage includes: clinical findings, symptoms, diagnoses, procedures, body structures, organisms and other etiologies, substances, pharmaceuticals, devices and specimens.\n\nSNOMED CT is maintained and distributed by SNOMED International, an international non-profit standards development organization, located in London, UK. SNOMED International is the trading name of the International Health Terminology Standards Development Organisation (IHTSDO), established in 2007.\n\nSNOMED CT provides for consistent information interchange and is fundamental to an interoperable electronic health record. It provides a consistent means to index, store, retrieve, and aggregate clinical data across specialties and sites of care. It also helps in organizing the content of electronic health records systems by reducing the variability in the way data are captured, encoded and used for clinical care of patients and research. SNOMED CT can be used to directly record clinical details of individuals in electronic patient records. It also provides the user with a number of linkages to clinical care pathways, shared care plans and other knowledge resources, in order to facilitate informed decision-making, and to support long-term patient care. The availability of free automatic coding tools and services, which can return a ranked list of SNOMED CT descriptors to encode any clinical report, could help healthcare professionals to navigate the terminology.\n\nSNOMED CT is a terminology that can cross-map to other international standards and classifications. Specific language editions are available which augment the international edition and can contain language translations, as well as additional national terms. For example, SNOMED CT-AU, released in December 2009 in Australia, is based on the international version of SNOMED CT, but encompasses words and ideas that are clinically and technically unique to Australia.\n\nSNOMED was started in 1965 as a Systematized Nomenclature of Pathology (SNOP) and was further developed into a logic-based health care terminology.\n\nSNOMED CT was created in 1999 by the merger, expansion and restructuring of two large-scale terminologies: SNOMED Reference Terminology (SNOMED RT), developed by the College of American Pathologists (CAP); and the Clinical Terms Version 3 (CTV3) (formerly known as the Read codes), developed by the National Health Service of the United Kingdom (NHS). The final product was released in January 2002.\n\nThe historical strength of SNOMED was its coverage of medical specialties. SNOMED RT, with over 120,000 concepts, was designed to serve as a common reference terminology for the aggregation and retrieval of pathology health care data recorded by multiple organizations and individuals. The strength of CTV3 was its terminologies for general practice. CTV3, with 200,000 interrelated concepts, was used for storing structured information about primary care encounters in individual, patient-based records. Currently, SNOMED CT contains more than 311,000 active concepts and provides the core general terminology for the electronic health record (EHR).\n\nIn July 2003, the National Library of Medicine (NLM), on behalf of the United States Department of Health and Human Services, entered into an agreement with the College of American Pathologists to make SNOMED CT available to U.S. users at no cost through the National Library of Medicine's Unified Medical Language System UMLS Metathesaurus. The contract provided NLM with a perpetual license for the core SNOMED CT (in Spanish and English) and its ongoing updates.\n\nIn April 2007, SNOMED CT intellectual property rights were transferred from the CAP to the International Health Terminology Standards Development Organisation (IHTSDO) in order to promote international adoption and use of SNOMED CT. Now trading as SNOMED International, the organization is responsible for \"ongoing maintenance, development, quality assurance, and distribution of SNOMED CT\" internationally\nand its Membership consists of a number of the world's leading e-health countries and territories, including: Argentina, Australia, Belgium, Brunei, Canada, Czech Republic, Chile, Denmark, Estonia, Hong Kong, Iceland, India, Ireland, Israel, Lithuania, Malaysia, Malta, Netherlands, New Zealand, Norway, Poland, Portugal, Singapore, Slovak Republic, Republic of Slovenia, Spain, Sweden, Switzerland, United Kingdom, United States and Uruguay.\n\nSNOMED CT is a multinational and multilingual terminology, which can manage different languages and dialects. SNOMED CT is currently available in American English, British English, Spanish, Danish and Swedish, with other translations underway or nearly completed in French and Dutch. SNOMED CT cross maps to other terminologies, such as: ICD-9-CM, ICD-10, ICD-O-3, ICD-10-AM, Laboratory LOINC and OPCS-4. It supports ANSI, DICOM, HL7, and ISO standards. SNOMED CT is currently used in a joint project with the World Health Organization (WHO) as the ontological basis of the upcoming ICD-11.\n\nSNOMED CT consists of four primary core components:\n\nSNOMED CT \"Concepts\" are representational units that categorize all the things that characterize healthcare processes and need to be recorded therein. In 2011, SNOMED CT included more than 311,000 concepts, which are uniquely identified by a concept ID, e.g. the concept 22298006 refers to \"Myocardial infarction\". All SNOMED CT concepts are organized into acyclic taxonomic (is-a) hierarchies; for example, \"Viral pneumonia\" IS-A \"Infectious pneumonia\" IS-A \"Pneumonia\" IS-A \"Lung disease\". Concepts may have multiple parents, for example \"Infectious pneumonia\" is also a child of \"Infectious disease\". The taxonomic structure allows data to be recorded and later accessed at different levels of aggregation.\nSNOMED CT concepts are linked by approximately 1,360,000 links, called \"relationships\".\n\nConcepts are further described by various clinical terms or phrases, called Descriptions, which are divided into \"Fully Specified Names\" (FSNs), \"Preferred Terms\" (PTs), and \"Synonyms\". Each Concept has exactly one FSN, which is unique across all of SNOMED CT. It has, in addition, exactly one PT, which has been decided by a group of clinicians to be the most common way of expressing the meaning of the concept. It may have zero to many Synonyms. Synonyms are additional terms and phrases used to refer to this concept. They do not have to be unique or unambiguous.\n\nSNOMED CT can be characterized as a multilingual thesaurus with an ontological foundation. Thesaurus-like features are concept–term relations such as the synonymous descriptions \"Acute coryza\", \"Acute nasal catarrh\", \"Acute rhinitis\", \"Common cold\" (as well as Spanish \"resfrío común\" and \"rinitis infecciosa\") for the concept 82272006.\n\nUnder ontological scrutiny, SNOMED-CT is a class hierarchy (with extensive overlap of classes in contrast to typical statistical classifications like ICD).\nThis means that the SNOMED CT concept 82272006 defines the class of all the individual disease instances that match the criteria for \"common cold\" (e.g., one patient may have \"head cold\" noted in their record, and another may have \"Acute coryza\"; both can be found as instances of \"common cold\").\nThe superclass (Is-A) Relation relates classes in terms of inclusion of their members. That is, all individual \"cold-processes\" are also included in all superclasses of the class Common Cold, such as Viral upper respiratory tract infection (Figure).\nSNOMED CT's relational statements are basically triplets of the form Concept – Relation – Concept, with Relation being from a small number of relation types (called linkage concepts), e.g. \"finding site\", \"due to\", etc. The interpretation of these triplets is (implicitly) based on the semantics of a simple Description logic (DL). E.g., the triplet \"Common Cold\" – causative agent – \"Virus\", corresponds to the first-order expression\n\ncodice_1\n\nor the more intuitive DL expression\n\ncodice_2\n\nIn the \"Common cold\" example the concept description is \"primitive\", which means that necessary criteria are given that must be met for each instance, without being sufficient for classifying a disorder as an instance of \"Common Cold\" . In contrast, the example \"Viral upper respiratory tract infection\" depicts a fully described concept, which is represented in description logic as follows:\n\nThis means that each and every individual disorder for which all definitional criteria are met can be classified as an instance of Viral upper respiratory tract infection.\n\nAs of 2011, SNOMED CT content limits itself to a subset of the EL++ formalism, restricting itself to the following operators:\n\nFor understanding the modelling, it is also important to look at the stated view of a concept versus the inferred view of the concept. In further considering the state view, SNOMED CT used in the past an modelling approach referred to as 'proximal parent' approach. After 2015, a superior approach called \"proximal primitive parent\" has been adopted.\n\nSNOMED CT provides a compositional syntax that can be used to create expressions that represent clinical ideas which are not explicitly represented by SNOMED CT concepts.\n\nFor example, there is no explicit concept for a \"third degree burn of left index finger caused by hot water\". However, using the compositional syntax it can be represented as\n\nSuch expressions are said to have been 'postcoordinated'. Post-coordination avoids the need to create large numbers of defined Concepts within SNOMED CT. However, many systems only allow for precoordinated representations. Reliable analysis and comparison of post-coordinated expressions is possible using appropriate algorithms machinery to efficiently process the expression taking account of the underlying description logic.\n\nMajor Electronic Health Record Systems (EHRS) have repeated complained to IHTSDO and other standards organizations about the \"complexity\" of post-coordinated expressions.\n\nFor example, the postcoordinated expression above can be transformed using a set of standard rules to the following \"normal form expression\" which enables comparison with similar concepts.\n\nThe international edition of SNOMED CT only includes human terms. In 2015, clearly veterinary concepts were moved into a SNOMED CT veterinary extension. This extension is managed by a team at Virginia Tech University.\n\nEarlier SNOMED versions had faceted structure ordered by semantic axes, requiring that more complex situations required to be coded by a coordination of different codes. This had two major shortcomings. On the one hand, the necessity of post-coordination was perceived as a user-unfriendly obstacle, which has certainly contributed to the rather low adoption of early SNOMED versions. On the other hand, uniform coding was difficult to obtain. E.g.,\"Acute appendicitis\" could be post-coordinated in three different ways with no means to compute semantic equivalences.\nSNOMED RT had addressed this problem by introducing description logic formula. With the addition of CTV3 a large number of concepts were redefined using formal expressions. However, the fusion with CTV3, as a historically grown terminology with many close-to user descriptions, introduced some problems which still affect SNOMED CT. In addition to a confusing taxonomic web of many hierarchical levels with massive multiple inheritance (e.g. there are 36 taxonomic ancestors for \"Acute appendicitis\"), many ambiguous, context-dependent concepts have found their way into SNOMED CT. Pre-coordination was sometimes pushed to extremes, so there are, for example, 350 different concepts for burns found on the head.\n\nA further phenomenon which characterizes parts of SNOMED CT is the so-called \"epistemic intrusion\".\nIn principle, the task of terminology (and even an ontology) should be limited to providing context-free term or class meanings. The contextualization of these representational units should be ideally the task of an information model.\nHuman language is misleading here, as we use syntactically similar expression to represent categorically distinct entities, e.g. \"Ectopic pregnancy\" vs. \"Suspected pregnancy\". The first one refers to a real pregnancy, the second one to a piece of (uncertain) \"information\". In SNOMED CT most (but not all) of these context-dependent concepts are concentrated in the subhierachy \"Situation with explicit context\". A major reason for why such concepts cannot be dispensed with is that SNOMED CT takes on, in many cases, the functionality of information models, as the latter do not exist in a given implementation.\n\nWith the establishment of IHTSDO, SNOMED CT became more accessible to a wider audience. Criticism of the state of the terminology was sparked by numerous substantive weaknesses as well as on the lack of quality assurance measures. From the beginning IHTSDO was open regarding such (also academic) criticism. In the last few years considerable progress has been made regarding quality assurance and tooling.\n\nThe need for a more principled ontological foundation was gradually accepted, as well as a better understanding of description logic semantics. Redesign priorities were formulated regarding observables, disorders, findings, substances, organisms etc. Translation guidelines were elaborated as well as guidelines for content submission requests and a strategy for the inclusion of pre-coordinated content. There are still known deficiencies regarding the \"ontological commitment\" of SNOMED CT, e.g., the clarification of which kind of entity is an instance of a given SNOMED CT concept. The same term can be interpreted as a disorder or a patient with a disorder, for example \"Tumour\" might denote a process or a piece of tissue; \"Allergy\" may denote an allergic reaction or just an allergic disposition. A more recent strategy is the use of rigorously typed upper-level ontologies to disambiguate SNOMED CT content.\n\nThe increased take-up of SNOMED CT for research into applications in daily use across the world to support patient care is leading to a larger engaged community. This has led to an increase in the resource allocated to authoring SNOMED CT terms as well as to an increase in collaboration to take SNOMED CT into a robust industry used standard. This is leading to an increase in the number of software tools and development of materials that contribute to knowledge base to support implementation. A number of on-line communities that focus on particular aspects of SNOMED CT and its implementation are also developing.\n\nIn theory, description logic reasoning can be applied to any new candidate post-coordinated expressions in order to assess whether it is a parent or ancestor of, a child or other descendent of, or semantically equivalent to any existing concept from the existing pre-coordinated concepts. However, partly as the continuing fall-out from the merger with CTV3, SNOMED still contains undiscovered semantically duplicate primitive and defined concepts. Additionally, many concepts remain primitive whilst their semantics can also be legitimately defined in terms of other primitives and roles concurrently in the system. Because of these omissions and actual or possible redundancies of semantic content, real-world performance of algorithms to infer subsumption or semantic equivalence will be unpredictably imperfect.\n\nUsing consistent rules is important for the quality of SNOMED CT. To that end, in 2009, a prototype Machine Readable Concept Model (MRCM) was created by the SNOMED CT team. In a follow up work, this model is being revised to utilize SNOMED CT expression constraints.\n\nSNOMED CT is a clinical terminology designed to capture and represent patient data for clinical purposes. The International Statistical Classification of Diseases and Related Health Problems (ICD) is an internationally used medical classification system; which is used to assign diagnostic and, in some national modifications, procedural codes in order to produce coded data for statistical analysis, epidemiology, reimbursement and resource allocation. Both systems use standardized definitions and form a common medical language used within electronic health record (EHR) systems. SNOMED CT enables information input into an EHR system during the course of patient care, while ICD facilitates information retrieval, or output, for secondary data purposes.\n\nLOINC is a terminology that contains laboratory tests. Since 2017, SNOMED International started creating terms for LOINC components and created a set of SNOMED CT expressions that capture the meaning of many LOINC terms.\n\nSNOMED CT is used in a number of different ways, some of which are:\n\nMore specifically, the following sample computer applications use SNOMED CT:\n\n\nSNOMED CT is maintained and distributed by SNOMED International, an international non-profit standards development organization, located in London, UK..\n\nThe use of SNOMED CT in production systems requires a license. There are two types of license:\n\nFor scientific research in medical informatics, for demonstrations or evaluation purposes SNOMED CT sources can be freely downloaded and used. The original SNOMED CT sources in tabular form are accessible by registered users of the Unified Medical Language System (UMLS) who have signed an agreement. Numerous online and offline browsers are available.\n\nThose wishing to obtain a license for its use and to download SNOMED CT should contact their National Release Centre, links to which are provided on the IHTSDO website.\n\nTo facilitate adoption of SNOMED CT and use of SNOMED CT in other standards, there are license free subsets. For example, a set of 7,314 codes and descriptions is free for use by users of DICOM-compliant software (without restriction to IHTSDO member countries).\n\nSNOMED CT concepts typically belong a single hierarchy (with the exception of drug-device combined concepts). Some hierarchies, have a concept model defined (e.g., clinical findings). For other domains (e.g., Organism, Substance, Qualifier value), there is no concept model yet defined.\n\nAs of 2016, the Event hierarchy does not have a concept model defined. In 2006, some concepts from the 'Clinical Finding' hierarchy were moved to the Event hierarchy. Those concepts retained some of their attributes. (e.g., causative agent)\n\nSNOMED International is working on creating a concept model for observable entities.\n\nPharmaceutical and biologic products are modeled using constructs of active ingredient, presentation strength, and basis of strength.\n\n\n"}
{"id": "26997", "url": "https://en.wikipedia.org/wiki?curid=26997", "title": "Scientist", "text": "Scientist\n\nA scientist is someone who conducts scientific research to advance knowledge in an area of interest.\n\nIn classical antiquity, there was no real ancient analog of a modern scientist. Instead, philosophers engaged in the philosophical study of nature called natural philosophy, a precursor of natural science. It was not until the 19th century that the term \"scientist\" came into regular use after it was coined by the theologian, philosopher, and historian of science William Whewell in 1833.\n\nIn modern times, many scientists have advanced degrees in an area of science and pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit environments.\n\nThe roles of \"scientists\", and their predecessors before the emergence of modern scientific disciplines, have evolved considerably over time. Scientists of different eras (and before them, natural philosophers, mathematicians, natural historians, natural theologians, engineers, and others who contributed to the development of science) have had widely different places in society, and the social norms, ethical values, and epistemic virtues associated with scientists—and expected of them—have changed over time as well. Accordingly, many different historical figures can be identified as early scientists, depending on which characteristics of modern science are taken to be essential.\n\nSome historians point to the Scientific Revolution that began in 16th century as the period when science in a recognizably modern form developed. It wasn't until the 19th century that sufficient socioeconomic changes occurred for scientists to emerge as a major profession.\n\nKnowledge about nature in classical antiquity was pursued by many kinds of scholars. Greek contributions to science—including works of geometry and mathematical astronomy, early accounts of biological processes and catalogs of plants and animals, and theories of knowledge and learning—were produced by philosophers and physicians, as well as practitioners of various trades. These roles, and their associations with scientific knowledge, spread with the Roman Empire and, with the spread of Christianity, became closely linked to religious institutions in most of European countries. Astrology and astronomy became an important area of knowledge, and the role of astronomer/astrologer developed with the support of political and religious patronage. By the time of the medieval university system, knowledge was divided into the \"trivium\"—philosophy, including natural philosophy—and the \"quadrivium\"—mathematics, including astronomy. Hence, the medieval analogs of scientists were often either philosophers or mathematicians. Knowledge of plants and animals was broadly the province of physicians.\n\nScience in medieval Islam generated some new modes of developing natural knowledge, although still within the bounds of existing social roles such as philosopher and mathematician. Many proto-scientists from the Islamic Golden Age are considered polymaths, in part because of the lack of anything corresponding to modern scientific disciplines. Many of these early polymaths were also religious priests and theologians: for example, Alhazen and al-Biruni were mutakallimiin; the physician Avicenna was a hafiz; the physician Ibn al-Nafis was a hafiz, muhaddith and ulema; the botanist Otto Brunfels was a theologian and historian of Protestantism; the astronomer and physician Nicolaus Copernicus was a priest. During the Italian Renaissance scientists like Leonardo Da Vinci, Michelangelo, Galileo Galilei and Gerolamo Cardano have been considered as the most recognizable polymaths.\n\nDuring the Renaissance, Italians made substantial contributions in science. Leonardo Da Vinci made significant discoveries in paleontology and anatomy. The Father of modern Science,\nGalileo Galilei, made key improvements on the thermometer and telescope which allowed him to observe and clearly describe the solar system. Descartes was not only a pioneer of analytic geometry but formulated a theory of mechanics and advanced ideas about the origins of animal movement and perception. Vision interested the physicists Young and Helmholtz, who also studied optics, hearing and music. Newton extended Descartes' mathematics by inventing calculus (contemporaneously with Leibniz). He provided a comprehensive formulation of classical mechanics and investigated light and optics. Fourier founded a new branch of mathematics — infinite, periodic series — studied heat flow and infrared radiation, and discovered the greenhouse effect. Girolamo Cardano, Blaise Pascal Pierre de Fermat, Von Neumann, Turing, Khinchin, Markov and Wiener, all mathematicians, made major contributions to science and probability theory, including the ideas behind computers, and some of the foundations of statistical mechanics and quantum mechanics. Many mathematically inclined scientists, including Galileo, were also musicians.\n\nThere are many compelling stories in medicine and biology, such as the development of ideas about the circulation of blood from Galen to Harvey.\n\nDuring the age of Enlightenment, Luigi Galvani, the pioneer of the bioelectromagnetics, discovered the animal electricity. He discovered that a charge applied to the spinal cord of a frog could generate muscular spasms throughout its body. Charges could make frog legs jump even if the legs were no longer attached to a frog. While cutting a frog leg, Galvani's steel scalpel touched a brass hook that was holding the leg in place. The leg twitched. Further experiments confirmed this effect, and Galvani was convinced that he was seeing the effects of what he called animal electricity, the life force within the muscles of the frog. At the University of Pavia, Galvani's colleague Alessandro Volta was able to reproduce the results, but was sceptical of Galvani's explanation.\n\nLazzaro Spallanzani is one of the most influential figures in experimental physiology and the natural sciences. His investigations have exerted a lasting influence on the medical sciences. He made important contributions to the experimental study of bodily functions and animal reproduction.\n\nFrancesco Redi discovered that microorganisms can cause disease. \n\nUntil the late 19th or early 20th century, scientists were still referred to as \"natural philosophers\" or \"men of science\".\n\nEnglish philosopher and historian of science William Whewell coined the term \"scientist\" in 1833, and it first appeared in print in Whewell's anonymous 1834 review of Mary Somerville's \"On the Connexion of the Physical Sciences\" published in the \"Quarterly Review\". Whewell's suggestion of the term was partly satirical, a response to changing conceptions of science itself in which natural knowledge was increasingly seen as distinct from other forms of knowledge. Whewell wrote of \"an increasing proclivity of separation and dismemberment\" in the sciences; while highly specific terms proliferated—chemist, mathematician, naturalist—the broad term \"philosopher\" was no longer satisfactory to group together those who pursued science, without the caveats of \"natural\" or \"experimental\" philosopher. Members of the British Association for the Advancement of Science had been complaining about the lack of a good term at recent meetings, Whewell reported in his review; alluding to himself, he noted that \"some ingenious gentleman proposed that, by analogy with \"artist\", they might form [the word] \"scientist\", and added that there could be no scruple in making free with this term since we already have such words as \"economist\", and \"atheist\"—but this was not generally palatable\".\n\nWhewell proposed the word again more seriously (and not anonymously) in his 1840 \"The Philosophy of the Inductive Sciences\":\n\nHe also proposed the term \"physicist\" at the same time, as a counterpart to the French word \"physicien\". Neither term gained wide acceptance until decades later; \"scientist\" became a common term in the late 19th century in the United States and around the turn of the 20th century in Great Britain. By the twentieth century, the modern notion of science as a special brand of information about the world, practiced by a distinct group and pursued through a unique method, was essentially in place.\n\nRamón y Cajal won the Nobel Prize in 1906 for his remarkable observations in neuroanatomy.\n\nMarie Curie became the first female to win the Nobel Prize and the first person to win it twice. Her efforts led to the development of nuclear energy and Radio therapy for the treatment of cancer. In 1922, she was appointed a member of the International Commission on Intellectual Co-operation by the Council of the League of Nations. She campaigned for scientist's right to patent their discoveries and inventions. She also campaigned for free access to international scientific literature and for internationally recognized scientific symbols.\n\nAs a profession, the scientist of today is widely recognized. \n\nIn modern times, many professional scientists are trained in an academic setting (e.g., universities and research institutes), mostly at the level of graduate schools. Upon completion, they would normally attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy (PhD), Doctor of Medicine (MD), Doctor of Engineering (DEng), or even a dual doctoral degree (e.g., MD, PhD). Although graduate education for scientists varies among institutions and countries, some common training requirements include specializing in an area of interest, publishing research findings in peer-reviewed scientific journals and presenting them at scientific conferences, giving lectures or teaching, and defending a thesis (or dissertation) during an oral examination. To aid them in this endeavor, graduate students often work under the guidance of a mentor, usually a senior scientist, which may continue after the completion of their doctorates whereby they work as postdoctoral researchers.\n\nAfter the completion of their training, many scientists pursue careers in a variety of work settings and conditions. In 2017, the British scientific journal \"Nature\" published the results of a large-scale survey of more than 5,700 doctoral students worldwide, asking them which sectors of the economy that would like to work in. A little over half of the respondents wanted to pursue a career in academia, with smaller proportions hoping to work in industry, government, and nonprofit environments.\n\nScientists are motivated to work in several ways. Many have a desire to understand why the world is as we see it and how it came to be. They exhibit a strong curiosity about reality. Other motivations are recognition by their peers and prestige. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, chemistry, and economics. \nSome scientists have a desire to apply scientific knowledge for the benefit of people's health, the nations, the world, nature, or industries (academic scientist and industrial scientist). Scientists tend to be less motivated by direct financial reward for their work than other careers. As a result, scientific researchers often accept lower average salaries when compared with many other professions which require a similar amount of training and qualification.\n\nScientists include experimentalists who mainly perform experiments to test hypotheses, and theoreticians who mainly develop models to explain existing data and predict new results. There is a continuum between two activities and the division between them is not clear-cut, with many scientists performing both tasks.\n\nThose considering science as a career often look to the frontiers. These include cosmology and biology, especially molecular biology and the human genome project. Other areas of active research include the exploration of matter at the scale of elementary particles as described by high-energy physics, and materials science, which seeks to discover and design new materials. Although there have been remarkable discoveries with regard to brain function and neurotransmitters, the nature of the mind and human thought still remains unknown.\n\n\nThe number of scientists is vastly different from country to country. For instance, there are only four full-time scientists per 10,000 workers in India while this number is 79 for the United Kingdom and the United States.\nAccording to the United States National Science Foundation 4.7 million people with science degrees worked in the United States in 2015, across all disciplines and employment sectors. The figure included twice as many men as women. Of that total, 17% worked in academia, that is, at universities and undergraduate institutions, and men held 53% of those positions. 5% of scientists worked for the federal government and about 3.5% were self-employed. Of the latter two groups, two-thirds were men. 59% of US scientists were employed in industry or business, and another 6% worked in non-profit positions.\n\nScientist and engineering statistics are usually intertwined, but they indicate that women enter the field far less than men, though this gap is narrowing. The number of science and engineering doctorates awarded to women rose from a mere 7 percent in 1970 to 34 percent in 1985 and in engineering alone the numbers of bachelor's degrees awarded to women rose from only 385 in 1975 to more than 11000 in 1985. \n\n\n\n\n\n"}
{"id": "3053980", "url": "https://en.wikipedia.org/wiki?curid=3053980", "title": "Slooh", "text": "Slooh\n\nSlooh is a robotic telescope service that can be viewed live through a web browser with Flash plug-in. It was not the first robotic telescope, but it was the first that offered \"live\" viewing through a telescope via the web. Other online telescopes traditionally email a picture to the recipient. The site has a patent on their live image processing method. Slooh is an online astronomy platform with live-views and telescope rental for a fee. Observations come from a global network of telescopes located in places including Spain and Chile.\n\nThe name Slooh comes from the word \"slew\" to indicate the movement of a telescope, modified with \"ooh\" to express pleasure and surprise.\n\nThe service was founded in 2002 by Michael Paolucci. As of October 2009, Paolucci remains as chairman. The service went online December 25, 2003 but was not available to the public until 2004.\n\nThe original astronomical observatory is located on the island Tenerife in the Canary Islands on the volcano called Teide. The site is at the elevation and situated away from city light pollution. This (Canary Islands) site includes 2 domes, each with 2 telescopes. Each dome has a high-magnification telescope and a wide-field telescope. One dome is optimized for planetary views (e.g., more magnification and a different CCD), and the other is optimized for deep sky objects (e.g., less magnification, more light sensitive CCD). Each dome offers 2 telescopic views: one high magnification (narrow field) view through a Celestron Schmidt-Cassegrain telescope; and a wide view through either a telephoto lens or an APO refractor. In 2012, the Slooh.com Canary Islands Observatory was assigned observatory code G40.\n\nOn February 14, 2009, Slooh launched a second observatory in the hills above La Dehesa, Chile. This site offers views from the Southern Hemisphere. In 2014, the Slooh.com Chile Observatory was assigned observatory code W88.\n\nUnlike Google Sky which features images from the Hubble Space Telescope, Slooh can take new images of the sky with its telescopes.\n\n\n"}
{"id": "36581725", "url": "https://en.wikipedia.org/wiki?curid=36581725", "title": "Stephen Hawking: Master of the Universe", "text": "Stephen Hawking: Master of the Universe\n\nStephen Hawking: Master of the Universe is a documentary television series produced by the television broadcaster Channel 5. The subject of the series is British theoretical physicist Stephen Hawking, known for his work on black hoes, who is also the presenter of the series. The series includes interviews with astrophysicist Kim Weaver, Bernard Carr, a student of Hawking's, and three theoretical physicists: Michio Kaku, Edward Witten, known for his work on superstring theory, and Lisa Randall. The first episode premiered in 2008, twenty years after the publication of Hawking's bestselling popular science book \"A Brief History of Time\". The title is derived from a \"Newsweek\" cover,\".\n\nThe series consisted of two episodes. The first describes Hawking's personal life, his challenges in overcoming his motor neurone disease, and his career in physics. It covers his childhood, education, marriage, family life, and his work on the Big Bang and black holes. The second episode discusses string theory and supersymmetry. Both episodes are 48 minutes long, and premiered as half hour-long programes. The series was released on CD by Channel 4 in the UK as a Region 2, one disc-DVD (B00140SGOM) on 18 March 2008.\n\nThe premiere of the first episode attracted 1.9 million viewers, and was considered a success. The second episode had 1.7 million viewers. James Walton of \"The Daily Telegraph\" wrote a positive review of the first episode, saying that it \"hadn’t done a bad job of trying to explain advanced physics to the science novice,\" even if it was \"extremely difficult stuff.\" Philip Wakefield, a television critic for \"stuff.co.nz\", listed the first episode in his \"Top TV picks\", calling it \"the neatest illustration of Einstein’s theory of relativity I’ve ever seen.\" Sam Wollastan of \"The Guardian\" was more critical of the series, but did praise it for showing \"the little glimpses of Prof Hawking's private life, like sharing a takeaway curry with a group of adoring young disciples.\"\n"}
{"id": "49577446", "url": "https://en.wikipedia.org/wiki?curid=49577446", "title": "Syed Iqbal Hasnain", "text": "Syed Iqbal Hasnain\n\nSyed Iqbal Hasnain is an Indian glaciologist, writer, educationist and the Chairman of the \"Glacier and Climate Change Commission\" of the Government of Sikkim. He is a former vice chancellor of the University of Calicut and a member of the \"United Nations Environment Program Committee on Global Assessment of Black Carbon and Troposphere Ozone\".\n\nHasnain has served the Jawaharlal Nehru University as a professor of glaciology and has been associated with The Energy and Resources Institute (TERI) and the Centre for Policy Research, a social science research institute affiliated to the Indian Council of Social Science Research (ICSSR). A distinguished Visiting Fellow of the Stimson Center, he has delivered several orations and has written articles and a book on glaciology. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2009, for his contributions to studies on environment.\n\n"}
{"id": "10117909", "url": "https://en.wikipedia.org/wiki?curid=10117909", "title": "The Pesticide Question", "text": "The Pesticide Question\n\nThe Pesticide Question: Environment, Economics and Ethics is a 1993 book edited by David Pimentel and Hugh Lehman. Use of pesticides has improved agricultural productivity, but there are also concerns about safety, health and the environment.\n\nThis book is the result of research by leading scientists and policy experts into the non-technical and social issues of pesticides. In examining the social policies related to pesticides use, they consider the costs as well as the benefits. The book says that Intensive farming cannot completely do without synthetic chemicals, but that it is technologically possible to reduce the amount of pesticides used in the United States by 35-50 per cent without reducing crop yields. The researchers show that to regain public trust, those who regulate and use pesticides must examine fair ethical questions and take appropriate action to protect public welfare, health, and the environment. Anyone concerned with reducing our reliance on chemical pesticides and how human activities can remain both productive and environmentally sound will find this volume a stimulating contribution to a troubling debate.\n\n\"The Pesticide Question\" builds on the 1962 best seller book \"Silent Spring\" by Rachel Carson. Carson did not reject the use of pesticides, but argued that their use was often indiscriminate and resulted in harm to people and the environment. She also highlighted the problem of pests becoming resistant to pesticides.\n\nCarson's work is referred to many times in \"The Pesticide Question\", which critically explores many non-technical issues associated with pesticide use, mainly in the United States. The book has 40 contributors, mainly academics from a wide range of disciplines. \"The Pesticide Question\" is divided into five main parts:\n\n\n\n"}
{"id": "40485649", "url": "https://en.wikipedia.org/wiki?curid=40485649", "title": "Thermodesulfobacterium commune", "text": "Thermodesulfobacterium commune\n\nThermodesulfobacterium commune is a species of Sulfate-reducing bacteria. It is small, Gram-negative, straight rod-shaped, obligately anaerobic and has an optimum growth temperature of . Its type strain is YSRA-1.\n\n\n"}
{"id": "59041691", "url": "https://en.wikipedia.org/wiki?curid=59041691", "title": "Thermogutta terrifontis", "text": "Thermogutta terrifontis\n\nThermogutta terrifontis is a thermophilic bacterium from the genus of \"Thermogutta\" which has been isolated from a hot spring from Kurils in Russia.\n"}
{"id": "11357006", "url": "https://en.wikipedia.org/wiki?curid=11357006", "title": "Woods–Saxon potential", "text": "Woods–Saxon potential\n\nThe Woods–Saxon potential is a mean field potential for the nucleons (protons and neutrons) inside the atomic nucleus, which is used to describe approximately the forces applied on each nucleon, in the nuclear shell model for the structure of the nucleus.\n\nThe form of the potential, in terms of the distance \"r\" from the center of nucleus, is:\n\nformula_1\n\nwhere \"V\" (having dimension of energy) represents the potential well depth,\n\"a\" is a length representing the \"surface thickness\" of the nucleus, and formula_2 is the nuclear radius where and \"A\" is the mass number.\n\nTypical values for the parameters are: , .\n\nFor large atomic number \"A\" this potential is similar to a potential well. It has the following desired properties\n\nWhen using the Schrödinger equation to find the energy levels of nucleons subjected to the Woods–Saxon potential, it cannot be solved analytically, and must be treated numerically.\n\n\n\n"}
