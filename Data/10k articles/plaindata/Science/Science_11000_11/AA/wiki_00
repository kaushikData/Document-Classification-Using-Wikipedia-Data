{"id": "21490880", "url": "https://en.wikipedia.org/wiki?curid=21490880", "title": "Academic specialization", "text": "Academic specialization\n\nIn academia, specialization (or specialisation) may be a course of study or major at an academic institution or may refer to the field that a specialist practices in. In the case of an educator, academic specialization pertains to the subject that he specializes in and teaches. It is considered a precondition of objective truth and works by restricting the mind's propensity for eclecticism through methodological rigor and studious effort. It is also employed as an information-management strategy, which operates by fragmenting an issue into different aspective fields or areas of expertise to obtain truth. \n\nAs the volume of knowledge accumulated by humanity became too great, increasing specialization in academia appeared in response. There are also cases when this concept emerged out of state policy-making to pursue goals such as national competitiveness. For instance, there is the case of Britain who began coordinating academic specialization - through the founding of the Imperial College - to catch up to the United States and Germany, particularly in the fields of scientific and technical education.\n"}
{"id": "4345305", "url": "https://en.wikipedia.org/wiki?curid=4345305", "title": "Academy of Social Sciences", "text": "Academy of Social Sciences\n\nThe Academy of Social Sciences is a representative body for social sciences in the UK. The Academy promotes social science through its sponsorship of the Campaign for Social Science, its links with Government on a variety of matters, and its own policy work in issuing public comment, responding to official consultations, and organising meetings and events about social science. It confers the title of Fellow upon nominated social scientists following a process of peer review. The Academy comprises over 1000 Fellows and 41 learned societies based in the UK and Europe.\n\nThe Academy’s origins lie in the formation of a representative body for the social science learned societies in 1982, the Association of Learned Societies in the Social Sciences (ALSISS). From 1999 to 2007 it was called the Academy of Learned Societies for the Social Sciences before changing to its current name. The Academy is run by a Council of 21 members, with Professor Roger Goodman FAcSS as its current Chair, and Professor Sir Ivor Crewe FAcSS, Master of University College, Oxford, as its current President. 7 Council members are elected by the Academy’s Fellows, 7 by its Learned Societies and 7 are appointed.\n\nThe Academy advocates social science by interacting with Government and other organisations, and co-ordinates the responses of social scientists to Government consultation documents. Past consultations include:\n\n\nThe Academy also puts forward suggestions to the Government about which social scientists should carry out its Foresight research projects, which look at important issues and how these might change over the next 20 to 80 years.\n\nA developing part of the Academy’s work is to bring researchers and organisations using research closer together to improve the evidence upon which public policy is based and to increase the impact of research.\n\nThe Academy has produced a series of ‘Making the Case for the Social Sciences’ booklets which give examples of important social science research which has made a difference to policy or practice. These are: Wellbeing; Ageing; Sustainability; the Environment and Climate Change; Crime; Sport and Leisure; Management; Scotland; Longitudinal Studies, Mental Wellbeing, Wales and Dementia. Further titles are in preparation. The Academy also publishes a cross-disciplinary peer-reviewed journal, \"Contemporary Social Science\". The Academy holds regular events, such as conferences on the ethics of social media research and the future of the Research Excellence Framework. It holds an Annual Lecture each summer, and its President's Lunch each winter. It also arranges (with the British Library) a public lecture series Enduring Ideas.\n\nPart of the Academy’s work is to recognise social scientists who are held in esteem by their peer group and whose life and work have had an impact in advancing social science. They are nominated and the nominations are then subject to peer review. Fellows are academics, policy-makers and practitioners, and are entitled to use the letters \"FAcSS\" after their name. In November 2014 there were 1000 Fellows, just over 1% of the 90,000 total membership of the 41 learned society members of the Academy.\n\nFellows were previously known as Academicians and used the post-nominal letter \"AcSS\". This was changed in July 2014 to bring the Academy in line with other British learned societies.\n\nThe Academy launched the Campaign for Social Science in January 2011 to advocate social science to Government and the general public. The Campaign is self-funded. It has campaigned for the restoration of the post of Government Chief Social Scientific Adviser, promotes social science in the media and on the web, and organises roadshows around the country to emphasise the value and importance of social science.\n\n\n\n"}
{"id": "37708117", "url": "https://en.wikipedia.org/wiki?curid=37708117", "title": "Addgene", "text": "Addgene\n\nAddgene is a non-profit plasmid repository. Addgene facilitates the exchange of genetic material between laboratories by offering plasmids and their associated cloning data to not-for-profit laboratories around the world. Addgene provides a free online database of plasmid cloning information and references, including lists of commonly used vector backbones, popular lentiviral plasmids and molecular cloning protocols.\n\nAddgene was founded in 2004, by Melina Fan, Kenneth Fan and Benjie Chen.\n\nAddgene's headquarters are located in Watertown, Massachusetts.\n\nAddgene accepts plasmids from researchers for distribution and archival. Addgene obtains revenues and licensing fees using the free depository for commercial sale, and is selective at will in selling acquired depository plasmids to 501 (c) 3 nonprofit organizations at the industrial price for profit gains for its putative non-profit operation. \n\nThe organization covers the operating costs of maintaining and improving the collection by charging a nominal fee to scientists requesting plasmids.\n\nAs of 2014 Addgene's repository comprised 30,000 plasmids, deposited by 1,700 labs. Its plasmid collection contains plasmids used for functions such as genome engineering (including CRISPRS), gene expression, shRNA knockdown, viral-mediated gene delivery, detection of miRNA and promoter activity. The plasmid collection includes:\n\n\nMolecular biology tools\n\nVector Database—A curated list of over 4,000 vector backbones, including relevant cloning information and bacterial growth conditions.\n\nSequence Analyzer—An Addgene software tool for creating plasmid maps from sequences with annotated features and restriction sites.\n\nMolecular Biology Reference—A collection of references for molecular biology reagents, such as primers, restriction enzymes and antibiotic concentrations.\n\nPlasmid Cloning Guides\n\nMolecular Cloning Guides—References to help scientists design plasmid cloning experiments, including tutorials on restriction enzyme digestion and PCR-based cloning.\n\nMolecular Cloning Protocols—Specific protocols for a variety of plasmid cloning techniques, such as isolation of bacterial colonies, DNA purification by gel electrophoresis and bacterial transformation.\n\nAddgene collaborates with institutes and consortia to curate plasmid collections for specific purposes. Examples of these collaborations include special collections from the Structural Genomics Consortium, Zinc Finger Consortium, the Cell Migration Consortium, the KLF collection and The Michael J. Fox Foundation. The plasmids are available to both academic and industry labs.\n\nNoteworthy depositors include:\n\nAddgene requires Material Transfer Agreements (MTAs) for all materials transferred through Addgene to protect the intellectual property of plasmid depositors. Addgene developed one of the first electronic systems for handling MTAs. By using the standard Universal Biological Material Transfer Agreement (UBMTA) and implementing electronic signatures, Addgene’s electronic MTA (eMTA) system expedites the approval process for plasmid orders.\n\nAddgene won awards for innovation and research including Mass Nonprofit Network Award for excellence in Innovations, Cambridge award program 2014 Award for Research & Development Laboratories, Mass Technology Leadership Award Finalist 2012.\n\n"}
{"id": "2889", "url": "https://en.wikipedia.org/wiki?curid=2889", "title": "Amorphous solid", "text": "Amorphous solid\n\nIn condensed matter physics and materials science, an amorphous (from the Greek \"a\", without, \"morphé\", shape, form) or non-crystalline solid is a solid that lacks the long-range order that is characteristic of a crystal. In some older books, the term has been used synonymously with glass. Nowadays, \"glassy solid\" or \"amorphous solid\" is considered to be the overarching concept, and glass the more special case: Glass is an amorphous solid that exhibits a glass transition. Polymers are often amorphous. Other types of amorphous solids include gels, thin films, and nanostructured materials such as glass.\n\nAmorphous materials have an internal structure made of interconnected structural blocks. These blocks can be similar to the basic structural units found in the corresponding crystalline phase of the same compound. Whether a material is liquid or solid depends primarily on the connectivity between its elementary building blocks so that solids are characterized by a high degree of connectivity whereas structural blocks in fluids have lower connectivity.\n\nIn pharmaceutical industry, the amorphous drugs were shown to have higher bioavailability than their crystalline counterparts due to the high solubility of amorphous phase. Moreover, certain compounds can undergo precipitation in their amorphous form \"in vivo\", and they can decrease each other's bioavailability if administered together.\nEven amorphous materials have some shortrange order at the atomic length scale due to the nature of chemical bonding (see structure of liquids and glasses for more information on non-crystalline material structure). Furthermore, in very small crystals a large fraction of the atoms are the crystal; relaxation of the surface and interfacial effects distort the atomic positions, decreasing the structural order. Even the most advanced structural characterization techniques, such as x-ray diffraction and transmission electron microscopy, have difficulty in distinguishing between amorphous and crystalline structures on these length scales.\n\nAmorphous phases are important constituents of thin films, which are solid layers of a few nanometres to some tens of micrometres thickness deposited upon a substrate. So-called structure zone models were developed to describe the micro structure and ceramics of thin films as a function of the homologous temperature \"T\" that is the ratio of deposition temperature over melting temperature. According to these models, a necessary (but not sufficient) condition for the occurrence of amorphous phases is that \"T\" has to be smaller than 0.3, that is the deposition temperature must be below 30% of the melting temperature. For higher values, the surface diffusion of deposited atomic species would allow for the formation of crystallites with long range atomic order.\n\nRegarding their applications, amorphous metallic layers played an important role in the discussion of a suspected superconductivity in amorphous metals. Today, optical coatings made from TiO, SiO, TaO etc. and combinations of them in most cases consist of amorphous phases of these compounds. Much research is carried out into thin amorphous films as a gas separating membrane layer. The technologically most important thin amorphous film is probably represented by few nm thin SiO layers serving as isolator above the conducting channel of a metal-oxide semiconductor field-effect transistor (MOSFET). Also, hydrogenated amorphous silicon, a-Si:H in short, is of technical significance for thin-film solar cells. In case of a-Si:H the missing long-range order between silicon atoms is partly induced by the presence by hydrogen in the percent range.\n\nThe occurrence of amorphous phases turned out as a phenomenon of particular interest for studying thin-film growth. Remarkably, the growth of polycrystalline films is often used and preceded by an initial amorphous layer, the thickness of which may amount to only a few nm. The most investigated example is represented by thin multicrystalline silicon films, where such as the unoriented molecule. An initial amorphous layer was observed in many studies. Wedge-shaped polycrystals were identified by transmission electron microscopy to grow out of the amorphous phase only after the latter has exceeded a certain thickness, the precise value of which depends on deposition temperature, background pressure and various other process parameters. The phenomenon has been interpreted in the framework of Ostwald's rule of stages that predicts the formation of phases to proceed with increasing condensation time towards increasing stability. Experimental studies of the phenomenon require a clearly defined state of the substrate surface and its contaminant density etc., upon which the thin film is deposited.\n\n\n"}
{"id": "801754", "url": "https://en.wikipedia.org/wiki?curid=801754", "title": "Andrey Arkhangelsky", "text": "Andrey Arkhangelsky\n\nAndrey Dmitriyevich Arkhangelsky () (December 8, 1879 – June 16, 1940) was a Russian geologist. He was a professor at Moscow State University.\n\nHe won the Lenin Prize in 1928.\n\nA crater on Mars and Arkhangel'skiy Nunataks in Antarctica are named after him.\n\n"}
{"id": "42076934", "url": "https://en.wikipedia.org/wiki?curid=42076934", "title": "Antique Telescope Society", "text": "Antique Telescope Society\n\nThe Antique Telescope Society (ATS) is a society for people interested in antique telescopes, binoculars, instruments, books, atlases, etc.\n\nThe society has an annual meeting. It also publishes the \"Journal of the Antique Telescope Society\" and has an active email list. The American astronomer Michael D. Reynolds has been President of the Antique Telescope Society.\n\n"}
{"id": "43439296", "url": "https://en.wikipedia.org/wiki?curid=43439296", "title": "Austin Glen Member", "text": "Austin Glen Member\n\nThe Austin Glen Member of the Normanskill Formation is an upper Middle Ordovician unit of interbedded greywackes and shales that outcrops in eastern New York State. It was deposited in a deep marine setting in a foreland basin during the Taconic orogeny. Its sediment source was mainly the erosion of preexisting sedimentary rocks. Graptolite fossils place it in the stratigraphic zones of \"Nematograptus gracilis\" and \"Climacograptus bicornis\", but its age could be Llandeilo or Trentonian (earliest to latest Darriwilian, ).\n"}
{"id": "19880944", "url": "https://en.wikipedia.org/wiki?curid=19880944", "title": "Banxing", "text": "Banxing\n\nBanxing or BX-1 (), is a small Chinese technology development satellite which was deployed from the Shenzhou 7 spacecraft at 11:27 GMT on 27 September 2008. Prior to deployment, the satellite was mounted on top of the Shenzhou 7 orbital module.\n\nBanxing was used to relay images of the Shenzhou 7 spacecraft. Weighing some 40 kilograms, and containing two cameras and communication equipment, it was maneuvered using an ammonia gas-based propulsion system. Following the re-entry of Shenzhou 7, Banxing remained in orbit as part of a formation-flying experiment with the discarded Shenzhou orbital module.\n\nA few hours after Banxing was launched it and the Shenzhou 7 orbital module passed unusually close to the International Space Station. This provoked some speculation that the experiment was intended to test military anti-satellite interception technology.\n\n"}
{"id": "4116", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": "Big Bang\n\nThe Big Bang theory is the prevailing cosmological model for the observable universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from a very high-density and high-temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background (CMB), large scale structure and Hubble's law (the farther away galaxies are, the faster they are moving away from Earth). If the observed conditions are extrapolated backwards in time using the known laws of physics, the prediction is that just before a period of very high density there was a singularity which is typically associated with the Big Bang. Physicists are undecided whether this means the universe began from a singularity, or that current knowledge is insufficient to describe the universe at that time. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billion years ago, which is thus considered the age of the universe. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements (mostly hydrogen, with some helium and lithium) later coalesced through gravity, eventually forming early stars and galaxies, the descendants of which are visible today. Astronomers also observe the gravitational effects of dark matter surrounding galaxies. Though most of the mass in the universe seems to be in the form of dark matter, Big Bang theory and various observations seem to indicate that it is not made out of conventional baryonic matter (protons, neutrons, and electrons) but it is unclear exactly what it \"is\" made out of.\n\nSince Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. The scientific community was once divided between supporters of two different theories, the Big Bang and the Steady State theory, but a wide range of empirical evidence has strongly favored the Big Bang which is now universally accepted. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1964, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.\n\nThe Belgian astronomer and Catholic priest Georges Lemaître proposed on theoretical grounds that the universe is expanding, which was observationally confirmed soon afterwards by Edwin Hubble. In 1927 in the \"Annales de la Société Scientifique de Bruxelles\" (\"Annals of the Scientific Society of Brussels\") under the title \"Un Univers homogène de masse constante et de rayon croissant rendant compte de la vitesse radiale des nébuleuses extragalactiques\" (\"A homogeneous Universe of constant mass and growing radius accounting for the radial velocity of extragalactic nebulae\"), he presented his new idea that the universe is expanding and provided the first observational estimation of what is known as the Hubble constant. What later will be known as the \"Big Bang theory\" of the origin of the universe, he called his \"hypothesis of the primeval atom\" or the \"Cosmic Egg\".\n\nAmerican astronomer Edwin Hubble observed that the distances to faraway galaxies were strongly correlated with their redshifts. This was interpreted to mean that all distant galaxies and clusters are receding away from our vantage point with an apparent velocity proportional to their distance: that is, the farther they are, the faster they move away from us, regardless of direction. Assuming the Copernican principle (that the Earth is not the center of the universe), the only remaining interpretation is that all observable regions of the universe are receding from all others. Since we know that the distance between galaxies increases today, it must mean that in the past galaxies were closer together. The continuous expansion of the universe implies that the universe was denser and hotter in the past.\n\nLarge particle accelerators can replicate the conditions that prevailed after the early moments of the universe, resulting in confirmation and refinement of the details of the Big Bang model. However, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is still poorly understood and an area of open investigation and speculation.\n\nThe first subatomic particles to be formed included protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.\n\nThe Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the CMB, large scale structure, and Hubble's Law. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology. The Lambda-CDM model is the current \"standard model\" of Big Bang cosmology, consensus is that it is the simplest model that can account for the various measurements and observations relevant to cosmology.\n\nExtrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity indicates that general relativity is not an adequate description of the laws of physics in this regime. Models based on general relativity alone can not extrapolate toward the singularity beyond the end of the Planck epoch.\n\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the standard model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event — otherwise known as the \"age of the universe\" — is 13.799 ± 0.021 billion years. The agreement of independent measurements of this age supports the ΛCDM model that describes in detail the characteristics of the universe.\n\nDespite being extremely dense at this time—far denser than is usually required to form a black hole—the universe did not re-collapse into a black hole. This may be explained by considering that commonly-used calculations and limits for gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not apply to rapidly expanding space such as the Big Bang.\n\nThe earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially during which time density fluctuations that occurred because of the uncertainty principle were amplified into the seeds that would later form the large-scale structure of the universe. After inflation stopped, reheating occurred until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.\n\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 10 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\n\nA few minutes into the expansion, when the temperature was about a billion (one thousand million) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei.\n\nAs the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years, the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old.\n\nOver a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an \"extended model\" which includes hot dark matter in the form of neutrinos, then if the \"physical baryon density\" formula_1 is estimated at about 0.023 (this is different from the 'baryon density' formula_2 expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density formula_3 is about 0.11, the corresponding neutrino density formula_4 is estimated to be less than 0.0062.\n\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate.\n\nDark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theoretically.\n\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. There is no well-supported model describing the action prior to 10 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.\n\nThe Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.\n\nThese ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\n\nIf the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.\n\nGeneral relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, are themselves specified using a coordinate chart or \"grid\" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). \nThis metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their \"coordinate\" distance (comoving distance) remains constant, the \"physical\" distance between two such co-moving points expands proportionally with the scale factor of the universe.\n\nThe Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. In other words, the Big Bang is not an explosion \"in space\", but rather an expansion \"of space\". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.\n\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe.\n\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\n\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a 1949 BBC radio broadcast, saying: \"These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.\"\n\nIt is popularly reported that Hoyle, who favored an alternative \"steady state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.\n\nThe Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist, proposed that the inferred recession of the nebulae was due to the expansion of the universe.\n\nIn 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\n\nStarting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the cosmological principle.\n\nIn the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, \"viz\"., that matter is eternal. A beginning in time was \"repugnant\" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.\n\nDuring the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.\n\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.\n\nIn 1968 and 1970 Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of general relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\n\nIn the mid-1990s, observations of certain globular clusters appeared to indicate that they were about 15 billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.\n\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\n\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the \"four pillars\" of the Big Bang theory.\n\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.\n\nObservations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nformula_5\nwhere\n\nHubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.\n\nThe theory requires the relation formula_9 to hold at all times, where formula_7 is the comoving distance, \"v\" is the recessional velocity, and formula_6, formula_12, and formula_7 vary as the universe expands (hence we write formula_8 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity formula_6. However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.\n\nThat space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.\n\nMeasurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the CMB over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.\n\nIn 1964 Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s the radiation was found to be approximately consistent with a black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.\n\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\nIn 1989, NASA launched the Cosmic Background Explorer satellite (COBE), which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 10, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 10. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\n\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\n\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.\n\nUsing the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for <chem>^4He/H</chem>, about 10 for <chem>^2H/H</chem>, about 10 for <chem>^3He/H</chem> and about 10 for <chem>^7Li/H</chem>.\n\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for <chem>^4He</chem>, and off by a factor of two for <chem>^7Li</chem>; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than <chem>^3He</chem>, and in constant ratios, too.\n\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\n\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently, appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\n\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis.\n\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.\n\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\n\nFuture gravitational waves observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\n\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.\n\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\n\nMeasurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\n\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.\n\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\n\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\n\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\n\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\n\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\n\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\n\nA resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\n\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.\n\nIf inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.\n\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\n\nThe magnetic monopole objection was raised in the late 1970s. Grand unified theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\n\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric (FLRW). The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be \"flat\".\n\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 10 of its critical value, or it would not exist as it does today.\n\nPhysics may conclude that time did not exist before 'Big Bang', but 'started' with the Big Bang and hence there might be no 'beginning', 'before' or potentially 'cause' and instead always existed. Quantum fluctuations, or other laws of physics that may have existed at the start of the Big Bang could then create the conditions for matter to occur.\n\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch.\n\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\n\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\n\nThe following is a partial list of misconceptions about the Big Bang model:\n\n\"The Big Bang as the origin of the universe:\" One of the common misconceptions about the Big Bang model is the belief that it was the origin of the universe. However, the Big Bang model does not comment about how the universe came into being. Current conception of the Big Bang model assumes the existence of energy, time, and space, and does not comment about their origin or the cause of the dense and high temperature initial state of the universe.\n\n\"The Big Bang was \"small\"\": It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\n\n\"Hubble's law violates the special theory of relativity\": Hubble's law predicts that galaxies that are beyond Hubble Distance recede faster than the speed of light. However, special relativity does not apply beyond motion through space. Hubble's law describes velocity that results from expansion \"of\" space, rather than \"through\" space.\n\n\"Doppler redshift vs cosmological red-shift\": Astronomers often refer to the cosmological red-shift as a normal Doppler shift, which is a misconception. Although similar, the cosmological red-shift is not identical to the Doppler redshift. The Doppler redshift is based on special relativity, which does not consider the expansion of space. On the contrary, the cosmological red-shift is based on general relativity, in which the expansion of space is considered. Although they may appear identical for nearby galaxies, it may cause confusion if the behavior of distant galaxies is understood through the Doppler redshift.\n\nWhile the Big Bang model is well established in cosmology, it is likely to be refined. The Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time; this infinite energy density is regarded as impossible in physics. Still, it is known that the equations are not applicable before the time when the universe cooled down to the Planck temperature, and this conclusion depends on various assumptions, of which some could never be experimentally verified. \"(Also see Planck epoch.)\"\n\nOne proposed refinement to avoid this would-be singularity is to develop a correct treatment of quantum gravity.\n\nIt is not known what could have preceded the hot dense state of the early universe or how and why it originated, though speculation abounds in the field of cosmogony.\n\nSome proposals, each of which entails untested hypotheses, are:\n\nProposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.\n\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, and some see its mention in their holy books, while others argue that Big Bang cosmology makes the notion of a creator superfluous.\n\n\n\n\n"}
{"id": "588214", "url": "https://en.wikipedia.org/wiki?curid=588214", "title": "Carl Jakob Sundevall", "text": "Carl Jakob Sundevall\n\nCarl Jakob Sundevall (22 October 1801, Högestad – 2 February 1875) was a Swedish zoologist.\nSundevall studied at Lund University, where he became a Ph.D. in 1823. After traveling to East Asia, he studied medicine, graduating as Doctor of Medicine in 1830.\n\nHe was employed at the Swedish Museum of Natural History, Stockholm from 1833, and was professor and keeper of the vertebrate section from 1839 to 1871. He wrote \"Svenska Foglarna\" (1856–87) which described 238 species of birds observed in Sweden. He classified a number of birds collected in southern Africa by Johan August Wahlberg. In 1835, he developed a phylogeny for the birds based on the muscles of the hip and leg that contributed to later work by Thomas Huxley. He then went on to examine the arrangement of the deep plantar tendons in the bird's foot. This latter information is still used by avian taxonomists. Sundevall was also an entomologist and arachnologist and in 1862 wrote a monograph proposing a universal phonetic alphabet, \"Om phonetiska bokstäver\".\n\nSundevall is commemorated in the scientific names of four species of reptiles: \"Elapsoidea sundevalli, Leptotyphlops sundewalli, Mochlus sundevalli\", and \"Prosymna sundevalli\".\n\n\n"}
{"id": "38067966", "url": "https://en.wikipedia.org/wiki?curid=38067966", "title": "Charity Engine", "text": "Charity Engine\n\nCharity Engine is a free PC app based on Berkeley University's BOINC software, ran by The Worldwide Computer Company Limited. The project works by selling spare home computing power to universities and corporations, then sharing the profits between eight partner charities and periodic cash prize draws for the users; those running the Charity Engine BOINC software on their home computers. When there are no corporations purchasing the computing power, Charity Engine donates it to existing volunteer computing projects such as Rosetta@home, Einstein@Home and Malaria Control and prize draws are funded by donations.\n\nIn August 2014 the Rosetta@home project reported Charity Engine had contributed over 125,000 new PCs to its grid.\n"}
{"id": "50414502", "url": "https://en.wikipedia.org/wiki?curid=50414502", "title": "Charles Skinner (geologist)", "text": "Charles Skinner (geologist)\n\nCharles Skinner is a geologist who is Head of Group Exploration at diamond miners De Beers.\n"}
{"id": "15184877", "url": "https://en.wikipedia.org/wiki?curid=15184877", "title": "Chemical stability", "text": "Chemical stability\n\nChemical stability when used in the technical sense in chemistry, means thermodynamic stability of a chemical system.\n\nThermodynamic stability occurs when a system is in its lowest energy state, or chemical equilibrium with its environment. This may be a dynamic equilibrium, where individual atoms or molecules change form, but their overall number in a particular form is conserved. This type of chemical thermodynamic equilibrium will persist indefinitely unless the system is changed. Chemical systems might include changes in the phase of matter or a set of chemical reactions.\n\nState A is said to be more thermodynamically stable than state B if the Gibbs energy of the change from A to B is positive.\n\nThermodynamic stability applies to a particular system. The reactivity of a chemical substance is a description of how it might react across a variety of potential chemical systems and, for a given system, how fast such a reaction could proceed.\n\nChemical substances or states can persist indefinitely even though they are not in their lowest energy state if they experience metastability - a state which is stable only if not disturbed too much. A substance (or state) might also be termed \"kinetically persistent\" if it is changing relatively slowly (and thus is not at thermodynamic equilibrium, but is observed anyway). Metastable and kinetically persistent species or systems are not considered truly stable in chemistry. Therefore, the term \"chemically stable\" should not be used by chemists as a synonym of \"unreactive\" because it confuses thermodynamic and kinetic concepts. On the other hand, highly chemically unstable species tend to undergo exothermic unimolar decompositions at high rates. Thus, high chemical instability may sometimes parallel unimolar decompositions at high rates.\n\nIn everyday language, and often in materials science, a chemical substance is said to be \"stable\" if it is not particularly reactive in the environment or during normal use, and retains its useful properties on the timescale of its expected usefulness. In particular, the usefulness is retained in the presence of air, moisture or heat, and under the expected conditions of application. In this meaning, the material is said to be unstable if it can corrode, decompose, polymerize, burn or explode under the conditions of anticipated use or normal environmental conditions.\n\n"}
{"id": "36425928", "url": "https://en.wikipedia.org/wiki?curid=36425928", "title": "Content sniffing", "text": "Content sniffing\n\nContent sniffing, also known as media type sniffing or MIME sniffing, is the practice of inspecting the content of a byte stream to attempt to deduce the file format of the data within it. Content sniffing is generally used to compensate for a lack of accurate metadata that would otherwise be required to enable the file to be interpreted correctly. Content sniffing techniques tend to use a mixture of techniques that rely on the redundancy found in most file formats: looking for file signatures and magic numbers, and heuristics including searching for well-known representative substrings, the use of byte frequency and \"n\"-gram tables, and Bayesian inference.\n\nMIME sniffing was, and still is, used by some web browsers, including notably Microsoft's Internet Explorer, in an attempt to help web sites which do not correctly signal the MIME type of web content display correctly. However, doing this opens up a serious security vulnerability, in which, by confusing the MIME sniffing algorithm, the browser can be manipulated into interpreting data in a way that allows an attacker to carry out operations that are not expected by either the site operator or user, such as cross-site scripting. Moreover, by making sites which do not correctly assign MIME types to content appear to work correctly in those browsers, it fails to encourage the correct labeling of material, which in turn makes content sniffing necessary for these sites to work, creating a vicious circle of incompatibility with web standards and security best practices.\n\nA specification exists for media type sniffing in HTML5, which attempts to balance the requirements of security with the need for reverse compatibility with web content with missing or incorrect MIME-type data. It attempts to provide a precise specification that can be used across implementations to implement a single well-defined and deterministic set of behaviors.\n\nThe UNIX file command can be viewed as a content sniffing application.\n\nNumerous web browsers use a more limited form of content sniffing to attempt to determine the character encoding of text files for which the MIME type is already known. This technique is known as charset sniffing or codepage sniffing and, for certain encodings, may be used to bypass security restrictions too. For instance, Internet Explorer 7 may be tricked to run JScript in circumvention of its policy by allowing the browser to guess that an HTML-file was encoded in UTF-7.\nThis bug is worsened by the feature of the UTF-7 encoding which permits multiple encodings of the same text and, specifically, alternative representations of ASCII characters.\n\nMost encodings do not allow evasive presentations of ASCII characters, so charset sniffing is less dangerous in general because, due to the historical accident of the ASCII-centric nature of scripting and markup languages, characters outside the ASCII repertoire are more difficult to use to circumvent security boundaries, and mis-interpretations of character sets tend to produce results no worse than the display of mojibake.\n\n\n"}
{"id": "2679315", "url": "https://en.wikipedia.org/wiki?curid=2679315", "title": "David J. C. MacKay", "text": "David J. C. MacKay\n\nSir David John Cameron MacKay (22 April 1967 – 14 April 2016) was a British physicist, mathematician, and academic. He was the Regius Professor of Engineering in the Department of Engineering at the University of Cambridge and from 2009 to 2014 was Chief Scientific Adviser to the UK Department of Energy and Climate Change (DECC). MacKay was the author of the book \"Sustainable Energy – Without the Hot Air\".\n\nMacKay was educated at Newcastle High School and represented Britain in the International Physics Olympiad in Yugoslavia in 1985, receiving the first prize for experimental work. He continued his education at Trinity College, Cambridge, and received a Bachelor of Arts degree in Natural Sciences (Experimental and theoretical physics) in 1988. He went to the California Institute of Technology (Caltech) as a Fulbright Scholar, where his supervisor was John Hopfield. He was awarded a PhD in 1992.\n\nIn January 1992 MacKay was appointed the Royal Society Smithson Research Fellow at Darwin College, Cambridge, continuing his cross-disciplinary research in the Cavendish Laboratory, the Department of Physics of the University of Cambridge. In 1995 he was made a University Lecturer in the Cavendish Laboratory. He was promoted in 1999 to a Readership, in 2003 to a Professorship in Natural Philosophy and in 2013 to the post of Regius Professorship of Engineering.\n\nMacKay's contributions in machine learning and information theory include the development of Bayesian methods for neural networks, the rediscovery (with Radford M. Neal) of low-density parity-check codes, and the invention of \"Dasher\", a software application for communication especially popular with those who cannot use a traditional keyboard. He cofounded the knowledge management company Transversal. In 2003, his book \"Information Theory, Inference, and Learning Algorithms\" was published.\n\nHis interests beyond research included the development of effective teaching methods and African development; he taught regularly at the African Institute for Mathematical Sciences in Cape Town from its foundation in 2003 to 2006. In 2008 he completed a book on energy consumption and energy production without fossil fuels called \"Sustainable Energy – Without the Hot Air\". MacKay used £10,000 of his own money to publish the book, and the initial print run of 5,000 sold within days. The book received praise from \"The Economist\", \"The Guardian\", and Bill Gates, who called it \"one of the best books on energy that has been written.\" Like his textbook on Information theory, MacKay made the book available for free online. In March 2012 he gave a TED talk on renewable energy.\n\nMacKay was appointed to be Chief Scientific Advisor of the Department of Energy and Climate Change, United Kingdom, in September 2009. In October 2014, at the end of his five-year term, he was succeeded by John Loughhead.\n\nMacKay was elected a Fellow of the Royal Society (FRS) in 2009. His certificate of election reads:\n\nIn the 2016 New Year Honours, MacKay was appointed a Knight Bachelor \"for services to Scientific Advice in Government and Science Outreach\", and therefore granted the title \"sir\".\n\nDavid MacKay was born the fifth child of Donald MacCrimmon MacKay and Valerie MacKay. His elder brother Robert S. MacKay FRS (born in 1956) is Professor of Mathematics at the University of Warwick. David was a vegetarian.\n\nHe married Ramesh Ghiassi in 2011. They had a son and a daughter.\n\nMacKay was diagnosed with inoperable stomach cancer (malignant adenocarcinoma) in July 2015, for which he underwent palliative chemotherapy, a process he documented in detail on his public personal blog. He died in the afternoon of 14 April 2016. He is survived by his wife and two children.\n"}
{"id": "14569165", "url": "https://en.wikipedia.org/wiki?curid=14569165", "title": "Demonic Males", "text": "Demonic Males\n\nDemonic Males: Apes and the Origins of Human Violence is a 1997 book by Richard Wrangham and Dale Peterson examining the evolutionary factors leading to human male violence.\n\n\"Demonic Males\" begins by explaining that humans, chimpanzees, bonobos, gorillas and orangutans are a group of genetically related Great Apes, that humans are genetically closer to chimps than chimps are to gorillas, and that chimps and bonobos are most closely genetically related. After speculating about what enabled humans' ancestors to leave the rainforest (the use of roots as sources of water and food), \"Demonic Males\" next provides a catalog of the types of violence practiced by male chimpanzees (intra-group hierarchical violence, violence against females and extra-group murdering raids). The high incidence of rape by non-alpha male orangutans and infanticide by male gorillas are also cited as examples of our mutual genetic heritage.\n\nThe authors present chimp society as extremely patriarchal, in that no adult male chimpanzee is subordinate to any female of any rank. They present evidence that most dominant human civilizations have always been likewise behaviorally patriarchal, and that male humans share male chimpanzees' innate propensity for dominance, gratuitous violence, war, rape and murder. They claim that the brain's prefrontal cortex is also a factor, as humans have been shown experimentally to make decisions based both on logic and prefrontal cortex-mediated emotion.\n\nIn the chapter \"The Peaceful Ape,\" the authors contrast chimpanzee behaviors with those of the bonobo, presenting logical biological reasons for the more pacific (although also aggressive and antagonistic) behaviors of the latter. Reasons include a bonobo female social organization that doesn't tolerate male aggression, the invisibility of bonobo ovulation (in chimps, ovulation has both olfactory and genital swelling manifestations, leading to ferocious male competition for mating), and overall social organization, whereby male bonobos don't form alliances as male chimps do. (Contested: Heisterman et al (1996) show that female bonobos display sexual swellings which they considered to be a reliable indicator of probability of ovulation).\n\nThe authors consider male violence to be evolutionarily undesirable and morally reprehensible (explicitly detailing the Hutu-Tutsi cross-genocides in Africa's Great Ape habitats, and citing Charlotte Perkins Gilman's female utopian novel \"Herland\" [1915]), and argue that the advent of modern weapons such as nerve gas and atomic bombs threaten our collective future. Like Steven Pinker's \"The Better Angels of Our Nature: Why Violence Has Declined\" (2012), which makes the case that violence has been decreasing in human society over time, \"Demonic Males\" makes the case that human males are genetically predisposed to violence, but that our species also has the intellectual capacity to override this flaw if we recognize that it is in our survival interest to do so.\n\nIn a political interpretation of \"Demonic Males,\" biologist Philip Regal says that the book is partly an attack on the deconstructivist feminist theory that male violence is a purely social construct. Regal also considers the book to be \"a broadside against the old utopian dreams of Atlantis, Eden, Elysium, a Golden Age, Romantic paintings, and the late Margaret Mead\" which imagined human beings as naturally peaceful.\n\n\"The New York Times\" called it \"enjoyable and easy to read\" and said it \"belongs to the emerging genre of serious scientific books that have something to say about questions of interest to many people, not just to specialists\".\n\nHeistermann, M., Mohle, U., Vervaecke, H., van Elsacker, L., Hodges, J.K. (1996) ‘Application of Urinary and Fecal Steroid Measurements for Monitoring Ovarian Function and Pregnancy in the Bonobo (Pan paniscus) and Evaluation of Perineal Swelling Patterns in Relation to Endocrine Events’. In Biology of Reproduction, 55, 844-853\n"}
{"id": "29647899", "url": "https://en.wikipedia.org/wiki?curid=29647899", "title": "Dibner Institute for the History of Science and Technology", "text": "Dibner Institute for the History of Science and Technology\n\nThe Dibner Institute for the History of Science and Technology (1992–2006) was a research institute established at MIT, and housed in a renovated building (E56) on campus at 38 Memorial Drive, overlooking the Charles River.\n\nAt the heart of the Institute was the Burndy Library on the ground floor, initially containing 37,000 volumes on the history of science and technology collected by the Dibner Fund. The Library also possessed a large collection of antique scientific instruments, such as astrolabes, telescopes, microscopes, early spectrometers, and a Wimshurst machine, which were on public display in a dedicated gallery outside the library. Also on display was a large collection of antique incandescent light bulbs, gas discharge tubes, electronic vacuum tubes, and other early examples of electrical and electronic technology. The Library would mount occasional special exhibits, such as \"The Afterlife of Immortality: Obelisks Outside Egypt\". \n\nAbove the Library and display space, on the second and third floor were offices and lecture and seminar rooms. The Institute held regular lectures, seminars, study programs, and an annual symposium in the history of science and technology. Over the period of its existence, the Institute supported over 340 short- and longer-term fellowships.\n\nThe Institute was named in honor of Bern Dibner (1897–1988), who had conceived of it before his death. The Institute was developed and supported by the Dibner Fund he had established in 1957, directed by his son David Dibner.The institute, from its inception was run by executive director, Evelyn Simha. On the academic side, the Institute was supported by a consortium of MIT, Boston University, Brandeis University and Harvard University.\n\nIn 1995, the 600-volume Babson Collection of historical material related to Isaac Newton was placed on permanent deposit with the Burndy Library. The collection had been assembled by Roger Babson, founder of Babson College in Wellesley, Massachusetts, and was previously housed at the College. In 1999, the addition of the 7,000-volume Volterra Collection from Italy increased the Burndy Library collection by more than a third. \n\nIn 2004 MIT decided not to renew its affiliation, and the Dibner family began looking for a new location to house the collection. David Dibner died unexpectedly in 2005. The Dibner Institute closed in 2006, and the Burndy Library and associated collections were transferred to The Huntington Library in San Marino, California, which now offers a Dibner History of Science Program to fund fellowships, a lecture series and annual conference. The acquisition of the Burndy Library (by then numbering 67,000 volumes) transformed the Huntington Library's collections in the history of science and technology into one of the world's largest in that field. \n\nThe Huntington houses a permanent exhibition, \"Beautiful Science: Ideas that Changed the World\", in the Dibner Hall of the History of Science that displays approximately 150 books, manuscripts, photographs and objects from both the Burndy Library and the Huntington's non-Burndy holdings in the history of science and medicine. Approximately 200 antique light bulbs from the Burndy Collection are on display in the \"Beautiful Science\" exhibition. The light bulbs are not available for reference or research use, except by special arrangement. The status and accessibility of the Burndy collection of gas tubes, vacuum tubes, and electronic artifacts is not clear from the Huntington website.\n\nThe Dibner Institute's former building was demolished in early 2007 to make way for new buildings for the MIT Sloan School of Management. The Dibner name remains at MIT, in the endowed Frances and David Dibner Professorship of the History of Engineering and Manufacturing.\n\n"}
{"id": "900498", "url": "https://en.wikipedia.org/wiki?curid=900498", "title": "Early adopter", "text": "Early adopter\n\nAn early adopter (sometimes misspelled as \"early adapter\" or \"early adaptor\") or lighthouse customer is an early customer of a given company, product, or technology. The term originates from Everett M. Rogers' \"Diffusion of Innovations\" (1962).\n\nTypically this will be a customer who, in addition to using the vendor's product or technology, will also provide considerable and candid feedback to help the vendor refine its future product releases, as well as the associated means of distribution, service, and support. \n\nThe relationship is synergistic, with the customer having early (and sometimes unique, or at least uniquely early) access to an advantageous new product or technology, but he or she also serves as a kind of guinea pig.\n\nIn exchange for being an early adopter, and thus being exposed to the problems, risks, and annoyances common to early-stage product testing and deployment, the lighthouse customer is sometimes given especially attentive vendor assistance and support, even to the point of having personnel at the customer's work site to assist with implementation.\nThe customer is sometimes given preferential pricing, terms, and conditions, although new technology is often very expensive, so the early adopter still often pays quite a lot.\n\nThe vendor, on the other hand, benefits from receiving early\nrevenues, and also from a lighthouse customer's endorsement and assistance in further developing the product and its go-to-market mechanisms. Acquiring lighthouse customers is a common step in new product development and implementation. The real-world focus that this type of relationship can bring to a vendor can be extremely valuable.\n\nEarly adoption does come with pitfalls: early versions of products may be buggy and/or prone to malfunction. Furthermore, more efficient, and sometimes less expensive, versions of the product usually appear a few months after the initial release (Apple iPhone). The trend of new technology costing more at release is often referred to as the \"early adopter tax\".\n\n"}
{"id": "24365881", "url": "https://en.wikipedia.org/wiki?curid=24365881", "title": "Energo-Chromo-Kinese", "text": "Energo-Chromo-Kinese\n\nEnergo-Chromo-Kinese, also named ECK, is a pseudo-scientific and esoteric-oriented new religious movement founded in October 1987 in Villefranche-sur-Mer by Patrick Véret, a former acupuncturist and homeopath, and his wife Danielle Drouant, now Danielle Didier.\n\nECK uses many associations and societies (including Connaissance ontologique universelle et recherche biologique énergétique (COURBE), Energo 8 international, Energo conseils, Jéricho 3000, Association pour la recherche en médecine énergétique and SOS Spasmophilie) and is particularly active in the therapeutic and medical fields. Centers and schools were subsequently established to teach the beliefs of the group, which won over business executives and major corporations, but especially doctors, dentists and kinesiotherapists. Customers who attend these four-degree courses become \"kinergists\". The doctrine is a \"gnostic pantheism\" and is explained in two books written by Patrick Véret: \"La Médecine énergétique\" and \"La spasmophilie enfin vaincue\", respectively written in 1981 and 1985. According to French cult consultant Jean-Marie Abgrall, ECK doctrines \"represent an amalgam (or a synthesis, according to its proponents) of various theories referring to human \"energy\" — mainly Chinese medicine and Vedic medicine. According to ECK, the human body has seven energy centers that vibrate on the same frequencies as certain colors or certain sounds.\"\n\nOn 22 February 1993, the French branch of the organization was dissolved by a court decision of the Tribunal de commerce of Paris and the founders split off. Véret founded the Nutrition énergétique des organes et des méridiens (NEOM), and his former wife Danielle Drouant led the Ordre du temple de la Jérusalem céleste (OTJC).\n\nThe movement is currently active in France, Switzerland, Spain, Italy, Canada, the United States, Portugal, Australia... In France, the group seems to be in the decline.\n\nECK was listed as a cult in the 1999 parliamentary report established by the Parliamentary Commission on Cults in France and also appeared in the 1997 Belgian parliamentary report. It was considered as dangerous because of its pseudo-medical speech which could be harmful to members' health, exaggerated requests for money, and indoctrination of the members who became dependent to the group. It was also criticized by anti-cult groups and former members.\n\nIn 2007, an academic thesis about the dangers of cults for health contained a large analysis of ECK beliefs and practices.\n"}
{"id": "152626", "url": "https://en.wikipedia.org/wiki?curid=152626", "title": "Ethnography", "text": "Ethnography\n\nEthnography (from Greek \"ethnos\" \"folk, people, nation\" and \"grapho\" \"I write\") is the systematic study of people and cultures. It is designed to explore cultural phenomena where the researcher observes society from the point of view of the subject of the study. An ethnography is a means to represent graphically and in writing the culture of a group. The word can thus be said to have a double meaning, which partly depends on whether it is used as a count noun or uncountable. The resulting field study or a case report reflects the knowledge and the system of meanings in the lives of a cultural group.\n\nAs a method of data collection ethnography entails examining the behaviour of the participants in a certain specific social situation and also understanding their interpretation of such behaviour. Dewan (2018) further elaborates that this behaviour may be shaped by the constraints the participants feel because of the situations they are in or by the society in which they belong. Ethnography, as the presentation of empirical data on human societies and cultures, was pioneered in the biological, social, and cultural branches of anthropology, but it has also become popular in the social sciences in general—sociology, communication studies, history—wherever people study ethnic groups, formations, compositions, resettlements, social welfare characteristics, materiality, spirituality, and a people's ethnogenesis. The typical ethnography is a holistic study and so includes a brief history, and an analysis of the terrain, the climate, and the habitat. In all cases, it should be reflexive, make a substantial contribution toward the understanding of the social life of humans, have an aesthetic impact on the reader, and express a credible reality. An ethnography records all observed behavior and describes all symbol-meaning relations, using concepts that avoid causal explanations. Traditionally, ethnography was focussed on the western gaze towards the far 'exotic' east, but now researchers are undertaking ethnography in their own social environment. According to Dewan (2018), even if we are the other, the ‘another’ or the ‘native’, we are still ‘another’ because there are many facades of ourselves that connect us to people and other facades that highlight our differences. \n\nThe word 'ethnography' is derived from the Greek ἔθνος (\"ethnos\"), meaning \"a company, later a people, nation\" and -graphy, meaning \"writing\". Ethnographic studies focus on large cultural groups of people who interact over time. Ethnography is a set of qualitative methods that are used in social sciences that focus on the observation of social practices and interactions. Its aim is to observe a situation without imposing any deductive structure or framework upon it and to view everything as strange or unique.\n\nThe field of anthropology originated from Europe and England designed in late 19th century. It spread its roots to the United States at the beginning of the 20th century. Some of the main contributors like E. B. Tylor (1832–1917) from Britain and Lewis H. Morgan (1818–1881), an American scientist were considered as founders of cultural and social dimensions. Franz Boas (1858–1942), Bronislaw Malinowski (1884—1942), Ruth Benedict (1887–1948), and Margaret Mead (1901–1978), were a group of researchers from the United States who contributed the idea of cultural relativism to the literature. Boas's approach focused on the use of documents and informants, whereas Malinowski stated that a researcher should be engrossed with the work for long periods in the field and do a participant observation by living with the informant and experiencing their way of life. He gives the viewpoint of the native and this became the origin of field work and field methods.\n\nSince Malinowski was very firm with his approach he applied it practically and traveled to Trobriand Islands which are located off the eastern coast of New Guinea. He was interested in learning the language of the islanders and stayed there for a long time doing his field work. The field of ethnography became very popular in the late 19th century, as many social scientists gained an interest in studying modern society. Again, in the latter part of the 19th century, the field of anthropology became a good support for scientific formation. Though the field was flourishing, it had a lot of threats to encounter. Postcolonialism, the research climate shifted towards post-modernism and feminism. Therefore, the field of anthropology moved into a discipline of social science.\n\nGerhard Friedrich Müller developed the concept of ethnography as a separate discipline whilst participating in the Second Kamchatka Expedition (1733–43) as a professor of history and geography. Whilst involved in the expedition, he differentiated \"Völker-Beschreibung\" as a distinct area of study. This became known as \"ethnography,\" following the introduction of the Greek neologism \"ethnographia\" by Johann Friedrich Schöpperlin and the German variant by A. F. Thilo in 1767. August Ludwig von Schlözer and Christoph Wilhelm Jacob Gatterer of the University of Göttingen introduced the term into the academic discourse in an attempt to reform the contemporary understanding of world history.\n\nHerodotus, known as the Father of History, had significant works on the cultures of various peoples beyond the Hellenic realm such as the Scythians, which earned him the title \"philobarbarian\", and may be said to have produced the first works of ethnography.\n\nThere are different forms of ethnography: confessional ethnography; life history; feminist ethnography etc. Two popular forms of ethnography are realist ethnography and critical ethnography. (Qualitative Inquiry and Research Design, 93)\n\nRealist ethnography is a traditional approach used by cultural anthropologists. Characterized by Van Maanen (1988), it reflects a particular instance taken by the researcher toward the individual being studied. It's an objective study of the situation. It's composed from a third person's perspective by getting the data from the members on the site. The ethnographer stays as omniscient correspondent of actualities out of sight. The realist reports information in a measured style ostensibly uncontaminated by individual predisposition, political objectives, and judgment. The analyst will give a detailed report of the everyday life of the individuals under study. The ethnographer also uses standard categories for cultural description (e.g., family life, communication network). The ethnographer produces the participant's views through closely edited quotations and has the final work on how the culture is to be interpreted and presented. (Qualitative Inquiry and Research Design, 93)\n\nCritical ethnography is a kind of ethnographic research in which the creators advocate for the liberation of groups which are marginalized in society. Critical researchers typically are politically minded people who look to take a stand of opposition to inequality and domination. For example, a critical ethnographer might study schools that provide privileges to certain types of students, or counseling practices that serve to overlook the needs of underrepresented groups. (Qualitative Inquiry and Research Design, 94). The important components of a critical ethnographer are to incorporate a value- laden introduction, empower people by giving them more authority, challenging the status quo, and addressing concerns about power and control. A critical ethnographer will study issues of power, empowerment, inequality inequity, dominance, repression, hegemony, and victimization. (Qualitative Inquiry and Research Design, 94)\n\nAccording to Dewan (2018) the researcher is not looking for generalizing the findings; rather, they are considering it in reference to the context of the situation. In this regard, the best way to integrate ethnography in a quantitative research would be to use it to discover and uncover relationships and then use the resultant data to test and explain the empirical assumptions \n\n\n\nThe ethnographic method is different from other ways of conducting social science approach due to the following reasons:\n\n\nAccording to the leading social scientist, John Brewer, data collection methods are meant to capture the \"social meanings and ordinary activities\" of people (informants) in \"naturally occurring settings\" that are commonly referred to as \"the field.\" The goal is to collect data in such a way that the researcher imposes a minimal amount of personal bias in the data. Multiple methods of data collection may be employed to facilitate a relationship that allows for a more personal and in-depth portrait of the informants and their community. These can include participant observation, field notes, interviews, and surveys.\n\nInterviews are often taped and later transcribed, allowing the interview to proceed unimpaired of note-taking, but with all information available later for full analysis. Secondary research and document analysis are also used to provide insight into the research topic. In the past, kinship charts were commonly used to \"discover logical patterns and social structure in non-Western societies\". In the 21st century, anthropology focuses more on the study of people in urban settings and the use of kinship charts is seldom employed.\n\nIn order to make the data collection and interpretation transparent, researchers creating ethnographies often attempt to be \"reflexive\". Reflexivity refers to the researcher's aim \"to explore the ways in which [the] researcher's involvement with a particular study influences, acts upon and informs such research\". Despite these attempts of reflexivity, no researcher can be totally unbiased. This factor has provided a basis to criticize ethnography.\n\nTraditionally, the ethnographer focuses attention on a community, selecting knowledgeable informants who know the activities of the community well. These informants are typically asked to identify other informants who represent the community, often using snowball or chain sampling. This process is often effective in revealing common cultural denominators connected to the topic being studied. Ethnography relies greatly on up-close, personal experience. Participation, rather than just observation, is one of the keys to this process. Ethnography is very useful in social research.\n\nYbema et al. (2010) examine the ontological and epistemological presuppositions underlying ethnography. Ethnographic research can range from a realist perspective, in which behavior is observed, to a constructivist perspective where understanding is socially constructed by the researcher and subjects. Research can range from an objectivist account of fixed, observable behaviors to an interpretive narrative describing \"the interplay of individual agency and social structure.\" Critical theory researchers address \"issues of power within the researcher-researched relationships and the links between knowledge and power.\"\n\nAnother form of data collection is that of the \"image.\" The image is the projection that an individual puts on an object or abstract idea. An image can be contained within the physical world through a particular individual's perspective, primarily based on that individual’s past experiences. One example of an image is how an individual views a novel after completing it. The physical entity that is the novel contains a specific image in the perspective of the interpreting individual and can only be expressed by the individual in the terms of \"I can tell you what an image is by telling you what it feels like.\" The idea of an image relies on the imagination and has been seen to be utilized by children in a very spontaneous and natural manner. Effectively, the idea of the image is a primary tool for ethnographers to collect data. The image presents the perspective, experiences, and influences of an individual as a single entity and in consequence, the individual will always contain this image in the group under study.\n\nThe ethnographic method is used across a range of different disciplines, primarily by anthropologists but also occasionally by sociologists. Cultural studies, (European) ethnology, sociology, economics, social work, education, design, psychology, computer science, human factors and ergonomics, ethnomusicology, folkloristics, religious studies, geography, history, linguistics, communication studies, performance studies, advertising, nursing, urban planning, usability, political science, social movement, and criminology are other fields which have made use of ethnography.\n\nCultural anthropology and social anthropology were developed around ethnographic research and their canonical texts, which are mostly ethnographies: e.g. \"Argonauts of the Western Pacific\" (1922) by Bronisław Malinowski, \"Ethnologische Excursion in Johore\" (1875) by Nicholas Miklouho-Maclay, \"Coming of Age in Samoa\" (1928) by Margaret Mead, \"The Nuer\" (1940) by E. E. Evans-Pritchard, \"Naven\" (1936, 1958) by Gregory Bateson, or \"The Lele of the Kasai\" (1963) by Mary Douglas. Cultural and social anthropologists today place a high value on doing ethnographic research. The typical ethnography is a document written about a particular people, almost always based at least in part on emic views of where the culture begins and ends. Using language or community boundaries to bound the ethnography is common. Ethnographies are also sometimes called \"case studies.\" Ethnographers study and interpret culture, its universalities, and its variations through the ethnographic study based on fieldwork. An ethnography is a specific kind of written observational science which provides an account of a particular culture, society, or community. The fieldwork usually involves spending a year or more in another society, living with the local people and learning about their ways of life. Neophyte Ethnographers are strongly encouraged to develop extensive familiarity with their subject prior to entering the field; otherwise, they may find themselves in difficult situations.\n\nEthnographers are participant observers. They take part in events they study because it helps with understanding local behavior and thought. Classic examples are Carol B. Stack's \"All Our Kin\", Jean Briggs' \"Never in Anger\", Richard Lee's \"Kalahari Hunter-Gatherers\", Victor Turner's \"Forest of Symbols\", David Maybry-Lewis' \"Akew-Shavante Society\", E.E. Evans-Pritchard's \"The Nuer,\" and Claude Lévi-Strauss' \"Tristes Tropiques\". Iterations of ethnographic representations in the classic, modernist camp include Joseph W. Bastien's \"Drum and Stethoscope\" (1992), Bartholomew Dean's recent (2009) contribution, \"Urarina Society, Cosmology, and History in Peruvian Amazonia\".\n\nA typical ethnography attempts to be holistic and typically follows an outline to include a brief history of the culture in question, an analysis of the physical geography or terrain inhabited by the people under study, including climate, and often including what biological anthropologists call habitat. Folk notions of botany and zoology are presented as ethnobotany and ethnozoology alongside references from the formal sciences. Material culture, technology, and means of subsistence are usually treated next, as they are typically bound up in physical geography and include descriptions of infrastructure. Kinship and social structure (including age grading, peer groups, gender, voluntary associations, clans, moieties, and so forth, if they exist) are typically included. Languages spoken, dialects, and the history of language change are another group of standard topics. Practices of childrearing, acculturation, and emic views on personality and values usually follow after sections on social structure. Rites, rituals, and other evidence of religion have long been an interest and are sometimes central to ethnographies, especially when conducted in public where visiting anthropologists can see them.\n\nAs ethnography developed, anthropologists grew more interested in less tangible aspects of culture, such as values, worldview and what Clifford Geertz termed the \"ethos\" of the culture. In his fieldwork, Geertz used elements of a phenomenological approach, tracing not just the doings of people, but the cultural elements themselves. For example, if within a group of people, winking was a communicative gesture, he sought to first determine what kinds of things a wink might mean (it might mean several things). Then, he sought to determine in what contexts winks were used, and whether, as one moved about a region, winks remained meaningful in the same way. In this way, cultural boundaries of communication could be explored, as opposed to using linguistic boundaries or notions about the residence. Geertz, while still following something of a traditional ethnographic outline, moved outside that outline to talk about \"webs\" instead of \"outlines\" of culture.\n\nWithin cultural anthropology, there are several subgenres of ethnography. Beginning in the 1950s and early 1960s, anthropologists began writing \"bio-confessional\" ethnographies that intentionally exposed the nature of ethnographic research. Famous examples include \"Tristes Tropiques\" (1955) by Lévi-Strauss, \"The High Valley\" by Kenneth Read, and \"The Savage and the Innocent\" by David Maybury-Lewis, as well as the mildly fictionalized \"Return to Laughter\" by Elenore Smith Bowen (Laura Bohannan).\n\nLater \"reflexive\" ethnographies refined the technique to translate cultural differences by representing their effects on the ethnographer. Famous examples include \"Deep Play: Notes on a Balinese Cockfight\" by Clifford Geertz, \"Reflections on Fieldwork in Morocco\" by Paul Rabinow, \"The Headman and I\" by Jean-Paul Dumont, and \"Tuhami\" by Vincent Crapanzano. In the 1980s, the rhetoric of ethnography was subjected to intense scrutiny within the discipline, under the general influence of literary theory and post-colonial/post-structuralist thought. \"Experimental\" ethnographies that reveal the ferment of the discipline include \"Shamanism, Colonialism, and the Wild Man\" by Michael Taussig, \"Debating Muslims\" by Michael F. J. Fischer and Mehdi Abedi, \"A Space on the Side of the Road\" by Kathleen Stewart, and \"Advocacy after Bhopal\" by Kim Fortun.\n\nThis critical turn in sociocultural anthropology during the mid-1980s can be traced to the influence of the now classic (and often contested) text, \"Writing Culture: The Poetics and Politics of Ethnography\", (1986) edited by James Clifford and George Marcus. \"Writing Culture\" helped bring changes to both anthropology and ethnography often described in terms of being 'postmodern,' 'reflexive,' 'literary,' 'deconstructive,' or 'poststructural' in nature, in that the text helped to highlight the various epistemic and political predicaments that many practitioners saw as plaguing ethnographic representations and practices.\n\nWhere Geertz's and Turner's interpretive anthropology recognized subjects as creative actors who constructed their sociocultural worlds out of symbols, postmodernists attempted to draw attention to the privileged status of the ethnographers themselves. That is, the ethnographer cannot escape the personal viewpoint in creating an ethnographic account, thus making any claims of objective neutrality highly problematic, if not altogether impossible. In regards to this last point, \"Writing Culture\" became a focal point for looking at how ethnographers could describe different cultures and societies without denying the subjectivity of those individuals and groups being studied while simultaneously doing so without laying claim to absolute knowledge and objective authority. Along with the development of experimental forms such as 'dialogic anthropology,' 'narrative ethnography,' and 'literary ethnography', \"Writing Culture\" helped to encourage the development of 'collaborative ethnography.' This exploration of the relationship between writer, audience, and subject has become a central tenet of contemporary anthropological and ethnographic practice. In certain instances, active collaboration between the researcher(s) and subject(s) has helped blend the practice of collaboration in ethnographic fieldwork with the process of creating the ethnographic product resulting from the research.\n\nSociology is another field which prominently features ethnographies. Urban sociology, Atlanta University (now Clark-Atlanta University), and the Chicago School, in particular, are associated with ethnographic research, with some well-known early examples being \"The Philadelphia Negro\" (1899) by W. E. B. Du Bois, \"Street Corner Society\" by William Foote Whyte and \"Black Metropolis\" by St. Clair Drake and Horace R. Cayton, Jr.. Major influences on this development were anthropologist Lloyd Warner, on the Chicago sociology faculty, and to Robert Park's experience as a journalist. Symbolic interactionism developed from the same tradition and yielded such sociological ethnographies as \"Shared Fantasy\" by Gary Alan Fine, which documents the early history of fantasy role-playing games. Other important ethnographies in sociology include Pierre Bourdieu's work on Algeria and France.\n\nJaber F. Gubrium's series of organizational ethnographies focused on the everyday practices of illness, care, and recovery are notable. They include \"Living and Dying at Murray Manor,\" which describes the social worlds of a nursing home; \"Describing Care: Image and Practice in Rehabilitation,\" which documents the social organization of patient subjectivity in a physical rehabilitation hospital; \"Caretakers: Treating Emotionally Disturbed Children,\" which features the social construction of behavioral disorders in children; and \"Oldtimers and Alzheimer's: The Descriptive Organization of Senility,\" which describes how the Alzheimer's disease movement constructed a new subjectivity of senile dementia and how that is organized in a geriatric hospital. Another approach to ethnography in sociology comes in the form of institutional ethnography, developed by Dorothy E. Smith for studying the social relations which structure people's everyday lives.\n\nOther notable ethnographies include Paul Willis's \"Learning to Labour,\" on working class youth; the work of Elijah Anderson, Mitchell Duneier, and Loïc Wacquant on black America, and Lai Olurode's \"Glimpses of Madrasa From Africa\". But even though many sub-fields and theoretical perspectives within sociology use ethnographic methods, ethnography is not the \"sine qua non\" of the discipline, as it is in cultural anthropology.\n\nBeginning in the 1960s and 1970s, ethnographic research methods began to be widely used by communication scholars. As the purpose of ethnography is to describe and interpret the shared and learned patterns of values, behaviors, beliefs, and language of a culture-sharing group, Harris, (1968), also Agar (1980) note that ethnography is both a process and an outcome of the research. Studies such as Gerry Philipsen's analysis of cultural communication strategies in a blue-collar, working-class neighborhood on the south side of Chicago, \"Speaking 'Like a Man' in Teamsterville\", paved the way for the expansion of ethnographic research in the study of communication.\n\nScholars of communication studies use ethnographic research methods to analyze communicative behaviors and phenomena. This is often characterized in the writing as attempts to understand taken-for-granted routines by which working definitions are socially produced. Ethnography as a method is a storied, careful, and systematic examination of the reality-generating mechanisms of everyday life (Coulon, 1995). Ethnographic work in communication studies seeks to explain \"how\" ordinary methods/practices/performances construct the ordinary actions used by ordinary people in the accomplishments of their identities. This often gives the perception of trying to answer the \"why\" and \"how come\" questions of human communication. Often this type of research results in a case study or field study such as an analysis of speech patterns at a protest rally, or the way firemen communicate during \"down time\" at a fire station. Like anthropology scholars, communication scholars often immerse themselves, and participate in and/or directly observe the particular social group being studied.\n\nThe American anthropologist George Spindler was a pioneer in applying the ethnographic methodology to the classroom.\n\nAnthropologists such as Daniel Miller and Mary Douglas have used ethnographic data to answer academic questions about consumers and consumption. In this sense, Tony Salvador, Genevieve Bell, and Ken Anderson describe design ethnography as being \"a way of understanding the particulars of daily life in such a way as to increase the success probability of a new product or service or, more appropriately, to reduce the probability of failure specifically due to a lack of understanding of the basic behaviors and frameworks of consumers.\" Sociologist Sam Ladner argues in her book, that understanding consumers and their desires requires a shift in \"standpoint,\" one that only ethnography provides. The results are products and services that respond to consumers' unmet needs.\n\nBusinesses, too, have found ethnographers helpful for understanding how people use products and services. Companies make increasing use of ethnographic methods to understand consumers and consumption, or for new product development (such as video ethnography). The \"Ethnographic Praxis in Industry\" (EPIC) conference is evidence of this. Ethnographers' systematic and holistic approach to real-life experience is valued by product developers, who use the method to understand unstated desires or cultural practices that surround products. Where focus groups fail to inform marketers about what people really do, ethnography links what people say to what they do—avoiding the pitfalls that come from relying only on self-reported, focus-group data. Modern developments in computing power and AI have enabled higher efficiencies in ethnographic data collection via multimedia and computational analysis using machine learning.\n\nThe Ethnographic methodology is not usually evaluated in terms of philosophical standpoint (such as positivism and emotionalism). Ethnographic studies need to be evaluated in some manner. No consensus has been developed on evaluation standards, but Richardson (2000, p. 254) provides five criteria that ethnographers might find helpful. Jaber F. Gubrium and James A. Holstein's (1997) monograph, \"The New Language of Qualitative Method,\" discusses forms of ethnography in terms of their \"methods talk.\"\n\n\nEthnography, which is a method dedicated entirely to field work, is aimed at gaining a deeper insight of a certain people's knowledge and social culture.\n\nEthnography's advantages are:\n\nHowever, there are certain challenges or limitations for the ethnographic method: \n\nAmong the dangers of ethnography are that it can become indistinguishable from a kind of embedded journalism or blog with academic jargon giving it the veneer of academic legitimacy but without actually meeting the classic requirements for ethnography. A series of tests can be applied to determine whether work is actually ethnography or academic journalism depending on the focus of the study (an \"ethnos\"), the scientific hypotheses and questions, whether there is model building, whether there are cross-cultural comparisons, and the purpose, uses and forms of the work.\n\nGary Alan Fine argues that the nature of ethnographic inquiry demands that researchers deviate from formal and idealistic rules or ethics that have come to be widely accepted in qualitative and quantitative approaches in research. Many of these ethical assumptions are rooted in positivist and post-positivist epistemologies that have adapted over time but are apparent and must be accounted for in all research paradigms. These ethical dilemmas are evident throughout the entire process of conducting ethnographies, including the design, implementation, and reporting of an ethnographic study. Essentially, Fine maintains that researchers are typically not as ethical as they claim or assume to be — and that \"each job includes ways of doing things that would be inappropriate for others to know\".\n\nFine is not necessarily casting blame at ethnographic researchers but tries to show that researchers often make idealized ethical claims and standards which in are inherently based on partial truths and self-deceptions. Fine also acknowledges that many of these partial truths and self-deceptions are unavoidable. He maintains that \"illusions\" are essential to maintain an occupational reputation and avoid potentially more caustic consequences. He claims, \"Ethnographers cannot help but lie, but in lying, we reveal truths that escape those who are not so bold\". Based on these assertions, Fine establishes three conceptual clusters in which ethnographic ethical dilemmas can be situated: \"Classic Virtues\", \"Technical Skills\", and \"Ethnographic Self\".\n\nMuch debate surrounding the issue of ethics arose following revelations about how the ethnographer Napoleon Chagnon conducted his ethnographic fieldwork with the Yanomani people of South America.\n\nWhile there is no international standard on Ethnographic Ethics, many western anthropologists look to the American Anthropological Association for guidance when conducting ethnographic work. In 2009 the Association adopted a code of ethics, stating: Anthropologists have \"moral obligations as members of other groups, such as the family, religion, and community, as well as the profession\". The code of ethics notes that anthropologists are part of a wider scholarly and political network, as well as human and natural environment, which needs to be reported on respectfully. The code of ethics recognizes that sometimes very close and personal relationship can sometimes develop from doing ethnographic work. The Association acknowledges that the code is limited in scope; ethnographic work can sometimes be multidisciplinary, and anthropologists need to be familiar with ethics and perspectives of other disciplines as well. The eight-page code of ethics outlines ethical considerations for those conducting Research, Teaching, Application and Dissemination of Results, which are briefly outlined below.\n\n\n\n\nThe following are commonly misconceived conceptions of ethnographers:\n\n\nAccording to Norman K. Denzin, ethnographers should consider the following eight principles when observing, recording, and sampling data:\n\n\n\n\n"}
{"id": "7667356", "url": "https://en.wikipedia.org/wiki?curid=7667356", "title": "Finding Darwin's God", "text": "Finding Darwin's God\n\nFinding Darwin's God: A Scientist's Search for Common Ground Between God and Evolution is a 2000 book by the American cell biologist and Roman Catholic Kenneth R. Miller wherein he argues that evolution does not contradict religious faith. Miller argues that evolution occurred, that the earth is not young, that science must work based on methodological naturalism, and that evolution cannot be construed as an effective argument for atheism.\n\n\n"}
{"id": "43399972", "url": "https://en.wikipedia.org/wiki?curid=43399972", "title": "Human Biology (textbook)", "text": "Human Biology (textbook)\n\nHuman Biology is a basic biology textbook published in 1993 by Jones & Bartlett Learning. It has been recognized as a \"good introductory text\" for students without a strong scientific background.\n\n"}
{"id": "1103247", "url": "https://en.wikipedia.org/wiki?curid=1103247", "title": "Imaging science", "text": "Imaging science\n\nImaging science is a multidisciplinary field concerned with the generation, collection, duplication, analysis, modification, and visualization of images, including imaging things that the human eye cannot detect. As an evolving field it includes research and researchers from physics, mathematics, electrical engineering, computer vision, computer science, and perceptual psychology. Currently, the only institution that has imaging science in their program is the Rochester Institute of Technology at the Carlson Center for Imaging science.\n\nThe foundation of imaging science as a discipline is the \"imaging chain\" – a conceptual model describing all of the factors which must be considered when developing a system for creating visual renderings (images). In general, the links of the imaging chain include:\n\n1. The human visual system. Designers must also consider the psychophysical processes which take place in human beings as they make sense of information received through the visual system.\n\n2. The subject of the image. When developing an imaging system, designers must consider the observables associated with the subjects which will be imaged. These observables generally take the form of emitted or reflected energy, such as electromagnetic energy or mechanical energy.\n\n3. The capture device. Once the observables associated with the subject are characterized, designers can then identify and integrate the technologies needed to capture those observables. For example, in the case of consumer digital cameras, those technologies include optics for collecting energy in the visible portion of the electromagnetic spectrum, and electronic detectors for converting the electromagnetic energy into an electronic signal.\n\n4. The processor. For all digital imaging systems, the electronic signals produced by the capture device must be manipulated by an algorithm which formats the signals so they can be displayed as an image. In practice, there are often multiple processors involved in the creation of a digital image.\n\n5. The display. The display takes the electronic signals which have been manipulated by the processor and renders them on some visual medium. Examples include paper (for printed, or \"hard copy\" images), television, computer monitor, or projector.\n\nNote that some imaging scientists will include additional \"links\" in their description of the imaging chain. For example, some will include the \"source\" of the energy which \"illuminates\" or interacts with the subject of the image. Others will include storage and/or transmission systems.\n\nSubfields within imaging science include: image processing, computer vision, 3D computer graphics, animations, atmospheric optics, astronomical imaging, digital image restoration, digital imaging, color science, digital photography, holography, magnetic resonance imaging, medical imaging, microdensitometry, optics, photography, remote sensing, radar imaging, radiometry, silver halide, ultrasound imaging, photoacoustic imaging, thermal imaging, visual perception, and various printing technologies.\n\n\n\n"}
{"id": "5694915", "url": "https://en.wikipedia.org/wiki?curid=5694915", "title": "Institute For Figuring", "text": "Institute For Figuring\n\nThe Institute For Figuring (IFF) is an organization based in Los Angeles, California that promotes the public understanding of the poetic and aesthetic dimensions of science, mathematics and the technical arts. Founded by Margaret Wertheim and Christine Wertheim, the Institute hosts public lectures and exhibitions, publishes books and maintains a website.\n\n\n"}
{"id": "179211", "url": "https://en.wikipedia.org/wiki?curid=179211", "title": "Lignotuber", "text": "Lignotuber\n\nA lignotuber is a woody swelling of the root crown possessed by some plants as a protection against destruction of the plant stem, such as by fire. The crown contains buds from which new stems may sprout, as well as stores of starch that can support a period of growth in the absence of photosynthesis. The term \"lignotuber\" was coined in 1924 by Australian botanist Leslie R. Kerr.\n\nPlants possessing lignotubers include \"Eucalyptus marginata\" (Jarrah), \"Eucalyptus brevifolia\" (Snappy Gum) and \"Eucalyptus ficifolia\" (Scarlet Gum) all of which can have lignotubers ten feet (3 meters) wide and three feet (one meter) deep as well as most mallees, and many \"Banksia\" species. Lignotubers develop from the cotyledonary bud in seedlings of several oak species including cork oak \"Quercus suber\", but do not develop in several other oak species, and are not apparent in mature cork oak trees.\n\nThe largest known lignotubers (also called \"root collar burls\") are those of the Coast Redwood \"(Sequoia sempervirens)\" of central and northern California and extreme southwestern Oregon. A lignotuber washed into Big Lagoon (30 miles (48 km) north of Eureka, California) by the full gale storm of 1977 was 41 feet (12.5 meters) in diameter and about half as tall and estimated to weigh 525 short tons (476.3  metric tonnes). The largest dicot lignotubers are those of the Chinese Camphor Tree, or Kusu \"(Cinnamomum camphora)\" of Japan, China and the Koreas. Ones at the Vergelegan Estate in Cape Town, South Africa which were planted in the late 1600s have muffin-shaped lignotubers up to six feet (2 meters) high and about thirty feet (9 meters) in diameter. Perhaps the largest lignotuber in Australia would be that of \"Old Bottle Butt\", a Red Bloodwood Tree (Corymbia gummifera) near Wauchope, New South Wales which has a lignotuber about eight feet (2.5 meters) in height, and seventeen feet thick (16.3 meters circumference) at breast height. \n\nMany plants with lignotubers grow in a shrubby habit, but with multiple stems arising from the lignotuber. The term lignotuberous shrub is used to describe this habit.\n\n"}
{"id": "10118279", "url": "https://en.wikipedia.org/wiki?curid=10118279", "title": "List of 3D graphics libraries", "text": "List of 3D graphics libraries\n\n3D graphics have become so popular, particularly in video games, that specialized APIs (application programming interfaces) have been created to ease the processes in all stages of computer graphics generation. These APIs have also proved vital to computer graphics hardware manufacturers, as they provide a way for programmers to access the hardware in an abstract way, while still taking advantage of the special hardware of any specific graphics card.\n\nThe first 3D graphics framework was probably Core, published by the ACM in 1977.\n\nThese APIs for 3D computer graphics are particularly popular:\n\n\nThere are also higher-level 3D scene-graph APIs which provide additional functionality on top of the lower-level rendering API. Such libraries under active development include:\n\nThere is more interest in web browser based high-level API for 3D graphics engines. Some are:\n\n\n"}
{"id": "14485286", "url": "https://en.wikipedia.org/wiki?curid=14485286", "title": "List of members of the National Academy of Sciences (Applied mathematical sciences)", "text": "List of members of the National Academy of Sciences (Applied mathematical sciences)\n"}
{"id": "44805034", "url": "https://en.wikipedia.org/wiki?curid=44805034", "title": "List of spiral DRAGNs", "text": "List of spiral DRAGNs\n\nSpiral DRAGNs are a type of galaxy; spiral galaxies which contain DRAGNs (Double Radio-source Associated with Galactic Nucleus), and are therefore also radio galaxies.\n\nMost DRAGNs are associated with elliptical galaxies, as are most double-lobed radio-galaxies.<ref name=\"VLA/14A-406\"> </ref> Spiral DRAGNs are inconsistent with currently known galaxy formation processes. As of 2015, there are 4 known spiral DRAGNs.\n"}
{"id": "48846633", "url": "https://en.wikipedia.org/wiki?curid=48846633", "title": "List of unsolved problems in medicine", "text": "List of unsolved problems in medicine\n\nThis article lists currently unsolved problems in medicine.\n\n\n(Note: diseases for which the etiology is fully or partially understood, but for which effective treatments are not yet available, are not included on the list. Some cancers would fall into this category.) \n"}
{"id": "1965902", "url": "https://en.wikipedia.org/wiki?curid=1965902", "title": "Long-range locator", "text": "Long-range locator\n\nA long-range locator is a class of devices purported to be a type of metal detector, supposedly able to detect a variety of substances, including gold, drugs and explosives; most are said to operate on a principle of resonance with the material being detected.\n\nSkeptics have examined the internals of many such devices and found those that have been examined to be incapable of operating as advertised, and have dismissed them as overpriced dowsing rods or similarly useless devices. Virtually all such devices claim to operate on a resonant frequency principle where the device is said to emit an electromagnetic signal, either through an antenna or a probe, that will respond to a specific substance such as gold, silver, or sometimes even paper money, and that the device will indicate the presence of such material by indicating a change in direction relative to the operator.\n\nThis theory of operation is not supported by scientific theory; the devices have not been shown to work in blind testing, and the resonance principle invoked has not been shown to work in laboratories (and is not consistently employed by LRL manufacturers). In addition, the Inverse-Square Law limits the effective possible signal strength of any putative LRL; moreover, not only does this attenuation apply to the supposed emissions from the LRL devices, but the return signals from the sought-after targets are further attenuated by the same constraints. Since most of these LRL devices are powered by low voltage, low current AA, AAA or 9v cells, the resultant power available for emissions is quite minuscule at best, and the return signal would suffer even greater attenuation. Examples exist of LRL devices having no internal power source at all, and these are advertised as being self-powered or powered by ambient static electricity; these are indistinguishable from dowsing rods.\n\nMany such devices contain non-functional circuitry or naively constructed approximations of radio transmitters. A few do have functional circuitry, putting out a weak signal with a function generator or a simple timer circuit, but are still largely useless in comparison with a coil-based metal detector; others have been found to contain intentionally obfuscated or completely superfluous components (from individual components such as inductors or ribbon cables up to, in some cases, pocket calculators), often indicative of intentional fraud, incompetence, or both, by the designer. Such functioning circuitry as exists in such devices usually has no obvious way (motor, solenoid, etc.) to connect to any rotating joint in the device either, meaning the devices are often entirely dependent on the ideomotor effect to function.\n\nAuthor Tom Clancy came under fire for including the DKL Lifeguard, a long-range locator purported to be useful for detecting people, in critical passages of his novel \"Rainbow Six\". A study by Sandia National Laboratories proved the Lifeguard to be completely useless, and other designs by the Lifeguard's creator Thomas Afilani have been shown to contain numerous dummy components with no clear function.\n\nAccusing the manufacturers of fraud, the UK banned export of the GT 200, used by the government of Thailand, and the ADE 651, used by the government of Iraq, in January 2010.\n\n\n"}
{"id": "46955256", "url": "https://en.wikipedia.org/wiki?curid=46955256", "title": "On a Piece of Chalk", "text": "On a Piece of Chalk\n\nOn a Piece of Chalk was an 1868 lecture by Thomas Henry Huxley to the working men of Norwich during a meeting of the British Association for the Advancement of Science. It was published as an essay in \"Macmillan's Magazine\" in London later that year. The piece reconstructs the geological history of Britain from a simple piece of chalk and demonstrates science as \"organized common sense\".\n\n\"On a Piece of Chalk\" was republished by Scribner in 1967 with an introduction by Loren Eiseley and illustrations by Rudolf Freund.\n\nIn 1967, Dael Wolfle of the AAAS gave a favorable review for \"On a Piece of Chalk\", writing:\nIn April 2015, physicist and Nobel laureate Steven Weinberg included \"On a Piece of Chalk\" in a personal list of \"the 13 best science books for the general reader\".\n\n\n"}
{"id": "13440061", "url": "https://en.wikipedia.org/wiki?curid=13440061", "title": "Paprika oleoresin", "text": "Paprika oleoresin\n\nPaprika oleoresin (also known as paprika extract and oleoresin paprika) is an oil-soluble extract from the fruits of \"Capsicum annuum\" or \"Capsicum frutescens\", and is primarily used as a colouring and/or flavouring in food products. It is composed of vegetable oil (often in the range of 97% to 98%), capsaicin, the main flavouring compound giving pungency in higher concentrations, and capsanthin and capsorubin, the main colouring compounds (among other carotenoids). It is much milder than capsicum oleoresin, often containing no capsaicin at all. \n\nExtraction is performed by percolation with a variety of solvents, primarily hexane, which are removed prior to use. Vegetable oil is then added to ensure a uniform color saturation.\n\nFoods coloured with paprika oleoresin include cheese, orange juice, spice mixtures, sauces, sweets and emulsified processed meats. In poultry feed, it is used to deepen the colour of egg yolks.\n\nIn the United States, paprika oleoresin is listed as a color additive “exempt from certification” ( Title 21 Code of Federal Regulations part 73). In Europe, paprika oleoresin (extract), and the compounds capsanthin and capsorubin are designated by E160c.\n"}
{"id": "518069", "url": "https://en.wikipedia.org/wiki?curid=518069", "title": "Philip Sheppard", "text": "Philip Sheppard\n\nProfessor Philip MacDonald Sheppard, F.R.S. (27 July 1921 – 17 October 1976) was a British geneticist and lepidopterist. He made advances in ecological and population genetics in lepidopterans, pulmonate land snails and humans. In medical genetics, he worked with Sir Cyril Clarke on Rh disease.\n\nHe was born on 27 July 1921 in Marlborough, Wiltshire, England and attended Marlborough College from 1935 to 1939.\n\n\nCyril Clarke answered an advert in an insect magazine for swallowtail butterfly pupa that had been placed by Sheppard. They met and began working together in their common interest of lepidoptery. They also worked on Rh disease.\n\nIn 1961 Sheppard started a colony of scarlet tiger moths by the Wirral Way, West Kirby, Merseyside, which were rediscovered in 1988 by Cyril Clarke, who continued to observe them in his retirement to study changes in the moth population.\n\nSheppard married Patricia Beatrice Lee in 1948. They had three sons. He died of acute leukemia on 17 October 1976.\n\n"}
{"id": "19333139", "url": "https://en.wikipedia.org/wiki?curid=19333139", "title": "Political forecasting", "text": "Political forecasting\n\nPolitical forecasting aims at predicting the outcome of elections.\n\nPeople have long been interested in predicting election outcomes. Quotes of betting odds on papal succession appear as early as 1503, when such wagering was already considered “an old practice.” Political betting also has a long history in Great Britain. As one prominent example, Charles James Fox, the late-eighteenth-century Whig statesman, was known as an inveterate gambler. His biographer, George Otto Trevelyan, noted that“(f)or ten years, from 1771 onwards, Charles Fox betted frequently, largely, and judiciously, on the social and political occurrences of the time.”\n\nBefore the advent of scientific polling in 1936, betting odds in the United States correlated strongly to vote results. Since 1936, opinion polls have been a basic part of political forecasting. More recently, prediction markets have been formed, starting in 1988 with Iowa Electronic Markets.\n\nWith the advent of statistical techniques, electoral data have become increasingly easy to handle. It is no surprise, then, that election forecasting has become a big business, for polling firms, news organizations, and betting markets as well as academic students of politics.\n\nAcademic scholars have constructed models of voting behavior to forecast the outcomes of elections. These forecasts are derived from theories and empirical evidence about what matters to voters when they make electoral choices. The forecast models typically rely on a few predictors in highly aggregated form, with an emphasis on phenomena that change in the short-run, such as the state of the economy, so as to offer maximum leverage for predicting the result of a specific election.\n\nAn early successful model which is still being used is The Keys to the White House by Allan Lichtman. Election forecasting in the United States was first brought to the attention of the wider public by Nate Silver and his FiveThirtyEight website in 2008. Currently, there are many competing models trying to predict the outcome of elections in the United States, the United Kingdom, and elsewhere.\n\nCombining poll data lowers the forecasting mistakes of a poll. Political forecasting models include averaged poll results, such as the RealClearPolitics poll average.\n\nPoll damping is when incorrect indicators of public opinion are not used in a forecast model. For instance, early in the campaign, polls are poor measures of the future choices of voters. The poll results closer to an election are a more accurate prediction. Campbell shows the power of poll damping in political forecasting.\n\nWhen discussing the likelihood of a particular electoral outcome, political forecasters tend to use one of a small range of shorthand phrases. These include:\n\n\nPrediction markets show very accurate forecasts of an election outcome. One example is the Iowa Electronic Markets. In a study, 964 election polls were compared with the five US presidential elections from 1988 to 2004. Berg et al. (2008) showed that the Iowa Electronic Markets topped the polls 74% of the time. However, damped polls have been shown to top prediction markets. Comparing damped polls to forecasts of the Iowa Electronic Markets, Erikson and Wlezien (2008) showed that the damped polls outperform all markets or models.\n\nPolitical scientists and economists oftentimes use regression models of past elections. This is done to help forecast the votes of the political parties – for example, Democrats and Republicans in the US. The information helps their party’s next presidential candidate forecast the future. Most models include at least one public opinion variable, a trial heat poll, or a presidential approval rating.\nBayesian statistics can also be used to estimate the posterior distributions of the true proportion of voters that will vote for each candidate in each state, given both the polling data available and the previous election results for each state. Each poll can be weighted based on its age and its size, providing a highly dynamic forecasting mechanism as Election day approaches. http://electionanalytics.cs.illinois.edu/ is an example of a site that employs such methods.\n\n"}
{"id": "3277654", "url": "https://en.wikipedia.org/wiki?curid=3277654", "title": "Print culture", "text": "Print culture\n\nPrint culture embodies all forms of printed text and other printed forms of visual communication. One prominent scholar in the field is Elizabeth Eisenstein, who contrasted print culture, which appeared in Europe in the centuries after the advent of the Western printing-press (and much earlier in China where woodblock printing was used from 594 AD), to \"scribal culture\". Walter Ong, by contrast, has contrasted written culture, including scribal, to oral culture. Ong is generally considered one of the first scholars to define print culture in contrast to oral culture. These views are related as the printing press brought a vast rise in literacy, so that one of its effects was simply the great expansion of written culture at the expense of oral culture. The development of printing, like the development of writing itself, had profound effects on human societies and knowledge. \"Print culture\" refers to the cultural products of the printing transformation.\n\nIn terms of image-based communication, a similar transformation came in Europe from the fifteenth century on with the introduction of the old master print and, slightly later, popular prints, both of which were actually much quicker in reaching the mass of the population than printed text.\n\nPrint culture is the conglomeration of effects on human society that is created by making printed forms of communication. Print culture encompasses many stages as it has evolved in response to technological advances. Print culture can first be studied from the period of time involving the gradual movement from oration to script as it is the basis for print culture. As the printing became commonplace, script became insufficient and printed documents were mass-produced. The era of physical print has had a lasting effect on human culture, but with the advent of digital text, some scholars believe the printed word is becoming obsolete.\n\nThe electronic media, including the World Wide Web, can be seen as an outgrowth of print culture.\n\nOral culture was all that existed. Oral culture gradually found the need to store what was said for long periods of time, and slowly developed scribal culture. Scribal culture being inaccurate and tedious at best developed into print culture. Each segment is rich with its own effects on the world. Scribal culture, defined by the written or physical conveying of ideas, is important to understand in achieving a grasp on the unfolding of print culture itself. Scholars disagree over when scribal culture developed. Walter Ong argues that scribal culture cannot exist until an alphabet is created, and a form of writing standardized. On the other hand, D. F. McKenzie argues that even communicative notches on a stick, or structure, represent “text”, and therefore scribal culture.\n\nOng suggests scribal culture is defined by an alphabet. McKenzie says that the key to scribal culture is non-verbal communication, which can be accomplished in more ways than using an alphabet. These two views give rise to the importance of print culture. In scribal culture, procuring documents was a difficult task, and documentation would then be limited to the rich only. Ideas are difficult to spread amongst large groups of people over large distances of land, not allowing for effective dissemination of knowledge.\n\nScribal culture also deals with large levels of inconsistency. It was always considered that the oldest document was the most accurate, as it had been copied the least. In the process of copying documents, many times the meaning became changed, and the words different. Reliance on the written text of the time was never exceedingly strong. Over time, a greater need for reliable, quickly reproduced, and a relatively inexpensive means of distributing written text arose. Scribal culture, transforming into print culture, was only replicated in manners of written text.\n\nThe Chinese invention of paper and woodblock printing, at some point before the first dated book in 868 (the Diamond Sutra) produced the world's first print culture. Hundreds of thousands of books, on subjects ranging from Confusion Classics to science and mathematics, were printed using woodblock printing.\n\nPaper and woodblock printing were introduced into Europe in the 15th Century, and the first printed books began appearing in Europe. Chinese movable type was spread to Korea during the Goryeo Dynasty. Around 1230, Koreans invented a metal type movable printing which was described by the French scholar Henri-Jean Martin as \"extremely similar to Gutenberg's\". East metal movable type was spread to Europe between the late 14th century and early 15th century. The invention of Johannes Gutenberg's printing press (circa 1450) greatly reduced the amount of labor required to produce a book leading to a tremendous increase in the number of books produced. Early printers tried to keep their printed copies of a text as faithful as possible to the original manuscript. Even so, the earliest publications were still often different from the original, for a short time, in some ways manuscripts still remaining more accurate than printed books.\n\nHand-copied illustrations were replaced by first woodcuts, later engravings that could be duplicated precisely, revolutionizing technical literature (Eisenstein 155).\n\nEisenstein has described how the high costs of copying scribal works often led to their abandonment and eventual destruction. Furthermore, the cost and time of copying led to the slow propagation of ideas. In contrast, the printing press allowed rapid propagation of ideas, resulting in knowledge and cultural movements that were far harder to destroy.\n\nEisenstein points to prior renaissances (rebirths) of classical learning prior to the printing press that failed. In contrast, the Renaissance was a permanent revival of classical learning because the printing of classical works put them into a permanent and widely read form.\n\nSimilarly, Eisenstein points to a large number of prior attempts in Western Europe to assert doctrines contrary to the ruling Catholic Church. In contrast, the Protestant Reformation spread rapidly and permanently due to the printing of non-conformist works such as the 95 Theses.\n\nWith the shift towards printing presses in the West, after Johannes Gutenberg developed a method that was cheap, fast, and filling the demand for books, the Renaissance truly came into its own. Even though the printing press was a paradigm shifting invention, printing had many critics, who were afraid that books could spread lies and subversion or corrupt unsuspecting readers. Also, they were afraid that the printed texts would spread heresy and sow religious discord. The Gutenberg Bible was the first book produced with moveable type in Europe. Martin Luther's Bible, which was published in German in 1522, started the Protestant Reformation. Latin's importance as a language started languishing with the rise of texts written in national languages. The shift from scholarly Latin to everyday languages marked an important turning point in print culture. The vernacular Bibles were important to other nations, as well. The King James Authorized Version was published in 1611, for example. Along with the religious tracts, the scientific revolution was largely due to the printing press and the new print culture. Scholarly books were more accessible, and the printing press provided more accurate diagrams and symbols. Along with scientific texts, like the works of Copernicus, Galileo, and Tycho Brahe, atlases and cartography started taking off within the new print culture, mostly due to the exploration of different nations around the world.\n\nWith the rise of literacy, books and other texts became more entrenched in the culture of the West. Along with literacy and more printed words also came censorship, especially from governments. In France, Voltaire and Rousseau were both imprisoned for their works. Other authors, like Montesquieu and Diderot, had to publish outside France. Censored books became a valuable commodity within this environment, and an underground network of book smugglers started operating within France. Diderot and Jean d'Alembert created the \"Encyclopedie\", which was published in 1751 in seventeen folio volumes with eleven volumes of engravings. This work embodied the essence of the Age of Enlightenment.\n\nNumerous eras throughout history have been defined through the use of print culture. The American Revolution was a major historical conflict fought after print culture brought the rise of literacy. Furthermore, print culture's ability to shape and guide society was a critical component before, during, and after the Revolution.\n\nMany different printed documents influenced the beginning of the revolution. The Magna Carta was originally a scribal document of 1215, recording an oral transaction restricting the power of English kings and defining rights of subjects. It was revitalized by being printed in the 16th century and widely read by the increasingly literate English and colonial population thereafter. The Magna Carta was used as a basis for the development of English liberties by Sir Edward Coke and became a basis for writing the Declaration of Independence.\n\nAdditionally, during the 18th century, the production of printed newspapers in the colonies greatly increased. In 1775, more copies of newspapers were issued in Worcester, Massachusetts than were printed in all of New England in 1754, showing that the existence of the conflict developed a need for print culture. This onslaught of printed text was brought about by the anonymous writings of men such as Benjamin Franklin, who was noted for his many contributions to the newspapers, including the Pennsylvania Gazette. This increase was primarily due to the easing of the government's tight control of the press, and without the existence of a relatively free press, the American Revolution may have never taken place. The production of so many newspapers can mostly be attributed to the fact that newspapers had a huge demand; printing presses were writing the newspapers to complain about the policies of the British government, and how the British government was taking advantage of the colonists.\n\nIn 1775, Thomas Paine wrote the pamphlet \"Common Sense,\" a pamphlet that introduced many ideas of freedom to the Colonial citizens. Allegedly, half a million copies were produced during the pre-revolution era. This number of pamphlets produced is significant as there were only a few million freed men in the colonies. However, \"Common Sense\" was not the only manuscript that influenced people and the tide of the revolution. Among the most influential were James Otis' \"Rights of the British Colonies\" and John Dickinson's \"Farmer's Letters\". Both of these played a key role in persuading the people and igniting the revolution.\n\nNewspapers were printed during the revolution covering battle reports and propaganda. These reports were usually falsified by Washington in order to keep morale up among American citizens and troops. Washington was not the only one to falsify these reports, as other generals (on both sides) used this technique as well. The newspapers also covered some of the battles in great detail, especially the ones that the American forces won, in order to gain support from other countries in hopes that they would join the American forces in the fight against the British.\n\nBefore the Revolution, the British placed multiple acts upon the colonies, such as the stamp act. Many newspaper companies worried that the British would punish them for printing papers without a British seal, so they were forced to temporarily discontinue their work or simply change the title of their paper. However, some patriotic publishers, particularly those in Boston, continued their papers without any alteration of their title.\n\nThe Declaration of Independence is a very important written document that was drafted by the original thirteen colonies, as a form of print culture that would declare their independence from the Kingdom of Great Britain and explained the justifications for doing so. While it was explicitly documented on July 4, 1776, it was not recognized by Great Britain until September 3, 1783, by the Treaty of Paris.\n\nAfter the signing of the Treaty of Paris, a cluster of free states in need of a government was created. The basis for this government was known as the Articles of Confederation, which were put to effect in 1778 and formed the first governing document of the United States of America. This document, however, was found to be unsuitable to outline the structure of the government, and thus showed an ineffectual use of print culture, and since printed texts were the most respected documents of the time, this called for an alteration in the document used to govern the confederation.\n\nIt was the job of the Constitutional Convention to reform the document, but they soon discovered that an entirely new text was needed in its place. The result was the United States Constitution. In the form of written word, the new document was used to grant more power to the central government, by expanding into branches. After it was ratified by all of the states in the union, the Constitution served as a redefinition of the modern government.\n\nThomas Jefferson was noted as saying, “The basis of our government being the opinion of the people, the very first object should be to keep that right; and were it left to me to decide whether we should have a government without newspapers, or newspapers without a government, I should not hesitate for a moment to prefer the latter.” This serves as an excellent example of how newspapers were highly regarded by the colonial people. In fact, much like other forms of 18th century print culture, newspapers played a very important role in the government following the Revolutionary War. Not only were they one of the few methods in the 18th century to voice the opinion of the people, they also allowed for the ideas to be disseminated to a wide audience, a primary goal of printed text. A famous example of the newspaper being used as a medium to convey ideas were the Federalist Papers. These were first published in New York City newspapers in 1788 and pushed for people to accept the idea of the United States Constitution by enumerating 85 different articles that justified its presence, adding to a series of texts designed to reinforce each other, and ultimately serving as a redefinition of the 18th century.\n\nToday, print has matured to a state where the majority of modern society has come to have certain expectations regarding the printed book:\n\nCopyright laws help to protect these standards. However, a few regions do exist in the world where literary piracy has become a standard commercial practice. In such regions, the preceding expectations are not the norm. (Johns 61)\n\nCurrently, there are still approximately 2.3 billion books still sold each year worldwide. However, this number is steadily decreasing due to the ever-growing popularity of the Internet and other forms of digital media.\n\nAs David J. Gunkel states in his article \"What's the Matter with Books?\", society is currently in the late age of the text; the moment of transition from print to electronic culture where it is too late for printed books and yet too early for electronic texts. Jay David Bolter, author of \"Writing Space\", also discusses our culture in what he calls \"the late age of print.\" The current debate going on in the literary world is whether or not the computer will replace the printed book as the repository and definition of human knowledge. There is still a very large audience committed to printed texts, who are not interested in moving to a digital representation of the repository for human knowledge. Bolter, in his own scholarship and also along with Richard Grusin in \"Remediation\", explains that despite current fears about the end of print, the format will never be erased but only remediated. New forms of technology (new media) will be created which utilize features of old media, thus preventing old media's (aka print's) erasure. At the same time, there are also concerns over whether obsolescence and deterioration make digital media unsuitable for long-term archival purposes. Much of the early paper used for print is highly acidic, and will ultimately destroy itself.\n\nThe way that information is transferred has also changed with this new age of digital text and the shift towards electronic media. Gunkel states that information now takes the form of immaterial bits of digital data that are circulated at the speed of light. Consequently, what the printed book states about the exciting new culture and economy of bits is abraded by the fact that this information has been delivered in the slow and outdated form of physical paper.\n\nIn the article, \"The First Amendment, Print Culture, and the Electronic Environment\", the author notes that expectations will change as information becomes less tied to specific locations, and as machines become networked and linked to other machines. This means that in the future certain goods will not be associated with their origins.\n\nThe article \"The First Amendment, Print Culture, and the Electronic Environment\" () also mentions how the new electronic age will make print better. Placing information into electronic form not only liberates the information from its pages but removes the need for specialized spaces to hold particular kinds of information. People have become increasingly accustomed to acquiring information from our homes that used to be only accessible from an office or library. Once computers are all networked, all information should, at least in theory, be accessible from all places. Print itself contained a set of invisible and inherent censors, which electronic media is helping to remove from the creation of text. Points of control that are present in print space are no longer present as distribution channels multiply, as copying becomes faster and cheaper, as more information is produced, as economic incentives for working with information increase, and as barriers and boundaries that inhibited working with information are crossed.\n\nThere are more online publications, journals, newspapers, magazines, and businesses than ever before. While this brings society closer, and makes publications more convenient and accessible, ordering a product online reduces contact with others. Many online articles are anonymous, making the 'death of the author' even more apparent. Anyone can post articles and journals online anonymously. In effect, the individual becomes separated from the rest of society.\n\nThe advances of technology in print culture can be separated into three shifts: \n\nThe written word has made history recordable and accurate. The printing press, some may argue, is not a part of print culture, but had a substantial impact upon the development of print culture through the times. The printing press brought uniform copies and efficiency in print. It allowed a person to make a living from writing. Most importantly, it spread print throughout society.\n\nThe advances made by technology in print also impact anyone using cell phones, laptops, and personal digital organizers. From novels being delivered via a cell phone, the ability to text message and send letters via e-mail clients, to having entire libraries stored on PDAs, print is being influenced by devices.\n\nSymbols, logos and printed images are forms of printed media that do not rely on text. They are ubiquitous in modern urban life. Analyzing these cultural products is an important part of the field of cultural studies. Print has given rise to a wider distribution of pictures in society in conjunction with the printed word. Incorporation of printed pictures in magazines, newspapers, and books gave printed material a wider mass appeal through the ease of visual communication.\n\nThere is a common miscommunication that occurs when discussing that which is\nprint and that which is text. In the literary world, notable scholars such as\nWalter Ong and D.F. McKenzie have disagreed on the meaning of text. The\npoint of the discussion at hand is to have a word that encompasses all forms\nof communication - that which is printed, that which is online media, even a\nbuilding or notches on a stick. According to Walter Ong text did not come\nabout until the development of the first alphabet, well after humanity existed.\nAccording to Mckenzie primitive humans did have a form of text they used to\ncommunicate with their cave drawings. This is discussed in literary theory. Print, however, is a representation of that which is printed, and\ndoes not encompass all forms of communication (e.g. a riot at a football game).\n\n\n\n"}
{"id": "51256", "url": "https://en.wikipedia.org/wiki?curid=51256", "title": "Public choice", "text": "Public choice\n\nPublic choice or public choice theory is \"the use of economic tools to deal with traditional problems of political science\". Its content includes the study of political behavior. In political science, it is the subset of positive political theory that studies self-interested agents (voters, politicians, bureaucrats) and their interactions, which can be represented in a number of ways – using (for example) standard constrained utility maximization, game theory, or decision theory. Public-c\nChoice analysis has roots in positive analysis (\"what is\") but is often used for normative purposes (\"what ought to be\") in order to identify a problem or to suggest improvements to constitutional rules (i.e., constitutional economics).\n\nThe \"Journal of Economic Literature\"s classification code regards public choice as a subarea of Microeconomics, under JEL: D7: \"Analysis of Collective Decision-Making\" (specifically, JEL: D72: \"Economic Models of Political Processes: Rent-Seeking, Elections, Legislatures, and Voting Behavior\"). \n\nPublic choice theory is also closely related to social-choice theory, a mathematical approach to aggregation of individual interests, welfares, or votes. Much early work had aspects of both, and both fields use the tools of economics and game theory. Since voter behavior influences the behavior of public officials, public-choice theory often uses results from social-choice theory. General treatments of public choice may also be classified under public economics.\n\nA precursor of modern public choice theory was the work of Knut Wicksell (1896), which treated government as political exchange, a \"quid pro quo\", in formulating a benefit principle linking taxes and expenditures.\n\nSome subsequent economic analysis has been described as treating government as though it attempted \"to maximize some kind sort of welfare function for society\" and as distinct from characterizations of economic agents, such as those in business. In contrast, public choice theory modeled government as made up of officials who, besides pursuing the public interest, might act to benefit themselves, for example in the budget-maximizing model of bureaucracy, possibly at the cost of efficiency.\n\nModern public-choice theory has been dated from the work of Duncan Black, sometimes called \"the founding father of public choice\". In a series of papers from 1948, which culminated in \"The Theory of Committees and Elections\" (1958), and later, Black outlined a program of unification toward a more general \"Theory of Economic and Political Choices\" based on common formal methods, developed underlying concepts of what would become median voter theory, and rediscovered earlier works on voting theory.\n\nKenneth J. Arrow's \"Social Choice and Individual Values\" (1951) influenced formulation of the theory. Among other important works are Anthony Downs (1957) \"An Economic Theory of Democracy\" and Mancur Olson (1965) \"The Logic of Collective Action\".\n\nJames M. Buchanan and Gordon Tullock coauthored \"\" (1962), considered one of the landmarks in public choice. In particular, the preface describes the book as \"about the \"political organization\"\" of a free society. But its methodology, conceptual apparatus, and analytics \"are derived, essentially, from the discipline that has as its subject the \"economic\" organization of such a society\" (1962, p. v). The book focuses on positive-economic analysis as to the development of constitutional democracy but in an ethical context of consent. The consent takes the form of a compensation principle like Pareto efficiency for making a policy change and unanimity or at least no opposition as a point of departure for social choice.\n\nSomewhat later, the probabilistic voting theory started to displace the median voter theory in showing how to find Nash equilibria in multidimensional space. The theory was later formalized further by Peter Coughlin.\n\nOne way to organize the subject matter studied by public choice theorists is to begin with the foundations of the state itself. According to this procedure, the most fundamental subject is the origin of government. Although some work has been done on anarchy, autocracy, revolution, and even war, the bulk of the study in this area has concerned the fundamental problem of collectively choosing constitutional rules. This work assumes a group of individuals who aim to form a government, then it focuses on the problem of hiring the agents required to carry out government functions agreed upon by the members.\n\nGeoffrey Brennan and Loren Lomasky claim that democratic policy is biased to favor \"expressive interests\" and neglect practical and utilitarian considerations. Brennan and Lomasky differentiate between instrumental interests (any kind of practical benefit, both monetary and non-monetary) and expressive interests (forms of expression like applause). According to Brennan and Lomasky, the paradox of voting can be resolved by differentiating between expressive and instrumental interests.\n\nThis argument has led some public choice scholars to claim that politics is plagued by irrationality. In articles published in the \"Econ Journal Watch\", economist Bryan Caplan contended that voter choices and government economic decisions are inherently irrational. Caplan's ideas are more fully developed in his book \"The Myth of the Rational Voter\" (Princeton University Press 2007). Countering Donald Wittman's arguments in \"The Myth of Democratic Failure\", Caplan claims that politics is biased in favor of irrational beliefs.\n\nAccording to Caplan, democracy effectively subsidizes irrational beliefs. Anyone who derives utility from potentially irrational policies like protectionism can receive private benefits while imposing the costs of such beliefs on the general public. Were people to bear the full costs of their \"irrational beliefs\", they would lobby for them optimally, taking into account both their instrumental consequences and their expressive appeal. Instead, democracy oversupplies policies based on irrational beliefs. Caplan defines rationality mainly in terms of mainstream price theory, pointing out that mainstream economists tend to oppose protectionism and government regulation more than the general population, and that more educated people are closer to economists on this score, even after controlling for confounding factors such as income, wealth or political affiliation. One criticism is that many economists do not share Caplan's views on the nature of public choice. However, Caplan does have data to support his position. Economists have, in fact, often been frustrated by public opposition to economic reasoning. As Sam Peltzman puts it: Economists know what steps would improve the efficiency of HSE [health, safety, and environmental] regulation, and they have not been bashful advocates of them. These steps include substituting markets in property rights, such as emission rights, for command and control ... The real problem lies deeper than any lack of reform proposals or failure to press them. It is our inability to understand their lack of political appeal.Public choice's application to government regulation was developed by George Stigler (1971) and Sam Peltzman (1976).\n\nPublic choice theory is often used to explain how political decision-making results in outcomes that conflict with the preferences of the general public. For example, many advocacy group and pork barrel projects are not the desire of the overall democracy. However, it makes sense for politicians to support these projects. It may make them feel powerful and important. It can also benefit them financially by opening the door to future wealth as lobbyists. The project may be of interest to the politician's local constituency, increasing district votes or campaign contributions. The politician pays little or no cost to gain these benefits, as he is spending public money. Special-interest lobbyists are also behaving rationally. They can gain government favors worth millions or billions for relatively small investments. They face a risk of losing out to their competitors if they don't seek these favors. The taxpayer is also behaving rationally. The cost of defeating any one government give-away is very high, while the benefits to the individual taxpayer are very small. Each citizen pays only a few pennies or a few dollars for any given government favor, while the costs of ending that favor would be many times higher. Everyone involved has rational incentives to do exactly what they are doing, even though the desire of the general constituency is opposite. Costs are diffused, while benefits are concentrated. The voices of vocal minorities with much to gain are heard over those of indifferent majorities with little to individually lose. However the notion that groups with concentrated interests will dominate politics is incomplete because it is only one half of political equilibrium. Something must incite those preyed upon to resist even the best organized concentrated interests. In his article on interest groups Gary Becker identified this countervailing force as being the deadweight loss from predation. His views capped what has come to be known as the Chicago school of political economy and it has come in sharp conflict with the so-called Virginia faction of public choice due to its assertion that politics will tend towards efficiency due to nonlinear deadweight losses and due to its claim that political efficiency renders policy advice irrelevant.\n\nWhile good government tends to be a pure public good for the mass of voters, there may be many advocacy groups that have strong incentives for lobbying the government to implement specific policies that would benefit them, potentially at the expense of the general public. For example, lobbying by the sugar manufacturers might result in an inefficient subsidy for the production of sugar, either direct or by protectionist measures. The costs of such inefficient policies are dispersed over all citizens, and therefore unnoticeable to each individual. On the other hand, the benefits are shared by a small special-interest group with a strong incentive to perpetuate the policy by further lobbying. Due to rational ignorance, the vast majority of voters will be unaware of the effort; in fact, although voters may be aware of special-interest lobbying efforts, this may merely select for policies which are even harder to evaluate by the general public, rather than improving their overall efficiency. Even if the public were able to evaluate policy proposals effectively, they would find it infeasible to engage in collective action in order to defend their diffuse interest. Therefore, theorists expect that numerous special interests will be able to successfully lobby for various inefficient policies. In public choice theory, such scenarios of inefficient government policies are referred to as \"government failure\" – a term akin to \"market failure\" from earlier theoretical welfare economics.\n\nA field that is closely related to public choice is the study of rent-seeking. This field combines the study of a market economy with that of government. Thus, one might regard it as a new political economy. Its basic thesis is that when both a market economy and government are present, government agents provide numerous special market privileges. Both the government agents and self-interested market participants seek these privileges in order to partake in the resulting monopoly rent. Rentiers gain benefits above what the market would have offered, but in the process allocate resources in sub-optimal fashion from a societal point of view.\n\nRent-seeking is broader than public choice in that it applies to autocracies as well as democracies and, therefore, is not directly concerned with collective decision making. However, the obvious pressures it exerts on legislators, executives, bureaucrats, and even judges are factors that public choice theory must account for in its analysis of collective decision-making rules and institutions. Moreover, the members of a collective who are planning a government would be wise to take prospective rent-seeking into account.\n\nAnother major claim is that much of political activity is a form of rent-seeking which wastes resources. Gordon Tullock, Jagdish Bhagwati, and Anne Osborn Krueger have argued that rent-seeking has caused considerable waste. In a parallel line of research Fred McChesney claims that rent extraction causes considerable waste, especially in the developing world. As the term implies, rent extraction happens when officials use threats to extort payments from private parties.\n\nAnother major sub-field is the study of bureaucracy. The usual model depicts the top bureaucrats as being chosen by the chief executive and legislature, depending on whether the democratic system is presidential or parliamentary. The typical image of a bureau chief is a person on a fixed salary who is concerned with pleasing those who appointed him or her. The latter have the power to hire and fire him or her more or less at will. The bulk of the bureaucrats, however, are civil servants whose jobs and pay are protected by a civil service system against major changes by their appointed bureau chiefs. This image is often compared with that of a business owner whose profit varies with the success of production and sales, who aims to maximize profit, and who can in an ideal system hire and fire employees at will. William Niskanen is generally considered the founder of public choice literature on the bureaucracy.\n\nFrom such results it is sometimes asserted that public choice theory has an anti-state tilt. But there is ideological diversity among public choice theorists. Mancur Olson for example was an advocate of a strong state and instead opposed political interest group lobbying. More generally, James Buchanan has suggested that public choice theory be interpreted as \"politics without romance\", a critical approach to a pervasive earlier notion of idealized politics set against market failure.\n\nThe British journalist, Alistair Cooke, commenting on the Nobel Prize awarded to James M. Buchanan in 1986, reportedly summarized the public choice view of politicians by saying, \"Public choice embodies the homely but important truth that politicians are, after all, no less selfish than the rest of us.\"\n\nSeveral notable public choice scholars have been awarded the Nobel Prize in Economics, including James M. Buchanan (1986), George Stigler (1982), Gary Becker (1992), Vernon Smith (2002) and Elinor Ostrom (2009). In addition, James Buchanan, Vernon Smith, and Elinor Ostrom were former presidents of the Public Choice Society.\n\nBuchanan and Tullock themselves outline methodological qualifications of the approach developed in their work \"The Calculus of Consent\" (1962), p. 30:\n\n"}
{"id": "23841113", "url": "https://en.wikipedia.org/wiki?curid=23841113", "title": "Science in Action (radio programme)", "text": "Science in Action (radio programme)\n\nScience in Action is a long-running weekly radio programme produced by the BBC World Service and currently hosted by British journalists Roland Pease and Marnie Chesterton, and scientist and broadcaster Professor Adam Hart. It is broadcast on Thursdays at 18.32 GMT and repeated twice the following day, at 01.32 and 08.32.\n\nA programme with the title \"Science in Action\" is believed to have begun life in 1964, when it replaced an earlier series, dating from the 1950s, called \"Science and Industry\". From September 1965 a short-lived series called \"Science in Action\" ran on the Home Service; it was broadcast at 19.30 on Thursdays, later 21.30. In December 1965 it was moved to 14.30 on Fridays. The present weekly World Service series, also called \"Science in Action\", began on Saturday 7 July 1979.\n"}
{"id": "58312722", "url": "https://en.wikipedia.org/wiki?curid=58312722", "title": "Significant Figures (book)", "text": "Significant Figures (book)\n\nSignificant Figures: The Lives and Work of Great Mathematicians is a 2017 nonfiction book by British author Ian Stewart, published by Basic Books. In the work, Stewart discusses the lives and contributions of 25 figures who are prominent in the history of mathematics.\n\nIn \"Kirkus Reviews\", it was written that \"even a popularizer as skilled and prolific as Stewart cannot expect general readers to fully digest his highly distilled explanations of what these significant figures did to resolve ever more complex conundrums as math advanced.\" However, the reviewer praised Stewart's sketches of the lives and times of the innovators. The book was described as \"a text for teachers, precocious students, and intellectually curious readers unafraid to tread unfamiliar territory\".\n"}
{"id": "45335171", "url": "https://en.wikipedia.org/wiki?curid=45335171", "title": "Sperm precedence", "text": "Sperm precedence\n\nSperm precedence, also known as sperm predominance, is tendency of a female who has been bred by multiple males to give birth to their offspring in unequal proportions. Sperm precedence is an important factor in the sperm competition.\n\nSperm precedence can be temporal, favoring either the first or last male to breed the female. (The former is known as \"first sperm precedence\" and the latter is known as \"last sperm precedence\"); or it can favor the male whose sperm are the most motile; or the male whose sperm were delivered closest to the female's ova.\n"}
{"id": "57095514", "url": "https://en.wikipedia.org/wiki?curid=57095514", "title": "TRIAD Berlin", "text": "TRIAD Berlin\n\nTRIAD Berlin is a German exhibition design firm based in Berlin with an office in Shanghai. The agency is best known for the development of two major sports museums: the German Football Museum in Dortmund and the FIFA World Football Museum in Zürich. \n\nThe Berlin agency was founded in 1994 by Lutz Engelke to realize the exhibition \"The Dream of Vision – Age of Televisions\" at Gasometer Oberhausen in the city of Oberhausen. After designing the theme pavilion \"Urban Planet\" at Expo 2010, the company also has an additional permanent office in Shanghai, China.\n\nThe agency focusses on spatial communication and sees itself as a \"Think & Do Tank\". Its main goal is to turn complex ideas into tangible experiences. Although the content of the projects varies, the method used for their development is always the same: the creation of stories that can be experienced from abstract ideas. A key component of this process is the development of a narrative that structures the messages and guides the visitor through the exhibition. Most of the times, the storytelling is enhanced by digital media. TRIAD exhibitions rely heavily on digital interactive exhibits with the intention of bridging the gap between the intellectual learning process and the physical experience. In many cases, TRIAD combines education and entertainment in a multimedia space. The result is the so-called edutainment, whose goal is to educate through entertainment.\n\nTRIAD has applied edutainment concepts to various projects focused on children's education, such as the 2018 exhibition on the MS Wissenschaft about working life of the future. In the same way, the company has used this approach in the Brandenburg Gate Museum in Berlin.\n\nThe company is also tackling other challenges that exhibitions face today, like the creation of barrier-free museums. An example of this is the exhibition \"Wilderness Dreams\" in Eifel National Park which is part of the United Nations Decade on Biodiversity.\n\nThe agency designs exhibitions, museums, experience centers, trade fairs, expo pavilions, edutainment concepts, brand environments and events. A lot of the company's projects focus on three main areas: sustainability, sports, and culture.\n\n\n\n"}
{"id": "32317248", "url": "https://en.wikipedia.org/wiki?curid=32317248", "title": "The Miracle of Life", "text": "The Miracle of Life\n\n\"The Miracle of Life\" was an episode produced by \"Nova\" about the human reproductive process. The episode won multiple awards including a Peabody and an Emmy. Photographed by Lennart Nilsson, the program was originally aired in Sweden as \"The Saga of Life.\" The BBC acquired the episode for the documentary series Horizon and aired it on October 11, 1982. The original Swedish version was broadcast on November 25, 1982. Many scenes were edited, and the intro in both versions (The BBC and PBS versions), are different, as well as the scene where the baby was born.\n\nThe Nova episode was written and produced by Bebe Nixon and narrated by Anita Sangiolo, airing on February 15, 1983. It later re-aired on December 15, 1983 as a part of a special presentation out of NOVA's original timeslot.\n\nA sequel called \"Life's Greatest Miracle\" aired on November 20, 2001 on PBS using microimagery taken by Lennart Nilsson with narration by John Lithgow.\n\nThe episode won the following awards:\n\n\n\n"}
{"id": "8359283", "url": "https://en.wikipedia.org/wiki?curid=8359283", "title": "Thomas Orde-Lees", "text": "Thomas Orde-Lees\n\nMajor Thomas Hans Orde-Lees, OBE, AFC (23 May 1877 – 1 December 1958) was a member of Sir Ernest Shackleton's Imperial Trans-Antarctic Expedition of 1914–1917, a pioneer in the field of parachuting, and was one of the first non-Japanese-born men known to have climbed Mount Fuji during the winter.\n\nThomas Hans Orde-Lees was born on 23 May 1877, officially during his parents' holiday in Aachen in what was then Prussia. In fact he was the illegitimate child of Thomas Orde Hastings Lees, a former barrister and the Chief constable of Northampton, and Ada Mary Pattenden (1852–1932), a daughter of the Reverend Canon George Edwin Pattenden, headmaster of Boston Grammar School. Ada was sent off to Thomas' brother's house in Aachen for the birth.\n\nThe Lees family was well off; they lived in the Northampton Chief Constable's house with a number of servants. Thomas the Elder's wife, Grace Lees (née Bateman), agreed to bring up young Thomas as her own. She was made godmother of Ada's nephew Frederick Geoffrey Lees Johnson (1880–1951), an arrangement that provided cover for Grace, Ada and Thomas the Elder to meet up regularly. Ada married Arthur John Coleridge Mackarness, a solicitor, (son of John Fielder Mackarness, Bishop of Oxford) in 1890. Following the death of Thomas the Elder in 1924, Grace took up residence with Arthur and Ada Mackarness at Petersfield. Thomas the Younger kept up with his biological mother until her death in 1932.\n\nOrde-Lees was educated at Marlborough College, the Royal Naval Academy at Gosport (whose headmaster was Ada's brother-in-law, Frederick George Johnson) and the Royal Military College, Sandhurst. He joined the Royal Marines, and was commissioned a second lieutenant in 1895, with promotion to lieutenant on 1 July 1896, and to captain on 16 April 1902. In 1900 he was posted to China and saw action during the Boxer Rebellion.\n\nIn 1910 Orde-Lees applied for a place on Scott's Terra Nova Expedition, but was turned down. When Shackleton was organizing the Imperial Trans-Antarctic Expedition he decided that he needed a representative from the Royal Navy in order to get political and military support for the expedition. Orde-Lees as a skier and motor expert fitted the bill, and after Shackleton applied to Churchill for permission, Orde-Lees was released from his military duties and allowed to join the expedition as storekeeper.\n\nOn board ship he proved unpopular with the rest of the crew — he had a surly, condescending manner and was undisguisedly lazy. Nevertheless, he was an efficient storekeeper. He had a keen interest in physical fitness and took his bicycle on the expedition; after the ship became trapped in the ice he frequently took cycling trips on the ice. Shackleton ordered him not to leave the ship unaccompanied after he became lost while searching for food, and encountered a fierce leopard seal. His cries brought Frank Wild (second in command) out of his tent, who shot the leopard seal at a distance of 10m (30 ft) from Orde-Lees.\n\nWhen the \"Endurance\" was crushed by pack ice, Shackleton took the three lifeboats and led the men over the ice to open water where they used the boats to travel to Elephant Island. Orde-Lees was assigned to the \"Dudley Docker\" under the command of Frank Worsley but failed to pitch in with the other men when a gale threatened to sink the small craft. Despite orders from Worsley, he climbed into his sleeping bag rather than helping with the rowing, although he immediately undertook strenuous and prolonged bailing duty when it looked as if the boat was going to sink.\n\nOnce the boats had arrived at Elephant Island, Shackleton and five men set out for South Georgia in the \"James Caird\" to fetch help. The remaining men, including Orde-Lees, were to spend months living in the remaining two boats, overturned and reinforced with stones and lit by blubber lamps. They were finally rescued on 30 August 1916. For his part in the expedition Orde-Lees received the Silver Polar Medal.\n\nOn Orde-Lees' return to England, World War I was raging. By now an honorary major, Orde-Lees returned to active service with the Royal Marines on 12 November 1916. After serving on the Western Front in the Balloon Corps, Orde-Lees, with the assistance of Shackleton, secured a place in the Royal Flying Corps on 1 August 1917 where he became an enthusiastic advocate for the use of parachutes. He jumped from Tower Bridge into the River Thames to prove their effectiveness and a Parachute Committee was formed with Orde-Lees as secretary to investigate the use of parachutes for pilots. He was awarded the Air Force Cross in the 1919 New Year Honours list, and was appointed an Officer of the Order of the British Empire (OBE) on 10 October. After the war, however, Orde-Lees resigned his commission on 25 April 1919 (reportedly rather than facing a Court Martial after his involvement with a parachuting course for women sponsored by the \"Daily Mail\") and moved to Japan where he taught parachuting techniques to the Japanese Air Force.\n\nIn Japan, Orde-Lees is best known for his winter climbs on Mount Fuji. After an unsuccessful attempt in January 1922, Orde-Lees and a climbing companion, H.S. Crisp, successfully summitted the iconic stratovolcano on 12 February 1922.\nAfter his parachute-training duties ceased, Orde-Lees continued to live in Tokyo. He worked for a time as a correspondent for \"The Times\", which led to an appointment at the British Embassy. His first wife having died, he remarried to a local Japanese woman, Hisako Hoya. He spent almost 20 years teaching English and reading the English news on Japanese Radio.\n\nWhen Japan entered World War II in 1941, Orde-Lees, as a resident alien and citizen of a hostile power, was allowed to leave with his family; they moved to Wellington, New Zealand. There he took a menial job at the New Zealand Correspondence School, although there were rumours that he was working as a spy for the British Government. After the war he wrote a regular children's travel column in the Southern Cross Newspaper and helped organise the Commonwealth Trans-Antarctic Expedition.\n\nHe died on 1 December 1958 after being confined to a mental hospital due to his dementia. He is buried in Karori Cemetery, Wellington, close to fellow \"Endurance\" crew member, Harry McNish.\n\n"}
{"id": "12176463", "url": "https://en.wikipedia.org/wiki?curid=12176463", "title": "Tumbala climbing rat", "text": "Tumbala climbing rat\n\nThe Tumbala climbing rat (\"Tylomys tumbalensis\") is a species of rodent in the family Cricetidae.\nIt is found in Mexico, where it is known only from one locality in Tumbalá, Chiapas. The species is threatened by deforestation.\n\n"}
{"id": "526227", "url": "https://en.wikipedia.org/wiki?curid=526227", "title": "Uhuru (satellite)", "text": "Uhuru (satellite)\n\nUhuru was the first satellite launched specifically for the purpose of X-ray astronomy. It was also known as the X-ray Explorer Satellite, SAS-A (for \"Small Astronomy Satellite\" A, being first of the three-spacecraft SAS series), SAS 1, or Explorer 42. The observatory was launched on December 12, 1970 into an initial orbit of about 560 km apogee, 520 km perigee, 3 degrees inclination, with a period of 96 minutes. The mission ended in March 1973. Uhuru was a scanning mission, with a spin period of ~12 minutes. It performed the first comprehensive survey of the entire sky for X-ray sources, with a sensitivity of about 0.001 times the intensity of the Crab nebula.\n\nThe main objectives of the mission were:\n\nThe payload consisted of two sets of proportional counters, each with ~0.084 m effective area. \nThe counters were sensitive with more than 10% efficiency to X-ray photons in the ~2–20 keV range. \nThe lower energy limit was determined by the attenuation of the beryllium windows of the counter plus a thin thermal shroud that was needed to maintain temperature stability of the spacecraft.\nThe upper energy limit was determined by the transmission properties of the counter filling gas. \nPulse-shape discrimination and anticoincidence techniques were used to filter out emissions of particles and undesirable high-energy photons in the background. \nPulse-height analysis in eight energy channels was used to obtain information on the energy spectrum of the incident photons. \nThe two sets of counters were placed back to back and were collimated to 0.52° × 0.52° and 5.2° × 5.2° (full width at half maximum) respectively. \nWhile the 0.52° detector gave finer angular resolution, the 5.2° detector had higher sensitivity for isolated sources.\n\nUhuru achieved several outstanding scientific advances, including the discovery and detailed study of the pulsing accretion-powered binary X-Ray sources such as Cen X-3, Vela X-1, and Her X-1, the identification of Cygnus X-1, the first strong candidate for an astrophysical black hole, and many important extragalactic sources.\nThe Uhuru Catalog, issued in four successive versions, the last being the 4U catalog, was the first comprehensive X-ray catalog, contains 339 objects and covers the whole sky in the 2—6 keV band.\nThe final version of the source catalog is known as the 4U Catalog; earlier versions were the 2U and 3U catalogs. Sources are referenced as, e.g., \"4U 1700-37\".\n\nThe satellite's name, \"Uhuru\", is the Swahili word for \"freedom\". It was named in recognition of the hospitality of Kenya from where it was launched, from the Italian San Marco launch platform near Mombasa.\n\n\n"}
{"id": "12178305", "url": "https://en.wikipedia.org/wiki?curid=12178305", "title": "Unus mundus", "text": "Unus mundus\n\nUnus mundus, Latin for \"one world\", is the concept of an underlying unified reality from which everything emerges and to which everything returns. \n\nThe idea was popularized in the 20th century by the Swiss psychoanalyst Carl Gustav Jung, though the term can be traced back to scholastics such as Duns Scotus and was taken up again in the 16th century by Gerhard Dorn, a student of the famous alchemist Paracelsus.\n\nJung, in conjunction with the physicist Wolfgang Pauli, explored the possibility that his concepts of archetypes and synchronicity might be related to the \"unus mundus\" - the archetype being an expression of \"unus mundus\"; synchronicity, or \"meaningful coincidence\", being made possible by the fact that both the observer and connected phenomenon ultimately stem from the same source, the \"unus mundus\".\n\nJung was always careful, however, to stress the tentative and provisional nature of such explorations into a unitarian idea of reality.\n\n\n\n"}
{"id": "53842281", "url": "https://en.wikipedia.org/wiki?curid=53842281", "title": "William F. Hoffmann", "text": "William F. Hoffmann\n\nWilliam F. Hoffmann from the University of Arizona, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Astrophysics in 1997, for \"his pioneering work in the field of balloon-borne far-infrared astronomy and discovery of far-infrared radiation from Galactic Center; successful construction of the Multi Mirror Telescope (MMT) and application of infrared array technology to astronomy.\"\n"}
{"id": "14287253", "url": "https://en.wikipedia.org/wiki?curid=14287253", "title": "Wrinkles in Time", "text": "Wrinkles in Time\n\nWrinkles in Time is a book on cosmology by the Nobel laureate physicist George Smoot and Keay Davidson, a science writer for the \"San Francisco Examiner\". It was published in 1994 by William Morrow in hardback.\n\nOn April 23, 1992, a scientific team led by astrophysicist George Smoot announced that it had found the primordial \"seeds\" from which the universe has grown. They analyzed data gathered by NASA's Cosmic Background Explorer satellite and discovered the oldest known objects in the universe—so called \"wrinkles\" in time—thus finding a long-anticipated missing piece in the Big Bang model. In this book, Smoot tells the remarkable tale of his quest for what has been called the cosmologists' Holy Grail. His quest for the seeds of structure in the universe consumed about twenty years. The book traces the obstacle course of discovery. In the book Smoot describes the adventure and along the way he brings the reader up to date in cosmology, giving brief introductions to some important concepts and discoveries in physics and cosmology.\n\nThe research in the book eventually resulted in Smoot winning the 2006 Nobel Prize in Physics, and the book was reprinted in 2007 as a result of the new interest generated by the award. On the cover of the reprint, theoretical physicist Stephen Hawking calls Smoot's observations in the book \"the scientific discovery of the century, if not of all time\".\n\n\n\n\"Wrinkles in Time\" was featured in an episode of \"Big Bang Theory\". In the episode \"The Terminator Decoupling\", Leonard Hofstadter was reading a copy of the book on the train. At the end of the episode, author George Smoot made a cameo appearance.\n\n"}
{"id": "3242821", "url": "https://en.wikipedia.org/wiki?curid=3242821", "title": "Yellow-tailed woolly monkey", "text": "Yellow-tailed woolly monkey\n\nThe yellow-tailed woolly monkey (\"Oreonax flavicauda\") is a New World monkey endemic to Peru. It is a rare primate species found only in the Peruvian Andes, in the departments of Amazonas and San Martin, as well as bordering areas of La Libertad, Huánuco, and Loreto. The yellow-tailed woolly monkey was at first classified in the genus \"Lagothrix\" along with other woolly monkeys, but due to debatable primary sources, they have been placed in \"Oreonax\". \"Oreonax\" has been proposed to be a subgenus of \"Lagothrix\", but others have regarded it as a full genus. A recent extensive study proves that the yellow-tailed woolly monkey may indeed be in \"Lagothrix\".\n\nThe species was first described by Alexander von Humboldt in 1812 under the name \"Simia flavicauda\", based on a skin found 10 years earlier, used by a local man as a horse saddle. Humboldt had never seen a live animal of this species nor a preserved specimen, and believed it belonged to the genus \"Alouatta\". For over 100 years, the species was reported on only a few isolated occasions, so was thought to be extinct.\n\nIn 1926, three specimens were collected in San Martin, which were then brought to the Museum of Natural History. They were believed to be of a new species, but further evidence made it clear that these specimens were of the yellow-tailed woolly monkey.\n\nIn 1974, a group of scientists, led by Russell Mittermeier, and funded by World Wide Fund for Nature, found a young yellow-tailed woolly monkey which was kept as a pet in the city of Pedro Ruiz Gallo, Amazonas. The rediscovery attracted the attention of national and international press, as well as conservation organizations that saw the need to know quickly the status of this species.\n\nIn the summer of 2004, scientists searched for yellow-tailed woolly monkeys in a remote area of San Martin, where the forest is tropical, humid, and quite mountainous. The forest area, threatened due to haphazard tree cutting, was believed to have at least a minor population of the species, and was studied along with two other areas of Peru.\n\nThe loss of habitat due to the tree cutting in the yellow-tailed woolly monkey habitat could prove problematic for the species as a whole. The introduction of farm plots in regions where this organism can be sustained is impacting the yellow-tailed woolly. The farmers of Peru are afraid of losing their farmland to conservation efforts. Farmers said that they did not hunt the monkeys, but that the land is necessary for growing coffee and raising cattle. The balance between avoiding the extinction of the species and maintaining the livelihood of the farmers of Peru is a major issue in the struggle for conservation efforts toward the yellow-tailed woolly monkey.\n\n\"Oreonax flavicauda\" is one of the rarest Neotropical primates and is one of Peru's largest endemic mammals. Adult head and body lengths can range from 51.3 to 53.5 cm, with tails even longer than the body, up to 63 cm (25 in). The average weight is 8 kg in adults, but some males reach 11.5 kg. Peruvian yellow-tailed woolly monkeys are similar in size to the common woolly monkey, also in the genus \"Lagothrix\". They live in large social groups (around 23 individuals) of both male and females. They have low reproductive rates and long interbirth intervals, which adds to their vulnerability for extinction. They are known to express aggressive behaviors upon initial encounters such as branch shaking, “mooning” of the scrotal tuft, and short barking calls. The yellow-tailed woolly monkeys' fur is longer and denser than other woolly monkeys, an adaptation to its cold montane forest habitat. The monkey's color is deep mahogany and copper with a whitish patch on its snout extending from the chin to between its eyes. Its fur gets darker towards its upper body, making its head seem almost black. It has a powerful prehensile tail, with a hairless patch on its underside and a yellowish pelage on the last third of the tail, giving this species its name. This coloration of the tail is not seen in infants and juveniles. The powerful tail is capable of supporting the animal's entire body weight while feeding or just hanging around; it also uses its tail to help travel through the canopy. The monkey is also known for its long, yellowish, pubic hair tuft. It has the ability to leap 15 m (49 ft).\n\nThe yellow-tailed woolly monkey is one of the least known of the primate species. It is also one of the largest neotropical primates. They are regularly found in the tropical Andes. Their habitat is characterized as rough terrain consisting of steep mountain sides and deep river gorges, with canopy heights of 20–25 m. Cloud forest, the habitat of this monkey, are in high altitudes and often have cloud coverage near or in them. The last estimated population count was less than 250 individuals. The current habitat of the yellow-tailed monkey is fragmented due to deforestation, as is the population. This can hinder reproduction, as it limits an already limited population. The Yellow-Tailed monkey has never been subject to a full census so exact numbers vary. A study was done to exam the population however the terrain and fragmented populations made this difficult.\n\nThe yellow-tailed woolly monkey lives in the montane cloud forests of the Peruvian Andes at elevations of above sea level in the departments of Amazonas and San Martin, as well as bordering areas of La Libertad, Huánuco, and Loreto. Its habitat is characterized by steep gorges and ravines. The original extent of its habitat is estimated to be around , but recent estimates put the remaining habitat at between .\n\nIts diet is primarily frugivorous, but leaves, flowers, insects and other invertebrates are also eaten.\nThe species is arboreal and diurnal. It has a multiple-male group social system and a polygamous mating system. They have a variety of vocalisations, including a loud, \"puppy-like\" bark which they use as a territorial or alarm call.\n\nYellow-tailed woolly monkeys participate in geophagy, the consumption of soil. Geophagy is a rare biological behavior, but the species benefits since it results in trace mineral intake of minerals and reduction of intestinal parasites; they tend to suffer from an iron-deficient diet. Their consumption of soil allows them to intake iron that they do not get from their regular diet.\n\nThe inaccessibility of its habitat protected the species until the 1950s. However, the construction of new roads, habitat loss and fragmentation from agriculture, logging and cattle ranching, and subsistence hunting, together with the monkey's naturally low population densities, slow maturation, and low reproductive rate, have led to a predicted decline of at least 80% over the next three generations. This and its restricted geographic distribution have led to this species' current critically endangered status.\n\nConservation work started soon after the species was rediscovered in the mid-1970s. This pioneering work by the Peruvian NGO APECO led to the creation of three protected areas, Rio Abiseo National Park, Alto Mayo Protected Forest, and Cordillera de Colán National Sanctuary. From the mid-1980s until recently, further conservation or research efforts were minimal. Starting in 2007, though, British NGO Neotropical Primate Conservation has been running conservation initiatives for the species throughout its range.\n\nThe species is considered one of \"The World's 25 Most Endangered Primates\".\n\nHabitat loss by deforestation is the biggest threat to the endangerment of yellow-tailed woolly monkeys. The Lima-Tarapoto highway which runs through the regions of San Martin and Amazonas has caused the immigration of people from coastal and high mountain regions leading to overpopulation. Due to the negligence of the regional government of Shipasbamba, Amazonas to the accept requests for conservation efforts, local lands have been the victims of slash-and-burn agriculture by local farmers to support the growing demand of local agricultural crops, as well as to support the increase in population size. With the deforestation and increased population, the monkeys have had their habitat range reduced, which increases their risk of extinction. Conservation efforts led by ASPROCOT have been made recently to help protect the endangered monkeys by turning to alternative forms of agriculture to preserve the remnants of the Amazonas forests. However, a lack of funding has slowed the conservation process.\n\nSeveral communities in Peru have made conservation efforts to preserve the yellow-tailed woolly monkeys through various ways. Community-based conservation efforts have been made in preserving the monkeys, such as in Los Chilchos valley, where the project is directed by the Apenheul Primate Conservation Trust. Efforts include preventing further immigration into areas home to the monkeys and beginning ecosystem protection initiatives. Neotropical Primate Conservation has begun using newly constructed roads in La Esperanza to access areas which are now being used to develop ecotourism initiatives to build awareness about the endangered monkey population and its habitat, which has helped local people understand the importance in preserving the monkeys and that the monkeys can be used as a valuable tourist attraction.\n\n"}
