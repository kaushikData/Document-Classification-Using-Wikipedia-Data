{"id": "42729168", "url": "https://en.wikipedia.org/wiki?curid=42729168", "title": "3076 virus", "text": "3076 virus\n\nThe 3076 virus is a strain of Mobala virus in the genus Arenavirus.\n"}
{"id": "1766622", "url": "https://en.wikipedia.org/wiki?curid=1766622", "title": "Abolfadl Harawi", "text": "Abolfadl Harawi\n\nAbolfadl Harawi () was a 10th-century astronomer from Rey, Persia who, along with al-Khujandi, studied under the patronage of the Buyid dynasty.\n\n"}
{"id": "32544756", "url": "https://en.wikipedia.org/wiki?curid=32544756", "title": "Acetabular fracture", "text": "Acetabular fracture\n\nFractures of the acetabulum occur when the head of the femur is driven into the pelvis. This injury is caused by a blow to either the side or front of the knee and often occurs as a dashboard injury accompanied by a fracture of the femur.\n\nThe acetabulum is a cavity situated on the outer surface of the hip bone, also called the coxal bone or innominate bone. It is made up of three bones, the ilium, ischium, and pubis. Together, the acetabulum and head of the femur form the hip joint.\n\nFractures of the acetabulum in young individuals usually result from a high energy injury like vehicular accident or feet first fall. In older individuals or those with osteoporosis, a trivial fall may result in acetabular fracture.\n\nIn 1964, French surgeons Robertt Judet, Jean Judet, and Emile Letournel first described the mechanism, classification, and treatment of acetabular fracture. They classified these fractures into elementary (simple two part) and associated (complex three or more part) fractures.\n\nTo understand the fracture pattern of a fractured acetabulum, it is essential to have minimum three x-ray views, though use of CT scan with 3-D reconstruction of images has made understanding of these fractures easier.\nTile's classification of acetabular fracture:\n\n\nIdeal x-ray visualization of an elementary fracture will depend on the fracture type:\nIn all cases, CT scan can assist in identifying impacted bone pieces, which may be found within the joint, and MRI may be done to identify the extent of potential injury to the sciatic nerve.\n\nThe broken bone pieces or the dislocated head of the femur may injure the sciatic nerve, causing paralysis of the foot; the patient may or may not recover sensation in the foot, depending on the extent of injury to the nerve. The posterior wall fragment may be one large piece, or multiple pieces, and may be associated with impaction of the bone. Sciatic nerve injury and stoppage of blood supply to femoral head at the time of accident or during surgery to treat may occur. Deep vein thrombosis and pulmonary embolism are other complications that may occur in any type of injury to the acetabulum.\n\nIf the femur head is dislocated, it should be reduced as soon as possible, to prevent damage to its blood supply. This is preferably done under anaesthesia, following which, leg is kept pulled by applying traction to prevent joint from dislocating.\n\nThe final management depends on the size of the fragment(s), stability and congruence of the joint. In some cases traction for six to eight weeks may be the only treatment required; however, surgical fixation using screw(s) and plate(s) may be required if the injury is more complex. The latter treatment will be called for if bone fragments do not fall into place, or if they are found in the joint, or if the joint itself is unstable.\n\nDepending on the stability achieved via initial treatment, the patient may be allowed to stand and walk with help of support within about six to eight weeks. Full function may return in about three months.\n\nAt the site of injury: After stabilizing an injured person and resuscitation, quick examination is done to check injury to vital organs.\n\nIf one suspects injury to the hip, it is imperative to immobilse the limb using some kind of support to prevent movements of the injured limb to prevent further damage\n\nA trained paramedic may be able to diagnose hip dislocation by noticing the position of the injured limb. It is essential to document status of nerves and vessels before starting any treatment to protect oneself from litigation\n\nOn arrival at the hospital, trained trauma surgeon will assess the patient and prescribe necessary tests including x-rays as described earlier.\n\nNon-surgical management consists of reducing the dislocated joint by maneuver under anaesthesia and applying traction to the limb to maintain position of joint and fractured bones. If non surgical management is preferred it may require six weeks to 3 months for recovery.\n\nThe surgical management requires high degree of training and well equipped centre. It should be carried out by experienced surgical team to get best results. The principles laid down for management are;\nInnominate bone is a flat bone with many curves. In most part the bone is thick enough and has broad surfaces that are amenable to primary fixation using lag screw(s) and to neutralize forces across the bone one needs to add plate(s) on the surface of the fractured fragments for it to heal without deformity.\n\nBefore surgery, patient needs tests to check fitness for surgery\n\nAnaesthesia : the surgery may be performed either under regional anaesthesia or general anaesthesia\n\nSurgical approaches. Following are the common approaches;\nImplants : normally lag screws and reconstruction plates are preferred implants\n\nPost operative management: would involve initial period or bed rest, followed by mobilsation by trained therapist\n\nTotal time to recover may be up to 3 months\n"}
{"id": "28913038", "url": "https://en.wikipedia.org/wiki?curid=28913038", "title": "Alice Glacier", "text": "Alice Glacier\n\nAlice Glacier () is a long tributary glacier in East Antarctica. It flows east from the Queen Alexandra Range to enter Beardmore Glacier at Sirohi Point. It was discovered by the British Antarctic Expedition, 1907–09, under Ernest Shackleton, and was named for the mother of Eric Marshall, a member of Shackleton's South Polar Party.\n\n"}
{"id": "2293952", "url": "https://en.wikipedia.org/wiki?curid=2293952", "title": "Ancient technology", "text": "Ancient technology\n\nDuring the growth of the ancient civilizations, ancient technology was the result from advances in engineering in ancient times. These advances in the history of technology stimulated societies to adopt new ways of living and governance.\n\nThis article includes the advances in technology and the development of several engineering arts in historic times before the Middle Ages, which began after the fall of the Western Roman Empire in AD 476, the death of Justinian I in the 6th century, the coming of Islam in the 7th century, or the rise of Charlemagne in the 8th century. For technologies developed in medieval societies, see Medieval technology and Inventions in medieval Islam.\n\nA significant number of inventions were developed in the Islamic world; a geopolitical region that has at times extended from al-Andalus and Africa in the west to the Indian subcontinent and Malay Archipelago in the east. Many of these inventions had direct implications for Fiqh related issues.\n\nThey were one of the first Bronze age people in the world. Early on they used copper, bronze and gold, and later they used iron. Palaces were decorated with hundreds of kilograms of these very expensive metals. Also, copper, bronze, and iron were used for armor as well as for different weapons such as swords, daggers, spears, and maces.\n\nAccording to the assyriologist Stephanie Dalley, the earliest pump was the Archimedes' screw, first used by Sennacherib, King of Assyria, for the water systems at the Hanging Gardens of Babylon and Nineveh in the 7th century BCE. This attribution, however, is refuted by the historian of ancient water-lifting devices Olseon in the same paper, who still credits, as well as most other scholars, Archimedes with the invention.\n\nPerhaps the most important advance made by the Mesopotamians was the invention of writing by the Sumerians. With the invention of writing came the first recorded laws called the Code of Hammurabi as well as the first major piece of literature called the Epic of Gilgamesh.\n\nAlthough archaeologists don't know for sure who invented the wheel, the oldest wheel discovered was found in Mesopotamia. It is likely the Sumer first used the wheel in making pottery in 3500BC and used it for their chariots in around 3200 BC.\n\nThe Mesopotamians used a sexagesimal number system with the base 60 (like we use base 10). They divided time up by 60s including a 60-second minute and a 60-minute hour, which we still use today. They also divided up the circle into 360 degrees. They had a wide knowledge of mathematics including addition, subtraction, multiplication, division, quadratic and cubic equations, and fractions. This was important in keeping track of records as well as in some of their large building projects. The Mesopotamians had formulas for figuring out the circumference and area for different geometric shapes like rectangles, circles, and triangles. Some evidence suggests that they even knew the Pythagorean Theorem long before Pythagoras wrote it down. They may have even discovered the number for pi in figuring the circumference of a circle.\n\nBabylonian astronomy was able to follow the movements of the stars, planets, and the Moon. Application of advanced math predicted the movements of several planets. By studying the phases of the Moon, the Mesopotamians created the first calendar. It had 12 lunar months and was the predecessor for both the Jewish and Greek calendars.\n\nBabylonian medicine used logic and recorded medical history to be able to diagnose and treat illnesses with various creams and pills. Mesopotamians had two kinds of medical practices, magical and physical. Unlike today they would use both on the same patient. They didn’t see it as one as good and one as bad.\n\nThe Mesopotamians made many technological discoveries. They were the first to use the potter's wheel to make better pottery, they used irrigation to get water to their crops, they used bronze metal (and later iron metal) to make strong tools and weapons, and used looms to weave cloth from wool.\n\nFor later medieval technologies developed in the Mesopotamian region, now known as Iraq, see Inventions in medieval Islam.\n\nThe Egyptians invented and used many simple machines, such as the ramp to aid construction processes. They were among the first to extract gold by large-scale mining using fire-setting, and the first recognisable map, the Turin papyrus shows the plan of one such mine in Nubia.\n\nEgyptian paper, made from papyrus, and pottery were mass-produced and exported throughout the Mediterranean basin. The wheel, however, did not arrive until foreign invaders introduced the chariot. They developed Mediterranean maritime technology including ships and lighthouses. Early construction techniques utilized by the Ancient Egyptians made use of bricks composed mainly of clay, sand, silt, and other minerals. These constructs would have been vital in flood control and irrigation, especially along the Nile delta.\n\nFor later technologies in Ptolemaic Egypt, Roman Egypt, and Arab Egypt, see Ancient Greek technology, Roman technology and Inventions in medieval Islam respectively.\n\nTechnology in Africa has a history stretching to the beginning of the human species, stretching back to the first evidence of tool use by hominid ancestors in the areas of Africa where humans are believed to have evolved. Africa saw the advent of some of the earliest ironworking technology in the Aïr Mountains region of what is today Niger and the erection of some of the world's oldest monuments, pyramids and towers in Egypt, Nubia, and North Africa. In Nubia and ancient Kush, glazed quartzite and building in brick was developed to a greater extent than in Egypt. Parts of the East African Swahili Coast saw the creation of the world's oldest carbon steel creation with high-temperature blast furnaces created by the Haya people of Tanzania.\n\nThe history of science and technology in India dates back to the earliest civilizations of the world. The Indus Valley civilization yields evidence of mathematics, hydrography, metrology, and sewage collection and disposal being practiced by its inhabitants.\n\nThe Indus Valley Civilization, situated in a resource-rich area, is notable for its early application of city planning and sanitation technologies. Cities in the Indus Valley offer some of the first examples of closed gutters, public baths, and communal granaries. The Takshashila University was an important seat of learning in the ancient world. It was the center of education for scholars from all over Asia. Many Greek, Persian and Chinese students studied here under great scholars including Kautilya, Panini, Jivaka, and Vishnu Sharma.\nThe ancient system of medicine in India, Ayurveda was a significant milestone in Indian history. It mainly uses herbs as medicines. Its origins can be traced back to origin of Atharvaveda. The Sushruta Samhita (400 BC) by Sushruta has details about performing cataract surgery, plastic surgery, etc.\n\nAncient India was also at the forefront of seafaring technology - a panel found at Mohenjo-daro, depicts a sailing craft. Ship construction is vividly described in the Yukti Kalpa Taru, an ancient Indian text on Shipbuilding. (The Yukti Kalpa Taru had been translated and published by Prof. Aufrecht in his 'Catalogue of Sanskrit Manuscripts').\n\nIndian construction and architecture, called 'Vaastu Shastra', suggests a thorough understanding or materials engineering, hydrology, and sanitation. Ancient Indian culture was also pioneering in its use of vegetable dyes, cultivating plants including indigo and cinnabar. Many of the dyes were used in art and sculpture. The use of perfumes demonstrates some knowledge of chemistry, particularly distillation and purification processes.\n\nThe history of science and technology in China shows significant advances in science, technology, mathematics, and astronomy. The first recorded observations of comets, solar eclipses, and supernovae were made in China. Traditional Chinese medicine, acupuncture and herbal medicine were also practiced. The Four Great Inventions of China: the compass, gunpowder, papermaking, and printing were among the most important technological advances, only known in Europe by the end of the Middle Ages.\n\nAccording to the Scottish researcher Joseph Needham, the Chinese made many first-known discoveries and developments. Major technological contributions from China include early seismological detectors, matches, paper, the double-action piston pump, cast iron, the iron plough, the multi-tube seed drill, the suspension bridge, natural gas as fuel, the magnetic compass, the raised-relief map, the propeller, the crossbow, the south-pointing chariot, and gunpowder. Other Chinese discoveries and inventions from the Medieval period, according to Joseph Needham's research, include: block printing and movable type, phosphorescent paint, and the spinning wheel.\n\nThe solid-fuel rocket was invented in China about 1150 AD, nearly 200 years after the invention of black powder (which acted as the rocket's fuel). At the same time that the age of exploration was occurring in the West, the Chinese emperors of the Ming Dynasty also sent ships, some reaching Africa. But the enterprises were not further funded, halting further exploration and development. When Ferdinand Magellan's ships reached Brunei in 1521, they found a wealthy city that had been fortified by Chinese engineers, and protected by a breakwater. Antonio Pigafetta noted that much of the technology of Brunei was equal to Western technology of the time. Also, there were more cannons in Brunei than on Magellan's ships, and the Chinese merchants to the Brunei court had sold them spectacles and porcelain, which were rarities in Europe.\n\nThe Qanat, a water management system used for irrigation, originated in Iran before the Achaemenid period of Persia. The oldest and largest known qanat is in the Iranian city of Gonabad which, after 2,700 years, still provides drinking and agricultural water to nearly 40,000 people.\n\nIn the 7th century AD, Persians in Afghanistan developed the first practical windmills. For later medieval technologies developed in Islamic Persia, see Inventions in medieval Islam.\n\nAncient Greek technology developed at an unprecedented speed during the 5th century BC, continuing up to and including the Roman period, and beyond. Some inventions that are credited to the ancient Greeks are the following: the gear, screw, bronze casting techniques, water clock, water organ(hydraulis), torsion siege engine, and the use of steam to operate some experimental machines and toys. Many of these inventions occurred late in the Greek period, often inspired by the need to improve weapons and tactics in war.\n\nGreek and Hellenistic engineers invented many technologies and improved upon pre-existing technologies, particularly during the Hellenistic period. Heron of Alexandria invented a basic steam engine and demonstrated knowledge of mechanic and pneumatic systems. Archimedes invented several machines. The Greeks were unique in pre-industrial times in their ability to combine scientific research with the development of new technologies. One example is the Archimedean screw; this technology was first conceptualized in mathematics, then built. Other technologies invented by Greek scientists include the ballistae, the piston pump, and primitive analog computers like the Antikythera mechanism. Greek architects were responsible for the first true domes, and were the first to explore the Golden ratio and its relationship with geometry and architecture.\n\nApart from Hero of Alexandria's steam aeolipile, Hellenistic technicians were the first to invent watermills and windwheels, making them global pioneers in three of the four known means of non-human propulsion prior to the Industrial Revolution (the fourth being sails). However, only water power was used extensively in antiquity.\n\nOther Greek inventions include torsion catapults, pneumatic catapults, crossbows, cranes, rutways, organs, the keyboard mechanism, gears, differential gears, screws, refined parchment, showers, dry docks, diving bells, odometer and astrolabes. In architecture, Greek engineers constructed monumental lighthouses such as the Pharos and devised the first central heating systems. The Tunnel of Eupalinos is the earliest tunnel in history which has been excavated with a scientific approach from both ends.\n\nAutomata like vending machines, automatic doors and many other ingenious devices were first built by Hellenistic engineers as Ctesibius, Philo of Byzantium and Heron. Greek technological treatises were scrupuously studied and copied by later Byzantine, Arabic and Latin European scholars and provided much of the foundation for further technological advances in these civilizations.\n\nRoman technology supported Roman civilization and made the expansion of Roman commerce and Roman military possible over nearly a thousand years. The Roman Empire had an advanced set of technology for their time. Some of the Roman technology in Europe may have been lost during the turbulent eras of Late Antiquity and the Early Middle Ages. Roman technological feats in many different areas like: civil engineering, construction materials, transport technology, and some inventions such as the mechanical reaper went unmatched until the 19th century.\nRomans developed an intensive and sophisticated agriculture, expanded upon existing iron working technology, created laws providing for individual ownership, advanced stonemasonry technology, advanced road-building (exceeded only in the 19th century), military engineering, civil engineering, spinning and weaving and several different machines like the Gallic reaper that helped to increase productivity in many sectors of the Roman economy. They also developed water power through building aqueducts on a grand scale, using water not just for drinking supplies but also for irrigation, powering water mills and in mining. They used drainage wheels extensively in deep underground mines, one device being the reverse overshot water-wheel. They were the first to apply hydraulic mining methods for prospecting for metal ores, and for extracting those ores from the ground when found using a method known as hushing.\n\nRoman engineers build monumental arches, amphitheatres, aqueducts, public baths, true arch bridges, harbours, dams, vaults and domes on a very large scale across their Empire. Notable Roman inventions include the book (Codex), glass blowing and concrete. Because Rome was located on a volcanic peninsula, with sand which contained suitable crystalline grains, the concrete which the Romans formulated was especially durable. Some of their buildings have lasted 2000 years, to the present day. Roman society had also carried over the design of a door lock with tumblers and springs from Greece. Like many other aspects of innovation and culture that were carried on from Greece to Rome, the lines between where each one originated from have become skewed over time. These mechanisms were highly sophisticated and intricate for the era.\n\nRoman civilization was highly urbanized by pre-modern standards. Many cities of the Roman Empire had over 100,000 inhabitants with the capital Rome being the largest metropolis of antiquity. Features of Roman urban life included multistory apartment buildings called insulae, street paving, public flush toilets, glass windows and floor and wall heating. The Romans understood hydraulics and constructed fountains and waterworks, particularly aqueducts, which were the hallmark of their civilization. They exploited water power by building water mills, sometimes in series, such as the sequence found at Barbegal in southern France and suspected on the Janiculum in Rome. Some Roman baths have lasted to this day. The Romans developed many technologies which were apparently lost in the Middle Ages, and were only fully reinvented in the 19th and 20th centuries. They also left texts describing their achievements, especially Pliny the Elder, Frontinus and Vitruvius.\n\nOther less known Roman innovations include cement, boat mills, arch dams and possibly tide mills.\n\nLacking suitable beasts of burden and inhabiting domains often too mountainous or boggy for wheeled transport, the ancient civilizations of the Americas did not develop wheeled transport or the mechanics associated with animal power. Nevertheless, they produced advanced engineering including above ground and underground acqeducts, quake-proof masonry, artificial lakes, dykes, 'fountains,' pressurized water, road ways and complex terracing. Equally, gold-working commenced early in Peru (2000 BCE), and eventually copper, tin, lead and bronze were used. Although metallurgy did not spread to Mesoamerica until the Middle Ages, it was employed here and in the Andes for sophisticated alloys and gilding. The Native Americans developed a complex understanding of the chemical properties or utility of natural substances, with the result that a majority of the world's early medicinal drugs and edible crops, many important adhesives, paints, fibres, plasters, and other useful items were the products of these civilizations. Perhaps the best-known Mesoamerican invention was rubber, which was used to create rubber bands, rubber bindings, balls, syringes, 'raincoats,' boots, and waterproof insulation on containers and flasks.\n\n\n"}
{"id": "45317477", "url": "https://en.wikipedia.org/wiki?curid=45317477", "title": "Asteroid Zoo", "text": "Asteroid Zoo\n\nAsteroid Zoo is a citizen science project run by the Zooniverse and Planetary Resources, to use volunteer classifications to find unknown asteroids using old Catalina Sky Survey data. The main goals of the project are to search for undiscovered asteroids in order to protect the planet by locating potentially harmful near-Earth Asteroids, locate targets for future asteroid mining, study the solar system, and study the potential uses and advantages of people looking through the images over computers. It was created along with the ARKYD project through Kickstarter, funded with just over 1.5 million dollars.\n\nThe AsteroidZoo community has exhausted the data that were available. With all the data examined they paused the experiment. Asteroid Zoo produced several scientific publications.\n\nZooniverse projects:\n"}
{"id": "35920227", "url": "https://en.wikipedia.org/wiki?curid=35920227", "title": "Aviation engineering", "text": "Aviation engineering\n\nAviation engineering is the science of designing, developing, and assembling aircraft. Aviation engineers focus on airspace development, airport design, aircraft navigation technologies, and aerodrome planning. Professionals working in this field experiment with innovations in technology and materials, test planes, simulate flight conditions, and more.\n\nAs a branch of law, it is art for it involves formulation of public policy, regulations, aviation laws pertaining to airspace, airlines, airports, aerodromes and the conduct of air services agreements through treaty.\n\nThis branch of engineering is distinct to aerospace engineering which deals with the science of flying craft and spacecraft.\n\nThe global airspace is divided into territorial airspace which then belongs to a country. Generally, airspace has to be engineered to benefit both military and civil users. Planning and designing airspace is important so as not to affect military operations and in order to designate air routes for commercial airlines to navigate freely without intervention by military authorities. For instance, not all of China can be used for commercial aeronautical navigation. Certain airspaces are designated as military-use only. Navigating outside commercial airspaces in the country may lead to risk of the astrayed aircraft.\n\nIn prior years, airspace has been limited to military and air mail services. The advancement in aerospace engineering brought to fore aircraft designs that lead to the development of commercial airliners. Governments around the world concluded air rights for their respective airlines and their corresponding aircraft. Government saw the economic potential of airspace as a state enterprise. This rise in the development of commercial aviation led to the study of the complexities of aircraft (plane) management, airport design and construction, international air services agreements (treaties).\n\nIn recent years, the global airline industry has demanded that China should reform its aviation policies.\n\nRecent designs of airports have been engineered to align with global environmental standards. Advancement in civil engineering and architecture make an interplay of the two disciplines.\n\nGovernments around the world hire aviation engineers for all sorts of reasons. In the United States, federal, state and local agencies all commonly employ aviation engineers for agencies such as the Department of Transportation. The Federal Aviation Administration, which is in charge of controlling aircraft navigation throughout the whole nation, maintaining navigation, licensing and certification for aviation engineers. FAA employs many aviation engineers to work on research and development problems, noise pollution and hypersonic aircraft among other things. \n\nEngineers are heavily involved in improving aviation technologies to support the advancement of military and commercial aviation. Aviation engineers are often employed in aerospace machine shops specializing in the technological advancement of aircraft parts and equipment. These shops produce aircraft components such as electrical connectors, oxygen generation systems, landing gear assemblies, and other pieces that require special attention.\n\nAviation engineers do more than work on planes. Aviation engineers also play a large role in airport design. They provide guidance for the construction and daily running of the airports, as well as help in the operation and maintenance.\n"}
{"id": "24493142", "url": "https://en.wikipedia.org/wiki?curid=24493142", "title": "Babak Amin Tafreshi", "text": "Babak Amin Tafreshi\n\nBabak Amin Tafreshi (born in 1978 in Tehran, Iran) is an Iranian photographer, science journalist, and amateur astronomer. He is the creator and director of The World At Night (TWAN), an international program in which photographers from around the world capture images of night skies as seen above notable landmarks of the planet. He is also a member of the board of advisors of Astronomers Without Borders and a project coordinator for the International Year of Astronomy (IYA2009).\n\nIn 2009, he won the Lennart Nilsson Award for best scientific photography, in joint effort with NASA's Cassini Imaging Director Carolyn Porco.\n"}
{"id": "2181792", "url": "https://en.wikipedia.org/wiki?curid=2181792", "title": "Bicameralism (psychology)", "text": "Bicameralism (psychology)\n\nBicameralism (the condition of being divided into \"two-chambers\") is a radical hypothesis in psychology that argues that the human mind once operated in a state in which cognitive functions were divided between one part of the brain which appears to be \"speaking\", and a second part which listens and obeys — a \"bicameral mind\". The term was coined by Julian Jaynes, who presented the idea in his 1976 book \"The Origin of Consciousness in the Breakdown of the Bicameral Mind\", wherein he made the case that a bicameral mentality was the normal and ubiquitous state of the human mind as recently as 3,000 years ago, near the end of the Mediterranean bronze age.\n\nJaynes uses governmental bicameralism as a metaphor to describe a mental state in which the experiences and memories of the right hemisphere of the brain are transmitted to the left hemisphere via auditory hallucinations. The metaphor is based on the idea of lateralization of brain function although each half of a normal human brain is constantly communicating with the other through the \"corpus callosum\". The metaphor is not meant to imply that the two halves of the bicameral brain were \"cut off\" from each other but that the bicameral mind was experienced as a different, non-conscious mental schema wherein volition in the face of novel stimuli was mediated through a linguistic control mechanism and experienced as auditory verbal hallucination.\n\nBicameral mentality would be non-conscious in its inability to reason and articulate about mental contents through meta-reflection, reacting without explicitly realizing and without the meta-reflective ability to give an account of \"why\" one did so. The bicameral mind would thus lack metaconsciousness, autobiographical memory, and the capacity for executive \"ego functions\" such as deliberate mind-wandering and conscious introspection of mental content. When bicamerality as a method of social control was no longer adaptive in complex civilizations, this mental model was replaced by the conscious mode of thought which, Jaynes argued, is grounded in the acquisition of metaphorical language learned by exposure to narrative practice.\nAccording to Jaynes, ancient people in the bicameral state of mind would have experienced the world in a manner that has some similarities to that of a schizophrenic. Rather than making conscious evaluations in novel or unexpected situations, the person would hallucinate a voice or \"god\" giving admonitory advice or commands and obey without question: One would not be at all conscious of one's own thought processes \"per se\". Jaynes's hypothesis is offered as a possible explanation of \"command hallucinations\" that often direct the behavior of those labeled schizophrenic, as well as other voice hearers.\n\nJaynes built a case for this hypothesis that human brains existed in a bicameral state until as recently as 3,000 years ago by citing evidence from many diverse sources including historical literature. He took an interdisciplinary approach, drawing data from many different fields. Jaynes asserted that, until roughly the times written about in Homer's \"Iliad\", humans did not generally have the self-awareness characteristic of consciousness as most people experience it today. Rather, the bicameral individual was guided by mental commands believed to be issued by external \"gods\" — commands which were recorded in ancient myths, legends and historical accounts. This is exemplified not only in the commands given to characters in ancient epics but also the very muses of Greek mythology which \"sang\" the poems. According to Jaynes, the ancients literally heard muses as the direct source of their music and poetry.\n\nJaynes asserts that in the \"Iliad\" and sections of the Old Testament no mention is made of any kind of cognitive processes such as introspection, and there is no apparent indication that the writers were self-aware. Jaynes suggests, the older portions of the Old Testament (such as the Book of Amos) have few or none of the features of some later books of the Old Testament (such as Ecclesiastes) as well as later works such as Homer's \"Odyssey\", which show indications of a profoundly different kind of mentality — an early form of consciousness.\n\nIn ancient times, Jaynes noted, gods were generally much more numerous and much more anthropomorphic than in modern times, and speculates that this was because each bicameral person had their own \"god\" who reflected their own desires and experiences.\n\nHe also noted that in ancient societies the corpses of the dead were often treated as though still alive (being seated, dressed, and even fed) as a form of ancestor worship, and Jaynes argued that the dead bodies were presumed to be still living and the source of auditory hallucinations. This adaptation to the village communities of 100 individuals or more formed the core of religion. Unlike today's hallucinations, the voices of ancient times were structured by cultural norms to produce a seamlessly functioning society.\n\nJaynes inferred that these \"voices\" came from the right brain counterparts of the left brain language centres; specifically, the counterparts to Wernicke's area and Broca's area. These regions are somewhat dormant in the right brains of most modern humans, but Jaynes noted that some studies show that auditory hallucinations correspond to increased activity in these areas of the brain.\n\nJaynes notes that even at the time of publication there is no consensus as to the cause or origins of schizophrenia. Jaynes argues that schizophrenia is a vestige of humanity's earlier bicameral state. Recent evidence shows that many schizophrenics do not just hear random voices but experience \"command hallucinations\" instructing their behavior or urging them to commit certain acts.\n\nAs support for Jaynes's argument, these command hallucinations are little different from the commands from gods which feature prominently in ancient stories. Indirect evidence supporting Jaynes's theory that hallucinations once played an important role in human mentality can be found in the recent book \"Muses, Madmen, and Prophets: Rethinking the History, Science, and Meaning of Auditory Hallucination\" by Daniel Smith.\n\nJaynes theorized that a shift from bicameralism marked the beginning of introspection and consciousness as we know it today. According to Jaynes, this bicameral mentality began malfunctioning or \"breaking down\" during the 2nd millennium BCE. He speculates that primitive ancient societies tended to collapse periodically: for example, Egypt's Intermediate Periods, as well as the periodically vanishing cities of the Mayas, as changes in the environment strained the socio-cultural equilibria sustained by this bicameral mindset.\n\nThe Bronze age collapse of the 2nd millennium BCE led to mass migrations and created a rash of unexpected situations and stresses which required ancient minds to become more flexible and creative. Self-awareness, or consciousness, was the culturally evolved solution to this problem. This necessity of communicating commonly observed phenomena among individuals who shared no common language or cultural upbringing encouraged those communities to become self-aware to survive in a new environment. Thus consciousness, like bicamerality, emerged as a neurological adaptation to social complexity in a changing world.\n\nJaynes further argues that divination, prayer, and oracles arose during this breakdown period, in an attempt to summon instructions from the \"gods\" whose voices could no longer be heard. The consultation of special bicamerally operative individuals, or of divination by casting lots and so forth, was a response to this loss, a transitional era depicted, for example, in the book of 1 Samuel. It was also evidenced in children who could communicate with the gods, but as their neurology was set by language and society they gradually lost that ability. Those who continued prophesying, being bicameral according to Jaynes, could be killed. Leftovers of the bicameral mind today, according to Jaynes, include religion, hypnosis, possession, schizophrenia, and the general sense of need for external authority in decision-making.\n\nAn early (1977) reviewer considered Jaynes's hypothesis worthy and offered conditional support, arguing the notion deserves further study.\n\n\"The Origin of Consciousness\" was financially successful, and has been reprinted several times.\n\nOriginally published in 1976 it was nominated for the National Book Award in 1978. It has been translated into Italian, Spanish, German, French, and Persian. A new edition, with an afterword that addressed some criticisms and restated the main themes, was published in the US in 1990 and in the UK (by Penguin Books) in 1993, re-issued in 2000.\n\nPhilip K. Dick, Terrence McKenna, and David Bowie all cited the book as an influence. \n\nJaynes's hypothesis remains controversial. The primary scientific criticism has been that the conclusions drawn by Jaynes had no basis in neuropsychiatric fact.\n\nAccording to Jaynes, language is a necessary, but not a sufficient condition for consciousness: Language existed thousands of years earlier, but consciousness could not have emerged without language. The idea that language is a necessary component of subjective consciousness and more abstract forms of thinking has gained the support of proponents including Andy Clark, Daniel Dennett, William H. Calvin, Merlin Donald, John Limber, Howard Margolis, Peter Carruthers, and José Luis Bermúdez.\n\nWilliams (2010) defended Jaynes against the criticism of Block (1981).\n\nAsaad & Shapiro (1987) questioned why Jaynes's theory was left out of a discussion on auditory hallucinations. The author's (1987) published response was: \"... Jaynes’ hypothesis makes for interesting reading and stimulates much thought in the receptive reader. It does not, however, adequately explain one of the central mysteries of madness: hallucination.\" (Moffic, 1987) However Moffic's claim that there is no evidence for involvement of the right temporal lobe in auditory hallucination was incorrect, even at the time that he wrote it. More recently, a number of studies have provided more evidence of involvement of the right hemisphere in auditory hallucinations. The new evidence for Jaynes's model of auditory hallucinations arising in the right temporal-parietal lobe and being transmitted to the left temporal-parietal lobe that these neuroimaging studies provide was specifically pointed out by Olin (1999) and by Sher (2000). For further discussion, see Marcel Kuijsten (2007).\n\nBrian J. McVeigh, a graduate student of Jaynes, maintains that many of the most frequent criticisms of Jaynes' theory are either incorrect or reflect serious misunderstandings of Jaynes' theory, especially Jaynes' more precise definition of consciousness. Jaynes defines consciousness — in the tradition of Locke and Descartes — as \"\"that which is introspectable\". Jaynes draws a sharp distinction between consciousness (\"introspectable mind-space\"\") and other mental processes such as cognition, learning, sensation, and perception. McVeigh argues that this distinction is frequently not recognized by those offering critiques of Jaynes' theory.\n\nRichard Dawkins in \"The God Delusion\" (2006) wrote of \"The Origin of Consciousness in the Breakdown of the Bicameral Mind\":It is one of those books that is either complete rubbish or a work of consummate genius; Nothing in between! Probably the former, but I'm hedging my bets.\n\nThe philosopher Daniel Dennett suggested that Jaynes may have been wrong about some of his supporting arguments – especially the importance he attached to hallucinations – but that these things are not essential to his main thesis:\nIf we are going to use this top-down approach, we are going to have to be bold. We are going to have to be speculative, but there is good and bad speculation, and this is not an unparalleled activity in science. ... Those scientists who have no taste for this sort of speculative enterprise will just have to stay in the trenches and do without it, while the rest of us risk embarrassing mistakes and have a lot of fun. — Daniel Dennett\n\nGregory Cochran, a physicist and adjunct professor of anthropology at the University of Utah, wrote:\"Genes affecting personality, reproductive strategies, cognition, are all able to change significantly over few-millennia time scales if the environment favors such change — and this includes the new environments we have made for ourselves, things like new ways of making a living and new social structures. ... There is evidence that such change has occurred. ... On first reading, \"Breakdown\" seemed one of the craziest books ever written, but Jaynes may have been on to something.\"\n\nAuthor and historian of science Morris Berman writes: \"\"[Jaynes's] description of this new consciousness is one of the best I have come across.\"\n\nDanish science writer Tor Nørretranders discusses Jaynes's theory favorably in his 1991 book.\n\nIain McGilchrist who published a similar idea, accepts Jayne's intention, but proposes that Jayne's hypothesis was the opposite of what happened:I believe he [Jayne] got one important aspect of the story back to front. His contention that the phenomena he describes came about because of a \"breakdown\" of the 'bicameral mind' – so that the two hemispheres, previously separate, now merged – is the precise inverse of what happened.\n\nAs an argument against Jaynes' proposed date of the transition from bicameralism to consciousness some critics have referred to the Epic of Gilgamesh. Early copies of the epic are many centuries older than even the oldest passages of the Old Testament, and yet it describes introspection and other mental processes that, according to Jaynes, were impossible for the bicameral mind.\n\nJaynes noted that the most complete version of the Gilgamesh epic dates to post-bicameral times (7th century BCE), dismisses these instances of introspection as the result of rewriting and expansion by later conscious scribes, and points to differences between the more recent version of \"Gilgamesh\" and surviving fragments of earlier versions: \"The most interesting comparison is in Tablet X.\"\" His answer, however, does not deal with the generally accepted dating of the \"Standard Version\" of the Gilgamesh epic to the later 2nd millennium BCE, nor does it account for the introspection that so often taken as characteristic of the \"Standard Version\" being thoroughly rooted in the Old Babylonian and Sumerian versions, especially so as our understanding of the Old Babylonian poem improves.\n\nJulian Jaynes' study is mostly based on the writings and culture of the Mediterranean and Near-Eastern regions, although he occasionally also refers to ancient writings of India and China.\n\nJaynes' proposal does not explain how such bicameralism could also have been near totally lost at the same time across the whole planet and in the entire human species. In particular the aborigine culture was completely separated from the rest of the world from 4000 BCE–1600 CE and appears today to be historically unchanged, but also self-conscious.\n\nDivination is also considerably older than that date and the early writings he claims show bicamerality: The oldest recorded Chinese Writing was on oracle bones, meaning that divination arose at the same time or even earlier than writing in Chinese society.\n\nWhile he said ancient societies engaged in ancestor worship before this date, non-ancient societies also engaged in it after that date; very advanced societies like the Aztecs and Egyptians mummified and deified rulers (see Pyramids and the philosopher Nezahualcoyotl). The Aztecs and Incans did so all the way up to their conquest by the Spanish.\n\nIt is now known that the sense of agency is closely connected with lateralization: The left parietal lobe is active when visualizing actions by the self, while the right parietal lobe is active for actions by others. Additionally, Wernicke's area processes the literal meaning of language, while the homologous region in the right hemisphere processes the intent of a speaker.\n\nIt has been found that people with damage to the right inferior parietal cortex experience alien hand syndrome, as do people who have had a corpus callosotomy. This reverses the relationship between the right and left hemispheres posited by Jaynes' proposed bicameralism: It is the left hemisphere that is responsible for speech and the right hemisphere that is responsible for self-awareness.\n\nVS Ramachandran, in his 2003 book \"The Emerging Mind\", proposes a similar concept, referring to the left cortical hemisphere as an \"\"apologist\", and the right cortical hemisphere as a \"revolutionary\"\".\n\nIn his book \"Neuroreality: A Scientifice Religion to Restore Meaning,or How 7 Brain Elements Create 7 Minds and 7 Realities\", Bruce E. Morton, formerly the University of Hawaii, similarly proposed such a concept.\n\nPsychiatrist Iain McGilchrist reviews scientific research into the role of the brain's hemispheres, and cultural evidence, in his book \"The Master and His Emissary\". Similar to Jaynes, McGilchrist proposes that since the time of Plato the left hemisphere of the brain (the \"emissary\" in the title) has increasingly taken over from the right hemisphere (the \"master\"), to our \"detriment\". McGilchrist, while accepting Jayne's intention, felt that Jayne's hypothesis was \"the precise inverse of what happened\" and that rather than a shift \"from\" bicameralism there evolved a separation of the hemispheres \"to\" bicameralism.(See McGilchrist quotation, above.)\n\nMichael Gazzaniga pioneered the split-brain experiments which led him to propose a similar theory called the left brain interpreter.\n\nNeuroscientist Michael Persinger, who co-invented the “God helmet” in the 1980s, believes that his invention may induce mystical experiences by having the separate right hemisphere consciousness intrude into the awareness of the normally-dominant left hemisphere.\n\n\n\n\n\n\nThe Julian Jaynes Society was founded by supporters of bicameralism in 1997, shortly after Jaynes' death. The society published a collection of essays on bicameralism in 2007, with contributors including psychological anthropologist Brian J. McVeigh, psychologists John Limber and Scott Greer, clinical psychologist John Hamilton, philosophers Jan Sleutels and David Stove, and sinologist Michael Carr (see \"shi\" “personator”). The book also contains an extensive biography of Julian Jaynes by historian of psychology William Woodward and June Tower, and a foreword by neuroscientist Michael Persinger.\n"}
{"id": "29323400", "url": "https://en.wikipedia.org/wiki?curid=29323400", "title": "Blodgett Iceberg Tongue", "text": "Blodgett Iceberg Tongue\n\nBlodgett Iceberg Tongue () is a large iceberg tongue that extends seaward from the vicinity of Cape Morse and Cape Carr on the east side of Porpoise Bay. It was named by the Advisory Committee on Antarctic Names for Gardner D. Blodgett, Office of Geography, Department of Interior, who, in 1955, prepared a sketch map of the coastal features of Antarctica between 84°E and 144°E from U.S. Navy Operation Highjump (1946–47) aerial photographs. Since the iceberg tongue was partially delineated for the first time on the 1955 sketch map by Blodgett, use of his name for it is considered appropriate.\n\n"}
{"id": "25128072", "url": "https://en.wikipedia.org/wiki?curid=25128072", "title": "Bombay Before the British: the Indo-portuguese layer.", "text": "Bombay Before the British: the Indo-portuguese layer.\n\nThe funding for the project began in September 2004 and ended in December 2007, when a final report was submitted containing the project's main conclusions. This report received the highest mark by FCT's evaluating panel. Both senior investigators of the \"BBB\", Dr. Walter Rossa and Dr. Paulo Varela Gomes, were and are professors at the Architecture Department, Faculty of Sciences and Technology, University of Coimbra (Portugal).\n\nThe project's main object of study was the territory of Greater Mumbai and its surrounding hinterland during the time of Portuguese colonial rule (1534–1739); during that period, the territory was known as the Baçaim (Vasai) District of the Northern Province of Portuguese India.\n\nIn 1997, Dr. Walter Rossa's book \"Indo-portuguese Cities\" was published by the Portuguese Discoveries Celebrations Committee (CNCDP). The following year, Dr. Rossa coordinated the topographical survey of the ruins of the Baçaim (Vasai). During this period (1996–1998), Dr. Paulo Varela Gomes was the delegate of the Oriente Foundation's branch in India (Goa). In 2000, both of them travelled to the area of Greater Mumbai, together with the second-year students of the Department of Architecture, University of Coimbra, visiting many of the historical sites pertaining to the territory's Indo-Portuguese layer. The research possibilities developed during this trip and the recent scholarly activity of both Rossa and Gomes led up to the proposal for a three-year scientific research project - the \"BBB\"\n\nThe Portuguese presence in the Baçaim (Vasai) District of the Northern Province between 1534 and 1739 - which included the present day region of Greater Mumbai - has left considerable traces on the present-day territory. These include not only ruins of military and religious structures but also structures founded by the Portuguese and later modified or rebuilt by other administrations or communities. Most of the Churches that cater to the Catholic population of Greater Mumbai today - the East Indians - were founded by Portuguese missionaries. Significant information exists within sixteenth and seventeenth century Portuguese sources to prove that Catholic Communities were inexistent in the Northern Province area at the time of Portuguese occupation in 1534 (although it is very probable that some Christian communities developed in the area during the 7th and 8th centuries A.D., having become defunct by the time of the Portuguese arrival). \nBeside the network of forts and churches, the Portuguese Northern Province also presented a network of villages that developed under colonial rule. Most of these villages were leased to Portuguese or Indo-Portuguese landed gentry. Especially in Salsette Island, this network of villages was still in place during the first decades of the 20th century and would later become the most important territorial matrix for the development of Suburban Mumbai. Indeed, the two main railway lines built during the 19th century in Salsette followed closely a pre-existing road pattern (from Kurla to Thane and from Bandra to Bhyandar) that crossed many East-Indian villages, with their distinct landscape, architecture and churches. It was only during the 1960s and 1970s that Mumbai's exponential growth submerged and, in many cases, erased this network of villages. A few survive along the northwestern coast of Salsette Island, with their unique East Indian communities and traditions.\n\nThe BBB's methodology included two main tasks: data gathering; and its interpretation through appropriate tools, namely GIS software.\nData gathered during the project's timeline included various documents from libraries and archives in Portugal, India and the United Kingdom and also material gathered first-hand directly at the sites, through the project's field missions. This data was then stored in a Database and connected to a GIS interactive map. The GIS map used satellite imagery acquired through the European Space Agency. Over the layer of satellite imagery was added a number of layers of vectorial information, using the symbology of present-day cartography. The GIS map's interface allowed for each symbol to be connected to a number of documents stored in the Database. This methodology allowed the team to confirm the matricial importance on the East-Indian village network upon the suburban growth of Greater Mumbai all along the 20th century, among other things\n\nThe original BBB team consisted of:\nDr. Walter Rossa (Investigator Responsible), Architecture Department, University of Coimbra; \nDr. Paulo Varela Gomes (Senior Investigator), Architecture Department, University of Coimbra; \nAndré Teixeira (Investigator), Centre for Overseas History, New University of Lisbon; \nSilvana Pires (Investigator), Centre for Overseas History, New University of Lisbon\n\nOther members:\nSidh Losa Mendiratta (Investigator), Architecture Department, University of Coimbra;\nMafalda Trinca (Scholarship holder), Architecture Department, University of Coimbra; Susana Freiria (Scholarship holder), Architecture Department, University of Coimbra; Cláudia Costa (Scholarship holder), Architecture Department, University of Coimbra; Isabel Almeida (Scholarship holder), Centre for Overseas History, New University of Lisbon; Pedro Nobre (Scholarship holder), Centre for Overseas History, New University of Lisbon.\n\nTapping into the considerable amount of resources and new lines of research created by the ongoing \"BBB\", two PhD dissertations and one Master dissertation were started by the following team members:\n\nAndré Teixeira, \"Baçaim e o seu território: administração, economia e sociedade, séculos XVI a XVIII\", tese de doutoramento em História, especialidade de História dos Descobrimentos e da Expansão Portuguesa à Faculdade de Ciências Sociais e Humanas da Universidade Nova de Lisboa, orientada pelo Prof. Doutor João Oliveira e Costa (2010).\n\nSidh Mendiratta, \"Dispositivos do Sistema Defensivo da Província do Norte do Estado da Índia, 1521-1739,\" tese de Doutoramento em História da Arquitectura ao Departamento de Arquitectura da Faculdade de Ciências e Tecnologia da Universidade de Coimbra, orientada pelo Prof. Doutor Walter Rossa e Prof. Doutor Paulo Varela Gomes, 2012.\n\nPedro Nobre, \"A Entrega de Bombaim ao Reino Unido (1661-1668) – um processo político-diplomático\", dissertação de Mestrado em História dos Descobrimentos e da Expansão Portuguesa à Faculdade de Ciências Sociais e Humanas da Universidade Nova de Lisboa, 2007.\n\nAlthough not officially included as part of the BBB's resulting scientific output, the following two Masters dissertations were accomplished in close association with the project's senior investigators:\n\nNuno Grancho, \"Bombaim: A Explosão Urbana Análise de Assentamentos e Vias\", dissertação de mestrado em Planeamento e Projecto do Ambiente Urbano apresentada à Universidade do Porto. 2008, orientada pelo Prof. Doutor Paulo Varela Gomes, 2008.\n\nTania Teixeira, \"Bombaim Meri Jaan: a vida, a cidade e a arquitectura em Bombaim\", Prova Final de Licenciatura do Departamento de Arquitectura da Faculdade de Ciências e Tecnologia da Universidade de Coimbra, orientada pelo Prof. Doutor Paulo Varela Gomes, 2009.\n\nAlso, a post-doctoral six-year research project is currently underway in close association with the project's senior investigators:\n\nSidh Mendiratta, \"Framing Identity: cityscapes and architecture of Mumbai’s catholic communities (16th to the 20th century)\", post-doctoral project based in Centro de Estudos Sociais da Universidade de Coimbra, with the advisers: Prof. Doctor Walter Rossa, Prof. Doctor Rochelle Pinto, and Prof. Doctor Vikas Dilawari.\n\n\n"}
{"id": "36216090", "url": "https://en.wikipedia.org/wiki?curid=36216090", "title": "Conversion electron Mössbauer spectroscopy", "text": "Conversion electron Mössbauer spectroscopy\n\nConversion electron Mössbauer spectroscopy (CEMS) is a Mössbauer spectroscopy technique based on conversion electron.\n\nThe CEM spectrum can be obtained either by collecting essentially all the electrons leaving the surface (integral technique), or by selecting the ones in a given energy range by means of a beta ray spectrometer (differential or depth selective CEMS).\n\nThis method allows the use of simple and inexpensive detecting equipment, mainly flow-type proportional detectors in which large counting rates can be obtained. This last characteristic makes possible the study of samples with the natural abundance of the Mössbauer isotope. The information furnished by the integral measurements can be increased by using various angles of incidence or by depositing thin layers of inert material on the sample.\n\nIn the energy range used in CEMS, the incident radiation can interact with the absorber through two kinds of processes: (a) conventional interactions – photoelectric and Compton effects, and (b) nuclear resonant absorption – Mössbauer effect. Due to conventional interactions the beam is attenuated and electrons are emitted from the sample. The nuclear de-excitation following the resonant absorption takes place by emission of either a gamma ray or an internal conversion (IC) electron. In the latter case, the atom is left in an ‘excited’ state with a hole in an inner shell; the energy excess is given away with emission of Auger electrons and/or X-rays. Thus, the electrons emitted from the sample as a consequence of the Mössbauer absorptions are: (a) primary (IC or Auger) electrons originated in the de-excitations of the nuclei excited by the incident beam, and (b) secondary electrons originated by conventional interactions of photons (or resonant absorption of gamma rays) emitted after resonant absorptions.\n\n"}
{"id": "766519", "url": "https://en.wikipedia.org/wiki?curid=766519", "title": "Dickin Medal", "text": "Dickin Medal\n\nThe PDSA Dickin Medal was instituted in 1943 in the United Kingdom by Maria Dickin to honour the work of animals in World War II. It is a bronze medallion, bearing the words \"For Gallantry\" and \"We Also Serve\" within a laurel wreath, carried on a ribbon of striped green, dark brown, and pale blue. It is awarded to animals that have displayed \"conspicuous gallantry or devotion to duty while serving or associated with any branch of the Armed Forces or Civil Defence Units\". The award is commonly referred to as \"the animals' Victoria Cross\" (although the Victoria Cross Trust has opposed this association).\n\nMaria Dickin was the founder of the People's Dispensary for Sick Animals (PDSA), a British veterinary charity. She established the award for any animal displaying conspicuous gallantry and devotion to duty whilst serving with British Empire armed forces or civil emergency services. The medal was awarded 54 times between 1943 and 1949 – to 32 pigeons, 18 dogs, 3 horses, and a ship's cat – to acknowledge actions of gallantry or devotion during the Second World War and subsequent conflicts. The awarding of the medal was revived in 2000 to honour Gander, a Newfoundland dog, who saved infantrymen during the Battle of Lye Mun. In early 2002, the medal was given in honour of three dogs for their role responding to the September 11 attacks; it was also awarded to two dogs serving with Commonwealth forces in Bosnia-Herzegovina and Iraq. In December 2007, 12 former recipients buried at the PDSA Animal Cemetery in Ilford, Essex, were afforded full military honours at the conclusion of a National Lottery-aided project to restore the cemetery.\n\nThe first recipients of the award, in December 1943, were three pigeons, serving with the Royal Air Force, all of whom contributed to the recovery of air crew from ditched aircraft during the Second World War. The most recent animal to be cited for the honour is Kuga, a Belgian Malinois who served with the Special Air Service Regiment in Afghanistan in 2012.\n\nAs of October 2018, the Dickin Medal has been awarded 70 times, plus one honorary award made in 2014 to all the animals that served in the First World War.\n\n\n\n"}
{"id": "21607296", "url": "https://en.wikipedia.org/wiki?curid=21607296", "title": "Discretization of continuous features", "text": "Discretization of continuous features\n\nIn statistics and machine learning, discretization refers to the process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals. This can be useful when creating probability mass functions – formally, in density estimation. It is a form of discretization in general and also of binning, as in making a histogram. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand.\n\nTypically data is discretized into partitions of \"K\" equal lengths/width (equal intervals) or K% of the total data (equal frequencies).\n\nMechanisms for discretizing continuous data include Fayyad & Irani's MDL method, which uses mutual information to recursively define the best bins, CAIM, CACC, Ameva, and many others\n\nMany machine learning algorithms are known to produce better models by discretizing continuous attributes.\n\nThis is a partial list of software that implement MDL algorithm.\n\n"}
{"id": "45239121", "url": "https://en.wikipedia.org/wiki?curid=45239121", "title": "Dunathan stereoelectronic hypothesis", "text": "Dunathan stereoelectronic hypothesis\n\nDunathan stereoelectronic hypothesis is a concept in chemistry to explain the stereospecefic cleavage of bonds using pyridoxal phosphate. This occurs because stereoelectronic effects controls the actions of the enzyme.\n\nBefore the correlation between fold type and reaction correlation of proteins were understood, Harmon C. Dunathan, a chemist at Haverford College proposed that the bond that is cleaved using pyridoxal is perpendicular to the system. Though an important concept in bioorganic chemistry, it is now known that enzyme conformations play a critical role in the final chemical reaction.\n\nThe transition state is stabilized by the extended pi bond network (formation of anion). Furthermore hyperconjugation caused by the extended network draws electrons from the bond to be cleaved, thus weakening the chemical bond and making it labile The sigma bond that is parallel to the pi bond network will break. The bond that has the highest chance of being cleaved is one with the largest HOMO-LUMO overlap. This effect might be effected by electrostatic effects within the enzyme.\n\nThis was seen in transferase and future interests lie in decarboxylation in various catalytic cycles.\n"}
{"id": "29213543", "url": "https://en.wikipedia.org/wiki?curid=29213543", "title": "ESPRESSO", "text": "ESPRESSO\n\nESPRESSO (Echelle SPectrograph for Rocky Exoplanet- and Stable Spectroscopic Observations) is a third-generation, fiber fed, cross-dispersed, echelle spectrograph mounted on the European Southern Observatory's Very Large Telescope (VLT). The unit saw its first light on September 25, 2016.\n\nESPRESSO is the successor of a line of echelle spectrometers that include CORAVEL, Elodie, Coralie, and HARPS. It measures changes in the light spectrum with great sensitivity, and will be used to search for Earth-size rocky exoplanets via the radial velocity method. For example, Earth induces a radial-velocity variation of 9 cm/s on the Sun; this gravitational \"wobble\" causes minute variations in the color of sunlight, invisible to the human eye but detectable by the instrument. The telescope light is fed to the instrument, located in the VLT Combined-Coude Laboratory 70 meters away from the telescope, where the light from up to 4 Unit Telescopes of the VLT can be combined. The Principal Investigator is Francesco Pepe.\n\nESPRESSO will build on the foundations laid by the High Accuracy Radial Velocity Planet Searcher (HARPS) instrument at the 3.6-metre telescope at ESO’s La Silla Observatory. ESPRESSO will benefit not only from the much larger combined light-collecting capacity of the four 8.2-metre VLT Unit Telescopes, but also from improvements in the stability and calibration accuracy that are now possible by laser frequency comb technology. The requirement is to reach 10 cm/s, but the aimed goal is to obtain a precision level of a few cm/s. This would mean a large step forward over current radial-velocity spectrographs like ESO's HARPS. The HARPS instrument can attain a precision of 97 cm/s (3.5 km/h), with an effective precision of the order of 30 cm/s, making it one of only two spectrographs worldwide with such accuracy. The ESPRESSO would greatly exceed this capability making detection of Earth-size planets from ground-based instruments possible. Commissioning of ESPRESSO at the VLT started late 2017.\n\nThe instrument is capable of operating in 1-UT mode (using one of the telescopes) and in 4-UT mode. In 4-UT mode, in which all the four 8-m telescopes are connected incoherently to form a 16-m equivalent telescope, the spectrograph will detect extremely faint objects.\n\nFor example, for G2V type stars:\n\nESPRESSO will focus the observations on the best-suited candidates: non-active, non-rotating, quiet G dwarfs to red dwarfs. It will operate at the peak of its efficiency for a spectral type up to M4-type stars.\n\nESPRESSO will use as calibration a laser frequency comb (LFC), with backup of two ThAr lamps. It will have three instrumental modes: singleHR, singleUHR and multiMR. In the singleHR mode ESPRESSO can be fed by any of the four UTs.\n\nAll design work was completed and finalised by April 2013, with the manufacturing phase of the project commencing thereafter.\nESPRESSO was tested on June 3, 2016.\nESPRESSO first light occurred on September 25, 2016, during which they spotted various objects, among them the star 60 Sgr A. After being shipped to Chile, installed at the VLT, ESPRESSO saw its first light there on 27 November 2017, in 1-UT mode, observing the star Tau Ceti; the first star observed in the 4-UT mode was on February 3, 2018. ESPRESSO is currently in testing phase, but by December 2018 it will officially begin its mission proper.\n\nThe main scientific objectives for ESPRESSO are:\n\n\nESPRESSO is being developed by a consortium consisting on the European Southern Observatory (ESO) and seven scientific institutes:\n\n\n\n"}
{"id": "3992015", "url": "https://en.wikipedia.org/wiki?curid=3992015", "title": "Eastern Agricultural Complex", "text": "Eastern Agricultural Complex\n\nThe Eastern Agricultural Complex was one of about 10 independent centers of plant domestication in the pre-historic world. By about 1,800 BCE the Native Americans of North America were cultivating for food several species of plants, thus transitioning from a hunter-gatherer economy to agriculture. After 200 BCE when maize from Mexico was introduced to what is now the eastern United States, the Native Americans of the present-day United States and Canada slowly changed from growing indigenous plants to a maize-based agricultural economy. The cultivation of indigenous plants declined and was eventually abandoned, the formerly domesticated plants reverting to their wild forms.\n\nThe initial four plants known to have been domesticated were goosefoot (\"Chenopodium berlandieri\"), sunflower (\"Helianthus annuus var. macrocarpus\"), marshelder (\"Iva annua var. macrocarpa\"), and squash (\"Cucurbita pepo ssp. ovifera\"). Several other species of plants were later domesticated.\n\nThe term Eastern Agricultural Complex (EAC) was popularized by anthropologist Ralph Linton in the 1940s. Linton suggested that the Eastern Woodland tribes integrated maize cultivation from Mexico into their own pre-existing agricultural practices. Ethnobotanists Volney H. Jones and Melvin R. Gilmore built upon Ralph Linton's understanding of Eastern Woodland agriculture with their work in cave and bluff dwellings in Kentucky and the Ozark Mountains in Arkansas. George Quimby also popularized the term \"Eastern complex\" in the 1940s. Authors Guy Gibbons and Kenneth Ames suggest that \"indigenous seed crops\" is a more appropriate term than \"complex\".\nSquash (\"Cucurbita pepo\" var. \"ozarkana\") is considered to be one of the first domesticated plants in the Eastern Woodlands, having been found in the region about 7,000 years ago, though possibly not domesticated in the region until about 3,000 years ago. The squash that was originally part of the complex was raised for edible seeds and to produce small containers (gourds), not for the thick flesh that is associated with modern varieties of squash. \"Cucurbita argyrosperma\" has been found in the region dated to circa 1300-1500 BCE. \"C. pepo\" cultivars crookneck, acorn, and scallop squash appeared later.\n\nOther plants of the EAC include little barley (\"Hordeum pusillum\"), goosefoot or lambsquarters (\"Chenopodium berlandieri\"), erect knotweed (\"Polygonum erectum\"), maygrass (\"Phalaris caroliniana\"), sumpweed or marsh elder (\"Iva annua\"), and sunflower (\"Helianthus annuus\").\n\nThe plants are often divided into \"oily\" or \"starchy\" categories. Sunflower and sumpweed have edible seeds rich in oil. Erect knotweed and goosefoot, a leafy vegetable, are starches, as are maygrass and little barley, both of which are grasses that yield grains that may be ground to make flour. (Note that erect knotweed is a distinct species from the Japanese knotweed (\"Polygonum cuspidatum\") that is considered an invasive species in the eastern United States today.)\n\nThe archaeological record suggests that humans were collecting these plants from the wild by 6000 BC. In the 1970s, archaeologists noticed differences between seeds found in the remains of prehistoric Native American hearths and houses and those growing in the wild. In a domestic setting, the seeds of some plants were much larger than in the wild, and the seeds were easier to extract from the shells or husks. This was evidence that Indigenous gardeners were selectively breeding the plants to make them more productive and accessible.\n\nMost experts had previously believed that agriculture in the U.S. was imported from Mexico, along with the trinity of subtropical crops: maize (corn), beans, and squash. What is now accepted is that the eastern United States was one of about ten regions in the world to become an \"independent center of agricultural origin.\"\n\nThe region of this early agriculture is in the middle Mississippi valley, from Memphis north to St. Louis and extending about 300 miles east and west of the river, mostly in Missouri, Illinois, Kentucky, and Tennessee. The oldest archaeological site known in the United States in which Native Americans were growing, rather than gathering, food is Phillips Spring in Missouri. At Phillips Spring, dating from 3,000 BCE, archaeologists found abundant walnuts, hickory nuts, acorns, grapes, elderberries, ragweed, bottle gourd, and the seeds of \"Cucurbita pepo\", a gourd with edible seeds that is the ancestor of pumpkins and most squashes. The seeds found at Phillips Spring were larger than those of wild \"C. pepo\". The agency for this change was surely human manipulation. Humans were selecting, planting, and tending seeds from plants that produced larger and tastier seeds. Ultimately, they would manipulate \"C. pepo\" to produce edible flesh.\n\nBy 1800 BC, Native Americans were cultivating several different plants. The Riverton Site in the Wabash River valley of Illinois, near the present day village of Palestine, is one of the best known early sites of cultivation. Ten house sites have been discovered at Riverton, indicating a population of 50 to 100 people in the community. Among the hearths and storage pits associated with the houses, archaeologists found a large number of plant remains, including a large number of seeds of chenopods (goosefoot or lamb's quarters) which are likely cultivated plants. Some of the chenopod (\"Chenopodium berlandieri\") seeds had husks only one third as thick as wild seeds. Riverton farmers had bred them selectively to produce a seed easier to access than wild varieties of the same plant.\n\nThe wild food guru of the 1960s, Euell Gibbons, gathered and ate chenopods. \"In rich soil,\" he said, \"lamb's quarters will grow four or five feet high if not disturbed, becoming much branched. It bears a heavy crop of tiny seeds in panicles at the end of every branch. In early winter, when the panicles are dry, it is quite easy to gather these seeds in considerable quantity. Just hold a pail under the branches and strip them off. Rub the husks between the hands to separate the seed and chaff, then winnow out the trash. I have collected several quarts of seed in an hour, using this method. The seeds are quite fine, being smaller than mustard seeds, and a dull blackish-brown color...I find it pretty good food for humans.\"\n\nAnother plant species at Riverton that can confidently be identified as domesticated was sunflower (\"Helianthus annuus\"). This is based on the larger size of the seed in the domesticated than in the wild varieties. Remains of plants that were used, but may or may not have been domesticated at Riverton, include bottle gourd (\"Lagenaria siceraria\"), squash (\"C. pepo\"), wild barley (\"Hordeum pusillum\") and marsh elder (\"Iva annua\").\n\nSome of the species cultivated by Native Americans for food are today considered undesirable weeds. Another name for marshelder is sumpweed; chenopods are derisively called pigweed, although one South American species with a more attractive name, quinoa, is a health food store favorite. Many plants considered weeds are the colonizers of disturbed soil, the first fast-growing weeds to spring up when a natural or man-made event, such as a fire, leaves a bare patch of soil.\n\nThe process of domestication of wild plants cannot be described with any precision. However, Bruce D. Smith and other scholars have pointed out that three of the domesticates (chenopods, \"I. annua\", and \"C. pepo\") were plants that thrived in disturbed soils in river valleys. In the aftermath of a flood, in which most of the old vegetation is killed by the high waters and bare patches of new, often very fertile, soil were created, these pioneer plants sprang up like magic, often growing in almost pure stands, but usually disappearing after a single season, as other vegetation pushed them out until the next flood.\n\nNative Americans learned early that the seeds of these three species were edible and easily harvested in quantity because they grew in dense stands. \"C. pepo\" was important also because the gourd could be made into a lightweight container that was useful to a seminomadic band. Chenopods have edible leaves, related to spinach and chard, that may have also been gathered and eaten by Native Americans. Chenopod seeds are starchy; marsh elder has a highly nutritious oily seed similar to sunflower seeds.\n\nIn gathering the seeds some were undoubtedly dropped in the sunny environment and disturbed soil of a settlement, and those seeds sprouted and thrived. Over time the seeds were sown and the ground was cleared of any competitive vegetation. The seeds which germinated quickest (i.e. thinner seed coats) and the plants which grew fastest were the most likely to be tended, harvested, and replanted. Through a process of unconscious selection and, later, conscious selection, the domesticated weeds became more productive. The seeds of some species became substantially larger and/or their seed coats were less thick compared to the wild plants. For example, the seed coats of domesticated chenopodium is less than 20 microns thick; the wild chenopodium of the same species is 40 to 60 microns thick. Conversely, when Native Americans quit growing these plants, as they did later, their seeds reverted within a few years to the thickness they had been in the wild.\n\nBy about 500 BC, seeds produced by six domesticated plants were an important part of the diet of Native Americans in the middle Mississippi River valley of the United States.\n\nThe indigenous crops were replaced slowly by other more productive crops developed in Mexico: maize, beans and additional varieties of squash. Maize, or corn, was a relative latecomer to the United States. The oldest known evidence of maize in Mexico dates from 6,700 BCE. The oldest evidence of maize cultivated in the United States is about 2,100 BCE at several locations in Arizona and New Mexico.\n\nMaize was first grown in the eastern United States around 200 BCE, and highly productive adapted strains became widely used around 900 CE. The spread was so slow because the seeds and knowledge of techniques for tending them had to cross inhospitable deserts and mountains, and more productive varieties of maize had to be developed to compete with indigenous crops and to suit the cooler climates and shorter growing seasons of the northern regions of the continent. Tropical maize does not flower under the long day conditions of summer north of Mexico, requiring genetic adaptation. It seems that maize was adopted first as a supplement to existing agricultural plants, but gradually came to dominate as its yields increased. Ultimately, the EAC was thoroughly replaced by maize-based agriculture. Most EAC plants are no longer cultivated, and some of them (such as little barley) are regarded as pests by modern farmers.\n\n\n\n\n"}
{"id": "55312961", "url": "https://en.wikipedia.org/wiki?curid=55312961", "title": "Explorer 12", "text": "Explorer 12\n\nExplorer 12 (also known as S3) was a United States Satellite built to measure Solar wind, Cosmic ray, and Magnetic field. It was launched on August 16 1961, aboard a Thor-Delta booster. Explorer 11 was the first of the 3 S3 series spacecraft. It ceased transmitting on December 6 due to power failure.\n\nExplorer 12 was designed to study space physics, and so had a multitude of instruments including a cosmic-ray detector, a particle trapper, and a magnetometer. Good data was recorded for 90% of the mission.\n"}
{"id": "1118171", "url": "https://en.wikipedia.org/wiki?curid=1118171", "title": "Flatness problem", "text": "Flatness problem\n\nThe flatness problem (also known as the oldness problem) is a cosmological fine-tuning problem within the Big Bang model of the universe. Such problems arise from the observation that some of the initial conditions of the universe appear to be fine-tuned to very 'special' values, and that small deviations from these values would have extreme effects on the appearance of the universe at the current time.\n\nIn the case of the flatness problem, the parameter which appears fine-tuned is the density of matter and energy in the universe. This value affects the curvature of space-time, with a very specific critical value being required for a flat universe. The current density of the universe is observed to be very close to this critical value. Since the total density departs rapidly from the critical value over cosmic time, the early universe must have had a density even closer to the critical density, departing from it by one part in 10 or less. This leads cosmologists to question how the initial density came to be so closely fine-tuned to this 'special' value.\n\nThe problem was first mentioned by Robert Dicke in 1969. The most commonly accepted solution among cosmologists is cosmic inflation, the idea that the universe went through a brief period of extremely rapid expansion in the first fraction of a second after the Big Bang; along with the monopole problem and the horizon problem, the flatness problem is one of the three primary motivations for inflationary theory.\n\nAccording to Einstein's field equations of general relativity, the structure of spacetime is affected by the presence of matter and energy. On small scales space appears flat – as does the surface of the Earth if one looks at a small area. On large scales however, space is bent by the gravitational effect of matter. Since relativity indicates that matter and energy are equivalent, this effect is also produced by the presence of energy (such as light and other electromagnetic radiation) in addition to matter. The amount of bending (or curvature) of the universe depends on the density of matter/energy present.\n\nThis relationship can be expressed by the first Friedmann equation. In a universe without a cosmological constant, this is:\n\nHere formula_2 is the Hubble parameter, a measure of the rate at which the universe is expanding. formula_3 is the total density of mass and energy in the universe, formula_4 is the scale factor (essentially the 'size' of the universe), and formula_5 is the curvature parameter — that is, a measure of how curved spacetime is. A positive, zero or negative value of formula_5 corresponds to a respectively closed, flat or open universe. The constants formula_7 and formula_8 are Newton's gravitational constant and the speed of light, respectively.\n\nCosmologists often simplify this equation by defining a critical density, formula_9. For a given value of formula_2, this is defined as the density required for a flat universe, i.e. . Thus the above equation implies\n\nSince the constant formula_7 is known and the expansion rate formula_2 can be measured by observing the speed at which distant galaxies are receding from us,\nformula_9 can be determined. Its value is currently around . The ratio of the actual density to this critical value is called Ω, and its difference from 1 determines the geometry of the universe: corresponds to a greater than critical density, , and hence a closed universe. gives a low density open universe, and Ω equal to exactly 1 gives a flat universe.\n\nThe Friedmann equation,\n\ncan be re-arranged into\n\nwhich after factoring formula_17, and using formula_18, leads to\n\nThe right hand side of the last expression above contains constants only and therefore the left hand side must remain constant throughout the evolution of the universe.\n\nAs the universe expands the scale factor formula_4 increases, but the density formula_3 decreases as matter (or energy) becomes spread out. For the standard model of the universe which contains mainly matter and radiation for most of its history, formula_3 decreases more quickly than formula_23 increases, and so the factor will decrease. Since the time of the Planck era, shortly after the Big Bang, this term has decreased by a factor of around formula_24 and so must have increased by a similar amount to retain the constant value of their product.\n\nThe value of Ω at the present time is denoted Ω. This value can be deduced by measuring the curvature of spacetime (since , or formula_25, is defined as the density for which the curvature ). The curvature can be inferred from a number of observations.\n\nOne such observation is that of anisotropies (that is, variations with direction - see below) in the Cosmic Microwave Background (CMB) radiation. The CMB is electromagnetic radiation which fills the universe, left over from an early stage in its history when it was filled with photons and a hot, dense plasma. This plasma cooled as the universe expanded, and when it cooled enough to form stable atoms it no longer absorbed the photons. The photons present at that stage have been propagating ever since, growing fainter and less energetic as they spread through the ever-expanding universe.\n\nThe temperature of this radiation is almost the same at all points on the sky, but there is a slight variation (around one part in 100,000) between the temperature received from different directions. The angular scale of these fluctuations - the typical angle between a hot patch and a cold patch on the sky - depends on the curvature of the universe which in turn depends on its density as described above. Thus, measurements of this angular scale allow an estimation of Ω.\n\nAnother probe of Ω is the frequency of Type-Ia supernovae at different distances from Earth. These supernovae, the explosions of degenerate white dwarf stars, are a type of standard candle; this means that the processes governing their intrinsic brightness are well understood so that a measure of \"apparent\" brightness when seen from Earth can be used to derive accurate distance measures for them (the apparent brightness decreasing in proportion to the square of the distance - see luminosity distance). Comparing this distance to the redshift of the supernovae gives a measure of the rate at which the universe has been expanding at different points in history. Since the expansion rate evolves differently over time in cosmologies with different total densities, Ω can be inferred from the supernovae data.\n\nData from the Wilkinson Microwave Anisotropy Probe (measuring CMB anisotropies) combined with that from the Sloan Digital Sky Survey and observations of type-Ia supernovae constrain Ω to be 1 within 1%. In other words, the term |Ω − 1| is currently less than 0.01, and therefore must have been less than 10 at the Planck era.\n\nThis tiny value is the crux of the flatness problem. If the initial density of the universe could take any value, it would seem extremely surprising to find it so 'finely tuned' to the critical value formula_9. Indeed, a very small departure of Ω from 1 in the early universe would have been magnified during billions of years of expansion to create a current density very far from critical. In the case of an overdensity this would lead to a universe so dense it would cease expanding and collapse into a Big Crunch (an opposite to the Big Bang in which all matter and energy falls back into an extremely dense state) in a few years or less; in the case of an underdensity it would expand so quickly and become so sparse it would soon seem essentially empty, and gravity would not be strong enough by comparison to cause matter to collapse and form galaxies. In either case the universe would contain no complex structures such as galaxies, stars, planets and any form of life.\n\nThis problem with the Big Bang model was first pointed out by Robert Dicke in 1969, and it motivated a search for some reason the density should take such a specific value.\n\nSome cosmologists agreed with Dicke that the flatness problem was a serious one, in need of a fundamental reason for the closeness of the density to criticality. But there was also a school of thought which denied that there was a problem to solve, arguing instead that since the universe must have some density it may as well have one close to formula_27 as far from it, and that speculating on a reason for any particular value was \"beyond the domain of science\". Enough cosmologists saw the problem as a real one, however, for various solutions to be proposed.\n\nOne solution to the problem is to invoke the anthropic principle, which states that humans should take into account the conditions necessary for them to exist when speculating about causes of the universe's properties. If two types of universe seem equally likely but only one is suitable for the evolution of intelligent life, the anthropic principle suggests that finding ourselves in that universe is no surprise: if the other universe had existed instead, there would be no observers to notice the fact.\n\nThe principle can be applied to solve the flatness problem in two somewhat different ways. The first (an application of the 'strong anthropic principle') was suggested by C. B. Collins and Stephen Hawking, who in 1973 considered the existence of an infinite number of universes such that every possible combination of initial properties was held by some universe. In such a situation, they argued, only those universes with exactly the correct density for forming galaxies and stars would give rise to intelligent observers such as humans: therefore, the fact that we observe Ω to be so close to 1 would be \"simply a reflection of our own existence.\"\n\nAn alternative approach, which makes use of the 'weak anthropic principle', is to suppose that the universe is infinite in size, but with the density varying in different places (i.e. an inhomogeneous universe). Thus some regions will be over-dense and some under-dense . These regions may be extremely far apart - perhaps so far that light has not had time to travel from one to another during the age of the universe (that is, they lie outside one another's cosmological horizons). Therefore, each region would behave essentially as a separate universe: if we happened to live in a large patch of almost-critical density we would have no way of knowing of the existence of far-off under- or over-dense patches since no light or other signal has reached us from them. An appeal to the anthropic principle can then be made, arguing that intelligent life would only arise in those patches with Ω very close to 1, and that therefore our living in such a patch is unsurprising.\n\nThis latter argument makes use of a version of the anthropic principle which is 'weaker' in the sense that it requires no speculation on multiple universes, or on the probabilities of various different universes existing instead of the current one. It requires only a single universe which is infinite - or merely large enough that many disconnected patches can form - and that the density varies in different regions (which is certainly the case on smaller scales, giving rise to galactic clusters and voids).\n\nHowever, the anthropic principle has been criticised by many scientists. For example, in 1979 Bernard Carr and Martin Rees argued that the principle “is entirely post hoc: it has not yet been used to predict any feature of the Universe.” Others have taken objection to its philosophical basis, with Ernan McMullin writing in 1994 that \"the weak Anthropic principle is trivial ... and the strong Anthropic principle is indefensible.\" Since many physicists and philosophers of science do not consider the principle to be compatible with the scientific method, another explanation for the flatness problem was needed.\n\nThe standard solution to the flatness problem invokes cosmic inflation, a process whereby the universe expands exponentially quickly (i.e. formula_4 grows as formula_29 with time formula_30, for some constant formula_31) during a short period in its early history. The theory of inflation was first proposed in 1979, and published in 1981, by Alan Guth. His two main motivations for doing so were the flatness problem and the horizon problem, another fine-tuning problem of physical cosmology.\n\nThe proposed cause of inflation is a field which permeates space and drives the expansion. The field contains a certain energy density, but unlike the density of the matter or radiation present in the late universe, which decrease over time, the density of the inflationary field remains roughly constant as space expands. Therefore, the term formula_17 increases extremely rapidly as the scale factor formula_4 grows exponentially. Recalling the Friedmann Equation\n\nand the fact that the right-hand side of this expression is constant, the term formula_35 must therefore decrease with time.\n\nThus if formula_35 initially takes any arbitrary value, a period of inflation can force it down towards 0 and leave it extremely small - around formula_37 as required above, for example. Subsequent evolution of the universe will cause the value to grow, bringing it to the currently observed value of around 0.01. Thus the sensitive dependence on the initial value of Ω has been removed: a large and therefore 'unsurprising' starting value need not become amplified and lead to a very curved universe with no opportunity to form galaxies and other structures.\n\nThis success in solving the flatness problem is considered one of the major motivations for inflationary theory.\n\nAlthough inflationary theory is regarded as having had much success, and the evidence for it is compelling, it is not universally accepted: cosmologists recognize that there are still gaps in the theory and are open to the possibility that future observations will disprove it. In particular, in the absence of any firm evidence for what the field driving inflation should be, many different versions of the theory have been proposed. Many of these contain parameters or initial conditions which themselves require fine-tuning in much the way that the early density does without inflation.\n\nFor these reasons work is still being done on alternative solutions to the flatness problem. These have included non-standard interpretations of the effect of dark energy and gravity, particle production in an oscillating universe, and use of a Bayesian statistical approach to argue that the problem is non-existent. The latter argument, suggested for example by Evrard and Coles, maintains that the idea that Ω being close to 1 is 'unlikely' is based on assumptions about the likely distribution of the parameter which are not necessarily justified. Despite this ongoing work, inflation remains by far the dominant explanation for the flatness problem.\n\nThe flatness problem is naturally solved by the Einstein–Cartan–Sciama–Kibble theory of gravity, without an exotic form of matter required in inflationary theory. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. It has no free parameters. Including torsion gives the correct conservation law for the total (orbital plus intrinsic) angular momentum of matter in the presence of gravity. The minimal coupling between torsion and Dirac spinors obeying the nonlinear Dirac equation generates a spin-spin interaction which is significant in fermionic matter at extremely high densities. Such an interaction averts the unphysical big bang singularity, replacing it with a bounce at a finite minimum scale factor, before which the Universe was contracting. The rapid expansion immediately after the big bounce explains why the present Universe at largest scales appears spatially flat, homogeneous and isotropic. As the density of the Universe decreases, the effects of torsion weaken and the Universe smoothly enters the radiation-dominated era.\n\n"}
{"id": "13789625", "url": "https://en.wikipedia.org/wiki?curid=13789625", "title": "Friedrich Ris", "text": "Friedrich Ris\n\nFriedrich Ris (1867 – 1931 in Glarus) was a Swiss physician and entomologist who specialised in Odonata.\nHe was Director of a psychiatric clinic in Rheinau, Switzerland.\n"}
{"id": "561369", "url": "https://en.wikipedia.org/wiki?curid=561369", "title": "Getaway Special", "text": "Getaway Special\n\nGetaway Special was a NASA program that offered interested individuals, or groups, opportunities to fly small experiments aboard the Space Shuttle. Over the 20-year history of the program, over 170 individual missions were flown. The program, which was officially known as the \"Small, Self-Contained Payloads program\", was canceled following the Space Shuttle Columbia disaster on February 1, 2003.\n\nThe program was conceived by NASA's Shuttle program manager John Yardley, and announced in the fall of 1976. The \"Getaway Special\" nickname originated from a special vacation fare for flights between Los Angeles and Honolulu being advertised by Trans World Airlines at the time around the program's conception.\n\nThe first Getaway Special was purchased by Gilbert Moore and donated to Utah State University. It was flown on Columbia during STS-4 in June/July 1982. The program was canceled after the Space Shuttle Columbia disaster on February 1, 2003. The last Getaway Special, which was carried aboard STS-107, was the Freestar experiment package, which carried six different experiments. Much of the data was lost when \"Columbia\" was destroyed, but some data was transmitted during the mission.\n\nAfter reorganization of the Shuttle Program, NASA cited the need for the remaining shuttle fleet to complete assembly of the ISS to justify its decision to cancel the program. The GAS program canisters and GAS Bridge combined weight were only usable on low orbit missions, which were rescheduled with higher priority payloads. With payload and program limits set on the remaining shuttle missions until the expected STS close-out in 2010, the GAS program was eliminated.\n\nTo assure that diverse groups would have access to space, NASA rotated GAS payload assignments among four major categories of users: educational, foreign, commercial, and U.S. government. GAS payloads had been reserved by foreign governments and individuals; U.S. industrialists, foundations, high schools, colleges and universities; professional societies; service clubs; and many others. Although persons and groups involved in space research obtained many of the reservations, a large number of spaces were reserved by persons and organizations outside the space community.\n\nGAS requests were first approved at NASA Headquarters in Washington, D.C., by the director of the Transportation Services Office. At that point NASA screened the propriety and objectives of each request. To complete the reservation process for GAS payloads, each request was accompanied or preceded by the payment of $500. Approved requests were assigned an identification number and referred to the GAS team at the Goddard Space Flight Center in Greenbelt, Maryland, the designated lead center for the project. The GAS team screened the proposals for safety and provided advice and consultation on payload design. It certified that proposed payloads would be safe and would not harm or interfere with the operations of the space shuttle, its crew, or other experiments on the flight. The costs of any physical testing required to answer safety questions before launch were borne by the GAS customer.\n\nThere were no stringent requirements to qualify for participation in the GAS program. However, each payload was required to meet specific safety criteria, have been screened for its propriety, as well as being evaluated for its educational, scientific or technological objectives. These guidelines preclude commemorative items, such as medallions, that are intended for sale as objects that have flown in space. NASA's Space Shuttle program had specific standards and conditions relating to GAS payloads. Payloads were required to have fit into NASA standard containers and weigh no more than . Two or more experiments could have been included in a single container if they fit while not exceeding weight limitations. The payload must have been self-powered, as experiments could not draw on the shuttle orbiter's electricity. In addition, the crew's involvement with GAS payloads was limited to six simple activities (such as turning on and off up to three payload switches), due to the fact that crew activity schedules do not provide opportunities to either monitor or service GAS payloads in flight.\n\nThe cost of this unique service depended on the size and weight of the experiment. Getaway specials of and cost $10,000; and , $5,000; and and , $3,000. The weight of the GAS container, experiment mounting plate and its attachment screws, and all hardware regularly supplied by NASA was not charged to the experimenter's weight allowance.\n\nThe GAS container provided internal pressure, which could be varied from near vacuum to about one atmosphere. The bottom and sides of the container were always thermally insulated, and the top may have been insulated or not, depending on the specific experiment. A lid that could be opened, or one with a window, may be required, and were offered as options at additional cost. The GAS containers were made of aluminum, and the circular end plates are ⁄ inch (16 mm) thick aluminum. The bottom of the container were reserved for NASA interface equipment, such as command decoders and pressure regulating systems. The container was a pressure vessel that could be evacuated before or during launch, or on orbit, and could be re-pressurized during re-entry, or on orbit, as required by the experimenter.\n\nThe getaway bridge, which was capable of holding 12 canisters, made its maiden flight on STS-61-C. The aluminum bridge fit across the payload bay of the orbiter and offered a convenient and economic way of flying several GAS canisters.\n\n\nSTS-107 happened on 1/16/03, not 2002. Freestar experiment\n\n"}
{"id": "20234154", "url": "https://en.wikipedia.org/wiki?curid=20234154", "title": "International Association for Mathematics and Computers in Simulation", "text": "International Association for Mathematics and Computers in Simulation\n\nThe International Association for Mathematics and Computers in Simulation (IMACS) has the goal to establish means of communication between researchers on simulation. It is incorporated in the United States and Belgium, with affiliates in other countries. IMACS organizes conferences, and publishes scientific journals and books in affiliation with commercial publishers.\n\n"}
{"id": "12599355", "url": "https://en.wikipedia.org/wiki?curid=12599355", "title": "International Noble Gas Experiment", "text": "International Noble Gas Experiment\n\nThe International Noble Gas Experiment (INGE) was formed in 1999 as an informal expert's group of developers of radioactive xenon measurement systems for the International Monitoring System for the Comprehensive Nuclear-Test-Ban Treaty (CTBT) (signed in 1997, but which has not entered into force). The group originally consisted of research and development groups from Germany, France, Russia, Sweden, and the United States, as well as personnel from Provisional Technical Secretariat of the Preparatory Commission for the Comprehensive Nuclear-Test-Ban Treaty Organization CTBTO.\n\nThe INGE group was formed to test aspects of measuring xenon fission product radionuclides released by nuclear explosions. The systems developed and participating in the INGE measure xenon isotopes in the atmosphere and includes Xe, Xe, Xe, and Xe.\n\nSince the INGE was formed in 1999, the group has expanded somewhat and now includes R&D and operational groups from many locations around the world. Although there is no official list of INGE members, the group is informally composed of scientists, engineers, and others from Argentina, Austria, Australia, Canada, China, France, Germany, Japan, Norway, South Korea, Sweden, Russia, the United States, and several other countries. These members regularly contribute to better understanding radioactive xenon measurements through operation of samplers, measurements of background at various locations, creation of data analysis routines, etc.\n\nStaff from the preparatory commission of the CTBTO oversaw the experiment, with technical assistance from a German group of noble gas experts from the BfS in Freiburg, Germany. As of 2009, the experiment was still on-going, and so far it had consisted of 3 phases:\n\nThe first phase of the INGE experiment took place in the laboratories. Four systems were developed to the point that they could measure xenon concentrations to specifications laid out by the CTBTO Preparatory Commission.\n\nThe second phase of the INGE experiment took place in Freiburg, Germany, a location far away from the developers laboratories.\n\nPhase 2: Use of Radioxenon Monitoring Equipment in Freiburg \n\nThe four measurement systems tested in Phase 2 had favorable results in that all of the systems met CTBTO minimum specifications and agreed with the independent analyses provided by BfS.\n\nPhase 3, which was separated into a, b, and c components was designed primarily to test commercial versions of the systems designed and tested in INGE Phase 2. The commercial version of the SAUNA (the SAUNA-II) is now being manufactured by Gammadata, Inc. and the SPALAX is being produced commercially by Environnement S.A, Radionuclides Division (formerly Societé Française d’Ingenierie: SFI).\n\nOne of the major aspects of the INGE that has been investigated is the variation of worldwide radioactive xenon backgrounds. Concentrations of the xenon isotopes are continuously measured throughout the INGE experiment, and it has been found so far that a major source of background is medical isotope production.\n\nThere has been a number of workshops to discuss various aspects of the experiment and to discuss worldwide backgrounds of radioxenon.\n\n1999 - Freiburg, Germany\n1999 - Freiburg, Germany\n2000 - Freiburg, Germany\n2000 - Freiburg, Germany\n2001 - Stockholm, Sweden\n2002 - Tahiti, French Polynesia\n2002 - Richland, Washington, United States\n2003 - Ottawa, Canada\n2004 - Strassoldo, Italy\n2005 - Stockholm, Sweden\n2006 - Melbourne, Australia\n2007 - Las Vegas, Nevada, United States\n2008 - St. Petersburg, Russian Federation\n2009 - Daejeon, South Korea\n2010 - Buenos Aires, Argentina\n2011 - Yogyakarta, Indonesia\n2012 - Mito, Japan\n2013 - Vienna, Austria\n2015 - Austin, Texas, United States\n2017 - London, United Kingdom\n"}
{"id": "19242322", "url": "https://en.wikipedia.org/wiki?curid=19242322", "title": "Johann Wolfgang von Goethe", "text": "Johann Wolfgang von Goethe\n\nJohann Wolfgang von Goethe (; ; 28 August 1749 – 22 March 1832) was a German writer and statesman. His works include four novels; epic and lyric poetry; prose and verse dramas; memoirs; an autobiography; literary and aesthetic criticism; and treatises on botany, anatomy, and colour. In addition, there are numerous literary and scientific fragments, more than 10,000 letters, and nearly 3,000 drawings by him extant.\n\nA literary celebrity by the age of 25, Goethe was ennobled by the Duke of Saxe-Weimar, Karl August in 1782 after taking up residence there in November 1775 following the success of his first novel, \"The Sorrows of Young Werther\" (1774). He was an early participant in the \"Sturm und Drang\" literary movement. During his first ten years in Weimar, Goethe was a member of the Duke's privy council, sat on the war and highway commissions, oversaw the reopening of silver mines in nearby Ilmenau, and implemented a series of administrative reforms at the University of Jena. He also contributed to the planning of Weimar's botanical park and the rebuilding of its Ducal Palace, which in 1998 were together designated a UNESCO World Heritage site under the name Classical Weimar.\n\nGoethe's first major scientific work, the \"Metamorphosis of Plants\", was published after he returned from a 1788 tour of Italy. In 1791, he was made managing director of the theatre at Weimar, and in 1794 he began a friendship with the dramatist, historian, and philosopher Friedrich Schiller, whose plays he premiered until Schiller's death in 1805. During this period, Goethe published his second novel, \"Wilhelm Meister's Apprenticeship\"; the verse epic \"Hermann and Dorothea\", and, in 1808, the first part of his most celebrated drama, \"Faust\". His conversations and various common undertakings throughout the 1790s with Schiller, Johann Gottlieb Fichte, Johann Gottfried Herder, Alexander von Humboldt, Wilhelm von Humboldt, and August and Friedrich Schlegel have come to be collectively termed Weimar Classicism.\n\nThe German philosopher Arthur Schopenhauer named \"Wilhelm Meister's Apprenticeship\" one of the four greatest novels ever written (along with \"Tristram Shandy\", \"La Nouvelle Héloïse\", and \"Don Quixote\"), while the American philosopher and essayist Ralph Waldo Emerson selected Goethe as one of six \"representative men\" in his work of the same name (along with Plato, Emanuel Swedenborg, Montaigne, Napoleon, and Shakespeare). Goethe's comments and observations form the basis of several biographical works, notably Johann Peter Eckermann's \"Conversations with Goethe\".\n\nGoethe's father, Johann Caspar Goethe, lived with his family in a large house in Frankfurt, then an Imperial Free City of the Holy Roman Empire. Though he had studied law in Leipzig and had been appointed Imperial Councillor, he was not involved in the city's official affairs. Johann Caspar married Goethe's mother, Catharina Elizabeth Textor at Frankfurt on 20 August 1748, when he was 38 and she was 17. All their children, with the exception of Johann Wolfgang and his sister, Cornelia Friederica Christiana, who was born in 1750, died at early ages.\nHis father and private tutors gave Goethe lessons in all the common subjects of their time, especially languages (Latin, Greek, French, Italian, English and Hebrew). Goethe also received lessons in dancing, riding and fencing. Johann Caspar, feeling frustrated in his own ambitions, was determined that his children should have all those advantages that he had not.\n\nAlthough Goethe's great passion was drawing, he quickly became interested in literature; Friedrich Gottlieb Klopstock and Homer were among his early favorites. He had a lively devotion to theater as well and was greatly fascinated by puppet shows that were annually arranged in his home; this is a recurrent theme in his literary work \"Wilhelm Meister's Apprenticeship\".\n\nHe also took great pleasure in reading works on history and religion. He writes about this period:\nGoethe became also acquainted with Frankfurt actors. Among early literary attempts, he was infatuated with \"Gretchen\", who would later reappear in his \"Faust\" and the adventures with whom he would concisely describe in \"Dichtung und Wahrheit\". He adored Caritas Meixner (27 July 1750 – 31 December 1773), a wealthy Worms trader's daughter and friend of his sister, who would later marry the merchant G. F. Schuler.\n\nGoethe studied law at Leipzig University from 1765 to 1768. He detested learning age-old judicial rules by heart, preferring instead to attend the poetry lessons of Christian Fürchtegott Gellert. In Leipzig, Goethe fell in love with Anna Katharina Schönkopf and wrote cheerful verses about her in the Rococo genre. In 1770, he anonymously released \"Annette\", his first collection of poems. His uncritical admiration for many contemporary poets vanished as he became interested in Gotthold Ephraim Lessing and Christoph Martin Wieland. Already at this time, Goethe wrote a good deal, but he threw away nearly all of these works, except for the comedy \"Die Mitschuldigen\". The restaurant Auerbachs Keller and its legend of Faust's 1525 barrel ride impressed him so much that Auerbachs Keller became the only real place in his closet drama \"Faust Part One\". As his studies did not progress, Goethe was forced to return to Frankfurt at the close of August 1768.\n\nGoethe became severely ill in Frankfurt. During the year and a half that followed, because of several relapses, the relationship with his father worsened. During convalescence, Goethe was nursed by his mother and sister. In April 1770, Goethe left Frankfurt in order to finish his studies at the University of Strasbourg.\n\nIn Alsace, Goethe blossomed. No other landscape has he described as affectionately as the warm, wide Rhine area. In Strasbourg, Goethe met Johann Gottfried Herder. The two became close friends, and crucially to Goethe's intellectual development, Herder kindled his interest in Shakespeare, Ossian and in the notion of \"Volkspoesie\" (folk poetry). On 14 October 1772 Goethe held a gathering in his parental home in honour of the first German \"Shakespeare Day\". His first acquaintance with Shakespeare's works is described as his personal awakening in literature.\n\nOn a trip to the village Sessenheim, Goethe fell in love with Friederike Brion, in October 1770, but, after ten months, terminated the relationship in August 1771. Several of his poems, like \", \" and \"\", originate from this time.\n\nAt the end of August 1771, Goethe acquired the academic degree of the \"Lizenziat\" (Licentia docendi) in Frankfurt and established a small legal practice. Although in his academic work he had expressed the ambition to make jurisprudence progressively more humane, his inexperience led him to proceed too vigorously in his first cases, and he was reprimanded and lost further ones. This prematurely terminated his career as a lawyer after only a few months. At this time, Goethe was acquainted with the court of Darmstadt, where his inventiveness was praised. From this milieu came Johann Georg Schlosser (who was later to become his brother-in-law) and Johann Heinrich Merck. Goethe also pursued literary plans again; this time, his father did not have anything against it, and even helped. Goethe obtained a copy of the biography of a noble highwayman from the German Peasants' War. In a couple of weeks the biography was reworked into a colourful drama. Entitled \"Götz von Berlichingen,\" the work went directly to the heart of Goethe's contemporaries.\n\nGoethe could not subsist on being one of the editors of a literary periodical (published by Schlosser and Merck). In May 1772 he once more began the practice of law at Wetzlar. In 1774 he wrote the book which would bring him worldwide fame, \"The Sorrows of Young Werther\". The outer shape of the work's plot is widely taken over from what Goethe experienced during his Wetzlar time with Charlotte Buff (1753–1828) and her fiancé, Johann Christian Kestner (1741–1800), as well as from the suicide of the author's friend Karl Wilhelm Jerusalem (1747–1772); in it, Goethe made a desperate passion of what was in reality a hearty and relaxed friendship. Despite the immense success of \"Werther\", it did not bring Goethe much financial gain because copyright laws at the time were essentially nonexistent. (In later years Goethe would bypass this problem by periodically authorizing \"new, revised\" editions of his \"Complete Works\".)\n\nIn 1775, Goethe was invited, on the strength of his fame as the author of \"The Sorrows of Young Werther\", to the court of Karl August, Duke of Saxe-Weimar-Eisenach, who would become Grand Duke in 1815. (The Duke at the time was 18 years of age, to Goethe's 26.) Goethe thus went to live in Weimar, where he remained for the rest of his life and where, over the course of many years, he held a succession of offices, becoming the Duke's friend and chief adviser.\n\nIn 1776, Goethe formed a close relationship to Charlotte von Stein, an older, married woman. The intimate bond with von Stein lasted for ten years, after which Goethe abruptly left for Italy without giving his companion any notice. She was emotionally distraught at the time, but they were eventually reconciled.\n\nGoethe, aside from official duties, was also a friend and confidant to the Duke, and participated fully in the activities of the court. For Goethe, his first ten years at Weimar could well be described as a garnering of a degree and range of experience which perhaps could be achieved in no other way. In 1779, Goethe took on the War Commission of the Grand Duchy of Saxe-Weimar, in addition to the Mines and Highways commissions. In 1782, when the chancellor of the Duchy's Exchequer left his office, Goethe agreed to act in his place for two and a half years; this post virtually made him prime minister and the principal representative of the Duchy. Goethe was ennobled in 1782 (this being indicated by the \"von\" in his name).\n\nAs head of the Saxe-Weimar War Commission, Goethe participated in the recruitment of mercenaries into the Prussian and British military during the American Revolution. The author W. Daniel Wilson claims that Goethe engaged in negotiating the forced sale of vagabonds, criminals, and political dissidents as part of these activities.\n\nGoethe's journey to the Italian peninsula and Sicily from 1786 to 1788 was of great significance in his aesthetic and philosophical development. His father had made a similar journey during his own youth, and his example was a major motivating factor for Goethe to make the trip. More importantly, however, the work of Johann Joachim Winckelmann had provoked a general renewed interest in the classical art of ancient Greece and Rome. Thus Goethe's journey had something of the nature of a pilgrimage to it. During the course of his trip Goethe met and befriended the artists Angelica Kauffman and Johann Heinrich Wilhelm Tischbein, as well as encountering such notable characters as Lady Hamilton and Alessandro Cagliostro (see Affair of the Diamond Necklace).\n\nHe also journeyed to Sicily during this time, and wrote intriguingly that \"To have seen Italy without having seen Sicily is to not have seen Italy at all, for Sicily is the clue to everything.\" While in Southern Italy and Sicily, Goethe encountered, for the first time genuine Greek (as opposed to Roman) architecture, and was quite startled by its relative simplicity. Winckelmann had not recognized the distinctness of the two styles.\n\nGoethe's diaries of this period form the basis of the non-fiction \"Italian Journey\". \"Italian Journey\" only covers the first year of Goethe's visit. The remaining year is largely undocumented, aside from the fact that he spent much of it in Venice. This \"gap in the record\" has been the source of much speculation over the years.\n\nIn the decades which immediately followed its publication in 1816, \"Italian Journey\" inspired countless German youths to follow Goethe's example. This is pictured, somewhat satirically, in George Eliot's \"Middlemarch\".\n\nIn late 1792, Goethe took part in the Battle of Valmy against revolutionary France, assisting Duke Karl August of Saxe-Weimar-Eisenach during the failed invasion of France. Again during the Siege of Mainz he assisted Carl August as a military observer. His written account of these events can be found within his \"Complete Works.\"\n\nIn 1794, Friedrich Schiller wrote to Goethe offering friendship; they had previously had only a mutually wary relationship ever since first becoming acquainted in 1788. This collaborative friendship lasted until Schiller's death in 1805.\n\nIn 1806, Goethe was living in Weimar with his mistress Christiane Vulpius, the sister of Christian A Vulpius, and their son Julius August Walter von Goethe. On 13 October, Napoleon's army invaded the town. The French \"spoon guards,\" the least disciplined soldiers, occupied Goethe's house: \n\nDays afterward, on 19 October 1806, Goethe legitimized their 18-year relationship by marrying Christiane in a quiet marriage service at the . They had already had several children together by this time, including their son, Julius August Walter von Goethe (25 December 1789 – 28 October 1830), whose wife, (31 October 1796 – 26 October 1872), cared for the elder Goethe until his death in 1832. August and Ottilie had three children: Walther, Freiherr von Goethe (9 April 1818 – 15 April 1885), (18 September 1820 – 20 January 1883) and (29 October 1827 – 29 September 1844). Christiane von Goethe died in 1816.\n\nAfter 1793, Goethe devoted his endeavours primarily to literature. By 1820, Goethe was on amiable terms with Kaspar Maria von Sternberg.\nIn 1823, having recovered from a near fatal heart illness, Goethe fell in love with Ulrike von Levetzow whom he wanted to marry, but because of the opposition of her mother he never proposed. Their last meeting in Carlsbad on 5 September 1823 inspired him to the famous \"Marienbad Elegy\" which he considered one of his finest works. During that time he also developed a deep emotional bond with the Polish pianist Maria Agata Szymanowska.\n\nIn 1821 Goethe's friend Carl Friedrich Zelter introduced him to the 12 year old Felix Mendelssohn. Goethe, now in his seventies, was greatly impressed by the child, leading to perhaps the earliest confirmed comparison with Mozart in the following conversation between Goethe and Zelter:\n\nMendelssohn was invited to meet Goethe on several later occasions, and set a number of Goethe's poems to music. His other compositions inspired by Goethe include the overture \"Calm Sea and Prosperous Voyage\" (Op. 27, 1828), and the cantata \"Die erste Walpurgisnacht\" (\"The First Walpurgis Night\", Op. 60, 1832).\n\nIn 1832, Goethe died in Weimar of apparent heart failure. His last words, according to his doctor Carl Vogel, were, \"\" (More light!), but this is disputed as Vogel was not in the room at the moment Goethe died. He is buried in the Ducal Vault at Weimar's Historical Cemetery.\n\nEckermann closes his famous work, \"Conversations with Goethe\", with this passage:\nThe first production of Richard Wagner's opera \"Lohengrin\" took place in Weimar in 1850. The conductor was Franz Liszt, who chose the date 28 August in honour of Goethe, who was born on 28 August 1749.\n\nThe most important of Goethe's works produced before he went to Weimar were \"Götz von Berlichingen\" (1773), a tragedy that was the first work to bring him recognition, and the novel \"The Sorrows of Young Werther\" (German: \"Die Leiden des jungen Werthers\") (1774), which gained him enormous fame as a writer in the \"Sturm und Drang\" period which marked the early phase of Romanticism. Indeed, \"Werther\" is often considered to be the \"spark\" which ignited the movement, and can arguably be called the world's first \"best-seller.\" During the years at Weimar before he met Schiller he began \"Wilhelm Meister's Apprenticeship\", wrote the dramas \"Iphigenie auf Tauris\" (\"Iphigenia in Tauris\"), \"Egmont\", \"Torquato Tasso\", and the fable \"Reineke Fuchs\".\n\nTo the period of his friendship with Schiller belong the conception of \"Wilhelm Meister's Journeyman Years\" (the continuation of \"Wilhelm Meister's Apprenticeship\"), the idyll of \"Hermann and Dorothea\", the \"Roman Elegies \" and the verse drama \"The Natural Daughter\". In the last period, between Schiller's death, in 1805, and his own, appeared \"Faust Part One\", \"Elective Affinities\", the \"West-Eastern Diwan\" (a collection of poems in the Persian style, influenced by the work of Hafez), his autobiographical \"Aus meinem Leben: Dichtung und Wahrheit\" (\"From My Life: Poetry and Truth\") which covers his early life and ends with his departure for Weimar, his \"Italian Journey\", and a series of treatises on art. His writings were immediately influential in literary and artistic circles.\n\nGoethe was fascinated by Kalidasa's \"Abhijñānaśākuntalam\", which was one of the first works of Sanskrit literature that became known in Europe, after being translated from English to German.\n\nThe short epistolary novel, \"Die Leiden des jungen Werthers\", or \"The Sorrows of Young Werther\", published in 1774, recounts an unhappy romantic infatuation that ends in suicide. Goethe admitted that he \"shot his hero to save himself\": a reference to Goethe's own near-suicidal obsession with a young woman during this period, an obsession he quelled through the writing process. The novel remains in print in dozens of languages and its influence is undeniable; its central hero, an obsessive figure driven to despair and destruction by his unrequited love for the young Lotte, has become a pervasive literary archetype. The fact that \"Werther\" ends with the protagonist's suicide and funeral—a funeral which \"no clergyman attended\"—made the book deeply controversial upon its (anonymous) publication, for on the face of it, it appeared to condone and glorify suicide. Suicide is considered sinful by Christian doctrine: suicides were denied Christian burial with the bodies often mistreated and dishonoured in various ways; in corollary, the deceased's property and possessions were often confiscated by the Church. However, Goethe explained his use of \"Werther\" in his autobiography. He said he \"turned reality into poetry but his friends thought poetry should be turned into reality and the poem imitated.\" He was against this reading of poetry. Epistolary novels were common during this time, letter-writing being a primary mode of communication. What set Goethe's book apart from other such novels was its expression of unbridled longing for a joy beyond possibility, its sense of defiant rebellion against authority, and of principal importance, its total subjectivity: qualities that trailblazed the Romantic movement.\n\nThe next work, his epic closet drama \"Faust\", was completed in stages. The first part was published in 1808 and created a sensation. Goethe finished \"Faust Part Two\" in the year of his death, and the work was published posthumously. Goethe's original draft of a Faust play, which probably dates from 1773–74, and is now known as the \"Urfaust\", was also published after his death.\nThe first operatic version of Goethe's Faust, by Louis Spohr, appeared in 1814. The work subsequently inspired operas and oratorios by Schumann, Berlioz, Gounod, Boito, Busoni, and Schnittke as well as symphonic works by Liszt, Wagner, and Mahler. Faust became the ur-myth of many figures in the 19th century. Later, a facet of its plot, i.e., of selling one's soul to the devil for power over the physical world, took on increasing literary importance and became a view of the victory of technology and of industrialism, along with its dubious human expenses. In 1919, the world premiere complete production of Faust was staged at the Goetheanum.\n\nGoethe's poetic work served as a model for an entire movement in German poetry termed \"Innerlichkeit\" (\"introversion\") and represented by, for example, Heine. Goethe's words inspired a number of compositions by, among others, Mozart, Beethoven (who idolised Goethe), Schubert, Berlioz and Wolf. Perhaps the single most influential piece is \"Mignon's Song\" which opens with one of the most famous lines in German poetry, an allusion to Italy: \"\"\"?\" (\"Do you know the land where the lemon trees bloom?\").\n\nHe is also widely quoted. Epigrams such as \"Against criticism a man can neither protest nor defend himself; he must act in spite of it, and then it will gradually yield to him\", \"Divide and rule, a sound motto; unite and lead, a better one\", and \"Enjoy when you can, and endure when you must\", are still in usage or are often paraphrased. Lines from \"Faust\", such as \", \", or \"\" have entered everyday German usage.\n\nSome well-known quotations are often incorrectly attributed to Goethe. These include Hippocrates' \"Art is long, life is short\", which is echoed in Goethe's \"Faust\" and \"Wilhelm Meister's Apprenticeship\".\n\nGoethe overcame emotional turmoil, relational conflicts and mood swings through self-reflection, political and scientific work, and writing. His striving to come to terms with adverse life events began with the death of his brother Hermann Jakob when Johann Wolfgang was ten years old and continued as he met emotional crises in his adolescence: \"And thus began that habit from which I could not break away my whole life through – the habit of turning into an image, into a poem, whatever delighted or troubled, or otherwise occupied me, and thus of coming to some definite conclusion with regard to it, so that I might both rectify my conceptions of external things and satisfy my inner cravings. To no one was the faculty for so doing more necessary than to me, for by nature I was constantly carried from one extreme to the other\".\n\nAlthough his literary work has attracted the greatest amount of interest, Goethe was also keenly involved in studies of natural science. He wrote several works on morphology, and colour theory. Goethe also had the largest private collection of minerals in all of Europe. By the time of his death, in order to gain a comprehensive view in geology, he had collected 17,800 rock samples.\n\nHis focus on morphology and what was later called homology influenced 19th century naturalists, although his ideas of transformation were about the continuous metamorphosis of living things and did not relate to contemporary ideas of \"transformisme\" or transmutation of species. Homology, or as Étienne Geoffroy Saint-Hilaire called it \"analogie\", was used by Charles Darwin as strong evidence of common descent and of laws of variation. Goethe's studies (notably with an elephant's skull lent to him by Samuel Thomas von Soemmerring) led him to independently discover the human intermaxillary bone, also known as \"Goethe's bone\", in 1784, which Broussonet (1779) and Vicq d'Azyr (1780) had (using different methods) identified several years earlier. While not the only one in his time to question the prevailing view that this bone did not exist in humans, Goethe, who believed ancient anatomists had known about this bone, was the first to prove its existence in all mammals. The elephant's skull that led Goethe to this discovery, and was subsequently named the Goethe Elephant, still exists and is displayed in the Ottoneum in Kassel, Germany.\n\nDuring his Italian journey, Goethe formulated a theory of plant metamorphosis in which the archetypal form of the plant is to be found in the \"leaf\" – he writes, \"from top to bottom a plant is all leaf, united so inseparably with the future bud that one cannot be imagined without the other\". In 1790, he published his \"Metamorphosis of Plants\". As one of the many precursors in the history of evolutionary thought, Goethe wrote in \"Story of My Botanical Studies\" (1831):\n\nGoethe's botanical theories were partly based on his gardening in Weimar.\n\nGoethe also popularized the Goethe barometer using a principle established by Torricelli. According to Hegel, \"Goethe has occupied himself a good deal with meteorology; barometer readings interested him particularly... What he says is important: the main thing is that he gives a comparative table of barometric readings during the whole month of December 1822, at Weimar, Jena, London, Boston, Vienna, Töpel... He claims to deduce from it that the barometric level varies in the same proportion not only in each zone but that it has the same variation, too, at different altitudes above sea-level\".\n\nIn 1810, Goethe published his \"Theory of Colours\", which he considered his most important work. In it, he contentiously characterized colour as arising from the dynamic interplay of light and darkness through the mediation of a turbid medium. In 1816, Schopenhauer went on to develop his own theory in \"On Vision and Colours\" based on the observations supplied in Goethe's book. After being translated into English by Charles Eastlake in 1840, his theory became widely adopted by the art world, most notably J. M. W. Turner. Goethe's work also inspired the philosopher Ludwig Wittgenstein, to write his \"Remarks on Colour\". Goethe was vehemently opposed to Newton's analytic treatment of colour, engaging instead in compiling a comprehensive \"rational description\" of a wide variety of colour phenomena. Although the accuracy of Goethe's observations does not admit a great deal of criticism, his aesthetic approach did not lend itself to the demands of analytic and mathematical analysis used ubiquitously in modern Science. Goethe was, however, the first to systematically study the physiological effects of colour, and his observations on the effect of opposed colours led him to a symmetric arrangement of his colour wheel, 'for the colours diametrically opposed to each other... are those which reciprocally evoke each other in the eye. (Goethe, \"Theory of Colours\", 1810). In this, he anticipated Ewald Hering's opponent colour theory (1872).\n\nGoethe outlines his method in the essay \"The experiment as mediator between subject and object\" (1772). In the Kurschner edition of Goethe's works, the science editor, Rudolf Steiner, presents Goethe's approach to science as phenomenological. Steiner elaborated on that in the books \"The Theory of Knowledge Implicit in Goethe's World-Conception\" and \"Goethe's World View\", in which he characterizes intuition as the instrument by which one grasps Goethe's biological archetype—\"The Typus\".\n\nNovalis, himself a geologist and mining engineer, expressed the opinion that Goethe was the first physicist of his time and 'epoch-making in the history of physics', writing that Goethe's studies of light, of the metamorphosis of plants and of insects were indications and proofs 'that the perfect educational lecture belongs in the artist's sphere of work'; and that Goethe would be surpassed 'but only in the way in which the ancients can be surpassed, in inner content and force, in variety and depth—as an artist actually not, or only very little, for his rightness and intensity are perhaps already more exemplary than it would seem'.\nMany of Goethe's works, especially \"Faust\", the \"Roman Elegies\", and the \"Venetian Epigrams\", depict erotic passions and acts. For instance, in \"Faust\", the first use of Faust's power after signing a contract with the devil is to seduce a teenage girl. Some of the \"Venetian Epigrams\" were held back from publication due to their sexual content. Goethe clearly saw human sexuality as a topic worthy of poetic and artistic depiction, an idea that was uncommon in a time when the private nature of sexuality was rigorously normative.\n\nThough in his novel Wilhelm Meister's Apprenticeship, Goethe described the beauty of the male body, he was attracted to women, starting with his first love \"Gretchen\" when he was 14 and ending with Ulrike von Levetzow when he was 73. \n\nIn a conversation on April 7 1830 Goethe stated that pederasty is an \"aberration\" that easily leads to \"animal, roughly material\" behavior. He continued, \"Pederasty is as old as humanity itself, and one can therefore say, that it resides in nature, even if it proceeds against nature...What culture has won from nature will not be surrendered or given up at any price.\" On another occasion he wrote, somewhat ambiguously: \"I like boys a lot, but the girls are even nicer. If I tire of her as a girl, she'll play the boy for me as well\". \nGoethe was a freethinker who believed that one could be inwardly Christian without following any of the Christian churches, many of whose central teachings he firmly opposed, sharply distinguishing between Christ and the tenets of Christian theology, and criticizing its history as a \"hodgepodge of fallacy and violence\". His own descriptions of his relationship to the Christian faith and even to the Church varied widely and have been interpreted even more widely, so that while Goethe's secretary Eckermann portrayed him as enthusiastic about Christianity, Jesus, Martin Luther, and the Protestant Reformation, even calling Christianity the \"ultimate religion,\" on one occasion Goethe described himself as \"not anti-Christian, nor un-Christian, but most decidedly non-Christian,\" and in his Venetian Epigram 66, Goethe listed the symbol of the cross among the four things that he most disliked. According to Nietzsche, Goethe had \"a kind of almost \"joyous\" and \"trusting fatalism\"\" that has \"faith that only in the totality everything redeems itself and appears good and justified.\"\n\nBorn into a Lutheran family, Goethe's early faith was shaken by news of such events as the 1755 Lisbon earthquake and the Seven Years' War. His later spiritual perspective incorporated elements of pantheism (heavily influenced by Spinoza), humanism, and various elements of Western esotericism, as seen most vividly in Part II of \"Faust\". A year before his death, in a letter to Sulpiz Boisserée, Goethe wrote that he had the feeling that all his life he had been aspiring to qualify as one of the Hypsistarians, an ancient Jewish-pagan sect of the Black Sea region who, in his understanding, sought to reverence, as being close to the Godhead, what came to their knowledge of the best and most perfect. Goethe's unorthodox religious beliefs led him to be called \"the great heathen\" and provoked distrust among the authorities of his time, who opposed the creation of a Goethe monument on account of his offensive religious creed. August Wilhelm Schlegel considered Goethe \"a heathen who converted to Islam.\"\n\nPolitically, Goethe described himself as a \"moderate liberal\", expressing sympathy for the liberalism of Étienne Dumont. At the time of the French Revolution, he thought the enthusiasm of the students and professors to be a perversion of their energy and remained skeptical of the ability of the masses to govern. Goethe sympathized with the American Revolution and later wrote a poem in which he declared \"America, you're better off than our continent, the old.\" He did not join in the anti-Napoleonic mood of 1812, and he distrusted the strident nationalism which started to be expressed. The medievalism of the Heidelberg Romantics was also repellent to Goethe's eighteenth-century ideal of a supra-national culture.\n\nGoethe was a Freemason, joining the lodge Amalia in Weimar in 1780, and frequently alluded to Masonic themes of universal brotherhood in his work. Although often requested to write poems arousing nationalist passions, Goethe would always decline. In old age, he explained why this was so to Eckermann:\n\nGoethe had a great effect on the nineteenth century. In many respects, he was the originator of many ideas which later became widespread. He produced volumes of poetry, essays, criticism, a theory of colours and early work on evolution and linguistics. He was fascinated by mineralogy, and the mineral goethite (iron oxide) is named after him. His non-fiction writings, most of which are philosophic and aphoristic in nature, spurred the development of many thinkers, including Georg Wilhelm Friedrich Hegel, Schopenhauer, Søren Kierkegaard, Friedrich Nietzsche, Ernst Cassirer, and Carl Jung. Along with Schiller, he was one of the leading figures of Weimar Classicism. Schopenhauer cited Goethe's novel \"Wilhelm Meister's Apprenticeship\" as one of the four greatest novels ever written, along with \"Tristram Shandy\", \"La Nouvelle Héloïse\" and \"Don Quixote\".\n\nGoethe embodied many of the contending strands in art over the next century: his work could be lushly emotional, and rigorously formal, brief and epigrammatic, and epic. He would argue that Classicism was the means of controlling art, and that Romanticism was a sickness, even as he penned poetry rich in memorable images, and rewrote the formal rules of German poetry. His poetry was set to music by almost every major Austrian and German composer from Mozart to Mahler, and his influence would spread to French drama and opera as well. Beethoven declared that a \"Faust\" Symphony would be the greatest thing for art. Liszt and Mahler both created symphonies in whole or in large part inspired by this seminal work, which would give the 19th century one of its most paradigmatic figures: Doctor Faustus.\nThe Faust tragedy/drama, often called ' (the drama of the Germans), written in two parts published decades apart, would stand as his most characteristic and famous artistic creation. Followers of the twentieth century esotericist Rudolf Steiner built a theatre named the Goetheanum after him—where festival performances of Faust are still performed.\n\nGoethe was also a cultural force. During his first meeting with Napoleon in 1808, the latter famously remarked: \"Vous étes un homme (You are a man)!\" The two discussed politics, the writings of Voltaire, and Goethe's \"Sorrows of Young Werther\", which Napoleon had read seven times and ranked among his favorites. Goethe came away from the meeting deeply impressed with Napoleon's enlightened intellect and his efforts to build an alternative to the corrupt old regime. Goethe always spoke of Napoleon with the greatest respect, confessing that \"nothing higher and more pleasing could have happened to me in all my life\" than to have met Napoleon in person. \n\nGermaine de Staël, in \"De L'Allemagne\" (1813), presented German Classicism and Romanticism as a potential source of spiritual authority for Europe, and identified Goethe as a living classic. She praised Goethe as possessing \"the chief characteristics of the German genius\" and uniting \"all that distinguishes the German mind.\" Staël's portrayal helped elevate Goethe over his more famous German contemporaries and transformed him into a European cultural hero. Goethe met with her and her partner Benjamin Constant, with whom he shared a mutual admiration.\n\nIn Victorian England, Goethe exerted a profound influence on George Eliot, whose partner George Henry Lewes wrote a \"Life of Goethe\". Eliot presented Goethe as \"eminently the man who helps us to rise to a lofty point of observation\" and praised his \"large tolerance\", which \"quietly follows the stream of fact and of life\" without passing moral judgments. Matthew Arnold found in Goethe the \"Physician of the Iron Age\" and \"the clearest, the largest, the most helpful thinker of modern times\" with a \"large, liberal view of life.\"\nIt was to a considerable degree due to Goethe's reputation that the city of Weimar was chosen in 1919 as the venue for the national assembly, convened to draft a new constitution for what would become known as Germany's Weimar Republic. Goethe became a key reference for Thomas Mann in his speeches and essays defending the republic. He emphasized Goethe's \"cultural and self-developing individualism\", humanism, and cosmopolitanism.\n\nThe Federal Republic of Germany's cultural institution, the Goethe-Institut is named after him, and promotes the study of German abroad and fosters knowledge about Germany by providing information on its culture, society and politics.\n\nThe literary estate of Goethe in the Goethe and Schiller Archives was inscribed on UNESCO's Memory of the World Register in 2001 in recognition of its historical significance.\n\nGoethe's influence was dramatic because he understood that there was a transition in European sensibilities, an increasing focus on sense, the indescribable, and the emotional. This is not to say that he was emotionalistic or excessive; on the contrary, he lauded personal restraint and felt that excess was a disease: \"There is nothing worse than imagination without taste\". Goethe praised Francis Bacon for his advocacy of science based on experiment and his forceful revolution in thought as one of the greatest strides forward in modern science. However, he was critical of Bacon's inductive method and approach based on pure classification. He said in \"Scientific Studies\":\n\nGoethe's scientific and aesthetic ideas have much in common with Denis Diderot, whose work he translated and studied. Both Diderot and Goethe exhibited a repugnance towards the mathematical interpretation of nature; both perceived the universe as dynamic and in constant flux; both saw \"art and science as compatible disciplines linked by common imaginative processes\"; and both grasped \"the unconscious impulses underlying mental creation in all forms.\" Goethe's \"Naturanschauer\" is in many ways a sequel to Diderot's \"interprète de la nature\". \n\nHis views make him, along with Adam Smith, Thomas Jefferson, and Ludwig van Beethoven, a figure in two worlds: on the one hand, devoted to the sense of taste, order, and finely crafted detail, which is the hallmark of the artistic sense of the Age of Reason and the neo-classical period of architecture; on the other, seeking a personal, intuitive, and personalized form of expression and society, firmly supporting the idea of self-regulating and organic systems. George Henry Lewes celebrated Goethe's revolutionary understanding of the organism.\n\nThinkers such as Ralph Waldo Emerson would take up many similar ideas in the 1800s. Goethe's ideas on evolution would frame the question that Darwin and Wallace would approach within the scientific paradigm. The Serbian inventor and electrical engineer Nikola Tesla was heavily influenced by Goethe's Faust, his favorite poem, and had actually memorized the entire text. It was while reciting a certain verse that he was struck with the epiphany that would lead to the idea of the rotating magnetic field and ultimately, alternating current.\n\n\n\n\n\n\n\n"}
{"id": "2052832", "url": "https://en.wikipedia.org/wiki?curid=2052832", "title": "Kadam virus", "text": "Kadam virus\n\nThe Kadam virus (or KAD, strain MP6640) is a tick-borne Flavivirus.\n\nThe virus was first isolated by the Uganda Virus Research Institute in Entebbe, Uganda after samples were taken from cattle in Karamoja in 1967. The viruses were usually only found from Rhipicephalus and Amblyomma ticks around Kenya and Uganda infecting cattle and humans.\n\nAround the beginning of the 1980s the Kadam virus was found to be spread in Saudi Arabia by Hyalomma ticks when found on a dead camel at Wadi Thamamah in Riyadh.\n"}
{"id": "37933612", "url": "https://en.wikipedia.org/wiki?curid=37933612", "title": "List of Galliformes by population", "text": "List of Galliformes by population\n\nThis is a list of Galliformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Galliformes have had their numbers quantified.\n"}
{"id": "43767078", "url": "https://en.wikipedia.org/wiki?curid=43767078", "title": "List of fossiliferous stratigraphic units in Eritrea", "text": "List of fossiliferous stratigraphic units in Eritrea\n\nThis is a list of fossiliferous stratigraphic units in Eritrea.\n\n\n"}
{"id": "11587676", "url": "https://en.wikipedia.org/wiki?curid=11587676", "title": "List of names on Mount Kenya", "text": "List of names on Mount Kenya\n\nMount Kenya () is the second highest mountain in Africa and the highest mountain in Kenya, after which the country is named. It lies just south of the equator and currently has eleven small glaciers. Various expeditions reached it in the following years. It was first climbed in 1899 by Halford Mackinder. The mountain became a national park in 1949, played a key role in the Mau Mau events in the 1950s, and became a UNESCO World Heritage Site in 1997. It is climbed and walked up by up to 15,000 tourists every year.\n\nMount Kenya received its current name by European missionaries who, wrote the name as 'Kenya' from the Akamba word 'kiinyaa'. The first missionaries, Johann Ludwig Krapf, and Johannes Rebmann, were led into Kenyas interior by Akamba long distance traders. So when they asked the name of the mountain, they were given the name 'kiima kya kenia'. 'kenia' in Kamba means to glitter, or to shine, hence the Akamba people referred to it as the mountain that glitters, or the shining mountain. So these missionaries recorded it as Mt. Kenya, a Kamba word, and the country was then later named after this mountain.\nOther ethnic groups living around the mountain such as the Agikuyu called this mountain 'Kirinyaga'; 'nyaga' in Kikuyu means white patches, hence it is also the Kikuyu word for Ostrich, and 'kiri' means with: so in essence they called it the mountain with ostriches. The Maasai call it \"Ol Donyo Eibor\" or \"Ol Donyo Egere\", which mean \"the White mountain\" or \"the speckled mountain\" respectively.\n\nKrapf was staying in a Wakamba village when he first saw the mountain.\nKrapf, however, recorded the name as both \"Kenia\" and \"Kegnia\". According to some sources, this is a corruption of the Wakamba \"Kiinyaa\".\nOthers however say that this was on the contrary a very precise notation of a native word pronounced \"\".\nNevertheless, the name was usually pronounced in English.\n\nIt is important to note that at the time this referred to the mountain without having to include \"mountain\" in the name. The current name \"Mount Kenya\" was used by some as early as 1894, but this was not a regular occurrence until 1920 when Kenya Colony was established. Before 1920 the area now known as Kenya was known as the British East Africa Protectorate and so there was no need to mention \"mount\" when referring to the mountain. \"Mount Kenya\" was not the only English name for the mountain as shown in Dutton's 1929 book\" Kenya Mountain.\" By the 1930s \"Kenya\" was becoming the dominant spelling, but \"Kenia\" was occasionally used.\nAt this time both were still pronounced \"\" in English.\n\nKenya achieved independence in 1963, and Jomo Kenyatta was elected as the first president.\nHe had previously assumed this name to reflect his commitment to freeing his country and his pronunciation of his name resulted in the pronunciation of Kenya in English changing back to an approximation of the original native pronunciation, the current '. So the country was named after the colony, which in turn was named after the mountain as it is a very significant landmark. To distinguish easily between the country and the mountain, the mountain became known as\" Mount Kenya \" with the current pronunciation '. Mount Kenya is featured on the coat of arms of Kenya.\n\nThe peaks of Mount Kenya have been given names from three different sources. Firstly, several Maasai chieftains have been commemorated, with names such as Batian, Nelion and Lenana. These names were suggested by Mackinder, on the suggestion of Sidney Langford Hinde, who was the resident officer in Maasailand at the time of Mackinder's expedition. They commemorate Mbatian, a Maasai Laibon (Medicine Man), Nelieng, his brother, and Lenana and Sendeyo, his sons. Terere is named after another Maasai headman.\n\nThe second type of names that were given to peaks are after climbers and explorers. Some examples of this are Shipton, Sommerfelt, Tilman, Dutton and Arthur. Shipton made the first ascent of Nelion, and Sommerfelt accompanied Shipton on the second ascent. Tilman made many first ascents of peaks with Shipton in 1930. Dutton and Arthur explored the mountain between 1910 and 1930. Arthur Firmin, who made many first ascents, has been remembered in Firmin's Col. Humphrey Slade, of Pt Slade, explored the moorland areas of the mountain in the 1930s, and possibly made the first ascent of Sendeyo.\n\nThe remaining names are after well-known Kenyan personalities, with the exception of John and Peter, which were named by the missionary Arthur after two disciples. Pigott was the Acting Administrator of Imperial British East Africa at the time of Gregory's expedition, and there is a group of four peaks to the east of the main peaks named after governors of Kenya and early settlers; Coryndon, Grigg, Delamere and McMillan.\n\nThe majority of the names were given by Melhuish and Dutton, with the exception of the Maasai names and Peter and John. Pt Thomson is not named after Joseph Thomson, who confirmed the mountain's existence, but after another J Thomson who was an official Royal Geographical Society photographer.\n\nRivers starting above are listed clockwise around the mountain from the north. Tributaries rivers which include the original name in their names are not listed, for example \"Liki North\" and \"Liki South\". The rivers on Mount Kenya have been named after the villages on the slopes of the mountain that they flow close to. The Thuchi River is the district boundary between Meru and Embu. Mount Kenya is a major water tower for the Tana river which in 1988 supplied 80% of Kenya's electricity using a series of seven hydroelectric power stations and dams.\n"}
{"id": "31706469", "url": "https://en.wikipedia.org/wiki?curid=31706469", "title": "List of people whose names are used in chemical element names", "text": "List of people whose names are used in chemical element names\n\nBelow is the list of people whose names are used in chemical element names. Of the 118 chemical elements, 19 are connected with the names of 20 people. 15 elements were named to honor 16 scientists. Four other elements have indirect connection to the names of non-scientists. On top of this, a 21st person, a 17th scientist, has an implied connection to a 20th element. Only gadolinium and samarium occur in nature (along with gallium). The rest are synthetic.\n\nThe following 19 elements are connected to the names of people. Seaborg and Oganessian were the only two who were alive at the time of being honored with having elements named after them. The four non-scientists in this table are connected with elements that were not named to honor the individual directly, but rather were named for a place or thing which in turn had been named for these people. Samarium was named for the mineral samarskite from which it was isolated. Americium, berkelium and livermorium were named after places that had been named for them. The cities of Berkeley, California and Livermore, California are the locations of the University of California Radiation Laboratory and Lawrence Livermore National Laboratory, respectively.\n\nOther element names have been proposed but failed to gain official international recognition. These include columbium (Cb), hahnium (Ha), joliotium (Jl), and kurchatovium (Ku), names connected to Christopher Columbus, Otto Hahn, Irène Joliot-Curie, and Igor Kurchatov (more at the article on element naming controversies).\n\nAlso, mythological entities have had a significant impact on the naming of elements. Helium, titanium, selenium, palladium, promethium, cerium, europium, mercury, thorium, uranium, neptunium and plutonium are all given names connected to mythological deities. With these five, that connection is indirect:\n\nTitanium is unique in the list above in that it refers to a group of deities rather than any particular individual. So Helios, Selene, Pallas, and Prometheus actually have two elements named in their honor.\n\nAnd for elements given a name connected with a group, there is also xenon, named for the Greek word \"ξένον\" (xenon), neuter singular form of \"ξένος\" (xenos), meaning 'foreign(er)', 'strange(r)', or 'guest'.\nIts discoverer William Ramsay intended this name to be an indication of the qualities of this element in analogy to the generic group of people.\n\n"}
{"id": "12586603", "url": "https://en.wikipedia.org/wiki?curid=12586603", "title": "List of the most intense tropical cyclones", "text": "List of the most intense tropical cyclones\n\nTropical cyclone intensity is a complex topic. Winds are often used to measure intensity as they commonly cause notable impacts over large areas, and most popular tropical cyclone scales are organized around sustained wind speeds. However, variations in the averaging period of winds in different basins make inter-comparison difficult. In addition, other impacts like rainfall, storm surge, area of wind damages, and tornadoes can vary significantly in storms with similar wind speeds. Pressure is often used to compare tropical cyclones because the measurements are easier to measure and are consistent. Tropical cyclones can attain some of the lowest pressures over large areas on Earth. However, although there is a strong connection between lowered pressures and higher wind speeds, storms with the lowest pressures may not have the highest wind speeds, as each storm's relationship between wind and pressure is slightly different.\n\nIn the most recent and reliable records, most tropical cyclones which attained a pressure of 900 hPa (mbar) (26.56 inHg) or less occurred in the Western North Pacific Ocean. The strongest tropical cyclone recorded worldwide, as measured by minimum central pressure, was Typhoon Tip, which reached a pressure of 870 hPa (25.69 inHg) on October 12, 1979. The following list is subdivided by basins. Data listed are provided by the official Regional Specialized Meteorological Centre, unless otherwise noted. On October 23, 2015, Hurricane Patricia attained the strongest 1-minute sustained winds on record at 215 mph (345 km/h).\n\nThe most intense storm in the North Atlantic by lowest pressure was Hurricane Wilma. The strongest storm by 1-minute sustained winds was Hurricane Allen.\n\nStorms which reached a minimum central pressure of or less are listed. Storm information has been compiled back to 1851, though measurements were rarer until aircraft reconnaissance started in the 1940s, and inexact estimates were still predominant until dropsondes were implemented in the 1970s. \n\nSee List of Category 5 Atlantic Hurricanes for additional information on strong storms in the Atlantic basin.\n\nSee Notable non-tropical pressures over the North Atlantic for intense extratropical low pressure values over the North Atlantic.\n\nThe most intense storm in the Eastern Pacific Ocean by both sustained winds and central pressure was Hurricane Patricia. Its sustained winds of 345 km/h (215 mph) are also the highest on record globally.\n\nStorms with a minimum central pressure of 925 hPa (27.32 inHg) or less are listed. Storm information was less reliably documented and recorded before 1949, and most storms since are only estimated because landfalls (and related reconnaissance) are less common in this basin.\n\nSee Category 5 Pacific Hurricanes for a full list of category 5 hurricanes in this basin.\n\nThe most intense storm by lowest pressure and peak 10-minute sustained winds was Typhoon Tip, which was also the most intense tropical cyclone ever recorded.\n\nStorms with a minimum pressure of 900 hPa (26.58 inHg) or less are listed. Storm information was less reliably documented and recorded before 1950.\n\nThe strongest tropical cyclone recorded in the North Indian Ocean is the 1999 Odisha cyclone, with 3-minute sustained winds of 260 km/h (160 mph) and a minimum pressure of .\n\nStorms with an intensity of or less are listed. Storm information was less reliably documented and recorded before 1971.\n\nThe most intense tropical cyclone in the South-West Indian Ocean is Cyclone Gafilo. By 10-minute sustained wind speed, the strongest tropical cyclone in the South-West Indian Ocean is Cyclone Fantala.\n\nStorms with an intensity of 920 hPa (27.17 inHg) or less are listed. Storm information was less reliably documented and recorded before 1985.\n\nThe most intense tropical cyclones in the Australian Region are Cyclone Gwenda and Cyclone Inigo. By 10-minute sustained wind speed, the strongest are Cyclone Orson and Cyclone Monica.\n\nStorms with an intensity of 920 hPa (27.17 inHg) or less are listed. Storm information was less reliably documented and recorded before 1985.\nA total of 16 cyclones are listed down below reaching/surpassing that intensity, which most of them occurred during El Niño seasons. Tropical cyclones that have been recorded since the start of the 1969–70 Tropical Cyclone year and have reached their peak intensity to the west of 160E are included in the list. The most intense tropical cyclone in the south Pacific, Cyclone Winston of 2016, is also the most intense storm in the Southern Hemisphere.\n\nStorms with an intensity of 920 hPa (27.17 inHg) or less are listed. Storm information was less reliably documented and recorded before 1985.\n\nUntil recently, it was not known that tropical cyclones could exist in the southern Atlantic. However, Hurricane Catarina in 2004, to date the only hurricane in the south Atlantic, brought additional review. A subsequent study found that there was an average of 1-2 subtropical or tropical cyclones per year in the Southern Atlantic in recent decades.\nNo official database of South Atlantic cyclones exists, but a partial list of tropical and subtropical systems with intensity below 1000 hPa (29.53 inHg) is listed.\n\n\n\n"}
{"id": "72687", "url": "https://en.wikipedia.org/wiki?curid=72687", "title": "List of waterfalls", "text": "List of waterfalls\n\nThis is a list of notable waterfalls of the world sorted by continent, then country, then province, state or territory. A waterfall is considered \"notable\" if it has an existing article specifically for it on Wikipedia, and the height or width is a minimum of , or the falls have some historical significance based on multiple reliable references.\n\nSee additional lists of waterfalls by height, flow rate and type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n (listed after \"Australia\" in the \"Oceania\" section below)\n\n\n\n\n\n\n\n\nSee also \n\n\n\n\n\n\n\n\n\nSee also \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14923774", "url": "https://en.wikipedia.org/wiki?curid=14923774", "title": "Ludwig Roth", "text": "Ludwig Roth\n\nLudwig Roth (June 10, 1909 – November 1, 1967) was the Aerospace engineer who was the head of the Peenemünde Future Projects Office which designed the Wasserfall and created advanced rockets designs such as the A9/A10 ICBM.\n\nRoth arrived in New York under Operation Paperclip on November 16, 1945 via the SS \"Argentina\" and served at Fort Bliss and Huntsville, Alabama. He and his family relocated to Palos Verdes, California. His son Axel Roth\nwent on to work for NASA as an engineer, and ended his career as Associate Director of Marshall Space Flight Center. His son Volker worked for Boeing as Space Lab Design Manager. His grandson Karl Roth currently works for COLSA Corporation supporting International Space Station Payload Ground System Integration.\n\n"}
{"id": "35461390", "url": "https://en.wikipedia.org/wiki?curid=35461390", "title": "Magneto-inertial fusion", "text": "Magneto-inertial fusion\n\nMagneto-inertial fusion (MIF) describes a class of fusion devices which combine aspects of magnetic confinement fusion and inertial confinement fusion in an attempt to lower the cost of fusion devices. MIF uses magnetic fields to confine an initial warm, low-density plasma, then compresses that plasma to fusion conditions using an impulsive driver or \"liner.\"\n\nMagneto-inertial fusion approaches differ in the degree of magnetic organization present in the initial target, as well as the nature and speed of the imploding liner. Laser, solid, liquid and plasma liners have all been proposed.\n\nMagneto-inertial fusion begins with a warm dense plasma target containing a magnetic field. Plasma's conductivity prevents it from crossing magnetic field lines. As a result, compressing the target amplifies the magnetic field.\n\nThe starships in Mike Kupari's novel \"Her Brother's Keeper\" are propelled in part by magneto-inertial fusion rockets.\n\n"}
{"id": "53848855", "url": "https://en.wikipedia.org/wiki?curid=53848855", "title": "Maury C. Goodman", "text": "Maury C. Goodman\n\nMaury C. Goodman from the Argonne National Laboratory, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Particles and Fields in 2008, for \"pioneering contributions to experimental neutrino physics, especially the initiation of worldwide programs of accelerator long-baseline neutrino oscillation experiments and of the new generation of reactor experiments to measure the theta-13 neutrino mixing parameter.\"\n"}
{"id": "28149071", "url": "https://en.wikipedia.org/wiki?curid=28149071", "title": "Maxwell's thermodynamic surface", "text": "Maxwell's thermodynamic surface\n\nMaxwell’s thermodynamic surface is an 1874 sculpture made by Scottish physicist James Clerk Maxwell (1831–1879). This model provides a three-dimensional space of the various states of a fictitious substance with water-like properties. This plot has coordinates volume (x), entropy (y), and energy (z). It was based on the American scientist Josiah Willard Gibbs’ graphical thermodynamics papers of 1873. The model, in Maxwell's words, allowed \"the principal features of known substances [to] be represented on a convenient scale.\"\n\nGibbs' papers defined what Gibbs called the \"thermodynamic surface,\" which expressed the relationship between the volume, entropy, and energy of a substance at different temperatures and pressures. However, Gibbs did not include any diagrams of this surface. After receiving reprints of Gibbs' papers, Maxwell recognized the insight afforded by Gibbs' new point of view and set about constructing physical three-dimensional models of the surface. This reflected Maxwell's talent as a strong visual thinker and prefigured modern scientific visualization techniques.\n\nMaxwell sculpted the original model in clay and made three plaster casts of the clay model, sending one to Gibbs as a gift, keeping the other two in his laboratory at Cambridge University. Maxwell's copy is on display at the Cavendish Laboratory of Cambridge University, while Gibbs' copy is on display at the Sloane Physics Laboratory of Yale University, where Gibbs held a professorship. A number of historic photographs were taken of these plaster casts during the middle of the twentieth century – including one by James Pickands II, published in 1942 – and these photographs exposed a wider range of people to Maxwell's visualization approach.\n\nMaxwell drew lines of equal pressure (isopiestics) and of equal temperature (isothermals) on his plaster cast by placing it in the sunlight, and \"tracing the curve when the rays just grazed the surface.\" He sent sketches of these lines to a number of colleagues. For example, his letter to Thomas Andrews of 15 July 1875 included sketches of these lines. Maxwell provided a more detailed explanation and a clearer drawing of the lines in the revised version of his book \"Theory of Heat\", and a version of this drawing appeared on a 2005 US postage stamp in honour of Gibbs.\n\nAs well as being on display in two countries, Maxwell's model lives on in the literature of thermodynamics, and books on the subject often mention it, though not always with complete historical accuracy. For example, the thermodynamic surface represented by the sculpture is often reported to be that of water, contrary to Maxwell's own statement.\n\nMaxwell's model was not the first plaster model of a thermodynamic surface: in 1871, even before Gibbs' papers, James Thomson had constructed a plaster pressure-volume-temperature plot, based on data for carbon dioxide collected by Thomas Andrews.\n\nAround 1900, the Dutch scientist Heike Kamerlingh Onnes, together with his student Johannes Petrus Kuenen and his assistant Zaalberg van Zelst, continued Maxwell's work by constructing their own plaster thermodynamic surface models. These models were based on accurate experimental data obtained in their laboratory, and were accompanied by specialised tools for drawing the lines of equal pressure.\n\n"}
{"id": "82836", "url": "https://en.wikipedia.org/wiki?curid=82836", "title": "Mormo", "text": "Mormo\n\nMormo (, \"Mormō\") was a female spirit in Greek folklore, whose name was invoked by mothers and nurses to frighten children to keep them from misbehaving.\n\nThe term mormolyce (; pl. \"mormolykeia\" ), also spelt mormolyceum ( \"mormolukeîon\"), is considered equivalent.\n\nThe name \"mormo\" has the plural form \"mormones\" which means \"fearful ones\" or \"hideous one(s)\", and is related to an array of words that signify \"fright\". \n\nThe variant \"mormolyce\" translates to \"terrible wolves\", with the stem \"-lykeios\" meaning \"of a wolf\".\n\nThe original Mormo was a woman of Corinth, who ate her children then flew out; according to an account only attested in a single source. Mormolyca (as the name appears in Doric Greek: ) is designated as the wetnurse () of Acheron by Sophron ( 430 BC).\n\nMormo or Moromolyce has been described as a female specter, phantom, or ghost by modern commentators. A mormolyce is one of several names given to the female \"phasma\" (phantom) in Philostratus's \"Life of Apollonius of Tyana\".\n\nMormo is glossed as equivalent to Lamia and \"mormolykeion\", considered to be frightening beings, in the \"Suda\", a lexicon of the Byzantine Periods. Mombro () or Mormo are a bugbear (), the \"Suda\" also says.\n\n\"Mormo\" and \"Gello\" were also aliases for Lamia according to one scholiast, who also claimed she was queen of the Laestrygonians, the race of man-eating giants.\n\nThe name of \"Mormo\" or the synonymous \"Mormolyceion\" was used by the Greeks as a bugbear or bogey word to frighten children.\n\nSome of its instances are found in Aristophanes. Mormo as an object of fear for infants was even recorded in the \"Alexiad\" written by a Byzantine princess around the First Crusade.\n\nA mormo or a lamia may also be associated with the empusa, a phantom sent by the goddess Hekate.\n\n\n\n\n\n\n\n"}
{"id": "33826251", "url": "https://en.wikipedia.org/wiki?curid=33826251", "title": "Neural basis of self", "text": "Neural basis of self\n\nThe neural basis of self is the idea of using modern concepts of neuroscience to describe and understand the biological processes that underlie human's perception of self-understanding. The neural basis of self is closely related to the psychology of self with a deeper foundation in neurobiology.\n\nIn order to understand how the human mind makes the human perception of self, there are different experimental techniques. One of the more common methods of determining brain areas that pertain to different mental processes is by using Functional Magnetic Resonance Imaging. fMRI data is often used to determine activation levels in portions of the brain. fMRI measures blood flow in the brain. Areas with higher blood flow as shown on fMRI scans are said to be activated. This is due to the assumption that portions of the brain receiving increased blood flow are being used more heavily during the moment of scanning.\nPositron emission tomography is another method used to study brain activity.\n\nTwo areas of the brain that are important in retrieving self-knowledge are the medial prefrontal cortex and the medial posterior parietal cortex.\nThe posterior cingulate cortex, the anterior cingulate cortex, and medial prefrontal cortex are thought to combine to provide humans with the ability to self-reflect. The insular cortex is also thought to be involved in the process of self-reference.\n\nThe sense of embodiment is critical to a person’s conception of self. Embodiment is the understanding of the physical body and its relation to oneself. The study of human embodiment currently has a large impact on the study of human cognition as a whole. The current study of embodiment suggests that sensory input and experiences impact human’s overall perception. This idea somewhat challenges previous ideas of human cognition because it challenges the idea of the human mind being innate.\n\nThere are two portions of the brain that have recently been found to have a large importance on a person’s perception of self. The temporoparietal junction, located in the cortex is one of these brain regions. The temporoparietal junction is thought to integrate sensory information. The second portion of the brain thought to be involved in perception of embodiment is the extrastriate body area. The extrastriate body area is located in the lateral occipitotemporal cortex. When people are shown images of body parts, the extrastriate body area is activated. The temporoparietal junction is involved in sensory integration processes while the extrastriate body area deals mainly with thoughts of and exposure to human body parts. It has been found that the brain responds to stimuli that involve embodiment differently from stimuli that involve localization. During task performance tests, a person’s body position (whether he or she is sitting or laying face up) affects how the extrastriate body area is activated. The temporoparietal junction, however, is not affected by a person’s particular body position. The temporoparietal junction deals with disembodied rather than embodied self-location, explaining why a person’s physical position does not affect its activation. Self-location as related to a person’s sense of embodiment is related to his or her actual location in space.\n\nThe information people remember as autobiographical memory is essential to their perception of self. These memories form the way people feel about themselves. The left dorsolateral prefrontal cortex and posterior cingulate cortex are involved in the memory of autobiographical information.\n\nMorality is an extremely important defining factor for humans. It often defines or contributes to people’s choices or actions, defining who a person is. Making moral decisions, much like other neural processes has a clear biological basis. The anterior and medial prefrontal cortex and the superior temporal sulcus are activated when people feel guilt, compassion, or embarrassment. Guilt and passion activate the mesolimbic pathway, and indignation and disgust are activated by the amygdala. There is clearly a network involved with the ideas of morality.\n\nIn order to explain how a human views him or herself, two different conceptual views of self-perception exist: the individualist and collectivistic views of self. The individualistic view of self involves people's perception of themselves as a stand-alone individual. This is thought of as a somewhat permanent perception of oneself that is unaffected by environmental and temporary cues and influences. People who view themselves in an individualistic sense describe themselves with personality traits that are permanent descriptions unrelated to particular situations. The collectivistic view of self, however, involves people's perception of themselves as members of a group or in a particular situation. The view people have of themselves in a collectivistic sense is entirely dependent on the situation they are in and the group with which they are interacting. These two ideas of self are also called self-construal styles.\nThere is neurobiological evidence supporting these two definitions of self-construal styles. fMRI data has been used to understand the biology of both the individualistic and collectivistic view of self. Certain people tend to view themselves in almost exclusively a collectivistic sense or an individualistic sense. When people have to describe themselves in a collectivistic way (as a part of a group), those who tend to view themselves collectivistically show greater fMRI activation in the medial prefrontal cortex than those who view themselves individualistically. The reverse is true when people describe themselves individualistically.\n\nThe study of the human mind in diseased states provides valuable insight into how the mind works in healthy individuals. A multitude of diseases are studied to understand altered perceptions of self and what causes these impairments.\n\nAutism is a disorder in which those affected experience impaired social interactions, communication, and behaviors. A new approach to studying autism is to focus on individuals’ perception of self rather than understanding the individual’s social interactions. A common thought is that understanding of the differences between the self and others are impaired. However, the exact biological mechanism of self-understanding in autistic children is currently unknown. It has been found that there are significant differences in brain activation in self and other situations in autistic children when compared to children who do not have autism. \nIn adults who do not have autism, during self-recognition tasks, the inferior frontal gyrus and the inferior parietal lobule in the right hemisphere are activated. Children who do not have autism show activation in these areas when performing face processing tasks for their own faces and those of others. Children with autism, however, only show activation in these areas when recognizing their own faces. The activation in the inferior frontal gyrus is less in children with autism than in those who do not have autism.\n\nThe cortical midline structure is extremely important in the understanding of the self, especially during the task of self-reflection. Many researchers believe that self-reference plays a role in the expression of psychoses. The disturbance of the individual’s self may be underlying the manifestation of these psychoses. Occurrences such as hallucinations and delusions may originate with disruptions of a person’s perception of the self. Understanding the differences in those who have psychoses and those who do not can aid in diagnoses and treatment of those diseases. Those who are prone to psychoses such as schizophrenia, when describing positive traits about themselves show increased activation in the left insula, right dorsomedial prefrontal cortex, and left ventromedial prefrontal cortex. When they use negative traits to describe themselves, those who are prone to psychoses show higher activation in the bilateral insula, anterior cingulate cortex, and right dorsomedial prefrontal cortex.\n\nSometimes after strokes patients' perception of self changes. Often after a stroke, patients report their perception of self in more negative terms than before their stroke.\nIt has been found that humans’ ideas of themselves are established early in life but that the perception can change as others ideas are combined with their own. \nThere are differences in the areas activated during self-knowledge retrieval between adults and children. This suggests a difference in self-knowledge neurobiologically due to normal aging. The prefrontal cortex and the medial posterior parietal cortex have been found to be activated when adults perform self-knowledge retrieval processes. Tests consist of presenting subjects with self-description phrases and allowing the subject to respond yes or no depending on whether or not the phrase describes him or herself. During this task, patients brains are fMRI scanned. These results can then be compared to fMRI data of the same patients when they are asked if the same phrases describe another individual, such as a well-known fictional character. The medial prefrontal cortex is activated more strongly for subjects when they are describing themselves than when they are describing others. However, children show greater medial prefrontal cortex activation than adults when performing self-knowledge retrieval tasks. Additionally, children and adults activate different specific regions in the medial prefrontal cortex. Adults activate the posterior precuneus more while children activate the anterior precuneus and the posterior cingulate.\nThe understanding of the areas of the brain most frequently activated in children and adults can also provide information about how children, adolescents, and adults view themselves differently. Older children more significantly activate the medial prefrontal cortex because they deal with introspection much less frequently than adults and adolescents. Children have decreased specificity in skills than adults, so they show greater activation during spatial tasks. This is explained by the idea that with increased expertise in a task, decreased interest in wide spatial parameters occurs. When a person is an expert, he or she is able to be more focused in his or her performance. The difference in performance between adults and children is thought to be attributable to different perceptions of the self whether it is more introspective or more concerned with the surroundings and environment.\n"}
{"id": "3296224", "url": "https://en.wikipedia.org/wiki?curid=3296224", "title": "Operating Manual for Spaceship Earth", "text": "Operating Manual for Spaceship Earth\n\nOperating Manual For Spaceship Earth is a short book by R. Buckminster Fuller, first published in 1968, following an address with a similar title given to the 50th annual convention of the American Planners Association in the Shoreham Hotel, Washington D.C., on 16 October 1967.\n\nThe book relates Earth to a spaceship flying through space. The spaceship has a finite amount of resources and cannot be resupplied.\n\nDescribes how people perceive his prognostications, and the conclusion that fairly reasonable forecasts can be made of approx 25 years.\n\nThe idea that specialization is society's vehicle, but perhaps unnatural.\n\nThe concept of the Great Pirate also emerges as the first peoples to undertake sea vessels and first sea traders.\n\nThe \"Great Pirate\" concept is explained in depth, and the source of their power is that they are the \"only\" masters of \"global\" information in a time where people are focused locally. Specifically, the \"Great Pirates\" are aware that resources are not evenly distributed around the world, so that items which are abundant in one area are scarce in another. This gives rise to trade which the \"Great Pirates\" exploit for their own advantage.\n\nPower struggles for waterways ensue, requiring people like Leonardo da Vinci and Michelangelo to design better defenses for the \"Great Pirates\". The Pirates establish governments in various areas and support leaders who will defend their trade routes.\n\nAs engineers become involved with the \"Great Pirates\" many new concepts appear, but the main one was of the Navy. As the size of the people in the \"Great Pirates\"' employment grow, training becomes a necessity, and the beginnings of schools and colleges ensue. Monarchs are encouraged to develop civil service systems to provide secure but \"specialized\" employment for their brightest subjects, which prevents them from competing with the \"Great Pirates\" in their lucrative global trading. Thus the \"Great Pirates\" guarded the advantages that their unique global perspective revealed.\n\nWorld War I emerges as the struggle between the 'out-pirates' (electronic and chemical warfare) and the 'in-pirates' (electromagnetics). This change from the visible to the invisible forced the \"Great Pirates\" to rely on experts, which causes the end of the \"Great Pirates\" (who previously had been the only ones that were truly multi-disciplined).\n\nThe public is unaware that \"Great Pirates\" have been ruling the earth, and the role falls back to kings and politicians, though the frameworks of trade, rating and accounting remain.\n\nThis chapter sets up the idea that the earth is a spaceship, with the sun as our energy supplier. \"We are all astronauts\" says Fuller.\n\nThe idea of the earth is as a mechanical vehicle that requires maintenance, and that if you do not keep it in good order it will cease to function.\n\nLikens humanity to a chick that has just broken out of its shell and is now ready to enter the next phase of its existence. Suggests \"How big can we think?\"\n\nWhere the whole of a system is greater than the sum of its parts. \"Ergo, only complete world desovereignization can permit the realization of an all humanity high standard support.\"\n\nWealth is expanded by the development of tools which go beyond what was integral to man. The highest priority need of world society is a realistic accounting system, instead of one where a top toolmaker in India gets paid in a month what he would make in a day in Detroit.\n\nDefines tools as either craft tools that can be invented by one man, such as bows and arrows, and industrial tools that can not be produced by one man, such as the S.S. \"Queen Mary\". Finds language to be the first industrial tool. States that craft tools were used to create industrial tools. States that to take advantage of potential wealth we must give life fellowships to each person who is or becomes unemployed, and states that for every 100,000 fellowships given out one person will come up with something so valuable that it will pay for the remaining 99,999 fellowships. Predicts that soon the great office buildings will be turned into residences and that all the work that had been done in them will be done in the basements of a few buildings. States that we \"must operate exclusively on our vast daily energy income from the powers of wind, tide, water, and the direct Sun radiation energy\".\nEnds with anecdotal comments about his travels using three watches to keep track of time, at the office, where he is, and where he is going next, and an admonition \"So, planners, architects, and engineers take the initiative\".\n\n\n"}
{"id": "2133291", "url": "https://en.wikipedia.org/wiki?curid=2133291", "title": "Pathovar", "text": "Pathovar\n\nA pathovar is a bacterial strain or set of strains with the same or similar characteristics, that is differentiated at infrasubspecific level from other strains of the same species or subspecies on the basis of distinctive pathogenicity to one or more plant hosts.\n\nPathovars are named as a ternary or quaternary addition to the species binomial name, for example the bacterium that causes citrus canker \"Xanthomonas axonopodis\", has several pathovars with different host ranges, \"X. axonopodis\" pv. \"citri\" is one of them; the abbreviation 'pv.' means pathovar.\n\nThe type strains of pathovars are pathotypes, which are distinguished from the types (holotype, neotype, etc.) of the species to which the pathovar belongs. \n\n"}
{"id": "7452926", "url": "https://en.wikipedia.org/wiki?curid=7452926", "title": "Potential applications of carbon nanotubes", "text": "Potential applications of carbon nanotubes\n\nCarbon nanotubes (CNTs) are cylinders of one or more layers of graphene (lattice). Diameters of single-walled carbon nanotubes (SWNTs) and multi-walled carbon nanotubes (MWNTs) are typically 0.8 to 2 nm and 5 to 20 nm, respectively, although MWNT diameters can exceed 100 nm. CNT lengths range from less than 100 nm to 0.5 m.\n\nIndividual CNT walls can be metallic or semiconducting depending on the orientation of the lattice with respect to the tube axis, which is called chirality. MWNT's cross-sectional area offers an elastic modulus approaching 1 TPa and a tensile strength of 100 GPa, over 10-fold higher than any industrial fiber. MWNTs are typically metallic and can carry currents of up to 10 A cm. SWNTs can display thermal conductivity of 3500 W m K, exceeding that of diamond.\n\n, carbon nanotube production exceeded several thousand tons per year, used for applications in energy storage, device modelling, automotive parts, boat hulls, sporting goods, water filters, thin-film electronics, coatings, actuators and electromagnetic shields. CNT-related publications more than tripled in the prior decade, while rates of patent issuance also increased. Most output was of unorganized architecture. Organized CNT architectures such as \"forests\", yarns and regular sheets were produced in much smaller volumes. CNTs have even been proposed as the tether for a purported space elevator.\nRecently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.\n\nResearchers from Rice University and State University of New York – Stony Brook have shown that the addition of low weight % of carbon nanotubes can lead to significant improvements in the mechanical properties of biodegradable polymeric nanocomposites for applications in tissue engineering including bone, cartilage, muscle and nerve tissue. Dispersion of low weight % of graphene (~0.02 wt.%) results in significant increases in compressive and flexural mechanical properties of polymeric nanocomposites. Researchers at Rice University, Stony Brook University, Radboud University Nijmegen Medical Centre and University of California, Riverside have shown that carbon nanotubes and their polymer nanocomposites are suitable scaffold materials for bone tissue engineering and bone formation.\n\nCNTs exhibit dimensional and chemical compatibility with biomolecules, such as DNA and proteins. CNTs enable fluorescent and photoacoustic imaging, as well as localized heating using near-infrared radiation.\n\nSWNT biosensors exhibit large changes in electrical impedance and optical properties, which is typically modulated by adsorption of a target on the CNT surface. Low detection limits and high selectivity require engineering the CNT surface and field effects, capacitance, Raman spectral shifts and photoluminescence for sensor design. Products under development include printed test strips for estrogen and progesterone detection, microarrays for DNA and protein detection and sensors for and cardiac troponin. Similar CNT sensors support food industry, military and environmental applications.\n\nCNTs can be internalized by cells, first by binding their tips to cell membrane receptors. This enables transfection of molecular cargo attached to the CNT walls or encapsulated by CNTs. For example, the cancer drug doxorubicin was loaded at up to 60 wt % on CNTs compared with a maximum of 8 to 10 wt % on liposomes. Cargo release can be triggered by near-infrared radiation. However, limiting the retention of CNTs within the body is critical to prevent undesirable accumulation.\n\nCNT toxicity remains a concern, although CNT biocompatibility may be engineerable. The degree of lung inflammation caused by injection of well-dispersed SWNTs was insignificant compared with asbestos and with particulate matter in air. Medical acceptance of CNTs requires understanding of immune response and appropriate exposure standards for inhalation, injection, ingestion and skin contact. CNT forests immobilized in a polymer did not show elevated inflammatory response in rats relative to controls. CNTs are under consideration as low-impedance neural interface electrodes and for coating of catheters to reduce thrombosis.\n\nCNT enabled x-ray sources for medical imaging are also in development. Relying on the unique properties of the CNTs, researchers have developed field emission cathodes that allow precise x-ray control and close placement of multiple sources. CNT enabled x-ray sources have been demonstrated for pre-clinical, small animal imaging applications, and are currently in clinical trials.\n\nIn November 2012 researchers at the American National Institute of Standards and Technology (NIST) proved that single-wall carbon nanotubes may help protect DNA molecules from damage by oxidation.\n\nA highly effective method of delivering carbon nanotubes into cells is Cell squeezing, a high-throughput vector-free microfluidic platform for intracellular delivery developed at the Massachusetts Institute of Technology in the labs of Robert S. Langer.\n\nCarbon nanotubes have furthermore been grown inside microfluidic channels for chemical analysis, based on electrochromatography. Here, the high surface-area-to-volume ratio and high hydrophobicity of CNTs are used in order to greatly decrease the analysis time of small neutral molecules that typically require large bulky equipment for analysis.\n\nBecause of the carbon nanotube's superior mechanical properties, many structures have been proposed ranging from everyday items like clothes and sports gear to combat jackets and space elevators. However, the space elevator will require further efforts in refining carbon nanotube technology, as the practical tensile strength of carbon nanotubes must be greatly improved.\n\nFor perspective, outstanding breakthroughs have already been made. Pioneering work led by Ray H. Baughman at the NanoTech Institute has shown that single and multi-walled nanotubes can produce materials with toughness unmatched in the man-made and natural worlds.\n\nCarbon nanotubes are also a promising material as building blocks in hierarchical composite materials given their exceptional mechanical properties (~1 TPa in modulus, and ~100 GPa in strength). Initial attempts to incorporate CNTs into hierarchical structures (such as yarns, fibres or films) has led to mechanical properties that were significantly lower than these potential limits. The hierarchical integration of multi-walled carbon nanotubes and metal/metal oxides within a single nanostructure can leverage the potentiality of carbon nanotubes composite for water splitting and electrocatalysis. Windle \"et al.\" have used an \"in situ\" chemical vapor deposition (CVD) spinning method to produce continuous CNT yarns from CVD-grown CNT aerogels. CNT yarns can also be manufactured by drawing out CNT bundles from a CNT forest and subsequently twisting to form the fibre (draw-twist method, see picture on right). The Windle group have fabricated CNT yarns with strengths as high as ~9 GPa at small gage lengths of ~1 mm, however, strengths of only about ~1 GPa were reported at the longer gage length of 20 mm. The reason why fibre strengths have been low compared to the strength of individual CNTs is due to a failure to effectively transfer load to the constituent (discontinuous) CNTs within the fibre. One potential route for alleviating this problem is via irradiation (or deposition) induced covalent inter-bundle and inter-CNT cross-linking to effectively 'join up' the CNTs, with higher dosage levels leading to the possibility of amorphous carbon/carbon nanotube composite fibres. Espinosa \"et al.\" developed high performance DWNT-polymer composite yarns by twisting and stretching ribbons of randomly oriented bundles of DWNTs thinly coated with polymeric organic compounds. These DWNT-polymer yarns exhibited an unusually high energy to failure of ~100 J·g (comparable to one of the toughest natural materials – spider silk), and strength as high as ~1.4 GPa. Effort is ongoing to produce CNT composites that incorporate tougher matrix materials, such as Kevlar, to further improve on the mechanical properties toward those of individual CNTs.\n\nBecause of the high mechanical strength of carbon nanotubes, research is being made into weaving them into clothes to create stab-proof and bulletproof clothing. The nanotubes would effectively stop the bullet from penetrating the body, although the bullet's kinetic energy would likely cause broken bones and internal bleeding.\n\nMWNTs were first used as electrically conductive fillers in metals, at concentrations as high as 83.78 percent by weight (wt%). MWNT-polymer composites reach conductivities as high as 10,000 S m at 10 wt % loading. In the automotive industry, CNT plastics are used in electrostatic-assisted painting of mirror housings, as well as fuel lines and filters that dissipate electrostatic charge. Other products include electromagnetic interference (EMI)–shielding packages and silicon wafer carriers.\n\nFor load-bearing applications, CNT powders are mixed with polymers or precursor resins to increase stiffness, strength and toughness. These enhancements depend on CNT diameter, aspect ratio, alignment, dispersion and interfacial interaction. Premixed resins and master batches employ CNT loadings from 0.1 to 20 wt%. Nanoscale stick-slip among CNTs and CNT-polymer contacts can increase material damping, enhancing sporting goods, including tennis racquets, baseball bats and bicycle frames.\n\nCNT resins enhance fiber composites, including wind turbine blades and hulls for maritime security boats that are made by enhancing carbon fiber composites with CNT-enhanced resin. CNTs are deployed as additives in the organic precursors of stronger 1-μm diameter carbon fibers. CNTs influence the arrangement of carbon in pyrolyzed fiber.\n\nToward the challenge of organizing CNTs at larger scales, hierarchical fiber composites are created by growing aligned forests onto glass, silicon carbide (SiC), alumina and carbon fibers, creating so-called \"fuzzy\" fibers. Fuzzy epoxy CNT-SiC and CNT-alumina fabric showed 69% improved crack-opening (mode I) and/or in-plane shear interlaminar (mode II) toughness. Applications under investigation include lightning-strike protection, deicing, and structural health monitoring for aircraft.\n\nMWNTs can be used as a flame-retardant additive to plastics due to changes in rheology by nanotube loading. Such additives can replace halogenated flame retardants, which face environmental restrictions.\n\nCNT/Concrete blends offer increased tensile strength and reduced crack propagation.\n\nBuckypaper (nanotube aggregate) can significantly improve fire resistance due to efficient heat reflection.\n\nThe previous studies on the use of CNTs for textile functionalization were focused on fiber spinning for improving physical and mechanical properties. Recently a great deal of attention has been focused on coating CNTs on textile fabrics. Various methods have been employed for modifying fabrics using CNTs. produced intelligent e-textiles for Human Biomonitoring using a polyelectrolyte-based coating with CNTs. Additionally, Panhuis et al. dyed textile material by immersion in either a poly (2-methoxy aniline-5-sulfonic acid) PMAS polymer solution or PMAS-SWNT dispersion with enhanced conductivity and capacitance with a durable behavior. In another study, Hu and coworkers coated single-walled carbon nanotubes with a simple “dipping and drying” process for wearable electronics and energy storage applications. In the recent study, Li and coworkers using elastomeric separator and almost achieved a fully stretchable supercapacitor based on buckled single-walled carbon nanotube macrofilms. The electrospun polyurethane was used and provided sound mechanical stretchability and the whole cell achieve excellent charge-discharge cycling stability. CNTs have an aligned nanotube structure and a negative surface charge. Therefore, they have similar structures to direct dyes, so the exhaustion method is applied for coating and absorbing CNTs on the fiber surface for preparing multifunctional fabric including antibacterial, electric conductive, flame retardant and electromagnetic absorbance properties.\n\nLater, CNT yarns and laminated sheets made by direct chemical vapor deposition (CVD) or forest spinning or drawing methods may compete with carbon fiber for high-end uses, especially in weight-sensitive applications requiring combined electrical and mechanical functionality. Research yarns made from few-walled CNTs have reached a stiffness of 357 GPa and a strength of 8.8 GPa for a gauge length comparable to the millimeter-long CNTs within the yarn. Centimeter-scale gauge lengths offer only 2-GPa gravimetric strengths, matching that of Kevlar.\n\nBecause the probability of a critical flaw increases with volume, yarns may never achieve the strength of individual CNTs. However, CNT's high surface area may provide interfacial coupling that mitigates these deficiencies. CNT yarns can be knotted without loss of strength. Coating forest-drawn CNT sheets with functional powder before inserting twist yields weavable, braidable and sewable yarns containing up to 95 wt % powder. Uses include superconducting wires, battery and fuel cell electrodes and self-cleaning textiles.\n\nAs yet impractical fibers of aligned SWNTs can be made by coagulation-based spinning of CNT suspensions. Cheaper SWNTs or spun MWNTs are necessary for commercialization. Carbon nanotubes can be dissolved in superacids such as fluorosulfuric acid and drawn into fibers in dry jet-wet spinning.\n\nDWNT-polymer composite yarns have been made by twisting and stretching ribbons of randomly oriented bundles of DWNTs thinly coated with polymeric organic compounds.\n\nBody armor—combat jackets Cambridge University developed the fibres and licensed a company to make them. In comparison, the bullet-resistant fiber Kevlar fails at 27–33 J/g.\n\nSynthetic muscles offer high contraction/extension ratio given an electric current.\n\nSWNT are in use as an experimental material for removable, structural bridge panels.\n\nIn 2015 researchers incorporated CNTs and graphene into spider silk, increasing its strength and toughness to a new record. They sprayed 15 Pholcidae spiders with water containing the nanotubes or flakes. The resulting silk had a fracture strength up to 5.4 GPa, a Young’s modulus up to 47.8 GPa and a toughness modulus up to 2.1 GPa, surpassing both synthetic polymeric high performance fibres (e.g. Kevlar49) and knotted fibers.\n\n\"Forests\" of stretched, aligned MWNT springs can achieve an energy density 10 times greater than that of steel springs, offering cycling durability, temperature insensitivity, no spontaneous discharge and arbitrary discharge rate. SWNT forests are expected to be able to store far more than MWNTs.\n\nAdding small amounts of CNTs to metals increases tensile strength and modulus with potential in aerospace and automotive structures. Commercial aluminum-MWNT composites have strengths comparable to stainless steel (0.7 to 1 GPa) at one-third the density (2.6 g cm), comparable to more expensive aluminium-lithium alloys.\n\nCNTs can serve as a multifunctional coating material. For example, paint/MWNT mixtures can reduce biofouling of ship hulls by discouraging attachment of algae and barnacles. They are a possible alternative to environmentally hazardous biocide-containing paints. Mixing CNTs into anticorrosion coatings for metals can enhance coating stiffness and strength and provide a path for cathodic protection.\n\nCNTs provide a less expensive alternative to ITO for a range of consumer devices. Besides cost, CNT's flexible, transparent conductors offer an advantage over brittle ITO coatings for flexible displays. CNT conductors can be deposited from solution and patterned by methods such as screen printing. SWNT films offer 90% transparency and a sheet resistivity of 100 ohm per square. Such films are under development for thin-film heaters, such as for defrosting windows or sidewalks.\n\nCarbon nanotubes forests and foams can also be coated with a variety of different materials to change their functionality and performance. Examples include silicon coated CNTs to create flexible energy-dense batteries, graphene coatings to create highly elastic aerogels and silicon carbide coatings to create a strong structural material for robust high-aspect-ratio 3D-micro architectures.\n\nThere is a wide range of methods how CNTs can be formed into coatings and films.\n\nA spray-on mixture of carbon nanotubes and ceramic demonstrates unprecedented ability to resist damage while absorbing laser light. Such coatings that absorb as the energy of high-powered lasers without breaking down are essential for optical power detectors that measure the output of such lasers. These are used, for example, in military equipment for defusing unexploded mines. The composite consists of multiwall carbon nanotubes and a ceramic made of silicon, carbon and nitrogen. Including boron boosts the breakdown temperature. The nanotubes and graphene-like carbon transmit heat well, while the oxidation-resistant ceramic boosts damage resistance. Creating the coating involves dispersing the nanotubes in toluene, to which a clear liquid polymer containing boron was added. The mixture was heated to . The result is crushed into a fine powder, dispersed again in toluene and sprayed in a thin coat on a copper surface. The coating absorbed 97.5 percent of the light from a far-infrared laser and tolerated 15 kilowatts per square centimeter for 10 seconds. Damage tolerance is about 50 percent higher than for similar coatings, e.g., nanotubes alone and carbon paint.\n\nRadars work in the microwave frequency range, which can be absorbed by MWNTs. Applying the MWNTs to the aircraft would cause the radar to be absorbed and therefore seem to have a smaller radar cross-section. One such application could be to paint the nanotubes onto the plane. Recently there has been some work done at the University of Michigan regarding carbon nanotubes usefulness as stealth technology on aircraft. It has been found that in addition to the radar absorbing properties, the nanotubes neither reflect nor scatter visible light, making it essentially invisible at night, much like painting current stealth aircraft black except much more effective. Current limitations in manufacturing, however, mean that current production of nanotube-coated aircraft is not possible. One theory to overcome these current limitations is to cover small particles with the nanotubes and suspend the nanotube-covered particles in a medium such as paint, which can then be applied to a surface, like a stealth aircraft.\n\nIn 2010, Lockheed Martin Corporation applied for a patent for just such a CNT based radar absorbent material, which was reassigned and granted to Applied NanoStructure Solutions, LLC in 2012. Some believe that this material is incorporated in the F-35 Lightning II.\n\nNanotube-based transistors, also known as carbon nanotube field-effect transistors (CNTFETs), have been made that operate at room temperature and that are capable of digital switching using a single electron. However, one major obstacle to realization of nanotubes has been the lack of technology for mass production. In 2001 IBM researchers demonstrated how metallic nanotubes can be destroyed, leaving semiconducting ones behind for use as transistors. Their process is called \"constructive destruction,\" which includes the automatic destruction of defective nanotubes on the wafer. This process, however, only gives control over the electrical properties on a statistical scale.\n\nSWNTs are attractive for transistors because of their low electron scattering and their bandgap. SWNTs are compatible with field-effect transistor (FET) architectures and high-k dielectrics. Despite progress following the CNT transistor's appearance in 1998, including a tunneling FET with a subthreshold swing of <60 mV per decade (2004), a radio (2007) and an FET with sub-10-nm channel length and a normalized current density of 2.41 mA μm at 0.5 V, greater than those obtained for silicon devices.\n\nHowever, control of diameter, chirality, density and placement remains insufficient for commercial production. Less demanding devices of tens to thousands of SWNTs are more immediately practical. The use of CNT arrays/transistor increases output current and compensates for defects and chirality differences, improving device uniformity and reproducibility. For example, transistors using horizontally aligned CNT arrays achieved mobilities of 80 cm V s, subthreshold slopes of 140 mV per decade and on/off ratios as high as 10. CNT film deposition methods enable conventional semiconductor fabrication of more than 10,000 CNT devices per chip.\n\nPrinted CNT thin-film transistors (TFTs) are attractive for driving organic light-emitting diode displays, showing higher mobility than amorphous silicon (~1 cm V s) and can be deposited by low-temperature, nonvacuum methods. Flexible CNT TFTs with a mobility of 35 cm V s and an on/off ratio of 6 were demonstrated. A vertical CNT FET showed sufficient current output to drive OLEDs at low voltage, enabling red-green-blue emission through a transparent CNT network. CNTs are under consideration for radio-frequency identification tags. Selective retention of semiconducting SWNTs during spin-coating and reduced sensitivity to adsorbates were demonstrated.\n\nThe International Technology Roadmap for Semiconductors suggests that CNTs could replace Cu in microelectronic interconnects, owing to their low scattering, high current-carrying capacity, and resistance to electromigration. For this, vias comprising tightly packed (>10 cm) metallic CNTs with low defect density and low contact resistance are needed. Recently, complementary metal oxide semiconductor (CMOS)–compatible 150-nm-diameter interconnects with a single CNT–contact hole resistance of 2.8 kOhm were demonstrated on full 200-mm-diameter wafers. Also, as a replacement for solder bumps, CNTs can function both as electrical leads and heat dissipaters for use in high-power amplifiers.\n\nLast, a concept for a nonvolatile memory based on individual CNT crossbar electromechanical switches has been adapted for commercialization by patterning tangled CNT thin films as the functional elements. This required development of ultrapure CNT suspensions that can be spin-coated and processed in industrial clean room environments and are therefore compatible with CMOS processing standards.\n\nCarbon nanotube field-effect transistors (CNTFETs) can operate at room temperature and are capable of digital switching using a single electron. In 2013, a CNT logic circuit was demonstrated that could perform useful work. Major obstacles to nanotube-based microelectronics include the absence of technology for mass production, circuit density, positioning of individual electrical contacts, sample purity, control over length, chirality and desired alignment, thermal budget and contact resistance.\n\nOne of the main challenges was regulating conductivity. Depending on subtle surface features, a nanotube may act as a conductor or as a semiconductor.\n\nAnother way to make carbon nanotube transistors has been to use random networks of them. By doing so one averages all of their electrical differences and one can produce devices in large scale at the wafer level. This approach was first patented by Nanomix Inc. (date of original application June 2002). It was first published in the academic literature by the United States Naval Research Laboratory in 2003 through independent research work. This approach also enabled Nanomix to make the first transistor on a flexible and transparent substrate.\n\nSince the electron mean free path in SWCNTs can exceed 1 micrometer, long channel CNTFETs exhibit near-ballistic transport characteristics, resulting in high speeds. CNT devices are projected to operate in the frequency range of hundreds of gigahertz.\n\nNanotubes can be grown on nanoparticles of magnetic metal (Fe, Co) that facilitates production of electronic (spintronic) devices. In particular control of current through a field-effect transistor by magnetic field has been demonstrated in such a single-tube nanostructure.\n\nIn 2001 IBM researchers demonstrated how metallic nanotubes can be destroyed, leaving semiconducting nanotubes for use as components. Using \"constructive destruction\", they destroyed defective nanotubes on the wafer. This process, however, only gives control over the electrical properties on a statistical scale. In 2003 room-temperature ballistic transistors with ohmic metal contacts and high-k gate dielectric were reported, showing 20–30x more current than state-of-the-art siliconMOSFETs. Palladium is a high-work function metal that was shown to exhibit Schottky barrier-free contacts to semiconducting nanotubes with diameters >1.7 nm.\n\nThe potential of carbon nanotubes was demonstrated in 2003 when room-temperature ballistic transistors with ohmic metal contacts and high-k gate dielectric were reported, showing 20–30x higher ON current than state-of-the-art Si MOSFETs. This presented an important advance in the field as CNT was shown to potentially outperform Si. At the time, a major challenge was ohmic metal contact formation. In this regard, palladium, which is a high-work function metal was shown to exhibit Schottky barrier-free contacts to semiconducting nanotubes with diameters >1.7 nm.\n\nThe first nanotube integrated memory circuit was made in 2004. One of the main challenges has been regulating the conductivity of nanotubes. Depending on subtle surface features a nanotube may act as a plain conductor or as a semiconductor. A fully automated method has however been developed to remove non-semiconductor tubes.\n\nIn 2013, researchers demonstrated a Turing-complete prototype micrometer-scale computer. Carbon nanotube transistors as logic-gate circuits with densities comparable to modern CMOS technology has not yet been demonstrated.\n\nIn 2014 networks of purified semiconducting carbon nanotubes were used as the active material in p-type thin film transistors. They were created using 3-D printers using inkjet or gravure methods on flexible substrates, including polyimide and polyethylene (PET) and transparent substrates such as glass. These transistors reliably exhibit high-mobilities (> 10 cm V s) and ON/OFF ratios (> 1000) as well as threshold voltages below 5 V. They offer current density and low power consumption as well as environmental stability and mechanical flexibility. Hysterisis in the current-voltage curses as well as variability in the threshold voltage remain to be solved.\n\nIn 2015 researchers announced a new way to connect wires to SWNTs that make it possible to continue shrinking the width of the wires without increasing electrical resistance. The advance was expected to shrink the contact point between the two materials to just 40 atoms in width and later less. They tubes align in regularly spaced rows on silicon wafers. Simulations indicated that designs could be optimized either for high performance or for low power consumption. Commercial devices were not expected until the 2020s.\n\nLarge structures of carbon nanotubes can be used for thermal management of electronic circuits. An approximately 1 mm–thick carbon nanotube layer was used as a special material to fabricate coolers, this material has very low density, ~20 times lower weight than a similar copper structure, while the cooling properties are similar for the two materials.\n\nBuckypaper has characteristics appropriate for use as a heat sink for chipboards, a backlight for LCD screens or as a faraday cage.\n\nOne of the promising applications of single-walled carbon nanotubes (SWNTs) is their use in solar panels, due to their strong UV/Vis-NIR absorption characteristics. Research has shown that they can provide a sizable increase in efficiency, even at their current unoptimized state. Solar cells developed at the New Jersey Institute of Technology use a carbon nanotube complex, formed by a mixture of carbon nanotubes and carbon buckyballs (known as fullerenes) to form snake-like structures. Buckyballs trap electrons, but they can't make electrons flow. Add sunlight to excite the polymers, and the buckyballs will grab the electrons. Nanotubes, behaving like copper wires, will then be able to make the electrons or current flow.\n\nAdditional research has been conducted on creating SWNT hybrid solar panels to increase the efficiency further. These hybrids are created by combining SWNT's with photo-excitable electron donors to increase the number of electrons generated. It has been found that the interaction between the photo-excited porphyrin and SWNT generates electro-hole pairs at the SWNT surfaces. This phenomenon has been observed experimentally, and contributes practically to an increase in efficiency up to 8.5%.\n\nNanotubes can potentially replace indium tin oxide in solar cells as a transparent conductive film in solar cells to allow light to pass to the active layers and generate photocurrent.\n\nCNTs in organic solar cells help reduce energy loss (carrier recombination) and enhance resistance to photooxidation. Photovoltaic technologies may someday incorporate CNT-Silicon heterojunctions to leverage efficient multiple-exciton generation at p-n junctions formed within individual CNTs. In the nearer term, commercial photovoltaics may incorporate transparent SWNT electrodes.\n\nIn addition to being able to store electrical energy, there has been some research in using carbon nanotubes to store hydrogen to be used as a fuel source. By taking advantage of the capillary effects of the small carbon nanotubes, it is possible to condense gases in high density inside single-walled nanotubes. This allows for gases, most notably hydrogen (H), to be stored at high densities without being condensed into a liquid. Potentially, this storage method could be used on vehicles in place of gas fuel tanks for a hydrogen-powered car. A current issue regarding hydrogen-powered vehicles is the on-board storage of the fuel. Current storage methods involve cooling and condensing the H gas to a liquid state for storage which causes a loss of potential energy (25–45%) when compared to the energy associated with the gaseous state. Storage using SWNTs would allow one to keep the H2 in its gaseous state, thereby increasing the storage efficiency. This method allows for a volume to energy ratio slightly smaller to that of current gas powered vehicles, allowing for a slightly lower but comparable range.\n\nAn area of controversy and frequent experimentation regarding the storage of hydrogen by adsorption in carbon nanotubes is the efficiency by which this process occurs. The effectiveness of hydrogen storage is integral to its use as a primary fuel source since hydrogen only contains about one fourth the energy per unit volume as gasoline. Studies however show that what is the most important is the surface area of the materials used. Hence activated carbon with surface area of 2600 m2/g can store up to 5,8% w/w. In all these carbonaceous materials, hydrogen is stored by physisorption at 70-90K.\n\nOne experiment sought to determine the amount of hydrogen stored in CNTs by utilizing elastic recoil detection analysis (ERDA). CNTs (primarily SWNTs) were synthesized via chemical vapor disposition (CVD) and subjected to a two-stage purification process including air oxidation and acid treatment, then formed into flat, uniform discs and exposed to pure, pressurized hydrogen at various temperatures. When the data was analyzed, it was found that the ability of CNTs to store hydrogen decreased as temperature increased. Moreover, the highest hydrogen concentration measured was ~0.18%; significantly lower than commercially viable hydrogen storage needs to be. A separate experimental work performed by using a gravimetric method also revealed the maximum hydrogen uptake capacity of CNTs to be as low as 0.2%.\n\nIn another experiment, CNTs were synthesized via CVD and their structure was characterized using Raman spectroscopy. Utilizing microwave digestion, the samples were exposed to different acid concentrations and different temperatures for various amounts of time in an attempt to find the optimum purification method for SWNTs of the diameter determined earlier. The purified samples were then exposed to hydrogen gas at various high pressures, and their adsorption by weight percent was plotted. The data showed that hydrogen adsorption levels of up to 3.7% are possible with a very pure sample and under the proper conditions. It is thought that microwave digestion helps improve the hydrogen adsorption capacity of the CNTs by opening up the ends, allowing access to the inner cavities of the nanotubes.\n\nThe biggest obstacle to efficient hydrogen storage using CNTs is the purity of the nanotubes. To achieve maximum hydrogen adsorption, there must be minimum graphene, amorphous carbon, and metallic deposits in the nanotube sample. Current methods of CNT synthesis require a purification step. However, even with pure nanotubes, the adsorption capacity is only maximized under high pressures, which are undesirable in commercial fuel tanks.\n\nVarious companies are developing transparent, electrically conductive CNT films and nanobuds to replace indium tin oxide (ITO) in LCDs, touch screens and photovoltaic devices. Nanotube films show promise for use in displays for computers, cell phones, Personal digital assistants, and automated teller machines. CNT diodes display a photovoltaic effect.\n\nMulti-walled nanotubes (MWNT coated with magnetite) can generate strong magnetic fields. Recent advances show that MWNT decorated with maghemite nanoparticles can be oriented in a magnetic field and enhance the electrical properties of the composite material in the direction of the field for use in electric motor brushes.\n\nA layer of 29% iron enriched single-walled nanotubes (SWNT) placed on top of a layer of explosive material such as PETN can be ignited with a regular camera flash.\n\nCNTs can be used as electron guns in miniature cathode ray tubes (CRT) in high-brightness, low-energy, low-weight displays. A display would consist of a group of tiny CRTs, each providing the electrons to illuminate the phosphor of one pixel, instead of having one CRT whose electrons are aimed using electric and magnetic fields. These displays are known as field emission displays (FEDs).\n\nCNTs can act as antennas for radios and other electromagnetic devices.\n\nConductive CNTs are used in brushes for commercial electric motors. They replace traditional carbon black. The nanotubes improve electrical and thermal conductivity because they stretch through the plastic matrix of the brush. This permits the carbon filler to be reduced from 30% down to 3.6%, so that more matrix is present in the brush. Nanotube composite motor brushes are better-lubricated (from the matrix), cooler-running (both from better lubrication and superior thermal conductivity), less brittle (more matrix, and fiber reinforcement), stronger and more accurately moldable (more matrix). Since brushes are a critical failure point in electric motors, and also don't need much material, they became economical before almost any other application.\n\nWires for carrying electric current may be fabricated from nanotubes and nanotube-polymer composites. Small wires have been fabricated with specific conductivity exceeding copper and aluminum; the highest conductivity non-metallic cables.\n\nCNT are under investigation as an alternative to tungsten filaments in incandescent light bulbs.\n\nMetallic carbon nanotubes have aroused research interest for their applicability\nas very-large-scale integration (VLSI) interconnects because of their high thermal stability, high thermal conductivity and large current carrying capacity. An isolated CNT can carry current\ndensities in excess of 1000 MA/sq-cm without damage even at an elevated temperature of , eliminating electromigration reliability concerns that plague Cu interconnects. Recent modeling work comparing the two has shown that CNT bundle interconnects can potentially offer advantages over copper. Recent experiments demonstrated resistances as low as 20 Ohms using different architectures, detailed conductance measurements over a wide temperature range were shown to agree with theory for a strongly disordered quasi-one-dimensional conductor.\n\nHybrid interconnects that employ CNT vias in tandem with copper interconnects may offer advantages from a reliability/thermal-management perspective. \nIn 2016, the European Union has funded a four million euro project over three years to evaluate manufacturability and performance of composite interconnects employing both CNT and copper interconnects. The project named CONNECT (CarbON Nanotube compositE InterconneCTs) involves the joint efforts of seven European research and industry partners on fabrication techniques and processes to enable reliable Carbon NanoTubes for on-chip interconnects in ULSI microchip production.\n\nWires for carrying electric current may be fabricated from pure nanotubes and nanotube-polymer composites. It has already been demonstrated that carbon nanotube wires can successfully be used for power or data transmission. Recently small wires have been fabricated with specific conductivity exceeding copper and aluminum; these cables are the highest conductivity carbon nanotube and also highest conductivity non-metal cables. Recently, composite of carbon nanotube and copper have been shown to exhibit nearly one hundred times higher current-carrying-capacity than pure copper or gold. Significantly, the electrical conductivity of such a composite is similar to pure Cu. Thus, this Carbon nanotube-copper (CNT-Cu) composite possesses the highest observed current-carrying capacity among electrical conductors. Thus for a given cross-section of electrical conductor, the CNT-Cu composite can withstand and transport one hundred times higher current compared to metals such as copper and gold.\n\nThe use of CNTs as a catalyst support in fuel cells can potentially reduce platinum usage by 60% compared with carbon black. Doped CNTs may enable the complete elimination of Pt.\n\nMIT Research Laboratory of Electronics uses nanotubes to improve supercapacitors. The activated charcoal used in conventional ultracapacitors has many small hollow spaces of various size, which create together a large surface to store electric charge. But as charge is quantized into elementary charges, i.e. electrons, and each such elementary charge needs a minimum space, a significant fraction of the electrode surface is not available for storage because the hollow spaces are not compatible with the charge's requirements. With a nanotube electrode the spaces may be tailored to size—few too large or too small—and consequently the capacity should be increased considerably.\n\nA 40-F supercapacitor with a maximum voltage of 3.5 V that employed forest-grown SWNTs that are binder- and additive-free achieved an energy density of 15.6 Wh kg and a power density of 37 kW kg. CNTs can be bound to the charge plates of capacitors to dramatically increase the surface area and therefore energy density.\n\nCarbon nanotubes' (CNTs) exciting electronic properties have shown promise in the field of batteries, where typically they are being experimented as a new electrode material, particularly the anode for lithium ion batteries. This is due to the fact that the anode requires a relatively high reversible capacity at a potential close to metallic lithium, and a moderate irreversible capacity, observed thus far only by graphite-based composites, such as CNTs. They have shown to greatly improve capacity and cyclability of lithium-ion batteries, as well as the capability to be very effective buffering components, alleviating the degradation of the batteries that is typically due to repeated charging and discharging. Further, electronic transport in the anode can be greatly improved using highly metallic CNTs.\n\nMore specifically, CNTs have shown reversible capacities from 300 to 600 mAhg, with some treatments to them showing these numbers rise to up to 1000 mAhg. Meanwhile, graphite, which is most widely used as an anode material for these lithium batteries, has shown capacities of only 320 mAhg. By creating composites out of the CNTs, scientists see much potential in taking advantage of these exceptional capacities, as well as their excellent mechanical strength, conductivities, and low densities.\n\nMWNTs are used in lithium ion batteries cathodes. In these batteries, small amounts of MWNT powder are blended with active materials and a polymer binder, such as 1 wt % CNT loading in cathodes and graphite anodes. CNTs provide increased electrical connectivity and mechanical integrity, which enhances rate capability and cycle life.\n\nA paper battery is a battery engineered to use a paper-thin sheet of cellulose (which is the major constituent of regular paper, among other things) infused with aligned carbon nanotubes. The potential for these devices is great, as they may be manufactured via a roll-to-roll process, which would make it very low-cost, and they would be lightweight, flexible, and thin. In order to productively use paper electronics (or any thin electronic devices), the power source must be equally thin, thus indicating the need for paper batteries. Recently, it has been shown that surfaces coated with CNTs can be used to replace heavy metals in batteries. More recently, functional paper batteries have been demonstrated, where a lithium-ion battery is integrated on a single sheet of paper through a lamination process as a composite with Li4Ti5O12 (LTO) or LiCoO2 (LCO). The paper substrate would function well as the separator for the battery, where the CNT films function as the current collectors for both the anode \"and\" the cathode. These rechargeable energy devices show potential in RFID tags, functional packaging, or new disposable electronic applications.\n\nImprovements have also been shown in lead-acid batteries, based on research performed by Bar-Ilan University using high quality SWCNT manufactured by OCSiAl. The study demonstrated an increase in the lifetime of lead acid batteries by 4.5 times and a capacity increase of 30% on average and up to 200% at high discharge rates.\n\nCNT can be used for desalination. Water molecules can be separated from salt by forcing them through electrochemically robust nanotube networks with controlled nanoscale porosity. This process requires far lower pressures than conventional reverse osmosis methods. Compared to a plain membrane, it operates at a 20 °C lower temperature, and at a 6x greater flow rate. Membranes using aligned, encapsulated CNTs with open ends permit flow through the CNTs' interiors. Very-small-diameter SWNTs are needed to reject salt at seawater concentrations. Portable filters containing CNT meshes can purify contaminated drinking water. Such networks can electrochemically oxidize organic contaminants, bacteria and viruses.\n\nCNT membranes can filter carbon dioxide from power plant emissions.\n\nCNT can be filled with biological molecules, aiding biotechnology.\n\nCNT have the potential to store between 4.2 and 65% hydrogen by weight. If they can be mass-produced economically, of CNT could contain the same amount of energy as a gasoline tank.\n\nCNTs can be used to produce nanowires of other elements/molecules, such as gold or zinc oxide. Nanowires in turn can be used to cast nanotubes of other materials, such as gallium nitride. These can have very different properties from CNTs—for example, gallium nitride nanotubes are hydrophilic, while CNTs are hydrophobic, giving them possible uses in organic chemistry.\n\nOscillators based on CNT have achieved speeds of > 50 GHz.\n\nCNT electrical and mechanical properties suggest them as alternatives to traditional electrical actuators. \n\nThe exceptional electrical and mechanical properties of carbon nanotubes have made them alternatives to the traditional electrical actuators for both microscopic and macroscopic applications. Carbon nanotubes are very good conductors of both electricity and heat, and they are also very strong and elastic molecules in certain directions.\n\nCarbon nanotubes have also been applied in the acoustics (such as loudspeaker and earphone). In 2008 it was shown that a sheet of nanotubes can operate as a loudspeaker if an alternating current is applied. The sound is not produced through vibration but thermoacoustically. In 2013, a carbon nanotube (CNT) thin yarn thermoacoustic earphone together with CNT thin yarn thermoacoustic chip was demonstrated by a research group of Tsinghua-Foxconn Nanotechnology Research Center in Tsinghua University, using a Si-based semi-conducting technology compatible fabrication process.\n\nNear-term commercial uses include replacing piezoelectric speakers in greeting cards.\n\n\n\n\nA CNT nano-structured sponge (nanosponge) containing sulfur and iron is more effective at soaking up water contaminants such as oil, fertilizers, pesticides and pharmaceuticals. Their magnetic properties make them easier to retrieve once the clean-up job is done. The sulfur and iron increases sponge size to around . It also increases porosity due to beneficial defects, creating buoyancy and reusability. Iron, in the form of ferrocene makes the structure easier to control and enables recovery using magnets. Such nanosponges increase the absorption of the toxic organic solvent dichlorobenzene from water by 3.5 times. The sponges can absorb vegetable oil up to 150 times their initial weight and can absorb engine oil as well.\n\nEarlier, a magnetic boron-doped MWNT nanosponge that could absorb oil from water. The sponge was grown as a forest on a substrate via chemical vapor disposition. Boron puts kinks and elbows into the tubes as they grow and promotes the formation of covalent bonds. The nanosponges retain their elastic property after 10,000 compressions in the lab. The sponges are both superhydrophobic, forcing them to remain at the water's surface and oleophilic, drawing oil to them.\n\nIt has been shown that carbon nanotubes exhibit strong adsorption affinities to a wide range of aromatic and aliphatic contaminants in water, due to their large and hydrophobic surface areas. They also showed similar adsorption capacities as activated carbons in the presence of natural organic matter. As a result, they have been suggested as promising adsorbents for removal of contaminant in water and wastewater treatment systems.\n\nMoreover, membranes made out of carbon nanotube arrays have been suggested as switchable molecular sieves, with sieving and permeation features that can be dynamically activated/deactivated by either pore size distribution (passive control) or external electrostatic fields (active control).\n\nCarbon nanotubes have been implemented in nanoelectromechanical systems, including mechanical memory elements (NRAM being developed by Nantero Inc.) and nanoscale electric motors (see Nanomotor or Nanotube nanomotor).\n\nCarboxyl-modified single-walled carbon nanotubes (so called zig-zag, armchair type) can act as sensors of atoms and ions of alkali metals Na, Li, K. In May 2005, Nanomix Inc. placed on the market a hydrogen sensor that integrated carbon nanotubes on a silicon platform. Since then, Nanomix has been patenting many such sensor applications, such as in the field of carbon dioxide, nitrous oxide, glucose, DNA detection, etc. End of 2014, Tulane University researchers have tested Nanomix's fast and fully automated point of care diagnostic system in Sierra Leone to help for rapid testing for Ebola. Nanomix announced that a product could be launched within three to six months.\n\nEikos Inc of Franklin, Massachusetts and Unidym Inc. of Silicon Valley, California are developing transparent, electrically conductive films of carbon nanotubes to replace indium tin oxide (ITO). Carbon nanotube films are substantially more mechanically robust than ITO films, making them ideal for high-reliability touchscreens and flexible displays. Printable water-based inks of carbon nanotubes are desired to enable the production of these films to replace ITO. Nanotube films show promise for use in displays for computers, cell phones, PDAs, and ATMs.\n\nA nanoradio, a radio receiver consisting of a single nanotube, was demonstrated in 2007.\n\nThe use in tensile stress or toxic gas sensors was proposed by Tsagarakis.\n\nA flywheel made of carbon nanotubes could be spun at extremely high velocity on a floating magnetic axis in a vacuum, and potentially store energy at a density approaching that of conventional fossil fuels. Since energy can be added to and removed from flywheels very efficiently in the form of electricity, this might offer a way of storing electricity, making the electrical grid more efficient and variable power suppliers (like wind turbines) more useful in meeting energy needs. The practicality of this depends heavily upon the cost of making massive, unbroken nanotube structures, and their failure rate under stress.\n\nCarbon nanotube springs have the potential to indefinitely store elastic potential energy at ten times the density of lithium-ion batteries with flexible charge and discharge rates and extremely high cycling durability.\n\nUltra-short SWNTs (US-tubes) have been used as nanoscaled capsules for delivering MRI contrast agents in vivo.\n\nCarbon nanotubes provide a certain potential for metal-free catalysis of inorganic and organic reactions. For instance, oxygen groups attached to the surface of carbon nanotubes have the potential to catalyze oxidative dehydrogenations or selective oxidations. Nitrogen-doped carbon nanotubes may replace platinum catalysts used to reduce oxygen in fuel cells. A forest of vertically aligned nanotubes can reduce oxygen in alkaline solution more effectively than platinum, which has been used in such applications since the 1960s. Here, the nanotubes have the added benefit of not being subject to carbon monoxide poisoning.\n\nWake Forest University engineers are using multiwalled carbon nanotubes to enhance the brightness of field-induced polymer electroluminescent technology, potentially offering a step forward in the search for safe, pleasing, high-efficiency lighting. In this technology, moldable polymer matrix emits light when exposed to an electric current. It could eventually yield high-efficiency lights without the mercury vapor of compact fluorescent lamps or the bluish tint of some fluorescents and LEDs, which has been linked with circadian rhythm disruption.\n\nCandida albicans has been used in combination with carbon nanotubes (CNT) to produce stable electrically conductive bio-nano-composite tissue materials that have been used as temperature sensing elements.\n\nThe SWNT production company OCSiAl developed a series of masterbatches for industrial use of single-wall CNTs in multiple types of rubber blends and tires, with initial trials showing increases in hardness, viscosity, tensile strain resistance and resistance to abrasion while reducing elongation and compression In tires the three primary characteristics of durability, fuel efficiency and traction were improved using SWNTs. The development of rubber masterbatches built on earlier work by the Japanese National Institute of Advanced Industrial Science & Technology showing rubber to be a viable candidate for improvement with SWNTs.\n\nIntroducing MWNTs to polymers can improve flame retardancy and retard thermal degradation of polymer. The results confirmed that combination of MWNTs and ammonium polyphosphates show a synergistic effect for improving flame retardancy.\n\n"}
{"id": "33718154", "url": "https://en.wikipedia.org/wiki?curid=33718154", "title": "Pure bending", "text": "Pure bending\n\nPure bending is a condition of stress where a bending moment is applied to a beam without the simultaneous presence of axial, shear, or torsional forces.\nPure bending occurs only under a constant bending moment (M) since the shear force (V), which is equal to formula_1, has to be equal to zero. In reality, a state of pure bending does not practically exist, because such a state needs an absolutely weightless member. The state of pure bending is an approximation made to derive formulas.\n\n\nNotes: Homogeneous means the material is of same kind throughout. Isotropic means that the elastic properties in all directions are equal.\n\n"}
{"id": "273712", "url": "https://en.wikipedia.org/wiki?curid=273712", "title": "Rotating furnace", "text": "Rotating furnace\n\nA rotating furnace is a device for making solid objects which have concave surfaces that are segments of axially symmetrical paraboloids. Usually, the objects are made of glass. The furnace makes use of the fact, which was known already to Newton, that the centrifugal-force-induced shape of the top surface of a spinning liquid is a concave paraboloid, identical to the shape of a reflecting telescope's primary focusing mirror.\n\nParaboloids can be used in various ways, including (after being silvered) as primary mirrors in reflecting telescopes and solar cookers.\n\nThe furnace includes a mechanism that rotates an open-topped container at constant speed around a vertical axis. A quantity of glass sufficient to make the mirror is placed in the container, heated until it is completely molten, and then allowed to cool while continuing to rotate until it has completely solidified. When the rotation is stopped, the glass is solid, so the paraboloidal shape of its top surface is preserved. This process is called spin casting.\n\nThe same process can be used to make a lens with a concave paraboloidal surface. The other surface is shaped by the container that holds the molten glass acting as a mould. Lenses made this way are sometimes used as objectives in refracting telescopes.\n\nThe word \"spin\" is frequently used in this context without implying that the rotation is rapid. Making a mirror with a focal length of five metres, for example, requires a rotation speed less than ten revolutions per minute. Since the speed is low, accurate dynamic balancing of the rotating components is not needed.\n\nThe axis of rotation becomes the axis of the paraboloid. It is not necessary for this axis to be in the centre of the container of glass, or even for it to pass through the container. By placing the container away from the axis, off-axis paraboloidal segments can be cast. This is done in the making of very large telescopes which have mirrors consisting of several segments.\n\nThe focal length of the paraboloid is related to the angular speed at which the liquid is rotated by the equation: formula_1, where formula_2 is the focal length, formula_3 is the rotation speed, and formula_4 is the acceleration due to gravity. They must be in compatible units so, for example, formula_2 can be in metres, formula_3 in radians per second, and formula_4 in metres per second-squared. The angle unit in formula_3 must be radians. 1 radian per second is about 9.55 rotations per minute (RPM). On the Earth's surface, formula_4 is about 9.81 metres per second-squared. Putting these numbers into the equation produces the approximation: formula_10, where formula_2 is the focal length in metres, and formula_12 is the rotation speed in RPM.\n\nGenerally, a spin-cast paraboloid is not sufficiently accurate to permit its immediate use as a telescope mirror or lens, so it is corrected by computer-controlled grinding machines. The amount of grinding done, and the mass of glass material wasted, are much less than would have been required without spinning.\n\nSpin casting can also be used, often with materials other than glass, to produce prototype paraboloids, such as spotlight reflectors or solar-energy concentrators, which do not need to be as exactly paraboloidal as telescope mirrors. Spin casting every paraboloid that is made would be too slow and costly, so the prototype is simply copied relatively quickly and cheaply and with adequate accuracy.\n\nLiquid mirror telescopes have rotating mirrors that consist of a liquid metal such as mercury or a low-melting alloy of gallium. The mirrors do not solidify, but are used while liquid and rotating. The rotation shapes them into paraboloids that are accurate enough to be used as primary reflectors in telescopes. No correction of the shape is necessary. Spin-cast glass mirrors need correction because of distortions that arise during and after solidification.\n\n"}
{"id": "11083533", "url": "https://en.wikipedia.org/wiki?curid=11083533", "title": "Saturn C-3", "text": "Saturn C-3\n\nThe Saturn C-3 was the third rocket in the Saturn C series studied from 1959 to 1962. The design was for a three-stage launch vehicle that could launch to low Earth orbit and send to the Moon via trans-lunar injection.\n\nU.S. President Kennedy's proposal on May 25, 1961, of an explicit manned lunar landing goal spurred NASA to solidify its launch vehicle requirements for a lunar landing. A week earlier, William Fleming (Office of Space Flight Programs, NASA Headquarters) chaired an ad hoc committee to conduct a six-week study of the requirements for a lunar landing. Judging the direct ascent approach to be the most feasible, they concentrated their attention accordingly, and proposed circumlunar flights in late 1965 using the Saturn C-3 launch vehicle.\n\nIn early June 1961, Bruce Lundin, deputy director of the Lewis Research Center, led a week-long study of six different rendezvous possibilities. The alternatives included Earth-orbital rendezvous, lunar-orbital rendezvous, Earth and lunar rendezvous, and rendezvous on the lunar surface, employing Saturn C-1s, C-3s, and Nova designs. Lundin's committee concluded that rendezvous enjoyed distinct advantages over direct ascent and recommended an Earth-orbital rendezvous using two or three Saturn C-3s.\n\nNASA announced on September 7, 1961, that the government-owned Michoud Ordnance Plant near New Orleans, Louisiana, would be the site for fabrication and assembly of the Saturn C-3 first stage as well as larger vehicles in the Saturn program. Finalists were two government-owned plants in St. Louis and New Orleans. The height of the factory roof at Michoud meant that a launch vehicle with eight F-1 engines (Nova class, Saturn C-8) could not be built; four or five engines would have to be the maximum.\n\nThis decision ended consideration of a Nova class launch vehicle for direct ascent to the Moon or as heavy-lift companion with the Saturn C-3 for Earth orbit rendezvous.\n\nThe Marshall Space Flight Center in Huntsville, Alabama developed an Earth orbit rendezvous proposal (EOR) for the Apollo program in 1960-1961. The proposal used a series of small rockets half the size of a Saturn V to launch different components of a spacecraft headed to the Moon. These components would be assembled in orbit around the Earth, then sent to the Moon via trans-lunar injection. \nIn order to test and validate the feasibility of the EOR approach for the Apollo program, Project Gemini was founded with this objective: \"To effect rendezvous and docking with another vehicle (Agena target vehicle), and to maneuver the combined spacecraft using the propulsion system of the target vehicle.\"\n\nThe Saturn C-3 was the primary launch vehicle for Earth orbit rendezvous. The booster consisted of a first stage containing two Saturn V F-1 engines, a second stage containing four powerful J-2 engines, and the S-IV stage from a Saturn I booster. Only the S-IV stage of the Saturn C-3 was developed and flown, but all of the specified engines were used on the Saturn V rocket which took men to the moon.\n\nThe concept of lunar orbit rendezvous (LOR) was studied at Langley Research Center as early as 1960. John Houbolt's memorandum advocating LOR for lunar missions in November 1961 to Robert Seamans outlined the usage of the Saturn C-3 launch vehicle, and avoiding complex large boosters and lunar landers.\n\nAfter six months of further discussion NASA, in the summer of 1962, selected the lunar orbit rendezvous (LOR) proposal from Langley Research Center for the Apollo program. By the end of 1962, the Saturn C-3 design was deemed not necessary for Apollo program requirements as larger boosters (Saturn C-4, Saturn C-5) were then proposed, hence further work on the Saturn C-3 was cancelled.\n\nSince 1961 a number of variants of the Saturn C-3 have been studied, proposed, and funded. The most extensive studies focused on the Saturn C-3B variants before end of 1962, when lunar orbit rendezvous was selected and Saturn C-5 development approved. The common theme of these variants is a first stage with at least 3,044,000 lbf (13,540 kN) of sea-level thrust (SL). These designs used two or three Rocketdyne F-1 engines in a S-IB-2 or S-IC stage and diameters ranging from 8 to 10 meters (27 to 33 feet) that could lift up to to Low Earth Orbit (LEO).\n\nThe lack of a Saturn C-3 launch vehicle in 1965 created a large payload gap (LEO) between the Saturn IB's 21,000 kg (46,000 lb) capacity and the two-stage Saturn V's 75,000 kg (165,000 lb) capability. In the mid-1960s NASA's Marshall Space Flight Center (MSFC) initiated several studies for a launch vehicle to fill this payload capacity gap and to extend the capabilities of the Saturn family. Three companies provided proposals to MSFC for this requirement: Martin Marietta (builder of Atlas, Titan vehicles), Boeing (builder of S-1B and S-1C first stages), and North American (builder of the S-II second stage).\n\nThe Saturn C-3B revision (1961) increased the total thrust of the three stages to 17,200 kN. The diameter of the first stage (S-IB-2) was increased to 33 feet (10 meters). The eventual first stage for the Saturn V (S-IC) would use this same diameter, but add 8 meters to its length. A further consideration added a third F-1 engine to the first stage.\n\nThe S-II, second stage diameter would be 8.3 meters (326 inches) and 21.3 meters (70 feet) in length. The S-IV, third stage diameter would be 5.5 meters and 12.2 meters in length.\n\nThe Saturn C-3BN revision (1961) would use the NERVA for the third stage in this launch vehicle. The NERVA technology has been studied and proposed since mid-1950s for future space exploration.\n\nOn October 7, 1966 Boeing submitted a Final Report to the NASA Marshall Space Flight Center, \"Studies of Improved Saturn V Vehicles and Intermediate Payload Vehicles\". That report outlined the Saturn INT-20, an intermediate two-stage launch vehicle with a S-1C first stage using three or four F-1 engines, and a S-IVB as the second stage with one J-2 engine. The vehicle's payload capacity for LEO would be 45,000 to 60,000 kg, comparable to the earlier Saturn C-3 design (1961). Boeing projected delivery and first flight in 1970, based on a decision by 1967.\n\nThe need for a launch vehicle of Saturn C-3 capacity (45 metric tons to LEO) continued beyond the Apollo program. Cape Canaveral Air Force Station Space Launch Complex 37, initially designed to serve the Saturn I and I-B, was planned for eventual Saturn C-3 usage, but it was deactivated in 1972. In 2001, Boeing refurbished the complex for its Delta IV EELV launch vehicle. The Delta IV Heavy variant can only launch 22.5 metric tons to LEO.\nThe 1986 Space Shuttle Challenger disaster and 2010 Space Launch System program resulted in renewed proposals for Saturn C-3 derivatives using the Rocketdyne F-1A engines with existing booster cores and tooling (10m - Saturn S-IC stage; 8.4m - Space Shuttle external tank; 5.1m - Delta IV Common Booster Core).\n\nAfter the Space Shuttle \"Challenger\" disaster, the United States Air Force (USAF) and National Aeronautics and Space Administration (NASA) conducted a joint Advanced Launch System study (1987-1990). Hughes Aircraft and Boeing dusted off the earlier Saturn C-3 design and submitted their proposal for the Jarvis launch vehicle. \n\nThe Jarvis would be a three-stage rocket, 58 m (190 ft) in height and 8.38 m (27.5 ft) in diameter. Designed to lift 38 tons to LEO, it would utilize F-1 and J-2 rocket engines and tooling in storage from the Saturn V rocket program along with more recent Shuttle-era technologies to provide lower launch costs.\n\n, Dynetics announced they were teaming with Pratt & Whitney Rocketdyne to resurrect the Saturn V rocket's mighty F-1 engine to power NASA's Space Launch System planned heavy-lift launch vehicle, saying the Apollo-era engine will offer significantly more performance than solid-fueled boosters currently under development. \n\nDynetics of Huntsville, Alabama, is leading the contractor team proposing the F-1 engine design. Pratt & Whitney Rocketdyne is the bid's propulsion partner and engine builder. Cook, NASA's former manager of the scrapped Ares rocket program, said each of the two Dynetics boosters, on an SLS mission would be propelled by a pair of RP-1/LOX F-1Bs, an advanced variant of the F-1, (1.5 million pounds of thrust) which was used on the Saturn V, and the F-1A. Developed during the later stages of the Apollo program, the F-1A was test fired, but never flew. Several were crated and stored by Rocketdyne (later Pratt & Whitney Rocketdyne). The company has also maintained an F-1/F-1A knowledge retention program for its engineers for the entire period the engine has been mothballed. Dynetics is now performing tests on engine components pulled from storage.\"Each of those engines (F-1A) can get up to 1.8 million pounds of thrust (8,000 kN),\" Cook said in an interview Wednesday. \"This booster is a very simple, very standard booster. It's 18 feet (5.5 m) in diameter. It uses the same attach points as the current five-segment solid rocket booster.\" \n\nThe Dynetics booster would attach at these points, in the SLS parallel staging design, which differs from the Saturn rockets' serial staging design. Because it applies thrust to an upper thrust beam in the SLS core, it lifts at the top rather than at the bottom (Saturn S-IC stage had a thrust structure). The proposed Dynetics booster is similar to the first stage of the Saturn C-3 in that it would employ two F-1 heritage engines.\n\n\n"}
{"id": "888895", "url": "https://en.wikipedia.org/wiki?curid=888895", "title": "Sector model", "text": "Sector model\n\nThe sector model, also known as the Hoyt model, is a model of urban land use proposed in 1939 by land economist Homer Hoyt. It is a modification of the concentric zone model of city development. The benefits of the application of this model include the fact it allows for an outward progression of growth. As with all simple models of such complex phenomena, its validity is limited.\n\nThis model applies to numerous British cities. Also, if it is turned 90 degrees counter-clockwise it fits the city of Mönchengladbach reasonably accurately. This may be because of the age of the cities when transportation was a key, as a general rule older cities follow the Hoyt model and more recent cities follow the Burgess (concentric zone) model.\n\nThe theory is based on early twentieth-century rail transport and does not make allowances for private cars that enable commuting from cheaper land outside city boundaries. This occurred in Calgary in the 1930s when many near-slums were established outside the city but close to the termini of the street car lines. These are now incorporated into the city boundary but are pockets of low cost housing in medium cost areas. The theory also does not take into account the new concepts of edge cities and boomburbs, which began to emerge in the 1980s, after the creation of the model. Since its creation, the traditional Central Business District has diminished in importance as many retail and office buildings have moved into the suburbs.\n\n\n"}
{"id": "18717981", "url": "https://en.wikipedia.org/wiki?curid=18717981", "title": "Sociology", "text": "Sociology\n\nSociology is the scientific study of society, patterns of social relationships, social interaction, and culture of everyday life. It is a social science that uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order, acceptance, and change or social evolution. Many sociologists aim to conduct research that may be applied directly to social policy and welfare, while others focus primarily on refining the theoretical understanding of social processes. Subject matter ranges from the micro-sociology level of individual agency and interaction to the macro level of systems and the social structure.\n\nThe different traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects, such as health, medical, economy, military and penal institutions, the Internet, education, social capital, and the role of social activity in the development of scientific knowledge.\n\nThe range of social scientific methods has also expanded. Social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century led to increasingly interpretative, hermeneutic, and philosophic approaches towards the analysis of society. Conversely, the end of the 1990s and the beginning of the 2000s have seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis.\n\nSocial research informs politicians and policy makers, educators, planners, legislators, administrators, developers, business magnates, managers, social workers, non-governmental organizations, non-profit organizations, and people interested in resolving social issues in general. There is often a great deal of crossover between social research, market research, and other statistical fields.\n\nSociological reasoning predates the foundation of the discipline. Social analysis has origins in the common stock of Western knowledge and philosophy, and has been carried out from as far back as the time of ancient Greek philosopher Plato, if not before. The origin of the survey, i.e., the collection of information from a sample of individuals, can be traced back to at least the Domesday Book in 1086, while ancient philosophers such as Confucius wrote about the importance of social roles. There is evidence of early sociology in medieval Arab writings. Some sources consider Ibn Khaldun, a 14th-century Arab Islamic scholar from North Africa (Tunisia), to have been the first sociologist and father of sociology (see Branches of the early Islamic philosophy); his \"Muqaddimah\" was perhaps the first work to advance social-scientific reasoning on social cohesion and social conflict.\n\nThe word \"sociology\" (or \"sociologie\") is derived from both Latin and Greek origins. The Latin word: \"socius\", \"companion\"; the suffix \"-logy\", \"the study of\" from Greek -λογία from λόγος, \"lógos\", \"word\", \"knowledge\". It was first coined in 1780 by the French essayist Emmanuel-Joseph Sieyès (1748–1836) in an unpublished manuscript. \"Sociology\" was later defined independently by the French philosopher of science, Auguste Comte (1798–1857) in 1838 as a new way of looking at society. Comte had earlier used the term \"social physics\", but that had subsequently been appropriated by others, most notably the Belgian statistician Adolphe Quetelet. Comte endeavoured to unify history, psychology, and economics through the scientific understanding of the social realm. Writing shortly after the malaise of the French Revolution, he proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in \"The Course in Positive Philosophy\" (1830–1842) and \"A General View of Positivism\" (1848). Comte believed a positivist stage would mark the final era, after conjectural theological and metaphysical phases, in the progression of human understanding. In observing the circular dependence of theory and observation in science, and having classified the sciences, Comte may be regarded as the first philosopher of science in the modern sense of the term.\n\nHerbert Spencer (27 April 1820 – 8 December 1903) was one of the most popular and influential 19th-century sociologists. It is estimated that he sold one million books in his lifetime, far more than any other sociologist at the time. So strong was his influence that many other 19th-century thinkers, including Émile Durkheim, defined their ideas in relation to his. Durkheim's \"Division of Labour in Society\" is to a large extent an extended debate with Spencer from whose sociology, many commentators now agree, Durkheim borrowed extensively. Also a notable biologist, Spencer coined the term \"survival of the fittest\". While Marxian ideas defined one strand of sociology, Spencer was a critic of socialism as well as strong advocate for a laissez-faire style of government. His ideas were closely observed by conservative political circles, especially in the United States and England.\n\nThe first formal Department of Sociology in the world was established by Albion Small - at the invitation of William Rainey Harper - at the University of Chicago in 1892, and the American Journal of Sociology was founded shortly thereafter in 1895 by Small as well. However, the institutionalization of sociology as an academic discipline was chiefly led by Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. While Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method, maintaining that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisting that they may retain the same objectivity, rationalism, and approach to causality. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his \"Rules of the Sociological Method\" (1895). For Durkheim, sociology could be described as the \"science of institutions, their genesis and their functioning\".\n\nDurkheim's monograph \"Suicide\" (1897) is considered a seminal work in statistical analysis by contemporary sociologists. \"Suicide\" is a case study of variations in suicide rates among Catholic and Protestant populations, and served to distinguish sociological analysis from psychology or philosophy. It also marked a major contribution to the theoretical concept of structural functionalism. By carefully examining suicide statistics in different police districts, he attempted to demonstrate that Catholic communities have a lower suicide rate than that of Protestants, something he attributed to social (as opposed to individual or psychological) causes. He developed the notion of objective \"sui generis\" \"social facts\" to delineate a unique empirical object for the science of sociology to study. Through such studies he posited that sociology would be able to determine whether any given society is 'healthy' or 'pathological', and seek social reform to negate organic breakdown or \"social anomie\".\n\nSociology quickly evolved as an academic response to the perceived challenges of modernity, such as industrialization, urbanization, secularization, and the process of \"rationalization\". The field predominated in continental Europe, with British anthropology and statistics generally following on a separate trajectory. By the turn of the 20th century, however, many theorists were active in the English-speaking world. Few early sociologists were confined strictly to the subject, interacting also with economics, jurisprudence, psychology and philosophy, with theories being appropriated in a variety of different fields. Since its inception, sociological epistemology, methods, and frames of inquiry, have significantly expanded and diverged.\n\nDurkheim, Marx, and the German theorist Max Weber (1864–1920) are typically cited as the three principal architects of sociology. Herbert Spencer, William Graham Sumner, Lester F. Ward, W. E. B. Du Bois, Vilfredo Pareto, Alexis de Tocqueville, Werner Sombart, Thorstein Veblen, Ferdinand Tönnies, Georg Simmel and Karl Mannheim are often included on academic curricula as founding theorists. Curricula also may include Charlotte Perkins Gilman, Marianne Weber and Friedrich Engels as founders of the feminist tradition in sociology. Each key figure is associated with a particular theoretical perspective and orientation.\n\nThe overarching methodological principle of positivism is to conduct sociology in broadly the same manner as natural science. An emphasis on empiricism and the scientific method is sought to provide a tested foundation for sociological research based on the assumption that the only authentic knowledge is scientific knowledge, and that such knowledge can only arrive by positive affirmation through scientific methodology.\n\nThe term has long since ceased to carry this meaning; there are no fewer than twelve distinct epistemologies that are referred to as positivism. Many of these approaches do not self-identify as \"positivist\", some because they themselves arose in opposition to older forms of positivism, and some because the label has over time become a term of abuse by being mistakenly linked with a theoretical empiricism. The extent of antipositivist criticism has also diverged, with many rejecting the scientific method and others only seeking to amend it to reflect 20th-century developments in the philosophy of science. However, positivism (broadly understood as a scientific approach to the study of society) remains dominant in contemporary sociology, especially in the United States.\n\nLoïc Wacquant distinguishes three major strains of positivism: Durkheimian, Logical, and Instrumental. None of these are the same as that set forth by Comte, who was unique in advocating such a rigid (and perhaps optimistic) version. While Émile Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method. Durkheim maintained that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisted that they should retain the same objectivity, rationalism, and approach to causality. He developed the notion of objective \"sui generis\" \"social facts\" to delineate a unique empirical object for the science of sociology to study.\n\nThe variety of positivism that remains dominant today is termed \"instrumental positivism\". This approach eschews epistemological and metaphysical concerns (such as the nature of social facts) in favour of methodological clarity, replicability, reliability and validity. This positivism is more or less synonymous with quantitative research, and so only resembles older positivism in practice. Since it carries no explicit philosophical commitment, its practitioners may not belong to any particular school of thought. Modern sociology of this type is often credited to Paul Lazarsfeld, who pioneered large-scale survey studies and developed statistical techniques for analysing them. This approach lends itself to what Robert K. Merton called middle-range theory: abstract statements that generalize from segregated hypotheses and empirical regularities rather than starting with an abstract idea of a social whole.\n\nReactions against social empiricism began when German philosopher Hegel voiced opposition to both empiricism, which he rejected as uncritical, and determinism, which he viewed as overly mechanistic. Karl Marx's methodology borrowed from Hegelian dialecticism but also a rejection of positivism in favour of critical analysis, seeking to supplement the empirical acquisition of \"facts\" with the elimination of illusions. He maintained that appearances need to be critiqued rather than simply documented. Early hermeneuticians such as Wilhelm Dilthey pioneered the distinction between natural and social science ('Geisteswissenschaft'). Various neo-Kantian philosophers, phenomenologists and human scientists further theorized how the analysis of the social world differs to that of the natural world due to the irreducibly complex aspects of human society, culture, and being.\n\nIn the Italian context of development of social sciences and of sociology in particular, there are oppositions to the first foundation of the discipline, sustained by speculative philosophy in accordance with the antiscientific tendencies matured by critique of positivism and evolutionism, so a tradition Progressist struggles to establish itself (Cfr. Guglielmo Rinzivillo, \"La scienza e l'oggetto. Autocritica del sapere strategico\", Milan, Franco Angeli, 2010, p. 52 e sg., ). \n\nAt the turn of the 20th century the first generation of German sociologists formally introduced methodological anti-positivism, proposing that research should concentrate on human cultural norms, values, symbols, and social processes viewed from a resolutely subjective perspective. Max Weber argued that sociology may be loosely described as a science as it is able to identify causal relationships of human \"social action\"—especially among \"ideal types\", or hypothetical simplifications of complex social phenomena. As a non-positivist, however, Weber sought relationships that are not as \"historical, invariant, or generalisable\" as those pursued by natural scientists. Fellow German sociologist, Ferdinand Tönnies, theorised on two crucial abstract concepts with his work on \"Gemeinschaft and Gesellschaft\" (lit. \"community\" and \"society\"). Tönnies marked a sharp line between the realm of concepts and the reality of social action: the first must be treated axiomatically and in a deductive way (\"pure sociology\"), whereas the second empirically and inductively (\"applied sociology\").\n\nBoth Weber and Georg Simmel pioneered the \"Verstehen\" (or 'interpretative') method in social science; a systematic process by which an outside observer attempts to relate to a particular cultural group, or indigenous people, on their own terms and from their own point of view. Through the work of Simmel, in particular, sociology acquired a possible character beyond positivist data-collection or grand, deterministic systems of structural law. Relatively isolated from the sociological academy throughout his lifetime, Simmel presented idiosyncratic analyses of modernity more reminiscent of the phenomenological and existential writers than of Comte or Durkheim, paying particular concern to the forms of, and possibilities for, social individuality. His sociology engaged in a neo-Kantian inquiry into the limits of perception, asking 'What is society?' in a direct allusion to Kant's question 'What is nature?'\n\nThe first college course entitled \"Sociology\" was taught in the United States at Yale in 1875 by William Graham Sumner. In 1883 Lester F. Ward, the first president of the American Sociological Association, published \"Dynamic Sociology—Or Applied social science as based upon statical sociology and the less complex sciences\" and attacked the laissez-faire sociology of Herbert Spencer and Sumner. Ward's 1200 page book was used as core material in many early American sociology courses. In 1890, the oldest continuing American course in the modern tradition began at the University of Kansas, lectured by Frank W. Blackmar. The Department of Sociology at the University of Chicago was established in 1892 by Albion Small, who also published the first sociology textbook: An introduction to the study of society 1894. George Herbert Mead and Charles Cooley, who had met at the University of Michigan in 1891 (along with John Dewey), would move to Chicago in 1894. Their influence gave rise to social psychology and the symbolic interactionism of the modern Chicago School. The \"American Journal of Sociology\" was founded in 1895, followed by the \"American Sociological Association\" (ASA) in 1905. The sociological \"canon of classics\" with Durkheim and Max Weber at the top owes in part to Talcott Parsons, who is largely credited with introducing both to American audiences. Parsons consolidated the sociological tradition and set the agenda for American sociology at the point of its fastest disciplinary growth. Sociology in the United States was less historically influenced by Marxism than its European counterpart, and to this day broadly remains more statistical in its approach.\n\nThe first sociology department to be established in the United Kingdom was at the London School of Economics and Political Science (home of the \"British Journal of Sociology\") in 1904. Leonard Trelawny Hobhouse and Edvard Westermarck became the lecturers in the discipline at the University of London in 1907. Harriet Martineau, an English translator of Comte, has been cited as the first female sociologist. In 1909 the \"Deutsche Gesellschaft für Soziologie\" (German Sociological Association) was founded by Ferdinand Tönnies and Max Weber, among others. Weber established the first department in Germany at the Ludwig Maximilian University of Munich in 1919, having presented an influential new antipositivist sociology. In 1920, Florian Znaniecki set up the first department in Poland. The \"Institute for Social Research\" at the University of Frankfurt (later to become the Frankfurt School of critical theory) was founded in 1923. International co-operation in sociology began in 1893, when René Worms founded the \"\", an institution later eclipsed by the much larger International Sociological Association (ISA), founded in 1949.\n\nThe contemporary discipline of sociology is theoretically multi-paradigmatic as a result of the contentions of classical social theory. In Randall Collins' well-cited survey of sociological theory he retroactively labels various theorists as belonging to four theoretical traditions: Functionalism, Conflict, Symbolic Interactionism, and Utilitarianism. Modern sociological theory descends predominately from functionalist (Durkheim) and conflict-centred (Marx and Weber) accounts of social structure, as well as the symbolic interactionist tradition consisting of micro-scale structural (Simmel) and pragmatist (Mead, Cooley) theories of social interaction. Utilitarianism, also known as Rational Choice or Social Exchange, although often associated with economics, is an established tradition within sociological theory. Lastly, as argued by Raewyn Connell, a tradition that is often forgotten is that of Social Darwinism, which brings the logic of Darwinian biological evolution and applies it to people and societies. This tradition often aligns with classical functionalism. It was the dominant theoretical stance in American sociology from around 1881 to 1915 and is associated with several founders of sociology, primarily Herbert Spencer, Lester F. Ward and William Graham Sumner. Contemporary sociological theory retains traces of each of these traditions and they are by no means mutually exclusive.\n\nA broad historical paradigm in both sociology and anthropology, functionalism addresses the social structure, referred to as social organization in among the classical theorists, as a whole and regarding the necessary function of its constituent elements. A common analogy (popularized by Herbert Spencer) is to regard norms and institutions as 'organs' that work towards the proper-functioning of the entire 'body' of society. The perspective was implicit in the original sociological positivism of Comte but was theorized in full by Durkheim, again with respect to observable, structural laws. Functionalism also has an anthropological basis in the work of theorists such as Marcel Mauss, Bronisław Malinowski and Radcliffe-Brown. It is in Radcliffe-Brown's specific usage that the prefix 'structural' emerged. Classical functionalist theory is generally united by its tendency towards biological analogy and notions of social evolutionism, in that the basic form of society would increase in complexity and those forms of social organization that promoted solidarity would eventually overcome social disorganization. As Giddens states: \"Functionalist thought, from Comte onwards, has looked particularly towards biology as the science providing the closest and most compatible model for social science. Biology has been taken to provide a guide to conceptualizing the structure and the function of social systems and to analysing processes of evolution via mechanisms of adaptation. functionalism strongly emphasizes the pre-eminence of the social world over its individual parts (i.e. its constituent actors, human subjects).\"\n\nFunctionalist theories emphasize \"cohesive systems\" and are often contrasted with \"conflict theories\", which critique the overarching socio-political system or emphasize the inequality between particular groups. The following quotes from Durkheim and Marx epitomize the political, as well as theoretical, disparities, between functionalist and conflict thought respectively:\n\nSymbolic interaction; often associated with Interactionism, Phenomenological sociology, Dramaturgy, Interpretivism, is a sociological tradition that places emphasis on subjective meanings and the empirical unfolding of social processes, generally accessed through micro-analysis. This tradition emerged in the \"Chicago School\" of the 1920s and 1930s, which prior to World War II \"had been \"the\" center of sociological research and graduate study\". The approach focuses on creating a framework for building a theory that sees society as the product of the everyday interactions of individuals. Society is nothing more than the shared reality that people construct as they interact with one another. This approach sees people interacting in countless settings using symbolic communications to accomplish the tasks at hand. Therefore, society is a complex, ever-changing mosaic of subjective meanings. Some critics of this approach argue that it only looks at what is happening in a particular social situation, and disregards the effects that culture, race or gender (i.e. social-historical structures) may have in that situation. Some important sociologists associated with this approach include Max Weber, George Herbert Mead, Erving Goffman, George Homans and Peter Blau. It is also in this tradition that the radical-empirical approach of Ethnomethodology emerges from the work of Harold Garfinkel.\n\nUtilitarianism is often referred to as exchange theory or rational choice theory in the context of sociology. This tradition tends to privilege the agency of individual rational actors and assumes that within interactions individuals always seek to maximize their own self-interest. As argued by Josh Whitford, rational actors are assumed to have four basic elements, the individual has (1) \"a knowledge of alternatives,\" (2) \"a knowledge of, or beliefs about the consequences of the various alternatives,\" (3) \"an ordering of preferences over outcomes,\" (4) \"A decision rule, to select among the possible alternatives\" Exchange theory is specifically attributed to the work of George C. Homans, Peter Blau and Richard Emerson. Organizational sociologists James G. March and Herbert A. Simon noted that an individual's rationality is bounded by the context or organizational setting. The utilitarian perspective in sociology was, most notably, revitalized in the late 20th century by the work of former ASA president James Coleman.\n\nFollowing the decline of theories of sociocultural evolution, in the United States, the interactionism of the Chicago School dominated American sociology. As Anselm Strauss describes, \"We didn't think symbolic interaction was a perspective in sociology; we thought it was sociology.\" After World War II, mainstream sociology shifted to the survey-research of Paul Lazarsfeld at Columbia University and the general theorizing of Pitirim Sorokin, followed by Talcott Parsons at Harvard University. Ultimately, \"the failure of the Chicago, Columbia, and Wisconsin [sociology] departments to produce a significant number of graduate students interested in and committed to general theory in the years 1936–45 was to the advantage of the Harvard department.\" As Parsons began to dominate general theory, his work predominately referenced European sociology—almost entirely omitting citations of both the American tradition of sociocultural-evolution as well as pragmatism. In addition to Parsons' revision of the sociological canon (which included Marshall, Pareto, Weber and Durkheim), the lack of theoretical challenges from other departments nurtured the rise of the Parsonian structural-functionalist movement, which reached its crescendo in the 1950s, but by the 1960s was in rapid decline.\n\nBy the 1980s, most functionalisms in Europe had broadly been replaced by conflict-oriented approaches and to many in the discipline, functionalism was considered \"as dead as a dodo.\" \"According to Giddens, the orthodox consensus terminated in the late 1960s and 1970s as the middle ground shared by otherwise competing perspectives gave way and was replaced by a baffling variety of competing perspectives. This third 'generation' of social theory includes phenomenologically inspired approaches, critical theory, ethnomethodology, symbolic interactionism, structuralism, post-structuralism, and theories written in the tradition of hermeneutics and ordinary language philosophy.\"\n\nWhile some conflict approaches also gained popularity in the United States, the mainstream of the discipline instead shifted to a variety of empirically oriented middle-range theories with no single overarching, or \"grand\", theoretical orientation. John Levi Martin refers to this \"golden age of methodological unity and theoretical calm\" as the \"Pax Wisconsana\", as it reflected the composition of the sociology department at the University of Wisconsin–Madison: numerous scholars working on separate projects with little contention. Omar Lizardo describes the \"Pax Wisconsana\" as: \"a Midwestern flavored, Mertonian resolution of the theory/method wars in which [sociologists] all agreed on at least two working hypotheses: (1) grand theory' is a waste of time; (2) [and] good theory has to be good to think with or goes in the trash bin.\" Despite the aversion to grand theory in the later half of the 20th century, several new traditions have emerged that propose various syntheses: structuralism, post-structuralism, cultural sociology and systems theory.\n\nThe structuralist movement originated primarily from the work of Durkheim as interpreted by two European anthropologists. Anthony Giddens' theory of structuration draws on the linguistic theory of Ferdinand de Saussure and the French anthropologist Claude Lévi-Strauss. In this context, 'structure' refers not to 'social structure' but to the semiotic understanding of human culture as a system of signs. One may delineate four central tenets of structuralism: First, structure is what determines the structure of a whole. Second, structuralists believe that every system has a structure. Third, structuralists are interested in 'structural' laws that deal with coexistence rather than changes. Finally, structures are the 'real things' beneath the surface or the appearance of meaning.\n\nThe second tradition of structuralist thought, contemporaneous with Giddens, emerges from the American school of social network analysis, spearheaded by the Harvard Department of Social Relations led by Harrison White and his students in the 1970s and 1980s. This tradition of structuralist thought argues that, rather than semiotics, social structure is networks of patterned social relations. And, rather than Levi-Strauss, this school of thought draws on the notions of structure as theorized by Levi-Strauss' contemporary anthropologist, Radcliffe-Brown. Some refer to this as \"network structuralism,\" and equate it to \"British structuralism\" as opposed to the \"French structuralism\" of Levi-Strauss.\n\nPost-structuralist thought has tended to reject 'humanist' assumptions in the conduct of social theory. Michel Foucault provides a potent critique in his \"Archaeology of the Human Sciences\", though Habermas and Rorty have both argued that Foucault merely replaces one such system of thought with another. The dialogue between these intellectuals highlights a trend in recent years for certain schools of sociology and philosophy to intersect. The anti-humanist position has been associated with \"postmodernism\", a term used in specific contexts to describe an \"era\" or \"phenomena\", but occasionally construed as a \"method\".\n\nOverall, there is a strong consensus regarding the central problems of sociological theory, which are largely inherited from the classical theoretical traditions. This consensus is: how to link, transcend or cope with the following \"big three\" dichotomies: subjectivity and objectivity, structure and agency, and synchrony and diachrony. The first deals with \"knowledge\", the second with \"action\", and the last with \"time\". Lastly, sociological theory often grapples with the problem of integrating or transcending the divide between micro, meso and macro-scale social phenomena, which is a subset of all three central problems.\n\nThe problem of subjectivity and objectivity can be divided into a concern over the general possibilities of social actions, and, on the other hand, the specific problem of social scientific knowledge. In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is \"intersubjectivity\" achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Also, some qualitative methods take a radical approach to objective description in situ.\n\nThe latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain. Bourdieu puts this problem rather succinctly:\n\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context 'agency' refers to the capacity of individuals to act independently and make free choices, whereas 'structure' relates to factors that limit or affect the choices and actions of individuals (such as social class, religion, gender, ethnicity, and so on). Discussions over the primacy of either structure and agency relate to the core of sociological epistemology (\"What is the social world made of?\", \"What is a cause in the social world, and what is an effect?\"). A perennial question within this debate is that of \"social reproduction\": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals?\n\nSynchrony and diachrony, or statics and dynamics, within social theory are terms that refer to a distinction emerging out of the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. The former slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a \"language\", while diachrony would refer to unfolding processes like actual \"speech\". In Anthony Giddens' introduction to \"Central Problems in Social Theory\", he states that, \"in order to show the interdependence of action and structure ... we must grasp the time space relations inherent in the constitution of all social interaction.\" And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data.\n\nMany people divide sociological research methods into two broad categories, although many others see research methods as a continuum:\n\n\nMany sociologists are divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics.\" Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the greater public, though this judgment continues to be challenged by antipositivists.\n\nThe choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative \"and\" qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to gain statistical patterns or a target sample, and then combined with a qualitative interview to determine the play of agency.\n\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population. In quantitative research, statistics are used to draw inferences from this sample regarding the population as a whole. The process of selecting a sample is referred to as 'sampling'. While it is usually best to sample randomly, concern with differences between specific subpopulations sometimes calls for stratified sampling. Conversely, the impossibility of random sampling sometimes necessitates nonprobability sampling, such as convenience sampling or snowball sampling.\n\n\"The following list of research methods is neither exclusive nor exhaustive:\"\n\nSociologists increasingly draw upon computationally intensive methods to analyse and model social phenomena. Using computer simulations, artificial intelligence, text mining, complex statistical methods, and new analytic approaches like social network analysis and social sequence analysis, computational sociology develops and tests theories of complex social processes through bottom-up modelling of social interactions.\n\nAlthough the subject matter and methodologies in social science differ from those in natural science or computer science, several of the approaches used in contemporary social simulation originated from fields such as physics and artificial intelligence. By the same token, some of the approaches that originated in computational sociology have been imported into the natural sciences, such as measures of network centrality from the fields of social network analysis and network science. In relevant literature, computational sociology is often related to the study of social complexity. Social complexity concepts such as complex systems, non-linear interconnection among macro and micro process, and emergence, have entered the vocabulary of computational sociology. A practical and well-known example is the construction of a computational model in the form of an \"artificial society\", by which researchers can analyse the structure of a social system.\n\nSociologists' approach to culture can be divided into a \"sociology of culture\" and \"cultural sociology\"—the terms are similar, though not entirely interchangeable. The sociology of culture is an older term, and considers some topics and objects as more-or-less \"cultural\" than others. Conversely, cultural sociology sees all social phenomena as inherently cultural. Sociology of culture often attempts to explain certain cultural phenomena as a product of social processes, while cultural sociology sees culture as a potential explanation of social phenomena.\n\nFor Simmel, culture referred to \"the cultivation of individuals through the agency of external forms which have been objectified in the course of history\". While early theorists such as Durkheim and Mauss were influential in cultural anthropology, sociologists of culture are generally distinguished by their concern for modern (rather than primitive or ancient) society. Cultural sociology often involves the hermeneutic analysis of words, artefacts and symbols, or ethnographic interviews. However, some sociologists employ historical-comparative or quantitative techniques in the analysis of culture, Weber and Bourdieu for instance. The subfield is sometimes allied with critical theory in the vein of Theodor W. Adorno, Walter Benjamin, and other members of the Frankfurt School. Loosely distinct from the sociology of culture is the field of cultural studies. Birmingham School theorists such as Richard Hoggart and Stuart Hall questioned the division between \"producers\" and \"consumers\" evident in earlier theory, emphasizing the reciprocity in the production of texts. Cultural Studies aims to examine its subject matter in terms of cultural practices and their relation to power. For example, a study of a subculture (such as white working class youth in London) would consider the social practices of the group as they relate to the dominant class. The \"cultural turn\" of the 1960s ultimately placed culture much higher on the sociological agenda.\n\nSociology of literature, film, and art is a subset of the sociology of culture. This field studies the social production of artistic objects and its social implications. A notable example is Pierre Bourdieu's 1992 \"Les Règles de L'Art: Genèse et Structure du Champ Littéraire\", translated by Susan Emanuel as \"Rules of Art: Genesis and Structure of the Literary Field\" (1996). None of the founding fathers of sociology produced a detailed study of art, but they did develop ideas that were subsequently applied to literature by others. Marx's theory of ideology was directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Weber's theory of modernity as cultural rationalization, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Adorno and Jürgen Habermas. Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's own work is clearly indebted to Marx, Weber and Durkheim.\n\nCriminologists analyse the nature, causes, and control of criminal activity, drawing upon methods across sociology, psychology, and the behavioural sciences. The sociology of deviance focuses on actions or behaviours that violate norms, including both formally enacted rules (e.g., crime) and informal violations of cultural norms. It is the remit of sociologists to study why these norms exist; how they change over time; and how they are enforced. The concept of social disorganization is when the broader social systems leads to violations of norms. For instance, Robert K. Merton produced a typology of deviance, which includes both individual and system level causal explanations of deviance.\n\nThe study of law played a significant role in the formation of classical sociology. Durkheim famously described law as the \"visible symbol\" of social solidarity. The sociology of law refers to both a sub-discipline of sociology and an approach within the field of legal studies. Sociology of law is a diverse field of study that examines the interaction of law with other aspects of society, such as the development of legal institutions and the effect of laws on social change and vice versa. For example, an influential recent work in the field relies on statistical analyses to argue that the increase in incarceration in the US over the last 30 years is due to changes in law and policing and not to an increase in crime; and that this increase significantly contributes to maintaining racial stratification.\n\nThe sociology of communications and information technologies includes \"the social aspects of computing, the Internet, new media, computer networks, and other communication and information technologies\".\n\nThe Internet is of interest to sociologists in various ways; most practically as a tool for research and as a discussion platform. The sociology of the Internet in the broad sense regards the analysis of online communities (e.g. newsgroups, social networking sites) and virtual worlds, thus there is often overlap with community sociology. Online communities may be studied statistically through network analysis or interpreted qualitatively through virtual ethnography. Moreover, organizational change is catalysed through new media, thereby influencing social change at-large, perhaps forming the framework for a transformation from an industrial to an informational society. One notable text is Manuel Castells' \"The Internet Galaxy\"—the title of which forms an inter-textual reference to Marshall McLuhan's \"The Gutenberg Galaxy\". Closely related to the sociology of the Internet, is digital sociology, which expands the scope of study to address not only the internet but also the impact of the other digital media and devices that have emerged since the first decade of the twenty-first century.\n\nAs with cultural studies, media study is a distinct discipline that owes to the convergence of sociology and other social sciences and humanities, in particular, literary criticism and critical theory. Though the production process or the critique of aesthetic forms is not in the remit of sociologists, analyses of socializing factors, such as ideological effects and audience reception, stem from sociological theory and method. Thus the 'sociology of the media' is not a subdiscipline \"per se\", but the media is a common and often-indispensable topic.\n\nThe term \"economic sociology\" was first used by William Stanley Jevons in 1879, later to be coined in the works of Durkheim, Weber and Simmel between 1890 and 1920. Economic sociology arose as a new approach to the analysis of economic phenomena, emphasizing class relations and modernity as a philosophical concept. The relationship between capitalism and modernity is a salient issue, perhaps best demonstrated in Weber's \"The Protestant Ethic and the Spirit of Capitalism\" (1905) and Simmel's \"The Philosophy of Money\" (1900). The contemporary period of economic sociology, also known as \"new economic sociology\", was consolidated by the 1985 work of Mark Granovetter titled \"Economic Action and Social Structure: The Problem of Embeddedness\". This work elaborated the concept of embeddedness, which states that economic relations between individuals or firms take place within existing social relations (and are thus structured by these relations as well as the greater social structures of which those relations are a part). Social network analysis has been the primary methodology for studying this phenomenon. Granovetter's theory of the strength of weak ties and Ronald Burt's concept of structural holes are two best known theoretical contributions of this field.\n\nThe sociology of work, or industrial sociology, examines \"the direction and implications of trends in technological change, globalization, labour markets, work organization, managerial practices and employment relations to the extent to which these trends are intimately related to changing patterns of inequality in modern societies and to the changing experiences of individuals and families the ways in which workers challenge, resist and make their own contributions to the patterning of work and shaping of work institutions.\"\n\nThe sociology of education is the study of how educational institutions determine social structures, experiences, and other outcomes. It is particularly concerned with the schooling systems of modern industrial societies. A classic 1966 study in this field by James Coleman, known as the \"Coleman Report\", analysed the performance of over 150,000 students and found that student background and socioeconomic status are much more important in determining educational outcomes than are measured differences in school resources (\"i.e.\" per pupil spending). The controversy over \"school effects\" ignited by that study has continued to this day. The study also found that socially disadvantaged black students profited from schooling in racially mixed classrooms, and thus served as a catalyst for desegregation busing in American public schools.\n\nEnvironmental sociology is the study of human interactions with the natural environment, typically emphasizing human dimensions of environmental problems, social impacts of those problems, and efforts to resolve them. As with other sub-fields of sociology, scholarship in environmental sociology may be at one or multiple levels of analysis, from global (e.g. world-systems) to local, societal to individual. Attention is paid also to the processes by which environmental problems become \"defined\" and \"known\" to humans. As argued by notable environmental sociologist John Bellamy Foster, the predecessor to modern environmental sociology is Marx's analysis of the metabolic rift, which influenced contemporary thought on sustainability. Environmental sociology is often interdisciplinary and overlaps with the sociology of risk, rural sociology and the sociology of disaster.\n\nHuman ecology deals with interdisciplinary study of the relationship between humans and their natural, social, and built environments. In addition to Environmental sociology, this field overlaps with architectural sociology, urban sociology, and to some extent visual sociology. In turn, visual sociology—which is concerned with all visual dimensions of social life—overlaps with media studies in that it uses photography, film and other technologies of media.\n\nSocial pre-wiring deals with the study of fetal social behavior and social interactions in a multi-fetal environment. Specifically, social pre-wiring refers to the ontogeny of social interaction. Also informally referred to as, \"wired to be social.\" The theory questions whether there is a propensity to socially oriented action already present \"before\" birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social.\n\nCircumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be contributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics.\n\nPrincipal evidence of this theory is uncovered by examining Twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed.\n\nThe social pre-wiring hypothesis was proved correct, \"The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions.\".\n\nFamily, gender and sexuality form a broad area of inquiry studied in many sub-fields of sociology. A family is a group of people who are related by kinship ties :- Relations of blood / marriage / civil partnership or adoption. The family unit is one of the most important social institutions found in some form in nearly all known societies. It is the basic unit of social organization and plays a key role in socializing children into the culture of their society. The sociology of the family examines the family, as an institution and unit of socialization, with special concern for the comparatively modern historical emergence of the nuclear family and its distinct gender roles. The notion of \"childhood\" is also significant. As one of the more basic institutions to which one may apply sociological perspectives, the sociology of the family is a common component on introductory academic curricula. Feminist sociology, on the other hand, is a normative sub-field that observes and critiques the cultural categories of gender and sexuality, particularly with respect to power and inequality. The primary concern of feminist theory is the patriarchy and the systematic oppression of women apparent in many societies, both at the level of small-scale interaction and in terms of the broader social structure. Feminist sociology also analyses how gender interlocks with race and class to produce and perpetuate social inequalities. \"How to account for the differences in definitions of femininity and masculinity and in sex role across different societies and historical periods\" is also a concern. Social psychology of gender, on the other hand, uses experimental methods to uncover the microprocesses of gender stratification. For example, one recent study has shown that resume evaluators penalize women for motherhood while giving a boost to men for fatherhood.\n\nThe sociology of health and illness focuses on the social effects of, and public attitudes toward, illnesses, diseases, mental health and disabilities. This sub-field also overlaps with gerontology and the study of the ageing process. Medical sociology, by contrast, focuses on the inner-workings of medical organizations and clinical institutions. In Britain, sociology was introduced into the medical curriculum following the Goodenough Report (1944).\n\nThe sociology of the body and embodiment takes a broad perspective on the idea of \"the body\" and includes \"a wide range of embodied dynamics including human and non-human bodies, morphology, human reproduction, anatomy, body fluids, biotechnology, genetics. This often intersects with health and illness, but also theories of bodies as political, social, cultural, economic and ideological productions. The ISA maintains a Research Committee devoted to \"The Body in the Social Sciences\".\n\nA subfield of the sociology of health and illness that overlaps with cultural sociology is the study of death, dying and bereavement, sometimes referred to broadly as the sociology of death. This topic is exemplifed by the work of Douglas Davies and Michael C. Kearl.\n\nThe sociology of knowledge is the study of the relationship between human thought and the social context within which it arises, and of the effects prevailing ideas have on societies. The term first came into widespread use in the 1920s, when a number of German-speaking theorists, most notably Max Scheler, and Karl Mannheim, wrote extensively on it. With the dominance of functionalism through the middle years of the 20th century, the sociology of knowledge tended to remain on the periphery of mainstream sociological thought. It was largely reinvented and applied much more closely to everyday life in the 1960s, particularly by Peter L. Berger and Thomas Luckmann in \"The Social Construction of Reality\" (1966) and is still central for methods dealing with qualitative understanding of human society (compare \"socially constructed reality\"). The \"archaeological\" and \"genealogical\" studies of Michel Foucault are of considerable contemporary influence.\n\nThe sociology of science involves the study of science as a social activity, especially dealing \"with the social conditions and effects of science, and with the social structures and processes of scientific activity.\" Important theorists in the sociology of science include Robert K. Merton and Bruno Latour. These branches of sociology have contributed to the formation of science and technology studies. Both the ASA and the BSA have sections devoted to the subfield of Science, Knowledge and Technology. The ISA maintains a Research Committee on Science and Technology\n\nSociology of leisure is the study of how humans organize their free time. Leisure includes a broad array of activities, such as sport, tourism, and the playing of games. The sociology of leisure is closely tied to the sociology of work, as each explores a different side of the work–leisure relationship. More recent studies in the field move away from the work–leisure relationship and focus on the relation between leisure and culture. This area of sociology began with Thorstein Veblen's \"Theory of the Leisure Class\".\n\nThis subfield of sociology studies, broadly, the dynamics of war, conflict resolution, peace movements, war refugees, conflict resolution and military institutions. As a subset of this subfield, military sociology aims towards the systematic study of the military as a social group rather than as an organization. It is a highly specialized sub-field which examines issues related to service personnel as a distinct group with coerced collective action based on shared interests linked to survival in vocation and combat, with purposes and values that are more defined and narrow than within civil society. Military sociology also concerns civilian-military relations and interactions between other groups or governmental agencies. Topics include the dominant assumptions held by those in the military, changes in military members' willingness to fight, military unionization, military professionalism, the increased utilization of women, the military industrial-academic complex, the military's dependence on research, and the institutional and organizational structure of military.\n\nHistorically, political sociology concerned the relations between political organization and society. A typical research question in this area might be: \"Why do so few American citizens choose to vote?\" In this respect questions of political opinion formation brought about some of the pioneering uses of statistical survey research by Paul Lazarsfeld. A major subfield of political sociology developed in relation to such questions, which draws on comparative history to analyse socio-political trends. The field developed from the work of Max Weber and Moisey Ostrogorsky.\n\nContemporary political sociology includes these areas of research, but it has been opened up to wider questions of power and politics. Today political sociologists are as likely to be concerned with how identities are formed that contribute to structural domination by one group over another; the politics of who knows how and with what authority; and questions of how power is contested in social interactions in such a way as to bring about widespread cultural and social change. Such questions are more likely to be studied qualitatively. The study of social movements and their effects has been especially important in relation to these wider definitions of politics and power.\n\nPolitical sociology has also moved beyond methodological nationalism and analysed the role of non-governmental organizations, the diffusion of the nation-state throughout the Earth as a social construct, and the role of stateless entities in the modern world society. Contemporary political sociologists also study inter-state interactions and human rights.\n\nDemographers or sociologists of population study the size, composition and change over time of a given population. Demographers study how these characteristics impact, or are impacted by, various social, economic or political systems. The study of population is also closely related to human ecology and environmental sociology, which studies a populations relationship with the surrounding environment and often overlaps with urban or rural sociology. Researchers in this field may study the movement of populations: transportation, migrations, diaspora, etc., which falls into the subfield known as Mobilities studies and is closely related to human geography. Demographers may also study spread of disease within a given population or epidemiology.\n\nPublic sociology refers to an approach to the discipline which seeks to transcend the academy in order to engage with wider audiences. It is perhaps best understood as a style of sociology rather than a particular method, theory, or set of political values. This approach is primarily associated with Michael Burawoy who contrasted it with professional sociology, a form of academic sociology that is concerned primarily with addressing other professional sociologists. Public sociology is also part of the broader field of science communication or science journalism. In a distinct but similar vein, applied sociology, also known as clinical sociology, policy sociology or sociological practice, applies knowledge derived from sociological research to solve societal problems.\n\nThe sociology of race and of ethnic relations is the area of the discipline that studies the social, political, and economic relations between races and ethnicities at all levels of society. This area encompasses the study of racism, residential segregation, and other complex social processes between different racial and ethnic groups. This research frequently interacts with other areas of sociology such as stratification and social psychology, as well as with postcolonial theory. At the level of political policy, ethnic relations are discussed in terms of either assimilationism or multiculturalism. Anti-racism forms another style of policy, particularly popular in the 1960s and 1970s.\n\nThe sociology of religion concerns the practices, historical backgrounds, developments, universal themes and roles of religion in society. There is particular emphasis on the recurring role of religion in all societies and throughout recorded history. The sociology of religion is distinguished from the philosophy of religion in that sociologists do not set out to assess the validity of religious truth-claims, instead assuming what Peter L. Berger has described as a position of \"methodological atheism\". It may be said that the modern formal discipline of sociology \"began\" with the analysis of religion in Durkheim's 1897 study of suicide rates among Roman Catholic and Protestant populations. Max Weber published four major texts on religion in a context of economic sociology and social stratification: \"The Protestant Ethic and the Spirit of Capitalism\" (1905), \"\" (1915), \"\" (1915), and \"Ancient Judaism\" (1920). Contemporary debates often centre on topics such as secularization, civil religion, the intersection of religion and economics and the role of religion in a context of globalization and multiculturalism.\n\nThe sociology of change and development attempts to understand how societies develop and how they can be changed. This includes studying many different aspects of society, for example demographic trends, political or technological trends, or changes in culture. Within this field, sociologists often use macrosociological methods or historical-comparative methods. In contemporary studies of social change, there are overlaps with international development or community development. However, most of the founders of sociology had theories of social change based on their study of history. For instance, Marx contended that the material circumstances of society ultimately caused the ideal or cultural aspects of society, while Weber argued that it was in fact the cultural mores of Protestantism that ushered in a transformation of material circumstances. In contrast to both, Durkheim argued that societies moved from simple to complex through a process of sociocultural evolution. Sociologists in this field also study processes of globalization and imperialism. Most notably, Immanuel Wallerstein extends Marx's theoretical frame to include large spans of time and the entire globe in what is known as world systems theory. Development sociology is also heavily influenced by post-colonialism. In recent years, Raewyn Connell issued a critique of the bias in sociological research towards countries in the Global North. She argues that this bias blinds sociologists to the lived experiences of the Global South, specifically, so-called, \"Northern Theory\" lacks an adequate theory of imperialism and colonialism.\n\nThere are many organizations studying social change, including the Fernand Braudel Center for the Study of Economies, Historical Systems, and Civilizations, and the Global Social Change Research Project.\n\nA social network is a social structure composed of individuals (or organizations) called \"nodes\", which are tied (connected) by one or more specific types of interdependency, such as friendship, kinship, financial exchange, dislike, sexual relationships, or relationships of beliefs, knowledge or prestige. Social networks operate on many levels, from families up to the level of nations, and play a critical role in determining the way problems are solved, organizations are run, and the degree to which individuals succeed in achieving their goals. An underlying theoretical assumption of social network analysis is that groups are not necessarily the building blocks of society: the approach is open to studying less-bounded social systems, from non-local communities to networks of exchange. Drawing theoretically from relational sociology, social network analysis avoids treating individuals (persons, organizations, states) as discrete units of analysis, it focuses instead on how the structure of ties affects and constitutes individuals and their relationships. In contrast to analyses that assume that socialization into norms determines behaviour, network analysis looks to see the extent to which the structure and composition of ties affect norms. On the other hand, recent research by Omar Lizardo also demonstrates that network ties are shaped and created by previously existing cultural tastes. Social network theory is usually defined in formal mathematics and may include integration of geographical data into Sociomapping.\n\nSociological social psychology focuses on micro-scale social actions. This area may be described as adhering to \"sociological miniaturism\", examining whole societies through the study of individual thoughts and emotions as well as behaviour of small groups. Of special concern to psychological sociologists is how to explain a variety of demographic, social, and cultural facts in terms of human social interaction. Some of the major topics in this field are social inequality, group dynamics, prejudice, aggression, social perception, group behaviour, social change, non-verbal behaviour, socialization, conformity, leadership, and social identity. Social psychology may be taught with psychological emphasis. In sociology, researchers in this field are the most prominent users of the experimental method (however, unlike their psychological counterparts, they also frequently employ other methodologies). Social psychology looks at social influences, as well as social perception and social interaction.\n\nSocial stratification is the hierarchical arrangement of individuals into social classes, castes, and divisions within a society. Modern Western societies stratification traditionally relates to cultural and economic classes arranged in three main layers: upper class, middle class, and lower class, but each class may be further subdivided into smaller classes (e.g. occupational). Social stratification is interpreted in radically different ways within sociology. Proponents of structural functionalism suggest that, since the stratification of classes and castes is evident in all societies, hierarchy must be beneficial in stabilizing their existence. Conflict theorists, by contrast, critique the inaccessibility of resources and lack of social mobility in stratified societies.\n\nKarl Marx distinguished social classes by their connection to the means of production in the capitalist system: the bourgeoisie own the means, but this effectively includes the proletariat itself as the workers can only sell their own labour power (forming the material base of the cultural superstructure). Max Weber critiqued Marxist economic determinism, arguing that social stratification is not based purely on economic inequalities, but on other status and power differentials (e.g. patriarchy). According to Weber, stratification may occur among at least three complex variables: (1) Property (class): A person's economic position in a society, based on birth and individual achievement. Weber differs from Marx in that he does not see this as the supreme factor in stratification. Weber noted how managers of corporations or industries control firms they do not own; Marx would have placed such a person in the proletariat. (2) Prestige (status): A person's prestige, or popularity in a society. This could be determined by the kind of job this person does or wealth. and (3) Power (political party): A person's ability to get their way despite the resistance of others. For example, individuals in state jobs, such as an employee of the Federal Bureau of Investigation, or a member of the United States Congress, may hold little property or status but they still hold immense power Pierre Bourdieu provides a modern example in the concepts of cultural and symbolic capital. Theorists such as Ralf Dahrendorf have noted the tendency towards an enlarged middle-class in modern Western societies, particularly in relation to the necessity of an educated work force in technological or service-based economies. Perspectives concerning globalization, such as dependency theory, suggest this effect owes to the shift of workers to the developing countries.\n\nUrban sociology involves the analysis of social life and human interaction in metropolitan areas. It is a discipline seeking to provide advice for planning and policy making. After the industrial revolution, works such as Georg Simmel's \"The Metropolis and Mental Life\" (1903) focused on urbanization and the effect it had on alienation and anonymity. In the 1920s and 1930s The Chicago School produced a major body of theory on the nature of the city, important to both urban sociology and criminology, utilizing symbolic interactionism as a method of field research. Contemporary research is commonly placed in a context of globalization, for instance, in Saskia Sassen's study of the \"Global city\". Rural sociology, by contrast, is the analysis of non-metropolitan areas. As agriculture and wilderness tend to be a more prominent social fact in rural regions, rural sociologists often overlap with environmental sociologists.\n\nOften grouped with urban and rural sociology is that of community sociology or the sociology of community. Taking various communities—including online communities—as the unit of analysis, community sociologists study the origin and effects of different associations of people. For instance, German sociologist Ferdinand Tönnies distinguished between two types of human association: \"Gemeinschaft\" (usually translated as \"community\") and \"Gesellschaft\" (\"society\" or \"association\"). In his 1887 work, \"Gemeinschaft und Gesellschaft\", Tönnies argued that \"Gemeinschaft\" is perceived to be a tighter and more cohesive social entity, due to the presence of a \"unity of will\". The 'development' or 'health' of a community is also a central concern of community sociologists also engage in development sociology, exemplified by the literature surrounding the concept of social capital.\n\nSociology overlaps with a variety of disciplines that study society, in particular anthropology, political science, economics, social work and social philosophy. Many comparatively new fields such as communication studies, cultural studies, demography and literary theory, draw upon methods that originated in sociology. The terms \"social science\" and \"social research\" have both gained a degree of autonomy since their origination in classical sociology. The distinct field of social anthropology or anthroposociology is the dominant constituent of anthropology throughout the United Kingdom and Commonwealth and much of Europe (France in particular), where it is distinguished from cultural anthropology. In the United States, social anthropology is commonly subsumed within cultural anthropology (or under the relatively new designation of sociocultural anthropology).\n\nSociology and applied sociology are connected to the professional and academic discipline of social work. Both disciplines study social interactions, community and the effect of various systems (i.e. family, school, community, laws, political sphere) on the individual. However, social work is generally more focused on practical strategies to alleviate social dysfunctions; sociology in general provides a thorough examination of the root causes of these problems. For example, a sociologist might study \"why\" a community is plagued with poverty. The applied sociologist would be more focused on practical strategies on \"what\" needs to be done to alleviate this burden. The social worker would be focused on \"action\"; implementing theses strategies \"directly\" or \"indirectly\" by means of mental health therapy, counselling, advocacy, community organization or community mobilization.\n\nSocial anthropology is the branch of anthropology that studies how contemporary living human beings behave in social groups. Practitioners of social anthropology, like sociologists, investigate various facets of social organization. Traditionally, social anthropologists analysed non-industrial and non-Western societies, whereas sociologists focused on industrialized societies in the Western world. In recent years, however, social anthropology has expanded its focus to modern Western societies, meaning that the two disciplines increasingly converge.\n\nSociocultural anthropology, which include linguistic anthropology, is concerned with the problem of difference and similarity within and between human populations. The discipline arose concomitantly with the expansion of European colonial empires, and its practices and theories have been questioned and reformulated along with processes of decolonization. Such issues have re-emerged as transnational processes have challenged the centrality of the nation-state to theorizations about culture and power. New challenges have emerged as public debates about multiculturalism, and the increasing use of the culture concept outside of the academy and among peoples studied by anthropology. These times are not \"business-as-usual\" in the academy, in anthropology, or in the world, if ever there were such times.\n\nIrving Louis Horowitz, in his \"The Decomposition of Sociology\" (1994), has argued that the discipline, while arriving from a \"distinguished lineage and tradition\", is in decline due to deeply ideological theory and a lack of relevance to policy making: \"The decomposition of sociology began when this great tradition became subject to ideological thinking, and an inferior tradition surfaced in the wake of totalitarian triumphs.\" Furthermore: \"A problem yet unmentioned is that sociology's malaise has left all the social sciences vulnerable to pure positivism—to an empiricism lacking any theoretical basis. Talented individuals who might, in an earlier time, have gone into sociology are seeking intellectual stimulation in business, law, the natural sciences, and even creative writing; this drains sociology of much needed potential.\" Horowitz cites the lack of a 'core discipline' as exacerbating the problem. Randall Collins, the Dorothy Swaine Thomas Professor in Sociology at the University of Pennsylvania and a member of the Advisory Editors Council of the Social Evolution & History journal, has voiced similar sentiments: \"we have lost all coherence as a discipline, we are breaking up into a conglomerate of specialities, each going on its own way and with none too high regard for each other.\"\n\nIn 2007, \"The Times Higher Education Guide\" published a list of 'The most cited authors of books in the Humanities' (including philosophy and psychology). Seven of the top ten are listed as sociologists: Michel Foucault (1), Pierre Bourdieu (2), Anthony Giddens (5), Erving Goffman (6), Jürgen Habermas (7), Max Weber (8), and Bruno Latour (10).\n\nThe most highly ranked general journals which publish original research in the field of sociology are the \"American Journal of Sociology\" and the \"American Sociological Review\". The \"Annual Review of Sociology\", which publishes original review essays, is also highly ranked. Many other generalist and specialized journals exist.\n\n\n"}
{"id": "6214086", "url": "https://en.wikipedia.org/wiki?curid=6214086", "title": "Spiral fracture", "text": "Spiral fracture\n\nA spiral fracture (a.k.a. torsion fracture) is a bone fracture occurring when torque (a rotating force) is applied along the axis of a bone. Spiral fractures often occur when the body is in motion while one extremity is planted. For example, a spiral fracture of the tibia (the shinbone) can occur in young children when they fall short on an extended leg while jumping. This occurrence is known as \"toddler's fracture\". Spiral fractures are also recognized as being suspicious in very young children since to obtain a fracture of this sort requires forceful twisting or jerking of the limbs. Child abuse (physical abuse) and certain conditions such as osteogenesis imperfecta (OI) are considered differentials when identifying spiral or torsion fractures.\n\nhttp://ghr.nlm.nih.gov/condition/osteogenesis-imperfecta"}
{"id": "1408241", "url": "https://en.wikipedia.org/wiki?curid=1408241", "title": "Spring line settlement", "text": "Spring line settlement\n\nSpring line settlements occur where a ridge of permeable rock lies over impermeable rock and there will be a line of springs along the boundary between the two layers.\n\nIt sometimes happens that a sequence of spring line (or springline) settlements will arise around these springs, becoming villages.\n\nIn each case to build higher up the hill would have meant difficulties with water supply; to build lower would have taken the settlement further away from useful grazing land or nearer to the floodplain.\n\nSpring line villages are often the principal settlements in strip parishes, with long, narrow parish boundaries stretching up to the top of the ridge and down to the river but being narrow in the direction of adjacent spring line villages.\n\n\n"}
{"id": "58953", "url": "https://en.wikipedia.org/wiki?curid=58953", "title": "Timeline of telescopes, observatories, and observing technology", "text": "Timeline of telescopes, observatories, and observing technology\n\nTimeline of telescopes, observatories, and observing technology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22202671", "url": "https://en.wikipedia.org/wiki?curid=22202671", "title": "Viewshed analysis", "text": "Viewshed analysis\n\nA viewshed is an area that is visible from a specific location. Viewshed analyses are a common function of most geographic information system (GIS) software. The analysis uses the elevation value of each cell of the digital elevation model (DEM) to determine visibility to or from a particular cell. The location of this particular cell varies depending on the needs of the analysis. \nFor example, a viewshed analysis is commonly used to locate communication towers or determining the view from a road. Viewsheds can be calculated using an individual point such as a tower or multiple points such as a line representing a road. When analyzing a line segment, each of the vertices along the line is calculated to determine its visible area. The process can also be reversed. For example, when locating a landfill, the analysis can determine from where the landfill is visible to keep it hidden from view.\n\nA viewshed analysis can be performed using one of many GIS programs, such as GRASS GIS (r.los, r.viewshed), LuciadLightspeed, LuciadMobile, SAGA GIS (Visibility), TNT Mips, ArcMap, Maptitude, ERDAS IMAGINE. A viewshed is created from a DEM by using an algorithm that estimates the difference of elevation from one cell (the viewpoint cell) to the next (the target cell). To determine the visibility of a target cell, each cell between the viewpoint cell and target cell is examined for line of sight. Where cells of higher value are between the viewpoint and target cells the line of sight is blocked. If the line of sight is blocked then the target cell is determined to not be part of the viewshed. If it is not blocked then it is included in the viewshed (Kim et al., 2004).\n\nThe algorithm is also based on a given set of variables. When performing a viewshed analysis, several variables can be used to limit or adjust the calculation. For example, if the analysis is to determine the location of a radio tower, the height of the tower could be added to the elevation of that location (cell value). If no height is given, then the viewshed analysis uses the cell value of the DEM in which the tower is located.\n\nAnother way to add the height of the tower is to use an offset variable. Offset values can be added to a sending tower as well as a receiving tower. The offset value is then added to the elevation value of the cell to obtain the actual elevation of each tower.\n\nThe viewshed analysis can also have a limited viewing angle. The viewing angle, or azimuth, of the radio tower can be incorporated into the calculation by adding two values. The first value is the lowest possible azimuth angle and the second value is the highest possible azimuth angle. The program will analyze the viewshed only within these given azimuth angles. \nA vertical angle can be added as well. The values for vertical angle are from 90° (looking straight up) to -90° (looking straight down). This variable would need to be added in cases where the radio tower emits a very narrow vertical beam.\nThe final variable used in the viewshed analysis is the radius value. In the case of the radio tower, if the radio signal has a limited range, perhaps 10 miles, then the radius variable can be set to limit the viewshed analysis to a 10-mile radius.\n\nBesides tower placement a viewshed analysis can be used for other applications. For example, a viewshed analysis could estimate the impact of the addition of a large building. The viewshed analysis would show all the areas from which the building could be seen as well as any views that would be obscured from any particular location. Viewshed analyses are also used to locate fire observation stations in mountain areas (Lee and Stucky, 1998). This allows the stations to be placed so that the entire forest can be observed for possible fires.\n\nAn example of using a viewshed analysis on a line segment is from the Wyoming State Office, Bureau of Land Management, in which the office used a viewshed analysis to determine the visibility from National Historic Trails across Wyoming. Within the Rock Springs Field Office area there are five different viewsheds to choose from. The example here is viewshed number one. The viewshed indicates the land areas visible from the trails at four different levels from not visible to visible, based on the number of times the area could be seen from the trail. This is an indication of what the pioneers could see as they traveled along the trails of the western frontier (Rock Springs Field Office).\n\nIn another example the United States Geological Survey (USGS) used a viewshed analysis to assist NASA's Mars Exploration Rover (MER) project. When NASA needed to find appropriate landing spots for the Mars rovers, they turned to the USGS to map the best possible sites. Part of the analysis included a viewshed of the possible site selections. In this case the viewshed indicates the areas which may or may not be visible by the Mars rovers from each landing site (MER Landing Site Viewshed Analysis).\n\n\n"}
{"id": "57111927", "url": "https://en.wikipedia.org/wiki?curid=57111927", "title": "Wendy Zukerman", "text": "Wendy Zukerman\n\nWendy Zukerman is an Australian science journalist and podcaster. She is best known as the host of \"Science Vs\", a program that dissects areas of scientific controversy and public confusion.\n\nZukerman was born in the United States but raised in Melbourne, Australia, which is also where she attended college at Monash University and received a biomedical science and law degree. \n\nShe has worked for the Australian Broadcasting Corporation (ABC) on the documentary television program, \"Catalyst,\" and the consumer affairs television program,\"The Checkout\". Additionally she has written for many different publications including \"New Scientist\" magazine, \"Cosmos\" magazine, \"The Age\", \"The Australian\", and \"Saturday Paper.\"\n\nIn 2014 and 2015, she spoke at Storyology, an annual festival of journalism and media. In March 2018, she spoke at a SXSW, in a panel event called, \"What Podcasts Ate for Dinner\".\n\nStarting in October 2015, her podcast \"Science Vs\", has been produced by Gimlet Media, a Brooklyn-based digital media company. She previously had her show produced by the Australian Broadcasting Corporation (ABC) as part of ABC Radio's First Run podcast program. The \"Science Vs\" podcast works on communicating science to non-scientists and topics on the show can be controversial, myth-busting or based on current news, and often have included immigration, gun control, ghosts, climate change, acne, antidepressants and more. The idea for \"Science Vs\" came from news headlines about Gwyneth Paltrow wanting women to participate in a \"health practice\" of vaginal steaming, Zukerman thought this seemed like a good time to highlight science and also add humor. Zukerman summarized her podcast's position against false equivalences with the comment, \"If there's a 95 percent consensus among scientists, you report the consensus.\"\n\n"}
