{"id": "33486274", "url": "https://en.wikipedia.org/wiki?curid=33486274", "title": "AAPG Bulletin", "text": "AAPG Bulletin\n\nThe AAPG Bulletin is a monthly peer-reviewed scientific journal covering geosciences and associated technologies relating to the energy industry. It is an official journal of the American Association of Petroleum Geologists.\n\nThis journal is abstracted and/or indexed in: GeoRef, GEOBASE, Scopus, PubMed, Current Contents, and Web of Science.\n"}
{"id": "779599", "url": "https://en.wikipedia.org/wiki?curid=779599", "title": "AS-105", "text": "AS-105\n\nAS-105 was the fifth and final orbital flight of a boilerplate Apollo spacecraft, and the third and final launch of a Pegasus micrometeroid detection satellite. It was launched by SA-10, the tenth and final Saturn I rocket, in 1965.\n\nAS-105 was an Apollo boilerplate spacecraft; boilerplate BP-9A was used for the flight. The spacecraft reentered on November 22, 1975. The Saturn launch vehicle (SA-10) was similar to those of missions AS-103 and AS-104. As on the previous mission, the boilerplate service module was equipped with a test installation of a reaction control engine package.\n\nThe primary flight objective was to continue demonstration of the launch vehicle's iterative guidance mode and evaluation of system accuracy.\n\nAS-105 was launched from Cape Kennedy Launch Complex 37B at 08:00 EST (13:00 GMT) on July 30, 1965, on the last Saturn I rocket, SA-10. A planned thirty-minute hold ensured that launch time coincided with the opening of the Pegasus launch window. The launch was normal and the payload was inserted into orbit approximately 10.7 minutes after lift-off. The total mass placed in orbit, including the spacecraft, Pegasus spacecraft, adapter, instrument unit, and S-IV stage, was 34,438 pounds (15,621 kg).\n\nThe spacecraft was separated 812 seconds after lift-off. The separation and ejection system operated as planned. The Pegasus 3 spacecraft, which was attached to the S-IV stage of the Saturn I and stowed inside the boilerplate service module, was deployed 40 seconds after command initiation at 872 seconds. Pegasus 3 was a 1423.6 kilogram (3138.6 pound) micrometeoroid detection satellite, which was bolted to the S-IV.\n\n"}
{"id": "23887706", "url": "https://en.wikipedia.org/wiki?curid=23887706", "title": "Alliance (taxonomy)", "text": "Alliance (taxonomy)\n\nAn alliance is an informal grouping used in biological taxonomy. The term \"alliance\" is not a taxonomic rank defined in any of the nomenclature codes. It is used for any group of species, genera or tribes to which authors wish to refer, that have at some time provisionally been considered to be closely related.\n\nThe term is often used for a group that authors are studying in further detail in order to refine the complex taxonomy. For example, a molecular phylogenetics study of the Aerides–Vanda Alliance (Orchidaceae: Epidendroideae) confirmed that the group is monophyletic, and clarified which species belong in each of the 14 genera. In other orchid groups, the various alliances that have been defined do not correspond well to clades.\n\nHistorically, some 19th century botanical authors used alliance to denote groups that would now be considered orders. This usage is now obsolete, and the ICN (Article 17.2) specifies that such taxa are treated as orders.\n\n"}
{"id": "690245", "url": "https://en.wikipedia.org/wiki?curid=690245", "title": "Art world", "text": "Art world\n\nThe art world comprises everyone involved in producing, commissioning, presenting, preserving, promoting, chronicling, criticizing, and selling fine art. \n\n\"Art world\" is indeed a wider term than art market, though that is a large part of it. Howard S. Becker describes it as \"the network of people whose cooperative activity, organized via their joint knowledge of conventional means of doing things, produce(s) the kind of art works that art world is noted for\" (Becker, 1982). In her book, \"Seven Days in the Art World\", Sarah Thornton describes it as \"a loose network of overlapping subcultures held together by a belief in art. They span the globe but cluster in art capitals like New York, London, Los Angeles, and Berlin.\" Other cities sometimes called \"art capitals\" include Beijing, Brussels, Hong Kong, Miami, Paris, Rome and Tokyo; due to their large art festivals, followings, and being the centers of art production.\n\nThe notion of the singular \"art world\" is problematic, since Becker and others show art worlds are, instead, independent multiplicities scattered worldwide that are always in flux: there is no \"center\" to the art world any more. In her analysis of the \"net art world\" (referring to network-aided art or net art), Amy Alexander states \"net.art had a movement, at the very least it had coherence, and although it aimed to subvert the art world, eventually its own sort of art world formed around it. It developed a culture, hype and mystique through lists and texts; it had a centre, insiders, outsiders, even nodes. This is of course not a failure; this is unavoidable: groups form; even anarchism is an institution.\" \nArt worlds exist at local and regional levels, as hidden or obscured subcultures, via primary and secondary art markets, through gallery circuits, around design movements, and, esoterically, as shared or perceived experiences. \n\nThe one globalized, all-encompassing art world exists only as myth; rather, there are multiplicities of intersecting, overlapping, self-similar art worlds, each expressing different views of the world as they see it.\n\n\"Whitehot Magazine\" artist/publisher Noah Becker has published over 3500 articles about the Art World.\"\n\n\"New York Magazine\" art critic Jerry Saltz has referred to William Powhida's and Jade Townsend's drawing \"Art Basel Miami Beach Hooverville\" as \"a great big art-world stinkbomb.\"\n\n\n"}
{"id": "59141451", "url": "https://en.wikipedia.org/wiki?curid=59141451", "title": "Asimina Arvanitaki", "text": "Asimina Arvanitaki\n\nAsimina Arvanitaki (b. 1980) is a Greek theoretical physicist and Stavros Niarchos Foundation Aristarchus Chair in Theoretical Physics at the Perimeter Institute for Theoretical Physics in Waterloo, Ontario, Canada. In 2017, she was awarded the New Horizons in Physics prize.\n\nArvanitaki's work has focused on finding novel experiments to explore topics in theoretical physics. Her work has been described as working at the \"precision frontier\" as it involves measuring very small variations in well-understood phenomena to illuminate theoretical predictions. \n\nIn 2016, Arvanitaki, Savas Dimopoulos and Ken Van Tilburg proposed a method of detecting dark matter as a matter wave, using conventional gravitational wave detectors. She has proposed using the Advanced Laser Interferometer Gravitational-Wave Observatory to observe the gravitational waves from black hole collisions to detect the presence of QCD axions, a candidate for explaining dark matter. She has also proposed using neutrino and gamma-ray telescopes, such as the Fermi telescope, Hess, or IceCube, to search for dark matter decay products predicted by certain theories of super-symmetry.\n\nArvanitaki pursued her post-doctoral research at Stanford University. She joined the Perimeter Institute in 2014. She is the first woman to hold a named chair at the institute. Arvanitaki suggested naming the chair for Aristarchus — the ancient Greek astronomer who surmised that the Earth rotated around the sun centuries before Coperniucs — in a nod to the cosmological scope of her research, the Greek background of the Stavros Niarchos Foundation, and her own Greek origin. \n"}
{"id": "15761777", "url": "https://en.wikipedia.org/wiki?curid=15761777", "title": "Astrobotic Technology", "text": "Astrobotic Technology\n\nAstrobotic Technology is an American privately held company that is developing space robotics technology for planetary missions. It was founded in 2008 by Carnegie Mellon professor Red Whittaker and his associates, with the goal of winning the Google Lunar X Prize. The company is based in Pittsburgh, Pennsylvania.\n\nIn 2012, the company signed collaborations with Carnegie Mellon University, International Rectifier, Ansys, AGI, Alcoa, and Caterpillar, in 2015 it started a collaboration with the German Aerospace Center (DLR), and in June 2016 it signed a collaboration agreement with Airbus Defence and Space and with DHL.\n\nThe team articulated an ambitious goal from the start in 2008: they hoped to be the first commercial operation to land their spacecraft \"Red Rover\" on the Moon, using the lander, named \"Artemis Lander\". Since its formation, Astrobotic has maintained a spot in the top three rankings for Evadot's third-party Google Lunar X Prize Scorecard. The company's first running prototype of Red Rover was completed the same year, and on July 28, 2008, NASA awarded Astrobotic funding for its \"Regolith Moving Methods\" proposal.\n\nIn 2009, Astrobotic began to receive a series of Small Business Innovation Research (SBIR) funding from NASA totaling over $795,000 to investigate prospecting for lunar resources.\n\nOn October 15, 2010, NASA awarded a contract to Astrobotic for Innovative Lunar Demonstrations Data (ILDD) firm-fixed price indefinite-delivery/indefinite-quantity contracts with a total value up to $30.1 million over a period of up to five years, and in December, NASA's $500,000 ILDD project for further Lunar Demonstrations Data was awarded to Astrobotic.\n\nOriginally named \"Red Rover\" and \"Artemis Lander\", respectively, Astrobotic indicated in 2011 that they were reserving naming rights, as well as selection of the planned location for the lunar landing, for their payload customers. \"We have to sell a lot of payload to make the economics work, ... the customers will decide where we go\". Later, the rover continued to be called \"Red Rover\" and the lander was now called \"Griffin\".\n\nAstrobotic's \"Technologies Enabling Exploration of Skylights, Lava Tubes, and Caves\", was a phase one selection for NASA Innovative Advanced Concepts (NIAC). In April 2011, Astrobotic received a $599,000 two-year contract to develop a scalable gravity offload device for testing rover mobility in simulated lunar gravity under NASA's Small Business Technology Transfer Program (STTR).\n\nIn May 2012, David Gump left the position of President of Astrobotic and John Thornton took the reins.\n\nOn April 30, 2014, NASA announced that Astrobotic Technologies was one of the three companies selected for the Lunar CATALYST initiative. NASA is negotiating a 3-year no-funds-exchanged Space Act Agreement (SAA). The Griffin Lander may be involved.\n\nOn June 2, 2016, Astrobotic Technology announced a new design of its lunar lander, \"\"peregrine\" Peregrine\", along with two new industry collaborators. Airbus Defence and Space signed a memorandum of understanding to provide engineering support for Astrobotic as it refines the lander's design, which Thornton said is approaching a preliminary design review. \"For us at Airbus Defence and Space, the Moon is a very important topic,\" said Bart Reijnen, senior vice president of on-orbit services and exploration at Airbus Defence and Space. \"Astrobotic is what we see as being the frontrunner in the world of commercial lunar transportation.\"\n\nAstrobotic also announced a separate agreements with shipping company DHL, which will serve as the official logistics provider for Astrobotic. DHL will provide shipping for components of the spacecraft, and for the completed \"Peregrine\" lander from Astrobotic's facilities to the launch pad. \"Moreover, we also see potential opportunities to develop the partnership further in the future, and explore how we can integrate our activities even more creatively with Astrobotic,\" said Arjan Sissing, senior vice president of corporate brand marketing at DHL. He said later that could include supporting \"extraterrestrial logistics in regard to Moon projects of the future.\"\n\nIn December 2016 Astrobotic slipped their estimated launch date to 2019 and separated from the Google Lunar X Prize.\n\nIn October 2017 NASA extended its Lunar CATALYST agreement for 2 years.\n\nOn November 29, 2018 Astrobotic was awarded a Commercial Lunar Payload Services contract by NASA, which makes it eligible to bid on delivering science and technology payloads to the Moon for NASA.\n\nBy 2010, the company had priced payload carried to the lunar surface at with an additional \"per-payload\" fee of \"to cover the cost of integration and to provide communications, power, thermal control and pointing services\".\n\nBy 2011, Astrobotic had raised the payload price and made a distinction between payload fixed to the lander and payload carried on the lunar rover. The revised baseline prices are for lander payload and for rover payload, with the additional integration fee unchanged at \"per-payload\".\nBeyond the standard inclusions of 300 Watt-hours of power, and 100 MB of data transfer, per kilogram of mass purchased, pricing has been established for the purchase of additional power or Lunar-to-Earth data transfer.\n\nIn April 2011, Astrobotic contracted with SpaceX for a Falcon 9 launch on a lunar mission for as early as December 2013. The mission was intended to \"deliver a lander, small rover and up to about of payload to the surface of the Moon\".\nA payload user's guide for researchers on preparation of their instruments was released in early March 2011.\n\nIn April 2011, Astrobotic stated that follow-on lunar missions were tentatively planned for 2015 and 2016. Both to be flown on Falcon 9 launch vehicles, with the same \"total\" mission payload as the first mission: , or customer payload if the rover is included on the mission. The 2015 mission was named \"Polar Excavator\" (now \"Icebreaker\"), would target the lunar north pole, and was nominally planned for July 2015. This expedition's rover was to be Polaris. By 2011, the 2016 mission was to be customer-driven, and land at a destination that was to be selected by the customer.\nBy August 2011, per version 2.4 of the User's Guide, there had been two small changes to the mission manifest with the first mission now aimed for either an Apollo site or a skylight entrance to a lava tube, and the launch date was changed to a range: December 2013 to April 2014.\n\nBy October 2011, Astrobotic had delayed the lunar mission launch date to \"late 2014 or early 2015\", indicating that they were still under contract to SpaceX for a Falcon 9 mission.\n\nBy 2012 the Astrobotic mission on the SpaceX Falcon 9 was rescheduled for October 2015. In October 2015, a \"Polaris\" rover was to carry out the same or similar tasks to NASA's RESOLVE. A model of the Polaris rover was unveiled in October 2012.\n\nBy February 2015, Astrobotic further delayed the Moon mission to the second half of 2016, but then contracted with two other GLXP teams including Team Hakuto and Team AngelicvM. The agreement was to launch the rovers of all teams on a single SpaceX Falcon 9 which would then use the Astrobotic \"Griffin\" lander to touch down on the surface of the Moon. After landing on the lunar surface, all teams would have competed against each other to achieve the objectives and win the GLXP prize. It is now planned for late 2019.\n\nIn June 2016, Astrobotic unveiled the \"Peregrine\" lunar lander, which is a smaller version of \"Griffin\", a lander Astrobotic has previously proposed for lunar missions. Among the differences between the two designs is a change in propulsion. While \"Griffin\" uses a single large thruster, \"Peregrine\" uses a cluster of five ISE-100 thrusters, built by Aerojet Rocketdyne and based on the Divert and Attitude Control System thrusters it developed for missile defense applications.\n\nIn July 2017, Astrobotic announced an agreement had been reached with United Launch Alliance (ULA) to launch the first \"Peregrine\" lander aboard an Atlas V. This announcement seemingly indicates Astrobotic canceled plans to launch on a Falcon 9. One item planned to be aboard the lander is a library, in micro print on nickel, which will include Wikipedia contents and Long Now Foundation's Rosetta Project. As of May 2018, its first lunar lander mission is reported to have 12 customers, and is planned to be launched in 2020 on an Atlas V rocket.\n\n\n"}
{"id": "11175098", "url": "https://en.wikipedia.org/wiki?curid=11175098", "title": "Astrolinguistics", "text": "Astrolinguistics\n\nAstrolinguistics is a field of linguistics connected with the search for extraterrestrial intelligence (SETI).\n\nArguably the first attempt to construct a language for interplanetary communication was made by the anarchist philosopher Wolf Gordin (brother of Abba Gordin) in his books \"Grammar of the Language of the Mankind AO\" (1920) and \"Grammar of the Language AO\" (1923). The \"AO\" language was presented at the First international exhibition of interplanetary machines and mechanisms (dedicated to the 10th anniversary of the Russian Revolution and the 70th anniversary of the birth of Tsiolkovsky) in Moscow, 1927 as a language for interplanetary travels. The declared goal of Gordin was to construct a language which would be non-\"fetishizing\", non-\"sociomorphic\", non-gender based and non-classist. The design of the language was inspired by the Russian Futuristic poetry, brothers Gordins' pan-anarchist philosophy and early Tsiolkovsky's remarks on possible cosmic messaging (which were in accord with later Freudenthal's insights).\n\nAn integral part of the SETI project in general is research in the field of the construction of messages for extraterrestrial intelligence, possibly to be transmitted into space from Earth. As far as such messages are based on linguistic principles, the research can be considered to belong to astrolinguistics. The first proposal in this field was put forward by the mathematician Hans Freudenthal at the University of Utrecht in the Netherlands, in 1960 – around the time of the first SETI effort at Greenbank in the US. Freudenthal conceived a complete Lingua Cosmica. His book \"LINCOS: Design of a Language for Cosmic Intercourse\" seems at first sight non-linguistic, because mathematical concepts are the core of the language. The concepts are, however, introduced in conversations between persons (\"Homo sapiens\"), de facto by linguistic means. This is witnessed by the innovative examples presented. The book set a landmark in astrolinguistics. This was witnessed by Bruno Bassi's review years later. Bassi noted: “LINCOS is there. In spite of its somewhat ephimeral 'cosmic intercourse' purpose it remains a fascinating linguistic and educational construction, deserving existence as another Toy of Man's Designing”.\n\nThe concept astrolinguistics in scientific research was coined as such, also with a view towards message construction for ETI, in 2013 in the monograph \"Astrolinguistics: Design of a Linguistic System for Interstellar Communication Based on Logic\", written by the astronomer and computer scientist Alexander Ollongren, University of Leiden, The Netherlands. This book presents a new Lingua Cosmica totally different from Freudenthal's design. It describes the way the logic of situations in human societies can be formulated in the lingua, also named LINCOS. This astrolinguistic system, also designed for use in interstellar communication, is based on modern constructive logic – which assures that all expressions are verifiable. At a deeper, more fundamental level, however, astrolinguistics is concerned with the question whether linguistic universalia can be identified which are potentially useful in communication across interstellar distances between intelligence species. In the view of the new LINCOS these might be certain logic descriptions of specific situations and relations (possibly in an Aristotelian sense). Kadri Tinn's (Astronomy for Humans) review of Ollongren's book recognised that aspect – she wrote:\n\n\n"}
{"id": "52294702", "url": "https://en.wikipedia.org/wiki?curid=52294702", "title": "Azoic hypothesis", "text": "Azoic hypothesis\n\nThe Azoic hypothesis (sometimes referred to as the Abyssus theory) is a superseded scientific theory proposed by Edward Forbes in 1843, stating that the abundance and variety of marine life decreased with increasing depth and, by extrapolation of his own measurements, Forbes calculated that marine life would cease to exist below .\n\nThe theory was based upon Forbes' findings aboard HMS \"Beacon\", a survey ship to which he had been appointed naturalist by the ship's commander Captain Thomas Graves. With Forbes aboard, HMS \"Beacon\" set sail around the Aegean Sea on 17 April 1841, from Malta. It was at this point that Forbes began to take dredging samples at various depths of the ocean, he observed that samples from greater depths displayed a narrower diversity of creatures which were generally smaller in size.\n\nForbes reported his findings from the Aegean Sea in his 1843 report to the British Association entitled \"Report on the Mollusca and Radiata of the Aegean Sea\". His findings were widely accepted by the scientific community and were bolstered by other scientific figures of the time. David Page (1814–1879), a respected geologist, confounded the theory by stating that \"according to experiment, water at the depth of 1000 feet is compressed th of its own bulk; and at this rate of compression we know that at great depths animal and vegetable life as known to us cannot possibly exist – the extreme depressions of seas being thus, like the extreme elevations of the land, barren and lifeless solitudes.\"\nThe theory was not disproven until the late 1860s when biologist Michael Sars, Professor of Zoology at Christiania (now Oslo) University, discovered life at a depth greater than 300 fathoms. Sars listed 427 animal species which had been found along the Norwegian coast at a depth of 450 fathoms, and gave a description of a crinoid \"Rhizocrinus lofotensis\" which his son had recovered from a depth of 300 fathoms in Lofoten.\nIn 1869, Charles Wyville Thomson dredged marine life from a depth of , finally dispelling Forbes' azoic theory.\n\nIn light of this evidence, the Azoic hypothesis would come to be seen as a false hypothesis and give way to vastly increased efforts in deep-sea exploration and associated marine life. Since being discredited, the theory has been referenced widely in popular culture, such as Keble Ball 2017 and alluded to in documentaries that explore and showcase deep-sea marine life.\n"}
{"id": "39564041", "url": "https://en.wikipedia.org/wiki?curid=39564041", "title": "Brain Games (National Geographic)", "text": "Brain Games (National Geographic)\n\nBrain Games is a popular science television series that explores cognitive science by focusing on illusions, psychological experiments, and counterintuitive thinking. Neil Patrick Harris was the unseen narrator in the first season, replaced by Jason Silva for the remainder of the series as its host and presenter; in addition, sleight-of-hand artist Apollo Robbins has been a frequent consultant and illusionist guest on the show. The show is interactive, encouraging television viewers, often along with a handful of live volunteers, to engage in visual, auditory, and other cognitive experiments, or \"brain games\", that emphasize the main points presented in each episode. The series debuted on the National Geographic Channel in 2011 as a special. Its return as an original series in 2013 set a record for the highest premiere rating for any National Geographic Channel original series with 1.5 million viewers. Season 7 aired in 2016. National Geographic announced that the show would return as a 2-hour live event in the fall of 2018.\n\nThe series is deemed acceptable for use toward E/I credits, and Litton Entertainment added repurposed reruns of the show to its \"One Magnificent Morning\" block in fall 2017.\n\n\n\nSeason 1 consists of three one-hour pilot episodes.\n\n"}
{"id": "2904845", "url": "https://en.wikipedia.org/wiki?curid=2904845", "title": "Cajeput oil", "text": "Cajeput oil\n\nCajuput oil is a volatile oil obtained by distillation from the leaves of the myrtaceous trees \"Melaleuca leucadendra\", \"Melaleuca cajuputi\", and probably other \"Melaleuca\" species. The trees yielding the oil are found throughout Maritime Southeast Asia and over the hotter parts of the Australian continent. The majority of the oil is produced on the Indonesian island of Sulawesi. The name “cajeput” is derived from its Indonesian name, “\"kayu putih\"” or \"white wood\".\n\nThe oil is prepared from leaves collected on a hot dry day, macerated in water, and distilled after fermenting for a night. This oil is extremely pungent, and has the odor of a mixture of turpentine and camphor. It consists mainly of cineol (see terpenes), from which cajuputene, having a hyacinth-like odor, can be obtained by distillation with phosphorus pentoxide. It is a typical volatile oil, and is used internally in doses of 2 to 3 minims, for the same purposes as, say, clove oil. It is frequently employed externally as a counterirritant. It is an ingredient in some liniments for sore muscles such as Tiger Balm and Indonesian traditional medicine Minyak Telon.\n\nIt is also used as an ingredient in inhalants/decongestants and topical pain/inflammation remedies such as Olbas Oil.\n\nIn October 1832 while in the port of Manila, the Asiatic or spasmodic cholera suddenly made its appearance on board the USS Peacock (1828). The first case was in a sailor named Peterson, sixty-three years old. The surgeon administered six grains of opium, in three doses; bad symptoms increasing, fifteen drops of cajeput oil were given in brandy and water, and repeated in half an hour. This treatment, however, apparently did not help the patient; Peterson died about eleven hours after being stricken, as did seven others, of \"the terrific and appalling effects produced by one of the greatest scourges that ever visited the world.\"\n\nCajeput is used for the treatment of bacterial or fungal infections in fish. Common brand names containing Cajeput are Melafix and Bettafix. Melafix is a stronger concentration and Bettafix is a lower concentration that makes it harder to overdose smaller fish, especially bettas. It is most commonly used to promote fin and tissue regrowth, but is also effective in treating other conditions, such as fin rot or velvet. The remedy is used mostly on betta fish.\n\n\n"}
{"id": "46713039", "url": "https://en.wikipedia.org/wiki?curid=46713039", "title": "Chief services officer", "text": "Chief services officer\n\nThe chief services officer (CSO) is a position at the head of a firm carrying significant service design responsibilities. The CSO typically is responsible for developing processes and tools, both internally and externally, for producing maximum value to all stakeholders with intelligent and efficient use of potentially fluctuating human resources.\n\nIn some organizations, the same person may hold this title along with that of chief operations officer (COO) as they both are same level roles. Alternatively, a company could have one or the other, or both occupied by separate people. Often, a CSO exists in heavily client-focussed companies, while a COO exists in product development focused companies. A CSO almost always has a strong operations background and advanced degree, whereas a COO often has a background in business development. Both CSO and COO report to the CEO or managing director of a company.\n"}
{"id": "31063926", "url": "https://en.wikipedia.org/wiki?curid=31063926", "title": "Climate of Saudi Arabia", "text": "Climate of Saudi Arabia\n\nThe climate of Saudi Arabia is marked by high temperatures during the day and low temperatures at night. Most of the country follows the pattern of the desert climate, with the exception of the southwest, which features a semi-arid climate.\n"}
{"id": "22369406", "url": "https://en.wikipedia.org/wiki?curid=22369406", "title": "CodonCode Aligner", "text": "CodonCode Aligner\n\nCodonCode Aligner is a commercial application for DNA sequence assembly, sequence alignment, and editing on Mac OS X and Windows.\n\nFeatures include chromatogram editing, end clipping, and vector trimming, sequence assembly and contig editing, aligning cDNA against genomic templates, sequence alignment and editing, alignment of contigs to each other with ClustalW, MUSCLE, or built-in algorithms, mutation detection, including detection of heterozygous single-nucleotide polymorphism, analysis of heterozygous insertions and deletions, start online BLAST searches, restriction analysis (find and view restriction cut sites), trace sharpening, and support for Phred, Phrap, ClustalW, and MUSCLE.\n\nThe first beta version of CodonCode Aligner was released in April 2003, followed by the first full version in June 2003. Major upgrades were released in 2003, 2004, 2005, 2006, 2007, and 2008.\n\nIn April 2009, CodonCode Aligner had been cited in more than 400 scientific publications. Citations cover a wide variety of biomedical research areas, including HIV research, biogeography and environmental biology, DNA methylation studies, genetic diseases, clinical microbiology, and evolution research and phylogenetics.\n\n\n"}
{"id": "58037802", "url": "https://en.wikipedia.org/wiki?curid=58037802", "title": "Compensatory conductance", "text": "Compensatory conductance\n\nThe compensatory root water uptake conductance (Kcomp) (formula_1) characterizes how a plant compensates its water uptake under heterogeneous water potential. \nIt controls the root water uptake in a soil where the water potential is not uniform.\n\n"}
{"id": "6385634", "url": "https://en.wikipedia.org/wiki?curid=6385634", "title": "Criticism of the Space Shuttle program", "text": "Criticism of the Space Shuttle program\n\nCriticism of the Space Shuttle program stemmed from claims that NASA's Shuttle program failed to achieve its promised cost and utility goals, as well as design, cost, management, and safety issues. More specifically, it failed in the goal of reducing the cost of space access. Space Shuttle incremental per-pound launch costs ultimately turned out to be considerably higher than those of expendable launchers.\n\nBy 2011, the incremental cost per flight of the Space Shuttle was estimated at $450 million, or $18,000 per kilogram (approximately $8,000 per pound) to low Earth orbit (LEO). By comparison, Russian Proton expendable cargo launchers (Atlas V rocket counterpart), still largely based on the design that dates back to 1965, are said to cost as little as $110 million, or around $5,000/kg (approximately $2,300 per pound) to LEO.\n\nWhen all design and maintenance costs are taken into account, the final cost of the Space Shuttle program, averaged over all missions and adjusted for inflation, was estimated to come out to $1.5 billion per launch, or $60,000/kg (approximately $27,000 per pound) to LEO. This should be contrasted with the originally envisioned costs of $118 per pound of payload in 1972 dollars (approximately $ per pound adjusting for inflation to 2011).\n\nIt failed in the goal of achieving reliable access to space, partly due to multi-year interruptions in launches following Shuttle failures. NASA budget pressures caused by the chronically high NASA Space Shuttle program costs have eliminated NASA manned space flight beyond low earth orbit since Apollo, and severely curtailed use of unmanned probes. NASA's promotion of and reliance on the Shuttle slowed domestic commercial expendable launch vehicle (ELV) programs until after the 1986 \"Challenger\" disaster.\n\n\"Space Transportation System\" (NASA's formal name for the overall Shuttle program) was created to transport crewmembers and payloads into low Earth orbits. It would afford the opportunity to conduct science experiments on board the shuttle to be used to study the effects of space flight on humans, animals and plants. Other experiments would study how things can be manufactured in space. The shuttle would also enable astronauts to launch satellites from the shuttle and even repair satellites already out in space. The Shuttle was also intended for research into the human response to zero-g.\n\nThe Shuttle was originally billed as a space vehicle that would be able to launch once a week and give low launch costs through amortization. Development costs were expected to be recouped through frequent access to space. These claims were made in an effort to obtain budgetary funding from the United States Congress. Beginning in 1981, the space shuttle began to be used for space travel. However, by the mid-1980s the concept of flying that many shuttle missions proved unrealistic and scheduled launch expectations were reduced 50%. Following the \"Challenger\" accident in 1986, missions were halted pending safety review. This hiatus became lengthy and ultimately lasted almost three years as arguments over funding and the safety of the program continued. Eventually the military resumed the use of expendable launch vehicles instead. Missions were put on hold again after the loss of \"Columbia\" in 2003. Overall, 135 missions were launched during the 30 years after the first orbital flight of \"Columbia\", averaging approximately one every 3 months.\n\nSome reasons for the higher-than-expected operational costs were:\n\nSome researchers have criticized a pervasive shift in NASA culture away from safety in order to ensure that launches took place in a timely fashion, sometimes called \"go fever\". Allegedly, NASA upper-level management embraced this decreased safety focus in the 1980s while some engineers remained wary. According to sociologist Diane Vaughan, the aggressive launch schedules arose in the Reagan years as a way to rehabilitate America's post-Vietnam prestige.\n\nThe physicist Richard Feynman, who was appointed to the official inquiry on the \"Challenger\" disaster, estimated the risk to be \"on the order of a percent\" in his report, adding, \"Official management, on the other hand, claims to believe the probability of failure is a thousand times less. One reason for this may be an attempt to assure the government of NASA perfection and success in order to ensure the supply of funds. The other may be that they sincerely believed it to be true, demonstrating an almost incredible lack of communication between themselves and their working engineers.\"\n\nDespite Feynman's warnings, and despite the fact that Vaughan served on safety boards and committees at NASA, the subsequent press coverage has found some evidence that NASA's relative disregard for safety might persist to this day. For example, NASA discounted the risk from small foam chunk breakage at launch and assumed that the lack of damage from prior foam collisions suggested the future risk was low.\n\nThe Shuttle was originally conceived to operate somewhat like an airliner. After landing, the orbiter would be checked out and start \"mating\" to the rest of the system (the ET and SRBs), and be ready for launch in as little as two weeks. Instead, this turnaround process usually took months; \"Atlantis\" set the pre-\"Challenger\" record by launching twice within 54 days, while \"Columbia\" set the post-\"Challenger\" record of 88 days. Naturally, the Shuttle program's goal of returning its crew to Earth safely conflicts with the goal of a rapid and inexpensive payload launch. Furthermore, because in many cases there are no survivable abort modes, many pieces of hardware simply must function perfectly and so must be carefully inspected before each flight. The result is high labor cost, with around 25,000 workers in Shuttle operations and labor costs of about $1 billion per year.\n\nSome shuttle features initially presented as important to Space Station support have proved superfluous:\n\nWhile the technical details of the \"Challenger\" and \"Columbia\" accidents are different, the organizational problems show similarities. Flight engineers' concerns about possible problems were not properly communicated to or understood by senior NASA managers. The vehicle gave ample warning beforehand of abnormal problems. A heavily layered, procedure-oriented bureaucratic structure inhibited necessary communication and action.\n\nWith \"Challenger\", an O-ring that should not have eroded at all did erode on earlier shuttle launches. Yet managers felt that because it had previously eroded by no more than 30%, this was not a hazard as there was \"a factor of three safety margin\". Morton-Thiokol designed and manufactured the SRBs, and during a pre-launch conference call with NASA, Roger Boisjoly, the Thiokol engineer most experienced with the O-rings, pleaded with management repeatedly to cancel or reschedule the launch. He raised concerns that the unusually low temperatures would stiffen the O-rings, preventing a complete seal during flexing of the rocket motor segments, which was exactly what happened on the fatal flight. However, Thiokol's senior managers, under pressure from NASA management, overruled him and allowed the launch to proceed. One week prior to the launch, Thiokol's contract to reprocess the solid rocket boosters was also due for review, and cancelling the flight was an action that Thiokol management wanted to avoid. \"Challenger\"'s O-rings eroded completely through as predicted, resulting in the complete destruction of the spacecraft and the loss of all seven astronauts on board.\n\n\"Columbia\" was destroyed because of damaged thermal protection from foam debris that broke off from the external tank during ascent. The foam had not been designed or expected to break off, but had been observed in the past to do so without incident. The original shuttle operational specification said the orbiter thermal protection tiles were not designed to withstand any debris hits at all. Over time NASA managers gradually accepted more tile damage, similar to how O-ring damage was accepted. The \"Columbia\" Accident Investigation Board called this tendency the \"normalization of deviance\" – a gradual acceptance of events outside the design tolerances of the craft simply because they had not been catastrophic to date.\n\nThe subject of missing or damaged thermal tiles on the Shuttle fleet only became an issue following the loss of \"Columbia\" in 2003, as it broke up on re-entry. In fact, Shuttles had previously come back missing as many as 20 tiles without any problem. STS-1 and STS-41 had all flown with missing thermal tiles from the orbital maneuvering system pods (visible to the crew).\n\nThis image from the NASA archives shows several missing tiles on the STS-1 OMS pods. The problem on \"Columbia\" was that the damage was sustained from a foam strike to the reinforced carbon-carbon leading edge panel of the wing, not the heat tiles. The first Shuttle mission, STS-1, had a protruding gap filler that diverted hot gas into the right wheel well on re-entry, resulting in a buckling of the right main landing gear door.\n\nWhile the system was \"developed\" within the original cost and time estimates given to President Richard M. Nixon in 1971, the operational costs, flight rate, payload capacity, and reliability by the time of the February 2003 \"Columbia\" accident proved to be much worse than originally anticipated. A year before STS-1's April 1981 launch, \"The Washington Monthly\" accurately forecast many of the Shuttle's issues, including an overambitious launch schedule and the consequently higher-than-expected marginal cost per flight; the risks of depending on the Shuttle for all payloads, civilian and military; the lack of a survivable abort scenario if a Solid Rocket Booster were to fail; and the fragility of the Shuttle's thermal protection system.\n\nIn order to get the Shuttle approved, NASA over-promised its economies and utility. To justify its very large fixed operational program cost, NASA initially forced all domestic, internal, and Department of Defense payloads to the shuttle. When that proved impossible (after the \"Challenger\" disaster), NASA used the International Space Station (ISS) as a justification for the shuttle. Some speculate that, had NASA avoided the Shuttle program and instead continued to use Saturn and commercially available boosters, costs might have been lower, freeing funds for manned exploration and more unmanned space science. In particular, NASA administrator Michael D. Griffin argued in a 2007 paper that the Saturn program, if continued, could have provided six manned launches per year — two of them to the Moon — at the same cost as the Shuttle program, with an additional ability to loft infrastructure for further missions:\nSome had argued that the Shuttle program was flawed. Achieving a reusable vehicle with early 1970s technology forced design decisions that compromised operational reliability and safety. Reusable main engines were made a priority. This necessitated that they not burn up upon atmospheric reentry, which in turn made mounting them on the orbiter itself (the one part of the Shuttle system where reuse was paramount) a seemingly logical decision. However, this had the following consequences:\n\nA concern expressed by the 1990 Augustine Commission was that \"the civil space program is overly dependent upon the Space Shuttle for access to space.\" The committee pointed out, \"that it was, for example, inappropriate in the case of \"Challenger\" to risk the lives of seven astronauts and nearly one-fourth of NASA's launch assets to place in orbit a communications satellite.\"\n\nThere are some NASA spin-off technologies related to the Space Shuttle program which have been successfully developed into commercial products, such as using heat-resistant materials developed to protect the Shuttle on reentry, in suits for municipal and aircraft rescue firefighters.\n\n\n\n"}
{"id": "25584664", "url": "https://en.wikipedia.org/wiki?curid=25584664", "title": "Discharge coefficient", "text": "Discharge coefficient\n\nIn a nozzle or other constriction, the discharge coefficient (also known as coefficient of discharge) is the ratio of the actual discharge to the theoretical discharge, i.e., the ratio of the mass flow rate at the discharge end of the nozzle to that of an ideal nozzle which expands an identical working fluid from the same initial conditions to the same exit pressures.\n\nMathematically the discharge coefficient may be related to the mass flow rate of a fluid through a straight tube of constant cross-sectional area through the following\n\nWhere:\n\nThis parameter is useful for determining the irrecoverable losses associated with a certain piece of equipment (constriction) in a fluid system, or the \"resistance\" that piece of equipment imposes upon the flow.\n\nThis flow resistance, often expressed as a dimensionless parameter, formula_10, is related to the discharge coefficient through the equation:\n\nwhich may be obtained by substituting formula_9 in the aforementioned equation with the resistance, formula_10, multiplied by the dynamic pressure of the fluid, formula_14.\n\nDue to complex behavior of fluids around some of the structures such as orifices, gates, and weirs etc., some assumptions are made for the theoretical analysis of the stage-discharge relationship. For example, in case of gates, the pressure at the gate opening is non-hydrostatic which is difficult to model; however, it is known that the pressure at the gate is very small. Therefore, engineers assume that the pressure is zero at the gate opening and following equation is obtained for discharge:\n\nwhere:\n\nHowever, the pressure is not actually zero at the gate; therefore, discharge coefficient, \"C\" is used as follows:\n\n\n"}
{"id": "24729471", "url": "https://en.wikipedia.org/wiki?curid=24729471", "title": "Duverney fracture", "text": "Duverney fracture\n\nDuverney fractures are isolated pelvic fractures involving only the iliac wing. They are caused by direct trauma to the iliac wing, and are generally stable fractures as they do not disrupt the weight bearing pelvic ring.\n\nThe fracture is named after the French surgeon Joseph Guichard Duverney who described the injury in his book \"Maladies des Os\" which was published posthumously in 1751.\n\nMalunion and deformity of the iliac wing can occur. Injury to the internal iliac artery can occur, leading to hypovolaemic shock. Perforation of the bowel can occur, leading to sepsis. Damage to the adjacent nerves of the lumbosacral plexus has also been described.\n\nDuverney fractures can usually be seen on pelvic X-rays, but CT scans are required to fully delineate the fracture and to look for associated fractures involving the pelvic ring.\n\nSince fractures that do not involve the weight bearing part of the pelvic ring tend to be stable fractures, they can often be managed without surgery. These fractures tend to be very painful, so walking aids such as crutches or walking frames may be needed until the pain settles.\n\nOpen reduction internal fixation is sometimes required to correct deformity, and surgery may be required if there is damage to blood vessels, nerves or organs, or if the fracture is open.\n"}
{"id": "15683766", "url": "https://en.wikipedia.org/wiki?curid=15683766", "title": "Earth Revealed: Introductory Geology", "text": "Earth Revealed: Introductory Geology\n\nEarth Revealed: Introductory Geology, originally titled Earth Revealed, is a 26-part video instructional series covering the processes and properties of the physical Earth, with particular attention given to the scientific theories underlying geological principles. The telecourse was produced by Intelecom and the Southern California Consortium, was funded by the Annenberg/CPB Project, and first aired on PBS in 1992 with the title \"Earth Revealed\". All 26 episodes are hosted by Dr. James L. Sadd, professor of environmental science at Occidental College in Los Angeles, California.\n\nSome footage used in \"Earth Revealed\" previously had been seen in the 1986 PBS series \"Planet Earth\".\n\n\n"}
{"id": "4814724", "url": "https://en.wikipedia.org/wiki?curid=4814724", "title": "Elysium Mons", "text": "Elysium Mons\n\nElysium Mons is a volcano on Mars located in the volcanic province Elysium, at , in the Martian eastern hemisphere. It stands about above the surrounding lava plains, and about above the Martian \"datum\". Its diameter is about , with a summit caldera about across. It is flanked by the smaller volcanoes Hecates Tholus to the northeast, and Albor Tholus to the southeast.\n\nElysium Mons was discovered in 1972 in images returned by the Mariner 9 orbiter.\n\nThe terrestrial volcano Emi Koussi (in Chad) has been studied as an analog of Elysium Mons. The two shield volcanoes have summit calderas of similar size, but Elysium Mons is 3.5 times larger in diameter and 6 times higher than its counterpart on Earth.\n\nA 6.5 km diameter crater at 29.674 N, 130.799 E, in the volcanic plains to the northwest of Elysium Mons has been identified as a possible source for the nakhlite meteorites, a family of similar basaltic Martian meteorites with cosmogenic ages of about 10.7 Ma, suggesting ejection from Mars by a single impact event. The dates of the igneous rocks of the nakhlites range from 1416 ± 7 Ma to 1322 ± 10 Ma. These dates plus the crater dimensions suggest a growth rate of the source volcano during that interval of 0.4–0.7 m per Ma, far slower than would be expected for a terrestrial volcano. This implies that Martian volcanism had slowed greatly by that point in history.\n\n"}
{"id": "57722636", "url": "https://en.wikipedia.org/wiki?curid=57722636", "title": "Explorer 47", "text": "Explorer 47\n\nExplorer 47, also known IMP-7 and Interplanetary Monitoring Platform IMP-H, was an American satellite launched as part of Explorers program. Explorer 47 as launched on 23 September 1972 on Cape Canaveral, Florida, U.S., with an Delta rocket. Explorer 47 was the seventh satellite of the Interplanetary Monitoring Platform.\n\nExplorer 47 continued the study begun by earlier IMP spacecraft of the interplanetary and magnetotail regions from a nearly circular orbit, near 37 earth radii. This 16 sided drum-shaped spacecraft was high and in diameter. It was designed to measure energetic particles, plasma and electric and magnetic fields. The spin axis was normal to the ecliptic plane, and the spin period was 1.3 s. The spacecraft was powered by solar cells and a chemical battery. Scientific data were telemetered at 1600 bps (with a secondary 400 bps rate available). \n\nThe spacecraft was turned off on October 31, 1978.\n"}
{"id": "4127125", "url": "https://en.wikipedia.org/wiki?curid=4127125", "title": "GHP formalism", "text": "GHP formalism\n\nThe GHP formalism (or Geroch–Held–Penrose formalism) is a technique used in the mathematics of general relativity that involves singling out a pair of null directions at each point of spacetime.\n\n"}
{"id": "1854663", "url": "https://en.wikipedia.org/wiki?curid=1854663", "title": "GLIMMER", "text": "GLIMMER\n\nIn bioinformatics, GLIMMER (Gene Locator and Interpolated Markov ModelER) is used to find genes in prokaryotic DNA. \"It is effective at finding genes in bacteria, archea, viruses, typically finding 98-99% of all relatively long protein coding genes\". GLIMMER was the first system that used the interpolated Markov model to identify coding regions. The GLIMMER software is open source and is maintained by Steven Salzberg, Art Delcher, and their colleagues at the \"Center for Computational Biology\" at Johns Hopkins University. The original GLIMMER algorithms and software were designed by Art Delcher, Simon Kasif and Steven Salzberg and applied to bacterial genome annotation in collaboration with Owen White. \n\nFirst Version of GLIMMER \"i.e., GLIMMER 1.0\" was released in 1998 and it was published in the paper \"Microbial gene identification using interpolated Markov model\". Markov models were used to identify microbial genes in GLIMMER 1.0. GLIMMER considers the local composition sequence dependencies which makes GLIMMER more flexible and more powerful when compared to fixed-order Markov model.\n\nThere was a comparison made between interpolated Markov model used by GLIMMER and fifth order Markov model in the paper \"Microbial gene identification using interpolated Markov models\". \"GLIMMER algorithm found 1680 genes out of 1717 annotated genes in Haemophilus influenzae where fifth order Markov model found 1574 genes. GLIMMER found 209 additional genes which were not included in 1717 annotated genes where fifth order Markov model found 104 genes.\"'\n\nSecond Version of GLIMMER i.e., GLIMMER 2.0 was released in 1999 and it was published in the paper \"Improved microbial identification with GLIMMER\". This paper provides significant technical improvements such as using interpolated context model instead of interpolated Markov model and resolving overlapping genes which improves the accuracy of GLIMMER.\n\nInterpolated context models are used instead of interpolated Markov model which gives the flexibility to select any base. In interpolated Markov model probability distribution of a base is determined from the immediate preceding bases. If the immediate preceding base is irrelevant amino acid translation, interpolated Markov model still considers the preceding base to determine the probability of given base where as interpolated context model which was used in GLIMMER 2.0 can ignore irrelevant bases. False positive predictions were increased in GLIMMER 2.0 to reduce the number of false negative predictions. Overlapped genes are also resolved in GLIMMER 2.0.\n\nVarious comparisons between GLIMMER 1.0 and GLIMMER 2.0 were made in the paper \"Improved microbial identification with GLIMMER\" which shows improvement in the later version. \"Sensitivity of GLIMMER 1.0 ranges from 98.4 to 99.7% with an average of 99.1% where as GLIMMER 2.0 has a sensitivity range from 98.6 to 99.8% with an average of 99.3%. GLIMMER 2.0 is very effective in finding genes of high density. The parasite Trypanosoma brucei, responsible for causing African sleeping sickness is being identified by GLIMMER 2.0\" \n\nThird version of GLIMMER, \"GLIMMER 3.0\" was released in 2007 and it was published in the paper \"Identifying bacterial genes and endosymbiont DNA with Glimmer\". This paper describes several major changes made to the GLIMMER system including improved methods to identify coding regions and start codon. Scoring of ORF in GLIMMER 3.0 is done in reverse order i.e., starting from stop codon and moves back towards the start codon. Reverse scanning helps in identifying the coding portion of the gene more accurately which is contained in the context window of IMM. GLIMMER 3.0 also improves the generated training set data by comparing the long-ORF with universal amino acid distribution of widely disparate bacterial genomes.\"GLIMMER 3.0 has an average long-ORF output of 57% for various organisms where as GLIMMER 2.0 has an average long-ORF output of 39%.\"\n\nGLIMMER 3.0 reduces the rate of false positive predictions which were increased in GLIMMER 2.0 to reduce the number of false negative predictions. \"GLIMMER 3.0 has a start-site prediction accuracy of 99.5% for 3'5' matches where as GLIMMER 2.0 has 99.1% for 3'5' matches. GLIMMER 3.0 uses a new algorithm for scanning coding regions, a new start site detection module, and architecture which integrates all gene predictions across an entire genome.\"\n\nMinimum description length\n\nThe GLIMMER project helped introduce and popularize the use of variable length models in Computational Biology and Bioinformatics that subsequently have been applied to numerous problems such as protein classification and others. Variable length modeling was originally pioneered by information theorists and subsequently ingeniously applied and popularized in data compression (e.g. Ziv-Lempel compression). Prediction and compression are intimately linked using Minimum Description Length Principles. The basic idea is to create a dictionary of frequent words (motifs in biological sequences). The intuition is that the frequently occurring motifs are likely to be most predictive and informative. In GLIMMER the interpolated model is a mixture model of the probabilities of these relatively common motifs. Similarly to the development of HMMs in Computational Biology, the authors of GLIMMER were conceptually influenced by the previous application of another variant of interpolated Markov models to speech recognition by researchers such as Fred Jelinek (IBM) and Eric Ristad (Princeton). The learning algorithm in GLIMMER is different from these earlier approaches.\n\nGLIMMER can be downloaded from The Glimmer home page (requires a C++ compiler).\nAlternatively, an online version is hosted by NCBI .\n\n1. GLIMMER primarily searches for long-ORFS. An open reading frame might overlap with any other open reading frame which will be resolved using the technique described in the sub section. Using these long-ORFS and following certain amino acid distribution GLIMMER generates training set data.\n\n2. Using these training data, GLIMMER trains all the six Markov models of coding DNA from zero to eight order and also train the model for noncoding DNA\n\n3. GLIMMER tries to calculate the probabilities from the data. Based on the number of observations, GLIMMER determines whether to use fixed order Markov model or interpolated Markov model.\n\n4. GLIMMER obtains score for every long-ORF generated using all the six coding DNA models and also using non-coding DNA model.\n\n5. If the score obtained in the previous step is greater than a certain threshold then GLIMMER predicts it to be a gene.\n\nThe steps explained above describes the basic functionality of GLIMMER. There are various improvements made to GLIMMER and some of them are described in the following sub-sections.\n\nGLIMMER system consists of two programs. First program called build-imm, which takes an input set of sequences and outputs the interpolated Markov model as follows.\n\nThe probability for each base i.e., A,C,G,T for all k-mers for 0 ≤ k ≤ 8 is computed. Then, for each k-mer, GLIMMER computes weight. New sequence probability is computed as follows.\n\nformula_1\n\nwhere n is the length of the sequence formula_2 is the oligomer at position x. formula_3, the formula_4-order interpolated Markov model score is computed as\n\nformula_5\n\n\"where formula_6 is the weight of the k-mer at position x-1 in the sequence S and formula_7 is the estimate obtained from the training data of the probability of the base located at position x in the formula_8-order model.\"\n\nThe probability of base formula_2 given the i previous bases is computed as follows.\n\nformula_10\n\n\"The value of formula_11 associated with formula_12 can be regarded as a measure of confidence in the accuracy of this value as an estimate of the true probability. GLIMMER uses two criteria to determine formula_11. The first of these is simple frequency occurrence in which the number of occurrences of context string formula_14 in the training data exceeds a specific threshold value, then formula_11 is set to 1.0. The current default value for threshold is 400, which gives 95% confidence. When there are insufficient sample occurrences of a context string, build-imm employ additional criteria to determine formula_16 value. For a given context string formula_14 of length i, build-imm compare the observed frequencies of the following base formula_18, formula_19, formula_20, formula_21 with the previously calculated interpolated Markov model probabilities using the next shorter context, formula_22, formula_23, formula_24, formula_25. Using a formula_26 test, build-imm determine how likely it is that the four observed frequencies are consistent with the IMM values from the next shorter context.\"\n\nThe second program called glimmer, then uses this IMM to identify putative gene in an entire genome. GLIMMER identifies all the open reading frame which score higher than threshold and check for overlapping genes. Resolving overlapping genes is explained in the next sub-section.\n\nEquations and explanation of the terms used above are taken from the paper 'Microbial gene identification using interpolated Markov models\"\"\n\nIn GLIMMER 1.0, when two genes A and B overlap, the overlap region is scored. If A is longer than B, and if A scores higher on the overlap region, and if moving B's start site will not resolve the overlap, then B is rejected.\n\nGLIMMER 2.0 provided a better solution to resolve the overlap. In GLIMMER 2.0, when two potential genes A and B overlap, the overlap region is scored. Suppose gene A scores higher, four different orientations are considered.\n\nIn the above case, moving of start sites does not remove the overlap. If A is significantly longer than B, then B is rejected or else both A and B are called genes, with a doubtful overlap.\n\nIn the above case, moving of B can resolve the overlap, A and B can be called non overlapped genes but if B is significantly shorter than A, then B is rejected.\n\nIn the above case, moving of A can resolve the overlap. A is only moved if overlap is a small fraction of A or else B is rejected.\n\nIn the above case, both A and B can be moved. We first move the start of B until the overlap region scores higher for B. Then we move the start of A until it scores higher. Then B again, and so on, until either the overlap is eliminated or no further moves can be made.\n\nThe above example has been taken from the paper 'Identifying bacterial genes and endosymbiont DNA with Glimmer\"\"\n\nRibosome binding site(RBS) signal can be used to find true start site position. GLIMMER results are passed as an input for RBSfinder program to predict ribosome binding sites. GLIMMER 3.0 integrates RBSfinder program into gene predicting function itself.\n\nELPH software( which was determined as highly effective at identifying RBS in the paper) is used for identifying RBS and is available at this website. Gibbs sampling algorithm is used to identify shared motif in any set of sequences. This shared motif sequences and their length is given as input to ELPH. ELPH then computes the position weight matrix(PWM) which will be used by GLIMMER 3 to score any potential RBS found by RBSfinder. The above process is done when we have a substantial amount of training genes. If there are inadequate number of training genes, GLIMMER 3 can bootstrap itself to generate a set of gene predictions which can be used as input to ELPH. ELPH now computes PWM and this PWM can be again used on the same set of genes to get more accurate results for start-sites. This process can be repeated for many iterations to obtain more consistent PWM and gene prediction results.\n\nGlimmer supports genome annotation efforts on a wide range of bacterial, archaeal, and viral species. In a large-scale reannotation effort at the DNA Data Bank of Japan (DDBJ, which mirrors Genbank). Kosuge \"et al.\" (2006) examined the gene finding methods used for 183 genomes. They reported that of these projects, Glimmer was the gene finder for 49%, followed by GeneMark with 12%, with other algorithms used in 3% or fewer of the projects. (They also reported that 33% of genomes used \"other\" programs, which in many cases meant that they could not identify the method. Excluding those cases, Glimmer was used for 73% of the genomes for which the methods could be unambiguously identified.) Glimmer was used by the DDBJ to re-annotate all bacterial genomes in the International Nucleotide Sequence Databases. It is also being used by this group to annotate viruses. Glimmer is part of the bacterial annotation pipeline at the National Center for Biotechnology Information (NCBI), which also maintains a web server for Glimmer, as do sites in Germany, Canada.\n\nAccording to Google Scholar, as of early 2011 the original Glimmer article (Salzberg et al., 1998) has been cited 581 times, and the Glimmer 2.0 article (Delcher et al., 1999) has been cited 950 times.\n\n"}
{"id": "14621107", "url": "https://en.wikipedia.org/wiki?curid=14621107", "title": "Gula Mons", "text": "Gula Mons\n\nGula Mons is a volcano in western Eistla Regio on Venus; it is high and located at approximately 22 degrees north latitude, 359 degrees east longitude.\n\nIts main feature is a NE-SW-oriented rift-like fracture set connecting two summit calderas. There is also a structure which links the northern caldera and ridge system to Idem Kuva corona located NW of Gula Mons. Radially spreading lava flows which have digitate and broad sheet-like forms extend from the summit, including radar-dark flows which overlay several older lava deposits. Radial and circumferential fractures are present on the flanks.\n"}
{"id": "2323690", "url": "https://en.wikipedia.org/wiki?curid=2323690", "title": "Howard Cary", "text": "Howard Cary\n\nHenry Cary (3 May 1908 – 20 December 1991) was an American engineer and the co-founder of the Applied Physics Corporation (later known as Cary Instruments), along with George W. Downs and William Miller. The Cary 14 UV-Vis-NIR and the Cary Model 81 Raman Spectrophotometer were particularly important contributions in scientific instrumentation and spectroscopy. Before starting Applied Physics, Cary was employed by Beckman Instruments, where he worked on the design of several instruments including the ubiquitous DU spectrophotometer. Howard Cary was a founder and the first president of the Optical Society of Southern California.\n\nHenry Howard Cary was born on 3 May 1908 in Los Angeles, California to Henry Gardner Cary and Bessie (Brown) Cary.\n\nThe 1940 US Census listed Cary as married to Barbara (Ward) Cary from Washington state. His occupation was recorded as research engineer and industry as laboratory.\nIn 1991, Cary died of pneumonia after a long illness at Orange, California.\n\nIn 1925, after graduating from Los Angeles High School, Cary entered the California Institute of Technology. He missed one year due to illness, and graduated in 1930 with a B.S. degree in civil engineering. In sports, he was captain of the varsity tennis squad. During his first year he won the junior travel prize. Cary was a member of Sigma Xi.\n\nAfter receiving his degree, Cary went to work in his father's plumbing construction business, H. G. Cary Co. He held a variety of engineering and accounting positions with the company during the early years of the Great Depression.\n\nAs of May 31, 1935, Cary was hired by Arnold Beckman of National Technical Laboratories (NTL) (later Beckman Instruments). By 1937, Cary was the chief design engineer on Beckman's research team. Cary distinguished himself in work relating to pH meters and glass electrodes, and became vice-president of development. By 1940 Cary and Beckman were developing a quartz spectrophotometer. They presented a paper on this work in July 1941 at MIT's Summer Conference on Spectroscopy. Cary made substantial contributions, including the design of a reliable ultraviolet phototube for the instrument. The DU spectrophotometer was the first easy-to-use single instrument containing both the optical and electronic components needed for ultraviolet-absorption spectrophotometry.\n\nDuring World War II, National Technical Laboratories worked on a number of then-secret projects, including one for the development of synthetic rubber. The Office of Rubber Reserve of the United States government contracted with NTL to produce an infrared spectrophotometer based on a single-beam design by Robert Brattain of Shell Development Company. The first Beckman IR-1 Spectrophotometer was shipped to Shell on September 18, 1942 barely six months after it was ordered. The IR-1 used a Littrow prism mounting featuring a single rock salt prism with a mirrored back, and an analog galvanometer for presenting results.\n\nCary and Beckman adapted features from the pH meter and the DU spectrophotometer to improve the design of the IR-1 spectrophotometer.\nBy 1945, the IR-2 spectrophotometer was in production, using an electronic vacuum tube amplifier instead of a galvanometer, and a thermocouple tube for the detection of infrared light.\n\nIn January 1946, Cary left NTL to form his own company, Applied Physics Corporation (later known as Cary Instruments) in Pasadena, California, with George W. Downs, William Miller, and Russell E. Vaniman. Cary and his company developed a range of scientific instruments, particularly dual-beam spectrophotometers. The Applied Physics Corporation made its first delivery, a Cary 11 UV-Vis spectrophotometer, to Mellon Institute in Pittsburgh, Pennsylvania, in April 1947. The Cary 11 was followed by the Cary 14 UV-Vis-NIR in 1954, the Cary 15 UV-Vis in 1961, the Cary 16 UV-Vis in 1964, and an expanded offering of instruments through the 70s, 80s, and 90s.\n\nThe Cary 14 spectrophotometer used a double folded-z-configuration monochromator. Appearing on the market in 1954, it was the first commercial UV-VIS-NIR instrument to fully extend into the near-infrared spectrum.\n\nThe Cary Model 81 Raman Spectrophotometer was an important contribution to high-performance Raman spectroscopy. Described as \"famous\" it gave the field of Raman spectrophotometry a \"tremendous boost\" in the United States.\n\nOther instruments included nondispersive infrared gas analyzers electrometers such as the Cary Model 31 and 36 Electrometers which used a vibrating reed with an ionization chamber and calorimeters such as the Cary Model 41 Calorimeter.\n\nIn 1966 the Applied Physics Corporation was one of a number of companies acquired by Varian Associates during a period of rapid expansion by Varian. Applied Physics Corporation was renamed Cary Instruments as a subsidiary of Varian. Also in 1966, groundbreaking for a new building for the company occurred in Monrovia, California. \nIn 1972 the company moved to Varian's facilities in Palo Alto, California. In 1982, it moved again, to Varian's Techtron facilities in Melbourne, Australia.\n\nIn 1959, Cary was the recipient of the American Chemical Society Division of Analytical Chemistry's Award in Chemical Instrumentation. Robert Brattain described Cary's approach to instrument design, when giving the award.\n\nIn 1969, Cary was awarded the David Richardson Medal by the Optical Society of America. Cary was honored for: \"his painstakingly careful and very valuable contributions to the design and production of highly precise instrumentation in areas which range from spectroscopy to chemical, medical and nuclear research.\"\n\nIn 1977, Howard Cary received the Maurice F. Hasler Award at Pittcon \"for his pioneering leadership in the development of instrumentation for absorption and Raman spectroscopy\".\n\nCary is listed on a considerable number of patents for his work at National Technical Laboratories and Cary Instruments, often as \"Henry H. Cary\" or \"H. H. Cary\". They include: \n\n\n\n"}
{"id": "1847015", "url": "https://en.wikipedia.org/wiki?curid=1847015", "title": "Interferometric visibility", "text": "Interferometric visibility\n\nThe interferometric visibility (also known as interference visibility and fringe visibility, or just visibility when in context) quantifies the contrast of interference in any system which has wave-like properties, such as optics, quantum mechanics, water waves, or electrical signals. Generally, two or more waves are combined and as the phase difference between them varies, the power or intensity (probability or population in quantum mechanics) of the resulting wave oscillates, forming an interference pattern. The pattern may be visible all at once because the phase difference varies as a function of space, as in a 2-slit experiment. Alternately, the phase difference may be manually controlled by the operator, for example by adjusting a vernier knob in an interferometer. The ratio of the size or amplitude of these oscillations to the sum of the powers of the individual waves is defined as the visibility.\n\nThe interferometric visibility gives a practical way to measure the coherence of two waves (or one wave with itself). A theoretical definition of the coherence is given by the degree of coherence, using the notion of correlation.\n\nIn linear optical interferometers (like the Mach-Zehnder interferometer, Michelson interferometer, and Sagnac interferometer), interference manifests itself as intensity oscillations over time or space, also called \"fringes\". Under these circumstances, the interferometric visibility is also known as the \"Michelson visibility\" or the \"fringe visibility.\" For this type of interference, the sum of the intensities (powers) of the two interfering waves equals the average intensity over a given time or space domain. The visibility is written as:\n\nin terms of the amplitude envelope of the oscillating intensity and the average intensity:\n\nSo it can be rewritten as:\n\nwhere \"I\" is the maximum intensity of the oscillations and \"I\" the minimum intensity of the oscillations. If the two optical fields are ideally monochromatic (consist of only single wavelength) point sources of the same polarization, then the predicted visibility will be\n\nwhere formula_6 and formula_7 indicate the intensity of the respective wave. Any dissimilarity between the optical fields will decrease the visibility from the ideal. In this sense, the visibility is a measure of the coherence between two optical fields. A theoretical definition for this is given by the degree of coherence. This definition of interference directly applies to the interference of water waves and electric signals.\n\nSince the Schrödinger equation is a wave equation and all objects can be considered waves in quantum mechanics, interference is ubiquitous. Some examples: Bose–Einstein condensates can exhibit interference fringes. Atomic populations show interference in a Ramsey interferometer. Photons, atoms, electrons, neutrons, and molecules have exhibited interference in double-slit interferometers.\n\n\n"}
{"id": "483214", "url": "https://en.wikipedia.org/wiki?curid=483214", "title": "Iosif Shklovsky", "text": "Iosif Shklovsky\n\nIosif Samuilovich Shklovsky (; sometimes transliterated \"Josif, Josif, Shklovskii, Shklovskij\") (July 1, 1916 – March 3, 1985) was a Soviet astronomer and astrophysicist. He is remembered for work in theoretical astrophysics and other topics, as well as for his 1962 book on extraterrestrial life, the revised and expanded version of which was co-authored by American astronomer Carl Sagan in 1966 as \"Intelligent Life in the Universe\". \n\nHe won the Lenin Prize in 1960 and the Bruce Medal in 1972. Asteroid 2849 Shklovskij and the crater Shklovsky (on the Martian moon Phobos) are named in his honor. He was a Corresponding Member of Soviet Academy of Sciences since 1966.\n\nShklovsky was born in Hlukhiv, a city in the Ukrainian part of the Russian Empire, into a poor Ukrainian Jewish family. After graduating from the seven-year secondary school, he worked as a foreman on building Baikal Amur Mainline. In 1933 Shklovsky entered the Physico-Mathematical Faculty of the Moscow State University. \n\nThere he studied until 1938, when he took a Postgraduate Course at the Astrophysics Department of the Sternberg State Astronomical Institute and remained working in the Institute until the end of his life. He died in Moscow, aged 68.\n\nHe specialized in theoretical astrophysics and radio astronomy, as well as the Sun's corona, supernovae, and cosmic rays and their origins. He showed, in 1946, that the radio-wave radiation from the Sun emanates from the ionized layers of its corona, and he developed a mathematical method for discriminating between thermal and nonthermal radio waves in the Milky Way. He is noted especially for his suggestion that the radiation from the Crab Nebula is due to synchrotron radiation, in which unusually energetic electrons twist through magnetic fields at speeds close to that of light. Shklovsky proposed that cosmic rays from supernova explosions within 300 light years of the sun could have been responsible for some of the mass extinctions of life on earth.\n\nIn 1959 Shklovsky examined the orbital motion of Mars's inner satellite Phobos. He concluded that its orbit was decaying, and noted that if this decay was attributed to friction with the Martian atmosphere, then the satellite must have an exceptionally low density. In this context he voiced a suggestion that Phobos might be hollow, and possibly of artificial origin. This interpretation has since been refuted by more detailed study, but the apparent suggestion of extraterrestrial involvement caught the public imagination, though there is some disagreement as to how seriously Shklovsky intended the idea to be taken. However, Shklovsky and Carl Sagan argued for serious consideration of \"paleocontact\" with extraterrestrials in the early historical era, and for examination of myths and religious lore for evidence of such contact. \n\nHis 1962 book, Вселенная, жизнь, разум (\"Universe, Life, Intelligence\"), was expanded upon and reissued in 1966 with American astronomer Carl Sagan as co-author under the title \"Intelligent Life in the Universe\" (1966). This was the first comprehensive discussion of this field. Discussing the biological as well as astronomical issues of the subject, its unique format, alternating paragraphs written by Shklovsky and Sagan, demonstrated the deep mutual regard between the two and allowed them to express their views without compromise. \n\nIn 1967, before the discovery of pulsars, Shklovsky examined the X-ray and optical observations of Scorpius X-1 and correctly concluded that the radiation comes from an accreting neutron star.\n\nIn the September 1965 issue of Soviet Life, he made the following statement regarding prospects for the future of humanity:\n\nHis memoir, \"Five Billion Vodka Bottles to the Moon: Tales of a Soviet Scientist,\" was published posthumously in 1991 by W.W. Norton & Co.\n\nIn “Five Billion Vodka Bottles to the Moon,” Shklovsky recalled his visit to Philip Morrison, who in 1959 had co-authored with Cornell University colleague Giuseppe Cocconi the paper in Nature magazine which marks the beginning of the modern search for extraterrestrial life, and their discussion of such issues. Bitter over Soviet anti-semitism, of the five pioneer investigators of the field, Cocconi, Morrison, Cornell University's Frank Drake (of the 1961 Project Ozma and the Drake equation), Sagan, and Shklovsky, Shklovsky was quite aware of sharing his Jewish identity with Sagan. Indeed, Sagan's Jewish heritage was also Ukrainian Jewish.\n\nHe was known for his sharp wit and extreme likability. Colleagues in the astronomy department at the University of California, Berkeley, remember fondly his visit there in the 1970s. Well known for his \"Intelligent Life in the Universe\", he was asked by a graduate student if UFO sightings are as common in the Soviet Union as in the United States. \"No,\" he replied. \"In this area the Americans are far more advanced than us.\"\n\n"}
{"id": "20845472", "url": "https://en.wikipedia.org/wiki?curid=20845472", "title": "Jerzy Jurka", "text": "Jerzy Jurka\n\nJerzy Władysław Jurka (June 4, 1950 – July 19, 2014) was a Polish-American computational and molecular biologist. He served as the assistant director of research at the Linus Pauling Institute prior to founding the Genetic Information Research Institute. He collaborated with several notable scientists including George Irving Bell, Roy Britten, Temple Smith, and Emile Zuckerkandl. His Erdős number is 3, using the path through Temple Smith and Stanislaw Ulam.\n\nDr. Jurka is best known for his work on eukaryotic transposable elements (TEs), including the discovery of the major families of Alu elements. He also proposed the mechanism of Alu proliferation and discovered their paternal transmission. The majority of known types of class II TEs, or DNA transposons, were discovered or co-discovered by his team at the Genetic Information Research Institute, based on DNA sequence analysis. The first one, reported in 2001 with Vladimir Kapitonov, became known as \"Helitron\", which is playing a major role in genomic evolution.\nIn 2006 they reported a study of a new, self-synthesizing transposable element called Polinton or Maverick, which is present many diverse eukaryotes. More recently, Jurka and his co-workers presented a hypothesis that links the origin of repetitive families (TE families), to population subdivision and speciation based on classical concepts of population genetics.\n\nJerzy Jurka is the founder of Repbase , which he developed since 1990 with his team and other contributors. Repbase is the primary reference database of TEs used in DNA annotation and analysis.\n"}
{"id": "26482577", "url": "https://en.wikipedia.org/wiki?curid=26482577", "title": "Juggi", "text": "Juggi\n\nThe Juggis, or Jughis, are members of an ethnic group of itinerant travelers found in north-central Afghanistan. They are looked down on by other local groups, and considered \"as blots on the ethnic landscape.\" Many Juggis can be found living in makeshift camps and tents in the outskirts of Mazar-i-Sharif. Most Juggis claim to descend from travelers from Uzbekistan.\n\nEdward Balfour noted a Muslim group known as \"Jughi\" in Bukhara, that he described in 1885 as gypsy-like, in which \"women go unveiled, and the men are careless in their religious duties.\" The group was known for practicing medicine, fortune-telling and horse-trading, and wandered between Bukhara, Samarkand, and Karakul.\n"}
{"id": "2180004", "url": "https://en.wikipedia.org/wiki?curid=2180004", "title": "Karl Friedrich Mohr", "text": "Karl Friedrich Mohr\n\nKarl Friedrich Mohr (November 4, 1806 – September 28, 1879) was a German chemist famous for his early statement of the principle of the conservation of energy. Ammonium iron(II) sulfate, (NH)Fe(SO).6HO, is named Mohr's salt after him.\n\nMohr was born in 1806 into the family of a prosperous druggist in Koblenz. The young Mohr received much of his early education at home, a great part of it in his father's laboratory. This experience may be responsible for much of the skill Mohr later showed in devising instruments and methods of chemical analysis. At the age of twenty-one he began to study chemistry under Leopold Gmelin, and, after five years in Heidelberg, Berlin and Bonn, he returned with the degree of PhD to join his father's establishment.\n\nMohr's father died during 1840 at which time Mohr assumed control of the family business. He retired from it for a life of scientific leisure in 1857, but at the age of fifty-seven some serious financial losses caused him to become a privatdozent in Bonn. In 1867 he was appointed, by the direct influence of the government, extraordinary professor of pharmacy.\n\nMohr was the leading scientific chemist of his time in Germany, and the inventor of many improvements in analytical methodology. He invented an improved burette which had a tip at the bottom and a clamp (a 'Mohr's clip'), which made it much easier to use than its predecessors, which were more similar to a graduated cylinder. His methods of volumetric analysis were expounded in his \"Lehrbuch der chemisch-analytischen Titrir-methode\" (1855) (Instructional Book of Titration Methods in Analytical Chemistry), which won special commendation from Liebig and ran to many editions. His \"Geschichte der Erde, eine Geologie auf neuer Grundlage\" (1866) (History of the Earth, a Geology on a New Basis), was also widely circulated.\n\nIn a paper \"Über die Natur der Wärme\" (1837), Mohr gave one of the earliest general statements of the doctrine of the conservation of energy:\n\n\n"}
{"id": "1642474", "url": "https://en.wikipedia.org/wiki?curid=1642474", "title": "Korean folklore", "text": "Korean folklore\n\nKorean folklore is well established, going back several thousand years. The folklore's basis derives from a variety of belief systems, including Shamanism, Confucianism, Buddhism and more recently Christianity. Mythical creatures often abound in the tales, including the Korean conception of goblins.\n\nThere are many types of folklore in Korean culture, including Imuldan (이물담), focused on supernatural beings such as monsters, goblins and ghosts. The most common of which are the Dokkaebi (도깨비), meaning goblin. However, this term differs from the European concept of 'goblin' in that they do not possess a evil or demonic characteristic. Instead, they are creatures with powers that seek to both bring delight to people and misery. These beings engage either in friendly or annoying behavior with humans. Their interactions with humans represent the belief in the supernatural and their interactions with humanity. The presence of these beings is meant to represent both difficulties and pleasures in life.\n\nA revival on internet sites occurred recently, providing inspiration for artists and illustrators.\n\nRecent achievements in keeping Korean folklore alive include the 150-part animated TV series, \"Animentary Korean Folklore\", telling old tales with a traditional 2-D Korean styled animation.\n\n\n"}
{"id": "49313951", "url": "https://en.wikipedia.org/wiki?curid=49313951", "title": "Kosmos. Problemy Nauk Biologicznych", "text": "Kosmos. Problemy Nauk Biologicznych\n\nKosmos. Problemy Nauk Biologicznych (eng. \"Cosmos. Problems of Biological Sciences\") is the scientific journal of the Polish Copernicus Society of Naturalists published from 1876 initially in Lviv, then in Warsaw. Current numbers are available in the online edition.\n\n"}
{"id": "588606", "url": "https://en.wikipedia.org/wiki?curid=588606", "title": "Law of demand", "text": "Law of demand\n\nIn microeconomics, the law of demand states that, \"conditional on all else being equal, as the price of a good increases (↑), quantity demanded decreases (↓); conversely, as the price of a good decreases (↓), quantity demanded increases (↑)\". In other words, the law of demand describes an inverse relationship between price and quantity demanded of a good. Alternatively, other things being constant, quantity demanded of a commodity is inversely related to the price of the commodity. For example, a consumer may demand 2 kilograms of apples at Rs 70 per kg; he may, however, demand 1kg if the price rises to Rs 80 per kg. This has been the general human behaviour on relationship between the price of the commodity and the quantity demanded. The factors held constant refer to other determinants of demand, such as the prices of other goods and the consumer's income. There are, however, some possible exceptions to the law of demand, such as Giffen goods and Veblen goods.\n\nMathematically, the inverse relationship described by the law of demand may be expressed as:\n\nwhere formula_2 is the quantity demanded of good \"formula_3\", formula_4 is the price of the good, formula_5 is the demand function, formula_6 is the partial derivative of the demand function with respect to formula_4, and formula_8 is the list of other parameters held constant. \n\nThe above equation, when plotted with quantity demanded (formula_2) on the \"formula_3\"-axis and price (formula_4) on the formula_12-axis, gives the demand curve, which is also known as the demand schedule. The downward sloping nature of a typical demand curve illustrates the inverse relationship between quantity demanded and price. Therefore, a downward sloping demand curve embeds the law of demand.\n\nNote that \"demand\" and \"quantity demanded\" are used to mean different things in economic jargon. On the one hand, \"demand\" refers to the entire demand curve, which is the relationship between quantity demanded and price. Changes in demand are due to changes in other determinants (formula_8), such as the income of consumers. Therefore, \"change in demand\" is used to mean that the relationship between quantity demanded and price has changed. Alfred Marshall worded this as: When then we say that a person's demand for anything increases, we mean that he will buy more of it than he would before at the same price, and that he will buy as much of it as before at a higher price.Changes in demand is depicted graphically by a shift in the demand curve. On the other hand, \"quantity demanded\" refers to the quantity of goods consumers want for a given price, conditional on the other determinants. \"Changes in quantity demanded\" is depicted graphically by a movement along the demand curve.\n\nThe law of demand was documented as early as 1892 by economist Alfred Marshall. Due to the law's general agreement with observation, economists have come to accept the validity of the law under most situations. Furthermore, researchers found that the success of the law of demand extends to animals such as rats, under laboratory settings.\n\nGenerally the amount demanded of a good increases with a decrease in price of the good and vice versa. In some cases, however, this may not be true. There are certain goods which do not follow this law. These include [[Veblen goods] [Giffen goods] and expectations of future price changes. Further exception and details are given in the sections below.\n\nInitially proposed by [Sir Robert Giffen], economists disagree on the existence of [[Giffen good]]s in the market. A Giffen good describes an inferior good that as the price increases, demand for the product increases. As an example, during the [[Great Famine (Ireland)|Irish Potato Famine]] of the 19th century, potatoes were considered a Giffen good. Potatoes were the largest staple in the Irish diet, so as the price rose it had a large impact on income. People responded by cutting out on [[luxury good]]s such as meat and vegetables, and instead bought more potatoes. Therefore, as the price of potatoes increased, so did the quantity demanded.\n\nIf an increase in the price of a commodity causes households to expect the price of a commodity to increase further, they may start purchasing a greater amount of the commodity even at the presently increased price. Similarly, if the household expects the price of the commodity to decrease, it may postpone its purchases. Thus, some argue that the law of demand is violated in such cases. In this case, the demand curve does not slope down from left to right; instead it presents a backward slope from the top right to down left. This curve is known as an exceptional demand curve.prestigious goods also fail law of demand\n\nThe goods which people need no matter how high the price is are basic or necessary goods. Medicines covered by insurance are a good example. An increase or decrease in the price of such a good does not affect its quantity demanded. These goods have a perfectly inelastic relationship, in that any change in price does not change the quantity demanded.\n\n\n[[Category:Economics laws]]\n[[Category:Demand]]"}
{"id": "16136171", "url": "https://en.wikipedia.org/wiki?curid=16136171", "title": "List of Ecma standards", "text": "List of Ecma standards\n\nThis is a list of standards published by Ecma International, formerly the European Computer Manufacturers Association.\n\n\n\n\n\n\n"}
{"id": "5847536", "url": "https://en.wikipedia.org/wiki?curid=5847536", "title": "List of astronomical instrument makers", "text": "List of astronomical instrument makers\n\nThe following is a list of astronomical instrument makers, along with lifespan and country of work, if available.\n\n"}
{"id": "61791", "url": "https://en.wikipedia.org/wiki?curid=61791", "title": "List of diseases (U)", "text": "List of diseases (U)\n\nThis is a list of diseases starting with the letter \"U\".\n\n\n\n"}
{"id": "36905374", "url": "https://en.wikipedia.org/wiki?curid=36905374", "title": "List of histologic stains that aid in diagnosis of cutaneous conditions", "text": "List of histologic stains that aid in diagnosis of cutaneous conditions\n\nA number of histologic stains are used in the field of dermatology that aid in the diagnosis of conditions of or affecting the human integumentary system.\n\n\n"}
{"id": "33445772", "url": "https://en.wikipedia.org/wiki?curid=33445772", "title": "List of largest craters in the Solar System", "text": "List of largest craters in the Solar System\n\nFollowing are the largest impact craters on various worlds of the Solar System. For a full list, \"see List of craters in the Solar System\".\n\n"}
{"id": "14485783", "url": "https://en.wikipedia.org/wiki?curid=14485783", "title": "List of members of the National Academy of Sciences (Genetics)", "text": "List of members of the National Academy of Sciences (Genetics)\n"}
{"id": "19198243", "url": "https://en.wikipedia.org/wiki?curid=19198243", "title": "Megawatts and Megatons", "text": "Megawatts and Megatons\n\nMegawatts and Megatons is a 2001 book by Richard L. Garwin and Georges Charpak. The book is said to be a good primer on nuclear power and also a detailed discussion of nuclear weapons and potential paths for weapons reduction.\nThe book presents detailed information about nuclear reactors and provides useful information on nuclear power program development in the United States and France. A discussion on nuclear weapons and non-proliferation follows.\n\n"}
{"id": "19020", "url": "https://en.wikipedia.org/wiki?curid=19020", "title": "Morphology", "text": "Morphology\n\nMorphology, from the Greek and meaning \"study of shape\", may refer to:\n\n\n\n"}
{"id": "16607288", "url": "https://en.wikipedia.org/wiki?curid=16607288", "title": "Operation Anvil (nuclear test)", "text": "Operation Anvil (nuclear test)\n\nOperation Anvil was a series of 21 nuclear tests conducted by the United States in 1975-1976 at the Nevada Test Site. These tests followed the \"Operation Bedrock\" series and preceded the \"Operation Fulcrum\" series.\n"}
{"id": "439569", "url": "https://en.wikipedia.org/wiki?curid=439569", "title": "Oscar Buneman", "text": "Oscar Buneman\n\nOscar Buneman (28 September 1913 – 24 January 1993) made advances in science, engineering, and mathematics. Buneman was a pioneer of computational plasma physics and plasma simulation.\n\nIn 1940 upon completion of his PhD with Douglas Hartree, Buneman joined Hartree's magnetron research group assisting the development of radar during World War II. They discovered the Buneman–Hartree criterion for the voltage threshold of a magnetron operation. After the war, Buneman developed theories and simulations of collision-less dissipation of currents called the Buneman instability. This is an example of anomalous resistivity or absorption. It is \"anomalous\" because the phenomenon does not depend on collisions. Buneman advanced elliptic equation solver methods and their associated applications (as well as for the fast Fourier transforms).\n\nOn January 24, 1993, Oscar Buneman at the age of 79 died near Stanford University. The computer scientist Peter Buneman is his son.\n\n\n"}
{"id": "242710", "url": "https://en.wikipedia.org/wiki?curid=242710", "title": "Outline of academic disciplines", "text": "Outline of academic disciplines\n\nAn academic discipline or field of study is a branch of knowledge, taught and researched as part of higher education. A scholar's discipline is commonly defined by the university faculties and learned societies to which he or she belongs and the academic journals in which he or she publishes research.\n\nDisciplines vary between well-established ones that exist in almost all universities and have well-defined rosters of journals and conferences and nascent ones supported by only a few universities and publications. A discipline may have branches, and these are often called sub-disciplines.\n\nThere is no consensus on how some academic disciplines should be classified, for example whether anthropology and linguistics are disciplines of the social sciences or of the humanities.\n\nThe following outline is provided as an overview of and topical guide to academic disciplines.\n\n\n\n\n\n\n\n\"Also a branch of electrical engineering\"\n\nPure mathematics\n\nApplied mathematics\n\n\nChemical Engineering\n\nCivil Engineering\n\nEducational Technology\n\nElectrical Engineering\n\nMaterials Science and Engineering\n\nMechanical Engineering\n\nSystems science\n\n\n\n"}
{"id": "4062934", "url": "https://en.wikipedia.org/wiki?curid=4062934", "title": "Phenomics", "text": "Phenomics\n\nPhenomics is an area of biology concerned with the measurement of phenomes (a phenome is the set of physical and biochemical traits belonging to a given organism) as they change in response to genetic mutation and environmental influences. It is used in functional genomics, pharmaceutical research, metabolic engineering and increasingly in phylogenetics.\n\nAn important field of research today is trying to improve, both qualitatively and quantitatively, the capacity to measure phenomes. This includes developing high-throughput measurement systems.\n\nFor example, the Australian Plant Phenomics Facility, an initiative of the Australian government, has developed a number of new instruments for comprehensive and fast measurements of phenotypes in both the lab and the field.\n\n\n"}
{"id": "10226222", "url": "https://en.wikipedia.org/wiki?curid=10226222", "title": "Royal Greenhouses of Laeken", "text": "Royal Greenhouses of Laeken\n\nThe Royal Greenhouses of Laeken (, ) are a vast complex of monumental heated greenhouses in the park of the Royal Castle of Laeken in the north of Brussels. The historic complex contains tropical, sub tropical and cold greenhouses. These greenhouses are world famous, but not a tourist attraction. The greenhouses are part of the Royal Park, and the royal private gardens and usually not open for visitors.\n\nThe gardens date back to the 18th century, but King Leopold II changed its garden-architecture. A brand new complex was commissioned by King Leopold II and designed by Alphonse Balat. Built between 1874 and 1895, the complex was finished with the completion of the so-called \"Iron Church\", a domed greenhouse, which would originally serve as the royal chapel. The total floor surface of this immense complex is 2.5 hectares (270,000 square feet). 800,000 liters (over 200,000 US gallons) of fuel oil are needed each year to heat the buildings.\nAfter the death of the king, the greenhouses were kept, but the \"Iron Church\" was converted into a private royal bathing house.\n\nFamous is the royal botanic collection, with old plants from Africa and various species of flowers which are cultivated inside the royal greenhouses for use at court.\nThough the current collection has lost many cultivars since the death of King Leopold II, the collection is still famous. In 1909, there were 314 species of camellias in the royal collection, with more than 1000 plants. Today, only 305 remain. The camellias are the world's largest and oldest collection in a greenhouse.\nThe Orange tree collection of Leopold II was renowned with 130 trees aged 200 to 300 years; one even 400 years. In the 1970s, only 45 trees were still alive.\nThe royal complex can only be visited each year during a two-week period in April–May, when most flowers are in full bloom. Other times, the greenhouses are visited by heads of state during official visits.\n\n\n\n"}
{"id": "9348311", "url": "https://en.wikipedia.org/wiki?curid=9348311", "title": "Secondary research", "text": "Secondary research\n\nSecondary research involves the summary, collation and/or synthesis of existing research. Secondary research is contrasted with primary research in that primary research involves the generation of data, whereas secondary research uses primary research sources as a source of data for analysis. A notable marker of primary research is the inclusion of a \"methods\" section, where the authors describe how the data was generated.\n\nCommon examples of secondary research include textbooks, encyclopedias, news articles, review articles, and meta analyses.\n\nWhen conducting secondary research, authors may draw data from published academic papers, government documents, statistical databases, and historical records.\n\nThe term is widely used in health research, legal research and market research. The principal methodology in health secondary research is the systematic review, commonly using meta-analytic statistical techniques, but other methods of synthesis, like realist reviews and meta-narrative reviews, have been developed in recent years. Such secondary research uses the primary research of others typically in the form of research publications and reports.\n\nIn a market research context, secondary research is taken to include the reuse, by a second party, of any data collected by a first party, such as telephone interviews or surveys.\n\n"}
{"id": "33828654", "url": "https://en.wikipedia.org/wiki?curid=33828654", "title": "Spatial intelligence (psychology)", "text": "Spatial intelligence (psychology)\n\nSpatial Intelligence is an area in the theory of multiple intelligences that deals with spatial judgment and the ability to visualize with the mind's eye. It is defined by Howard Gardner as a human computational capacity that provides the ability or mental skill to solve spatial problems of navigation, visualization of objects from different angles and space, faces or scenes recognition, or to notice fine details. Gardner further explains that Spatial Intelligence could be more effective to solve problems in areas related to realistic, thing-oriented, and investigative occupations. This capability is a brain skill that is also found in people with visual impairment. As researched by Gardner, a blind person can recognize shapes in a non-visual way. The spatial reasoning of the blind person allows them to translate tactile sensations into mental calculations of length and visualizations of form.\n\nSpatial intelligence is one of the nine intelligences on Howard Gardner’s Theory of Multiple Intelligences, each of which is composed of a number of separate sub capacities. An intelligence provides the ability to solve problems or create products that are valued in a particular culture. Each intelligence is a neurally based computational system that is activated by internal or external information. Intelligences are always an interaction between biological proclivities and the opportunities for learning that exist in a culture. The application of this theory in the general practice covers a product range from scientific theories to musical compositions to successful political campaigns. Gardner suggested a general correspondence between each capability with an occupational role in the workplace, for examples: for those individuals with linguistic intelligence he pointed journalists, speakers and trainers; scientists, engineers, financiers and accountants on logical-mathematical intelligence; sales people, managers, teachers and counselors on the personal intelligence; athletes, contractors and actors on bodily-kinesthetic intelligence; taxonomists, ecologists and veterinarians on naturalistic intelligence; clergy and philosophers on existential intelligence and designers, architects and taxi drivers, astronauts, airplane pilots and race car drivers and stunt men on spatial intelligence.\n\nIn the article, Early Education for spatial intelligence: Why, What and How, Nora Newcombe and Andrea Frick apply the concept of spatial intelligence to the educational realm. Newcombe and Frick approached the concept in different ways:\n\n\nDaniel Ness, Stephen Farenga, and Salvatore Garofalo argue that along with verbal intelligence and logico-mathematical intelligence, spatial intelligence is one of three cognitive domains on which individuals are assessed at some point in their lives. Unlike verbal and logico-mathematical intelligence, however, spatial intelligence is often not assessed on most standardized tests and secondary-level or tertiary-level entrance examinations. Its lack of inclusion on these assessments is problematic because success on questions based on verbal intelligence and logico-mathematical intelligence may fail to tap populations skilled in spatial relations and orientations. Ness, Farenga, and Garofalo also posit that experiences with certain physical objects allow for greater dividends in spatial intelligence. To this end, objects with greater affordance, such as certain LEGO bricks, may impede spatial intelligence while objects with limited affordance, such as cuboidal blocks (i.e., planks) provide for increased spatial intelligence.\n\nThe architect Leon van Schaik formulates the adoption of spatial intelligence in the field of architecture and design. His first assumption relates to the origin of architecture in the human computational capacity to organize themselves spatially; based on people's own ideas about space, histories in space and communal mental space; all have been a combination that has evolved into society over millennia. Van Schaik explains how spatial intelligence works and how it is linked to the way individuals assess their surroundings. His comments are based on the research done by Roger Penrose, Shadows of the Mind. The awareness of what happens around someone comes from cells in the human body with enormous calculating capabilities, \"intelligence is a distributed system: not something held like a command centre in the brain and then distributed, but something that is present throughout the organism, and linked together through the nervous system\". An example to explain this human capability is similar to the ways spatial intelligence works in kinetic environments. Like the ability in which football players compute and execute the exact angle and force required to score a goal from a free kick. Another example of distributed intelligence at work is in the Australian Aboriginal Art. Aboriginal dot painting is a representation of the landscape inhabited by them with a surprising resemblance to the real space. It shows watercourses, animal shelter, where the edible plant are and all dimensions and spatial arrangement has been learned through a constant exposure to the world surrounding them, by walking, hunting, stalking, spearing.\nVan Schaik argues about the influence of spatial intelligence in the creation of engaging spaces. His assumption is to create a better relation between internal and external environments, and it requires the use of the best available knowledge, which in his opinion, involves the designer’s spatial intelligence and mental space; the new informational environment that enables the professional designer to communicate more interactively and inclusively. In van Schaik point of view, this new process of understanding space will provide the chance to forge new kinds of unity between architecture and the communities it seeks to serve: the commission of spatial intelligence is leading architecture’s domain into a new discipline that venture into new spatial formulations, new roles and new approaches. Van Schaik also pointed that architecture has to be more than the production of branded consumable, it has to be capable to influence the individuals deeper history, a benign and malevolent influence in people’s lives. For van Schaik, some of the most influencing architects using spatial intelligence in combination of their community’s mental space are: Peter Zumthor, Sean Godsell, Herzog and de Meuron, Zaha Hadid and Kathryn Findlay.\n\nNicos Komninos applies the concept of spatial intelligence to cities, and defined the idea as the ability of a community to use its intellectual capital, institutions and material infrastructure to deal with a range of problems and challenges. Spatial intelligence emerges from the agglomeration and integration of three types of intelligence: the inventiveness, creativity and intellectual capital of the city's population; the collective intelligence of the city's institutions and social capital; and the artificial intelligence of public and citywide smart infrastructure, virtual environments and intelligent agents. Using this spatially combined intellectual capacity, cities can respond effectively to changing socio-economic conditions, address challenges, plan their future, and sustain the prosperity and well-being of citizens.\n\nBrian Bethune defines spatial intelligence as the ability to grasp a changing whole and anticipate its next stage; the ability to make quick decisions; to size up all the relationships in a fast-changing array and understand them. A related notion is that of situational awareness: a heightened consciousness of the individual’s surroundings and both the intentions of the people around and their anticipated actions. Bethune claims that the power of spatial intelligence and situational awareness are fully explained in the practice of hockey. Bethune explains that hockey reveals and rewards situational and spatial intelligence like no other sport. Bethune’s example refers to the ability of the hockey player Wayne Gretzky as a gift of spatial and situational intelligence: knowing what is going to happen in three seconds, anticipating the pattern approaching by seeing the pattern instantaneously, sussing out the goalie's next decision and other players' eventual trajectories in what would be a single glance if a glance were even taken. “Gretzky is the extreme expression of the common skill the game demands”.\n\nJane Rendell and Peg Rawes research on Spatial Imagination in Design demonstrates that an individual’s sensory and perceptual engagement with an environment or space is, in part, constructed by their powers of imagination. For Rendell and Rawes spatial imagination works in a specific political and cultural imagination as belonging to the individual designer and user. The results of this contextual understanding will inform and reflect the specific cultural, historical and political diversity and value of the architectural and built environment to the design community and beyond.\n\n\n\n"}
{"id": "14337857", "url": "https://en.wikipedia.org/wiki?curid=14337857", "title": "Urgent computing", "text": "Urgent computing\n\nUrgent computing is prioritized and immediate access on supercomputers and grids for emergency computations such as severe weather prediction during matters of immediate concern. \n\nApplications that provide decision makers with information during critical emergencies cannot waste time waiting in job queues and need access to computational resources as soon as possible.\n"}
{"id": "2014633", "url": "https://en.wikipedia.org/wiki?curid=2014633", "title": "Useless rules", "text": "Useless rules\n\nIn theoretical computer science, in particular in the theory of formal languages, useless rules of a formal grammar are those rules of symbol production that are unreachable or unproductive, that is, that can or need never be applied.\n\nGiven a context-free grammar, a nonterminal symbol \"X\" is called productive, or generating, if there is a derivation \"X\" ⇒ \"w\" for some string \"w\" of terminal symbols. A nonterminal symbol \"X\" is called reachable if there is a derivation \"S\" ⇒ α\"X\"β for some strings α, β of non-terminal and terminal symbols, and where \"S\" denotes the grammar's start symbol.\n\nA rule with an unproductive or unreachable symbol on its left-hand side can be deleted from the grammar without changing the accepted (a.k.a. generated) language.\nLikewise, an alternative containing such a symbol can be deleted from the right-hand side of a rule without changing the language.\nSuch rules and alternatives are called useless.\n\nFor formal grammars that are not context-free, similar definitions apply.\n\nDenoting nonterminal and terminal symbols by upper- and lower-case letters, respectively,\nin the following regular grammar with start symbol \"S\"\nthe nonterminal \"D\" is unreachable, and \"E\" is unproductive.\nHence, omitting the last two rules doesn't change the language accepted by the grammar, nor does omitting the alternative \"| \"Ee\"\" from the right-hand side of the rule for \"S\".\n\nHopcroft, et al. give an algorithm to eliminate useless rules from a context-free grammar.\n\nAiken and Murphy give a fixpoint algorithm to detect which nonterminals of a given regular tree grammar are unproductive.\n"}
