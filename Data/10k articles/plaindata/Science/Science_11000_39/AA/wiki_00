{"id": "22610527", "url": "https://en.wikipedia.org/wiki?curid=22610527", "title": "400 Years of the Telescope", "text": "400 Years of the Telescope\n\n400 Years of the Telescope: A Journey of Science, Technology and Thought is a 2009 American documentary film that was created to coincide with the International Year of Astronomy in 2009. Directed by Kris Koenig, it chronicles the history of the telescope from the time of Galileo and features interviews with leading astrophysicists and cosmologists from around the world, who explain concepts ranging from Galileo's first use of the telescope to view the moons of Jupiter, to the latest discoveries in space, including new ideas about life on other planets and dark energy, a mysterious vacuum energy that is accelerating the expansion of the universe.\n\n\n\nThe film's development team included Donald Goldsmith, a well-known astronomy writer on the Carl Sagan Cosmos team, and Albert Van Helden, a leading authority on the history of the telescope. It was shot on RED Digital Cinema at the world's leading universities and observatories including the European Southern Observatory, Institute for Astronomy, SETI Institute, Space Telescope Science Institute, Anglo-Australian Observatory, and Harvard University. Among the production team's challenges were shooting the Atacama Large Millimeter Array (ALMA) at 5000m on the Atacama Desert. The original score was composed by Mark Slater and recorded by the London Symphony Orchestra at Abbey Road Studios.\n\n\n\n\n\n"}
{"id": "26998311", "url": "https://en.wikipedia.org/wiki?curid=26998311", "title": "BSSN formalism", "text": "BSSN formalism\n\nThe BSSN formalism is a formalism of general relativity that was developed by Thomas W. Baumgarte, Stuart L. Shapiro, Masaru Shibata and Takashi Nakamura between 1987 and 1999. It is a modification of the ADM formalism developed during the 1950s.\n\nThe ADM formalism is a Hamiltonian formalism that does not permit stable and long-term numerical simulations. In the BSSN formalism, the ADM equations are modified by introducing auxiliary variables. The formalism has been tested for a long-term evolution of linear gravitational waves and used for a variety of purposes such as simulating the non-linear evolution of gravitational waves or the evolution and collision of black holes.\n\n"}
{"id": "5741651", "url": "https://en.wikipedia.org/wiki?curid=5741651", "title": "Bacillus phage", "text": "Bacillus phage\n\nA Bacillus phage is a member of a group of bacteriophages known to have bacteria in the genus \"Bacillus\" as host species. These bacteriophages have been found to belong to the families Myoviridae, Siphoviridae, Podoviridae or Tectiviridae.\n\nThe DNA polymerase of Bacillus phage phi29 is a unique and efficient polymerase with proofreading activity.\n\n"}
{"id": "37970127", "url": "https://en.wikipedia.org/wiki?curid=37970127", "title": "Buttered toast phenomenon", "text": "Buttered toast phenomenon\n\nThe buttered toast phenomenon is an observation that buttered toast tends to land butter-side down after it falls. It is used an idiom representing pessimistic outlooks. Various people have attempted to determine whether there is an actual tendency for bread to fall in this fashion, with varying results.\n\nThe phenomenon is said to be an old proverb from \"the north country.\" Written accounts can be traced to the mid-19th century. The phenomenon is often attributed to a parodic poem of James Payn from 1884:\n\nIn the past, this has often been considered just a pessimistic belief. A study by the BBC's television series \"Q.E.D.\" found that when toast is thrown in the air, it lands butter-side down just one-half of the time (as would be predicted by chance). However, several scientific studies have found that when toast is dropped from a table (as opposed to being thrown in the air), it does fall butter-side down. A study by Robert A J Matthews won the Ig Nobel Prize in 1996.\n\nWhen toast falls out of one's hand, it does so at an angle, by nature of it having slipped from its previous position, then the toast rotates. Given that tables are usually between two and six feet (0.7 to 1.83 meters), there is enough time for the toast to rotate about one-half of a turn, and thus lands upside down relative to its original position. Since the original position is usually butter-side up, the toast lands butter-side down. However, if the table is over 10 feet (3 meters) tall, the toast will rotate a full 360 degrees, and land butter-side up. Also, if the toast travels horizontally at over 3.6 miles per hour (1.6 m/s), the toast will not rotate enough to land butter-side down. In fact, the phenomenon is caused by fundamental physical constants.\n\nThe added weight of the butter has no effect on the falling process, since the butter spreads throughout the slice.\n\nThe following findings are from Mythbusters:\n\nIn the 2010 M. Night Shyamalan film \"Devil\", a group of adults in an elevator become trapped by unknown forces. Slowly over the course of the film, people begin to die in the elevator as the power blinks on and off, pinning the people against each other and creating a false narrative that the others are murderers (However, it is common opinion that the name of the film spoiled this false narrative). In the film, supporting cast member and fictional security guard Ramirez, in his suspicion that there may be unholy forces at play, dropped a slice of toast with jelly on one side, and dropped it on the floor. The toast landed on the jelly-side, after which Ramirez coined the phrase \"Jelly Side Down,\" and subsequently 'proved' that the devil was nearby. However, there is some dispute in the film about whether or not this did actually prove the devil was near.\n\nThis phenomenon was also demonstrated and parodied in the October 29, 2013 \"Nostalgia Critic\" review of \"Devil\", (Reuploaded on July 23, 2016 by Channel Awesome on YouTube) in which actor Doug Walker plays a priest who uses multiple food items to determine whether or not the Devil is near. After the other items show no sign of the Devil, the priest tosses a piece of toast as the last test. The toast lands jelly side down which means the Devil is near and the end of the world is imminent. Everybody in the church screams and panics as the priest repeatedly shouts, \"Jelly side down!\"\n\n"}
{"id": "31454437", "url": "https://en.wikipedia.org/wiki?curid=31454437", "title": "Cesare Maria Tapparone-Canefri", "text": "Cesare Maria Tapparone-Canefri\n\nCesare Maria Tapparone-Canefri (1838- 6 August 1891, Quattordio, Italy) was an Italian malacologist.\n\n\n"}
{"id": "45587750", "url": "https://en.wikipedia.org/wiki?curid=45587750", "title": "Charlier (Martian crater)", "text": "Charlier (Martian crater)\n\nCharlier is a crater in the northwest of Mare Australe quadrangle of Mars, located in the south of Terra Sirenum and east of the 180th meridian. The crater is located at 68.56°S latitude and 168.67°W longitude. It is 106.28 km in diameter and was named after Henri A. Perrotin, a French astronomer who studied dark lineations on the planet. Its name was approved in 1973.\n\nNearby prominent craters are Reynolds further south, Richardson almost southwest and Suess to the west. East is Chico Valles, the approximate area marking the boundaries of the terrae (lands) Sirenum and Aonia.\n\nDunes are dominant inside the crater, gullies are not commonly founded as to craters to the north. Most of the northern portion are mare up of layered material made by earlier glaciations.\n\nThe crater was first imaged by Mariner 9 in 1972, more detailed images were made later including the Viking Orbiters.\n\n"}
{"id": "10444089", "url": "https://en.wikipedia.org/wiki?curid=10444089", "title": "Chinese pangolin", "text": "Chinese pangolin\n\nThe Chinese pangolin (\"Manis pentadactyla\") is a pangolin found in northern India, Nepal, Bhutan, Bangladesh, Myanmar, northern Indochina, through most of Taiwan, and southern China (including the islands of Hainan). The Chinese pangolin is one of eight species of pangolins. All eight of these shy and secretive animals are facing moderate to severe population scarcity in recent times. Asian pangolin species, especially the Chinese pangolin and the Sunda pangolin, are the most endangered of all the pangolin species. The IUCN reports that the number of Chinese pangolins has declined greatly over the past 15 years. Despite being listed as critically endangered by the IUCN and being protected by CITES, poaching continues to be the main cause of their decline in numbers. Deforestation has also contributed to their depletion.\n\nThe Chinese pangolin has the appearance of a scaly anteater. Its head and body measure about 40–58 cm and its tail measures about . A mature Chinese pangolin weighs from . It has 18 rows of overlapping scales accompanied by hair, a rare combination in mammals. It has a small, narrow mouth and a little, pointed head. Also its claws grow in as it grows older. The female gives birth to a single offspring at a time.\n\nA newborn pangolin weighs about , its length is about . The Chinese pangolin reproduces in April and May when the weather warms. The young also have scales; however, they remain very soft for at least two days, then harden. Although the young pangolin can walk on its first day, the mother carries it on her back or tail. If the mother feels threatened, she immediately folds her baby onto her belly with the help of her tail. Male pangolins have been observed allowing the female and baby to share his burrow.\nChinese pangolins are rather secretive, nocturnal creatures. They move very slowly and are known for their nonaggressive behavior. Their hard scales work as a protective cover from predators, and when they feel threatened, they curl themselves into balls. For further defense, they can climb trees, although this is uncommon.\n\nThey mainly eat insects, particularly termites and ants. They dig into ant nests and termite mounds with their large fore claws and extract their prey with their long, sticky tongues.\n\nIn Vietnam and Hong Kong, Chinese pangolins are considered a delicacy. They are hunted on a wide scale for human consumption. Factors such as habitat destruction and hunting constantly challenge their survival. Chinese pangolins are now on the IUCN Red List of Threatened Species, however, since the forests they inhabit are difficult to patrol, preventing people from hunting these animals is difficult.\n\nThe Chinese pangolin digs long burrows in the ground, which they use to sleep and hunt termites.\n\nHabitats include:\n\nA study done by the \"Chinese Journal of Applied and Environmental Biology\" identified the Chinese pangolin as a \"susceptible species due to its food specialization and stenophagy (only eating several species of ants and termites)”. Due to the pangolin's very specific diet, it can become arduous to provide the appropriate food for them while they are being observed and maintained. Pangolins are typically held in zoos due to their abilities to feed and preserve the rare animals. However, since the 1970s, \"pangolins are now almost unknown to visitors and are exhibited infrequently in zoos\", and have \"historically been difficult to maintain, with most captive animals dying within a short period after capture\". When in their natural habitat, this species lives \"on a diet of ants, termites, and various other invertebrates including bee larvae, flies, worms, earthworms, and crickets”. After carefully creating new, more sustainable recipes in zoos, some of the ingredients used have included \"egg, meat (ground beef, horse, canned feline diet), evaporated milk products, milk powder, fish protein, orchid leaves, commercial chows, psyllium seed, carrots, yeast, multivitamins, and insects (mixtures of silkworm larvae, earth, ants, termites, meal worms, or crickets)\". A number of zoos that have kept pangolins under observation have found that the animals died most commonly after a few years, without breeding successfully. Researchers claim this outcome is correlated to the \"poor acceptance of captive diets and digestive problems.\" The Chinese Pangolin is considered to be high risk in terms of extinction.\n\nThe Convention on International Trade in Endangered Species in 2002 prohibited selling pangolins across national borders. Although China has already passed laws to protect the pangolin, it might not be enough to save the species. The Convention on International Trade in Endangered Species of Wild Fauna and Flora reports that pangolins are the most trafficked and poached mammal. The Chinese pangolin is hunted for its meat, claws, and scales. Pangolin meat, which is considered a delicacy in parts of China and Vietnam, has been reported to sell for as high as US$200/kg. Pangolin scales and blood are in demand in Asia for their supposed medicinal qualities. Chinese pangolin scales are sold to treat a wide variety of ailments, from cancer to upset stomach to asthma. Other pangolin body parts are also used in traditional Chinese medicine. According to one survey composed in 2013, certain Nepalese natives believe pangolin scales are also good-luck charms. Each pangolin has about of scales which can be sold for roughly US$350 on the black market.\nThough pangolins have been protected by legislation since the 1970s and 1980s, people still choose to hunt these endangered animals. After random inspections on May 28, 2014, at the Kwai Chung cargo port in Hong Kong, officials detained scales from nearly 8,000 pangolins. Just two weeks later, Hong Kong officials seized a second shipment that contained scales from about 5,000 pangolins.\n\nThe journal \"Frontiers in Ecology & the Environment\" estimated that the remains of about 10,000 pangolins are intercepted each year. Zhao-Min Zhou and Macdonald from \"Frontiers in Ecology & the Environment\" claim from their records that 220 living pangolins and the remains of 4,909 dead pangolins were seized in 43 law enforcement actions since 2010.\n\nThe Indonesian Forestry Ministry director of investigations and forest observation, Raffles Panjaitan, told the \"Jakarta Post\" that in October 2011, his agency had 587 cases of pangolin trafficking since 2006. The estimated value is US$4.3 million worth of pangolins on the illegal market.\n\nIn April 2013, a Philippine coast guard inspected a boat where they found of pangolin meat. They also discovered 400 boxes containing thousands of frozen skinned pangolins and scaly anteaters from Indonesia. The Regional Trial Court in Puerto Princesa city in Palawan province sentenced the boat captain to 12 years in prison and each crew member received from six to 10 years. Each member of the crew was also fined $100,000.\n\nIn 2016, CITES moved the Chinese pangolin from its Appendix II, designating species not directly threatened with extinction but in need of protection to prevent exploitation, to Appendix I, reserved for species most directly threatened with extinction. The Appendix I listing prohibits commercial trade in wild-caught specimens. Many of the countries where the Chinese pangolin resides have already passed legislation to protect them. Below is a list of the different countries' legislation in order from oldest to newest laws:\n\nChina has passed much more legislation for pangolin protection than other countries, because the species' population has drastically declined in China over the last few decades. This is the direct result of extreme poaching for pangolin scales and meat.\n\nOn May 2016, a poll representing 1,892 Chinese adults revealed that more than 80% of the Chinese public want pangolins to be protected. The survey was led by Aita Foundation and Humane Society International.\n\nThe IUCN SSC Pangolin Specialist Group made a proposal in July 2014 to increase awareness, funding, and research for pangolin conservation. Some of the plan's highlights include making protocols to monitor pangolin populations, establishing a consumption index of pangolin products, using DNA analysis to determine variation between and within species, and identifying species strongholds to determine best allocation of resources. Furthermore, the conservation plan aims to increase patrol-based monitoring around stronghold populations, increase awareness and education about the severity of the problem, and, most importantly, implement a demand reduction strategy for pangolin meat and scales.\n\nAnother alternative to legislation, proposed by IUCN, includes offering positive incentives, like monetary payments or control over land's resources, to local communities for their involvement in conservation efforts. Other researchers have proposed the importance of finding biological substitutes for endangered species used in traditional medicines. DNA barcoding and analysis could be used to determine what common species are genetically similar enough and produce similar effects as the Chinese pangolin scales. To crack down on poaching, the barcoding technique could also be used for accurate detection of species products being imported and exported.\n\nThe China Biodiversity Conservation and Green Development Foundation (CBCGDF), a non-profit and non-governmental organization is particularly proactive in doing research and collecting field data for the conservation of Chinese pangolins. Since July 2017, the Beijing-based NGO has established three Chinese pangolin Community Conservation Areas in Hunan, Guangxi and Jiangxi provinces.\n\nIn late August 2017, CBCGDF confiscated 33 Chinese pangolins to Chinese poachers and released them to the Guangxi Wildlife Rescue Centre. Suggestions from the foundation such as the improvement of their rescue regulations and procedures; their immediate release to the wild after full recovery; keeping information open to the public; revising the endangered species conservation laws and regulations in Guangxi, followed the release. However, only 14 Chinese pangolins survived.\nAt the World Pangolin Day 2017, CBCGDF called for deletion of pangolin scales from the Traditional Chinese Medicine Book.\nCBCGDF also called for the burning of confiscated pangolin scales illegally trafficked to China.\nMoreover in July 2018, a conference organised by CBCGDF hosted the Global Biodiversity Information Facility (GBIF). During that conference, CBCGDF openly shared recent and precious data on Chinese pangolins with GBIF.\n\nThe Chinese pangolin is probably \"The Critter\", one of the pets of the Raven FACs at their secret base in Long Tieng during the covert war in Laos. It was described as a foot-long \"prehistoric\" beast, covered in armor plating with a long tail and a pointed nose, a \"cross between a sloth and an armadillo\", by the US pilots.\n\nAfter its accidental death, the Critter's body was preserved in a one-gallon jar filled with alcohol. A picture taken of the preserved animal was sent to the Smithsonian Institution in Washington, DC, as well as to the natural history department of La Sorbonne in Paris, but no positive reply was forthcoming. For a long time, nobody knew what kind of animal it was until one of the pilots stationed in Laos happened to see the animal on a Laotian postage stamp, part of a stamp series on indigenous animals from Laos, under the name \"Panis Auritas\".\n\n\n"}
{"id": "320498", "url": "https://en.wikipedia.org/wiki?curid=320498", "title": "Computer-mediated communication", "text": "Computer-mediated communication\n\nComputer-mediated communication (CMC) is defined as any human communication that occurs through the use of two or more electronic devices. While the term has traditionally referred to those communications that occur via computer-mediated formats (e.g., instant messaging, email, chat rooms, online forums, social network services), it has also been applied to other forms of text-based interaction such as text messaging. Research on CMC focuses largely on the social effects of different computer-supported communication technologies. Many recent studies involve Internet-based social networking supported by social software.\n\nComputer-mediated communication can be broken down into two forms: synchronous and asynchronous. Synchronous computer-mediated communication refers to communication which occurs in real time. All parties are engaged in the communication simultaneously; however, they are not necessarily all in the same location. Examples of synchronous communication are video chats and FaceTime audio calls. On the contrary, asynchronous computer-mediated communication refers to communication which takes place when the parties engaged are not communicating in unison. In other words, the sender does not receive an immediate response from the receiver. Most forms of computer mediated technology are asynchronous. Examples of asynchronous communication are text messages and emails.\n\nScholars from a variety of fields study phenomena that can be described under the umbrella term of computer mediated communication (CMC) (see also Internet studies). For example, many take a sociopsychological approach to CMC by examining how humans use \"computers\" (or digital media) to manage interpersonal interaction, form impressions and form and maintain relationships. These studies have often focused on the differences between online and offline interactions, though contemporary research is moving towards the view that CMC should be studied as embedded in everyday life . Another branch of CMC research examines the use of paralinguistic features such as emoticons, pragmatic rules such as turn-taking and the sequential analysis and organization of talk, and the various sociolects, styles, registers or sets of terminology specific to these environments (see Leet). The study of language in these contexts is typically based on text-based forms of CMC, and is sometimes referred to as \"computer-mediated discourse analysis\".\n\nThe way humans communicate in professional, social, and educational settings varies widely, depending upon not only the environment but also the method of communication in which the communication occurs, which in this case is through computers or other information and communication technologies (ICTs). The study of communication to achieve collaboration—common work products—is termed computer-supported collaboration and includes only some of the concerns of other forms of CMC research.\n\nPopular forms of CMC include e-mail, video, audio or text chat (text conferencing including \"instant messaging\"), bulletin board systems, list-servs and MMOs. These settings are changing rapidly with the development of new technologies. Weblogs (blogs) have also become popular, and the exchange of RSS data has better enabled users to each \"become their own publisher\".\n\nCommunication occurring within a computer-mediated format has an effect on many different aspects of an interaction. Some of those that have received attention in the scholarly literature include impression formation, deception, group dynamics, disclosure reciprocity, disinhibition and especially relationship formation.\n\nCMC is examined and compared to other communication media through a number of aspects thought to be universal to all forms of communication, including (but not limited to) synchronicity, persistence or \"recordability\", and anonymity. The association of these aspects with different forms of communication varies widely. For example, instant messaging is intrinsically synchronous but not persistent, since one loses all the content when one closes the dialog box unless one has a message log set up or has manually copy-pasted the conversation. E-mail and message boards, on the other hand, are low in synchronicity since response time varies, but high in persistence since messages sent and received are saved. Properties that separate CMC from other media also include transience, its multimodal nature, and its relative lack of governing codes of conduct. CMC is able to overcome physical and social limitations of other forms of communication and therefore allow the interaction of people who are not physically sharing the same space.\n\nThe medium in which people choose to communicate influences the extent to which people disclose personal information. CMC is marked by higher levels of self-disclosure in conversation as opposed to face-to-face interactions. Self disclosure is any verbal communication of personally relevant information, thought, and feeling which establishes and maintains interpersonal relationships. This is due in part to visual anonymity and the absence of nonverbal cues which reduce concern for losing positive face. According to Walther’s (1996) hyperpersonal communication model, computer-mediated communication is valuable in providing a better communication and better first impressions. Moreover, Ramirez and Zhang (2007) indicate that computer-mediated communication allows more closeness and attraction between two individuals than a face-to-face communication. Online impression management, self-disclosure, attentiveness, expressivity, composure and other skills contribute to competence in computer mediated communication. In fact, there is a considerable correspondence of skills in computer-mediated and face-to-face interaction even though there is great diversity of online communication tools.\n\nAnonymity and in part privacy and security depends more on the context and particular program being used or web page being visited. However, most researchers in the field acknowledge the importance of considering the psychological and social implications of these factors alongside the technical \"limitations\".\n\nCMC is widely discussed in language learning because CMC provides opportunities for language learners to practice their language. For example, Warschauer conducted several case studies on using email or discussion boards in different language classes. Warschauer claimed that information and communications technology “bridge the historic divide between speech...and writing”. Thus, considerable concern has arisen over the reading and writing research in L2 due to the booming of the Internet.\n\nThe nature of CMC means that it is easy for individuals to engage in communication with others regardless of time or location. CMC allows for individuals to collaborate on projects that would otherwise be impossible due to such factors as geography. In addition, CMC can also be useful for allowing individuals who might be intimidated due to factors like character or disabilities to participate in communication. By allowing an individual to communicate in a location of their choosing, CMC call allow a person to engage in communication with minimal stress. Making an individual comfortable through CMC also plays a role in self-disclosure, which allows a communicative partner to open up more easily and be more expressive. When communicating through an electronic medium, individuals are less likely to engage in stereotyping and are less self-conscious about physical characteristics. The role that anonymity plays in online communication can also encourage some users to be less defensive and form relationships with others more rapidly.\n\nWhile computer-mediated communication can be beneficial, technological mediation can also inhibit the communication process. Unlike face-to-face communication, nonverbal cues such as tone and physical gestures, which assist in conveying the message, are lost through computer-mediated communication. As a result, the message being communicated is more vulnerable to being misunderstood due to a wrong interpretation of tone or word meaning. Moreover, according to Dr. Sobel-Lojeski of Stony Brook University and Professor Westwell of Flinders University, the virtual distance that is fundamental to computer-mediated communication can create a psychological and emotional sense of detachment, which can contribute to sentiments of societal isolation.\n\n\n"}
{"id": "1674384", "url": "https://en.wikipedia.org/wiki?curid=1674384", "title": "Culture of Northern Ireland", "text": "Culture of Northern Ireland\n\nThe Culture of Northern Ireland relates to the traditions of Northern Ireland. Elements of the Culture of Ulster and the Culture of the United Kingdom are to be found.\n\nSince 1998, the Ulster Museum, Armagh Museum, Ulster Folk and Transport Museum and the Ulster American Folk Park have been administered by the National Museums and Galleries of Northern Ireland.\n\nThe Linen Hall Library, the oldest library in Belfast, has endured many changes of fortune since its foundation in 1788, but has maintained a vision of providing access to literature and local studies to the population at large.\n\n\nNorthern Ireland's best known chefs include Paul Rankin and Michael Deane.\n\nThe best known traditional dish in Northern Ireland is the Ulster fry.\nTwo other popular meals are fish and chips or 'Bangers and Mash' (Sausages and Creamed Potatoes)\n\nA unique speciality to Northern Ireland is Yellowman. Yellowman is a chewy toffee-textured honeycomb and is sold in non-standard blocks and chips and is associated with the Ould Lammas Fair in Ballycastle, County Antrim, where it is sold along with other confectionery and often dulse.\n\nDulse is commonly used in Ireland, where it can be used to make white soda bread. It can be found in many health food stores or fish markets and can be ordered directly from local distributors. it is also traditionally sold at the Ould Lammas Fair. It is particularly popular along the Causeway Coast. Although a fast-dying tradition, many gather their own dulse. Along the Ulster coastline from County Down to County Donegal in the Republic of Ireland, it is eaten dried and uncooked as a snack.\n\nEnglish is the most spoken language in Northern Ireland. There are also two recognised regional languages in Northern Ireland: the Irish language (\"see Irish language in Northern Ireland\") and the local variety of Scots known as Ulster Scots. Northern Ireland Sign Language and Irish Sign Language have been recognised since 29 March 2004. A third, British Sign Language is also used.\n\nAt the 2001 census, Chinese was the most widely spoken minority language in Northern Ireland, with Shelta, Arabic and Portuguese also spoken by a significant number of people. Since the census, however, an influx of people from recent EU accession states is likely to have significantly increased numbers of speakers of languages from these countries. Detailed figures on these changes are not yet available.\n\nSome team sports are played on an All-Ireland basis, while in others Northern Ireland fields its own team.\n\nInternationally well-known sports people include:\nDespite its small geographical size, Northern Ireland prolifically produces internationally renowned writers and poets from a wide variety of disciplines. Irish language literature was the predominant literature in the pre-Plantation period. The Ulster Cycle is pertinent to the history of literature in the territory of present-day Northern Ireland. Ulster Scots literature first followed models from Scotland, with the \"rhyming weavers\", such as James Orr, developing an indigenous tradition of vernacular literature. Writers in the counties which now form Northern Ireland participated in the Gaelic Revival.\n\n\nNoted visual artists from Northern Ireland include:\n\nNoted actors from Northern Ireland include:\n\n\"See also Cinema of Northern Ireland\"\n\nNorthern Ireland Screen, a government agency financed by Invest NI and the European Regional Development Fund, provides financial support to film and television productions in Northern Ireland. Among the works it has supported is the 2011 HBO television series \"Game of Thrones\", which is filmed principally in Belfast's Paint Hall studios and on location elsewhere in Northern Ireland.\n\nBelfast hosts the Belfast Film Festival and the \"CineMagic\" film festival, as well as several independent cinemas including Queen's Film Theatre and Strand Cinema.\n\nNoted musicians from Northern Ireland include:\n\nAugust Craft Month is an annual coordinated programme of events that showcase the work of craft makers in Northern Ireland and from across the United Kingdom, Ireland and Europe. It is organised by Craft Northern Ireland.\n\nA traditional song of the Unionist and Loyalist communities is The Sash, which may be considered offensive or at least distateful by the Nationalist communities, particularly when it is used to threaten or incite violence.\n\nUnionists tend to use the Union Flag and sometimes the Ulster Banner, while nationalists usually use the Flag of Ireland, or sometimes the Flag of Ulster. Both sides also occasionally use the flags of secular and religious organisations they belong to. Some groups, including the Irish Rugby Football Union and the Church of Ireland use the Flag of St. Patrick as a symbol of Ireland which lacks the same nationalist or unionist connotations.\n\nThe flax flower, representing the linen industry, has been used as a neutral symbol – as for the Northern Ireland Assembly.\n\nSt. Patrick's Day is celebrated by both nationalists and unionists, while \"The Twelfth\" is celebrated only by unionists.\nCelebrations to mark the anniversary of the Battle of the Boyne are held every Twelfth of July and draw huge crowds. The Apprentice Boys of Derry also organise commemorative events. The bowler hat is a symbol of Orangeism.\n\n\n"}
{"id": "39311118", "url": "https://en.wikipedia.org/wiki?curid=39311118", "title": "Devendra Lal", "text": "Devendra Lal\n\nDevendra Lal FRS (14 February 1929 – 1 December 2012) was an Indian geophysicist.\n\nHe was born in Varanasi, India. \nHe graduated from Banaras Hindu University.\nHe graduated from Bombay University; his thesis was on cosmic ray physics; his thesis adviser was Bernard Peters.\n\nHe was Director, of the Physical Research Laboratory, Ahmedabad from 1972 to 1983.\n\nHe was Visiting Professor at Scripps Institution of Oceanography, University of California, San Diego, from 1989 to 2012.\nDevendra Lal was President of the International Union of Geodesy and Geophysics (IUGG) from 1983 to 1987.\n\n"}
{"id": "18323478", "url": "https://en.wikipedia.org/wiki?curid=18323478", "title": "ExoMars Trace Gas Orbiter", "text": "ExoMars Trace Gas Orbiter\n\nThe ExoMars Trace Gas Orbiter (TGO or ExoMars Orbiter) is a collaborative project between the European Space Agency (ESA) and Roscosmos that sent an atmospheric research orbiter and the \"Schiaparelli\" demonstration lander to Mars in 2016 as part of the European-led ExoMars programme.\n\nThe Trace Gas Orbiter delivered the \"Schiaparelli\" lander on 16 October 2016, which crashed on the surface. \n\nThe orbiter began aerobraking in March 2017 to lower its initial orbit of . Aerobraking concluded on 20 February 2018 when a final thruster firing resulted in an orbit of . Additional thruster firings every few days raised the orbiter to a circular \"science\" orbit of , which was achieved on 9 April 2018.\n\nA key goal is to gain a better understanding of methane () and other trace gases present in the Martian atmosphere that could be evidence for possible biological activity. The programme will follow with the Surface Science Platform and the ExoMars rover in 2020, which will search for biomolecules and biosignatures; the TGO will operate as the communication link for the 2020 ExoMars rover and the Surface Science Platform and provide communication for other Mars surface probes with Earth.\n\nInvestigations with space and Earth-based observatories have demonstrated the presence of a small amount of methane on the atmosphere of Mars that seems to vary with location and time. This may indicate the presence of microbial life on Mars, or a geochemical process such as volcanism or hydrothermal activity.\n\nThe challenge to discern the source of methane in the atmosphere of Mars prompted the independent planning by ESA and NASA of one orbiter each that would carry instruments in order to determine if its formation is of biological or geological origin, as well as its decomposition products such as formaldehyde and methanol.\n\nExoMars Trace Gas Orbiter was born out of the nexus of ESA's Aurora programme ExoMars flagship and NASA's 2013 and 2016 Mars Science Orbiter (MSO) concepts. It became a flexible collaborative proposal within NASA and ESA to send a new orbiter-carrier to Mars in 2016 as part of the European-led ExoMars mission. On the ExoMars side, ESA authorised about half a billion Euros in 2005 for a rover and mini-station; eventually this evolved into being delivered by an orbiter rather than a cruise stage.\n\nNASA's Mars Science Orbiter (MSO) was originally envisioned in 2008 as an all-NASA endeavour aiming for a late 2013 launch. NASA and ESA officials agreed to pool resources and technical expertise and collaborate to launch only one orbiter. The agreement, called the Mars Exploration Joint Initiative, was signed on July 2009 and proposed to use an Atlas rocket launcher instead of a Soyuz rocket, which significantly altered the technical and financial setting of the European ExoMars mission. Since the ExoMars rover was originally planned to be launched along with the TGO, a prospective agreement would require that the rover lose enough weight to fit aboard the Atlas launch vehicle with NASA's orbiter. Instead of reducing the rover's mass, it was nearly doubled when the mission was combined with other projects to a multi-spacecraft programme divided over two Atlas V launches: the ExoMars Trace Gas Orbiter (TGO) was merged into the project, carrying a meteorological lander planned for launch in 2016. The European orbiter would carry several instruments originally meant for NASA's MSO, so NASA scaled down the objectives and focused on atmospheric trace gases detection instruments for their incorporation in ESA's ExoMars Trace Gas Orbiter.\n\nUnder the FY2013 budget President Barack Obama released on 13 February 2012, NASA terminated its participation in ExoMars due to budgetary cuts in order to pay for the cost overruns of the James Webb Space Telescope. With NASA's funding for this project cancelled, most of ExoMars' plans had to be restructured.\n\nOn 15 March 2012, the ESA's ruling council announced it would press ahead with its ExoMars program in partnership with the Russian space agency Roscosmos, which planned to contribute two heavy-lift Proton launch vehicles and an additional entry, descent and landing system to the 2020 rover mission.\n\nUnder the collaboration proposal with Roscosmos, the ExoMars mission is split into two parts: the orbiter/lander mission in March 2016 that includes the TGO and a diameter stationary lander built by ESA named \"Schiaparelli\"; this will be followed by the ExoMars rover mission in 2020, also to be launched with a Russian Proton rocket.\n\nThe Trace Gas Orbiter and descent module \"Schiaparelli\" completed testing and were integrated to a Proton rocket at the Baikonur Cosmodrome in Kazakhstan in mid-January 2016. The launch occurred at 09:31 UTC on 14 March 2016. Four rocket burns occurred in the following 10 hours before the descent module and orbiter were released. A signal from the spacecraft was received at 21:29 UTC that day, confirming that the launch was successful and the spacecraft were functioning properly.\n\nShortly after separation from the probes, a Brazilian ground telescope recorded small objects in the vicinity of the Briz-M upper booster stage, suggesting that the Briz-M stage exploded a few kilometres away, without damaging the orbiter or lander. Briefing reporters in Moscow, the head of Roscosmos denied any anomaly and made all launch data available for inspection.\n\nThe \"Schiaparelli\" lander separated from the TGO orbiter on 16 October 2016, three days before it arrived on Mars, and entered the atmosphere at . \"Schiaparelli\" transmitted about 600 megabytes of telemetry during its landing attempt, before it impacted the surface at .\n\nThe TGO was injected into Mars orbit on 19 October 2016 and underwent 11 months of aerobraking (March 2017 to February 2018), reducing its orbital speed by and its orbit from an initial down to . Additional thruster firings through mid-April circularised the spacecraft's orbit to , and full science activities began on 21 April 2018.\n\n\nThe TGO separated from the ExoMars \"Schiaparelli\" demonstration lander and would have provided it with telecommunication relay for 8 Martian solar days (sols) after landing. Then the TGO gradually underwent aerobraking for seven months into a more circular orbit for science observations and will provide communications relay for the ExoMars rover to be launched in 2020, and will continue serving as a relay satellite for future landed missions.\n\nThe FREND instrument will map hydrogen levels to a maximum depth of beneath the Martian surface. Locations where hydrogen is found may indicate water-ice deposits, which could be useful for future crewed missions.\n\nParticularly, the mission will characterise spatial, temporal variation, and localisation of sources for a broad list of atmospheric trace gases. If methane () is found in the presence of propane () or ethane (), that will be a strong indication that biological processes are involved. However, if methane is found in the presence of gases such as sulfur dioxide (), that would be an indication that the methane is a byproduct of geological processes.\n\n\nThe nature of the methane source requires measurements of a suite of trace gases in order to characterise potential biochemical and geochemical processes at work. The orbiter has very high sensitivity to (at least) the following molecules and their isotopomers:\nwater (), hydroperoxyl (), nitrogen dioxide (), nitrous oxide (), methane (), acetylene (), ethylene (), ethane (), formaldehyde (), hydrogen cyanide (), hydrogen sulfide (), carbonyl sulfide (), sulfur dioxide (), hydrogen chloride (), carbon monoxide () and ozone (). Detection sensitivities are at levels of 100 parts per trillion, improved to 10 parts per trillion or better by averaging spectra which could be taken at several spectra per second.\n\nLike the \"Mars Reconnaissance Orbiter\", the Trace Gas Orbiter is a hybrid science and telecom orbiter. Its scientific payload mass is about and consists of:\n\n\n\nDue to the challenges of entry, descent and landing, Mars landers are highly constrained in mass, volume and power. For landed missions, this places severe constraints on antenna size and transmission power, which in turn greatly reduce direct-to-Earth communication capability in comparison to orbital spacecraft. As an example, the capability downlinks on \"Spirit\" and \"Opportunity\" rovers have only the capability of the \"Mars Reconnaissance Orbiter\" downlink. Relay communication addresses this problem by allowing Mars surface spacecraft to communicate using higher data rates over short-range links to nearby Mars orbiters, while the orbiter takes on the task of communicating over the long-distance link back to Earth. This relay strategy offers a variety of key benefits to Mars landers: increased data return volume, reduced energy requirements, reduced communications system mass, increased communications opportunities, robust critical event communications and \"in situ\" navigation aid. NASA provided an Electra telecommunications relay and navigation instrument to assure communications between probes and rovers on the surface of Mars and controllers on Earth. The TGO will provide the ExoMars rover with a telecommunication relay; it will also serve as a relay satellite for future landed missions.\n\nThe spacecraft took its first photos of the surface of Mars on 15 April 2018. Data on the first atmospheric occultation is being analysed.\n\n"}
{"id": "4006412", "url": "https://en.wikipedia.org/wiki?curid=4006412", "title": "Fityk", "text": "Fityk\n\nFityk is a curve fitting and data analysis application, predominantly used to fit analytical,\nbell-shaped functions to experimental data. It is positioned to fill the gap between general plotting software and programs specific for one field, e.g. crystallography or XPS.\n\nOriginally, Fityk was developed to analyse powder diffraction data. It is also used in other fields that require peak analysis and peak-fitting, like chromatography or various kinds of spectroscopy.\n\nFityk is distributed under the terms of GNU General Public License, but since version 1.0.0, subscription is required for downloading binaries. It runs on Linux, macOS, Microsoft Windows, FreeBSD and other platforms. It operates either as a command line program or with a graphical user interface.\n\nIt is written in C++, using wxWidgets, and providing bindings for Python and other scripting languages.\n\n\nThe programs peak-o-mat and MagicPlot have similar scope.\n\nMore generic data analysis programs with spread-sheet capabilities include the proprietary Origin and its free clones QtiPlot and SciDAVis.\n\n\n"}
{"id": "17663065", "url": "https://en.wikipedia.org/wiki?curid=17663065", "title": "Flags of the Republic of Macedonia", "text": "Flags of the Republic of Macedonia\n\nThis is a list of flags which have been, or are still today, used on the territory of the Republic of Macedonia or by ethnic Macedonians.\n\n"}
{"id": "3471221", "url": "https://en.wikipedia.org/wiki?curid=3471221", "title": "Forensic facial reconstruction", "text": "Forensic facial reconstruction\n\nForensic facial reconstruction (or forensic facial approximation) is the process of recreating the face of an individual (whose identity is often not known) from their skeletal remains through an amalgamation of artistry, anthropology, osteology, and anatomy. It is easily the most subjective—as well as one of the most controversial—techniques in the field of forensic anthropology. Despite this controversy, facial reconstruction has proved successful frequently enough that research and methodological developments continue to be advanced.\n\nIn addition to remains involved in criminal investigations, facial reconstructions are created for remains believed to be of historical value and for remains of prehistoric hominids and humans.\n\nThere are two forms pertaining to identification in forensic anthropology: circumstantial and positive.\n\n\nFacial reconstruction presents investigators and family members involved in criminal cases concerning unidentified remains with a unique alternative when all other identification techniques have failed. Facial approximations often provide the stimuli that eventually lead to the positive identification of remains.\n\nIn the U.S., the Daubert Standard is a legal precedent set in 1993 by the Supreme Court regarding the admissibility of expert witness testimony during legal proceedings, set in place to ensure that expert testimony is based on sufficient facts or data, derived from proper application of reliable principles and methods. When multiple forensic artists produce approximations for the same set of skeletal remains, no two reconstructions are ever the same and the data from which approximations are created are largely incomplete. Because of this, forensic facial reconstruction does not uphold the Daubert Standard, is not considered a legally recognized technique for positive identification, and is not admissible as expert testimony. Currently, reconstructions are only produced to aid the process of positive identification in conjunction with verified methods.\n\nTwo-dimensional facial reconstructions are based on ante mortem photographs, and the skull. Occasionally skull radiographs are used but this is not ideal since many cranial structures are not visible or at the correct scale. This method usually requires the collaboration of an artist and a forensic anthropologist. A commonly used method of 2D facial reconstruction was pioneered by Karen T. Taylor of Austin, Texas during the 1980s. Taylor's method involves adhering tissue depth markers on an unidentified skull at various anthropological landmarks, then photographing the skull. Life-size or one-to-one frontal and lateral photographic prints are then used as a foundation for facial drawings done on transparent vellum. Recently developed, the F.A.C.E. and C.A.R.E.S. computer software programs quickly produce two-dimensional facial approximations that can be edited and manipulated with relative ease. These programs may help speed the reconstruction process and allow subtle variations to be applied to the drawing, though they may produce more generic images than hand-drawn artwork.\n\nThree-dimensional facial reconstructions are either: 1) sculptures (made from casts of cranial remains) created with modeling clay and other materials or 2) high-resolution, three-dimensional computer images. Like two-dimensional reconstructions, three-dimensional reconstructions usually require both an artist and a forensic anthropologist. Computer programs create three-dimensional reconstructions by manipulating scanned photographs of the unidentified cranial remains, stock photographs of facial features, and other available reconstructions. These computer approximations are usually most effective in victim identification because they do not appear too artificial. This method has been adapted by the National Center for Missing & Exploited Children, which uses this method often to show approximations of an unidentified decedent to release to the public in hopes to identify the subject.\n\nSuperimposition is a technique that is sometimes included among the methods of forensic facial reconstruction. It is not always included as a technique because investigators must already have some kind of knowledge about the identity of the skeletal remains with which they are dealing (as opposed to 2D and 3D reconstructions, when the identity of the skeletal remains are generally completely unknown). Forensic superimpositions are created by superimposing a photograph of an individual suspected of belonging to the unidentified skeletal remains over an X-ray of the unidentified skull. If the skull and the photograph are of the same individual, then the anatomical features of the face should align accurately.\n\nHermann Welcker in 1883 and Wilhelm His, Sr. in 1895, were the first to reproduce three-dimensional facial approximations from cranial remains. Most sources, however, acknowledge His as the forerunner in advancing the technique. His also produced the first data on average facial tissue thickness followed by Kollmann and Buchly who later collected additional data and compiled tables that are still referenced in most laboratories working on facial reproductions today.\n\nFacial reconstruction originated in two of the four major subfields of anthropology. In biological anthropology, they were used to approximate the appearance of early hominid forms, while in archaeology they were used to validate the remains of historic figures. In 1964, Mikhail Gerasimov was probably the first to attempt paleo-anthropological facial reconstruction to estimate the appearance of ancient peoples\n\nAlthough students of Gerasimov later used his techniques to aid in criminal investigations, it was Wilton M. Krogman who popularized facial reconstruction's application to the forensic field. Krogman presented his method for facial reconstruction in his 1962 book, detailing his method for approximation. Others who helped popularize three-dimensional facial reconstruction include Cherry (1977), Angel (1977), Gatliff (1984), Snow (1979), and Iscan (1986).\n\nIn 2004 it was for Dr. Andrew Nelson of the University of Western Ontario, Department of Anthropology that noted Canadian artist Christian Corbet created the first forensic facial reconstruction of an approximately 2,200-year-old mummy based on CT and laser scans. This reconstruction is known as the Sulman Mummy project.\n\nBecause a standard method for creating three-dimensional forensic facial reconstructions has not been widely agreed upon, multiple methods and techniques are used. The process detailed below reflects the method presented by Taylor and Angel from their chapter in Craniofacial Identification in Forensic Medicine, pgs 177-185. This method assumes that the sex, age, and race of the remains to undergo facial reconstruction have already been determined through traditional forensic anthropological techniques.\n\nThe skull is the basis of facial reconstruction; however, other physical remains that are sometimes available often prove to be valuable. Occasionally, remnants of soft tissue are found on a set of remains. Through close inspection, the forensic artist can easily approximate the thickness of the soft tissue over the remaining areas of the skull based on the presence of these tissues. This eliminates one of the most difficult aspects of reconstruction, the estimation of tissue thickness. Additionally, any other bodily or physical evidence found in association with remains (e.g. jewelry, hair, glasses, etc.) are vital to the final stages of reconstruction because they directly reflect the appearance of the individual in question.\n\nMost commonly, however, only the bony skull and minimal or no other soft tissues are present on the remains presented to forensic artists. In this case, a thorough examination of the skull is completed. This examination focuses on, but is not limited to, the identification of any bony pathologies or unusual landmarks, ruggedness of muscle attachments, profile of the mandible, symmetry of the nasal bones, dentition, and wear of the occlusal surfaces. All of these features have an effect on the appearance of an individual's face.\n\nOnce the examination is complete, the skull is cleaned and any damaged or fragmented areas are repaired with wax. The mandible is then reattached, again with wax, according to the alignment of teeth, or, if no teeth are present, by averaging the vertical dimensions between the mandible and maxilla. Undercuts (like the nasal openings) are filled in with modeling clay and prosthetic eyes are inserted into the orbits centered between the superior and inferior orbital rims. At this point, a plaster cast of the skull is prepared. Extensive detail of the preparation of such a cast is presented in the article from which these methods are presented.\n\nAfter the cast is set, colored plastics or the colored ends of safety matches are attached at twenty-one specific \"landmark\" areas that correspond to the reference data. These sites represent the average facial tissue thickness for persons of the same sex, race, and age as that of the remains. From this point on, all features are added using modeling clay.\n\nFirst, the facial muscles are layered onto the cast in the following order: temporalis, masseter, buccinator and occipito-frontals, and finally the soft tissues of the neck. Next, the nose and lips are reconstructed before any of the other muscles are formed. The lips are approximately as wide as the interpupillary distance. However, this distance varies significantly with age, sex, race, and occlusion. The nose is one of the most difficult facial features to reconstruct because the underlying bone is limited and the possibility of variation is expansive. The nasal profile is constructed by first measuring the width of the nasal aperture and the nasal spine. Using a calculation of three times the length of the spine plus the depth of tissue marker number five will yield the approximate nose length. Next, the pitch of the nose is determined by examining the direction of the nasal spine - down, flat, or up. A block of clay that is the proper length is then placed on the nasal spine and the remaining nasal tissue is filled in using tissue markers two and three as a guide for the bridge of the nose. The alae are created by first marking a point five millimeters below the bottom of the nasal aperture. After the main part of the nose is constructed, the alae are created as small egg-shaped balls of clay, that are five millimeters in diameter at the widest point, these are positioned on the sides of the nose corresponding with the mark made previously. The alae are then blended to the nose and the overall structure of the nose is rounded out and shaped appropriately.\n\nThe muscles of facial expression and the soft tissue around the eyes are added next. Additional measurements are made according to race (especially for those with eye folds characteristic of Asian descent) during this stage. Next, tissues are built up to within one millimeter of the tissue thickness markers and the ears (noted as being extremely complicated to reproduce) are added. Finally, the face is \"fleshed,\" meaning clay is added until the tissue thickness markers are covered, and any specific characterization is added (for example, hair, wrinkles in the skin, noted racial traits, glasses, etc.). The skull of Mozart was the basis of his facial reconstruction from anthropological data. The bust was unveiled at the \"Salon du Son\", Paris, in 1991.\n\nThere are multiple outstanding problems associated with forensic facial reconstruction.\n\nThe most pressing issue relates to the data used to average facial tissue thickness. The data available to forensic artists are still very limited in ranges of ages, sexes, and body builds. This disparity greatly affects the accuracy of reconstructions. Until this data is expanded, the likelihood of producing the most accurate reconstruction possible is largely limited.\n\nA second problem is the lack of a methodological standardization in approximating facial features. A single, official method for reconstructing the face has yet to be recognized. This also presents major setback in facial approximation because facial features like the eyes and nose and individuating characteristics like hairstyle - the features most likely to be recalled by witnesses - lack a standard way of being reconstructed. Recent research on computer-assisted methods, which take advantage of digital image processing, pattern recognition, promises to overcome current limitations in facial reconstruction and linkage.\n\nReconstructions only reveal the type of face a person \"may\" have exhibited because of artistic subjectivity. The position and general shape of the main facial features are mostly accurate because they are greatly determined by the skull.\n\nIn recent years, the presence of forensic facial reconstructions in the entertainment industry and the media has increased. The way the fictional criminal investigators and forensic anthropologists utilize forensics and facial reconstructions are, however, often misrepresented (an influence known as the \"CSI effect\"). For example, the fictional forensic investigators will often call for the creation of a facial reconstruction as soon as a set of skeletal remains is discovered. In many instances, facial reconstructions have been used as a last resort to stimulate the possibility of identifying an individual. \n\nFacial reconstruction has been featured as part of an array of forensic science methods in fictional TV shows like \"\" and \"NCIS\" and their respective spinoffs in the \"CSI\" and \"NCIS\" franchises.\n\nIn \"Bones\", a long-running TV series centered around forensic analysis of decomposed and skeletal human remains, facial reconstruction is featured in the majority of episodes, used much like a police artist sketch in police procedurals. Regular cast character Angela Montenegro, the Bones team's facial reconstruction specialist, employs 3D software and holographic projection to \"give victims back their faces\" (as noted in the episode, \"A Boy in a Bush\").\n\nIn the \"MacGyver\" episode \"The Secret Of Parker House\", MacGyver reconstructs the skull of Penny's aunt Betty while investigating her house.\n\nThe facial reconstruction of Egypt's Tutankhamun, popularly known as King Tut, made the June 2005 cover of National Geographic Magazine.\n\nA variety of facial reconstruction kits toys are available, with \"crime scene\" versions, also, reconstructions of famous historical figures, like King Tut and the dinosaur T-Rex.\n\nRecently, facial reconstruction has been part of the process used by researchers attempting to identify human remains of two Canadian Army soldiers lost in World War I. One soldier was identified through DNA analysis in 2007, but due to DNA deterioration, identifying the second using the same techniques failed. In 2011, the second of the soldiers' remains discovered at Avion, France were identified through a combination of 3-D printing software, reconstructive sculpture and use of isotopic analysis of bone.\n\n\n"}
{"id": "41723517", "url": "https://en.wikipedia.org/wiki?curid=41723517", "title": "Functional periodicity", "text": "Functional periodicity\n\nFunctional periodicity is a term that emerged around the late 19th century around the belief, later to be found invalid, that women suffered from physical and mental impairment during their menstrual cycle. Men held a higher status and were regarded as superior to women at this period in time. Many prominent male psychologists promoted the idea of functional periodicity. Women were not seen as being fit for certain types of work, responsibilities, and roles because of this idea. The idea of functional periodicity stems from ancient taboos and rituals that were passed on from generation to generation. It then developed into an actual theory in the twentieth century.\n\nFunctional periodicity was investigated by a female psychologist named Leta Hollingworth. She made key contributions in the research of functional periodicity, as well as in the feminist movement at the time. Hollingworth, along with her husband Harry Hollingworth, established exceptional research on the idea of functional periodicity and created research studies investigating the science of motor/learning tasks involving the human body. Her research impacted how society viewed women, despite the patriarchal opinions held by many.\n\nFunctional periodicity was the idea of women being functionally impaired during their menstruation cycle. The untested hypothesis of the time was supported by men, because at this time they dominated society, and this idea helped to keep women in a subordinate position. Women were viewed as not qualified for certain types of work, achievements, and certain responsibilities. The belief also reinforced the stereotype of women being fragile in physical and emotional well-being. Multiple studies were performed to look at mental and physical (motor) abilities during menstruation. Many men concluded that women were not fit for certain work and responsibilities. For example, in the eighteenth and nineteenth century, there was debate about whether or not women should participate in higher education. Many argued that women should not go on to pursue higher education because of the dangers that may be involved relating to physiological circumstances. An example of reasoning using functional periodicity is a quote by Henry Maudsley in 1874. He further states:\n\n‘\"This is a matter of physiology, not a matter of sentiment… not a question of two bodies and minds that are in equal physical condition, but if one body and mind capable of sustained and regular hard labor, and of another body and mind which for one quarter of each month, during the best years of life, is more or less sick and unfit for hard work.\"\n\nThis quote exemplifies the sexist beliefs expressed during this time period. Because of these opinions and beliefs, the idea of functional periodicity became more relevant in American society.\n\nThe idea of functional periodicity stems from past cultural superstitions. In the past, menstruation has been thought of as superstitious and taboo. An example of this line of thought comes from the British Medical Journal, which discusses the question of whether or not a menstruating woman can contaminate or damage food by touching it. Many individuals believed in this superstition and reinforced it.\n\nLeta Hollingworth was born on the Nebraska frontier in 1886. When she was an adolescent, her mother died, which lead to rough household conditions within the family. Despite the difficulties, she did exceptionally well in school. This drove her to pursue her education further, which resulted in her graduating from University of Nebraska. While studying there, she met Harry Hollingworth and married him soon after. She began teaching while her husband enrolled in a doctoral program at Columbia University, but because she was married she could no longer teach in the state of New York. At the time, this was against the law. This barrier fueled Hollingworth's feminist activism. When she finally got the opportunity to enroll in the psychology program at Columbia University under Edward Thorndike, she took it. At the end of the program, she decided to study functional periodicity for her dissertation research.\n\nHollingworth wanted to investigate the idea of functional periodicity and its assumptions about women. She started by designing two separate studies. The first study that was designed tested men and women on a series of mental and motor tasks while the second study monitored only females over a 30-day period. Her results showed there was no significant difference in physical and emotional tasks when a woman is menstruating and when she is not.\n\nLeta Hollingworth was a clinical psychologist and feminist activist who conducted psychological research on several theories involving women. Some of these theories included the variability hypothesis and functional periodicity. She was quite instrumental in disproving the theory of functional periodicity, which was widely believed to be true by scientists as well as the general public.\n\nFor her dissertation research at Columbia Teacher's College, Hollingworth decided to conduct an investigation on functional periodicity, which she considered to be an unfair assumption about women that lacked any scientific grounds to justify itself. Hollingworth completed her dissertation under the supervision of the psychologist Edward Thorndike, a major proponent of the variability hypothesis. Though they held conflicting views, Hollingworth thanked him for aiding her. Hollingworth saw it as her duty as a feminist to prove through scientific study that women were equally as capable and intelligent as men in all of their pursuits, even during menstruation. Her dissertation included three studies dealing with functional periodicity among women, two of which were intensive and one which was extensive. Her studies on functional periodicity helped to expand the view of menstruation and reduce bias towards women.\n\nLeta Hollingworth decided to use tests that had been used previously by psychologists to complete her study. She decided to use a familiar tapping test and steadiness test to assess motor ability. She had a total of 8 participants, including 6 women and 2 men. The participants ranged from 23–45 years old.\n\nThe tapping test was administered by having each participant tap a brass rod that was connected to a brass plate 400 times with their right hand to record maximal speed. It should be noted that Stanley Hall approved of this apparatus and stated how important it was for measuring muscle control. This is important because G.S. Hall is one of the leading male figures who stated, \"Women... can make less accurate and energetic movements, and the mental activities are less brilliant.\"\n\nAfter the data was analyzed, the researchers noted that there is no data from this study that suggests that women are experiencing more fatigue, less will power, and diminished energy during their menstrual cycle. Fatigue from the tests occurred similarly for men and women. The first 200 taps were faster compared to the last 200 taps.\n\nThe steadiness test was measured by having each participant hold a brass rod 2.5 mm in diameter at arms length. This rod was in a hole 6 mm in diameter. While standing they were asked to hold it for 30 seconds and make the least amount of contact with the hole. Each contact was measured by an electric counter. Due to high variability of the averages the data was deemed unreliable. This was proved by the fact that outside or external forces could affect the outcome greatly. Examples of this would be if the participant coughed, took a breath or got startled by a noise.\n\nNot only did Leta Hollingworth study motor ability, she also sought to study mental ability as well. This was done by using two specific tests, color naming and saying opposites. Color naming was observed by having a card face down laid in front of the participant. Each participant was to name the color on the card as quickly as possible. The opposites test used a list of 50 words to test for mental ability as well. The words were presented to the individual in two columns and the words were typed. The participants went through the list as quickly as possible, naming the opposite to each word.\n\nLastly, she decided to do one more experiment involving 17 females. This study was to observe steadiness, tapping, and the opposites test. In this study, ages ranged from 20 to 40 years old. This extensive experiment was much the same as the intensive experiments involving the 8 participants. They were conducted every 3rd day for 30 days. Two trials were administered at every sitting to help with reliability. The experiment provided results that were very similar to her prior intensive experiment.\n"}
{"id": "40371530", "url": "https://en.wikipedia.org/wiki?curid=40371530", "title": "Georg Ludwig Engelhard Krebs", "text": "Georg Ludwig Engelhard Krebs\n\nGeorg Ludwig Engelhard Krebs (19 July 1792 - 11 May 1844) was a German apothecary and natural history collector who spent his entire career in South Africa.\n\nHe was born in Wittingen, then a part of Hanover, the son of Johan Krebs and Cecilia Engtlingen, and brother to Sophie Margaretha Dorothea Krebs and Georg Krebs. \n\nSoon after qualifying as apothecary, he was recruited to work in South Africa by the firm of \"Pallas & Poleman\" of Cape Town. Arriving in May 1817, Krebs started collecting, often in the company of Karl Heinrich Bergius who died at age 28 of pulmonary tuberculosis in January 1818, poverty-stricken and abandoned by his former sponsors and employers. Krebs also got to know and collected in the company of Mund and Maire, Joachim Brehm, von Chamisso, Delalande, and Dr. William Gill, who became a close friend and medical adviser during their stay in the Eastern Cape. \n\nKrebs asked his brother, Georg, a medical student in Berlin, to find out whether the Berlin's Natural History Museum was interested in purchasing natural history specimens. When Martin Lichtenstein agreed to this, some large consignments were shipped in 1820 and 1821.\n\nAt the end of his four year contract with Pallas & Poleman, Krebs was succeeded by Carl Friedrich Drège, the brother of Johann Franz Drège, leaving him free to sail to Algoa Bay where he arrived in November 1821. He left immediately for the inland town of Uitenhage where he was to stay the next three years. Back in Germany, Lichtenstein had obtained permission for Krebs to collect on behalf of the Berlin's Natural History Museum, resulting in the splendid title of \"Cape Naturalist to the King of Prussia\".\n\nIn 1822 Krebs collected at Uitenhage, Sundays River, Galgenbosch, Van Stadens River, Zuurberg and Swartkops, sending four consignments to Lichtenstein, which consisted mainly of birds, mammals and insects, but with some bulbs and succulent plants, assisted and advised by James Bowie. In 1823 he roamed further afield visiting the Bushmans, Kariega and Kowie Rivers, sending off his eighth consignment of 3800 insects, some 100 birds and no plants.\n\nIn August 1823 he visited the newly established Fort Beaufort, collecting for a few months along the Baviaans River, now called Baviaanskloof River, and eastwards to the Great Fish River, returning to Uitenhage by January 1824. His 9th and 10th consignments were shipped from Cape Town in January 1825.\n\nIn 1824 Lichtenstein warned him that his contract for supplying specimens to the Berlin Museum was about to end and that they would only accept 3 further consignments. Krebs sailed to Cape Town in November 1824 to discuss his future with Poleman. Krebs had thought of starting a pharmacy somewhere in the Cape, and wrote to his brother inviting him to join in the enterprise. Early in 1825 he returned to Algoa Bay, making his way once more to the Baviaans River, where he stayed again on the Boschfontein farm belonging to Field Cornet Cornelius van der Nest, a farm which was also periodically visited by Clemenz Wehdemann.\n\nIn the following summer Krebs explored and collected beyond the Winterberg in the area of present-day Queenstown, and soon dispatched his eleventh consignment made up of six cases. By June 1826 he had started a pharmacy in Grahamstown, taking on Leopold Schmidt as a partner, and hoped to persuade his two nephews, Carl and Heinrich Kemper, two trained apothecaries, to also join him. \n\nIn March 1828 his health suffered a blow when, while on a visit to Cape Town, he was stricken by rheumatism, a condition he was to endure for the rest of his life. Consequently he convalesced for six months, staying with Pieter Heinrich Poleman. At about this time his application for citizenship was approved, enabling him to buy a farm called \"Doornkroon\" on the Baviaans River, and which he later renamed \"Lichtenstein\" in honour of his patron, Martin Lichtenstein. Carl Kemper arrived in Cape Town in January 1829 and joined Krebs at Baviaans River. In November of 1829 Krebs sent through his 12th consignment made up of material collected over several years; included were more than 7000 preserved plant specimens and a barrel with a complete Bushman pickled in brine, some 900 birds and over 7000 insects. These specimens had been collected from Baviaans River, Tembuland, and the Tarka and Orange Rivers.\n\nHis brother Georg arrived in Cape Town in May 1834 and was granted a medical licence, joining the other members of the family at \"Lichtenstein\" in July. Much work went into building up the farm after it had been through the Sixth Kaffir War of 1834/5. Krebs' letters about the war survive. Georg set up a practice in Graaff-Reinet, later becoming District Surgeon. Krebs' partner Leopold Scmidt died in July 1836 and Krebs disposed of all his Grahamstown interests, but continued dispensing from the farm. \n\nFor a long time Ludwig Krebs had been thinking about an expedition north of the Orange and Vaal Rivers. Accordingly, accompanied by his nephew Carl Klemper, he left his farm in March 1838 with four wagons. His route was through the present-day Aliwal North, Jammersberg near Wepener, Thaba Nchu, north-east toward Clocolan, Mpharane near Ficksburg, and northwards to the Makwassie area. He crossed the Sand River near the present Senekal and followed it westward, and then northwards to the Vals River. He went west again crossing the Vaal River, probably at Kommando Drift, and then northwards to the Makwassie highlands where he collected for several weeks. His return leg followed a more westerly route, and he arrived at 'Lichtenstein' toward the end of December 1838. Another expedition was planned for Natal, but his chronic rheumatism made this extremely difficult, though there is evidence that Carl Klemper may have visited the region in 1839 or 1840.\n\nHe died at \"Lichtenstein\" farm near Bedford, Eastern Cape, South Africa at the age of 51.\n\nLudwig Krebs put together one of the most extensive natural history collections from Africa to reach Europe. Their value was slightly debased by Lichtenstein's breaking up the collections for auctioning and citing their provenance as \"Kaffraria\", whereas Krebs had provided detailed notes on the localities on his original labels.\n\nKrebs is commemorated in numerous plant and animal specific names including \"Gazania krebsiana\" and uniquely in the monotypic \"Glekia krebsiana\" belonging to the Scrophulariaceae, the genus formed from his initials by Olive Mary Hilliard, and the species by George Bentham - this is an alpine species found on the high mountains of Lesotho and the Eastern Cape. 'Krebs's fat mouse', \"Steatomys krebsii\" Peters, 1852, was also named in his honour.\n\n\n"}
{"id": "55561850", "url": "https://en.wikipedia.org/wiki?curid=55561850", "title": "Golden binary", "text": "Golden binary\n\nIn gravitational wave astronomy, a golden binary is a binary black hole collision event whose inspiral and ringdown phases have been measured accurately enough to provide separate measurements of the initial and final black hole masses.\n\nCurrent LIGO/Virgo protocol relies on its library of several hundred thousand precomputed templates of black hole collisions conceivably detectable in their frequency range. A putative binary black hole collision signal consists of inspiral, merger, and ringdown phases. The complete signal is compared with the template library, and event parameters and significance are based on an analysis of such matches.\n\nThis allows for self-consistency checks of general relativity. In order to test certain competing theories of gravity, one faces the problem that only general relativity has been studied enough that the complete merger phase is known. Therefore, only a signal that can be matched separately in the inspiral and ringdown phases can be used to allow or contradict such theories.\n\nGW150914 was a golden binary, indeed, this led to additional internal checks done by LIGO. GW151226 and LVT151012 were not.\n"}
{"id": "50262203", "url": "https://en.wikipedia.org/wiki?curid=50262203", "title": "Human uses of birds", "text": "Human uses of birds\n\nHuman uses of birds have included both economic uses such as food and symbolic uses such as art, music and religion, for thousands of years.\n\nIn terms of economic uses, birds have been hunted for food since Palaeolithic times. They have been captured and bred as poultry to provide meat and eggs since at least the time of ancient Egypt. Some species have been used, too, to help locate or to catch food, as with cormorant fishing and the use of honeyguides. Feathers have long been used for bedding, as well as for quill pens and for fletching arrows.\nToday, many species face habitat loss and other threats caused by humans; bird conservation groups work to protect birds and to influence governments to do so.\n\nBirds have appeared in the mythologies and religions of many cultures since ancient Sumer. For example, the dove was the symbol of the ancient Mesopotamian goddess Inanna, the Canaanite mother goddess Asherah, and the Greek goddess Aphrodite. Athena, the Greek goddess of wisdom, had a little owl as her symbol, and, in ancient India, the peacock represented Mother Earth. Birds have often been seen as symbols, whether bringing bad luck and death, being sacred, or being used in heraldry. \nIn terms of entertainment, raptors have been used in falconry, while cagebirds have been kept for their song. Other birds have been raised for the traditional sports of cockfighting and pigeon racing. Birdwatching, too, has grown to become a major leisure activity.\nBirds feature in a wide variety of art forms, including in painting, sculpture, poetry and prose, film and fashion. Birds also appear in music as well as traditional dance and ballet. In certain cases, such as the bird-and-flower painting of China, birds are central to an artistic genre.\n\nCulture consists of the social behaviour and norms found in human societies and transmitted through social learning. Cultural universals in all human societies include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers physical expressions such as technology, architecture and art, whereas immaterial culture includes principles of social organization, mythology, philosophy, literature, and science. This article describes the roles played by birds in human culture, so defined.\n\nBirds are important economically, providing substantial amounts of food, especially protein, largely but not exclusively from the domestic chicken; feathers and down are used for bedding, insulation, and other purposes.\n\nBirds were among the wild animals hunted for food before the Neolithic revolution and the development of agriculture. For example, in the Epipaleolithic of the Levant, between c. 14,500 and 11,500 BP, both waterfowl and migratory birds were eaten. Archaeologists have studied the return in terms of energy from captured food compared to the energy expended to capture it; birds provide a smaller return than larger game such as deer, but better than many plant materials. For example, waterfowl captured in a drive can yield a return of around 2,000 kcal/hour, whereas an antelope can yield as much as 31,000 kcal/hour, and wild rye around 1,000 kcal/hour.\n\nBirds have been domesticated and bred as poultry for use as food for at least four thousand years. The most important species is the chicken. It appears to have been domesticated by 5000 BC in northeastern China, likely for cockfighting, and only later used for food. In ancient Egypt, poultry including ducks, geese, and pigeons were captured in nets and then bred in captivity.\n\nChicken now provides some 20% of the animal protein eaten by the world's human population in the form of meat and eggs. Chickens are often raised intensively in battery farms; this facilitates production but has been criticised on animal welfare grounds. Other species including ducks, geese, pheasants, guineafowl and turkeys are significant economically around the world. Less commonly raised species such as the ostrich are starting to be farmed for their meat, which is low in cholesterol; they have also been kept for their feathers, and for leather from their skin.\n\nBirds are hunted in many countries around the world. In the developed world, ducks such as mallard, wigeon, shoveler and teal have for centuries been captured by wildfowlers, while pheasants, partridges, grouse, and snipe are among the terrestrial birds that are hunted for sport, generally with guns. In other parts of the world, traditional subsistence hunting still continues, as in rural Northern Papua, where cassowaries, crowned pigeons, hornbills and megapodes are captured for food. Seabirds such as muttonbirds, penguins and auks have been hunted for food, formerly with sufficient intensity to threaten many populations and to make some, such as the great auk, extinct. Seabird hunting continues at more moderate levels today, for instance with the traditional Māori harvest of sooty shearwater chicks.\n\nThe archaeological and historical records suggest interdependence between humans and vultures for millions of years. Like other animal species, early hominins probably used these birds as beacons signalling the location of meat, in the form of carcasses, in the landscape.\n\nCormorant fishing is a traditional fishing method in which trained cormorants are used to catch fish in rivers. Historically, cormorant fishing has taken place in Japan and China since about 960 AD.\n\nThe greater honeyguide guides people in some parts of Africa to the nests of wild bees. A guiding bird attracts a person's attention with a chattering call, and flies in short bounds towards a bees' nest. When the human honey-hunter has taken their honey, the honeyguide eats what is left. The Boran people of East Africa use a specific whistle, which doubles the encounter rate with honeyguides; they find that using a honeyguide reduces the time to find honey by two-thirds. The Bushmen of the Kalahari thank the honeyguide with a gift of honey.\n\nFeathers are used to make warm and soft bedding, including eiderdowns from the belly down of the eider duck, and winter clothing as they have high \"loft\", trapping a large amount of air for their weight. \nFeathers were used also for quill pens, for fletching arrows, and to decorate fishing lures.\n\nBird bones were used by stone age peoples to make awls and other tools.\nGuano, the droppings of seabirds, rich in nitrogen, phosphorus, and potassium, was once important as an agricultural fertiliser and is still used in organic farming. The War of the Pacific in 1865 was in part about which country had control of the territory containing valuable guano sources. \nToday, birds such as the chicken and the Japanese quail are used as model organisms in ornithological and more generally in biological research, for instance in toxicology.\n\nFeathers have been important and colourful items of clothing and fashion from before the birth of civilisation. Elaborate, brightly coloured headdresses containing feathers are worn by indigenous peoples of the Americas such as the Bororo of the Mato Grosso. In Polynesia, \"sega ula\" lory bird feathers were major trade items, used to decorate high quality mats in Samoa and Tonga.\nIn Western culture, feathers are used in boas and decorating elaborate hats and other items of ladies' clothing. Feathers in fashion were a status symbol well into the nineteenth and twentieth centuries. The Belle Epoque draped its clothing in feathers as ornaments. Ostrich plumes were a luxury commodity in Europe for centuries, leading to serious harm to wild ostrich populations, and subsequent establishment of ostrich farms. Classical 1930s Hollywood films used feathers in abundance, arguably as a metaphor for female sexuality. For example, in the 1935 musical \"Top Hat\", Ginger Rogers danced \"Cheek to Cheek\" covered in white plumes that emphasised her movements. Late twentieth century designers such as Yves Saint Laurent and Alexander McQueen used feathers to make fashion statements.\n\nRaptors from eagles to small falcons have for centuries been used in falconry, often to catch other birds, whether for pleasure or for food.\n\nCockfighting is an ancient spectator sport. It formed part of the culture of the ancient Indians, Chinese, Greeks, and Romans. It continues to be practised in South America and across South and Southeast Asia, often combined with betting on the result. It is practised in religious ceremonies in Hindu temples in Bali, but is now banned in many countries on grounds of cruelty.\n\nPigeon racing involves releasing specially trained racing pigeons to return to their homes over a measured distance of between and . The sport was popularised in Belgium in the 19th century, and is now competitive worldwide. Also in Belgium and Flanders is \"vinkensport\", in which participants have male chaffinches compete to make the most bird calls in an hour.\n\nBirdwatching has since the nineteenth century become a major leisure activity. Millions of people around the world, amounting to nearly half of all households in some developed countries, put out birdfeeders to attract birds to their gardens or windowsills, at a cost of billions of dollars each year.\n\nCagebirds such as canaries, budgerigars, cockatoos, lovebirds, quails, finches, and parrots are popular pets, whether for their song, their behaviour, their colourful plumage, or their ability to mimic speech. Among reasons for their popularity is that they can be kept in homes too small or otherwise unsuitable for dogs or cats. The cagebird trade in some parts of the world threatens certain species with extinction, when birds are illegally captured in the wild. For example, in Indonesia, at least 13 species are close to extinction including the Indonesian national bird, the Javan hawk-eagle, while five subspecies including the scarlet-breasted lorikeet may have become extinct in the wild.\nPet birds are kept in their millions, as are domestic fowls, bantams, and pigeons. These last had an important effect on evolutionary biology, as Charles Darwin took an especial interest in pigeon fancying, adopted the hobby himself, and made use of the wide variation between breeds as an argument for the power of selection in his 1859 \"Origin of Species\".\n\nThe nature writers Mark Cocker and Richard Mabey, reviewing people's love of birds, observe that people are touched by feelings for birds in a variety of ways, such as enjoying the lapwing's \"joyous display\", or the \"beauty and mystery\" of the tawny owl's call on a cold winter's night. They argue that people feel the simple companionship of birds, are inspired by them to create art, let them mark the seasons and provide a sense of place, and use them \"as symbols of joy and love\". A former statesman, Edward Grey, 1st Viscount Grey of Fallodon, was able to express his feeling for birds in his 1927 book \"The Charm of Birds\". Such feelings, in turn, have stimulated the intention to conserve birds and their habitats. Around the same time as Grey was writing, the first conservation organisations were coming into being, starting in Britain, triggered by the rapid disappearance of familiar species as they were captured for their feathers or for food. A substantial folklore rich in symbolism has accrued around birds; it was documented early in the 20th century as something that was already fading from memory. For example, the house sparrow has been associated with \"sex and lechery\" since ancient Egypt, where libidinousness was written with the sparrow hieroglyph. In the same vein, in the classical era the sparrow was sacred to the goddess of love, Aphrodite or Venus; the sparrow features in an erotic poem by Catullus for the same reason. Chaucer describes the summoner in his \"Canterbury Tales\" as being as \"lecherous as a sparwe\".\n\nStudies have shown how important birds are to individual societies, touching on all aspects of life. In Andean societies such as the Moche (1–800 AD), Nazca (100–700 AD) and Chimu (1150–1450 AD), bright parrot and macaw feathers were traded from the Amazon rainforest to the mountains and the Pacific coast, while guano was collected as a fertiliser, and artists and craftsmen were inspired to create textiles, metal jewellery, and ceramics depicting condors, cormorants, ducks, hummingbirds, owls, vultures, and waders. Their religions, too, endowed birds with symbolic meaning.\n\nThe Audubon society, reviewing the importance of birds in 2013, obtained statements from many people with differing perspectives. Among them, the society's science director, Gary Langham, noted that what is good for birds is also good for humans. The writer David Allen Sibley observed that birds bring a little wildness into parks and gardens. The writer Barbara Kingsolver noted that birds are part of life on earth. The actress Jane Alexander wrote \"Birds remind us that there are angels.\" The forensic ornithologist Carla Dove noted that birds are biological indicators of habitat health, climate change, and the coming of spring.\n\nBirds have been seen as symbols, and used as such, though perceptions of bird species vary widely across cultures: some birds have a positive image in some regions, a negative image in others. Owls are associated with bad luck, witchcraft, and death in parts of Africa, but are regarded as wise across much of Europe. Hoopoes were considered sacred in Ancient Egypt and symbols of virtue in Persia, but were thought of as thieves across much of Europe, and harbingers of war in Scandinavia.\n\nIn heraldry, birds, especially eagles, often appear in coats of arms.\nIn Britain, over 3000 pubs have birds in their names, sometimes commemorating a local family with a bird from their coat of arms, sometimes for other reasons. There are dozens of pubs named \"Crow's Nest\" (nautical), \"Dog & Duck\" (wildfowling), \"Eagle & Child\" (heraldic), and \"Falcon\" (heraldic, or falconry), while over 600 pubs are named for swans.\n\nBirds, too, may symbolise human attributes such as stupidity or talkativeness. People have been called \"birdbrain[ed]\" or \"cuckoo\", among many other animal epithets. Birds feature prominently in often derogatory similes like \"noisy as a goose\" and metaphors including \"to parrot\".\n\nBirds have appeared in mythology and religion in a variety of guises.\n\nBirds have featured as gods from the time of ancient Egypt, where the sacred ibis was venerated as a symbol of the god Thoth. In India, the peacock is perceived as Mother Earth among Dravidian peoples, while the Mughal and Persian emperors displayed their godlike authority by sitting in a Peacock Throne. In the Yazidi religion, Melek Taus the \"Peacock Angel\" is the central figure of their faith. In the cult of Makemake, the Tangata manu birds of Easter Island served as chiefs.\n\nBirds have been seen as spirit messengers of the gods. In Norse mythology, Hugin and Munin were ravens who whispered news into the ears of the god Odin. In the Etruscan and Roman religions of ancient Italy, priests were involved in augury, interpreting the words of birds while the \"auspex\" watched their activities to foretell events. In the Inca and Tiwanaku empires of South America, birds are depicted transgressing the boundaries between the earthly and underground spiritual realms. Indigenous peoples of the central Andes maintain legends of birds passing to and from metaphysical worlds. The mythical chullumpi bird is said to mark the existence of a portal between such worlds, and to transform itself into a llama. Among the Parsees of India and Iran, and among practitioners of Vajrayana Buddhism who believe in the transmigration of souls in Sikkim, Mongolia, Bhutan and Nepal, sky burial has been practised for centuries. In this ritual, corpses are left exposed for griffon vultures to pick clean. The practice is declining, not least because of the loss of most of the vulture population across South Asia to accidental poisoning by the anti-inflammatory veterinary drug diclofenac.\n\nBirds have sometimes served as religious symbols. In ancient Mesopotamia, doves were prominent animal symbols of Inanna (later known as Ishtar), the goddess of love, sexuality, and war, and, in the ancient Levant, doves were used as symbols for the Canaanite mother goddess Asherah. In ancient Greece, Athena, the goddess of wisdom and patron deity of the city of Athens, had a little owl as her symbol. In Greek iconography, Athena is often shown accompanied by an owl and the owl was used as a symbol of Athens on Athenian coinage. In classical antiquity, doves were sacred to the Greek goddess Aphrodite, who absorbed this association with doves from Inanna-Ishtar. Aphrodite frequently appears with doves in ancient Greek pottery and, during Aphrodite's main festival, the Aphrodisia, her altars would be purified with the blood of a sacrificed dove.\n\nIn Medieval Christian iconography, the cormorant's \"wing-drying\" pose represents the Christian cross, and hence is a figure of Christ. In John Milton's \"Paradise Lost\", on the other hand, the bird's cross-like pose is a travesty of Christ: \"Then up he flew, and... Sat like a cormorant; yet not true life Thereby regained, but sat devising death To them who lived\".\n\nIn mythology, birds were sometimes monsters, like the Roc and the Māori's \"Pouākai\", a giant bird capable of snatching humans. In Persian mythology, the simurgh was a gigantic bird, the first to come into existence, and it nested on the tree of plant life that grew in the great ocean beside the tree of immortality. Its task was to shake the seeds of all the plants out of the tree.\n\nBirds have been depicted throughout the arts from the earliest times to the present, including in painting and sculpture, in literature, in music, in theatre, in traditional dance and ballet, and in film.\n\nBirds have been depicted in paintings, sculptures and other art objects from the earliest times, including in cave paintings.\n\nIn Chinese art, bird-and-flower painting forms one of the three major subjects (the others being landscapes and figures), from the time of the Five Dynasties in the 10th century. Huang Quan created the naturalistic \"xiesheng\" style for bird paintings.\nBirds have long been celebrated in the arts of Japan, including in painting, woodblock printing, cloisonné, ceramics and indeed poetry from the 18th and 19th centuries. Print artists like Utamaro and Hokusai made use of Western and Chinese influences to give a sophisticated effect, while Hiroshige reworked the traditional bird-and-flower genre.\n\nIn modern art, some of the paintings of Joan Miró include \"A tangle of lines and small, colored ideograms suggesting birds, allegorical characters, stars, and animals\". In modern sculpture, Pablo Picasso's 1932 bronze \"Coq\" (Cockerel) is an assemblage of \"spiky, elongated forms.\"\n\nIn public statuary, the Magyars's mythical Turul symbolises national power and nobility, and is represented by many statues in Hungary, including the largest bird statue in the world, on a mountain near Tatabánya.\n\nBirds have been celebrated in poetry since ancient times, when for example the Roman poet Catullus wrote in one of his most famous works about a girl and her pet sparrow in \"Passer, deliciae meae puellae\", \"Sparrow, delight of my girl\".\n\nBirds featured in medieval poetry, for example forming the characters of the 1177 Persian poem \"The Conference of the Birds\", where the birds of the world assemble under the wisest bird, the hoopoe, to decide who is to be their king.\n\nIn English romantic poetry, John Keats's 1819 \"Ode to a Nightingale\" and Percy Bysshe Shelley's 1820 \"To a Skylark\" are popular classics. Bird poems by Gerard Manley Hopkins include \"Sea and Skylark\" and \"The Windhover\" (on the kestrel). More recently, Ted Hughes's 1970 collection of poems about a bird character, \"Crow\", is considered one of his most important works.\n\nBirds have similarly appeared in literature from ancient times. Among Aesop's Fables are The Wolf and the Crane and The Fox and the Stork; these fables, which have analogues in eastern traditions such as the Buddhist \"Javasakuna Jataka\",\nuse birds to imply moral conclusions about human behaviour.\n\nMore recently, birds have appeared in books illustrated by some exceptional artists, producing images that were accurate and beautiful, and that made use of the latest available printing techniques. The wood engraver Thomas Bewick's 1797–1804 \"A History of British Birds\" brought affordable illustrations to the public for the first time, and the book formed in effect the first field guide to birds, while John James Audubon's enormous and impressive images of birds in his 1827–1838 \"Birds of America\" are among the most admired by art critics and by collectors: early editions fetch among the highest prices paid for any printed books. The ornithologist John Gould's bird illustrations, in books such as \"A Century of Birds hitherto unfigured from the Himalaya Mountains\" (1830–1833) with 80 plates, and his 7-volume \"The Birds of Australia\" (1840–1848) with 600 plates, related directly to his research, were both beautiful and scientifically useful.\n\nBirds are popular characters in children's books, which are often handsomely illustrated. Beatrix Potter's 1908 \"The Tale of Jemima Puddle-duck\" created an enduringly popular bird heroine. Other authors followed with many bird characters in books for children of different ages.\n\nIn books for adults, birds may have symbolic or psychological significance. For instance, Paul Gallico's 1940 \"\" was a parable about the regenerative power of friendship in wartime; the goose symbolises both the hero, Rhayader, a wounded artist, and the world wounded by war. T. H. White's 1951 \"The Goshawk\" describes the author's \"monstrous and often cruel battles\" to train his bird of prey, while Helen Macdonald's 2014 \"H is for Hawk\", which references White's book, tells how her obsession with the same species as a falconer helped her through the loss of her father.\n\nIn music, birdsong has influenced composers and musicians in several ways: they can be inspired by birdsong; they can intentionally imitate bird song in a composition, as Vivaldi and Beethoven did, along with many later composers; they can incorporate recordings of birds into their works, first seen in the work of Ottorino Respighi; or as Beatrice Harrison did in 1924 with a nightingale, and David Rothenberg did in 2000 with a laughingthrush, they can duet with birds.\n\nAt least two groups of scientists, namely Luis Felipe Baptista and Robin A. Keister in 2005, and Adam Tierney and colleagues in 2011, have argued that birdsong has a similar structure to music. Baptista and Keister argue that the way birds use variations of rhythm, relationships of musical pitch, and combinations of notes is somewhat musical, perhaps because some birds exploit variation in song to avoid monotony, or mimic other species. Tierney argues that the similar motor constraints on human and avian song drive these to have similar song structures, including \"arch-shaped and descending melodic contours in musical phrases\", long notes at the ends of phrases, and typically small differences in pitch between adjacent notes.\n\nBirds feature as central characters in dance traditions around the world.\nFor example, Goldie's bird of paradise is celebrated in Papua New Guinea in a \"beautiful\" dance by two men who dress in grass skirts with the bird's plumes on the rump; they carry cassowary feathers in their hands and on their armbands, and imitate the bird's calls while they dance. It is performed on important occasions, carrying \"special magic\", and the performers are obliged to prepare for a week, avoiding certain foods, and undergoing a prolonged submergence in a cold stream to prepare their minds. The dance is preceded by a \"magic chant\" to the bird of paradise.\nIn Balinese dance, the cendrawasih dance illustrates the bird-of-paradise's mating rituals.\n\nIn Africa, the Ewe people of Ghana, who were said to have been guided from Dahomey to Ghana by a bird, incorporate the flapping of the bird's wings in dances such as \"Agbadza\", \"Atsiagbekor\", and \"Gakpa\".\n\nIn ballet, Tchaikovsky's classical 1895 \"Swan Lake\" and Igor Stravinsky's 1910 \"The Firebird\" have central bird characters.\n\nIn theatre, Aristophanes's 414 BC comedy \"The Birds\" (Greek: \"Ornithes\") is an acclaimed fantasy with effective mimicry of birds. The play's chorus consists of characters playing many identifiable species, including the kingfisher, turtledove, and sparrowhawk; birds feature as messengers and dancers, and several Athenians are compared to specific birds.\n\nIn film, birds can feature as the major driving force in a story, as in Alfred Hitchcock's acclaimed 1963 \"The Birds\". Loosely based on Daphne du Maurier's story of the same name, it tells the tale of sudden attacks on people by violent flocks of birds. A bird plays the role of an outlet for a person's feelings in Ken Loach's much admired 1969 \"Kes\". The film is based on Barry Hines's 1968 novel \"A Kestrel for a Knave\", and tells the story of a young boy who comes of age by training a kestrel that he has taken from the nest.\n\nBirds feature also in the mass media with iconic animated cartoon characters such as Walt Disney's Donald Duck, Warner Bros.'s Tweety Pie, and Walter Lantz's Woody Woodpecker. The species involved are not always discernible, though Woody has been claimed to be based on the acorn or pileated woodpeckers.\n\nThough human activities have allowed the expansion of a few species, such as the barn swallow and European starling, they have caused population decreases or extinction in many other species. Over a hundred bird species have gone extinct in historical times, including the dodo and the great auk, although the most dramatic human-caused avian extinctions, eradicating an estimated 750–1800 species, occurred during the human colonisation of Melanesian, Polynesian, and Micronesian islands. Many bird populations are declining worldwide, with 1,227 species listed as threatened by BirdLife International and the IUCN in 2009.\n\nThe most commonly cited human threat to birds is habitat loss. Other threats include overhunting; accidental mortality due to collisions with buildings and vehicles; long-line fishing bycatch; pollution (including oil spills and pesticide use), competition; predation; hybridisation from nonnative introduced or invasive species; and climate change. The collection of specimens for taxidermy and eggs from the wild has at times had a serious effect on some species. It is now forbidden in many countries, such as by the British Wildlife and Countryside Act 1981.\n\nEffects are not all negative; for example, wind farms produce renewable energy, helping to mitigate the single greatest threat to birds, climate change. But, especially if wind farms are poorly sited, they may affect bird populations through disturbance, direct or indirect habitat loss, and collisions. Well sited wind farms benefit birds; poorly sited ones can kill many birds in collisions. For example, at the Altamont Pass in California, the golden eagle has been reduced by 80%, and nesting has ceased in the area. Thus, there is a trade-off in the siting of any wind farm.\n\nGovernments and conservation groups work to protect birds, either by passing laws that preserve and restore bird habitat, or by establishing captive populations for reintroductions. Such projects have produced some successes; one study estimated that conservation efforts saved 16 species of bird that would otherwise have gone extinct between 1994 and 2004, including the California condor and Norfolk parakeet. The British Royal Society for the Protection of Birds, founded as the Plumage League in 1889 to protect birds such as the egret from hunting for their plumes, used in fashion, has grown to have over a million members; it has been followed by similar societies in other countries. A more specialised organisation, the Wildfowl & Wetlands Trust founded in 1946, works to conserve waterfowl and their wetland habitats, with projects around the world.\n"}
{"id": "20647020", "url": "https://en.wikipedia.org/wiki?curid=20647020", "title": "IC 1059", "text": "IC 1059\n\nIC 1059 is a galaxy in the constellation Libra. It was discovered in 1893 by Stephane Javelle.\n"}
{"id": "4288891", "url": "https://en.wikipedia.org/wiki?curid=4288891", "title": "Integrated engineering", "text": "Integrated engineering\n\nIntegrated Engineering is a multi-disciplinary, design-project-based engineering degree program.\n\nIntegrated Engineering is a program created to meet the demand for engineers skilled in various disciplines, combining aspects from traditional engineering studies and liberal arts. The demand arises from the current state of industry, where both the products manufactured and the plants which make the products are progressing towards greater diversity and sophistication. Recent studies had shown concern in both Canada and in the United States that engineering graduates were not well-prepared for many of today's multi-disciplinary and project-based workplaces. Several committees have been formed to study this and have published some material. One Canadian study was done by the Canadian Academy of Engineering and two of its main conclusions were:\n\n\nIntegrated Engineers acquire background in core disciplines such as: materials, solid mechanics, fluid mechanics, and systems involving chemical, electro-mechanical, and biological components.\n\nCurrently, the following academic institutions are known to offer Integrated Engineering programs:\n<br>\n<br>Canada\n\nUK\n\nUnited States\n\nGermany\n\nEstonia\n\nKorea\n\nTrinidad and Tobago\n\nIntegrated Engineering originated at the University of Western Ontario in Ontario, Canada and in 2000 the Applied Science Faculty of the University of British Columbia also began a degree program for Integrated Engineering.\n\nIn Canada, the program has been fully accredited by the Canadian Engineering Accreditation Board and engineers are able to obtain a Professional Engineer (P.Eng) Certificate.\n\nIn 1988, the Engineering Council UK, identified the need for routes to qualification for Chartered (Professional) Engineers that:\nmeet the identified needs of industry,\nincrease access to engineering education by more students,\nprovide a balanced curriculum combining the subjects that engineers use most often and directed towards the needs of the majority of engineers.\nThis is the fundamental definition for Integrated Engineering.\n\nThe qualities looked for by industry when recruiting graduates were identified as:\nflexibility and broad education,\nability to understand non engineering functions,\nability to solve problems,\nknowledge of the principles of engineering and ability to apply them in practical situations,\ninformation skills,\nexperience of project work, especially cross linked projects,\nability to work as a member of a team,\npresentation and communication skills.\n\"Engineering Council UK, 1988, An Integrated Engineering Degree Programme.\nEngineering Council UK, 1988, Admissions to Universities - Action to increase the supply of engineers.\"\n\nFollowing open competition for additional funding provided by the UK Department for Technology and Industry, and industrial supporters including British Petroleum, six universities were selected from thirty three applicants. Four \"Pilot Programmes\" were launched at Cardiff University, Nottingham Trent University, Portsmouth University and Sheffield Hallam University.\n\nIn 1989, The Nottingham Trent University (UK) admitted students to first of the Engineering Council's new Integrated Engineering Degree Programme courses. The course was accredited, at the CEng and European Engineer level, by the Institutions of Mechanical Engineers, Electrical Engineers and Manufacturing Engineers.\n\nGeneric engineering programmes are common. Integrated Engineering is distinct through emphasizing the development of personal competencies, especially the ability of students to work within groups. It is design led, and integration of all the subjects of study is a defining characteristic, achieved partly through the medium of project based learning.\n\nFollowing the successful experience at The Nottingham Trent University, Integrated Engineering programmes were established in 1993, at selected universities in Bulgaria and Hungary, with the aid of European Union funding granted under the Tempus Programme.\n\nIn University of Liverpool, the Integrated Engineering Program is accredited by the Institution of Mechanical Engineers and the Institution of Electrical Engineers, and can lead to Chartered Engineer status.\n\nIn Anglia Ruskin University, the Integrated Engineering Program is accredited by the Institution of Engineering and Technology, and can lead to Incorporated Engineer status.\n\nIn the U.S. there are several Integrated engineering education programs.\n\nSouthern Utah University requires its students to pass the Fundamentals of Engineering exam (FE) before they graduate; and received ABET accreditation in 2004 that extended retroactively through October 2003. The graduates are also able to obtain a Professional Engineer (P.E.) license.\n\nMinnesota State University, Mankato has developed a collaborative Integrated engineering program to provide engineering education at MNSCU Community Colleges in the Northern Higher Education District in former Iron Range communities. This partnership allows students to stay near home, while earning a bachelor of science in integrated engineering while focusing on local engineering needs of manufacturers and businesses. As part of the program students are also required to sit for the FE examination prior to graduation and are eligible to sit for the P.E. exam license as the program is also ABET accredited.\n\nIn Germany the [(Baden-Wuerttemberg Cooperative State University (DHBW))] introduced a flexible M.Eng. program in 2015, to fit to the industrial demand for generally educated engineers for Integrated Industry, known as Industry 4.0 in Germany. The graduated school program \"Integrated Engineering\" is administered at the Center for Advances Studies in Heilbronn and requires at least two years professional experience as an engineer for admission.\n\nIn Korea the Department of Integrative Engineering at Chung-Ang University aims to develop human resources that will contribute to building a knowledge infrastructure by effectively responding to rapid educational and social changes.\n\nThe department will focus on developing fundamental and application technologies by realizing future-oriented converging technologies and, through a global network, on strengthening convergence-related competitiveness at the university and national level. To accomplish the goals, based on imaginative education using an innovative system, the department will develop “integrative engineering” people who are equipped with initiative research abilities.\n\nThe Bachelor of Applied Science (B.A.S.c) and Master of Engineering (M.Eng) programs in Utilities Engineering was validated in December 2008 at the University of Trinidad and Tobago. These programs are geared towards the Electrical and Mechanical engineering disciplines that exist within the broad area of Integrated Engineering.\n\nPrior to the introduction of the programs most of the engineers in the utilities sector were specialized in one branch of engineering mainly Electrical or Mechanical. The sector required an engineer who was multi-skilled and versed in both disciplines. The Utilities Engineer therefore performs a wide range of maintenance and operational duties in the following industries:\nProcess Industry,\nElectric Utilities (generation, distribution and efficient utilization),\nTransportation Industry,\nProcessing and Manufacturing Industry,\nWater and sanitation industry,\nMining and Smelting Industry,\nRenewable and Green Energy Industry.\n\n"}
{"id": "54747853", "url": "https://en.wikipedia.org/wiki?curid=54747853", "title": "InterAmerican Network of Academies of Sciences", "text": "InterAmerican Network of Academies of Sciences\n\nThe InterAmerican Network of Academies of Sciences (IANAS) is a regional network made up of National Academy of Sciences from the nations in the Americas. It was founded in 2004 as an organization utilizing science diplomacy to \"build scientific and technological capacities and strengthen relationships among the countries of the Americas as a tool for societal development.\" The creation of the organization was in part inspired by the InterAcademy Panel (IAP), to function as a regional network in the manner of the Network of African Science Academies (previously founded with help from the IAP). Support also came from the Organization of American States and the Inter-American Development Bank for the creation of IANAS. The IAP continues to be its principal source of financial support. Current co-chairs are Juan Asenjo, President of the Chilean Academy of Science and Jeremy McNeil, Foreign Secretary of the Royal Society of Canada.\n\nThe purpose of the network is to connect and develop the scientific community and institutions in each country, and thus to contribute to sustainable development and help build research capacity . Its main objectives are:\n\n1. \"To help improve national scientific capacities, especially as a tool for societal development. This is to be accomplished by strengthening relationships in science and technology among the countries of the Americas.\n\n2. To assist member Academies in building their internal capacity, by creating a forum for the exchange of information and experience.\n\n3. To promote the creation of new Academies in interested countries of the Americas.\n\n4. To contribute to the scientific decision-making processes in the Americas, \"with the goal of promoting prosperity and equity in the hemisphere.\"\n\nIANAS organizes its efforts through programs focused on six key issues : Water, Energy, Science Education, Women for Science, Capacity Building, and Food Security. Each program has a leadership team, and carries out its own activities . Generally each program collects experts from different countries to prepare documents for policy makers in the Americas, often in collaboration with organizations such as UNESCO and the IAP The Network also conducts multi-national surveys, for instance to assist in a study investigating the presence of women in national academies.\n"}
{"id": "31175393", "url": "https://en.wikipedia.org/wiki?curid=31175393", "title": "Level of consciousness (Esotericism)", "text": "Level of consciousness (Esotericism)\n\nConsciousness is a loosely defined concept that addresses the human awareness of both internal and external stimuli. This can refer to spiritual recognition, psychological understanding, medically altered states, or more modern-day concepts of life purpose, satisfaction, and self-actualization.\n\nMost proposals map consciousness in a series of levels, some stages of which are more continuous or complex than others. Movement between stages is often bidirectional depending on internal and external conditions, with each mental ascension precipitating a change in reactivity. In the most basic sense, this alteration might lead to a reduced responsiveness as seen in anesthesiology; more abstract facets of tiered consciousness describe characteristics of profoundness, insight, perception, or understanding.\n\nFirst appearing in the historical records of the ancient Mayan and Incan civilizations, proposals of multiple levels of consciousness have pervaded spiritual, psychological, medical, and moral speculations in both Eastern and Western cultures. Because of occasional and sometimes substantial overlap between hypotheses, there have recently been attempts to combine perspectives to form new models that integrate components of separate viewpoints.\n\nWhich if any of these proposals, models or viewpoints can be verified or falsified is open to question.\n\nAlthough many cultures have incorporated theories of the layered consciousness into their belief structure, particularly for spiritual means before the separation of church and state within any given civilization, the Ancient Mayans were among the first to propose an organized sense of each level, its purpose, and its temporal connection to humankind.\n\nThe pyramid of consciousness has defined Mayan thought since the dawn of its civilization around 2000 BCE. Shamans and priests defined consciousness as an awareness of being aware, commonly referred to as a branch of metacognition. Because consciousness incorporates stimuli from the environment as well as internally, the Mayans believed it to be the most basic form of existence.\n\nThis existence, which they referred to as a loose translation of \"Cosmos\", was made up of nine underworlds, depicted concretely through the nine-storied Pyramid of the Plumed Serpent in Chichen Itza, the Temple of the Jaguar in Tikal, and the Temple of the Inscriptions in Palenque. Within these nine underworlds are a specified \"day\" and \"night\", symbolizing periods of enlightenment, increased consciousness, and a heightened ability to interact with the universe.\n\nA common cause for debate is the exponentially accelerating dates separating each level of consciousness, where each stage occurs roughly 20 times faster than the previous one.\n\nWhereas the Ancient Mayans defined consciousness in almost evolutionary terms, the Inca civilization considered it a progression of awareness and concern for others, similar to the teachings of Siddhartha Gautama.\n\nAlthough historical views of the separation of consciousness into various layers do not exactly mirror modern-day perspectives, many parallels can be gathered from the overarching themes found in Eastern and Western cultures.\n\nMany specific similarities have been drawn between Ancient Incan and historical Eastern views of tiered consciousness. Within most Eastern belief structures is the principle of the Cosmos as a joint entity with human awareness. Many branches stress the importance of AUM, also written Om, as the first sound produced after the world was created. Within Christianity this concept can be likened to the first words of Genesis regarding the holiness of the Word.\n\nThe majority of Eastern perspectives assert that while consciousness originates from the sound of AUM, it has incorporated itself into flesh, which therefore gives humankind the goal of attaining oneness with the universe once more. Unlike Incan tradition, this oneness eliminates the separation of external and internal changes into one general indication of movement from stage to stage, commonly known as the Seven Shamanic Levels of Consciousness.\n\nLike the Seven Shamanic Levels of Consciousness, yoga meditation practices as well as the teachings of Vedanta and Tantra emphasize the importance of self-realization, a concept that has become increasingly popular in Western philosophy after Abraham Maslow's and Carl Rogers's research in Humanistic Psychology.\n\nIn particular, the Advaita Vedanta school of Hindu philosophy has been a topic of extensive study in both Eastern and Western cultures for its tiered depiction of the steps toward attaining self-realization. Unlike the unidirectional nature of Mayan, Inca, and ancient shamanic perspectives, however, this particular belief structure arranges the attainment of oneness with OM through rows and domains, each of which constitutes a fragment of this vibratory sound.\n\nSimilarly, the seven levels of consciousness defined by modern-day OM mantras strive to reach Absolute Reality through the same four realms described in the Advaita Vedanta, with three transitional tiers in between each.\n\n\nThe ancient Indian Vedas texts have lent a comparable view of unified consciousness, with a key difference in the purpose of human ascension from stage to stage. Instead of oneness with the universe, the Vedic vision of consciousness emphasizes the importance of attaining knowledge and pure intelligence.\n\nThe Ananda Sangha movement has evolved following the teachings of the late yogi and guru Paramhansa Yogananda. Compared to the multi-dimensional theories of consciousness in shamanic and OM mantra perspectives, this particular ideological faction stresses simplicity rather than detail.\n\n\nFluctuations in consciousness theories are not particular to Eastern cultures. A surprising degree of overlap can be found within the field of health and social sciences with regard to dulled, standard, and heightened intensities of awareness, both naturally and as a result of injury or disorder.\n\nLike many psychological theories within the particular field of psychoanalysis, one of the most popular theories of consciousness was proposed by Sigmund Freud, who described three facets of the psychic apparatus: the unconscious (id) or instinctual facet, the preconscious (ego) or rational facet, and the conscious (superego) or moral facet.\n\nAlthough not unlike the Vedic vision of consciousness as a form of intelligence, Jean Piaget's theory of cognitive development is not commonly considered a form of knowledge awareness but instead as the evolution of the brain's capacity for thought throughout the human lifespan.\n\nSimilar to previously mentioned psychological views, medical and pathological perspectives often hypothesize tiered consciousness as a result of disease or disorders. The Altered Levels of Consciousness (ALC) theory is one such measure, in which a person's arousability and responsiveness to environmental stimuli are classified by their behavioral response.\n\nAlthough many such ALC tests take place in hospital settings, the primary evaluation of patient alertness is the Glasgow Coma Scale, which separates levels of consciousness from standard conscious awareness to a comatose state.\n\n\nRecent hypotheses have incorporated these ALC theories into the psychopathological study of schizophrenia, suggesting that each altered level of awareness is connected to a degree of suffering or shock experienced by the patient, arguably traversing the Qliphoth in the process. As the situation increases in seriousness, patients will descend to lower levels of consciousness and consequentially lose the capacity to cry, to smile, or to exhibit a wide range of emotions when reacting to the environment.\n\nIn more physiologically based studies, scientists have found that while the reticular formation controls alertness, wakefulness, and arousal in the brain, many mental responses to internal and external stimuli are dictated through signals relayed to and from the thalamus. Propofol and other consciousness-altering drugs are therefore antagonists of thalamus activity, possibly leading to a drug-induced comatose state.\n\nAlthough many of the previously mentioned theories are still widely held today in various groups, beliefs, and areas of study, a majority of commonly accepted perspectives stem from just the past decade. These hypothesized structures of awareness draw from many historical and early eighteenth- or nineteenth-century theories to form an integrated and overarching generalization of consciousness as a means of determining inner and outer recognition of stimuli.\n\nDerived loosely from his philosophy of the Kung Fu system, Philip Holder offers three levels of consciousness that feature distinct differences in the way in which they are reached.\n\nSimilarly, Richard Barrett proposes seven stages of consciousness that progress in a logical order. The progression focuses on “existential” needs directly connected to and dependent on the human condition, all of which are motivating factors for daily interactions.\n\nDr. Bob Günius Gibson, left-handed author of \"Notes on Personal Integration and Health\" and often recognized as a psychic healer, hypothesized the existence of four tiers of extrasensory awareness. Beyond being more applicable to internal states rather than reactions to the external environment, these stages contrast markedly with the previously mentioned modern theories through their emphasis on humankind's immediate interactions. Gibson does not focus on life progression or individual power to move between levels, but rather on momentary instances of personal experience.\n\nTimothy Leary and Robert Anton Wilson proposed the Eight-Circuit Model of Consciousness, a psychologically based theory that unifies various interpretations of main altered states of awareness into a single mega-theory, or a hypothesis' about an already existing hypothesis. In this case, Leary and Wilson state that the altered levels of consciousness defined in medical fields are products of eight differing brain structures within the human nervous system.\n\nThis concept not only connects psychology and the more medically focused studies of neurology and biology, but also incorporates elements of sociology, anthropology, physics, chemistry, and advanced mathematical formulas. Furthermore, critics argue that the inspiration for his theory stems at least indirectly from the Hindu chakra system.\n\nSimilar to Dr. Rondell Gibson's view of a simplified hierarchy of conscious states, Alain Morin describes a four-tiered integration of nine past awareness models, focusing explicitly on the two common aspects underlying each belief structure: the perception of the self in time and the complexity of those self-representations.\n\nIn summary, Morin concludes that from the many concepts discussed above it is near impossible to settle for only one theory without accepting at least a fraction of another. Although each hypothesis has been debated either in scientific or more spiritually focused literature, he states that consciousness is related most directly to the subjective perception of self-recognition and language, both of which are determined by culture and our external environment as a whole.\n\nRobert Allan Monroe became known for his research into altered consciousness and \"out-of-body experience\". His book 1985 \"Far Journeys\" showed numerous levels of consciousness and infinite expansion of consciousness.\n\n\"“The plants exist on levels of consciousness from one through seven. They are on a vibrational rate on the levels one through seven. It is the same pattern.\"\n\n\"Animals exist on the levels of consciousness from eight through fourteen, and when a person attains, when a consciousness attains level fourteen, it can no longer go any higher unless it is willing to change its form of consciousness.\"\n\n\"Levels of consciousness from fifteen through twenty-one are what you call human life on this earth.\"\n\n\"When a person progresses to level of consciousness twenty one, he then has the choice of going higher or staying within the realm of human form, but he cannot go higher unless he is willing to give up human form.”\" \n\nSome analytical psychological methods have presented methodology to engage in deeper levels of inner expansive transcendence, depth that may lead to a reconciliation process. Propositions such as \"active imagination,\" engaging in a conversation with the \"Ego\" and \"Shadow\" for instance. \"Invocation,\" calling upon an archetypal imago to dialogue with, perhaps an image of saint or God, \"Admiring Mature Paragon Examples,\" having conversations with wise old men and women; those who have accessed wisdom, for example, and \"Acting 'As If,'\" getting into character, acting like the character archetype one wishes to emulate.\n\n\n"}
{"id": "5283351", "url": "https://en.wikipedia.org/wiki?curid=5283351", "title": "Lightweight methodology", "text": "Lightweight methodology\n\nA lightweight methodology is a software development method that has only a few rules and practices, or only ones that are easy to follow. In contrast, a complex method with many rules is considered a \"heavyweight methodology.\"\"\n\nExamples of lightweight methodologies include:\n\n\nMost of these lightweight processes emphasize the need to deal with change in requirements and change in environment or technology by being flexible and adaptive.\n"}
{"id": "201178", "url": "https://en.wikipedia.org/wiki?curid=201178", "title": "List of Hudson Bay rivers", "text": "List of Hudson Bay rivers\n\nThis list of Hudson Bay rivers includes the principal rivers draining into the Hudson, James and Ungava bays of the Arctic Ocean. The total surface area of the Hudson Bay watershed is about , with a mean discharge of about . The drainage basin includes parts of five Canadian provinces (Alberta, Saskatchewan, Manitoba, Ontario and Quebec), territories (Northwest Territories and Nunavut) and four American states (Montana, South Dakota, North Dakota and Minnesota). The two principal waterways are the La Grande Rivière, in Québec, and the Nelson River in Manitoba, each with an average waterflow of over .\n\nThe rivers are presented by coastline, clockwise, starting with the George River in northeastern Quebec, just south of Cape Chidley and the entrance to the Atlantic Ocean.\n\n\n\n\n\n\n\n\n"}
{"id": "351136", "url": "https://en.wikipedia.org/wiki?curid=351136", "title": "List of Latin names of mountains", "text": "List of Latin names of mountains\n\nUsers of Neo-Latin have taken the Latin language to places the Romans never went; hence a need arose to make Latin names of mountains that did not exist when Latin was a living language.\n\n\n"}
{"id": "38944097", "url": "https://en.wikipedia.org/wiki?curid=38944097", "title": "List of botanists by author abbreviation (K–L)", "text": "List of botanists by author abbreviation (K–L)\n\nTo find entries for A–J, use the table of contents above.\n\n\n\nTo find entries for M–Z, use the table of contents above.\n"}
{"id": "236335", "url": "https://en.wikipedia.org/wiki?curid=236335", "title": "List of diseases (C)", "text": "List of diseases (C)\n\nThis is a list of diseases starting with the letter \"C\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharcot\nCharcot d\nCharcot–Marie–Tooth disease\n\n\n\n\n\n\n\n\n\n\nChromom–Chromop\nChromos\nChromosoma\nChromosome\nChromosome 1\nChromosome 10 – Chromosome 12\nChromosome 13 – Chromosome 15\nChromosome 16 – Chromosome 1q\nChromosome 2\nChromosome 20 – Chromosome 22\nChromosome 3\nChromosome 4 – Chromosome 5\nChromosome 6 – Chromosome 7\nChromosome 8 – Chromosome 9\nChromosomes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCongenital a – Congenital b\nCongenital c – Congenital g\nCongenital h – Congenital l\nCongenital m – Congenital s\nCongenital t – Congenital v\n\n\n\n\n\n\n\n\n\n\n\nCranioa–Craniom\nCranios–Craniot\n\n\n\n\n\n\n"}
{"id": "7120026", "url": "https://en.wikipedia.org/wiki?curid=7120026", "title": "List of volcanoes in Greece", "text": "List of volcanoes in Greece\n\nThis is a list of active and extinct volcanoes in Greece. \n\n"}
{"id": "20021", "url": "https://en.wikipedia.org/wiki?curid=20021", "title": "Marine biology", "text": "Marine biology\n\nMarine biology is the scientific study of marine life, organisms in the sea. Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy.\n\nA large proportion of all life on Earth lives in the ocean. The exact size of this \"large proportion\" is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) in length. Marine ecology is the study of how marine organisms interact with each other and the environment.\n\nMarine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate. Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.\n\nMany species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.\n\nThe study of marine biology dates back to Aristotle (384–322 BC), who made many observations of life in the sea around Lesbos, laying the foundation for many future discoveries. In 1768, Samuel Gottlieb Gmelin (1744–1774) published the \"Historia Fucorum\", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves. The British naturalist Edward Forbes (1815–1854) is generally regarded as the founder of the science of marine biology. The pace of oceanographic and marine biology studies quickly accelerated during the course of the 19th century.\n\nThe observations made in the first studies of marine biology fueled the age of discovery and exploration that followed. During this time, a vast amount of knowledge was gained about the life that exists in the oceans of the world. Many voyages contributed significantly to this pool of knowledge. Among the most significant were the voyages of where Charles Darwin came up with his theories of evolution and on the formation of coral reefs. Another important expedition was undertaken by HMS \"Challenger\", where findings were made of unexpectedly high species diversity among fauna stimulating much theorizing by population ecologists on how such varieties of life could be maintained in what was thought to be such a hostile environment. This era was important for the history of marine biology but naturalists were still limited in their studies because they lacked technology that would allow them to adequately examine species that lived in deep parts of the oceans.\n\nThe creation of marine laboratories was important because it allowed marine biologists to conduct research and process their specimens from expeditions. The oldest marine laboratory in the world, Station biologique de Roscoff, was established in France in 1872. In the United States, the Scripps Institution of Oceanography dates back to 1903, while the prominent Woods Hole Oceanographic Institute was founded in 1930. The development of technology such as sound navigation ranging, scuba diving gear, submersibles and remotely operated vehicles allowed marine biologists to discover and explore life in deep oceans that was once thought to not exist.\n\nAs inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system. Microbes are responsible for virtually all the photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.\n\nMicroscopic life undersea is incredibly diverse and still poorly understood. For example, the role of viruses in marine ecosystems is barely being explored even in the beginning of the 21st century.\n\nThe role of phytoplankton is better understood due to their critical position as the most numerous primary producers on Earth. Phytoplankton are categorized into cyanobacteria (also called blue-green algae/bacteria), various types of algae (red, green, brown, and yellow-green), diatoms, dinoflagellates, euglenoids, coccolithophorids, cryptomonads, chrysophytes, chlorophytes, prasinophytes, and silicoflagellates.\n\nZooplankton tend to be somewhat larger, and not all are microscopic. Many Protozoa are zooplankton, including dinoflagellates, zooflagellates, foraminiferans, and radiolarians. Some of these (such as dinoflagellates) are also phytoplankton; the distinction between plants and animals often breaks down in very small organisms. Other zooplankton include cnidarians, ctenophores, chaetognaths, molluscs, arthropods, urochordates, and annelids such as polychaetes. Many larger animals begin their life as zooplankton before they become large enough to take their familiar forms. Two examples are fish larvae and sea stars (also called starfish).\n\nMicroscopic algae and plants provide important habitats for life, sometimes acting as hiding places for larval forms of larger fish and foraging places for invertebrates.\n\nAlgal life is widespread and very diverse under the ocean. Microscopic photosynthetic algae contribute a larger proportion of the world's photosynthetic output than all the terrestrial forests combined. Most of the niche occupied by sub plants on land is actually occupied by macroscopic algae in the ocean, such as \"Sargassum\" and kelp, which are commonly known as seaweeds that create kelp forests.\n\nPlants that survive in the sea are often found in shallow waters, such as the seagrasses (examples of which are eelgrass, \"Zostera\", and turtle grass, \"Thalassia\"). These plants have adapted to the high salinity of the ocean environment. The intertidal zone is also a good place to find plant life in the sea, where mangroves or cordgrass or beach grass might grow. \n\nAs on land, invertebrates make up a huge portion of all life in the sea. Invertebrate sea life includes Cnidaria such as jellyfish and sea anemones; Ctenophora; sea worms including the phyla Platyhelminthes, Nemertea, Annelida, Sipuncula, Echiura, Chaetognatha, and Phoronida; Mollusca including shellfish, squid, octopus; Arthropoda including Chelicerata and Crustacea; Porifera; Bryozoa; Echinodermata including starfish; and Urochordata including sea squirts or tunicates. Invertebrates have no backbone. There are over a million species.\n\nOver 1500 species of fungi are known from marine environments. These are parasitic on marine algae or animals, or are saprobes on algae, corals, protozoan cysts, sea grasses, wood and other substrata, and can also be found in sea foam. Spores of many species have special appendages which facilitate attachment to the substratum. A very diverse range of unusual secondary metabolites is produced by marine fungi.\n\nA reported 33,400 species of fish, including bony and cartilaginous fish, had been described by 2016, more than all other vertebrates combined. About 60% of fish species live in saltwater.\n\nReptiles which inhabit or frequent the sea include sea turtles, sea snakes, terrapins, the marine iguana, and the saltwater crocodile. Most extant marine reptiles, except for some sea snakes, are oviparous and need to return to land to lay their eggs. Thus most species, excepting sea turtles, spend most of their lives on or near land rather than in the ocean. Despite their marine adaptations, most sea snakes prefer shallow waters nearby land, around islands, especially waters that are somewhat sheltered, as well as near estuaries. Some extinct marine reptiles, such as ichthyosaurs, evolved to be viviparous and had no requirement to return to land.\n\nBirds adapted to living in the marine environment are often called seabirds. Examples include albatross, penguins, gannets, and auks. Although they spend most of their lives in the ocean, species such as gulls can often be found thousands of miles inland.\n\nThere are five main types of marine mammals, namely cetaceans (toothed whales and baleen whales); sirenians such as manatees; pinnipeds including seals and the walrus; sea otters; and the \npolar bear. All are air-breathing, and while some such as the sperm whale can dive for prolonged periods, all must return to the surface to breathe.\n\nMarine habitats can be divided into coastal and open ocean habitats. Coastal habitats are found in the area that extends from the shoreline to the edge of the continental shelf. Most marine life is found in coastal habitats, even though the shelf area occupies only seven percent of the total ocean area. Open ocean habitats are found in the deep ocean beyond the edge of the continental shelf. Alternatively, marine habitats can be divided into pelagic and demersal habitats. Pelagic habitats are found near the surface or in the open water column, away from the bottom of the ocean and affected by ocean currents, while demersal habitats are near or on the bottom. Marine habitats can be modified by their inhabitants. Some marine organisms, like corals, kelp and sea grasses, are ecosystem engineers which reshape the marine environment to the point where they create further habitat for other organisms.\n\nIntertidal zones, the areas that are close to the shore, are constantly being exposed and covered by the ocean's tides. A huge array of life can be found within this zone. Shore habitats span from the upper intertidal zones to the area where land vegetation takes prominence. It can be underwater anywhere from daily to very infrequently. Many species here are scavengers, living off of sea life that is washed up on the shore. Many land animals also make much use of the shore and intertidal habitats. A subgroup of organisms in this habitat bores and grinds exposed rock through the process of bioerosion.\n\nEstuaries are also near shore and influenced by the tides. An estuary is a partially enclosed coastal body of water with one or more rivers or streams flowing into it and with a free connection to the open sea. Estuaries form a transition zone between freshwater river environments and saltwater maritime environments. They are subject both to marine influences—such as tides, waves, and the influx of saline water—and to riverine influences—such as flows of fresh water and sediment. The shifting flows of both sea water and fresh water provide high levels of nutrients both in the water column and in sediment, making estuaries among the most productive natural habitats in the world.\n\nReefs comprise some of the densest and most diverse habitats in the world. The best-known types of reefs are tropical coral reefs which exist in most tropical waters; however, reefs can also exist in cold water. Reefs are built up by corals and other calcium-depositing animals, usually on top of a rocky outcrop on the ocean floor. Reefs can also grow on other surfaces, which has made it possible to create artificial reefs. Coral reefs also support a huge community of life, including the corals themselves, their symbiotic zooxanthellae, tropical fish and many other organisms.\n\nMuch attention in marine biology is focused on coral reefs and the El Niño weather phenomenon. In 1998, coral reefs experienced the most severe mass bleaching events on record, when vast expanses of reefs across the world died because sea surface temperatures rose well above normal. Some reefs are recovering, but scientists say that between 50% and 70% of the world's coral reefs are now endangered and predict that global warming could exacerbate this trend.\n\nThe open ocean is relatively unproductive because of a lack of nutrients, yet because it is so vast, in total it produces the most primary productivity. The open ocean is separated into different zones, and the different zones each have different ecologies. Zones which vary according to their depth include the epipelagic, mesopelagic, bathypelagic, abyssopelagic, and hadopelagic zones. Zones which vary by the amount of light they receive include the photic and aphotic zones. Much of the aphotic zone's energy is supplied by the open ocean in the form of detritus.\n\nThe deepest recorded oceanic trench measured to date is the Mariana Trench, near the Philippines, in the Pacific Ocean at . At such depths, water pressure is extreme and there is no sunlight, but some life still exists. A white flatfish, a shrimp and a jellyfish were seen by the American crew of the bathyscaphe \"Trieste\" when it dove to the bottom in 1960. In general, the deep sea is considered to start at the aphotic zone, the point where sunlight loses its power of transference through the water. Many life forms that live at these depths have the ability to create their own light known as bio-luminescence. Marine life also flourishes around seamounts that rise from the depths, where fish and other sea life congregate to spawn and feed. Hydrothermal vents along the mid-ocean ridge spreading centers act as oases, as do their opposites, cold seeps. Such places support unique biomes and many new microbes and other lifeforms have been discovered at these locations .\n\nThe marine ecosystem is large, and thus there are many sub-fields of marine biology. Most involve studying specializations of particular animal groups, such as phycology, invertebrate zoology and ichthyology. Other subfields study the physical effects of continual immersion in sea water and the ocean in general, adaptation to a salty environment, and the effects of changing various oceanic properties on marine life. A subfield of marine biology studies the relationships between oceans and ocean life, and global warming and environmental issues (such as carbon dioxide displacement). Recent marine biotechnology has focused largely on marine biomolecules, especially proteins, that may have uses in medicine or engineering. Marine environments are the home to many exotic biological materials that may inspire biomimetic materials.\n\nMarine biology is a branch of biology. It is closely linked to oceanography and may be regarded as a sub-field of marine science. It also encompasses many ideas from ecology. Fisheries science and marine conservation can be considered partial offshoots of marine biology (as well as environmental studies). Marine Chemistry, Physical oceanography and Atmospheric sciences are closely related to this field.\n\nAn active research topic in marine biology is to discover and map the life cycles of various species and where they spend their time. Technologies that aid in this discovery include pop-up satellite archival tags, acoustic tags, and a variety of other data loggers. Marine biologists study how the ocean currents, tides and many other oceanic factors affect ocean life forms, including their growth, distribution and well-being. This has only recently become technically feasible with advances in GPS and newer underwater visual devices.\n\nMost ocean life breeds in specific places, nests or not in others, spends time as juveniles in still others, and in maturity in yet others. Scientists know little about where many species spend different parts of their life cycles especially in the infant and juvenile years. For example, it is still largely unknown where juvenile sea turtles and some year-1 sharks travel. Recent advances in underwater tracking devices are illuminating what we know about marine organisms that live at great Ocean depths. The information that pop-up satellite archival tags give aids in certain time of the year fishing closures and development of a marine protected area. This data is important to both scientists and fishermen because they are discovering that by restricting commercial fishing in one small area they can have a large impact in maintaining a healthy fish population in a much larger area.\n\n\n"}
{"id": "43482230", "url": "https://en.wikipedia.org/wiki?curid=43482230", "title": "Milestone fee", "text": "Milestone fee\n\nIn contracts, research and development, research administration, and project management, milestone fees are fees for the development of a deliverable to be completed under the applicable statement of work. Milestone fees do not include sales tax or any other taxes.\n"}
{"id": "1735545", "url": "https://en.wikipedia.org/wiki?curid=1735545", "title": "Moussaieff Red Diamond", "text": "Moussaieff Red Diamond\n\nThe Moussaieff Red Diamond (formerly known as the Red Shield Diamond) is a diamond measuring 5.11 carats (1.022 g) with a triangular brilliant cut (sometimes called a trillion or a trilliant cut), rated in color as Fancy Red by the Gemological Institute of America. It is the world's largest known red diamond, the rarest of them all. \n\nThe Moussaieff Red was discovered in the 1990s by a Brazilian farmer in the Abaetezinho river in 1990, in a region known as Alto Paranaiba. The rough stone weighed 13.9 carats (2.78 g). The diamond was purchased and cut by the William Goldberg Diamond Corp., where it went by its original name, the Red Shield. It was purchased in 2001 or 2002 by Shlomo Moussaieff, an Israeli-born jewelry dealer in London, and is currently owned by Moussaieff Jewellers Ltd.\n\nThe Moussaieff Red was displayed in 2003 as part of the Smithsonian Institution's \"The Splendor of Diamonds\" exhibit, alongside The De Beers Millennium Star and The Heart of Eternity.\n\n"}
{"id": "47290570", "url": "https://en.wikipedia.org/wiki?curid=47290570", "title": "National Museum of Natural History, Colombo", "text": "National Museum of Natural History, Colombo\n\nThe National Museum of Natural History is a museum that covers the natural heritage of Sri Lanka. The museum is located closer to the National Museum of Colombo. It was established on September 23, 1986 and became only one museum in Sri Lanka that represents natural history and natural heritage.\n\nThe National Museum of Natural History exhibits rare and threatened with extinction such as natural heritage of plant and animal species endemic to Sri Lanka, over 5,000 specimens of mammals, jurassic period indigenous fossils and various kinds of geological rocks.\n\nThe museum is open from 8.30 AM to 5.00 PM and close on public holidays.\n\n"}
{"id": "39319254", "url": "https://en.wikipedia.org/wiki?curid=39319254", "title": "Odin-OSIRIS", "text": "Odin-OSIRIS\n\nOSIRIS (Optical Spectrograph and InfraRed Imager System) is an instrument that measures vertical profiles of spectrally dispersed, limb scattered sunlight from the upper troposphere into the lower mesosphere. OSIRIS is one of two instruments on the Odin satellite, launched February, 2001 (the other instrument being a sub-mm radiometer) into a sun-synchronous, 6 pm/6 am local time orbit at 600 km. This restricts OSIRIS sunlit observations to the Northern hemisphere in May, June, July August and the Southern hemisphere in November, December, January and February. Global coverage from 82°S to 82°N occurs on the months adjoining the equinoxes. OSIRIS measurements began November, 2001 and continue to the present.\n\n\nThe OSIRIS spectrograph measures from 274 nm to 810 nm with a single line of sight that is scanned through a range of tangent altitudes. Each scan typically ranges from 7 km to 65 km and takes 40 seconds to acquire. The measurements are used to produce height profiles of O3, NO2, and stratospheric aerosols.\n\nThe Odin satellite was operated until June 2007 as a joint mission between astronomy and aeronomy disciplines. 50% of the total observation time was dedicated to each discipline where time was split into 1 day segments. Odin has operated as a purely aeronomy mission since June, 2007, and continues to the present, with almost complete coverage.\n\nOSIRIS is a Canadian instrument, operated by the Canadian Space Agency. The mission PI is Dr. Doug Degenstein, University of Saskatchewan. Odin is operated by the Swedish Space Corporation, with funds from the European Space Agency as a Third Party Mission.\n\nOSIRIS data is publicly available on the Odin-OSIRIS website, odin-osiris.usask.ca.\n\nLevel 0: Radiance Data\n\nLevel 1: Calibrated Radiance Data\n\nLevel 2: Number Density and Volume Mixing Ratio (VMR) profiles as a function of altitude for O3, Stratospheric Aerosols and NO2.\n\n\n\n"}
{"id": "4816671", "url": "https://en.wikipedia.org/wiki?curid=4816671", "title": "Pelagonius", "text": "Pelagonius\n\nPelagonius (4th century AD) was an influential Latin writer on veterinary medicine, especially on horses. He is one of the many authors whose work was compiled and preserved in the \"Hippiatrica\". Remains of his texts still exist in Latin and Greek. One of his sources was Columella. He was used by Vegetius.\n\nThere is an edition of his texts in Teubner (1980), \"De veterinaria medicina\", by K.D. Fischer.\n"}
{"id": "1835316", "url": "https://en.wikipedia.org/wiki?curid=1835316", "title": "Philosophical Transactions of the Royal Society", "text": "Philosophical Transactions of the Royal Society\n\nPhilosophical Transactions, titled Philosophical Transactions of the Royal Society (often abbreviated as Phil. Trans.) from 1776, is a scientific journal published by the Royal Society. In its earliest days, it was a private venture of the Royal Society's secretary. It became an official society publication in 1752. It was established in 1665, making it the first journal in the world exclusively devoted to science, and therefore also the world's longest-running scientific journal. The use of the word \"philosophical\" in the title refers to natural philosophy, which was the equivalent of what would now be generally called \"science\".\n\nIn 1887 the journal expanded and divided into two separate publications, one serving the physical sciences (\"\") and the other focusing on the life sciences (\"\"). Both journals now publish themed issues and issues resulting from papers presented at the Discussion Meetings of the Royal Society. Primary research articles are published in the sister journals \"Proceedings of the Royal Society\", \"Biology Letters\", \"Journal of the Royal Society Interface\", and \"Interface Focus\".\n\nThe first issue, published in London on 6 March 1665, was edited and published by the Society's first secretary, Henry Oldenburg, four-and-a-half years after the Royal Society was founded. The full title of the journal, as given by Oldenburg, was \"Philosophical Transactions, Giving some Account of the present Undertakings, Studies, and Labours of the Ingenious in many considerable parts of the World\". The society's council minutes dated 1 March 1664 (in the Julian calendar; equivalent to 11 March 1665 in the modern Gregorian system) ordered that \"the Philosophical Transactions, to be composed by Mr Oldenburg, be printed the first Munday of every month, if he have sufficient matter for it, and that that tract be licensed by the Council of this Society, being first revised by some Members of the same\". Oldenburg published the journal at his own personal expense and seems to have entered into an agreement with the society's council allowing him to keep any resulting profits. He was to be disappointed, however, since the journal performed poorly from a financial point of view during his lifetime, just about covering the rent on his house in Piccadilly. Oldenburg put out 136 issues of the \"Transactions\" before his death in 1677.\n\nThe familiar functions of the scientific journal – registration (date stamping and provenance), certification (peer review), dissemination and archiving − were introduced at inception by \"Philosophical Transactions\". The beginnings of these ideas can be traced in a series of letters from Oldenburg to Robert Boyle:\n\n\nThe printed journal replaced much of Oldenburg's letter-writing to correspondents, at least on scientific matters, and as such can be seen as a labour-saving device. Oldenburg also described his journal as \"one of these philosophical commonplace books\", indicating his intention to produce a collective notebook between scientists.\n\nIssue 1 contained such articles as: an account of the improvement of optic glasses; the first report on the Great Red Spot of Jupiter; a prediction on the motion of a recent comet (probably an Oort cloud object); a review of Robert Boyle's \"Experimental History of Cold\"; Robert Boyle's own report of a deformed calf; \"A report of a peculiar lead-ore from Germany, and the use thereof\"; \"Of an Hungarian Bolus, of the Same Effect with the Bolus Armenus\"; \"Of the New American Whale-Fishing about the Bermudas\", and \"A Narrative Concerning the Success of Pendulum-Watches at Sea for the Longitudes\". The final article of the issue concerned \"The Character, Lately Published beyond the Seas, of an Eminent Person, not Long Since Dead at Tholouse, Where He Was a Councellor of Parliament\". The eminent person in question was Pierre de Fermat, although the issue failed to mention his last theorem.\n\nOldenburg referred to himself as the \"compiler\" and sometimes \"Author\" of the \"Transactions\", and always claimed that the journal was entirely his sole enterprise – although with the Society's imprimatur and containing reports on experiments carried out and initially communicated by of many of its Fellows, many readers saw the journal as an official organ of the Society. It has been argued that Oldenburg benefitted from this ambiguity, retaining both real and perceived independence (giving the publication an air of authenticity) and the prospect of monetary gain, while simultaneously enjoying the credibility afforded by the association. The Society also enjoyed the benefits of ambiguity: it was able to communicate advances in natural philosophy, undertaken largely in its own name, without the worry that it was directly responsible for its content. In the aftermath of the Interregnum, the potential for censorship was very real. Certainly the tone of the early volumes was set by Oldenburg, who often related things he was told by his contacts, translated letters and manuscripts from other languages, and reviewed books, always being sure to indicate the provenance of his material and even to use this to impress the reader.\n\nBy reporting ongoing and often unfinished scientific work that may otherwise have not been reported, the journal had a central function of being a scientific news service. At the time of \"Philosophical Transactions\"' foundation, print was heavily regulated, and there was no such thing as a free press. In fact, the first English newspaper, \"The London Gazette\" (which was an official organ of government and therefore seen as sanitized), did not appear until after \"Philosophical Transactions\" in the same year.\n\nOldenburg's compulsive letter writing to foreign correspondents led to him being suspected of being a spy for the Dutch and interned in the Tower of London in 1667. A rival took the opportunity to publish a pirate issue of \"Philosophical Transactions\", with the pretense of it being Issue 27. Oldenburg repudiated the issue by publishing the real 27 upon his release.\n\nUpon Oldenburg's death, following a brief hiatus, the position of Editor was passed down through successive secretaries of the Society as an unofficial responsibility and at their own expense. Robert Hooke changed the name of the journal to \"Philosophical Collections\" in 1679 – a name that remained until 1682, when it changed back. The position of editor was sometimes held jointly and included William Musgrave (Nos 167 to 178) and Robert Plot (Nos 144 to 178).\n\nBy the mid-eighteenth century, the most notable editors, besides Oldenburg, were Hans Sloane, James Jurin and Cromwell Mortimer. In virtually all cases the journal was edited by the serving secretary of the society (and occasionally by both secretaries working in tandem). These editor-secretaries carried the financial burden of publishing the \"Philosophical Transactions\". By the early 1750s, the \"Philosophical Transactions\" came under attack, most prominently by John Hill, an actor, apothecary, and naturalist. Hill published three works in two years, ridiculing the Royal Society and the \"Philosophical Transactions\". The Society was quick to point out that it was not officially responsible for the journal. Yet, in 1752 the Society took over the \"Philosophical Transactions\". The journal would henceforth be published \"for the sole use and benefit of this Society\"; it would be financially carried by the members' subscriptions; and it would be edited by the Committee of Papers.\n\nAfter the takeover of the journal by the Royal Society, management decisions including negotiating with printers and booksellers, were still the task of one of the Secretaries—but editorial control was exercised through the Committee of Papers. The Committee mostly based its judgements on which papers to publish and which to decline on the 300 to 500-word abstracts of papers read during its weekly meetings. But the members could, if they desired, consult the original paper in full. Once the decision to print had been taken, the paper appeared in the volume for that year. It would feature the author's name, the name of the Fellow who had communicated the paper to the Society, and the date on which it was read. The Royal Society covered paper, engraving and printing costs. The Society found the journal to be a money-losing proposition: it cost, on average, upwards of £300 annually to produce, of which they seldom recouped more than £150. Because two-fifths of the copies were distributed for free to the journal's natural market, sales were generally slow, and although back issues sold out gradually it would usually be ten years or more before there were fewer than 100 left of any given print run.\n\nDuring the Presidency of Joseph Banks the work of the Committee of Papers continued fairly efficiently, with the President himself in frequent attendance. There was a number of ways in which the President and Secretaries could bypass or subvert the Royal Society's publishing procedures. Papers could be prevented from reaching the Committee by not allowing them to be read in the first place. Also—though papers were rarely subjected to formal review—there is evidence of editorial intervention, with Banks himself or a trusted deputy proposing cuts or emendations to particular contributions. Publishing in the \"Philosophical Transactions\" carried a high degree of prestige and Banks himself attributed an attempt to unseat him, relatively early in his Presidency, to the envy of authors whose papers had been rejected from the journal.\n\n\"Transactions\" continued steadily through the turn of the century and into the 1820s. In the late 1820s and early 1830s, a movement to reform the Royal Society rose. The reformers felt that the scientific character of the Society had been undermined by the admission of too many gentleman dilettantes under Banks. In proposing a more limited membership, to protect the Society's reputation, they also argued for systematic, expert evaluation of papers for \"Transactions\" by named referees.\n\nSectional Committees, each with responsibility for a particular group of disciplines, were initially set up in the 1830s to adjudicate the award of George IV's Royal Medals. But individual members of these committees were soon put to work reporting on and evaluating papers submitted to the Royal Society. These evaluations began to be used as the basis of recommendations to the Committee of Papers, who would then rubber-stamp decisions made by the Sectional Committees. Despite its flaws – it was inconsistent in its application and not free of abuses – this system remained at the heart of the Society's procedures for publishing until 1847, when the Sectional Committees were dissolved. However, the practice of sending most papers out for review remained.\n\nDuring the 1850s, the cost of the \"Transactions\" to the Society was increasing again (and would keep doing so for the rest of the century); illustrations were always the largest outgoing. Illustrations had been a natural and essential aspect of the scientific periodical since the later seventeenth century. Engravings (cut into metal plates) were used for detailed illustrations, particularly where realism was required; while wood-cuts (and, from the early nineteenth century, wood-engravings) were used for diagrams, as they could be easily combined with letterpress.\n\nBy the mid-1850s, the \"Philosophical Transactions\" was seen as a drain on the Society's finances and the treasurer, Edward Sabine, urged the Committee of Papers to restrict the length and number of papers published in the journal. In 1852, for example, the amount expended on the \"Transactions\" was £1094, but only £276 of this was offset by sales income. Sabine felt this was more than the Society could comfortably sustain. The print run of the journal was 1000 copies. Around 500 of these went to the fellowship, in return for their membership dues, and since authors now received up to 150 off-prints for free, to circulate through their personal networks, the demand for the \"Transactions\" through the book trade must have been limited. The concerns with cost eventually led to a change in printer in 1877 from Taylor & Francis to Harrison & Sons – the latter was a larger commercial printer, able to offer the Society a more financially viable contract, although it was less experienced in printing scientific works.\nWhile expenditure was a worry for the Treasurer, as Secretary (from 1854), George Gabriel Stokes was preoccupied with the actual content of the \"Transactions\" and his extensive correspondence with authors over his thirty-one-years as Secretary took up most of his time beyond his duties as Lucasian Professor at Cambridge. Stokes was paramount in establishing a more formalized refereeing process at the Royal Society. It was not until Stokes's Presidency ended, in 1890, that his influence over the journal diminished. The introduction of fixed terms for Society officers precluded subsequent editors from taking on Stokes's mantle, and meant that the Society operated its editorial practices more collectively than it had done since the mechanisms for it were established in 1752.\n\nBy the mid-nineteenth century, the procedure of getting a paper published in the \"Transactions\" still relied on the reading of papers by a Fellow. Many papers were sent immediately for printing in abstract form in \"Proceedings of the Royal Society\". But those which were being considered for printing in full in \"Transactions\" were usually sent to two referees for comment before the final decision was made by the Committee of Papers. During Stokes's time, authors were given the opportunity to discuss their paper at length with him before, during and after its official submission to the Committee of Papers.\n\nIn 1887, the \"Transactions\" split into series \"A\" and \"B\", dealing with the physical and biological sciences respectively. In 1897, the model of collective responsibility for the editing of the \"Transactions\" was emphasized by the re-establishment of the Sectional Committees. The six sectional committees covered mathematics, botany, zoology, physiology, geology, and (together) chemistry and physics, and were composed of Fellows of the Society with relevant expertise. The Sectional Committees took on the task of managing the refereeing process after papers had been read before the Society. Referees were usually Fellows, except in a small number of cases where the topic was beyond the knowledge of the fellowship (or at least, of those willing to referee). The Sectional Committees communicated referee reports to authors; and sent reports to the Committee of Papers for final sanction. The Sectional Committees were intended to reduce the burden on the Secretaries and Council. Consequently, the Secretary in the 1890s, Arthur Rucker, no longer coordinated the refereeing of papers, nor did he generally correspond extensively with authors about their papers as Stokes had done. However, he continued to be the first port of call for authors submitting papers.\n\nAuthors were increasingly expected to submit manuscripts in a standardized format and style. From 1896, they were encouraged to submit typed papers on foolscap-folio-sized paper to lighten the work of getting papers ready for printing, and to reduce the chance of error in the process. A publishable paper now had to present its information in an appropriate manner, as well as being of remarkable scientific interest. For a brief period between 1907 and 1914, authors were under even more pressure to conform to the society's expectations, due to a decision to discuss cost estimates of candidate papers alongside referees' reports. The committees could require authors to reduce the number of illustrations or tables or, indeed, the overall length of the paper, as a condition of acceptance. It was hoped that this policy would reduce the still-rising costs of production, which had reached £1747 in 1906; but the effect appears to have been negligible, and the cost estimates ceased to be routine practice after 1914.\n\nIt was only after the Second World War that the Society's concerns about the cost of its journals were finally allayed. There had been a one-off surplus in 1932, but it was only from 1948 that the \"Transactions\" began regularly to end the year in surplus. That year, despite a three-fold increase in production costs (it was a bumper year for papers), there was a surplus of almost £400. Part of the post-war financial success of the \"Transactions\" was due to the rising subscriptions received, and a growing number of subscriptions from British and international institutions, including universities, industry, and government; this was at the same time as private subscriptions, outside of fellows, were non-existent. By the early 1970s, institutional subscription was the main channel of income from publication sales for the society. In 1970–1971, 43,760 copies of \"Transactions\" were sold, of which casual purchasers accounted for only 2070 copies.\n\nAll of the Society's publications now had a substantial international circulation; in 1973, for example, just 11% of institutional subscriptions were from the United Kingdom; 50% were from the United States. Contributions, however, were still mostly from British authors: 69% of Royal Society authors were from the United Kingdom in 1974. A Publications Policy Committee suggested that more overseas scientists could be encouraged to submit papers if the requirement to have papers communicated by Fellows was dropped. This did not happen until 1990. There was also a suggestion to create a \"C\" journal for molecular sciences to attract more authors in that area, but the idea never materialized. The conclusion in 1973 was a general appeal to encourage more British scientists (whether Fellows or not) to publish papers with the Society and to pass on the message to their overseas colleagues; by the early 2000s, the proportion of non-UK authors had risen to around a half; and by 2017 it had passed 80%.\n\nAs the twentieth century came to a close, the editing of the \"Transactions\" and the Society's other journals became more professional with the employment of a growing in-house staff of editors, designers and marketers. In 1968 there were about eleven staff in the Publishing Section; by 1990, the number had risen to twenty-two. The editorial processes were also transformed. In 1968 the Sectional Committees had been abolished (again). Instead, the secretaries, Harrie Massey (physicist) and Bernard Katz (physiologist), were each assigned a group of Fellows to act as Associate Editors for each series (\"A\" and \"B\") of the \"Transactions\". The role of the Committee of Papers was abolished in 1989 and since 1990 two Fellows (rather than the Secretaries) have acted as the Editors with assistance from associate editors. The editors serve on the Publishing Board, established in 1997 to monitor publishing and report to the Council. In the 1990s, as these changes to the publishing and editorial teams were implemented, the Publishing Section acquired its first computer for administration; the \"Transactions\" were first published online in 1997.\n\nOver the centuries, many important scientific discoveries have been published in the \"Philosophical Transactions\". Famous contributing authors include:\n\nIn July 2011 programmer Greg Maxwell released through \"The Pirate Bay\" the nearly 19 thousand articles that had been published before 1923 and were therefore in the public domain in the United States, to support Aaron Swartz in his case. The articles had been digitized for the Royal Society by JSTOR for a cost of less than US$100,000 and public access to them was restricted through a paywall.\n\nIn August 2011, users uploaded over 18,500 articles to the collections of the Internet Archive. The collection received 50 thousands views per month by November 2011.\n\nIn October of the same year, the Royal Society released for free the full text all its articles prior to 1941, but denied that this decision had been influenced by Maxwell's actions.\n\nIn 2017, the Royal Society launched a completely re-digitised version of the complete journal archive back to 1665 in high resolution and with enhanced metadata. All the out of copyright material is completely free to access without a login.\n\n\n"}
{"id": "12390747", "url": "https://en.wikipedia.org/wiki?curid=12390747", "title": "Pleurodema somuncurense", "text": "Pleurodema somuncurense\n\nPleurodema somuncurense (the Somuncura frog or El Rincon stream frog, in Spanish \"rana de Somuncura\") is a species of frog in the family Leptodactylidae. It is endemic to the Somuncura Plateau in Patagonia, Argentina.\n\nFemales reach in total length. They are slender with fairly small head and large protruding, gold-coloured eyes. Fingers and toes are long and slender, with the toes being about one-third webbed. Eyes have two symmetrical rounded structures on the centre of the upper and lower border of the iris. The skin is smooth. Colouration is bright yellowish-brown on the upper surfaces of the head, body and legs. There are irregular dark spots across the back, and wavy dark reticulated lines on the sides of the body and backs of the thighs. There is a characteristic yellowish stripe that runs centrally down the top of the head and half of the back. The belly is purplish-yellow with dark grey reticulated spots. The lower surface of the thighs is purplish-rose and bears faint grey reticulated spots.\n\n\"Pleurodema somuncurense\" is a fully aquatic frog that inhabits geothermal springs and streams. It is threatened by predation by introduced rainbow trout and by habitat loss from canalization of spring water. Also livestock farming has negative impacts through overgrazing and chemical pollution.\n"}
{"id": "2014452", "url": "https://en.wikipedia.org/wiki?curid=2014452", "title": "Project Genoa", "text": "Project Genoa\n\nProject Genoa was a software project commissioned by the United States' DARPA which was designed to analyze large amounts of data and metadata to help human analysts counter terrorism.\n\nGenoa's primary function was intelligence analysis in order to assist human analysts.\n\nThe program was designed to support both top-down and bottom-up approaches; a policy maker could hypothesize a possible attack and use Genoa to look for supporting evidence of such a plot, or it would compile pieces of intelligence into a diagram and suggest possible outcomes. Human analysts would then be able to modify the diagram to test various cases.\n\nCompanies such as Integral Visuals, Saffron Technology, and Syntek Technologies were involved in Genoa's development. It cost a total of $42 million to complete the program.\n\nGenoa was conceived in late 1995 by retired Rear Admiral John Poindexter, a chief player in the Iran-Contra Affair. At the time, Poindexter was working at Syntek, a company often contracted to do work for the Department of Defense. He proposed a computer system that would help humans crunch large amounts of data in order to more effectively predict potential national security threats. Poindexter brought his ideas to former colleagues working with the United States National Security Council.\n\nThat year, a team of researchers was assembled for the project and began studying various historical events to which Genoa could be applied. The Tokyo subway sarin attack in March was the primary focus. Instead of analyzing the attack itself, the researchers looked into the history of Aum Shinrikyo, the group that perpetrated the attack, to find evidence that could've suggested their intentions.\n\nIn order to pitch their ideas, the researchers set up a mock crisis command center in DARPA's main building, full of monitors staffed by actors. An audience would watch as a fictitious scenario would unfold before them, guided along by an animated video segment. Poindexter called the presentation \"A Day in the Life of an Analyst.\" Another mock center was set up near the DARPA building with the help of a Hollywood set designer to serve the same purpose. Prominent viewers of the exhibition included Richard A. Clarke, John Michael McConnell, and James R. Clapper.\n\nGenoa was commissioned in 1996 for development overseen by DARPA and completed in the 2002 fiscal year, becoming a component of the Total Information Awareness program. It was concluded that while Genoa helped officials better understand complex situations, it operated at a slow speed. The research initiated by the project was continued in its immediate follow-on program, Genoa II. One of the goals of this successor was to increase the speed of analyses.\nThe program was actively utilized by the Defense Intelligence Agency.\n\n"}
{"id": "46695267", "url": "https://en.wikipedia.org/wiki?curid=46695267", "title": "Ruedi-Allgower classification", "text": "Ruedi-Allgower classification\n\nThe Ruedi-Allgower classification is a system of categorizing pilon fractures of the distal tibia.\n\n"}
{"id": "700141", "url": "https://en.wikipedia.org/wiki?curid=700141", "title": "Semiclassical physics", "text": "Semiclassical physics\n\nSemiclassical physics, or simply semiclassical refers to a theory in which one part of a system is described quantum-mechanically whereas the other is treated classically. For example, external fields will be constant, or when changing will be classically described. In general, it incorporates a development in powers of Planck's constant, resulting in the classical physics of power 0, and the first nontrivial approximation to the power of (−1). In this case, there is a clear link between the quantum-mechanical system and the associated semi-classical and classical approximations, as it is similar in appearance to the transition from physical optics to geometric optics.\n\nFour examples of a semiclassical approximation include:\n\n\n\n"}
{"id": "993492", "url": "https://en.wikipedia.org/wiki?curid=993492", "title": "Sociology of scientific knowledge", "text": "Sociology of scientific knowledge\n\nThe sociology of scientific knowledge (SSK) is the study of science as a social activity, especially dealing with \"the social conditions and effects of science, and with the social structures and processes of scientific activity.\" The sociology of scientific ignorance (SSI) is complementary to the sociology of scientific knowledge. For comparison, the sociology of knowledge studies the impact of human knowledge and the prevailing ideas on societies and relations between knowledge and the social context within which it arises.\n\nSociologists of scientific knowledge study the development of a scientific field and attempt to identify points of contingency or interpretative flexibility where ambiguities are present. Such variations may be linked to a variety of political, historical, cultural or economic factors. Crucially, the field does not set out to promote relativism or to attack the scientific project; the aim of the researcher is to explain why one interpretation rather than another succeeds due to external social and historical circumstances.\n\nThe field emerged in the late 1960s and early 1970s and at first was an almost exclusively British practice. Other early centers for the development of the field were in France, Germany, and the United States (notably at Cornell University). Major theorists include Barry Barnes, David Bloor, Sal Restivo, Randall Collins, Gaston Bachelard, Harry Collins, Paul Feyerabend, Steve Fuller, Martin Kusch, Bruno Latour, Mike Mulkay, Derek J. de Solla Price, Lucy Suchman and Anselm Strauss.\n\nThe sociology of scientific knowledge in its Anglophone versions emerged in the 1970s in self-conscious opposition to the sociology of science associated with the American Robert K. Merton, generally considered one of the seminal authors in the sociology of science. Merton's was a kind of \"sociology of scientists,\" which left the cognitive content of science out of sociological account; SSK by contrast aimed at providing sociological explanations of scientific ideas themselves, taking its lead from aspects of the work of Thomas S. Kuhn, but especially from established traditions in cultural anthropology (Durkheim, Mauss) as well as the later Wittgenstein. David Bloor, one of SSK's early champions, has contrasted the so-called 'weak programme' (or 'program' — either spelling is used) which merely gives social explanations for erroneous beliefs, with what he called the 'strong programme', which considers sociological factors as influencing all beliefs.\n\nThe \"weak\" programme is more of a description of an approach than an organised movement. The term is applied to historians, sociologists and philosophers of science who merely cite sociological factors as being responsible for those beliefs that went wrong. Imre Lakatos and (in some moods) Thomas Kuhn might be said to adhere to it. The \"strong\" programme is particularly associated with the work of two groups: the 'Edinburgh School' (David Bloor, Barry Barnes, and their colleagues at the Science Studies Unit at the University of Edinburgh) in the 1970s and '80s, and the 'Bath School' (Harry Collins and others at the University of Bath) in the same period. \"Edinburgh sociologists\" and \"Bath sociologists\" promoted, respectively, the Strong Programme and Empirical Programme of Relativism (EPOR). Also associated with SSK in the 1980s was discourse analysis as applied to science (associated with Michael Mulkay at the University of York), as well as a concern with issues of reflexivity arising from paradoxes relating to SSK's relativist stance towards science and the status of its own knowledge-claims (Steve Woolgar, Malcolm Ashmore).\n\nThe sociology of scientific knowledge (SSK) has major international networks through its principal associations, 4S and EASST, with recently established groups in Japan, South Korea, Taiwan and Latin America. It has made major contributions in recent years to a critical analysis of the biosciences and informatics.\n\nStudies of mathematical practice and quasi-empiricism in mathematics are also rightly part of the sociology of knowledge, since they focus on the community of those who practice mathematics and their common assumptions. Since Eugene Wigner raised the issue in 1960 and Hilary Putnam made it more rigorous in 1975, the question of why fields such as physics and mathematics should agree so well has been debated. Proposed solutions point out that the fundamental constituents of mathematical thought, space, form-structure, and number-proportion are also the fundamental constituents of physics. It is also worthwhile to note that physics is nothing but a modeling of reality, and seeing causal relationships governing repeatable observed phenomena, and much of mathematics, especially in relation to the growth of the calculus, has been developed precisely for the goal of developing these models in a rigorous fashion. Another approach is to suggest that there is no deep problem, that the division of human scientific thinking through using words such as 'mathematics' and 'physics' is only useful in their practical everyday function to categorize and distinguish.\n\nFundamental contributions to the sociology of mathematical knowledge have been made by Sal Restivo and David Bloor. Restivo draws upon the work of scholars such as Oswald Spengler (\"The Decline of the West\", 1918), Raymond Louis Wilder and Leslie Alvin White, as well as contemporary sociologists of knowledge and science studies scholars. David Bloor draws upon Ludwig Wittgenstein and other contemporary thinkers. They both claim that mathematical knowledge is socially constructed and has irreducible contingent and historical factors woven into it. More recently Paul Ernest has proposed a social constructivist account of mathematical knowledge, drawing on the works of both of these sociologists.\n\nSSK has received criticism from theorists of the Actor-network theory (ANT) school of science and technology studies. These theorists criticise SSK for sociological reductionism and a human centered universe. SSK, they say, relies too heavily on human actors and social rules and conventions settling scientific controversies. The debate is discussed in an article \"Epistemological Chicken\".\n\n\n"}
{"id": "29518", "url": "https://en.wikipedia.org/wiki?curid=29518", "title": "Statistical physics", "text": "Statistical physics\n\nStatistical physics is a branch of physics that uses methods of probability theory and statistics, and particularly the mathematical tools for dealing with large populations and approximations, in solving physical problems. It can describe a wide variety of fields with an inherently stochastic nature. Its applications include many problems in the fields of physics, biology, chemistry, neurology, and even some social sciences, such as sociology and linguistics. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.\n\nIn particular, statistical mechanics develops the phenomenological results of thermodynamics from a probabilistic examination of the underlying microscopic systems. Historically, one of the first topics in physics where statistical methods were applied was the field of mechanics, which is concerned with the motion of particles or objects when subjected to a force.\n\nStatistical mechanics provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic or bulk properties of materials that can be observed in everyday life, therefore explaining thermodynamics as a natural result of statistics, classical mechanics, and quantum mechanics at the microscopic level. Because of this history, the statistical physics is often considered synonymous with statistical mechanics or statistical thermodynamics.\n\nOne of the most important equations in statistical mechanics (analogous to formula_1 in Newtonian mechanics, or the Schrödinger equation in quantum mechanics) is the definition of the partition function formula_2, which is essentially a weighted sum of all possible states formula_3 available to a system.\n\nwhere formula_5 is the Boltzmann constant, formula_6 is temperature and formula_7 is energy of state formula_3. Furthermore, the probability of a given state, formula_3, occurring is given by\n\nHere we see that very-high-energy states have little probability of occurring, a result that is consistent with intuition.\n\nA statistical approach can work well in classical systems when the number of degrees of freedom (and so the number of variables) is so large that exact solution is not possible, or not really useful. Statistical mechanics can also describe work in non-linear dynamics, chaos theory, thermal physics, fluid dynamics (particularly at high Knudsen numbers), or plasma physics.\n\nAlthough some problems in statistical physics can be solved analytically using approximations and expansions, most current research utilizes the large processing power of modern computers to simulate or approximate solutions. A common approach to statistical problems is to use a Monte Carlo simulation to yield insight into the dynamics of a complex system.\n\nQuantum statistical mechanics is statistical mechanics applied to quantum mechanical systems. In quantum mechanics a statistical ensemble (probability distribution over possible quantum states) is described by a density operator \"S\", which is a non-negative, self-adjoint, trace-class operator of trace 1 on the Hilbert space \"H\" describing the quantum system. This can be shown under various mathematical formalisms for quantum mechanics. One such formalism is provided by quantum logic.\n\nA significant contribution (at different times) in development of statistical physics was given by Satyendra Nath Bose, James Clerk Maxwell, Ludwig Boltzmann, J. Willard Gibbs, Marian Smoluchowski, Albert Einstein, Enrico Fermi, Richard Feynman, L. Landau, Vladimir Fock, Werner Heisenberg, Nikolay Bogolyubov, Benjamin Widom, Lars Onsager, Benjamin and Jeremy Chubb (also inventors of the titanium sublimation pump), and others. Statistical physics is studied in the nuclear center at Los Alamos. Also, Pentagon has organized a large department for the study of turbulence at the University of Princeton. Work in this area is also being conducted by Saclay (Paris), Max Planck Institute, Netherlands Institute for Atomic and Molecular Physics and other research centers.\n\nStatistical physics allowed us to explain and quantitatively describe superconductivity, superfluidity, turbulence, collective phenomena in solids and plasma, and the structural features of liquid. It underlies the modern astrophysics. It is statistical physics that helped us to create such intensively developing study of liquid crystals and to construct a theory of phase transition and critical phenomena. Many experimental studies of matter are entirely based on the statistical description of a system. These include the scattering of cold neutrons, X-ray, visible light, and more.\nStatistical physics plays a major role in Physics of Solid State Physics, Materials Science, Nuclear Physics, Astrophysics, Chemistry, Biology and Medicine (e.g. study of the spread of infectious diseases), Information Theory and Technique but also in those areas of technology owing to their development in the evolution of Modern Physics. It still has important applications in theoretical sciences such as Sociology and Linguistics and is useful for researchers in higher education, corporate governance and industry.\n\n\nThermal and Statistical Physics (lecture notes, Web draft 2001) by Mallett M., Blumler P.\nby Harald J W Müller-Kirsten (University of Kaiserslautern, Germany)\n"}
{"id": "37914845", "url": "https://en.wikipedia.org/wiki?curid=37914845", "title": "Stuart Ballantine Medal", "text": "Stuart Ballantine Medal\n\nThe Stuart Ballantine Medal was a science and engineering award presented by the Franklin Institute, of Philadelphia, Pennsylvania, USA. It was named after the US inventor Stuart Ballantine.\n\n\n"}
{"id": "185907", "url": "https://en.wikipedia.org/wiki?curid=185907", "title": "Subgenus", "text": "Subgenus\n\nIn biology, a subgenus (plural: subgenera) is a taxonomic rank directly below genus.\n\nIn the International Code of Zoological Nomenclature, a subgeneric name can be used independently or included in a species name, in parentheses, placed between the generic name and the specific epithet: e.g. the tiger cowry of the Indo-Pacific, \"Cypraea\" (\"Cypraea\") \"tigris\" Linnaeus, which belongs to the subgenus \"Cypraea\" of the genus \"Cypraea\". However, it is not mandatory, or even customary, when giving the name of a species, to include the subgeneric name.\n\nIn the International Code of Nomenclature for algae, fungi, and plants, the subgenus is one of the possible subdivisions of a genus. There is no limit to the number of divisions that are permitted within a genus by adding the prefix \"sub-\" or in other ways as long as no confusion can result. The secondary ranks of section and series are subordinate to subgenus.\n\nIn zoological nomenclature, when a genus is split into subgenus, the originally described population is retained as the \"nominotypical subgenus\" or \"nominate subgenus\", which repeats the same name as the genus. For example, \"Panthera Panthera pardus\", a leopard.\n\n"}
{"id": "39117056", "url": "https://en.wikipedia.org/wiki?curid=39117056", "title": "The Emperor's New Drugs", "text": "The Emperor's New Drugs\n\nThe Emperor's New Drugs – Exploding the Antidepressant Myth is a 2009 book by Irving Kirsch, arguing that the chemical imbalance theory of depression is wrong and that antidepressants have little or no direct effect on depression but, because of their common serious side-effects, they are a powerful placebo.\n\nKirsch is Associate Director of the Program in Placebo Studies and a lecturer in medicine at the Harvard Medical School and Beth Israel Deaconess Medical Center, and professor emeritus of psychology at the Universities of Hull and Plymouth in the United Kingdom, and the University of Connecticut in the United States. His research interests include placebo effects, antidepressants, expectancy, and hypnosis. He is the originator of response expectancy theory.\n\nWhile analyzing antidepressant trials as part of his research into the placebo effect, Kirsch realised that drug companies do not publish all of their disappointing antidepressant trial results, but most decisions about the efficacy of an antidepressant are based only on published results. Using the Freedom of Information Act, he and his colleagues acquired from the US Food and Drug Administration the unpublished trial results for six antidepressants. When the results from both published and unpublished studies were averaged, the researchers concluded that the drugs produced a small but clinically meaningless improvement in mood compared with an inert placebo (sugar pill). (Some researchers have questioned the statistical basis of this study suggesting that it underestimates the effect size of antidepressants and other studies have reached a range of supporting and conflicting conclusions).\n\nTo determine whether their averaging of results was hiding a meaningful benefit to more-severely depressed patients by combining their results with those of moderately and mildly depressed patients, he and his colleagues undertook another study, this time of the four new-generation antidepressants for which all (published and unpublished) trial data were available, and concluded that the difference between drug and placebo effect was greater for more-severely depressed patients, and that this difference was clinically meaningful (but still relatively small) only at the upper end of the very severely depressed category. They attributed this difference to very seriously depressed patients being less responsive to the inert placebo.\n\nKirsch also addresses the conclusions of the a 2004 study, that if one antidepressant doesn't work on a patient, another should be tried in its place, and then another, until hopefully one will be found to be effective. The 2004 study found that although only 37% of patients were helped by the first antidepressant tried, 67% had found some relief by the time they had tried the fourth. Kirsch cites a 1957 study in which volunteers were given a drug that induces nausea and one to treat nausea. If the anti-nausea drug failed to prevent nausea, they were given another. If that failed, another was tried; and so on. All volunteers experienced complete relief from nausea by the sixth treatment, yet every treatment was a placebo. He concludes that the results of the 2004 antidepressant study are also likely due to the placebo effect.\n\nSince the chemical-imbalance theory of depression is based on the efficacy of antidepressants, Kirsch concludes, \"It now seems beyond question that the traditional account of depression as a chemical imbalance in the brain is simply wrong.\"\n\nThe European Psychiatric Association published a position paper in 2012 that described Kirsch's argument as \"misleading\". The organization argues that \n\nMarcia Angell's review of The Emperor's New Drugs welcomes Kirsch's work as a long overdue application of the scientific method to a field lacking rigorous scientific analysis, stating \"Kirsch is a faithful proponent of the scientific method, and his voice therefore brings a welcome objectivity to a subject often swayed by anecdotes, emotions, or, as we will see, self-interest.\"\n\nPsychiatrist Daniel Carlat called the book \"an important book, with the reservation that Kirsch’s selective use of data gives him the appearance of an anti-antidepressant partisan.\" He states that Irving's conclusions are \"provocative but unconvincing\", noting that many drugs such as benzodiazepines have been tested for antidepressant activity and found inactive. Carlat argues that if antidepressants were acting purely via a placebo effect, any benzodiazepines and other drugs would show activity as well.\n\nIn a 2012 episode of CBS's \"60 Minutes\" featuring Kirsch and his book, host Lesley Stahl said, \"The medical community is at war – battling over the scientific research and writings of a psychologist named Irving Kirsch ... Kirsch and his studies have triggered a furious counterattack, mainly from psychiatrists.\"\n\nAfter the program's airing, Jeffrey Lieberman, the American Psychiatric Association's president elect at the time, said, \"Dr. Kirsch is mistaken and confused, and he's ideologically biased in his thinking. He is conducting an analysis and interpreting the data to support his ideologically biased perspective. What he is concluding is inaccurate, and what he is communicating is misleading to people and potentially harmful to those who really suffer from depression and would be expected to benefit from antidepressant medication. To say that antidepressants are no better than placebo is just plain wrong.\" \n\nReviews in the lay press have been largely positive, and the book was shortlisted in 2010 for the Mind Book of the Year award.\n\nThere have been two English-language editions of the book – one in the UK, one in the US – and it has been translated into Japanese, French, Italian, Turkish, and Polish.\n"}
{"id": "20496314", "url": "https://en.wikipedia.org/wiki?curid=20496314", "title": "The Hacker Ethic and the Spirit of the Information Age", "text": "The Hacker Ethic and the Spirit of the Information Age\n\nThe Hacker Ethic and the Spirit of the Information Age is a book released in 2001, and written by Pekka Himanen, with prologue written by Linus Torvalds and the epilogue written by Manuel Castells.\n\nPekka Himanen defines himself as a philosopher. Manuel Castells is an internationally well-known sociologist. Linus Torvalds is the creator of the Linux kernel. The book has the .\n\n\n"}
{"id": "3129664", "url": "https://en.wikipedia.org/wiki?curid=3129664", "title": "Timeline of psychology", "text": "Timeline of psychology\n\nThis article is a general timeline of psychology. A more general description of the development of the subject of psychology can be found in the History of psychology article. Related information can be found in the Timeline of psychiatry article. A more specific review of important events in the development of psychotherapy can be found in the Timeline of psychotherapy article.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25453345", "url": "https://en.wikipedia.org/wiki?curid=25453345", "title": "Tropical year", "text": "Tropical year\n\nA tropical year (also known as a solar year) is the time that the Sun takes to return to the same position in the cycle of seasons, as seen from Earth; for example, the time from vernal equinox to vernal equinox, or from summer solstice to summer solstice. Because of the precession of the equinoxes, the seasonal cycle does not remain exactly synchronized with the position of the Earth in its orbit around the Sun. As a consequence, the tropical year is about 20 minutes shorter than the time it takes Earth to complete one full orbit around the Sun as measured with respect to the fixed stars (the sidereal year).\n\nSince antiquity, astronomers have progressively refined the definition of the tropical year. The entry for \"year, tropical\" in the \"Astronomical Almanac Online Glossary\" (2015) states:\n\nAn equivalent, more descriptive, definition is \"The natural basis for computing passing tropical years is the mean longitude of the Sun reckoned from the precessionally moving equinox (the dynamical equinox or equinox of date). Whenever the longitude reaches a multiple of 360 degrees the mean Sun crosses the vernal equinox and a new tropical year begins\" .\n\nThe mean tropical year in 2000 was 365.24219 ephemeris days; each ephemeris day lasting 86,400 SI seconds. This is 365.24217 mean solar days .\n\nThe word \"tropical\" comes from the Greek \"tropikos\" meaning \"turn\" . Thus, the tropics of Cancer and Capricorn mark the extreme north and south latitudes where the Sun can appear directly overhead, and where it appears to \"turn\" in its annual seasonal motion. Because of this connection between the tropics and the seasonal cycle of the apparent position of the Sun, the word \"tropical\" also lent its name to the \"tropical year\". The early Chinese, Hindus, Greeks, and others made approximate measures of the tropical year.\n\nIn the 2nd century BC Hipparchus measured the time required for the Sun to travel from an equinox to the same equinox again. He reckoned the length of the year to be 1/300 of a day less than 365.25 days (365 days, 5 hours, 55 minutes, 12 seconds, or 365.24667 days). Hipparchus used this method because he was better able to detect the time of the equinoxes, compared to that of the solstices .\n\nHipparchus also discovered that the equinoctial points moved along the ecliptic (plane of the Earth's orbit, or what Hipparchus would have thought of as the plane of the Sun's orbit about the Earth) in a direction opposite that of the movement of the Sun, a phenomenon that came to be named \"precession of the equinoxes\". He reckoned the value as 1° per century, a value that was not improved upon until about 1000 years later, by Islamic astronomers. Since this discovery a distinction has been made between the tropical year and the sidereal year .\n\nDuring the Middle Ages and Renaissance a number of progressively better tables were published that allowed computation of the positions of the Sun, Moon and planets relative to the fixed stars. An important application of these tables was the reform of the calendar.\n\nThe Alfonsine Tables, published in 1252, were based on the theories of Ptolemy and were revised and updated after the original publication; the most recent update in 1978 was by the French National Centre for Scientific Research. The length of the tropical year was given as 365 solar days 5 hours 49 minutes 16 seconds (≈ 365.24255 days). This length was used in devising the Gregorian calendar of 1582 .\n\nIn the 16th century Copernicus put forward a heliocentric cosmology. Erasmus Reinhold used Copernicus' theory to compute the Prutenic Tables in 1551, and gave a tropical year length of 365 solar days, 5 hours, 55 minutes, 58 seconds (365.24720 days), based on the length of a sidereal year and the presumed rate of precession. This was actually less accurate than the earlier value of the Alfonsine Tables.\n\nMajor advances in the 17th century were made by Johannes Kepler and Isaac Newton. In 1609 and 1619 Kepler published his three laws of planetary motion . In 1627, Kepler used the observations of Tycho Brahe and Waltherus to produce the most accurate tables up to that time, the Rudolphine Tables. He evaluated the mean tropical year as 365 solar days, 5 hours, 48 minutes, 45 seconds (365.24219 days; ).\n\nNewton's three laws of dynamics and theory of gravity were published in his \"Philosophiæ Naturalis Principia Mathematica\" in 1687. Newton's theoretical and mathematical advances influenced tables by Edmund Halley published in 1693 and 1749 and provided the underpinnings of all solar system models until Albert Einstein's theory of General relativity in the 20th century.\n\nFrom the time of Hipparchus and Ptolemy, the year was based on two equinoxes (or two solstices) a number of years apart, to average out both observational errors and the effects of nutation (periodic motions of the axis of rotation of the earth, the main cycle being 18.6 years) and the movement of the Sun caused by the gravitational pull of the planets. These effects did not begin to be understood until Newton's time. To model short-term variations of the time between equinoxes (and prevent them from confounding efforts to measure long-term variations) requires either precise observations or an elaborate theory of the apparent motion of the Sun. The necessary theories and mathematical tools came together in the 18th century due to the work of Pierre-Simon de Laplace, Joseph Louis Lagrange, and other specialists in celestial mechanics. They were able to express the mean longitude of the Sun as\n\nwhere \"T\" is the time in Julian centuries. The inverse of the derivative of \"L\", \"dT/dL\" gives the length of the tropical year as a linear function of T. When this is computed, an expression giving the length of the tropical year as a function of T results.\n\nTwo equations are given in the table. Both equations estimate that the tropical year gets roughly a half second shorter each century.\n\nNewcomb's tables were successful enough that they were used by the joint American-British \"Astronomical Almanac\" for the Sun, Mercury, Venus, and Mars through 1983 .\n\nThe length of the mean tropical year is derived from a model of the solar system, so any advance that improves the solar system model potentially improves the accuracy of the mean tropical year. Many new observing instruments became available, including\n\nThe complexity of the model used for the solar system must be limited to the available computation facilities. In the 1920s punched card equipment came into use by L. J. Comrie in Britain. For the \"American Ephemeris\" an electromagnetic computer, the IBM Selective Sequence Electronic Calculator was used since 1948. When modern computers became available, it was possible to compute ephemerides using numerical integration rather than general theories; numerical integration came into use in 1984 for the joint US-UK almanacs .\n\nEinstein's General Theory of Relativity provided a more accurate theory, but the accuracy of theories and observations did not require the refinement provided by this theory (except for the advance of the perihelion of Mercury) until 1984. Time scales incorporated general relativity beginning in the 1970s .\n\nA key development in understanding the tropical year over long periods of time is the discovery that the rate of rotation of the earth, or equivalently, the length of the mean solar day, is not constant. William Ferrel in 1864 and Charles-Eugène Delaunay in 1865 indicated the rotation of the Earth was being retarded by tides. In 1921 William H Shortt invented the Shortt-Synchronome clock, the most accurate commercially produced pendulum clock; it was the first clock capable of measuring variations in the Earth's rotation. The next major time-keeping advance was the quartz clock, first built by Warren Marrison and J. W. Horton in 1927; in the late 1930s quartz clocks began to replace pendulum clocks as time standards .\n\nA series of experiments beginning in the late 1930s led to the development of the first atomic clock by Louis Essen and J. V. L. Parry in 1955. Their clock was based on a transition in the cesium atom . Due to the accuracy the General Conference on Weights and Measures in 1960 redefined the second in terms of the cesium transition. The atomic second, often called the SI second, was intended to agree with the ephemeris second based on Newcomb's work, which in turn makes it agree with the mean solar second of the mid-19th century .\n\nAs mentioned in History, advances in time-keeping have resulted in various time scales. One useful time scale is Universal Time, especially the UT1 variant, which is the mean solar time at 0 degrees longitude (the Greenwich meridian). One second of UT is 1/86,400 of a mean solar day. This time scale is known to be somewhat variable. Since all civil calendars count actual solar days, they must ultimately be based on UT (but the actual timing of official midnight is based on UTC).\n\nThe other time scale has two parts. Ephemeris time (ET) is the independent variable in the equations of motion of the solar system, in particular, the equations in use from 1960 to 1984 . That is, the length of the second used in the solar system calculations could be adjusted until the length that gives the best agreement with observations is found. With the introduction of atomic clocks in the 1950s, it was found that ET could be better realized as atomic time. This also means that ET is a uniform time scale, as is atomic time. ET was given a new name, Terrestrial Time (TT), and for most purposes ET = TT = International Atomic Time + 32.184 SI seconds. TT is ahead of UT1 by 69.184 seconds (International Earth Rotation Service 2017; ).\n\nAs explained below, long-term estimates of the length of the tropical year were used in connection with the reform of the Julian calendar, which resulted in the Gregorian calendar. Participants in that reform were unaware of the non-uniform rotation of the Earth, but now this can be taken into account to some degree. The amount that TT is ahead of UT1 is known as Δ\"T\", or Delta \"T\". The table below gives Morrison and Stephenson's (S & M) 2004 estimates and standard errors (\"σ\") for dates significant in the process of developing the Gregorian calendar.\n\nThe low-precision extrapolations are computed with an expression provided by \n\nwhere \"t\" is measured in Julian centuries from 1820. The extrapolation is provided only to show Δ\"T\" is not negligible when evaluating the calendar for long periods; cautions that \"many researchers have attempted to fit a parabola to the measured Δ\"T\" values in order to determine the magnitude of the deceleration of the Earth's rotation. The results, when taken together, are rather discouraging.\"\n\nAn oversimplified definition of the tropical year would be the time required for the Sun, beginning at a chosen ecliptic longitude, to make one complete cycle of the seasons and return to the same ecliptic longitude. Before considering an example, the equinox must be examined. There are two important planes in solar system calculations, the plane of the ecliptic (the Earth's orbit around the Sun), and the plane of the celestial equator (the Earth's equator projected into space). These two planes intersect in a line. This \"direction\" is given the symbol (the symbol looks like the horns of a ram because it used to be toward the constellation Aries). The opposite \"direction\" is given the symbol (because it used to be toward Libra). Because of the precession of the equinoxes and nutation these directions change, compared to the direction of distant stars and galaxies, whose directions have no measurable motion due to their great distance (see International Celestial Reference Frame).\n\nThe ecliptic longitude of the Sun is the angle between and the Sun, measured eastward along the ecliptic. This creates a complicated measurement, because as the Sun is moving, the direction the angle is measured from is also moving. It is convenient to have a fixed (with respect to distant stars) direction to measure from; the direction of at noon January 1, 2000 fills this role and is given the symbol .\n\nUsing the oversimplified definition, there was an equinox on March 20, 2009, 11:44:43.6 TT. The 2010 March equinox was March 20, 17:33:18.1 TT, which gives a duration of 365 days 5 hours 48 minutes 34.5 seconds (Astronomical Applications Dept., 2009). While the Sun moves, moves in the opposite direction . When the Sun and met at the 2010 March equinox, the Sun had moved east 359°59'09\" while had moved west 51\" for a total of 360° (all with respect to ; , expression for p).\n\nIf a different starting longitude for the Sun is chosen, the duration for the Sun to return to the same longitude will be different. This is because although changes at a nearly steady rate there is considerable variation in the angular speed of the Sun. Thus, the 50 or so arcseconds that the Sun does not have to move to complete the tropical year \"saves\" varying amounts of time depending on the position in the orbit.\n\nAs already mentioned, there is some choice in the length of the tropical year depending on the point of reference that one selects. But during the period when return of the Sun to a chosen longitude was the method in use by astronomers, one of the equinoxes was usually chosen because it was easier to detect when it occurred. When tropical year measurements from several successive years are compared, variations are found which are due to nutation, and to the planetary perturbations acting on the Sun. provided the following examples of intervals between northward equinoxes:\n\nUntil the beginning of the 19th century, the length of the tropical year was found by comparing equinox dates that were separated by many years; this approach yielded the \"mean\" tropical year .\n\nValues of mean time intervals between equinoxes and solstices were provided by for the years 0 and 2000.\n\nThe mean tropical year on January 1, 2000 was or 365 ephemeris days, 5 hours, 48 minutes, 45.19 seconds. This changes slowly; an expression suitable for calculating the length of a tropical year in ephemeris days, between 8000 BC and 12000 AD is\n\nwhere T is in Julian centuries of 36,525 days of 86,400 SI seconds measured from noon January 1, 2000 TT (in negative numbers for dates in the past; , calculated from planetary model of ).\n\nModern astronomers define the tropical year as time for the Sun's mean longitude to increase by 360°. The process for finding an expression for the length of the tropical year is to first find an expression for the Sun's mean longitude (with respect to ), such as Newcomb's expression given above, or Laskar's expression (1986, p. 64). When viewed over a one-year period, the mean longitude is very nearly a linear function of Terrestrial Time. To find the length of the tropical year, the mean longitude is differentiated, to give the angular speed of the Sun as a function of Terrestrial Time, and this angular speed is used to compute how long it would take for the Sun to move 360° (; Astronomical Almanac for the year 2011, L8).\n\nThe above formulae give the length of the tropical year in ephemeris days (equal to 86,400 SI seconds), not solar days. It is the number of solar days in a tropical year that is important for keeping the calendar in synch with the seasons (see below).\n\nThe Gregorian calendar, as used for civil and scientific purposes, is an international standard. It is a solar calendar that is designed to maintain synchrony with the mean tropical year . It has a cycle of 400 years (146,097 days). Each cycle repeats the months, dates, and weekdays. The average year length is 146,097/400 = = 365.2425 days per year, a close approximation to the mean tropical year .\n\nThe Gregorian calendar is a reformed version of the Julian calendar. By the time of the reform in 1582, the date of the vernal equinox had shifted about 10 days, from about March 21 at the time of the First Council of Nicaea in 325, to about March 11. According to , the real motivation for reform was not primarily a matter of getting agricultural cycles back to where they had once been in the seasonal cycle; the primary concern of Christians was the correct observance of Easter. The rules used to compute the date of Easter used a conventional date for the vernal equinox (March 21), and it was considered important to keep March 21 close to the actual equinox .\n\nIf society in the future still attaches importance to the synchronization between the civil calendar and the seasons, another reform of the calendar will eventually be necessary. According to Blackburn and Holford-Strevens (who used Newcomb's value for the tropical year) if the tropical year remained at its 1900 value of days the Gregorian calendar would be 3 days, 17 min, 33 s behind the Sun after 10,000 years. Aggravating this error, the length of the tropical year (measured in Terrestrial Time) is decreasing at a rate of approximately 0.53 s per century. Also, the mean solar day is getting longer at a rate of about 1.5 ms per century. These effects will cause the calendar to be nearly a day behind in 3200. The number of solar days in a \"tropical millennium\" is decreasing by about 0.06 per millennium (neglecting the oscillatory changes in the real length of the tropical year). This means there should be fewer and fewer leap days as time goes on. A possible reform would be to omit the leap day in 3200, keep 3600 and 4000 as leap years, and thereafter make all centennial years common except 4500, 5000, 5500, 6000, etc. But the quantity ΔT is not sufficiently predictable to form more precise proposals .\n\n\n"}
{"id": "9861915", "url": "https://en.wikipedia.org/wiki?curid=9861915", "title": "Vladimir Engelgardt", "text": "Vladimir Engelgardt\n\nVladimir Aleksandrovich Engelgardt () (December 3, 1894, in Moscow – July 10, 1984, in Moscow) was a Soviet biochemist, academician of the Soviet Academy of Medical Sciences (1944), academician of the Soviet Academy of Sciences (1953), and Hero of Socialist Labor (1969). He was the founder and the first director of the Institute of Molecular Biology of the Russian Academy of Sciences (later renamed the Engelhardt Institute of Molecular Biology in his honor).\n\nVladimir Engelgardt is considered to be one of the founders of molecular biology in the Soviet Union.\n\n"}
{"id": "72048", "url": "https://en.wikipedia.org/wiki?curid=72048", "title": "X-ray fluorescence", "text": "X-ray fluorescence\n\nX-ray fluorescence (XRF) is the emission of characteristic \"secondary\" (or fluorescent) X-rays from a material that has been excited by bombarding with high-energy X-rays or gamma rays. The phenomenon is widely used for elemental analysis and chemical analysis, particularly in the investigation of metals, glass, ceramics and building materials, and for research in geochemistry, forensic science, archaeology and art objects such as paintings and murals.\n\nWhen materials are exposed to short-wavelength X-rays or to gamma rays, ionization of their component atoms may take place. Ionization consists of the ejection of one or more electrons from the atom, and may occur if the atom is exposed to radiation with an energy greater than its ionization energy. X-rays and gamma rays can be energetic enough to expel tightly held electrons from the inner orbitals of the atom. The removal of an electron in this way makes the electronic structure of the atom unstable, and electrons in higher orbitals \"fall\" into the lower orbital to fill the hole left behind. In falling, energy is released in the form of a photon, the energy of which is equal to the energy difference of the two orbitals involved. Thus, the material emits radiation, which has energy characteristic of the atoms present. The term \"fluorescence\" is applied to phenomena in which the absorption of radiation of a specific energy results in the re-emission of radiation of a different energy (generally lower).\n\nEach element has electronic orbitals of characteristic energy. Following removal of an inner electron by an energetic photon provided by a primary radiation source, an electron from an outer shell drops into its place. There are a limited number of ways in which this can happen, as shown in Figure 1. The main transitions are given names: an L→K transition is traditionally called K, an M→K transition is called K, an M→L transition is called L, and so on. Each of these transitions yields a fluorescent photon with a characteristic energy equal to the difference in energy of the initial and final orbital. The wavelength of this fluorescent radiation can be calculated from Planck's Law:\n\nThe fluorescent radiation can be analysed either by sorting the energies of the photons (energy-dispersive analysis) or by separating the wavelengths of the radiation (wavelength-dispersive analysis). Once sorted, the intensity of each characteristic radiation is directly related to the amount of each element in the material. This is the basis of a powerful technique in analytical chemistry. Figure 2 shows the typical form of the sharp fluorescent spectral lines obtained in the wavelength-dispersive method (see Moseley's law).\n\nIn order to excite the atoms, a source of radiation is required, with sufficient energy to expel tightly held inner electrons. Conventional X-ray generators are most commonly used, because their output can readily be \"tuned\" for the application, and because higher power can be deployed relative to other techniques. However, gamma ray sources can be used without the need for an elaborate power supply, allowing an easier use in small portable instruments. When the energy source is a synchrotron or the X-rays are focused by an optic like a polycapillary, the X-ray beam can be very small and very intense. As a result, atomic information on the sub-micrometre scale can be obtained. X-ray generators in the range 20–60 kV are used, which allow excitation of a broad range of atoms. The continuous spectrum consists of \"bremsstrahlung\" radiation: radiation produced when high-energy electrons passing through the tube are progressively decelerated by the material of the tube anode (the \"target\"). A typical tube output spectrum is shown in Figure 3.\n\nIn energy dispersive analysis, the fluorescent X-rays emitted by the material sample are directed into a solid-state detector which produces a \"continuous\" distribution of pulses, the voltages of which are proportional to the incoming photon energies. This signal is processed by a multichannel analyser (MCA) which produces an accumulating digital spectrum that can be processed to obtain analytical data.\n\nIn wavelength dispersive analysis, the fluorescent X-rays emitted by the material sample are directed into a diffraction grating monochromator. The diffraction grating used is usually a single crystal. By varying the angle of incidence and take-off on the crystal, a single X-ray wavelength can be selected. The wavelength obtained is given by Bragg's law:\n\nwhere \"d\" is the spacing of atomic layers parallel to the crystal surface.\n\nIn energy dispersive analysis, dispersion and detection are a single operation, as already mentioned above. Proportional counters or various types of solid-state detectors (PIN diode, Si(Li), Ge(Li), Silicon Drift Detector SDD) are used. They all share the same detection principle: An incoming X-ray photon ionises a large number of detector atoms with the amount of charge produced being proportional to the energy of the incoming photon. The charge is then collected and the process repeats itself for the next photon. Detector speed is obviously critical, as all charge carriers measured have to come from the same photon to measure the photon energy correctly (peak length discrimination is used to eliminate events that seem to have been produced by two X-ray photons arriving almost simultaneously). The spectrum is then built up by dividing the energy spectrum into discrete bins and counting the number of pulses registered within each energy bin. EDXRF detector types vary in resolution, speed and the means of cooling (a low number of free charge carriers is critical in the solid state detectors): proportional counters with resolutions of several hundred eV cover the low end of the performance spectrum, followed by PIN diode detectors, while the Si(Li), Ge(Li) and Silicon Drift Detectors (SDD) occupy the high end of the performance scale.\n\nIn wavelength dispersive analysis, the single-wavelength radiation produced by the monochromator is passed into a photomultiplier, a detector similar to a Geiger counter, which counts individual photons as they pass through. The counter is a chamber containing a gas that is ionised by X-ray photons. A central electrode is charged at (typically) +1700 V with respect to the conducting chamber walls, and each photon triggers a pulse-like cascade of current across this field. The signal is amplified and transformed into an accumulating digital count. These counts are then processed to obtain analytical data.\n\nThe fluorescence process is inefficient, and the secondary radiation is much weaker than the primary beam. Furthermore, the secondary radiation from lighter elements is of relatively low energy (long wavelength) and has low penetrating power, and is severely attenuated if the beam passes through air for any distance. Because of this, for high-performance analysis, the path from tube to sample to detector is maintained under vacuum (around 10 Pa residual pressure). This means in practice that most of the working parts of the instrument have to be located in a large vacuum chamber. The problems of maintaining moving parts in vacuum, and of rapidly introducing and withdrawing the sample without losing vacuum, pose major challenges for the design of the instrument. For less demanding applications, or when the sample is damaged by a vacuum (e.g. a volatile sample), a helium-swept X-ray chamber can be substituted, with some loss of low-Z (Z = atomic number) intensities.\n\nThe use of a primary X-ray beam to excite fluorescent radiation from the sample was first proposed by Glocker and Schreiber in 1928. Today, the method is used as a non-destructive analytical technique, and as a process control tool in many extractive and processing industries. In principle, the lightest element that can be analysed is beryllium (Z = 4), but due to instrumental limitations and low X-ray yields for the light elements, it is often difficult to quantify elements lighter than sodium (Z = 11), unless background corrections and very comprehensive inter-element corrections are made.\n\nIn energy dispersive spectrometers (EDX or EDS), the detector allows the determination of the energy of the photon when it is detected. Detectors historically have been based on silicon semiconductors, in the form of lithium-drifted silicon crystals, or high-purity silicon wafers.\n\nThese consist essentially of a 3–5 mm thick silicon junction type p-i-n diode (same as PIN diode) with a bias of −1000 V across it. The lithium-drifted centre part forms the non-conducting i-layer, where Li compensates the residual acceptors which would otherwise make the layer p-type. When an X-ray photon passes through, it causes a swarm of electron-hole pairs to form, and this causes a voltage pulse. To obtain sufficiently low conductivity, the detector must be maintained at low temperature, and liquid-nitrogen cooling must be used for the best resolution. With some loss of resolution, the much more convenient Peltier cooling can be employed.\n\nMore recently, high-purity silicon wafers with low conductivity have become routinely available. Cooled by the Peltier effect, this provides a cheap and convenient detector, although the liquid nitrogen cooled Si(Li) detector still has the best resolution (i.e. ability to distinguish different photon energies).\n\nThe pulses generated by the detector are processed by pulse-shaping amplifiers. It takes time for the amplifier to shape the pulse for optimum resolution, and there is therefore a trade-off between resolution and count-rate: long processing time for good resolution results in \"pulse pile-up\" in which the pulses from successive photons overlap. Multi-photon events are, however, typically more drawn out in time (photons did not arrive exactly at the same time) than single photon events and pulse-length discrimination can thus be used to filter most of these out. Even so, a small number of pile-up peaks will remain and pile-up correction should be built into the software in applications that require trace analysis. To make the most efficient use of the detector, the tube current should be reduced to keep multi-photon events (before discrimination) at a reasonable level, e.g. 5–20%.\n\nConsiderable computer power is dedicated to correcting for pulse-pile up and for extraction of data from poorly resolved spectra. These elaborate correction processes tend to be based on empirical relationships that may change with time, so that continuous vigilance is required in order to obtain chemical data of adequate precision.\n\nEDX spectrometers are different from WDX spectrometers in that they are smaller, simpler in design and have fewer engineered parts, however the accuracy and resolution of EDX spectrometers are lower than for WDX. EDX spectrometers can also use miniature X-ray tubes or gamma sources, which makes them cheaper and allows miniaturization and portability. This type of instrument is commonly used for portable quality control screening applications, such as testing toys for lead (Pb) content, sorting scrap metals, and measuring the lead content of residential paint. On the other hand, the low resolution and problems with low count rate and long dead-time makes them inferior for high-precision analysis. They are, however, very effective for high-speed, multi-elemental analysis. Field Portable XRF analysers currently on the market weigh less than 2 kg, and have limits of detection on the order of 2 parts per million of lead (Pb) in pure sand.\n\nIn wavelength dispersive spectrometers (WDX or WDS), the photons are separated by diffraction on a single crystal before being detected. Although wavelength dispersive spectrometers are occasionally used to scan a wide range of wavelengths, producing a spectrum plot as in EDS, they are usually set up to make measurements only at the wavelength of the emission lines of the elements of interest. This is achieved in two different ways:\n\n\nIn order to keep the geometry of the tube-sample-detector assembly constant, the sample is normally prepared as a flat disc, typically of diameter 20–50 mm. This is located at a standardized, small distance from the tube window. Because the X-ray intensity follows an inverse-square law, the tolerances for this placement and for the flatness of the surface must be very tight in order to maintain a repeatable X-ray flux. Ways of obtaining sample discs vary: metals may be machined to shape, minerals may be finely ground and pressed into a tablet, and glasses may be cast to the required shape. A further reason for obtaining a flat and representative sample surface is that the secondary X-rays from lighter elements often only emit from the top few micrometres of the sample. In order to further reduce the effect of surface irregularities, the sample is usually spun at 5–20 rpm. It is necessary to ensure that the sample is sufficiently thick to absorb the entire primary beam. For higher-Z materials, a few millimetres thickness is adequate, but for a light-element matrix such as coal, a thickness of 30–40 mm is needed.\n\nThe common feature of monochromators is the maintenance of a symmetrical geometry between the sample, the crystal and the detector. In this geometry the Bragg diffraction condition is obtained.\n\nThe X-ray emission lines are very narrow (see figure 2), so the angles must be defined with considerable precision. This is achieved in two ways:\n\nThe Soller collimator is a stack of parallel metal plates, spaced a few tenths of a millimetre apart. To improve angle resolution, one must lengthen the collimator, and/or reduce the plate spacing. This arrangement has the advantage of simplicity and relatively low cost, but the collimators reduce intensity and increase scattering, and reduce the area of sample and crystal that can be \"seen\". The simplicity of the geometry is especially useful for variable-geometry monochromators.\n\nThe Rowland circle geometry ensures that the slits are both in focus, but in order for the Bragg condition to be met at all points, the crystal must first be bent to a radius of 2R (where R is the radius of the Rowland circle), then ground to a radius of R. This arrangement allows higher intensities (typically 8-fold) with higher resolution (typically 4-fold) and lower background. However, the mechanics of keeping Rowland circle geometry in a variable-angle monochromator is extremely difficult. In the case of fixed-angle monochromators (for use in simultaneous spectrometers), crystals bent to a logarithmic spiral shape give the best focusing performance. The manufacture of curved crystals to acceptable tolerances increases their price considerably.\n\nThe spectral lines used for chemical analysis are selected on the basis of intensity, accessibility by the instrument, and lack of line overlaps. Typical lines used, and their wavelengths, are as follows:\nOther lines are often used, depending on the type of sample and equipment available.\n\nThe desirable characteristics of a diffraction crystal are:\n\nCrystals with simple structure tend to give the best diffraction performance. Crystals containing heavy atoms can diffract well, but also fluoresce themselves, causing interference. Crystals that are water-soluble, volatile or organic tend to give poor stability.\n\nCommonly used crystal materials include LiF (lithium fluoride), ADP (ammonium dihydrogen phosphate), Ge (germanium), graphite, InSb (indium antimonide), PE (\"tetrakis\"-(hydroxymethyl)-methane: penta-erythritol), KAP (potassium hydrogen phthalate), RbAP (rubidium hydrogen phthalate) and TlAP (thallium(I) hydrogen phthalate). In addition, there is an increasing use of \"layered synthetic microstructures\", which are \"sandwich\" structured materials comprising successive thick layers of low atomic number matrix, and monatomic layers of a heavy element. These can in principle be custom-manufactured to diffract any desired long wavelength, and are used extensively for elements in the range Li to Mg.\n\nDetectors used for wavelength dispersive spectrometry need to have high pulse processing speeds in order to cope with the very high photon count rates that can be obtained. In addition, they need sufficient energy resolution to allow filtering-out of background noise and spurious photons from the primary beam or from crystal fluorescence. There are four common types of detector:\nGas flow proportional counters are used mainly for detection of longer wavelengths. Gas flows through it continuously. Where there are multiple detectors, the gas is passed through them in series, then led to waste. The gas is usually 90% argon, 10% methane (\"P10\"), although the argon may be replaced with neon or helium where very long wavelengths (over 5 nm) are to be detected. The argon is ionised by incoming X-ray photons, and the electric field multiplies this charge into a measurable pulse. The methane suppresses the formation of fluorescent photons caused by recombination of the argon ions with stray electrons. The anode wire is typically tungsten or nichrome of 20–60 μm diameter. Since the pulse strength obtained is essentially proportional to the ratio of the detector chamber diameter to the wire diameter, a fine wire is needed, but it must also be strong enough to be maintained under tension so that it remains precisely straight and concentric with the detector. The window needs to be conductive, thin enough to transmit the X-rays effectively, but thick and strong enough to minimize diffusion of the detector gas into the high vacuum of the monochromator chamber. Materials often used are beryllium metal, aluminised PET film and aluminised polypropylene. Ultra-thin windows (down to 1 μm) for use with low-penetration long wavelengths are very expensive. The pulses are sorted electronically by \"pulse height selection\" in order to isolate those pulses deriving from the secondary X-ray photons being counted.\n\nSealed gas detectors are similar to the gas flow proportional counter, except that the gas does not flow through it. The gas is usually krypton or xenon at a few atmospheres pressure. They are applied usually to wavelengths in the 0.15–0.6 nm range. They are applicable in principle to longer wavelengths, but are limited by the problem of manufacturing a thin window capable of withstanding the high pressure difference.\n\nScintillation counters consist of a scintillating crystal (typically of sodium iodide doped with thallium) attached to a photomultiplier. The crystal produces a group of scintillations for each photon absorbed, the number being proportional to the photon energy. This translates into a pulse from the photomultiplier of voltage proportional to the photon energy. The crystal must be protected with a relatively thick aluminium/beryllium foil window, which limits the use of the detector to wavelengths below 0.25 nm. Scintillation counters are often connected in series with a gas flow proportional counter: the latter is provided with an outlet window opposite the inlet, to which the scintillation counter is attached. This arrangement is particularly used in sequential spectrometers.\n\nSemiconductor detectors can be used in theory, and their applications are increasing as their technology improves, but historically their use for WDX has been restricted by their slow response (see EDX).\n\nAt first sight, the translation of X-ray photon count-rates into elemental concentrations would appear to be straightforward: WDX separates the X-ray lines efficiently, and the rate of \"generation\" of secondary photons is proportional to the element concentration. However, the number of photons \"leaving the sample\" is also affected by the physical properties of the sample: so-called \"matrix effects\". These fall broadly into three categories:\nAll elements \"absorb\" X-rays to some extent. Each element has a characteristic absorption spectrum which consists of a \"saw-tooth\" succession of fringes, each step-change of which has wavelength close to an emission line of the element. Absorption attenuates the secondary X-rays leaving the sample. For example, the mass absorption coefficient of silicon at the wavelength of the aluminium Kα line is 50 m²/kg, whereas that of iron is 377 m²/kg. This means that a given concentration of aluminium in a matrix of iron gives only one seventh of the count rate compared with the same concentration of aluminium in a silicon matrix. Fortunately, mass absorption coefficients are well known and can be calculated. However, to calculate the absorption for a multi-element sample, the composition must be known. For analysis of an unknown sample, an iterative procedure is therefore used. It will be noted that, to derive the mass absorption accurately, data for the concentration of elements not measured by XRF may be needed, and various strategies are employed to estimate these. As an example, in cement analysis, the concentration of oxygen (which is not measured) is calculated by assuming that all other elements are present as standard oxides.\n\nEnhancement occurs where the secondary X-rays emitted by a heavier element are sufficiently energetic to stimulate additional secondary emission from a lighter element. This phenomenon can also be modelled, and corrections can be made provided that the full matrix composition can be deduced.\n\nSample macroscopic effects consist of effects of inhomogeneities of the sample, and unrepresentative conditions at its surface. Samples are ideally homogeneous and isotropic, but they often deviate from this ideal. Mixtures of multiple crystalline components in mineral powders can result in absorption effects that deviate from those calculable from theory. When a powder is pressed into a tablet, the finer minerals concentrate at the surface. Spherical grains tend to migrate to the surface more than do angular grains. In machined metals, the softer components of an alloy tend to smear across the surface. Considerable care and ingenuity are required to minimize these effects. Because they are artifacts of the method of sample preparation, these effects can not be compensated by theoretical corrections, and must be \"calibrated in\". This means that the calibration materials and the unknowns must be compositionally and mechanically similar, and a given calibration is applicable only to a limited range of materials. Glasses most closely approach the ideal of homogeneity and isotropy, and for accurate work, minerals are usually prepared by dissolving them in a borate glass, and casting them into a flat disc or \"bead\". Prepared in this form, a virtually universal calibration is applicable.\n\nFurther corrections that are often employed include background correction and line overlap correction. The background signal in an XRF spectrum derives primarily from scattering of primary beam photons by the sample surface. Scattering varies with the sample mass absorption, being greatest when mean atomic number is low. When measuring trace amounts of an element, or when measuring on a variable light matrix, background correction becomes necessary. This is really only feasible on a sequential spectrometer. Line overlap is a common problem, bearing in mind that the spectrum of a complex mineral can contain several hundred measurable lines. Sometimes it can be overcome by measuring a less-intense, but overlap-free line, but in certain instances a correction is inevitable. For instance, the Kα is the only usable line for measuring sodium, and it overlaps the zinc Lβ (L-M) line. Thus zinc, if present, must be analysed in order to properly correct the sodium value.\n\nIt is also possible to create a characteristic secondary X-ray emission using other incident radiation to excite the sample:\nWhen radiated by an X-ray beam, the sample also emits other radiations that can be used for analysis:\nThe de-excitation also ejects Auger electrons, but Auger electron spectroscopy (AES) normally uses an electron beam as the probe.\n\nConfocal microscopy X-ray fluorescence imaging is a newer technique that allows control over depth, in addition to horizontal and vertical aiming, for example, when analysing buried layers in a painting.\n\nA 2001 review, addresses the application of portable instrumentation from QA/QC perspectives. It provides a guide to the development of a set of SOPs if regulatory compliance guidelines are not available. \n\n\n"}
{"id": "47824602", "url": "https://en.wikipedia.org/wiki?curid=47824602", "title": "Zadoc Benedict", "text": "Zadoc Benedict\n\nZadoc Benedict established the first hat factory on Main Street in Danbury, Connecticut in 1780. It had 3 employees, and made 18 hats weekly.\n\nLegend holds that Benedict plugged a hole in his shoe with rabbit fur, and found that over time the fur turned into felt, and that he then developed a process of making hats from fur of locally available animals like rabbit and beaver.\n"}
