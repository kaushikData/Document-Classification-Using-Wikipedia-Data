{"id": "5024517", "url": "https://en.wikipedia.org/wiki?curid=5024517", "title": "4C +37.11", "text": "4C +37.11\n\n4C +37.11 or Galaxy 0402+379 is a radio galaxy and elliptical galaxy featuring binary supermassive black holes with the least separation of any directly observed binaries, as of 2006. The separation between the two is 24 light-years or 7.3 parsecs, with an orbital period of 30,000 years. The two supermassive black holes, about 750 million light years from earth, have a combined mass of about 15 billion .\n\nOther supermassive binary black hole candidates suggest the smaller separation distances expected as they eventually merge, but have not been confirmed. For example, quasar OJ 287 is inferred to have a binary supermassive black hole pair with an orbital period of 12 years, and thus be much closer together. However these have not been directly measured and additional observation, possibly over extended time periods, is needed.\n\nThe eventual collision of the pair, which should stay apart for at least a few million more years, would result in strong gravitational waves.\n\n"}
{"id": "7897913", "url": "https://en.wikipedia.org/wiki?curid=7897913", "title": "Adolf Busemann", "text": "Adolf Busemann\n\nAdolf Busemann (20 April 1901 – 3 November 1986) was a German aerospace engineer and influential Nazi-era pioneer in aerodynamics, specialising in supersonic airflows. He introduced the concept of swept wings, and after immigrating in 1947 to the United States (\"see\" Operation Paperclip) invented the shockwave free Busemann's Biplane.\n\nBorn in Lübeck, Germany, Busemann attended the TU Braunschweig, receiving his Ph.D. in engineering in 1924. The next year he was given the position of aeronautical research scientist at the Max-Planck Institute where he joined the famed team led by Ludwig Prandtl, including Theodore von Kármán, Max Munk and Jakob Ackeret. In 1930 he was promoted to professor at Georg-August University of Göttingen. He held various positions within the German scientific community during this period, and during the war he was the director of the Braunschweig Laboratory, a famous research establishment.\n\nBusemann discovered the benefits of the swept wing for aircraft at high speeds, presenting a paper on the topic at the Volta Conference in Rome in 1935. The paper concerned supersonic flow only. At the time of his proposal, flight much beyond 300 miles per hour had not been achieved and it was considered an academic curiosity. Nevertheless, he continued working with the concept, and by the end of the year had demonstrated similar benefits in the transonic region as well. As director of the Braunschweig labs, he started an experimental wind tunnel test series of the concept, and by 1942 had amassed a considerable amount of useful technical data. As the need for higher speed aircraft became pressing in Germany, the Messerschmitt Me P.1101 was developed to flight test these designs.\n\nWhen World War II ended, a team of American aerodynamists travelled to Germany as part of Operation Lusty. The team included von Kármán, Tsien Hsue-shen, Hugh Dryden and George S. Schairer from Boeing. They reached the Braunschweig labs on 7 May, where they found a mass of data on the swept wing concept. When they asked Busemann about it, \"his face lit up\" and he said, \"Oh, you remember, I read a paper on it at the Volta Conference in 1935\". Several members of the team did remember the presentation, but had completely forgotten the details in terms of what the presentation was actually about. Realizing its importance, Schairer immediately wrote to Boeing and told them to investigate the concept, leading to a re-modeling of the B-47 Stratojet with a swept wing. Busemann's work, along with similar work by Robert T. Jones in the US, led to a revolution in aircraft design.\n\nNear the end of the war, Busemann started studies of airflow around delta wings, leading to the development of his supersonic conical flow theory. This reduced the complexity of the airflow to a conformal mapping in the complex plane, and was used for some time in the industry.\n\nBusemann moved to the United States in 1947 and started work at NACA's Langley Research Center. In 1951 he gave a talk where he described the fact that air at near supersonic speeds no longer varied in diameter with speed according to Bernoulli's theorem but remained largely incompressible and acting as fixed diameter pipes, or as he put it, 'streampipes'. He jokingly referred to aerodynamicists as needing to become 'pipe fitters'. This talk lead an attendee, Richard Whitcomb, to try and work out what these pipes were doing in a transonic test he was performing, inventing the Whitcomb area rule a few days later.\n\nAt Langley, he worked primarily on the problems of sonic booms, and spent a considerable amount of effort looking at ways to characterize them, and potentially eliminate them. He later invented Busemann's Biplane, a supersonic design he originally proposed in 1936 that emits no shock waves and has no wave drag, at the cost of having no lift. Busemann also did early work on magneto-hydrodynamics in the 1920s, as well as on cylindrical focusing of shock waves and non-steady gas dynamics.\n\nBusemann held a professorship at the University of Colorado from 1963 and suggested the use of ceramic tiles on the space shuttle, which were adopted by NASA. He was awarded the Ludwig-Prandtl-Ring from the Deutsche Gesellschaft für Luft- und Raumfahrt (German Society for Aeronautics and Astronautics) for \"outstanding contribution in the field of aerospace engineering\" in 1966. He died at age 85 in Boulder, Colorado.\n\n"}
{"id": "7795454", "url": "https://en.wikipedia.org/wiki?curid=7795454", "title": "Alarsite", "text": "Alarsite\n\nAlarsite (AlAsO) is an aluminium arsenate mineral with its name derived from its composition: aluminium and arsenate. It occurs as brittle subhedral grains which exhibit trigonal symmetry. It has a Mohs hardness of 5-5.5 and a specific gravity of 3.32. It is semitransparent, colorless with pale yellow tints and shows a vitreous luster. It is optically uniaxial (+) with refractive indices of n = 1.596 and n = 1.608.\n\nIt was reported from fumaroles in the Tolbachik volcano, Kamchatka, Far Eastern Region, Russia. It occurs in association with fedotovite, klyuchevskite, lammerite, nabokoite, atlasovite, langbeinite, hematite and tenorite.\n"}
{"id": "7082603", "url": "https://en.wikipedia.org/wiki?curid=7082603", "title": "Aqueous Wastes from Petroleum and Petrochemical Plants", "text": "Aqueous Wastes from Petroleum and Petrochemical Plants\n\nAqueous Wastes from Petroleum and Petrochemical Plants is a book about the composition and treatment of the various wastewater streams produced in the hydrocarbon processing industries (i.e., oil refineries, petrochemical plants and natural gas processing plants). When it was published in 1967, it was the first book devoted to that subject.\n\nThe book is notable for being the first technical publication of a method for the rigorous tray-by-tray design of steam distillation towers for removing hydrogen sulfide from oil refinery wastewaters. Such towers are commonly referred to as sour water strippers. The design method was also presented at a World Petroleum Congress Meeting shortly after the book was published.\n\nThe subjects covered in the book include wastewater pollutants and the pertinent governmental regulations, oil refinery and petrochemical plant wastewater effluents, treatment methods, miscellaneous effluents, data on the cost of various wastewater treatment methods, and an extensive reference list.\n\nThe book became a classic in its field and is available in major university, public and industrial libraries worldwide. The book has no ISBN because they were not in use in 1967. The Library of Congress catalog number (LCCN) is 67019834 and the British Library system number is 012759691. It is no longer in print, but photocopies can be obtained from the ProQuest Company's \"Books On Demand\" service.\n\nOne of the book reviews is that of Dr. Nelson V. Nemerow, a Civil Engineering professor at the University of Syracuse in New York state, published in 1968 in the American Chemical Society's journal \"Environmental Science and Technology\".\n"}
{"id": "2313526", "url": "https://en.wikipedia.org/wiki?curid=2313526", "title": "Astrograph", "text": "Astrograph\n\nAn astrograph (astrographic camera) is a telescope designed for the sole purpose of astrophotography. Astrographs are mostly used in wide field astronomical surveys of the sky and for detection of objects such as asteroids, meteors, and comets.\n\nMost research telescopes in this class are refractors, although there are many (usually larger) reflecting designs such as the Ritchey-Chrétien and catadioptrics such as the Schmidt camera. The main parameters of an Astrograph are the diameter and f-ratio of the objective, which determine the field of view and image scale on the photographic plate or CCD detector. The objective of an astrograph is usually not very large, on the order of .\n\nThe shape of the focal plane is often designed to work in conjunction with a specific shaped photographic plate or CCD detector. The objective is designed to produce a particularly large (for example, ), flat, and distortion-less image at the focal plane. They may even be designed to focus certain wavelengths of light to match the type of film they are designed to use (early astrographs were corrected to work in blue wavelengths to match photographic emulsions of the time).\nWide-angle astrographs with short f-ratios are used for photographing a huge area of sky. Astrographs with higher f-ratios are used in more precise measurements. Many observatories of the world are equipped with the so-called normal astrographs with an aperture of around and a focal length of . The purpose of a \"normal astrograph\" is to create images where the scale of the image at the focal plane is a standard of approximately 60 arcsecs/mm.\n\nAstrographs used in astrometry record images that are then used to \"map\" the positions of objects over a large area of the sky. These maps are then published in catalogs to be used in further study or to serve as reference points for deep-space imaging.\n\nAstrographs used for stellar classification sometimes consist of two identical telescopes on the same mount (a double astrograph). Each sky field can be simultaneously photographed in two colors (usually blue and yellow). Each telescope may have individually designed non-achromatic objectives to focus the desired wavelength of light which is paired with the respective color-sensitive (black-and-white) photographic plate. In other cases a single telescope is used to make two exposures of the same part of the sky with different filters and color sensitive film used on each exposure. Two-color photography lets astronomers measure the color, as well as the brightness (magnitude), of each star imaged. Colors tell the star's \"temperature”. Knowing the color type and magnitudes lets astronomers determine the distance of a star. Sky fields that are photographed twice, decades apart in time, will reveal a nearby star's proper motion when measured against the background of distant stars or galaxies.\n\nBy taking two exposures of the same section of the sky days or weeks apart, it is possible to find objects such as asteroids, meteors, comets, variable stars, novae, and even unknown planets. By comparing the pair of images, using a device such as a blink comparator, astronomers are able to find objects that moved or changed brightness between the two exposures or simply appear in one image only, as in the case of a nova or meteor. Sometimes objects can even be found in one exposure since a fast moving object will appear as a \"line\" in a long exposure.\n\nOne well-known case of an astrograph used in a discovery is Clyde Tombaugh’s discovery of the dwarf planet Pluto in 1930. Tombaugh was given the job of hunting for a suspected \"9th planet\" to be achieved by systematically photographing the area of the sky around the ecliptic. Tombaugh used Lowell Observatory's (3 lens element), f/5.3 refractor astrograph, which recorded images on glass plates.\n\nIn the amateur astronomy field there are many types of commercial and amateur built telescopes designed for astrophotography labeled \"astrographs\". Optical designs of amateur astrographs vary widely but include apochromatic refractors, variations of Cassegrain reflectors including Ritchey-Chretien and Dall-Kirkham designs, or Newtonian reflectors. Most optical designs do not produce large, flat, and well-corrected imaging fields and therefore require some type of optical correction by way of field flatteners or coma correctors. Amateur astrographs typically have purpose-built focusers, are constructed of thermally stable materials like carbon fiber and are paired with heavy duty mounts to facilitate accurate tracking of deep sky objects for long periods of time.\n\n\n"}
{"id": "24737489", "url": "https://en.wikipedia.org/wiki?curid=24737489", "title": "Beneath the City Streets", "text": "Beneath the City Streets\n\nBeneath the City Streets: A Private Inquiry into the Nuclear Preoccupations of Government is a 1970 book by British author Peter Laurie. It details the existence and necessity of underground bunkers, food depots, and government safe havens throughout underground London.\n\n"}
{"id": "1889654", "url": "https://en.wikipedia.org/wiki?curid=1889654", "title": "Bernard du Bus de Gisignies", "text": "Bernard du Bus de Gisignies\n\nJonkheer Bernard Amé Léonard du Bus de Gisignies (June 21, 1808 in Sint-Joost-ten-Node – July 6, 1874 in Bad Ems) was a Dutch nobleman and later on a Belgian politician, ornithologist and paleontologist. He was the second son of Leonard Pierre Joseph du Bus de Gisignies. He married \"Petronilla Truyts\" on 19 May 1845, together they had two children; Viscount Bernard Daniel (Sint-Joost-ten-Node, 7 October 1832-Brussels, 17 February 1917) and Viscount Chretien (Sint-Joost-ten-Node, 4 November 1845-Jabbeke, 3 July 1883).\n\nHe studied law at the State University of Louvain, but soon became more interested in ornithology. In 1835 he presented a manuscript to the Royal Academy of Belgium in which described the bird \"Leptorhynchus pectoralis\". He was a member of parliament for Soignies.\n\nHe became the first director of the Royal Belgian Institute of Natural Sciences in 1846. On this occasion he donated 2474 birds from his own collection to the museum. In 1860, during the construction of new fortifications around Antwerp he became involved in paleontology. The fossils found were mainly from whales. He also obtained the skeletons from a bowhead whale (\"Balaena mysticetus\") and a young Blue Whale (\"Balaenoptera musculus\"), which are still on display in the museum. In 1860 the skeleton of a mammoth was found near Lier and was brought to the museum (on display since 1869). At that time the only other skeleton of a mammoth was on display in the museum of Saint Petersburg (Russia).\n\nIn 1867 he became the director of the \"science section\" of the Royal Academy of Belgium.\n\n"}
{"id": "83222", "url": "https://en.wikipedia.org/wiki?curid=83222", "title": "CONTOUR", "text": "CONTOUR\n\nThe CO\"met \"N\"ucleus \"TOUR (CONTOUR) was a NASA Discovery-class space probe that failed shortly after its July 2002 launch. It had as its primary objective close flybys of two comet nuclei with the possibility of a flyby of a third known comet or an as-yet-undiscovered comet.\n\nThe two comets scheduled to be visited were Encke and Schwassmann-Wachmann-3, and the third target was d'Arrest. It was hoped that a new comet would have been discovered in the inner Solar System between 2006 and 2008, in which case the spacecraft trajectory would have been changed if possible to rendezvous with the new comet. Scientific objectives included imaging the nuclei at resolutions of up to , performing spectral mapping of the nuclei at resolutions of up to , and obtaining detailed compositional data on gas and dust in the near-nucleus environment, with the goal of improving knowledge of the characteristics of comet nuclei.\n\nAfter the solid rocket motor intended to inject the spacecraft into solar orbit was ignited on August 15, 2002, contact with the probe could not be re-established. Ground-based telescopes later found three objects along the course of the satellite, leading to the speculation that it had disintegrated. Attempts to contact the probe were ended on December 20, 2002. The probe thus accomplished none of its primary scientific objectives, but did prove some spaceflight technologies, such as the APL-developed non-coherent spacecraft navigation technique, which was later used on the New Horizons spacecraft.\n\nThe \"CONTOUR\" spacecraft was constructed in-house at the Johns Hopkins University Applied Physics Laboratory. \"CONTOUR\" was shaped as an octagonal prism, measuring at tall and long, had a total fueled mass of at launch, not including the mass of the STAR 30 booster it was attached to, during the launch phase of the mission. The spacecraft was fitted with a whipple shield, similar to the one used on \"Stardust\", on its leading face, designed with four layers of nextel fabric and seven layers of kevlar. The shield was built to allow the spacecraft to withstand the respective 28.2 and 14 km/s velocity flybys of comets Encke and Schwassmann-Wachmann-3, where the spacecraft would be subjected to numerous particles ejecting from the nuclei of the comets. Although mission scientists predicted that the spacecraft would take no significant damage during the Encke and Schwassmann-Wachmann-3 encounters, the shield and its prototypes were tested vigorously during the construction of the spacecraft, including one where a shield prototype was shot at with surrogate nylon particles. The results of the earlier tests allowed mission planners to determine a safe distance from which the \"CONTOUR\" would pass by comets targeted on the mission. Three of the four scientific instruments aboard the spacecraft are embedded within this heat shield.\n\nPower for \"CONTOUR\" derives from solar cells, which are mounted onto the spacecraft, decorating the sides and rear and generating up to 670 watts of power. A nickel–cadmium battery designed to last up to nine ampere-hours was also installed aboard the spacecraft in the event that the solar cell system fails, or does not provide enough power for the spacecraft or its instruments to function.\n\n\n\n\n\nCONTOUR launched on a Delta 7425 (a Delta II Lite launch vehicle with four strap-on solid-rocket boosters and a Star 27 third stage) on July 3, 2002, at 6:47:41 UT (2:47:41 a.m. EDT) from Cape Canaveral Air Force Station. It was launched into a high-apogee Earth orbit with a period of 5.5 days. Following a series of phasing orbits, the Star 30 solid rocket motor was used to perform an injection maneuver on August 15, 2002, to put CONTOUR in the proper trajectory for an Earth flyby in August 2003 followed by an encounter with comet Encke on November 12, 2003, at a distance of 100 to 160 km and a flyby speed of 28.2 km/s, 1.07 AU from the Sun and 0.27 AU from Earth. During the August 2002 injection maneuver, the probe was lost.\n\nThree more Earth flybys would have followed, in August 2004, February 2005, and February 2006. On June 18, 2006, CONTOUR would have encountered comet Schwassmann-Wachmann-3 at 14 km/s, 0.95 AU from the Sun and 0.33 AU from Earth. Two more Earth flybys were scheduled in February 2007 and 2008, and a flyby of comet d'Arrest might have occurred on 16 August 2008 at a relative velocity of 11.8 km/s, 1.35 AU from the Sun and 0.36 AU from Earth. All flybys would have had a closest encounter distance of about 100 km and would have occurred near the period of maximum activity for each comet. After the comet Encke encounter, CONTOUR might have been retargeted towards a new comet if one was discovered with the desired characteristics (e.g. active, brighter than absolute magnitude 10, perihelion within 1.5 AU).\nAccording to NASA: \"An investigation board concluded that the most likely cause of the mishap was structural failure of the spacecraft due to plume heating during the solid-rocket motor burn. Alternate possible but less likely causes determined were catastrophic failure of the solid rocket motor, collision with space debris, and loss of dynamic control of the spacecraft.\"\n\nAfter the loss of CONTOUR, a replacement spacecraft – CONTOUR 2 – was proposed, scheduled for launch in 2006. However, the replacement did not ultimately materialize.\n\n\n"}
{"id": "42630360", "url": "https://en.wikipedia.org/wiki?curid=42630360", "title": "Chronic Effects of Neurotrauma Consortium", "text": "Chronic Effects of Neurotrauma Consortium\n\nThe Chronic Effects of Neurotrauma Consortium or CENC is a federally funded research project devised to address the long-term effects of mild traumatic brain injury in military service personnel (SMs) and Veterans. Announced by President Barack Obama on August 20, 2013, the CENC was one of two major initiatives developed in response to the injuries incurred by U.S. service personnel during Operation Enduring Freedom and Operation Iraqi Freedom. The project is jointly funded in the amount of $62.175 million by the Department of Defense (DoD) and the Department of Veterans Affairs (VA). The CENC is led by Dr. David X. Cifu of the Virginia Commonwealth University.\n\nIn short, critical gaps exist in the literature, with a paucity of prospective, controlled studies on late-life outcomes and neurodegeneration after mTBI and related basic science research. These research gaps are particularly prominent in the injuries and difficulties seen in combat-exposed populations. The existing research, although suggestive, is not rigorous or robust enough to allow for a clear understanding of the relationships, risks, and potential effective interventions for mTBI, chronic symptoms, and neurodegeneration. To date, no controlled prospective longitudinal study has examined the late-life cognitive, behavioral, systemic, and functional effects of TBI of any severity. Given the absence of prospective studies, the association between TBI and early neurodegeneration is merely theoretical, and the actual risk factors and rate/extent of physiologic and clinical decline over time are unknown. It is also unclear whether a single TBI may be enough to begin a degenerative cascade in select individuals or whether a critical number (dose threshold) of TBIs is needed to “prime” the central nervous system for degeneration. As the majority of TBIs in the military are mild, prospective studies of cognitive outcomes from mild injury are necessary to determine the long-term risks posed to SMs and Veterans. The potential link between mTBI and the development of early dementia is a significant concern for not only at-risk SMs, Veterans, and their families, but also for DoD and VA resource planning, given the high service utilization in the DoD and VA health systems associated with dementia.\n\nGiven these gaps in scientific research and knowledge, the military- and Veteran-specific issues involved, and the importance of a uniform approach to this critical set of problems, the Department of Defense and the Department of Veterans Affairs jointly issued a request for proposals to fund a 5-year, $62.175 million project to address these concerns. After a competitive application process, a consortium led by Virginia Commonwealth University prevailed and was announced as the recipient of the award by President Obama on August 20, 2013. At the time of the award, this was the single largest grant ever awarded to Virginia Commonwealth University.\n\nThe mission of the CENC is to fill the gaps in knowledge about the basic science of mild TBI (also termed concussion), to determine its effects on late-life outcomes and neurodegeneration, to identify service members most susceptible to these effects, and to identify the most effective treatment strategies. The CENC is a multi-center collaboration linking premiere basic science, translational, and clinical neuroscience researchers from the DoD, VA, academic universities, and private research institutes to effectively address the scientific, diagnostic, and therapeutic ramifications of mild TBI and its long-term effects.\n\nThe CENC has oversight from a Government Steering Committee (GSC). Members of the GSC are DoD/VA appointed and is composed of both government representatives and non-government subject matter experts. The GSC approves all studies to be conducted, recommends new studies, and identifies existing and new requirements as they arise. The GSC is the overall main governing and management committee for the project and the committee through which the DoD and VA interact and collaborate with the CENC. The GSC determines all major scientific decisions, and clinical studies proposed by the Consortium Committee proceed into the implementation stage only with the approval of the GSC. \n"}
{"id": "1393633", "url": "https://en.wikipedia.org/wiki?curid=1393633", "title": "Comfort zone", "text": "Comfort zone\n\nA comfort zone is a psychological state in which things feel familiar to a person and they are at ease and in control of their environment, experiencing low levels of anxiety and stress. In this zone, a steady level of performance is possible.\n\nBardwick defines the term as \"a behavioral state where a person operates in an anxiety-neutral position.\" Brown describes it as \"Where our uncertainty, scarcity and vulnerability are minimized—where we believe we'll have access to enough love, food, talent, time, admiration. Where we feel we have some control.\"\n\nStepping out of a comfort zone raises anxiety and generates a stress response. This results in an enhanced level of concentration and focus. \n\nWhite (2009) refers to the \"optimal performance zone\", in which performance can be enhanced by some amount of stress. Yerkes (1907) who reported, \"Anxiety improves performance until a certain optimum level of arousal has been reached. Beyond that point, performance deteriorates as higher levels of anxiety are attained.\" Beyond the optimum performance zone, lies the \"danger zone\" in which performance declines rapidly under the influence of greater anxiety.\n\nHowever, stress in general can have an adverse effect on decision making: Fewer alternatives are tried out and more familiar strategies are used, even if they are not helpful anymore.\n\nOptimal performance management requires maximizing time in the optimum performance zone.\n\n"}
{"id": "31696461", "url": "https://en.wikipedia.org/wiki?curid=31696461", "title": "D. S. Malik", "text": "D. S. Malik\n\nDavender S. Malik is an Indian American mathematician and professor of mathematics and computer science at Creighton University.\n\nMalik attended the University of Delhi in New Delhi, India, receiving his bachelor's and master's degrees in mathematics, where he won the Prof. Ram Behari Gold Medal in 1980 for his high marks. Then at the University of Waterloo in Ontario, Canada, he received a master's degree in pure mathematics. In the United States, Malik went to Ohio University, earning an M.S. in computer science, and a Ph.D. in mathematics in 1985, writing his dissertation on \"A Study of Q-Hypercyclic Rings.\"\n\nIn 1985, Malik joined the faculty of Creighton University, teaching in the mathematics department. In 2013 he became the first holder of the Frederick H. and Anna K. Scheerer Endowed Chair in Mathematics. His research has focused on ring theory, abstract algebra, information science, and fuzzy mathematics, including fuzzy automata theory, fuzzy logic, and applications of fuzzy set theory in other disciplines.\n\nIn the academic community, Malik has been a member of the American Mathematical Society and Phi Kappa Phi. Within his community, co-created a Creighton program in which faculty help area high school students pursue scientific research, to be published in their own student journal.\n\nMalik has published more than 45 papers and 18 books. He has created a computer science line of textbooks that includes extensive and complete programming examples, exercises, and case studies throughout using programming languages such as C++ and Java.\n\nThe books he has written include:\n\n"}
{"id": "45588925", "url": "https://en.wikipedia.org/wiki?curid=45588925", "title": "DNA read errors", "text": "DNA read errors\n\nIn bioinformatics, a DNA read error occurs when a sequence assembler changes one DNA base for a different base. The reads from the sequence assembler is then used to create a de Bruijn graph which is used in various ways to find the read errors.\n\nFrom the way a de Bruijn graph is formed, we can see that there is a possibility of 4^k different nodes to make arrangements of a genome. The number of nodes used to create the graph can be reduced in number by considering only the k-mers found within the DNA strand of interest. Given sequence 1, we can determine the nodes of size 7, or 7-mers, that will be in the graph. These 7-mers then create the graph shown in figure 1.\n\nThe graph shown in figure 1 is a very simple version of what a graph could look like... This graph is formed by taking the last 6 elements of the 7-mer and linking it to the node whose first 6 elements are the same. Figure 1 is the most simplistic a de Bruijn graph can be, since each node has exactly one path into it and one path out. Most of the time, you will most likely see a graph where there is more than one edge directed to a node and/or more than one edge leaving a node. This happens due to the way nodes are connected. The nodes are connected by edges pointing to nodes if, and only if, the last \"k-1\" elements of the \"k\"-mer you are looking at matches the first \"k-1\" elements of any node. This allows for a multiple-edged de Bruijn graph to form. These more complicated graphs happen due to either read errors or variations in DNA strands. Both causes make it difficult to determine the correct structure of the DNA, and what is causing the differences. Since most DNA strands will likely include read errors and variations, scientists hope to use an assembly process that can merge nodes of the graph when they are unambiguously connected after the graph has been cleaned of vertices and edges created by the errors.\n\nWhen a graph is formed from sequenced data, the read errors form tips and bubbles. A tip is where an error occurred during the sequencing process and has caused the graph to end prematurely and includes both correct and incorrect \"k\"-mers. A bubble is also formed when an error occurs during the sequence reading process; however, wherever the error happens, there is a path for the \"k\"-mer reads to reconnect with the main graph and continue as though nothing had ever happened. When there are tips and bubbles present in a de Bruijn graph formed from the data, they may be removed only if an error is what caused the tip or bubble to appear. When scientists are using a reference genome, they can quickly and easily tell where tips are located by comparing the graph of the reference genome and the graph of the sequence. If there is not a reference genome, these tips are eliminated by tracing the branches backward until a point of ambiguity is found. These tips are then removed only if the branch containing the tip is shorter than a set threshold length. The process of removing bubbles is slightly more complicated. The first thing that needs done is to identify the beginning of the bubble. From there, each path from the beginning of the bubble is followed until the point of reconnection. The point of reconnection can be different for each path. Since there can be paths of various lengths from the beginning node, the path which has a lower coverage is removed.\n\nGiven a sequence of any length, the first step that needs done is to enter the sequence into a sequencing program, have it sequenced, and return base pair (bp) reads of a certain length. Since there is not a sequencing program that is completely accurate, there will always be some reads which contain errors. The most common sequencing method is the shotgun method, which is the method most probably used on sequence 2. Once a method is decided on, you have to specify the length of the bp reads you would like it to return. In the case of sequence 2, it returned 7-bp reads with all errors made during the process noted in red.\n\nOnce the reads are obtained, they are hashed into \"k\"-mers. The \"k\"-mers then are recorded in a table with how many times each \"k\"-mer appeared in the reads. For this example, each read was hashed into \"4\"-mers and if there was an error it was recorded in red. All of the \"4\"-mers were then recorded, with their frequency in the following table.\n\nEach individual cell of the table will then form a node, allowing a de Bruijn graph to be formed from the given \"k\"-mers. In figure 2, linear stretches are identified and then another graph, figure 3, is formed where the linear stretches have become a single node, of a different \"k\"-mer size, allowing for a more concise graph. In this simplified graph, it is easy to identify various tips and bubbles, as shown in figure 4. These bubbles and tips can then be removed, since we can identify that they were formed from errors in the bp reads, giving us a graph structure that should accurately and completely reflect the original sequence. If you follow the de Bruijn graph shown in figure 5, you will see that the sequence formed does indeed match the DNA sequence given in sequence 2.\n\nWhen comparing two strands of DNA, colored de Bruijn graphs are frequently used to identify errors. These errors, often polymorphisms, cause bubbles, similar to the ones mentioned above, to form. Currently there are four main algorithms used to generalize the data and locate bubbles. The four algorithms extend de Bruijn graphs by allowing the nodes and edges in the graph to be colored by the samples from which they were observed\n\nThe simplest use of a colored de Bruijn graph is known as the bubble calling algorithm. This algorithm looks, and locates, bubbles on the genome that differ from the original. These bubbles must be “clean”, or simply a divergence from the reference genome, but cannot be caused by deletions of DNA bases. This algorithm can have high false positive rates since there is a difficulty of separating repeat- and variant-induced bubbles; however, there is often a reference genome to help improve reliability. The reference genome also helps in the detection of variants and is essential to detect variant sites. Recently, scientists have discovered a way to use the bubble calling algorithm with copy number variation detection to allow for an opportunity of unbiased detection of these variations in the future\n\nWhen looking at complex variants, there is a very low chance that they will make a clean contig. Since this is the case most often, the path divergence algorithm is useful, especially when considering where deletions occur and the variant is so complex it is constrained to the reference allele. When there is a bubble formed, the path divergence algorithm is used the most frequently and allows detected bubbles to get deleted in a very systematic procedure. The algorithm first locates each point of divergence. Then from each point of divergence, the strands that form the bubble are traced to find where the two paths join after \"n\" nodes. If the two paths join, then the path with a lower coverage is removed and stored in a file.\n\nUsing multiple samples substantially improves the power and false discovery rate of detecting variants. In the simplest cases, the samples are combined into a group of a single color and the data is analyzed as described previously. However, by maintaining separate colors for each sample set, additional information on how the bubbles were formed, whether by error or by repeats, presents itself. In 1997, the Department of Technology Development at Genzyme Genetics in Framingham, Massachusetts developed a new approach that provided a breakthrough in dealing with bubbles using the multiplex allele-specific diagnostic assay (MASDA). This program combines forward dot-blot, complex simultaneous probe hybridization and direct mutation detection to help solve the dual problem of multiple sample analysis.\n\nThe colored de Bruijn graphs can be used to genotype any DNA sample at a known loci, even when the coverage is less than sufficient for variant assembly. The first step to this process is to construct a graph of the reference allele, known variants and data from the sample. The algorithm then calculates the likelihood of each genotype and accounts for the structure of the graph, both of the local and genome-wide sequence. This then generalizes to multiple allelic types and helps genotype complex and compound variants. This algorithm is used frequently, as there are no bubbles formed to deal with. This also directly helps find the more complicated issues in genes more direct than any of the three algorithms previously mentioned.\n"}
{"id": "7274018", "url": "https://en.wikipedia.org/wiki?curid=7274018", "title": "Debye–Falkenhagen effect", "text": "Debye–Falkenhagen effect\n\nThe increase in the conductivity of an electrolyte solution when the applied voltage has a very high frequency is known as Debye–Falkenhagen effect. Impedance measurements on water-p-dioxane and the methanol-toluene systems have confirmed Falkenhagen's predictions made in 1929.\n\n"}
{"id": "12538509", "url": "https://en.wikipedia.org/wiki?curid=12538509", "title": "Demonic tube-nosed fruit bat", "text": "Demonic tube-nosed fruit bat\n\nThe demonic tube-nosed fruit bat (\"Nyctimene masalai\") is a species of bat in the family Pteropodidae. It is endemic to Papua New Guinea. The holotype specimen was collected in 1979 on New Ireland, in the Bismarck Archipelago.\nIt was described as a new species in 1983.\nThe range of the species may extend to other islands, however the extent of the range is not presently known.\n\nBased on its morphology, it was placed into the \"cephalotes\" species group of its genus when it was described. Species groups are hypotheses of evolutionary relationships, so it is thought that the demonic tube-nosed fruit bat is most closely related to the other members of the species group:\nMore recently, others have placed it in the \"albiventer\" species group.\nOther members of the \"albiventer\" group included in this study were:\nThe specific epithet comes from the Tolai word \"masalai\", which Smith and Hood translate as \"forest demon or devil\".\nSome sources place the demonic tube-nosed fruit bat as synonymous with the common tube-nosed fruit bat, \"Nyctimene albiventer\".\n\nBats of the Nyctimene genus are distinguished by their elongated, tubular nostrils.\nIndividual bats are long.\nThey weigh .\nThe forearm is long.\nThe back is mottled reddish-brown, with a thin, black stripe running from the back of the head to the base of the tail.\nHairs on the dorsal side of the body are long, at long.\nThe ventral side is grayish-white with a yellowish-brown tint.\nThe flight membranes are dark brown.\nThe wings have pale yellowish-brown spots, while the ears have white spots.\nThe forearm also has large white splotches.\nThe nostrils are whitish.\nIts snout is short and narrow.\nIts dental formula is , for a total of 24 teeth.\n\nIts diet consists of fruit.\n\nSo far, it is only known from New Ireland, in the Bismarck Archipelago, which is part of Papua New Guinea.\nIt is possible that its range includes other islands in the Bismarck Archipelago, though this is currently unconfirmed.\nMembers of this species identified so far have been found at above sea level.\n\nIt is currently evaluated as data deficient by the IUCN.\nPreviously in 1996, the IUCN had assessed this species as vulnerable.\nThere has been little research regarding this species, and its population trend and geographic range are unknown.\nIt is unknown if there are any threats to this species.\n\n"}
{"id": "10986830", "url": "https://en.wikipedia.org/wiki?curid=10986830", "title": "Efraim Racker", "text": "Efraim Racker\n\nEfraim Racker (June 28, 1913 – September 9, 1991) was an Austrian biochemist who was responsible for identifying and purifying Factor 1 (F1), the first part of the ATP synthase enzyme to be characterised. F1 is only a part of a larger ATP synthase complex known as Complex V. It is a peripheral membrane protein attached to component Fo, which is integral to the membrane. \nEfraim Racker was born to a Jewish family in 1913 in Neu Sandez, Poland (then Austrian Galicia), but he grew up in Vienna. His elder brother, Heinrich Racker, was to become a famous psychoanalyst. Efraim Racker was studying medicine at the University of Vienna when Hitler invaded in 1938. Racker fled to Great Britain, where he took a job in a mental hospital in Wales. His research focused on the biochemical causes for mental diseases. During the war, Racker was given the opportunity to practice medicine, but he decided to move to the United States to continue his research.\n\nIn the U.S., he accepted a position as a research associate in physiology at the University of Minnesota in Minneapolis from 1941 to 1942. While investigating the biochemical basis for brain diseases, he discovered that the polio virus inhibited glycolysis in the brains of mice. He eventually left his research position for a job as a physician at the Harlem Hospital in New York City. In 1944 he became an associate professor of microbiology at the New York University Medical School, where he continued his work on glycolysis.\n\nIn 1952 he accepted a position at Yale Medical School, but left after two years to accept the position of chief of the Nutrition and Physiology Department at the Public Health Research Institute of the City of New York. It was here that Racker demonstrated that glycolysis was dependent on ATPase and the continuous regeneration of ADP and phosphate. Maynard E. Pullam joined Racker's staff in 1953, and decided to uncover the mechanism of ATP synthesis in mitochondria and chloroplasts. Joined by Anima Datta and Harvey S. Penefsky, they set out to identify the enzymes used in ATP synthesis.\n\nRacker left the Public Health Research Institute in 1966 to found the biochemistry department at Cornell University. He continued his research at Cornell, and was awarded many honors and prizes, including the Warren Triennial Prize in 1974, the National Medal of Science in 1976, the Gairdner Award in 1980, and the America Society of Biological Chemistry's Sober Memorial Lectureship. In addition, he was appointed to the American Academy of Arts and Sciences and the National Academy of Sciences. Efraim Racker died in 1991, leaving colleague and friend Dr. Mossaad Abdel-Ghany to take care of his lab and graduate students, but not before coining the phrase, \"Don’t waste clean thinking on dirty enzymes,\" which is often quoted as one of \"The Ten Commandments of Enzymology\".\n\nRacker and his associates, Anima Datta, Maynard Pullmand, and Harvey Penefsky, worked to isolate the enzymes involved in ATP synthesis. They observed that isolated mitochondrial fragments were capable of respiration but not able to synthesize ATP. Racker and his co-workers concluded that oxidative phosphorylation could be restored by addition of the supernatant from the centrifuging. The complex making this restoration possible was named Factor 1 or F1 as it is a necessary coupling factor for ATPase activity. This discovery of the first enzyme of oxidative phosphorylation was identified and purified in 1960.\n\nThe factor that binds F1 to the membrane, Fo, was discovered later in conjunction with Yasuo Kagawa. This particle was found to be sensitive to the antibiotic oligomycin and thus named Fo. This discovery had the added benefit of silencing any critics of the role of F1 in oxidative phosphorylation because it conferred oligomycin sensitivity on the ATPase activity complex.\nOnce both of these factors were identified Racker was able to confirm Peter D. Mitchell's hypothesis that contrary to popular opinion, ATP synthesis was not coupled to respiration through a high-energy intermediate but instead by a transmembrane proton gradient.\nF1 is a critical part of ATP synthesis within the mitochondria. In its absence, Complex V is not able to create the proton gradient necessary to produce ATP. It is responsible for coupling the oxidation of nutrients to the synthesis of ATP from ADP and inorganic phosphates.\n\nIn September 6, 1991, Racker was felled by a severe stroke, and died in Syracuse three days after.\n\n"}
{"id": "11915015", "url": "https://en.wikipedia.org/wiki?curid=11915015", "title": "Electro-gyration", "text": "Electro-gyration\n\nThe electrogyration effect is the spatial dispersion phenomenon, that consists in the change of optical activity (gyration) of crystals by a constant or time-varying electric field. Being a spatial dispersion effect, the induced optical activity exhibit different behavior under the operation of wave vector reversal, when compared with the Faraday effect: the optical activity increment associated with the electrogyration effect changes its sign under that operation, contrary to the Faraday effect. Formally, it is a special case of gyroelectromagnetism obtained when the magnetic permeability tensor is diagonal.\n\nThe electrogyration effect linear in the electric field occurs in crystals of all point groups of symmetry except for the three cubic – m3m, 432 and formula_1. The effect proportional to the square of the electric field can exist only in crystals belonging to acentric point groups of symmetry.\n\nThe changes in the optical activity sign induced by the external electric field have been observed for the first time in ferroelectric crystals LiH(SeO) by H. Futama and R. Pepinsky in 1961 \n, while switching enantiomorphous ferroelectric domains (the change in the point symmetry group of the crystal being 2/m«m). The observed phenomenon has been explained as a consequence of specific domain structure (a replacement of optic axes occurred under the switching), rather than the electrogyration induced by spontaneous polarization. \nThe first description of electrogyration effect induced by the biasing field and spontaneous polarization at ferroelectric phase transitions has been proposed by K. Aizu in 1963 on the basis of third-rank axial tensors (the manuscript received on September 9, 1963). Probably, K. Aizu has been the first who defined the electro-gyration effect (”the rate of change of the gyration with the biasing electric field at zero value of the biasing electric field is provisionally referred to as “electrogyration””) and introduced the term “electrogyration” itself. \nAlmost simultaneously with K. Aizu, I.S. Zheludev has suggested tensor description of the electrogyration in 1964 (the manuscript received on February 21, 1964). In this paper the electrogyration has been referred to as “electro-optic activity”.\nIn 1969, O.G. Vlokh has measured for the first time the electrogyration effect induced by external biasing field in the quartz crystal and determined the coefficient of quadratic electro-gyration effect (the manuscript received on July 7, 1969).\nThus, the electrogyration effect has been predicted simultaneously by Aizu K. and Zheludev I.S. in 1963–1964 and revealed experimentally in quartz crystals by Vlokh O.G. in 1969. \nLater in 2003, the gyroelectricity has been extended to gyroelectromagnetic media, which account for ferromagnetic semiconductors and engineered metamaterials, for which gyroelectricity and gyromagnetism (Faraday effect) may occur at the same time.\n\nThe electric field and the electric displacement vectors of electromagnetic wave propagating in gyrotropic crystals may be written respectively as:\n\nor\nwhere formula_4 is the optical frequency impermeability tensor, formula_5 the dielectric permittivity tensor, formula_6, formula_7 the mean refractive index, formula_8 - induction, formula_9, formula_10 polar third rank tensors, formula_11 the unit antisymmetric Levi-Civit pseudo-tensor, formula_12 the wave vector, and formula_13, formula_14 the second rank gyration pseudo-tensors. The specific rotation angle of the polarization plane formula_15 caused by the natural optical activity is defined by the relation:\nwhere formula_17 is the refractive index, formula_18 the wavelength, formula_19, formula_20 the transformation coefficients between the Cartesian and spherical coordinate systems (formula_21, formula_22), and formula_23 the pseudo-scalar gyration parameter.\nThe electro-gyration increment of gyration tensor occurred under the action of electric field formula_24 or/and formula_25 is written as:\n\nwhere formula_27 and formula_28 are third- and fourth-rank axial tensors describing the linear and quadratic electrogyration, respectively. In the absence of linear birefringence, electrogyration increment of the specific rotatory power is given by:\nThe electrogyration effect may be also induced by spontaneous polarization formula_30 appearing in the course of ferroelectric phase transitions \n\nThe electrogyration effect can be easy explained on the basis of Curie and Neumann symmetry principles. In the crystals that exhibit centre of symmetry, natural gyration can not exist, since, due to the Neumann principle, the point symmetry group of the medium should be a subgroup of the symmetry group that describes the phenomena, which are properties of this medium. As a result, the gyration tensor possessing a symmetry of second-rank axial tensor - formula_32 is not a subgroup of centrosymmetric media and so the natural optical activity cannot exist in such media. According to the Curie symmetry principle, external actions reduce the symmetry group of the medium down to the group defined by intersection of the symmetry groups of the action and the medium. When the electric field (with the symmetry of polar vector, formula_33) influences the crystal which possess the inversion centre, the symmetry group of the crystal should be lowered to the acentric one, thus permitting the appearance of gyration. However, in case of the quadratic electrogyration effect, the symmetry of the action should be considered as that of the dyad product formula_34 or, what is the same, the symmetry of a polar second-rank tensor (formula_35). Such a centrosymmetric action cannot lead to lowering of centrosymmetric symmetry of crystal to acentric states. This is the reason why the quadratic electrogyration exists only in the acentric crystals.\n\nIn a general case of light propagation along optically anisotropic directions, the eigenwaves become elliptically polarized in the presence of electrogyration effect, including rotation of the azimuth of polarization ellipse. Then the corresponding ellipticity formula_36 and the azimuth formula_37 are defined respectively by the relations\nwhere formula_40 is the polarization azimuth of the incident light with respect to the principal indicatrix axis, formula_41 the linear birefringence, formula_42 the phase retardation, formula_43, and formula_44. In the case of light propagation along optically isotropic directions (i.e., the optic axes), the eigenwave become circularly polarized (formula_45), with different phase velocities and different signs of circular polarization (left and right ones). Hence the relation (8) may be simplified so as to describe a pure polarization plane rotation:\nor \nwhere formula_48 - is the sample thickness along the direction of light propagation.\nFor the directions of light propagation far from the optic axis, the ellipticity formula_36 is small and so one can neglect the terms proportional to formula_50 in Eq.(8). Thus, in order to describe the polarization azimuth at formula_51 and the gyration tensor, simplified relations\n\nor\nare often used.\nAccording to Eq.(11), when the light propagates along anisotropic directions, the gyration (or the electro-gyration) effects manifest themselves as oscillations of the azimuth of polarization ellipse occurring with changing phase retardation formula_54 .\n\nThe electrogyration effect has been revealed for the first time in quartz crystals [2] as an effect quadratic in the external field. Later on, both the linear and quadratic \n, LiIO \n, PbMoO, NaBi(MoO), PbSiO(VO), PbSeO(VO), PbGeO(VO), alums \n, ferroelectric (TGS, Rochelle Salt, PbGeO and KDP families etc.) \n, as well as the photorefractive (BiSiO, BiGeO, BiTiO) materials \n\n. The electro-gyration effect induced by a powerful laser radiation (a so-called self-induced or dynamic electro-gyration) has been studied in the works \n. The influence of electro-gyration on the photorefraction storage has been investigated in \n, too. From the viewpoint of nonlinear electrodynamics, the existence of gradient of the electric field of optical wave in the range of the unit cell corresponds to macroscopic gradient of the external electrical field, if only the frequency transposition is taken into account. In that sense, the electrogyration effect represents the first of the gradient nonlinear optical phenomena ever revealed.\n\n"}
{"id": "3109206", "url": "https://en.wikipedia.org/wiki?curid=3109206", "title": "Endurance: Shackleton's Incredible Voyage", "text": "Endurance: Shackleton's Incredible Voyage\n\nEndurance: Shackleton's Incredible Voyage is a bestselling book written by Alfred Lansing. It was first published in 1959.\n\nThe book recounts the failure of the Imperial Trans-Antarctic Expedition led by Sir Ernest Shackleton in its attempt to cross the Antarctic continent in 1914 and the subsequent struggle for survival endured by the twenty-eight man crew for almost two years. The book's title refers to the ship Shackleton used for the expedition, the \"Endurance\". The ship was beset and eventually crushed by ice floes in the Weddell Sea leaving the men stranded on the pack ice. All in all the crew drifted on the ice for just over a year. They were able to launch their boats and somehow managed to land them safely on Elephant Island. Shackleton then led a crew of five aboard the \"James Caird\" through the Drake Passage and miraculously reached South Georgia Island 650 nautical miles away. He then took two of those men on the first successful overland crossing of the island. Three months later he was finally able to rescue the remaining crew members they had left behind on Elephant Island.\n\nVirtually every diary kept during the expedition was made available to the author and almost all the surviving members at the time of writing submitted to lengthy interviews. The most significant contribution came from Dr. Alexander Macklin, one of the ship's surgeons, who provided Lansing with many diaries, a detailed account of the perilous journey the crew made to Elephant Island, and months of advice.\n\n"}
{"id": "45061245", "url": "https://en.wikipedia.org/wiki?curid=45061245", "title": "Flow Cytometry Standard", "text": "Flow Cytometry Standard\n\nFlow Cytometry Standard (FCS) is a data file standard for the reading and writing of data from flow cytometry experiments. The FCS specification has traditionally been developed and maintained by the International Society for Advancement of Cytometry (ISAC). FCS used to be the only widely adopted file format in flow cytometry. Recently, additional standard file formats have been developed by ISAC.\n\nThe FCS file format describes a file that is a combination of textual data followed by binary data. The order of the file layout is as follows:\n\nThe HEADER segment is an ASCII text string that begins by identifying the version of the FCS standard used, followed by three pairs of byte offsets that designate the positions of the TEXT, DATA, and ANALYSIS segments. An example header segment is given below\n\nBecause the field width of the header segment byte positions is constrained by 8 characters, the maximum position it is capable of storing is 99,999,999. Anything beyond that is encoded as a 0 for both the start and end position, and the corresponding TEXT segment keyword is used instead.\n\nThe text segment is an ASCII text string that is divided into a series of key-value pairs that are delimited by some chosen character, e.g. '|'. The first character immediately following the header segment is the delimiter. An example of a header and text segment is given below\n\nTo be a valid FCS file, the text segment must contain all required keywords, which describe the DATA segment format and encoding. For FCS version 3.1, the required FCS primary TEXT segment keywords are as follows: \nThe DATA segment of the FCS file follows after the TEXT segment and is laid out event-wise (row-wise) according to the order described in the parameters (a.k.a. channels) $P1N $P2N...$PnN. An event is either an actual biological cell or some other mass that was large enough to trigger the data acquisition capturing device of the flow cytometer instrument. Data segments hold the following layout:\n\nEach event is laid out according to the number of bytes described by $PnB for each parameter. These bytes are to be interpreted according to the combination specified by $BYTEORD and $DATATYPE.\nFlow cytometry data is typically saved for analysis in the form of an array, with fluorescence and scatter channels represented in columns, and individual \"events\" (most of which are cells) forming the rows. \nThe number of events acquired from each sample usually ranges between the low thousands and the low millions.\nThe first version of a Flow Cytometry Standard (FCS) was developed in 1984. Since then, FCS became the standard file format supported by all flow cytometry software and hardware vendors. FCS is a binary file format with three main segments: a text segment containing meta data in keyword/value pairs structures, a data segment usually containing a matrix of detected expression values (so called list mode format), and a rarely used analysis segment.\n\nOver the years, updates were incorporated to adapt to technological advancements in both flow cytometry and computing technologies.\n\nIn 1990, FCS 2.0 was introduced. Features introduced in FCS 2.0 included the option of multiple data sets within a data file, the use of different byte orders accommodating hardware variations on different computing platforms, and basic compensation and scaling information. FCS 2.0 was followed by FCS 3.0 in 1997, which introduced the possibility of storing data sets larger than 100MB.\n\nThe latest version, FCS 3.1, was introduced in 2010. It retains the basic FCS file structure and most features of previous versions of the standard. Changes included in FCS 3.1 address potential ambiguities in the previous versions and provide a more robust standard. They include simplified support for international characters and improved support for storing compensation. The major additions are support for preferred display scale, a standardized way of capturing the sample volume, information about the origins of the data file, and support for plate and well identification in high throughput, plate based experiments.\n\n"}
{"id": "544817", "url": "https://en.wikipedia.org/wiki?curid=544817", "title": "Form (botany)", "text": "Form (botany)\n\nIn botanical nomenclature, a form (\"forma\", plural \"formae\") is one of the \"secondary\" taxonomic ranks, below that of variety, which in turn is below that of species; it is an infraspecific taxon. If more than three ranks are listed in describing a taxon, the \"classification\" is being specified, but only three parts make up the \"name\" of the taxon: a genus name, a specific epithet, and an infraspecific epithet.\n\nThe abbreviation \"f.\" or the full \"forma\" should be put before the infraspecific epithet to indicate the rank. It is not italicised.\n\nFor example:\n\nA form usually designates a group with a noticeable morphological deviation. The usual taxonomic practice is that the individuals classified within the form are not necessarily known to be closely related (they may not form a clade). For instance, white-flowered plants of species that usually have coloured flowers can be grouped and named (e.g., as \"f. \"alba\"\"). Formae apomicticae are sometimes named among plants that reproduce asexually, by apomixis. There are theoretically countless numbers of forms based on minor genetic differences, and only a few that have particular significance are likely to be named.\n\n"}
{"id": "20661185", "url": "https://en.wikipedia.org/wiki?curid=20661185", "title": "Gustavo Lozano Contreras", "text": "Gustavo Lozano Contreras\n\nWith Eduino Carbonó, he described 125 species of plants endemic to Sierra Nevada de Santa Marta in Colombia.\n\nHe served as Director of Botany in the Science Faculty of the National University of Colombia in Bogotá.\n"}
{"id": "509649", "url": "https://en.wikipedia.org/wiki?curid=509649", "title": "Informedia Digital Library", "text": "Informedia Digital Library\n\nThe Informedia Digital Library is an ongoing research program at Carnegie Mellon University to build search engines and information visualization technology for many types of media.\n\nThe program has carried out research on Spoken Document Retrieval, Video Information Retrieval, Video Segmentation, face recognition, and Cross-language information retrieval.\n\nThe Lycos search engine was an early product of the Informedia Digital Library Project.\n\nThe project is led by Howard Wactlar. Researchers on the project have included: Michael Mauldin, Alex Hauptmann, Michael Christel, Michael Witbrock, Raj Reddy, Takeo Kanade and Scott Stevens.\n\n"}
{"id": "14868", "url": "https://en.wikipedia.org/wiki?curid=14868", "title": "International Mathematical Union", "text": "International Mathematical Union\n\nThe International Mathematical Union (IMU) is an international non-governmental organization devoted to international cooperation in the field of mathematics across the world. It is a member of the International Council for Science (ICSU) and supports the International Congress of Mathematicians. Its members are national mathematics organizations from more than 80 countries.\n\nThe objectives of the International Mathematical Union (IMU) are: promoting international cooperation in mathematics, supporting and assisting the International Congress of Mathematicians (ICM) and other international scientific meetings/conferences, acknowledging outstanding research contributions to mathematics through the awarding of scientific prizes, and encouraging and supporting other international mathematical activities, considered likely to contribute to the development of mathematical science in any of its aspects, whether pure, applied, or educational.\n\nThe IMU was established in 1920, but dissolved in September 1932 and then re-established 1950 de facto at the Constitutive Convention in New York, de jure on September 10, 1951, when ten countries had become members. The last milestone was the General Assembly in March 1952, in Rome, Italy where the activities of the new IMU were inaugurated and the first Executive Committee, President and various commissions where elected. In 1952 the IMU was also readmitted to the ICSU. The past president of the Union is Ingrid Daubechies (2011–2014). The current president is Shigefumi Mori who is the first head of the group from Asia.\n\nAt the 16th meeting of the IMU General Assembly in Bangalore, India, in August 2010, Berlin was chosen as the location of the permanent office of the IMU, which was opened on January 1, 2011, and is hosted by the Weierstrass Institute for Applied Analysis and Stochastics (WIAS), an institute of the Gottfried Wilhelm Leibniz Scientific Community, with about 120 scientists engaging in mathematical research applied to complex problems in industry and commerce.\n\nIMU has a close relationship to mathematics education through its International Commission on Mathematical Instruction (ICMI). This commission is organized similarly to IMU with its own Executive Committee and General Assembly.\n\nDeveloping countries are a high priority for the IMU and a significant percentage of its budget, including grants received from individuals, mathematical societies, foundations, and funding agencies, is spent on activities for developing countries. Since 2011 this has been coordinated by the Commission for Developing Countries (CDC).\n\nThe Committee for Women in Mathematics (CWM) is concerned with issues related to women in mathematics worldwide. It organizes the World Meeting for Women in Mathematics formula_1 as a satellite event of ICM.\n\nThe International Commission on the History of Mathematics (ICHM) is operated jointly by the IMU and the Division of the History of Science (DHS) of the International Union of History and Philosophy of Science (IUHPS).\n\nThe Committee on Electronic Information and Communication (CEIC) advises IMU on matters concerning mathematical information, communication, and publishing.\n\nThe scientific prizes awarded by the IMU are deemed to be the highest distinctions in the mathematical world. The opening ceremony of the International Congress of Mathematicians (ICM) is where the awards are presented: Fields Medals (two to four medals are given since 1936), the Rolf Nevanlinna Prize (since 1986), the Carl Friedrich Gauss Prize (since 2006), and the Chern Medal Award (since 2010).\n\nThe IMU's members are Member Countries and each Member country is represented through an Adhering Organization, which may be its principal academy, a mathematical society, its research council or some other institution or association of institutions, or an appropriate agency of its government. A country starting to develop its mathematical culture and interested in building links to mathematicians all over the world is invited to join IMU as an Associate Member. For the purpose of facilitating jointly sponsored activities and jointly pursuing the objectives of the IMU, multinational mathematical societies and professional societies can join IMU as an Affiliate Member. Every four years the IMU membership gathers in a General Assembly (GA) which consists of delegates appointed by the Adhering Organizations, together with the members of the Executive Committee. All important decisions are made at the GA, including the election of the officers, establishment of commissions, the approval of the budget, and any changes to the statutes and by-laws.\n\nThe International Mathematical Union is administered by an Executive Committee (EC) which conducts the business of the Union. The EC consists of the President, two Vice-Presidents, the Secretary, six Members-at-Large, all elected for a term of four years, and the Past President. The EC is responsible for all policy matters and for tasks, such as choosing the members of the ICM Program Committee and various prize committees.\n\nEvery two months IMU publishes an electronic newsletter, \"IMU-Net\", that aims to improve communication between IMU and the worldwide mathematical community by reporting on decisions and recommendations of the Union, major international mathematical events and developments, and on other topics of general mathematical interest. IMU Bulletins are published annually with the aim to inform IMU’s members about the Union’s current activities. In 2009 IMU published the document \"Best Current Practices for Journals\".\n\nThe IMU took its first organized steps towards the promotion of mathematics in developing countries in the early 1970s and has, since then supported various activities. In 2010 IMU formed the Commission for Developing Countries (CDC) which brings together all of the past and current initiatives in support of mathematics and mathematicians in the developing world.\n\nSome IMU Supported Initiatives:\n\nIMU also supports the \"International Commission on Mathematical Instruction\" (ICMI) with its programmes, exhibits and workshops in emerging countries, especially in Asia and Africa.\n\nIMU released a report in 2008, \"Mathematics in Africa: Challenges and Opportunities\", on the current state of mathematics in Africa and on opportunities for new initiatives to support mathematical development. In 2014, the IMU's Commission for Developing Countries CDC released an update of the report.\n\nAdditionally, reports about \"Mathematics in Latin America and the Caribbean and South East Asia\". were published.\n\nIn July 2014 IMU released the report: The International Mathematical Union in the Developing World: Past, Present and Future (July 2014).\n\nIn 2014, the IMU held a day-long symposium prior to the opening of the International Congress of Mathematicians (ICM), entitled \"Mathematics in Emerging Nations: Achievements and Opportunities\" (MENAO). Approximately 260 participants from around the world, including representatives of embassies, scientific institutions, private business and foundations attended this session. Attendees heard inspiring stories of individual mathematicians and specific developing nations.\n\n\nList of presidents of the International Mathematical Union from 1952 to the present:\n\n1952–1954: Marshall Harvey Stone (vice: Émile Borel, Erich Kamke)\n\n1955–1958: Heinz Hopf (vice: Arnaud Denjoy, W. V. D. Hodge)\n\n1959–1962: Rolf Nevanlinna (vice: Pavel Alexandrov, Marston Morse)\n\n1963–1966: Georges de Rham (vice: Henri Cartan, Kazimierz Kuratowski)\n\n1967–1970: Henri Cartan (vice: Mikhail Lavrentyev, Deane Montgomery)\n\n1971–1974: K. S. Chandrasekharan (vice: Abraham Adrian Albert, Lev Pontryagin)\n\n1975–1978: Deane Montgomery (vice: J. W. S. Cassels, Miron Nicolescu, Gheorghe Vrânceanu)\n\n1979–1982: Lennart Carleson (vice: Masayoshi Nagata, Yuri Vasilyevich Prokhorov)\n\n1983–1986: Jürgen Moser (vice: Ludvig Faddeev, Jean-Pierre Serre)\n\n1987–1990: Ludvig Faddeev (vice: Walter Feit, Lars Hörmander)\n\n1991–1994: Jacques-Louis Lions (vice: John H. Coates, David Mumford)\n\n1995–1998: David Mumford (vice: Vladimir Arnold, Albrecht Dold)\n\n1999–2002: Jacob Palis (vice: Simon Donaldson, Shigefumi Mori)\n\n2003–2006: John M. Ball (vice: Jean-Michel Bismut, Masaki Kashiwara)\n\n2007–2010: László Lovász (vice: Zhi-Ming Ma, Claudio Procesi)\n\n2011–2014: Ingrid Daubechies (vice: Christiane Rousseau, Marcelo Viana)\n\n2015–2018: Shigefumi Mori (vice: Alicia Dickenstein, Vaughan Jones)\n\n2019–2022: Carlos Kenig (vice: Nalini Joshi, Loyiso Nongxa )\n\n\n"}
{"id": "34576538", "url": "https://en.wikipedia.org/wiki?curid=34576538", "title": "James Scott Maclaurin", "text": "James Scott Maclaurin\n\nJames Scott Maclaurin (8 November 1864 – 19 January 1939) was a New Zealand analytical chemist. He was born in Unst, Shetland, Scotland on 8 November 1864.\n"}
{"id": "3981465", "url": "https://en.wikipedia.org/wiki?curid=3981465", "title": "Juice fasting", "text": "Juice fasting\n\nJuice fasting, also known as juice cleansing, is a fad diet in which a person consumes only fruit and vegetable juices while abstaining from solid food consumption. It is used for detoxification, an alternative medicine treatment, and is often part of detox diets. The diet can typically last for two to seven days and involve a number of fruits and vegetables and even spices that are not among the juices typically sold or consumed in the average Western diet. \n\nThis diet is sometimes promoted with implausible and unsubstantiated claims about its health benefits.\n\nCatherine Collins, chief dietician of St George's Hospital Medical School in London, England, states that: \"The concept of ‘detox’ is a marketing myth rather than a physiological entity. The idea that an avalanche of vitamins, minerals, and laxatives taken over a 2 to 7 day period can have a long-lasting benefit for the body is also a marketing myth.\"\n\nDetox diets, depending on the type and duration, are potentially dangerous and can cause various health problems including muscle loss and an unhealthy regaining of fat after the detox ends.\n\nJuice mixes containing grapefruit juice may adversely interact with some prescription drugs.\n\n"}
{"id": "21501041", "url": "https://en.wikipedia.org/wiki?curid=21501041", "title": "Largest prehistoric animals", "text": "Largest prehistoric animals\n\nThe largest prehistoric organisms include both vertebrate and invertebrate species. Many are described below, along with their typical range of size (for the general dates of extinction, see the link to each). Many species mentioned might not actually be the largest representative of their clade due to the incompleteness of the fossil record and many of the sizes given are merely estimates since no complete specimen have been found. Their body mass, especially, is mostly conjecture because soft tissue was rarely fossilized. Generally the size of extinct species was subject to energetic and biomechanical constraints.\n\nThe largest-known monotreme (egg-laying mammal) ever was the extinct long-beaked echidna species known as \"Zaglossus hacketti\", known from a couple of bones found in Western Australia. It was the size of a sheep, weighing probably up to .\n\n\n\n\nThe largest cingulate known is \"Doedicurus\", at long and reaching a mass of approximately \"Glyptodon\" easily topped and .\n\nThe largest-known animal of this group was \"Deinogalerix\", measuring up to in total length, with a skull up to long. It occupied the same ecological niche as dogs and cats today.\n\nThe largest-known prehistoric lagomorph is Minorcan giant lagomorph (\"Nuralagus rex\") at .\nThe largest-known cimolestid is \"Coryphodon\", high at the shoulder and long.\n\n\n\n\n\n\nThe largest-known astrapotherians weighed about , including the genus \"Granastrapotherium\" and some species of \"Parastrapotherium\" (\"P. martiale\").\n\nThe largest-known arsinoithere was \"Arsinoitherium\". When alive, it would have been tall at the shoulders, and long.\n\nThe largest-known condylarth is \"Phenacodus\". It was long and weighted up to .\n\nThe largest-known dinoceratan was \"Uintatherium\". It was about the size of a rhinoceros. Despite its large size, it had a brain only about as large as an orange.\n\nThe largest-known desmostylian was a species comparable in size to the Steller's Sea Cow.\n\nThe largest-known litoptern was \"Macrauchenia\", which had three hoofs per foot. It was a relatively large animal, with a body length of around .\n\nThe largest notoungulate known of complete remains is \"Toxodon\". It was about in body length, and about high at the shoulder and resembled a heavy rhinoceros. Although is not complete, the preserved fossils suggests that \"Mixotoxodon\" were the most massive member of the group, with a weight about .\n\nThe largest-known oxyaenid was \"Sarkastodon\" weighing in at .\n\nThe largest hyaenodontid was \"Megistotherium\" at 500 kg.\n\nThe largest mesonychid was \"Mongolonyx robustus\". This predator would have been tall at the shoulders and long.\n\nThe herbivorous \"Cotylorhynchus hancocki\" was the largest of the pelycosaurs, with an estimated length and weight of at least and . The biggest carnivorous pelycosaur was \"Dimetrodon angelensis\", which could reach and . The largest members of the genus Dimetrodon was also the world's first fully terrestrial apex predators.\n\nThe plant-eating dicynodont \"Lisowicia bojani\" is the largest-known of all non-mammal synapsids, at and . Among the largest carnivorous synapsids was the therapsid \"Anteosaurus\", which was long, and weighed .\n\n\n\nThe longest known plesiosauroid was \"Styxosaurus\" at long. Other elasmosaurs, such as \"Albertonectes\" and \"Thalassomedon\", rivaled the aforementioned \"Styxosaurus\" in size.\n\nThere is much controversy over the largest-known of these reptiles. Fossil remains of a pliosaur nicknamed as \"Predator X\" have been discovered and excavated from Norway in 2008. This pliosaur has been estimated at in length and in weight. However, in 2002, a team of paleontologists in Mexico discovered the remains of a pliosaur nicknamed as \"Monster of Aramberri\", which is also estimated at in length. This species is, however, claimed to be a juvenile and has been attacked by a larger pliosaur. Some media sources claimed that \"Monster of Aramberri\" was a \"Liopleurodon\" but its species is unconfirmed thus far.\nAnother very large pliosaur was \"Pliosaurus macromerus\", known from a single incomplete mandible. It may have reached , assuming the skull was about 17% of the total body length.\n\nThe largest-known ichthyosaur was \"Shastasaurus sikanniensis\" at in length. In April 2018, paleontologists announced the discovery of a previously unknown ichthyosaur that may have reached lengths of 26 m (85 ft)- 30 m (100 feet long) making it one of the largest animals known, rivaling the Blue Whale in size. Another giant Ichthyosaur was found as well and it was larger than Lilstock Monster, possibly surpassing Blue Whale in size.\n\n\nThe heavy built \"Moradisaurus grandis\", with a length of , is the largest known captorhinid.\n\nThe largest-known is \"Scutosaurus\", up to in length, with bony armor, and a number of spikes decorating its skull.\n\nSome of the largest-known phytosaurs include \"Redondasaurus\" with a length of 6.4 meters and \"Smilosuchus\" with a size of 6.8 meters.\n\n\n\nMany large sauropods are still unnamed and may rival the current record holders.\n\n\nThe largest-known thyreophoran was \"Ankylosaurus\" at in length and in weight. \"Stegosaurus\" was also long but around tonnes in weight.\n\nThe largest ceratopsian known is \"Triceratops horridus\", along with the closely related \"Eotriceratops xerinsularis\" both with estimated lengths of . Ojoceratops and several other ceratopsians rival them in size.\n\nThe very largest-known ornithopods, like \"Shantungosaurus\" were as heavy as medium-sized sauropods at up to , and in length.\n\nThe largest-known birds of all time might have been the elephant birds of Madagascar. Of almost the same size was the Australian \"Dromornis stirtoni\". Both were about tall. The elephant birds were up to and \"Dromornis stirtoni\" was up to in weight. The tallest bird ever was the giant moa (\"Dinornis maximus\") at tall.\n\nThe largest-known flight-capable bird was \"Argentavis magnificens\" which a wingspan of , and a body weight of .\n\nThe largest-known waterfowl of all time belonged to the Dromornithidae (e.g. \"Dromornis stirtoni\").\n\nThe largest-known of Ciconiiformes was \"Leptoptilos robustus\", standing tall and weighing an estimated .\n\nThe largest-known of the hesperornithines was \"Canadaga arctica\" at long.\n\nThe largest-known diatryma was \"Gastornis\" tall, with large individuals up to tall.\n\nThe largest-known teratorn and the largest flying bird ever was \"Argentavis\". The immense bird had a wingspan estimated up to and a weight up to . It was as high as an adult human when standing.\n\nThe largest-known-ever gruiform and largest phorusrhacid or \"terror bird\" (highly predatory, flightless birds of South America) was \"Brontornis\", which was about tall at the shoulder, could raise its head above the ground and could have weighed as much as . The immense phorushacid \"Kelenken\" stood tall with a skull long ( of which was beak), had the largest head of any known bird. The largest North American phorusrhacid is \"Titanis\", which is about tall, as tall as a forest elephant.\n\nThe largest-known bird of prey ever was the enormous Haast's eagle (\"Harpagornis moorei\"), with a wingspan of , relatively short for their size. Total length was probably up to in female and they weighed about . The largest extinct \"Titanohierax\" was a giant hawk about 8 kilograms that lived in the Antilles, where it was among the top predators.\nThe largest-known in this group was a giant flightless \"Sylviornis\", a bird long and weighing up to about .\n\nThe largest-known songbird is the extinct giant grosbeak (\"Chloridops regiskongi\") at long.\n\nThe largest-known cormorant was the spectacled cormorant of the North Pacific (\"Phalacrocorax perspicillatus\"), which became extinct around 1850, was larger still, averaging around and .\n\nThe largest-known in this group—which has been variously allied with Procellariiformes, Pelecaniformes and Anseriformes—and the largest flying birds of all time other than \"Argentavis\" were the huge \"Pelagornis\", \"Cyphornis\", \"Dasornis\", \"Gigantornis\" and \"Osteodontornis\". They had a wingspan of and stood about tall. Exact size estimates and judging which one was largest are not yet possible for these birds, as their bones were extremely thin-walled, light and fragile, and thus most are only known from very incomplete remains.\n\nThe largest-known woodpecker is the possibly extinct imperial woodpecker (\"Campephilus imperialis\") with a total length of about . The largest woodpecker confirmed to be extant is the great slaty woodpecker (\"Mulleripicus pulverulentus\").\n\nThe largest-known parrot is the extinct Norfolk Island kaka (\"Nestor productus\"), about long.\n\nThe largest-known penguin of all time was \"Anthropornis nordenskjoeldi\" of New Zealand and Antarctica. It stood in height and was in weight. Similar in size were the New Zealand giant penguin (\"Pachydyptes pondeorsus\") with a height of and weighing possibly around and over, and \"Icadyptes salasi\" at tall.\n\nThe largest-known owl of all time was the Cuban \"Ornimegalonyx\" at tall probably exceeding .\n\nThe largest-known amphibian of all time was the long temnospondyli \"Prionosuchus\".\nAnother huge temnospondyli was \"Koolasuchus\" at long, but only high.\n\nThe largest-known frog ever was a yet unnamed species frog that was about .\n\nThe largest-known diacectid, \"Diadectes\", was a heavily built animal, long, with thick vertebrae and ribs.\n\nThe largest-known anthracosaur was \"Anthracosaurus\", a predator. It could reach up to in length. \"Eogyrinus\" commonly reached ; however, it was more lightly built.\n\nThe largest-known temnospondyl amphibian is \"Prionosuchus\", which grew to lengths of .\nThe largest-known of these was the long \"Rhizodus\".\nThe largest-known bony fish of all time was the pachycormid, \"Leedsichthys problematicus\", at around long. Claims of larger individuals persist.\n\nThe largest-known of ichthyodectiform fish was \"Xiphactinus\", which measured long.\nAn extinct megatoothed shark, \"C. megalodon\" is by far the biggest shark known. This giant shark reached a total length of more than . \"C. megalodon\" may have approached a maximum of in total length and in mass.\n\nThe largest-known symmoriid was \"Stethacanthus\" at long.\n\nThe largest-known eugenedont is a yet unnamed species of \"Helicoprion\" discovered in Idaho. The specimens suggest an animal that possibly exceeded in length.\n\nAnother fairly large eugenedont is Parahelicoprion. The specimens suggest an animal that grew to the same size (), but was much less slender and overall less heavy.\n\nThe largest-known hydontiformid is \"Ptychodus\", which was about long.\n\nThe largest-known placoderm was the long \"Dunkleosteus\". It is estimated to have weighed around . Its relative, \"Titanichthys\", may have rivaled it in size.\n\nThe largest-known is \"Aegirocassis\" at least long.\n\nThe largest-known in this group was \"Jaekelopterus rhenaniae\" at in length. A close contender was \"Pterygotus\" at in length.\n\nThere are two contenders for largest-known ever arachnid: \"Pulmonoscorpius kirktonensis\" and \"Brontoscorpio anglicus\". Pulmonoscorpius was Brontoscorpio was . The biggest difference is that \"Brontoscorpio\" was aquatic, and \"Pulmonoscorpius\" was terrestrial. \"Brontoscorpio\" is not to be confused with various Eurypterids: it was a true scorpion with a venomous stinger.\n\nThe largest-known myriapod by far was the giant \"Arthropleura\". Measuring and wide, it was the largest-known terrestrial arthropod of all time. Like its modern-day relatives, Arthropleura would have likely sprayed hydrogen cyanide at potential predators, although its sheer size and tough exoskeleton protected it from attack.\n\nSome of these extinct marine arthropods exceeded in length. A nearly complete specimen of \"Isotelus rex\" from Manitoba attained a length over , and an \"Ogyginus forteyi\" from Portugal was almost as long. Fragments of trilobites suggest even larger record sizes. An isolated pygidium of \"Hungioides bohemicus\" implies that the full animal was long.\n\nThe largest-known of this group was the giant ant \"Titanomyrma giganteum\" at , with queens growing to . It had a wingspan of .\n\nThe largest-known in this group was probably \"Meganeura\" with a wingspan of . Another enormous and possibly larger species was \"Meganeuropsis permiana\".\n\nThe largest-known in this group was probably \"Saurophthirus\", growing to in length. It possibly sucked the blood of pterosaurs.\n\nThe largest-known insect of this order was \"Mazothairos\", with a wingspan of up to .\n\nThe largest-known of this group were in the genus \"Campanile\", with the extinct \"Campanile giganteum\" having shell lengths up to .\n\nThe largest-known bivalve ever was \"Platyceramus platinus\", a giant that usually had an axial length of , but some individuals could reach an axial length of up to .\n\nThe largest-known ammonite was \"Parapuzosia seppenradensis\". A partial fossil specimen found in Germany had a shell diameter of , but the living chamber was incomplete, so the estimated shell diameter was probably about when it was alive.\n\nThe largest-known belemnite was \"Megateuthis gigantea\" with a guard of in length and an estimated total length long.\n\nThe longest- and largest-known of this group was \"Cameroceras\" with a shell length of .\n\nBoth \"Tusoteuthis\" and \"Yezoteuthis\" are estimated to be similar in size to the modern-day giant squid.\n\n"}
{"id": "22315662", "url": "https://en.wikipedia.org/wiki?curid=22315662", "title": "Leonard Hussey", "text": "Leonard Hussey\n\nLeonard Duncan Albert Hussey, OBE (6 June 1891 – 25 February 1964) was an English meteorologist, archaeologist, explorer, medical doctor and member of Ernest Shackleton's Imperial Trans-Antarctic and Shackleton–Rowett Expeditions. During the latter, he was with Shackleton at his death, and transported the body part-way back to England.\n\nHussey was also a member of the armed forces during World War I, serving in France and with Shackleton in Russia. After returning to private practice, Hussey rejoined the war effort in 1940 and became a decorated medical officer with the Royal Air Force during the Second World War. Returning once again to civilian practice in 1946, he was a member of the Royal College of Physicians, a lecturer, author, and Boy Scouts leader prior to retirement. Many of the items he collected during his career were donated to a number of museums.\n\nHussey was born to James and Eliza Hussey (née Aitken) in Norman House, Norman Road in Leytonestone, in London. He was the sixth child of the family, and the only son to his father, a machine ruler at the local stationery printing factory. By 1900, the family resided at 342 Kingsland Road in Leytonestone with three further children, leaving Hussey one of eleven children; the eldest of four boys – the others being James, William and Percy – alongside five daughters, Maude, Beatrice, May, Blanche and Daisy.\n\nHussey attended Strand School and Hackney P.T. Centre. On 6 October 1909, Hussey entered the University of London, taking as course in psychology and gaining a bachelor of science second class at King's College London, as well as degrees in meteorology and anthropology.\n\nFrom 1913, Hussey had undertaken employment as an anthropologist and archaeologist at a dig in Jebel Moya, Sudan as part of Henry Wellcome's Expedition. Alongside O. G. S. Crawford, Hussey worked on a monthly pay of £8. While in the Sudan, he read in a month-old newspaper about Shackleton's intention to embark on an Antarctic Expedition, and 'The idea gripped me'. He wrote to Shackleton expressing his interest in joining the project. Shackleton replied telling Hussey to call on him when he returned to London. 'My luck in this respect was later explained to me by Shackleton, who said he was greatly amused to find amongst nearly five thousand applications to join the expedition, one that came from the heart of Africa.' Shackleton agreed to select him for the expedition, later telling him that he did so because he \"looked funny\".\n\nHussey joined the Imperial Trans-Antarctic Expedition of 1914–1917's Weddell Sea party as a meteorologist, keeping a leather-bound diary of the entire expedition. He was a popular member of the group due to his humour and perpetual playing of his five-string banjo, in company with Dr. James McIlroy's imitations of the trombone and bagpipes. Frank Worsley stated \"Hussey was a brilliant wit, and his keen repartee was one of the few joys left to us. Often we would combine to provoke him just for the pleasure of hearing his clever retorts and invariably he would emerge the victor, no matter how many of us tried to best him.\" On 22 June 1915, Hussey and the crew staged a four-hour \"smoking concert\" and costume party, during which Hussey dressed as a black minstrel. Roland Huntford recorded in \"Shackleton\" of Hussey's tendency to be \"determinedly cheery to the point of egregiousness\".\n\nThe instrument, weighing twelve pounds, was rescued from the wreck of the \"Endurance\" as \"vital mental medicine\" by Shackleton, who made an exception of his instructions that each person could take only 2 pounds in weight of personal belongings, so that it could be saved. On 24 April 1916, while Shackleton took five other men from camp on South Georgia on the \"James Caird\" to find help, Hussey was one of the 22 men left behind on Elephant Island to await rescue, and continued to use his banjo to improve morale.\n\nHussey initially joined the London University contingent of the Officers' Training Corps, and was commissioned as a second lieutenant in the cadets on 13 November 1912. Hussey progressed to the full armed forces in the later years of the First World War, and was commissioned as a temporary second lieutenant in the Royal Garrison Artillery on 19 January 1917, serving in France, as well as operating with Shackleton on operations at Murmansk in northern Russia as part of the Polar Bear Expedition. He was promoted to temporary lieutenant on 19 July 1918, and to the temporary rank of captain on 8 October. Hussey was demobilised on 14 May 1919, retaining the rank of captain.\n\nFollowing the end of World War I, Hussey qualified in medicine and returned to Shackleton for his expedition to Antarctica aboard the \"Quest\" in 1921–22. \n\nHussey was asked by Frank Wild, following Shackleton's death in harbour in South Georgia, to escort the body to England while Wild himself assumed command of the \"Quest\". Hussey arrived in South America and cabled England with news of the explorer's passing. Shackleton's widow responded that the explorer should be buried at Grytviken in South Georgia, and Hussey carried out these instructions.\n\nFollowing his return to England, Hussey practised medicine in London until 1940. He had become a member of the Royal College of Surgeons and a Licentiate of the Royal College of Physicians, while residing at 328 Clapham Road, in London.\n\nHussey joined the RAF as a medical officer during the Second World War, with the rank of flight lieutenant and the serial number 87314. He served in Iceland as First Senior Medical Officer in the temporary rank of squadron leader, to which he was promoted on 1 July 1943 and then at RAF Benson in Oxfordshire. He was Mentioned in Despatches on two occasions in 1945, on 1 January and 14 May. Appointed an Officer of the Order of the British Empire in the 1946 New Year Honours, Hussey retained his links to the RAF for a time after the end of the war, serving in the Royal Air Force Volunteer Reserve as a squadron leader, until his retirement on 10 February 1954.\n\nFollowing the end of the war, Hussey resumed his medical practice, operating as a GP in Hertfordshire up until 1957. He was appointed Officer of the Order of the British Empire (OBE) in the 1946 New Year Honours for his war service, and in 1949 he served on the SS \"Clan Macauley\" as a ship's surgeon, sailing from England to South Africa and Australia. He also published his account of the Trans-Antarctic expedition entitled \"South with Endurance\". By 1957, having retired from his practice, Hussey became President of the Antarctic Club. A Shackleton-Hussey trophy was created and awarded by Hussey to several scout movements from the 1960s, including the Chorleywood Scout pack, of which Hussey was the at one time the President of as part of his growing involvement in the scout movement following his retirement.\n\nIn 1960, Hussey retired to Worthing, and was forced to curtail his lecturing career due to poor health. His notes and lantern slides were donated to Ralph Gullett, a local scout leader and friend, who took over his lecturing. His banjo, which he took along with him on expeditions and later had on display at his practice, was signed by all the members of \"Endurance\", and donated to the National Maritime Museum and was valued in 2004 at over £150,000. Hussey died in London in 1964 at the age of 72, and was survived by his wife, who died in 1980. The couple had no children, and his estate was passed to his housekeeper, Margaret Mock, until her own death in 1999.\n\nHussey published a number of works throughout his life, including editing the records of both of Shackleton's expeditions, and a number of articles in partnership with other expedition members.\n\n\nHe was also the recipient of a number of decorations for his work in both world wars and as a member of Shackleton's expeditions.\n\n\n\n\n"}
{"id": "49636205", "url": "https://en.wikipedia.org/wiki?curid=49636205", "title": "Lepersonnite-(Gd)", "text": "Lepersonnite-(Gd)\n\nLepersonnite-(Gd) is a very rare rare-earth and uranium mineral with the formula Ca(Gd,Dy)(UO)(SiO)(CO)(OH)·48HO. It occurs associated with bijvoetite-(Y) in the Shinkolobwe deposit in the Democratic Republic of the Congo, famous for rare uranium minerals. Lepersonnite-(Gd) is unique in being the only confirmed mineral with essential gadolinium.\n"}
{"id": "3086038", "url": "https://en.wikipedia.org/wiki?curid=3086038", "title": "List of Kyoto Prize winners", "text": "List of Kyoto Prize winners\n\nThis is a list of Kyoto Prize winners, awarded annually by the Inamori Foundation.\n\nSource: Kyoto Prize\nSource: Kyoto Prize\nSource: Kyoto Prize\n\n"}
{"id": "1885549", "url": "https://en.wikipedia.org/wiki?curid=1885549", "title": "List of Schedule 1 substances (CWC)", "text": "List of Schedule 1 substances (CWC)\n\nSchedule 1 substances, in the sense of the Chemical Weapons Convention, are chemicals which can either be used as chemical weapons themselves or used in the manufacture of chemical weapons and which have no, or very limited, uses outside of chemical warfare.\n\nThese may be produced or used for research, medical, pharmaceutical or chemical weapon defence testing (called \"protective testing\" in the treaty) purposes but production above 100 grams per year must be declared to the OPCW in accordance with Part VI of the \"Verification Annex\". A country is limited to possessing a maximum of one tonne of these materials.\n\nThey are sub-divided into Part A substances, which are chemicals that can be used directly as weapons, and Part B which are precursors useful in the manufacture of chemical weapons. Examples are mustard and nerve agents, and substances which are solely used as precursor chemicals in their manufacture. A few of these chemicals have very small-scale non-military applications; for example, minute quantities of nitrogen mustard are used to treat certain cancers.\n\nChemicals which can be used as weapons, or used in their manufacture, but which have legitimate applications as well are listed in Schedule 2 (small-scale applications) and Schedule 3 (large-scale applications).\n\nThe following criteria shall be taken into account in considering whether a toxic chemical or precursor should be included in Schedule 1:\n\n\n\n\n\n\n"}
{"id": "325218", "url": "https://en.wikipedia.org/wiki?curid=325218", "title": "List of countries by system of government", "text": "List of countries by system of government\n\nThis is a list of countries by system of government. There is also a political mapping of the world that shows what form of government each country has, as well as a brief description of what each form of government entails. The list is colour-coded according to the type of government, for example: blue represents a republic with an executive head of state, and pink is a constitutional monarchy with a ceremonial head of state. The colour-coding also appears on the following map, representing the same government categories. The legend of what the different colours represent is found just below the map.\n\nNote that Afghanistan, Iran, Pakistan, and Mauritania are Islamic Republics.\n\nThe following states control their territory and are recognized by at least one member state.\n\nThe following states/governments control their territory, but are not recognised by any UN member state.\n\n\nNote: this chart represent \"de jure\" systems of government, not the \"de facto\" degree of democracy. Several states constitutionally republics, broadly appear as authoritarian states.\n\n\"Italics\" indicate states with limited recognition.\n\nThese are systems in which a president is the active head of the executive branch of government, and is elected and remains in office independently of the legislature.\n\nIn full presidential systems, the president is both head of state and head of government. There is generally no prime minister, although if one exists, in most cases, he or she serves purely at the discretion of the president (with the exceptions being Belarus and Kazakhstan, where the prime minister is effectively the head of government).\n\nThe following list includes democratic and non-democratic states:\n\nThe President has full executive powers.\n\n\nNote: Iran may be considered to be a theocracy as the government is intertwined with the religious hierarchy\n\nIn semi-presidential systems, there is always both a president and a prime minister. In such systems, the president has genuine executive authority, unlike in a parliamentary republic, but the role of a head of government may be exercised by the prime minister.\n\nThe president chooses the prime minister and cabinet, but only the parliament may remove them from office with a \"vote of no confidence\". The president does \"not\" have the right to dismiss the prime minister or the cabinet.\nThe president chooses the prime minister without the confidence vote from the parliament, and the cabinet but must have the support of the parliament majority for his choice. In order to remove a prime minister or the whole cabinet from power, the president can dismiss them or the assembly can remove them by a \"vote of no confidence\".\n\nA parliamentary republic is a system in which a prime minister is the active head of the executive branch of government and also leader of the legislature. The president's degree of executive power may range from being significant (e.g. South Africa), to little (e.g. India), to only having certain limited reserve powers (e.g. Ireland) or none at all (e.g. Germany). Where the president holds little executive power, his or her function is primarily that of a symbolic figurehead.\n\n\nA combined head of state and government is elected by the legislature in the form of an executive president, however they are not subject to parliamentary confidence during their term (although their cabinet may be); the exceptions are South Africa, where the president may be forced to resign by the Parliament's will, and Kiribati, where the president is popularly elected and a successful parliamentary motion of no confidence automatically triggers a new presidential election.\nIn the directorial system a council jointly exercises both state functions and governmental powers (the council is the collective head of state and government). The council is elected by the parliament, but it is not subject to political confidence during its term which has a fixed duration.\n\nThese are systems in which the head of state is a constitutional monarch; the existence of their office and their ability to exercise their authority is established and restrained or held back by constitutional law.\n\nSystems in which a prime minister is the active head of the executive branch of government. In some cases the prime minister is also leader of the legislature, in other cases the executive branch is clearly separated from legislature although the entire cabinet or individual ministers must step down in the case of a vote of no confidence. The head of state is a constitutional monarch who normally only exercises his or her powers with the consent of the government, the people or their representatives.\n\nThe prime minister is the nation's active executive, but the monarch still has considerable political powers that can be used at their own discretion.\nNote: Andorra may be considered a theocracy as the monarch is a joint head of state alongside a religious figure\n\nSpecifically, monarchies in which the monarch's exercise of power is unconstrained by any substantive constitutional law.\nNote: Vatican City may be considered a theocracy as the monarch is elected by a religious body\n\nStates in which political power is \"by law\" concentrated within one political party whose operations are largely fused with the government hierarchy (as opposed to states where the law establishes a multi-party system but this fusion is not achieved anyway through electoral fraud or simple inertia). However, some do have elected governments.\nThe nation's military control the organs of government and all high-ranking political executives are also members of the military hierarchy.\n\nStates which have a system of government which is in transition or turmoil and are classified with the current direction of change.\n\nA state governed as a single power in which the central government is ultimately supreme and any administrative divisions (sub-national units) exercise only the powers that the central government chooses to delegate. \nThe majority of states in the world have a unitary system of government. Of the 193 UN member states, 165 are governed as unitary states.\n\nStates in which the central government has delegated some of its powers to regional authorities.\n\nStates in which the federal government shares power with semi-independent regional governments. The central government may or may not be (in theory) a creation of the regional governments.\n\nA union of sovereign states, united for purposes of common action often in relation to other states. Usually created by a treaty, confederations of states tend to be established for dealing with critical issues, such as defense, foreign relations, internal trade or currency, with the general government being required to provide support for all its members.\n\nThe exact political character of the European Union is debated, some arguing that it is \"sui generis\" (unique), but others arguing that it has features of a federation or a confederation. It has elements of intergovernmentalism, with the European Council acting as its collective \"president\", and also elements of supranationalism, with the European Commission acting as its executive and bureaucracy. But it is not easily placed in any of the above categories.\n\n\n"}
{"id": "2956912", "url": "https://en.wikipedia.org/wiki?curid=2956912", "title": "List of psychology disciplines", "text": "List of psychology disciplines\n\nThis non-exhaustive list contains many of the sub-fields within the field of psychology:\n\n\n\n"}
{"id": "7120312", "url": "https://en.wikipedia.org/wiki?curid=7120312", "title": "List of submarine volcanoes", "text": "List of submarine volcanoes\n\nA list of active and extinct submarine volcanoes and seamounts located under the world's oceans. There are estimated to be 40,000 to 55,000 seamounts in the global oceans. Almost all are not well-mapped and many may not have been identified at all. Most are unnamed and unexplored. This list is therefore confined to seamounts that are notable enough to have been named and/or explored.\n\nThis is a list of seamounts with summit depths less than 200 meters.\n\n"}
{"id": "43537662", "url": "https://en.wikipedia.org/wiki?curid=43537662", "title": "Lists of fossiliferous stratigraphic units in North America", "text": "Lists of fossiliferous stratigraphic units in North America\n\n\nLists of fossiliferous stratigraphic units\n"}
{"id": "23014670", "url": "https://en.wikipedia.org/wiki?curid=23014670", "title": "Marginal utility", "text": "Marginal utility\n\nIn economics, utility is the satisfaction or benefit derived by consuming a product; thus the marginal utility of a good or service is the change in the utility from an increase in the consumption of that good or service. \n\nIn the context of cardinal utility, economists sometimes speak of a law of diminishing marginal utility, meaning that the first unit of consumption of a good or service yields more utility than the second and subsequent units, with a continuing reduction for greater amounts. Therefore, the fall in marginal utility as consumption increases is known as diminishing marginal utility. Mathematically:\n\nThe term \"marginal\" refers to a small change, starting from some baseline level. As Philip Wicksteed explained the term,\n\nMarginal considerations are considerations which concern a slight increase or diminution of the stock of anything which we possess or are considering\n\nFrequently the marginal change is assumed to start from the endowment, meaning the total resources available for consumption (see Budget constraint). This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made by the individual himself or herself and by others.\n\nFor reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. Under this assumption, marginal concepts, including marginal utility, may be expressed in terms of differential calculus. Marginal utility can then be defined as the first derivative of total utility—the total satisfaction obtained from consumption of a good or service—with respect to the amount of consumption of that good or service.\n\nIn practice the smallest relevant division may be quite large. Sometimes economic analysis concerns the marginal values associated with a change of one unit of a discrete good or service, such as a motor vehicle or a haircut. For a motor vehicle, the total number of motor vehicles produced is large enough for a continuous assumption to be reasonable: this may not be true for, say, an aircraft carrier.\n\nDepending on which theory of \"utility\" is used, the interpretation of marginal utility can be meaningful or not. Economists have commonly described utility as if it were \"quantifiable\", that is, as if different levels of utility could be compared along a numerical scale. This has affected the development and reception of theories of marginal utility. Quantitative concepts of utility allow familiar arithmetic operations, and further assumptions of continuity and differentiability greatly increase tractability.\n\nContemporary mainstream economic theory frequently defers metaphysical questions, and merely notes or assumes that preference structures conforming to certain rules can be usefully \"proxied\" by associating goods, services, or their uses with quantities, and \"defines\" \"utility\" as such a quantification.\n\nAnother conception is Benthamite philosophy, which equated usefulness with the production of pleasure and avoidance of pain, assumed subject to arithmetic operation. British economists, under the influence of this philosophy (especially by way of John Stuart Mill), viewed utility as \"the feelings of pleasure and pain\" and further as a \"\"quantity\" of feeling\" (emphasis added).\n\nThough generally pursued outside of the mainstream methods, there are conceptions of utility that do not rely on quantification.\nFor example, the Austrian school generally attributes value to \"the satisfaction of wants\", and sometimes rejects even the \"possibility\" of quantification.\nIt has been argued that the Austrian framework makes it possible to consider rational preferences that would otherwise be excluded.\n\nIn any standard framework, the same object may have different marginal utilities for different people, reflecting different preferences or individual circumstances.\n\nThe concept in cardinal utility theory that marginal utilities diminish across the ranges relevant to decision-making is called the \"law of diminishing marginal utility\" (and is also known as Gossen's First Law). This refers to the increase in utility an individual gains from increasing their consumption of a particular good. \"The law of diminishing marginal utility is at the heart of the explanation of numerous economic phenomena, including time preference and the value of goods ... The law says, first, that the marginal utility of each homogenous unit decreases as the supply of units increases (and vice versa); second, that the marginal utility of a larger-sized unit is greater than the marginal utility of a smaller-sized unit (and vice versa). The first law denotes the law of diminishing marginal utility, the second law denotes the law of increasing total utility.\"\n\nIn modern economics, choice under conditions of certainty at a single point in time is modeled via ordinal utility, in which the numbers assigned to the utility of a particular circumstance of the individual have no meaning by themselves, but which of two alternative circumstances has higher utility \"is\" meaningful. With ordinal utility, a person's preferences have no unique marginal utility, and thus whether or not marginal utility is diminishing is not meaningful. In contrast, the concept of diminishing marginal utility is meaningful in the context of cardinal utility, which in modern economics is used in analyzing intertemporal choice, choice under uncertainty, and social welfare.\n\nThe law of diminishing marginal utility is similar to the law of diminishing returns which states that as the amount of one factor of production increases as all other factors of production are held the same, the marginal return (extra output gained by adding an extra unit) decreases.\n\nAs the rate of commodity acquisition increases, \"marginal\" utility decreases. If commodity consumption continues to rise, marginal utility at some point may fall to zero, reaching maximum total utility. Further increase in consumption of units of commodities causes marginal utility to become negative; this signifies dissatisfaction. For example,\n\nDiminishing marginal utility is traditionally a microeconomic concept and often holds for an individual, although the marginal utility of a good or service might be \"increasing\" as well. For example:\n\nAs suggested elsewhere in this article, occasionally one may come across a situation in which marginal utility increases even at a macroeconomic level. For example, the provision of a service may only be viable if it accessible to most or all of the population, and the marginal utility of a raw material required to provide such a service will increase at the \"tipping point\" at which this occurs. This is similar to the position with very large items such as aircraft carriers: the numbers of these items involved are so small that marginal utility is no longer a helpful concept, as there is merely a simple \"yes\" or \"no\" decision.\n\nMarginalism explains choice with the hypothesis that people decide whether to effect any given change based on the marginal utility of that change, with rival alternatives being chosen based upon which has the greatest marginal utility.\n\nIf an individual possesses a good or service whose marginal utility to him is less than that of some other good or service for which he could trade it, then it is in his interest to effect that trade. Of course, as one thing is sold and another is bought, the respective marginal gains or losses from further trades will change. If the marginal utility of one thing is diminishing, and the other is not increasing, all else being equal, an individual will demand an increasing ratio of that which is acquired to that which is sacrificed. (One important way in which all else might not be equal is when the use of the one good or service complements that of the other. In such cases, exchange ratios might be constant.) If any trader can better his position by offering a trade more favorable to complementary traders, then he will do so.\n\nIn an economy with money, the marginal utility of a quantity is simply that of the best good or service that it could purchase. In this way it is useful for explaining supply and demand, as well as essential aspects of models of imperfect competition.\n\nThe \"paradox of water and diamonds\", usually most commonly associated with Adam Smith, though recognized by earlier thinkers, is the apparent contradiction that water possesses a value far lower than diamonds, even though water is far more vital to a human being.\n\nPrice is determined by both marginal utility and marginal cost, and here the key to the \"paradox\" is that the marginal cost of water is far lower than that of diamonds.\n\nThat is not to say that the price of any good or service is simply a function of the marginal utility that it has for any one individual nor for some ostensibly typical individual. Rather, individuals are willing to trade based upon the respective marginal utilities of the goods that they have or desire (with these marginal utilities being distinct for each potential trader), and prices thus develop constrained by these marginal utilities.\n\nUnder the special case in which usefulness can be quantified, the change in utility of moving from state formula_1 to state formula_2 is\nMoreover, if formula_1 and formula_2 are distinguishable by values of just one variable formula_6 which is itself quantified, then it becomes possible to speak of the ratio of the marginal utility of the change in formula_6 to the size of that change:\n\n(where “c.p.” indicates that the \"only\" independent variable to change is formula_6).\n\nMainstream neoclassical economics will typically assume that the limit\nexists, and use “marginal utility” to refer to the partial derivative\nAccordingly, diminishing marginal utility corresponds to the condition\n\nThe concept of marginal utility grew out of attempts by economists to explain the determination of price. The term “marginal utility”, credited to the Austrian economist Friedrich von Wieser by Alfred Marshall, was a translation of Wieser's term “Grenznutzen” (\"border-use\").\n\nPerhaps the essence of a notion of diminishing marginal utility can be found in Aristotle's \"Politics\", wherein he writes (There has been marked disagreement about the development and role of marginal considerations in Aristotle's value theory.)\n\nA great variety of economists have concluded that there is \"some\" sort of interrelationship between utility and rarity that affects economic decisions, and in turn informs the determination of prices. Diamonds are priced higher than water because their marginal utility is higher than water .\n\nEighteenth-century Italian mercantilists, such as Antonio Genovesi, Giammaria Ortes, Pietro Verri, Marchese Cesare di Beccaria, and Count Giovanni Rinaldo Carli, held that value was explained in terms of the general utility and of scarcity, though they did not typically work-out a theory of how these interacted. In \"Della moneta\" (1751), Abbé Ferdinando Galiani, a pupil of Genovesi, attempted to explain value as a ratio of two ratios, \"utility\" and \"scarcity\", with the latter component ratio being the ratio of quantity to use.\n\nAnne Robert Jacques Turgot, in \"Réflexions sur la formation et la distribution de richesse\" (1769), held that value derived from the general utility of the class to which a good belonged, from comparison of present and future wants, and from anticipated difficulties in procurement.\n\nLike the Italian mercantists, Étienne Bonnot, Abbé de Condillac, saw value as determined by utility associated with the class to which the good belong, and by estimated scarcity. In \"De commerce et le gouvernement\" (1776), Condillac emphasized that value is not based upon cost but that costs were paid because of value.\n\nThis last point was famously restated by the Nineteenth Century proto-marginalist, Richard Whately, who in \"Introductory Lectures on Political Economy\" (1832) wrote (Whatley's student Senior is noted below as an early marginalist.)\n\nThe first unambiguous published statement of any sort of theory of marginal utility was by Daniel Bernoulli, in “Specimen theoriae novae de mensura sortis”. This paper appeared in 1738, but a draft had been written in 1731 or in 1732. In 1728, Gabriel Cramer had produced fundamentally the same theory in a private letter. Each had sought to resolve the St. Petersburg paradox, and had concluded that the marginal desirability of money decreased as it was accumulated, more specifically such that the desirability of a sum were the natural logarithm (Bernoulli) or square root (Cramer) thereof. However, the more general implications of this hypothesis were not explicated, and the work fell into obscurity.\n\nIn “A Lecture on the Notion of Value as Distinguished Not Only from Utility, but also from Value in Exchange”, delivered in 1833 and included in \"Lectures on Population, Value, Poor Laws and Rent\" (1837), William Forster Lloyd explicitly offered a general marginal utility theory, but did not offer its derivation nor elaborate its implications. The importance of his statement seems to have been lost on everyone (including Lloyd) until the early 20th century, by which time others had independently developed and popularized the same insight.\n\nIn \"An Outline of the Science of Political Economy\" (1836), Nassau William Senior asserted that marginal utilities were the ultimate determinant of demand, yet apparently did not pursue implications, though some interpret his work as indeed doing just that.\n\nIn “De la mesure de l’utilité des travaux publics” (1844), Jules Dupuit applied a conception of marginal utility to the problem of determining bridge tolls.\n\nIn 1854, Hermann Heinrich Gossen published \"Die Entwicklung der Gesetze des menschlichen Verkehrs und der daraus fließenden Regeln für menschliches Handeln\", which presented a marginal utility theory and to a very large extent worked-out its implications for the behavior of a market economy. However, Gossen's work was not well received in the Germany of his time, most copies were destroyed unsold, and he was virtually forgotten until rediscovered after the so-called Marginal Revolution.\n\nMarginalism eventually found a foothold by way of the work of three economists, Jevons in England, Menger in Austria, and Walras in Switzerland.\n\nWilliam Stanley Jevons first proposed the theory in “A General Mathematical Theory of Political Economy” (PDF), a paper presented in 1862 and published in 1863, followed by a series of works culminating in his book \"The Theory of Political Economy\" in 1871 that established his reputation as a leading political economist and logician of the time. Jevons' conception of utility was in the utilitarian tradition of Jeremy Bentham and of John Stuart Mill, but he differed from his classical predecessors in emphasizing that \"value depends entirely upon utility\", in particular, on \"final utility upon which the theory of Economics will be found to turn.\" He later qualified this in deriving the result that in a model of exchange equilibrium, price ratios would be proportional not only to ratios of \"final degrees of utility,\" but also to costs of production.\n\nCarl Menger presented the theory in \"Grundsätze der Volkswirtschaftslehre\" (translated as \"Principles of Economics\") in 1871. Menger's presentation is peculiarly notable on two points. First, he took special pains to explain \"why\" individuals should be expected to rank possible uses and then to use marginal utility to decide amongst trade-offs. (For this reason, Menger and his followers are sometimes called “the Psychological School”, though they are more frequently known as “the Austrian School” or as “the Vienna School”.) Second, while his illustrative examples present utility as quantified, his essential assumptions do not. (Menger in fact crossed-out the numerical tables in his own copy of the published \"Grundsätze\".) Menger also developed the law of diminishing marginal utility. Menger's work found a significant and appreciative audience.\n\nMarie-Esprit-Léon Walras introduced the theory in \"Éléments d'économie politique pure\", the first part of which was published in 1874 in a relatively mathematical exposition. Walras's work found relatively few readers at the time but was recognized and incorporated two decades later in the work of Pareto and Barone.\n\nAn American, John Bates Clark, is sometimes also mentioned. But, while Clark independently arrived at a marginal utility theory, he did little to advance it until it was clear that the followers of Jevons, Menger, and Walras were revolutionizing economics. Nonetheless, his contributions thereafter were profound.\n\nAlthough the Marginal Revolution flowed from the work of Jevons, Menger, and Walras, their work might have failed to enter the mainstream were it not for a second generation of economists. In England, the second generation were exemplified by Philip Henry Wicksteed, by William Smart, and by Alfred Marshall; in Austria by Eugen von Böhm-Bawerk and by Friedrich von Wieser; in Switzerland by Vilfredo Pareto; and in America by Herbert Joseph Davenport and by Frank A. Fetter.\n\nThere were significant, distinguishing features amongst the approaches of Jevons, Menger, and Walras, but the second generation did not maintain distinctions along national or linguistic lines. The work of von Wieser was heavily influenced by that of Walras. Wicksteed was heavily influenced by Menger. Fetter referred to himself and Davenport as part of “the American Psychological School”, named in imitation of the Austrian “Psychological School”. (And Clark's work from this period onward similarly shows heavy influence by Menger.) William Smart began as a conveyor of Austrian School theory to English-language readers, though he fell increasingly under the influence of Marshall.\n\nBöhm-Bawerk was perhaps the most able expositor of Menger's conception. He was further noted for producing a theory of interest and of profit in equilibrium based upon the interaction of diminishing marginal utility with diminishing marginal productivity of time and with time preference. (This theory was adopted in full and then further developed by Knut Wicksell and, with modifications including formal disregard for time-preference, by Wicksell's American rival Irving Fisher.)\n\nMarshall was the second-generation marginalist whose work on marginal utility came most to inform the mainstream of neoclassical economics, especially by way of his \"Principles of Economics\", the first volume of which was published in 1890. Marshall constructed the demand curve with the aid of assumptions that utility was quantified, and that the marginal utility of money was constant (or nearly so). Like Jevons, Marshall did not see an explanation for supply in the theory of marginal utility, so he synthesized an explanation of demand thus explained with supply explained in a more classical manner, determined by costs which were taken to be objectively determined. (Marshall later actively mischaracterized the criticism that these costs were themselves ultimately determined by marginal utilities.)\n\nKarl Marx acknowledged that \"nothing can have value, without being an object of utility\", but, in his analysis, \"use-value as such lies outside the sphere of investigation of political economy\", with labor being the principal determinant of value under capitalism.\n\nThe doctrines of marginalism and the Marginal Revolution are often interpreted as somehow a response to Marxist economics. However the first volume of \"Das Kapital\" was not published until July 1867, after the works of Jevons, Menger, and Walras were written or well under way (In 1874 Walras published Éléments d'économie politique pure and Carl Menger published Principles of Economics in 1871) ; and Marx was still a relatively minor figure when these works were completed. It is unlikely that any of them knew anything of him. (On the other hand, Hayek or Bartley has suggested that Marx, voraciously reading at the British Museum, may have come across the works of one or more of these figures, and that his inability to formulate a viable critique may account for his failure to complete any further volumes of \"Kapital\" before his death.)\n\nNonetheless, it is not unreasonable to suggest that the generation who followed the preceptors of the Revolution succeeded partly because they could formulate straightforward responses to Marxist economic theory. The most famous of these was that of Böhm-Bawerk, \"Zum Abschluss des Marxschen Systems\" (1896), but the first was Wicksteed's \"The Marxian Theory of Value. \"Das Kapital\": a criticism\" (1884, followed by \"The Jevonian criticism of Marx: a rejoinder\" in 1885). Initially there were only a few Marxist responses to marginalism, of which the most famous were Rudolf Hilferding's \"Böhm-Bawerks Marx-Kritik\" (1904) and \"Politicheskoy ekonomni rante\" (1914) by Никола́й Ива́нович Буха́рин (Nikolai Bukharin). However, over the course of the 20th century a considerable literature developed on the conflict between marginalism and the labour theory of value, with the work of the neo-Ricardian economist Piero Sraffa providing an important critique of marginalism.\n\nIt might also be noted that some followers of Henry George similarly consider marginalism and neoclassical economics a reaction to \"Progress and Poverty\", which was published in 1879.\n\nIn the 1980s John Roemer and other analytical Marxists have worked to rebuild Marxian theses on a marginalist foundation.\n\nIn his 1881 work \"Mathematical Psychics\", Francis Ysidro Edgeworth presented the indifference curve, deriving its properties from marginalist theory which assumed utility to be a differentiable function of quantified goods and services. Later work attempted to generalize to the indifference curve formulations of utility and marginal utility in avoiding unobservable measures of utility.\n\nIn 1915, Eugen Slutsky derived a theory of consumer choice solely from properties of indifference curves. Because of the World War, the Bolshevik Revolution, and his own subsequent loss of interest, Slutsky's work drew almost no notice, but similar work in 1934 by John Richard Hicks and R. G. D. Allen derived much the same results and found a significant audience. (Allen subsequently drew attention to Slutsky's earlier accomplishment.)\n\nAlthough some of the third generation of Austrian School economists had by 1911 rejected the quantification of utility while continuing to think in terms of marginal utility, most economists presumed that utility must be a sort of quantity. Indifference curve analysis seemed to represent a way to dispense with presumptions of quantification, albeit that a seemingly arbitrary assumption (admitted by Hicks to be a \"rabbit out of a hat\") about decreasing marginal rates of substitution would then have to be introduced to have convexity of indifference curves.\n\nFor those who accepted that indifference curve analysis superseded earlier marginal utility analysis, the latter became at best perhaps pedagogically useful, but \"old fashioned\" and observationally unnecessary.\n\nWhen Cramer and Bernoulli introduced the notion of diminishing marginal utility, it had been to address a paradox of gambling, rather than the paradox of value. The marginalists of the revolution, however, had been formally concerned with problems in which there was neither risk nor uncertainty. So too with the indifference curve analysis of Slutsky, Hicks, and Allen.\n\nThe expected utility hypothesis of Bernoulli and others was revived by various 20th century thinkers, with early contributions by Ramsey (1926), von Neumann and Morgenstern (1944), and Savage (1954). Although this hypothesis remains controversial, it brings not only utility, but a quantified conception of utility (cardinal utility), back into the mainstream of economic thought.\n\nA major reason why quantified models of utility are influential today is that risk and uncertainty have been recognized as central topics in contemporary economic theory. Quantified utility models simplify the analysis of risky decisions because, under quantified utility, diminishing marginal utility implies risk aversion. In fact, many contemporary analyses of saving and portfolio choice require stronger assumptions than diminishing marginal utility, such as the assumption of prudence, which means convex marginal utility.\n\nMeanwhile, the Austrian School continued to develop its ordinalist notions of marginal utility analysis, formally demonstrating that from them proceed the decreasing marginal rates of substitution of indifference curves.\n\n"}
{"id": "12310747", "url": "https://en.wikipedia.org/wiki?curid=12310747", "title": "Metropolis Project", "text": "Metropolis Project\n\nThe Metropolis Project is an international network of researchers, policy-makers, international organizations and civil society organizations for the development of comparative research and policy-relevant knowledge on migration, diversity, and immigrant integration in cities in Canada and around the world.\n\nThe Metropolis Project's principal decision-making body is an International Steering Committee of representatives from over 40 countries. The Project is managed by a Secretariat with offices in Ottawa, Amsterdam and in Asia with functions distributed across organizations in Seoul, Manila and Beijing. The Ottawa Secretariat is responsible for establishing the Project’s strategic directions. Howard Duncan is the Project's Executive Head.\n\nThe Metropolis Project launched in April 2014 a new training program for senior policy-makers, senior managers of settlement agencies, officials of international organizations and the private sector. The programme is intended to provide participants with information, analysis and tools on the management of migration and integration. A pilot training will be offered in June 2014. Metropolis Professional Development has been developed in collaboration with worldwide renowned experts such as Graeme Hugo (University of Adelaide), Jan Niessen (Migration Policy Group), Imelda Nicolas (Commission on Filipinos Overseas) and Peter Schatzer (IOM) among many others.\n\nThe Metropolis Project partners with Springer in the publication of an academic journal, the \"Journal of International Migration and Integration\" (\"Revue de l'intégration et de la migration internationale\"). It is published quarterly, in both English and French; its first issue was in Winter, 2000. The managing editor for the 2013-2014 issue is Lori Wilkinson of the University of Manitoba.\n\nThe project has hosted an international conference concerning research and policy on human migration annually since 1996. The 2015 meeting will be held in Mexico City, Mexico in September 2015 (Metropolis 2015 site) with the slogan \"Migrants: Key Players in the 21st Century\".\n\nPrevious conference locations have been:\n\n"}
{"id": "709029", "url": "https://en.wikipedia.org/wiki?curid=709029", "title": "Moduli (physics)", "text": "Moduli (physics)\n\nIn quantum field theory, the term moduli (or more properly moduli fields) is sometimes used to refer to scalar fields whose potential energy function has continuous families of global minima. Such potential functions frequently occur in supersymmetric systems. The term \"modulus\" is borrowed from mathematics, where it is used synonymously with \"parameter\". The word moduli (\"Moduln\" in German) first appeared in 1857 in Bernhard Riemann's celebrated paper \"Theorie der Abel'schen Functionen\".\n\nIn quantum field theories, the possible vacua are usually labelled by the vacuum expectation values of scalar fields, as Lorentz invariance forces the vacuum expectation values of any higher spin fields to vanish. These vacuum expectation values can take any value for which the potential function is a minimum. Consequently, when the potential function has continuous families of global minima, the space of vacua for the quantum field theory is a manifold (or orbifold), usually called the vacuum manifold. This manifold is often called the moduli space of vacua, or just the moduli space, for short.\n\nThe term moduli is also used in string theory to refer to various continuous parameters which label possible string backgrounds: the expectation value of the dilaton field, the parameters (e.g. the radius and complex structure) which govern the shape of the compactification manifold, et cetera. These parameters are represented, in the quantum field theory that approximates the string theory at low energies, by the vacuum expectation values of massless scalar fields, making contact with the usage described above. In string theory, the term \"moduli space\" is often used specifically to refer to the space of all possible string backgrounds.\n\nIn general quantum field theories, even if the classical potential energy is minimized over a large set of possible expectation values, generically once quantum corrections are included nearly all of these configurations cease to minimize the energy. The result is that the set of vacua of the quantum theory is generally much smaller than that of the classical theory. A notable exception occurs when the various vacua in question are related by a symmetry which guarantees that their energy levels remain exactly degenerate.\n\nThe situation is very different in supersymmetric quantum field theories. In general these possess large moduli spaces of vacua which are not related by any symmetry, for example the masses of the various excitations may differ at various points on the moduli space. The moduli spaces of supersymmetric gauge theories are in general easier to calculate than those of nonsupersymmetric theories because supersymmetry restricts the allowed geometries of the moduli space even when quantum corrections are included.\n\nThe more supersymmetry there is, the stronger the restriction on the vacuum manifold. Therefore, if a restriction appears below for a given number N of spinors of supercharges, then it also holds for all greater values of N.\n\nThe first restriction on the geometry of a moduli space was found in 1979 by Bruno Zumino and published in the article Supersymmetry and Kähler Manifolds. He considered an N=1 theory in 4-dimensions with global supersymmetry. N=1 means that the fermionic components of the supersymmetry algebra can be assembled into a single Majorana supercharge. The only scalars in such a theory are the complex scalars of the chiral superfields. He found that the vacuum manifold of allowed vacuum expectation values for these scalars is not only complex but also a Kähler manifold.\n\nIf gravity is included in the theory, so that there is local supersymmetry, then the resulting theory is called a supergravity theory and the restriction on the geometry of the moduli space becomes stronger. The moduli space must not only be Kähler, but also the Kähler form must lift to integral cohomology. Such manifolds are called Hodge manifolds. The first example appeared in the 1979 article Spontaneous Symmetry Breaking and Higgs Effect in Supergravity Without Cosmological Constant and the general statement appeared 3 years later in Quantization of Newton's Constant in Certain Supergravity Theories.\n\nIn extended 4-dimensional theories with N=2 supersymmetry, corresponding to a single Dirac spinor supercharge, the conditions are stronger. The N=2 supersymmetry algebra contains two representations with scalars, the vector multiplet which contains a complex scalar and the hypermultiplet which contains two complex scalars. The moduli space of the vector multiplets is called the Coulomb branch while that of the hypermultiplets is called the Higgs branch. The total moduli space is locally a product of these two branches, as nonrenormalization theorems imply that the metric of each is independent of the fields of the other multiplet.(See for example Argyres, Non-Perturbative Dynamics Of Four-Dimensional Supersymmetric Field Theories, pp. 6–7, for further discussion of the local product structure.)\n\nIn the case of global N=2 supersymmetry, in other words in the absence of gravity, the Coulomb branch of the moduli space is a special Kähler manifold. The first example of this restriction appeared in the 1984 article Potentials and Symmetries of General Gauged N=2 Supergravity: Yang-Mills Models by Bernard de Wit and Antoine Van Proeyen, while a general geometric description of the underlying geometry, called special geometry, was present by Andrew Strominger in his 1990 paper Special Geometry.\n\nThe Higgs branch is a hyperkähler manifold as was shown by Luis Alvarez-Gaume and Daniel Freedman in their 1981 paper Geometrical Structure and Ultraviolet Finiteness in the Supersymmetric Sigma Model. Including gravity the supersymmetry becomes local. Then one needs to add the same Hodge condition to the special Kahler Coulomb branch as in the N=1 case. Jonathan Bagger and Edward Witten demonstrated in their 1982 paper Matter Couplings in N=2 Supergravity that in this case the Higgs branch must be a quaternionic Kähler manifold.\n\nIn extended supergravities with N>2 the moduli space must always be a symmetric space.\n\n"}
{"id": "198608", "url": "https://en.wikipedia.org/wiki?curid=198608", "title": "Molecular dynamics", "text": "Molecular dynamics\n\nMolecular dynamics (MD) is a computer simulation method for studying the physical movements of atoms and molecules. The atoms and molecules are allowed to interact for a fixed period of time, giving a view of the dynamic evolution of the system. In the most common version, the trajectories of atoms and molecules are determined by numerically solving Newton's equations of motion for a system of interacting particles, where forces between the particles and their potential energies are often calculated using interatomic potentials or molecular mechanics force fields. The method was originally developed within the field of theoretical physics in the late 1950s but is applied today mostly in chemical physics, materials science and the modelling of biomolecules.\n\nBecause molecular systems typically consist of a vast number of particles, it is impossible to determine the properties of such complex systems analytically; MD simulation circumvents this problem by using numerical methods. However, long MD simulations are mathematically ill-conditioned, generating cumulative errors in numerical integration that can be minimized with proper selection of algorithms and parameters, but not eliminated entirely.\n\nFor systems which obey the ergodic hypothesis, the evolution of one molecular dynamics simulation may be used to determine macroscopic thermodynamic properties of the system: the time averages of an ergodic system correspond to microcanonical ensemble averages. MD has also been termed \"statistical mechanics by numbers\" and \"Laplace's vision of Newtonian mechanics\" of predicting the future by animating nature's forces and allowing insight into molecular motion on an atomic scale.\n\nFollowing the earlier successes of Monte Carlo simulations, the method was first developed by Fermi, Pasta, Ulam and Tsingou in the mid 50s. In 1957, Alder and Wainwright used an IBM 704 computer to simulate perfectly elastic collisions between hard spheres. In 1960, Gibson et al., simulated radiation damage of solid copper by using a Born-Mayer type of repulsive interaction along with a cohesive surface force. In 1964, Rahman published landmark simulations of liquid argon that used a Lennard-Jones potential. Calculations of system properties, such as the coefficient of self-diffusion, compared well with experimental data.\n\nEven before it became possible to simulate molecular dynamics with computers, some undertook the hard work of trying it with physical models such as macroscopic spheres. The idea was to arrange them to replicate the properties of a liquid. J.D. Bernal said, in 1962: \"\"... I took a number of rubber balls and stuck them together with rods of a selection of different lengths ranging from 2.75 to 4 inches. I tried to do this in the first place as casually as possible, working in my own office, being interrupted every five minutes or so and not remembering what I had done before the interruption.\"\n\nBeginning in theoretical physics, the method of MD gained popularity in materials science and since the 1970s also in biochemistry and biophysics. MD is frequently used to refine 3-dimensional structures of proteins and other macromolecules based on experimental constraints from X-ray crystallography or NMR spectroscopy. In physics, MD is used to examine the dynamics of atomic-level phenomena that cannot be observed directly, such as thin film growth and ion-subplantation, and also to examine the physical properties of nanotechnological devices that have not or cannot yet be created. In biophysics and structural biology, the method is frequently applied to study the motions of macromolecules such as proteins and nucleic acids, which can be useful for interpreting the results of certain biophysical experiments and for modeling interactions with other molecules, as in ligand docking. In principle MD can be used for ab initio prediction of protein structure by simulating folding of the polypeptide chain from random coil.\n\nThe results of MD simulations can be tested through comparison to experiments that measure molecular dynamics, of which a popular method is NMR spectroscopy. MD-derived structure predictions can be tested through community-wide experiments in Critical Assessment of protein Structure Prediction (CASP), although the method has historically had limited success in this area. Michael Levitt, who shared the Nobel Prize partly for the application of MD to proteins, wrote in 1999 that CASP participants usually did not use the method due to \"... a central embarrassment of molecular mechanics, namely that energy minimization or molecular dynamics generally leads to a model that is less like the experimental structure.\"\" Improvements in computational resources permitting more and longer MD trajectories, combined with modern improvements in the quality of force field parameters, have yielded some improvements in both structure prediction and homology model refinement, without reaching the point of practical utility in these areas; many identify force field parameters as a key area for further development.\n\nLimits of the method are related to the parameter sets used, and to the underlying molecular mechanics force fields. One run of an MD simulation optimizes the potential energy, rather than the free energy of the protein , meaning that all entropic contributions to thermodynamic stability of protein structure are neglected, including the conformational entropy of the polypeptide chain (the main factor that destabilizes protein structure) and hydrophobic effects (the main driving forces of protein folding). Another important factor are intramolecular hydrogen bonds, which are not explicitly included in modern force fields, but described as Coulomb interactions of atomic point charges. This is a crude approximation because hydrogen bonds have a partially quantum mechanical and chemical nature. Furthermore, electrostatic interactions are usually calculated using the dielectric constant of vacuum, although the surrounding aqueous solution has a much higher dielectric constant. Using the macroscopic dielectric constant at short interatomic distances is questionable. Finally, van der Waals interactions in MD are usually described by Lennard-Jones potentials based on the Fritz London theory that is only applicable in vacuum. However, all types of van der Waals forces are ultimately of electrostatic origin and therefore depend on dielectric properties of the environment. The direct measurement of attraction forces between different materials (as Hamaker constant) shows that \"the interaction between hydrocarbons across water is about 10% of that across vacuum\". The environment-dependence of van der Waals forces is neglected in standard simulations, but can be included by developing polarizable force fields.\n\nDesign of a molecular dynamics simulation should account for the available computational power. Simulation size (n=number of particles), timestep, and total time duration must be selected so that the calculation can finish within a reasonable time period. However, the simulations should be long enough to be relevant to the time scales of the natural processes being studied. To make statistically valid conclusions from the simulations, the time span simulated should match the kinetics of the natural process. Otherwise, it is analogous to making conclusions about how a human walks when only looking at less than one footstep. Most scientific publications about the dynamics of proteins and DNA use data from simulations spanning nanoseconds (10 s) to microseconds (10 s). To obtain these simulations, several CPU-days to CPU-years are needed. Parallel algorithms allow the load to be distributed among CPUs; an example is the spatial or force decomposition algorithm.\n\nDuring a classical MD simulation, the most CPU intensive task is the evaluation of the potential as a function of the particles' internal coordinates. Within that energy evaluation, the most expensive one is the non-bonded or non-covalent part. In Big O notation, common molecular dynamics simulations scale by formula_1 if all pair-wise electrostatic and van der Waals interactions must be accounted for explicitly. This computational cost can be reduced by employing electrostatics methods such as particle mesh Ewald summation ( formula_2 ), particle–particle-particle–mesh (P3M), or good spherical cutoff methods ( formula_3 ). \n\nAnother factor that impacts total CPU time needed by a simulation is the size of the integration timestep. This is the time length between evaluations of the potential. The timestep must be chosen small enough to avoid discretization errors (i.e., smaller than the period related to fastest vibrational frequency in the system). Typical timesteps for classical MD are in the order of 1 femtosecond (10 s). This value may be extended by using algorithms such as the SHAKE constraint algorithm, which fix the vibrations of the fastest atoms (e.g., hydrogens) into place. Multiple time scale methods have also been developed, which allow extended times between updates of slower long-range forces.\n\nFor simulating molecules in a solvent, a choice should be made between explicit and implicit solvent. Explicit solvent particles (such as the TIP3P, SPC/E and SPC-f water models) must be calculated expensively by the force field, while implicit solvents use a mean-field approach. Using an explicit solvent is computationally expensive, requiring inclusion of roughly ten times more particles in the simulation. But the granularity and viscosity of explicit solvent is essential to reproduce certain properties of the solute molecules. This is especially important to reproduce chemical kinetics.\n\nIn all kinds of molecular dynamics simulations, the simulation box size must be large enough to avoid boundary condition artifacts. Boundary conditions are often treated by choosing fixed values at the edges (which may cause artifacts), or by employing periodic boundary conditions in which one side of the simulation loops back to the opposite side, mimicking a bulk phase (which may cause artifacts too).\n\nIn the microcanonical ensemble, the system is isolated from changes in moles (N), volume (V), and energy (E). It corresponds to an adiabatic process with no heat exchange. A microcanonical molecular dynamics trajectory may be seen as an exchange of potential and kinetic energy, with total energy being conserved. For a system of N particles with coordinates formula_4 and velocities formula_5, the following pair of first order differential equations may be written in Newton's notation as\n\nThe potential energy function formula_8 of the system is a function of the particle coordinates formula_4. It is referred to simply as the \"potential\" in physics, or the \"force field\" in chemistry. The first equation comes from Newton's laws of motion; the force formula_10 acting on each particle in the system can be calculated as the negative gradient of formula_8.\n\nFor every time step, each particle's position formula_4 and velocity formula_5 may be integrated with a symplectic integrator method such as Verlet integration. The time evolution of formula_4 and formula_5 is called a trajectory. Given the initial positions (e.g., from theoretical knowledge) and velocities (e.g., randomized Gaussian), we can calculate all future (or past) positions and velocities.\n\nOne frequent source of confusion is the meaning of temperature in MD. Commonly we have experience with macroscopic temperatures, which involve a huge number of particles. But temperature is a statistical quantity. If there is a large enough number of atoms, statistical temperature can be estimated from the \"instantaneous temperature\", which is found by equating the kinetic energy of the system to nkT/2 where n is the number of degrees of freedom of the system.\n\nA temperature-related phenomenon arises due to the small number of atoms that are used in MD simulations. For example, consider simulating the growth of a copper film starting with a substrate containing 500 atoms and a deposition energy of 100 eV. In the real world, the 100 eV from the deposited atom would rapidly be transported through and shared among a large number of atoms (formula_16 or more) with no big change in temperature. When there are only 500 atoms, however, the substrate is almost immediately vaporized by the deposition. Something similar happens in biophysical simulations. The temperature of the system in NVE is naturally raised when macromolecules such as proteins undergo exothermic conformational changes and binding.\n\nIn the canonical ensemble, amount of substance (N), volume (V) and temperature (T) are conserved. It is also sometimes called constant temperature molecular dynamics (CTMD). In NVT, the energy of endothermic and exothermic processes is exchanged with a thermostat.\n\nA variety of thermostat algorithms are available to add and remove energy from the boundaries of a MD simulation in a more or less realistic way, approximating the canonical ensemble. Popular methods to control temperature include velocity rescaling, the Nosé-Hoover thermostat, Nosé-Hoover chains, the Berendsen thermostat, the Andersen thermostat and Langevin dynamics. The Berendsen thermostat might introduce the flying ice cube effect, which leads to unphysical translations and rotations of the simulated system.\n\nIt is not trivial to obtain a canonical ensemble distribution of conformations and velocities using these algorithms. How this depends on system size, thermostat choice, thermostat parameters, time step and integrator is the subject of many articles in the field.\n\nIn the isothermal–isobaric ensemble, amount of substance (N), pressure (P) and temperature (T) are conserved. In addition to a thermostat, a barostat is needed. It corresponds most closely to laboratory conditions with a flask open to ambient temperature and pressure.\n\nIn the simulation of biological membranes, isotropic pressure control is not appropriate. For lipid bilayers, pressure control occurs under constant membrane area (NPAT) or constant surface tension \"gamma\" (NPγT).\n\nThe replica exchange method is a generalized ensemble. It was originally created to deal with the slow dynamics of disordered spin systems. It is also called parallel tempering. The replica exchange MD (REMD) formulation tries to overcome the multiple-minima problem by exchanging the temperature of non-interacting replicas of the system running at several temperatures.\n\nA molecular dynamics simulation requires the definition of a potential function, or a description of the terms by which the particles in the simulation will interact. In chemistry and biology this is usually referred to as a force field and in materials physics as an interatomic potential. Potentials may be defined at many levels of physical accuracy; those most commonly used in chemistry are based on molecular mechanics and embody a classical mechanics treatment of particle-particle interactions that can reproduce structural and conformational changes but usually cannot reproduce chemical reactions.\n\nThe reduction from a fully quantum description to a classical potential entails two main approximations. The first one is the Born–Oppenheimer approximation, which states that the dynamics of electrons are so fast that they can be considered to react instantaneously to the motion of their nuclei. As a consequence, they may be treated separately. The second one treats the nuclei, which are much heavier than electrons, as point particles that follow classical Newtonian dynamics. In classical molecular dynamics, the effect of the electrons is approximated as one potential energy surface, usually representing the ground state.\n\nWhen finer levels of detail are needed, potentials based on quantum mechanics are used; some methods attempt to create hybrid classical/quantum potentials where the bulk of the system is treated classically but a small region is treated as a quantum system, usually undergoing a chemical transformation.\n\nEmpirical potentials used in chemistry are frequently called force fields, while those used in materials physics are called interatomic potentials.\n\nMost force fields in chemistry are empirical and consist of a summation of bonded forces associated with chemical bonds, bond angles, and bond dihedrals, and non-bonded forces associated with van der Waals forces and electrostatic charge. Empirical potentials represent quantum-mechanical effects in a limited way through ad-hoc functional approximations. These potentials contain free parameters such as atomic charge, van der Waals parameters reflecting estimates of atomic radius, and equilibrium bond length, angle, and dihedral; these are obtained by fitting against detailed electronic calculations (quantum chemical simulations) or experimental physical properties such as elastic constants, lattice parameters and spectroscopic measurements.\n\nBecause of the non-local nature of non-bonded interactions, they involve at least weak interactions between all particles in the system. Its calculation is normally the bottleneck in the speed of MD simulations. To lower the computational cost, force fields employ numerical approximations such as shifted cutoff radii, reaction field algorithms, particle mesh Ewald summation, or the newer particle–particle-particle–mesh (P3M).\n\nChemistry force fields commonly employ preset bonding arrangements (an exception being \"ab initio\" dynamics), and thus are unable to model the process of chemical bond breaking and reactions explicitly. On the other hand, many of the potentials used in physics, such as those based on the bond order formalism can describe several different coordinations of a system and bond breaking. Examples of such potentials include the Brenner potential for hydrocarbons and its\nfurther developments for the C-Si-H and C-O-H systems. The\nReaxFF potential can be considered a fully reactive hybrid between bond order potentials and chemistry force fields.\n\nThe potential functions representing the non-bonded energy are formulated as a sum over interactions between the particles of the system. The simplest choice, employed in many popular force fields, is the \"pair potential\", in which the total potential energy can be calculated from the sum of energy contributions between pairs of atoms. An example of such a pair potential is the non-bonded Lennard–Jones potential (also termed the 6–12 potential), used for calculating van der Waals forces.\n\nAnother example is the Born (ionic) model of the ionic lattice. The first term in the next equation is Coulomb's law for a pair of ions, the second term is the short-range repulsion explained by Pauli's exclusion principle and the final term is the dispersion interaction term. Usually, a simulation only includes the dipolar term, although sometimes the quadrupolar term is also included.(Usually termed Buckingham potential model)\n\nIn many-body potentials, the potential energy includes the effects of three or more particles interacting with each other. In simulations with pairwise potentials, global interactions in the system also exist, but they occur only through pairwise terms. In many-body potentials, the potential energy cannot be found by a sum over pairs of atoms, as these interactions are calculated explicitly as a combination of higher-order terms. In the statistical view, the dependency between the variables cannot in general be expressed using only pairwise products of the degrees of freedom. For example, the Tersoff potential, which was originally used to simulate carbon, silicon, and germanium, and has since been used for a wide range of other materials, involves a sum over groups of three atoms, with the angles between the atoms being an important factor in the potential. Other examples are the embedded-atom method (EAM), the EDIP, and the Tight-Binding Second Moment Approximation (TBSMA) potentials, where the electron density of states in the region of an atom is calculated from a sum of contributions from surrounding atoms, and the potential energy contribution is then a function of this sum.\n\nSemi-empirical potentials make use of the matrix representation from quantum mechanics. However, the values of the matrix elements are found through empirical formulae that estimate the degree of overlap of specific atomic orbitals. The matrix is then diagonalized to determine the occupancy of the different atomic orbitals, and empirical formulae are used once again to determine the energy contributions of the orbitals.\n\nThere are a wide variety of semi-empirical potentials, termed tight-binding potentials, which vary according to the atoms being modeled.\n\nMost classical force fields implicitly include the effect of polarizability, e.g., by scaling up the partial charges obtained from quantum chemical calculations. These partial charges are stationary with respect to the mass of the atom. But molecular dynamics simulations can explicitly model polarizability with the introduction of induced dipoles through different methods, such as Drude particles or fluctuating charges. This allows for a dynamic redistribution of charge between atoms which responds to the local chemical environment.\n\nFor many years, polarizable MD simulations have been touted as the next generation. For homogenous liquids such as water, increased accuracy has been achieved through the inclusion of polarizability. Some promising results have also been achieved for proteins. However, it is still uncertain how to best approximate polarizability in a simulation.\n\nIn classical molecular dynamics, one potential energy surface (usually the ground state) is represented in the force field. This is a consequence of the Born–Oppenheimer approximation. In excited states, chemical reactions or when a more accurate representation is needed, electronic behavior can be obtained from first principles by using a quantum mechanical method, such as density functional theory. This is named Ab Initio Molecular Dynamics (AIMD). Due to the cost of treating the electronic degrees of freedom, the computational cost of these simulations is far higher than classical molecular dynamics. This implies that AIMD is limited to smaller systems and shorter times.\n\n\"Ab initio\" quantum mechanical and chemical methods may be used to calculate the potential energy of a system on the fly, as needed for conformations in a trajectory. This calculation is usually made in the close neighborhood of the reaction coordinate. Although various approximations may be used, these are based on theoretical considerations, not on empirical fitting. \"Ab initio\" calculations produce a vast amount of information that is not available from empirical methods, such as density of electronic states or other electronic properties. A significant advantage of using \"ab initio\" methods is the ability to study reactions that involve breaking or formation of covalent bonds, which correspond to multiple electronic states.\n\nQM (quantum-mechanical) methods are very powerful. However, they are computationally expensive, while the MM (classical or molecular mechanics) methods are fast but suffer from several limits (require extensive parameterization; energy estimates obtained are not very accurate; cannot be used to simulate reactions where covalent bonds are broken/formed; and are limited in their abilities for providing accurate details regarding the chemical environment). A new class of method has emerged that combines the good points of QM (accuracy) and MM (speed) calculations. These methods are termed mixed or hybrid quantum-mechanical and molecular mechanics methods (hybrid QM/MM).\n\nThe most important advantage of hybrid QM/MM method is the speed. The cost of doing classical molecular dynamics (MM) in the most straightforward case scales O(n), where n is the number of atoms in the system. This is mainly due to electrostatic interactions term (every particle interacts with every other particle). However, use of cutoff radius, periodic pair-list updates and more recently the variations of the particle-mesh Ewald's (PME) method has reduced this to between O(n) to O(n). In other words, if a system with twice as many atoms is simulated then it would take between two and four times as much computing power. On the other hand, the simplest \"ab initio\" calculations typically scale O(n) or worse (restricted Hartree–Fock calculations have been suggested to scale ~O(n)). To overcome the limit, a small part of the system is treated quantum-mechanically (typically active-site of an enzyme) and the remaining system is treated classically.\n\nIn more sophisticated implementations, QM/MM methods exist to treat both light nuclei susceptible to quantum effects (such as hydrogens) and electronic states. This allows generating hydrogen wave-functions (similar to electronic wave-functions). This methodology has been useful in investigating phenomena such as hydrogen tunneling. One example where QM/MM methods have provided new discoveries is the calculation of hydride transfer in the enzyme liver alcohol dehydrogenase. In this case, quantum tunneling is important for the hydrogen, as it determines the reaction rate.\n\nAt the other end of the detail scale are coarse-grained and lattice models. Instead of explicitly representing every atom of the system, one uses \"pseudo-atoms\" to represent groups of atoms. MD simulations on very large systems may require such large computer resources that they cannot easily be studied by traditional all-atom methods. Similarly, simulations of processes on long timescales (beyond about 1 microsecond) are prohibitively expensive, because they require so many time steps. In these cases, one can sometimes tackle the problem by using reduced representations, which are also called coarse-grained models.\n\nExamples for coarse graining (CG) methods are discontinuous molecular dynamics (CG-DMD) and Go-models. Coarse-graining is done sometimes taking larger pseudo-atoms. Such united atom approximations have been used in MD simulations of biological membranes. Implementation of such approach on systems where electrical properties are of interest can be challenging owing to the difficulty of using a proper charge distribution on the pseudo-atoms. The aliphatic tails of lipids are represented by a few pseudo-atoms by gathering 2 to 4 methylene groups into each pseudo-atom.\n\nThe parameterization of these very coarse-grained models must be done empirically, by matching the behavior of the model to appropriate experimental data or all-atom simulations. Ideally, these parameters should account for both enthalpic and entropic contributions to free energy in an implicit way. When coarse-graining is done at higher levels, the accuracy of the dynamic description may be less reliable. But very coarse-grained models have been used successfully to examine a wide range of questions in structural biology, liquid crystal organization, and polymer glasses.\n\nExamples of applications of coarse-graining:\n\nThe simplest form of coarse-graining is the \"united atom\" (sometimes called \"extended atom\") and was used in most early MD simulations of proteins, lipids, and nucleic acids. For example, instead of treating all four atoms of a CH methyl group explicitly (or all three atoms of CH methylene group), one represents the whole group with one pseudo-atom. It must, of course, be properly parameterized so that its van der Waals interactions with other groups have the proper distance-dependence. Similar considerations apply to the bonds, angles, and torsions in which the pseudo-atom participates. In this kind of united atom representation, one typically eliminates all explicit hydrogen atoms except those that have the capability to participate in hydrogen bonds (\"polar hydrogens\"). An example of this is the CHARMM 19 force-field.\n\nThe polar hydrogens are usually retained in the model, because proper treatment of hydrogen bonds requires a reasonably accurate description of the directionality and the electrostatic interactions between the donor and acceptor groups. A hydroxyl group, for example, can be both a hydrogen bond donor, and a hydrogen bond acceptor, and it would be impossible to treat this with one OH pseudo-atom. About half the atoms in a protein or nucleic acid are non-polar hydrogens, so the use of united atoms can provide a substantial savings in computer time.\n\nIn many simulations of a solute-solvent system the main focus is on the behavior of the solute with little interest of the solvent behavior particularly in those solvent molecules residing in regions far from the solute molecule. Solvents may influence the dynamic behavior of solutes via random collisions and by imposing a frictional drag on the motion of the solute through the solvent. The use of non-rectangular periodic boundary conditions, stochastic boundaries and solvent shells can all help reduce the number of solvent molecules required and enable a larger proportion of the computing time to be spent instead on simulating the solute. It is also possible to incorporate the effects of a solvent without needing any explicit solvent molecules present. One example of this approach is to use a potential mean force (PMF) which describes how the free energy changes as a particular coordinate is varied. The free energy change described by PMF contains the averaged effects of the solvent.\n\nA long range interaction is an interaction in which the spatial interaction falls off no faster than formula_19 where formula_20 is the dimensionality of the system. Examples include charge-charge interactions between ions and dipole-dipole interactions between molecules. Modelling these forces presents quite a challenge as they are significant over a distance which may be larger than half the box length with simulations of many thousands of particles. Though one solution would be to significantly increase the size of the box length, this brute force approach is less than ideal as the simulation would become computationally very expensive. Spherically truncating the potential is also out of the question as unrealistic behaviour may be observed when the distance is close to the cut off distance.\n\nSteered molecular dynamics (SMD) simulations, or force probe simulations, apply forces to a protein in order to manipulate its structure by pulling it along desired degrees of freedom. These experiments can be used to reveal structural changes in a protein at the atomic level. SMD is often used to simulate events such as mechanical unfolding or stretching.\n\nThere are two typical protocols of SMD: one in which pulling velocity is held constant, and one in which applied force is constant. Typically, part of the studied system (e.g., an atom in a protein) is restrained by a harmonic potential. Forces are then applied to specific atoms at either a constant velocity or a constant force. Umbrella sampling is used to move the system along the desired reaction coordinate by varying, for example, the forces, distances, and angles manipulated in the simulation. Through umbrella sampling, all of the system's configurations—both high-energy and low-energy—are adequately sampled. Then, each configuration's change in free energy can be calculated as the potential of mean force. A popular method of computing PMF is through the weighted histogram analysis method (WHAM), which analyzes a series of umbrella sampling simulations.\n\nMolecular dynamics is used in many fields of science.\n\nThe following biophysical examples illustrate notable efforts to produce simulations of a systems of very large size (a complete virus) or very long simulation times (up to 1.112 milliseconds):\n\n\n\n\n\n\n\n\n\n"}
{"id": "239389", "url": "https://en.wikipedia.org/wiki?curid=239389", "title": "Otto Bütschli", "text": "Otto Bütschli\n\nJohann Adam Otto Bütschli (3 May 1848 – 2 February 1920) was a German zoologist and professor at the University of Heidelberg. He specialized in invertebrates and insect development. Many of the groups of protists were first recognized by him.\n\nBütschli was born Frankfurt am Main. He studied mineralogy, chemistry, and paleontology in Karlsruhe and became assistant of Karl Alfred von Zittel (geology and paleontology). He moved to Heidelberg in 1866 and worked with Robert Bunsen (chemistry). He received his PhD from the University of Heidelberg in 1868, after passing examinations in geology, paleontology, and zoology. He joined Rudolf Leuckart at the University of Leipzig in 1869. After leaving his studies to serve as an officer in the Franco-Prussian War (1870-1871), Bütschli worked in his private laboratory and then for two years (1873-1874) with Karl Möbius at the University of Kiel. After that, he worked privately. In 1876, he made Habilitation. He became professor at the University of Heidelberg, as successor of Alexander Pagenstecher, in 1878. He held this position for over 40 years.\n\n"}
{"id": "30034297", "url": "https://en.wikipedia.org/wiki?curid=30034297", "title": "Philosophy of Time Society", "text": "Philosophy of Time Society\n\nThe Philosophy of Time Society is an organization which grew out of a National Endowment for the Humanities Summer Seminar on the Philosophy of Time offered by George N. Schlesinger in 1991. The organization itself was formed in 1993. Its stated goal is \"to promote the study of the philosophy of time from a broad analytic perspective, and to provide a forum as an affiliated group with the American Philosophical Association, to discuss the issues in and related to the philosophy of time.\"\n\nThe Philosophy of Time Society's meetings are held at the meetings of the American Philosophical Association. In the past, they have included many notable scholars such as Hud Hudson, Robin Le Poidevin, Ned Markosian, Hugh Mellor, John Perry, Theodore Sider, Michael Tooley, Dean Zimmerman. Topics of papers have varied widely.\n\n"}
{"id": "3126648", "url": "https://en.wikipedia.org/wiki?curid=3126648", "title": "Point-defence", "text": "Point-defence\n\nPoint-defence (or point-defense; see spelling differences) is the defence of a single object or a limited area, e.g. a ship, building or an airfield, now usually against air attacks and guided missiles. Point-defence weapons have a smaller range in contrast to area-defence systems and are placed near or on the object to be protected.\n\nPoint-defence may include:\n\nCoastal artillery to protect harbours is similar conceptually, but is generally not classified as point-defence. Similarly, passive systems—electronic countermeasures, decoys, chaff, flares, barrage balloons—are not considered point-defence.\n\n\n"}
{"id": "16832751", "url": "https://en.wikipedia.org/wiki?curid=16832751", "title": "Pre-integration complex", "text": "Pre-integration complex\n\nThe pre-integration complex (PIC) is a nucleoprotein complex of viral genetic material and associated viral and host proteins which is capable of inserting a viral genome into a host genome. The PIC forms after uncoating of a viral particle after entry into the host cell. In the case of the human immunodeficiency virus (HIV), the PIC forms after the Reverse Transcription Complex (RTC) has reverse transcribed the viral RNA code into DNA code. The PIC consists of viral proteins (including Vpr, matrix and integrase), host proteins (including Barrier to autointegration factor 1) and the viral DNA. The PIC enters the cellular nucleus through the nuclear pore complex without disrupting the nuclear envelope, thus allowing HIV and related retroviruses to replicate in non-dividing cells. Following nuclear entry, the PIC's DNA payload may be integrated into the host DNA as a \"provirus\".\n\n\n"}
{"id": "28990145", "url": "https://en.wikipedia.org/wiki?curid=28990145", "title": "Proteostasis", "text": "Proteostasis\n\nProteostasis, a portmanteau of the words protein and homeostasis, is the concept that there are competing and integrated biological pathways within cells that control the biogenesis, folding, trafficking and degradation of proteins present within and outside the cell. The concept of proteostasis maintenance is central to understanding the cause of diseases associated with excessive protein misfolding and degradation leading to loss-of-function phenotypes, as well as aggregation-associated degenerative disorders. Therefore, adapting proteostasis should enable the restoration of proteostasis once its loss leads to pathology. Cellular proteostasis is key to ensuring successful development, healthy aging, resistance to environmental stresses, and to minimize homeostasis perturbations by pathogens such as viruses. Mechanisms by which proteostasis is ensured include regulated protein translation, chaperone assisted protein folding and protein degradation pathways. Adjusting each of these mechanisms to the demand for proteins is essential to maintain all cellular functions relying on a correctly folded proteome.\n\nOne of the first points of regulation for proteostasis is during translation. This is accomplished via the structure of the ribosome, a complex central to translation. These two characteristics shape the way the protein folds and influences the proteins future interactions. The synthesis of a new peptide chain using the ribosome is very slow and the ribosome can even be stalled when it encounters a rare codon, a codon found at low concentrations in the cell. These pauses provide an opportunity for an individual protein domain to have the necessary time to become folded before the production of following domains. This facilitates the correct folding of multi-domain proteins.\nThe newly synthesized peptide chain exits the ribosome into the cellular environment through the narrow ribosome exit channel (width: 10Å to 20Å, length 80Å). Due to space restriction in the exit channel the nascent chain already forms secondary and limited tertiary structures. For example, an alpha helix is one such structural property that is commonly induced in this exit channel. At the same time the exit channel also prevents premature folding by impeding large scale interactions within the peptide chain which would require more space.\n\nIn order to maintain protein homeostasis post-translationally, the cell makes use of molecular chaperones sometimes including chaperonins, which aid in the assembly or disassembly of proteins. They recognize exposed segments of hydrophobic amino acids in the nascent peptide chain and then work to promote the proper formation of noncovalent interactions that lead to the desired folded state. \nChaperones begin to assist in protein folding as soon as a nascent chain longer than 60 amino acids emerges from the ribosome exit channel. One of the most studied ribosome binding chaperones is trigger factor. Trigger factor works to stabilize the peptide, promotes its folding, prevents aggregation, and promotes refolding of denatured model substrates. Trigger factor not only directly works to properly fold the protein but also recruits other chaperones to the ribosome, such as Hsp70. Hsp70 surrounds an unfolded peptide chain, thereby preventing aggregation and promoting folding.\n\nChaperonins are a special class of chaperones that promote native state folding by cyclically encapsulating the peptide chain. Chaperonins are divided into two groups. Group 1 chaperonins are commonly found in bacteria, chloroplasts, and mitochondria. Group 2 chaperonins are found in both the cytosol of eukaryotic cells as well as in archaea. Group 2 chaperonins also contain an additional helical component which acts as a lid for the cylindrical protein chamber, unlike Group 1 which instead relies on an extra cochaperone to act as a lid. All chaperonins exhibit two states (open and closed), between which they can cycle. This cycling process is important during the folding of an individual polypeptide chain as it helps to avoid undesired interactions as well as to prevent the peptide from entering into kinetically trapped states.\n\nThe third component of the proteostasis network is the protein degradation machinery. Protein degradation occurs in proteostasis when the cellular signals indicate the need to decrease overall cellular protein levels. The effects of protein degradation can be local, with the cell only experiencing effects from the loss of the degraded protein itself or widespread, with the entire protein landscape changing due to loss of other proteins’ interactions with the degraded protein. Multiple substrates are targets for proteostatic degradation. These degradable substrates include nonfunctional protein fragments produced from ribosomal stalling during translation, misfolded or unfolded proteins, aggregated proteins, and proteins that are no longer needed to carry out cellular function. Several different pathways exist for carrying out these degradation processes. When proteins are determined to be unfolded or misfolded, they are typically degraded via the unfolded protein response (UPR) or endoplasmic-reticulum-associated protein degradation (ERAD). Substrates that are unfolded, misfolded, or no longer required for cellular function can also be ubiquitin tagged for degradation by ATP dependent proteases, such as the proteasome in eukaryotes or ClpXP in prokaryotes. Autophagy, or self engulfment, lysosomal targeting, and phagocytosis (engulfment of waste products by other cells) can also be used as proteostatic degradation mechanisms.\n\nProtein misfolding is detected by mechanisms that are specific for the cellular compartment in which they occur. Distinct surveillance mechanisms that respond unfolded protein have been characterized in the cytoplasm, ER and mitochondria. This response acts locally in a cell autonomous fashion but can also extend to intercellular signaling to protect the organism from anticipated proteotoxic stress.\n\nCellular stress response pathways detect and alleviate proteotoxic stress which is triggered by imbalances in proteostasis. The cell-autonomous regulation occurs through direct detection of misfolded proteins or inhibition of pathway activation by sequestering activating components in response to heat shock. Cellular responses to this stress signaling include transcriptional activation of chaperone expression, increased efficiency in protein trafficking and protein degradation and translational reduction.\n\nThe cytosolic HSR is mainly mediated by the transcription factor family HSF (heat shock family). HSF is constitutively bound by Hsp90. Upon a proteotoxic stimulus Hsp90 is recruited away from HSF which can then bind to heat response elements in the DNA and upregulate gene expression of proteins involved in the maintenance of proteostasis.\n\nThe unfolded protein response in the endoplasmatic reticulum (ER) is activated by imbalances of unfolded proteins inside the ER and the proteins mediating protein homeostasis. Different “detectors” - such as IRE1, ATF6 and PERK - can recognize misfolded proteins in the ER and mediate transcriptional responses which help alleviate the effects of ER stress.\n\nThe mitochondrial unfolded protein response detects imbalances in protein stoichiometry of mitochondrial proteins and misfolded proteins. The expression of mitochondrial chaperones is upregulated by the activation of the transcription factors ATF-1 and/or DVE-1 with UBL-5.\n\nStress responses can also be triggered in a non-cell autonomous fashion by intercellular communication. The stress that is sensed in one tissue could thereby be communicated to other tissues to protect the proteome of the organism or to regulate proteostasis systemically. Cell non-autonomous activation can occur for all three stress responses.\n\nWork on the model organism \"C. elegans\" has shown that neurons play a role in this intercellular communication of cytosolic HSR. Stress induced in the neurons of the worm can in the long run protect other tissues such as muscle and intestinal cells from chronic proteotoxicity. Similarly ER and mitochondrial UPR in neurons are relayed to intestinal cells . These systemic responses have been implicated in mediating not only systemic proteostasis but also influence organismal aging.\n\nDysfunction in proteostasis can arise from errors in or misregulation of protein folding. The classic examples are missense mutations and deletions that change the thermodynamic and kinetic parameters for the protein folding process. These mutations are often inherited and range in phenotypic severity from having no noticeable effect to embryonic lethality. Disease develops when these mutations render a protein significantly more susceptible to misfolding, aggregation, and degradation. If these effects only alter the mutated protein, the negative consequences will only be local loss of function. However, if these mutations occur in a chaperone or a protein that interacts with many other proteins, dramatic global alterations in the proteostasis boundary will occur. Examples of diseases resulting from proteostatic changes from errors in protein folding include cystic fibrosis, Huntington’s disease, Alzheimer’s disease, lysosomal storage disorders, and others.\n\nSmall animal model systems have been and continue to be instrumental in the identification of functional mechanisms that safeguard proteostasis. Model systems of diverse misfolding-prone disease proteins have so far revealed numerous chaperone and co-chaperone modifiers of proteotoxicity.\n\nThe unregulated cell division that marks cancer development requires increased protein synthesis for cancer cell function and survival. This increased protein synthesis is typically seen in proteins that modulate cell metabolism and growth processes. Cancer cells are sometimes susceptible to drugs that inhibit chaperones and disrupt proteostasis, such as Hsp90 inhibitors or proteasome inhibitors. Furthermore, cancer cells tend to produce misfolded proteins, which are removed mainly by proteolysis. Inhibitors of proteolysis allow accumulation of both misfolded protein aggregates, as well as apoptosis signaling proteins in cancer cells. This can can change the sensitivity of cancer cells to antineoplastic drugs; cancer cells either die at a lower drug concentration, or survive, depending on the type of proteins that accumulate, and the function these proteins have. Proteasome inhibitor bortezomib was the first drug of this type to receive approval for treatment of multiple myeloma.\n\nA hallmark of cellular proteostatic networks is their ability to adapt to stress via protein regulation. Metabolic disease, such as that associated with obesity, alters the ability of cellular proteostasis networks adapt to stress, often with detrimental health effects. For example, when insulin production exceeds the cell’s insulin secretion capacity, proteostatic collapse occurs and chaperone production is severely impaired. This disruption leads to the disease symptoms exhibited in individuals with diabetes.\n\nOver time, the proteostasis network becomes burdened with proteins modified by reactive oxygen species and metabolites that induce oxidative damage. These byproducts can react with cellular proteins to cause misfolding and aggregation (especially in nondividing cells like neurons). This risk is particularly high for intrinsically disordered proteins. The IGFR-1 pathway has been shown in \"C. elegans\" to protect against these harmful aggregates, and some experimental work has suggested that upregulation of insulin growth factor receptor 1 (IGFR-1) may stabilize proteostatic network and prevent detrimental effects of aging. Expression of the chaperome, the ensemble of chaperones and co-chaperones that interact in a complex network of molecular folding machines to regulate proteome function, is dramatically repressed in human aging brains and in the brains of patients with neurodegenerative diseases. Functional assays in \"C. elegans\" and human cells have identified a conserved chaperome sub-network of 16 chaperone genes, corresponding to 28 human orthologs as a proteostasis safeguard in aging and age-onset neurodegenerative disease.\n\nThere are two main approaches that have been used for therapeutic development targeting the proteostatic network: pharmacologic chaperones and proteostasis regulators. The principle behind designing pharmacologic chaperones for intervention in diseases of proteostasis is to design small molecules that stabilize proteins exhibiting borderline stability. Previously, this approach has been used to target and stabilize G-protein coupled receptors, neurotransmitter receptors, glycosidases, lysosomal storage proteins, and the mutant CFTR protein that causes cystic fibrosis and transthyretin, which can misfiled and aggregate leading to amyloidoses. Vertex Pharmaceuticals and Pfizer sell regulatory agency approved pharmacologic chaperones for ameliorating cystic fibrosis and the transthyretin amyloidoses, respectively. Amicus sells a regulatory agency approved pharmacologic chaperone for Fabry disease–a lysosomal storage disease.\nThe principle behind proteostasis regulators is different, these molecules alter the biology of protein folding and / or degradation by altering the stoichiometry of the proteostasis network components in a given sub cellular compartment. For example, some proteostasis regulators initiate stress responsive signaling, such as the unfolded protein response, which transcriptionally reprograms the endoplasmic reticulum proteostasis network. It has been suggested that this approach could even be applied prophylactically, such as upregulating certain protective pathways before experiencing an anticipated severe cellular stress. One theoretical mechanism for this approach includes upregulating the heat shock response response to rescue proteins from degradation during cellular stress.\n\n"}
{"id": "21772551", "url": "https://en.wikipedia.org/wiki?curid=21772551", "title": "Regional Center for Renewable Energy and Energy Efficiency", "text": "Regional Center for Renewable Energy and Energy Efficiency\n\nThe Regional Center for Renewable Energy and Energy Efficiency (RCREEE) is an intergovernmental organization with diplomatic status that aims to enable and increase the adoption of renewable energy and energy efficiency practices in the Arab region. RCREEE teams with regional governments and global organizations to initiate and lead clean energy policy dialogues, strategies, technologies and capacity development in order to increase Arab states’ share of tomorrow’s energy.\n\nThrough its solid alliance with the League of Arab States, RCREEE is committed to tackle each country’s specific needs and objectives through collaborating with Arab policy makers, businesses, international organizations and academic communities in key work areas: capacity development and learning, policies and regulations, research and statistics, and technical assistance. The center is also involved in various local and regional projects and initiatives that are tailored to specific objectives.\n\nHaving today 17 Arab countries among its members (Algeria, Bahrain, Djibouti, Egypt, Iraq, Jordan, Kuwait, Lebanon, Libya, Mauritania Morocco, Palestine, Somalia Sudan, Syria, Tunisia, and Yemen), RCREEE strives to lead renewable energy and energy efficiency initiatives and expertise in all Arab states based on five core strategic impact areas: facts and figures, policies, people, institutions, and finance.\n\nRCREEE is financed through its member state contributions, government grants provided by Germany through the German Development Cooperation (GIZ) GmbH (link), Denmark through the Danish International Development Agency (DANIDA) (link), and Egypt through the New and Renewable Energy Authority (NREA) (link). RCREEE is also financed through selected fee-for-service contracts.\n\nHistory\n\nRCREEE was set up based on Cairo Declaration which was signed in June, 2008 by government representatives from ten Arab countries. The declaration outlined the following two core objectives for establishing the center:\nRCREEE acquired its legal status in August, 2010 as an independent not-for-profit international organization through a Host Country Agreement with the government of Egypt.\n\na. Board of Trustees\n\nRCREEE is governed by a Board of Trustees (BoT) which is responsible for setting the center's strategic direction, approving annual work plans, and evaluating performance. The board consists of governmental representatives from member states.\nMembers of the Board of the Trustees\n\nb. Executive Committee\n\nIn addition, the Board of Trustees has elected the RCREEE Executive Committee.The Executive Committee consists of five members from the governmental and private sectors. Its role is to oversea the implementation of the strategic direction approved by the BoT, oversee and advise the Secretariat in the center's management.\n\nMembers of the executive committee\n\nc.National Focal Points\n\nThe national focal points assist the Secretariat with the implementation of the work plan at the national level.\n\nNational Focal Points\n\nRCREEE is a partner with regional and international bodies focused on international development, sustainable energy, and environmental cooperation. These include the World Bank, (link), United Nations Development Programme (UNDP) (link), United Nations Industrial Development Organization (UNIDO)(link), United Nations Environment Program (UNEP) (link), United Nations Economic and Social Commission for Western Asia (UN-ESCWA) (link), League of Arab States (link), International Renewable Energy Agency (IRENA) (link), and Renewable Energy Solutions for Mediterranean (RES4MED) (link).\n\n"}
{"id": "1458875", "url": "https://en.wikipedia.org/wiki?curid=1458875", "title": "Rigidity (mathematics)", "text": "Rigidity (mathematics)\n\nIn mathematics, a rigid collection \"C\" of mathematical objects (for instance sets or functions) is one in which every \"c\" ∈ \"C\" is uniquely determined by less information about \"c\" than one would expect.\n\nThe above statement does not define a mathematical property. Instead, it describes in what sense the adjective rigid is typically used in mathematics, by mathematicians.\n\nSome examples include:\n\n"}
{"id": "32866550", "url": "https://en.wikipedia.org/wiki?curid=32866550", "title": "SciGirls", "text": "SciGirls\n\nSciGirls is an American children's television series that premiered on February 11, 2010 on PBS Kids Go!. It has a mix of live-action and animated segments. It is produced by Twin Cities PBS.\n\nIt is an educational outreach program for elementary and middle-school children based on proven best practices for science, technology, engineering and math (STEM) education for girls. It was launched in February 2010 and produced by Twin Cities Public Television, the episodes are broadcast on most PBS stations, and the project's website is at http://pbskids.org/scigirls. \"SciGirls\" is designed to encourage girls to pursue STEM careers, in response to the low numbers of women in many scientific careers.\n\nThe show was awarded a Daytime Emmy Award in 2011 for Outstanding New Approaches in a Children's TV Show and Best Emmy for Summer 2011 and Metal Trophy for Winter 2011. \"SciGirls\" developed from the award-winning PBS children's television series DragonflyTV.\n\n\nAlongside Izzie and Jake, episodes feature several different groups of real SciGirls.\n\n\nSeason 2 launched in October 2012, Season 3 launched in April 2015, Season 4 launched in February 2018.\n\nEach \"SciGirls\" half-hour episode depicts the STEM-themed activities of a group of middle-school girls including engineering a mini-wind farm, creating a turtle habitat, designing an electronic dress, and more. Additionally, women scientists and engineers mentor the girls, demonstrating that interest in STEM subjects can lead to a rewarding lifelong pursuit. The series is unified by two animated characters, Izzie and Jake, who emphasize how science and technology can help solve problems in everyday life. These characters also appear on the series website, which is integrated into the TV episodes.\n\n<onlyinclude></onlyinclude>\n\nThe entire approach of \"SciGirls\" is based on published research into best practices for engaging girls in STEM. A quarter of a century of studies have converged on a set of common strategies which are built into the project's educational framework, and called the SciGirls Seven:\n\n\n"}
{"id": "31836597", "url": "https://en.wikipedia.org/wiki?curid=31836597", "title": "Specific-information", "text": "Specific-information\n\nIn information theory, specific-information is the generic name given to the family of state-dependent measures that in expectation converge to the mutual information. There are currently three known varieties of specific information usually denoted formula_1, formula_2, and formula_3.\nThe specific-information between a random variable formula_4 and a state formula_5 is written as :formula_6.\n\n"}
{"id": "56366707", "url": "https://en.wikipedia.org/wiki?curid=56366707", "title": "Sunil Kumar Manna", "text": "Sunil Kumar Manna\n\nSunil Kumar Manna (27 August 1968) is an Indian immunologist and the head of the immunology lab of the Centre for DNA Fingerprinting and Diagnostics. He is known for his studies on cell signaling and apoptosis. The Department of Biotechnology of the Government of India awarded him the National Bioscience Award for Career Development, one of the highest Indian science awards, for his contributions to biosciences, in 2009.\n\nSunil Kumar Manna, born on 27 August 1968, pursued his post-graduate education at the University of Calcutta and after earning an MSc, he did his doctoral research at the Indian Institute of Chemical Biology which secured him a PhD from Jadavpur University. His post-doctoral work was at the University of Texas MD Anderson Cancer Center under the guidance of Bharat Aggarwal and on his return to India, he was reportedly offered a faculty position at the Presidency University which he declined to take up a position at the Centre for DNA Fingerprinting and Diagnostics, where he serves as the head of the Lab of Immunology. His research focus is in the field of immunology and he is known to have carried out work on Cytokine mediated immunoregulation as well as apoptosis in cancer. His studies have been documented by way of a number of articles and ResearchGate, an online repository of scientific articles has listed 115 of them. He also holds a patent for an anticancer agent, jointly developed by him.\n\nThe Department of Biotechnology (DBT) of the Government of India awarded him the National Bioscience Award for Career Development, one of the highest Indian science awards in 2009. He resides in Nacharam, a suburb of Hyderabad in the south Indian state of Telangana.\n\nIn 2012, a Japanese scientist accused Manna of plagiarism with regard to some of the images used by him in one of his articles published in the Journal of Biological Chemistry. Two of his articles were immediately retracted, and some were corrected, leading to seven of his papers being finally retracted. He was suspended from his job, after the institute found prima facie evidence against him but he was later reinstated.\n"}
{"id": "20311865", "url": "https://en.wikipedia.org/wiki?curid=20311865", "title": "Team FREDNET", "text": "Team FREDNET\n\nTeam FREDNET is an international Open Source and Open Participation competitor in the Google Lunar X PRIZE competition. Uniquely, the team also allows organizations and individuals to participate freely in its mission through the team's website. Their strategy is to utilize the same approach for developing open source software in order to build a lunar lander and a lunar rovers capable of winning the Google Lunar X Prize. Team FREDNET plans to establish an Open Space Foundation that provides incentives, education, and funding to future individuals and organizations seeking to develop their own space projects. In addition, they hope to foster greater public interest and education in Space Exploration and Research.\n\nTeam FREDNET is led by Fred J. Bourgeois, III. Dr. Sean Casey of the Universities Space Research Association manages business development for Team FREDNET. Mike Barrucco is the principal guidance, navigation, and control engineer for the team. Richard Core is Team FREDNET's project manager.\n\nTeam FREDNET has affiliations with a number of clubs, schools, and businesses.\n\n\n, the team had two sponsors.\n\n\nThe team's charter is \"Make Cool Stuff\". This objective applies towards \"stuff\" that can be used in Space and towards building tools that will make it easier to build design the space \"stuff\". Essentially, the goal is to build a catalog of Space Components in order to make Space Commercialization more open, cost-effective, productive, and accessible.\n\nOrganizing this far-flung group posed perhaps the biggest and earliest challenge. Open source software teams can normally download a program and add their own contributions, but Team FREDNET had to translate its many individual ideas into rocket engines and rover gears.\n\nTo address this challenge, Team FREDNET took major steps in August and September 2009 to make it easier for globally distributed collaboration to occur by providing guidelines for people who wanted to make contributions.\n\nTeam FREDNET relies heavily on Open Source Software Tools to accomplish their mission.\n\n\nTeam FREDNET plans to use a simple architecture in their bid to win the Google Lunar X PRIZE, consisting of a small lander that will deploy a small lunar rover which in turn will use the lander as a communication relay back to Earth.\n\nThere are three main components to the Team FREDNET mission which include (a) the transfer mission (i.e. getting the rover from the Earth to the Moon), (b) the Moon mission (i.e. directing the rover to accomplish the tasks needed to win the Google Lunar X-Prize), and (c) the Earth mission (i.e. receiving the data that the rover must transmit to Earth to win the Google Lunar X PRIZE).\n\n\n\n\nThe team wants to offer educational institutions an opportunity to add science projects fitting inside the parameters of the mission to promote awareness of commercial space travel and space exploration.\n\nAs part of this education effort, the team has contributed towards a team that created a LEGO Mindstorms based rover that is controlled by Bluetooth technology. A future elementary school level competition will be used to name the Rover that the team will eventually send to the moon.\n\n"}
{"id": "15848953", "url": "https://en.wikipedia.org/wiki?curid=15848953", "title": "Tohru Fukuyama", "text": "Tohru Fukuyama\n\nFukuyama studied chemistry at Nagoya University with degrees Bachelor's (1971) and Master's (1973) degrees. As a graduate student, he then worked at Harvard University, where he received his doctorate in 1977 as an academic student of Yoshito Kishi. Until 1978, he continued his research as a postdoc in the Department of Chemistry of Harvard University and then moved to Rice University as an assistant professor, where in 1988 he obtained the rank of a chair holder. In 1995, he accepted a professorship in Pharmaceutical Sciences from the University of Tokyo, Japan. Since 2013, Fukuyama has been working as a professor at the Nagoya University - more precisely: Designated Professor of Pharmaceutical Sciences.\n\nThe 2015 Nobel Prize in Physiology or Medicine winner Satoshi Ōmura is his old friend.\n\n\n\n"}
{"id": "729900", "url": "https://en.wikipedia.org/wiki?curid=729900", "title": "Typhoon Lee", "text": "Typhoon Lee\n\nTyphoon Lee (; born 1948) is an astrophysicist and geochemist at Academia Sinica, Taiwan, where he specializes in isotope geochemistry and nuclear astrophysics .\n\nLee received his Ph.D in astronomy at the University of Texas in 1977.\n\nHis honors include the Robert J. Trumpler Award in 1978 from the Astronomical Society of the Pacific, and Outstanding Researcher Awards from the National Science Council in 1985-87 and 1988-90.\n\nA selection of his publications includes:\n\n"}
