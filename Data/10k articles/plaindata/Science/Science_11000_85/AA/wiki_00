{"id": "37701203", "url": "https://en.wikipedia.org/wiki?curid=37701203", "title": "Alexander Mirtchev", "text": "Alexander Mirtchev\n\nAlexander Vassilev Mirtchev (Bulgarian: Александър Василев Мирчев) (April 28, 1957) is an economist, academic, macroeconomic advisor and author. He is a former fellow at the Woodrow Wilson International Center for Scholars, Executive Chairman of Royal United Services Institute for Defence and Security Studies (RUSI) International and president and founder of Krull Corp. USA.\n\nMirtchev is an American born in Sofia, Bulgaria. He earned an LLM degree in international and comparative law from George Washington University and a PhD from St. Kliment Ohridski University (also known as Sofia University). In addition, he studied economics and finance at the London School of Economics and Political Science at Boston University as well as Harvard Business School.\n\nMirtchev is an academic and has served as a fellow and member of numerous universities and educational institutions, including his role as American Founding Council Member of the Kissinger Institute on China and the United States. In addition, he has served as a Senior Fellow of the Russian and Bulgarian Academy of Sciences, as Doctor Emeritus of the University of Foreign Trade and Finance, Ukraine, and as a member of the Russian Academy of Natural Sciences.\n\nAlexander Mirtchev is Executive Chairman of the Royal United Services Institute for Defense and Security Studies International (RUSI) and Vice President of the Royal United Services Institute for Defence and Security Studies (RUSI), London. He is also the president of Krull Corp. USA. He is a Board Director and Member of the Executive Committee of the Atlantic Council of the United States, and was a member of the Wilson National Cabinet at the Woodrow Wilson International Center for Scholars. He serves as an independent member of the advisory board of Washington & Jefferson College's Center for Energy Policy and Management, independent director of the sovereign wealth fund Samruk-Kazyna and has served as director of the Law Offices of Stewart & Stewart.\n\nMirtchev has appeared as an economics analyst by BBC News, Reuters, Bloomberg, European Energy Review, CNBC, Al Jazeera English, Voice of America, E&ETV, Focus Washington, The Globalist, Europe's World, and RealClearWorld, among other publications and television news programs. He is the author of a number of articles and has served as editor and publisher of a range of academic and professional journals. He also co-authored the Atlantic Council of the United States/RUSL report \"Global Perspectives on Europe's Strategic Future.\" Mirtchev wrote the introduction to the Russian-language edition of Lee Kuan Yew's memoirs, \"From Third World to First, The Singapore Story, 1965-2000\", while the forward to the book was written by Henry Kissinger. Mirtchev is presently a Forbes contributor and often writes about alternative energy technologies.\n"}
{"id": "51067", "url": "https://en.wikipedia.org/wiki?curid=51067", "title": "Anomie", "text": "Anomie\n\nAnomie () is a \"condition in which society provides little moral guidance to individuals\". It is the breakdown of social bonds between an individual and the community, e.g., under unruly scenarios resulting in fragmentation of social identity and rejection of self-regulatory values. \n\nThe term is commonly understood to mean \"normlessness\", and believed to have been popularized by French sociologist Émile Durkheim in his influential book \"Suicide\" (1897). However, Durkheim first introduces the concept of anomie in his 1893 work \"The Division of Labour In Society\". Durkheim never used the term \"normlessness\"; rather, he described anomie as \"derangement\", and \"an insatiable will\". Durkheim used the term \"the malady of the infinite\" because desire without limit can never be fulfilled; it only becomes more intense.\n\nFor Durkheim, anomie arises more generally from a mismatch between personal or group standards and wider social standards, or from the lack of a social ethic, which produces moral deregulation and an absence of legitimate aspirations. This is a nurtured condition:\nIn 1893 Durkheim introduced the concept of \"anomie\" to describe the mismatch of collective guild labour to evolving societal needs when the guild was homogeneous in its constituency. He equated homogeneous (redundant) skills to \"mechanical solidarity\" whose inertia retarded adaptation. He contrasted this with the self-regulating behaviour of a division of labour based on differences in constituency, equated to \"organic solidarity\", whose lack of inertia made it sensitive to needed changes.\n\nDurkheim observed that the conflict between the evolved organic division of labour and the homogeneous mechanical type was such that one could not exist in the presence of the other.\n\nWhen solidarity is organic, anomie is impossible. Sensitivity to mutual needs promotes evolution in the division of labour. \"Producers, being near consumers, can easily reckon the extent of the needs to be satisfied. Equilibrium is established without any trouble and production regulates itself.\" Durkheim contrasted the condition of anomie as being the result of a malfunction of organic solidarity after the transition to mechanical solidarity: \n\nDurkheim's use of the term anomie was about the phenomenon of industrialization—mass-regimentation that could not adapt due to its own inertia—its resistance to change, which causes disruptive cycles of collective behavior e.g. economics, due to the necessity of a prolonged buildup of sufficient force or momentum to overcome the inertia.\n\nLater in 1897, in his studies of suicide, Durkheim associated anomie to the influence of a lack of norms or norms that were too rigid. But such normlessness or norm-rigidity was a \"symptom of anomie\", caused by the lack of differential adaptation that would enable norms to evolve naturally due to self-regulation, either to develop norms where none existed or to change norms that had become rigid and obsolete.\n\nIn 1938 Robert K. Merton linked anomie with deviance and argued that the discontinuity between culture and structure have the dysfunctional consequence of leading to deviance within society. He described 5 types of deviance in terms of the acceptance or rejection of social goals and the institutionalized means of achieving them.\n\nThe word, \"a reborrowing with French spelling of \"anomy\"\", comes from Greek \"lawlessness\", namely the privative alpha prefix (\"a-\" \"without\"), and \"nomos\" \"law\". The Greeks distinguished between \"nomos\" (, \"law\"), and \"arché\" (, \"starting rule, axiom, principle\"). For example, a monarch is a single ruler but he or she might still be subject to, and not exempt from, the prevailing laws, i.e. \"nomos\". In the original city state democracy, the majority rule was an aspect of \"arché\" because it was a rule-based, customary system, which might or might not make laws, i.e. \"nomos\". Thus, the original meaning of \"anomie\" defined anything or anyone against or outside the law, or a condition where the current laws were not applied resulting in a state of illegitimacy or lawlessness.\n\nThe contemporary English understanding of the word \"anomie\" can accept greater flexibility in the word \"norm\", and some have used the idea of normlessness to reflect a similar situation to the idea of anarchy. But, as used by Émile Durkheim and later theorists, \"anomie\" is a reaction against or a retreat from the regulatory social controls of society, and is a completely separate concept from anarchy, which consists of the absence of the roles of rulers and submitted.\n\nThe nineteenth century French pioneer sociologist Émile Durkheim borrowed the word from French philosopher Jean-Marie Guyau and used it in his influential book \"Suicide\" (1897), outlining the social (and not individual) causes of suicide, characterized by a rapid change of the standards or values of societies (often erroneously referred to as normlessness), and an associated feeling of alienation and purposelessness. He believed that \"anomie\" is common when the surrounding society has undergone significant changes in its economic fortunes, whether for better or for worse and, more generally, when there is a significant discrepancy between the ideological theories and values commonly professed and what was actually achievable in everyday life. This was contrary to previous theories on suicide which generally maintained that suicide was precipitated by negative events in a person's life and their subsequent depression.\n\nIn Durkheim's view, traditional religions often provided the basis for the shared values which the anomic individual lacks. Furthermore, he argued that the division of labor that had been prevalent in economic life since the Industrial Revolution led individuals to pursue egoistic ends rather than seeking the good of a larger community. Robert King Merton also adopted the idea of anomie to develop strain theory, defining it as the discrepancy between common social goals and the legitimate means to attain those goals. In other words, an individual suffering from anomie would strive to attain the common goals of a specific society yet would not be able to reach these goals legitimately because of the structural limitations in society. As a result, the individual would exhibit deviant behavior. Friedrich Hayek notably uses the word \"anomie\" with this meaning.\n\nAccording to one academic survey, psychometric testing confirmed a link between anomie and academic dishonesty among university students, suggesting that universities needed to foster codes of ethics among students in order to curb it. In another study, anomie was seen as a \"push factor\" in tourism.\n\nAs an older variant, the 1913 \"Webster's Dictionary\" reports use of the word \"anomie\" as meaning \"disregard or violation of the law\" but anomie as a social disorder is not to be confused with anarchy. Proponents of anarchism claim that anarchy does not necessarily lead to anomie and that hierarchical command actually increases lawlessness. Some anarcho-primitivists argue that complex societies, particularly industrial and post-industrial societies, directly cause conditions such as anomie by depriving the individual of self-determination and a relatively small reference group to relate to, such as the band, clan, or tribe.\n\nIn Albert Camus's existentialist novel \"The Stranger\", the bored, alienated protagonist Meursault struggles to construct an individual system of values as he responds to the disappearance of the old. He exists largely in a state of anomie, as seen from the apathy evinced in the opening lines: \"\" (\"Today mother died. Or maybe yesterday, I don't know\").\n\nFyodor Dostoyevsky expressed a similar concern about anomie in his novel \"The Brothers Karamazov\". The Grand Inquisitor remarks that in the absence of God and immortal life, everything would be lawful. In other words, that any act becomes thinkable, that there is no moral compass, which leads to apathy and detachment.\n\n\n"}
{"id": "43558209", "url": "https://en.wikipedia.org/wiki?curid=43558209", "title": "Anya Reading", "text": "Anya Reading\n\nDr Anya Reading is a senior lecturer in geophysics at both the School of Earth Sciences and the Centre of Excellence in Ore Deposits (CODES) at the University of Tasmania. \n\nShe received a BSc (Hons) in geophysics from the University of Edinburgh, UK, and was awarded a PhD from the University of Leeds, UK, for research on subduction zone structure beneath southern North Island, New Zealand. Dr Reading spent five years in marine and land-based geophysics with the British Antarctic Survey before she joined the University of Edinburgh as a lecturer in 1998. In 1999 she received a Diploma of Music from the Open University. In 2000 she moved to Australia and was a research fellow at ANU before she joined the University of Tasmania as an academic staff member in 2007.\n\nDr Reading's research interests include computational methods for analysing data from the natural world, seismological techniques, using ambient seismic energy, the tectonic evolution of Australasia and Antarctica, archaeological and other near-surface geophysics.\n"}
{"id": "28913755", "url": "https://en.wikipedia.org/wiki?curid=28913755", "title": "Argo Glacier", "text": "Argo Glacier\n\nArgo Glacier () is a glacier in the Miller Range, long, flowing northeast to enter Marsh Glacier just south of Macdonald Bluffs. It was named by the New Zealand Geological Survey Antarctic Expedition (1961–62) after the \"Argo\", the vessel sailed by Jason in Greek mythology.\n\n"}
{"id": "49641950", "url": "https://en.wikipedia.org/wiki?curid=49641950", "title": "Castellaroite", "text": "Castellaroite\n\nCastellaroite is a rare arsenate mineral with formula Mn(AsO)•4HO. It is related to the phosphate mineral metaswitzerite. Castellaroite is monoclinic, with space group \"P\"2/\"n\". The other natural manganese arsenate hydrate is manganohörnesite, which is an octahydrate.\n"}
{"id": "4897934", "url": "https://en.wikipedia.org/wiki?curid=4897934", "title": "Coil (chemistry)", "text": "Coil (chemistry)\n\nA coil, in chemistry, is a tube, frequently in spiral form, used commonly to cool steam originating from a distillation and thus to condense it in liquid form. Usually it is of copper or another material that conducts heat easily. Coils are often used in chemical processes in batch reaction or mixing tank as internal source of heat transfer.\n"}
{"id": "49641658", "url": "https://en.wikipedia.org/wiki?curid=49641658", "title": "Dargaite", "text": "Dargaite\n\nDargaite is a rare mineral with formula BaCa(SiO)(SO)O. It is the barium-analogue of nabimusaite, also differing from it in the lack of fluorine. It is one of many recently approved new minerals coming from the Hatrurim complex. Dargaite, as nabimusaite, is trigonal (space group \"R\"-3\"m\").\n"}
{"id": "39723043", "url": "https://en.wikipedia.org/wiki?curid=39723043", "title": "Der Philosophische Arzt", "text": "Der Philosophische Arzt\n\nDer Philosophische Arzt is a medical publication published in the late 18th century by , a prominent German physician and physician to the Russian Empress, Catherine II.\n\nThe first edition of \"Der Philosophische Arzt\" was first published in 1775, but perhaps as early as 1770. It was initially published anonymously as was the second edition, though it was widely believed that Weikard was the author. The reason for doing so is unclear but was probably due to anticipated critical reactions to its publication from several sources. A principal one was the Prince-Bishop of Fulda, to whom Weikard served as physician and in a Catholic region where Weikard worked as a spa doctor being supported by the state. According to Otto Schmitt's biography of Weikard published in 1970, the reaction of organized religion to the publication of his textbook was widespread condemnation. This was likely due to his attacks in the textbook on various religious practices for curing medical illnesses. According to Schmidt, the attacks against Weikard continued throughout his career but his patron, Prince Heinrich von Bibra, maintained an amicable relationship with him and supported him financially late in his career despite many people who criticized the Prince for doing so.\n\n"}
{"id": "5775549", "url": "https://en.wikipedia.org/wiki?curid=5775549", "title": "Diabasbrottet Quarry", "text": "Diabasbrottet Quarry\n\nThe Diabasbrottet Quarry (), located on Mt. Hunneberg, Västergötland, Sweden, is the location of the Global Boundary Stratotype Section and Point (GSSP) which marks the lower boundary of the Floian stage of the Lower Ordovician.\n\nThe lower boundary of the Floian is defined as the first appearance of \"Tetragraptus approximatus\", a graptolite species. The rock section, dominated by shale, is unusually fossiliferous, including fossilized remains of graptolites, conodonts, and trilobites. It is highly compressed, and the Ordovician section is only 12 m thick.\n\nThe Floian stage is named after Flo Parish, a village 5 km to the southeast of the Diabasbrottet quarry.\n\nIt was selected over \"The Ledge\" in western Newfoundland in 2000 and ratified as the GSSP by the International Union of Geological Sciences in 2002.\n\n"}
{"id": "4605472", "url": "https://en.wikipedia.org/wiki?curid=4605472", "title": "Dimitar Ivanov Popov", "text": "Dimitar Ivanov Popov\n\nDimitar Ivanov Popov () (October 13, 1894 – October 25, 1975) was a Bulgarian organic chemist and an academician of the Bulgarian Academy of Sciences.\n\nProf. D. Ivanov is known by his father's name Ivanov rather than his family's name Popov.\n\nHe is the namesake of the Ivanov reaction in chemistry.\n\n \n"}
{"id": "6365732", "url": "https://en.wikipedia.org/wiki?curid=6365732", "title": "Ecological thinning", "text": "Ecological thinning\n\nEcological thinning is a silvicultural technique used in forest management that involves cutting trees to improve functions of a forest other than timber production.\n\nAlthough thinning originated as a man-made forest management tool, aimed at increasing timber yields, the shift from production forests to multifunctional forests brought with it the cutting of trees to manipulate an ecosystem for various reasons, ranging from removing non-native species from a plot to removing poplars growing on a riverside beach aimed at recreational use.\n\nSince the 1970s, leaving the thinned trees on the forest floor has become an increasingly common policy: wood can be decomposed in a more natural fashion, playing an important role in increasing biodiversity by providing habitat to various invertebrates, birds and small mammals. Many fungi (e.g. \"Calocera viscosa\") and mosses are saproxylic or epixylic as well (e.g. \"Marchantiophyta\") – some moss species completing their entire life-cycle on a single log.\n\nWhere trees are managed under a commercial regime, competition is reduced by removing adjacent stems that exhibit less favourable timber quality potential. When left in a natural state trees will \"self-thin\", but this process can be unreliable in some circumstances. Examples of this can be found in the Buxus – Ironbark forests and woodlands of Victoria (Australia) where a large proportion of trees are coppice, resultant from timber cutting in decades gone by.\n\nThinning decreases canopy closure and increases the penetration of solar radiation into the canopy. The photosynthetic efficiency of this energy is improved, and needle retention is prolonged, especially in the lower parts of the crown. The root system, crown length, crown diameter, and crown area all increase after thinning. Even if soil evaporation and individual tree transpiration increases after thinning, total evapo-transpiration at stand level tends to decrease; canopy water interception is reduced and throughfall increased, so that tree-water status usually improves after thinning.\n\nRadial annual growth is an integrative index of tree physiological response to environmental variation. Working with Norway spruce, which can be expected to behave in some respects similarly to white spruce, Misson et al. showed that a reduction in stand density alters the classical climate–growth relationship. At individual tree level, thinning could be used to increase tree resistance to drought stress. Nevertheless, this effect is limited when site conditions are limiting. Misson et al. concluded that heavy thinning should be applied when forest decline is expected from drought stress. Furthermore, stands on dry sites should be thinned more heavily because such sites cannot support high-density stands.\n\nAussenac, also working with Norway spruce, investigated the response to thinning. Thinning altered the temporal evolution of radial growth at all frequencies in radial growth chronologies. Previous studies had shown, not surprisingly, that thinning decreases the canopy closure and encourages solar radiation to penetrate the canopy. Solar energy then becomes more important inside the crown. The improved photosynthetic efficiency of this energy influences the retention time of needles, especially in the lower part of the crown. Furthermore, the length, diameter, and area, of the crown, and the size of the root system all increase after thinning.\n\nThese modifications at the crown level influence photosynthate production positively as long as water supply is not limiting. Even if soil evaporation and individual tree transpiration are more important after thinning, total evapo-transpiration at stand level tends to decrease. Furthermore, with the reduced importance of leaf area index (LAI), thinning reduces canopy water interception and increases throughfall. This explains why the soil moisture content increases in a thinned stand. Thus, during the vegetation period, tree-water status is usually better in a thinned stand than in a high-density stand.\n\nFor semi-tolerant species such as Norway spruce and white spruce, solar energy is less limiting than for intolerant species. This can account for the exponential relationship found by Misson et al. between thinning intensity and mean radial growth. Only when thinning was relatively severe did important radial growth variation occur. Furthermore, the ecophysiological advantage of an improved water supply could be counterbalanced by limiting site conditions.\n\nFor Norway spruce in the Belgian Ardennes, Misson et al. recommended that stand basal area should not exceed 26 m²/ha on a dry site, or 29 m²/ha on a moist site if the objective is to maintain high long-term radial growth.\n\nMisson et al. found that adaptations to thinning cover a continuous range of different time scales, with long-, medium-, and short-term growth variations being controlled by very different internal factors influenced by the environment. Misson et al. ascribed what they called long- and medium-term radial growth variation of individual trees mainly to structural adaptation, such as enlargement of the crown or the root system. Short-term radial growth variations, on the other hand, were considered to be due mainly to physiological acclimatisation, with response to factors such as stomatal conductance regulation, variation of photosynthetic capacity, and respiration. Nevertheless, it is clear that the adaptations are linked, and that radial growth is the integrative response of this mutual inter-relationship.\n\nResearch programs under way in various parts of the world (e.g. United States and Australia) are aimed at providing an alternate approach in forest management where conservation objectives are a high priority. Methods of ecological thinning being developed on silvicultural techniques for local forest types. Ecological thinning is being developed using two principles: 1. appropriate stem reduction to reduce competition and 2. retention of trees (selection) that are more suitable for wildlife (i.e. not timber production). An example of ecological thinning research is the project in Victoria's Box-Ironbark forests, investigating various thinning and timber removal methods under an adaptive management or AEM framework. The primary objective is to generate (over time) a number of forest habitat values (i.e. tree hollows) that are crucial for wildlife conservation.\n\n"}
{"id": "9284012", "url": "https://en.wikipedia.org/wiki?curid=9284012", "title": "Facial feedback hypothesis", "text": "Facial feedback hypothesis\n\nThe facial feedback hypothesis states that facial movement can influence emotional experience. For example, an individual who is forced to smile during a social event will actually come to find the event more of an enjoyable experience.\n\nCharles Darwin was among the first to suggest that physiological changes caused by an emotion had a direct impact \"on\", rather than being just the consequence \"of\" that emotion. He wrote:\n\nThe free expression by outward signs of an emotion intensifies it. On the other hand, the repression, as far as this is possible, of all outward signs softens our emotions... Even the simulation of an emotion tends to arouse it in our minds.\n\nFollowing on this idea, William James proposed that, contrary to common belief, awareness of bodily changes activated by a stimulus \"\"is\" the emotion\". If no bodily changes are felt, there is only an intellectual thought, devoid of emotional warmth. In \"The Principles of Psychology\", James wrote: \"Refuse to express a passion, and it dies\".\n\nThis proved difficult to test, and little evidence was available, apart from some animal research and studies of people with severely impaired emotional functioning. The facial feedback hypothesis, \"that skeletal muscle feedback from facial expressions plays a causal role in regulating emotional experience and behaviour\", developed almost a century after Darwin.\n\nWhile James included the influence of all bodily changes on the creation of an emotion, \"including among them visceral, muscular, and cutaneous effects\", modern research mainly focuses on the effects of facial muscular activity. One of the first to do so, Silvan Tomkins wrote in 1962 that \"the face expresses affect, both to others and the self, via feedback, which is more rapid and more complex than any stimulation of which the slower moving visceral organs are capable\".\n\nTwo versions of the facial feedback hypothesis appeared, although \"these distinctions have not always been consistent\".\n\n\nAccording to Jeffrey Browndyke, \"the strongest evidence for the facial feedback hypothesis to date comes from research by Lanzetta et al. (1976)\" (but see \"Studies using Botox\" below for more recent and powerful evidence). Participants had lower skin conductance and subjective ratings of pain when hiding the painfulness of the shocks they endured, compared with those who expressed intense pain.\n\nHowever, in all research, difficulty remained in how to measure an effect without alerting the participant to the nature of the study and how to ensure that the connection between facial activity and corresponding emotion is not implicit in the procedure.\n\nOriginally, the facial feedback hypothesis studied the enhancing or suppressing effect of facial efference on emotion in the context of spontaneous, \"real\" emotions, using stimuli. This resulted in \"the inability of research using spontaneous efference to separate correlation from causality\". Laird (1974) used a cover story (measuring muscular facial activity with electrodes) to induce particular facial muscles contraction in his participants without mentioning any emotional state. However, the higher funniness ratings of the cartoons obtained by those participants \"tricked\" into smiling may have been caused by their recognising the muscular contraction and its corresponding emotion: the \"self-perception mechanism\", which Laird (1974) thought was at the root of the facial feedback phenomenon. Perceiving physiological changes, people \"fill the blank\" by feeling the corresponding emotion. In the original studies, Laird had to exclude 16% (Study 1) and 19% (Study 2) of the participants as they had become aware of the physical and emotional connection during the study.\n\nAnother difficulty is whether the process of manipulation of the facial muscles did not cause so much exertion and fatigue that those, partially or wholly, caused the physiological changes and subsequently the emotion.\nFinally, the presence of physiological change may have been induced or modified by cognitive process.\n\nIn an attempt to provide a clear assessment of the theory that a purely physical facial change, involving only certain facial muscles, can result in an emotion, Strack, Martin, & Stepper (1988) devised a cover story that would ensure the participants adopt the desired facial posing without being able to perceive either the corresponding emotion or the researchers' real motive. Told they were taking part in a study to determine the difficulty for people without the use of their hands or arms to accomplish certain tasks, participants held a pen in their mouth in one of two ways. The Lip position would contract the orbicularis oris muscle, resulting in a frown. The Teeth position would cause the zygomaticus major or the risorius muscle, resulting in a smile. The control group would hold the pen in their nondominant hand. All had to fill a questionnaire in that position and rate the difficulty involved. The last task, which was the real objective of the test, was the subjective rating of the funniness of a cartoon. The test differed from previous methods in that there were no emotional states to emulate, dissimulate or exaggerate.\n\nAs predicted, participants in the Teeth condition reported significantly higher amusement ratings than those in the Lips condition. The cover story and the procedure were found to be very successful at initiating the required contraction of the muscles without arising suspicion, 'cognitive interpretation of the facial action, and avoiding significant demand and order effects. It has been suggested that more effort may be involved in holding a pen with the lips compared with the teeth.\n\nTo avoid the possible effort problem, Zajonc, Murphy and Inglehart (1989) had subjects repeat different vowels, provoking smiles with \"ah\" sounds and frowns with \"ooh\" sounds for example, and again found a measurable effect of facial feedback. Ritual chanting of smile vowels has been found to be more pleasant than chanting of frown vowels, which may explain their comparative prevalence in religious mantra traditions.\n\nThese studies have resolved many of the methodological issues associated with the facial feedback hypothesis. Darwin's theory can be demonstrated, and the moderate, yet significant effect of this theory of emotions opens the door to new research on the \"multiple and nonmutually exclusive plausible mechanisms\" of the effects of facial activity on emotions.\n\nDoubts about the robustness of these findings was voiced in 2016 when a replication series of the original 1988 experiment coordinated by Eric-Jan Wagenmakers and conducted in 17 labs did not find any support for the hypothesis.\n\nBecause facial expressions involve both motor (efferent) and sensory (afferent) mechanisms, it is possible that effects attributed to facial feedback are due solely to feedback mechanisms, or feed-forward mechanisms, or some combination of both. Recently, strong experimental support for a facial feedback mechanism is provided through the use of botulinum toxin (commonly known as Botox) to temporarily paralyze facial muscles. Botox selectively blocks muscle feedback by blocking presynaptic acetylcholine receptors at the neuromuscular junction. Thus, while motor efference commands to the facial muscles remain intact, sensory afference from extrafusal muscle fibers, and possibly intrafusal muscle fibers, is diminished.\n\nSeveral studies have examined the correlation of botox injections and emotion and these suggest that the toxin could be used as a treatment for depression. Further studies have used experimental control to test the hypothesis that botox affects aspects of emotional processing. It has been suggested that the treatment of nasal muscles would reduce the ability of the person to form a disgust response which could offer a reduction of symptoms associated with obsessive compulsive disorder.\n\nIn a functional neuroimaging study, Andreas Hennenlotter and colleagues asked participants to perform a facial expression imitation task in an fMRI scanner before and two weeks after receiving botox injections in the corrugator supercilii muscle used in frowning. During imitation of angry facial expressions, botox decreased activation of brain regions implicated in emotional processing and emotional experience (namely, the amygdala and the brainstem), relative to activations before botox injection. These findings show that facial feedback modulates neural processing of emotional content, and that botox changes how the human brain responds to emotional situations.\n\nIn a study of cognitive processing of emotional content, David Havas and colleagues asked participants to read emotional (angry, sad, happy) sentences before and two weeks after botox injections in the corrugator supercilii muscle used in frowning. Reading times for angry and sad sentences were longer after botox injection than before injection, while reading times for happy sentences were unchanged. This finding shows that facial muscle paralysis has a selective effect on processing of emotional content. It also demonstrates that cosmetic use of botox affects aspects of human cognition - namely, the understanding of language.\n\nA study by Mariëlle Stel, Claudia van den Heuvel, and Raymond C. Smeets has shown that the facial feedback hypothesis does not hold for people with autism spectrum disorders (ASD); that is, \"individuals with ASD do not experience feedback from activated facial expressions as controls do\".\n\n\n\n"}
{"id": "2394904", "url": "https://en.wikipedia.org/wiki?curid=2394904", "title": "Firmament", "text": "Firmament\n\nIn biblical cosmology, the firmament is the structure above the atmosphere of Earth, conceived as a vast solid dome. According to the Genesis creation narrative, God created the firmament to separate the \"waters above\" the earth from the \"waters below\" the earth. The word is anglicized from the Latin \"firmamentum\", which appears in the Vulgate, a late-4th-century Latin translation of the Bible.\n\nThe firmament is described in in the Genesis creation narrative:\n\nThen God said, “Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.” Thus God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament; and it was so. And God called the firmament Heaven. So the evening and the morning were the second day.\n\nThe word \"firmament\" is first recorded in a Middle English narrative based on scripture dated 1250. It later appeared in the King James Bible. The word is anglicised from Latin \"firmamentum\", used in the Vulgate (4th century). This in turn is derived from the Latin root \"firmus\", a cognate with \"firm\". The word is a Latinization of the Greek \"stereōma\", which appears in the Septuagint (c. 200 BCE).\n\nThe word \"firmament\" is used to translate \"rāqîaʿ\" (), a word used in Biblical Hebrew. It is derived from the root \"raqqəʿ\" (), meaning \"to beat or spread out thinly\", e.g., the process of making a dish by hammering thin a lump of metal.\n\nLike most ancient peoples, the Hebrews believed the sky was a solid dome with the Sun, Moon, planets and stars embedded in it. According to The \"Jewish Encyclopedia\":\n\nThe Hebrews regarded the earth as a plain or a hill figured like a hemisphere, swimming on water. Over this is arched the solid vault of heaven. To this vault are fastened the lights, the stars. So slight is this elevation that birds may rise to it and fly along its expanse.\n\nA detailed Christian view of the universe, based on various Biblical texts and earlier theories by Theophilus of Antioch and Clement of Alexandria, was formulated by the 6th century Egyptian monk, Cosmas Indicopleustes. He described a flat rectangular world surrounded by four seas; at the far edges of the seas, four immense vertical walls supported a vaulted roof, the firmament, above which in a further vaulted space lived angels who moved the heavenly bodies and controlled rainfall from a vast cistern. Augustine wrote that too much learning had been expended on the nature of the firmament. \"We may understand this name as given to indicate not it is motionless but that it is solid.\" he wrote. Saint Basil argued for a fluid firmament. According to St. Thomas Aquinas, the firmament had a \"solid nature\" and stood above a \"region of fire, wherein all vapor must be consumed.\"\n\nThe Copernican Revolution of the 16th century led to reconsideration of these matters. In 1554, John Calvin proposed that \"firmament\" be interpreted as clouds. \"He who would learn astronomy and other recondite arts, let him go elsewhere,\" wrote Calvin. \"As it became a theologian, [Moses] had to respect \"us\" rather than the stars,\" Calvin wrote. Calvin's doctrine of accommodation allowed Protestants to accept the findings of science without rejecting the authority of scripture.\n\nThe Greeks and Stoics adopted a model of celestial spheres after the discovery of the spherical Earth in the 4th to 3rd centuries BCE. The Medieval Scholastics adopted a cosmology that fused the ideas of the Greek philosophers Aristotle and Ptolemy. This cosmology involved celestial orbs, nested concentrically inside one another, with the earth at the center. The outermost orb contained the stars and the term \"firmament\" was then transferred to this orb. There were seven inner orbs for the seven wanderers of the sky, and their ordering is preserved in the naming of the days of the week.\n\nEven Copernicus' heliocentric model included an outer sphere that held the stars (and by having the earth rotate daily on its axis it allowed the firmament to be completely stationary). Tycho Brahe's studies of the nova of 1572 and the Comet of 1577 were the first major challenges to the idea that orbs existed as solid, incorruptible, material objects.\n\nIn 1584, Giordano Bruno proposed a cosmology without firmament: an infinite universe in which the stars are actually suns with their own planetary systems. After Galileo began using a telescope to examine the sky, it became harder to argue that the heavens were perfect, as Aristotelian philosophy required. By 1630, the concept of solid orbs was no longer dominant.\n\n\n"}
{"id": "230072", "url": "https://en.wikipedia.org/wiki?curid=230072", "title": "Four-force", "text": "Four-force\n\nIn the special theory of relativity, four-force is a four-vector that replaces the classical force.\n\nThe four-force is defined as the rate of change in the four-momentum of a particle with respect to the particle's proper time:\n\nFor a particle of constant invariant mass formula_2, formula_3 where formula_4 is the four-velocity, so we can relate the four-force with the four-acceleration formula_5 as in Newton's second law:\n\nHere\n\nand\n\nwhere formula_9, formula_10 and formula_11 are 3-space vectors describing the velocity and the momentum of the particle and the force acting on it respectively.\n\nFrom the formulae of the previous section it appears that the time component of the four-force is the power expended, formula_12, apart from relativistic corrections formula_13. This is only true in purely mechanical situations, when heat exchanges vanish or can be neglected.\n\nIf the full thermo-mechanical case, not only work, but also heat contributes to the change in energy, which is the time component of the energy–momentum covector. The time component of the four-force includes in this case a heating rate formula_14, besides the power formula_12. Note that work and heat cannot be meaningfully separated, though, as they both carry inertia. This fact extends also to contact forces, that is, to the stress-energy-momentum tensor.\n\nTherefore, in thermo-mechanical situations the time component of the four-force is \"not\" proportional to the power formula_12 but has a more generic expression, to be given case by case, which represents the supply of internal energy from the combination of work and heat, and which in the Newtonian limit becomes formula_17.\n\nIn general relativity the relation between four-force, and four-acceleration remains the same, but the elements of the four-force are related to the elements of the four-momentum through a covariant derivative with respect to proper time.\n\nIn addition, we can formulate force using the concept of coordinate transformations between different coordinate systems. Assume that we know the correct expression for force in a coordinate system at which the particle is momentarily at rest. Then we can perform a transformation to another system to get the corresponding expression of force. In special relativity the transformation will be a Lorentz transformation between coordinate systems moving with a relative constant velocity whereas in general relativity it will be a general coordinate transformation.\n\nConsider the four-force formula_19 acting on a particle of mass formula_20 which is momentarily at rest in a coordinate system. The relativistic force formula_21 in another coordinate system moving with constant velocity formula_22, relative to the other one, is obtained using a Lorentz transformation:\n\nformula_23\n\nformula_24\n\nwhere formula_25.\n\nIn general relativity, the expression for force becomes\n\nformula_26\n\nwith covariant derivative formula_27. The equation of motion becomes\n\nformula_28\n\nwhere formula_29 is the Christoffel symbol. If there is no external force, this becomes the equation for geodesics in the curved space-time. The second term in the above equation, plays the role of a gravitational force. If formula_30 is the correct expression for force in a freely falling frame formula_31, we can use then the equivalence principle to write the four-force in an arbitrary coordinate formula_32:\n\nformula_33\n\nIn special relativity, Lorentz four-force (four-force acting to charged particle situated in electromagnetic field) can be expressed as:\n\nwhere \n\n\n"}
{"id": "44497716", "url": "https://en.wikipedia.org/wiki?curid=44497716", "title": "Frank Geyer", "text": "Frank Geyer\n\nFrank Geyer (July 28, 1853 – October 4, 1918) was detective from Philadelphia, Pennsylvania, best known for his investigation of H. H. Holmes, one of America's first serial killers. Geyer was a longtime city employee of the Philadelphia Police Department, and in 1894 was assigned to investigate the Holmes-Pitezel Case. He published the story in his book, \"The Holmes-Pitezel Case: a history of the greatest crime of the century and of the search for the missing Pitezel children.\"\n\nSon of Reuben K. Geyer and Camilla Buck, Frank Geyer died at the age of 65 due to La Grippe (Spanish Flu) and his funeral was attended by hundreds of policemen and detectives.\n\nH H Holmes's recorded crimes began in Chicago in 1893 when he opened a hotel called The World's Fair Hotel for the World's Columbian Exposition. The structure, built by Holmes, would later be known as the 'Murder Castle' because of the labyrinthine constructions on the top two floors, which he used to torture and kill many of his victims. In a 1937 article, the Chicago Tribune described: \"There were rooms that had no doors. There were doors that had no rooms. A mysterious house it was indeed -- a crooked house, a reflex of the builder's own distorted mind. In that house occurred dark and eerie deeds.\"\nBoston police inspectors and a Pinkerton detective apprehended Holmes in 1894 in Boston on a coroner's warrant for insurance fraud perpetrated in Philadelphia; however, Boston officials did not find the warrant sufficient to hold Holmes so they contacted Fort Worth, Texas for an outstanding warrant of horse theft. Holmes volunteered to be extradited to Philadelphia for the insurance fraud as he felt he would receive a much lighter sentence. Texas was notorious for rendering harsh sentences to horse thieves. The City of Philadelphia Police Department sent Detective Thomas Crawford to Boston to bring H. H. Holmes and his accomplice, Mrs. Carrie Pitezel, to Philadelphia for a trial.\n\nPhiladelphia city detective Frank Geyer was tasked with investigating and the trail led him through the Mid West and Toronto, Canada, where he found the remains of two of the Pitezel children. They were the children of Benjamin Pitezel, Holmes's former partner in crime, which he had murdered to commit life insurance fraud. Pitezel, however, was only involved in fraud and had no knowledge of the murders.\n\nThe initial investigation was concerned with the insurance fraud but it soon became apparent that Holmes killed Pitezel. In June 1895 Frank Geyer left Philadelphia to retrace Holmes's steps. His findings in Toronto led to further investigations of Holmes's Chicago property, which sealed his fate. Geyer used information from the unsent letters written by the Pitezel children which, for an unknown reason, were kept by Holmes. In Toronto, he found the bodies of Alice and Nellie Pitezel. He continued his search and found the burnt remains of Howard Pitezel, the third child, in a house Holmes had rented in Irvington, Indianapolis.\n\nHolmes was found guilty of four counts of murder in the first degree and six counts of attempted murder and executed in May 1896 at the age of 34. His total number of victims is estimated at around 200.\n\nThat same year Frank Geyer published his book detailing the case. He described the story as \"one of the most marvellous [sic] stories of modern times\".\n\nSeveral popular books falsely claimed Detective Geyer's wife and twelve-year-old daughter died in a fire shortly after he was assigned to investigate H. H. Holmes and the three missing Pitezel children.\n\nHowever, Geyer's beloved wife and daughter never died in a fire and continued to live well past his death in 1918.\n\nIn 1896, Detective Geyer became an author and inventor. He authored the \" Holmes-Pitezel case: a history of the greatest crime of the century and of the search for the missing Pitezel children\", which became an instant best seller. Shortly after its release, his \"Shutter or Door Fastener\" patent application was approved by the United States Patent Office on March 10, 1896, Patent No. 556,141. After 27 years with the City of Philadelphia Police Department, Geyer opened the Frank P. Geyer Detective Agency, located at 1328 Arch Street in Philadelphia and investigated high profile cases, mostly in the Pennsylvania and New Jersey areas. In 1907, he invented the \"Safety-Lock for Pocket Books and Hand Bags, which was approved by the Patent Office December 3, 1907, Patent No. 872,619.\n"}
{"id": "54564975", "url": "https://en.wikipedia.org/wiki?curid=54564975", "title": "G18P", "text": "G18P\n\nG18P is a pigeon rotavirus infecting and killing domestic pigeons. This disease is found in Western Australia, Victoria and South Australia.\n"}
{"id": "37214162", "url": "https://en.wikipedia.org/wiki?curid=37214162", "title": "Givi Maisuradze", "text": "Givi Maisuradze\n\nGivi Maisuradze is a Georgian geologist, Professor, Dr.Sc. He and his spouse Nina Klopotovskaia (paleontologist) were part of the research team that discovered early hominin skulls and later skeletons dating 1.8 million years old in Dmanisi, Georgia.\n\nHe was born on February 11, 1934 in the capital of Georgia, Tbilisi.\nIn 1952 he graduated from school # 6 in Tbilisi. Between 1952 and 1957 he attended Tbilisi State University specializing in both; Geography and Geology. In 1957 obtained a title of Engineer Geologist at Tbilisi State University.\nAs part of his research he has traveled to Middle Asia, Europe, Canada and China.\n\n130 research works published, 21 out of which at Impaqt Journal.\n\n1976 - Order of Merit of the Soviet Union's Science Society\n1982 - INQUA international congress awarded by B. Sokolov\n1983 – Recognition of Georgian Society of Science for the participation in the creation of Georgian Red Book about the nature protection (Tsiteli Tsigni).\n2001- Received Order of Merit from the president of Georgia for the discoveries made in Dmanisi region.\n\nYoung (Quaternary) volcanoes, terrestrial magnetism, neotectonics, archeology, statgraphics of quaternary layers and their regional and world correlations.\n\nBetween 2002 and 2004, Givi Maisuradze conducted a research on Caucasian Seismic Information Network for Hazard and Risk Assessment.\nBetween 1997 and 2005 worked on Dmanisi sight where archeological research was conducted for several years after the discovery of Homo Erectus in Dmanisi.\n\nIII, Reid Ferring, Antje Justus, Medea Nioradze, Merab Tvalchrelidze,\nSusan C. Antón, Gerhard Bosinski, Olaf Jöris, Marie-A.-de Lumley, Givi\nMajsuradze, Aleksander Mouskhelishvili, 2000. Earliest Pleistocene\nHominid Cranial Remains from Dmanisi, Republic of Georgia: Taxonomy,\nGeological Setting, and Age .Vol.288. no.5468, pp. 1019 – 1025\n"}
{"id": "20895829", "url": "https://en.wikipedia.org/wiki?curid=20895829", "title": "History of rockets", "text": "History of rockets\n\nThe first rockets were used as propulsion systems for arrows, and may have appeared as early as the 10th century Song dynasty China. However more solid documentary evidence does not appear until the 13th century. The technology probably spread across Eurasia in the wake of the Mongol invasions of the mid-13th century. Usage of rockets as weapons before modern rocketry is attested in China, Korea, Indian subcontinent, and Europe. One of the first recorded rocket launchers is the \"wasp nest\" fire arrow launcher produced by the Ming dynasty in 1380. In Europe rockets were also used in the same year at the Battle of Chioggia. The Joseon kingdom of Korea used a type of mobile multiple rocket launcher known as the \"Munjong Hwacha\" by 1451. Iron-cased rockets, known as Mysorean rockets, were developed in Kingdom of Mysore by the mid 18th century in India, and were later copied by the British. The later models and improvements were known as the Congreve rocket and used in the Napoleonic Wars.\n\nUse of liquid propellants instead of gunpowder greatly improved the effectiveness of rocket artillery in World War I, and opened up the possibility of manned spaceflight after 1918.\n\nThe dating of the invention of the first rocket, otherwise known as the gunpowder propelled fire arrow, is disputed. The History of Song attributes the invention to two different people at different times, Feng Zhisheng in 969 and Tang Fu in 1000. However Joseph Needham argues that rockets could not have existed befoire the 12th century, since the gunpowder formulas listed in the Wujing Zongyao are not suitable as rocket propellant.\n\nRockets may have been used as early as 1232, when reports appeared describing fire arrows and 'iron pots' that could be heard for 5 leagues (25 km, or 15 miles) when they exploded upon impact, causing devastation for a radius of , apparently due to shrapnel. Rockets are recorded to have been used by the Song navy in a military exercise dated to 1245. Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the 'ground-rat,' a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son the Emperor Lizong.\n\nSubsequently, rockets are included in the military treatise \"Huolongjing\", also known as the Fire Drake Manual, written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text mentions the first known multistage rocket, the 'fire-dragon issuing from the water' (huo long chu shui), thought to have been used by the Chinese navy.\n\nRocket launchers known as \"wasp nests\" were ordered by the Ming army in 1380.\n\nThe American historian Frank H. Winter proposed in \"The Proceedings of the Twentieth and Twenty-First History Symposia of the International Academy of Astronautics\" that southern China and the Laotian community rocket festivals might have been key in the subsequent spread of rocketry in the Orient.\n\nThe Chinese fire arrow was adopted by the Mongols in northern China, who employed Chinese rocketry experts as mercenaries in the Mongol army. Rockets are thought to have spread via the Mongol invasions to other areas of Eurasia in the mid 13th century.\n\nRocket-like weapons are reported to have been used at the Battle of Mohi in the year 1241.\n\nBetween 1270 and 1280, Hasan al-Rammah wrote his \"al-furusiyyah wa al-manasib al-harbiyya\" (\"The Book of Military Horsemanship and Ingenious War Devices\"), which included 107 gunpowder recipes, 22 of which are for rockets. According to Ahmad Y Hassan, al-Rammah's recipes were more explosive than rockets used in China at the time. The terminology used by al-Rammah indicates a Chinese origin for the gunpowder weapons he wrote about, such as rockets and fire lances. Ibn al-Baitar, an Arab from Spain who had immigrated to Egypt, described saltpeter as \"snow of China\" ( ). Al-Baytar died in 1248. The earlier Arab historians called saltpeter \"Chinese snow\" and \" Chinese salt.\"\nThe Arabs used the name \"Chinese arrows\" to refer to rockets. The Arabs called fireworks \"Chinese flowers\". While saltpeter was called \"Chinese Snow\" by Arabs, it was called \"Chinese salt\" ( \"namak-i čīnī\") by the Iranians, or \"salt from the Chinese marshes\" ( ).\n\nIn 1300 Mongol mercenaries in India are recorded to have used hand held rockets. By the mid-14th century Indians were also using rockets in warfare.\n\nThe Korean kingdom of Joseon started producing gunpowder in 1374 and was producing cannons and rockets by 1377. However the multiple rocket launching carts known as the \"Munjong hwacha\" did not appear until 1451.\n\nIn Europe, Roger Bacon mentions gunpowder in his \"Opus Majus\" of 1267.\n\nHowever rockets do not feature in European warfare until the 1380 Battle of Chioggia.\n\nKonrad Kyeser described rockets in his famous military treatise Bellifortis around 1405.\n\nJean Froissart (c. 1337 – c. 1405) had the idea of launching rockets through tubes, so that they could make more accurate flights. Froissart's idea is a forerunner of the modern bazooka.\n\nAccording to the 18th-century historian Ludovico Antonio Muratori, rockets were used in the war between the Republics of Genoa and Venice at Chioggia in 1380. It is uncertain whether Muratori was correct in his interpretation, as the reference might also have been to bombard, but\nMuratori is the source for the widespread claim that the earliest recorded European use of rocket artillery dates to 1380.\nKonrad Kyeser described rockets in his famous military treatise Bellifortis around 1405.\nKyeser describes three types of rockets, swimming, free flying and captive.\n\nJoanes de Fontana in \"Bellicorum instrumentorum liber\" (c. 1420) described flying rockets in the shape of doves, running rockets in the shape of hares, and a large car driven by three rockets, as well as a large rocket torpedo with the head of a sea monster.\n\nIn the mid-16th century, Conrad Haas wrote a book that described rocket technology that combined fireworks and weapons technologies. This manuscript was discovered in 1961, in the Sibiu public records (Sibiu public records \"Varia II 374\"). His work dealt with the theory of motion of multi-stage rockets, different fuel mixtures using liquid fuel, and introduced delta-shape fins and bell-shaped nozzles.\n\nThe name \"Rocket\" comes from the Italian \"rocchetta\", meaning \"bobbin\" or \"little spindle\", given due to the similarity in shape to the bobbin or spool used to hold the thread to be fed to a spinning wheel. The Italian term was adopted into German in the mid 16th century, by Leonhard Fronsperger in a book on rocket artillery published in 1557, using the spelling \"rogete\", and by Conrad Haas as \"rackette\"; adoption into English dates to ca. 1610. Johann Schmidlap, a German fireworks maker, is believed to have experimented with staging in 1590.\n\n\"Lagari Hasan Çelebi\" was a legendary Ottoman aviator who, according to an account written by Evliya Çelebi, made a successful manned rocket flight. Evliya Çelebi purported that in 1633 Lagari Hasan Çelebi launched in a 7-winged rocket using 50 okka (63.5 kg, or 140 lbs) of gunpowder from Sarayburnu, the point below Topkapı Palace in Istanbul.\n\n\"Artis Magnae Artilleriae pars prima\" (\"Great Art of Artillery, the First Part\", also known as \"The Complete Art of Artillery\"), first printed in Amsterdam in 1650, was translated to French in 1651, German in 1676, English and Dutch in 1729 and Polish in 1963. For over two centuries, this work of Polish-Lithuanian Commonwealth nobleman Kazimierz Siemienowicz was used in Europe as a basic artillery manual. The book provided the standard designs for creating rockets, fireballs, and other pyrotechnic devices. It contained a large chapter on caliber, construction, production and properties of rockets (for both military and civil purposes), including multi-stage rockets, batteries of rockets, and rockets with delta wing stabilizers (instead of the common guiding rods).\n\nIn 1792, the first iron-cased rockets were successfully developed and used by Mysorean rulers of the Kingdom of Mysore in India against the larger British East India Company forces during the Anglo-Mysore Wars. The British then took an active interest in the technology and developed it further during the 19th century. The Mysore rockets of this period were much more advanced than the British had previously seen, chiefly because of the use of iron tubes for holding the propellant; this enabled higher thrust and longer range for the missile (up to 2 km range). After Tipu's eventual defeat in the Fourth Anglo-Mysore War and the capture of the Mysore iron rockets, they were influential in British rocket development, inspiring the Congreve rocket, which was soon put into use in the Napoleonic Wars.\n\nWilliam Congreve, son of the Comptroller of the Royal Arsenal, Woolwich, London, became a major figure in the field. From 1801, Congreve researched on the original design of Mysore rockets and set on a vigorous development program at the Arsenal's laboratory. Congreve prepared a new propellant mixture, and developed a rocket motor with a strong iron tube with conical nose. This early Congreve rocket weighed about 32 pounds (14.5 kilograms). The Royal Arsenal's first demonstration of solid fuel rockets was in 1805. The rockets were effectively used during the Napoleonic Wars and the War of 1812. Congreve published three books on rocketry.\n\nFrom there, the use of military rockets spread throughout the western world. At the Battle of Baltimore in 1814, the rockets fired on Fort McHenry by the rocket vessel HMS \"Erebus\" were the source of the \"rockets' red glare\" described by Francis Scott Key in The Star-Spangled Banner. Rockets were also used in the Battle of Waterloo.\n\nEarly rockets were very inaccurate. Without the use of spinning or any controlling feedback loop, rockets had a strong tendency to veer sharply off of their intended course. The early Mysorean rockets and their successor British Congreve rockets reduced this somewhat by attaching a long stick to the end of a rocket (similar to modern bottle rockets) to make it harder for the rocket to change course. The largest of the Congreve rockets was the 32-pound (14.5 kg) Carcass, which had a 15-foot (4.6 m) stick. Originally, sticks were mounted on the side, but this was later changed to mounting in the center of the rocket, reducing drag and enabling the rocket to be more accurately fired from a segment of pipe.\n\nIn 1815, Alexander Dmitrievich Zasyadko began his work on creating military gunpowder rockets. He constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices. Zasyadko elaborated a tactic for military use of rocket weaponry. In 1820, Zasyadko was appointed head of the Petersburg Armory, Okhtensky Powder Factory, pyrotechnic laboratory and the first Highest Artillery School in Russia. He organized rocket production in a special rocket workshop and created the first rocket sub-unit in the Russian army.\n\nArtillery captain Józef Bem of the Kingdom of Poland started experiments with what was then called in Polish \"raca kongrewska\". These culminated in his 1819 report \"Notes sur les fusees incendiares\" (German edition: \"Erfahrungen über die Congrevischen Brand-Raketen bis zum Jahre 1819 in der Königlischen Polnischen Artillerie gesammelt\", Weimar 1820). The research took place in the Warsaw Arsenal, where captain Józef Kosiński also developed the multiple-rocket launchers adapted from horse artillery gun carriage. The 1st Rocketeer Corps was formed in 1822, it first saw combat during the Polish–Russian War 1830–31.\n\nThe accuracy problem was greatly improved in 1844 when William Hale modified the rocket design so that thrust was slightly vectored, causing the rocket to spin along its axis of travel like a bullet. The Hale rocket removed the need for a rocket stick, travelled further due to reduced air resistance, and was far more accurate.\n\nIn 1865 the British Colonel Edward Mounier Boxer built an improved version of the Congreve rocket placing two rockets in one tube, one behind the other.\n\nAt the beginning of the 20th century, there was a burst of scientific investigation into interplanetary travel, largely driven by the inspiration of fiction by writers such as Jules Verne and H. G. Wells as well as philosophical movements like Russian cosmism. Scientists seized on the rocket as a technology that was able to achieve this in real life, a possibility first recognized in 1861 by William Leitch.\n\nIn 1903, high school mathematics teacher Konstantin Tsiolkovsky (1857–1935), inspired by Verne and Cosmism, published \"Исследование мировых пространств реактивными приборами\" (\"The Exploration of Cosmic Space by Means of Reaction Devices\"), the first serious scientific work on space travel. The Tsiolkovsky rocket equation—the principle that governs rocket propulsion—is named in his honor (although it had been discovered previously). He also advocated the use of liquid hydrogen and oxygen for propellant, calculating their maximum exhaust velocity. His work was essentially unknown outside the Soviet Union, but inside the country it inspired further research, experimentation and the formation of the Society for Studies of Interplanetary Travel in 1924.\nIn 1912, Robert Esnault-Pelterie published a lecture on rocket theory and interplanetary travel. He independently derived Tsiolkovsky's rocket equation, did basic calculations about the energy required to make round trips to the Moon and planets, and he proposed the use of atomic power (i.e. radium) to power a jet drive.\nIn 1912 Robert Goddard, inspired from an early age by H.G. Wells, began a serious analysis of rockets, concluding that conventional solid-fuel rockets needed to be improved in three ways. First, fuel should be burned in a small combustion chamber, instead of building the entire propellant container to withstand the high pressures. Second, rockets could be arranged in stages. Finally, the exhaust speed (and thus the efficiency) could be greatly increased to beyond the speed of sound by using a De Laval nozzle. He patented these concepts in 1914. He also independently developed the mathematics of rocket flight.\n\nDuring World War I Yves Le Prieur, a French naval officer and inventor, later to create a pioneering scuba diving apparatus, developed air-to-air solid-fuel rockets. The aim was to destroy observation captive balloons (called saucisses or Drachens) used by German artillery. These rather crude black powder, steel-tipped incendiary rockets (made by Ruggieri) were first tested from a Voisin aircraft, wing-bolted on a fast Picard Pictet sports car and then used in battle on real aircraft. A typical layout was eight electrically fired Le Prieur rockets fitted on the interpane struts of a Nieuport aircraft. If fired at sufficiently short distance, a spread of Le Prieur rockets proved to be quite deadly. Belgian ace Willy Coppens claimed dozens of Drachen kills during World War I.\n\nIn 1920, Goddard published these ideas and experimental results in \"A Method of Reaching Extreme Altitudes\". The work included remarks about sending a solid-fuel rocket to the Moon, which attracted worldwide attention and was both praised and ridiculed. A \"New York Times\" editorial suggested:\n\nIn 1923, Hermann Oberth (1894–1989) published \"Die Rakete zu den Planetenräumen\" (\"The Rocket into Planetary Space\"), a version of his doctoral thesis, after the University of Munich had rejected it.\n\nIn 1924, Tsiolkovsky also wrote about multi-stage rockets, in 'Cosmic Rocket Trains'.\n\nModern rockets originated when Goddard attached a supersonic (de Laval) nozzle to the combustion chamber of a liquid-fueled rocket engine. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%. On 16 March 1926 Robert Goddard launched the world's first liquid-fueled rocket in Auburn, Massachusetts.\n\nDuring the 1920s, a number of rocket research organizations appeared worldwide. In 1927 the German car manufacturer Opel began to research rocket vehicles together with Mark Valier and the solid-fuel rocket builder Friedrich Wilhelm Sander. In 1928, Fritz von Opel drove a rocket car, the Opel-RAK.1 on the Opel raceway in Rüsselsheim, Germany. In 1928 the Lippisch Ente flew: rocket power launched the manned glider, although it was destroyed on its second flight. In 1929 von Opel started at the Frankfurt-Rebstock airport with the Opel-Sander RAK 1-airplane, which was damaged beyond repair during a hard landing after its first flight.\n\nIn the mid-1920s, German scientists had begun experimenting with rockets that used liquid propellants capable of reaching relatively high altitudes and distances. In 1927 and also in Germany, a team of amateur rocket engineers had formed the \"Verein für Raumschiffahrt\" (Society for Space Travel, or VfR), and in 1931 launched a liquid propellant rocket (using oxygen and gasoline).\n\nRocketry in the Soviet Union also began with amateur societies, foremost was the Group for the Study of Reactive Propulsion (GIRD) headed by Friedrich Zander and Sergei Korolev. From 1931 to 1937 in the Soviet Union, extensive scientific work on rocket engine design occurred at the Gas Dynamics Laboratory (GDL) in Leningrad, which was merged with GIRD in 1933 bringing rocketry fully under government control. The well-funded and -staffed laboratory built over 100 experimental engines under the direction of Valentin Glushko. The work included regenerative cooling, hypergolic propellant ignition, and fuel injector designs that included swirling and bi-propellant mixing injectors. However, Glushko's arrest during Stalinist purges in 1938 curtailed the development.\n\nSimilar work was also done from 1932 onwards by the Austrian professor Eugen Sänger, who migrated from Austria to Germany in 1936. He worked there on rocket-powered spaceplanes such as Silbervogel (sometimes called the \"antipodal\" bomber).\n\nOn November 12, 1932 at a farm in Stockton NJ, the American Interplanetary Society's attempt to static-fire their first rocket (based on German Rocket Society designs) failed in a fire.\n\nIn 1936, a British research programme based at Fort Halstead under the direction of Dr Alwyn Crow started work on a series of unguided solid-fuel rockets that could be used as anti-aircraft weapons. In 1939, a number of test firings were carried out in the British colony of Jamaica, on a purpose built range.\n\nIn the 1930s, the German \"Reichswehr\" (which in 1935 became the \"Wehrmacht\") began to take an interest in rocketry. Artillery restrictions imposed by the 1919 Treaty of Versailles limited Germany's access to long-distance weaponry. Seeing the possibility of using rockets as long-range artillery fire, the Wehrmacht initially funded the VfR team, but because their focus was strictly scientific, created its own research team. At the behest of military leaders, Wernher von Braun, at the time a young aspiring rocket scientist, joined the military (followed by two former VfR members) and developed long-range weapons for use in World War II by Nazi Germany.\n\nAt the start of the war, the British had equipped their warships with unrotated projectile unguided anti-aircraft rockets, and by 1940, the Germans had developed a surface-to-surface multiple rocket launcher, the \"Nebelwerfer\" and the Soviets already had introduced the RS-132 air-to-ground rocket. All of these rockets were developed for a variety of roles, notably the Katyusha rocket.\n\nIn 1943, production of the V-2 rocket began in Germany. It had an operational range of and carried a warhead, with an amatol explosive charge. It normally achieved an operational maximum altitude of around , but could achieve if launched vertically. The vehicle was similar to most modern rockets, with turbopumps, inertial guidance and many other features. Thousands were fired at various Allied nations, mainly Belgium, as well as England and France. While they could not be intercepted, their guidance system design and single conventional warhead meant that they were insufficiently accurate against military targets. A total of 2,754 people in England were killed, and 6,523 were wounded before the launch campaign was ended. There were also 20,000 deaths of slave labour during the construction of V-2s. While it did not significantly affect the course of the war, the V-2 provided a lethal demonstration of the potential for guided rockets as weapons.\n\nIn parallel with the guided missile programme in Nazi Germany, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 \"Natter\") or for powering them (Me 163, etc.). During the war Germany also developed several guided and unguided air-to-air, ground-to-air and ground-to-ground missiles (see list of World War II guided missiles of Germany).\n\nAt the end of World War II, competing Russian, British, and US military and scientific crews raced to capture technology and trained personnel from the German rocket program at Peenemünde. Russia and Britain had some success, but the United States benefited the most. The US captured a large number of German rocket scientists, including von Braun, and brought them to the United States as part of Operation Paperclip. In America, the same rockets that were designed to rain down on Britain were used instead by scientists as research vehicles for developing the new technology further. The V-2 evolved into the American Redstone rocket, used in the early space program.\n\nAfter the war, rockets were used to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further research; notably the Bell X-1, the first manned vehicle to break the sound barrier. This continued in the US under von Braun and the others, who were destined to become part of the US scientific community.\n\nIndependently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev. With the help of German technicians, the V-2 was duplicated and improved as the R-1, R-2, and R-5 missiles. German designs were abandoned in the late 1940s, and the foreign workers were sent home. A new series of engines built by Glushko and based on inventions of Aleksei Mihailovich Isaev formed the basis of the first ICBM, the R-7. The R-7 launched the first satellite, Sputnik 1, and later Yuri Gagarin, the first man into space, and the first lunar and planetary probes. This rocket is still in use today. These prestigious events attracted the attention of top politicians, along with additional funds for further research.\n\nOne problem that had not been solved was atmospheric reentry. It had been shown that an orbital vehicle easily had enough kinetic energy to vaporize itself, and yet it was known that meteorites can make it down to the ground. The mystery was solved in the US in 1951 when H. Julian Allen and A. J. Eggers, Jr. of the National Advisory Committee for Aeronautics (NACA) made the counterintuitive discovery that a blunt shape (high drag) permitted the most effective heat shield. With this type of shape, around 99% of the energy goes into the air rather than the vehicle, and this permitted safe recovery of orbital vehicles.\n\nThe Allen and Eggers discovery, initially treated as a military secret, was eventually published in 1958. Blunt body theory made possible the heat shield designs that were embodied in the Mercury, Gemini, Apollo, and Soyuz space capsules, enabling astronauts and cosmonauts to survive the fiery re-entry into Earth's atmosphere. Some spaceplanes such as the Space Shuttle made use of the same theory. At the time the STS was being conceived, Maxime Faget, the Director of Engineering and Development at the Manned Spacecraft Center, was not satisfied with the purely \"lifting re-entry\" method (as proposed for the cancelled X-20 \"Dyna-Soar\"). He designed a space shuttle which operated as a blunt body by entering the atmosphere at an extremely high angle of attack of 40° with the underside facing the direction of flight, creating a large shock wave that would deflect most of the heat around the vehicle instead of into it. The Space Shuttle essentially uses a combination of a \"ballistic entry\" (Blunt body theory) and then at an altitude of about , the re-entry interface takes place. Here the atmosphere is dense enough for the Space Shuttle to begin its \"lifting re-entry\" by reducing the angle-of-attack, pointing the nose down and using the lift its wings generate to \"start flying\" (gliding) towards the landing site.\n\nRockets became extremely important militarily as modern intercontinental ballistic missiles (ICBMs) when it was realized that nuclear weapons carried on a rocket vehicle were essentially impossible for existing defense systems to stop once launched, and launch vehicles such as the R-7, Atlas, and Titan became delivery platforms for these weapons.\nFueled partly by the Cold War, the 1960s became the decade of rapid development of rocket technology particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15 and X-20 Dyna-Soar aircraft). There was also significant research in other countries, such as France, Britain, Japan, Australia, etc., and a growing use of rockets for Space exploration, with pictures returned from the far side of the Moon and unmanned flights for Mars exploration.\n\nIn America, the manned spaceflight programs, Project Mercury, Project Gemini, and later the Apollo program, culminated in 1969 with the first manned landing on the moon using the Saturn V, causing the New York Times to retract its earlier editorial implying that spaceflight couldn't work:\n\nIn the 1970s, the United States made five more lunar landings before cancelling the Apollo program in 1975. The replacement vehicle, the partially reusable Space Shuttle, was intended to be cheaper, but no large reduction in costs was achieved. Meanwhile, in 1973, the expendable Ariane programme was begun, a launcher that by the year 2000 would capture much of the geosat market.\n\n\n"}
{"id": "22451625", "url": "https://en.wikipedia.org/wiki?curid=22451625", "title": "Imagemakers", "text": "Imagemakers\n\nImagemakers (IM), is a heritage interpretation and design consultancy based in Sticklepath, Devon. Established in 1989, Imagemakers work in the specialist field of strategic planning and design for heritage and tourism projects. Specific services include the production of interpretive strategies and funding packages, and the design of multi-faceted exhibitions, new media installations, outdoor displays, public art and websites.\n\n\n"}
{"id": "38448356", "url": "https://en.wikipedia.org/wiki?curid=38448356", "title": "Index of physics articles (K)", "text": "Index of physics articles (K)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "1759251", "url": "https://en.wikipedia.org/wiki?curid=1759251", "title": "Inundative application", "text": "Inundative application\n\nInundative application of a biological control or natural enemy of a pest refers to the release of overwhelming numbers of a mass-produced biological control agent in the expectation of either achieving a rapid reduction of a pest population, or to improve the long term survival of the biocontrol agent.\n"}
{"id": "977546", "url": "https://en.wikipedia.org/wiki?curid=977546", "title": "Justus Ludwig Adolf Roth", "text": "Justus Ludwig Adolf Roth\n\nJustus Ludwig Adolf Roth (September 15, 1818, Hamburg – April 1, 1892) was a German geologist and mineralogist.\n\nIn 1844 he obtained his doctorate from the University of Jena and spent the next few years working as a pharmacist in Hamburg. In 1848 he relocated to Berlin, where he came under the influence of Gustav Rose and Heinrich Ernst Beyrich. In 1867 he became an associate professor of mineralogy at the University of Berlin.\n\nHe may be regarded as one of the founders of petrographical science. In his published papers he dealt with metamorphism and crystalline schists, discussed the origin of serpentine, and wrote on the rocks of Mount Vesuvius and Ponza Island.\n\nHis separate works included:\n"}
{"id": "53842252", "url": "https://en.wikipedia.org/wiki?curid=53842252", "title": "Kenneth W. Gentle", "text": "Kenneth W. Gentle\n\nKenneth W. Gentle from the University of Texas, Austin, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Plasma Physics in 1996, for \"his pioneering experiments on wave-particle and wave-wave interactions which have illuminated the fundamental nonlinear phenomena in collisionless plasmas, and for his leadership in the development of experiments which directly measure the fundamental processes of transport in Tokamak plasmas.\"\n"}
{"id": "164610", "url": "https://en.wikipedia.org/wiki?curid=164610", "title": "Latent heat", "text": "Latent heat\n\nLatent heat is thermal energy released or absorbed, by a body or a thermodynamic system, during a constant-temperature process — usually a first-order phase transition.\n\nLatent heat can be understood as heat energy in hidden form which is supplied or extracted to change the state of a substance without changing its temperature. Examples are latent heat of fusion and latent heat of vaporization involved in phase changes, i.e. a substance condensing or vaporizing at a specified temperature and pressure. \n\nThe term was introduced around 1762 by British chemist Joseph Black. It is derived from the Latin \"latere\" (\"to lie hidden\"). Black used the term in the context of calorimetry where a heat transfer caused a volume change in a body while its temperature was constant.\n\nIn contrast to latent heat, sensible heat is a heat transfer that results in a temperature change in a body.\n\nThe terms ″sensible heat″ and ″latent heat″ refer to types of heat transfer between a body and its surroundings; they depend on the properties of the body. ″Sensible heat″ is ″sensed″ or felt in a process as a change in the body's temperature. ″Latent heat″ is heat transferred in a process without change of the body's temperature, for example, in a phase change ( solid / liquid / gas ). \n\nBoth sensible and latent heats are observed in many processes of transfer of energy in nature. Latent heat is associated with the change of phase of atmospheric or ocean water, vaporization, condensation, freezing or melting, whereas sensible heat is energy transferred that is evident in change of the temperature of the atmosphere or ocean, or ice, without those phase changes, though it is associated with changes of pressure and volume.\n\nThe original usage of the term, as introduced by Black, was applied to systems that were intentionally held at constant temperature. Such usage referred to \"latent heat of expansion\" and several other related latent heats. These latent heats are defined independently of the conceptual framework of thermodynamics.\n\nWhen a body is heated at constant temperature by thermal radiation in a microwave field for example, it may expand by an amount described by its \"latent heat with respect to volume\" or \"latent heat of expansion\", or increase its pressure by an amount described by its \"latent heat with respect to pressure\".\nLatent heat is energy released or absorbed, by a body or a thermodynamic system, during a constant-temperature process.\nTwo common forms of latent heat are latent heat of fusion (melting) and latent heat of vaporization (boiling). These names describe the direction of energy flow when changing from one phase to the next: from solid to liquid, and liquid to gas.\n\nIn both cases the change is endothermic, meaning that the system absorbs energy.\nFor example, when water evaporates, energy is required for the water molecules to overcome the forces of attraction between them, the transition from water to vapor requires an input of energy.\n\nIf the vapor then condenses to a liquid on a surface, then the vapor's latent energy absorbed during evaporation is released as the liquid's sensible heat onto the surface.\n\nThe large value of the enthalpy of condensation of water vapor is the reason that steam is a far more effective heating medium than boiling water, and is more hazardous.\n\nIn meteorology, latent heat flux is the flux of heat from the Earth's surface to the atmosphere that is associated with evaporation or transpiration of water at the surface and subsequent condensation of water vapor in the troposphere. It is an important component of Earth's surface energy budget. Latent heat flux has been commonly measured with the Bowen ratio technique, or more recently since the mid-1900s by the Jonathan Beaver method.\n\nThe English word \"latent\" comes from Latin \"latēns\", meaning \"lying hidden\". The term \"latent heat\" was introduced into calorimetry around 1750 when Joseph Black, commissioned by producers of Scotch whisky in search of ideal quantities of fuel and water for their distilling process, to studying system changes, such as of volume and pressure, when the thermodynamic system was held at constant temperature in a thermal bath. James Prescott Joule characterised latent energy as the energy of interaction in a given configuration of particles, i.e. a form of potential energy, and the sensible heat as an energy that was indicated by the thermometer, relating the latter to thermal energy.\n\nA \"specific\" latent heat (\"L\") expresses the amount of energy in the form of heat (\"Q\") required to completely effect a phase change of a unit of mass (\"m\"), usually , of a substance as an intensive property:\nIntensive properties are material characteristics and are not dependent on the size or extent of the sample. Commonly quoted and tabulated in the literature are the specific latent heat of fusion and the specific latent heat of vaporization for many substances.\n\nFrom this definition, the latent heat for a given mass of a substance is calculated by\nwhere:\n\nThe following table shows the specific latent heats and change of phase temperatures (at standard pressure) of some common fluids and gases.\n\nThe specific latent heat of condensation of water in the temperature range from −25 °C to 40 °C is approximated by the following empirical cubic function:\nwhere the temperature formula_4 is taken to be the numerical value in °C.\n\nFor sublimation and deposition from and into ice, the specific latent heat is almost constant in the temperature range from −40 °C to 0 °C and can be approximated by the following empirical quadratic function:\n\nAs the temperature (or pressure) rises to the critical point the LHOV falls to zero :\n\n"}
{"id": "3899211", "url": "https://en.wikipedia.org/wiki?curid=3899211", "title": "Lewis and Clark Lake", "text": "Lewis and Clark Lake\n\nLewis and Clark Lake is a 31,400 acre (130 km²) artificial lake located on the border of the U.S. States of Nebraska and South Dakota on the Missouri River. The lake is approximately in length with over of shoreline and a maximum water depth of . The lake is impounded by Gavins Point Dam and is managed by the U.S. Army Corps of Engineers. The lake is located within Cedar and Knox Counties in Nebraska and Bon Homme and Yankton Counties in South Dakota. The lake is located approximately west or upstream of Yankton, South Dakota. \n\nThe Missouri River Valley Area is abound with history involving several early Native American Tribes, Pioneers, and other settlers to the area due to ease of river transportation and abundant resources. Lewis and Clark Lake is named after explorers Meriwether Lewis and William Clark of the Lewis and Clark Expedition. The lake is located along the Lewis and Clark National Historic Trail.\n\nThe archaeological record in the area dates back to the Archaic Period, sometime around 3,000 to 5,000 B.C. The Archaic Period people lived along small tributary streams that flow into the Missouri Valley. Later, Woodland Period people (500 B.C. – 1,000 A.D.) lived in the area. More recent inhabitants include the Ponca, Yankton Sioux and Omaha tribes in the late 18th and 19th centuries. The Minnesota Santee Sioux arrived on the river shore in the mid-1800's and remain in the area. In 1804, while traveling up the Missouri River on their epic journey to the Pacific Ocean, Lewis and Clark participated in a Grand council with the Yankton Sioux at a site below Calumet Bluff. This significant meeting was the first meeting with a Sioux tribe on their journey upstream. The lake area is a part of the Lewis and Clark National Historic Trail. In 1874, the Bon Homme Colony of Hutterites, a branch of the Mennonite movement exiled from Austria, settled on what is now the north shore of Lewis and Clark Lake. They are the first Hutterite Colony in South Dakota and the United States. The colony maintains a traditional communal way of life. The lake was filled in 1957 with the completion of construction of Gavins Point Dam across the river valley. \n\nLewis and Clark Lake is a very popular regional tourist destination in the upper Midwest for camping, water sports, hiking, bird watching, hunting, fishing, swimming, and biking. Average annual public visitation exceeds one-million visitors per year to the lake area. There are 27 public recreation areas around the lake containing boat ramps, marinas, campgrounds, and day-use areas. The upper stretches of the lake are renowned for their superior waterfowl viewing and hunting opportunities along the Missouri River flyway.\n\nThe U.S. Army Corps of Engineers manages several campgrounds and parks below Gavins Point Dam. The Lewis & Clark Recreation Area, a South Dakota state park is a popular recreational destination, several boat ramps, a full-service marina and resort, and over 400 campsites are available for public use. Lewis & Clark State Recreation Area is a popular state recreation area in Nebraska along the lake's southern shore with several campgrounds, rental cabins, full-service marina and several trails. Springfield Recreation Area near the upper part of the lake is also popular with visitors for camping and lake access. There are two golf courses along the lakeshore, Crofton-Lakeview Golf Course, near Crofton and Springfield Golf Course near Springfield. Located downstream of the lake is the 59-mile reach of the Missouri National Recreational River (MNRR) which stretches eastward from the dam to Ponca State Park, upstream of the lake is the 39-mile reach of the MNRR which stretches westward to Fort Randall Dam. \n\nThe Lewis and Clark Visitor Center is located just south of Gavins Point Dam atop Calumet Bluff with stunning views of Lewis and Clark Lake, Lake Yankton, and the Missouri River below the dam. The visitor center is open daily from Memorial Day weekend through Labor Day weekend and open weekdays during other times of the year. The visitor center interprets the history of the Missouri River Basin, including Native Americans, pioneers, the Lewis and Clark Expedition (which traveled through the area); along with local wildlife and the history of the Corps of Engineers in the area. A theater shows educational videos on the Lewis and Clark Expedition, construction of Gavins Point Dam, and the natural history of the Missouri River Region. A bookstore offers educational books, videos, and other merchandise for sale. The visitor center is known for exceptional viewing of the majestic American Bald Eagle, which frequents the Missouri River below the dam, especially in winter months. The visitors center is operated and staffed by U.S. Army Corps of Engineers Park Rangers, who also give guided tours of Gavins Point Dam and the power plant.\n\nThe natural resources and public lands on and around the lake are cooperatively managed by the U.S. Army Corps of Engineers, Nebraska Game and Parks Commission, and the South Dakota Department of Game, Fish, and Parks. Common game species around the lake include White-tailed deer, Wild Turkey, many species of waterfowl, Pheasant, Cottontail rabbit, Mourning Dove, and squirrel. The American Bald Eagle is commonly seen around the dam and lake area, especially in the winter months. Each January the Lewis and Clark Visitor Center hosts \"Bald Eagle Days\" a live-bird program that is popular with visitors.\n\nSpecies of fish present include walleye, northern pike, sauger, sunfish, yellow perch, common carp, black bullhead, channel catfish, and smallmouth bass.. Fishing below Gavins Point Dam is very popular, especially for the annual paddlefish snagging season in October and bowfishing in June. The \"Fishing Wall\" immediately below the dam's spillway is popular for fishing year-round as the dam keeps the river free of ice in the winter months.\n\nThe U.S. Army Corps of Engineers and the U.S. Fish and Wildlife Service monitor and manage threatened and endangered species on the lake and river. Species of concern include the Pallid sturgeon, least tern, and piping plover. A branch of the Corps known as the Missouri River Recovery Program monitors these species and helps to restore native habitat that was lost as a result of dam construction and channelization of the Missouri River. The Gavins Point National Fish Hatchery is located just downstream of the lake.\n\nThere are several issues impacting recreation, wildlife, and other issues. In 2014 zebra mussels, an aquatic invasive mussel were discovered in the lake and have infested the reservoir and the Missouri River downstream of Gavins Point Dam.\n\nLewis and Clark Lake is significantly impacted by sedimentation and siltation issues, diminishing the overall water surface area, water storage capacity, and recreational opportunities. Sediment carried by the Missouri River and Niobrara River is slowed and trapped within the reservoir due to the dam impounding and thus slowing the natural river flow. Studies show approximately 5.1 tons of sediment are deposited in the lake each year, which contributes to the lake's increasing size of delta area on the western portions of the lake. Approximately 60% of the sediment comes from the Nebraska Sandhills via the Niobrara River. As of 2016, approximately 30% of the lake's overall surface area has diminished due to sedimentation deposits, and some figures project by 2045 approximately 50% of the lake will be diminished due to sedimentation deposits. Presently, there is no plan or solution to remove or slow the progression of the siltation within the lake.\n\n\n\n"}
{"id": "207846", "url": "https://en.wikipedia.org/wiki?curid=207846", "title": "List of Swedish scientists", "text": "List of Swedish scientists\n\nThis is a list of Swedish scientists.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "26569281", "url": "https://en.wikipedia.org/wiki?curid=26569281", "title": "List of medical schools in Europe", "text": "List of medical schools in Europe\n\nThe following is a list of medical schools (or universities with a medical school) in Europe.\n\nPublic\nPrivate\n\n\n\n\n\n\n\n\n\n\n\n\nMany French universities have a medical school, called \" de Médecine\", where stands for \"Unité de Formation et de Recherche\", or \"Unit for training and research\" in English.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity of Perugia, in Perugia and in Terni\n\n\n\n\nPublic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "28170916", "url": "https://en.wikipedia.org/wiki?curid=28170916", "title": "List of medical schools in Japan", "text": "List of medical schools in Japan\n\nThis is a list of medical schools located in Japan.\n\n\n"}
{"id": "38490037", "url": "https://en.wikipedia.org/wiki?curid=38490037", "title": "List of presidents of the Geologists' Association", "text": "List of presidents of the Geologists' Association\n\nThis is a list of Presidents of the Geologists' Association.\n\n\n"}
{"id": "21536356", "url": "https://en.wikipedia.org/wiki?curid=21536356", "title": "List of production functions", "text": "List of production functions\n\nThis is a list of production functions that have been used in the economics literature. Production functions are a key part of modelling national output and national income.\n\nThe production functions listed below, and their properties are shown for the case of two factors of production, capital (K), and labor (L), mostly for heuristic purposes. These functions and their properties are easily generalizable to include additional factors of production (like land, natural resources, entrepreneurship, etc.)\n\nThere are three common ways to incorporate technology (or the efficiency with which factors of production are used) into a production function (here \"A\" is a scale factor, \"F\" is a production function, and \"Y\" is the amount of physical output produced):\n\nThe elasticity of substitution between factors of production is a measure of how easily one factor can be substituted for another. With two factors of production, say, \"K\" and \"L\", it is a measure of the curvature of a production isoquant. The mathematical definition is:\n\nwhere \"slope\" denotes the slope of the isoquant, given by\n\nReturns to scale can be\n\n\n\n\n"}
{"id": "40280800", "url": "https://en.wikipedia.org/wiki?curid=40280800", "title": "List of things named after Richard Feynman", "text": "List of things named after Richard Feynman\n\nThis refers to a list of things named after physicist Richard Feynman\n\n"}
{"id": "1586871", "url": "https://en.wikipedia.org/wiki?curid=1586871", "title": "Lochium Funis", "text": "Lochium Funis\n\nLochium Funis (Latin for \"the log and line\") was a constellation created by Johann Bode in 1801 next to the constellation Pyxis, an earlier invention of Nicolas Louis de Lacaille. It represented the log and line used by seamen for measuring a ship's speed through the water. It was never used by other astronomers.\n\n"}
{"id": "170097", "url": "https://en.wikipedia.org/wiki?curid=170097", "title": "Mean free path", "text": "Mean free path\n\nIn physics, the mean free path is the average distance travelled by a moving particle (such as an atom, a molecule, a photon) between successive impacts (collisions), which modify its direction or energy or other particle properties.\n\nThe following table lists some typical values for air at different pressures at room temperature.\nIn gamma-ray radiography the \"mean free path\" of a pencil beam of mono-energetic photons is the average distance a photon travels between collisions with atoms of the target material. It depends on the material and the energy of the photons:\n\nwhere \"μ\" is the linear attenuation coefficient, \"μ/ρ\" is the mass attenuation coefficient and \"ρ\" is the density of the material. The mass attenuation coefficient can be looked up or calculated for any material and energy combination using the National Institute of Standards and Technology (NIST) databases\n\nIn X-ray radiography the calculation of the \"mean free path\" is more complicated, because photons are not mono-energetic, but have some distribution of energies called a spectrum. As photons move through the target material, they are attenuated with probabilities depending on their energy, as a result their distribution changes in process called spectrum hardening. Because of spectrum hardening, the \"mean free path\" of the X-ray spectrum changes with distance.\n\nSometimes one measures the thickness of a material in the \"number of mean free paths\". Material with the thickness of one \"mean free path\" will attenuate 37% (1/\"e\") of photons. This concept is closely related to half-value layer (HVL): a material with a thickness of one HVL will attenuate 50% of photons. A standard x-ray image is a transmission image, an image with negative logarithm of its intensities is sometimes called a \"number of mean free paths\" image.\n\nElectrical mobility, a value directly related to electrical conductivity, is:\n\nwhere \"e\" is the charge, formula_3 is the mean free time, \"m\" is the effective mass, \"d\" is the mean free path, and \"v\" is the Fermi velocity of the charge carrier. The Fermi velocity can easily be derived from the Fermi energy via the non-relativistic kinetic energy equation. In thin films, however, the film thickness can be smaller than the predicted mean free path, making surface scattering much more noticeable, effectively increasing the resistivity.\n\nIf one takes a suspension of non-light-absorbing particles of diameter \"d\" with a volume fraction \"Φ\", the mean free path of the photons is:\n\nwhere \"Q\" is the scattering efficiency factor. \"Q\" can be evaluated numerically for spherical particles using Mie theory.\n\nIn an otherwise empty cavity, the mean free path of a single particle bouncing off the walls is:\n\nwhere \"V\" is volume of the cavity, \"S\" is total inside surface area of cavity and \"F\" a constant related to the shape of the cavity. For most simple cavity shapes \"F\" is approximately 4.\nThis relation is used in the derivation of the Sabine equation in acoustics, using a geometrical approximation of sound propagation.\n\nImagine a beam of particles being shot through a target, and consider an infinitesimally thin slab of the target (see the figure). The atoms (or particles) that might stop a beam particle are shown in red. The magnitude of the mean free path depends on the characteristics of the system. An expression for the MFP is\n\nwhere is the mean free path, is the number of target particles per unit volume, and is the effective cross-sectional area for collision.\n\nThe area of the slab is , and its volume is . The typical number of stopping atoms in the slab is the concentration times the volume, i.e., . The probability that a beam particle will be stopped in that slab is the net area of the stopping atoms divided by the total area of the slab:\n\nwhere is the area (or, more formally, the \"scattering cross-section\") of one atom.\n\nThe drop in beam intensity equals the incoming beam intensity multiplied by the probability of the particle being stopped within the slab:\n\nThis is an ordinary differential equation:\n\nwhose solution is known as Beer-Lambert law and has the form formula_10, where is the distance traveled by the beam through the target, and is the beam intensity before it entered the target; is called the mean free path because it equals the mean distance traveled by a beam particle before being stopped. To see this, note that the probability that a particle is absorbed between and is given by\n\nThus the expectation value (or average, or simply mean) of is\n\nThe fraction of particles that are not stopped (attenuated) by the slab is called transmission formula_13, where is equal to the thickness of the slab .\n\nIn kinetic theory the \"mean free path\" of a particle, such as a molecule, is the average distance the particle travels between collisions with other moving particles. The formula formula_14 still holds for a particle with a high velocity relative to the velocities of an ensemble of identical particles with random locations. If, on the other hand, the velocities of the identical particles have a Maxwell distribution, the following relationship applies:\n\nand using formula_16 (ideal gas law) and formula_17 (effective cross-sectional area for spherical particles with radius formula_18), it may be shown that the mean free path is\n\nwhere \"k\" is the Boltzmann constant.\n\nIn practice, the diameter of gas molecules is not well defined. In fact, the kinetic diameter of a molecule is defined in terms of the mean free path. Typically, gas molecules do not behave like hard spheres, but rather attract each other at larger distances and repel each other at shorter distances, as can be described with a Lennard-Jones potential. One way to deal with such \"soft\" molecules is to use the Lennard-Jones σ parameter as the diameter. Another way is to assume a hard-sphere gas that has the same viscosity as the actual gas being considered. This leads to a mean free path\n\nwhere \"m\" is the molecular mass, and \"μ\" is the viscosity.\n\nThese different definitions of the molecular diameter can lead to slightly different values of the mean free path.\n\nIn particle physics the concept of the mean free path is not commonly used, being replaced by the similar concept of attenuation length. In particular, for high-energy photons, which mostly interact by electron–positron pair production, the radiation length is used much like the mean free path in radiography.\n\nIndependent-particle models in nuclear physics require the undisturbed orbiting of nucleons within the nucleus before they interact with other nucleons.\n\n\n"}
{"id": "3898021", "url": "https://en.wikipedia.org/wiki?curid=3898021", "title": "Nyctography", "text": "Nyctography\n\nNyctography is a form of substitution cipher writing created by Lewis Carroll (Charles Lutwidge Dodgson) in 1891.\n\nNyctography is written with a nyctograph (also invented by Carroll) and uses a system of dots and strokes all based on a dot placed in the upper left corner. Using the Nyctograph, one could quickly jot down ideas or notes without the aid of light.\n\nCarroll invented the Nyctograph and Nyctography because he was often awakened during the night with thoughts that needed to be written down immediately, and didn't want to go through the lengthy process of lighting a lamp just to have to extinguish it shortly thereafter.\n\nThe device consisted of a gridded card with sixteen square holes, each a quarter inch wide, and system of symbols representing an alphabet of Carroll's design, which could then be transcribed the following day. \n\nHe first named it \"typhlograph\", but at the suggestion of one of his brother-students, this was subsequently changed into \"Nyctograph\".\n\nInitially, Carroll used an oblong of card with an oblong cut out of the centre to guide his writing in the dark. This did not appear to be satisfactory as the results were illegible. The new and final version of the nyctograph is recorded in his journal of September 24, 1891, and is the subject of a letter to \"The Lady\" magazine of October 29, 1891: \nFrom the description it appears that Carroll’s nyctograph was a single row of 16 boxes cut from a piece of card. Carroll would enter one of his symbols in each box, then move the card down to the next line (which, in the darkness, probably, he would have to estimate) and then repeat the process.\n\nEach character had a large dot or circle in the upper-left corner. Beside the 26 letters of the alphabet, there were five additional characters for 'and', 'the', the corners of the letter 'f' to indicate that the following characters were digits ('figures'), the corners of the letter 'l' to indicate that they were letters, and the corners of the letter 'd' to indicate that the following six characters were a date in DDMMYY format. There was no capitalization, punctuation or digits \"per se\", though modern font designers have created them (e.g. capitals may be double-scored, punctuation marks may have the large dot at the bottom right corner, digits at the bottom left). \n\nAs in braille, letters were assigned to represent digits. The values were taken from his Memoria Technica, which assigned two consonants to each digit, with vowels unassigned, so that any number could be read off as a word. For nyctography, one of the consonants was used for each digit. Most are the initials of the numerals, as follows. (In brackets are the other values of the Memoria Technica, which apart from leftover \"j\" for 3 have their own motivations.)\n\n\n\n"}
{"id": "25654089", "url": "https://en.wikipedia.org/wiki?curid=25654089", "title": "Optical force", "text": "Optical force\n\nThe optical force is a phenomenon whereby beams of light can attract and repel each other. The force acts along an axis which is perpendicular to the light beams. Because of this, parallel beams can be induced to converge or diverge. The optical force works on a microscopic scale, and cannot currently be detected at larger scales. It was discovered by a team of Yale researchers led by electrical engineer Hong Tang.\n\n"}
{"id": "6473930", "url": "https://en.wikipedia.org/wiki?curid=6473930", "title": "Outline of military science and technology", "text": "Outline of military science and technology\n\nThe following outline is provided as an overview of and topical guide to military science:\n\nMilitary science – study of the technique, psychology, practice and other phenomena which constitute war and armed conflict. It strives to be a scientific system that if properly employed, will greatly enhance the practitioner's ability to prevail in an armed conflict with any adversary. To this end, it is unconcerned whether that adversary is an opposing military force, guerrillas or other irregulars, or any adversary who knows of or utilizes military science in response.\n\n\nWeapon\n\n\nMilitary history\n\n\n\n\n\n\n\n\nFollowing are examples from throughout history of prominently influential military strategists:\n\n\n\n\n"}
{"id": "26085456", "url": "https://en.wikipedia.org/wiki?curid=26085456", "title": "Pavonazzo marble", "text": "Pavonazzo marble\n\nPavonazzetto marble also known as Docimaean marble or Synnadic marble, is a white marble originally from Docimium, or modern Iscehisar, Turkey. \n\nThe name derives from the Italian word for peacock (pavone). \"In natural stone trade, Pavonazzo is often simply called a Marble.\" It is one of the many varieties of Carrara marble, distinguished by black/gray-veined white marble. Also referred to as \"pavonazzetto\", and distinguished as:\n\nThe marble has been used in Rome since the Augustan age, when large-scale quarrying began at Docimium, and columns of it were used in the House of Augustus, as well as in the Temple of Mars Ultor, which also had pavonazzo floor tiles in the cella. Pavonazzetto statues of kneeling Phrygian barbarians existed in the Basilica Aemilia and Horti Sallustiani. Giant statue groups carved from Docimaean marble were discovered at Tiberius's Villa in Sperlonga. Pavonazzetto was not widely or extensively used before the Roman period; there is no evidence of it in circulation before the last two decades B.C.\nLater it was used in Rome in Trajan's Markets and for the Memoria Petri, the tomb of Saint Peter and internationally in the influential Baroque Revival-style historic buildings the Church of St. Ignatius Loyola, in New York City, and Belfast City Hall in Belfast, Northern Ireland.\n\n"}
{"id": "52748452", "url": "https://en.wikipedia.org/wiki?curid=52748452", "title": "Professor Aristóteles Orsini Planetarium", "text": "Professor Aristóteles Orsini Planetarium\n\nThe Professor Aristóteles Orsini Planetarium (), also known as the Ibirapuera Planetarium (), is a planetarium in Ibirapuera Park, São Paulo. It opened in January 1957, and was the first planetarium in Brazil and Latin America. It is one of three planetaria in São Paulo, with the others being Carmo Planetarium and the Johannes Kepler Planetarium at Sabina Escola Parque do Conhecimento.\n\nThe planetarium was proposed in 1951 by Aristóteles Orsini, then director of the Associação dos Amadores de Astronomia de São Paulo, as part of the 400th anniversary celebrations of the foundation of São Paulo. A Zeiss projector was then purchased, at a cost of Cr $ 3,000,000 (including transportation; 1952 values), arriving at the Port of Santos by 29 May 1952. Also in 1952, Orsini was commissioned to study the installation of the projector in a planetarium in the city, for which he visited Urania Sternwarte, Switzerland, and interned at the planetarium at Palais de la Découverte, Paris; he then recommended the creation of a science Museum in the same style as that in Paris, with the planetarium attached, which was never realised.\n\nThe architects of the final building were Antonio Carlos, Eduardo Corona and Roberto G. Tibau Pitombo, and was constructed by Construtora Politécnica Ltda.\n\nBy the time of the 400th anniversary, the planetarium was still being constructed, and the projector was being held by customs in Santos; it wasn't released until June 1955, when it was subsequently placed into storage in Viveiro Manequinho Lopes in Ibirapuera Park. By the end of 1956 the construction of the building was completed, three years behind the initial schedule. This delay caused the cost of the originally-planned metal dome to soar from Cr$700,000 to Cr$5,000,000 due to exchange rate fluctuations, far in excess of the budget for it. As a result, José Carlos Figueiredo Ferraz designed and installed a concrete projection dome instead, which was the first time such material had been used for this type of structure.\n\nThe planetarium opened on 26 January 1957, at which time it was the only planetarium in Brazil; it was opened by Orsini, who subsequently led the initial operation of the planetarium, at the invitation of Maury de Freitas Julião and Wladimir de Toledo Pizza Sobrinho.\n\nThe historical building is recorded by the Conselho Municipal de Preservação do Patrimônio Histórico, Cultural e Ambiental da Cidade de São Paulo (Conpresp; 1997) and Conselho de Defesa do Patrimônio Histórico, Arqueológico, Artístico e Turístico (Condephaat; 1992).\n\nIt closed in 1995–1997. In 1999 the building was closed by structural problems resulting in a roof leak, as well as a termite infestation. It was restored at a cost of R$9.6 million, re-opening on 22 September 2006. During the restoration, led by Carl Zeiss AG, modern projectors were installed.\n\nThe planetarium closed in May 2013 for renovation after a lightning strike damaged the projector. It reopened on 24 January 2016, a day before the city's anniversary.\n\nThe planetarium uses a Starmaster projector, manufactured by Carl Zeiss AG, which was installed in 2006 to replace the original Universarium III purchased in 1952. 44 additional projectors are also used. The projection dome has a capacity of 300 people, with a circular arrangement of reclining seats and projection at the top of the dome. The dome spans , and is on one floor with a mezzanine. The metal and concrete half-dome is surrounded by a yellow disk, and the interior is made of wood.\n\nPlanetarium shows last 30 minutes. The planetarium is open to pre-arranged school visits on Wednesdays, Thursdays and Fridays. It opens to the general public on Saturdays, Sundays, some holidays, and some weekdays in summer months via a ticket system, with 256 tickets available to the general public and 52 tickets reserved for disabled, pregnant, elderly or obese people and companions, with seven wheelchair spaces. In the 1990s it received around 350,000 visitors per year.\n\nThe planetarium is located around 5 minutes walk from the pedestrian Gate 10 of Ibirapuera Park, and is near to car parking at Gate 3.\n\nAdjacent to the planetarium is the Municipal School of Astronomy (). It opened in January 1961 to teach astronomy courses, due to demand from the planetarium, which opened four years earlier. It has three classrooms, as well as a 100-seat auditorium. Courses range from introductory astronomy; solar system astronomy; stellar evolution; and cosmology. There is also an exhibition room, a reading room, and a telescope-building room.\n"}
{"id": "4635563", "url": "https://en.wikipedia.org/wiki?curid=4635563", "title": "Quinarian system", "text": "Quinarian system\n\nThe quinarian system was a method of zoological classification which was popular in the mid 19th century, especially among British naturalists. It was largely developed by the entomologist William Sharp Macleay in 1819. The system was further promoted in the works of Nicholas Aylward Vigors, William John Swainson and Johann Jakob Kaup. Swainson's work on ornithology gave wide publicity to the idea. The system had opponents even before the publication of Charles Darwin's \"On the Origin of Species\" (1859), which paved the way for evolutionary trees.\n\nQuinarianism gets its name from the emphasis on the number five: it proposed that all taxa are divisible into five subgroups, and if fewer than five subgroups were known, quinarians believed that a missing subgroup remained to be found.\n\nPresumably this arose as a chance observation of some accidental analogies between different groups, but it was erected into a guiding principle by the quinarians. It became increasingly elaborate, proposing that each group of five classes could be arranged in a circle, with those closer together having greater affinities. Typically they were depicted with relatively advanced groups at the top, and supposedly degenerate forms towards the bottom. Each circle could touch or overlap with adjacent circles; the equivalent overlapping of actual groups in nature was called osculation.\n\nAnother aspect of the system was the identification of \"analogies\" across groups:\nQuinarianism was not widely popular outside the United Kingdom (some followers like William Hincks persisted in Canada); it became unfashionable by the 1840s, during which time more complex \"maps\" were made by Hugh Edwin Strickland and Alfred Russel Wallace. Strickland and others specifically rejected the use of relations of \"analogy\" in constructing natural classifications. These systems were eventually discarded in favour of principles of genuinely natural classification, namely based on evolutionary relationship.\n"}
{"id": "666901", "url": "https://en.wikipedia.org/wiki?curid=666901", "title": "Rind et al. controversy", "text": "Rind et al. controversy\n\nThe Rind \"et al\". controversy was a debate in the scientific literature, public media, and government legislatures in the United States regarding a 1998 peer reviewed meta-analysis of the self-reported harm caused by child sexual abuse (CSA). The debate resulted in the unprecedented condemnation of the paper by both Houses of the United States Congress. The social science research community was concerned that the condemnation by government legislatures might have a chilling effect on the future publication of controversial research results.\n\nThe study's lead author is psychologist Bruce Rind, and it expanded on a 1997 meta-analysis for which Rind is also lead author. The authors stated their goal was to determine whether CSA caused pervasive, significant psychological harm for both males and females, controversially concluding that the harm caused by child sexual abuse was not necessarily intense or pervasive, that the prevailing construct of CSA was not scientifically valid, as it failed empirical verification, and that the psychological damage caused by the abusive encounters depends on other factors such as the degree of coercion or force involved. The authors concluded that even though CSA may not result in lifelong, significant harm to all victims, this does not mean it is not morally wrong and indicated that their findings did not imply current moral and legal prohibitions against CSA should be changed.\n\nThe Rind \"et al\". study has been criticized by various scientists and researchers, notably Stephanie Dallam (2001; 2002), on the grounds that its methodology and conclusions are poorly designed and statistically flawed. Its definition of \"harm\", for example, has been subject to debate because it only examined long-term psychological effects, and harm can result in a number of ways, including short-term or medical harm (for example, sexually transmitted infections or injuries), a likelihood of revictimization, and the amount of time the victim spent attending therapy for the abuse. Seven years after the publication of the Rind \"et al\". study, Heather Marie Ulrich, with two colleagues, replicated it in \"The Scientific Review of Mental Health Practice\" and confirmed many of its main findings, but did not necessarily endorse all of its authors' conclusions.\n\nThe Rind paper has been quoted by people and organizations advocating age of consent reform, pedophile or pederasty groups in support of their efforts to change attitudes towards pedophilia and to decriminalize sexual activity between adults and minors (children or adolescents), and by defense attorneys who have used the study to minimize harm in child sexual abuse cases.\n\nIn 1997, psychology professor Bruce Rind from Temple University and doctoral student Philip Tromovitch from the University of Pennsylvania published a literature review in \"The Journal of Sex Research\" of seven studies regarding adjustment problems of victims of child sexual abuse (CSA). To avoid the sampling bias that, they argued, existed in most studies of CSA (which drew from samples mostly in the mental health or legal systems and thus were, as a sample, unlike the population as a whole), the 1997 study combined data from studies using only national samples of individuals expected to be more representative of the population of child sexual abuse victims. This study examined 10 independent samples designed to be nationally representative, based on data from more than 8,500 participants. Four of the studies came from the United States, and one each came from Great Britain, Canada, and Spain.\n\nBased on the results, they concluded that the general consensus associating CSA with intense, pervasive harm and long-term maladjustment was incorrect. The following year, Rind, Tromovitch and Robert Bauserman (then a professor at the University of Michigan) published a meta-analysis in the \"Psychological Bulletin\" of 59 studies (36 published studies, 21 unpublished doctoral dissertations, and 2 unpublished master's theses) with an aggregate sample size of 35,703 college students (13,704 men and 21,999 women). In most of the 59 studies, CSA was defined by the authors based on legal and moral criteria.\n\nIntegrating the sometimes disparate and conflicting definitions, CSA was defined as \"a sexual interaction involving either physical contact or no contact (e.g., exhibitionism) between either a child or adolescent and someone significantly older, or between two peers who are children or adolescents when coercion is used.\" \"Child\" was sometimes defined, not biologically, but as underaged or as a minor under the legal age of consent.\n\nAll these studies were included in the meta-analysis because many CSA researchers, as well as lay persons, view all types of socio-legally defined CSA as morally and/or psychologically harmful. When this research, the U.S. Congress, and the APA refer to CSA and \"children\" in the context of sexual relations with adults, they are not referring simply to biological (prepubescent) children but to adolescents under the age of consent as well, which varies between 16 and 18 years old in the U.S.\n\nThe results of the meta-analysis indicated that college students who had experienced CSA were slightly less well-adjusted compared to other students who had not experienced CSA, but that family environment was a significant confound that may be responsible for the association between CSA and harm. Intense, pervasive harm and long-term maladjustment were due to confounding variables in most studies rather than to the sexual abuse itself (though exceptions were noted for abuse accompanied by force or incest). Both studies addressed four \"assumed properties\" of CSA, identified by the authors: gender equivalence (both genders affected equally), causality (CSA causes harm), pervasiveness (most victims of CSA are harmed) and intensity (the harm is normally significant and long-term), concluding that all four \"assumed properties\" were questionable and had several potential confounds.\n\nBased on the closely mirrored results of both studies, Rind, Tromovitch and Bauserman questioned the scientific validity of a single term \"child sexual abuse\" and suggested a variety of different labels for sexual contact between adults and non-adults based on age and the degree to which the child was forced or coerced into participating. They concluded with a discussion of the legal and moral implications of the article, stating that the \"wrongfulness\" and \"harmfulness\" of sexual acts are not inherently linked, and finished with the statement:\nThe paper was first published by the American Psychological Association (APA) in July, 1998, in \"Psychological Bulletin\" to little reaction, though strong reactions were ultimately demonstrated by social conservatives / religious fundamentalists, and psychotherapists and psychiatrists who treat victims of sexual abuse who were concerned about the implications. The first substantial and public reaction was a December criticism by the National Association for Research & Therapy of Homosexuality, an organization dedicated to the discredited view that homosexuality is a mental illness that can be cured by psychotherapy.\n\nIn March 1999, talk show host Laura Schlessinger criticized the study as \"junk science\" and stated that since its conclusions were contrary to conventional wisdom, its findings should never have been released. She criticized the study's use of meta-analysis, saying. \"I frankly have never seen this in general science. ... This [pooling of studies] is so outrageous!\" \"This was not a study! They didn’t do a study! They arbitrarily found 59 studies that other people had done [and] combined them all.\"\n\nShortly thereafter, the North American Man/Boy Love Association posted an approving review of the study on their website, furthering the impression that the piece was an endorsement of pedophilia. The paper eventually provoked a reaction from several conservative American members of Congress, notably the Republican representatives Matt Salmon of Arizona and Tom DeLay of Texas, who both condemned the study as advocating for the normalization of pedophilia. In the process Delay confused the American Psychological Association with the American Psychiatric Association, an error also made by Schlessinger.\n\nIn response, the APA declared in a press statement that child sexual abuse is harmful and wrong, and that the study was in no way an endorsement of pedophilia. The APA mandated a policy change by which APA journal editors would alert the organization of potentially controversial topics in order to be more proactive with politicians, the media and other groups. In an internal organization email, APA Executive Vice-President Raymond D. Fowler stated that because of the controversy, the article's methodology, analysis and the process by which it had been approved for publication was reviewed and found to be sound. In June 1999, Fowler announced in an open letter to DeLay that there would be an independent review of the paper and stated that from a public policy perspective, some language used in the article was inflammatory and inconsistent with the position of the APA's stance on CSA. The APA also implemented a series of actions designed to prevent the study from being used in legal circumstances to defend CSA and stated an independent review would be undertaken of the scientific accuracy and validity of the report. The request for an outside review of a controversial report by an independent scientific association was unprecedented in APA's 107-year history.\n\nIn April 1999, a resolution was introduced in the Alaska Legislature condemning the article, with similar resolutions introduced in California, Illinois, Louisiana, Oklahoma, and Pennsylvania over the subsequent two months. Some of these states' psychological associations reacted by asking the APA to take action. On July 12, 1999, the United States House of Representatives passed HRC resolution 107 by a vote of 355-0, (with 13 Members voting \"Present\", the latter all members of the Democratic Party) declaring sexual relations between children and adults are abusive and harmful, and condemned the study on the basis that it was being used by pro-pedophilia activists and organizations to promote and justify child sexual abuse. The condemnation of a scientific study by Congress was, at that time, an unprecedented event. The resolution passed the Senate by a voice vote (100-0) on July 30, 1999 and was greeted among psychologists with concern due to the perceived chilling effect it may have among researchers. Representative Brian Baird, who has a Ph.D. in clinical psychology and was one of the 13 Congressmen to abstain from the condemnation of the study, stated that of the 535 members of the House and Senate fewer than 10 had actually read the study, and even fewer were qualified to evaluate it based on its merit. In September 1999, the American Association for the Advancement of Science (AAAS), upon a request by the APA to independently review the article, stated that it saw no reason to second-guess the peer review process that approved it initially and that it saw no evidence of improper methodology or questionable practices by the authors. The AAAS also expressed concern that the materials reviewed demonstrated a grave lack of understanding of the study on the part of the media and politicians and were also concerned about the misrepresentation of its findings. The AAAS stated that the responsibility for discovering problems with the article lay with the initial peer reviewers, and declined to evaluate the article, concluding with a statement that the decision to not review the article was neither an endorsement, nor a criticism of it.\n\nIn August 2000, the APA drafted and adopted a position statement in response to the Rind \"et al\". controversy that opposed any efforts to censor controversial or surprising research findings and asserting that researchers must be free to investigate and report findings as long as the research has been conducted within appropriate ethical and research standards.\n\nA series of 2001 papers published in the \"Journal of Child Sexual Abuse\" discussed and criticized the findings of the Rind \"et al\". study. Stephanie Dallam stated that, after reviewing the evidence, the paper was best described as \"an advocacy article that inappropriately uses science in an attempt to legitimatize its findings\". Four other researchers also discussed possible flaws in the methodology and generalizability of Rind's findings, and concluded the paper's results were scientifically invalid. The criticisms were co-published in the 2001 book \"Misinformation Concerning Child Sexual Abuse and Adult Survivors\". In 2002, a rebuttal to many of the claims made by critics was submitted to the APA journal, the \"American Psychologist\" by Scott Lilienfeld. After passing a normal peer review, the editor of the journal re-submitted the article in secret and, on the basis of this second review, the paper was rejected. Lilienfeld reported this subsequent rejection on several psychology Internet fora, which produced an intense response and resulted in the APA and \"American Psychologist\" ultimately printing the article as part of a special issue focusing on the controversy.\n\nThe paper has been criticized for restricting its analysis to convenience samples of college students, possibly introducing systematic bias by excluding victims so traumatized that they did not go on to attend college. Another possibility was that Rind \"et al\".'s conclusions may not be generalizable beyond college populations in general as individuals with a history of CSA were more likely than non-abused individuals to drop out of college after a single semester.\n\nRind, Bauserman and Tromovitch responded to this criticism by saying that \"the representativeness of college samples is in fact irrelevant to the stated goals and conclusions of our study\" since the purpose of their research was \"to examine the validity of the clinical concept\" of CSA. They added that according to the commonly understood definition of the term, child sexual abuse is extremely and pervasively harmful, meaning that \"in any population sampled - drug addicts, psychiatric patients, or college students - persons who have experienced CSA should show strong evidence of the assumed properties of CSA.\" The authors of the study say that because the college sample did not show pervasive harm, \"the broad and unqualified claims about the properties of CSA are contradicted\". Rind \"et al\". also said that using college samples was appropriate because their study found similar prevalence rates and experiences of severity and outcomes between college samples and national samples.\n\nDallam \"et al\". said that Rind \"et al\". did not standardize their definition of child sexual abuse, leaving out certain studies that were appropriate, and including studies that were inappropriate. That is, they allege that Rind \"et al\". uncritically combined data from studies of CSA with data from studies looking at other phenomena such as consensual peer experiences, sexual experiences that occurred during adulthood, and homosexual approaches during adolescence.\n\nRind, \"et al\". have also responded to this criticism, defending the appropriateness of including all five of the studies (Landis, 1956; Schultz and Jones, 1983; Sedney and Brooks, 1984; Greenwald, 1994; and Sarbo, 1985) specifically identified by Dallam \"et al\". as inappropriate to a study about child sexual abuse.\n\nDallam \"et al\". said that the first three studies focused on all types of child sexual activity, not just child sexual abuse. Rind \"et al\". reject this criticism. In regard to the Landis study, Rind \"et al\". note that it has been used by many other sex researchers (e.g., Finkelhor, Fishman, Fromuth & Burkhart, Sarbo, and others) as an example of an early study about child sexual abuse. In regard to the Shultz and Jones study, Rind \"et al\". concede that the study \"looked at all types of 'sexual acts' before age 12,\" but explained that the respondents in the study were all asked \"if their experience was with a person over the age of 16,\" thus allowing Rind \"et al\". to include only the relationships that were age-discrepant. In regard to the Sedney and Brooks study, Rind \"et al\". admit that the study used a broad definition of child sexual abuse, but explain that the researchers themselves chose to use such a definition \"because of the difficulty posed by a priori decisions about what type of sexual experiences are 'problems.'\"\n\nSimilarly, psychiatrist David Spiegel said that the inclusion of Landis' 1956 study was unjustified. He argued that, while weighting larger studies more than smaller makes sense, combining the results of a large study examining very mild trauma (such as fending off an attacker) with studies of long-term physical and sexual abuse was inappropriate and led to erroneous conclusions. Rind \"et al\". replied that Spiegel misrepresented their analysis, since they did not use Landis' study in the meta-analysis of childhood sexual abuse – symptom correlations, but only for examining the self-reported effects of CSA. They contend that the way they handled Landis' data maximized negative reports and minimized the possible deflating effect of Landis' data on the overall effect sizes.\n\nThe last two studies, according to Dallam \"et al\"., were inappropriate because they included respondents who were over the age of 17 when the CSA occurred. Persons 18 years old or older are above the legal age of consent in all states of the USA, and thus are not \"children\" even in the loosest definition of the term. Rind \"et al\". responded to Dallam \"et al\". by saying that, in the effect-size calculations of the Sarbo and Greenwald studies (i.e., the calculations that show the alleged harmfulness of CSA), they had included only respondents aged 16 and 15 and under, respectively, at the time of the CSA and all CSA incidents included in Sarbo's and Greenwald's original analysis occurred before the age of 17.\n\nSpiegel criticized that Rind \"et al\". included a long list of measured variables in order to appear comprehensive, but remarkably omitted posttraumatic stress disorder - \"the most salient symptom\" - from their analysis. Rind \"et al\". replied that including PTSD was impossible due to the fact that the original studies did not examine it. Furthermore, they cited Kendall-Tackett \"et al\". to illustrate the lack of a common pattern of symptoms in children who have been abused.\n\nDavid Spiegel also argued that Rind \"et al\".'s suggestion of relabeling some forms of sexual encounters between adults and children/adolescents as adult-child (or adult-adolescent) sex is fundamentally flawed, because children cannot give meaningful consent to sexual relations with an adult. Some critics also argued that using value-neutral terminology would normalize CSA and that redefining terminology is not in the interest of the general public because it confuses the underlying moral issues. Rind \"et al\". replied that the construct of consent used in their study was misinterpreted by critics; they only asserted that children/adolescents are capable of simple consent (willingness) as opposed to informed consent used in legal contexts, and used this as a variable in their study simply because it was used in the original studies - where it had predictive validity. Thus, they conclude that although the construct of willingness might be morally unacceptable, it is a scientifically valid term. A similar argument was put forward by Oellerich, who stated that considering all adult/non-adult sexual behavior as abusive and lacking consent can lead to bias in scientific research in the area, and that recognizing this distinction does not necessarily lead to considering adult/non-adult sexual interactions as morally permissible.\n\nDallam \"et al\". also contend that Rind \"et al\". miscoded or misreported significant amounts of the underlying study data, thereby skewing the results. Dallam \"et al\". contend that Rind \"et al\". incorrectly used \"Pearson's r\" instead of \"Cohen's d\" to calculate the effect size, which resulted in a failure to correct for base-rate differences of CSA in male and female samples, and which led to the finding that males were less harmed by CSA. After correcting for base-rate attenuation, Dallam \"et al\". said they arrived at identical effect sizes for male and female samples.\n\nIn response to this criticism, Rind \"et al\". contend that they did indeed describe the contrast between the effect size estimates as \"nonsignificant, z = 1.42, p > .10, two-tailed\". However, they argue, \"What [they] did report as significantly different was the contrast between male and female effect size estimates for the all-types-of-consent groups, where \"rs\" = .04 and .11, respectively. In \"follow[ing] Dallam \"et al\". (2001) [by] apply[ing] Becker's correction formula to these values, they become rs = .06 and .12 for men and women, respectively. The contrast is still statistically significant (z = 2.68, p < .01. two-tailed), contrary to Dallam \"et al\".'s (2001) claim\".\n\nRind \"et al\". said that their own \"handling of Pearson's r in the face of base-rate differences was methodologically proper and produced no important bias, if any at all.\" Furthermore, they contend that Dallam's criticisms \"exhibited bias ... [by] selectively ignoring key clarifying quotes ... and citing them elsewhere in their critique to argue different points, and [by] ignoring or overlooking a key caveat by Becker (1986) regarding appropriate use of his correction formula\".\n\nCritics also argued that Rind \"et al\".'s statistical approach for controlling for family environment as a cause of maladjustment was conceptually and methodologically invalid. Spiegel stated that inferring the source of maladjustment from analyzing the shared variance between CSA and family environment does not answer the question of which variable explains maladjustment better; the authors answered that this statement shows a misunderstanding of the statistical procedure used their meta-analysis. Dallam, however, addressed the topic of several prior studies having found statistically significant relations between CSA and maladjustment even after controlling for family environment.\n\nRind \"et al\".'s model of \"assumed properties of child sexual abuse,\" (that is, of universal and pervasive harm in all victims of CSA) has been criticized as a straw man assertion in that it is both simplistic and misleading. The reactions of victims in their adult lives have been found to be extremely varied, ranging from severe to nearly unnoticeable, and many pathologies are not diagnosable in the strictly clinical sense Rind uses. Victims often have a flawed or distorted appraisal of their abuse, and fail to connect distressing and sometimes debilitating pathologies with their experiences. Further, these studies make no accounting for emotional support of the victim's family, clinical treatment of the victim prior to the study, or personal resiliency, which can easily account for less severe outcomes.\n\nRind, Bauserman and Tromovitch stated that research findings can be skewed by an investigator's personal biases, and in Rind \"et al\". claimed that \"[r]eviewers who are convinced that CSA is a major cause of adult psychopathology may fall prey to confirmation bias by noting and describing study findings indicating harmful effects but ignoring or paying less attention to findings indicating nonnegative outcomes\". They defended their deliberate choice of non-legal and non-clinical samples, accordingly avoiding individuals who received psychological treatment or were engaged in legal proceedings as a way of correcting this bias through the use of a sample of college students.\n\nDallam and Anne Salter have stated that Rind and Bauserman have associated with age of consent reform organizations in the past. In the years before the paper was written, both Rind and Bauserman had published articles in \"Paidika: The Journal of Paedophilia\", a journal which was dedicated to \"[demonstrating] that pedophilia has been, and remains, a legitimate and productive part of the totality of human experience\". In addition, Dallam and Salter stated that Rind and Bauserman were keynote speakers at a pedophile advocacy conference occurring in the Netherlands. Another article described Bauserman and Tromovitch's involvement as \"[presenting] their meta-analytic findings to a group of clinicians in the Netherlands [and] Robert Bauserman (1989), had published an article in \"Paidika\", a Dutch journal that had previously featured manuscripts tolerant of pedophilia\".\n\nDespite the authors' comments that the findings of the paper \"do not imply that moral or legal definitions of or views on behaviors currently classified as CSA\" should be changed, it caught the attention of, and was used by, advocates for pedophilia. The paper was cited, reviewed, and posted to the Internet by numerous advocacy groups. It has been used to argue that the age of consent should be lowered or abolished, and it has been used in criminal court in the U.S. by attorneys defending those accused of child sexual offenses.\n\nSocial psychologist Carol Tavris noted several other groups that reacted negatively to the study. The anti-homosexuality group National Association for Research & Therapy of Homosexuality (NARTH), who \"[endorse] the long-discredited psychoanalytic notion that homosexuality is a mental disorder and that it is a result of seduction in childhood by an adult\", objected to the study's implications that boys who are sexually abused are not traumatized for life and do not become homosexuals as a result. Therapists who supported the existence of recovered memories and recovered-memory therapy, as well as those who attributed mental illnesses such as dissociative identity disorder, depression and eating disorders to repressed memories of sexual abuse also rejected the study. Tavris attributed this rejection to the fear of malpractice lawsuits. Tavris herself believed that the study could have been interpreted positively as an example of psychological resilience in the face of adversity, and noted that CSA causing little or no harm in some individuals is not an endorsement of the act, nor does it make it any less illegal.\n\nNumerous studies and professional clinical experience in the field of psychology, both before and after Rind \"et al\".'s publications, have long supported the stance that children cannot consent to sexual activity and that child and adolescent sexual abuse cause harm. The then American Psychological Association CEO Raymond D. Fowler succinctly reiterated the prevailing view in a 1999 letter to Congressman Delay \"that children cannot consent to sexual activity with adults,\" and \"sexual activity between children and adults should never be considered or labeled as harmless or acceptable\". Others, like Rind \"et al\". and Ulrich \"et al\"., counter that that prevailing \"simplistic\" view of CSA fails to completely account for the variety and complexity of documented sexual experience that many insist, for strong moral reasons, \"cannot\" exist.\n\nA study published in The \"Scientific Review of Mental Health Practice\" attempted to replicate the Rind study, correcting for methodological and statistical problems identified by Dallam and others. It supported some of the Rind findings, both with respect to the percentage of variance in later psychological outcomes accounted for by sexual abuse and in relation to the finding that there was a gender difference in the experience of child sexual abuse, such that females reported more negative effects. It, however, acknowledged the limitations of the findings (college student sample, self-report data), and did not endorse Rind's recommendation to abandon the use of the term \"child sexual abuse\" in cases of apparent consent in favor of the term \"adult-child sex.\" In their conclusion, the authors address the objection that Rind's work and their own would give support to those who deny that child sexual abuse can cause harm: \"The authors of the current research would hesitate to support such a general statement. Instead, our results, and the results of the Rind \"et al\". meta-analysis, can be interpreted as providing a hopeful and positive message to therapists, parents, and children. Child sexual abuse does not necessarily lead to long-term harm.\"\n\nThere has been greater emphasis in subsequent work on the range of responses that are possible from victims. For example, a few studies make reference to the paper's findings about \"consensual\" encounters, but approach it from the opposite direction (i.e., that the use of force causes more intense negative outcomes). Heather Ulrich, author of the aforementioned replication of the meta-analysis, later drew on the findings to study the reasons for the variability in outcomes of CSA victims, such as attributional style (individual's causal explanations for why the abuse occurred), family environment, and social support.\n\n"}
{"id": "5576664", "url": "https://en.wikipedia.org/wiki?curid=5576664", "title": "Shuttle Amateur Radio Experiment", "text": "Shuttle Amateur Radio Experiment\n\nThe Shuttle Amateur Radio Experiment (SAREX), later called the Space Amateur Radio Experiment, was a program that promoted and supported the use of amateur (\"ham\") radio by astronauts in low earth orbit aboard the United States Space Shuttle to communicate with other amateur radio stations around the world. It was superseded by the Amateur Radio on the International Space Station (ARISS) program. SAREX was sponsored by NASA, AMSAT (The Radio Amateur Satellite Corporation), and the ARRL (American Radio Relay League).\n\nShortly after the launch of STS-9, On November 28, 1983 Owen Garriott (W5LFL) became the first amateur radio operator active in space. Garriott had already flown on Skylab 3, but did not operate radio equipment on that trip. On STS-9, he used a handheld 2-meter radio to talk to his mother, senator Barry Goldwater, King Hussein of Jordan (JY1), and many others. Garriott made approximately 300 calls and convinced NASA that amateur radio was useful to get students involved in space. Thus began the Space Amateur Radio Experiment, also known as SAREX. \n\nThe second successful use of amateur radio in space was carried out by Anthony W. England (W0ORE) on \"Challenger\" flight STS-51F in 1985. He completed 130 contacts and sent 10 images via slow-scan television. In 1991, STS-37 became the first voyage to space on which the entire crew were licensed amateur radio operators. \n\nAfter these flights, amateur radios were often taken on the shuttles, as many as twenty-five before the program became known as ARISS. Licensed hams were able to participate during their free time. \n\nMost amateur radio operators used SAREX to speak with licensed astronauts during their down times. SAREX, however, has been very educational for young students from kindergarten to fifth grade involved in a program similar to young astronauts, in which elementary school children learn about astronauts' daily activities and what it is like in space. Students also have had the opportunity to communicate via video when the shuttles have had suitable equipment. Teachers have found out about how to link their classes with the SAREX program through the Amateur Radio in Space Guide distributed by NASA.\n\nAn amateur operator license is needed before operating an amateur station. The license can be obtained from the U.S. Federal Communications Commission's (FCC) Amateur Radio Service. No special SAREX license is required for operation, but certain regulations come into play for space communications.\n"}
{"id": "48581800", "url": "https://en.wikipedia.org/wiki?curid=48581800", "title": "Spiroplasma mirum", "text": "Spiroplasma mirum\n\nSpiroplasma mirum is a bacterium in the genus Spiroplasma. A strain of it, called \"Spiroplasma mirum\" strain SMCA (Suckling mouse cataract agent), causes cataracts in suckling mice.\n\n"}
{"id": "37889300", "url": "https://en.wikipedia.org/wiki?curid=37889300", "title": "State formation", "text": "State formation\n\nState formation is the process of the development of a centralized government structure in a situation where one did not exist prior to its development. State formation has been a study of many disciplines of the social sciences for a number of years, so much so that Jonathan Haas writes that \"One of the favorite pastimes of social scientists over the course of the past century has been to theorize about the evolution of the world's great civilizations.\" The study of state formation is divided generally into either the study of early states (those that developed in stateless societies) or the study of modern states (particularly of the form that developed in Europe in the 17th century and spread around the world). Academic debate about various theories is a prominent feature in fields like Anthropology, Sociology, Economics and Political Science.\n\nA state is a political system with a centralized government, a military force, a civil service, an arranged society, and literacy. Though, there is no clear agreement on the defining characteristics of a state and the definition can vary significantly, based upon the focus of the particular definition. The state is considered to be territoriality bound and is distinct from tribes or units without centralized institutions.\n\nAccording to Painter & Jeffrey, there are 5 distinctive features of the modern state:\n\n1) They are ordered by precise boundaries with administrative control across the whole;\n\n2) They occupy large territories with control given to organized institutions;\n\n3) They have a capital city and are endowed with symbols that embody state power;\n\n4) The government within state creates organizations to monitor, govern and control its population through surveillance and record keeping;\n\n5) They increase monitoring over time.\n\nAdditionally, Herbst holds that there is another relevant characteristic of modern states: nationalism. This feeling of belonging to a certain territory plays a central role in state formation since it increases citizens' willingness to pay taxes.\n\nTheories of state formation have two distinct focuses, depending largely on the field of study:\n\n\nStates are minimally defined by anthropologist David S. Sandeford as socially stratified and bureaucratically governed societies with at least four levels of settlement hierarchy (e.g., a large capital, cities, villages, and hamlets). Primary states are those state societies that developed in regions where no states existed before. These states developed by strictly internal processes and interaction with other non-states societies. The exact number of cases which qualify as primary states is not clearly known because of limited information about political organization before the development of writing in many places, but Sandeford lists ten likely cases of primary state formation in Eurasia, the Americas, and the Pacific.\n\nStudies on the formation of early states tend to focus on processes that create and institutionalize a state in a situation where a state did not exist before. Examples of early states which developed in interaction with other states include the Aegean Bronze Age Greek civilizations and the Malagasy civilization in Madagascar. Unlike primary state formation, early state formation does not require the creation of the first state in that cultural context or development autonomously, independently from state development nearby. Early state formation causation can thus include borrowing, imposition, and other forms of interaction with already existing states.\n\nTheories on the formation of modern states focus on the processes that support the development of modern states, particularly those that formed in late-medieval Europe and then spread around the world with colonialism. Starting in the 1940s and 1950s, with decolonization processes underway, attention began to focus on the formation and construction of modern states with significant bureaucracies, ability to tax, and territorial sovereignty around the world. However, some scholars hold that the modern state model formed in other parts of the world prior to colonialism, but that colonial structures replaced it.\n\nThere are a number of different theories and hypotheses regarding early state formation that seek generalizations to explain why the state developed in some places but not others. Other scholars believe that generalizations are unhelpful and that each case of early state formation should be treated on its own.\n\nVoluntary theories contend that diverse groups of people came together to form states as a result of some shared rational interest. The theories largely focus on the development of agriculture, and the population and organizational pressure that followed and resulted in state formation. The argument is that such pressures result in integrative pressure for rational people to unify and create a state. Much of the social contract philosophical tradition proposed a voluntary theory for state formation.\n\nOne of the most prominent theories of early and primary state formation is the \"hydraulic hypothesis\", which contends that the state was a result of the need to build and maintain large-scale irrigation projects. The theory was most significantly detailed Karl August Wittfogel's argument that, in arid environments, farmers would be confronted by the production limits of small-scale irrigation. Eventually different agricultural producers would join together in response to population pressure and the arid environment, to create a state apparatus that could build and maintain large irrigation projects.\n\nIn addition to this, is what Carneiro calls the \"automatic hypothesis\", which contends that the development of agriculture easily produces conditions necessary for the development of a state. With surplus food stocks created by agricultural development, creation of distinct worker classes and a division of labor would automatically trigger creation of the state form.\n\nA third voluntary hypothesis, particularly common with some explanations of early state development, is that long distance trade networks created an impetus for states to develop at key locations: such as ports or oases. For example, the increased trade in the 16th century may have been a key to state formation in West African states such as Whydah, Dahomey, and the Benin Empire.\n\nConflict theories of state formation regard conflict and dominance of some population over another population as key to the formation of states. In contrast with voluntary theories, these arguments believe that people do not voluntarily agree to create a state to maximize benefits, but that states form due to some form of oppression by one group over others. A number of different theories rely on conflict, dominance, or oppression as a causal process or as a necessary mechanism within certain conditions and they may borrow from other approaches. In general the theories highlight: \"economic stratification\", \"conquest of other peoples\", conflict in \"circumscribed areas\", and the neoevolutionary growth of bureaucracy. \n\nOther aspects are highlighted in different theories as of contributing importance. It is sometimes claimed that technological development, religious development, or socialization of members are crucial to state development. However, most of these factors are found to be secondary in anthropological analysis. In addition to conquest, some theories contend that the need for defense from military conquest or the military organization to conquer other peoples is the key aspect leading to state formation.\n\nSome theories proposed in the 19th century and early 20th century have since been largely discredited by anthropologists. Carneiro writes that theories \"with a racial basis, for example, are now so thoroughly discredited that they need not be dealt with...We can also reject the belief that the state is an expression of the 'genius' of a people, or that it arose through a 'historical accident.' Such notions make the state appear to be something metaphysical or adventitious, and thus place it beyond scientific understanding.\" Similarly, social Darwinist perspectives like those of Walter Bagehot in \"Physics and Politics\" argued that the state form developed as a result of the best leaders and organized societies gradually gaining power until a state resulted. Such explanations are not considered sufficient to explain the formation of the state.\n\nIn the medieval period (500-1400) in Europe, there were a variety of authority forms throughout the region. These included feudal lords, empires, religious authorities, free cities, and other authorities. Often dated to the 1648 Peace of Westphalia, there began to be the development in Europe of modern states with large-scale capacity for taxation, coercive control of their populations, and advanced bureaucracies. The state became prominent in Europe over the next few centuries before the particular form of the state spread to the rest of the world via the colonial and international pressures of the 19th century and 20th century. Other modern states developed in Africa and Asia prior to colonialism, but were largely displaced by colonial rule.\n\nPolitical scientists, sociologists, and anthropologists began studying the state formation processes in Europe and elsewhere in the 17th century—beginning significantly with Max Weber. However, state formation became a primary interest in the 1970s. The question was often framed as a contest between state forces and society forces and the study of how the state became prominent over particular societies. A number of theories developed regarding state development in Europe. Other theories focused on the creation of states in late colonial and post-colonial societies. The lessons from these studies of the formation of states in the modern period are often used in theories about State-building. Other theories contend that the state in Europe was constructed in connection with peoples from outside Europe and that focusing on state formation in Europe as a foundation for study silences the diverse history of state formation.\n\nBased on the model of European states, it has been commonly assumed that development is the natural path that states will eventually walk through. However, Herbst holds that in the case African states, as well as in developing countries of other regions, development need not be the natural step. States that struggle their consolidation could remain permanently weak.\n\nTwo related theories are based on military development and warfare, and the role that these forces played in state formation. Charles Tilly developed an argument that the state developed largely as a result of \"state-makers\" who sought to increase the taxes they could gain from the people under their control so they could continue fighting wars. According to Tilly, the state makes war and war makes states. In the constant warfare of the centuries in Europe, coupled with expanded costs of war with mass armies and gunpowder, warlords had to find ways to finance war and control territory more effectively. The modern state presented the opportunity for them to develop taxation structures, the coercive structure to implement that taxation, and finally the guarantee of protection from other states that could get much of the population to agree. Taxes and revenue raising have been repeatedly pointed out as a key aspect of state formation and the development of state capacity. Economist Nicholas Kaldor emphasized on the importance of revenue raising and warned about the dangers of the dependence on foreign aid. Tilly argues, state making is similar to organized crime because it is a \"quintessential protection racket with the advantage of legitimacy.\"\n\nMichael Roberts and Geoffrey Parker, in contrast, finds that the primary causal factor was not the \"state-makers\" themselves, but simply the military revolutions that allowed development of larger armies. The argument is that with the expanded state of warfare, the state became the only administrative unit that could endure in the constant warfare in the Europe of this period, because only it could develop large enough armies. \nThis view—that the modern state replaced chaos and general violence with internal disciplinary structures—has been challenged as ethnocentric, and ignoring the violence of modern states.\n\nWar has played a key role not only in the consolidation of European states but also of some third world states. According to Herbst, external security threats have had a fundamental role in the development of the South Korean and Taiwanese states. A 2017 study which tests the predictions of warfare theories of Tilly and others found that the predictions do not match the empirical record. The study found that median state size decreased from 1100 to 1800, and that the number of states increases rapidly between the twelfth and thirteen centuries and remained constant until 1800.\n\nStein Rokkan and others have argued that the modern territorial state developed in places that were peripheral to the commercial \"city belt\" (\"a central regional band extending, roughly, in an arc from the Low Countries, through the Rhineland and into Northern Italy\") that ran through Central Europe. The existence of prosperous urban centers that relied on commerce in Central Europe prevented rulers from consolidaing their rule over others. The elites in those urban centers could rely on their wealth and on collective security institutions (like the Hanseatic or Swabian league) with other urban centers to sustain their independence. A lower density of urban centers in England and France made it easier for rulers to establish rule over expansive territories.\n\nAnother argument contends that the state developed out of economic and social crises that were prominent in late-medieval Europe. Religious wars between Catholics and Protestants, and the involvement of leaders in the domains of other leaders under religious reasons was the primary problem dealt with in the Peace of Westphalia. In addition, Marxist theory contends that the economic crisis of feudalism forced the aristocracy to adapt various centralized forms of organization so they could retain economic power, and this resulted in the formation of the modern state.\n\nSome scholarship, linked to wider debates in Anthropology, has increasingly emphasized the state as a primarily cultural artifact, and focuses on how symbolism plays a primary role in state formation. Most explicitly, some studies emphasize how the creation of national identification and citizenship were crucial to state formation. The state then is not simply a military or economic authority, but also includes cultural components creating consent by people by giving them rights and shared belonging.\n\nWhile modern states existed without European influence around the world before colonialism, post-colonial state formation has received the most significant attention. While warfare is primary in theories about state formation in Europe, the development of the international norm of non-interventionism means that other processes of state formation have become prominent outside Europe (including colonial imposition, assimilation, borrowing, and some internal political processes. John W. Meyer's \"World Society Theory\" contends that the state form was exported from Europe, institutionalized in the United Nations, and gradually the modern nation-state became the basis for both those in power and those challenging power. In addition, because many of the early modern states like the United Kingdom and France had significant empires, their institutional templates became standard for application globally.\n\n\n\n"}
{"id": "46602356", "url": "https://en.wikipedia.org/wiki?curid=46602356", "title": "Study 329", "text": "Study 329\n\nStudy 329 was a clinical trial conducted in North America from 1994 to 1998 to study the efficacy of paroxetine, an SSRI anti-depressant, in treating 12- to 18-year-olds diagnosed with major depressive disorder. Led by Martin Keller, then professor of psychiatry at Brown University, and funded by the British pharmaceutical company SmithKline Beecham—known since 2000 as GlaxoSmithKline (GSK)—the study compared paroxetine with imipramine, a tricyclic antidepressant, and placebo (an inert pill). SmithKline Beecham had released paroxetine in 1991, marketing it as Paxil in North America and Seroxat in the UK. The drug attracted sales of $11.7 billion in the United States alone from 1997 to 2006, including $2.12 billion in 2002, the year before it lost its patent.\n\nPublished in July 2001 in the \"Journal of the American Academy of Child and Adolescent Psychiatry\" (\"JAACAP\"), which listed Keller and 21 other researchers as co-authors, study 329 became controversial when it was discovered that the article had been ghostwritten by a PR firm hired by SmithKline Beecham; had made inappropriate claims about the drug's efficacy; and had downplayed safety concerns. The controversy led to several lawsuits and strengthened calls for drug companies to disclose all their clinical research data. \"New Scientist\" wrote in 2015: \"You may never have heard of it, but Study 329 changed medicine.\"\n\nSmithKline Beecham acknowledged internally in 1998 that the study had failed to show efficacy for paroxetine in adolescent depression. In addition, more patients in the group taking paroxetine had experienced suicidal thinking and behaviour. Although the \"JAACAP\" article included these negative results, it did not account for them in its conclusion; on the contrary, it concluded that paroxetine was \"generally well tolerated and effective for major depression in adolescents\". The company relied on the \"JAACAP\" article to promote paroxetine for off-label use in teenagers.\n\nIn 2003 Britain's Medicines and Healthcare Products Regulatory Agency (MHRA) analysed study 329 and other GSK studies of paroxetine, concluding that, while there was no evidence of paroxetine's efficacy in children and adolescents, there was \"robust evidence\" of a causal link between the drug and suicidal behaviour. The following month the MHRA and US Food and Drug Administration (FDA) advised doctors not to prescribe paroxetine to the under-18s. The MHRA launched a criminal inquiry into GSK's conduct, but announced in 2008 that there would be no charges. In 2004 New York State Attorney Eliot Spitzer sued GSK for having withheld data, and in 2012 the United States Department of Justice fined the company $3 billion, including a sum for withholding data on paroxetine, unlawfully promoting it for the under-18s, and preparing a misleading article on study 329. The company denied that it had withheld data, and said it was only when data from its nine paediatric trials on paroxetine were analysed together that \"an increased rate of suicidal thinking or attempted suicide [was] revealed\".\n\nThe \"JAACAP\" article on study 329 was never retracted. The journal's editors say the negative findings are included in a table, and that therefore there are no grounds to withdraw the article. In September 2015 the \"BMJ\" published a re-analysis of the study. This concluded that neither paroxetine nor imipramine had differed in efficacy from placebo in treating depression, that the paroxetine group had experienced more suicidal ideation and behaviour, and that the imipramine group had experienced more cardiovascular problems.\n\nFunded by SmithKline Beecham, the acute phase of study 329 was an eight-week, double-blind, randomized clinical trial conducted in 12 university or hospital psychiatric departments in the United States and Canada between 1994 and 1997. The study compared paroxetine, a selective serotonin reuptake inhibitor marketed as Paxil and Seroxat, with imipramine, a tricyclic antidepressant marketed as Tofranil, in teenagers aged 12–18 with a diagnosis of major depressive disorder of at least eight weeks duration. Martin Keller, then professor of psychiatry at Brown University, had proposed the trial to the company in 1992 as the largest study until then to examine the efficacy of SSRIs in children.\n\nAfter a screening phase from April 1994, 275 male and female patients were randomly assigned paroxetine, imipramine or placebo (an inert pill). Of the 275, 93 were given paroxetine, 95 imipramine and 89 placebo. The paroxetine group were given 20 mg daily for four weeks, rising to 30 mg at week five and 40 mg at week six if the clinician thought it appropriate. The last study visit was in May 1997, and the blind was broken in October.\n\nThe trial's protocol had described two primary and six secondary outcomes by which it would measure efficacy. The data showed that, according to those eight outcomes, paroxetine was no more effective than placebo. According to Melanie Newman, writing for the \"BMJ\", \"[t]he drug only produced a positive result when four new secondary outcome measures, which were introduced following the initial data analysis, were used instead. Fifteen other new secondary outcome measures failed to throw up positive results.\"\n\nEleven subjects on paroxetine, compared to five on imipramine and two on placebo, experienced serious adverse events (SAE), including behavioral problems and emotional lability. The researchers defined an event as an SAE if it resulted in hospitalization, involved suicidal gestures, or was regarded as serious by the subject's doctor. In the 93 taking paroxetine, the SAEs consisted of one subject experiencing headache while tapering off, and 10 experiencing psychiatric problems. Seven of the 10 were hospitalized. Two of the 10 experienced worsening depression; two conduct problems such as aggression; one euphoria; and five emotional lability, including suicidal ideation and behaviour. Of the 95 patients on imipramine and the 89 on placebo, one in each group experienced emotional lability. Yet Keller's article in the \"Journal of the American Academy of Child and Adolescent Psychiatry\" concluded that, of the 11 patients who had experienced SAEs while taking paroxetine, \"only headache (1 patient) was considered by the treating investigator to be related to paroxetine treatment\".\n\nIn October 1998 the neurosciences division of SmithKline Beecham's Central Medical Affairs (CMAT) department distributed a position paper, \"Seroxat/Paxil Adolescent Depression: Position piece on the phase III clinical studies\", that discussed studies 329 and 377. The latter was a 12-week trial, comparing paroxetine and placebo in teenagers, conducted from 1995 to 1998.\n\nThe SmithKline Beecham position paper explained that the company had decided not to submit trial data from studies 329 and 377 to regulators, and discussed how to \"effectively manage the dissemination of these data in order to minimise any potential negative commercial impact\". An attached memo noted that the results were disappointing and would not support a label claim that paroxetine could be used to treat adolescents: \"The best that could have been achieved was a statement that, although safety data was reassuring, efficacy had not been demonstrated.\" The paper said: \"it would be commercially unacceptable to include a statement that efficacy had not been demonstrated, as this would undermine the profile of paroxetine.\"\n\nStudy 329 had shown \"trends in efficacy in favour of Seroxat/Paxil across all indices of depression ...\", according to the paper, \"[but had] failed to demonstrate a statistically significant difference from placebo on the primary efficacy measures\". Study 377 had shown a high placebo response rate and had \"failed [to] demonstrate any separation of Seroxat/Paxil from placebo\". SmithKline Beecham decided to publish study 329 but not 377, and not to submit either trial to the regulators, because they were \"insufficiently robust to support a label change\" for adolescent use\".\n\nThe document was leaked during a lawsuit and first published by the \"Canadian Medical Association Journal\" in March 2004. In response a GSK spokesperson said that \"the memo draws an inappropriate conclusion and is not consistent with the facts ... GSK abided by all regulatory requirements for submitting safety data. We also communicated safety and efficacy data to physicians through posters, abstracts, and other publications.\"\n\nAlthough the \"JAACAP\" article listed its authors as Martin Keller and 21 other physicians or researchers, the article had in fact been written by Scientific Therapeutics Information (STI), a PR company in Springfield, New Jersey, specializing in communications for the pharmaceutical industry. The \"JAACAP\" article did not mention STI; the only mention of Laden was: \"Editorial assistance was provided by Sally K. Laden, M.S.\" The list of authors included James P. McCafferty of GSK, but the article did not disclose his company affiliation.\n\nSTI had worked with SmithKline Beecham on its promotion of paroxetine since the early 1990s. In April 1998 Sally K. Laden and John A. Romankiewicz of STI sent SmithKline Beecham an estimate of $17,250 to work on six drafts of the study 329 paper, including the final draft, to cover the period up to March 1999. The sum was payable in installments: $8,500 upon initiation, $5,125 after draft three, and $3,625 upon submission to the journal.\n\nThe estimate covered all writing, editing, library research, copy editing, art work and coordination with the physicians and others who would be named as authors. Martin Keller would be listed as the main author. The first draft was ready by December 1998. SmithKline Beecham documents show that Laden and STI coordinated the entire publication process, including writing the cover letter to the journal that published the article, \"JAACAP\", which she sent to Keller with the instruction that he transfer it to his own letterhead.\n\nSTI first submitted the article to the \"Journal of the American Medical Association\" (\"JAMA\"), which rejected it in November 1999. Concerns cited by \"JAMA\" reviewers included that \"the main finding of the study is the high placebo response rate\". They also suggested that the named authors confirm they had been \"granted full access to the data set to verify the accuracy of the report\".\n\nEarly drafts of the paper for \"JAMA\" did not mention the serious adverse events (SAEs). A SmithKline Beecham scientist, James McCafferty, added a paragraph about these in July 1999, adding that 11 patients on paroxetine had experienced SAEs, against two on placebo: \"worsening depression, emotional lability, headache, and hostility were considered related or possibly related to treatment.\" This was changed in the final draft to: \"Of the 11 patients, only headache (1 patient) was considered by the treating investigator to be related to paroxetine treatment.\"\n\nIn December 1999 Laden submitted the rewritten paper to \"JAACAP\", led at the time by Mina K. Dulcan, editor-in-chief. According to Melanie Newman in the \"BMJ\", \"JAACAP's\" reviewers wrote that the results did not \"clearly demonstrate efficacy for paroxetine\", and asked whether, because of the high placebo response rate, SSRIs should be regarded as first-line therapy. \"JAACAP\" accepted the article in January 2001, and published it in July.\n\nThe article concluded: \"Paroxetine is generally well tolerated and effective for major depression in adolescents.\" McCafferty's paragraph about worsening depression and emotional lability possibly being related to the treatment had been removed. The only SAE attributed to paroxetine in the \"JAACAP\" article was in one patient who had reported headache. The article continued: \"Because these serious adverse events were judged by the investigator to be related to treatment in only 4 patients (paroxetine, 1; imipramine, 2; placebo, 1), causality cannot be determined conclusively.\" It concluded: \"The findings of this study provide evidence of the efficacy and safety of the SSRI, paroxetine, in the treatment of adolescent depression.\"\n\nGSK used the \"JAACAP\" article to promote paroxetine to doctors for use in their teenage patients. The drug had not been approved for use in children and adolescents. Drug companies are prohibited from promoting drugs for unapproved uses, but doctors are permitted to prescribe drugs for what is known as off-label use. In the UK 32,000 prescriptions of paroxetine were written for children and adolescents in 1999, and in the US that figure rose to 2.1 million in 2002, earning GSK $55 million.\n\nOn 7 August 2001 Sally Laden of STI, apparently the main author of the \"JAACAP\" article, arranged for GSK to buy 500 reprints of the article—300 for Keller and 200 for Zachary Hawkins of GSK's Paxil Product Management team—to be distributed to the company's neuroscience sales force. On 16 August 2001 Zachary Hawkins sent a memo about study 329 to \"All Sales Representatives Selling \"Paxil\"\", calling study 329 a \"cutting-edge,' landmark study\", the first to compare efficacy of a selective serotonin reuptake inhibitor and a tricyclic antidepressant with placebo in the treatment of depressed adolescents. \"\"Paxil\" demonstrates REMARKABLE Efficacy and Safety in the treatment of adolescent depression,\" he wrote.\n\nThe memo continued that paroxetine was \"significantly more effective than placebo\" on certain outcomes: \"\"Paxil\" was generally well tolerated in this adolescent population and most adverse events were not serious. The most common adverse events occurred at rates that were similar to rates in the placebo group.\" It ended with:\nIn conclusion, the findings of this study provide evidence of the efficacy and safety of \"Paxil\" in the treatment of adolescent depression. Here's another example of GlaxoSmithKline's commitment to Psychiatry by bringing forth 'cutting edge' scientific data. \"Paxil\" is truly a REMARKABLE product that continues to demonstrate efficacy, even in this understudied population.\"\n\nScottish reporter Shelley Jofre presented four investigative programmes on paroxetine for BBC \"Panorama\" between 2002 and 2007, including one devoted to study 329, \"Secrets of the Drug Trials\", in January 2007. The 2007 programme was based on thousands of internal company documents produced during lawsuits pursued against GSK by patients and families.\n\nJofre's interest in paroxetine was triggered by the July 2001 case of \"Timothy J. Tobin v. SmithKline Beecham Pharmaceuticals\" in the United States. The family of 60-year-old Donald Schell sued the company after Schell shot and killed his wife, daughter and baby granddaughter, then committed suicide, 48 hours after starting a course of paroxetine in 1998. A Wyoming jury awarded the plaintiffs $6.4 million.\n\nThe first of Jofre's programmes, \"The Secrets of Seroxat\", aired on 13 October 2002, and covered the Schell case, study 329, and GSK's efforts to market the drug for use in children. (At the time the Summary of Product Characteristics for paroxetine in Europe said that its use in children was \"not recommended as safety and efficacy have not been established in this population\".) Discussing study 329 and the paediatric use of paroxetine, Alistair Benbow, head of European psychiatry for GlaxoSmithKline, told Jofre that, during study 329, paroxetine had been \"generally well tolerated by this difficult to treat population\".\nTo examine the issues the \"Panorama\" had raised, Britain's Medicines and Healthcare Products Regulatory Agency (MHRA) set up an ad hoc group of experts, which held a meeting with GSK on 14 November 2002. The MHRA asked GSK about its clinical trials in children. GSK was planning to apply for pediatric indications for paroxetine. According to the MHRA, \"GSK did not raise any concern about lack of efficacy or adverse reactions in the clinical trials in the paediatric population at that meeting.\"\n\nJofre's second \"Panorama\" programme on paroxetine, \"Emails from the edge\" (11 May 2003) focused on the 67,000 calls and 1,400 e-mails the BBC received, after the first programme, from people taking the drug. They reported withdrawal symptoms, as well as acts of violence and self-harm that they believed were attributable to paroxetine. During this programme, Benbow told Jofre: \"We have been asked by the regulatory authorities to provide all our information related to suicides and I can tell you the data that we provide to them clearly shows no link between Seroxat and an increased risk of suicide—no link.\"\n\nIn February 2003 the MHRA's Committee on the Safety of Medicines (CSM) set up an Expert Working Group to investigate SSRIs and safety. In preparation for its first meeting, the MHRA met GSK on 21 May 2003 to make sure that GSK had supplied all information relevant to paroxetine and safety, and to discuss Jofre's second \"Panorama\" programme.\n\nToward the end of the meeting, GSK handed over a 79-page briefing paper, \"Paroxetine: Critical evaluation of paroxetine hydrochloride for the treatment of Paediatric Obsessive Compulsive Disorder and Social Anxiety Disorder in children and adolescents\", dated 20 May 2003. The paper included data from nine clinical trials GSK had conducted on paroxetine and children between April 1994 and September 2002:\n\nThe briefing paper concluded that \"analysis of the safety data demonstrates that paroxetine is generally well tolerated by paediatric patients ...,\" but suggested a label change to the effect that efficacy had not been established in children with major depressive disorder, and that adverse reactions could include hyperkinesia, hostility, emotional lability and agitation. The paper said these had occurred around twice as much in the paroxetine group than in those taking placebo.\n\nBy \"emotional lability\", the paper alluded in particular to suicidal thoughts and behaviour. Of 20 reports of adverse events in the paroxetine groups, 12 had been suidical thoughts or suicide attempts (none successful), three self-mutilation and five general emotional lability. There had been eight adverse events in the patients taking placebo, of which four were suicidal thoughts or behaviour, one self-mutilation and three emotional lability.\n\nThe paper suggested a label change regarding withdrawal symptoms, which it said had occurred with paroxetine at roughly twice the rate of placebo.\n\nAlasdair Breckenridge, then-chair of the MHRA, told \"Panorama\" that the GSK briefing document caused \"a very dramatic change in our thinking about Seroxat and children\". The MHRA asked GSK to submit the full clinical data, which they did on 27 May 2003. The data provided \"robust evidence\" of a causal link between paroxetine and suicidality, and no evidence that paroxetine was effective in treating depression in children. The MHRA wrote:\nOn examination of the full clinical trial data in children submitted by GSK urgently on 27 May 2003 in response to requests from the Agency, it became clear that the evidence base for the safety concern of an increased risk of suicidal behaviour was derived from pooled analysis of all the trials (a meta-analysis). It was only when the trials were analysed together that the safety issue became apparent. These trials had been conducted over a number of years and some had been published in part, however the publications gave an incomplete and partial picture of the full data. Importantly, the trials conducted in a range of conditions in children and adolescents failed to demonstrate that Seroxat was effective in the treatment of depressive illness.\n\nThe analysis suggested an increased rate of suicidal thinking and behaviour of 3.4 percent on paroxetine versus 1.2 percent on placebo. The committee concluded that the risks outweighed the benefits, and on 10 June 2003 issued an advisory to physicians not to prescribe paroxetine to the under-18s. The US Food and Drug Administration followed suit nine days later.\n\nThe MHRA launched a criminal inquiry in October 2003 into GSK's conduct. This was based on two concerns: (a) the length of time between the end of the trials and GSK's passing the safety concerns to the MHRA; and (b) the manner in which the material had been handed over. Rather than alerting the MHRA of a risk, GSK had supplied the data in relation to an application to extend the indications of paroxetine to children. The MHRA deemed this inappropriate for an urgent safety concern because of the length of time such applications can take.\n\nMedical ethicists Linsey Goey and Emily Jackson argued that the 1998 SmithKline Beecham position paper, in which the company said it had decided not to show studies 329 and 377 to regulators, represented a \"prima facie\" breach of the \"Medicines Act 1968\" and \"Medicines for Human Use Regulations\", which required pharmaceutical companies to pass to the regulator trial data that had safety and efficacy implications.\n\nThe MHRA reviewed around one million pages of documentation in the course of the inquiry. After a four-year investigation, independent counsel instructed by the MHRA advised, according to an MHRA report, that \"no offence ha[d] been committed contrary to the 1994 Regulations [\"Medicines for Human Use (Marketing Authorisations Etc.) Regulations 1994\"]\", because GSK's clinical trials and alleged failure to provide data from them \"most likely did not fall within the regime implemented by those Regulations\". If the 1994 Regulations did apply, the report said, the \"relevant provisions were not sufficiently clear so as to permit a criminal sanction for their breach\". The MHRA announced in March 2008 that there would be no prosecution. In October 2008 the 1994 Regulations were amended to prevent a repetition of the case.\n\nIn November 1995 Alison Bass of \"The Boston Globe\" began investigating Brown University's psychiatry department, chaired by Martin Keller, who led Study 329. There were allegations that the department had taken $218,000 of government funds for research that apparently had not been conducted. In October 1999 she reported Keller's financial relationship with the pharmaceutical industry, which included receipt of $500,000 in consulting fees the previous year. Bass's work developed into a book about GlaxoSmithKline, paroxetine, and Study 329, \"Side Effects: A Prosecutor, a Whistleblower, and a Bestselling Antidepressant on Trial\" (2008).\n\nIn March 2004 the FDA mandated that drug companies review the use of their SSRIs in children. In 2006 GSK researchers published a review of five of their trials involving paroxetine and adolescents or children, including study 329 and the unpublished study 377. They wrote that suicidal ideation or behaviour had occurred in 22 of 642 patients on paroxetine (3.4 percent) against five of 549 on placebo (0.9 percent). The article concluded: \"Adolescents treated with paroxetine showed an increased risk of suicide-related events. ... The presence of uncontrolled suicide risk factors, the relatively low incidence of these events, and their predominance in adolescents with MDD make it difficult to identify a single cause for suicidality in these pediatric patients.\n\nIn June 2004 New York State Attorney Eliot Spitzer filed a lawsuit against GSK in the New York State Supreme Court for having withheld clinical trial data about paroxetine, including from study 329. GSK denied any wrongdoing and said it had disclosed the data to regulators, and to physicians at medical conventions and in other ways.\n\nGSK settled the case in August 2004, agreeing to pay $2.5 million, make its trial data about paroxetine and children available on its website, and establish a clinical trial register that would host summaries of all company-sponsored trials going back to 27 December 2000. By October 2004 other drug companies, including Pfizer, Eli Lilly and Merck, had agreed to create their own registers. In 2013 GSK joined AllTrials, a British campaign to have all clinical trials registered and the results reported.\n\nBy 2009 GSK had paid almost $1 billion to settle paroxetine-related lawsuits related to 450 suicides, withholding data, as well as addiction, antitrust and other claims. An additional 600 unsettled claims related to birth defects. The lawsuits produced thousands of internal company documents, some of which entered the public domain. These formed the basis of some of Alison Bass's work and that of Shelley Jofre for the BBC.\n\nIn October 2011 the United States Department of Justice filed a lawsuit under the False Claims Act accusing GSK of promoting drugs for unapproved uses, failing to report safety data, reporting false prices to Medicaid, and paying kickbacks to physicians in the form of gifts, trips and sham consultancy fees. The complaint included preparing the \"JAACAP\" article about study 329, exaggerating paroxetine's efficacy while downplaying the risks, and using the article to promote the drug for adolescent use, which was not approved by the FDA.\n\nGSK pleaded guilty in 2012 and paid a $3 billion settlement, including a criminal fine of $1 billion. The fine included an amount for \"preparing, publishing and distributing a misleading medical journal article that misreported that a clinical trial of Paxil demonstrated efficacy in the treatment of depression in patients under age 18, when the study failed to demonstrate efficacy\".\n\nChild psychiatrist Jon Jureidini of the Women's and Children's Hospital in Adelaide and Ann Tonkin of the University of Adelaide asked \"JAACAP\" in 2003 to retract the study 329 paper.\n\nIn 2005 the philosopher Leemon McHenry complained to \"JAACAP's\" editor, Mina Dulcan, that Keller and some of the other researchers named as authors had worked for GSK but had not declared their conflict of interest and had violated the journal's policy regarding authorship. Keller had acted as a consultant for several drug companies. \"The Boston Globe\" reported in 1999 that he had earned $500,000 the previous year from consultancy work, which, the newspaper said, he did not disclose to the journals that published his work or to the American Psychiatric Association. Dulcan replied to McHenry that \"unless there is a specific accusation of research fraud, it is not the role of scientific journals to police authorship.\" \nJureidini and McHenry called again for the paper's retraction in 2009. Editor-in-chief Andrés Martin replied that there was no justification for retraction, and that the journal had \"conformed to the best publication practices prevailing at the time\". In April 2013 Jureidini asked GSK's CEO Andrew Witty to request retraction.\n\nIn July 2013 Jureidini announced his intention to produce a new write-up of study 329 in accordance with the RIAT initiative (restoring invisible and abandoned trials). The RIAT researchers—Joanna Le Noury, John M. Nardo, David Healy, Jon Jureidini, Melissa Raven, Catalin Tufanaru, and Elia Abi-Jaoude—published their re-analysis in the \"BMJ\" in September 2015. They concluded that \"[t]he efficacy of paroxetine and imipramine was not statistically or clinically significantly different from placebo for any prespecified primary or secondary efficacy outcome,\" and that there were \"clinically significant increases in ... suicidal ideation and behaviour and other serious adverse events in the paroxetine group and cardiovascular problems in the imipramine group.\"\n\nIn 2007 the FDA required that all anti-depressants include a boxed warning of an increased risk of suicidal thoughts and behaviour in young adults (18–24 years) during the first one to two months of treatment. A 2012 Cochrane review on the use of SSRIs in children and adolescents concluded that there is evidence of an increased suicide risk in patients treated with antidepressants. It added: \"However, given the risks of untreated depression in terms of completed suicide and impacts on functioning, if a decision to use medication is agreed, then fluoxetine might be the medication of first choice given guideline recommendations.\"\n\n\nLetters\n\n"}
{"id": "2663672", "url": "https://en.wikipedia.org/wiki?curid=2663672", "title": "Swell (ocean)", "text": "Swell (ocean)\n\nA swell, in the context of an ocean, sea or lake, is a series of mechanical waves that propagate along the interface between water and air and thus are often referred to as surface gravity waves. These series of surface gravity waves are not wind waves, which are generated by the immediate local wind, but instead are generated by distant weather systems, where wind blows for a duration of time over a fetch of water. More generally, a swell consists of wind-generated waves that are not—or are hardly—affected by the local wind at that time. Swell waves often have a long wavelength, but this varies due to the size, strength and duration of the weather system responsible for the swell and the size of the water body. Swell wavelength also varies from event to event. Occasionally, swells which are longer than 700 m occur as a result of the most severe storms. Swells have a narrower range of frequencies and directions than locally generated wind waves, because swell waves have dispersed from their generation area, have dissipated and therefore lost an amount of randomness, taking on a more defined shape and direction. Swell direction is the direction from which the swell is coming. It is measured in degrees (as on a compass), and often referred to in general directions, such as a NNW or SW swell.\n\nLarge breakers observed on a beach may result from distant weather systems over a fetch of ocean. Five factors influence the formation of wind waves which will go on to become ocean swell:\n\nAll of these factors work together to determine the size of wind waves:\n\n\nA fully developed sea has the maximum wave size theoretically possible for a wind of a specific strength, duration, and fetch. Further exposure to that specific wind could only cause a loss of energy due to the breaking of wave tops and formation of \"whitecaps\". Waves in a given area typically have a range of heights. For weather reporting and for scientific analysis of wind wave statistics, their characteristic height over a period of time is usually expressed as \"significant wave height\". This figure represents an average height of the highest one-third of the waves in a given time period (usually chosen somewhere in the range from 20 minutes to twelve hours), or in a specific wave or storm system. The significant wave height is also the value a \"trained observer\" (e.g. from a ship's crew) would estimate from visual observation of a sea state. Given the variability of wave height, the largest individual waves are likely to be somewhat less than twice the reported significant wave height for a particular day or storm.\n\nWind waves are generated by many kinds of disturbances such as seismic events, gravity, and crossing wind. The generation of wind waves is initiated by the disturbances of cross wind field on the surface of the water. Two major mechanisms of surface wave formation by winds (the Miles-Phillips Mechanism) and other sources (ex. earthquakes) of wave formation can explain the generation of wind waves.\nHowever, if one set a flat water surface (Beaufort Point,0) and abrupt cross wind flows on the surface of the water, then the generation of surface wind waves can be explained by following two mechanisms which are initiated by normal pressure fluctuations of turbulent winds and parallel wind shear flows.\n\n1) Starts from \"Fluctuations of wind\" (O.M.Phillips) : the wind wave formation on water surface by wind is started by a random distribution of normal pressure acting on the water from the wind. By the mechanism developed by O.M. Phillips (in 1957), the water surface is initially at rest and the generation of wave is\ninitiated by adding turbulent wind flows and then, by the fluctuations of the wind, normal pressure acting on the water surface. This pressure fluctuation arise normal and tangential stresses to the surface water, and generates wave behavior on the water surface.\n1. water originally at rest\n2. water is inviscid \n3. Water is irrotational \n4. Random distribution of normal pressure to the water surface from the turbulent wind\n\n2) starts from \"wind shear forces\" on the water surface (J.W.Miles, applied to mainly 2D deep water gravity waves) ; John W. Miles suggested a surface wave generation mechanism which is initiated by turbulent wind shear flows Ua(y), based on the inviscid Orr-Sommerfeld equation in 1957. He found the energy transfer from wind to water surface as a wave speed, c is proportional to the curvature of the velocity profile of wind Ua’’(y) at point where the mean wind speed is equal to the wave speed (Ua=c, where, Ua is the Mean turbulent wind speed). Since the wind profile Ua(y) is logarithmic to the water surface, the curvature Ua’’(y) have negative sign at the point of Ua=c. This relations show the wind flow transferring its kinetic energy to the water surface at their interface, and arises wave speed, c.\n\nthe growth-rate can be determined by the curvature of the winds ((d^2 Ua)/(dz^2 )) at the steering height (Ua (z=z_h)=c) for a given wind speed Ua\n1. 2D parallel shear flow, Ua(y)\n2. incompressible, inviscid water / wind\n3. irrotational water\n\nGenerally, these wave formation mechanisms occur together on the ocean surface and arise wind waves and grows up to the fully developed waves.\n\nFor example,\n\nIf we suppose a very flat sea surface (Beaufort number, 0), and sudden wind flow blows steadily across the sea surface, physical wave generation process will be like;\n\n1. Turbulent wind flows form random pressure fluctuations at the sea surface. Small waves with a few centimeters order of wavelengths is generated by the pressure fluctuations. (The Phillips mechanism)\n\n2. The cross wind keeps acting on the initially fluctuated sea surface, then the waves become larger. As the waves become larger, the pressure differences increase along with the wave growth, then the wave growth rate increases. Then the shear instability expedites the wave growth exponentially. (The Miles mechanism)\n\n3. The interactions between the waves on the surface generate longer waves (Hasselmann et al., 1973) and the interaction will transfer wave energy from the shorter waves generated by the Miles mechanism to the waves have slightly lower frequencies than the frequency at the peak wave magnitudes, then finally the waves will be faster than the cross wind speed (Pierson & Moskowitz).\n\nThe dissipation of swell energy is much stronger for short waves, which is why swells from distant storms are only long waves. The dissipation of waves with periods larger than 13 seconds is very weak but still significant at the scale of the Pacific Ocean. These long swells lose half of their energy over a distance that varies from over 20,000 km (half the distance round the globe) to just over 2,000 km. \nThis variation was found to be a systematic function of the swell steepness: the ratio of the swell height to the wavelength. The reason for this behaviour is still unclear, but it is possible that this dissipation is due to the friction at the air-sea interface.\n\nSwells are often created by storms thousands of nautical miles away from the beach where they break, and the propagation of the longest swells is only limited by shorelines. For example, swells generated in the Indian Ocean have been recorded in California after more than half a round-the-world trip. This distance allows the waves comprising the swells to be better sorted and free of chop as they travel toward the coast. Waves generated by storm winds have the same speed and will group together and travel with each other, while others moving at even a fraction of a metre per second slower will lag behind, ultimately arriving many hours later due to the distance covered. The time of propagation from the source \"t\" is proportional to the distance \"X\" divided by the wave period \"T\". In deep water it is formula_1 where g is the acceleration of gravity. For a storm located 10,000 km away, swells with a period \"T\"=15 s will arrive 10 days after the storm, followed by 14 s swells another 17 hours later, and so forth.\n\nThis dispersive arrivals of swells, long periods first with a reduction in the peak wave period over time, can be used to tell the distance at which swells were generated.\n\nWhereas the sea state in the storm has a frequency spectrum with more or less the same shape (i.e. a well defined peak with dominant frequencies within plus or minus 7% of the peak), the swell spectra are more and more narrow, sometimes as 2% or less, as waves disperse further and further away. The result is that wave groups (called sets by surfers) can have a large number of waves. From about seven waves per group in the storm, this rises to 20 and more in swells from very distant storms.\n\nJust like for all water waves, the energy flux is proportional to the significant wave height squared times the group velocity. In deep water, this group velocity is proportional to the wave period. Hence swells with longer periods can transfer more energy than shorter wind waves. Also, the amplitude of infragravity waves increases dramatically with the wave period (approximately the square of the period), which results in \nhigher run-up.\n\nAs swell waves typically have long wavelengths (and thus a deeper wave base), they begin the refraction process (see water waves) at greater distances offshore (in deeper water) than locally generated waves.\n\nSince swell-generated waves are mixed with normal sea waves, they can be difficult to detect with the naked eye (particularly away from the shore) if they are not significantly larger than the normal waves. From a signal analysis point of view, swells can be thought of as a fairly regular (though not continual) wave signal existing in the midst of strong noise (i.e., normal waves and chop).\n\nSwells were used by Polynesian navigators to maintain course when no other clues were available, such as on foggy nights.\n\n"}
{"id": "44409131", "url": "https://en.wikipedia.org/wiki?curid=44409131", "title": "Technology transfer in computer science", "text": "Technology transfer in computer science\n\nTechnology transfer in computer science refers to the transfer of technology developed in computer science or applied computing research, from universities and governments to the private sector. These technologies may be abstract, such as algorithms and data structures, or concrete, such as open source software packages.\n\nNotable examples of technology transfer in computer science include:\n"}
{"id": "20072948", "url": "https://en.wikipedia.org/wiki?curid=20072948", "title": "Tone (magazine)", "text": "Tone (magazine)\n\nTone was a bi-monthly magazine combining coverage of technological developments in New Zealand and from around the world with reviews on the latest consumer products available in New Zealand. \n\nIt was Parkside Media's third magazine, following \"NZ Classic Car\" and \"NZ Performance Car\". \"Tone\" was started in 1999. Until issue 32, the magazine was bi-monthly. A change was made to monthly, but as of issue 73 (November/December 2008), it returned to bi-monthly.\n\n\"Tone\"'s offices were in Grey Lynn, Auckland, New Zealand. The magazine ceased publication in December 2011.\n\n\"Tone\"'s logo featured a small coloured triangle. Internally this was called 'Jerry'. Jerry changes colour each issue to match the cover design. Version 2 of the logo appeared in issue 32 when the magazine changed to monthly. Version 3 of the logo appeared in issue 44. Version 4 of the logo appeared in issue 67 and changed the tagline to \"Gadgets | Hi-fi | Home theatre\", from \"Technology to change your life\".\n\nAs of the November/December 2008 issue, the typical magazine contents included:\n\n"}
{"id": "18802919", "url": "https://en.wikipedia.org/wiki?curid=18802919", "title": "Wave shoaling", "text": "Wave shoaling\n\nIn fluid dynamics, wave shoaling is the effect by which surface waves entering shallower water change in wave height. It is caused by the fact that the group velocity, which is also the wave-energy transport velocity, changes with water depth. Under stationary conditions, a decrease in transport speed must be compensated by an increase in energy density in order to maintain a constant energy flux. Shoaling waves will also exhibit a reduction in wavelength while the frequency remains constant.\n\nIn shallow water and parallel depth contours, non-breaking waves will increase in wave height as the wave packet enters shallower water. This is particularly evident for tsunamis as they wax in height when approaching a coastline, with devastating results.\n\nWaves nearing the coast change wave height through different effects. Some of the important wave processes are refraction, diffraction, reflection, wave breaking, wave–current interaction, friction, wave growth due to the wind, and \"wave shoaling\". In the absence of the other effects, wave shoaling is the change of wave height that occurs solely due to changes in mean water depth – without changes in wave propagation direction and dissipation. Pure wave shoaling occurs for long-crested waves propagating perpendicular to the parallel depth contour lines of a mildly sloping sea-bed. Then the wave height formula_1 at a certain location can be expressed as:\nwith formula_3 the shoaling coefficient and formula_4 the wave height in deep water. The shoaling coefficient formula_3 depends on the local water depth formula_6 and the wave frequency formula_7 (or equivalently on formula_6 and the wave period formula_9). Deep water means that the waves are (hardly) affected by the sea bed, which occurs when the depth formula_6 is larger than about half the deep-water wavelength formula_11 \n\nFor non-breaking waves, the energy flux associated with the wave motion, which is the product of the wave energy density with the group velocity, between two wave rays is a conserved quantity (i.e. a constant when following the energy of a wave packet from one location to another). Under stationary conditions the total energy transport must be constant along the wave ray – as first shown by William Burnside in 1915.\nFor waves affected by refraction and shoaling (i.e. within the geometric optics approximation), the rate of change of the wave energy transport is:\nwhere formula_13 is the co-ordinate along the wave ray and formula_14 is the energy flux per unit crest length. A decrease in group speed formula_15 and distance between the wave rays formula_16 must be compensated by an increase in energy density formula_17. This can be formulated as a shoaling coefficient relative to the wave height in deep water.\n\nFor shallow water, when the wavelength is much larger than the water depth, wave shoaling satisfies Green's law:\nwith formula_6 the mean water depth, formula_1 the wave height and formula_21 the fourth root of formula_22\n\nFollowing Phillips (1977) and Mei (1989), denote the phase of a wave ray as \nThe local wave number vector is the gradient of the phase function, \nand the angular frequency is proportional to its local rate of change, \nSimplifying to one dimension and cross-differentiating it is now easily seen that the above definitions indicate simply that the rate of change of wavenumber is balanced by the convergence of the frequency along a ray; \nAssuming stationary conditions (formula_27), this implies that wave crests are conserved and the frequency must remain constant along a wave ray as formula_28.\nAs waves enter shallower waters, the decrease in group velocity caused by the reduction in water depth leads to a reduction in wave length formula_29 because the nondispersive shallow water limit of the dispersion relation for the wave phase speed, \ndictates that \ni.e., a steady increase in \"k\" (decrease in formula_32) as the phase speed decreases under constant formula_33.\n\n"}
{"id": "45436748", "url": "https://en.wikipedia.org/wiki?curid=45436748", "title": "Western Indian Ocean Marine Science Association", "text": "Western Indian Ocean Marine Science Association\n\nWestern Indian Ocean Marine Science Association (WIOMSA) is a regional professional, non-governmental, non-profit, membership organization, registered in Zanzibar, Tanzania. The organization is dedicated to promoting the educational, scientific and technological development of all aspects of marine sciences throughout the region of Western Indian Ocean (Somalia, Kenya, Tanzania, Mozambique, South Africa, Comoros, Madagascar, Seychelles, Mauritius, Réunion (France)), with a view toward sustaining the use and conservation of its marine resources. The association has about 1000 individual members as well as about 50 institutional members from within and outside the region.\n\nThe organization's inter-disciplinary memberships consist of marine scientists, coastal practitioners, and institutions involved in the advancement of marine science research and development. The association:\n\n\nWIOMSA promotes marine science research through the award of research grants under the Marine Science for Management (MASMA) and the Marine Research Grant (MARG) programmes. MASMA is a competitive research grant scheme designed to support research activities in the region as well as organisation of training courses/workshop.\n\nMarine and coastal management requires scientific and technical information on both natural (physical, chemical, biological, etc.) and social (institutions, knowledge, perceptions, economic and cultural values, etc.) processes to identify and define priority environment issues as well as to define alternative solutions and strategies. It is therefore essential that appropriate environmental information is available for assessment of impacts of existing and planned activities, and that a sound scientific base exists which can accommodate the changing needs of environmental management institutions as well as society at large. However, experience shows that knowledge generated is often not effectively used in management processes.\n\nIn the past two decades, the Western Indian Ocean (WIO) region has seen a significant increase in research studies conducted on different aspects of the marine and coastal environment at a local, national or regional level. This has led to the strengthening of the knowledge base within these environments and increased awareness of important marine and coastal issues. Research capacity has also been strengthened in many disciplines.\n\nAt both national and regional levels, a number of important initiatives have been put in place in the recent past, seeking to improve both understanding and management of the marine and coastal environment. Governments’ efforts, together with those of donor-supported projects and programmes, have allowed countries in the region to considerably strengthen the management of their marine and environment. The cumulative effect of such initiatives is evidenced, for example, by the fact that all the governments in the WIO region have initiated the implementation of integrated coastal zone management to a greater or lesser extent, as well as the process of formulating policies and legislation focusing on these marine and coastal environments. In many cases governments have established dedicated units for dealing with marine and coastal issues. In addition, a number of marine protected areas have been established in priority areas.\n\nDespite these efforts, the management of the marine and coastal environment in the WIO remains a challenge. While most countries in the WIO region have put in place policy, legal, regulatory and institutional frameworks that are relevant to the protection and management of the marine and coastal environment, most countries have not succeeded in reversing the trend of degradation in coastal and marine ecosystems. This is attributed to, amongst others factors, sometimes inappropriate, ineffective and/or inadequate governance structures. These weaknesses in governance are reflected as policy and legislative inadequacies; limited institutional capacity; inadequate awareness; inadequate financial mechanisms; and poor knowledge management \n\nResearch in the WIO region is undertaken mainly by Universities, Government-affiliated research institutes, national and regional NGOs, and by scientists from outside of the region. With the exception of the research initiatives undertaken by Government-affiliated research institutes, research by other organizations is not necessarily aligned to the needs of management authorities. The research agenda of Government-affiliated research institutes such as the Kenya Marine and Fisheries Research Institute (KMFRI); Tanzania Fisheries Research Institute (TAFIRI) and IIP of Mozambique should generally be guided by the management objectives of the departments or Ministries to which they are affiliated.\n\nThe strategic objective and priorities of WIOMSA are:\n\n\nBased on these priorities, WIOMSA work initially emphasised three programme areas i.e. scientific research, capacity building and communication & extension; with more focus recently on capacity development, information dissemination, partnerships and networking. In this regard, WIOMSA’s activities fall within five broad programmatic areas:\n\n\nOverall goal of the association’s capacity development programme is to develop technical and managerial capacity as well as professionalism, and build the capability of scientists and practitioners to meet the existing and future challenges of coastal and marine management in the WIO region and beyond. WIOMSA and its partners have been at the forefront of initiating innovative and pioneering capacity building programme, which have often been replicated in other regions. The capacity development programme includes short and long-term training courses, development and dissemination of training materials and tools, and the provision of technical support to organizations e.g. recent international training course in mangrove ecosystems (http://inweh.unu.edu/mangrove-wio-region/). Through its Marine and Coastal Science for Management (MASMA) Programme, WIOMSA has supported experts from the region to pursue MSc and PhD degrees, while the Marine Research Grants (MARG) Program has provided partial support to individuals registered at universities within the region to enable travel for research purposes.\n\n\nWIOMSA uses two competitive grant programmes, MASMA and the MARG as mechanisms to address priority issues, improving research quality and developing research capacity in all relevant disciplines. MASMA is designed to support regional research activities as well as organisation of training courses/workshop and publication of books/manuals.\n\nThere are two competitive research grants under the MASMA Programme, the ‘Open’ Competitive research grant programme which covers any topic within the priority themes as suggested by applicants, and the ‘Commissioned’ Competitive research grant programme which covers specific topics that emerge as critical during the course of the Programme. Ideas for ‘Commissioned’ grants come from the Programme Committee.\n\nMARG aims at providing young and upcoming scientists with a reliable and flexible mechanism to turn their ideas into research projects and also offers opportunity for presentation of research results at various regional and international fora. There are three types of MARG grants, MARG I (research), MARG II (travel to laboratories outside your country for data analysis) and MARG III (travel grant to attend conferences).\n\n\nInformation dissemination and communication are core activities of WIOMSA and are integrated into all objectives of the association. WIOMSA’s approaches to information dissemination and communication include publication of books/manuals, policy briefs , regional Journal/peer-reviewed papers , newsletters, magazines , flyers and brochures and production of CD-ROMS, DVDs, and TV programmes designed to serve the needs of a wide range of audiences. The WIOMSA website and blog provide news and announcements, while events such as the biennial WIOMSA Scientific Symposium serve as major hubs for exchange and dissemination of information.\n\n\nWIOMSA promotes partnerships/networks through a number of approaches such as linking multiple scientific domains e.g. connecting social and natural sciences through the competitive grant programme; institutional partnerships beyond the academic sphere where a number of science to policy initiatives have been facilitated e.g. through WIO-C, a network of implementing agencies active in marine conservation, Nairobi Convention (Nairobi Convention UNEP/Nairobi Convention Secretariat and WIOMSA, 2009); and linkages across geographic boundaries e.g. WIO-COMPAS Programme with the Coastal Resource Center of the University of Rhode Island (http://wiomsa.net/wiocompas/) and Memoranda of Understanding with different organizations e.g. Odinafrica.\n\nWIOMSA provides opportunities through its activities for networking amongst researchers and between researchers and other stakeholders in the region. Most of WIOMSA’s activities are carried out in partnership with national institutions and regional and international organizations e.g. The Nairobi Convention Nairobi Convention\n\nThe Marine and Coastal Science for Management (MASMA) programme is the regions’ first competitive research grant mechanism and was established in 2000 to provide funding and technical support for coastal and marine research, capacity development, and communication of research results in the WIO region. The MASMA programme aims to contribute to the knowledge base on the coastal and marine environments of the region, to raise awareness on important issues, to conduct and coordinate research activities of national and regional importance, as well as to disseminate information and data in support of the sustainable use of coastal and marine resources. The MASMA programme is also seen as an effective means to bolster and maintain regional research capacity in key research areas central to sustainable coastal resource management in the WIO region.\n\nMASMA encourages and supports multi/trans-disciplinary research efforts and promotes regional collaboration in research amongst experts from the different countries in the region and partnerships between scientists from within and outside the region. It also aims at building professionalism and competence amongst regional researchers in designing and coordinating regional research projects.\n\nMASMA provides an exceptional foundation for addressing the pressing environmental issues of the region in a collaborative and structured manner. As a regional programme, MASMA provides a number of benefits:\n\nWith limited human capacity and financial resources to fund research at the national level, MASMA as a regional programme provides an opportunity for countries in the region to access funds and human capacity that they would not have been able to mobilize in normal circumstances.\n\nThe MASMA programme contributes towards facilitating access for regional experts to international research and networks leading to the internationalization of their institutions and research work.\n\nBetween 2000 and 2011 WIOMSA supported at least 40 regional projects, which have contributed to the increased knowledge base on the coastal and marine environments of the region and raised awareness on important management issues. However, outputs and outcomes of these projects are not well known outside project teams as there has been no strategy both at the project and WIOMSA level to use these projects as flagships for the purpose of raising their visibility as well as that of WIOMSA.\n\nIn the current phase, six projects have been approved so far covering topics ranging from adaptation to climate change at local government and community level; integration of science into MPA management; and linking marine science, traditional knowledge and cultural perceptions to develop future marine management frameworks using spatial simulation tools and educational games; identification and effective integrated management of dugongs and their habitats; use of the adaptive management approach to build the capacity of communities to implement fisheries management interventions; and assessment of the scale and impacts of bycatch in fisheries across the region.\n\nThese projects are not only multi-disciplinary with involvement of social and natural scientists working at different levels from the community to regional levels, but are implemented in all countries of the WIO except Somalia, and involve management authorities, research institutions, non-governmental organizations and the private sector. Further, all these projects have developed coherent communication and dissemination plans outlining how their outputs and results will be communicated to the potential end users including the general public, policy makers and the scientific community.\n\n"}
{"id": "1683995", "url": "https://en.wikipedia.org/wiki?curid=1683995", "title": "Émile Blanchard", "text": "Émile Blanchard\n\nCharles Émile Blanchard (6 March 1819 – 11 February 1900) was a French zoologist and entomologist.\n\nBlanchard was born in Paris. His father was an artist and naturalist and Émile began natural history very early in life. When he was 14 years old, Jean Victoire Audouin (1797—1841), allowed him access to the laboratory of the Muséum national d'Histoire naturelle. In 1838, he became a technician or \"préparateu\"r in this then, as now, famous institution. In 1841, he became assistant-naturalist.\n\nHe accompanied Henri Milne-Edwards (1800—1885) and Jean Louis Armand de Quatrefages de Breau (1810—1892) to Sicily on a marine zoology expedition. He published, in 1845 a \"Histoire des insectes\", or History of the insects and, in 1854—1856 \"Zoologie agricole\" or Agricultural Zoology. This last work is remarkable: it presents in a precise way the harmful or pest species and the damage they cause to various crop plants. This work was illustrated by his father. Blanchard was critical of Darwinism. He argued that Charles Darwin's pigeon studies were unscientific and that his ideas about evolution were false and unoriginal.\n\nIn 1870, Blanchard and Charles-Philippe Robin opposed the election of Darwin as a corresponding member of the French Academy of Sciences.\n\nHe published an atlas of the anatomy of the vertebrates which appeared between 1852 and 1864. This publication raised his hopes to obtain the chair of reptiles and fish at the Natural History Museum left vacant by the death of Auguste Duméril (1812—1870) but it was finally Léon Vaillant (1834—1914) who was selected. However, in 1862, he was given the chair of natural history of Crustacea, Arachnida and Insects. He left this in 1894 following his infirmity. He was elected, in 1862 into the Academy of Science. He began to lose his sight after 1860 and became blind in 1890. He died in Paris.\n\n"}
