{"id": "10131345", "url": "https://en.wikipedia.org/wiki?curid=10131345", "title": "Abutter", "text": "Abutter\n\nAn abutter is a person (or entity) whose property is adjacent to the property of another. In jurisdictions such as Massachusetts, New Hampshire, and Nova Scotia, it is a defined legal term. Some jurisdictions, such as Virginia, may use the term adjacent landowner, while others, such as California, use the term adjoining landowner, and the United States Environmental Protection Agency defines rights of contiguous property owners (CPO).\nIn land use regulations, concerns of an abutter may be given special attention, being the one most likely to suffer specific harm from a hasty, uninformed decision. For example, a developer requesting a subdivision may be required to notify (or pay to notify) all abutters of the proposal and invite them to a public hearing. Regulations may also provide an abutter with the right to be heard at the hearing, unlike others who must request permission to be heard, at the discretion of the board.\n\nIn the spirit of land use politics, even the unified voices of the concerned abutters may sound only faintly against the machinery of \"progress\" or well-funded special interests. However, the courts will objectively consider a proper case brought by an abutter whose rights have been arguably under-appreciated. Generally, the more abutters interested in a project, the more likely someone will object to it. \n\nSome regulations otherwise expand or limit the participation of local owners, as where notice may be required for \"anyone whose property is within 200 feet of any point of the parcel under consideration.\" Another expansive definition would include those whose properties are across a public way or flowing waterway, where the parcels do not actually touch. Contrarily, regulations may define \"abutter\" to include only those people who hold record title to an adjacent parcel, thus undermining the rights of tenants, associations and partial owners (e.g., mineral rights and easement owners) to be notified, let alone heard on a proposal. This would also eliminate participation of owners of unrecorded title, such as adverse possession or those who have simply failed to record a deed or settle an estate involving the adjacent property.\n"}
{"id": "12486885", "url": "https://en.wikipedia.org/wiki?curid=12486885", "title": "Adaptability", "text": "Adaptability\n\nAdaptability ( \"fit to, adjust\") is a feature of a system or of a process. This word has been put to use as a specialised term in different disciplines and in business operations. Word definitions of adaptability as a specialised term differ little from dictionary definitions. According to Andresen and Gronau adaptability in the field of organizational management can in general be seen as an ability to change something or oneself to fit to occurring changes. In ecology, adaptability has been described as the ability to cope with unexpected disturbances in the environment. \n\nWith respect to business and manufacturing systems and processes, adaptability has come to be seen increasingly as an important factor for their efficiency and economic success. In contrast, adaptability and efficiency are held to be in opposition to each other in biological and ecological systems, requiring a trade-off, since both are important factors in the success of such systems. To determine the adaptability of a process or a system, it should be validated concerning some criteria.\n\nIn the life sciences the term adaptability is used variously. At one end of the spectrum, the ordinary meaning of the word suffices for understanding. At the other end, there is the term as introduced by Conrad, referring to a particular information entropy measure of the biota of an ecosystem, or of any subsystem of the biota, such as a population of a single species, a single individual, cell, protein or gene.\n\nIn the technical research field this feature has been considered only since the late 1990s. H. P. Wiendahl first introduced adaptability as a necessary feature of a manufacturing system in 1999. The need to consider adaptability arose in the context of factory planning, where it is an objective to develop modular, adaptable systems. It has now become an important consideration for manufacturing and system engineers.\n\nAdaptability is to be understood here as the ability of a system (e.g. a computer system) to adapt itself efficiently and fast to changed circumstances. An adaptive system is therefore an open system that is able to fit its behaviour according to changes in its environment or in parts of the system itself. That is why a requirement to recognize the demand for change without any other factors involved can be expressed.\n\n"}
{"id": "5343419", "url": "https://en.wikipedia.org/wiki?curid=5343419", "title": "Affectional bond", "text": "Affectional bond\n\nIn psychology, an affectional bond is a type of attachment behavior one individual has for another individual, typically a caregiver for her or his child, in which the two partners tend to remain in proximity to one another. The term was coined and subsequently developed over the course of four decades, from the early 1940s to the late 1970s, by psychologist John Bowlby in his work on attachment theory. The core of the term \"affectional bond\", according to Bowlby, is the attraction one individual has for another individual. The central features of the concept of affectional bonding can be traced to Bowlby's 1958 paper, \"The Nature of the Child's Tie to his Mother\".\n\nBowlby referred to attachment bonds as a specific type of \"affectional\" bond, as described by him and developmental psychologist Mary Ainsworth. She established five criteria for affectional bonds between individuals, and a sixth criterion for attachment bonds:\n\nAn attachment bond has an additional criterion: the person seeks security and comfort in the relationship.\n\nBowlby believed that there were four distinguishing characteristics of attachment. These included:\n\n\nBowlby's thoughts on distinguishing attachment led the way for many other psychologists' viewpoints on attachment and the various bonds that attachment coincides with.\n\nChild psychologist Mary Ainsworth further expanded on Bowlby's research by conducting an experiment that is known as the \"Strange Situation\" experiment. In the experiment, a parent and child are alone in the room. A stranger then walks into the room and talks to the parent. After some amount of time, the parent quietly exits the room, leaving the stranger and child in the room. The child then reacts to the situation at hand and then the parent re-enters the room and comforts the child. From this groundbreaking study, Ainsworth developed different forms of attachment styles that infants display with the people they are close to.\n\nSecure attachment in infancy was characterized by noticeable distress when the parent left the room. When the parent returned, the child was extremely happy to see them. Infants are more likely to choose a parent over any other person, especially a stranger. As they embark on childhood, those who maintain secure attachment have an easier time making friends and meeting new people and hold a trustworthy bond with their parents. Adolescents benefit from parental support, but they are also beginning to make the transitions from relying heavily on their parents to a more independent environment with more freedom. In adulthood, they are more likely to have long-lasting relationships, high self-esteem, find pleasure from romantic relationships and are able to easily talk with their partners.\n\nAmbivalent attachment is characterized by a strong wariness towards strangers. Children get extremely uncomfortable when they do not have a noticeable face in the room. When the parent returns, the child receives no comfort from their return and is inconsolable. In childhood, these children tend to act \"clingy\" towards their parents and tend to heavily rely on others. In adulthood, they have difficulty with trust and feel that their partner does not exhibit the same feelings towards them. Insecurely attached adults tend to act cold and experience many break-ups throughout their life.\n\nInfants and children with avoidant attachment tend to show no preference between their caregivers and the stranger. They do not actively seek much comfort of contact from their parents and usually tend to avoid them. In adulthood, those with avoidant attachment have difficulty maintaining relationships due to the inability to display emotions. They are more likely to engage in casual sex and think about other people or things while they are having sex. Finally, they are not likely to be supportive when other people are in stressful situations and have an inability to look at ideas in another person's perspective.\n\nDisorganized Attachment in infants tends to display avoidant and unsure behavior. They tend to be in a daze and seem confused about the situation that they are in. They tend not show any clear signs of attachment at any point in their lives.\n\nThis fourth form of attachment was observed in later studies conducted by Main and Solomon. Numerous studies have supported Ainsworth's observations. These studies have also shown that attachment styles developed in infancy can influence behavior later in life. Children in this group tend to take on a parental role and act like a caregiver toward their parent. They display an overall inconsistent form of behavior. Research by Main and Hesse showed that parents who use tactics of fear and assurance contribute to this disorganized form of attachment.\n\nAccording to Bowlby's ideas of attachment, goal-corrected partnership is the last stage that a child experiences. It usually happens around age three. As the child begins spending more time with their caregiver, they begin to realize that their caregiver has goals and ideas of their own that may not concern the child. Because of this, the child begins to \"mold their behavior in order to please or impress the caregiver\". This type of bond is most likely to occur between the infant and the mother.\n\nCaregivers play an important role in children's lives for several reasons. It is important for the child to have an affectional bond with the person who is caring for that child. According to Bowlby, caregivers can be anyone who is caring for the child but is usually the mother or father of that child. Children place a high value on their relationship with their parents and will alter their behavior to meet the desired behavior from their parents. Bowlby explains by the time the child has reached twelve months of age the child has developed a strong relationship with his mother. Freud who is cited in Bowlby's article \"The Nature of the Child's Tie to his Mother\" says that a child's first love is a satisfaction of the need for food and an object for food, so either the mother's breast or bottle of milk.\n\nBowlby has four theories that explain how the attachment and bond are created between a child and his or her caregiver.\n\n\nThe bond between mothers and infants has been determined to be the core bond during the lifespan for mothers and children. At birth, mothers go through a postpartum period where they feel detached from their infant and need to create a new bond different from the one that was created during the prenatal period. The bond between mother and infant is just as important to the mother as it is to the infant. This bond can be formed after the once believed critical period of postpartum skin contact. This first emotional bond is the basis of all future relationships and bonds in the child's future.\n\nThe bond between father and child has been found to be more important than previously believed, however it has not been found to be as important as the bond between mother and child. Children do bond with fathers as a caregiver but the level of the strength of the bond is dependent upon the level of involvement by the father. However, there is not sufficient research on the subject to draw any other conclusions on the bond and on father's involvement.\n\nThe bond between sexual partners is characterized with three components which are reproductive, attachment and caregiver, and they may be more prevalent in certain relationships than in others. In some sexual partnerships there is only a reproductive component, with no emotional attachment. When an attachment is formed on top of the reproductive bond, the male is likely to take on a caregiver role with offspring as well as his mate; which in western culture is usually his wife. In western culture the pair often exchanges levels of care, and attachment throughout the lifespan. In traditional married couples the level of sexual attachment changes throughout the lifespan of the relationship thus stating that there are other important aspects of the bond between man and wife. Couples share an emotional and intellectual bond on top of the sexual one. In western society traditional gender roles are being challenged which is impacting the level of male caregiver attachment to his wife; however there is not a large field of research on the subject.\n\nHomosexual couples are hypothesized to share much of the same attachment components; however there is little research on the subject to draw conclusions from.\n\nThe bond formed between friends, companions and intimates are essential bonds to the lifetime. These bonds are essential for social development because they differ from the bonds formed with parents and other kin. Humans are naturally social creatures thus forming bonds with other people comes naturally. These relationships are often formed through common interests, and proximity. Friendships begin in early childhood, and last throughout adulthood. Many different friendships are formed throughout the lifespan and they can be any length of time. Again, these bonds tend to not be as strong as those formed with other kin, but the social aspects of them are essential to development.\n\nThe bond between siblings and other kin is largely dependent upon the age of the kin, and the level of involvement with the child. Older siblings can take on more of a parental role with younger siblings thus creating more of a parental bond. This parental bond is found when a parent dies, or when the level of parental involvement is low, however this area of research is still fairly new. Siblings that are close in age often have more of a friendship bond. Siblings can also have a different type of bond that is not seen in other relationships because siblings have a close bond but may have more indecisive feelings towards each other.\n\nThe bond between other kin is largely dependent on the society that the child grows up in. In more collectivist cultures the bond between kin is stronger than in the individualistic ones. The level of bond between kin is often because of shared values, culture, background, and personal experiences. There is little research on this subject thus the level of information is still low.\n\nAttachment is not something that is only limited to humans, it is seen in non-human animals as well. A classic study demonstrating attachment in animals was done by Harry Harlow with his macaque monkeys. His study suggests that an infant not only feels attachment to his/her mother because of needs for nutrients and protection, but they feel attachment to their mother for needs of comfort as well.\n\nIn Harry Harlow's experiment he separated infant monkeys from their mothers 6–12 hours after birth and raised them in a laboratory, isolated from humans and other monkeys. In each cage these infant monkeys had two \"mothers.\" One mother was made solely from wire, and the other mother was made from a block of wood and sponge rubber which was then wrapped with terry cloth; both radiated heat. In one condition only the wire mother nursed, and in the other condition only the terry cloth mother nursed.\n\nThe results show that infant monkeys spent significantly more time with the terry cloth mother whether she provided nutrients or not. This demonstrates that infants not only need their mothers to provide food and protection, but they need to provide some sort of comfort as well.\n\n\n"}
{"id": "51841948", "url": "https://en.wikipedia.org/wiki?curid=51841948", "title": "Aleksandr Iosifovich Popov", "text": "Aleksandr Iosifovich Popov\n\nAleksandr Iosifovich Popov (; 7 June 1913 in Dorpat, Governorate of Livonia, Russian Empire – 23 April 1993 in Moscow, Russia) was a Soviet permafrost researcher at the Moscow State University. He served as head of the Department of Cryolithology and Glaciology. Popov was a member of the International Commission on Periglacial Morphology of the International Geographical Union. He organized and led numerous expeditions to investigate permafrost and deep seasonal freezing in the Soviet Union. Popov edited the geocryological map of the Soviet Union that was published in 1986.\n\n"}
{"id": "40646055", "url": "https://en.wikipedia.org/wiki?curid=40646055", "title": "Alignment-free sequence analysis", "text": "Alignment-free sequence analysis\n\nIn bioinformatics, alignment-free sequence analysis approaches to molecular sequence and structure data provide alternatives over alignment-based approaches.\n\nThe emergence and need for the analysis of different types of data generated through biological research has given rise to the field of bioinformatics. Molecular sequence and structure data of DNA, RNA, and proteins, gene expression profiles or microarray data, metabolic pathway data are some of the major types of data being analysed in bioinformatics. Among them sequence data is increasing at the exponential rate due to advent of next-generation sequencing technologies. Since the origin of bioinformatics, sequence analysis has remained the major area of research with wide range of applications in database searching, genome annotation, comparative genomics, molecular phylogeny and gene prediction. The pioneering approaches for sequence analysis were based on sequence alignment either global or local, pairwise or multiple sequence alignment. Alignment-based approaches generally give excellent results when the sequences under study are closely related and can be reliably aligned, but when the sequences are divergent, a reliable alignment cannot be obtained and hence the applications of sequence alignment are limited. Another limitation of alignment-based approaches is their computational complexity and are time-consuming and thus, are limited when dealing with large-scale sequence data. The advent of next-generation sequencing technologies has resulted in generation of voluminous sequencing data. The size of this sequence data poses challenges on alignment-based algorithms in their assembly, annotation and comparative studies.\n\nAlignment-free methods can broadly be classified into four categories: a) methods based on \"k\"-mer/word frequency, b) methods based on substrings, c) methods based on information theory and d) methods based on graphical representation. Alignment-free approaches have been used in sequence similarity searches, clustering and classification of sequences, and more recently in phylogenetics (Figure 1).\n\nSuch molecular phylogeny analyses employing alignment-free approaches are said to be part of \"next-generation phylogenomics\". A number of review articles provide in-depth review of alignment-free methods in sequence analysis.\n\nThe popular methods based on \"k\"-mer/word frequencies include feature frequency profile (FFP), Composition vector (CV), Return time distribution (RTD), frequency chaos game representation (FCGR). and Spaced Words \n\nThe methodology involved in FFP based method starts by calculating the count of each possible \"k\"-mer (possible number of \"k\"-mers for nucleotide sequence: 4, while that for protein sequence: 20) in sequences. Each \"k\"-mer count in each sequence is then normalized by dividing it by total of all \"k\"-mers' count in that sequence. This leads to conversion of each sequence into its feature frequency profile. The pair wise distance between two sequences is then calculated Jensen–Shannon (JS) divergence between their respective FFPs. The distance matrix thus obtained can be used to construct phylogenetic tree using clustering algorithms like neighbor-joining, UPGMA etc.\n\nIn this method frequency of appearance of each possible \"k\"-mer in a given sequence is calculated. The next characteristic step of this method is the subtraction of random background of these frequencies using Markov model to reduce the influence of random neutral mutations to highlight the role of selective evolution. The normalized frequencies are put a fixed order to form the composition vector (CV) of a given sequence. Cosine distance function is then used to compute pairwise distance between CVs of sequences. The distance matrix thus obtained can be used to construct phylogenetic tree using clustering algorithms like neighbor-joining, UPGMA etc. This method can be extended through resort to efficient pattern matching algorithms to include in the computation of the composition vectors: (i) all \"k\"-mers for any value of \"k\", (ii) all substrings of any length up to an arbitrarily set maximum \"k\" value, (iii) all maximal substrings, where a substring is maximal if extending it by any character would cause a decrease in its occurrence count.\n\nThe RTD based method does not calculate the count of \"k\"-mers in sequences, instead it computes the time required for the reappearance of \n\"k\"-mers. The time refers to the number of residues in successive appearance of particular \"k\"-mer. Thus the occurrence of each \"k\"-mer in a sequence is calculated in the form of RTD, which is then summarised using two statistical parameters mean (μ) and standard deviation (σ). Thus each sequence is represented in the form of numeric vector of size 2·4 containing \"μ\" and \"σ\" of 4 RTDs. The pair wise distance between sequences is calculated using Euclidean distance measure. The distance matrix thus obtained can be used to construct phylogenetic tree using clustering algorithms like neighbor-joining, UPGMA etc.\n\nThe FCGR methods have evolved from chaos game representation (CGR) technique, which provides scale independent representation for genomic sequences. The CGRs can be divided by grid lines where each grid square denotes the occurrence of oligonucleotides of a specific length in the sequence. Such representation of CGRs is termed as Frequency Chaos Game Representation (FCGR). This leads to representation of each sequence into FCGR. The pair wise distance between FCGRs of sequences can be calculated using the Pearson distance, the Hamming distance or the Euclidean distance.\n\nWhile most alignment-free algorithms compare the word-composition of sequences, Spaced Words uses a pattern of care and don't care positions. The occurrence of a spaced word in a sequence is then defined by the characters at the match positions only, while the characters at the don't care positions are ignored. Instead of comparing the frequencies of contiguous words in the input sequences, this approach compares the frequencies of the spaced words according to the pre-defined pattern. Note that the pre-defined pattern can be selected by analysis of the Variance of the number of matches, the probability of the first occurrence on several models, or the Pearson correlation coefficient between the expected word frequency and the true alignment distance.\n\nThe methods in this category employ the similarity and differences of substrings in a pair of sequences. These algorithms\nwere mostly used for string processing in computer science.\n\nIn this approach, for a chosen pair of sequences (A and B of lengths \"n\" and \"m\" respectively), longest substring starting at some position is identified in one sequence (A) which exactly matches in the other sequence (B) at any position. In this way, lengths of longest substrings starting at different positions in sequence A and having exact matches at some positions in sequence B are calculated. All these lengths are averaged to derive a measure formula_1. Intuitively, larger the formula_1, the more similar the two sequences are. To account for the differences in the length of sequences, formula_1 is normalized [i.e. formula_4]. This gives the similarity measure between the sequences.\n\nIn order to derive a distance measure, the inverse of similarity measure is taken and a correction term is subtracted from it to assure that formula_5 will be zero. Thus\n\nThis measure formula_7 is not symmetric, so one has to compute formula_8, which gives final ACS measure between the two strings (A and B). The subsequence/substring search can be efficiently performed by\nusing suffix trees.\n\nThis approach is a generalization of the ACS approach. To define the distance between two DNA or protein sequences, kmacs estimates for each position \"i\" of the first sequence the longest substring starting at \"i\" and matching a substring of the second sequence with up to \"k\" mismatches. It defines the average of these values as a measure of similarity between the sequences and turns this into a symmetric distance measure. Kmacs does not compute exact \"k\"-mismatch substrings, since this would be computational too costly, but approximates such substrings.\n\nThis approach is closely related to the ACS, which calculates the number of substitutions per site between two DNA sequences using the shortest \nabsent substring (termed as shustring).\n\nInformation Theory has provided successful methods for alignment-free sequence analysis and comparison. The existing applications of information theory include global and local characterization of DNA, RNA and proteins, estimating genome entropy to motif and region classification. It also holds promise in gene mapping, next-generation sequencing analysis and metagenomics.\n\nBase–base correlation (BBC) converts the genome sequence into a unique 16-dimensional numeric vector using the following equation,\n\nThe formula_10 and formula_11 denotes the probabilities of bases \"i\" and \"j\" in the genome. The formula_12 indicates the probability of bases \"i\" and \"j\" at distance \"ℓ\" in the genome. The parameter \"K\" indicates the maximum distance between the bases \"i\" and \"j\". The variation in the values of 16 parameters reflect variation in the genome content and length.\n\nIC-PIC (information correlation and partial information correlation) based method employs the base correlation property of DNA sequence. IC and PIC were calculated using following formulas,\n\nThe final vector is obtained as follows:\n\nwhich defines the range of distance between bases.\n\nThe pairwise distance between sequences is calculated using Euclidean distance measure. The distance matrix thus obtained can be used to construct phylogenetic tree using clustering algorithms like neighbor-joining, UPGMA, etc..\n\nLempel-Ziv complexity uses the relative information between the sequences. This complexity is measured by the number of steps required to generate a string given the prior knowledge of another string and a self-delimiting production process. This measure has a relation to measuring \"k\"-words in a sequence, as they can be easily used to generate the sequence. It is computational intensive method. Otu and Sayood (2003) used this method to construct five different distance measures for phylogenetic tree construction.\n\nIn the context modeling complexity the next-symbol predictions, of one or more statistical models, are combined or competing to yield a prediction that is based on events recorded in the past. The algorithmic information content derived from each symbol prediction can be used to compute algorithmic information profiles with a time proportional to the length of the sequence. The process has been applied to DNA sequence analysis.\n\nThe use of iterated maps for sequence analysis was first introduced by HJ Jefferey in 1990 when he proposed to apply the Chaos Game to map genomic sequences into a unit square. That report coined the procedure as Chaos Game Representation (CGR). However, only 3 years later this approach was first dismissed as a projection of a Markov transition table by N Goldman. This objection was overruled by the end of that decade when the opposite was found to be the case – that CGR bijectively maps Markov transition is into a fractal, order-free (degree-free) representation. The realization that iterated maps provide a bijective map between the symbolic space and numeric space led to the identification of a variety of alignment-free approaches to sequence comparison and characterization. These developments were reviewed in late 2013 by JS Almeida in. A number of web apps such as https://usm.github.com, are available to demonstrate how to encode and compare arbitrary symbolic sequences in a manner that takes full advantage of modern MapReduce distribution developed for cloud computing.\n\n\n"}
{"id": "6143326", "url": "https://en.wikipedia.org/wiki?curid=6143326", "title": "Automated Planet Finder", "text": "Automated Planet Finder\n\nThe Automated Planet Finder Telescope (APF) a.k.a. Rocky Planet Finder, is a fully robotic 2.4-meter optical telescope at Lick Observatory, situated on the summit of Mount Hamilton, east of San Jose, California, USA. It is designed to search for extrasolar planets in the range of five to twenty times the mass of the Earth. The instrument will examine about 10 stars per night. Over the span of a decade, the telescope is expected to study 1,000 nearby stars for planets. Its estimated cost was $10 million. The total cost-to-completion of the APF project was $12.37 million. First light was originally scheduled for 2006, but delays in the construction of the major components of the telescope pushed this back to August 2013. It was commissioned in August 2013.\n\nThe telescope uses high-precision radial velocity measurements to measure the gravitational reflex motion of nearby stars caused by the orbiting of planets. The design goal is to detect stellar motions as small as one meter per second, comparable to a slow walking speed. The main targets will be stars within about 100 light years of the Earth.\n\nEarly tests show that the performance of the Ken and Gloria Levy Doppler Spectrometer is meeting the design goals. The spectrometer has high throughput and is meeting the design sensitivity of (1.0 m/s), similar to the radial velocity precision of HARPS and HIRES.\n\nParts for the telescopes were constructed by international companies:\n\nThe telescope will also be used to search for optical signals coming from laser transmissions from hypothetical extraterrestrial civilizations (search for extraterrestrial intelligence - SETI). This undertaking is for the heavily funded Breakthrough Listen to the Berkeley SETI Research Center.\n\n\n"}
{"id": "19799928", "url": "https://en.wikipedia.org/wiki?curid=19799928", "title": "Basic Palaeontology", "text": "Basic Palaeontology\n\nBasic Palaeontology is a basic textbook on the study of paleontology written by the palaeontologists Michael J. Benton and David A.T. Harper, and published by Prentice Hall in 1997.\n"}
{"id": "5692356", "url": "https://en.wikipedia.org/wiki?curid=5692356", "title": "Cape Canaveral Air Force Station Launch Complex 14", "text": "Cape Canaveral Air Force Station Launch Complex 14\n\nLaunch Complex 14 (LC-14) is a launch site at Cape Canaveral Air Force Station in Florida. LC-14 was used for various manned and unmanned Atlas launches, including the \"Friendship 7\" flight aboard which John Glenn became the first American to orbit the Earth.\n\nLC-14 was the first Atlas pad in operation and hosted the initial test flights in 1957–58. It was also the only one of the original four Atlas pads to never have a booster explode on it. By 1959, it was decided to convert the pad for the Atlas D missile and space launches, and a large service tower was added early in the year. The first Atlas flown from the renovated LC-14 was Missile 7D on May 18, however a problem with the launcher hold-down arms damaged the missile and caused its explosion shortly after launch. This was traced to improper procedures during the renovation of the pad and was quickly fixed. The first space launch off of LC-14 was the Big Joe Mercury test in September. As the designated Mercury-Atlas facility, LC-14 was thus the only Atlas pad sporting the infrastructure needed for manned launches. The first MIDAS satellites, one Atlas-Able launch, and a few more ICBM tests were conducted from LC-14 before it was completely turned over to NASA.\n\nLC-14 is most well known as the launch site for NASA's Mercury-Atlas 6 flight, which made Glenn the first American in orbit. It was also the launch site of the remaining three Mercury-Atlas flights and various unmanned Atlas launches. Later, it was the site for Atlas-Agena launches for the Agena Target Vehicles for Project Gemini.\n\nFollowing decommissioning and abandonment as an active launch site, LC-14 slowly fell into decay. The proximity to the Atlantic Ocean created an ideal environment for corrosion of metal components, and the complex's red metal gantry structures were dismantled for safety purposes in the 1970s.\n\nIn 1997, the 45th Space Wing embarked on a partial restoration of LC-14 under the aegis of the 45th Operations Support Squadron and its commander, Lt Col Dennis Hilley, USAF. Although extensive repairs were made by Boeing and Johnson Controls, with additional assistance from Lockheed Martin and Brown and Root, the restoration utilized no military construction or military operations and maintenance funding and was effected strictly with military, DoD civilian, NASA civilian, DoD contractor and NASA contractor volunteers. Several months later, the exterior and interior of the original blockhouse and its nearby astronaut parking area had been restored, with the blockhouse converted into a conference facility for military, NASA and contractor use.\n\nPresent at the dedication in May 1998 were former Mercury astronauts Colonel Gordon Cooper, USAF (Ret.) and Commander Scott Carpenter, USN (Ret.); Mrs. Betty Grissom, widow of Lt Col Gus Grissom, USAF; and comedian Bill Dana, known for his \"José Jiménez, the reluctant astronaut\" character. The character caught on among the seven Mercury astronauts. Among other Mercury astronauts, former U.S. Senator (and Colonel, USMC (Ret.)) John Glenn could not attend due to preparations for his then-pending Space Shuttle flight (STS-95), and Captain Wally Schirra, USN (Ret.) could not attend due to a scheduling conflict. Deke Slayton had died in 1993, while Rear Admiral Alan Shepard, USN (Ret.) extended his regrets due to illness. Largely unknown at the time was that Shepard was suffering from terminal leukemia, and he died shortly after the dedication.\n\nIn addition to the total interior renovation, the blockhouse contains historical documents, photos and memorabilia from Project Mercury, as well as photos of the blockhouse area before, during and after the restoration. Future improvements to the pad itself are also planned as time and contributory funding permits.\n\nThe entrance road to LC-14 is marked by several memorials and signs commemorating Project Mercury and the four of six manned Mercury missions launched there. This includes a large sculpture of the Project Mercury symbol constructed of titanium, under which is buried a time capsule containing technical documents of the Mercury program. The time capsule is scheduled to be opened in 2464, 500 years after the official conclusion of the program. With its withdrawal from operational status, a memorial marker in granite was also placed at the beginning of the concrete ramp that leads to LC-14's launch pad and two outdoor kiosks were erected to contain historical photos.\n\n"}
{"id": "523770", "url": "https://en.wikipedia.org/wiki?curid=523770", "title": "Caramelization", "text": "Caramelization\n\nCaramelization is the browning of sugar, a process used extensively in cooking for the resulting sweet nutty flavor and brown color. The brown colours are produced by three groups of polymers: caramelans (CHO), caramelens (CHO), and caramelins (CHO). As the process occurs, volatile chemicals such as diacetyl are released, producing the characteristic caramel flavor.\n\nLike the Maillard reaction, caramelization is a type of non-enzymatic browning. However, unlike the Maillard reaction, caramelization is pyrolytic, as opposed to being a reaction with amino acids.\n\nWhen caramelization involves the disaccharide sucrose, it is broken down into the monosaccharides fructose and glucose.\n\nCaramelization is a complex, poorly understood process that produces hundreds of chemical products, and includes the following types of reaction:\n\nThe process is temperature-dependent. Specific sugars each have their own point at which the reactions begin to proceed readily. Impurities in the sugar, such as the molasses remaining in brown sugar, greatly speed the reactions. \nThe caramelization reactions are also sensitive to the chemical environment. By controlling the level of acidity (pH), the reaction rate (or the temperature at which the reaction occurs readily) can be altered. The rate of caramelization is generally lowest at near-neutral acidity (pH around 7), and accelerated under both acidic (especially pH below 3) and basic (especially pH above 9) conditions.\n\nCaramelization is used to produce several foods, including:\n\n\nNote that the preparation of many \"caramelized\" foods also involves the Maillard reaction; particularly recipes involving protein and/or amino acid -rich ingredients.\n\n\n"}
{"id": "21591042", "url": "https://en.wikipedia.org/wiki?curid=21591042", "title": "Copernicus Science Centre", "text": "Copernicus Science Centre\n\nCopernicus Science Centre () is a science museum standing on the bank of the Vistula River in Warsaw, Poland. It contains over 450 interactive exhibits that enable visitors to single-handedly carry out experiments and discover the laws of science for themselves. The Centre is the largest institution of its type in Poland and one of the most advanced in Europe. In 2015 it has been visited by over 5 million people since its opening.\n\nThe first module of the Centre building was opened on 5 November 2010 with five galleries (On the move, Humans and the environment, Roots of civilization, Lightzone, Bzzz!); the exhibit for teenagers – RE: generation was opened 3 March 2011; a planetarium The Heavens of Copernicus opened on 19 June, the Discovery Park on 15 July, chemistry laboratory - 18 October; biology laboratory - 15 November, robotics workshop - 6 December, and physics laboratory - 20 December.\n\nSince 2008, the Copernicus Science Centre together with Polish Radio has organized the Science Picnic - Europe's largest outdoor science-popularization event.\nIn 2011 the Centre hosted the ECSITE conference (European Network of Science Centres and Museums) – one of the most important events in the field of science centres and museums in the world.\n\n\nThe Copernicus Science Centre building has been erected on the bank of the Vistula River in the very heart of Warsaw (the corner of Wybrzeże Kościuszkowskie and Zajęcza streets, above the Wisłostrada tunnel). The building design was developed by young Polish architects from the firm RAr-2 in Ruda Śląska, who won an architectural competition for the Copernicus Science Centre facility in December 2005, with engineers Buro Happold.\n\nThe Centre complex comprises: \n\nPermanent Exhibition in the Copernicus Science Centre consists of over 400 interactive exhibits. The exhibition is divided into six sections concerning various fields of knowledge:\n\n\nThe Heavens of Copernicus is a modern planetarium where you can see more than just images of the starry sky and related films. The shows concern a variety of popular science issues, including from the field of astronomy, natural science and ethnography.\n\nThe shows are displayed on a spherical screen surrounding the audience on all sides. Thanks to this solution, the audience experiences a feeling of immersion in the displayed world, which is strengthened by the high quality sound system deployed around the dome.\n\n\"Experiment!\" is the first exhibition to be organized by the Copernicus Science Centre. It made its premiere appearance at the Warsaw Science Picnic in June 2006, where more than 10,000 individuals visited it in a single day. Since September 2006, it has been traveling around large cities and small towns to give their residents an opportunity to try out the hands-on experiments on their own.\n\nThe Copernicus Science Centre also organizes \"Family Workshops\", where children (5–8 years old) together with their parents or carers can carry out experiments to help them better understand everyday phenomena (e.g. \"where does the current in electrical outlets come from, how does a TV set work or why does yeast make dough rise\"). The children can easily repeat the experiments themselves at home.\n\nThe Centre takes part in various events popularizing science: the \"Summer\" and \"Winter in the City\" events, \"Science Festivals\" and the \"Summer with Radio\" event. For teachers, the Copernicus Science Centre organizes trainings and competitions.\n\nThe Copernicus Science Centre is a cultural institution established and financed by the City of Warsaw, the Minister of Science and Higher Education, and the Minister of National Education.\n\n"}
{"id": "97914", "url": "https://en.wikipedia.org/wiki?curid=97914", "title": "Differential scanning calorimetry", "text": "Differential scanning calorimetry\n\nDifferential scanning calorimetry, or DSC, is a thermoanalytical technique in which the difference in the amount of heat required to increase the temperature of a sample and reference is measured as a function of temperature. Both the sample and reference are maintained at nearly the same temperature throughout the experiment. Generally, the temperature program for a DSC analysis is designed such that the sample holder temperature increases linearly as a function of time. The reference sample should have a well-defined heat capacity over the range of temperatures to be scanned.\n\nThe technique was developed by E. S. Watson and M. J. O'Neill in 1962, and introduced commercially at the 1963 Pittsburgh Conference on Analytical Chemistry and Applied Spectroscopy. The first adiabatic differential scanning calorimeter that could be used in biochemistry was developed by P. L. Privalov and D. R. Monaselidze in 1964 at Institute of Physics in Tbilisi, Georgia. The term DSC was coined to describe this instrument, which measures energy directly and allows precise measurements of heat capacity.\n\nTypes of DSC:\n\nThe basic principle underlying this technique is that when the sample undergoes a physical transformation such as phase transitions, more or less heat will need to flow to it than the reference to maintain both at the same temperature. Whether less or more heat must flow to the sample depends on whether the process is exothermic or endothermic. For example, as a solid sample melts to a liquid, it will require more heat flowing to the sample to increase its temperature at the same rate as the reference. This is due to the absorption of heat by the sample as it undergoes the endothermic phase transition from solid to liquid. Likewise, as the sample undergoes exothermic processes (such as crystallization) less heat is required to raise the sample temperature. By observing the difference in heat flow between the sample and reference, differential scanning calorimeters are able to measure the amount of heat absorbed or released during such transitions. DSC may also be used to observe more subtle physical changes, such as glass transitions. It is widely used in industrial settings as a quality control instrument due to its applicability in evaluating sample purity and for studying polymer curing.\n\nAn alternative technique, which shares much in common with DSC, is differential thermal analysis (DTA). In this technique it is the heat flow to the sample and reference that remains the same rather than the temperature. When the sample and reference are heated identically, phase changes and other thermal processes cause a difference in temperature between the sample and reference. Both DSC and DTA provide similar information. DSC measures the energy required to keep both the reference and the sample at the same temperature whereas DTA measures the difference in temperature between the sample and the reference when the same amount of energy has been introduced into both.\n\nThe result of a DSC experiment is a curve of heat flux versus temperature or versus time. There are two different conventions: exothermic reactions in the sample shown with a positive or negative peak, depending on the kind of technology used in the experiment. This curve can be used to calculate enthalpies of transitions. This is done by integrating the peak corresponding to a given transition. It can be shown that the enthalpy of transition can be expressed using the following equation:\n\nformula_1\n\nwhere formula_2 is the enthalpy of transition, formula_3 is the calorimetric constant, and formula_4 is the area under the curve. The calorimetric constant will vary from instrument to instrument, and can be determined by analyzing a well-characterized sample with known enthalpies of transition.\n\nDifferential scanning calorimetry can be used to measure a number of characteristic properties of a sample. Using this technique it is possible to observe fusion and crystallization events as well as glass transition temperatures \"T\". DSC can also be used to study oxidation, as well as other chemical reactions.\n\nGlass transitions may occur as the temperature of an amorphous solid is increased. These transitions appear as a step in the baseline of the recorded DSC signal. This is due to the sample undergoing a change in heat capacity; no formal phase change occurs.\n\nAs the temperature increases, an amorphous solid will become less viscous. At some point the molecules may obtain enough freedom of motion to spontaneously arrange themselves into a crystalline form. This is known as the crystallization temperature (\"T\"). This transition from amorphous solid to crystalline solid is an exothermic process, and results in a peak in the DSC signal. As the temperature increases the sample eventually reaches its melting temperature (\"T\"). The melting process results in an endothermic peak in the DSC curve. The ability to determine transition temperatures and enthalpies makes DSC a valuable tool in producing phase diagrams for various chemical systems.\n\nDifferential scanning calorimetry can also be used to obtain valuable thermodynamics information about proteins. The thermodynamics analysis of proteins can reveal important information about the global structure of proteins, and protein/ligand interaction. For example, many mutations lower the stability of proteins, while ligand binding usually increases protein stability. Using DSC, this stability can be measured by obtaining Gibbs Free Energy values at any given temperature. This allows researchers to compare the free energy of unfolding between ligand-free protein and protein-ligand complex, or wild type and mutant proteins. DSC can also be used in studying protein/lipid interactions, nucleotides, drug-lipid interactions. In studying protein denaturation using DSC, the thermal melt should be at least to some degree reversible, as the thermodynamics calculations rely on chemical equlibreum.\n\nThe technique is widely used across a range of applications, both as a routine quality test and as a research tool. The equipment is easy to calibrate, using low melting indium at 156.5985 °C for example, and is a rapid and reliable method of thermal analysis.\n\nDSC is used widely for examining polymeric materials to determine their thermal transitions. The observed thermal transitions can be utilized to compare materials, although the transitions do not uniquely identify composition. The composition of unknown materials may be completed using complementary techniques such as IR spectroscopy. Melting points and glass transition temperatures for most polymers are available from standard compilations, and the method can show polymer degradation by the lowering of the expected melting point, \"T\", for example. \"T\" depends on the molecular weight of the polymer and thermal history, so lower grades may have lower melting points than expected. The percent crystalline content of a polymer can be estimated from the crystallization/melting peaks of the DSC graph as reference heats of fusion can be found in the literature. DSC can also be used to study thermal degradation of polymers using an approach such as Oxidative Onset Temperature/Time (OOT), however, the user risks contamination of the DSC cell, which can be problematic. Thermogravimetric Analysis (TGA) may be more useful for decomposition behavior determination. Impurities in polymers can be determined by examining thermograms for anomalous peaks, and plasticisers can be detected at their characteristic boiling points. In addition, examination of minor events in first heat thermal analysis data can be useful as these apparently \"anomalous peaks\" can in fact also be representative of process or storage thermal history of the material or polymer physical aging. Comparison of first and second heat data collected at consistent heating rates can allow the analyst to learn about both polymer processing history and material properties.\n\nDSC is used in the study of liquid crystals. As some forms of matter go from solid to liquid they go through a third state, which displays properties of both phases. This anisotropic liquid is known as a liquid crystalline or mesomorphous state. Using DSC, it is possible to observe the small energy changes that occur as matter transitions from a solid to a liquid crystal and from a liquid crystal to an isotropic liquid.\n\nUsing differential scanning calorimetry to study the stability to oxidation of samples generally requires an airtight sample chamber. Usually, such tests are done isothermally (at constant temperature) by changing the atmosphere of the sample. First, the sample is brought to the desired test temperature under an inert atmosphere, usually nitrogen. Then, oxygen is added to the system. Any oxidation that occurs is observed as a deviation in the baseline. Such analysis can be used to determine the stability and optimum storage conditions for a material or compound.\n\nDSC makes a reasonable initial safety screening tool. In this mode the sample will be housed in a non-reactive crucible (often gold or gold-plated steel), and which will be able to withstand pressure (typically up to 100 bar). The presence of an exothermic event can then be used to assess the stability of a substance to heat. However, due to a combination of relatively poor sensitivity, slower than normal scan rates (typically 2–3 °C/min, due to much heavier crucible) and unknown activation energy, it is necessary to deduct about 75–100 °C from the initial start of the observed exotherm to \"suggest\" a maximal temperature for the material. A much more accurate data set can be obtained from an adiabatic calorimeter, but such a test may take 2–3 days from ambient at a rate of a 3 °C increment per half-hour.\n\nDSC is widely used in the pharmaceutical and polymer industries. For the polymer chemist, DSC is a handy tool for studying curing processes, which allows the fine tuning of polymer properties. The cross-linking of polymer molecules that occurs in the curing process is exothermic, resulting in a negative peak in the DSC curve that usually appears soon after the glass transition.\n\nIn the pharmaceutical industry it is necessary to have well-characterized drug compounds in order to define processing parameters. For instance, if it is necessary to deliver a drug in the amorphous form, it is desirable to process the drug at temperatures below those at which crystallization can occur.\n\nFreezing-point depression can be used as a purity analysis tool when analysed by differential scanning calorimetry. This is possible because the temperature range over which a mixture of compounds melts is dependent on their relative amounts. Consequently, less pure compounds will exhibit a broadened melting peak that begins at lower temperature than a pure compound.\n\n\n"}
{"id": "1833232", "url": "https://en.wikipedia.org/wiki?curid=1833232", "title": "Donald Baxter MacMillan", "text": "Donald Baxter MacMillan\n\nDonald Baxter MacMillan (November 10, 1874 – September 7, 1970) was an American explorer, sailor, researcher and lecturer who made over 30 expeditions to the Arctic during his 46-year career. He pioneered the use of radios, airplanes, and electricity in the Arctic, brought back films and thousands of photographs of Arctic scenes, and put together a dictionary of the Inuktitut language.\n\nBorn in Provincetown, Massachusetts in 1874, MacMillan lived in Freeport, Maine after the deaths of both his parents in 1883 (his father died while captaining a Grand Banks fishing schooner) and 1886 (his mother died suddenly), and was educated at Bowdoin College in Brunswick, graduating in 1898 with a degree in geology. He later taught at Worcester Academy from 1903 to 1908.\n\nOn March 18, 1935, MacMillan married Miriam Norton Look, the daughter of his long-time friends Jerome and Amy Look. Though MacMillan at first refused to let her accompany him north, Miriam soon convinced him of her willingness and ability to participate in his Arctic travels. She participated in several of his scientific and exploration trips to the Arctic and elsewhere.\n\nAfter five years as a high school teacher, MacMillan caught the attention of explorer and fellow Bowdoin graduate Robert E. Peary when he saved the lives of nine shipwrecked people in two nights. Peary subsequently invited MacMillan to join his 1908 journey to the North Pole. Although MacMillan himself had to turn back at 84°29' on March 14 because of frozen heels, Peary allegedly reached the Pole 26 days later.\n\nMacMillan spent the next few years traveling in Labrador, carrying out ethnological studies among the Innu and Inuit. He organized and commanded the ill-fated Crocker Land Expedition to northern Greenland in 1913. Unfortunately Crocker Land turned out to be a mirage. The expedition members were stranded until 1917, when Captain Robert A. Bartlett of the ship \"Neptune\" finally rescued them. Journals from the expedition by Maurice C. Tanquary, Walter E. Ekblaw, Donald B. and Miriam MacMillan are available online at the George J. Mitchell Department of Special Collections & Archives website. Digitization of materials at Bowdoin College related to the Crocker Land Expedition was generously funded by the Gladys Krieble Delmas Foundation in 2016.\n\nShortly after the armistice which ended the First World War, MacMillan was commissioned an ensign in the Naval Reserve Flying Corps on 24 December 1918. (MacMillan was 44 years old at the time, making him one of the oldest ensigns in the history of the U.S. Navy.) After the war, MacMillan began raising money for another Arctic expedition. In 1921, the schooner \"Bowdoin\"—named for MacMillan's alma mater—was launched from East Boothbay, Maine and set sail for Baffin Island, where MacMillan and his crew spent the winter. The expedition was notable for taking along an amateur radio operator, Don Mix, who used station WNP (\"Wireless North Pole\") to keep them in contact with the outside world.\n\nIn 1923 there was concern about a new ice age and he sailed toward the North Pole aboard the schooner \"Bowdoin\", sponsored by the National Geographical Society to look for evidence of advancing glaciers. A member of this expedition was aviator Richard E. Byrd who would rise to international fame three years later when he and Floyd Bennett became the first men to fly over the North Pole.\n\nIn 1925 MacMillan was commissioned as a lieutenant commander in the United States Naval Reserve Force. Thereafter, he was frequently referred to as \"Commander MacMillan\".\n\nIn September 1926 MacMillan led a group of explorers which included three women and five scientists to Sydney, Nova Scotia. The team spent several months beforehand collecting flora and fauna in Labrador and Greenland. He believed it was possible that the ancient ruins off Sculpin Island, twenty miles from Nain, Labrador, are the remains of a Norse settlement 1,000 years old. On the side bordering the mainland MacMillan found what he considered the vestiges of ten or twelve houses. He estimated the age of the dwellings to be hundreds of years old according to the lichens which partially covered their foundations. However MacMillan could not say for certain if these had been built by Vikings. According to Inuit tradition the \"stone igloos\" were constructed by men who came from the sea in ships. Inuit called the site \"Tunitvik\", meaning \"the place of the Norseman\". MacMillan said the strongest argument that the Sculpin dwellings were of Viking origin was their resemblance to those he found in Greenland the previous year.\n\nMacMillan was placed on the Naval Reserve Honorary Retired List with the rank of lieutenant commander on his 64th birthday in 1938. Despite being past retirement age, he volunteered for active duty with the Navy during World War II. On May 22, 1941, he transferred the \"Bowdoin\" to the Navy for the duration of the war and served as her initial commanding officer before being transferred to the Hydrographic Office in Washington, DC. He was promoted to the rank of commander on 13 June 1942.\n\nAfter the war, MacMillan continued his trips to the Arctic, taking researchers north and carrying supplies for the MacMillan-Moravian School he established in 1929. On June 25, 1954 MacMillan was promoted, by a special act of Congress, to rank of rear admiral on the Naval Reserve retired list in honor of his lifetime of service and achievement.\n\nAdmiral MacMillan made his final trip to the Arctic in 1957 at age 82, and died in 1970 at the age of 95. He is buried in Provincetown, Massachusetts, where a main wharf is named after him.\n\nIn 1927, the Boy Scouts of America made MacMillan an \"Honorary Scout\", a new category of Scout created that same year. This distinction was given to \"American citizens whose achievements in outdoor activity, exploration and worthwhile adventure are of such an exceptional character as to capture the imagination of boys...\". The other eighteen who were awarded this distinction were: Roy Chapman Andrews; Robert Bartlett; Frederick Russell Burnham; Richard E. Byrd; George Kruck Cherrie; James L. Clark; Merian C. Cooper; Lincoln Ellsworth; Louis Agassiz Fuertes; George Bird Grinnell; Charles A. Lindbergh; Clifford H. Pope; George Palmer Putnam; Kermit Roosevelt; Carl Rungius; Stewart Edward White; and Orville Wright.\n\nMacMillan Pier in Provincetown, Massachusetts is named in his honor.\n\n\n"}
{"id": "36544979", "url": "https://en.wikipedia.org/wiki?curid=36544979", "title": "Encyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures", "text": "Encyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures\n\nEncyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures is an encyclopedia edited by Helaine Selin and published by Kluwer Academic Publishers in 1997, with a second edition in 2008, and third edition in 2016.\n\nFrom the Preface:\nThe first edition (1997) has 600 articles by a range of experts. The arrangement is alphabetical from \"Abacus\" to \"Zu Chongzi\". It includes an index from page 1079 to page 1117. K. V. Sarma contributed 35 articles, Greg De Young 13, Boris A. Rosenfeld 12, and Emilia Calvo and Ho Peng Yoke 11 each. Fabrizio Pregadio contributed 10 articles, Julio Samo wrote 9, and Richard Bertschinger, Radha Charan Gupta and David A. King wrote 8 each. Dozens of other contributors wrote fewer articles.\n\nIn 2008 the \"Encyclopaedia\" was split into two volumes and extended to 1000 articles for a second edition.\n\n\n"}
{"id": "8081063", "url": "https://en.wikipedia.org/wiki?curid=8081063", "title": "Engineering education", "text": "Engineering education\n\nEngineering education is the activity of teaching knowledge and principles to the professional practice of engineering. It includes an initial education (bachelor's and/or master's degree), and any advanced education and specializations that follow. Engineering education is typically accompanied by additional postgraduate examinations and supervised training as the requirements for a professional engineering license. The length of education, and training to qualify as a basic professional engineer, is typically 8–12 years, with 15–20 years for an engineer who takes responsibility for major projects.\n\nScience, technology, engineering, and mathematics (STEM) education in primary and secondary schools often serves as the foundation for engineering education at the university level. In the United States, engineering education is a part of the STEM initiative in public schools. Service-learning in engineering education is gaining popularity within the variety of disciplinary focuses within engineering education including chemical engineering, mechanical engineering, industrial engineering, computer engineering, electrical engineering, and other engineering education.\n\nEngineering training in Kenya is typically provided by the universities. Registration of engineers is governed by the Engineers Registration Act. A candidate stands to qualify as a registered engineer, R.Eng, if he/she is a holder of a minimum four years post-secondary Engineering Education and a minimum of three years of postgraduate work experience.\n\nAll registrations are undertaken by the Engineers Registration Board which is a statutory body established through an Act of the Kenyan Parliament in 1969. A minor revision was done in 1992 to accommodate Technician Engineer grade. The Board has been given the responsibility of regulating the activities and conduct of Practicing Engineers in the Republic of Kenya in accordance with the functions and powers conferred upon it by the Act. Under CAP 530 of the Laws of Kenya, it is illegal for an engineer to practice or call himself an engineer if not registered with the Board. Registration with the Board is thus a license to practice engineering in Kenya.\n\nEngineering training in South Africa is typically provided by the universities, universities of technology and colleges for Technical and Vocational Education and Training (previously Further Education and Training). The qualifications provided by these institutions must have an Engineering Council of South Africa (ECSA) accreditation for the qualification for graduates and diplomats of these institutions to be registered as Candidate Certificated Engineers,Candidate Engineers, Candidate Engineering Technologists and Candidate Engineering Technicians.\n\nThe academic training performed by the universities is typically in the form of a four-year BSc(Eng), BIng or BEng degree. For the degree to be accredited, the course material must conform to the ECSA Exit Level Outcomes (ELO).\n\nProfessional Engineers (Pr Eng) are persons that are accredited by ECSA as engineering professionals. Legally, a Professional Engineer's sign off is required for any major project to be implemented, in order to ensure the safety and standards of the project. Professional Engineering Technologists (Pr Tech Eng) and Professional Engineering Technicians (Pr Techni Eng) are other members of the engineering team.\n\nProfessional Certificated Engineers (Pr Cert Eng) are people who hold one of seven Government Certificates of Competency and who have been registered by ECSA as engineering professionals.\n\nThe categories of professionals are differentiated by the degree of complexity of work carried out, where Professional Engineers are expected to solve complex engineering problems, Professional Engineering Technologists and Professional Certificated Engineers, broadly defined engineering problems and Professional Engineering Technicians, well-defined engineering problems.\n\nEngineering training in Tanzania is typically provided by various universities and technical institutions in the country. Graduate engineers are registered by the Engineers Registration Board (ERB) after undergoing three years of practical training. A candidate stands to qualify as a professional engineer, P.Eng, if he/she is a holder of a minimum four years post-secondary Engineering Education and a minimum of three years of postgraduate work experience. Engineers Registration Board is a statutory body established through an Act of the Tanzanian Parliament in 1968. Minor revision was done in 1997 to address the issue of engineering professional excellence in the country.\n\nThe Board has been given the responsibility of regulating the activities and conduct of Practicing Engineers in the United Republic of Tanzania in accordance with the functions and powers conferred upon it by the Act. According to Tanzania Laws, it is illegal for an engineer to practice or call himself an engineer if not registered with the Board. Registration with the Board is thus a license to practice engineering in United Republic of Tanzania.\n\n\nMore than 5,000 universities and colleges offer engineering courses in India.\n\n\nActivities on engineering education in Malaysia are spearheaded by the Society of Engineering Education Malaysia (SEEM). SEEM was established in 2007 and launched on 23 February 2009. The idea of establishing the Society of Engineering Education was initiated on April, 2005 with the creating of a Pro-team Committee for SEEM. The objectives of this society are to contribute to the development of education in the fields of engineering education and science and technology, including teaching and learning, counseling, research, service and public relations.\n\n\nIn Pakistan, engineering education is accredited by the Pakistan Engineering Council, a statutory body, constituted under the PEC Act No. V of 1976 of the constitution of Pakistan and amended vide Ordinance No.XXIII of 2006, to regulate the engineering profession in the country. It aims to achieve rapid and sustainable growth in all national, economic and social fields. The council is responsible for maintaining realistic and internationally relevant standards of professional competence and ethics for engineers in the country. PEC interacts with the Government, both at the Federal and Provincial level by participating in Commissions, Committees and Advisory Bodies. PEC is a fully representative body of the engineering community in the country. PEC has a full signatory status with Washington Accord.\n\nThe Professional Regulation Commission is the regulating body for engineers in the Philippines.\n\nEngineering is one of the most popular majors among universities in Taiwan. The engineering degrees are over a quarter of the bachelor's degrees in Taiwan.\n\nIn Austria, similar to Germany, an engineering degree can be obtained from either Universities or Fachhochschulen (Universities of Applied Sciences). As in most of Europe, the education usually consists of a 3-year Bachelor's Degree and a 2-year Master's Degree.\n\nA lower engineering degree is offered by Höheren Technische Lehranstalten, (HTL, Higher Technical Institute), a form of secondary college which reaches from grade 9 to 13. There are disciplines like civil engineering, electronics, information technology, etc.\n\nIn the 5th year of HTL, as in other secondary schools in Austria, there is a final exam, called Matura. Graduates obtain an \"Ingenieur\" engineering degree after three years of work in the studied field.\n\nThe beginning of higher engineering education in Bulgaria is established by the Law for Establishing a Higher Technical School in Sofia in 1941. Only two years later however because of the bombs flying over Sofia, the school was evacuated in Lovech, and the regular classes were discontinued. The learning process started again in 1945 when the University became a State Polytechnic.\n\nIn Bulgaria, engineers are trained in the three basic degrees - bachelor, master and doctor. Since the Bologna declaration, students receive a bachelor's degree (4 years of studies), optionally followed by a master's degree (1 years of studies). The science and engineering courses include lecture and laboratory education. The main subjects to be studied are mathematics, physics, chemistry, electrical engineering and etc. The degree received after completing with a state exam or defense of a thesis. Absolvents are awarded with the \"Ing\". title always put in front of one's name.\n\nSome of engineering specialties are completely traditional, such as machine building, computer and software engineering, automation, electrical engineering, electronics. Newer specialties are engineering design, mechatronics, aviation engineering, industrial engineering.\n\nThe following technical universities prepare mainly engineers in Bulgaria:\n\n\nThe Bulgarian engineers are united in the Federation of Scientific and Technical Unions, established in 1949. It comprises 33 territorial and 19 national unions.\n\nIn Denmark, the engineering degree is delivered by either Universities or Engineering Colleges (e.g. Engineering College of Aarhus).\n\nStudents receive first a baccalaureate degree (3 years of studies) followed by a master's degree (1–2 years of studies) according to the principles of the Bologna declaration, though traditionally. The engineering doctorate degree is the \"PhD\" (3 years of studies).\n\nThe quality of Danish engineering expertise has long been much vaunted. Danish engineers especially from Engineering Colleges have also been praised at being very practical (i.e. skilled at physical work related to their discipline), ascribed to the high quality of the apprenticeship courses many Danish engineers go through as part of their education.\n\nFinland's system is derived from Germany's system. Two kinds of universities are recognized, the universities and the universities of applied sciences [Ammattikorkeakoulu].\n\nUniversities award typically 'Bachelor of Science in Technology' and 'Master of Science in Technology' degrees. Bachelor's degree is a three-year degree as master's degree is equivalent for two-year full-time studies. In Finnish the master's degree is called diplomi-insinööri, similarly as in Germany (\"Diplom-Ingenieur\"). The degrees are awarded by engineering schools or faculties in universities (in Aalto University, Oulu, Turku, Vaasa and Åbo Akademi University) or by separate universities of technology (Tampere UT and Lappeenranta UT). The degree is a scientific, theoretical taught master's degree. Master's thesis is important part of master's degree studies. Master's degree qualifies for further study into Licentiate or Doctorate. Because of the Bologna process, the degree \"tekniikan kandidaatti\" (\"Bachelor of Technology\"), corresponding to three years of study into the master's degree, has been introduced.\n\nThe Universities of applied sciences are regional universities that award 3.5-, to 4-year engineer degrees insinööri (amk). An engineer's degree is noramlly 240 ECTS. There are 20 universities of applied sciences in Finland with a vide range of disciplines. The aim of the degree is professional competency with an emphasis on practical problem solving in engineering. Normally the teaching language is Finnish but there are also universities with Swedish as language of instruction, and most universities of applied sciences offer some degrees in English, too. These universities also award a Master of Engineering degree, designed for engineers already involved in the working life with at least two years of professional experience.\n\nIn France, the engineering degree is mainly delivered by \"Grandes Écoles d'Ingénieurs\" (graduate schools of engineering) upon completion of 3 years of Master's studies. Many Écoles recruit undergraduate students from CPGE (two- or three-year high level program after the Baccalauréat), even though some of them include an integrated undergraduate cycle. Other students accessing these Grandes Ecoles may come from other horizons, such as DUT or BTS (technical two-year university degrees) or standard two-year university degrees. In all cases, recruitment is highly selective. Hence graduate engineers in France have studied a minimum of five years after the baccalaureate. Since 2013, the French engineering degree is recognized by the AACRAO as a Master of Science in Engineering.\nTo be able to deliver the engineering degree, an École Master 's curriculum has to be validated by the Commission des titres d'ingénieur (Commission of the Engineering Title). It is important for the external observer to note that the system in France is extremely demanding in its entrance requirements (numerus clausus, using student rank in exams as the only criterion), despite being almost free of tuition fees, and much stricter in regards to the academic level of applying students than many other systems. The system focuses solely on selecting students by their engineering fundamental disciplines (mathematics, physics) abilities rather than their financial ability to finance large tuition fees, thus enabling a wider population access to higher education. In fact, being a graduate engineer in France is considered as being near/at the top of the social/professional ladder. The engineering profession grew from the military and the nobility in the 18th century. Before the French Revolution, engineers were trained in schools for technical officers, like \"École d'Arts et Métiers\" (Arts et Métiers ParisTech) established in 1780. Then, other schools were created, for instance the École Polytechnique and the Conservatoire national des arts et métiers which was established in 1794. Polytechnique is one of the \"grandes écoles\" that have traditionally prepared technocrats to lead French government and industry, and has been one of the most privileged routes into the elite divisions of the civil service known as the \"grands corps de l'État\".\n\nInside a French company the title of \"Ingénieur\" refers to a rank in qualification and is not restricted. Therefore, there are sometimes \"Ingénieurs des Ventes\" (Sales Engineers), \"Ingénieur Marketing\", \"Ingénieur Bancaire\" (Banking Engineer), \"Ingénieur Recherche & Développement\" (R&D Engineer), etc.\n\nIn Germany, the term \"Ingenieur\" (engineer) is legally protected and may only be used by graduates of a university degree program in engineering. Such degrees are offered by universities (\"Universitäten\"), including \"Technische Universitäten\" (universities of technology), or \"Fachhochschulen\" (universities of applied sciences), including \"Technische Hochschulen\".\n\nSince the Bologna reforms, students receive a bachelor's degree (3–4 years of studies), optionally followed by a master's degree (1–2 years of studies). Prior to the country adopting the Bologna system, the first and only pre-doctorate degree received after completing engineering education at university was the German \"Diplomingenieur\" (Dipl.-Ing.). The engineering doctorate is the \"Doktoringenieur\" (Dr.-Ing.).\n\nThe quality of German engineering expertise has long been much vaunted, especially in the field of mechanical engineering. This is supported by the degree to which the various theories governing aerodynamics and structural mechanics are named after German scientists and engineers such as Ludwig Prandtl. German engineers have also been praised at being very practical (i.e. skilled at physical work related to their discipline), ascribed to the high quality of the apprenticeship courses many German engineers go through as part of their education.\n\nIn Italy, the engineering degree and \"engineer\" title is delivered by Polytechnic Universities upon completion of 3 years of studies (laurea). Additional master's degree (2 years) and doctorate programs (3 years) provide the title of \"dottore di ricerca in ingegneria\". Students that started studies in Polytechnic Universities before 2005 (when Italy adopted the Bologna declaration) need to complete a 5 years program to get the engineer title. In this case the master's degree is obtained after 1 year of studies.\nOnly people with an engineer title can be employed as \"engineers\". Still, some with competence and experience in an engineering field that do not have such a title, can still be employed to perform engineering tasks as \"specialist\", \"assistant\", \"technologist\" or \"technician\". But, only engineers can take legal responsibility and provide guarantee upon the work done by a team in their area of expertise. Sometimes a company working in this area, which temporarily does not have any employees with an engineer title must pay for an external service of an engineering audit to provide legal guarantee for their products or services.\n\nIn the Netherlands there are two ways to study engineering, i.e. at the Dutch 'technical hogeschool', which is a professional school (equivalent to a polytechnic in the UK and a university of applied sciences internationally) and awards a practically orientated degree with the pre-nominal \"ing.\" after four years study. Or at the university, which offers a more academically oriented degree with the pre-nominal \"ir.\" after five years study. Both are abbreviations of the title Ingenieur.\n\nIn 2002 when the Netherlands switched to the Bachelor-Master system. This is a consequence of the Bologna process. In this accord 29 European countries agreed to harmonize their higher education system and create a European higher education area. In this system the professional schools award bachelor's degrees like \"BEng\" or \"BASc\" after four years study. And the universities with engineering programs award the bachelor's degree \"BSc\" after the third year. A university bachelor is usually continuing his education for one or two more years to earn his master's degree \"MSc\". Adjacent to these degrees, the old titles of the pre-populated system are still in use. A \"vocational\" bachelor may be admitted to a university master's degree program although often they are required to take additional courses.\n\nIn Poland after 3,5–4 years of technical studies, one gets inżynier degree (inż.), which corresponds to \"B.Sc.\" or \"B.Eng.\" After that, one can continue studies, and after 2 years of post-graduate programme (supplementary studies) can obtain additional \"M.Sc.\" (or \"M.Eng.\") degree, called magister, mgr, and that time one has two degrees: magister inżynier, mgr inż. (literally: \"master engineer\"). The \"mgr\" degree formerly (until full adaptation of Bologna process by university) could be obtained in integrated 5 years B.Sc-M.Sc. programme studies. Graduates having \"magister inżynier\" degree, can start 4 years doctorate studies (Ph.D.), which require opening of doctoral proceedings (\"przewód doktorski\"), carrying out own research, passing some exams (\"e.g.\" foreign language, philosophy, economy, leading subjects), writing and defense of doctoral thesis. Some Ph.D. students have also classes with undergraduate students (B.Sc., M.Sc.). Graduate of doctorate studies of technical university holds \"scientific degree\" of doktor nauk technicznych, dr inż., (literally: \"doctor of technical sciences\") or other \"e.g.\" \"Doktor Nauk Chemicznych\" (lit. \"doctor of chemical sciences\").\n\nIn Portugal, there are two paths to study engineering: the polytechnic and the university paths. In theory, but many times not so much in practice, the polytechnic path is more practical oriented, the university path being more research oriented.\nIn this system, the polytechnic institutes award a \"licenciatura\" (bachelor) in engineering degree after three years of study, that can be complemented by a \"mestrado\" (master) in engineering after two plus years of study.\n\nRegarding the universities, they offer both engineering programs similar to those of the polytechnics (three years \"licenciatura\" plus two years \"mestrado\") as \"mestrado integrados\" (integrated master's) in engineering programs. The \"mestrado integrado\" programs take five years of study to complete, awarding a \"licenciatura\" degree in engineering sciences after the first three years and a \"mestrado\" degree in engineering after the whole five years. Further, the universities also offer \"doutoramento\" (Ph.D.) programs in engineering.\n\nBeing an holder of an academic degree in engineering is not enough to practice the profession of engineer and to have the legal right of the use of the title \"engenheiro\" (engineer) in Portugal. For that, it is necessary to be admitted and be a member of the Ordem dos Engenheiros (Portuguese institution of engineers). At the Ordem dos Engenheiros, an engineer is classified as an E1, E2 or E3 grade engineer, accordingly with the higher engineer degree he or she holds. Holders of the ancient pre-Bologna declaration five years \"licenciatura\" degrees in engineering are classified as E2 engineers.\n\nIn Romania, the engineering degree and \"engineer\" title is delivered by technology and polytechnics universities upon completion of 4 years of studies. Additional master's degree (2 years) and doctorate programs (4–5 years) provide the title of \"doctor inginer\". Students that started studies in Polytechnic Universities before 2005 (when Romania adopted the Bologna declaration) needed to complete a 5 years program to get the engineer title. In this case the master's degree is obtained after 1 year of studies.\nOnly people with an engineer title can be employed as \"engineers\". Still, some with competence and experience in an engineering field that do not have such a title, can still be employed to perform engineering tasks as \"specialist\", \"assistant\", \"technologist\" or \"technician\". But, only engineers can take legal responsibility and provide guarantee upon the work done by a team in their area of expertise. Sometimes a company working in this area, which temporarily does not have any employees with an engineer title must pay for an external service of an engineering audit to provide legal guarantee for their products or services.\n\nMoscow School of Mathematics and Navigation was a first Russian educational institution founded by Peter the Great in 1701. It provided Russians with technical education for the first time and much of its curriculum was devoted to producing sailors, engineers, cartographers and bombardiers to support Russian expanding navy and army.\nThen in 1810, the Saint Petersburg Military engineering-technical university becomes the first engineering higher learning institution in the Russian Empire, after addition of officers classes and application of five-year term of teaching. So initially more rigorisms of standards and teaching terms became the traditional historical feature of the Russian engineering education in the 19th century.\n\nIn Slovakia, an engineer (\"inžinier\") is considered to be a person holding master's degree in technical sciences or economics. Several technical and economic universities offer 4-5-year master study in the fields of chemistry, agriculture, material technology, computer science, electrical and mechanical engineering, nuclear physics and technology or economics. A bachelor's degree in similar field is prerequisite. Absolvents are awarded with the \"Ing.\" title always put in front of one's name; eventual follow-up doctoral study is offered both by universities and some institutes of the Slovak Academy of Sciences.\n\nIn Spain, the engineering degree is delivered by Universities in Engineering Schools, called \"Escuelas de Ingeniería\". Like with any other degree in Spain, students need to pass a series of examinations based on Bachillerato's subjects (Selectividad), select their bachelor's degree, and their marks determine whether they are access the degree they want or not.\n\nStudents receive first a grado degree (4 years of studies) followed by a master's degree (1–2 years of studies) according to the principles of the Bologna declaration, though traditionally, the degree received after completing an engineering education is the Spanish title of \"Ingeniero\". Using the title \"Ingeniero\" is legally regulated and limited to the according academic graduates.\n\nAn institution offering engineering education is called \"teknisk högskola\" (institute of technology). These schools primarily offers five-year programmes resulting in the \"civilingenjör\" degree (not to be confused with the narrower English term \"civil engineer\"), internationally corresponding to a Master of Science in Engineering degree. These programmes typically offers a strong backing in the natural sciences, and the degree also opens up for doctoral (PHD) studies towards the degree \"teknologie doktor\". Civilingenjör programmes are offered in a broad range of fields: Engineering physics, Chemistry, Civil engineering, surveying, Industrial engineering and management, etc. There also are shorter three-year programmes called \"högskoleingenjör\" (Bachelor of Science in Engineering) that are typically more applied.\n\nIn Turkey, engineering degrees range from a bachelor's degree in engineering (for a four-year period), to a master's degree (adding two years), and to a doctoral degree (usually four to five years).\n\nThe title is limited by law to people with an engineering degree, and the use of the title by others (even persons with much more work experience) is illegal.\n\nThe Union of Chambers of Turkish Engineers and Architects (UCTEA) was established in 1954 and separates engineers and architects to professional branches, with the condition of being within the framework of laws and regulations and in accordance with the present conditions, requirements and possibilities and to also establishes new Chambers for the group of engineers and architects, whose professional or working areas are similar or the same.\n\nUCTEA is maintaining its activities with its 23 Chambers, 194 branches of its Chambers and 39 Provincial Coordination Councils. Approximately, graduates of 70 related academic disciplines in engineering, architecture and city planning are members of the Chambers of UCTEA.\n\nIn the UK, like in the United States and Canada, most professional engineers are trained in universities, but some can start in a technical apprenticeship and either enroll in a university engineering degree later, or enroll in one of the Engineering Council UK programmes (level 6 - bachelor's and 7 - master's) administered by the City and Guilds of London Institute. A recent trend has seen the rise of both bachelor's and master's degree higher engineering apprenticeships. All accredited engineering courses and apprenticeships are assessed and approved by the various professional engineering institutions reflecting the subject by engineering discipline covered; IMechE, IET, BCS, ICE, IStructE etc. Many of these institutions date back to the 19th century, and have previously administered their own engineering examination programmes. They have become globally renowned as premier learned societies.\n\nThe degree then counts in part to qualifying as a Chartered Engineer after a period (usually 4–8 years beyond the first degree) of structured professional practice, professional practice peer review and, if required, further exams to then become a corporate member of the relevant professional body. The term 'Chartered Engineer' is regulated by Royal Assent and its use is restricted only to those registered; the awarding of this status is devolved to the professional institutions by the Engineering Council.\n\nIn the UK, most engineering courses take three years for an undergraduate bachelors (BEng) and four years for an undergraduate master's. Students who read a four-year engineering course are awarded a Masters of Engineering (as opposed to Masters of Science in Engineering) Some universities allow a student to opt out after one year before completion of the programme and receive a Higher National Diploma if a student has successfully completed the second year, or a Higher National Certificate if only successfully completed year one. Many courses also include an option of a year in industry, which is usually a year before completion. Students who opt for this are awarded a 'sandwich' degree.\n\nBEng graduates may be registered as an \"Incorporated Engineer\" by the Engineering Council after a period of structured professional practice, professional practice peer review and, if required, further exams to then become a member of the relevant professional body. Again, the term 'Incorporated Engineer' is regulated by Royal Assent and its use is restricted only to those registered; the awarding of this status is devolved to the professional institutions by the Engineering Council.\n\nUnlike the US and Canada, engineers do not require a licence to practice the profession in the UK. In the UK, the term \"engineer\" is applied to non-degree vocations such as technologists, technicians, draftsmen, machinists, mechanics, plumbers, electricians, repair people, semi-skilled and even unskilled occupations.\n\nIn recent developments by government and industry, to addressing the growing skills deficit in many fields of UK engineering, there has been a strong emphasis placed on dealing with engineering in school and providing students with positive role models from a young age.\n\nEngineering degree education in Canada is highly regulated by the Canadian Council of Professional Engineers (Engineers Canada) and its Canadian Engineering Accreditation Board (CEAB). In Canada, there are 43 institutions offering 278 engineering accredited programs delivering a bachelor's degree after a term of 4 years. Many schools also offer graduate level degrees in the applied sciences. \"Accreditation\" means that students who successfully complete the accredited program will have received sufficient engineering knowledge in order to meet the knowledge requirements of licensure as a Professional Engineer. Alternately, Canadian graduates of unaccredited 3-year diploma, BSc, B.Tech, or B.Eng programs can qualify for professional license by association examinations. Some of the schools include: Concordia University, École de technologie supérieure, École Polytechnique de Montréal, University of Toronto, University of Manitoba, University of Saskatchewan, University of Victoria, University of Calgary, University of Alberta, University of British Columbia, McGill University, Dalhousie University, Ryerson University, York University, University of Regina, Carleton University, McMaster University, University of Ottawa, Queen's University, University of New Brunswick, UOIT, University of Waterloo, University of Guelph, University of Windsor, Memorial University of Newfoundland, and Royal Military College of Canada just to name a few. Every university offering engineering degrees in Canada needs to be accredited by the CEAB (Canadian Engineering Accreditation Board), thus ensuring high standards are enforced at all universities. Engineering degrees in Canada are distinct from degrees in engineering technology which are more applied degrees or diplomas. An engineering education in Canada is held in very high esteem culminating by qualifying as a professional engineer (P.Eng) licensee. Many graduate engineers enter other areas of professional practice including management consulting, law, medicine, and business administration.\n\nIn the case of Mexico, the education in Engineering field could be taken from public and private Universities. Both types of Colleges and Universities can confers degrees of B.Eng., B.Sc., M.Eng., M.Sc. and Ph.D. through the presentation and dissertation of a thesis or other kind of requirements such as Technical reports, knowledge exams among others.\n\nThe first University on Mexico in offers degrees in some Engineering fields was the Pontifical and royal University of Mexico, established under the Spanish rule; the degrees offered by includes Mines Engineering and Physical Mathematical state of the art knowledge from Europe.\n\nEntered the 19th century and lack of political stability the Universities founded under Spanish rule were closed and reopened and the Engineering teaching tradition was lost; the University of Mexico, University of Guadalajara and University of Mérida suffered this. then the liberal rule create the Arts and Handcraft schools were opened without the same success as the Universities. Entered on 20th Century and with the success of Mexican Revolution some of that old colleges were reopened and the old Arts and Handcraft schools were joined to the new Universities. On 1936 the National Polytechnic Institute of Mexico was created as an educational alternative for workers son and it's families, few time later the Regional Institutes of Technology were founded as a branch of the Polytechnic Institute on few states of the republic, the most of them do not have any University in own territory.\n\nRight now the Regional Institutes of Technology were merged into one single entity labeled as \"Mexican National Technological Institute\". The National Polytechnic Institute is the ensign university of the Mexican federal government on engineering education.\n\nSome of the first engineers designed irrigation canals, buildings, dams, and many other things to satisfy the needs of the people. Early engineers during wartime designed weapons and war machines. Engineering education has changed since the times of the early engineers. \"By the middle of the 20th century there were almost 1 million engineers in the United States.\"\n\nThe first professional degree in engineering is a bachelor's degree with few exceptions. This being said, interest in engineering has grown since 1999; the number of bachelor's degrees issued has increased by 20%.\n\nMost bachelor's degree engineering programs are four years long and require about two years of core courses followed by two years of specialized discipline specific courses. This is where a typical engineering student would learn mathematics (single- and multi-variable calculus and elementary differential equations), general chemistry, English composition, general and modern physics, computer science (typically programming), and introductory engineering in several areas that are required for a satisfactory engineering background and to be successful in their program of choice. Several courses in social sciences or humanities are often also required to be taken, but are commonly elective courses from a broad choice. Required common engineering courses typically include engineering drawing/drafting, materials engineering, statics and dynamics, strength of materials, electrical engineering, thermodynamics, fluid mechanics, and perhaps some systems or industrial engineering. The science and engineering courses include lecture and laboratory education, either in the same course(s) or in separate courses. However, some professors and educators believe that engineering programs should change to focus more on professional engineering practice, and engineering courses should be taught more by professional engineering practitioners and not by engineering researchers.\n\nBy the end of the first year an engineering student should be looking to decide what specialization they would like to study. Specializations could include the following: civil (including structural), mechanical, electrical (often including computers), chemical, biological, industrial, aerospace, materials (including metallurgical), agricultural, and many other specializations. After choosing a specialization an engineering student will begin to take class that will build on the education that they have received and focus their future education toward their specialization or field of study. Towards the end of their undergraduate education, engineering students often undertake a design or other special project specific to their field.\n\nAfter formal education, the engineer will often enter an internship or engineer in training status for approximately four years. After that time the engineer in training can decide whether or not to take a state licensing test to make them a Professional Engineer. After successful completion of that test, the Professional engineer can place the initials P.E. after their name signifying that they are now a Professional Engineer.\nThere are also graduate degree options for an engineer. Many engineers decide to complete a master's degree in some field of engineering or business administration or get education in law, medicine, or other field.\n\nTwo types of doctorate are available also, the traditional Ph.D. or the doctor of engineering. The Ph.D. focuses on research and academic excellence, whereas the doctor of engineering focuses on practical engineering. The education requirements are the same for both degrees; however, the dissertation required is different. The Ph.D. requires the standard research problem, where the doctor of engineering focuses on a practical dissertation.\n\nIn present undergraduate engineering education, the emphasis on linear systems develops a way of thinking that dismisses nonlinear dynamics as spurious oscillations. The linear systems approach oversimplifies the dynamics of nonlinear systems. Hence, the undergraduate students and teachers should recognize the educational value of chaotic dynamics. Practicing engineers will also have more insight of nonlinear circuits and systems by having an exposure to chaotic phenomena.\n\nAfter graduation, continuing education courses may be needed to keep a government-issued professional engineer (PE) license valid, to keep skills fresh, to expand skills, or to keep up with new technology.\n\nIn Brazil, education in engineering is offered by both public and private institutions. A degree in engineering requires 5 to 6 years of studies, comprising the core courses, specific subjects, an internship and a \"Course Completion Paper\".\n\nDue to the nature of college admissions in Brazil, most students have to declare their major before entering college. This said, the first 2 years of a degree in engineering consist mostly of the core courses (calculus, physics, programming, etc.) along with a few specific subjects as well as some courses in humanities. After this period, some institutions offer specializations within the different fields of engineering (i.e. a student majoring in electrical engineering can choose to specialize in electronics or telecommunications) although most institutions balance their workload in order to give the students a consistent knowledge of every specialiation.\n\nTowards the end of their undergratuate education, students are required to develop the Course Completion Paper under the guidance of an adviser to be presented to and graded by a number of professors. In some institutions, studends are also required to pursue an internship (the amount of time depends on the institution).\n\nIn order to pursue a career in engineering, graduates must first register with and abide by the rules of the Regional Counsel of Engineering and Agronomy of their state, a regional representative of the Federal Counsel of Engineering and Agronomy, a certification board for engineers, agronomists, geologists and other professionals of the applied sciences.\n\n\n"}
{"id": "21859781", "url": "https://en.wikipedia.org/wiki?curid=21859781", "title": "Fishless cycling", "text": "Fishless cycling\n\nFishless cycling is a form of \"maturing\" an aquarium. In this process, ammonia is provided to allow beneficial bacteria to colonize. Fishless cycling can reduce the chance of fish loss resulting from insufficient populations of these bacteria.\n\nFishless cycling takes place over a period of several weeks, during which the aquarist provides an ammonia source for the development of the nitrifying bacterial colony. Nitrifying bacteria in the aquarium grow on all surfaces, but particularly in areas of high water flow and high surface area such as the filter. Allowing ammonia to be converted to nitrite, and on to the less harmful nitrate, minimizes stress and injury to aquarium fish. The \"nitrogen cycle\" may take several weeks to complete, but may be quicker under certain conditions. Higher water temperatures, greater dissolved oxygen and bacterial seeding seeding from an established tank may cut down the time required to less than a week.\n\nThe most significant advantage of fishless cycling is that it can reduce fish loss due to ammonia and nitrite spikes. Fish loss can be very discouraging for beginners of fish keeping, so indirectly, fishless cycling can also help beginners get a good start.\n\nCycling aquariums using feeder fish is risky because it infects the aquarium with any disease or parasite they happen to have. Fish raised as feeders do not get the same degree of care as non feeders. Fishless cycling avoids this potential problem.\n\nFishless cycling also allows the aquarium to be partially stocked from the moment it is cycled, if the nitrogen cycle is completed with high concentration of pure ammonia added daily. This makes for faster stocking than having to wait several weeks between each new group of additions to the tank. It can also be extremely useful when the fish keeper plan to stock a tank full of territorial aggressive fish such as African cichlids, where the later added fish can be at a disadvantage.\n\n\nNote that these are not problems when ammonium hydroxide or ammonium chloride is used as the source of ammonia.\n\n\n"}
{"id": "39756603", "url": "https://en.wikipedia.org/wiki?curid=39756603", "title": "Generalized filtering", "text": "Generalized filtering\n\nGeneralized filtering is a generic Bayesian filtering scheme for nonlinear state-space models. It is based on a variational principle of least action, formulated in generalized coordinates. Note that the concept of \"generalized coordinates\" as used here differs from the concept of generalized coordinates of motion as used in (multibody) dynamical systems analysis. Generalized filtering furnishes posterior densities over hidden states (and parameters) generating observed data using a generalized gradient descent on variational free energy, under the Laplace assumption. Unlike classical (e.g., Kalman-Bucy or particle) filtering, generalized filtering eschews Markovian assumptions about random fluctuations. Furthermore, it operates online, assimilating data to approximate the posterior density over unknown quantities, without the need for a backward pass. Special cases include variational filtering, dynamic expectation maximization and generalized predictive coding.\n\nDefinition: Generalized filtering rests on the tuple formula_1:\nHere ~ denotes a variable in generalized coordinates of motion: formula_11\n\nThe objective is to approximate the posterior density over hidden and control states, given sensor states and a generative model – and estimate the (path integral of) model evidence formula_12 to compare different models. This generally involves an intractable marginalization over hidden states, so model evidence (or marginal likelihood) is replaced with a variational free energy bound. Given the following definitions:\n\nDenote the Shannon entropy of the density formula_15 by formula_16. We can then write the variational free energy in two ways:\n\nThe second equality shows that minimizing variational free energy (i) minimizes the Kullback-Leibler divergence between the variational and true posterior density and (ii) renders the variational free energy (a bound approximation to) the negative log evidence (because the divergence can never be less than zero). Under the Laplace assumption formula_18 the variational density is Gaussian and the precision that minimizes free energy is formula_19. This means that free-energy can be expressed in terms of the variational mean (omitting constants):\n\nThe variational means that minimize the (path integral) of free energy can now be recovered by solving the generalized filter:\n\nwhere formula_22 is a block matrix derivative operator of identify matrices such that formula_23\n\nGeneralized filtering is based on the following lemma: \"The self-consistent solution to\" formula_24 \"satisfies the variational principle of stationary action, where action is the path integral of variational free energy\"\n\nProof: self-consistency requires the motion of the mean to be the mean of the motion and (by the fundamental lemma of variational calculus)\n\nPut simply, small perturbations to the path of the mean do not change variational free energy and it has the least action of all possible (local) paths.\n\nRemarks: Heuristically, generalized filtering performs a gradient descent on variational free energy in a moving frame of reference: formula_27, where the frame itself minimizes variational free energy. For a related example in statistical physics, see Kerr and Graham who use ensemble dynamics in generalized coordinates to provide a generalized phase-space version of Langevin and associated Fokker-Planck equations.\n\nIn practice, generalized filtering uses local linearization over intervals formula_28 to recover discrete updates\n\nThis updates the means of hidden variables at each interval (usually the interval between observations).\n\nUsually, the generative density or model is specified in terms of a nonlinear input-state-output model with continuous nonlinear functions:\n\nThe corresponding generalized model (under local linearity assumptions) obtains the from the chain rule\n\nGaussian assumptions about the random fluctuations formula_32 then prescribe the likelihood and empirical priors on the motion of hidden states\n\nThe covariances formula_34 factorize into a covariance among variables and correlations formula_35 among generalized fluctuations that encodes their autocorrelation:\n\nHere, formula_37 is the second derivative of the autocorrelation function evaluated at zero. This is a ubiquitous measure of roughness in the theory of stochastic processes. Crucially, the precision (inverse variance) of high order derivatives fall to zero fairly quickly, which means it is only necessary to model relatively low order generalized motion (usually between two and eight) for any given or parameterized autocorrelation function.\n\nWhen time series are observed as a discrete sequence of formula_38 observations, the implicit sampling is treated as part of the generative process, where (using Taylor's theorem)\n\nIn principle, the entire sequence could be used to estimate hidden variables at each point in time. However, the precision of samples in the past and future falls quickly and can be ignored. This allows the scheme to assimilate data online, using local observations around each time point (typically between two and eight).\n\nFor any slowly varying model parameters of the equations of motion formula_40 or precision formula_41 generalized filtering takes the following form (where formula_42 corresponds to the variational mean of the parameters)\n\nHere, the solution formula_44 minimizes variational free energy, when the motion of the mean is small. This can be seen by noting formula_45. It is straightforward to show that this solution corresponds to a classical Newton update.\n\nClassical filtering under Markovian or Wiener assumptions is equivalent to assuming the precision of the motion of random fluctuations is zero. In this limiting case, one only has to consider the states and their first derivative formula_46. This means generalized filtering takes the form of a Kalman-Bucy filter, with prediction and correction terms:\n\nSubstituting this first-order filtering into the discrete update scheme above gives the equivalent of (extended) Kalman filtering.\n\nParticle filtering is a sampling-based scheme that relaxes assumptions about the form of the variational or approximate posterior density. The corresponding generalized filtering scheme is called variational filtering. In variational filtering, an ensemble of particles diffuse over the free energy landscape in a frame of reference that moves with the expected (generalized) motion of the ensemble. This provides a relatively simple scheme that eschews Gaussian (unimodal) assumptions. Unlike particle filtering it does not require proposal densities—or the elimination or creation of particles.\n\nVariational Bayes rests on a mean field partition of the variational density:\n\nThis partition induces a variational update or step for each marginal density—that is usually solved analytically using conjugate priors. In generalized filtering, this leads to dynamic expectation maximisation. that comprises a D-step that optimizes the sufficient statistics of unknown states, an E-step for parameters and an M-step for precisions.\n\nGeneralized filtering is usually used to invert hierarchical models of the following form\n\nThe ensuing generalized gradient descent on free energy can then be expressed compactly in terms of prediction errors, where (omitting high order terms):\n\nHere, formula_51 is the precision of random fluctuations at the \"i\"-th level. This is known as generalized predictive coding [11], with linear predictive coding as a special case.\n\nGeneralized filtering has been primarily applied to biological timeseries—in particular functional magnetic resonance imaging and electrophysiological data. This is usually in the context of dynamic causal modelling to make inferences about the underlying architectures of (neuronal) systems generating data. It is also used to simulate inference in terms of generalized (hierarchical) predictive coding in the brain.\n\n\n"}
{"id": "58938242", "url": "https://en.wikipedia.org/wiki?curid=58938242", "title": "Giant Radio Array for Neutrino Detection", "text": "Giant Radio Array for Neutrino Detection\n\nThe Giant Radio Array for Neutrino Detection (GRAND) is a proposed large-scale detector designed to collect ultra-high energy cosmic particles as cosmic rays, neutrinos and photons with energies exceeding 10 eV. This project aims at solving the mystery of their origin and the early stages of the universe itself. The proposal, formulated by an international group of researchers, calls for an array of 200,000 receivers to be placed on mountain ranges around the world.\n\nThe GRAND detector would search for neutrinos, exotic particles emitted by some and the black holes in the center of galaxies. These neutrinos could help astronomers find the source of other energetic particles called ultra-high-energy cosmic rays. When neutrinos reach Earth, they often collide with particles either in the air or on the ground, creating showers of secondary particles. These secondary particles can be picked up by the radio antennas, which lets researchers calculate the trajectory of the initial neutrinos and trace them back to their source. The concept was first published in 2017. \n\nThe giant radio detector array would comprise 200,000 low-cost antennas in groups of 10,000 spread out over nearly 200,000 km (80,000 square miles) at different locations around the world. This would make it the largest detector in the world. Construction, installation and networking the 200,000 antennae, would cost approximately $226 million, excluding the price for renting the land and manpower.\n\nThe strategy of GRAND is to detect the radio emission coming from particle showers that develop in the terrestrial atmosphere as a result of the interaction of ultra-high energy (UHE) cosmic rays, gamma rays, and neutrinos. Astrophysical tau neutrinos () can be detected through extensive air showers (EAS) induced by tau () decays in the atmosphere. The short-lived tau decays in the atmosphere generates an EAS that emits measurable electromagnetic emissions up to frequencies of hundreds of MHz. The antennae are foreseen to operate in the 60-200 MHz band to avoid the short-wave background noise at lower frequencies. \n\nEach individual antenna is a simple Bow-tie design, featuring 3 perpendicular bows with an additional vertical arm to sample all three polarization directions. Each antenna is mounted on a single 5-meter-tall pole, and each antenna in the grid is spaced at 1 km within a square grid. If the full array of 200,000 antennae is build, GRAND would reach an all-flavor sensitivity of 4 x10 GeV cm s sr above 5 x10 eV. Because of its sub-degree angular resolution, GRAND will also search for point sources of UHE neutrinos, steady and transient, potentially starting UHE neutrino astronomy, allowing for the discovery and follow-up of large numbers of radio transients, fast radio bursts, giant radio pulses, and for precise studies of the epoch of reionization.\n\nThe researchers estimate that GRAND could allow not just the detection of neutrinos, but could also allow a differentiation of the source types, such as galaxy clusters with central sources, fast-spinning newborn pulsars, active galactic nuclei, and afterglows of gamma-ray bursts.\n\nSimulation and experimental work is ongoing on technological development and background rejection strategies. Phase one is called GRANDProto35, that includes 35 antennas and 24 scintillators, deployed in the Tian Shan mountains in China. If a pulse is observed simultaneously in the signals from three or more scintillators, the signals are recorded. As of October 2018, GRANDProto35 is in commissioning phase. So far, the system achieves 100% detection efficiency for trigger rates up to 20 kHz. \n\nThe following step is planned for 2020, and its is a dedicated setup called GRANDProto300 within an area of 300 km. The baseline layout is a square grid with a 1 km inter-antenna spacing, just as for later stages. Because GRANDProto300 will not be large enough to detect cosmogenic neutrinos, the viability will be tested using instead extensive air showers initiated by very inclined cosmic rays, thus providing an opportunity to do cosmic-ray science. The site would be hosted at the Chinese provinces of XinJiang, Inner Mongolia, Yunnan, and Gansu. If funded, the later phases would build GRAND10k in 2025, and finally GRAND200k (200,000 receivers) in the 2030s.\n\n"}
{"id": "31831357", "url": "https://en.wikipedia.org/wiki?curid=31831357", "title": "Glossary of elementary quantum mechanics", "text": "Glossary of elementary quantum mechanics\n\nThis is a glossary for the terminology often encountered in undergraduate quantum mechanics courses.\n\nCautions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart of a wave function of particle(s). See \"total wave function of a particle\".\n\nSynonymous to \"spin wave function\".\n\nPart of a wave function of particle(s). See \"total wave function of a particle\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48443254", "url": "https://en.wikipedia.org/wiki?curid=48443254", "title": "HerpMapper", "text": "HerpMapper\n\nHerpMapper is a cooperative citizen science project designed to gather and share information about reptile and amphibian (herp) observations across the planet. Contributors create records of their herp observations online or via its mobile application. Data are available to HerpMapper Partners – groups who use your recorded observations for research, conservation, and preservation purposes. In addition, the HerpMapper mobile application is used by multiple other herpetological atlas projects, and the Minnesota Nongame Wildlife Program.\n\nHerpMapper's primary goal is to share data with professional conservation and research organizations to better conserve herpetofauna around the world. Because of this, HerpMapper does not share point-location information publicly because of over-collection and poaching concerns. This sets HerpMapper apart from many other citizen-science projects.\n\nHerpMapper.org was officially launched in September 2013. It is currently a volunteer-run organization of professional herpetologists, IT specialists, and field herpers with decades of experience. HerpMapper is a registered 501(c)(3) nonprofit organization.\n\nThe HerpMapper Advisory Team includes nonprofit organization staff, state agency biologists, university faculty, IT professionals, and field herpers with decades of experience. Learn more on the About Us webpage.\n\nData are made freely available to professional conservation and research organizations. Data are made available via real-time online access or via one-time data transfers.\n"}
{"id": "346382", "url": "https://en.wikipedia.org/wiki?curid=346382", "title": "Image analysis", "text": "Image analysis\n\nImage analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.\n\nComputers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information. On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers. For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.\n\nDigital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. \nIt involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing. This field of computer science developed in the 1950s at academic institutions such as the MIT A.I. Lab, originally as a branch of artificial intelligence and robotics.\n\nIt is the quantitative or qualitative characterization of two-dimensional (2D) or three-dimensional (3D) digital images. 2D images are, for example, to be analyzed in computer vision, and 3D images in medical imaging. The field was established in the 1950s—1970s, for example with pioneering contributions by Azriel Rosenfeld, Herbert Freeman, Jack E. Bresenham, or King-Sun Fu.\n\nThere are many different techniques used in automatically analysing images. Each technique may be useful for a small range of tasks, however there still aren't any known methods of image analysis that are generic enough for wide ranges of tasks, compared to the abilities of a human's image analysing capabilities. Examples of image analysis techniques in different fields include:\n\nThe applications of digital image analysis are continuously expanding through all areas of science and industry, including:\n\n\"Object-Based Image Analysis\" (OBIA) employs two main processes, segmentation and classification. Traditional image segmentation is on a per-pixel basis. However, OBIA groups pixels into homogeneous objects. These objects can have different shapes and scale. Objects also have statistics associated with them which can be used to classify objects. Statistics can include geometry, context and texture of image objects. The analyst defines statistics in the classification process to generate for example land cover. The technique is implemented in software such as eCognition or the Orfeo toolbox.\n\nWhen applied to earth images, OBIA is known as \"Geographic Object-Based Image Analysis\" (GEOBIA), defined as \"a sub-discipline of geoinformation science devoted to (...) partitioning remote sensing (RS) imagery into meaningful image-objects, and assessing their characteristics through spatial, spectral and temporal scale\".\nThe international GEOBIA conference has been held biannually since 2006.\n\nObject-based image analysis is also applied in other fields, such as cell biology or medicine. It can for instance detect changes of cellular shapes in the process of cell differentiation.\n\nLand cover and land use change detection using remote sensing and geospatial data provides baseline information for assessing the climate change impacts on habitats and biodiversity, as well as natural resources, in the target areas.\n\n\n\n"}
{"id": "58136551", "url": "https://en.wikipedia.org/wiki?curid=58136551", "title": "Inclination instability", "text": "Inclination instability\n\nAn inclination instability is a dynamical instability that can occur in a disk of objects with eccentric orbits, causing it to form into a conical shape. The gravity of the objects causes an exponential growth of their inclinations while reducing their eccentricities. The inclination instability also results in a clustering of the arguments of perihelion of the objects orbits, similar to what has been observed among the extreme trans-Neptunian objects with semi-major axes greater than 150 AU, it does not produce an alignment of the longitudes of perihelion, however. For an inclination instability to be responsible for the observed clustering, a disk with a mass of 1-10 Earth masses must have existed for over a billion years. This is more than is estimated from current observations, and longer than the timescale of the depletion of the planetesimal disk in models of the early Solar System.\n\nIn a flat disk of objects with eccentric orbits a small initial vertical perturbation is amplified by the inclination instability. The initial perturbation exerts an vertical force. On very long timescales relative to the period of an object's orbit this force produces a net torque on the orbit due to the object spending more time near aphelion. This torque causes the plane of the orbit to roll on its major axis. In a disk this results in the orbits rolling with respect to each other so that the orbits are no longer co-planar. The gravity of the objects now exerts forces on each other that are out of planes of their orbits. Unlike the force due to the initial perturbation these forces are in opposite directions, up and down respectively, on the inbound and outbound portions of their orbits. The resulting torque causes their orbits to rotate about their minor axes, lifting their aphelia, causing the disk to form a cone. The angular momentum of the orbit is also increased due to this torque resulting in reduction of the eccentricity of the orbits. The inclination instability requires an initial eccentricity of 0.6 or larger, and saturates when inclinations reach ~1 radian, after which orbits precess due to the gravity toward the cone's axis of symmetry.\n"}
{"id": "13033965", "url": "https://en.wikipedia.org/wiki?curid=13033965", "title": "Institutional logic", "text": "Institutional logic\n\nInstitutional logic is a core concept in sociological theory and organizational studies. It focuses on how broader belief systems shape the cognition and behavior of actors.\n\nFriedland and Alford (1991) defined Institutions as \"both supraorganizational patterns of activity by which individuals and organizations produce and reproduce their material subsistence and organize time and space. They are also symbolic systems, ways of ordering reality, thereby rendering experience of time and space meaningful\". Thornton and Ocasio (1999: 804) define institutional logics as \"the socially constructed, historical patterns of material practices, assumptions, values, beliefs, and rules by which individuals produce and reproduce their material subsistence, organize time and space, and provide meaning to their social reality\".\n\nFocusing on macro-societal phenomena, Friedland and Alford (1991: 232) identified several key Institutions: the Capitalist market, bureaucratic state, democracy, nuclear family, and Christianity that are each guided by a distinct institutional logic. Thornton (2004) revised Friedland and Alford’s (1991) inter-institutional scheme to six institutional orders, i.e., the market, the corporation, the professions, the state, the family, and religions. More recently, Thornton, Ocasio and Lounsbury (2012), in more fully fleshing out the institutional logic perspective, added community as another key institutional order. This revision to a theoretically abstract and analytically distinct set of ideal types makes it useful for studying multiple logics in conflict and consensus, the hybridization of logics, and institutions in other parts of society and the world. While building on Friedland and Alford’s scheme, the revision addresses the confusion created by conflating institutional sectors with ideology (democracy) and means of organization (bureaucracy), variables that can be characteristic several different institutional sectors. The institutional logic of Christianity leaves out other religions in the US and other religions that are dominant in other parts of the world. Thornton and Ocasio (2008) discuss the importance of not confusing the ideal types of the inter-institutional system with a description of the empirical observations in a study—that is to use the ideal types as meta theory and method of analysis.\n\nOrganizational theorists operating within the new institutionalism (see also institutional theory) have begun to develop the institutional logics concept by empirically testing it. One variant emphasizes how logics can focus the attention of key decision-makers on a particular set of issues and solutions (Ocasio, 1997), leading to logic-consistent decisions (Thornton, 2002). A fair amount of research on logics has focused on the importance of dominant logics and shifts from one logic to another (e.g., Lounsbury, 2002; Thornton, 2002; Suddaby & Greenwood, 2005). Haveman and Rao (1997) showed how the rise of Progressive thought enabled a shift in savings and loan organizational forms in the U.S. in the early 20th century. Scott et al. (2000) detailed how logic shifts in healthcare led to the valorization of different actors, behaviors and governance structures. Thornton and Ocasio (1999) analyzed how a change from professional to market logics in U.S. higher education publishing led to corollary changes in how executive succession was carried out.\n\nWhile much earlier work focused on ambiguity as a result of multiple and conflicting institutional logics, at the levels of analysis of society and individual roles, Friedland and Alford (1991:248-255) discussed in theory multiple and competing logics at the macro level of analysis. Recent empirical research, inspired by the work of Bourdieu, is developing a more pluralistic approach by focusing on multiple competing logics and contestation of meaning. By focusing on how some fields are composed of multiple logics, and thus, multiple forms of institutionally-based rationality, institutional analysts can provide new insight into practice variation and the dynamics of practice. Multiple logics can create diversity in practice by enabling variety in cognitive orientation and contestation over which practices are appropriate. As a result, such multiplicity can create enormous ambiguity, leading to logic blending, the creation of new logics, and the continued emergence of new practice variants. Thornton, Jones, and Kury (2005) showed how competing logics may never resolve but share the market space as in the case of architectural services.\n\nRecent research has also documented the co-existence or potential conflict of multiple logics within particular organizations. Zilber (2002), for example, described the organizational consequences of a shift from one logic to another within an Israeli rape crisis center, in which new organization members reshaped the center and its practices to reflect a new dominant logic that they have carried into the organization. Tilcsik (2010) documented a logic shift in a post-Communist government agency, describing a conflict between the agency's old guard (carriers of the logic of Communist state bureaucracies) and its new guard (carriers of a market logic). This study shows that, paradoxically, an intra-organizational group's efforts to resist a particular logic might in fact open the organization's door to carriers of that very logic. Almandoz (2012) examined the embeddedness of new local banks' founding teams in a community logic or a financial logic, linking institutional logics to new banking venture's establishment and entrepreneurial success. As these studies demonstrate, the institutional logics perspective offers valuable insights into important intra-organizational processes affecting organizational practices, change, and success. These studies represent an effort to understand institutional complexity due to conflicting or inconsistent logics within particular organizations, a situation that might results from the entry of new organization members or the layering (or \"sedimentation\") of new organizational imprints upon old ones over time.\n\n\n"}
{"id": "12163987", "url": "https://en.wikipedia.org/wiki?curid=12163987", "title": "Jean Antoine Coquebert de Montbret", "text": "Jean Antoine Coquebert de Montbret\n\nJean Antoine Coquebert de Montbret (1753, Paris- 6 April 1825) was a French entomologist.\nHe wrote \"Illustratio iconographica insectorum quae in musaeis parisinis observavit et in lucem edidit Joh. Christ. Fabricius, praemissis ejusdem descriptionibus; accedunt species plurimae, vel minus aut nondum cognitae\", Paris: P. Didot, 1799-1804 an illustrated work on insect specimens in the Museum d'Histoire Naturelle in Paris. The insects appear as inside an insect box.\n\n\n"}
{"id": "53853398", "url": "https://en.wikipedia.org/wiki?curid=53853398", "title": "János Bergou", "text": "János Bergou\n\nJános Bergou (born 15 March 1947) is a Hungarian physicist and academic who is currently a professor at Hunter College in New York. In 2009, he was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Laser Science in 2009, for \"outstanding work in quantum optics and quantum information, in particular work on the theory of correlated emission lasers, the effect of pump statistics on the nature of the electromagnetic field produced in lasers and micromasers, and on quantum state discrimination.\"\n\nBergou earned a master's in science (1970) and earned his PhD \"summa cum laude\" in Theoretical Physics (1975) from Eötvös Loránd University in Budapest. From the Hungarian Academy of Sciences, he received his Habilitation (CSc) (1982) and Doctor of Sciences (DSc) (1994).\n"}
{"id": "33036905", "url": "https://en.wikipedia.org/wiki?curid=33036905", "title": "Laurie Glimcher", "text": "Laurie Glimcher\n\nLaurie Hollis Glimcher is an American physician-scientist who was appointed President and CEO of Dana-Farber Cancer Institute in October 2016. \n\nGlimcher's research has focused on the immune system; she is known for early work with T cell differentiation, her discovery that Schnurri-3 regulates osteoblasts which led to a collaboration with Merck & Co., and her discovery of the role played by XBP-1 in lipogenesis and the unfolded protein response. Glimcher's role helped discover Schnurri-3 (Shn3 for short) is a large zinc finger protein distantly related to Drosophila. Shn is a potent and essential regulator of adult bone formation. Her research has had implications for understanding asthma, HIV, inflammatory bowel disease, and osteoporosis, and around 2016, on cancer immunotherapy.\n\nShe joined the board of directors of Bristol-Myers Squibb in 1997 and retired from the board in 2017. Her research laboratory received funding from Merck & Co for a project focused on developing new therapies for the treatment of osteoporosis in 2008. \n\nGlimcher was the Irene Heinz Given Professor of Immunology at the Harvard School of Public Health, a professor of medicine at Harvard Medical School. Clinically, she is a specialist in osteoporosis.\n\nFrom 2012 to 2016 Glimcher served as the Stephen and Suzanne Weiss Dean of Weill Cornell Medical College and the Cornell University Provost for Medical Affairs.\n\nIn February 2016, Laurie Glimcher was named the next president and CEO of Dana-Farber Cancer Institute. Glimcher was considered for the position for the Dean of Harvard Medical School but turned the position down in order to become the president of the Dana-Farber Cancer Institute. At Dana-Farber, Glimcher is collaborating on research which strives to find methods of combatting cancer from within the human immune system. The Dana-Farber Cancer Institute is an institution that is affiliated with Harvard, as it currently is one of its teaching hospitals. Glimcher, who was the first female dean of any medical school in New York state, became the first female to lead the Dana-Farber Cancer Institute.\n\nGlimcher became interested in immunology during her first year of medical school at Harvard. There she took interest dysregulation in autoimmune diseases and, in her fourth year at Harvard, discovered the protein known as Nk1.1(see natural killer T cell), which soon became widely recognized across the field of immunology. For this discovery, Glimcher became the first woman to receive the Soma Weiss Award, an honor her father had received 26 years earlier. During this time, Glimcher worked with mentor Bill Paul, who strongly encouraged her to continue her research independently after completing medical school.\nDuring her second year of residency, Glimcher realized why she felt so drawn to this area of research. “What always fascinated me was not so much treating patients with disease, but figuring out why they had the disease,” says Glimcher.\nGlimcher currently heads her own lab for research in immunology. She has been interested in studying the ties between ER stress system in neurons and immune function and neuro-degeneration. Her past work has involved regulation of immune function and has shifted towards osteobiology with a focus on the bone disease osteoporosis. Her Harvard lab has a three-year contract with Merck for the drug Fosamax, a treatment for osteoporosis. Glimcher’s more current research looks to answer the question, “how does the immune system and the ER stress system in neurons impact neurodegenerative diseases?”.\n\nGlimcher is a member of the National Academy of Sciences. She received the L'Oréal-UNESCO Awards for Women in Science in 2014 for her work in the field of immunology and her research regarding the control of immune responses. She received the in 2015. In 2018, she received the American Association of Immunologists Lifetime Achievement Award.\n\nLaurie Glimcher is a part of many different organizations and memberships. She is a member of:\n\nLaurie Glimcher's father is Melvin Glimcher, who was a pioneer in the development of artificial limbs while the chair of the Massachusetts General Hospital Orthopedics Department. Glimcher followed in the footsteps of her father by later becoming a full professor at Harvard Medical School at the age of 39; the two became research partners. Her mother was Geraldine Lee Bogolub. Her husband is Gregory Petsko, who was Director of the Rosenstiel Basic Medical Sciences Research Center and Chair of Biochemistry at Brandeis University prior to moving to Weill Cornell, where he became Director of the Helen and Robert Appel Alzheimer's Disease Research Institute. Glimcher was previously married to Hugh Auchincloss, who was a chief of Transplant Surgery at Brigham & Women's Hospital, with whom she had three children, Kalah, Hugh and Jake Auchincloss. In 2016, she had one grandchild. Her daughter, Kalah Auchincloss, J.D., M.P.H., is an Senior Vice President, Regulatory Compliance and Deputy General Counsel for Greenleaf Health following six years at the Food & Drug Administration as Deputy Chief of Staff for two FDA Commissioners. Her older son, Dr. Hugh Glimcher Auchincloss, is also a physician, who was in training as a cardiothoracic surgeon at Massachusetts General Hospital in 2016. Her younger son, Jake Auchincloss, is a cybersecurity professional and a Newton, Massachusetts City Counselor-at-large following service as an infantry officer in the Marine Corps.\n\nLaurie Glimcher has been considered a champion of women's rights in the scientific by many of her peers. While she was at Harvard, she hired lab technicians with her own research fund to support her postdoctoral fellows after they had babies so that they were allowed to leave by 6. This carried on into Glimcher's involvement with the National Institutes of Health to create a similar postdoc grant program caring for family members.\n\nGlimcher served on the 2005 Larry Summers Task Force for Women in Science and Engineering, where she expressed her disappointment in the rate of progress for women in science. She joined this task force after a controversy was sparked when former Harvard president Larry Summers suggested that women might be able to innately do less in science. Although she was on the Larry Summers committee, Glimcher still believes that there is still more work to be done. She was quoted as saying: \"There are not enough women in senior leadership positions, period. It hasn't gotten a heck of a lot better since I was in medical school\". After she was appointed to Cornell's medical school she immediately made changes regarding women's rights. She established paid maternity leave, created day care centers and another postdoc grant program for primary caregivers. Upon arriving at Cornell there were 0 out of 19 clinical department chairs filled by women; as of today there are 2.\n\nFrom October 2015 to February 2016, Glimcher was the target of eight protests by animal rights activists angered by New York Blood Center's abandonment of 66 chimpanzees that had been used in medical research; Glimcher was a member of the board of directors, which had voted to stop paying for care of the chimpanzees before she joined it. Glimcher said that the decision was made before her term on the board and said: \"As a scientist, I strongly support the ethical and humane treatment of animals used in research... I have a great respect for these animals and recognize the value they bring in our pursuit of new cures for devastating human diseases.\" The New York Blood Center had funded a Liberian lab since 1974 where the chimps had been used for testing of different viruses. The New York Blood Center ended their funding of the Liberian lab in March 2015.\n\n"}
{"id": "39753811", "url": "https://en.wikipedia.org/wiki?curid=39753811", "title": "List of colors: G–M", "text": "List of colors: G–M\n\n\"For the continuation of the list of colors, please go to .\"\n"}
{"id": "760639", "url": "https://en.wikipedia.org/wiki?curid=760639", "title": "List of file systems", "text": "List of file systems\n\nThe following lists identify, characterize, and link to more thorough information on computer file systems.\n\nMany older operating systems support only their one \"native\" file system, which does not bear any name apart from the name of the operating system itself.\n\nDisk file systems are usually block-oriented. Files in a block-oriented file system are sequences of blocks, often featuring fully random-access read, write, and modify operations.\n\n\nThese file systems have built-in checksumming and either mirroring or parity for extra redundancy on one or several block devices:\n\nSolid state media, such as flash memory, are similar to disks in their interfaces, but have different problems. At low level, they require special handling such as wear leveling and different error detection and correction algorithms. Typically a device such as a solid-state drive handles such operations internally and therefore a regular file system can be used. However, for certain specialized installations (embedded systems, industrial applications) a file system optimized for plain flash memory is advantageous.\n\n\nIn record-oriented file systems files are stored as a collection of records. They are typically associated with mainframe and minicomputer operating systems. Programs read and write whole records, rather than bytes or arbitrary byte ranges, and can seek to a record boundary but not within records. The more sophisticated record-oriented file systems have more in common with simple databases than with other file systems.\n\n\nShared-disk file systems (also called \"shared-storage file systems\", SAN file system, Clustered file system or even \"cluster file systems\") are primarily used in a storage area network where all nodes directly access the block storage where the file system is located. This makes it possible for nodes to fail without affecting access to the file system from the other nodes. Shared-disk file systems are normally used in a high-availability cluster together with storage on hardware RAID. Shared-disk file systems normally do not scale over 64 or 128 nodes.\n\nShared-disk file systems may be symmetric where metadata is distributed among the nodes or asymmetric with centralized metadata servers.\n\n\nDistributed file systems are also called network file systems. Many implementations have been made, they are location dependent and they have access control lists (ACLs), unless otherwise stated below.\n\n\nDistributed fault-tolerant replication of data between nodes (between servers or servers/clients) for high availability and offline (disconnected) operation.\n\n\nDistributed parallel file systems stripe data over multiple servers for high performance. They are normally used in high-performance computing (HPC).\n\nSome of the distributed parallel file systems use object storage device (OSD) (In Lustre called OST) for chunks of data together with centralized metadata servers.\n\n\nDistributed file systems, which also are parallel and fault tolerant, stripe and replicate data over multiple servers for high performance and to maintain data integrity. Even if a server fails no data is lost. The file systems are used in both high-performance computing (HPC) and high-availability clusters.\n\nAll file systems listed here focus on high availability, scalability and high performance unless otherwise stated below.\n\nIn development:\n\n\nSome of these may be called cooperative storage cloud.\n\n\n\n\n\nThese are not really file systems; they allow access to file systems from an operating system standpoint.\n\n\n\n"}
{"id": "38353349", "url": "https://en.wikipedia.org/wiki?curid=38353349", "title": "List of open-access journals", "text": "List of open-access journals\n\nThis is a list of open-access journals by field. The list contains notable journals which have a policy of full open access. It does not include delayed open access journals, hybrid open access journals, or related collections or indexing services.\n\nTrue open-access journals can be split into two categories : \n\nThe list below is focused on open-access journals with no fees. \nHowever, some fields like biology and medicine have a stronger tradition of article processing charge,\nthe corresponding journals below then have a footnote indicating such fees.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1230056", "url": "https://en.wikipedia.org/wiki?curid=1230056", "title": "List of quantum gravity researchers", "text": "List of quantum gravity researchers\n\nThis is a list of (some of) the researchers in quantum gravity.\n\n\n"}
{"id": "31970422", "url": "https://en.wikipedia.org/wiki?curid=31970422", "title": "List of taxonomic authorities by name", "text": "List of taxonomic authorities by name\n\nFollowing is a partial list of taxonomic authorities by name — for taxonomists with some common surnames.\n\n\n\n\"See for species named after taxonomic authorities named Clark\".\n\n\"See for species named after taxonomic authorities named Graii\".\n\n\n\n\"See for species named after taxonomic authorities named Smith\".\n\n\n\"See for species named after taxonomic authorities named Walker\".\n\n"}
{"id": "37051625", "url": "https://en.wikipedia.org/wiki?curid=37051625", "title": "List of things named after Adrien-Marie Legendre", "text": "List of things named after Adrien-Marie Legendre\n\nAdrien-Marie Legendre (1752–1833) is the eponym of all of the things listed below.\n\n"}
{"id": "4755911", "url": "https://en.wikipedia.org/wiki?curid=4755911", "title": "List of vegetable oils", "text": "List of vegetable oils\n\n<onlyinclude> \nVegetable oils are triglycerides extracted from plants. These oils have been part of human culture for millennia. Edible vegetable oils are used in food, both in cooking and as supplements. Many oils, edible and otherwise, are burned as fuel, such as in oil lamps and as a substitute for petroleum-based fuels. Some of the many other uses include wood finishing, oil painting, and skin care.</onlyinclude>\n\nThere are several types of plant oils, distinguished by the method used to extract the oil from the plant. The relevant part of the plant may be placed under pressure to extract the oil, giving an expressed (or pressed) oil. The oils included in this list are of this type. Oils may also be extracted from plants by dissolving parts of plants in water or another solvent. The solution may be separated from the plant material and concentrated, giving an extracted or leached oil. The mixture may also be separated by distilling the oil away from the plant material. Oils extracted by this latter method are called essential oils. Essential oils often have different properties and uses than pressed or leached vegetable oils. Finally, macerated oils are made by infusing parts of plants in a base oil, a process called liquid–liquid extraction.\n\nThe term \"vegetable oil\" can be narrowly defined as referring only to substances that are liquid at room temperature, or broadly defined without regard to a substance's state of matter at a given temperature. While a large majority of the entries in this list fit the narrower of these definitions, some do not qualify as vegetable oils according to all understandings of the term.\n\nAlthough most plants contain some oil, only the oil from certain major oil crops complemented by a few dozen minor oil crops is widely used and traded.\n\nVegetable oils can be classified in several ways, for example:\n\n\nThe vegetable oils are grouped below in common classes of use.\n\n<onlyinclude>\n\nThese oils make up a significant fraction of worldwide edible oil production. All are also used as fuel oils.\n\n\nNut oils are generally used in cooking, for their flavor. Most are quite costly, because of the difficulty of extracting the oil.\n\n\nA number of citrus plants yield pressed oils. Some, such as lemon and orange oil, are used as essential oils, which is uncommon for pressed oils. The seeds of many if not most members of the citrus family yield usable oils.\n\n\nMembers of the Cucurbitaceae include gourds, melons, pumpkins, and squashes. Seeds from these plants are noted for their oil content, but little information is available on methods of extracting the oil. In most cases, the plants are grown as food, with dietary use of the oils as a byproduct of using the seeds as food.\n\n\nA number of oils are used as food supplements (or \"nutraceuticals\"), for their nutrient content or purported medicinal effect. Borage seed oil, blackcurrant seed oil, and evening primrose oil all have a significant amount of gamma-Linolenic acid (GLA) (about 23%, 15–20% and 7–10%, respectively), and it is this that has drawn the interest of researchers.\n\n\n\n\n\n\n\nA number of oils are used for biofuel (biodiesel and Straight Vegetable Oil) in addition to having other uses. Other oils are used only as biofuel.\n\nAlthough diesel engines were invented, in part, with vegetable oil in mind, diesel fuel is almost exclusively petroleum-based. Vegetable oils are evaluated for use as a biofuel based on:\n\n\nThe oils listed immediately below are all (primarily) used for other purposes all but tung oil are edible but have been considered for use as biofuel.\n\n\nThese oils are extracted from plants that are cultivated solely for producing oil-based biofuel. These, plus the major oils described above, have received much more attention as fuel oils than other plant oils.\n\n\nDrying oils are vegetable oils that dry to a hard finish at normal room temperature. Such oils are used as the basis of oil paints, and in other paint and wood finishing applications. In addition to the oils listed here, walnut, sunflower and safflower oil are also considered to be drying oils.\n\nA number of pressed vegetable oils are either not edible, or not used as an edible oil.\n\n\n\n"}
{"id": "30612838", "url": "https://en.wikipedia.org/wiki?curid=30612838", "title": "Marion Dietrich", "text": "Marion Dietrich\n\nMarion Dietrich (1926–1974) was a pilot and one of the Mercury 13 who underwent the same NASA testing in the early 1960s as the Mercury 7 astronauts.\n\nBorn in San Francisco in 1926, Dietrich was the daughter of Richard Dietrich, who worked in the import business, and his wife, Marion. Dietrich began flying at an early age, getting a student pilot certificate at age 16. She and identical twin sister Janet Dietrich were the only girls in an aviation class at Burlingame High School.\n\nIn 1947, Dietrich and sister Janet entered the inaugural Chico-to-San Mateo Air Race and took first place, defeating experienced men. Dietrich graduated from the University of California, Berkeley in 1949, with degrees in mathematics and psychology. After placing in other local racess, the flying twins collected the second-place trophy in the 1951 All-Women's Transcontinental Air Race, known as the Powder Puff Derby. Dietrich worked for a time as a newspaper reporter for the \"Oakland Tribune\", flying supersonic as a passenger in a fighter aircraft on a story assignment. She also became a commercial transport pilot, flying charter and ferry flights.\n\nIn 1960, Dietrich and her sister were among a select group of female aviators invited to the Lovelace Clinic in Albuquerque, where experts had screened potential NASA astronauts. The women underwent the same medical tests and examinations as Alan Shepard, John Glenn, and the other men who eventually traveled into space. The extensive exams included everything from swallowing 3 feet of rubber hose to drinking radioactive water. Though only 5 feet 3 inches tall and 100 pounds, Dietrich completed the regimen of tests, as did her sister and 11 other women.\n\nWhile the women waited for the next phase of their program in July 1961, the testing was halted without warning or explanation. It would be two more decades before the United States launched its first woman into space, Sally Ride, an astrophysicist turned astronaut.\n\nDietrich died in 1974 from cancer.\n\nIn 2006, the International Women's Air & Space Museum opened an exhibit honoring the Mercury 13 - Mercury Women: Forgotten Link to the Future. And in May 2007, the women of Mercury 13 received honorary doctor of science degrees from the University of Wisconsin-Oshkosh.\n"}
{"id": "53901341", "url": "https://en.wikipedia.org/wiki?curid=53901341", "title": "Marrow adipose tissue", "text": "Marrow adipose tissue\n\nMarrow adipose tissue (MAT) , also known as bone marrow adipose tissue (BMAT), increases in states of low bone density -osteoporosis, anorexia nervosa/ caloric restriction, skeletal unweighting, anti-diabetes therapies). The marrow adipocytes originate from mesenchymal stem cell (MSC) progenitors that also give rise to osteoblasts, among other cell types. Thus, it is thought that MAT results from preferential MSC differentiation into the adipocyte, rather than osteoblast, lineage in the setting of osteoporosis. Since MAT is increased in the setting of obesity and is suppressed by endurance exercise, or vibration, it is likely that MAT physiology- in the setting of mechanical input/exercise- approximates that of white adipose tissue (WAT). The first study to demonstrate exercise regulation of MAT in rodents was published in 2014; now, exercise regulation of MAT has been confirmed in a human study as well adding clinical significance to this work.\n\nMAT has qualities of both white and brown fat. Subcutaneous white fat contain excess energy, indicating a clear evolutionary advantage during times of scarcity. WAT is also the source of adipokines and inflammatory markers which have both positive (e.g., adiponectin) and negative effects on metabolic and cardiovascular endpoints. Visceral abdominal fat (VAT) is a distinct type of WAT that is \"proportionally associated with negative metabolic and cardiovascular morbidity\", regenerates cortisol, and recently has been tied to decreased bone formation Both types of WAT substantially differ from brown adipose tissue (BAT) as by a group of proteins that help BAT’s thermogenic role. MAT, by its \"specific marrow location, and its adipocyte origin from at least LepR+ marrow MSC is separated from non-bone fat storage by larger expression of bone transcription factors\", and likely indicates a different fat phenotype. Recently, MAT was noted to \"produce a greater proportion of adiponectin - an adipokine associated with improved metabolism - than WAT\", suggesting an endocrine function for this depot, akin, but different, from that of WAT.\n\nMAT increases in states of bone fragility. MAT is thought to result from preferential MSC differentiation into an adipocyte, rather than osteoblast lineage in osteoporosis based on the inverse relationship between bone and MAT in bone-fragile osteoporotic states. An increase in MAT is noted in osteoporosis clinical studies measured by MR Spectroscopy. Estrogen therapy in postmenopausal osteoporosis reduces MAT. Antiresorptive therapies like risedronate or zoledronate also decrease MAT while increasing bone density, supporting an inverse relationship between bone quantity and MAT. During aging, bone quantity declines and fat redistributes from subcutaneous to sites such as bone marrow, muscle, and liver. Aging is associated with lower osteogenic and greater adipogenic biasing of MSC. This aging-related biasing of MSC away from osteoblast lineage may represent higher basal PPARγ expression or decreased Wnt10b. Thus, bone fragility, osteoporosis, and osteoporotic fractures are thought to be linked to mechanisms which promote MAT accumulation.\n\nIn order to understand the physiology of MAT, various analytic methods have been applied. Marrow adipocytes are difficult to isolate and quantify because they are interspersed with bony and hematopoietic elements. Until recently, qualitative measurements of MAT have relied on bone histology, which is subject to site selection bias and cannot adequately quantify the volume of fat in the marrow. Nevertheless, histological techniques and fixation make possible visualization of MAT, quantification of adipocyte size, and MAT’s association with the surrounding endosteum, milieu of cells, and secreted factors.\n\nRecent advances in cell surface and intracellular marker identification and single-cell analyses led to greater resolution and high-throughput \"ex-vivo\" quantification. Flow cytometric quantification can be used to purify adipocytes from the stromal vascular fraction of most fat depots. Early research with such machinery cited adipocytes as too large and fragile for cytometer-based purification, rendering them susceptible to lysis; however, recent advances have been made to mitigate this; nevertheless, this methodology continues to pose technical challenges and is inaccessible to much of the research community.\n\nTo improve quantification of MAT, novel imaging techniques have been developed as a means to visualize and quantify MAT. Although proton magnetic resonance spectroscopy (1H-MRS) has been used with success to quantify vertebral MAT in humans, it is difficult to employ in laboratory animals. Magnetic resonance imaging (MRI) provides MAT assessment in the vertebral skeleton in conjunction with μCT-based marrow density measurements. A volumetric method to identify, quantify, and localize MAT in rodent bone has been recently developed, requiring osmium staining of bones and μCT imaging, followed by advanced image analysis of osmium-bound lipid volume (in mm) relative to bone volume. This technique provides reproducible quantification and visualization of MAT, enabling the ability to consistently quantify changes in MAT with diet, exercise, and agents that constrain precursor lineage allocation. Although the osmium method is quantitatively precise, osmium is toxic and cannot be compared across batched experiments. Recently, researchers developed and validated a 9.4T MRI scanner technique that allows localization and volumetric (3D) quantification that can be compared across experiments. \nHematopoietic cells (also known as blood cells) reside in the bone marrow along with marrow adipocytes. These hematopoietic cells are derived from hematopoietic stem cells (HSC) which give rise to diverse cells: cells of the blood, immune system, as well as cells that break down bone (osteoclasts). HSC renewal occurs in the marrow stem cell niche, a microenvironment that contains cells and secreted factors that promote appropriate renewal and differentiation of HSC. The study of the stem cell niche is relevant to the field of oncology in order to improve therapy for multiple hematologic cancers. As such cancers are often treated with bone marrow transplantation, there is interest in improving the renewal of HSC. Recent work demonstrates that marrow adipocytes secrete factors that promote HSC renewal in most bones.\n\n"}
{"id": "57984778", "url": "https://en.wikipedia.org/wiki?curid=57984778", "title": "Nanodumbbell", "text": "Nanodumbbell\n\nA nanodumbell is a pair of spheres attached together that may be made of silica or zinc oxide. \n\nThey have been used in a Purdue University experiment where they were made to spin in a vacuum at 60 billion rotations per minute.\n\nThe nanodumbbells are first created in the lab using a hydro-thermal process. The resulting dumbbell consists of two joined silica spheres, making it 320 nanometers long and around 170 nanometers wide in size.\n\nNanodumbbells are also being studied for possible use in photodynamic therapy, a way of treating cancer.\n\nHighly focused circularly polarized light laser light bombards the levitated dumbbell to set it spinning.\n\nThe speed of the rotation is a world record that beats previous records. In 2008, a small motor rotated at 1 million rotations per minute. In 2010, a slice of graphene was made to spin at 60 million spins per minute. Around 2013, a sphere measuring just 4 micrometers was spun at 600 million spins per minute.\n\n"}
{"id": "1257241", "url": "https://en.wikipedia.org/wiki?curid=1257241", "title": "National Radiological Protection Board", "text": "National Radiological Protection Board\n\nThe National Radiological Protection Board (NRPB) was a public authority in the UK created by the Radiological Protection Act 1970. Its statutory functions were to conduct research on radiological protection and provide advice and information on the subject to Government Departments and others. It was also authorized to provide technical services and charge for them. Originally NRPB dealt only with ionizing radiation, but its functions were extended in 1974 to non-ionizing radiation.\n\nThe Board consisted of a chairman and a maximum of nine other members, later increased to twelve, all appointed by Health Ministers. Throughout its existence, NRPB had 300 members of staff on average. They were located at the headquarters in Chilton near Oxford and at laboratories in Leeds and Glasgow. The Department of Health funded the difference between the cost of NRPB and its income by annual grant.\n\nResearch on ionizing radiation included: plutonium exposure; internal dosimetry; radioactive discharges; nuclear accidents and wastes; radon hazards; medical x rays; epidemiology and molecular biology. Research on non-ionizing radiation included the physics and biology of exposure to ultraviolet sources, electricity supplies, and mobile phones.\n\nAs well as a full range of technical services - from personnel dosimetry to radiation surveys - NRPB also engaged in projects such as: the safe transport of radioactive materials; preparedness for nuclear emergencies; exposure to cosmic rays; optimization of protection; improved radiation instruments; training courses; a wide selection of publications.\n\nMembers of staff contributed to major public inquiries about the nuclear industry in the UK and supported the UK response to the Chernobyl disaster. They also participated in the work of the: International Commission on Radiological Protection; International Commission on Non-Ionizing Radiation Protection; United Nations Scientific Committee on the Effects of Atomic Radiation; Nuclear Energy Agency; various committees of the Commission of the European Communities.\n\nThe Health Protection Agency Act 2004 repealed the Radiological Protection Act. On 1 April 2005, NRPB became the Radiation Protection Division of the Health Protection Agency (HPA). Under the terms of the Health and Social Care Act 2012, the HPA was abolished, and responsibility for radiation protection functions was assigned to appropriate Government authorities in the UK.\n\n\n"}
{"id": "12753475", "url": "https://en.wikipedia.org/wiki?curid=12753475", "title": "Nonthermal surface reaction", "text": "Nonthermal surface reaction\n\nIn the field of physical chemistry, a nonthermal surface reaction refers to an elementary reaction between a thermally accommodated adsorbed surface species and a reactant which has not yet thermally accommodated to the surface.\nThe two main mechanisms classified as nonthermal are the Eley-Rideal and hot atom mediated mechanisms.\n"}
{"id": "5841368", "url": "https://en.wikipedia.org/wiki?curid=5841368", "title": "Oliver Bulman", "text": "Oliver Bulman\n\nOliver Meredith Boone Bulman (20 May 1902 – 18 February 1974) was a British palaeontologist. He was Woodwardian Professor of Geology at the University of Cambridge.\n\nOliver Bulman was born in Chelsea to artist Henry Herbert Bulman and his wife Beatrice Elizabeth Boone. He was the second of three children.\n\nBulman went to Battersea Grammar School in 1910, but wishing to study geology, which the school did not teach, he became an evening, and later day, student at Chelsea Polytechnic. He gained a London University scholarship in 1920 and went to Imperial College to study geology and zoology. He graduated with a first class BSc in geology in 1923. Bulman went on to a PhD degree jointly with James Stubblefield on the lower Palaeozoic of the Wrekin district, of Shropshire in 1926.\n\nAwarded a senior studentship, he worked for a year on Permian amphibians with Walter Frederick Whittard and for two years at Sidney Sussex College, Cambridge, where he studied dendroid graptolites under Gertrude Lilian Elles. Work on the Palaeontographical Society's monograph \"British Dendroid Graptolites\" (1927 and 1928) earned him a Cambridge PhD degree in 1928. He then worked as demonstrator at Imperial College and at Cambridge. He became reader in palaeozoology in 1944 and Woodwardian Professor of Geology in 1955.\n\nBulman was elected an FRS in 1940 and was president of the geology section of the British Association, the Palaeontological Association (1960–62), the Geological Society (1962–4), and the Palaeontographical Society (1971–4). The Geological Society awarded him the Lyell Medal in 1953.\n\nIn 1938 he married Marguerite Fearnsides, daughter of William Fearnsides, the professor of geology at Sheffield. They had a son and three daughters. He died at home in Cambridge in 1974, and was cremated.\n"}
{"id": "21142899", "url": "https://en.wikipedia.org/wiki?curid=21142899", "title": "Pasteur's quadrant", "text": "Pasteur's quadrant\n\nPasteur's quadrant is a classification of scientific research projects that seek fundamental understanding of scientific problems, while also having immediate use for society. Louis Pasteur's research is thought to exemplify this type of method, which bridges the gap between \"basic\" and \"applied\" research. The term was introduced by Donald Stokes in his book, \"Pasteur's Quadrant\".\n\nAs shown in the following table, scientific research can be classified by whether it advances human knowledge by seeking a fundamental understanding of nature, or whether it is primarily motivated by the need to solve immediate problems.\n\nThe result is three distinct classes of research:\n\nPasteur's quadrant is useful in distinguishing various perspectives within science, engineering and technology. For example, Daniel A. Vallero and Trevor M. Letcher in their book \"Unraveling Environmental Disasters\". applied the device to disaster preparedness and response. University science programs are concerned with knowledge-building, whereas engineering programs at the same university will apply existing and emerging knowledge to address specific technical problems. Governmental agencies employ the knowledge from both to solve societal problems. Thus, the U.S. Army Corps of Engineers expects its engineers to apply general scientific principles to design and upgrade flood control systems. This entails selecting the best levee designs for the hydrologic conditions. However, the engineer would also be interested in more basic science to enhance designs in terms of water retention and soil strength. The university scientist is much like Bohr, with the major motivation being new knowledge. The governmental engineer is behaving like Edison, with the greatest interest in utility, and considerably less interest in knowledge for knowledge's sake.\n\nThe university engineering researcher's interests on the other hand, may fall between Bohr and Edison, looking to enhance both knowledge and utility. It is not likely that many single individuals fall within the Pasteur cell, since both basic and applied science are highly specialized. Thus, modern science and technology employ what might be considered a systems engineering approach, where the Pasteur cell consists of numerous researchers, professionals and practitioners to optimize solutions. Note that modifications to the quadrant model to more precisely reflect how research and development interact continue to be suggested.\n"}
{"id": "8494507", "url": "https://en.wikipedia.org/wiki?curid=8494507", "title": "Priming (agriculture)", "text": "Priming (agriculture)\n\nNano Seed Priming in botany and agriculture is a form of seed planting preparation in which the seeds are pre-soaked in nanoparticle solution.Seeds are considered to be an important part of crop life cycle as it influences the propagation of critical phases like germination and dormancy. Seed priming before sowing is considered to be one of the promising ways to provide value-added solutions to maximize the natural potential of seed to set the plant for maximum yield potential with respect to both quality and quantity. Positive effect on the shoot and root growth of seedlings of wheat (Triticum aestivum L.) when treated with iron-oxide nanoparticles. This innovative cost-effective and user-friendly method of biofortification has proven to increase grain iron deposition upon harvesting. Hence, the intervention of nanotechnology in terms of seed priming could be an economical and user-friendly smart farming approach to increase the nutritive value of the grains in an eco-friendly manner.\n\nPriming is not an extremely widely used method. In general, most kinds of seeds experimented with so far have shown an overall advantage over seeds that are not primed. Many have shown a faster emergence time (the time it takes for seeds to rise above the surface of the soil), a higher emergence rate (the number of seeds that make it to the surface), and better growth, suggesting that the head-start helps them get a good root system down early and grow faster. This method can be useful to farmers because it saves them the money and time spent for fertilizers, re-seeding, and weak plants.\n\n"}
{"id": "23497542", "url": "https://en.wikipedia.org/wiki?curid=23497542", "title": "Process tracing", "text": "Process tracing\n\nProcess tracing is a method used to evaluate and develop theories in psychology, political science, or usability studies.\n\nIn process tracing studies, multiple data points are collected in comparison to simple input-output methods, where only one measurement per task is available.\n\nThinking aloud protocols are a type of verbal protocols, used for eliciting and analyzing verbal data. Once transcribed, the verbalizations can be categorized into a defined scheme. This makes the data manageable.\n\nA thinking aloud protocol is one way to assess cognitive processes by letting people verbalize aloud what they currently think (concurrent verbal protocol) or what they were thinking (retrospective verbal protocol) while performing a task. Furthermore, depending on the type of the question asked, it can be distinguished structured verbal protocols and unstructured verbal protocols.\n\nIt is assumed that the report of this verbal stream can be used as a proxy for the content of working memory and hence reflects the cognitive processes while performing a task. Verbal protocols can be used in any research that focusses on understanding cognitive processes. Specifically, thinking aloud protocols can be helpful to study human decision making.\n\nThe use of thinking aloud protocols was introduced into decision process analysis in the 1970s by Montgomery and Svenson. Since then, the method has continually developed and has made valuable contributions to decision research.\n\n1) development of the task, based upon a hypothesis\n2) instruction and training of the participant\n3) recording audio of the performance\n\n4) transcription of the verbalizations\n5) definition of units of analysis (components, sequences, or complete models)\n6) definition of exclusive and exhaustive encoding categories\n7) encoding of the units of analysis into this scheme\n\n8) quantification of the prevalence of components\n9) interpretation, in terms of the theory\n\nThe quality of thinking aloud protocols depends on characteristics of the decision problem, as complexity, familiarity, importance, or whether singular or repeated. It is recommended to construct a decision problem with a medium level of complexity and importance and a low level of familiarity and little repetition.\n\nOne general problem with thinking aloud protocols is their validity (Russo, Johnson & Stephens, 1989). We cannot prove whether the participant really says what he is thinking. Particularly the validity of retrospective verbal protocols is debatable.\nVerbal protocols are vulnerable to non-veridicality. A verbal protocol lacks of veridicality if it fails to adequately represent the process that should be described. Errors of omission and errors of commission are typical reasons for a non-veridical measurement. Errors of omission happen if thoughts that are part of the process are not mentioned. Errors of commission happen if the verbal protocol contains statements about thoughts that were not actually part of the process.\n\nAnother important problem is the potential reactivity of concurrent verbal protocols . A verbal protocol is reactive if the verbalization process changes the cognitive process (Russo, Johnson & Stephens, 1989). This is particularly problematical when asking the participant why he’s doing something.\n\nFinally thinking aloud protocols have the problem of reliability because of possibly biased interpretation of the researcher. Statements about the participants „thinking steps” have to be transcribed and coded into a categorization scheme. This requires training in order to overcome the possibly biased interpretation of these verbalizations (Trickett & Trafton, 2009).\n\nHow to deal with these problems\nIt is important to obtain a cross-validation of findings from verbal data, e.g. with physiological or neural correlates. Furthermore it is recommended to add a silent control condition. Only then can we assess the influence that verbalization has on the primary process. Finally, to establish the interrater reliability, more than one researcher should code and interpret the data.\n\nAccording to (Walker, 2004):\n\n\nAs eye fixations are “the” way that humans gather information and every decision requires the acquisition of information it becomes obvious that eye tracking is a way of investigating decision processes with a lot of potential. Yet the method is still underutilized.\n\nAdvantages of eye tracking are that it is possible to trace a lot of information used during a decision task, it can sometimes be applied as a substitute for working memory and because eye movement are not so well controllable and difficult to censor, they can be recorded nonreactively.\n\nHowever, there are also problems one has to deal with when working with the eye tracking method. Most of times an eye tracker is not part of a standard lab and it is still expensive. Another, more functional problem is the interpretation of eye fixations: The data reveals where participants are looking, but not what they actually are thinking. Yet, this is exactly what we want to find out more about.\n\nIn cognitive psychology studies, the mostly concerned movements are saccades. During these movements no information can be acquired, because the vision is suppressed. This makes it very interesting especially for judgment and decision making tasks.\n\nEye fixation data could be used to test predictions: For example that magic tricks work because of misdirecting the eye gaze and failing to actually see what happens even if it happens right in front of ones eyes. One can also compare eye fixations between two groups: between experts and novices or a healthy and clinical population.\nBy looking at the distribution of fixations the investigation of display properties becomes possible. For example varying the size of products’ displays drives fixations’ amount, which is predictive for sales. This design does not enable causal hypothesis testing, however with the suitable design, the distribution of fixations could answer causal questions as well.\nStudies show that fixation frequencies are an indicator of relative importance: the more fixations on an alternative the greater the importance of this alternative. It has been shown that the frequency of fixations on an attribute can also be transformed into a rank order of the attributes\nThe fixation order tells us something about the evaluation of objects. Particularly the first and last fixations can point out important features of the decision making process.\nRusso (1994) defined sequences of fixations as different stages. The fixations before any refixations are defined as an initial phase of orientation or screening. The last fixations should reflect the elimination of alternatives as a last checking phase prior to the announcement of ones choice.\n\nThe total time fixation as well as the total time of a sequence of fixations can reveal important aspects of the decision process.\nThe mean fixation duration is an indicator of the processing depth or effort, and thus, reveal an important aspect of the decision process too. There is evidence ( Pieters and Warlop, 1999) that fixation duration on a chosen brand is longer than on a nonchosen alternative. There are mostly pair comparison and fixation duration is indicating that there are different checking processes (Russo & Leclerec, 1994).\nOpen question: Duration of saccade may improve the accuracy of the total duration.\nSequences of fixation\nThere are within-alternative and between attributes transitions. The definition of by-alternative processing is a fixation transition within an attribute between alternatives. A single transition within an attribute but between alternatives is defined as by-attribute processing. There is evidence that an increase in involvement (heightened by giving participants whichever brand they chose) not only led to longer fixation durations but also to more by-brand and fewer by-attribute transitions. ( Van Raaij, 1977)\nThere are research that focus on how to distinguish between these two processes, such as Russo & Dosher (1983). They defined by-alternative processing as sequence that all three attributes had to be fixated without interruption. A by-attribute comparisons required contiguous by-attribute evaluations of all three attribute. There is evidence that by-attribute processing is the preferred process for decision making (Arieli et al., 2009), even if it is not possible to decide accurately e.g. between two gambles without by-alternative processing.\nEye fixations as complementary data\nDifferent kinds of verbal protocol and eye fixation as a complementary data were compared (Gog et al., 2005). Such analysis reveals more information than only a single method.\nFixations as a monitor of attention\nSome experiments investigate the peripheral process. In such studies, it is essential to ensure that an individual gaze at a fixation point and to identify if the eye has moved. With the gaze-contingent stimulus alteration (McConkie & Rayner, 1975) it can be imposed whererver an individual is currently looking. With this method the impact of emotional words, when presented parafoveally (Calvo & Castillo, 2009) and the left hemisphere advantage for peripherally presented words (Jordan et al., 2009).\nCritics:\n- Eye fixation show where people are looking, but not what they are thinking. A clear interpretation is hardly doable as a fixation e.g. can occur to an elimination or to another consideration of other alternatives, or it can be learning or eliminating.\n- There are only a few JDM theories that specify a decision process at the level of fixations on individual alternatives or even smaller units such as individual attributes. (e.g., a theory that propose a more or less continuously developing decision process or another theory that addresses a model of the differentiation of value over time).\n\nHistory: Eye tracking for information of cognitive activity of decisions. [History of Eye tracking is already described in Wikipedia]\nRusso investigated 1978 in different methods of process tracing, such as eye tracking, verbal protocol or information display boards. They assume that different methods have different advantages and disadvantages. Further, they propose that a combined usage of different methods may reveal more information of process tracing background than a single method. They define eye fixation as a method with high quality but mention the difficulty of a valid interpretation and the difficulty to isolate relevant cognitive parts of behaviour.\nA big step forward in process tracing with the method eye tracking is provided from Russo & Leclerc (1994).\n\nWhy use eye tracking to measure decision processes?\nOne potential of the eye tracking method is that new theories, predicting behavior at the level of eye fixation might elicit, thus entail the possibility to find out more about decision processes. At the same time it brings up the question how process based theories should look like in the future\n\nIn the eye of eye tracking data, how should process based theories look like?\nWhy move from simple theories to the complex detail of process-based theories? – depends on paradigm\nWhat kind of jdm processes might eye fixation data illuminate?\nExample: Identifying heuristics (1. Approximate one attribute to zero, either because its difference was small or its importance was negligible. 2. Count majority of confirming attributes)\nData might prompt the devising of experimental designs and visual displays that could provide a new class of tests\nInvestigation of already existing heuristics. Preference drives perception: People look more at what they prefer. – Will inducing people to look at a stimulus longer or more often increases their preference for it? Will attention drive preference?\n\nEye fixations are the primary way that humans acquire information. To be able to use this immense amount of data, JDM theories need to be developed that require such detailed information acquisition data.\n\nMouselab is a computer program that records information acquisition. The basic idea behind process techniques is the record of eye movements. Mouselab, as a computer-based technique, models eye movements by mouse movements.\nThe information is presented in information cells which are ranged in a matrix. They are covered by an overlay, but can be opened by moving the mouse pointer across the information cell. During the time the cursor is in the cell, information can be acquired.\nMouselab is harken back to the information board idea (e.g. Payne 1976). Here, the information is hidden in envelopes attached to a cardboard. To obtain information, a card has to be pulled out.\n\nFor setting up a Mouselab study, there are two programs which can be used online and offline (e.g. MouselabWeb; Willemsen & Johnson, 2008). The way information is displayed or labeled can have a huge influence on the decision.\nPresenting the information in a certain order can lead to effects which might be a result of display layout (e.g. people tend todward following the natural reading order, which influences the starting box). Counterbalancing the position of different types of information can help to antagonize ordering effects. Exact labeling of the cells can prevent exploratory search - interpretation of information acquisition is easier when the participants focused on the text in the cells. Information in the cells should not be too simple, because participants might memorize the content and therefore stop acquisition.\n\nThe output of a MouselabStudy consists of hundreds of information pieces. Variables that are measured are: Time per trial, average looking at each box, the number of acquisitions, time between acquisitions.\nUnmotivated participants can be identified by unrealistically short trial times and unrealistically long ones or long times between acquisitions. About 5-10% of the observations have to be removed from analysis. Very short acquisitions, due to mouse movements, are filtered out (less than 200ms).\n\nThe process data can be presented by icon graphs (Johnson et al. 2002). The height of each icon is proportional to the number of acquisitions for the information cell. Icon size represents a measure of attention. With the help of arrows, transitions between boxes can be displayed.\n\nAdditional predictions can be made for reading and choice phases: The size of the boxes represents the amount of attention, participants give to the information cell. Arrows indicate direction and comparisons which are possible during information acquisition.\n\nThe method works by extracting all of the observable implications of a theory, rather than merely the observable implications regarding the dependent variable. Once these observable implications are extracted (particularly with reference to the microfoundations of how a theory's independent variable causes the predicated change in the dependent variable) they are then tested empirically, often through the method of elite interviews but also often through other rigorous forms of data analysis.\n\nIt is often used to complement comparative case study methods. By tracing the causal process from the independent variable of interest to the dependent variable, it may be possible to rule out potentially intervening variables in imperfectly matched cases. This can create a stronger basis for attributing causal significance to the remaining independent variables.\n\n"}
{"id": "31546299", "url": "https://en.wikipedia.org/wiki?curid=31546299", "title": "Quantum finance", "text": "Quantum finance\n\nQuantum finance is an interdisciplinary research field, applying theories and methods developed by quantum physicists and economists in order to solve problems in finance. It is a branch of econophysics.\n\nFinance theory is heavily based on financial instrument pricing such as stock option pricing. Many of the problems facing the finance community have no known analytical solution. As a result, numerical methods and computer simulations for solving these problems have proliferated. This research area is known as computational finance. Many computational finance problems have a high degree of computational complexity and are slow to converge to a solution on classical computers. In particular, when it comes to option pricing, there is additional complexity resulting from the need to respond to quickly changing markets. For example, in order to take advantage of inaccurately priced stock options, the computation must complete before the next change in the almost continuously changing stock market. As a result, the finance community is always looking for ways to overcome the resulting performance issues that arise when pricing options. This has led to research that applies alternative computing techniques to finance.\n\nOne of these alternatives is quantum computing. Just as physics models have evolved from classical to quantum, so has computing. Quantum computers have been shown to outperform classical computers when it comes to simulating\nquantum mechanics as well as for\nseveral other algorithms such as Shor's algorithm for factorization and Grover's algorithm for quantum search, making them an attractive area to research for solving computational finance problems.\n\nMost quantum option pricing research typically focuses on the quantization of the classical Black–Scholes–Merton equation from the perspective of continuous equations like the Schrödinger equation. Haven builds on the work of Chen and others, but considers the market from the perspective of the Schrödinger equation. The key message in Haven's work is that the Black–Scholes–Merton equation is really a special case of the Schrödinger equation where markets are assumed to be efficient. The Schrödinger-based equation that Haven derives has a parameter ħ (not to be confused with the complex conjugate of h) that represents the amount of arbitrage that is present in the market resulting from a variety of sources including non-infinitely fast price changes, non-infinitely fast information dissemination and unequal wealth among traders. Haven argues that by setting this value appropriately, a more accurate option price can be derived, because in reality, markets are not truly efficient.\n\nThis is one of the reasons why it is possible that a quantum option pricing model could be more accurate than a classical one. Baaquie has published many papers on quantum finance and even written a book that brings many of them together. Core to Baaquie's research and others like Matacz are Feynman's path integrals.\n\nBaaquie applies path integrals to several exotic options and presents analytical results comparing his results to the results of Black–Scholes–Merton equation showing that they are very similar. Piotrowski et al. take a different approach by changing the Black–Scholes–Merton assumption regarding the behavior of the stock underlying the option. Instead of assuming it follows a Wiener-Bachelier process, they assume that it follows an Ornstein-Uhlenbeck process. With this new assumption in place, they derive a quantum finance model as well as a European call option formula.\n\nOther models such as Hull-White\nand Cox-Ingersoll-Ross have successfully used the same approach in the classical setting with interest rate\nderivatives. Khrennikov builds on the work of Haven and others and further bolsters the idea that the market efficiency assumption made by the Black–Scholes–Merton equation may not be appropriate. To support this idea, Khrennikov builds on a framework of contextual probabilities using agents as a way of overcoming criticism of applying quantum theory to finance. Accardi and Boukas again quantize the Black–Scholes–Merton equation, but in this case, they also consider the underlying stock to have both Brownian and Poisson processes.\n\nChen published a paper in 2001, where he presents a quantum binomial options pricing model or simply abbreviated as the quantum binomial model. Metaphorically speaking, Chen's quantum binomial options pricing model (referred to\nhereafter as the quantum binomial model) is to existing quantum finance models what the Cox-Ross-Rubinstein classical binomial options pricing model was to the Black–Scholes–Merton model: a discretized and simpler version of the same result. These simplifications make the respective theories not only easier to analyze but also easier to implement on a computer.\n\nIn the multi-step model the quantum pricing formula is:\nwhich is the equivalent of the Cox-Ross-Rubinstein binomial options pricing model formula as follows:\n\nThis shows that assuming stocks behave according to Maxwell-Boltzmann classical statistics, the quantum binomial model does indeed collapse to the classical binomial model.\n\nQuantum volatility is as follows as per Meyer:\n\nMaxwell–Boltzmann statistics can be replaced by the quantum Bose–Einstein statistics resulting in the following option price formula:\n\nThe Bose-Einstein equation will produce option prices that will differ from those produced by the Cox-Ross-Rubinstein option\npricing formula in certain circumstances. This is because the stock is being treated like a quantum boson particle instead of a\nclassical particle.\n\nResearch by Rebentrost in 2018 shows that an algorithm exists for quantum computers capable of pricing financial derivatives with a square root advantage over classical methods. This development marks a shift from using quantum mechanics to gain insight into computational finance, to using quantum systems- quantum computers, to perform those calculations.\n\n"}
{"id": "12555158", "url": "https://en.wikipedia.org/wiki?curid=12555158", "title": "Raster Navigational Charts (NOAA)", "text": "Raster Navigational Charts (NOAA)\n\nRaster Navigational Charts (RNC's) are created by the National Oceanic and Atmospheric Administration (NOAA) of the United States Government. Each original chart is scanned at high resolution with color separate overlays. The raster file also contains data that is Geo-referencing; enabling computer based navigation attached to a GPS to locate and display the chart.\n\nThe charts are stored in BSB format. Image manipulation tools such as GDAL can read the image information, but there also is georeferenced data in the navigational charts.\n\n"}
{"id": "24586279", "url": "https://en.wikipedia.org/wiki?curid=24586279", "title": "Revolution in Time", "text": "Revolution in Time\n\nRevolution in Time: Clocks and the Making of the Modern World, is an influential history book by David S. Landes. Its focus is on the history of the measure of time and its interdependence with the evolution of the various civilisations over the centuries.\n\nThe book was first published in 1983 by Belknap Press of the Harvard University Press (hardback , paperback ) and has been expanded in its second edition, published in 2000: , copyright 1983, 2000 by the President and Fellows of Harvard College. The book deals with innovations and inventions that brought about modernization and technological developments in timekeeping in the whole world. It is considered to be one of the preeminent works on the history of horology.\n"}
{"id": "34517181", "url": "https://en.wikipedia.org/wiki?curid=34517181", "title": "SCALD", "text": "SCALD\n\nThe structured computer-aided logic design (SCALD) software, was a computer aided design system developed for building the S-1 computer. It used the Stanford University Drawing System (SUDS), and it was developed by Thomas M. McWilliams and Lawrence Curtis Widdoes, Jr. The work led to the start of the Valid Logic Systems company.\n\n\n"}
{"id": "1327869", "url": "https://en.wikipedia.org/wiki?curid=1327869", "title": "Segond fracture", "text": "Segond fracture\n\nThe Segond fracture is a type of avulsion fracture (soft tissue structures tearing off bits of their bony attachment) of the lateral tibial condyle of the knee, immediately beyond the surface which articulates with the femur.\n\nBecause of the high rate of associated ligamentous and meniscal injury, the presence of a Segond or reverse Segond fracture requires that these other pathologies must be specifically ruled out. Increasingly, reconstruction of the ACL is combined with reconstruction of the ALL when this associated pathology is present. It is often associated with an increased 'pivot shift' on physical exam.\n\nSegond and reverse Segond fractures are characterized by a small avulsion, or \"chip\", fragment of characteristic size that is best seen on plain radiography in the anterior-posterior plane. The chip of bone may be very difficult to see on the plain x-ray exam, and may be better seen on computed tomography. MRI may be useful for visualization of the associated bone marrow edema of the underlying tibial plateau on fat- saturated T2W and STIR images, as well as the associated findings of ligamentous and/or meniscal injury.\n\nOriginally described by Dr. Paul Segond in 1879 after a series of cadaveric experiments, the Segond fracture occurs in association with tears of the anterior cruciate ligament (ACL) (75–100%) and injury to the medial meniscus (66–75%), lateral capsular ligament (now known as the Anterolateral ligament, or ALL), as well as injury to the structures behind the knee.\n\nA rare, mirror image of the Segond fracture has also been described. The so-called \"reverse Segond fracture\" can occur after an avulsion fracture of the tibial component of the medial collateral ligament (MCL) in association with posterior cruciate ligament (PCL) and medial meniscal tears.\n\nSegond fracture is typically the result of abnormal varus, or \"bowing\", stress to the knee, combined with internal rotation of the tibia. Reverse Segond fracture, as its name suggests, is caused by abnormal valgus, or \"knock-knee\", stress and external rotation.\n\nOriginally thought to be a result of avulsion of the medial third of the lateral collateral ligament, the Segond fracture has been shown by more recent research to relate also to the insertion of the iliotibial tract (ITT) and the anterior oblique band (AOB), a ligamentous attachment of the fibular collateral ligament (FCL), to the midportion of the lateral tibia and to be associated with avulsion by the anterolateral ligament (ALL). (Roberts CC, Towers JD, Spangehl MJ et-al. Advanced MR imaging of the cruciate ligaments. Radiol. Clin. North Am. 2007;45 (6): 1003-16, vi-vii.)\"\n"}
{"id": "716742", "url": "https://en.wikipedia.org/wiki?curid=716742", "title": "Shmita", "text": "Shmita\n\nThe sabbath year (shmita , literally \"release\") also called the sabbatical year or \"shǝvi'it\" (, literally \"seventh\") is the seventh year of the seven-year agricultural cycle mandated by the Torah for the Land of Israel, and still observed in contemporary Judaism.\n\nDuring \"shmita\", the land is left to lie fallow and all agricultural activity, including plowing, planting, pruning and harvesting, is forbidden by \"halakha\" (Jewish law). Other cultivation techniques (such as watering, fertilizing, weeding, spraying, trimming and mowing) may be performed as a preventive measure only, not to improve the growth of trees or other plants. Additionally, any fruits or herbs which grow of their own accord and where no watch is kept over them are deemed \"hefker\" (ownerless) and may be picked by anyone. A variety of laws also apply to the sale, consumption and disposal of \"shmita\" produce. All debts, except those of foreigners, were to be remitted.\n\nChapter 25 of the Book of Leviticus promises bountiful harvests to those who observe the \"shmita\", and describes its observance as a test of religious faith. There is little notice of the observance of this year in Biblical history and it appears to have been much neglected.\n\nIt is still discussed among scholars of the Ancient Near East whether or not there is clear evidence for a seven-year cycle in Ugaritic texts. It is also debated how the biblical seventh fallow year would fit in with, for example Assyrian practice of a four-year cycle and crop rotation, and whether the one year in seven was an extra fallow year. Jehuda Felix suggests that the land may have been farmed only 3 years in seven. Borowski (1987) takes the fallow year as one year in seven.\n\nA sabbath (shmita) year is mentioned several times in the Hebrew Bible by name or by its pattern of six years of activity and one of rest: \n\nThe 2 Kings passage (and its parallel in Isaiah 37:30) refers to a sabbath (\"shmita\") year followed by a jubilee (\"yovel\") year. The text says that in the first year the people were to eat \"what grows of itself,\" which is expressed by one word in the Hebrew, \"saphiah\" (ספיח). In Leviticus 25:5, the reaping of the \"saphiah\" is forbidden for a Sabbath year, explained by rabbinic commentary to mean the prohibition of reaping in the ordinary way (with, for example, a sickle), but permitted to be plucked in a limited way by one's own hands for one's immediate needs during the Sabbath year. \n\nThere is an alternative explanation used to rectify what appears to be a discrepancy in the two biblical sources, taken from Adam Clarke's 1837 Bible commentary. The Assyrian siege had lasted until after planting time in the fall of 701 BCE, and although the Assyrians left immediately after the prophecy was given (2 Kings 19:35), they had consumed the harvest of that year before they left, leaving only the \"saphiah\" to be gleaned from the fields. In the next year, the people were to eat \"what springs from that\", Hebrew \"sahish\" (סחיש). Since this word occurs only here and in the parallel passage in Isaiah 37:30, where it is spelled שחיס, there is some uncertainty about its exact meaning. If it is the same as the \"shabbat ha-arets\" (שבת הארץ) that was permitted to be eaten in a Sabbath year in Leviticus 25:6, then there is a ready explanation why there was no harvest: the second year, i.e. the year starting in the fall of 700 BCE, was a Sabbath year, after which normal sowing and reaping resumed in the third year, as stated in the text.\n\nAnother interpretation obviates all of the speculation about the Sabbath year entirely, translating the verse as: \"And this shall be the sign for you, this year you shall eat what grows by itself, and the next year, what grows from the tree stumps, and in the third year, sow and reap, and plant vineyards and eat their fruit.\" According to the Judaica Press commentary, it was Sennacherib's invasion that prevented the people of Judah from sowing in the first year and Isaiah was promising that enough plants would grow to feed the population for the rest of the first year and the second year. Therefore, Isaiah was truly providing a sign to Hezekiah that God would save the city of Jerusalem, as explicitly stated, and not an injunction concerning the Sabbath (\"shmita\") or jubilee (\"yovel\") years, which are not mentioned at all in the passage.\n\nThe rabbis of the Talmud and later times interpreted the Shmita laws in various ways to ease the burden they created for farmers and the agricultural industry. The \"heter mechira\" (leniency of sale), developed for the Shmita year of 1888–1889, permitted Jewish farmers to sell their land to non-Jews so that they could continue to work the land as usual during Shmita. This temporary solution to the impoverishment of the Jewish settlement in those days was later adopted by the Chief Rabbinate of Israel as a permanent edict, generating ongoing controversy between Zionist and Haredi leaders to this day. There is a major debate among halakhic authorities as to what is the nature of the obligation of the Sabbatical year nowadays. Some say it is still biblically binding, as it has always been. Others hold that it is rabbinically binding, since the Shmita only biblically applies when the Jubilee year is in effect, but the Sages of the Talmud legislated the observance of the Shmita anyway as a reminder of the biblical statute. And yet others hold that the Shmita has become purely voluntary. An analysis by respected posek and former Sephardic Chief Rabbi Ovadiah Yosef in his responsa \"Yabi'a Omer\" (Vol. 10), accorded with the middle option, that the Biblical obligation holds only when a majority of the Jewish people is living in the Biblical Land of Israel and hence the Shmita nowadays is a rabbinic obligation in nature. This approach potentially admits for some leniencies which would not be possible if the Shemitah were biblical in origin, including the aforementioned sale of the land of Israel. Haredi authorities, on the other hand, generally follow the view of the Chazon Ish, that the Shmita continues to be a Biblical obligation.\n\nThe Sma, who holds that Shmita nowadays is only a Rabbinic obligation, holds that the Biblical promise of bounty for those who observe the Shmita (Leviticus 25:20–22) only applies when the Biblical obligation is in effect, and hence that the Biblical promise of bounty is not in effect today. However, the Chazon Ish, who holds that the Biblical obligation of Shmita observance remains in effect today, holds that the Biblical promise of bounty follows it and Divine bounty is promised to Jews living in the Land of Israel today, just as it was promised in ancient times. However, he holds that Jews should generally not demand miracles from Heaven and hence that one should not rely on this promise for one's sustenance, but should instead make appropriate arrangements and rely on permissible leniencies.\n\nHaredi Jews tell stories of groups of Israeli Jews who kept the Shmita and experienced remarkable agricultural events which they describe as representative of miracles in fulfillment of the Biblical promise of bounty. One famous story is told about the then-two-year-old village of Komemiyut during the 1952 Shmita. The village was one of the few who refrained from working the land that year. At the end of the Shmita, farmers searching for seed to plant found only wormy, inferior seed that had been rotting for years in an abandoned shed. Rabbi Binyamin Mendelson advised them to sow this seed anyway, saying \"The Almighty who causes wheat to sprout from good seed will bless your inferior seed as well,\" even though it was three months after neighboring villages had planted their fields. They did. That year the fall rains came late, the day after the Komemiyut seed was sown. As a result, the neighboring villages had a meager harvest, while the village of Komemiyut, who sowed from the old store, had a bumper crop.\n\nAccording to the laws of shmita, land owned by Jews in the Land of Israel is left unfarmed. The law does not apply to land in the Diaspora. (Under the Torah of Moses, Leviticus [which may or may not have actually been much followed in ancient Israel], any naturally growing produce was not to be formally harvested, but could have been eaten by its owners, as well as left to be taken by poor people, passing strangers, and beasts of the field.) While naturally growing produce such as grapes growing on existing vines can be harvested, it cannot be sold or used for commercial purposes; it must be given away or consumed. Personal debts are considered forgiven at sunset on 29 Elul. Since this aspect of shmita is not dependent on the land, it applies to Jews both in Israel and elsewhere.\n\nAs produce grown on land in Israel owned by Jewish farmers cannot be sold or consumed, fruits and vegetables sold in a shmita year may be derived from five sources:\n\n\nHalakhic authorities prohibit removing produce with Sabbatical sanctity (\"shevi'it\" produce) from the Land of Israel or purchasing such produce outside the land of Israel. Some authorities hold that tourists should be careful not to carry any such produce on an airplane leaving Israel even for consumption mid-air.\n\nThere is a requirement that \"shevi'it\" produce be consumed for personal use and cannot be sold or put in trash. For this reason, there are various special rules regarding the religious use of products that are normally made from agricultural produce. Some authorities hold that Hannukah candles cannot be made from \"shevi'it\" oils because the light of Channukah candles is not supposed to be used for personal use, while Shabbat candles can be because their light can be used for personal use. For similar reasons, some authorities hold that if the Havdalah ceremony is performed using wine made from \"shevi'it\" grapes, the cup should be drunk completely and the candle should not be dipped into the wine to extinguish the flame as is normally done.\n\nThe \"otzar beit din\" system is structured in such a way that \"biur\" remains the responsibility of members of individual households and hence warehoused produce does not have to be moved to a public place or reclaimed at the \"biur\" time. Households only have to perform \"biur\" on produce they receive before the \"biur\" time, not on produce they receive after it.\n\nBecause the Orthodox rules of Kashrut have strictures requiring certain products, such as wine, to be produced by Jews, the leniency of selling one's land to non-Jews is unavailable for these products, since these strictures would render the wine non-Kosher. Accordingly, wine made from grapes grown in the land of Israel during the Shmita year is subject to the full strictures of Shmita. New vines cannot be planted. Although grapes from existing vines can be harvested, they and their products cannot be sold.\n\nWhile obligatory to the Orthodox as a matter of religious observance, observance of the rules of Shmita is voluntary so far as the civil government is concerned in the contemporary State of Israel. Civil courts do not enforce the rules. A debt would be transferred to a religious court for a document of prosbul only if both parties voluntarily agreed to do so. Many non-religious Israeli Jews do not observe these rules, although some non-religious farmers participate in the symbolic sale of land to non-Jews to permit their produce to be considered kosher and sellable to Orthodox Jews who permit the leniency. Despite this, during Shmita, crop yields in Israel fall short of requirements so importation is employed from abroad.\n\nAccording to the Talmud, observance of the Sabbatical year is of high accord, and one who does not do so may not be allowed to be a witness in an Orthodox rabbinical court. Nonetheless, Rabbinic Judaism has developed Halakhic (religious-law) devices to be able to maintain a modern agricultural and commercial system while giving heed to the Biblical injunctions. Such devices represent examples of flexibility within the Halakhic system\n\nHillel the Elder, in the first century BCE, used the rule that remittance of debts applies only to debts between Jews, to develop a device known as Prosbul in which the debt is transferred to a Beit Din (religious court). When owed to the court rather than to an individual, the debt survives the Sabbatical year. This device, formulated early in the era of Rabbinic Judaism when the Temple in Jerusalem was still standing, became a prototype of how Judaism was later to adapt to the destruction of the Second Temple and maintain a system based on Biblical law under very different conditions.\n\nThe rabbis of the Jerusalem Talmud created rules to impose order on the harvesting process including a rule limiting harvesters working on others' land to taking only enough to feed themselves and their families. They also devised a system, called \"otzar beit din\", under which a rabbinical court supervised a communal harvesting process by hiring workers who harvested the fields, stored it in communal storage facilities, and distributed it to the community.\n\nThere exists a major difference of opinion between two Acharonim, the Beit Yosef and the Mabit, as to whether produce grown on land in Israel which is owned by non-Jews also has sanctity. According to the Beit Yosef, such produce has no sanctity and may be used and/or discarded in the same way as any produce grown outside of Israel. According to the Mabit, the fact that this produce was grown in Israel, even by non-Jews, gives it sanctity, and it must be treated in the special ways detailed above.\n\nThe Chazon Ish, a noted Haredi halakhic authority who issued key rulings on Jewish agricultural law (\"mitzvos tlu'os ba'aretz\")in the 1930s and 1940s, ruled like the Mabit, holding that produce grown on land in Israel owned by non-Jews has sanctity. The Chazon Ish's ruling was adopted first by the religious families of Bnei Brak and is popularly called \"Minhag Chazon Ish\" (the custom of the Chazon Ish).\n\nThe rabbis of Jerusalem, on the other hand, embraced the opinion of the Beit Yosef that produce farmed on land owned by non-Jews has no sanctity. This opinion is now called \"Minhag Yerushalayim\" (the custom of Jerusalem), and was adopted by many Haredi families, by British Mandate Palestine, and by the Chief Rabbinate of Israel.\n\nThese respective opinions are reflected in the way the various kashrut-certifying organizations publicize their Shmita and non-Shmita produce. The Edah HaChareidis, which follows \"Minhag Yerushalayim,\" buys produce from non-Jewish farms in Israel and sells it as \"non-Shmita produce.\" The Shearit Yisrael certifying organization, which subscribes to \"Minhag Chazon Ish,\" also buys from non-Jewish farmers in Israel, but labels the produce as such so that customers who keep \"Minhag Chazon Ish\" will treat these fruits and vegetables with appropriate sanctity.\n\nIn halakha (Jewish law), produce of the seventh year that is subject to the laws of Shmita is called \"sheviit,\" (\"sheviis\" in Ashkenazic Hebrew). \"Shevi'it\" produce has sanctity requiring special rules for its use:\n\n\nBy Biblical law, Jews who own land are required to make their land available during the Shmita to anyone who wishes to come in and harvest. If the land is fenced etc., gates must be left open to enable entrance. These rules apply to all outdoor agriculture, including private gardens and even outdoor potted plants. Plants inside a building are exempt. However, the Rabbis of the Mishna and Jerusalem Talmud imposed rabbinic ordinances on harvesters to ensure an orderly and equitable process and to prevent a few individuals from taking everything. Harvesters on others' land are permitted to take only enough to feed themselves and their families.\n\nIn the late 19th century, in the early days of Zionism, Rabbi Yitzchak Elchanan Spektor came up with a halakhic means of allowing agriculture to continue during the Shmita year. After ruling in favor of \"Minhag Yerushalayim\", that the biblical prohibition consists of not cultivating the land owned by Jews (\"\"your\" land\", Exodus 23:10), Rabbi Spektor devised a mechanism by which the land could be sold to a non-Jew for the duration of that year under a trust agreement. Under this plan, the land would belong to the non-Jew temporarily, and revert to Jewish ownership when the year was over. When the land was sold under such an arrangement, Jews could continue to farm it. Rabbi Abraham Isaac Kook, the first Chief Rabbi of British Mandate Palestine, allowed this principle, not as an ideal, but rather as a limited permit for individuals and times which are considered by Halacha of great need (\"b'shas hadchak\"), which became known as the \"heter mechira\" (lit. \"sale permit\"). Rabbi Kook explained in a lengthy responsum that the ideal is not to rely on the leniency of heter mechira, but rather to observe shmita according to all opinions. He noted that he himself did not rely on the leniency, it was intended only in a limited time of great need, for those unable to observe the shmita without the leniency.\n\nThe \"heter mechira\" was accepted by Modern Orthodox Judaism and is one of the classic examples of the Modern Orthodox approach toward adapting classical Jewish law to the modern world. However, this approach has not been universally accepted in the Orthodox community and has met with opposition, particularly from Haredi \"poskim\" (authorities of Jewish law).\n\nIn contemporary religious circles these rabbinic leniencies have received wide but not universal acceptance. In Israel, the Chief Rabbinate obtains permission from all farmers who wish to have their land sold. The land is then legally sold to a non-Jew for a large sum of money. The payment is made by a cheque post-dated to after the end of the Sabbatical year. When the cheque is returned or not honoured at the end of the year the land reverts to its original owners. Thus, the fields can be farmed with certain restrictions.\n\nAlthough the Orthodox Union's Kashrut Division accepts \"Minhag Yerushalayim\" and hence regards the produce of land owned by non-Jews as ordinary produce, it does not currently rely on the \"heter mechira\" because of doubts about whether the trust arrangement involved effects a valid transfer of ownership.\n\nSome Haredi farmers do not avail themselves of this leniency and seek other pursuits during the Shmita year.\n\nThe ancient idea of an \"otzar beit din\" (storehouse of the rabbinical court) is mentioned in the Tosefta (\"Sheviit 8, 1\"). Under an \"otzar beit din\", a community rabbinical court supervises harvesting by hiring workers to harvest, store, and distribute food to the community. Members of the community pay the \"beit din\", but this payment represents only a contribution for services, and not a purchase or sale of the food. This Talmudic device was revived in modern times as an alternative to the \"heter mechira\".\n\nBecause under this approach land cannot be sown but existing plants can be tended and harvested, the approach is applied to orchards, vineyards, and other perennial crops. A \"beit din,\" or rabbinical court supervising the process, hires farmers as its agents to tend and harvest the crops, and appoints the usual distributors and shopkeepers as its agents to distribute them. Individual consumers appoint the court and its designees as their agents and pay monies to court-appointed designees as agents of the court. Thus, under this approach, a legal arrangement is created whereby the crops themselves are never bought or sold, but rather people are merely paid for their labor and expenses in providing certain services. In modern Israel, the \"badatz\" is notable for adapting and supervising such arrangements.\n\nThe Orthodox Union notes that \"to some, the modern-day \"otzar\" might seem to be nothing more than a legal sleight of hand. All the regular players are still in place, and distribution rolls along as usual. However, in reality, it is identical only in appearance as prices are controlled, and may correspond only to expenses, with no profit allowed. In addition, the \"otzar beit din\" does not own the produce. Since it is simply a mechanism for open distribution, any individual is still entitled to collect produce from a field or orchard on his own. Furthermore, all agents of the \"beit din\" are appointed only if they commit to distributing the produce in accordance with the restrictions that result from its sanctity.\"\n\nUnder the rules of the Shmita, produce with Sabbatical sanctity (\"shevi'it\") can only be stored as long as plants of the same species (e.g. plants sprouting by themselves) are available to animals in the fields. Once a species is no longer available in the land, halakha requires that it be removed, made ownerless, and made available to anyone who wishes to take it through a procedure called \"biur\".\n\nThe Orthodox Union describes the contemporary application of the rules of \"biur\" as follows:\n\nThus, while the obligation of making one's produce available to the public and permitted to all takers can be performed in such a way as to minimize the risk that this availability will actually be utilized, this risk cannot be entirely eliminated. The community at large, including members of the poor, must be afforded some opportunity to take the produce.\n\n\"Biur\" only applies to produce that has \"shevi'it\" sanctity. For this reason, it does not apply to produce grown under the \"heter mechira\" for those who accept it. (Under the reasoning of the \"heter mechira\" the shmita does not apply to land owned by non-Jews, so its produce does not have \"shevi'it\" sanctity.)\n\nThe first Shmita year in the modern State of Israel was 1951-52 (5712 in the Hebrew calendar). Subsequent Shmita years have been 1958–59 (5719), 1965–66 (5726), 1972–73 (5733), 1979–80 (5740), 1986–87 (5747), 1993–94 (5754), 2000–01 (5761), 2007–08 (5768), and 2014-15 (5775). The last Shmita year began on Rosh Hashanah in September 2014, corresponding to the Hebrew calendar year 5775. The 50th year of the land, which is also a Shabbat of the land, is called \"Yovel\" in Hebrew, which is the origin of the Latin term \"Jubilee\", also meaning 50th. According to the Torah, observance of Jubilee only applies when the Jewish people live in the land of Israel according to their tribes. Thus, with the exile of the tribes of Reuben, Gad, and Menashe (about 600 BCE) Jubilee has not been applicable. In 2000, Sefardic Chief Rabbi Eliyahu Bakshi-Doron withdrew religious certification of the validity of permits for the sale of land to non-Jews during the Shmita year following protests against his endorsement of the leniency by members of the Haredi community.\n\nAuthorities who prohibit farming in Israel generally permit hydroponics farming in greenhouses structured so that the plants are not connected to the soil. As a result, hydroponics use has been increasing in Haredi farming communities.\n\nDuring the 2007–08 Shmita, the Chief Rabbinate of Israel attempted to avoid taking a potentially divisive position on the dispute between Haredi and Modern Orthodox views about the correctness of the \"heter mechira\" leniency by ruling that local rabbis could make their own decisions about whether or not to accept this device as valid. The Israel Supreme Court, however, ordered the Chief Rabbinate to rescind its ruling and to devise a single national ruling. The Israel Supreme Court opined that divergent local rulings would be harmful to farmers and trade and could implicate competition. The issue of secular courts ordering the rabbinate to rule in particular ways on religious matters aroused a debate within the Knesset. Israeli wineries often address this issue by making separate batches of Shmita wine, labeled as such, and giving away bottles of Shmita wine as a free bonus to purchasers of non-Shmita wine.\n\n\n \n"}
{"id": "42788108", "url": "https://en.wikipedia.org/wiki?curid=42788108", "title": "Simon Dinnerstein", "text": "Simon Dinnerstein\n\nSimon Dinnerstein (born February 16, 1943) is an American artist best known for \"The Fulbright Triptych\".\n\nDinnerstein was born in Brownsville, Brooklyn, New York in 1943 to pharmacist Louis and homemaker Sarah Dinnerstein. One of two children, his older brother Harvey Dinnerstein is also an artist.\n\nDinnerstein holds a Bachelor of Arts in History from the City College of New York. He studied painting and drawing at the Brooklyn Museum Art School with Louis Grebenak, David Levine, and Richard Mayhew. He was a member of the faculty at the New School for Social Research, Parsons School of Design, and New York City Technical College. He lectures widely and has lectured at Pennsylvania State University.\n\nDinnerstein's art is mostly in the figurative style, with folk, expressionistic, and surrealistic influences, possessing a \"narrative\" and \"psychological edge\". He uses a variety of media, pencils, charcoal, and oil paints. Dinnerstein renders still-life's, but most of his work involves portraiture or human figures. He often \"paints the figure in unexpected juxtaposition with landscape or interior elements,\" of which Dinnerstein says,\n\nWhat interests me is the ability of Degas, Balthus, Lucien Freud and Antonio Lopez Garcia…to deal with the figure…to create art…rich in scale, yet abstract adventurous, experimental…deeply human…a combination of modernism and tradition of skill medium and…a fresh, personal response to the human form in art…Hopefully my work speaks to these issues.\n\nOften the human figures are portrayed against a background of hyperreality, or in dreamy surreal landscapes. Light plays an important role in Dinnerstein's work achieving \"an inwardness…in the play of light that radiates from the object and renders it mysterious.\" or makes \"Brooklyn sunlight on an ordinary floor seem supernatural.\" The use of light contributes to Dinnerstein's paintings being described as \"magical realism\". In early Dinnerstein works, strong left-right symmetry prevails, although later works are noted for their asymmetry.\nDinnerstein draws on diverse sources for inspiration: Northern European art (Albrecht Dürer, Hieronymus Bosch), Mexican art (Frida Kahlo, Diego Rivera), as well as literature (D. H. Lawrence, August Strindberg).\n, and film (Ingmar Bergman, Alfred Hitchcock).\n\nDinnerstein's most notable painting, \"The Fulbright Triptych\", was started in Germany in 1971 while he served as a Fulbright Scholar in Graphics. It was completed in 1974. A largely autobiographical work, it combines stark realism with American figurative tradition to produce a secular rendering of the usually religious form, the triptych.\n\nWriter Jonathan Lethem commented: \"Simon Dinnerstein's 'The Fulbright Triptych' is one of those singular and astonishing works of art which seem to imply a description of the whole world merely by insisting on a scrupulous gaze at one perfect instant.\" The oil-on-wood painting consists of three panels approximately 14 feet wide, depicting a graphic artist's studio. Three figures, representing the Dinnerstein family, occupy the outer panels. The central panel consists of the artist's desk, engraving tools, a copper disk of the commissioned Fulbright engraving project, and an outward view in perspective of Hessisch Lichtenau (near to Kassel). Plants, photographs, old master's paintings, children's grade school writing, and an exit visa from Russia, appear tacked to the wall of the studio. The \"Triptych\" is noted for its symmetry, meticulous detail, mixture of textures, and sense of space.\n\nWidely praised, with each viewer bringing a different sensibility and interpretation of the work, the painting is the subject of numerous essays, articles, and books, including \"The Suspension of Time: Reflections of Simon Dinnerstein's The Fulbright Triptych\" edited by Daniel Slager, published 2011. Among the many who have commented on the painting are art critic John Russell, Guggenheim Foundation director Thomas M. Messer, art historian Albert Boime, artist George Tooker, writer Anthony Doerr, composer George Crumb, poet Dan Beachy-Quick, actor John Turturro, and Pulitzer Prize winner Jhumpa Lahiri.\n\nIn 1965, Dinnerstein married Renée Sudler, a noted educational consultant. Renée Dinnerstein is the author of the book \"Choice Time: How to Deepen Learning Through Inquiry and Play, PreK-2\" published in August 2016. She runs the popular blog, Investigating Choice Time: Inquiry, Exploration and Play. They have a daughter, Simone Dinnerstein, the virtuoso concert pianist. Both wife and daughter (as an infant) figure prominently in \"The Fulbright Triptych\" as well as other works.\n\nDinnerstein resides in Brooklyn, where, in addition to practicing his art, he teaches classes on art history and appreciation.\n\n\n\n\n\n\n\n"}
{"id": "18528108", "url": "https://en.wikipedia.org/wiki?curid=18528108", "title": "Social interface", "text": "Social interface\n\nSocial interface is a concept from social science (particularly, media ecology (Marshall McLuhan) and sociology of technology). \n\nIt can be approached from a theoretical or a practical perspective.\n\nAs a concept of social interface theory, social interface is defined by Norman Long (1989, 2001). In 2001 his revised definition was:\n\nIn other words, interfaces are the areas in which social friction can be experienced and where diffusion of new technology is leading to structural discontinuities (which can be both positive or negative), the interface is where they will occur. Long continues to say that:\n\nIdentifying these interfaces and analyzing their effects shows how they are changed by everyday life, and how in return everyday life is changed by the interfaces.\n\nAs practical concept of social interface design, social interface is seen in the studies of human-computer interaction (in particular, its computer interface aspect). The basic thesis is that where a computer interface is more akin to another human, it can facilitate correct responses from users during human-to-computer interaction. Software that can provide such humanizing cues often does it by creating interface with human-like quality (such as giving recognizable gender to a software agent). Studies are often concerned with how should such agents (like the Microsoft Agent) be designed to make them more appealing (is having facial expressions efficient, should the agent be anthropomorphic, and so on). \n\n\n 7. Turnbull, B. E. (1998). Street children and their helpers: a social interface analysis (Doctoral dissertation, University of Sussex).\n\n"}
{"id": "13995258", "url": "https://en.wikipedia.org/wiki?curid=13995258", "title": "Traveler's Rest (Lolo, Montana)", "text": "Traveler's Rest (Lolo, Montana)\n\nTraveler's Rest was a stopping point of the Lewis and Clark Expedition, located about one mile south of Lolo, Montana. The expedition stopped from September 9 to September 11, 1805, before crossing the Bitterroot Mountains, and again on the return trip from June 30 to July 3, 1806. Traveler's Rest is at the eastern end of the Lolo Trail. It was declared a National Historic Landmark in 1960 and added to the National Register of Historic Places in 1966. The boundaries were subsequently revised, and mostly lie within the Traveler's Rest State Park, which is operated by the Montana Fish, Wildlife & Parks. Significant archeological findings made in 2002, including latrine sites with traces of mercury and fire hearths, make this the only site on the Lewis and Clark National Historic Trail that has yielded physical proof of the explorers' presence. Records made by Lewis and Clark often spell \"Traveler's\" as \"Traveller's\". This spot is largely unchanged from the days of Lewis and Clark. From this location, Lewis and Clark split up to explore Montana during their return trip, not reuniting until they reached Sanish, North Dakota.\n\nAt the time of landmark designation in 1960, the exact location of the expedition's campsite was unknown. Boundaries were formalized on December 12, 1983. Subsequent investigations revealed that errors had been made in setting the boundaries of the landmark. Detailed historical and scientific investigations resulted in a 55-page request for boundary corrections, submitted on May 10, 2004, and approved on March 21, 2006. A new road and bridge were built in 2006.\n\nAfter departing here in 1806, Lewis' part of the expedition traveled to what is now the Alice Creek Historic District.\n\n\n"}
