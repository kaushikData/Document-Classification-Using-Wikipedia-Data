{"id": "5918011", "url": "https://en.wikipedia.org/wiki?curid=5918011", "title": "A363 road", "text": "A363 road\n\nThe A363 is a main road in the United Kingdom, which runs through Bath and North East Somerset and Wiltshire. It provides a link between the small market towns of West Wiltshire and Bath, the M4 westbound, and the M5. It runs from the A4 at Bathford to the A350 at Yarnbrook, running through Bradford-on-Avon and Trowbridge on the way. It also links Westbury by merging with the A350.\n\nThe road starts off bypassing Bathford to the west. It then climbs up the hill known locally as \"Sally in the Woods\", on the edge of the Cotswolds Area of Outstanding Natural Beauty, which has been the scene of a fatal accident.\n\nThe road summits at the turn-off to Monkton Farleigh, at an altitude of 144 metres. After this, the road quality improves in that it widens and is straighter. This continues up to the approach to Bradford-on-Avon. Through Bradford-on-Avon itself, the road descends a steep 1 in 7 (14%) hill, known as \"Masons Lane\". This is the road's main bottleneck since this leads to Bradford-on-Avon's one and only vehicle bridge over the River Avon (which splits the town in two), and at this stage travelers have merged with traffic from Melksham, Corsham and Chippenham (B3109) which is fed here.\n\nThe road passes on towards Trowbridge. 1/4 mile (400 m) after \"Town Bridge\", the traffic to Frome splits off upon the restart of the B3109 which had been concurrent with the A363 for a short distance, thus easing congestion. The road passes over a low hill, then descends into Trowbridge.\n\nIt is concurrent for a short distance with the A361, then emerges as \"Bradley Road\". There are three roundabouts as the road goes through North Bradley via the White Horse Business Park, to the south. Until the 1980s it passed through North Bradley's residential lane, but since the opening of the business park, the older road through North Bradley became declassified and no longer has A- or B-road status. The A363, then terminates at a roundabout with the A350, in Yarnbrook.\n\nWhen it was first designated in the 1920s, the road continued on from Yarnbrook as far as Chippenham, but by 1948 this stretch had been redesignated as a continuation of the A350.\n"}
{"id": "214096", "url": "https://en.wikipedia.org/wiki?curid=214096", "title": "Active optics", "text": "Active optics\n\nActive optics is a technology used with reflecting telescopes developed in the 1980s, which actively shapes a telescope's mirrors to prevent deformation due to external influences such as wind, temperature, mechanical stress. Without active optics, the construction of 8 metre class telescopes is not possible, nor would telescopes with segmented mirrors be feasible.\n\nThis method is used by, among others, the Nordic Optical Telescope, the New Technology Telescope, the Telescopio Nazionale Galileo and the Keck telescopes, as well as all of the largest telescopes built since the mid-1990s.\n\nActive optics is not to be confused with adaptive optics, which operates at a shorter timescale and corrects atmospheric distortions.\n\nMost modern telescopes are reflectors, with the primary element being a very large mirror. Historically, primary mirrors were quite thick in order to maintain the correct surface figure in spite of forces tending to deform it, like wind and the mirror's own weight. This limited their maximum diameter to 5 or 6 metres (200 or 230 inches), such as Palomar Observatory's Hale telescope.\n\nA new generation of telescopes built since the 1980s uses thin, lighter weight mirrors instead. They are too thin to maintain themselves rigidly in the correct shape, so an array of actuators is attached to the rear side of the mirror. The actuators apply variable forces to the mirror body to keep the reflecting surface in the correct shape over repositioning. The telescope may also be segmented into multiple smaller mirrors, which reduce the sagging due to weight that occurs for large, monolithic mirrors.\n\nThe combination of actuators, an image quality detector, and a computer to control the actuators to obtain the best possible image, is called \"active optics\".\n\nThe name \"active\" optics means that the system keeps a mirror (usually the primary) in its optimal shape against environmental forces such as wind, sag, thermal expansion, and telescope axis deformation. Active optics compensate for distorting forces that change relatively slowly, roughly on timescales of seconds. The telescope is therefore \"actively\" still, in its optimal shape.\n\nActive optics should not be confused with adaptive optics, which operates on a much shorter timescale to compensate for atmospheric effects, rather than for mirror deformation. The influences that active optics compensate (temperature, gravity) are intrinsically slower (1 Hz) and have a larger amplitude in aberration. Adaptive optics on the other hand corrects for atmospheric distortions that affect the image at 100–1000 Hz (the Greenwood frequency,\ndepending on wavelength and weather conditions). These corrections need to be much faster, but also have smaller amplitude. Because of this, adaptive optics uses smaller corrective mirrors. This used to be a separate mirror not integrated in the telescope's light path, but nowadays this can be the second, third or fourth mirror in a telescope.\n\nComplicated laser set-ups and interferometers can also be actively stabilized.\n\nA small part of the beam leaks through beam steering mirrors and a four-quadrant-diode is used to measure the position of a laser beam and another in the focal plane behind a lens is used to measure the direction. The system can be sped up or made more noise-immune by using a PID controller. For pulsed lasers the controller should be locked to the repetition rate. A continuous (non-pulsed) pilot beam can be used to allow for up to 10 kHz bandwidth of stabilization (against vibrations, air turbulence, and acoustic noise) for low repetition rate lasers.\n\nSometimes Fabry–Pérot interferometers have to be adjusted in length to pass a given wavelength. Therefore, the reflected light is extracted by means of a Faraday rotator and a polarizer. Small changes of the incident wavelength generated by an acousto-optic modulator or interference with a fraction of the incoming radiation delivers the information whether the Fabry Perot is too long or too short.\nLong optical cavities are very sensitive to the mirror alignment. A control circuit can be used to peak power. One possibility is to perform small rotations with one end mirror. If this rotation is about the optimum position, no power oscillation occurs. Any beam pointing oscillation can be removed using the beam steering mechanism mentioned above.\n\nX-ray active optics, using actively deformable grazing incidence mirrors, are also being investigated.\n\n\n"}
{"id": "142417", "url": "https://en.wikipedia.org/wiki?curid=142417", "title": "Apollo 13 (film)", "text": "Apollo 13 (film)\n\nApollo 13 is a 1995 American space docudrama film directed by Ron Howard and starring Tom Hanks, Kevin Bacon, Bill Paxton, Gary Sinise, and Ed Harris. The screenplay by William Broyles, Jr., and Al Reinert dramatizes the aborted 1970 Apollo 13 lunar mission and is an adaptation of the book \"Lost Moon: The Perilous Voyage of Apollo 13\" by astronaut Jim Lovell and Jeffrey Kluger. The film depicts astronauts Lovell, Jack Swigert, and Fred Haise aboard Apollo 13 for America's third Moon landing mission. En route, an on-board explosion deprives their spacecraft of most of its oxygen supply and electric power, forcing NASA's flight controllers to abort the Moon landing, and turning the mission into a struggle to get the three men home safely.\n\nHoward went to great lengths to create a technically accurate movie, employing NASA's technical assistance in astronaut and flight controller training for his cast, and obtaining permission to film scenes aboard a reduced gravity aircraft for realistic depiction of the \"weightlessness\" experienced by the astronauts in space.\n\nReleased to cinemas in the United States on June 30, 1995, \"Apollo 13\" was nominated for nine Academy Awards, including Best Picture (winning for Best Film Editing and Best Sound). In total, the film grossed over $355 million worldwide during its theatrical releases. The film was very positively received by critics.\n\nIn July 1969, astronaut Jim Lovell hosts a house party where guests watch Neil Armstrong's televised first human steps on the Moon. Afterwards Lovell, who had orbited the Moon on Apollo 8, tells his wife Marilyn that he intends to return to the Moon to walk on its surface.\n\nThree months later, as Lovell conducts a VIP tour of NASA's Vertical Assembly Building, his boss Deke Slayton informs him that because of problems with Alan Shepard's crew, his crew will fly Apollo 13 instead of 14. Lovell, Ken Mattingly, and Fred Haise train for their new mission. A few days before launch, Mattingly is exposed to the measles, and the flight surgeon demands his replacement with Mattingly's backup, Jack Swigert. Lovell resists breaking up his team, but relents when Slayton threatens to bump his crew to a later mission. As the launch date approaches, Marilyn has a nightmare about her husband getting killed in space, but goes to the Kennedy Space Center the night before launch to see him off.\n\nOn April 11, 1970, Flight Director Gene Kranz gives the go-ahead from Houston's Mission Control Center for the Apollo 13 launch. As the Saturn V rocket climbs through the atmosphere, a second stage engine cuts off prematurely, but the craft reaches its Earth parking orbit. After the third stage fires to send Apollo 13 to the Moon, Swigert performs the maneuver to connect the Command/Service Module \"Odyssey\" to the Lunar Module \"Aquarius\" and pull it away from the spent rocket.\n\nThree days into the mission, the crew makes a television transmission, which the networks decline to broadcast live. After Swigert turns on the liquid oxygen tank stirring fans as requested, one of the tanks explodes, emptying its contents into space and sending the craft tumbling. The other tank is soon found to be leaking. They attempt to stop the leak by shutting off fuel cells #1 and #3, but to no avail. With the fuel cells closed, the Moon landing must be aborted, and Lovell and Haise must hurriedly power up \"Aquarius\" to use as a \"lifeboat\" for the return home, as Swigert shuts down \"Odyssey\" before its battery power runs out. In Houston, Kranz rallies his team to come up with a plan to bring the astronauts home safely, declaring \"failure is not an option\". Controller John Aaron recruits Mattingly to help him invent a procedure to restart \"Odyssey\" for the landing on Earth.\n\nAs Swigert and Haise watch the Moon pass beneath them, Lovell laments his lost chance of walking on its surface, then turns their attention to the business of getting home. With \"Aquarius\" running on minimal electrical power, the crew suffers freezing conditions, and Haise contracts a urinary infection and a fever. Swigert suspects Mission Control is withholding their inability to get them home; Haise angrily blames Swigert's inexperience for the accident; and Lovell quickly squelches the argument. When carbon dioxide approaches dangerous levels, ground control must quickly invent a way to make the Command Module's square filters work in the Lunar Module's round receptacles. With the guidance systems on \"Aquarius\" shut down, the crew must make a difficult but vital course correction by manually igniting the Lunar Module's engine.\n\nMattingly and Aaron struggle to find a way to turn on the Command Module systems without drawing too much power, and finally transmit the procedure to Swigert, who restarts \"Odyssey\" by transferring extra power from \"Aquarius\". When the crew jettisons the Service Module, they are surprised to see the extent of the damage. As they release \"Aquarius\" and re-enter the Earth's atmosphere, no one is sure that \"Odyssey\"s heat shield is intact. The tense period of radio silence due to ionization blackout is longer than normal, but the astronauts report all is well and splash down in the Pacific Ocean.\n\nAs helicopters bring the three men aboard the recovery ship USS \"Iwo Jima\" for a hero's welcome, Lovell's voice-over describes the subsequent investigation into the explosion, and the careers of Haise, Swigert, Mattingly, and Kranz. He wonders if and when mankind will return to the Moon.\n\n\nThe real Jim Lovell appears as captain of the recovery ship USS \"Iwo Jima\"; Howard had intended to make him an admiral, but Lovell himself, having retired as a captain, chose to appear in his actual rank. Horror film director Roger Corman, a mentor of Howard, appears as a congressman being given a VIP tour by Lovell of the Vehicle Assembly Building, as it had become something of a tradition for Corman to make a cameo appearance in his protégés' films.<ref name=\"https://www.npr.org/templates/story/story.php?storyId=128873305\"></ref> The real Marilyn Lovell appeared among the spectators during the launch sequence. CBS News anchor Walter Cronkite appears in archive news footage and can be heard in newly recorded announcements, some of which he edited himself to sound more authentic.\n\nIn addition to his brother, Clint Howard, several other members of Ron Howard's family appear in the movie:\nBrad Pitt was offered a role in the film, but turned it down to star in \"Se7en\".<ref name=\"http://www.quickoverview.com/overviews/brad-pitt-overview.html\"></ref> Reportedly, the real Pete Conrad expressed interest in appearing in the film.\n\nJeffrey Kluger appears as a television reporter.\n\nThe screenplay by William Broyles, Jr. and Al Reinert was rewritten by John Sayles after Hanks had been cast and construction of the spacecraft sets had begun.\n\nWhile planning the film, director Ron Howard decided that every shot of the film would be original and that no mission footage would be used. The spacecraft interiors were constructed by the Kansas Cosmosphere and Space Center's Space Works, which also restored the Apollo 13 Command Module. Two individual Lunar Modules and two Command Modules were constructed for filming. While each was a replica, composed of some of the original Apollo materials, they were built so that different sections were removable, which enabled filming to take place inside the capsules. Space Works also built modified Command and Lunar Modules for filming inside a Boeing KC-135 reduced-gravity aircraft, and the pressure suits worn by the actors, which are exact reproductions of those worn by the Apollo astronauts, right down to the detail of being airtight. When the actors put the suits on with their helmets locked in place, air was pumped into the suits to cool them down and allow them to breathe, exactly as in launch preparations for the real Apollo missions.\n\nThe real Mission Control Center consisted of two control rooms located on the second and third floors of Building 30 at the Johnson Space Center in Houston, Texas. NASA offered the use of the control room for filming, but Howard declined, opting instead to make his own replica from scratch. Production designer Michael Corenblith and set decorator Merideth Boswell were in charge of the construction of the Mission Control set at Universal Studios. The set was equipped with giant rear-screen projection capabilities and a complex set of computers with individual video feeds to all the flight controller stations. The actors playing the flight controllers were able to communicate with each other on a private audio loop. The Mission Control room built for the film was on the ground floor. One NASA employee, who was a consultant for the film, said that the set was so realistic that he would leave at the end of the day and look for the elevator before remembering he was not in Mission Control. By the time the film was made, the USS \"Iwo Jima\" had been scrapped, so her sister ship, the USS \"New Orleans\", was used as the recovery ship instead.\nHoward anticipated difficulty in portraying weightlessness in a realistic manner. He discussed this with Steven Spielberg, who suggested using a KC-135 airplane, which can be flown in such a way as to create about 23 seconds of weightlessness, a method NASA has always used to train its astronauts for space flight. Howard obtained NASA's permission and assistance in filming in the realistic conditions aboard multiple KC-135 flights.<ref name=\"http://industrycentral.net/director_interviews/RH01.HTM\"></ref>\n\nIn Los Angeles, Ed Harris and all the actors portraying flight controllers enrolled in a Flight Controller School led by Gerry Griffin, an Apollo 13 flight director, and flight controller Jerry Bostick. The actors studied audiotapes from the mission, reviewed hundreds of pages of NASA transcripts, and attended a crash course in physics. Astronaut Dave Scott was impressed with their efforts, stating that each actor was determined to make every scene technically correct, word for word.\n\nThe score to \"Apollo 13\" was composed and conducted by James Horner. The soundtrack was released in 1995 by MCA Records and has seven tracks of score, eight period songs used in the film, and seven tracks of dialogue by the actors at a running time of nearly seventy-eight minutes. The music also features solos by vocalist Annie Lennox and Tim Morrison on the trumpet. The score was a critical success and garnered Horner an Academy Award nomination for Best Original Score.\n\nThe film was released on June 30, 1995 in North America and on September 22, 1995 in the UK.\n\nIn September 2002 the film was re-released in IMAX. It was the first film to be digitally remastered using IMAX DMR technology.\n\nThe film was a box-office success, bringing in $355,237,933 worldwide. The film's widest release was 2,347 theaters.\nThe film's opening weekend and the following two weeks placed it at #1 with a US gross of $25,353,380, which made up 14.7% of the total US gross.\n\nReview aggregator Rotten Tomatoes reports that the film has an overall approval rating of 95%, based on 85 reviews, with a weighted average rating of 8.2/10. The site's critical consensus reads, \"In recreating the troubled space mission, \"Apollo 13\" pulls no punches: it's a masterfully told drama from director Ron Howard, bolstered by an ensemble of solid performances.\" Metacritic, which assigns a normalized rating to reviews from mainstream critics, gave the film an average score of 77 out of 100, based on 22 critics, indicating \"generally favorable reviews\".\n\nRoger Ebert of the \"Chicago Sun-Times\" praised the film in his review saying: \"A powerful story, one of the year's best films, told with great clarity and remarkable technical detail, and acted without pumped-up histrionics.\" Richard Corliss from \"Time\" highly praised the film, saying: \"From lift-off to splashdown, Apollo 13 gives one hell of a ride.\" Edward Guthmann of \"San Francisco Chronicle\" gave a mixed review and wrote: \"I just wish that \"Apollo 13\" worked better as a movie, and that Howard's threshold for corn, mush and twinkly sentiment weren't so darn wide.\" Peter Travers from \"Rolling Stone\" praised the film and wrote: \"Howard lays off the manipulation to tell the true story of the near-fatal 1970 Apollo 13 mission in painstaking and lively detail. It's easily Howard's best film.\"\n\nJanet Maslin made the film an \"NYT\" Critics' Pick, calling it an \"absolutely thrilling\" film that \"unfolds with perfect immediacy, drawing viewers into the nail-biting suspense of a spellbinding true story.\" According to Maslin, \"like \"Quiz Show\", \"Apollo 13\" beautifully evokes recent history in ways that resonate strongly today. Cleverly nostalgic in its visual style (Rita Ryack's costumes are especially right), it harks back to movie making without phony heroics and to the strong spirit of community that enveloped the astronauts and their families. Amazingly, this film manages to seem refreshingly honest while still conforming to the three-act dramatic format of a standard Hollywood hit. It is far and away the best thing Mr. Howard has done (and \"Far and Away\" was one of the other kind).\" The academic critic Raymond Malewitz focuses on the DIY aspects of the \"mailbox\" filtration system to illustrate the emergence of an unlikely hero in late 20th-century American culture—\"the creative, improvisational, but restrained thinker—who replaces the older prodigal cowboy heroes of American mythology and provides the country a better, more frugal example of an appropriate 'husband'.\"\n\nRon Howard stated that, after the first test preview of the film, one of the comment cards indicated \"total disdain\"; the audience member had written that it was a \"typical Hollywood\" ending and that the crew would never have survived. Marilyn Lovell praised Quinlan's portrayal of her, stating she felt she could feel what Quinlan's character was going through, and remembered how she felt in her mind.\n\nA 10th-anniversary DVD of the film was released in 2005; it included both the theatrical version and the IMAX version, along with several extras. The IMAX version has a 1.66:1 aspect ratio.\n\nIn 2006, \"Apollo 13\" was released on HD DVD and on April 13, 2010 it was released on Blu-ray disc as the 15th-anniversary edition on the 40th anniversary of the Apollo 13 accident. The Film was released on 4K UHD Blu-Ray on October 17, 2017.\n\nThe film depicts the crew hearing a bang quickly after Swigert followed directions from mission control to stir the oxygen and hydrogen tanks. In reality, the crew heard the bang 93 seconds later.\n\nThe film portrays the Saturn V launch vehicle being rolled out to the launch pad two days before launch. In reality, the launch vehicle was rolled out on the mobile launch platform using the crawler-transporter weeks before the launch date.\n\nThe movie depicts Swigert and Haise arguing about who was at fault. The show \"The Real Story: Apollo 13\" broadcast on the Smithsonian Channel includes Haise stating that no such argument took place and that there was no way anyone could have foreseen that stirring the tank would cause problems.\n\nThe dialogue between ground control and the astronauts was taken nearly verbatim from transcripts and recordings, with the exception of one of the taglines of the film, \"Houston, we have a problem.\" (This quote was voted #50 on the list \"AFI's 100 Years... 100 Movie Quotes\".) According to the mission transcript, the actual words uttered by Jack Swigert were \"Hey, we've got a problem here\" (talking over Haise, who had started \"Okay, Houston\"). Ground control responded by saying \"This is Houston, say again please.\" Jim Lovell then repeated, \"Houston, we've had a problem.\"\n\nOne other incorrect dialogue is after the re-entry blackout. In the movie, Tom Hanks (as Lovell) says \"Hello Houston... this is \"Odyssey\"... it's good to see you again.\" In the actual re-entry, the Command Module's transmission was finally acquired by a Sikorsky SH-3D Sea King recovery aircraft which then relayed communications to Mission Control. CAPCOM and fellow astronaut Joe Kerwin (not Mattingly, who serves as CAPCOM in this scene in the movie) then made a call to the spacecraft \"Odyssey, Houston standing by. Over.\" Jack Swigert, not Lovell, replied \"Okay, Joe,\" and unlike in the movie, this was well before the parachutes deployed; the celebrations depicted at Mission Control were triggered by visual confirmation of their deployment.\n\nThe tagline \"Failure is not an option\", stated in the film by Gene Kranz, also became very popular, but was not taken from the historical transcripts. The following story relates the origin of the phrase, from an e-mail by Apollo 13 Flight Dynamics Officer Jerry Bostick:\nAs far as the expression \"Failure is not an option,\" you are correct that Kranz never used that term. In preparation for the movie, the script writers, Al Reinart and Bill Broyles, came down to Clear Lake to interview me on \"What are the people in Mission Control really like?\" One of their questions was \"Weren't there times when everybody, or at least a few people, just panicked?\" My answer was \"No, when bad things happened, we just calmly laid out all the options, and failure was not one of them. We never panicked, and we never gave up on finding a solution.\" I immediately sensed that Bill Broyles wanted to leave and assumed that he was bored with the interview. Only months later did I learn that when they got in their car to leave, he started screaming, \"That's it! That's the tag line for the whole movie, Failure is not an option. Now we just have to figure out who to have say it.\" Of course, they gave it to the Kranz character, and the rest is history.\n\nIn the film, Flight Director Gene Kranz and his White Team are portrayed as managing all of the essential parts of the flight, from liftoff to landing. Consequently, the actual role of the other flight directors and teams, especially Glynn Lunney and his Black Team, were neglected. In fact, it was Flight Director Lunney and his Black Team who got Apollo 13 through its most critical period in the hours immediately after the explosion, including the mid-course correction that sent Apollo 13 on a \"free return\" trajectory around the Moon and back to the Earth. Astronaut Ken Mattingly, who was replaced as Apollo 13 Command Module Pilot at the last minute by Jack Swigert, later said:\nIf there was a hero, Glynn Lunney was, by himself, a hero, because when he walked in the room, I guarantee you, nobody knew what the hell was going on. Glynn walked in, took over this mess, and he just brought calm to the situation. I've never seen such an extraordinary example of leadership in my entire career. Absolutely magnificent. No general or admiral in wartime could ever be more magnificent than Glynn was that night. He and he alone brought all of the scared people together. And you've got to remember that the flight controllers in those days were—they were kids in their thirties. They were good, but very few of them had ever run into these kinds of choices in life, and they weren't used to that. All of a sudden, their confidence had been shaken. They were faced with things that they didn't understand, and Glynn walked in there, and he just kind of took charge.\n\nA DVD commentary track, recorded by Jim and Marilyn Lovell and included with both DVD versions, mentions several inaccuracies included in the film, all done for reasons of artistic license:\n\n\n\n"}
{"id": "17419853", "url": "https://en.wikipedia.org/wiki?curid=17419853", "title": "Chézy formula", "text": "Chézy formula\n\nIn fluid dynamics, the Chézy formula describes the mean flow velocity of steady, turbulent open channel flow:\n\nwhere\n\nThe formula is named after Antoine de Chézy, the French hydraulics engineer who devised it in 1775.\n\nThis formula can also be used with Manning's Roughness Coefficient, instead of Chézy's coefficient. Manning derived the following relation to C based upon experiments:\n\nwhere\n\nUnlike the Manning equation, which is empirical, the Chézy equation is derived from hydrodynamics theory.\n\n\n"}
{"id": "36294615", "url": "https://en.wikipedia.org/wiki?curid=36294615", "title": "Citation graph", "text": "Citation graph\n\nIn information science and bibliometrics, a citation graph (or citation network) is a directed graph in which each vertex represents a document and in which each edge represents a citation from the current publication to another.\n\nThe best known example is probably the citation graph where academic papers are the vertices, as described in the classic 1965 article \"Networks of Scientific Papers\" by Derek J. de Solla Price. Another example is formed by court judgements in which judges refer to earlier judgements to support their decisions. Citation analysis in a legal context is therefore an important commercial field. Patents are another well known example since they must refer to earlier patents which are known as prior art.\n\nA typical application for a citation graph is to try to measure the impact may be used in citation analysis as the basis for calculating measures of scientific impact such as the h-index, and for studying the structure and development of different fields of academic inquiry.\n\nThe actual construction of citation graphs can be complicated in practice as there is often no standard format for the citations in a bibliography. This makes the record linkage of the citations in a document to their cited document time consuming. Worse there are often errors in the citations introduced at any one of many stages of the process of publishing a document. However there is a long history of creating a database of citations, also known as a citation index, and so there is lot of information about such problems.\n\nIn principle each document will have a unique publication date and can only refer to earlier documents. This means that the edges of the graph are not only directed but they are also acyclic, that is there are no loops in the graph. So a perfect citation graph is an example of a directed acyclic graph. In practice this is not always true, for instance an academic paper goes through several versions in the publishing process and it may be possible to update bibliographies at different times in such a way to lead to edges that apparently point forward in time. However in practice it appears such citations are often less than 1% of the total number of links.\n\n\n"}
{"id": "9054351", "url": "https://en.wikipedia.org/wiki?curid=9054351", "title": "Clab", "text": "Clab\n\nThe Clab, also known as \"Centralt mellanlager för använt kärnbränsle\" (Swedish for 'Central holding storage for spent nuclear fuel') is an interim radioactive waste repository located about 25 kilometers north of Oskarshamn Nuclear Power Plant and is owned by Oskarshamnsverkets Kraftgrupp AB (OKG), in Oskarshamn. It was opened in 1985 for the storage of spent nuclear fuel from all Swedish nuclear power plants. The fuel is stored for 30 to 40 years, in preparation for final storage.\n\nThe facility currently contains approximately 6,500 tons of high-level waste, submerged in 8 meters of water, in pools 30 meters below the surface. Contaminated reactor components, such as control rods, are also stored at the facility. Waste produced from Sweden's nuclear power plants will continue be stored at the facility until the Swedish Nuclear Fuel and Waste Management Company can complete construction of a more permanent storage site at Forsmark.\n\n"}
{"id": "1844569", "url": "https://en.wikipedia.org/wiki?curid=1844569", "title": "Coming to Power", "text": "Coming to Power\n\nComing to Power: Writings and Graphics on Lesbian S/M is a 1981 book edited by members of the lesbian feminist S/M organisation SAMOIS. It is an anthology of lesbian S/M writings.\n\nIt was quickly out-of-print and reached a worldwide audience the following year when it was reprinted by Alyson Publications. The book alternates short stories with advice on techniques, a model that has been used by various other BDSM books since then.\n\nA sequel \"\" was published in 1996, edited by Pat Califia and Robin Sweeney.\n\nMany of the authors in \"Coming to Power\" characterise sadomasochism as the epitome of eroticism.\n\nIt is described in \"Coming on Strong: Gay Politics and Culture\" as containing lesbian erotica.\n"}
{"id": "10069097", "url": "https://en.wikipedia.org/wiki?curid=10069097", "title": "Compiler Description Language", "text": "Compiler Description Language\n\nCompiler Description Language, or CDL, is a programming language based on affix grammars. It is very similar to Backus–Naur form (BNF) notation. It was designed for the development of compilers. It is very limited in its capabilities and control flow; and intentionally so. The benefits of these limitations are twofold. On the one hand they make possible the sophisticated data and control flow analysis used by the CDL2 optimizers resulting in extremely efficient code. The other benefit is that they foster a highly verbose naming convention. This in turn leads to programs that are to a great extent self-documenting.\n\nThe language looks a bit like Prolog (this is not surprising since both languages arose at about the same time out of work on affix grammars). As opposed to Prolog however, control flow in CDL is deterministically based on success/failure i.e., no other alternatives are tried when the current one succeeds. This idea is also used in parsing expression grammars.\n\nCDL3 is the third version of the CDL language, significantly different from the previous two versions.\n\nThe original version, designed by Cornelis H. A. Koster at the University of Nijmegen emerged in 1971 had a rather unusual concept: it had no core. A typical programming language source is translated to machine instructions or canned sequences of those instructions. Those represent the core, the most basic abstractions that the given language supports. Such primitives can be the additions of numbers, copying variables to each other and so on. CDL1 lacks such a core, it is the responsibility of the programmer to provide the primitive operations in a form that can then be turned into machine instructions by means of an assembler or a compiler for a traditional language. The CDL1 language itself has no concept of primitives, no concept of data types apart from the machine word (an abstract unit of storage - not necessarily a real machine word as such). The evaluation rules are rather similar to the Backus–Naur form syntax descriptions; in fact, writing a parser for a language described in BNF is rather simple in CDL1.\n\nBasically, the language consists of rules. A rule can either succeed or fail. A rule consists of alternatives that are sequences of other rule invocations. A rule succeeds if any of its alternatives succeeds; these are tried in sequence. An alternative succeeds if all of its rule invocations succeed. The language provides operators to create evaluation loops without recursion (although this is not strictly necessary in CDL2 as the optimizer achieves the same effect) and some shortcuts to increase the efficiency of the otherwise recursive evaluation but the basic concept is as above. Apart from the obvious application in context-free grammar parsing, CDL is also well suited to control applications, since a lot of control applications are essentially deeply nested if-then rules.\n\nEach CDL1 rule, while being evaluated, can act on data, which is of unspecified type. Ideally the data should not be changed unless the rule is successful (no side effects on failure). This causes problems as although this rule may succeed, the rule invoking it might still fail, in which case the data change should not take effect. It is fairly easy (albeit memory intensive) to assure the above behavior if all the data is dynamically allocated on a stack but it is rather hard when there's static data, which is often the case. The CDL2 compiler is able to flag the possible violations thanks to the requirement that the direction of parameters (input,output,input-output) and the type of rules (can fail: test, predicate; cannot fail: function, action; can have side effect: predicate, action; cannot have side effect: test, function) must be specified by the programmer.\n\nAs the rule evaluation is based on calling simpler and simpler rules, at the bottom there should be some primitive rules that do the actual work. That is where CDL1 is very surprising: it does not have those primitives. You have to provide those rules yourself. If you need addition in your program, you have to create a rule that has two input parameters and one output parameter and the output is set to be the sum of the two inputs by your code. The CDL compiler uses your code as strings (there are conventions how to refer to the input and output variables) and simply emits it as needed. If you describe your adding rule using assembly, then you will need an assembler to translate the CDL compiler's output to machine code. If you describe all the primitive rules (macros in CDL terminology) in Pascal or C, then you need a Pascal or C compiler to run after the CDL compiler. This lack of core primitives can be very painful when you have to write a snippet of code even for the simplest machine instruction operation but on the other hand it gives you great flexibility in implementing esoteric abstract primitives acting on exotic abstract objects (the 'machine word' in CDL is more like 'unit of data storage', with no reference to the kind of data stored there). Additionally large projects made use of carefully crafted libraries of primitives. These were then replicated for each target architecture and OS allowing the production of highly efficient code for all.\n\nTo get a feel for the language, here is a small code fragment adapted from the CDL2 manual:\nThe primitive operations are here defined in terms of Java (or C). This is not a complete program; we must define the Java array \"items\" elsewhere.\n\nCDL2, which appeared in 1976, kept the principles of CDL1 but made the language suitable for large projects. It introduced modules, enforced data-change-only-on-success and extended the capabilities of the language somewhat. The optimizers in the CDL2 compiler and especially in the CDL2 Laboratory (an IDE for CDL2) were world class and not just for their time. One feature of the CDL2 Laboratory optimizer is almost unique: it can perform optimizations across compilation units, i.e., treating the entire program as a single compilation.\n\nCDL3 is a more recent language. It gave up the open-ended feature of the previous CDL versions and it provides primitives to basic arithmetic and storage access. The extremely puritan syntax of the earlier CDL versions (the number of keywords and symbols both run in single digits) have also been relaxed and some basic concepts are now expressed in syntax rather than explicit semantics. In addition, data types have been introduced to the language.\n\nThe commercial mbp Cobol (a Cobol compiler for the PC) as well as the MProlog system (an industrial strength Prolog implementation that ran on numerous architectures (IBM mainframe, VAX, PDP-11, Intel 8086, etc.) and OS-s (DOS/OS/CMS/BS2000, VMS/Unix, DOS/Windows/OS2)). The latter in particular is testimony to CDL2's portability.\n\nWhile most programs written with CDL have been compilers, there is at least one commercial GUI application that was developed and maintained in CDL. This application was a dental image acquisition application now owned by DEXIS. A dental office management system was also once developed in CDL.\n\nThe software for the Mephisto III chess computer was written with CDL2.\n\n"}
{"id": "7236219", "url": "https://en.wikipedia.org/wiki?curid=7236219", "title": "Dry needling", "text": "Dry needling\n\nDry needling, also known as myofascial trigger point dry needling, is an unproven technique in alternative medicine similar to acupuncture. It involves the use of either solid filiform needles or hollow-core hypodermic needles for therapy of muscle pain, including pain related to myofascial pain syndrome. Dry needling is sometimes also known as intramuscular stimulation (IMS).\n\nWhile many studies have been performed to test the efficacy of dry needling as a treatment for muscle pain, there remains no scientific consensus as to whether or not it is effective. Some results suggest that it is an effective treatment for certain kinds of muscle pain, while other studies have shown no benefit compared to a placebo. There are not enough high-quality studies of the technique to draw clear conclusions about its efficacy.\n\nThe origin of the term \"dry needling\" is attributed to Janet G. Travell. In her book, \"Myofascial Pain and Dysfunction: Trigger Point Manual\", Travell uses the term \"dry needling\" to differentiate between two hypodermic needle techniques when performing trigger point therapy. However, Travell did not elaborate on the details on the techniques of dry needling; the current techniques of dry needling were based on the traditional and western medical acupuncture. The two techniques Travell described are the injection of a local anesthetic and the mechanical use of a hypodermic needle without injecting a solution (Travell, Simons, & Simons, 1999, pp. 154–155). Travell preferred a 22-gauge, 1.5-in hypodermic needle for trigger point therapy and used this needle for both injection therapy and dry needling. Travell never used an acupuncture needle. Travell had access to acupuncture needles but reasoned that they were far too thin for trigger point therapy. She preferred hypodermic needles because of their strength and tactile feedback: \"A 22-gauge, 3.8-cm (1.5-in) needle is usually suitable for most superficial muscles. In hyperalgesic patients, a 25-gauge, 3.8-cm (1.5-in) needle may cause less discomfort, but will not provide the clear \"feel\" of the structures being penetrated by the needle and is more likely to be deflected by the dense contraction knots that are the target... A 27-gauge needle, 3.8-cm (1.5-in) needle is even more flexible; the tip is more likely to be deflected by the contraction knots and it provides less tactile feedback for precision injection\" (Travell, Simons, & Simons, 1999, p. 156).\n\nThe use of a hypodermic needle for dry needling was described by Chang-Zern Hong in his research paper on \"Lidocaine Injection Versus Dry Needling to Myofascial Trigger Point\". In his research, he describes the procedure for trigger point or MTrP injection and dry needling by using a 27-gauge hypodermic needle -in long (Hong, 1994). Both Travell and Hong used hypodermic needles for dry needling. Hong, like Travell, did not use an acupuncture needle for dry needling.\n\nAlthough dry needling originally utilized only hypodermic needles due to the concern that solid needles had neither the strength or tactile feedback that hypodermic needles provided and that the needle could be deflected by \"dense contraction knots\", those concerns have proven unfounded and many healthcare practitioners who perform dry needling have found that the acupuncture needles not only provides better tactile feedback but also penetrate the \"dense muscle knots\" better and are easier to manage and caused less discomfort to patients. For that reason, both the use of hypodermic needles and the use of acupuncture needles are now accepted in dry needling practice. Ofttimes practitioners who use hypodermic needles also provide trigger point injection treatment to patients and therefore find the use of hypodermic needles a better choice. As their use became more common, some dry needling practitioners without acupuncture in their scope of practice, started to refer to these needles by their technical design term as \"solid filiform needles\" as opposed to the FDA designation \"acupuncture needle\".\n\nThe \"solid filiform needle\" used in dry needling is regulated by the FDA as a Class II medical device described in the code titled \"Sec. 880.5580 Acupuncture needle\" as \"a device intended to pierce the skin in the practice of acupuncture\". Per the Food and Drug Act of 1906 and the subsequent Amendments to said act, the FDA definition applies to how the needles can be marketed and does not mean that acupuncture is the only medical procedure where these needles can be used. Dry needling using such a needle contrasts with the use of a hollow hypodermic needle to inject substances such as saline solution, botox or corticosteroids to the same point. In a small number of studies, the use of a solid needle has been found to be as effective as injection of substances in such cases as relief of pain in muscles and connective tissue.\n\nThe Founder of Integrative Systemic Dry Needling (ISDN), Yun-Tao Ma, has been spearheading the \"dry needling\" movement in the United States. Ma states, \"Although ISDN originated in traditional Chinese methods, it has developed from the ancient empirical approach to become modern medical art rooted in evidence-based thinking and practice.\" Ma then contradicts himself stating, \"Dry needling technique is a modern Western medical modality that is not related to traditional Chinese acupuncture in any way. Dry needling has its own theoretical concepts, terminology, needling technique and clinical application.\" \nMa realizing both the self-contradictions and the legal ramifications of dry needling being rooted in acupuncture and Chinese medicine has since taken down all information in his bios regarding his education in Chinese Medicine and being a Licensed Acupuncturist in the United States.\n\nThe American Academy of Orthopedic Manual Physical Therapists (AAOMPT) states:\n\nThe statement above is self-explanatory on the functional, physiological and medical aspect of treatment. His book Manual of Dry Needling Techniques Color Edition (2) (Volume 1) is a basic reference text for the therapists who are trained in the method of dry needling procedures in accordance with the norm of the practice of their respective countries. The basic steps given in the book can make a practicing therapist use dry needling technique for the subjects in different clinical conditions. The text focus not only on the steps needed to be performed but also focus on what should not be done by a therapist while performing the procedure. At work, we have taken all the guidelines given by OSHA for blood borne diseases as well as WHO guideline on workplace and hand hygiene.\n\nDry needling for the treatment of myofascial (muscular) trigger points is based on theories similar, but not exclusive, to traditional acupuncture; both acupuncture and dry needling target the trigger points, which is a direct and palpable source of patient pain. However, dry needling theory is only beginning to describe the complex sensation referral patterns that have been documented as \"channels\" or \"meridians\" in Chinese Medicine. Dry needling, and its treatment techniques and desired effects, would be most directly comparable to the use of 'a-shi' points in acupuncture. What further distinguishes dry needling from traditional acupuncture is that it does not use the full range of traditional theories of Chinese Medicine which is used to treat not only pain but other non-musculoskeletal issues which often are the cause of pain. The distinction between trigger points and acupuncture points for the relief of pain is blurred. As reported by Melzack, et al., there is a high degree of correspondence (71% based on their analysis) between published locations of trigger points and classical acupuncture points for the relief of pain. The debated distinction between dry needling and acupuncture has become a controversy because it relates to an issue of scope of practice of various professions.\n\nIn the treatment of trigger points for persons with myofascial pain syndrome, dry needling is an invasive procedure in which a filiform needle is inserted into the skin and muscle directly at a myofascial trigger point. A myofascial trigger point consists of multiple contraction knots, which are related to the production and maintenance of the pain cycle. Deep dry needling for treating trigger points was first introduced by Czech physician Karel Lewit in 1979.\nLewit had noticed that the success of injections into trigger points in relieving pain was apparently unconnected to the analgesic used.\n\nProper dry needling of a myofascial trigger point will elicit a local twitch response (LTR), which is an involuntary spinal cord reflex in which the muscle fibers in the taut band of muscle contract. The LTR indicates the proper placement of the needle in a trigger point. Dry needling that elicits LTRs improves treatment outcomes, and may work by activating endogenous opioids. The activation of the endogenous opioids is for an analgesic effect using the Gate Control Theory of Pain. Inserting the needle can itself cause considerable pain, although when done by well-trained practitioners that is not a common occurrence. No study to date has reported the reliability of trigger point diagnosis and physical diagnosis cannot be recommended as a reliable test for the diagnosis of trigger points.\n\nThere is currently no standardized form of dry needling, no body of evidence that indicates its efficacy, and there is no medical action pathway that provides a theoretical basis for why dry needling should be efficacious. Many of the studies published about dry needling do not have strong evidence; either the studies were not randomized, contained small sample sizes, had high dropout rates, used active interventions in the control group, did not follow the minimally acceptable criteria to diagnose a myofascial trigger point, or did not clearly state that myofascial trigger points were the sole cause for the pain. For example, in a systematic review on needling therapies in the management of myofascial trigger points, only 8 of the 23 trials described the minimally acceptable criteria for diagnosing a trigger point. Locating the trigger point for dry needling is the basis for performing dry needling and should, therefore, be documented in each study performing this technique. In the same review, two studies tested the efficacy beyond placebo of dry needling in the treatment of myofascial trigger point pain, but, in one, the dropout rate was 48% and it was neither blinded nor randomized, and the other study used potentially active interventions in the control group. Another concluded that dry needling can reduce pain, thus improving mood, function, and disability. The study used the dry needling on trigger points to relieve pain in patients with chronic myofascial pain.\n\nAnother systematic review concluded that dry needling for the treatment of myofascial pain syndrome in the lower back appeared to be a useful addition to standard therapies, but stated clear recommendations could not be made because the published studies are small and of low quality. A 2007 meta-analysis examining dry needling of myofascial trigger points concluded that the effect of needling was not significantly different to that of placebo controls, though the trend in the results could be compatible with a treatment effect. One study (Lorenzo et al. 2004) did show a short-term reduction in shoulder pain in stroke patients who received needling with standard rehabilitation compared to those who received standard care alone, but the study was open-label and measurement timings differed, limiting the use of the study. Again the small sample size and poor quality of studies was highlighted.\nA 2013 systematic review and meta-analysis released by JOSPT on \"effectiveness of dry needling for upper-quarter myofascial pain\" recommends the usage of dry needling, compared to sham or placebo, for decreasing pain immediately after treatment and at 4 weeks in patients with upper quarter myofascial pain syndrome. However, the authors caution that \"the limited number of studies performed to date, combined with methodological flaws in many of the studies, prompts caution in interpreting the results of the meta-analysis performed\".\n\nA 2014 review of dry needling found insufficient high-quality evidence for the use of direct dry needling for short and long-term pain and disability reduction in patients with musculoskeletal pain syndromes.\nThe same review found that robust evidence validating the clinical diagnostic criteria for trigger point identification or diagnosis is lacking and that high-quality studies demonstrate that manual examination for the identification and localization of a trigger point is neither valid nor reliable between-examiners. A 2017 systematic review and meta-analysis found very little evidence supporting the use of trigger point dry needling to treat upper shoulder pain and dysfunction.\n\nA 2018 literature review collated 16 eligible studies and found that although it appeared to be a superior technique to acupuncture for lower back pain current evidence is not robust enough to draw a clear conclusion about safety and efficacy. Another found that although dry needling appears to reduce pain for patients in all 11 studies considered, it was not clear what, if any other than placebo, mechanism was acting.\n\nMany physical therapists and chiropractors have asserted that they are not practicing acupuncture when dry needling. They assert that much of the basic physiological and biomechanical knowledge that dry needling utilizes is taught as part of their core physical therapy and chiropractic education and that the specific dry needling skills are supplemental to that knowledge and not exclusive to acupuncture. However, the originators and proponents of dry needling acknowledged that certain aspects of this techniques were inspired by acupuncture although they also acknowledge that the medical basis for it is purely Western Medicine in nature and therefore is not validly a subset of acupuncture and is a separate medical process. Many acupuncturists have argued that dry needling appears to be an acupuncture technique requiring minimal training that has been re-branded under a new name (\"dry needling\"). Whether dry needling is considered to be acupuncture depends on the definition of acupuncture, and it is argued that trigger points do not correspond to acupuncture points or meridians. They correspond by definition to the ad hoc category of 'a-shi' acupoints. It is important to note that this category of points is not necessarily distinct from other formal categories of acupoints. In 1983, Janet Travell et al. described trigger point locations as 92% in correspondence with known acupuncture points. In 2006, Peter T. Dorsher, acupuncturist at the Mayo Clinic, concludes that the two point systems are in over 90% agreement. In 2009, Dorsher and Fleckenstein conclude that the strong (up to 91%) consistency of the distributions of trigger point regions’ referred pain patterns to acupuncture meridians provides evidence that trigger points most likely represent the same physiological phenomenon as acupuncture points in the treatment of pain disorders. An article in Acupuncture Today (May 2011, p. 3, \"Scope and Standards for Acupuncture: Dry Needling?\") further corroborates the 92% correspondence of trigger points to acupuncture points. In 2011, The Council of Colleges of Acupuncture and Oriental Medicine (CCAOM) published a position paper describing dry needling as an acupuncture technique.\nThe North Carolina Acupuncture Licensing Board has published a position statement asserting that dry needling is acupuncture and thus is covered by the North Carolina Acupuncture Licensing law, and is not within the present scope of practice of Physical Therapists, and Physical Therapists are not among the professions exempt from the law. The Attorney General was asked for an opinion by the North Carolina Acupuncture Licensing Board which he gave dated Dec 1st, 2011 saying that \"In our opinion, the Board of Physical Therapy Examiners\nmay determine that dry needling is within the scope of practice of physical therapy if it conducts\nrulemaking under the Administrative Procedure Act and adopts rules that relate dry needling to\nthe statutory definition of practice of physical therapy.\" But that is a matter of opinion and not a matter of law. The North Carolina Rules Review Committee of the legislative branch found that the North Carolina Physical Therapy Board had no statutory authority for the proposed rule. The Physical Therapy board subsequently decided that they had the right to declare dry needling within scope anyway \"the Board believes physical therapists can continue to perform dry needling so long as they possess the requisite education and training required by N.C.G.S. § 90-270.24(4), but there are no regulations to set the specific requirements for engaging in dry needling.\".\n\nIn May 2011 the Oregon Board of Chiropractic Examiners ruled to allow \"dry needling\" into the chiropractic scope of practice with 24 hours of training. In July 2011 the Court of Appeals of the State of Oregon issued an injunction, preventing chiropractors from practicing dry needling until the case is heard in court. The document issued by the court states that \"dry needling\" is \"substantially the same\" as acupuncture and that the \"respondent has not explained how 24 hours of training, with no clinical component, provides sufficient training to chiropractors to adequately protect patients.\" In September 2011, the Oregon Board of Chiropractic Examiners And Oregon Attorney General appealed said order on the grounds that they feel the commissioner who issued the order was mistaken in his assertion. On November 10, 2011, The Court of Appeals of the State of Oregon issued an Order Denying the Motion for Reconsideration. The effect of said ruling is that the entire Appeals Court will now determine if the stay was appropriate. The stay is relevant \"only\" in the State of Oregon.\n\nIn January 2014, The Oregon Court of Appeals ruled that the Oregon Board of Chiropractic Examiners did not have the statutory authority to include dry needling in the scope of practice for chiropractors in that state. The ruling did not address whether chiropractors have the medical expertise to use dry needling or whether the training they were being given was adequate. Pending further discussion of training requirements the Oregon Physical Therapist Licensing Board has advised all Oregon physical therapists against practicing dry needling. They have not changed their ruling that dry needling is within the scope of practice for Oregon Physical Therapists.\n\nThe American Medical Association adopted a policy in 2016 that said physical therapists and other non-physicians practicing dry needling should – at a minimum – have standards that are similar to the ones for training, certification, and continuing education that exist for acupuncture. AMA board member Russell W. H. Kridel, M.D. stated \"Lax regulation and nonexistent standards surround this invasive practice. For patients' safety, practitioners should meet standards required for licensed acupuncturists and physicians.\"\n\n"}
{"id": "1982545", "url": "https://en.wikipedia.org/wiki?curid=1982545", "title": "Ex-Cubs Factor", "text": "Ex-Cubs Factor\n\nThe Ex-Cubs Factor (or Ex-Cub Factor) is a seemingly spurious correlation that was seen as essentially a corollary to the Curse of the Billy Goat. Widely published in the 1990s, the hypothesis asserted that since the appearance by the Chicago Cubs in the 1945 World Series, any baseball team headed into the World Series with three or more former Cubs on its roster has \"a critical mass of Cubness\" and a strong likelihood of failure.\n\nThe theory was developed in October 15, 1981 by Ron Berler, a freelance journalist and Cubs fan. Berler posited in an article that \"it is utterly impossible for a team with three or more ex-Cubs to win the series.\" Berler based this on a pattern that he observed in the post-1945 era; 1945 being the last time the Chicago Cubs made it to a World Series until 2016.\n\nBerler cited many examples of teams with three or more ex-Cubs on their teams that reached the World Series and lost: including the 1958 Milwaukee Braves, the 1966 Los Angeles Dodgers, and the 1978 Los Angeles Dodgers. The 1978 Dodgers, according to Berler, had lost the 1977 World Series with three ex-Cubs on their roster and seemed to be doing well the next season when they traded one of those ex-Cubs (Mike Garman) away and began playing excellently. However, four weeks later, the Dodgers traded for ex-Cub Bill North and, in the words of Berler, the \"team suffered an immediate tailspin and barely beat Cincinnati to the pennant.\" The 1978 Dodgers lost the World Series to the Yankees leading to Berler's hypothesis of three ex-Cubs making it impossible to win a championship. In addition, the 1980 Kansas City Royals lost the World Series as well with 3 ex-Cubs.\n\nIn the original article, Berler predicted that based on this pattern the 1981 New York Yankees would not win the World Series because they had five ex-Cubs on their roster (Oscar Gamble, Bobby Murcer, Dave LaRoche, Rick Reuschel, and Barry Foote), this prediction went against the odds which heavily favored the Yankees at the time. His prediction about the 1981 World Series based on this hypothesis was proved correct with the Yankees losing to the Dodgers, four games to two.\n\nBerler relates the relationship to the inherent \"cubness\" that ex-Cubs take to their future teams. In the original article, he wrote that \"the ballclub possesses eerie, bewitching powers over its players\" and that \"'Cubness'...is synonymous with the rankest sort of abject failure, and is a condition chronic among all Cubs, past and present.\"\n\nMike Royko, who popularized the term in his columns in Chicago, wrote that cubness was a \"virus\" where \"Three or more ex-Cubs could infect an entire team with the will to lose, no matter how skillful that team might appear.\" Berler adopted a similar explanation in later articulations, writing that the virus \"attacks all who've played for the Cubs, even if only for a single day. There is no inoculation, no cure. When traded to another team, ex-Cubs become carriers of this debilitating disease—the ticks of baseball. Any World Series team infested with three or more of them turns addled and confused, losing all ability to win.\"\n\nMike Royko developed an additional hypothesis contending that \"A team with no ex-Cubs probably has the edge on a team that has even one.\" The key evidence provided for this was the 1986 World Series which had the New York Mets (with no ex-Cubs) defeat the Boston Red Sox (with a dramatic error by ex-Cub Bill Buckner). This hypothesis was largely discredited in the 2003 World Series with the Florida Marlins (who had ex-Cub Lenny Harris) defeating the New York Yankees (who had no ex-Cubs), though Harris did not play in the World Series.\n\nSince 1945, of the 23 teams to reach the World Series with three or more Cubs players, only 3 have won the World Series (the 1960 Pirates, the 2001 Diamondbacks, and the 2008 Phillies). Since its articulation in 1981, the Ex-Cubs Factor has been used to predict and explain post-season and World Series defeats for many different baseball teams.\n\nDuring the 1980s, the ex-Cubs factor was used to explain a number of losses by teams. It was used, in a letter to the editor, as a reason for the loss by the San Diego Padres (who ironically beat the Cubs to get to the Series) in the 1984 World Series (with three ex-Cubs) and by the collapse of the 1985 Toronto Blue Jays who had added ex-Cub Cliff Johnson on August 28 and then blew a 3-1 lead over the Royals in the 1985 American League Championship Series.\n\nWhile not falling under the curse in the traditional sense, Bill Buckner's infamous gaffe in Game 6 of the 1986 World Series can be interpreted to fit the Ex-Cubs Factor. Buckner, a former Cub, booted a ground ball hit by New York Mets batter Mookie Wilson, allowing Ray Knight to come around and score the winning run. The Mets would go on to win the series in seven games, and Buckner would never win the World Series in his career. Upon video analysis, Buckner was shown to be wearing a Cubs batting glove under his mitt when he made the error.\n\nThe theory became more prominent in 1990 when it was popularized by syndicated \"Chicago Tribune\" columnist Mike Royko and continued predictions by Ron Berler. On October 16, 1990, Berler again asserted the ex-Cubs factor as the reason that the favored Oakland Athletics (with ex-Cubs Scott Sanderson, Dennis Eckersley, and Ron Hassey) would lose in the 1990 World Series (which they did by getting swept by the Cincinnati Reds). It was again cited by Berler as a reason for the defeat of the league-leading 1991 Pittsburgh Pirates in the 1991 National League Championship Series to the Atlanta Braves. Mike Royko used the ex-Cubs factor to predict the playoff collapse of the 104-win 1993 Atlanta Braves (which did occur in the 1993 National League Championship Series).\n\nThe ex-Cubs factor hypothesis was used to predict the San Francisco Giants (with three ex-Cubs Shawon Dunston, Benito Santiago and Tim Worrell) loss to the Anaheim Angels in the 2002 World Series.\n\nThe ex-Cubs factor hypothesis has also been used to explain the results of the 2004 American League Championship Series (ALCS) and the 2004 World Series, both won by the Boston Red Sox. In the 2004 ALCS, the Yankees (with six ex-Cubs) lost a 3-0 game lead to the Boston Red Sox, the first time in Major League Baseball history. The Red Sox (with only two ex-Cubs) then went on to defeat the St. Louis Cardinals (with three ex-Cubs) in the World Series. The 2009 World Series resulted in a win by the New York Yankees (with two ex-Cubs) over the Philadelphia Phillies (with three ex-Cubs).\n\nBerler's study of the ex-Cub factor since 1945 revealed only one exception since 1945: the 1960 Pittsburgh Pirates, who won the World Series with three ex-Cubs on their roster. Berler contends that one of those ex-Cubs, Don Hoak, spent so little time in Chicago, that he did not fully develop what Berler called his \"Cubness\". Berler quoted pitcher Jim Brosnan as saying that \"Hoak is quite possibly the only man who ever conquered his Cubness\".\n\nThe 2001 Arizona Diamondbacks similarly had four ex-Cubs on their roster (Miguel Batista, Mark Grace, Mike Morgan, and Luis Gonzalez) but defeated the New York Yankees in the 2001 World Series. Mark Grace declared in an post-game interview that \"We beat the ex-Cub Factor!\". The sympathy many fans felt for New York City following the September 11 attacks, topped off by the four ex-Cubs on the Arizona roster, seemed to stack up against the Diamondbacks. But Arizona won Game 6 in a lopsided score, and then won Game 7 in a come-from-behind finish, scoring a pair of runs in the ninth inning to win the Series. In fact, two of the four former Cubs played prominent roles in that ninth inning, Mark Grace getting a lead off single and Luis Gonzalez driving in the winning run with a single.\n\nThe 1960 Pittsburgh Pirates and the 2001 Arizona Diamondbacks (both with victories over the New York Yankees) are the only teams that have won the World Series in Game 7 walk-off victories.\n\nThe 2008 Philadelphia Phillies team was declared prior to the post-season by Berler as \"doomed this year by the Ex-Cub Factor\" (with three ex-Cubs Scott Eyre, Jamie Moyer, and Matt Stairs on their roster). However, the Phillies defeated the Los Angeles Dodgers (a team also with three ex-Cubs) in the National League Championship Series and the Tampa Bay Rays (one ex-Cub) in the 2008 World Series.\n\nThe Ex-Cubs Factor hypothesis has been applied to other professional baseball teams in both positive and negative references. For example, sports writer Jeff Blair has argued there exists an \"ex-Expos factor\" where number of former Montreal Expos players correlates with post-season success. Jim Caple for ESPN.com has similarly proposed an ex-Mariners factor (or XMF) which explores the excellent play by ex-Mariners for other teams in postseason baseball.\n\n\n"}
{"id": "22425059", "url": "https://en.wikipedia.org/wiki?curid=22425059", "title": "Federally funded research and development centers", "text": "Federally funded research and development centers\n\nFederally funded research and development centers (FFRDCs) are public-private partnerships which conduct research for the United States Government. They are administered in accordance with U.S. Code of Federal Regulations, Title 48, Part 35, Section 35.017 by universities and corporations. There are currently 42 recognized FFRDCs that are sponsored by the U.S. government. FFRDCs are similar to the University Affiliated Research Centers run by the United States Department of Defense.\n\nDuring World War II scientists, engineers, mathematicians, and other specialists became part of the United States massive war effort—leading to evolutions in radar, aircraft, computing and, most famously, the development of nuclear weapons through the Manhattan Project. The end of armed conflict did not end the need for organized research and development in support of the government.\n\nAs the Cold War became the new reality, government officials and their scientific advisors advanced the idea of a systematic approach to research, development, and acquisitions, one independent of the ups and downs of the marketplace and free of the restrictions on civil service. From this idea arose the concept of FFRDCs—private entities that would work almost exclusively on behalf of the government, be free of organizational conflicts of interest, and maintain a stable workforce composed of highly trained technical talent.\n\nThe U.S. Air Force created the first FFRDC, the RAND Corporation, in 1947. Others grew directly out of their wartime roles. For example, MIT Lincoln Laboratory, founded in 1951, originated as the Radiation Laboratory at MIT, and the Navy's Operation Research Group evolved into the Center for Naval Analyses. The first FFRDCs served the Department of Defense. Since then, other government organizations have sponsored FFRDCs to meet their specific needs. In 1969, the number of FFRDCs peaked at 74.\n\nThe following list includes all current FFRDCs:\n\n\n"}
{"id": "31071816", "url": "https://en.wikipedia.org/wiki?curid=31071816", "title": "Geoprofessions", "text": "Geoprofessions\n\nGeoprofessions is a term coined by the Geoprofessional Business Association to connote various technical disciplines that involve engineering, earth and environmental services applied to below-ground (“subsurface”), ground-surface, and ground-surface-connected conditions, structures, or formations. The principal disciplines include, as major categories:\n\nEach discipline involves specialties, many of which are recognized through professional designations that governments and societies or associations confer based upon a person’s education, training, experience, and educational accomplishments. In the United States, engineers must be licensed in the state or territory where they practice engineering. Most states license geologists and several license environmental “site professionals.” Several states license engineering geologists and recognize geotechnical engineering through a geotechnical-engineering titling act.\n\nAlthough geotechnical engineering is applied for a variety of purposes, it is essential to foundation design. As such, geotechnical engineering is applicable to every existing or new structure on the planet; every building and every highway, bridge, tunnel, harbor, airport, water line, reservoir, or other public work. Commonly, the geotechnical-engineering service comprises a study of subsurface conditions using various sampling, in-situ testing, and/or other site-characterization techniques. The instrument of professional service in those cases typically is a report through which geotechnical engineers relate the information they have been retained to provide, typically: their findings; their opinions about subsurface materials and conditions; their judgment about how the subsurface materials and conditions assumed to exist probably will behave when subjected to loads or used as building material; and their preliminary recommendations for materials usage or appropriate foundation systems, the latter based on their knowledge of a structure’s size, shape, weight, etc., and the subsurface/structure interactions likely to occur. Civil engineers, structural engineers, and architects, feasibly among other members of the project team, apply the geotechnical findings and preliminary recommendations to take the structure’s design forward. They realize these preliminary recommendations are subject to change, however, because – as a matter of practical necessity related to the observational method inherent to geotechnical engineering – geotechnical engineers base their recommendations on the composition of samples taken from a tiny portion of a site whose actual subsurface conditions are unknowable before excavation, because they are hidden by earth and/or rock and/or water. For this reason, as a key component of a complete geotechnical engineering service, geotechnical engineers employ construction-materials engineering and testing (CoMET) to observe subsurface materials as they are exposed through excavation. To help achieve economies on their clients’ behalf, geotechnical engineers assign their field representatives – specially educated and trained paraprofessionals – to observe the excavated materials and the excavations themselves in light of conditions the geotechnical engineers opined to exist. When differences are discovered, the geotechnical engineers evaluate the new findings and, when necessary, modify their design and construction recommendations. Because such changes could require other members of the design and construction team to modify their designs, specifications, and proposed methods, many owners have their geotechnical engineers serve as active members of the project team from project inception to conclusion, working with others to help ensure appropriate application of geotechnical information and judgments.\n\nIn other cases, geotechnical engineering goes beyond a study and construction recommendations to include design of soil and rock structures. The most common of these are the pavements that make up our streets and highways, airport runways, and bridge and tunnel decks, among other paved improvements. Geotechnical engineers design the pavements in terms of the subgrade, subbase, and base layers of materials to be used, and the thickness and composition of each. Geotechnical engineers also design the earth-retention walls associated with structures such as levees, earthen dams, reservoirs, and landfills. In other cases, the design is applied to contain earth, via structures such as excavation-support systems and retaining walls. Sometimes referred to as geostructural engineering or geostructural design, these services are also intrinsic to hydraulic engineering, hydrogeologic engineering, coastal engineering, geologic engineering and water-resources engineering. Geotechnical-engineering design is also applied for structures such as tunnels, bridges, dams, and other structures beneath, on, or connected to the surface of the earth. Geotechnical engineering, like geology, engineering geology, and geologic engineering, also involves the specialties of rock mechanics and soil mechanics, and often requires knowledge of geotextiles and geosynthetics, as well as an array of instrumentation and monitoring equipment, to help ensure specified conditions are achieved and maintained.\n\nEarthquake engineering and landslide detection, remediation, and prevention are geoprofessional services associated with specialized types of geotechnical engineering (as well as geophysics; see below), as is forensic geotechnical engineering, a geoprofessional service applied to determine why a certain applicable type of event – usually a failure of some sort – occurred. (Virtually all geoprofessional services can be performed for forensic purposes, commonly as litigation-support/expert witness services.) Railway-systems engineering is another type of specialized geotechnical engineering, as are the design of piers and bulkheads, drydocks, on-shore and off-shore wind-turbine systems, and systems that stabilize oil platforms and other marine structures to the sea floor.\n\nGeotechnical engineers have long been involved in sustainability initiatives, including (among many others) the use of excavated materials; the safe application of contaminated subsurface materials; the recycling of asphalt, concrete, and building rubble and debris; and the design of permeable pavements.\n\nAll civil-engineering specialties and projects – roads and highways, bridges, rail systems, ports and other waterfront structures, airport terminals, etc. – require the involvement of geotechnical engineers and engineering, meaning that many civil-engineering pursuits are geoprofessional pursuits to a greater or lesser degree. However, geotechnical engineering has for centuries also been associated with military engineering; sappers (in general) and miners (whose tunneling design services (known as landmining and undermining) were used in military-siege operations).\n\nEngineering geologist.\n(a) Elements of the engineering geologist specialty. \nThe practice of engineering geology involves the interpretation, evaluation, analysis, and application of geological information and data to civil works. Geotechnical soil and rock units are designated, characterized, and classified, using standard engineering soil and rock classification systems. Relationships are interpreted between landform development, current and past geologic processes, ground and surface water, and the strength characteristics of soil and rock. Processes evaluated include both surficial processes (for example, slope, fluvial, and coastal processes), and deep-seated processes (for example, volcanic activity and seismicity). Geotechnical zones or domains are designated based on soil and rocked geological strength characteristics, common landforms, related geologic processes, or other pertinent factors. Proposed developmental modifications are evaluated and, where appropriate, analyzed to predict potential or likely changes in types and rates of surficial geologic processes. Proposed modifications may include such things as vegetation removal, using various types of earth materials in construction, applying loads to shallow or deep foundations, constructing cut or fill slopes and other grading, and modifying ground and surface water flow. The effects of surficial and deep-seated geologic processes are evaluated and analyzed to predict their potential effect on public health, public safety, land use, or proposed development.\n(b) Typical engineering geologic applications and types of projects. Engineering geology is applied during all project phases, from conception through planning, design, construction, maintenance, and, in some cases, reclamation and closure. Planning-level engineering geologic work is commonly conducted in response to forest practice regulations, critical areas ordinances, and the State Environmental Policy Act. Typical planning-level engineering geologic applications include timber harvest planning, proposed location of residential and commercial developments and other buildings and facilities, and alternative route selection for roads, rail lines, trails, and utilities. Site-specific engineering geologic applications include cuts, fills, and tunnels for roads, trails, railroads, and utility lines; foundations for bridges and other drainage structures, retaining walls and shoring, dams, buildings, water towers, slope, channel and shoreline stabilization facilities, fish ladders and hatcheries, ski lifts and other structures; landings for logging and other work platforms; airport landing strips; rock bolt systems; blasting; and other major earthwork projects such as for aggregate sources and landfills.\n\nWhile engineering geology is applicable principally to planning, design and construction activities, other specialties of geology are applied in a variety of geoprofessional specialty fields, such as mining geology, petroleum geology, and environmental geology. Note that mining geology and mining engineering are different geoprofessional fields.\n\nGeological engineering is a hybrid discipline that comprises elements of civil engineering, mining engineering, petroleum engineering, and earth sciences. Geological engineers often become licensed as both engineers and geologists. There are thirteen geological-engineering (or geoengineering) programs in the United States that are accredited by the Engineering Accreditation Commission (EAC) of ABET: (1) Colorado School of Mines, (2) Michigan Technological University, (3) Missouri University of Science and Technology, (4) Montana Tech of the University of Montana, (5) South Dakota School of Mines and Technology, (6) University of Alaska-Fairbanks, (7) University of Minnesota Twin Cities, (8) University of Mississippi, (9) University of Nevada, Reno (10) University of North Dakota, (11) University of Texas at Austin, (12) University of Utah, and (13) University of Wisconsin-Madison.\n\nOther schools offer programs or classes in geological engineering, including the University of Arizona.\n\nGeoengineering or geological engineering, engineering geology, and geotechnical engineering deal with the discovery, development, and production and use of subsurface earth resources, as well as the design and construction of earthworks. Geoengineering is the application of geosciences, where mechanics, mathematics, physics, chemistry, and geology are used to understand and shape our interaction with the earth.\n\nGeoengineers work in areas of\n\nProfessional geoscience organizations such as the American Rock Mechanics Association or the Geo-Institute and academic degrees such as the bachelor of geoengineering accredited by ABET acknowledge the broad scope of work practiced by geoengineers and stress fundamentals of science and engineering methods for the solution of complex problems. Geoengineers study the mechanics of rock, soil, and fluids to improve the sustainable use of earth’s finite resources, where problems appear with competing interests, for example, groundwater and waste isolation, offshore oil drilling and risk of spills, natural gas production and induced seismicity.\n\nGeophysics is the study of the physical properties of the earth using quantitative physical methods to determine what lies beneath the earth's surface. The physical properties of concern include the propagation of elastic waves (seismic), magnetism, gravity, electrical resistivity/conductivity, and electromagnetism. Geophysics has historically been most commonly used in oil exploration and mining, but its popularity in non-destructive investigative work has flourished since the early 1990s. It is also used in groundwater exploration and protection, geo-hazard studies (e.g., faults and landslides), alignment studies (e.g., proposed roadway, underground utilities, and pipelines), foundation studies, contamination characterization and remediation, landfill investigations, unexploded-ordnance investigations, vibration monitoring, dam-safety evaluation, location of underground storage tanks, identification of subsurface voids, and assisting in archeological investigations. (definition from Association of Environmental & Engineering Geologists)\n\nGeophysical engineering is the application of geophysics to the engineering design of facilities including roads, tunnels, and mines.\n\nEnvironmental science and environmental engineering are the geoprofessions commonly associated with the identification, remediation, and prevention of environmental contamination. These services range from phase-one and phase-two environmental site-assessments – research designed to assess the likelihood that a property is contaminated and subsurface exploration conducted to identify the nature and extent of contamination, respectively – up through the design of processes and systems to remediate contaminated sites for the protection of human health and the environment.\n\nEnvironmental geology is one of the principal geoprofessions engaged in assessing and remediating contaminated sites. Environmental geologists help identify the subsurface stratigraphy in which contaminants are located and through which they migrate. Environmental chemistry is the geoprofession that encompasses the study of chemical compounds in the soil. These compounds are categorized as pollutants or contaminants when introduced into the environment by human factors (e.g., waste, mining processes, radioactive release) and are not of natural origin. Environmental chemistry assesses interactions or these compounds with soil, rock, and water to determine their fate and transport, the techniques to measure the levels of contaminants in the environment, and technologies to destroy or reduce the toxicity of contaminants in wastes or compounds that have been released to the environment. Environmental engineering is often applied to assess contaminated sites, but more often is used in the design of systems to remediate contaminated soil and groundwater.\n\nHydrogeology is the geoprofession involved when environmental studies involve subsurface water. Hydrogeology applications range from securing safe, plentiful underground drinking-water sources to identifying the nature of groundwater contamination in order to facilitate remediation. Environmental toxicology is a geoprofession when used to identify the source, fate, transformation, effects, and risks of pollutants on the environment, including soil, water, and air. Wetlands science is a geoprofessional pursuit that incorporates several scientific disciplines, such as botany, biology, and limnology. It involves, among other activities, the delineation, conservation, restoration, and preservation of wetlands. These services are sometimes conducted by geoprofessional specialists called wetlands scientists. Ecology is a closely related environmental geoprofession involving studies into the distribution of organisms and biodiversity within an environmental context.\nNumerous geoprofessional disciplines contribute to the redevelopment of brownfields, sites (typically urban) that are underused or abandoned because they are or are assumed to be contaminated by hazardous materials. Geoprofessionals are engaged to evaluate the degree to which such sites are contaminated and the steps that can be taken to achieve the sites’ safe reuse. Environmental engineers and scientists work with developers to identify and design remediation strategies and exposure-barrier designs that protect future site users from unacceptable exposure to environmental contamination resulting from previous uses of the site. Because these previous uses often resulted in degraded soil conditions and the presence of abandoned, underground structures, geotechnical engineers often are needed to design special foundations for the new structures.\n\nConstruction-materials engineering and testing (CoMET) comprises an array of licensed-engineer-directed professional services applied principally for purposes of construction quality assurance and quality control. CoMET services commonly are provided as a separate discipline by firms that also practice geotechnical engineering, possibly among other geoprofessional disciplines. The geoprofessional-service industry has evolved in this manner because geotechnical engineering employs the observational method. Karl von Terzaghi and Ralph B. Peck – the creators of modern geotechnical engineering – used the observational method and multiple working hypotheses to expedite and economize the subsurface-exploration process, by using sampling and testing to form a judgment about subsurface conditions, and then observing excavated conditions and materials to confirm or modify those judgments and related recommendations, and then finalize them. To economize still further, geoprofessionals educated and trained paraprofessionals to represent them on site (hence the term “field representative”), especially to apply their judgment (much as a geotechnical engineer would) in comparing observed conditions with those the geotechnical engineer believed would exist. Over time, geotechnical engineers expanded their CoMET services by providing the additional education and training their field representatives needed to evaluate constructors’ attainment of conditions commonly specified by geoprofessionals; e.g., subsurface preparation for foundations of buildings, roadways, and other structures; materials used for subgrade, subbase, and base purposes; site grading; construction of earthen structures (earth dams, levees, reservoirs, landfills, et al.) and earth-retaining structures (e.g., retaining walls); and so on. Because many of the materials involved, such as concrete, are used in other elements of construction projects and structures, geoprofessional firms expanded their field representatives’ skill sets still more, to encompass observation and testing of numerous additional materials (e.g., reinforced concrete, structural steel, masonry, wood, and fireproofing), processes (e.g., cutting and filling and rebar placement), and outcomes (e.g., the effectiveness of welds). Laboratory services are a common element of many CoMET operations. Also operating under the direction of a licensed engineer, they are applied in geotechnical engineering to evaluate subsurface-material samples. In overall CoMET operations, laboratories operate with the equipment and personnel required to evaluate a variety of construction materials.\n\nCoMET services applied to evaluate the actual composition of a site’s subsurface are part of a complete geotechnical engineering service. For purposes of short-term economy, however, some owners select a firm not associated with the geotechnical engineer of record to provide these and all other CoMET services. This approach precludes the geotechnical engineer of record from providing a complete service. It also aggravates risk, because the individuals engaged to evaluate actual subsurface conditions are not “briefed” by the geotechnical engineer of record before they go to the project site and seldom communicate with the geotechnical engineer of record when they discern differences, in large part because the firm associated with the geotechnical engineer of record is regarded as a competitor of the firm employing the field representatives. In some cases, the field representatives in question lack the specific project background information and/or the education and training required to discern those differences.\n\nCoMET services applied to evaluate constructor’s attainment of specified conditions take the form of quality-assurance (QA) or quality-control (QC) services. QA services are performed directly or indirectly for the owner. The owner specifies the nature and extent of QA services that the owner believes is appropriate. Some owners specify none at all or only those that may be required by law. Those required by law are imposed via a jurisdiction’s building code. Almost all U.S. jurisdictions base their building codes on “model codes” developed by associations of building officials. The International Code Council (ICC) is the most prominent of these groups and its International Building Code (IBC) is the most commonly used model. As a result, many jurisdictions now require IBC “Special Inspection,” a term defined by the IBC as “the required examination of the materials, installation, fabrication, erection, or placement of components and connections requiring special expertise to ensure compliance with approved construction documents and referenced standards.” Special Inspection requirements vary from jurisdiction to jurisdiction based on the provisions adopted by the local building official. While some of the services involved may be similar to or the same as conventional CoMET services, Special Inspection is handled differently. Most commonly, the owner or the owner’s agent is required to retain a building-official-approved Special Inspection-services provider. Special Inspection is often required to obtain a certificate of occupancy.\n\nQC services are those applied by or on behalf of a constructor to ensure the constructor has attained conditions the constructor has contractually agreed to attain. Most CoMET consultants are engaged far more to provide QA services than QC services.\n\nMany CoMET procedures are specified in standards developed by standards-developing organizations (SDOs) such as the American Society of Civil Engineers (ASCE), ASTM International, and American Concrete Institute (ACI), using standards-development protocols approved by the American National Standards Institute (ANSI) and/or the International Organization for Standardization (ISO). All such standards identify what is minimally required to conform. Likewise, several organizations have developed programs to accredit CoMET field and laboratory services to perform certain types of testing and inspection. Some of these programs are more comprehensive than others; e.g., requiring regular calibration of equipment, participation in proficiency testing programs, and implementation and documentation of a (quality) management system to demonstrate technical competence. As with all such programs, of course, accreditation identifies what is least acceptable. Many CoMET laboratories go far beyond minimum requirements in an effort to attain higher levels of quality.\n\nA variety of organizations – including local building departments – have developed personnel-certification protocols and requirements. In many jurisdictions, only appropriately certified individuals are permitted to perform certain evaluations. Individuals typically are required to meet certain prerequisites for certification and must pass examinations, in some cases involving performance observation in the field. The prerequisite for higher degrees of certification often include a requirement that the individual has met requirements for a lower degree of certification (e.g., Soils Technician I is in some cases a prerequisite for Soils Technician II). It should be noted that field representatives are sometimes referred to as “soil testers,” “technicians,” “technicians/technologists,” or “engineering technicians.” The Geoprofessional Business Association developed the term “field representative” to encompass all the many types of paraprofessionals involved (e.g., those involved with specific types of materials, such as reinforced concrete, soil, or steel; those who observe or inspect processes or conditions, such as welding inspectors, caisson inspectors, and foundation inspectors), and especially to underscore their significant, mutual responsibility, that purpose titles such as “technician” fail to signify. In fact, the engineers who direct CoMET operations are personally and professionally responsible and liable for their field representatives’ acts and statements while representing the engineer on site.\n\nEspecially because CoMET consultants have more hands-on experience with construction activities than many other design-team members, many owners involve them (among other geoprofessionals) from the outset of a project, during the design phase, to help the owner and/or design team members develop technical specifications and establish testing and inspection requirements, instrumentation requirements and procedures, and observation programs. Geotechnical engineers employ CoMET services during the earliest stages of a project, to oversee subsurface sampling procedures, such as drilling.\n\nMany of the CoMET services performed for construction projects are performed for environmental projects as well, but requirements tend to be less rigid because they involve fewer licensing and related requirements. For example, individuals may perform federally mandated all-appropriate inquiries – typically a phase-one environmental site assessment – without a license of any kind.\n\nTo the extent that archeology and paleontology require systematic subsurface excavation to recover artifacts, they, too, are considered geoprofessions. Many geoprofessional-services firms offer these services to those of their clients that need to satisfy federal and/or state regulations that require paleontological and/or archeological inquiry before site development or redevelopment activities can proceed.\n\n\n"}
{"id": "14582412", "url": "https://en.wikipedia.org/wiki?curid=14582412", "title": "Gladstone–Dale relation", "text": "Gladstone–Dale relation\n\nThe Gladstone–Dale relation (J. H. Gladstone and T. P. Dale, 1864) is a mathematical relation used for optical analysis of liquids, the determination of composition from optical measurements. It can also be used to calculate the density of a liquid for use in fluid dynamics (e.g., flow visualization; Merzkirch 1987). The relation has also been used to calculate refractive index of glass and minerals in optical mineralogy (Mandarino 2007).\n\nIn the Gladstone–Dale relation, (n−1)/ρ = sum(km), the index of refraction (n) or the density (ρ in g/cm) of miscible liquids that are mixed in mass fraction (m) can be calculated from characteristic optical constants (the molar refractivity k in cm/g) of pure molecular end-members. For example, for any mass (m) of ethanol added to a mass of water, the alcohol content is determined by measuring density or index of refraction (Brix refractometer).\n\nMass (m) per unit volume (V) is the density m/V. Mass is conserved on mixing, but the volume of 1 cm of ethanol mixed with 1 cm of water is reduced to less than 2 cm due to the formation of ethanol-water bonds. The plot of volume or density versus molecular fraction of ethanol in water is a quadratic curve. However, the plot of index of refraction versus molecular fraction of ethanol in water is linear, and the weight fraction equals the fractional density (d; Teertstra 2005).\n\nThe Gladstone–Dale relation can be expressed as an equation of state by re-arranging the terms to (n−1)V = sum(kdm). The macroscopic values (n) and (V) determined on bulk material are now calculated as a sum of atomic or molecular properties. Each molecule has a characteristic mass (due to the atomic weights of the elements) and atomic or molecular volume that contributes to the bulk density, and a characteristic refractivity due to a characteristic electric structure that contributes to the net index of refraction.\n\nThe refractivity of a single molecule is the refractive volume k(MW)/An in nm, where MW is the molecular weight and An is Avogadro's number. To calculate the optical properties of materials using the polarizability or refractivity volumes in nm, the Gladstone–Dale relation competes with the Kramers–Kronig relation and Lorentz–Lorenz relation but differs in optical theory (Jaffe 1988).\n\nThe index of refraction (n) is calculated from the change of angle of a collimated monochromatic beam of light from vacuum into liquid using Snell's law for refraction. Using the theory of light as an electromagnetic wave (Iksander 1992), light takes a straight-line path through water at reduced speed (v) and wavelength (λ). The ratio v/λ is a constant equal to the frequency (ν) of the light, as is the quantized (photon) energy using Planck's constant and E = hν. Compared to the constant speed of light in a vacuum (c), the index of refraction of water is n = c/v.\n\nThe Gladstone–Dale term (n−1) is the non-linear optical path length or time delay. Using Isaac Newton's theory of light as a stream of particles refracted locally by (electric) forces acting between atoms, the optic path length is due to refraction at constant speed by displacement about each atom. For light passing through 1 m of water with n = 1.33, light traveled an extra 0.33 m compared to light that traveled 1 m in a straight line in vacuum. As the speed of light is a ratio (distance per unit time in m/s), light also took an extra 0.33 s to travel through water compared to light traveling 1 s in vacuum.\n\nThe Gladstone–Dale relation requires a particle model of light because the continuous wave-front required by wave theory cannot be maintained if light encounters atoms or molecules that maintain a local electric structure with a characteristic refractivity. Similarly, the wave theory cannot explain the photoelectric effect or absorption by individual atoms and one requires a local particle of light (see wave–particle duality).\n\nIn the 1900s, the Gladstone–Dale relation was applied to glass, synthetic crystals and minerals. Average values for the refractivity of oxides such as MgO or SiO give good to excellent agreement between the calculated and measured average indices of refraction of minerals (Mandarino 2007). However, specific values of refractivity are required to deal with different structure-types (Eggleton 1991), and the relation required modification to deal with structural polymorphs and the birefringence of anisotropic crystal structures.\n\nIn recent optical crystallography, Gladstone–Dale constants for the refractivity of ions were related to the inter-ionic distances and angles of the crystal structure. The ionic refractivity depends on 1/d, where d is the inter-ionic distance, indicating that a particle-like photon refracts locally due to the electrostatic Coulomb force between ions (Teertstra 2008a).\nA local model of light consistent with these electrostatic refraction calculations occurs if the electromagnetic energy is restricted to a finite region of space. An electric-charge monopole must occur perpendicular to dipole loops of magnetic flux, but if local mechanisms for propagation are required, a periodic oscillatory exchange of electromagnetic energy occurs with transient mass. In the same manner, a change of mass occurs as an electron binds to a proton. This local photon has zero rest mass and no net charge, but has wave properties with spin-1 symmetry on trace over time. In this modern version of Newton's corpuscular theory of light, the local photon acts as a probe of the molecular or crystal structure (Teertstra 2008b).\n\n\n"}
{"id": "55443695", "url": "https://en.wikipedia.org/wiki?curid=55443695", "title": "Google Clips", "text": "Google Clips\n\nGoogle Clips is a miniature clip-on camera device developed by Google. It was announced during a Google event on 4 October, 2017. It was released for sale on January 27, 2018.\n\nWith a flashing LED that indicates it's recording, Google Clips automatically captures video clips at moments its machine learning algorithms determined to be interesting or relevant.\n\nIt has a built-in 16 GB Storage and can record clips for up to 3 hours.\nThis Camera is priced at $249 in United States.\n\n\"The Independent\" wrote that Google Clips is \"an impressive little device, but one that also has the potential to feel very creepy.\"\n"}
{"id": "50838248", "url": "https://en.wikipedia.org/wiki?curid=50838248", "title": "Hugo Maisnik", "text": "Hugo Maisnik\n\nHugo Maisnik is a printer and inventor, best known as the man behind the Angelyne billboards, and as the inventor of Hugo's Amazing Tape.\n\nMaisnik is the father of Katherine Saltzberg, who wrote the one-woman play, Los Angelyne, about her experience growing up in the shadow of the Angelyne billboards.\n"}
{"id": "13275145", "url": "https://en.wikipedia.org/wiki?curid=13275145", "title": "International Association of People-Environment Studies", "text": "International Association of People-Environment Studies\n\nThe International Association of People-Environment Studies (IAPS), has been promoting the interdisciplinary exchange of ideas between planning and social scientists for 35 years – above all between spatial planning, architecture, psychology, and sociology. IAPS was officially founded in 1981, although its origins can be traced back to a series of successful conferences in several European countries from 1969 to 1979.\n\nThe objectives of IAPS are:\n\nPeople-environment studies, originating from environmental psychology (Lewin, Barker, Brunswik), have always tried to close the “mind gaps” between natural sciences, engineering, arts, and social sciences by an epistemological approach that encompasses denotations (objects and techniques) as well as connotations (subjective social, cultural meanings). \n\nBenefits of membership include:\n\nThe biannual conference is the main event organised under auspices of the association. In the past years, the following conferences have been organised:\n\n\nA database of all 4,400 abstracts from conferences since 1969, which permits a full-text search through the history of environmental psychology.\n\n"}
{"id": "35001481", "url": "https://en.wikipedia.org/wiki?curid=35001481", "title": "Laws of association", "text": "Laws of association\n\nThe principal laws of association are contiguity, repetition, attention, pleasure-pain, and similarity. The basic laws were formulated by Aristotle in approximately 300 B.C. and by John Locke in the seventeenth century. Both philosophers taught that the mind at birth is a blank slate and that all knowledge has to be acquired by learning. The laws they taught still make up the backbone of modern learning theory.\n\nDavid Hartley taught that \"contiguity\" is the main law of association, and, believing that it is the primary source, Hartley ignored David Hume’s law of resemblance (Warren, 1921).\n"}
{"id": "368910", "url": "https://en.wikipedia.org/wiki?curid=368910", "title": "Leaflet (botany)", "text": "Leaflet (botany)\n\nA leaflet (occasionally called foliole) in botany is a leaf-like part of a compound leaf. Though it resembles an entire leaf, a leaflet is not borne on a main plant stem or branch, as a leaf is, but rather on a petiole or a branch of the leaf. Compound leaves are common in many plant families and they differ widely in morphology. The two main classes of compound leaf morphology are palmate and pinnate.\nFor example, a \"hemp\" plant has palmate compound leaves, whereas some species of \"Acacia\" have pinnate leaves.\n\nThe ultimate free division (or leaflet) of a compound leaf, or a pinnate subdivision of a multipinnate leaf is called a pinnule or pinnula.\n\n"}
{"id": "12924979", "url": "https://en.wikipedia.org/wiki?curid=12924979", "title": "List of Fellows of the Australian Academy of Science", "text": "List of Fellows of the Australian Academy of Science\n\nThe Fellowship of the Australian Academy of Science is made up of about 500 Australian scientists.\n\nScientists judged by their peers to have made an exceptional contribution to knowledge in their field may be elected to Fellowship of the Academy. Fellows are often denoted using the post-nominal FAA (Fellow of the Australian Academy of Science).\n\nA small number of distinguished foreign scientists with substantial connections to Australian science are elected as Corresponding Members.\n\nFellows are appointed for life; this table also contains deceased fellows.\n\n\n"}
{"id": "843347", "url": "https://en.wikipedia.org/wiki?curid=843347", "title": "List of alternative country names", "text": "List of alternative country names\n\nMost sovereign states have alternative names. Some countries have also undergone name changes for political or other reasons. This article attempts to give all known alternative names and initialisms for all nations, countries, and sovereign states, in English and any predominant or official languages of the country in question.\n\nCountries are listed alphabetically by their \"description\", the most common name or term that is politically neutral and unambiguous. This may be followed by a note as to the status of the description used.\n\n"}
{"id": "34269733", "url": "https://en.wikipedia.org/wiki?curid=34269733", "title": "List of coronal mass ejections", "text": "List of coronal mass ejections\n\nThe following contains a list of coronal mass ejections. A coronal mass ejection (CME) is a massive burst of solar wind and magnetic fields rising above the solar corona or being released into space. Most ejections originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares.\n\n"}
{"id": "29800948", "url": "https://en.wikipedia.org/wiki?curid=29800948", "title": "List of digital forensics tools", "text": "List of digital forensics tools\n\nDuring the 1980s, most digital forensic investigations consisted of \"live analysis\", examining digital media directly using non-specialist tools. In the 1990s, several freeware and other proprietary tools (both hardware and software) were created to allow investigations to take place without modifying media. This first set of tools mainly focused on computer forensics, although in recent years similar tools have evolved for the field of mobile device forensics. This list includes notable examples of digital forensic tools.\n\nMemory forensics tools are used to acquire and/or analyze a computer's volatile memory (RAM). They are often used in incident response situations to preserve evidence in memory that would be lost when a system is shutdown, and to quickly detect stealthy malware by directly examining the operating system and other running software in memory.\n\nMobile forensics tools tend to consist of both a hardware and software component. Mobile phones come with a diverse range of connectors, the hardware devices support a number of different cables and perform the same role as a write blocker in computer devices.\n\nSoftware forensics is the science of analyzing software source code or binary code to determine whether intellectual property infringement or theft occurred. It is the centerpiece of lawsuits, trials, and settlements when companies are in dispute over issues involving software patents, copyrights, and trade secrets. Software forensics tools can compare code to determine correlation, a measure that can be used to guide a software forensics expert.\n"}
{"id": "17964555", "url": "https://en.wikipedia.org/wiki?curid=17964555", "title": "List of ecoregions in the United States (EPA)", "text": "List of ecoregions in the United States (EPA)\n\nThis list of ecoregions in the United States provides an overview of United States ecoregions designated by the U.S. Environmental Protection Agency (EPA) and the Commission for Environmental Cooperation (CEC). The CEC was established in 1994 by the member states of Canada, Mexico, and the United States to address regional environmental concerns under the North American Agreement on Environmental Cooperation (NAAEC), the environmental side accord to the North American Free Trade Agreement (NAFTA). The Commission's 1997 report, \"Ecological Regions of North America\", provides a framework that may be used by government agencies, non-governmental organizations, and academic researchers as a basis for risk analysis, resource management, and environmental study of the continent's ecosystems. In the United States, the EPA and the United States Geological Survey (USGS) are the principal federal agencies working with the CEC to define and map ecoregions. Ecoregions may be identified by similarities in geology, physiography, vegetation, climate, soils, land use, wildlife distributions, and hydrology.\n\nThe classification system has four levels, but only Levels I and III are shown on this list. Level I divides North America into 15 broad ecoregions; of these, 12 lie partly or wholly within the United States. Fifty Level II regions were created to allow for a narrower delineation of Level I areas. Three level I areas were not subdivided for level 2. Level III subdivides the continent into 182 smaller ecoregions; of these, 104 lie partly or wholly with the United States. Level IV is a further subdivision of Level III ecoregions. Level IV mapping is still underway but is complete across most of the United States. For an example of Level IV data, see List of ecoregions in Oregon and the associated articles. The classification system excludes the U.S. state of Hawaii, which is not part of the North American mainland.\n\n\nThe corresponding CEC ecoregion in Canada is called the Pacific Maritime Ecozone.\n\n\nThe corresponding CEC ecoregion in Canada is called the Montane Cordillera Ecozone.\n\n\n\nThe corresponding name in Canada for the same ecoregion is the Prairies Ecozone.\n\nThese forests stretch from the Southern Appalachians towards Canada, up to the northern Midwest. For a general description of these forests, refer to Temperate Deciduous Forest. The standard reference is \"The Deciduous Forest of Eastern North America\". The adjoining forests in Canada are generally referred to as the Mixedwood Plains Ecozone or the Great Lakes-St.Lawrence Forest Region.\n\n\nThe corresponding name in Canada for the same ecoregions are the Boreal Shield and the Atlantic Maritime Ecozones.\n\n\n\n\n\n\n\n"}
{"id": "35806777", "url": "https://en.wikipedia.org/wiki?curid=35806777", "title": "List of fault zones", "text": "List of fault zones\n\nThis list covers all faults and fault-systems that are either geologically important or connected to prominent seismic activity. It is not intended to list every notable fault, but only major fault zones.\n\n"}
{"id": "14485797", "url": "https://en.wikipedia.org/wiki?curid=14485797", "title": "List of members of the National Academy of Sciences (Geology)", "text": "List of members of the National Academy of Sciences (Geology)\n"}
{"id": "35283246", "url": "https://en.wikipedia.org/wiki?curid=35283246", "title": "List of mental health occupations", "text": "List of mental health occupations\n\nThe following is a list of mental health occupations.\n\n"}
{"id": "36850123", "url": "https://en.wikipedia.org/wiki?curid=36850123", "title": "List of social gaming networks", "text": "List of social gaming networks\n\nThis is a list of major Social gaming networks.\n\nThe list is not exhaustive and is limited to notable, well-known services.\n"}
{"id": "18789405", "url": "https://en.wikipedia.org/wiki?curid=18789405", "title": "MyOcean", "text": "MyOcean\n\nMyOcean is a series of projects granted by the European Commission within the GMES Program (Seventh Framework Program), whose objective is to define and to set up a concerted and integrated pan-European capacity for ocean monitoring and forecasting. The activities benefit several specified areas of use: Maritime security, oil spill prevention, marine resources management, climate change, seasonal forecasting, coastal activities, ice sheet surveys, water quality and pollution. The series of MyOcean projects ended in 2015, and their services are now continued by the Copernicus Programme.\n\nMyOcean's objective is to set up (definition, design, development and validation) an integrated pan-European capability for ocean monitoring and forecasting, using nationally available skills and resources.\n\nAlthough the budgetary frame of the project is labelled \"Research\", the priority is not to conduct further scientific research in the field of operational oceanography, even if this aspect is also taken into account. It is more a question of developing a System of Systems (in the industrial sense), according to the European quality standards, and to achieve operational qualification and eventually qualification of Service.\n\nThe first MyOcean project started on 1 January 2009, scheduled to last for 39 months. It is a follow-up to the MERSEA project (FP6: system implementation phase: 2004-2008), and also integrates some service lines developed as part of ESA's GSE (GMES Service Elements) projects, particularly Marcoast and PolarView.\n\nThe follow up project MyOcean2 runs from April 2012 to September 2014 to ensure a controlled continuation and extension of the services and systems already implemented.\n\nMyOcean is a consortium of 60 partners in 28 countries (the 22 states of the EU-27 that have a sea coastline, plus Norway, Russia, Ukraine, Morocco, Israel and Canada). \nTwo European bodies (JRC and ECMWF) are also partners of 'MyOcean'. The (European Environment Agency) and the (European Maritime Safety Agency) are represented on the Board.\n\nThe total budget was €55M, of which €33.8M comes from a European Commission subsidy (representing 61% of the total budget), over 36 months.\nThis budget essentially corresponded to staff costs (83.5%), with the second largest item being mission expenses (7.3% of the total budget). Management costs (and external communication costs) represented 4.1% of the total budget. Equipment fees represented 3%.\n\nThe total workload corresponded to the equivalent of 190 full-time employees, but in practical terms, there were more than 350 people involved in the project.\n\nThe total budget og MyOcean2 was €41M.\n\nThe project is coordinated between the executive level (the Executive Committee) and the strategic level (the Board). The role of the 'Governing Body' is to take high-level decisions that affect the project at a strategic level, the major themes, the budget or the make-up of the consortium.\n\nOn the 'Board' the following are represented:\n- Senior experts representing the main parties (INGV, Met Office, NERSC, DMI, Ifremer, CLS and Mercator Ocean)\n- The Chairmen of the Advisory Bodies (Core User Group and Scientific Advisory Committee)\n- Representatives of the European Stakeholders (EEA, EMSA, etc.)\n\nThe Board and the Governing Body are chaired by Pierre Bahurel.\n\nTheir role is to collect the measurements or observations, whether satellite or in situ, and to calibrate, validate, edit, archive and distribute them. There are 5 TACs (WP Leader shown in brackets):\nTAC involved into MyOcean project are organized into 7 regions, each region is linked to the corresponding area defined into SeaDataNet project in order to improve efforts on data management and exchange of datasets.\n\nThey correspond to the 6 European 'basins', plus the Global Ocean. By assimilating observation data in 3D Models, they are to predict the state of the ocean (or to say what the state of the ocean was between two observations). There are 7 of them:\n\nAfter the MyOcean project (2009–2012), the second Milestone \"MyOcean2\" (2012–2014) came to an end on September 30 and let the floor to MyOcean follow-On, from October 1, 2014, until March 31, 2015. On 11 November 2014, the European Commission signed an agreement with the French privately owned non-profit company Mercator Ocean, that entrusts the latter with the setting-up of the future Copernicus Marine Service as of April 2015. The services provided of the former MyOcean site \"myocean.eu\" are now hosted by marine.copernicus.eu.\n\n"}
{"id": "2194126", "url": "https://en.wikipedia.org/wiki?curid=2194126", "title": "Narrative inquiry", "text": "Narrative inquiry\n\nNarrative inquiry or narrative analysis emerged as a discipline from within the broader field of qualitative research in the early 20th century. Narrative inquiry uses field texts, such as stories, autobiography, journals, field notes, letters, conversations, interviews, family stories, photos (and other artifacts), and life experience, as the units of analysis to research and understand the way people create meaning in their lives as narratives.\n\nNarrative inquiry has been employed as a tool for analysis in the fields of cognitive science, organizational studies, knowledge theory, sociology, occupational science and education studies, among others. Other approaches include the development of quantitative methods and tools based on the large volume capture of fragmented anecdotal material, and that which is self signified or indexed at the point of capture. Narrative Inquiry challenges the philosophy behind quantitative/grounded data-gathering and questions the idea of “objective” data, however, it has been criticized for not being “theoretical enough.\"\n\nNarrative inquiry is a form of qualitative research, that emerged in the field of management science and later also developed in the field of knowledge management, which shares the sphere of Information Management. Thus Narrative Inquiry focuses on the organization of human knowledge more than merely the collection and processing of data. It also implies that knowledge itself is considered valuable and noteworthy even when known by only one person.\n\nKnowledge management was coined as a discipline in the early 1980s as a method of identifying, representing, sharing, and communicating knowledge. Knowledge management and Narrative Inquiry share the idea of Knowledge transfer, a theory which seeks to transfer unquantifiable elements of knowledge, including experience. Knowledge, if not communicated, becomes arguably useless, literally unused.\n\nPhilosopher Andy Clark speculates that the ways in which minds deal with narrative (second-hand information) and memory (first-hand perception) are cognitively indistinguishable. Narrative, then, becomes an effective and powerful method of transferring knowledge.\n\nNarrative is a powerful tool in the transfer, or sharing, of knowledge, one that is bound to cognitive issues of memory, constructed memory, and perceived memory. Jerome Bruner discusses this issue in his 1990 book, \"Acts of Meaning\", where he considers the narrative form as a non-neutral rhetorical account that aims at “illocutionary intentions,” or the desire to communicate meaning. This technique might be called “narrative” or defined as a particular branch of storytelling within the narrative method. Bruner’s approach places the narrative in time, to “assume an experience of time” rather than just making reference to historical time.\n\nThis narrative approach captures the emotion of the moment described, rendering the event active rather than passive, infused with the latent meaning being communicated by the teller. Two concepts are thus tied to narrative storytelling: memory and notions of time, both as time as found in the past and time as re-lived in the present.\n\nA narrative method accepts the idea that knowledge can be held in stories that can be relayed, stored, and retrieved.\n\n1. Develop a research question\n2. Select or produce raw data \n\n3. Organize data\n\n\n4. Interpret data\n\n\n\nThe idea of imagination is where narrative inquiry and storytelling converge within narrative methodologies. Within narrative inquiry, storytelling seeks to better understand the “why” behind human action. Story collecting as a form of narrative inquiry allows the research participants to put the data into their own words and reveal the latent “why” behind their assertions.\n\n“Interpretive research” is a form of field research methodology that also searches for the subjective \"why.\" Interpretive research, using methods such as those termed “storytelling” or “narrative inquiry,” does not attempt to predefine independent variables and dependent variables, but acknowledges context and seeks to “understand phenomena through the meanings that people assign to them.”\n\nTwo influential proponents of a narrative research model are Mark Johnson and Alasdair MacIntyre. In his work on experiential, embodied metaphors, Johnson encourages the researcher to challenge “how you see knowledge as embodied, embedded in a culture based on narrative unity,” the “construct of continuity in individual lives.”\n\nThe seven “functions of narrative work” as outlined by Riessman\n1. Narrative constitutes past experiences as it provides ways for individuals to make sense of the past. \n2. Narrators argue with stories.\n3. Persuading. Using rhetorical skill to position a statement to make it persuasive/to tell it how it “really” happened. To give it authenticity or ‘truth’.\n4. Engagement, keeping the audience in the dynamic relationship with the narrator. \n5. Entertainment.\n6. Stories can function to mislead an audience.\n7. Stories can mobilize others into action for progressive change.\n\nNarrative analysis therefore can be used to acquire a deeper understanding of the ways in which a few individuals organize and derive meaning from events \n. It can be particularly useful for studying the impact of social structures on an individual and how that relates to identity, intimate relationships, and family. For example:\n\n\n\n"}
{"id": "21316635", "url": "https://en.wikipedia.org/wiki?curid=21316635", "title": "NeuroLex", "text": "NeuroLex\n\nNeuroLex is a dynamic lexicon of neuroscience concepts. It is a structured as a semantic wiki, using Semantic MediaWiki. NeuroLex is supported by the Neuroscience Information Framework project.\n\nThe NeuroLex is intended to help improve the way that neuroscientists communicate about their data, so that information systems like the NIF can find data more easily and provide more powerful means of integrating data that occur across distributed resources. One of the big roadblocks to data integration in neuroscience is the inconsistent use of terminology in databases and other resources like the literature. When one uses the same terms to mean different things, one cannot easily ask questions that span across multiple resources. For example, if three databases have information about what genes are expressed in cortex, but they all use different definitions of cerebral cortex, then one cannot compare them easily.\n\nThe NIF enables discovery and access to public research data and tools worldwide through an open source, networked environment. Funded by the NIH Blueprint for Neuroscience Research, the NIF enables scientists and students to discover global neuroscience web resources that cut across traditional boundaries – from experimental, clinical and translational neuroscience databases, to knowledge bases, atlases, and genetic/genomic resources.\n\nUnlike general search engines, NIF provides deeper access to a more focused set of resources that are relevant to neuroscience, search strategies tailored to neuroscience, and access to content that is traditionally “hidden” from web search engines. The Framework is a dynamic inventory of neuroscience databases, annotated and integrated with a unified system of biomedical terminology (i.e. NeuroLex). NIF supports concept-based queries across multiple scales of biological structure and multiple levels of biological function, making it easier to search for and understand the results.\n\nAs part of the NIF, a search interface to many different sources of neuroscience information and data is provided. To make this search more effective, the NIF is constructing ontologies to help organize neuroscience concepts into category hierarchies, e.g. stating that a neuron is a cell. This allows users to perform more effective searches and also to organize and understand the information that is returned. But an important adjunct to this activity is to clearly define all of the terms that are used to describe data, e.g., anatomical terms, techniques, organism names.\n\nThe initial entries in the NeuroLex were built from the NIFSTD ontologies which subsumed an earlier vocabulary BIRNLex. It currently contains concepts that span gross anatomy, cells of the nervous system, subcellular structures, diseases, functions and techniques. NIF is soliciting community input to add more content and correct what is there.\n\n\nNIF was featured in a special issue of \"Neuroinformatics\", published in September 2008:\n\n"}
{"id": "3541802", "url": "https://en.wikipedia.org/wiki?curid=3541802", "title": "Pilot experiment", "text": "Pilot experiment\n\nA pilot study, pilot project, pilot test, or pilot experiment is a small scale preliminary study conducted in order to evaluate feasibility, time, cost, adverse events, and improve upon the study design prior to performance of a full-scale research project. Pilot studies, therefore, may not be appropriate for case studies.\n\nPilot experiments are frequently carried out before large-scale quantitative research, in an attempt to avoid time and money being wasted on an inadequately designed project. A pilot study is usually carried out on members of the relevant population, but not on those who will form part of the final sample.\n\nA pilot study is often used to test the design of the full-scale experiment which then can be adjusted. It is a potentially valuable insight and, should anything be missing in the pilot study, it can be added to the full-scale (and more expensive) experiment to improve the chances of a clear outcome.\n\nIn sociology, pilot studies can be referred to as small-scale studies that will help identify design issues before the main research is done.\n\nAlthough pilot experiments have a well-established tradition in public action, their usefulness as a strategy for change has been questioned, at least in the domain of environmental management. It is argued that extrapolation from a pilot study to large scale environmental strategy cannot be assumed to be possible, partly due to the exceptional resources and favourable conditions that often accompany a pilot study.\n\n\n"}
{"id": "35300495", "url": "https://en.wikipedia.org/wiki?curid=35300495", "title": "Quoats", "text": "Quoats\n\nQuoats is a LINK research project funded by government, levy boards and industry groups that aims to develop and apply state-of-the-art genomic and metabolomic tools for the genetic improvement of oats. Its name is a portmanteau of the words \"quality\" and \"oats\". Its focus is on the understanding and manipulation of key traits that will enhance the value of oats in human health improvement, to capitalise on the value of oats as a low input cereal crop, increase the environmental and economic sustainability of cereal based rotations, realise the potential of oats as a high value animal feed and develop new opportunities for using oats for industrial use through advanced fractionation. The project objective is to deliver powerful enabling genetic technologies for the identification of specific genes and markers that will drive the development of breeder–friendly tools accelerating the production of improved oat varieties that will be evaluated and marketed by industrial partners. Quoats is a multi-disciplinary research programme which combines modern phenotyping methodologies with the expertise of genomics researchers, oat breeders and end-users. It also addresses long term breeding goals by developing experimental oat populations which are polymorphic for agronomically important traits but more amenable to mapping and forward genetic approaches than conventional agronomic lines.\n\n"}
{"id": "1148992", "url": "https://en.wikipedia.org/wiki?curid=1148992", "title": "Range fractionation", "text": "Range fractionation\n\nRange fractionation is a term used in biology used to denote varying firing thresholds for different stimuli intensities. Sense organs are usually composed of many sensory receptors measuring the same property. These sensory receptors show a limited degree of precision due to an upper limit in firing rate. If the receptors are endowed with distinct transfer functions in such a way that the points of highest sensitivity are scattered along the axis of the quality being measured, the precision of the sense organ as a whole can be increased. This was shown for the chordotonal organ in the locust leg.\n"}
{"id": "21886367", "url": "https://en.wikipedia.org/wiki?curid=21886367", "title": "Rocket Science (TV series)", "text": "Rocket Science (TV series)\n\nRocket Science is a BBC television documentary series, first broadcast in March 2009 on BBC Two, exploring new ways to teach science to children. Across the UK, fewer and fewer youngsters want to study chemistry and physics, so with the help of physics teacher Andy Smith, Rocket Science sets out to convert a small sample by teaching them everything safe there is to know about fireworks. The series was filmed over a period of nine months.\n\nAndy Smith won the secondary school teacher of the year for the north west region in 2005. He has also filmed shows for the digital channel Teacher's TV. He is a big Doctor Who fan and has assisted in many conventions in which previous stars of the series attend, notably the Who in the Cavern, which raises money for the Liverpool children's hospital Alder Hey. He once famously presided over a quiz between fifth doctor Peter Davison and sixth doctor Colin Baker.\n\n"}
{"id": "1671667", "url": "https://en.wikipedia.org/wiki?curid=1671667", "title": "Rough Science", "text": "Rough Science\n\nRough Science is a British documentary reality television series made by the BBC in collaboration with the Open University. Six series were made between 2000 and 2005. It was broadcast in prime time on BBC Two and is considered something of a \"break-out hit\" for the Open University. \"Rough Science\" was first shown in 2000.\n\nThe series' formula consists of a group of four or five scientists with specialities in different fields who are given a task that they must complete using the natural resources of the surrounding area together with a small set of supplies. Each programme features a different task and follows the scientists as they use their knowledge and ingenuity in attempting to fulfil it and, in the process, educate the viewing public — despite failure being common.\n\nEach episode either requires the team to work together in smaller groups to create requirements for the overall challenge, for the small groups to fulfil vaguely related challenges or even to complete the same task in competition. It has been set in a different scenic location each series, typically somewhere with plenty of plants for use by the group. The presenter is Kate Humble and groups of scientists — most of the scientists appear in several series.\n\nThis series is set on the Mediterranean island of Capraia and features four episodes, and various tasks:\n\n\nThe scientists featured are:\n\n\nThis series takes place on the Caribbean island of Carriacou and features five scientists, including three (Mike Leahy, Mike Bullivant, and Jonathan Hare) from the first series. It has six episodes and many challenges:\n\n\nNew scientists featured in this series were:\n\nAnna Lewington and Vanessa Griffiths from the first series were not featured in this series.\n\nThis series takes place on the West Coast of the South Island of New Zealand, has six episodes, and features the same scientists as the previous series.\n\n\nThis series takes place at Darwin Mine, adjacent to Death Valley National Park in California and has six episodes, all involving space exploration. Four scientists from the previous series (Ellen McCallie, Jonathan Hare, Mike Bullivant, and Kathy Sykes) are featured plus a new scientist, Iain Stewart, the show's first geologist. Mike Leahy from the previous series is not featured in this series.\n\n\nThis series takes place on the coast of Zanzibar and has a strong ecological theme running through the challenges. The line-up of scientists is Ellen McCallie, Jonathan Hare, Mike Bullivant, and Kathy Sykes. The episode challenges for this series are:\n\n\nThis series, shown in 2005, takes place in the San Juan Mountains in Colorado. For this series, geomorphologist Hermione Cockburn replaced Kathy Sykes.\n\n\n\n\n"}
{"id": "57641154", "url": "https://en.wikipedia.org/wiki?curid=57641154", "title": "S-55 (satellite)", "text": "S-55 (satellite)\n\nS-55 was an American satellite launched by NASA on 30 June 1961 as part of the Explorers program. Explorer (S-55), also known as Meteoroid Sat A, was launched using a Scout 5 rocket from the Wallops Flight Facility. Its mission was to evaluate the launch vehicle, and investigate micrometeoroid impact and penetration. The mission failed because the third stage failed to ignite and the spacecraft did not achieve orbit.\n"}
{"id": "45579735", "url": "https://en.wikipedia.org/wiki?curid=45579735", "title": "Science and technology", "text": "Science and technology\n\nScience and technology is a topic that encompasses science, technology, and the interactions between the two. Science is a systematic enterprise that builds and organizes knowledge in the form of explanations and predictions about nature and the universe. Technology is the collection of techniques, methods or processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation, or any other consumer demands.\n\nScience may drive technological development, by generating demand for new instruments to address a scientific question, or by illustrating technical possibilities previously unconsidered. In turn, technology may drive scientific investigation, by creating demand for technological improvements that can only be produced through research, and by raising questions about the underlying principles that a new technology relies on.\n\nFor the majority of human history, technological improvements were achieved by chance, trial and error, or spontaneous inspiration. When the modern scientific enterprise matured in the Enlightenment, it primarily concerned itself with basic questions of nature. Research and development directed towards immediate technical application is a relatively recent occurrence, arising with the Industrial Revolution and becoming commonplace in the 20th century.\n\nAs academic fields, science and technology are often grouped with engineering and mathematics, as the STEM fields.\n\nThe history of science and technology examines how humanity's understanding of science and technology has changed over the centuries.\n\nScience and technology are often studied together with society, in terms of their mutual interactions.\n"}
{"id": "25792968", "url": "https://en.wikipedia.org/wiki?curid=25792968", "title": "Sediment trap (geology)", "text": "Sediment trap (geology)\n\nIn geology, a sediment trap is any topographic depression where sediments substantially accumulate over time. The size of a sediment trap can vary from a small lagoon to a large basin such as the Persian Gulf.\n"}
{"id": "34926734", "url": "https://en.wikipedia.org/wiki?curid=34926734", "title": "Seed library", "text": "Seed library\n\nA seed library is an institution that lends or shares seed. It is distinguished from a seedbank in that the main purpose is not to store or hold germplasm or seeds against possible destruction, but to disseminate them to the public which preserves the shared plant varieties through propagation and further sharing of seed.\n\nSeed libraries usually maintain their collections through donations from members. but may also operate as pure charity operations intent on serving gardeners and farmers. A common attribute of many seed libraries is to preserve agricultural biodiversity by focusing on rare, local, and heirloom seed varieties.\n\nSeed libraries use varied methods for sharing seeds, primarily by:\n\nSeed libraries may function as programs of public libraries, such as the programs of the Richmond Public Library in California (the \"Richmond Grows\" program is the \"unofficial spiritual center of the [public library seed library] movement\") and the New Port Richey Public Library (Florida). Seed library initiatives in public libraries garner patron participation as a novelty supplement to book check-outs. Seed packets are usually located next to everyday circulated items like books, audiobooks, CDs, and DVDs. Seed libraries in public libraries have been successful because they catch patron hobby curiosities. Public libraries are an appropriate space for seed libraries because they make seeds and plants available to everyone.\n\nThey are also located in college libraries, such as Hampshire College's seed library; museums, such as the Hull-House Heirloom Seed Library, a program of the Jane Addams Hull-House Museum. or as membership based online programs like the Hudson Valley Seed Library. Some have developed as programs of botanical gardens, such as that of the VanDusen Botanical Garden, or from gardening associations and research institutes, such as the Heritage Seed Library of Garden Organic. Other seed libraries have evolved from community sustainability or resilience efforts, such as the Bay Area Seed Interchange Library (BASIL) (the United States' oldest seed library, which developed from the Berkeley, California Ecology Center); and still others from the Slow Food movement, such as Grow Gainesville's seed program.\n\nWhile \"lending\" is straightforward, \"returning\" or re-depositing seeds presents a challenge, since the new seeds are not necessarily well-described, and may be inadvertent hybrids.\n\nSeed libraries complement the preservationist activities of seedbanks, by collecting local and heirloom varieties that might otherwise be lost, and by collecting new local varieties. In theory, lending and returning seed libraries will also promote local agriculture over time, by growing collections of seeds locally adapted to the region.\n\n\n\n"}
{"id": "4385708", "url": "https://en.wikipedia.org/wiki?curid=4385708", "title": "Setting circles", "text": "Setting circles\n\nSetting circles are used on telescopes equipped with an equatorial mount to find astronomical objects in the sky by their equatorial coordinates often used in star charts or ephemerides.\n\nSetting circles consist of two graduated disks attached to the axes – right ascension (RA) and declination (DEC) – of an equatorial mount. The RA disk is graduated into hours, minutes, and seconds. The DEC disk is graduated into degrees, arcminutes, and arcseconds.\n\nSince the RA coordinates are fixed onto the celestial sphere, the RA disk is usually driven by a clock mechanism in sync with sidereal time. Locating an object on the celestial sphere using setting circles is similar to finding a location on a terrestrial map using latitude and longitude. Sometimes the RA setting circle has two scales on it: one for the Northern Hemisphere and one for the Southern.\n\nHistorically setting circles have rivaled the telescopes optics as far as difficulty in construction. Making a set of setting circles required a lot of precision crafting on a dividing engine. Setting circles usually had a large diameter and when combined with a vernier scale could point a telescope to nearly an arc minute of accuracy. In the 20th century setting circles were replaced with electronic encoders on most research telescopes.\n\nIn amateur astronomy, setting up a portable telescope equipped with setting circles requires:\n\n\nAccuracy of pointing the telescope can be hard to achieve. Some sources of error are:\n\n\nIt is common to blame an unlevel tripod as a source of error, however when a proper polar alignment is performed, any induced error is factored out. \n\nThese sources of error add up and cause the telescope to point far from the desired object. They are also hard to control; for example, Polaris is often used as the celestial north pole for alignment purposes, but it is over half a degree away from the true pole. Also, even the finest graduations on setting circles are usually more than a degree apart, which makes them difficult to read accurately, especially in the dark. Nothing can be done if the optical tube is not perpendicular to the declination axis or if the R.A. and Dec axes are not perpendicular, because these problems are next to impossible to fix.\n\nIn the southern hemisphere the Right Ascension scale operates in reverse from in the Northern Hemisphere. The term Right Ascension took its name from early northern hemisphere observers for whom \"ascending stars\" were on the east or right hand side. In the southern hemisphere the east is on the left when an equatorial mount is aligned on the south pole. Many Right Ascension setting circles therefore carry two sets of numbers, one showing the value if the telescope is aligned in the northern hemisphere, the other for the southern. \n\nEven with some inaccuracies in polar alignment or the perpendicularity of the mount, setting circles can be used to roughly get to a desired object's coordinates, where a star chart can be used to apply the necessary correction. Alternatively, it is possible to point to a bright star very close to the object, rotate the circles to match the star's coordinates, and \"then\" point to the desired object's coordinates. Setting circles are also used in a modified version of star hopping where the observer points the telescope at a known object and then moves it a set distance in RA or declination to the location of a desired object.\n\nDigital setting circles (DSC) consist of two rotary encoders on both axis of the telescope mount and a digital readout. They give a highly accurate readout of where the telescope is pointed and their lit display makes them easier to read in the dark. They have also been combined with microcomputers to give the observer a large database of celestial objects and even guide the observer in correctly pointing their telescope.\n\nIn contrast to a GOTO telescope mount, a mount equipped with DSC alone is sometimes called a \"PUSH TO\" mount.\n\n\n"}
{"id": "54809900", "url": "https://en.wikipedia.org/wiki?curid=54809900", "title": "Sivasubramanian Srikantan", "text": "Sivasubramanian Srikantan\n\nDr. Sivasubramanian Srikantan (Popularly known as Dr.S.Srikantan) (6 September 1933 - 19 August 2006) was an Indian scientist, research lead, former Managing Director – Karnataka State Electronics Development Corporation Ltd (KEONICS), Chairman – Yokogawa Keonics Ltd, Director – Krone Communications Ltd, Managing Director – Andhra Pradesh Electronics Development Corporation Ltd (APEDC), Head, Computer Group – Electronics Corporation of India Ltd (ECIL), Group Leader – Babha Atomic Research Centre (BARC), and Chairman – ASM Technologies Ltd. (1992-2006)\n\nDr. Sivasubramanian Srikantan was born in the village of Kaveripattinam (Dharmapuri), around 100 Km away from Bangalore. Dr. Srikantan completed his schooling at the local government school in Kaveripattinam and did his Pre-University at St. Joseph's College, Bangalore. He then joined the College of Engineering, Guindy at Anna University, Chennai, where he completed his Electrical Engineering in 1954. After successfully becoming an Electrical Engineer, Dr. S. Srikantan underwent training at the Atomic Energy Establishment training school in Mumbai which was headed by Homi J. Bhabha. With the support from Government of India and encouragement from Dr. Homi J Bhabha, he was later sent to the University of Pennsylvania in Philadelphia, USA where he completed his Master's in 1962 and Ph.D. in 1964 from the Moore School of Electrical Engineering.\n\nSon of T.Sivasubramanyam, was married to Nirmala Srikantan. The couple had two children, son Rabindra Srikantan – a tech entrepreneur and daughter Geetha Srikantan – a technology expert. Rabindra has two sons Nikhil Rabindra and Akhil Rabindra.\n\nDr. Srikantan, returned to India after his Ph.D. and led a team of fresh graduate engineers and scientists to work on TDC-12 (Trombay Digital Computer – 12) ‘the first Indian-built electronic digital computer at Bhabha Atomic Research Centre on January 21, 1969\n\nDr. Srikantan started the computer division of Electronics Corporation of India Limited (ECIL) and was responsible for introducing a range of digital computers, to name the most significant of them TDC-16 and TDC-332 He laid the foundation for ECIL, a commercial venture which started as a R&D activity at Bhabha Atomic Research Centre (BARC)\n\n\nDr. S.Srikantan has received the VIKRAM SHARABHAI RESEARCH AWARD endowed by Hari Om Ashram in the year 1974 in the field of Electronics and Telecommunications.\n\nASM Technologies Limited under its CSR initiative along with The Institution of Electronics & Telecommunication Engineers (IETE) Bangalore Centre has been organizing ‘Summer School in Electronics & Computers’ (SUSIEC) for 9th and 10th standard students during their summer vacations. The students compete in the science projects and are awarded Dr. S Srikantan Memorial Award for the best projects\n\n"}
{"id": "55313286", "url": "https://en.wikipedia.org/wiki?curid=55313286", "title": "Sociological Francoism", "text": "Sociological Francoism\n\nSociological Francoism () is an expression used in Spain which attests to the social characteristics typical of Francoism that survived in Spanish society after the death of Francisco Franco in 1975 and continue to the present day.\n\nThe root causes of sociological Francoism are found in the prolonged state of repression that existed during the forty years of the Franco dictatorship (1936-1975), and the fear of a repetition of the Spanish Civil War and a clashing of the so-called two Spains. A further reason for its durability is the positive role attributed to Francoism in the Spanish economic boom (the Spanish miracle,1959-1975), while avoiding reference to the mass Spanish emigration or the period of economic recession that prevailed during the ten years following the Transition (1975-1985). All of this led the Spanish social majority, including even those identified with the anti-Francoist opposition, to perpetuate the conservative and survivalist behaviours that were learned and transmitted from generation to generation since the 1940s. These include self-censorship and the voluntary submission and conformity to authority – which in extreme cases could even be classified as servility (most commonly identified with the \"silent majority\") – which provided the regime with its cheapest, most effective and most ubiquitous form of repression.\n\nIn an interview with Xavier Moret, the writer Manuel Vázquez Montalbán described the phenomenon in the following way:There was a sociological Francoism which existed before and still exists to a greater or lesser extent today, coupled with Francoist rhetoric in which only the best years – those of 1962 or 1963 and the first part of the 1970s – are remembered, omitting the years of misery and the economic recession that existed prior to the Spanish Civil War and continued to grow under Francoism. The economically prosperous years have been mythologized within sociological Francoism; however, we should remind ourselves that this success was based on exporting the unemployed first to Catalonia and the Basque Country and then later to Europe.In a similar vein, the philosopher José Luis López Aranguren has written that \"Francoism, while originally a political system, transformed into a way of life for the Spanish people\".\n\nIn the exercise of political power, sociological Francoism is defined as \"the political culture of identification with the [Francoist] regime\".\n\nHowever, the journalist Antonio Maestre adds a definition that goes beyond the political sphere, describing sociological Francoism as:The collection of citizens and politicians that, having lived well under Francoism and supported its ideas, were in favour of opening up the regime to a certain degree to ensure that the Transition would proceed in a tolerable direction.The popularity of Franco during his dictatorship was not measured in opinion polls, but in the legitimacy of the army and the charismatic legitimacy of his person (caudillismo), as well as the widespread social binding that took place through the Movimiento nacional (all of which are typical elements of fascism). Together, these ensured massive public demonstrations of support and the practically unanimous results in sporadically held national referenda (such as the Spanish organic law referendum of 1966). In 1969, in one of his last televised Christmas addresses, Franco spoke of Spain's future, saying that all was \"atado y bien atado\" (literally \"tied and well tied\", referring to the institutionalization of his regime), which became a popular saying in Spain. Any clues to his personal health were carefully scrutinized, as were his cryptically-expressed intentions, such as the phrase \"no hay mal que por bien no venga\" (roughly equivalent to \"every cloud has a silver lining\"), which he used in reference to the assassination of Luis Carrero Blanco in 1973 by the Basque separatist group ETA, and whose meaning was the subject of endless speculation. In a secret 1971 interview with Vernon Walters, envoy of Richard Nixon, Franco expressed his opinion that upon his death, Spanish society would carry out a political evolution that would not break with his legacy, as the now larger and more well-off Spanish middle class would avoid risking another civil war.\n\nIn the regime change that followed Franco's death in 1975, however, those that remained most loyal to Francoism were relegated to far-right movements that failed to gain a single seat in 1977 (and only elected a single MP, Blas Piñar, in the second general elections in 1979 under the coalitionist Unión nacional banner). The political right, represented by a coalition of former Francoist administrators under the banner of the Alianza Popular (referred to as \"aperturistas\", those in favour of social reform), attempted to strike a balance between the need to connect with the social majority while at the same time minimizing associations with the past, though with very little electoral success. During the 1970s and 1980s, the social majority tended to vote for parties from the centre (such as the UCD, led by Adolfo Suárez) or the left (PSOE).\n\nIn a book about the Transition, Alfonso Osorio, a member of the 1977 Adolfo Suárez government, describes the reasons behind Alianza Popular's failure to capitalize on sociological Francoism:What the Alianza Popular intended for, in essence, was to use sociological francoism to their advantage. But as it worked out, what they actually capitalized on was the lingering, and less significant, political Francoism...In the meantime, this sociological Francoism...favoured other democratic options closer to the centre.Yet the political system continued to show elements inherited from Francoism. The debate between a post-Franco \"reforma o ruptura\" (political reform or rupture) ended in a compromise reform established by constitutional consensus, as left-wing parties were conscious that their own weakness prevented a full rupture with the old regime. From 1976 onwards, King Juan Carlos I (designated by Franco as his successor in 1969) and his team of close advisors (essentially Torcuato Fernández-Miranda and Adolfo Suárez) implemented the agreed-upon reform, which left both the \"inmovilistas\" (ultraconservatives) and the best-known \"aperturistas\" (progressives such as Manuel Fraga and José María de Areilza) sidelined from the process. The degree of democracy thus achieved is questioned by some writers, including Armando López Salinas, who considers it essentially controlled reform, in some sense similar to Giuseppe Tomasi di Lampedusa's characterization of the Italian unification in his novel \"The Leopard\": The ruling classes need to change something so that everything remains the same\".\n\nA sign of the survival of Francoist sentiment in a large segment of the population was, among other things, the widespread success of far-right sympathiser Fernando Vizcaíno Casas' satirical novels (\"Al tercer año resucitó\", \"De camisa vieja a chaqueta nueva\"), which express viewpoints associated with the then-popular saying \"Con Franco vivíamos mejor\" (literally, \"We lived better with Franco\"). Even in 2007, in the context of the debates surrounding the Historical Memory Law, there was still resistance among large segments of society and the political establishment to condemn Francoism, as demonstrated by Spanish right-wing politician Jaime Mayor Oreja:Why should I have to condemn Francoism when there were many families that lived it with naturalness and normality? In my Basque lands there were endless myths. The [Spanish Civil] war was much worse than Francoism. Some say that the persecutions in Basque towns were terrible, but it can't have been the case if all the Civil Guards from Galicia were asking to be sent to the Basque Country. It was a situation of extraordinary tranquility. Let's leave the commentaries on Francoism to the historians.The so-called \"traditional values\" also remained identified with Francoism: country, religion, and family. As such, \"Francoism\" is used, in some respects, as synonymous with conservatism, patriarchy, traditionalist conservatism, or authoritarianism; all long-standing phenomena that predate Franco. Indeed, some even reverse the cause and effect between Franco and sociologicial Francoism, positing Franco as the effect of a \"pre-existing\" sociological Francoism, as described by former \"El País\" editor-in-chief Juan Luis Cebrián:I don't believe that Franco was the cause, but rather the consequence. I don't think that Franco or Francoism were a sort of military group that seized power, but rather the physical manifestation, or result, of a way of understanding Spain. And a large part of that way of understanding Spain has been transmitted from generation to generation among the sectors of the Spanish right-wing which former president Aznar belongs to, and in which I was educated. I went to the same school as Aznar; my family is, sociologically, like Aznar's; I studied where he studied, which is to say, the Salamanca neighbourhood of Madrid. That is the sociological Francoism to which I belonged, and thus know so well.The Spanish journalist Enrique Gil Calvo adds \"desarrollismo\" – the quick and unscrupulous economic growth that brought about the so-called Spanish Miracle – as another area that has remained identified with Francoism, describing Madrid as having changed from the \"red breakwater of all the Spains\" to an \"ostentatious showcase of upstart sociological neo-Francoism\".\n\nThe writer Manuel Vázquez Montalbán has been attributed with penning the satirical derivative phrase \"contra Franco vivíamos\" \"mejor\" (literally \"We lived better against Franco\").\n\nAnother saying that remains present in Spanish society is \"Esto con Franco no pasaba\" (literally \"This never used to happen with Franco\"). It was initially used as a way of denouncing behaviours, arising soon after the death of Franco, that contradicted the norms of the then-pervasive ultra-Catholic morality (see: the \"destape\" (literally \"uncovering\") period of Spanish cinema that followed the abolition of censorship, or the Movida Madrileña).\n\nCurrently it is only used in a rhetorical way to point out the irony that, despite living in a liberal democracy, some freedoms that were permitted by the Francoist regime, such as smoking in public places or barbecuing in the mountains or the beach, have been taken away. Similarly, it is used to criticize current problems in society that did not exist in the Franco era, like the Spanish property bubble, and the consequent delay in young adults leaving the family home.\n\nDebates have continued as to whether, in a democratic sense, the lasting effects of Francoism were greater or lesser than the actual changes. One aspect described as an inheritance from the Francoist past is the notable individualism of Spain's leadership (e.g. Adolfo Suarez, Felipe González, José María Aznar, José Luis Rodríguez Zapatero and Mariano Rajoy) coupled with the extraordinary sway that the government holds over the parliament, much greater than in other European democracies. Although the 1978 Constitution cannot be described as a presidential system, the powers held by the Prime Minister are ample. On the other hand, the investiture of the Prime Minister has always occurred without too many issues – the inconclusive 2015 general election notwithstanding – especially compared to other parliamentary democracies (such as Italy); government mandates have been stable (except for the failed \"coup d'état\" of 1981, or perhaps precisely because of it) and lengthy (except the government of Leopoldo Calvo-Sotelo, due to that same circumstance), and have never resorted to coalition government. A final aspect is the fact that the Spanish government has traditionally been a two-party system with smaller, peripheral nationalist/regionalist parties, although the recent rise of Podemos has produced a more fragmented parliament.\n\nIn an essay, the Spanish academic Carlos Ollero expressed the following reservations about the concept of sociological Francoism:I think that this expression is imprecise and can lend itself to misunderstandings. It is necessary to distinguish between two interrelated, yet different, meanings, with varying degrees of effectiveness. The first, stricter, meaning refers to the complex of socioeconomic structures and concrete interests that are created, maintained and strengthened by the Francoist system. The second, too broad, includes under the umbrella term of \"sociological Francoism\" what, in general terms, can be understood as the ensemble of sociopolitical attitudes, consistencies in personal and collective behaviour, and passive or indecisive inertia prompted by forty years of steadfast exercise of personal power.\n"}
{"id": "38458", "url": "https://en.wikipedia.org/wiki?curid=38458", "title": "Space Shuttle program", "text": "Space Shuttle program\n\nThe Space Shuttle program was the fourth human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished routine transportation for Earth-to-orbit crew and cargo from 1981 to 2011. Its official name, Space Transportation System (STS), was taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development.\n\nThe Space Shuttle—composed of an orbiter launched with two reusable solid rocket boosters and a disposable external fuel tank—carried up to eight astronauts and up to of payload into low Earth orbit (LEO). When its mission was complete, the orbiter would re-enter the Earth's atmosphere and land like a glider at either the Kennedy Space Center or Edwards Air Force Base.\n\nThe Shuttle is the only winged manned spacecraft to have achieved orbit and landing, and the only reusable manned space vehicle that has ever made multiple flights into orbit (the Russian shuttle Buran was very similar and was designed to have the same capabilities but made only one unmanned spaceflight before it was cancelled). Its missions involved carrying large payloads to various orbits (including segments to be added to the International Space Station (ISS)), providing crew rotation for the space station, and performing service missions. The orbiter also recovered satellites and other payloads (e.g., from the ISS) from orbit and returned them to Earth, though its use in this capacity was rare. Each vehicle was designed with a projected lifespan of 100 launches, or 10 years' operational life, though original selling points on the shuttles were over 150 launches and over a 15-year operational span with a 'launch per month' expected at the peak of the program, but extensive delays in the development of the International Space Station never created such a peak demand for frequent flights.\n\nVarious shuttle concepts had been explored since the late 1960s. The program formally commenced in 1972, becoming the sole focus of NASA's manned operations after the Apollo, Skylab, and Apollo-Soyuz programs in 1975. The Shuttle was originally conceived of and presented to the public in 1972 as a 'Space Truck' which would, among other things, be used to build a United States space station in low Earth orbit during the 1980s and then be replaced by a new vehicle by the early 1990s. The stalled plans for a U.S. space station evolved into the International Space Station and were formally initiated in 1983 by President Ronald Reagan, but the ISS suffered from long delays, design changes and cost over-runs and forced the service life of the Space Shuttle to be extended several times until 2011 when it was finally retired—serving twice as long than it was originally designed to do. In 2004, according to President George W. Bush's Vision for Space Exploration, use of the Space Shuttle was to be focused almost exclusively on completing assembly of the ISS, which was far behind schedule at that point.\n\nThe first experimental orbiter \"Enterprise\" was a high-altitude glider, launched from the back of a specially modified Boeing 747, only for initial atmospheric landing tests (ALT). \"Enterprise\"'s first test flight was on February 18, 1977, only five years after the Shuttle program was formally initiated; leading to the launch of the first space-worthy shuttle \"Columbia\" on April 12, 1981 on STS-1. The Space Shuttle program finished with its last mission, STS-135 flown by \"Atlantis\", in July 2011, retiring the final Shuttle in the fleet. The Space Shuttle program formally ended on August 31, 2011.\n\nBefore the Apollo 11 moon landing in 1969, NASA began early studies of space shuttle designs. In 1969, President Richard Nixon formed the Space Task Group, chaired by Vice President Spiro Agnew. This group outlined ambitious post-Apollo missions centered on a large permanently manned space station, a small reusable logistics vehicle that would support it, and ultimately a manned mission to Mars. Smaller goals included a variety of space vehicles for moving spacecraft around in orbit.\n\nPresenting the plans to Nixon, Agnew was told that the administration would not commit to a Mars mission, and limited activity to low Earth orbit for the immediate future. He was then told to select one of the two remaining proposals. After some debate between the station and the vehicle, the vehicle was chosen; suitably designed, such a spacecraft could perform some longer-duration missions and thus fill some of the goals of the station, and over the longer run, could help lower the cost of access to space and make the station less expensive.\n\nThe goal, as presented by NASA to Congress, was to provide a much less-expensive means of access to space that would be used by NASA, the Department of Defense, and other commercial and scientific users.\n\nDuring early shuttle development there was great debate about the optimal shuttle design that best balanced capability, development cost and operating cost. Ultimately chosen was a design using a reusable winged orbiter, reusable solid rocket boosters, and an expendable external fuel tank for the orbiter's main engines.\n\nThe shuttle program was formally launched on January 5, 1972, when President Nixon announced that NASA would proceed with the development of a reusable space shuttle system. The stated goals of \"transforming the space frontier...into familiar territory, easily accessible for human endeavor\" was to be achieved by launching as many as 50 missions per year, with hopes of driving down per-mission costs.\n\nThe prime contractor for the program was North American Rockwell (later Rockwell International, now Boeing), the same company responsible for building the Apollo Command/Service Module. The contractor for the Space Shuttle Solid Rocket Boosters was Morton Thiokol (now part of Northrop Grumman Innovation Systems), for the external tank, Martin Marietta (now Lockheed Martin), and for the Space Shuttle main engines, Rocketdyne (now Aerojet Rocketdyne).\n\nThe first orbiter was originally planned to be named \"Constitution\", but a massive write-in campaign from fans of the \"Star Trek\" television series convinced the White House to change the name to \"Enterprise\". Amid great fanfare, \"Enterprise\" (designated OV-101) was rolled out on September 17, 1976, and later conducted a successful series of glide-approach and landing tests in 1977 that were the first real validation of the design.\n\nAll Space Shuttle missions were launched from the Kennedy Space Center (KSC). The weather criteria used for launch included, but were not limited to: precipitation, temperatures, cloud cover, lightning forecast, wind, and humidity. The Shuttle was not launched under conditions where it could have been struck by lightning.\n\nThe first fully functional orbiter was \"Columbia\" (designated OV-102), built in Palmdale, California. It was delivered to Kennedy Space Center (KSC) on March 25, 1979, and was first launched on April 12, 1981—the 20th anniversary of Yuri Gagarin's space flight—with a crew of two.\n\n\"Challenger\" (OV-099) was delivered to KSC in July 1982, \"Discovery\" (OV-103) in November 1983, \"Atlantis\" (OV-104) in April 1985 and \"Endeavour\" in May 1991. \"Challenger\" was originally built and used as a Structural Test Article (STA-099), but was converted to a complete orbiter when this was found to be less expensive than converting \"Enterprise\" from its Approach and Landing Test configuration into a spaceworthy vehicle.\n\nOn April 24, 1990, \"Discovery\" carried the Hubble Space Telescope into space during STS-31.\n\nIn the course of 135 missions flown, two orbiters (\"Columbia\" and \"Challenger\") suffered catastrophic accidents, with the loss of all crew members, totaling 14 astronauts.\n\nThe accidents led to national level inquiries and detailed analysis of why the accidents occurred. There was a significant pause where changes were made before the Shuttles returned to flight. The \"Columbia\" disaster occurred in 2003, but STS took more than a year off before returning to flight in June 2005 with the STS-114 mission. The previous break was between January 1986 (when the \"Challenger\" disaster occurred) and 32 months later when STS-26 was launched on September 29, 1988.\n\nThe longest Shuttle mission was STS-80 lasting 17 days, 15 hours. The final flight of the Space Shuttle program was STS-135 on July 8, 2011.\n\nSince the Shuttle's retirement in 2011, many of its original duties are performed by an assortment of government and private vessels. The European ATV Automated Transfer Vehicle supplied the ISS between 2008 and 2015. Classified military missions are being flown by the US Air Force's unmanned space plane, the X-37B. By 2012, cargo to the International Space Station was already being delivered commercially under NASA's Commercial Resupply Services by SpaceX's partially reusable Dragon spacecraft, followed by Orbital Sciences' Cygnus spacecraft in late 2013. Crew service to the ISS is currently provided by the Russian Soyuz while work on the Commercial Crew Development program proceeds; the first crewed flight of this is planned for January 27, 2019, on the SpaceX Falcon 9 with Dragon 2 crew capsule. For missions beyond low Earth orbit, NASA is building the Space Launch System and the Orion (spacecraft).\n\nSpace Shuttle missions have included:\n\n\nEarly during development of the space shuttle, NASA had estimated that the program would cost $7.45 billion ($43 billion in 2011 dollars, adjusting for inflation) in development/non-recurring costs, and $9.3M ($54M in 2011 dollars) per flight. Early estimates for the cost to deliver payload to low earth orbit were as low as $118 per pound ($260/kg) of payload ($635/lb or $1,400/kg in 2011 dollars), based on marginal or incremental launch costs, and assuming a 65,000 pound (30 000 kg) payload capacity and 50 launches per year. A more realistic projection of 12 flights per year for the 15-year service life combined with the initial development costs would have resulted in a total cost projection for the program of roughly $54 billion (in 2011 dollars).\n\nThe total cost of the actual 30-year service life of the shuttle program through 2011, adjusted for inflation, was $196 billion. The exact breakdown into non-recurring and recurring costs is not available, but, according to NASA, the average cost to launch a Space Shuttle as of 2011 was about $450 million per mission.\n\nNASA's budget for 2005 allocated 30%, or $5 billion, to space shuttle operations; this was decreased in 2006 to a request of $4.3 billion. Non-launch costs account for a significant part of the program budget: for example, during fiscal years 2004 to 2006, NASA spent around $13 billion on the space shuttle program, even though the fleet was grounded in the aftermath of the \"Columbia\" disaster and there were a total of three launches during this period of time. In fiscal year 2009, NASA budget allocated $2.98 billion for 5 launches to the program, including $490 million for \"program integration\", $1.03 billion for \"flight and ground operations\", and $1.46 billion for \"flight hardware\" (which includes maintenance of orbiters, engines, and the external tank between flights.)\n\nPer-launch costs can be measured by dividing the total cost over the life of the program (including buildings, facilities, training, salaries, etc.) by the number of launches. With 135 missions, and the total cost of US$192 billion (in 2010 dollars), this gives approximately $1.5 billion per launch over the life of the shuttle program. A 2017 study found that carrying one kilogram of cargo to the ISS on the shuttle cost $272,000 in 2017 dollars, twice the cost of Cygnus and three times that of Dragon.\n\nNASA used a management philosophy known as success-oriented management during the Space Shuttle program which was described by historian Alex Roland in the aftermath of the \"Columbia\" disaster as \"hoping for the best\". Success-oriented management has since been studied by several analysts in the area.\n\nIn the course of 135 missions flown, two orbiters were destroyed, with loss of crew totalling 14 astronauts:\n\nThere was also one abort-to-orbit and some fatal accidents on the ground during launch preparations.\n\nClose-up video footage of \"Challenger\" during its final launch on January 28, 1986 clearly show it began due to an O-ring failure on the right solid rocket booster (SRB). The hot plume of gas leaking from the failed joint caused the collapse of the external tank, which then resulted in the orbiter's disintegration due to high aerodynamic stress. The accident resulted in the loss of all seven astronauts on board. \"Endeavour\" (OV-105) was built to replace \"Challenger\" (using structural spare parts originally intended for the other orbiters) and delivered in May 1991; it was first launched a year later.\n\nAfter the loss of \"Challenger\", NASA grounded the shuttle program for over two years, making numerous safety changes recommended by the Rogers Commission Report, which included a redesign of the SRB joint that failed in the \"Challenger\" accident. Other safety changes included a new escape system for use when the orbiter was in controlled flight, improved landing gear tires and brakes, and the reintroduction of pressure suits for shuttle astronauts (these had been discontinued after STS-4; astronauts wore only coveralls and oxygen helmets from that point on until the \"Challenger\" accident). The shuttle program continued in September 1988 with the launch of \"Discovery\" on STS-26.\n\nThe accidents did not just affect the technical design of the orbiter, but also NASA.\nQuoting some recommendations made by the post-\"Challenger\" Rogers commission:\n\nThe shuttle program operated accident-free for seventeen years after the \"Challenger\" disaster, until \"Columbia\" broke up on re-entry, killing all seven crew members, on February 1, 2003. The accident began when a piece of foam shed from the external tank struck the leading edge of the orbiter's left wing, puncturing one of the reinforced carbon-carbon (RCC) panels that covered the wing edge and protected it during re-entry. As \"Columbia\" re-entered the atmosphere, hot gas penetrated the wing and destroyed it from the inside out, causing the orbiter to lose control and disintegrate.\n\nAfter the \"Columbia\" disaster, the International Space Station operated on a skeleton crew of two for more than two years and was serviced primarily by Russian spacecraft. While the \"Return to Flight\" mission STS-114 in 2005 was successful, a similar piece of foam from a different portion of the tank was shed. Although the debris did not strike \"Discovery\", the program was grounded once again for this reason.\n\nThe second \"Return to Flight\" mission, STS-121 launched on July 4, 2006, at 14:37 (EDT). Two previous launches were scrubbed because of lingering thunderstorms and high winds around the launch pad, and the launch took place despite objections from its chief engineer and safety head. A five-inch (13 cm) crack in the foam insulation of the external tank gave cause for concern; however, the Mission Management Team gave the go for launch. This mission increased the ISS crew to three. \"Discovery\" touched down successfully on July 17, 2006 at 09:14 (EDT) on Runway 15 at Kennedy Space Center.\n\nFollowing the success of STS-121, all subsequent missions were completed without major foam problems, and the construction of ISS was completed (during the STS-118 mission in August 2007, the orbiter was again struck by a foam fragment on liftoff, but this damage was minimal compared to the damage sustained by \"Columbia\").\n\nThe \"Columbia\" Accident Investigation Board, in its report, noted the reduced risk to the crew when a shuttle flew to the International Space Station (ISS), as the station could be used as a safe haven for the crew awaiting rescue in the event that damage to the orbiter on ascent made it unsafe for re-entry. The board recommended that for the remaining flights, the shuttle always orbit with the station. Prior to STS-114, NASA Administrator Sean O'Keefe declared that all future flights of the shuttle would go to the ISS, precluding the possibility of executing the final Hubble Space Telescope servicing mission which had been scheduled before the \"Columbia\" accident, despite the fact that millions of dollars worth of upgrade equipment for Hubble were ready and waiting in NASA warehouses. Many dissenters, including astronauts , asked NASA management to reconsider allowing the mission, but initially the director stood firm. On October 31, 2006, NASA announced approval of the launch of \"Atlantis\" for the fifth and final shuttle servicing mission to the Hubble Space Telescope, scheduled for August 28, 2008. However SM4/STS-125 eventually launched in May 2009.\n\nOne impact of \"Columbia\" was that future crewed launch vehicles, namely the Ares I, had a special emphasis on crew safety compared to other considerations.\n\nNASA maintains extensive, warehoused catalogs of recovered pieces from the two destroyed orbiters.\n\nThe Space Shuttle program was extended several times beyond its originally envisioned 15-year life span because of the delays in building the United States space station in low Earth orbit—a project which eventually evolved into the International Space Station. It was formally scheduled for mandatory retirement in 2010 in accord with the directives President George W. Bush issued on January 14, 2004 in his Vision for Space Exploration.\n\nA$2.5 billion spending provision allowing NASA to fly the Space Shuttle beyond its then-scheduled retirement in 2010 passed the Congress in April 2009, although neither NASA nor the White House requested the one-year extension.\n\nThe final Space Shuttle launch was that of \"Atlantis\" on July 8, 2011. Although the retirement was planned and roughly in line with what was expected out of STS without further upgrades, the planned replacement for the STS, the Constellation program was cancelled the year before STS concluded. The two programs that took its place, two undetermined commercial crew vehicles and SLS with Orion needed even more time to have a new launcher. NASA has continued to fly to the station via its Roscosmos partner, who still has a functioning manned launcher, the Soyuz system. Manned launch systems have been proposed by other countries, such as the ESA's mini-shuttle Hermes launched by an Ariane rocket, which was cancelled in 1992.\n\nOut of the five fully functional shuttle orbiters built, three remain. \"Enterprise\", which was used for atmospheric test flights but not for orbital flight, had many parts taken out for use on the other orbiters. It was later visually restored and was on display at the National Air and Space Museum's Steven F. Udvar-Hazy Center until April 19, 2012. \"Enterprise\" was moved to New York City in April 2012 to be displayed at the Intrepid Sea, Air & Space Museum, whose Space Shuttle Pavilion opened on July 19, 2012. \"Discovery\" replaced \"Enterprise\" at the National Air and Space Museum's Steven F. Udvar-Hazy Center. \"Atlantis\" formed part of the Space Shuttle Exhibit at the Kennedy Space Center visitor complex and has been on display there since June 29, 2013 following its refurbishment.\n\nOn October 14, 2012, \"Endeavour\" completed an unprecedented drive on city streets from Los Angeles International Airport to the California Science Center, where it has been on display in a temporary hangar since late 2012. The transport from the airport took two days and required major street closures, the removal of over 400 city trees, and extensive work to raise power lines, level the street, and temporarily remove street signs, lamp posts, and other obstacles. Hundreds of volunteers, and fire and police personnel, helped with the transport. Large crowds of spectators waited on the streets to see the shuttle as it passed through the city. \"Endeavour\" will be displayed permanently beginning in 2017 at the Samuel Oschin Air and Space Center (an addition to the California Science Center currently under construction), where it will be mounted in the vertical position complete with solid rocket boosters and an external tank.\n\n \nOne area of Space Shuttle applications is an expanded crew. Crews of up to eight have been flown in the Orbiter, but it could have held at least a crew of ten. Various proposals for filling the payload bay with additional passengers were also made as early as 1979. One proposal by Rockwell provided seating for 74 passengers in the Orbiter payload bay, with support for three days in Earth orbit. With a smaller 64 seat orbiter, costs for the late 1980s would be around 1.5 million USD per seat per launch. The Rockwell passenger module had two decks, four seats across on top and two on the bottom, including a 25-inch (63.5 cm) wide isle and extra storage space.\n\nAnother design was Space Habitation Design Associates 1983 proposal for 72 passengers in the Space Shuttle Payload bay. Passengers were located in 6 sections, each with windows and its own loading ramp at launch, and with seats in different configurations for launch and landing. Another proposal was based on the Spacelab habitation modules, which provided 32 seats in the payload bay in addition to those in the cockpit area.\n\nThere were some efforts to analyze commercial operation of STS. Using the NASA figure for average cost to launch a Space Shuttle as of 2011 at about $450 million per mission, a cost per seat for a 74 seat module envisioned by Rockwell came to less than $6 million, not including the regular crew. Some passenger modules used hardware similar to existing equipment, such as the tunnel, which was also needed for Spacehab and Spacelab\n\nDuring the three decades of operation, various follow-on and replacements for the STS Space Shuttle were partially developed but not finished.\n\nExamples of possible future space vehicles to supplement or supplant STS:\n\nOne effort in the direction of space transportation was the Reusable Launch Vehicle (RLV) program, initiated in 1994 by NASA. This led to work on the X-33 and X-34 vehicles. NASA spent about 1 billion USD on developing the X-33 hoping for it be in operation by 2005. Another program around the turn of the millennium was the Space Launch Initiative, which was a next generation launch initaive.\n\nThe Space Launch Initiative program was started in 2001, and in late 2002 it was evolved into two programs, the Orbital Space Plane Program and the Next Generation Launch Technology program. OSP was oriented towards provided access to the International Space Station.\n\nOther vehicles that would have taken over some of the Shuttles responsibilities were the HL-20 Personnel Launch System or the NASA X-38 of the Crew Return Vehicle program, which were primarily for getting people down from ISS. The X-38 was cancelled in 2002, and the HL-20 was cancelled in 1993. Several other programs in this existed such as the Station Crew Return Alternative Module (SCRAM) and Assured Crew Return Vehicle (ACRV)\n\nAccording to the 2004 Vision for Space Exploration, the next manned NASA program was to be Project Constellation with its Ares I and Ares V launch vehicles and the Orion Spacecraft; however, the Constellation program was never fully funded, and in early 2010 the Obama administration asked Congress to instead endorse a plan with heavy reliance on the private sector for delivering cargo and crew to LEO.\n\nThe Commercial Orbital Transportation Services (COTS) program began in 2006 with the purpose of creating commercially operated unmanned cargo vehicles to service the ISS. The first of these vehicles, SpaceX's Dragon, became operational in 2012, and the second, Orbital Sciences' Cygnus did so in 2014.\n\nThe Commercial Crew Development (CCDev) program was initiated in 2010 with the purpose of creating commercially operated manned spacecraft capable of delivering at least four crew members to the ISS, staying docked for 180 days and then returning them back to Earth. These spacecraft, like the SpaceX Dragon V2 and Sierra Nevada Corporation's Dream Chaser are expected to become operational around the end of 2018.\n\nAlthough the Constellation program was canceled, it has been replaced with a very similar beyond low Earth orbit program. The Orion spacecraft has been left virtually unchanged from its previous design. The planned Ares V rocket has been replaced with the smaller Space Launch System (SLS), which is planned to launch both Orion and other necessary hardware. Exploration Flight Test-1 (EFT-1), an unmanned test flight of the Orion spacecraft, launched on December 5, 2014 on a Delta IV Heavy rocket. Exploration Mission-1 (EM-1) is the unmanned initial launch of the SLS, which is planned for 2019. Exploration Mission-2 (EM-2) is the first manned flight of Orion and SLS and is scheduled for 2023. EM-2 is a 10-14-day mission planned to place a crew of four into Lunar orbit. , the destination for EM-3 and immediate destination focus for this new program is still in-flux.\n\nThe Space Shuttle program occupied over 654 facilities, used over 1.2 million line items of equipment, and employed over 5,000 people. The total value of equipment was over $12 billion. Shuttle-related facilities represented over a quarter of NASA's inventory. There were over 1,200 active suppliers to the program throughout the United States. NASA's transition plan had the program operating through 2010 with a transition and retirement phase lasting through 2015. During this time, the Ares I and Orion as well as the Altair Lunar Lander were to be under development, although these programs have since been canceled.\n\nin the 2010s, two major programs for human spaceflight were Commercial Crew Development and the Space Launch System with the Orion capsule. Kennedy Space Center Launch Complex 39 was, for example, used to launch a Falcon Heavy rocket.\n\nThe Space Shuttle program has been criticized for failing to achieve its promised cost and utility goals, as well as design, cost, management, and safety issues. Others have argued that the Shuttle program was a step backwards from the Apollo Program, which, while extremely dangerous, accomplished far more scientific and space exploration endeavors than the Shuttle ever could.\n\nAfter both the \"Challenger\" disaster and the \"Columbia\" disaster, high-profile boards convened to investigate the accidents with both committees returning praise and serious critiques to the program and NASA management. Some of the most famous of the criticisms, most of management, came from Nobel Prize winner Richard Feynman, in his report that followed his appointment to the commission responsible for investigating the \"Challenger\" disaster.\n\nMany other vehicles were used in support of the Space Shuttle program, mainly terrestrial transportation vehicles.\n\n\n\n\n\n\n"}
{"id": "39478785", "url": "https://en.wikipedia.org/wiki?curid=39478785", "title": "Tanagra (machine learning)", "text": "Tanagra (machine learning)\n\nTanagra is a free suite of machine learning software for research and academic purposes\ndeveloped by Ricco Rakotomalala at the Lumière University Lyon 2, France.\nTanagra supports several standard data mining tasks such as: Visualization, Descriptive statistics, Instance selection, feature selection, feature construction, regression, factor analysis, clustering, classification and association rule learning.\n\nTanagra is an academic project. It is widely used at French-speaking universities. Tanagra is frequently used in real studies and in the software comparison papers.\n\nThe development of Tanagra was started in June 2003. The first version is distributed in December 2003. Tanagra is the successor of Sipina, another free data mining tool which is intended only for the supervised learning tasks (classification), especially an interactive and visual construction of decision trees. Sipina is still available online and is maintained.\nTanagra is an \"open source project\" as every researcher can access to the source code, and add his own algorithms, as far as he agrees and conforms to the software distribution license.\n\nThe main purpose of Tanagra project is to give researchers and students a user-friendly data mining software, conforming to the present norms of the software development in this domain (especially in the design of its GUI and the way to use it), and allowing to analyze either real or synthetic data.\n\nFrom 2006, Ricco Rakotomalala made an important documentation effort. A large number of tutorials are published on a dedicated website. They describe the statistical and machine learning methods and their implementation with Tanagra on real case studies. The use of the other free data mining tools on the same problems is also widely described. The comparison of the tools enables to the readers to understand the possible differences in the presenting of results.\n\nTanagra works as the current data mining tools. The user can design visually a data mining process in a diagram. Each node is a statistical or machine learning technique, the connection between two nodes represents the data transfer. But unlike of the majority of the tools which are based on the workflow paradigm, Tanagra is very simplified. The treatments are represented in a tree diagram. The results are displayed in a HTML format. So it is easy to export the outputs in order to visualize the results in a browser. It is also possible to copy the result tables to a spreadsheet.\n\nTanagra makes a good compromise between the statistical approaches (e.g. parametric and nonparametric statistical tests), the multivariate analysis methods (e.g. factor analysis, correspondence analysis, cluster analysis, regression) and the machine learning techniques (e.g. neural network, support vector machine, decision trees, random forest).\n\n\n"}
{"id": "14662510", "url": "https://en.wikipedia.org/wiki?curid=14662510", "title": "The Mysterious Universe", "text": "The Mysterious Universe\n\nThe Mysterious Universe is a popular science book by the British astrophysicist Sir James Jeans, first published in 1930 by the Cambridge University Press. In the United States, it was published by Macmillan.\n\nThe book is an expanded version of the Rede Lecture delivered at the University of Cambridge in 1930. It begins with a full-page citation of the famous passage in Plato's \"Republic\", Book VII, laying out the allegory of the cave. The book made frequent reference to the quantum theory of radiation, begun by Max Planck in 1900, to Einstein's general relativity, and to the new theories of quantum mechanics of Heisenberg and Schrödinger, of whose philosophical perplexities the author seemed well aware.\n\nThe book was denounced by the Cambridge philosopher Ludwig Wittgenstein, because \"Jeans has written a book called \"The Mysterious Universe\" and I loathe it and call it misleading. Take the title...I might say that the title \"The Mysterious Universe\" includes a kind of idol worship, the idol being Science and the Scientist.\"\n\nA second edition appeared in 1931. The book was reprinted 15 times between 1930 and 1938 and in September 2007 ().\n\n\nThere are two pages of photographic plates:\n\nThe US edition has woodcut illustrations by the painter Walter Tandy Murch.\n\n"}
{"id": "46619052", "url": "https://en.wikipedia.org/wiki?curid=46619052", "title": "Timeline of Silurian research", "text": "Timeline of Silurian research\n\nThis timeline of Silurian research is a chronological listing of events in the history of geology and paleontology focused on the study of earth during the span of time lasting from 443.4–419.2 million years ago and the legacies of this period in the rock and fossil records.\n\n1825\n\n1844\n\n1859\n\n1937\n\n1972\n\n2010\n\n"}
{"id": "22421276", "url": "https://en.wikipedia.org/wiki?curid=22421276", "title": "Vostok-K", "text": "Vostok-K\n\nThe Vostok-K ( meaning \"East\"), GRAU index 8K72K was an expendable carrier rocket used by the Soviet Union for thirteen launches between 1960 and 1964, six of which were manned. It was derived from the earlier Vostok-L; however, it featured uprated engines to improve performance, and enlarge its payload capacity. It was a member of the R-7 family of rockets.\n\nThe Vostok-K made its maiden flight on 22 December 1960, three weeks after the retirement of the Vostok-L. The third stage engine failed 425 seconds after launch, and the payload, a Korabl'-Sputnik spacecraft, failed to reach orbit. The spacecraft was recovered after landing, and the two dogs aboard the spacecraft survived the flight.\n\nOn 12 April 1961, a Vostok-K rocket was used to launch Vostok 1, the first manned spaceflight, which made Yuri Gagarin the first human to fly in space. All six manned missions of the Vostok programme were launched using Vostok-K rockets. The first two Zenit reconnaissance satellites were also launched with the Vostok-K, but it was soon replaced in that capacity with the uprated 8A92 booster. After the conclusion of the Vostok program, there were two remaining 8K72Ks left; these were used to launch four Elektron scientific satellites on January 30 and July 10, 1964.\n"}
{"id": "28319064", "url": "https://en.wikipedia.org/wiki?curid=28319064", "title": "We Have Never Been Modern", "text": "We Have Never Been Modern\n\nWe Have Never Been Modern is a 1991 book by Bruno Latour, originally published in French as \"Nous n'avons jamais été modernes : Essai d'anthropologie symétrique\" (English translation: 1993).\n\nThe book is an \"anthropology of science\" that explores the dualistic distinction modernity makes between nature and society. Pre-modern peoples, argues Latour, made no such division. Contemporary matters of public concern such as global warming, the HIV/AIDS pandemic and emerging biotechnologies mix politics, science, popular and specialist discourse to such a degree that a tidy nature/culture dualism is no longer possible. This inconsistency has given rise to post-modern and anti-modern movements. Latour attempts to reconnect the social and natural worlds by arguing that the modernist distinction between nature and culture never existed. He claims we must rework our thinking to conceive of a \"Parliament of Things\" wherein natural phenomena, social phenomena and the discourse about them are not seen as separate objects to be studied by specialists, but as hybrids made and scrutinized by the public interaction of people, things and concepts.\n\n"}
{"id": "5934705", "url": "https://en.wikipedia.org/wiki?curid=5934705", "title": "Wholeness and the Implicate Order", "text": "Wholeness and the Implicate Order\n\nWholeness and the Implicate Order is a book by theoretical physicist David Bohm. It was originally published 1980 by Routledge, Great Britain.\n\nThe book is considered a basic reference for Bohm's concepts of undivided wholeness and of implicate and explicate orders, as well as of Bohm's rheomode - an experimental language based on verbs. The book is cited, for example, by philosopher Steven M. Rosen in his book \"The Self-evolving Cosmos\", by mathematician and theologian Kevin J. Sharpe in his book \"David Bohm's World\", by theologian Joseph P. Farrell in \"Babylon's Banksters\", and by theologian John C. Polkinghorne in his book \"One World\".\n\n\n"}
