{"id": "25413532", "url": "https://en.wikipedia.org/wiki?curid=25413532", "title": "Alexander Filippou", "text": "Alexander Filippou\n\nAlexander C. Filippou (born 19 August 1958, Thessaloniki, Greece) has been a Professor of Inorganic Chemistry at the Rheinische-Friedrich-Wilhelms-University Bonn since 2005.\n\n\nFilippou has made a significant contribution in the research of heavier homologues of carbon (Si, Ge, Sn, Pb) regarding the ability to form a triple bond to a metal.\n\n"}
{"id": "7670122", "url": "https://en.wikipedia.org/wiki?curid=7670122", "title": "Analog models of gravity", "text": "Analog models of gravity\n\nAnalog models of gravity are attempts to model various phenomena of general relativity (e.g., black holes or cosmological geometries) using other physical systems such as acoustics in a moving fluid, superfluid helium, or Bose–Einstein condensate; gravity waves in water; and propagation of electromagnetic waves in a dielectric medium. These analogs (or analogies) serve to provide new ways of looking at problems, permit ideas from other realms of science to be applied, and may create opportunities for practical experiments within the analogue that can be applied back to the source phenomena.\n\nAnalog models of gravity have been used in hundreds of published articles in the last decade. The use of these analogues can be traced back to the very start of general relativity, with Einstein and Newton.\n\n"}
{"id": "5416952", "url": "https://en.wikipedia.org/wiki?curid=5416952", "title": "Associazione Friulana di Astronomia e Meteorologia", "text": "Associazione Friulana di Astronomia e Meteorologia\n\nThe Associazione Friulana di Astronomia e Meteorologia (AFAM) is a non-profit cultural association whose goal is the promotion of astronomy and meteorology to the public and the development of scientific research activities, often in collaboration with professional scientists.\nEstablished in 1969, now AFAM has its own operating structures in Remanzacco (Friuli, Italy). \n\nAFAM is member of the Unione Astrofili Italiani (the Italian union of amateur astronomers).\n\n\n"}
{"id": "36750998", "url": "https://en.wikipedia.org/wiki?curid=36750998", "title": "Bennett's laws", "text": "Bennett's laws\n\nBennett's laws of quantum information are:\n\nwhere formula_1 indicates \"can do the job of\".\n\nThese principles were formulated around 1993 by Charles H. Bennett.\n\n"}
{"id": "16316127", "url": "https://en.wikipedia.org/wiki?curid=16316127", "title": "Biology by Team", "text": "Biology by Team\n\nBiology by Team in German \"Biologie im Team\" - is the first Austrian biology contest for Upper Secondary Schools.\n\nStudents at upper secondary schools, who are especially interested in biology, can deepen their knowledge and\nbroaden their competence in experimental biology within the frame work of this contest.\nEach year, a team of teachers choose modules of key themes on which students work in the form of a voluntary exercise.\nThe evaluation focuses in particular on the practical work, and, since the school year 2004/05, also on teamwork. In April, a two-day closing competition takes place, in which six groups of students from participating schools are given various\nproblems to solve. A jury (persons from the science and corporate communities) evaluate the results and how they are presented.\n\nThe concept was developed by a team of teachers in co-operation with the AHS (Academic Secondary Schools) - Department of the Pedagogical Institute in Carinthia. \nSince 2008 it is situated at the Science departement of the University College of Teacher Training Carinthia.\nThe first contest in the school year 2002/03 took place under the motto: \"Hell is loose in the ground under us\".\nOther themes included \"Beautiful but dangerous\", \"www-worldwide water 1 and 2\", \"Expedition forest\", \"Relationship boxes\", \"Mole's view\", \"Biological timetravel\", \"Biology at the University\", \"Ecce Homo\", Biodiversity, \"Death in tin cans\", \"Sex sells\", \"Without a trace\", \"Biologists see more\" and \"Quo vadis biology?\" \nThe theme for the year 2018/19 is \"Biology without limits\"?\nTill now the following schools were participating:\n• BG/BRG Mössingerstraße Klagenfurt\n• BG/BRG St. Martinerstraße Villach\n• BG/BRG Peraustraße Villach\n• Österreichisches Gymnasium Prag\n• Europagymnasium Klagenfurt\n• BRG Viktring Klagenfurt\n• BORG Wolfsberg Wolfsberg\n• Stiftsgymnasium St. Paul im Lavanttal St. Paul im Lavanttal\n\nBIT was submitted for the German Innovations-prize for Sustainable Education and placed among the 13 „best of\"all nominated projects.\nWith these prerequisites the base concept of „Biology By Team\" can be replicated for other science and instructional fields and could provide an important contribution for the improvement of the subject and also team competence of our youth.\n\nBIT - the only Austrian biology competition, enabled 2008 for the first time the Austrian participation at the EUSO - European Union Science Olympiad.\n\n\n"}
{"id": "2808327", "url": "https://en.wikipedia.org/wiki?curid=2808327", "title": "Black brane", "text": "Black brane\n\nIn general relativity, a black brane is a solution of the equations that generalizes a black hole solution but it is also extended—and translationally symmetric—in \"p\" additional spatial dimensions. That type of solution would be called a black \"p\"-brane.\n\nIn string theory, the term black brane describes a group of D1-branes that are surrounded by a horizon. With the notion of a horizion in mind as well as identifying points as zero-branes, a generalization of a black hole is a black p-brane. However, many physicists tend to define a black brane separate from a black hole, making the distinction that the singularity of a black brane is not a point like a black hole, but instead a higher dimensional object.\n\nA BPS black brane is similar to a BPS black hole. They both have electric charges. Some BPS black branes have magnetic charges.\n\nThe metric for a black \"p\"-brane in a \"n\"-dimensional spacetime is:\n\nwhere:\n"}
{"id": "28836822", "url": "https://en.wikipedia.org/wiki?curid=28836822", "title": "Bryneglwys Fault", "text": "Bryneglwys Fault\n\nBryneglwys Fault is a geological fault in Wales.\n\n"}
{"id": "14676401", "url": "https://en.wikipedia.org/wiki?curid=14676401", "title": "Changes in the Land", "text": "Changes in the Land\n\nChanges in the Land: Indians, Colonists and the Ecology of New England is a 1983 nonfiction book by historian William Cronon.\n\nIn this work, Cronon demonstrated the impact on the land of the widely disparate conceptions of ownership held by Native Americans and English colonists. English law objectified land, making it an object of which the purchaser had ownership of every aspect. Native American law conceived only the possibility of usufruct rights, the right, that is, to own the nuts or fish or wood that land or bodies of water produced, or the right to hunt, fish or live on the land, there was no possibility of owning the land itself. The second innovative aspect of Cronon's work was to reconceptualize Native Americans as actors capable of changing the ecosystems with which they interacted. Native Americans could, in Cronon's recounting, alter the nature of the forests or exterminate species. Nevertheless, because their technological capabilities were limited and, therefore, native populations were small, their impact on the land was limited. For these reasons, \"the shift from Indian to European dominance entailed important changes\".\n\nEcosystems are never actually inert, even without human interaction, and some ecological changes are due to climatic changes, disease, drought, and natural fire. These changes are more negligible, and Cronon showed how the Native Americans and Europeans both distinctly altered the environment. However, the “Indian” relationship to the ecosystem was decisively less volatile. Having a far greater familiarity with the New England ecosystem, Native Americans understood the cyclical nature of the seasons. They moved and responded to the need for food. Without agriculture in the North, Indians depended on this understanding of the ecosystem since they lived chiefly as hunters and gatherers.\n\nThe northern Indians' refusal to store food for the winter was seen in Chapter Three as the great paradox of “Want in the Land of Plenty.” Europeans could not understand the Indians willingness to go hungry during the winter.\n\nCronon felt the best evidence of an extant symbiotic relationship between the Indians and the environment was the early naturalist’s depictions of the extraordinary abundance of trees, fish, birds, and mammals. While the Native Americans certainly altered and manipulated the environment, their controlled burning actually had a reciprocal ecological benefit for both the Native Americans themselves and the indigenous animals. Thinning the canopy and forming an edge effect attracted more game, helped re-populate game, and increased the rate at which nutrients returned to the soil. When Europeans arrived, New England was not a pristine forest as many people imagine.\n\n"}
{"id": "8964357", "url": "https://en.wikipedia.org/wiki?curid=8964357", "title": "Cloud Appreciation Society", "text": "Cloud Appreciation Society\n\nThe Cloud Appreciation Society is a society founded by Gavin Pretor-Pinney from the United Kingdom in January 2005. The society aims to foster understanding and appreciation of clouds, and has over 42,000 members worldwide from 115 different countries, as of January 2017.\n\nYahoo named the society's website as \"the most weird and wonderful find on the internet for 2005\". The group and its founder were the focus of a BBC documentary \"Cloudspotting\", based on Pretor-Pinney's book \"The Cloudspotter's Guide\". During an episode of Taskmaster, comedian Hugh Dennis revealed he is a member of the society.\n\n\n"}
{"id": "56120104", "url": "https://en.wikipedia.org/wiki?curid=56120104", "title": "Disrupted planet", "text": "Disrupted planet\n\n\"Disrupted planet\" is an astronomical term for a planet, or exoplanet, that has been disrupted, or destroyed, by a nearby, or passing, astronomical body or object, such as a star. The result of such a disruption may be the production of excessive amounts of related gas, dust and debris, that may eventually surround the parent star in the form of a circumstellar disk or debris disk. As a consequence, the orbiting debris field may be an \"uneven ring of dust\", causing erratic light fluctuations in the apparent luminosity of the parent star, as may have been responsible for the oddly flickering light curves associated with the starlight observed from certain variable stars, such as that from KIC 8462852, RZ Piscium and WD 1145+017. Excessive amounts of infrared radiation may be detected from such stars, suggestive evidence in itself that dust and debris may be orbiting the stars.\n\nExamples of planets (or their related remnants), considered to have been a disrupted planet, or part of such a planet, include: 'Oumuamua and WD 1145+017 b, as well as asteroids, hot Jupiters and those that are hypothetical planets, like Fifth planet, Phaeton, Planet V and Theia.\n\nExamples of parent stars, considered to have caused a planet to have been disrupted, include: EPIC 204278916, KIC 8462852 (Tabby's Star), PDS 110, RZ Piscium, WD 1145+017 and 47 Ursae Majoris.\n\nKIC 8462852 is an F-type main-sequence star exhibiting unusual light fluctuations, including up to a 22% dimming in brightness. Several hypotheses have been proposed to explain these irregular changes, but none to date fully explain all aspects of the curve. One explanation is that an \"uneven ring of dust\" orbits KIC 8462852.\n\n"}
{"id": "58842024", "url": "https://en.wikipedia.org/wiki?curid=58842024", "title": "Engineering economics (civil engineering)", "text": "Engineering economics (civil engineering)\n\nEngineering Economics in Civil Engineering, also known generally as engineering economics, or alternatively engineering economy, is a subset of economics, more specifically, microeconomics. It is defined as a \"guide for the economic selection among technically feasible alternatives for the purpose of a rational allocation of scarce resources.\" \nIts goal is to guide entities, private or public that are confronted with the fundamental problem of economics, it wishes to accomplish more than its resources will permit. \nThis fundamental problem of economics consists of two fundamental questions that must be answered, namely what objectives should be investigated or explored and how should these be achieved? Economics as a social science answers those questions and is defined as the knowledge used for selecting among “…technically feasible alternatives for the purpose of a rational allocation of scarce resources.” Correspondingly, all problems involving \"...profit maximizing or cost-minimizing are engineering problems with economic objectives\nand are properly described by the label \"engineering economy\".\n\nAs a subdiscipline practiced by civil engineers, engineering economics narrows the definition of the fundamental economic problem and related questions to that of problems related to the investment of capital, public or private in a broad array of infrastructure projects. Civil engineers confront more specialized forms of the fundamental problem in the form of inadequate economic evaluation of engineering projects.\n\nCivil engineers under constant pressure to deliver infrastructure effectively and efficiently confront complex problems associated with allocating scarce resources for ensuring quality, mitigating risk and controlling project delivery. Civil engineers must be educated to recognize the role played by engineering economics as part of the evaluations occurring at each phase in the project lifecycle. \n\nThus, the application of engineering economics in the practice of civil engineering focuses on the decision-making process, its context, and environment in project execution and delivery. \nIt is pragmatic by nature, integrating microeconomic theory with civil engineering practice but, it is also a simplified application of economic theory in that it avoids a number of microeconomic concepts such as price determination, competition and supply and demand. \nThis poses new, underlying economic problems of resource allocation for civil engineers in delivering infrastructure projects and specifically, resources for project management, planning and control functions. \n\nCivil engineers address these fundamental economic problems using specialized engineering economics knowledge as a framework for continuously “… probing economic feasibility…using a stage-wise approach…” throughout the project lifecycle. The application of this specialized civil engineering knowledge can be in the form of engineering analyses of life-cycle cost, cost accounting, cost of capital and the economic feasibility of engineering solutions for design, construction and project management. The civil engineer must have the ability to use engineering economy methodologies for the “formulation of objectives, specification of alternatives, prediction of outcomes” and estimation of minimum acceptability for investment and optimization. \n\nThey must also be capable of integrating these economic considerations into appropriate engineering solutions and management plans that predictably and reliably meet project stakeholder expectations in a sustainable manner. \n\nThe civil engineering profession provides a special function in our society and economy where investing substantial sums of funding in public infrastructure requires \"...some assurance that it will perform its intended function.\" \n\nThus, the civil engineer exercising their professional judgment in making decisions about fundamental problems relies upon the profession’s knowledge of engineering economics to provide \"the practical certainty\" that makes the social investment in public infrastructure feasible. \n\nHistorically, coursework and curricula in engineering economics for civil engineers has focused on Capital budgeting: \"...when to replace capital equipment, and which of several alternative investments to make.\n\n"}
{"id": "3289308", "url": "https://en.wikipedia.org/wiki?curid=3289308", "title": "Evidence-based practice", "text": "Evidence-based practice\n\nEvidence-based practice (EBP) is an interdisciplinary approach to clinical practice that has been gaining ground following its formal introduction in 1992. It started in medicine as evidence-based medicine (EBM) and spread to allied health professions, educational fields, and others. EBP is traditionally defined in terms of a \"three legged stool\" integrating three basic principles: (1) the best available research evidence bearing on whether and why a treatment works, (2) clinical expertise (clinical judgment and experience) to rapidly identify each patient's unique health state and diagnosis, their individual risks and benefits of potential interventions, and (3) client preferences and values.\n\nEvidence-based behavioral practice (EBBP) \"entails making decisions about how to promote health or provide care by integrating the best available evidence with practitioner expertise and other resources, and with the characteristics, state, needs, values and preferences of those who will be affected. This is done in a manner that is compatible with the environmental and organizational context. Evidence is research findings derived from the systematic collection of data through observation and experiment and the formulation of questions and testing of hypotheses\".\n\nIn recent years, EBP has been stressed by professional organizations such as the American Psychological Association, the American Occupational Therapy Association, the American Nurses Association, and the American Physical Therapy Association, which have also strongly recommended their members to carry out investigations to provide evidence supporting or rejecting the use of specific interventions. Equivalent recommendations apply to the Canadian equivalent of these associations. Pressure toward EBP has also come from public and private health insurance providers, which have sometimes refused coverage of practices lacking in systematic evidence of usefulness.\n\nAreas of professional practice, such as medicine, psychology, psychiatry, rehabilitation and so forth, have had periods in their pasts where practice was based on loose bodies of knowledge. Some of the knowledge was lore that drew upon the experiences of generations of practitioners, and much of it had no valid scientific evidence on which to justify various practices.\n\nIn the past, this has often left the door open to quackery perpetrated by individuals who had no training at all in the domain, but who wished to convey the impression that they did, for profit or other motives. As the scientific method became increasingly recognized as the means to provide sound validation for such methods, the need for a way to exclude quack practitioners became clear, not only as a way of preserving the integrity of the field (particularly medicine), but also of protecting the public from the dangers of their \"cures.\" Furthermore, even where overt quackery was not present, it was recognized that there was a value in identifying what actually does work so it could be improved and promoted.\n\nThe notion of evidence based practice has also had an influence in the field of education. Here, some commentators have suggested that the putative lack of any conspicuous progress is attributable to practice resting in the unconnected and noncumulative experience of thousands of individual teachers, each re-inventing the wheel and failing to learn from hard scientific evidence about 'what works'. Opponents of this view argue that hard scientific evidence is a misnomer in education; knowing that a drug works (in medicine) is entirely different from knowing that a teaching method works, for the latter will depend on a host of factors, not least those to do with the style, personality and beliefs of the teacher and the needs of the particular children (Hammersley 2013). Some opponents of EBP in education suggest that teachers need to develop their own personal practice, dependent on personal knowledge garnered through their own experience. Others argue that this must be combined with research evidence, but without the latter being treated as a privileged source.\n\nEvidence-based practice is an approach that tries to specify the way in which professionals or other decision-makers should make decisions by identifying such evidence that there may be for a practice and rating it according to how scientifically sound it may be. Its goal is to eliminate unsound or excessively risky practices in favor of those that have better outcomes.\n\nEBP uses various methods (e.g., carefully summarizing research, putting out accessible research summaries, educating professionals in how to understand and apply research findings) to encourage, and in some instances to force, professionals and other decision-makers to pay more attention to evidence that can inform their decision-making. Where EBP is applied, it encourages professionals to use the best evidence possible, i.e., the most appropriate information available.\n\nThe core activities at the root of evidence-based practice can be identified as:\n\nMuch of the credit for today’s EBP techniques belongs to Archie Cochrane, an epidemiologist, author of the book, \"Effectiveness and Efficiency: Random Reflections on Health Services\". Cochrane suggested that because resources would always be limited, they should be used to provide forms of health care which had been shown in properly designed evaluations to be effective. Cochrane maintained that the most reliable evidence was that which came from randomised controlled trials (RCTs).\n\nOne of the main reasons that EBPs have been so successfully incorporated into treatment services is the vast amount of studies linking clients’ improved health outcomes and the general attitude that treatments should be based in scientific evidence (Institute of Medicine, 2001; Sackett & Haynes, 1995). It is now assumed that professionals must be well-informed and up-to-date with the newest knowledge in order to best serve their clients and remain professionally relevant (Gibbs, 2003; Pace, 2008; Patterson et al., 2012).\n\nEvidence-based practice (EBP) involves complex and conscientious decision-making which is based not only on the available evidence but also on patient characteristics, situations, and preferences. It recognizes that care is individualized and ever changing and involves uncertainties and probabilities.\n\nEBP develops individualized guidelines of best practices to inform the improvement of whatever professional task is at hand. Evidence-based practice is a philosophical approach that is in opposition to rules of thumb, folklore, and tradition. Examples of a reliance on \"the way it was always done\" can be found in almost every profession, even when those practices are contradicted by new and better information.\n\nHowever, in spite of the enthusiasm for EBP over the last decade or two, some authors have redefined EBP in ways that contradict, or at least add other factors to, the original emphasis on empirical research foundations. For example, EBP may be defined as treatment choices based not only on outcome research but also on practice wisdom (the experience of the clinician) and on family values (the preferences and assumptions of a client and his or her family or subculture).\n\nResearch oriented scientists, as opposed to authors, test whether particular practices work better for different subcultures or personality types, rather than just accept received wisdom. For example, the MATCH Study run at many sites around the US by the National Institute on Alcohol Abuse and Alcoholism (NIAAA) tested whether particular types of clients with alcohol dependence would benefit differentially from three different treatment approaches to which they were randomly assigned. The idea was not to test the approaches but the matching of clients to treatments, and though this missed the question of client choice, it did demonstrate a lack of difference between the different approaches regardless of most client characteristics, with the exception that clients with high anger scores did better with the non-confrontational Motivational Enhancement approach which has been demonstrated superior in a meta-analysis of alcohol treatment outcome research and only required four as opposed to twelve session within Project MATCH.\n\nThe theories of evidence based practice are becoming more commonplace in nursing care. Nurses who are “baccalaureate prepared are expected to seek out and collaborate with other types of nurses to demonstrate the positives of a practice that is based on evidence.” Looking at a few types of articles to examine how this type of practice has influenced the standard of care is important but rarely internally valid. None of the articles specify what their biases are. Evidence based practice has gotten its reputation by examining the reasons why any and all procedures, treatments, and medicines are given. This is important for refining practice so the goal of assuring patient safety is met.\n\nEvidence-based design and development decisions are made after reviewing information from repeated rigorous data gathering instead of relying on rules, single observations, or custom. Evidence-based medicine and evidence-based nursing practice are the two largest fields employing this approach. In psychiatry and community mental health, evidence-based practice guides have been created by such organizations as the Substance Abuse and Mental Health Services Administration and the Robert Wood Johnson Foundation, in conjunction with the National Alliance on Mental Illness. Evidence-based practice has now spread into a diverse range of areas outside of health where the same principles are known by names such as results-focused policy, managing for outcomes, evidence-informed practice etc.\n\nThis model of care has been studied for 30 years in universities and is gradually making its way into the public sector. It effectively moves away from the old “medical model” (You have a disease, take this pill.) to an “evidence presented model” using the patient as the starting point in diagnosis. EBPs are being employed in the fields of health care, juvenile justice, mental health and social services among others. The theories of evidence based practice are becoming more commonplace in the nursing care. Nurses who are “baccalaureate prepared are expected to seek out and collaborate with other types of nurses to demonstrate the positives of a practice that is based on evidence.”\n\nKey elements in using the best evidence to guide the practice of any professional include the development of questions using research-based evidence, the level and types of evidence to be used, and the assessment of effectiveness after completing the task or effort. One obvious problem with EBP in any field is the use of poor quality, contradictory, or incomplete evidence. Evidence-based practice continues to be a developing body of work for professions as diverse as education, psychology, economics, nursing, social work and architecture.\n\nEvidence-based practice of psychology requires practitioners to follow psychological approaches and techniques that are based on a particular kind of research evidence (Sackett, Straus, Richardson, Rosenberg, & Haynes, 2000).\n\nCriteria for empirically supported therapies have been defined by Chambless and Hollon (1998). Accordingly, a therapy is considered \"efficacious and specific\" if there is evidence from at least two settings that it is superior to a pill or psychological placebo or another bona fide treatment. If there is evidence from two or more settings that the therapy is superior to no treatment it is considered \"efficacious\". If there is support from one or more studies from just a single setting, the therapy is considered \"possibly efficacious\" pending replication. Following these guidelines, cognitive behavior therapy (CBT) stands out as having the most empirical support for a wide range of symptoms in adults, adolescents, and children. The term \"evidence-based practice\" is not always used in such a rigorous fashion, and many psychologists claim to follow \"evidence-based approaches\" even when the methods they use do not meet established criteria for \"efficacy\" (Berke, Rozell, Hogan, Norcross, and Karpiak, 2011). In reality, not all mental health practitioners receive training in evidence-based approaches, and members of the public are often unaware that evidence-based practices exist. However, there is no guarantee that mental health practitioners trained in \"evidence-based approaches\" are more effective or safer than those trained in other modalities. Consequently, patients do not always receive the most effective, safe, and cost effective treatments available. To improve dissemination of evidence-based practices, the Association for Behavioral and Cognitive Therapies (ABCT) and the Society of Clinical Child and Adolescent Psychology (SCCAP, Division 53 of the American Psychological Association) maintain updated information on their websites on evidence-based practices in psychology for practitioners and the general public. It should be noted that \"evidence-based\" is a technical term, and there are many treatments with decades of evidence supporting their efficacy that are not considered \"evidence-based.\"\n\nSome discussions of EBP in clinical psychology settings distinguish the latter from \"empirically supported treatments\" (ESTs). ESTs have been defined as \"clearly specified psychological treatments shown to be efficacious in controlled research with a delineated population.\" Those who distinguish EBP from ESTs highlight the greater emphasis in EBP on integrating the \"three legs\" of research evidence, clinician expertise, and client values. From the latter perspective, ESTs are understood to place primary or exclusive emphasis on the first \"leg,\" namely, research evidence.\n\nBecause conclusions about research results are made in a probabilistic manner, it is impossible to work with two simple categories of outcome research reports. Research evidence does not fall simply into \"evidence-based\" and \"non-evidence-based\" classes, but can be anywhere on a continuum from one to the other, depending on factors such as the way the study was designed and carried out. The existence of this continuum makes it necessary to think in terms of \"levels of evidence\", or categories of stronger or weaker evidence that a treatment is effective. To classify a research report as strong or weak evidence for a treatment, it is necessary to evaluate the quality of the research as well as the reported outcome.\n\nEvaluation of research quality can be a difficult task requiring meticulous reading of research reports and background information. It may not be appropriate simply to accept the conclusion reported by the researchers; for example, in one investigation of outcome studies, 70% were found to have stated conclusions unjustified by their research design.\n\nAlthough early consideration of EBP issues by psychologists provided a stringent but simple definition of EBP, requiring two independent randomized controlled trials supporting the effectiveness of a treatment, it became clear that additional factors needed to be considered. These included both the need for lower but still useful levels of evidence, and the need to require even the \"gold standard\" randomized trials to meet further criteria.\n\nA number of protocols for the evaluation of research reports have been suggested and will be summarized here. Some of these divide research evidence dichotomously into EBP and non-EBP categories, while others employ multiple levels of evidence. As the reader will see, although the criteria used by the various protocols overlap to some extent, they do not do so completely.\n\nThe Kaufman Best Practices Project approach did not use an EBP category per se, but instead provided a protocol for selecting the most acceptable treatment from a group of interventions intended to treat the same problems. To be designated as \"best practice\", a treatment would need to have a sound theoretical base, general acceptance in clinical practice, and considerable anecdotal or clinical literature. This protocol also requires absence of evidence of harm, at least one randomized controlled study, descriptive publications, a reasonable amount of necessary training, and the possibility of being used in common settings. Missing from this protocol are the possibility of nonrandomized designs (in which clients or practitioners decide whether an individual will receive a certain treatment), the need to specify the type of comparison group used, the existence of confounding variables, the reliability or validity of outcome measures, the type of statistical analysis required, or a number of other factors required by some evaluation protocols.\n\nA protocol suggested by Saunders et al. assigns research reports to six categories, on the basis of research design, theoretical background, evidence of possible harm, and general acceptance. To be classified under this protocol, there must be descriptive publications, including a manual or similar description of the intervention. This protocol does not consider the nature of any comparison group, the effect of confounding variables, the nature of the statistical analysis, or a number of other criteria. Interventions are assessed as belonging to Category 1, well-supported, efficacious treatments, if there are two or more randomized controlled outcome studies comparing the target treatment to an appropriate alternative treatment and showing a significant advantage to the target treatment. Interventions are assigned to Category 2, supported and probably efficacious treatment, based on positive outcomes of nonrandomized designs with some form of control, which may involve a non-treatment group. Category 3, supported and acceptable treatment, includes interventions supported by one controlled or uncontrolled study, or by a series of single-subject studies, or by work with a different population than the one of interest. Category 4, promising and acceptable treatment, includes interventions that have no support except general acceptance and clinical anecdotal literature; however, any evidence of possible harm excludes treatments from this category. Category 5, innovative and novel treatment, includes interventions that are not thought to be harmful, but are not widely used or discussed in the literature. Category 6, concerning treatment, is the classification for treatments that have the possibility of doing harm, as well as having unknown or inappropriate theoretical foundations.\n\nA protocol for evaluation of research quality was suggested by a report from the Centre for Reviews and Dissemination, prepared by Khan et al. and intended as a general method for assessing both medical and psychosocial interventions. While strongly encouraging the use of randomized designs, this protocol noted that such designs were useful only if they met demanding criteria, such as true randomization and concealment of the assigned treatment group from the client and from others, including the individuals assessing the outcome. The Khan et al. protocol emphasized the need to make comparisons on the basis of \"intention to treat\" in order to avoid problems related to greater attrition in one group. The Khan et al. protocol also presented demanding criteria for nonrandomized studies, including matching of groups on potential confounding variables and adequate descriptions of groups and treatments at every stage, and concealment of treatment choice from persons assessing the outcomes. This protocol did not provide a classification of levels of evidence, but included or excluded treatments from classification as evidence-based depending on whether the research met the stated standards.\n\nAn assessment protocol has been developed by the U.S. National Registry of Evidence-Based Practices and Programs (NREPP). Evaluation under this protocol occurs only if an intervention has already had one or more positive outcomes, with a probability of less than .05, reported, if these have been published in a peer-reviewed journal or an evaluation report, and if documentation such as training materials has been made available. The NREPP evaluation, which assigns quality ratings from 0 to 4 to certain criteria, examines reliability and validity of outcome measures used in the research, evidence for intervention fidelity (predictable use of the treatment in the same way every time), levels of missing data and attrition, potential confounding variables, and the appropriateness of statistical handling, including sample size.\n\nA protocol suggested by Mercer and Pignotti uses a taxonomy intended to classify on both research quality and other criteria. In this protocol, evidence-based interventions are those supported by work with randomized designs employing comparisons to established treatments, independent replications of results, blind evaluation of outcomes, and the existence of a manual. Evidence-supported interventions are those supported by nonrandomized designs, including within-subjects designs, and meeting the criteria for the previous category. Evidence-informed treatments involve case studies or interventions tested on populations other than the targeted group, without independent replications; a manual exists, and there is no evidence of harm or potential for harm. Belief-based interventions have no published research reports or reports based on composite cases; they may be based on religious or ideological principles or may claim a basis in accepted theory without an acceptable rationale; there may or may not be a manual, and there is no evidence of harm or potential for harm. Finally, the category of potentially harmful treatments includes interventions such that harmful mental or physical effects have been documented, or a manual or other source shows the potential for harm.\n\nProtocols for evaluation of research quality are still in development. So far, the available protocols pay relatively little attention to whether outcome research is relevant to efficacy (the outcome of a treatment performed under ideal conditions) or to effectiveness (the outcome of the treatment performed under ordinary, expectable conditions).\n\nA process has been specified that provides a standardised route for those seeking to produce evidence of the effectiveness of interventions. Originally developed to establish processes for the production of evidence in the housing sector, the standard is general in nature and is applicable across a variety of practice areas and potential outcomes of interest.\n\nWhen there are many small or weak studies of an intervention, a statistical meta-analysis can be used to co-ordinate the studies' results and to draw a stronger conclusion about the outcome of the treatment. This can be an important contribution to the establishment of a foundation of evidence about an intervention.\n\nIn other situations, facts about a group of study outcomes may be gathered and discussed in the form of a systematic research synthesis (SRS). A SRS can be more or less useful, depending on the evaluation protocol chosen, and errors in choice or use of a protocol have led to fallacious reports. The meaningfulness of a SRS report on an intervention is limited by the quality of the research under consideration, but SRS reports can be helpful to readers seeking to understand EBP-related choices.\n\nMiller et al. provide an excellent example and explication of the use of meta-analysis examining treatment outcome research, incorporating the principles of rigorous empirical research from the strong end of the continuum of levels of evidence. This textbook also explicates how the research included was selected (e.g. controlled study looking at two different approaches, appearing in a peer reviewed journal, sufficient power to find significant differences if they occurred) and how each study was checked for validity (how was the outcome measured?) and reliability (did the research do what they said they did?), etc. to create a Cumulative Evidence Score weighted by the quality of the study (and not by the outcome) such that better studies with \"stronger designs\" and better methodological quality ratings carry more weight than weaker studies. The results lead to a rank ordering of the 48 treatment modalities included and provide a basis for selecting supportable treatment approaches beyond anecdotes, traditions and lore.\n\nThere are increasing demands for the whole range of social policy and other decisions and programs run by government and the NGO sector to be based on sound evidence as to their effectiveness. This has seen an increased emphasis on the use of a wide range of Evaluation approaches directed at obtaining evidence about social programs of all types. A research collaboration called the Campbell Collaboration has been set up in the social policy area to provide evidence for evidence-based social policy decision-making. This collaboration follows the approach pioneered by the Cochrane Collaboration in the health sciences. Using an evidence-based approach to social policy has a number of advantages because it has the potential to decrease the tendency to run programs which are socially acceptable (e.g. drug education in schools) but which often prove to be ineffective when evaluated.\nMore recently the Alliance for Useful Evidence has been established to champion the use of evidence in social policy and practice. It is a UK-wide network that promotes the use of high quality evidence to inform decisions on strategy, policy and practice. The agency published a useful practice guide with Nesta's Innovation Skills Team on the effective use of research evidence in 2016.\n\nThe concept of Evidence-based policy and practice within international development is similarly being emphasized. For instance, in a literature review focused on development, an integrated, participatory, structured and enpowering approach to using evidence and data in decision-making to inform development decisions was tied to improved results. \n\n NREPP National Registry of Evidence-based Programs and Practices\n"}
{"id": "14698871", "url": "https://en.wikipedia.org/wiki?curid=14698871", "title": "Geography of kendo", "text": "Geography of kendo\n\nKendo originated in Japan, but is today practiced worldwide.\n\nThe size and depth of kendo skill varies widely from country to country. Some countries have few kendo practitioners, while Japan has several million.\n\nGenerally, kendo has stronger traditions in countries with strong historical ties to Japan, like Korea and Taiwan, as well as countries with large Japanese immigrant communities such as the United States, Canada and Brazil. While the term kendo is used all over the world, the term Kumdo is used in Korea.\n\nThe following international organisations administer, manage, promote, or have an interest in the development of kendo.\n\n\n\nMany national and regional organisations manage and promote kendo, some are affiliated to international kendo organisations, while other organisations are independent of international kendo organisations.\n\n\n\nKendo was introduced to Malawi in 1992 when a Japanese volunteer took on a group of local children as his students. The Kendo Association of Malawi was formed in 1999 and has seen significant growth in recent years. The Kendo Association of Malawi works closely with the Embassy of Japan in Malawi to promote kendo as a sport and to encourage cultural exchange and interaction between the peoples of the two nations. The majority of the Kendo Associations activities take place in Blantyre and Lilongwe, with some activity in Mzuzu. The Association holds regular weekly practice sessions in each city, and holds at least two local tournaments each year which are patronized by all manner of people including Embassy officials and Japanese expatriates and their families.\n\nThe major challenge that the Kendo Association of Malawi faces is that of resources, as kendo equipment tends to be expensive, and funding is often difficult to secure. However, the Embassy of Japan and well wishers have been a great help in this regard, donating all manner of equipment and resource.\n\nAlthough there is a great interest in tournaments of all levels, a challenge that continues to face kendo players in Malawi is a lack of opportunity to compete at a regional and international level. This hinders the progress and development of players and stunts the growth of the sport. It is currently not possible to test for Dan rank within Malawi, and as of March 2014 there are less than 10 Malawians who have obtained Dan rank in kendo.\n\nThe European Kendo Federation (EKF) is member of International Kendo Federation (FIK), which 35 countries/regions belong to, also promotes jodo and iaido. European kendo championships have been held since 1974. Championships are held every year that there is no world championship. Some national organisations are affiliated to EKF, while other organisations are independent of EKF.\n\n\n\n\nHawaii Kendo Federation (HKF) The Hawaii Budo Kyokai was established in 1947 (even before the All Japan Kendo Federation) and was renamed Hawaii Kendo Federation in 1955. The HKF consists of 16 dojo practicing kendo and iaido on the islands of Oahu, Hawaii, Kauai and Maui. The HKF is an affiliate organisation of the FIK.\n\n\nIn South America, the practice of Kendo has existed since the arrival of Japanese immigrants as early as 1908. Since then and with Brazil as its centre, kendo has spread over South America. Now kendo practitioners and kendo federations exist in many countries in South America such as: Brazil, Argentina, Venezuela, Colombia, Ecuador, Peru, Uruguay, Aruba and Chile.\n\nAt the December 2006 meeting of the International Kendo Federation (FIK) held in Taiwan, the South American Kendo Confederation (CSK) was discussed and voted upon, as a result the Confederation was admitted as an FIK affiliate.\n\nArgentina, Aruba, Chile, Brazil and Venezuela are affiliated with the FIK. The next South American Kendo championships will be held in Mexico during 2011.\n\nKendo in Guatemala started in 1992. The Guatemalan Kendo Association was founded in 1992. It consists of about 150 kenshi, is part of the CLAK (Latin American Kendo Confederation), and holds Kendo championships annually.\n"}
{"id": "35487375", "url": "https://en.wikipedia.org/wiki?curid=35487375", "title": "Herbert Gleiter", "text": "Herbert Gleiter\n\nHerbert Gleiter (born 13 October 1938 in Stuttgart) is a German researcher in physics and nanotechnology. \n\nIn 1966, he received his Ph.D. in physics from the University of Stuttgart in Germany. He received the Gottfried Wilhelm Leibniz Prize in 1988 for contributions to the field of nanotechnology. He became the Chair Professor of the Institute of Material Science at Saarland University, Germany in 1979. He has also held positions at Harvard University, the Massachusetts Institute of Technology, and the University of Bochum.\n\nSince 2012, he is Director and Chair Professor of the 'Herbert Gleiter Institute of Nanoscience' of 'Nanjing University of Science and Technology' of Nanjing in China.\n\n"}
{"id": "18333986", "url": "https://en.wikipedia.org/wiki?curid=18333986", "title": "Hiroshi Inoue", "text": "Hiroshi Inoue\n\nInoue's botanical publications are from Japan. He described or recognized many species of liverworts.\n\n"}
{"id": "7064233", "url": "https://en.wikipedia.org/wiki?curid=7064233", "title": "History of nanotechnology", "text": "History of nanotechnology\n\nThe history of nanotechnology traces the development of the concepts and experimental work falling under the broad category of nanotechnology. Although nanotechnology is a relatively recent development in scientific research, the development of its central concepts happened over a longer period of time. The emergence of nanotechnology in the 1980s was caused by the convergence of experimental advances such as the invention of the scanning tunneling microscope in 1981 and the discovery of fullerenes in 1985, with the elucidation and popularization of a conceptual framework for the goals of nanotechnology beginning with the 1986 publication of the book \"Engines of Creation\". The field was subject to growing public awareness and controversy in the early 2000s, with prominent debates about both its potential implications as well as the feasibility of the applications envisioned by advocates of molecular nanotechnology, and with governments moving to promote and fund research into nanotechnology. The early 2000s also saw the beginnings of commercial applications of nanotechnology, although these were limited to bulk applications of nanomaterials rather than the transformative applications envisioned by the field. .\n\nThe earliest evidence of the use and applications of nanotechnology can be traced back to carbon nanotubes, cementite nanowires found in the microstructure of wootz steel manufactured in ancient India from the time period of 600 BC and exported globally.\n\nAlthough nanoparticles are associated with modern science, they were used by artisans as far back as the ninth century in Mesopotamia for creating a glittering effect on the surface of pots.\n\nIn modern times, pottery from the Middle Ages and Renaissance often retains a distinct gold- or copper-colored metallic glitter. This luster is caused by a metallic film that was applied to the transparent surface of a glazing, which contains silver and copper nanoparticles dispersed homogeneously in the glassy matrix of the ceramic glaze. These nanoparticles are created by the artisans by adding copper and silver salts and oxides together with vinegar, ochre, and clay on the surface of previously-glazed pottery. The technique originated in the Muslim world. As Muslims were not allowed to use gold in artistic representations, they sought a way to create a similar effect without using real gold. The solution they found was using luster.\n\nThe American physicist Richard Feynman lectured, \"There's Plenty of Room at the Bottom,\" at an American Physical Society meeting at Caltech on December 29, 1959, which is often held to have provided inspiration for the field of nanotechnology. Feynman had described a process by which the ability to manipulate individual atoms and molecules might be developed, using one set of precise tools to build and operate another proportionally smaller set, so on down to the needed scale. In the course of this, he noted, scaling issues would arise from the changing magnitude of various physical phenomena: gravity would become less important, surface tension and Van der Waals attraction would become more important.\n\nAfter Feynman's death, scholars studying the historical development of nanotechnology have concluded that his actual role in catalyzing nanotechnology research was limited, based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, found that the published versions of Feynman’s talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s. This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by K. Eric Drexler in his 1986 book, \"Engines of Creation: The Coming Era of Nanotechnology\", which took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves via computer control instead of control by a human operator; and in a cover article headlined \"Nanotechnology\", published later that year in a mass-circulation science-oriented magazine, \"Omni\". Toumey’s analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.\n\nThese and other developments hint that the retroactive rediscovery of Feynman’s “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman. Feynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past.\n\nThe Japanese scientist called Norio Taniguchi of Tokyo University of Science was first to use the term \"nano-technology\" in a 1974 conference, to describe semiconductor processes such as thin film deposition and ion beam milling exhibiting characteristic control on the order of a nanometer. His definition was, \"'Nano-technology' mainly consists of the processing of, separation, consolidation, and deformation of materials by one atom or one molecule.\" However, the term was not used again until 1981 when Eric Drexler, who was unaware of Taniguchi's prior use of the term, published his first paper on nanotechnology in 1981.\n\nIn the 1980s the idea of nanotechnology as a deterministic, rather than stochastic, handling of individual atoms and molecules was conceptually explored in depth by K. Eric Drexler, who promoted the technological significance of nano-scale phenomena and devices through speeches and two influential books.\nIn 1980, Drexler encountered Feynman's provocative 1959 talk \"There's Plenty of Room at the Bottom\" while preparing his initial scientific paper on the subject, “Molecular Engineering: An approach to the development of general capabilities for molecular manipulation,” published in the \"Proceedings of the National Academy of Sciences\" in 1981. The term \"nanotechnology\" (which paralleled Taniguchi's \"nano-technology\") was independently applied by Drexler in his 1986 book \"Engines of Creation: The Coming Era of Nanotechnology\", which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity. He also first published the term \"grey goo\" to describe what might happen if a hypothetical self-replicating machine, capable of independent operation, were constructed and released. Drexler's vision of nanotechnology is often called \"Molecular Nanotechnology\" (MNT) or \"molecular manufacturing.\"\n\nHis 1991 Ph.D. work at the MIT Media Lab was the first doctoral degree on the topic of molecular nanotechnology and (after some editing) his thesis, \"Molecular Machinery and Manufacturing with Applications to Computation,\" was published as \"Nanosystems: Molecular Machinery, Manufacturing, and Computation,\" which received the Association of American Publishers award for Best Computer Science Book of 1992. Drexler founded the Foresight Institute in 1986 with the mission of \"Preparing for nanotechnology.” Drexler is no longer a member of the Foresight Institute.\n\nNanotechnology and nanoscience got a boost in the early 1980s with two major developments: the birth of cluster science and the invention of the scanning tunneling microscope (STM). These developments led to the discovery of fullerenes in 1985 and the structural assignment of carbon nanotubes a few years later\n\nThe scanning tunneling microscope, an instrument for imaging surfaces at the atomic level, was developed in 1981 by Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory, for which they were awarded the Nobel Prize in Physics in 1986. Binnig, Calvin Quate and Christoph Gerber invented the first atomic force microscope in 1986. The first commercially available atomic force microscope was introduced in 1989.\n\nIBM researcher Don Eigler was the first to manipulate atoms using a scanning tunneling microscope in 1989. He used 35 Xenon atoms to spell out the IBM logo. He shared the 2010 Kavli Prize in Nanoscience for this work.\n\nInterface and colloid science had existed for nearly a century before they became associated with nanotechnology. The first observations and size measurements of nanoparticles had been made during the first decade of the 20th century by Richard Adolf Zsigmondy, winner of the 1925 Nobel Prize in Chemistry, who made a detailed study of gold sols and other nanomaterials with sizes down to 10 nm using an ultramicroscope which was capable of visualizing particles much smaller than the light wavelength. Zsigmondy was also the first to use the term \"nanometer\" explicitly for characterizing particle size. In the 1920s, Irving Langmuir, winner of the 1932 Nobel Prize in Chemistry, and Katharine B. Blodgett introduced the concept of a monolayer, a layer of material one molecule thick. In the early 1950s, Derjaguin and Abrikosova conducted the first measurement of surface forces.\n\nIn 1974 the process of atomic layer deposition for depositing uniform thin films one atomic layer at a time was developed and patented by Tuomo Suntola and co-workers in Finland.\n\nIn another development, the synthesis and properties of semiconductor nanocrystals were studied. This led to a fast increasing number of semiconductor nanoparticles of quantum dots.\n\nFullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry. Smalley's research in physical chemistry investigated formation of inorganic and semiconductor clusters using pulsed molecular beams and time of flight mass spectrometry. As a consequence of this expertise, Curl introduced him to Kroto in order to investigate a question about the constituents of astronomical dust. These are carbon rich grains expelled by old stars such as R Corona Borealis. The result of this collaboration was the discovery of C and the fullerenes as the third allotropic form of carbon. Subsequent discoveries included the endohedral fullerenes, and the larger family of fullerenes the following year.\n\nThe discovery of carbon nanotubes is largely attributed to Sumio Iijima of NEC in 1991, although carbon nanotubes have been produced and observed under a variety of conditions prior to 1991. Iijima's discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods in 1991 and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, then they would exhibit remarkable conducting properties helped create the initial buzz that is now associated with carbon nanotubes. Nanotube research accelerated greatly following the independent discoveries by Bethune at IBM and Iijima at NEC of \"single-walled\" carbon nanotubes and methods to specifically produce them by adding transition-metal catalysts to the carbon in an arc discharge.\n\nIn the early 1990s Huffman and Kraetschmer, of the University of Arizona, discovered how to synthesize and purify large quantities of fullerenes. This opened the door to their characterization and functionalization by hundreds of investigators in government and industrial laboratories. Shortly after, rubidium doped C was found to be a mid temperature (Tc = 32 K) superconductor. At a meeting of the Materials Research Society in 1992, Dr. T. Ebbesen (NEC) described to a spellbound audience his discovery and characterization of carbon nanotubes. This event sent those in attendance and others downwind of his presentation into their laboratories to reproduce and push those discoveries forward. Using the same or similar tools as those used by Huffman and Kratschmer, hundreds of researchers further developed the field of nanotube-based nanotechnology.\n\nThe National Nanotechnology Initiative is a United States federal nanotechnology research and development program. “The NNI serves as the central point of communication, cooperation, and collaboration for all Federal agencies engaged in nanotechnology research, bringing together the expertise needed to advance this broad and complex field.\" Its goals are to advance a world-class nanotechnology research and development (R&D) program, foster the transfer of new technologies into products for commercial and public benefit, develop and sustain educational resources, a skilled workforce, and the supporting infrastructure and tools to advance nanotechnology, and support responsible development of nanotechnology. The initiative was spearheaded by Mihail Roco, who formally proposed the National Nanotechnology Initiative to the Office of Science and Technology Policy during the Clinton administration in 1999, and was a key architect in its development. He is currently the Senior Advisor for Nanotechnology at the National Science Foundation, as well as the founding chair of the National Science and Technology Council subcommittee on Nanoscale Science, Engineering and Technology.\n\nPresident Bill Clinton advocated nanotechnology development. In a 21 January 2000 speech at the California Institute of Technology, Clinton said, \"Some of our research goals may take twenty or more years to achieve, but that is precisely why there is an important role for the federal government.\" Feynman's stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, as mentioned in President Clinton's speech:\n\nPresident George W. Bush further increased funding for nanotechnology. On December 3, 2003 Bush signed into law the 21st Century Nanotechnology Research and Development Act, which authorizes expenditures for five of the participating agencies totaling US$3.63 billion over four years. The NNI budget supplement for Fiscal Year 2009 provides $1.5 billion to the NNI, reflecting steady growth in the nanotechnology investment.\n\n\"Why the future doesn't need us\" is an article written by Bill Joy, then Chief Scientist at Sun Microsystems, in the April 2000 issue of \"Wired\" magazine. In the article, he argues that \"Our most powerful 21st-century technologies — robotics, genetic engineering, and nanotech — are threatening to make humans an endangered species.\" Joy argues that developing technologies provide a much greater danger to humanity than any technology before it has ever presented. In particular, he focuses on genetics, nanotechnology and robotics. He argues that 20th-century technologies of destruction, such as the nuclear bomb, were limited to large governments, due to the complexity and cost of such devices, as well as the difficulty in acquiring the required materials. He also voices concern about increasing computer power. His worry is that computers will eventually become more intelligent than we are, leading to such dystopian scenarios as robot rebellion. He notably quotes the Unabomber on this topic. After the publication of the article, Bill Joy suggested assessing technologies to gauge their implicit dangers, as well as having scientists refuse to work on technologies that have the potential to cause harm.\n\nIn the AAAS Science and Technology Policy Yearbook 2001 article titled \"A Response to Bill Joy and the Doom-and-Gloom Technofuturists\", Bill Joy was criticized for having technological tunnel vision on his prediction, by failing to consider social factors. In Ray Kurzweil's \"The Singularity Is Near\", he questioned the regulation of potentially dangerous technology, asking \"Should we tell the millions of people afflicted with cancer and other devastating conditions that we are canceling the development of all bioengineered treatments because there is a risk that these same technologies may someday be used for malevolent purposes?\".\n\n\"Prey\" is a 2002 novel by Michael Crichton which features an artificial swarm of nanorobots which develop intelligence and threaten their human inventors. The novel generated concern within the nanotechnology community that the novel could negatively affect public perception of nanotechnology by creating fear of a similar scenario in real life.\n\nRichard Smalley, best known for co-discovering the soccer ball-shaped “buckyball” molecule and a leading advocate of nanotechnology and its many applications, was an outspoken critic of the idea of molecular assemblers, as advocated by Eric Drexler. In 2001 he introduced scientific objections to them attacking the notion of universal assemblers in a 2001 \"Scientific American\" article, leading to a rebuttal later that year from Drexler and colleagues, and eventually to an exchange of open letters in 2003.\n\nSmalley criticized Drexler's work on nanotechnology as naive, arguing that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Smalley believed that such assemblers were not physically possible and introduced scientific objections to them. His two principal technical objections, which he had termed the “fat fingers problem\" and the \"sticky fingers problem”, argued against the feasibility of molecular assemblers being able to precisely select and place individual atoms. He also believed that Drexler’s speculations about apocalyptic dangers of molecular assemblers threaten the public support for development of nanotechnology.\n\nSmalley first argued that \"fat fingers\" made MNT impossible. He later argued that nanomachines would have to resemble chemical enzymes more than Drexler's assemblers and could only work in water. He believed these would exclude the possibility of \"molecular assemblers\" that worked by precision picking and placing of individual atoms. Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible.\n\nSmalley also believed that Drexler's speculations about apocalyptic dangers of self-replicating machines that have been equated with \"molecular assemblers\" would threaten the public support for development of nanotechnology. To address the debate between Drexler and Smalley regarding molecular assemblers \"Chemical & Engineering News\" published a point-counterpoint consisting of an exchange of letters that addressed the issues.\n\nDrexler and coworkers responded to these two issues in a 2001 publication. Drexler and colleagues noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in \"Nanosystems\". Drexler maintained that both were straw man arguments, and in the case of enzymes, Prof. Klibanov wrote in 1994, \"...using an enzyme in organic solvents eliminates several obstacles...\" Drexler also addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. Drexler had difficulty in getting Smalley to respond, but in December 2003, \"Chemical & Engineering News\" carried a 4-part debate.\n\nRay Kurzweil spends four pages in his book 'The Singularity Is Near' to showing that Richard Smalley's arguments are not valid, and disputing them point by point. Kurzweil ends by stating that Drexler's visions are very practicable and even happening already.\n\nThe Royal Society and Royal Academy of Engineering's 2004 report on the implications of nanoscience and nanotechnologies was inspired by Prince Charles' concerns about nanotechnology, including molecular manufacturing. However, the report spent almost no time on molecular manufacturing. In fact, the word \"Drexler\" appears only once in the body of the report (in passing), and \"molecular manufacturing\" or \"molecular nanotechnology\" not at all. The report covers various risks of nanoscale technologies, such as nanoparticle toxicology. It also provides a useful overview of several nanoscale fields. The report contains an annex (appendix) on grey goo, which cites a weaker variation of Richard Smalley's contested argument against molecular manufacturing. It concludes that there is no evidence that autonomous, self replicating nanomachines will be developed in the foreseeable future, and suggests that regulators should be more concerned with issues of nanoparticle toxicology.\n\nThe early 2000s saw the beginnings of the use of nanotechnology in commercial products, although most applications are limited to the bulk use of passive nanomaterials. Examples include titanium dioxide and zinc oxide nanoparticles in sunscreen, cosmetics and some food products; silver nanoparticles in food packaging, clothing, disinfectants and household appliances such as Silver Nano; carbon nanotubes for stain-resistant textiles; and cerium oxide as a fuel catalyst. As of March 10, 2011, the Project on Emerging Nanotechnologies estimated that over 1300 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.\n\nThe National Science Foundation funded researcher David Berube to study the field of nanotechnology. His findings are published in the monograph Nano-Hype: The Truth Behind the Nanotechnology Buzz. This study concludes that much of what is sold as “nanotechnology” is in fact a recasting of straightforward materials science, which is leading to a “nanotech industry built solely on selling nanotubes, nanowires, and the like” which will “end up with a few suppliers selling low margin products in huge volumes.\" Further applications which require actual manipulation or arrangement of nanoscale components await further research. Though technologies branded with the term 'nano' are sometimes little related to and fall far short of the most ambitious and transformative technological goals of the sort in molecular manufacturing proposals, the term still connotes such ideas. According to Berube, there may be a danger that a \"nano bubble\" will form, or is forming already, from the use of the term by scientists and entrepreneurs to garner funding, regardless of interest in the transformative possibilities of more ambitious and far-sighted work.\n\n\n"}
{"id": "35223708", "url": "https://en.wikipedia.org/wiki?curid=35223708", "title": "History of the center of the Universe", "text": "History of the center of the Universe\n\nThe center of the Universe is a concept that lacks a coherent definition in modern astronomy; according to standard cosmological theories on the shape of the universe, it has no center.\n\nHistorically, the center of the Universe had been believed to be a number of locations. Many mythological cosmologies included an \"axis mundi\", the central axis of a flat Earth that connects the Earth, heavens, and other realms together. In the 4th century BCE Greece, the geocentric model was developed based on astronomical observation, proposing that the center of the Universe lies at the center of a spherical, stationary Earth, around which the sun, moon, planets, and stars rotate. With the development of the heliocentric model by Nicolaus Copernicus in the 16th century, the sun was believed to be the center of the Universe, with the planets (including Earth) and stars orbiting it.\n\nIn the early 20th century, the discovery of other galaxies and the development of the Big Bang theory led to the development of cosmological models of a homogeneous, isotropic Universe (which lacks a central point) that is expanding at all points.\n\nIn religion or mythology, the \"axis mundi\" (also cosmic axis, world axis, world pillar, columna cerului, center of the world) is a point described as the center of the world, the connection between it and Heaven, or both.\n\nMount Hermon was regarded as the axis mundi in Caananite tradition, from where the sons of God are introduced descending in 1 Enoch (1En6:6). The ancient Greeks regarded several sites as places of earth's \"omphalos\" (navel) stone, notably the oracle at Delphi, while still maintaining a belief in a cosmic world tree and in Mount Olympus as the abode of the gods. Judaism has the Temple Mount and Mount Sinai, Christianity has the Mount of Olives and Calvary, Islam has Mecca, said to be the place on earth that was created first, and the Temple Mount (Dome of the Rock). In Shinto, the Ise Shrine is the omphalos. In addition to the Kun Lun Mountains, where it is believed the peach tree of immortality is located, the Chinese folk religion recognizes four other specific mountains as pillars of the world.\n\nSacred places constitute world centers (omphalos) with the altar or place of prayer as the axis. Altars, incense sticks, candles and torches form the axis by sending a column of smoke, and prayer, toward heaven. The architecture of sacred places often reflects this role. \"Every temple or palace--and by extension, every sacred city or royal residence--is a Sacred Mountain, thus becoming a Centre.\" The stupa of Hinduism, and later Buddhism, reflects Mount Meru. Cathedrals are laid out in the form of a cross, with the vertical bar representing the union of earth and heaven as the horizontal bars represent union of people to one another, with the altar at the intersection. Pagoda structures in Asian temples take the form of a stairway linking earth and heaven. A steeple in a church or a minaret in a mosque also serve as connections of earth and heaven. Structures such as the maypole, derived from the Saxons' Irminsul, and the totem pole among indigenous peoples of the Americas also represent world axes. The calumet, or sacred pipe, represents a column of smoke (the soul) rising form a world center. A mandala creates a world center within the boundaries of its two-dimensional space analogous to that created in three-dimensional space by a shrine.\n\nIn medieval times some Christians thought of Jerusalem as the center of the world (Latin: \"umbilicus mundi\", Greek: \"Omphalos\"), and was so represented in the so-called T and O maps. Byzantine hymns speak of the Cross being \"planted in the center of the earth.\"\n\nThe Flat Earth model is a belief that the Earth's shape is a plane or disk covered by a firmament containing heavenly bodies. Most pre-scientific cultures have had conceptions of a Flat Earth, including Greece until the classical period, the Bronze Age and Iron Age civilizations of the Near East until the Hellenistic period, India until the Gupta period (early centuries AD) and China until the 17th century. It was also typically held in the aboriginal cultures of the Americas, and a flat Earth domed by the firmament in the shape of an inverted bowl is common in pre-scientific societies.\n\n\"Center\" is well-defined in a Flat Earth model. A flat earth would have a definite geographic center. There would also be a unique point at the exact center of a spherical firmament (or a firmament that was a half-sphere).\n\nThe Flat Earth model gave way to an understanding of a Spherical Earth. Aristotle (384–322 BCE) provided observational arguments supporting the idea of a spherical Earth, namely that different stars are visible in different locations, travelers going south see southern constellations rise higher above the horizon, and the shadow of Earth on the Moon during a lunar eclipse is round, and spheres cast circular shadows while discs generally do not.\n\nThis understanding was accompanied by models of the Universe that depicted the Sun, Moon, stars, and naked eye planets circling the spherical Earth, including the noteworthy models of Aristotle (see Aristotelian physics) and Ptolemy. This geocentric model was the dominant model from the 4th century BCE until the 17th century CE.\n\nHeliocentrism, or heliocentricism, is the astronomical model in which the Earth and planets revolve around a relatively stationary Sun at the center of our Solar System. The word comes from the Greek ( \"helios\" \"sun\" and \"kentron\" \"center\").\n\nThe notion that the Earth revolves around the Sun had been proposed as early as the 3rd century BCE by Aristarchus of Samos, but had received no support from most other ancient astronomers.\n\nNicolaus Copernicus' major theory of a heliocentric model was published in \"De revolutionibus orbium coelestium\" (\"On the Revolutions of the Celestial Spheres\"), in 1543, the year of his death, though he had formulated the theory several decades earlier. Copernicus' ideas were not immediately accepted, but they did begin a paradigm shift away from the Ptolemaic geocentric model to a heliocentric model. The Copernican revolution, as this paradigm shift would come to be called, would last until Isaac Newton’s work over a century later.\n\nJohannes Kepler published his first two laws about planetary motion in 1609, having found them by analyzing the astronomical observations of Tycho Brahe. Kepler's third law was published in 1619. The first law was \"The orbit of every planet is an ellipse with the Sun at one of the two foci.\"\n\nOn 7 January 1610 Galileo used his telescope, with optics superior to what had been available before. He described \"three fixed stars, totally invisible by their smallness\", all close to Jupiter, and lying on a straight line through it. Observations on subsequent nights showed that the positions of these \"stars\" relative to Jupiter were changing in a way that would have been inexplicable if they had really been fixed stars. On 10 January Galileo noted that one of them had disappeared, an observation which he attributed to its being hidden behind Jupiter. Within a few days he concluded that they were orbiting Jupiter: Galileo stated that he had reached this conclusion on 11 January. He had discovered three of Jupiter's four largest satellites (moons). He discovered the fourth on 13 January.\n\nHis observations of the satellites of Jupiter created a revolution in astronomy: a planet with smaller planets orbiting it did not conform to the principles of Aristotelian Cosmology, which held that all heavenly bodies should circle the Earth, and many astronomers and philosophers initially refused to believe that Galileo could have discovered such a thing.\n\nNewton made clear his heliocentric view of the solar system – developed in a somewhat modern way, because already in the mid-1680s he recognised the \"deviation of the Sun\" from the centre of gravity of the solar system. For Newton, it was not precisely the centre of the Sun or any other body that could be considered at rest, but rather \"the common centre of gravity of the Earth, the Sun and all the Planets is to be esteem'd the Centre of the World\", and this centre of gravity \"either is at rest or moves uniformly forward in a right line\" (Newton adopted the \"at rest\" alternative in view of common consent that the centre, wherever it was, was at rest).\n\nBefore the 1920s, it was generally believed that there were no galaxies other than our own (see for example The Great Debate). Thus, to astronomers of previous centuries, there was no distinction between a hypothetical center of the galaxy and a hypothetical center of the universe.\nIn 1750 Thomas Wright, in his work \"An original theory or new hypothesis of the Universe\", correctly speculated that the Milky Way might be a body of a huge number of stars held together by gravitational forces rotating about a Galactic Center, akin to the solar system but on a much larger scale. The resulting disk of stars can be seen as a band on the sky from our perspective inside the disk. In a treatise in 1755, Immanuel Kant elaborated on Wright's idea about the structure of the Milky Way.\n\nThe 19th century astronomer Johann Heinrich von Mädler proposed the Central Sun Hypothesis, according to which the stars of the universe revolved around a point in the Pleiades.\n\nIn 1917, Heber Doust Curtis observed a nova within what then was called the \"Andromeda Nebula\". Searching the photographic record, 11 more novae were discovered. Curtis noticed that novas in Andromeda were drastically fainter than novas in the Milky Way. Based on this, Curtis was able to estimate that Andromeda was 500,000 light-years away. As a result, Curtis became a proponent of the so-called \"island Universes\" hypothesis, which held that objects previously believed to be spiral nebulae within the Milky Way were actually independent galaxies.\n\nIn 1920, the Great Debate between Harlow Shapley and Curtis took place, concerning the nature of the Milky Way, spiral nebulae, and the dimensions of the Universe. To support his claim that the Great Andromeda Nebula (M31) was an external galaxy, Curtis also noted the appearance of dark lanes resembling the dust clouds in our own Galaxy, as well as the significant Doppler shift. In 1922 Ernst Öpik presented a very elegant and simple astrophysical method to estimate the distance of M31. His result put the Andromeda Nebula far outside our Galaxy at a distance of about 450,000 parsec, which is about 1,500,000 ly. Edwin Hubble settled the debate about whether other galaxies exist in 1925 when he identified extragalactic Cepheid variable stars for the first time on astronomical photos of M31. These were made using the 2.5 metre (100 in) Hooker telescope, and they enabled the distance of Great Andromeda Nebula to be determined. His measurement demonstrated conclusively that this feature was not a cluster of stars and gas within our Galaxy, but an entirely separate galaxy located a significant distance from our own. This proved the existence of other galaxies.\n\nHubble also demonstrated that the redshift of other galaxies is approximately proportional to their distance from the Earth (Hubble's law). This raised the appearance of our galaxy being in the center of an expanding Universe, however, Hubble rejected the findings philosophically:\n\nThe redshift observations of Hubble, in which galaxies appear to be moving away from us at a rate proportional to their distance from us, are now understood to be a result of the metric expansion of space. This is the increase of the distance between two distant parts of the Universe with time, and is an intrinsic expansion whereby the scale of space itself changes. As Hubble theorized, all observers anywhere in the Universe will observe a similar effect.\n\nThe Copernican principle, named after Nicolaus Copernicus, states that the Earth is not in a central, specially favored position. Hermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the geocentric Ptolemaic system.\n\nThe cosmological principle is an extension of the Copernican principle which states that the Universe is homogeneous (the same observational evidence is available to observers at different locations in the Universe) and isotropic (the same observational evidence is available by looking in any direction in the Universe). A homogeneous, isotropic Universe does not have a center.\n"}
{"id": "28202032", "url": "https://en.wikipedia.org/wiki?curid=28202032", "title": "Hybrid genome assembly", "text": "Hybrid genome assembly\n\nIn bioinformatics, hybrid genome assembly refers to utilizing various sequencing technologies to achieve the task of assembling a genome from fragmented, sequenced DNA resulting from shotgun sequencing. Genome assembly presents one of the most challenging tasks in genome sequencing as most modern DNA sequencing technologies can only produce reads that are, on average, 25-300 base pairs in length. This is magnitudes smaller than the average size of a genome (the genome of the octoploid plant \"Paris japonica\" is 149 billion base pairs ). This assembly is computationally difficult and has some inherent challenges, one of these challenges being that genomes often contain complex tandem repeats of sequences that can be thousands of base pairs in length. These repeats can be long enough that second generation sequencing reads are not long enough to bridge the repeat, and, as such, determining the location of each repeat in the genome can be difficult. Resolving these tandem repeats can be accomplished by utilizing long third generation sequencing reads, such as those obtained using the PacBio RS DNA sequencer. These sequences are, on average, 10,000-15,000 base pairs in length and are long enough to span most repeated regions. Using a hybrid approach to this process can increase the fidelity of assembling tandem repeats by being able to accurately place them along a linear scaffold and make the process more computationally efficient.\n\nThe term genome assembly refers to the process of taking a large number of DNA fragments that are generated during shotgun sequencing and assembling them into the correct order such as to reconstruct the original genome. Sequencing involves using automated machines to determine the order of nucleic acids in the DNA of interest (the nucleic acids in DNA are adenine, cytosine, guanine and thymine) to conduct genomic analyses involving an organism of interest. The advent of next generation sequencing has presented significant improvements in the speed, accuracy and cost of DNA sequencing and has made the sequencing of entire genomes a feasible process. There are many different sequencing technologies that have been developed by various biotechnology companies, each of which produce different sequencing reads in terms of accuracy and read length. Some of these technologies include Roche 454, Illumina, SOLiD, and IonTorrent. These sequencing technologies produce relatively short reads (50-700 bases) and have a high accuracy (>98%). Third generation sequencing include technologies as the PacBio RS system which can produce long reads (maximum of 23kb) but have a relatively low accuracy.\nGenome assembly is normally done by one of two methods: assembly using a reference genome as a scaffold, or \"de novo\" assembly. The scaffolding approach can be useful if the genome of a similar organism has been previously sequenced. This process involves assembling the genome of interest by comparing it to a known genome or scaffold. \"De novo\" genome assembly is used when the genome to be assembled is not similar to any other organisms whose genomes have been previously sequenced. This process is carried out by assembling single reads into contiguous sequences (contigs) which are then extended in the 3’ and 5’ directions by overlapping other sequences. The latter is preferred because it allows for the conservation of more sequences.\nThe \"de novo\" assembly of DNA sequences is a very computationally challenging process and can fall into the NP-hard class of problems if the Hamiltonian-cycle approach is used. This is because millions of sequences must be assembled to reconstruct a genome. Within genomes, there are often tandem repeats of DNA segments that can be thousands of base pairs in length, which can cause problems during assembly.\nAlthough next generation sequencing technology is now capable of producing millions of reads, the assembly of these reads can cause a bottleneck in the entire genome assembly process. As such, extensive research is being done to develop new techniques and algorithms to streamline the genome assembly process and make it a more computationally efficient process and to increase the accuracy of the process as a whole.\n\nOne hybrid approach to genome assembly involves supplementing short, accurate second-generation sequencing data (i.e. from IonTorrent, Illumina or Roche 454) with long less accurate third generation sequencing data (i.e. from PacBio RS) to resolve complex repeated DNA segments. The main limitation of single-molecule third-generation sequencing that prevents it from being used alone is its relatively low accuracy, which causes inherent errors in the sequenced DNA. Using solely second-generation sequencing technologies for genome assembly can miss or lead to the incomplete assembly of important aspects of the genome. Supplementation of third generation reads with short, high-accuracy second generation sequences can overcome these inherent errors and completed crucial details of the genome. This approach has been used to sequence the genomes of some bacterial species including a strain of \"Vibrio cholerae\". Algorithms specific for this type of hybrid genome assembly have been developed, such as the PacBio corrected Reads algorithm.\n\nThere are inherent challenges when utilizing sequence reads from various technologies to assemble a sequenced genome; data coming from different sequencers can have different characteristics. An example of this can be seen when using the overlap-layout-consensus (OLC) method of genome assembly, which can be difficult when using reads of substantially different lengths. Currently, this challenge is being overcome by using multiple genome assembly programs. An example of this can be seen in Goldberg et al. where the authors paired 454 reads with Sanger reads. The 454 reads were first assemble using the Newbler assembler (which is optimized to use short reads) generating pseudo reads that were then paired with the longer Sanger reads and assembled using the Celera assembler.\n\nHybrid genome assembly can also be accomplished using the Eulerian path approach. In this approach, the length of the assembled sequences does not matter as once a k-mer spectrum has been constructed, the lengths of the reads are irrelevant.\n\nThe authors of this study developed a correction algorithm called the PacBio corrected Reads (PBcR) algorithm which is implemented as part of the Celera assembly program. This algorithm calculates an accurate hybrid consensus sequence by mapping higher accuracy short reads (from second generation sequencing technologies) to individual lower accuracy long reads (from third generation sequencing technologies). This mapping allows for trimming and correction of the long reads to improve the read accuracy from as low as 80% to over 99.9%. In the best example of this application from this paper, the contig size was quintupled when compared to the assemblies using only second-generation reads.\n\nThis study offers an improvement over the typical programs and algorithms used to assemble uncorrected PacBio reads. ALLPATHS-LG (another program that can assemble PacBio reads) uses the uncorrected PacBio reads to assist in scaffolding and for the closing of gaps in short sequence assemblies. Due to computational limitations, this approach limits assembly to relatively small genomes (maximum of 10Mbp). The PBcR algorithm allows for the assembly of much larger genomes with higher fidelity and using uncorrected PacBio reads.\n\nThis study also shows that using a lower coverage of corrected long reads is similar to using a higher coverage of shorter reads; 13x PBcR data (corrected using 50x Illumina data) was comparable to an assembly constructed using 100x paired-end Illumina reads. The N50 for the corrected PBcR data was also longer than the Illumina data (4.65MBp compared to 3.32 Mbp for the Illumina reads). A similar trend was seen in the sequencing of the \"Escherichia coli\" JM221 genome: a 25x PBcR assembly had a N50 triple that of 50x 454 assembly.\n\nThis study employed two different methods for hybrid genome assembly: a scaffolding approach that supplemented currently available sequenced contigs with PacBio reads, as well as an error correction approach to improve the assembly of bacterial genomes. The first approach in this study started with high-quality contigs constructed from sequencing reads from second-generation (Illumina and 454) technology. These contigs were supplemented by aligning them to PacBio long reads to achieve linear scaffolds that were gap-filled using PacBio long reads. These scaffolds were then supplemented again, but using PacBio strobe reads (multiple subreads from a single contiguous fragment of DNA ) to achieve a final, high-quality assembly. This approach was used to sequence the genome of a strain of \"Vibrio cholerae\" that was responsible for a cholera outbreak in Haiti.\n\nThis study also used a hybrid approach to error-correction of PacBio sequencing data. This was done by utilizing high-coverage Illumina short reads to correct errors in the low-coverage PacBio reads. BLASR (a long read aligner from PacBio) was used in this process. In areas where the Illumina reads could be mapped, a consensus sequence was constructed using overlapping reads in that region.\n\nOne area of the genome where the use of the long PacBio reads was especially helpful was the ribosomal operon. This region is usually greater than 5kb in size and occurs seven time throughout the genome with an average identity ranging from 98.04% to 99.94%. Resolving these regions using only short second generation reads would be very difficult but the use of long third generation reads makes the process much more efficient. Utilization of the PacBio reads allowed for unambiguous placement of the complex repeated along the scaffold.\n\nThis study employs a hybrid genome assembly approach that only uses sequencing reads generated using SOLiD sequencing (a second-generation sequencing technology). The genome of \"C. pseudotuberculosis\" was assembled twice: once using a classical reference genome approach, and once using a hybrid approach. The hybrid approach consisted of three contiguous steps. Firstly, contigs were generated de novo, secondly, the contigs were ordered and concatenated into supercontigs, and, thirdly, the gaps between contigs were closed using an iterative approach. The initial de novo assembly of contigs was achieved in parallel using Velvet, which assembles contigs by manipulating De Bruijn graphs, and Edena, which is an OLC-based assembler\n\nComparing the assembly constructed using the hybrid approach to the assembly created using the traditional reference genome approach showed that, with the availability of a reference genome, it is more beneficial to utilize an hybrid de novo assembly strategy as it preserves more genome sequences.\n\nThe authors of this paper present Cerulean, a hybrid genome assembly program that differs from traditional hybrid assembly approaches. Normally, hybrid assembly involved mapping short high quality reads to long low quality reads, but this still introduces errors in the assembled genomes. This process is also computationally expensive and require a large amount of running time, even for relatively small bacterial genomes.\n\nCerulean, unlike other hybrid assembly approaches, doesn’t use the short reads directly, instead it uses an assembly graph that is created in a similar manner to the OLC method or the De Bruijn method. This graph is used to assemble a skeleton graph, which only uses long contigs with the edges of the graph representing the putative genomic connection between the contigs. The skeleton graph is a simplified version of a typical De Bruijn graph, which means that unambiguous assembly using the skeleton graph is more favourable than traditional methods.\n\nThis method was tested by assembling the genome of an ‘’Escherichia coli’’ strain. First, short reads were assembled using the ABySS assembler. These reads were then mapped to the long reads using BLASR. The results from the ABySS assembly were used to create the assembly graph, which were used to generate scaffolds using the filtered BLASR data .\nThe advantages of cerulean are that it requires minimal resources and results in assembled scaffolds with high accuracy. These characteristics make it better suited for up-scaling to be used on larger eukaryotic genomes, but the efficiency of cerulean when applied to larger genomes remains to be verified.\n\nThe current challenges in genome assembly are related to the limitation of modern sequencing technologies. Advances in sequencing technology aim to develop systems that are able to produce long sequencing reads with very high fidelity but, at this point, these two things are mutually exclusive. The advent of third-generation sequencing technology is expanding the limits of genomic research as the cost of generating high quality sequencing data is decreasing.\n\nThe idea of using multiple sequencing technologies to facilitate genome assembly may become an idea of the past as the quality of long sequencing reads (hundreds or thousands of base pairs) approaches and exceeds the quality of current second generation sequencing reads. The computational difficulties that are encountered during genome assembly will also become a concept of the past as computation efficiency and performance increases. The development of more efficient sequencing algorithms and assembly programs is needed to develop more effective assembly approaches that can tandemly incorporate sequencing reads from multiple technologies.\n\nMany of the current limitations in genomic research revolve around the ability to produce large amounts of high quality sequencing data and to assemble entire genomes of organisms of interest. Developing more effective hybrid genome assembly strategies is taking the next step in advancing sequence assembly technology and these strategies are guaranteed to become more effective as more powerful technologies emerge. \nHybrid Error Correction and De Novo Assembly of Single-Molecule Sequencing Reads\n\nVirtual Poster: Hybrid Genome Assembly of a Nocturnal Lemur\n\nNational Center for Biotechnology Information: Genome Assembly\n"}
{"id": "38838781", "url": "https://en.wikipedia.org/wiki?curid=38838781", "title": "International Council for Scientific and Technical Information", "text": "International Council for Scientific and Technical Information\n\nThe International Council for Scientific and Technical Information (ICSTI) aims to promote cooperation among all those engaged in the scientific communication process by engaging national scientific unions and their respective scientific communities. It is a broad-based, international, not-for-profit membership organization based in Paris, France. \n\nThe highest authority is the General Assembly consisting of representatives of all members, which convenes once a year. There are normally two meetings in Europe followed by one in North America or another region. \n\nThe organization is led by an Executive Board which has all power to carry on the business of ICSTI between meetings of the General Assembly. Jan Brase (TIB - DataCite) is the present President for the 2013-2016 triennium .\n\nRecent meetings of ICSTI have covered the topics of:\n\n\"Non-Textual Information - Strategy and Innovation Beyond Text\", held on 16-17 March 2013 in Hannover, Germany, hosted by the German National Library of Science and Technology - TIB\n\n\"Science, Ethics and the Law\" ’, held on 15-16 October 2012 in Washington DC, hosted by the Library of Congress\n\n\"Delivering Data in Science\", held on 3-4 March 2012 in Paris, France, at the Headquarters of ICSU - the International Council for Science\n\n\"Upgrading Information to Knowledge\", held on 7-8 June 2011 in Beijing, China, hosted by ISTIC, the Institute of Scientific and Technical Information of China\n\n"}
{"id": "495454", "url": "https://en.wikipedia.org/wiki?curid=495454", "title": "International Union of Radio Science", "text": "International Union of Radio Science\n\nThe International Union of Radio Science (abbreviated URSI, after its French name, ) is one of 26 international scientific unions affiliated to the International Council for Science (ICSU).\n\nURSI was officially created in 1919, during the Constitutive Assembly of the International Research Council (now ICSU), based on the earlier (1913–1914) when the only radio communication system was radiotelegraphy. It has held a general assembly every three years from 1922. Fifty years ago URSI was one of the most important promoters of the International Geophysical Year. It co-sponsors the \"Radio Science\" journal (co-sponsored by the American Geophysical Union) as well as the \"Journal of Atmospheric and Solar-Terrestrial Physics\".\n\nURSI's original objective (to encourage \"scientific studies of radiotelegraphy, especially those which require international cooperation\") has been broadened to include all radio science, from telecommunications to radio astronomy, acquisition of radar information about distant passive objects, studies of the radiation stimulated or spontaneously emitted by these objects, biological effects of electromagnetic radiation and active modification of objects by radio waves, within the spectrum from extremely low frequency to the optical domain.\n\n\nA few Commissions are engaged with international projects in cooperation with other international bodies, for example with the Committee on Space Research in the project International Reference Ionosphere.\n\n\n"}
{"id": "4756952", "url": "https://en.wikipedia.org/wiki?curid=4756952", "title": "Jan Tesánek", "text": "Jan Tesánek\n\nJan Tesánek () (1728–1788) was a Bohemian scholar and author of scientific literature.\n\nTesánek studied at a gymnasium (school) in Prague and later at Faculty of Philosophy of Charles University. In 1745, he became a Jesuit and studied mathematics, physics and astronomy under Joseph Stepling, a student of Ignatz Mühlwenzel. Stepling introduced Tesánek to the works of Isaac Newton. After finishing under the Faculty of Philosophy, Tesánek continued with study of theology. He was then ordained a priest and became professor of physics at Charles University. Later, he taught mathematics at the University of Olomouc. Two years later he returned to Prague to assume a professorship of high mathematics at the University. He remained at the University after the dissolution of the Jesuit order in 1773 and assumed the position of head of the Department of Mathematics and Physics in 1778. Tesánek is known for his many writings on the science of the day, helping to spread knowledge of scientific findings throughout Europe.\n\n\n"}
{"id": "1486259", "url": "https://en.wikipedia.org/wiki?curid=1486259", "title": "Lectern", "text": "Lectern\n\nA lectern (from the Latin \"lectus\", past participle of \"legere\", \"to read\") is a reading desk, with a slanted top, usually placed on a stand or affixed to some other form of support, on which documents or books are placed as support for reading aloud, as in a scripture reading, lecture, or sermon. To facilitate eye-contact and improve posture when facing an audience, lecterns may have adjustable height and slant. People generally use lecterns while standing.\n\nIn pre-modern usage, the word lectern was used to refer specifically to the \"reading desk or stand ... from which the Scripture lessons (\"lectiones\") ... are chanted or read.\" One 1905 dictionary states that \"the term is properly applied only to the class mentioned [church book stands] as independent of the pulpit.\" By the 1920s, however, the term was being used in a broader sense, for example, in reference to a memorial service in Carnegie Hall, it was stated that \"the lectern from which the speakers talked was enveloped in black.\"\n\nLecterns used in academia—generally in seminar rooms and lecture theatres—may have certain features that common lecterns lack, based on the technological sophistication of the venue. These features usually include a microphone stand, audio-visual controls, sometimes even an integrated computer and recording system. Lecterns of this sort are generally attached or integrated into a large desk, as the amount of support material tends to be larger in academic contexts than in straightforward public talks.\n\nIn the Christian Church, the lectern is usually the stand on which the Bible rests and from which the \"lessons\" (reading from Scripture) are read during the service. The lessons may be read or chanted by a priest, deacon, minister, or layperson, depending upon the liturgical traditions of the community. The lectern is normally set in front of the pews, so that the reader or speaker faces the congregation.\n\nLecterns are often made of wood. They may be either fixed in place or portable. A lectern differs from a pulpit, the latter being used for sermons. Churches that have both a lectern and a pulpit will often place them on opposite sides. The lectern will generally be smaller than the pulpit, and both may be adorned with antipendia in the color of the liturgical season.\n\nIn monastic churches and cathedrals, a separate lectern is commonly set in the centre of the choir. Originally this would have carried the antiphonal book, for use by the cantor or precentor leading the singing of the divine office. Lecterns are often eagle-shaped to symbolise John the Apostle. Especially in North America and Great Britain lecterns are sometimes made as 'angel lecterns'.\n\nBecause the Torah scrolls are generally large, the central feature of the bimah in a synagogue is a table large enough to hold an open Torah along with a tikkun or Chumash (reference books used to check the reading). In some synagogues, this table may resemble a large lectern.\n\nIn traditional yeshivas and some synagogues, students and members of the congregation may use small desks called \"shtenders\" (). These closely resemble conventional lecterns, and indeed, one shtender may be used as a lectern by the Hazzan leading the service. Note however that each study group in a yeshivah may have its own shtender and in some older synagogues, individual members of the congregation may have their own shtenders.\n\nTraditional shtenders frequently incorporate a locker under the desktop where prayerbooks and study material may be locked when not in use, and many feature a footrest for comfort during extended study sessions or standing prayers. Some older synagogues have large collections of shtenders.\n\n"}
{"id": "55225061", "url": "https://en.wikipedia.org/wiki?curid=55225061", "title": "Leonello Tarabella", "text": "Leonello Tarabella\n\nLeonello Tarabella (Forte dei Marmi, Italy, 1948) is an Italian researcher, musician and composer. His activity runs on the academic/artistic double track.\n\nGraduated in Computer Science at University of Pisa, during the '70s he started his research work under the direction of M°Pietro Grossi who was pioneering Computer Music at the CNR (National Counsel of Research) in Pisa, Italy. Later, as a study grant holder, he specialized on the technology of computer music at the EMS (Electronic Music Studio), MIT-Boston and at CCRMA (Center for Computer Research on Music and Acoustics), Stanford University.\n\nIn his research activity at CNR in Pisa he designed and carried out languages for algorithmic composition and natural gesture recognition systems and devices for giving expression to interactive electro-acoustic music; to be quoted two of them he currently uses in his performances: PalmDriver, based on infrared beams technology and Handel, based on realtime processing of video captured images. As a musician he composes and performs with these systems his own computer music: Madrid, The Netherlands, Shanghai, Thessaloniki, NewYork, New Orleans, La Habana, Barcelona, Paris, Bourges, Venezia(Biennale), Kopenhagen, Utrecht, Esbjerg, Boston, Bratislava, Dublin, Istanbul, Athens...\n\nAs Professor at the Computer Science Faculty of Pisa's University, he used (1990-2012) to teach computer music and to be supervisor of a number of degree-thesis. He published many scientific articles also participating to many editions of International Conferences on computer music. Besides, he published 2 books: “Informatica e Musica” Jackson Libri (1992) and “Musica Informatica, filosofia, storia e tecnologia della computer music” Maggioli/Apogeo (2014).\n\nHe organized events, workshops and European projects such as: -International WS on man-machine interaction in live performance; -Interactive Arts in Italy, (collaboration with Summer Music Program, NewYork University); - Third International Symposium CMMR 2005, Computer Music Modeling and Retrieval”; - MOSART (Music Orchestration System in Algorithmic Research and Technology,HPRN-CT-2000-00115; - MODEM, Music Open Distance Excahange Model, Programma Leonardo da Vinci nr. I/05/B/F/PP-154059.\n\nHe has been invited both as speaker and performer at national RAI-TV networks (Mediamente, Futura City..) and at many editions of “La notte dei Ricercatori” (Pisa, Roma, La Spezia, Ferrara, Livorno...) and \"Festival della Scienza\", Genova and recently at RomeCUP, TEDxArezzo. Thanks to a collaboration with the Steinhardt School of NewYork University he has been invited at Interactive ART Festival'99 and NYCElectronic Music Festival'13 in New York, Tulane University in New Orleans. With Lucrezia de Domizio, curator of artist Joseph Beuys’s opera, he has been present in many Contemporary Art events such as La Biennale di Venezia (49a e 53a), ArsAevi Museum di Sarajevo.\n\nOn the occasion of the Centenary of the first radio telegraphy transmission across the Atlantic by Guglielmo Marconi (Celebrazioni Marconiane, 2001) he was awarded the “Premio Marconi 2002 per l’Arte Tecnologica” for the electro-acoustic opera “KITE”.\nAfter researches on the peculiar reverberation characteristics of the Pisa’s Baptistry, he composed the concert “SiderisVOX” where the monument is considered a “musical instrument” using computer generated anechoic sounds (June 2006 and June 2016).\n\nRecently he started the project “Collisions” together with Alessandro Baris (percussionist and composer) realizing a set/concert in the consumer/avant-garde music area: Collisions has been performed in many Contemporary Art and Music Festivals such as roBOt Festival, Bologna, Dancity (Foligno) and Electropark (Genova).\n\n\n\nIEEE Spectrum, Dec. 1997. Electronic Music Interfaces, New ways to play, J.Paradiso. The Times, 9 dic.1999, pag.20. The Herald Tribune, 1999. New York Times, 13 aprile 1999. Panorama, n. 30, 2001. Newton n. 7, 2000. Machine Musicianship, Robert Rowe, The MIT Press, 2002. Deotisalvi, L’architetto del secolo d'oro. Pierotti P., Benassi L., Pacini Editore, 2002. Servizio televisivo trasmissione FuturaCity, RAI, 2004.\n\n"}
{"id": "2554464", "url": "https://en.wikipedia.org/wiki?curid=2554464", "title": "List of Greek flags", "text": "List of Greek flags\n\nThis is a list of flags used in the modern state of Greece or historically used by Greeks.\n\n\n"}
{"id": "2088579", "url": "https://en.wikipedia.org/wiki?curid=2088579", "title": "List of Nebraska state symbols", "text": "List of Nebraska state symbols\n\nThe following is a list of official symbols of the U.S. state of Nebraska, listed in the order adopted by the Nebraska Legislature:\n\n\n"}
{"id": "602504", "url": "https://en.wikipedia.org/wiki?curid=602504", "title": "List of North Carolina state symbols", "text": "List of North Carolina state symbols\n\nThe state of North Carolina has 42 official state emblems, as well as other designated places and events. The majority are determined by acts of the North Carolina General Assembly and record in Chapters 144, 145, and 149 of the North Carolina General Statutes. The state's nicknames – \"The Old North State\" and \"The Tar Heel State\" – are both traditional, but have never been passed into law by the General Assembly.\n\nThe first symbol was the Seal of North Carolina, which was made official in 1871. The original seal also contained the future state motto. It served as the state's only emblem for 14 years until the adoption of the state flag in 1885. Enacted by law in 2013, the newest symbols of North Carolina are the state art medium, clay; the state fossil, the Megalodon teeth; the state frog, the Pine Barrens tree frog; the state marsupial, the Virginia opossum; and the state salamander, the Marbled salamander.\n\n\n"}
{"id": "37900505", "url": "https://en.wikipedia.org/wiki?curid=37900505", "title": "List of birds by population", "text": "List of birds by population\n\nThis is a list of bird species by global population, divided by bird classification. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology. Contributing organizations include the IUCN, BirdLife International, and Partners in Flight.\n\nThis list is incomplete, because experts have not estimated all bird numbers. For example, the spectacled flowerpecker was only discovered in 2010, and has yet to be classified with a Linnean name, but would add to the other 73 new bird species described by ornithologists from 2000 – 2009. Global population estimates for many of these at this time would lack accuracy.\n\nAll numbers are estimates, because they are taken by observation, and a given number of 50 slender-billed curlews does not necessarily mean there are 10 more of this species than the black stilt, which has been estimated at 40: there is a possibility that the latter species has a larger population than the former. This list should not be taken that literally. An estimate of 250 shore dotterels compared with 4,500 – 5,000 wrybills, on the other hand, means that the latter has well over one order of magnitude more individuals than the former. The wrybill only has approximately one tenth the population of great skuas (48,000), which are outnumbered ~10:1 by the pigeon guillemot (470,000). It is these large differences between species that this list tries to convey.\n"}
{"id": "10484461", "url": "https://en.wikipedia.org/wiki?curid=10484461", "title": "List of chemical engineering societies", "text": "List of chemical engineering societies\n\nThis is the list of chemical engineering societies in the world. They are sorted by continent and alphabetically. They include national or international ones, but not student societies or those otherwise restricted to a particular university or institution.\n\n\n\n\n\n\n"}
{"id": "54032905", "url": "https://en.wikipedia.org/wiki?curid=54032905", "title": "List of international presidential trips made by Emmanuel Macron", "text": "List of international presidential trips made by Emmanuel Macron\n\nThis is a list of international presidential trips made by Emmanuel Macron, the 25th and current President of France. As of , Emmanuel Macron has made 73 presidential trips to 43 states internationally since his inauguration on 14 May 2017. National trips are not included. The number of visits per country where he travelled are:\n\nThe following are the international trips made by the President in 2017:\n\nDuring the visit of the Prime Minister of Israel, Benjamin Netanyahu, to Paris on July 16, 2017, Macron said that he would visit his counterpart in Israel.\n\nDuring the visit of the President of Guatemala, Jimmy Morales, to Paris on June 8, 2017, Macron said that he would visit his counterpart in Guatemala.\n\nOn 18 July 2018, during a press conference with Serbia's president Aleksandar Vučić, President Macron confirmed he would pay a visit to Serbia later that year, possibly for Armistice Day.\n\n"}
{"id": "32850182", "url": "https://en.wikipedia.org/wiki?curid=32850182", "title": "List of national parks of El Salvador", "text": "List of national parks of El Salvador\n\nThere are four National Parks located in El Salvador.\n\n\nhttp://factsking.com/countries/el-salvador/\n"}
{"id": "7672374", "url": "https://en.wikipedia.org/wiki?curid=7672374", "title": "List of protein structure prediction software", "text": "List of protein structure prediction software\n\nThis list of protein structure prediction software summarizes commonly used software tools in protein structure prediction, including homology modeling, protein threading, \"ab initio\" methods, secondary structure prediction, and transmembrane helix and signal peptide prediction.\n\nBelow is a list which separates programs according to the method used for structure prediction.\n\nDetailed list of programs can be found at List of protein secondary structure prediction programs\n\n"}
{"id": "7120292", "url": "https://en.wikipedia.org/wiki?curid=7120292", "title": "List of volcanoes in Yemen", "text": "List of volcanoes in Yemen\n\nThis is a list of active and extinct volcanoes in Yemen. \n"}
{"id": "8956567", "url": "https://en.wikipedia.org/wiki?curid=8956567", "title": "Material World (radio programme)", "text": "Material World (radio programme)\n\nMaterial World was a weekly science magazine programme on BBC Radio 4 broadcast on a Thursday afternoon. The programme's regular presenter was Quentin Cooper, with contributions from scientists researching areas under discussion in each programme.\n\nThe programme began as \"The Material World\" in April 1998. It was presented by Trevor Phillips, a chemistry graduate of Imperial College. In September 2000 Phillips was told that he could no longer work at the BBC due to his close links with the Labour Party, which broke BBC rules of impartiality. He was one of the few regular black broadcasters on Radio 4. The programme was presented by Quentin Cooper from 2000 to its end in 2013.\n\nMaterial World was one of the BBC's main conduits for up-to-date scientific news, along with \"Frontiers\", \"Science in Action\", and \"Bang Goes the Theory\".\n\nFrom 5 April 2010 the programme was repeated on a Monday evening at 21.00, in the former slot of \"Costing the Earth\". For a short time, when programmes on 5 Live began webstreaming with video, \"Material World\" was also webcast.\n\nOn 14 June 2013 it was announced that the show was to be cancelled, to be replaced by a new show, \"Inside Science\". The last programme presented by Quentin Cooper was broadcast on 20 June 2013 with the final episode airing a week later on 27 June 2013, presented by Gareth Mitchell.\n\nA typical episode programme covered three or four topics, giving each 7-10 minutes. For many years the programme was divided into two sections of fifteen minutes on separate topics. It took the form of interviewing a guest scientist or engineer. Cooper often ended the programme with a terrible scientific pun.\n\nMany past programmes are available for online listening via the programme's website. Some sequential sets of programmes were made in collaboration with the Open University.\n\n\n"}
{"id": "27209853", "url": "https://en.wikipedia.org/wiki?curid=27209853", "title": "Materiality (digital text)", "text": "Materiality (digital text)\n\nWhen referring to digital text, pictures and documents, the term materiality refers to the physical medium used to store and convey the text, as apart from the text itself. This concept is important to archivists and historians, who often require access to the physical medium of documents or correspondence in order to understand the transitions that the document underwent between initial conception and final publication.\n"}
{"id": "1568394", "url": "https://en.wikipedia.org/wiki?curid=1568394", "title": "Mayor's Award for Excellence in Science and Technology", "text": "Mayor's Award for Excellence in Science and Technology\n\nThe Mayor's Award for Excellence in Science and Technology is given annually to recognise important members of the science and engineering communities in New York City. Candidates must live or work in the city.\n\nNominations are submitted in five categories:\n\nThe Mayor chooses winners from a list of finalists submitted by the New York Academy of Sciences and the New York City Department of Cultural Affairs.\n"}
{"id": "37930814", "url": "https://en.wikipedia.org/wiki?curid=37930814", "title": "Multistakeholder governance model", "text": "Multistakeholder governance model\n\nThe multistakeholder governance model, sometimes known as a multistakeholder initiative (MSI), is a governance structure that seeks to bring stakeholders together to participate in the dialogue, decision making, and implementation of solutions to common problems or goals. The principle behind such a structure is that if enough input is provided by all actors involved in a question, the eventual consensual decision gains more legitimacy, and therefore better reflects a set of perspectives rather than a single source of validation.\n\nA stakeholder refers to an individual, group, or organization that has a direct or indirect interest or stake in a particular organization, these may be businesses, civil society, governments, research institutions, and non-government organizations.\n\nThe multistakeholder model is used in Internet governance by entities such as the ICANN and IETF It has been the foundation of local governance entities such as New York City's Community Boards.\n\nNorbert Bollow, co-coordinator on the Civil Society Internet Governance Forum distinguishes between \"representative\" multistakeholderism, using as examples the United Nation's MAG and ECWG, in which a limited number of seats are distributed to representatives through some selection process, and \"open\" multistakeholderism, as represented by the IETF and RIRs, which relies on participants self-selecting to balance perspectives.\n\n"}
{"id": "57398059", "url": "https://en.wikipedia.org/wiki?curid=57398059", "title": "Najm al‐Din al‐Misri", "text": "Najm al‐Din al‐Misri\n\nNajm al‐Dīn al‐Miṣrī () was a 13th-century Egyptian astronomer mostly known for writing a large astronomical table that had nearly 415,000 entries. The table is considered to be the largest of its kind ever produced by one person during the Middle Ages. Although the main purpose of the work was astronomical timekeeping, it can also be used to solve all problems of spherical trigonometry by changing the arguments of the table.\n\nNajm al‐Din also wrote an important illustrated treatise that describes more than 100 different astronomical instruments, including ones he invented himself. This work is of high importance for modern scholars and one of the main sources on the subject.\n"}
{"id": "11201559", "url": "https://en.wikipedia.org/wiki?curid=11201559", "title": "Newbury principles", "text": "Newbury principles\n\nThe Newbury Principles collectively refer to an urban planning guideline stating that decisions should be made based only on the planning considerations relevant to the current development, even if the consideration of ulterior purposes may lead to a greater public good. In practice, the principles are used as a test to verify the validity of conditions to be imposed by a planning authority. \n\nSpecifically, the decision of the House of Lords in \"Newbury District Council v Secretary of State for the Environment\", contains the following three principles when considering the reasonableness of imposing conditions on consents:\n\n\nThe Newbury principles are applied in Australia, and have been cited by courts in New South Wales and Western Australia. The Newbury test also remains in general application in the courts of New Zealand.\n\n"}
{"id": "765292", "url": "https://en.wikipedia.org/wiki?curid=765292", "title": "Nigel Weiss", "text": "Nigel Weiss\n\nNigel Oscar Weiss FRS (born 16 December 1936) is an astronomer and mathematician, and leader in the field of astrophysical and geophysical fluid dynamics. He is currently Emeritus Professor of Mathematical Astrophysics at the University of Cambridge.\n\nBorn in South Africa, Weiss studied at Hilton College, Natal, Rugby School and Clare College, Cambridge, and has been a fellow of Clare College since 1965.\n\nIn 1987 he became Professor of Mathematical Astrophysics at the University of Cambridge.\n\nBetween 2000 and 2002 he was President of the Royal Astronomical Society, and in 2007 was awarded the Gold Medal, the society's highest award.\n\nWeiss has published extensively in the field of mathematical astrophysics, specialising in solar and stellar magnetic fields, astrophysical and geophysical fluid dynamics and nonlinear dynamical systems.\n\nIn 1966 he was the first to demonstrate and describe the process of 'flux expulsion' by which a conducting fluid undergoing rotating motion acts to expel the magnetic flux from the region of motion, a process now known to occur in the photosphere of the Sun and other stars.\n\nWeiss was elected a Fellow of the Royal Society (FRS) in 1992. His nomination reads \n"}
{"id": "30369393", "url": "https://en.wikipedia.org/wiki?curid=30369393", "title": "Palmenhaus Schönbrunn", "text": "Palmenhaus Schönbrunn\n\nThe Palmenhaus Schönbrunn is a large greenhouse in Vienna, Austria featuring plants from around the world. It opened in 1882. It is the most prominent of the four greenhouses in Schönbrunn Palace Park, and is also among the largest botanical exhibits of its kind in the world, with around 4,500 plant species.\n\nSeveral forerunners were built in the Palace Park in the 18th and 19th centuries, under Emperors Francis I and Joseph II. The present building was opened in 1882, under Franz Joseph I. Since 1918 it has been run by the \"Bundesgärten\" (Federal Gardens).\n\nA heavy bomb attack on Schönbrunn Palace in February 1945 destroyed most of the glazing of the Palmenhaus. Many plants died, although some were saved by being transferred to the nearby Sonnenuhrhaus. The rebuilding began in 1948, and the Palmenhaus was reopened in 1953.\n\nThe building was closed to the public in 1976 as a safety measure following the collapse of the Reichsbrücke. Renovations were carried out between 1986 and 1990.\n\nBuilt of steel, the Palmenhaus is 111 metres long, 28 metres wide and 25 metres high, and has 45,000 glass tiles. There are annexes on the north and south sides, serving as a coldhouse and a hothouse respectively.\n\n\n\n\n"}
{"id": "4394258", "url": "https://en.wikipedia.org/wiki?curid=4394258", "title": "Pierre-Marie Termier", "text": "Pierre-Marie Termier\n\nPierre-Marie Termier (July 3, 1859 – October 23, 1930) was a French geologist.\n\nHe was born in Lyon, in Rhône, France, the son of Joseph François Termier and Jeanne Mollard. At the age of 18 he entered the Polytechnic School, then the Paris School of Mines in 1880. After graduation, he became professor at the school of mines at Saint-Etienne. In 1894 he left for Paris, where he would teach for the remainder of his career.\n\nHe was elected as a member of the French Academy of Sciences in 1909, in the mineralogy section. Two years later he became the director of the French geological cartography service. In 1930 he became vice-president of the Academy.\n\nDuring his career he performed geological studies of the Alps, as well as Corsica and North Africa. He was a proponent of the nappe concept and of tectonics as a mountain-building force.\n\nThe wrinkle ridge Dorsum Termier on the Moon is named after him.\n\n"}
{"id": "16070384", "url": "https://en.wikipedia.org/wiki?curid=16070384", "title": "Pignistic probability", "text": "Pignistic probability\n\nIn decision theory, a pignistic probability is a probability that a rational person will assign to an option when required to make a decision. \n\nA person may have, at one level certain beliefs or a lack of knowledge, or uncertainty, about the options and their actual likelihoods. However, when it is necessary to make a decision (such as deciding whether to place a bet), the behaviour of the rational person would suggest that the person has assigned a set of regular probabilities to the options. These are the \"pignistic probabilities\".\n\nThe term was coined by Philippe Smets, and stems from the Latin \"pignus\", a bet. He contrasts the \"pignistic\" level, where one might take action, with the \"credal\" level, where one interprets the state of the world:\n\nA \"pignistic probability transform\" will calculate these pignistic probabilities from a structure that describes belief structures.\n\n"}
{"id": "23842", "url": "https://en.wikipedia.org/wiki?curid=23842", "title": "Planets beyond Neptune", "text": "Planets beyond Neptune\n\nFollowing the discovery of the planet Neptune in 1846, there was considerable speculation that another planet might exist beyond its orbit. The search began in the mid-19th century and culminated at the start of the 20th with Percival Lowell's quest for Planet X. Lowell proposed the Planet X hypothesis to explain apparent discrepancies in the orbits of the giant planets, particularly Uranus and Neptune, speculating that the gravity of a large unseen ninth planet could have perturbed Uranus enough to account for the irregularities.\n\nClyde Tombaugh's discovery of Pluto in 1930 appeared to validate Lowell's hypothesis, and Pluto was officially named the ninth planet. In 1978, Pluto was conclusively determined to be too small for its gravity to affect the giant planets, resulting in a brief search for a tenth planet. The search was largely abandoned in the early 1990s, when a study of measurements made by the \"Voyager 2\" spacecraft found that the irregularities observed in Uranus's orbit were due to a slight overestimation of Neptune's mass. After 1992, the discovery of numerous small icy objects with similar or even wider orbits than Pluto led to a debate over whether Pluto should remain a planet, or whether it and its neighbours should, like the asteroids, be given their own separate classification. Although a number of the larger members of this group were initially described as planets, in 2006 the International Astronomical Union (IAU) reclassified Pluto and its largest neighbours as dwarf planets, leaving Neptune the farthest known planet in the Solar System.\n\nWhile the astronomical community widely agrees that Planet X, as originally envisioned, does not exist, the concept of an as-yet-unobserved planet has been revived by a number of astronomers to explain other anomalies observed in the outer Solar System. As of March 2014, observations with the WISE telescope have ruled out the possibility of a Saturn-sized object (95 Earth masses) out to 10,000 AU, and a Jupiter-sized (≈318 Earth masses) or larger object out to 26,000 AU.\n\nIn 2014, based on similarities of the orbits of a group of recently discovered extreme trans-Neptunian objects, astronomers hypothesized the existence of a super-Earth planet, 2 to 15 times the mass of the Earth and beyond 200 AU with possibly a high inclined orbit at some 1,500 AU. In 2016, further work showed this unknown distant planet is likely on an inclined, eccentric orbit that goes no closer than about 200 AU and no farther than about 1,200 AU from the Sun. The orbit is predicted to be anti-aligned to the clustered extreme trans-Neptunian objects. Because Pluto is no longer considered a planet by the IAU, this new hypothetical object has become known as Planet Nine.\n\nIn the 1840s, the French mathematician Urbain Le Verrier used Newtonian mechanics to analyse perturbations in the orbit of Uranus, and hypothesised that they were caused by the gravitational pull of a yet-undiscovered planet. Le Verrier predicted the position of this new planet and sent his calculations to German astronomer Johann Gottfried Galle. On 23 September 1846, the night following his receipt of the letter, Galle and his student Heinrich d'Arrest discovered Neptune, exactly where Le Verrier had predicted. There remained some slight discrepancies in the giant planets' orbits. These were taken to indicate the existence of yet another planet orbiting beyond Neptune.\n\nEven before Neptune's discovery, some speculated that one planet alone was not enough to explain the discrepancy. On 17 November 1834, the British amateur astronomer the Reverend Thomas John Hussey reported a conversation he had had with French astronomer Alexis Bouvard to George Biddell Airy, the British Astronomer Royal. Hussey reported that when he suggested to Bouvard that the unusual motion of Uranus might be due to the gravitational influence of an undiscovered planet, Bouvard replied that the idea had occurred to him, and that he had corresponded with Peter Andreas Hansen, director of the Seeberg Observatory in Gotha, about the subject. Hansen's opinion was that a single body could not adequately explain the motion of Uranus, and postulated that two planets lay beyond Uranus.\n\nIn 1848, Jacques Babinet raised an objection to Le Verrier's calculations, claiming that Neptune's observed mass was smaller and its orbit larger than Le Verrier had initially predicted. He postulated, based largely on simple subtraction from Le Verrier's calculations, that another planet of roughly 12 Earth masses, which he named \"Hyperion\", must exist beyond Neptune. Le Verrier denounced Babinet's hypothesis, saying, \"[There is] absolutely nothing by which one could determine the position of another planet, barring hypotheses in which imagination played too large a part.\"\n\nIn 1850 James Ferguson, Assistant Astronomer at the United States Naval Observatory, noted that he had \"lost\" a star he had observed, GR1719k, which Lt. Matthew Maury, the superintendent of the Observatory, claimed was evidence that it must be a new planet. Subsequent searches failed to recover the \"planet\" in a different position, and in 1878, CHF Peters, director of the Hamilton College Observatory in New York, showed that the star had not in fact vanished, and that the previous results had been due to human error.\n\nIn 1879, Camille Flammarion noted that the comets 1862 III and 1889 III had aphelia of 47 and 49 AU, respectively, suggesting that they might mark the orbital radius of an unknown planet that had dragged them into an elliptical orbit. Astronomer George Forbes concluded on the basis of this evidence that two planets must exist beyond Neptune. He calculated, based on the fact that four comets possessed aphelia at around 100 AU and a further six with aphelia clustered at around 300 AU, the orbital elements of a pair of hypothetical trans-Neptunian planets. These elements concorded suggestively with those made independently by another astronomer named David Peck Todd, suggesting to many that they might be valid. However, sceptics argued that the orbits of the comets involved were still too uncertain to produce meaningful results. George Forbes is today considered to be the first describing Planet Nine.\n\nIn 1900 and 1901, Harvard College Observatory director William Henry Pickering led two searches for trans-Neptunian planets. The first was begun by Danish astronomer Hans Emil Lau who, after studying the data on the orbit of Uranus from 1690 to 1895, concluded that one trans-Neptunian planet alone could not account for the discrepancies in its orbit, and postulated the position of two planets he believed were responsible. The second was launched when Gabriel Dallet suggested that a single trans-Neptunian planet lying at 47 AU could account for the motion of Uranus. Pickering agreed to examine plates for any suspected planets. In neither case were any found.\n\nIn 1909, Thomas Jefferson Jackson See, an astronomer with a reputation as an egocentric contrarian, opined \"that there is certainly one, most likely two and possibly three planets beyond Neptune\". Tentatively naming the first planet \"Oceanus\", he placed their respective distances at 42, 56 and 72 AU from the Sun. He gave no indication as to how he determined their existence, and no known searches were mounted to locate them.\n\nIn 1911, Indian astronomer Venkatesh P. Ketakar suggested the existence of two trans-Neptunian planets, which he named Brahma and Vishnu, by reworking the patterns observed by Pierre-Simon Laplace in the planetary satellites of Jupiter and applying them to the outer planets. The three inner Galilean moons of Jupiter, Io, Europa and Ganymede, are locked in a complicated 1:2:4 resonance called a Laplace resonance. Ketakar suggested that Uranus, Neptune and his hypothetical trans-Neptunian planets were locked in Laplace-like resonances. His calculations predicted a mean distance for Brahma of 38.95 AU and an orbital period of 242.28 Earth years (3:4 resonance with Neptune). When Pluto was discovered 19 years later, its mean distance of 39.48 AU and orbital period of 248 Earth years were close to Ketakar's prediction (Pluto in fact has a 2:3 resonance with Neptune). Ketakar made no predictions for the orbital elements other than mean distance and period. It is not clear how Ketakar arrived at these figures, and his second planet, Vishnu, was never located.\n\nIn 1894, with the help of William Pickering, Percival Lowell, a wealthy Bostonian, founded the Lowell Observatory in Flagstaff, Arizona. In 1906, convinced he could resolve the conundrum of Uranus's orbit, he began an extensive project to search for a trans-Neptunian planet, which he named \"Planet X\", a name previously used by Gabriel Dallet. The \"X\" in the name represents an unknown and is pronounced as the letter, as opposed to the Roman numeral for 10 (at the time, Planet X would have been the ninth planet). Lowell's hope in tracking down Planet X was to establish his scientific credibility, which had eluded him due to his widely derided belief that channel-like features visible on the surface of Mars were canals constructed by an intelligent civilization.\n\nLowell's first search focused on the ecliptic, the plane encompassed by the zodiac where the other planets in the Solar System lie. Using a 5-inch photographic camera, he manually examined over 200 three-hour exposures with a magnifying glass, and found no planets. At that time Pluto was too far above the ecliptic to be imaged by the survey. After revising his predicted possible locations, Lowell conducted a second search from 1914 to 1916. In 1915, he published his \"Memoir of a Trans-Neptunian Planet\", in which he concluded that Planet X had a mass roughly seven times that of Earth—about half that of Neptune—and a mean distance from the Sun of 43 AU. He assumed Planet X would be a large, low-density object with a high albedo, like the giant planets. As a result, it would show a disc with diameter of about one arcsecond and an apparent magnitude of between 12 and 13—bright enough to be spotted.\n\nSeparately, in 1908, Pickering announced that, by analysing irregularities in Uranus's orbit, he had found evidence for a ninth planet. His hypothetical planet, which he termed \"Planet O\" (because it came after \"N\", i.e. Neptune), possessed a mean orbital radius of 51.9 AU and an orbital period of 373.5 years. Plates taken at his observatory in Arequipa, Peru, showed no evidence for the predicted planet, and British astronomer P. H. Cowell showed that the irregularities observed in Uranus's orbit virtually disappeared once the planet's displacement of longitude was taken into account. Lowell himself, despite his close association with Pickering, dismissed Planet O out of hand, saying, \"This planet is very properly designated \"O\", [for it] is nothing at all.\" Unbeknownst to Pickering, four of the photographic plates taken in the search for \"Planet O\" by astronomers at the Mount Wilson Observatory in 1919 captured images of Pluto, though this was only recognised years later. Pickering went on to suggest many other possible trans-Neptunian planets up to the year 1932, which he named \"P\", \"Q\", \"R\", \"S\", \"T\" and \"U\"; none were ever detected.\n\nLowell's sudden death in 1916 temporarily halted the search for Planet X. Failing to find the planet, according to one friend, \"virtually killed him\". Lowell's widow, Constance, engaged in a legal battle with the observatory over Lowell's legacy which halted the search for Planet X for several years. In 1925, the observatory obtained glass discs for a new wide-field telescope to continue the search, constructed with funds from Abbott Lawrence Lowell, Percival's brother. In 1929 the observatory's director, Vesto Melvin Slipher, summarily handed the job of locating the planet to Clyde Tombaugh, a 22-year-old Kansas farm boy who had only just arrived at the Lowell Observatory after Slipher had been impressed by a sample of his astronomical drawings.\n\nTombaugh's task was to systematically capture sections of the night sky in pairs of images. Each image in a pair was taken two weeks apart. He then placed both images of each section in a machine called a blink comparator, which by exchanging images quickly created a time lapse illusion of the movement of any planetary body. To reduce the chances that a faster-moving (and thus closer) object be mistaken for the new planet, Tombaugh imaged each region near its opposition point, 180 degrees from the Sun, where the apparent retrograde motion for objects beyond Earth's orbit is at its strongest. He also took a third image as a control to eliminate any false results caused by defects in an individual plate. Tombaugh decided to image the entire zodiac, rather than focus on those regions suggested by Lowell.\nBy the beginning of 1930, Tombaugh's search had reached the constellation of Gemini. On 18 February 1930, after searching for nearly a year and examining nearly 2 million stars, Tombaugh discovered a moving object on photographic plates taken on 23 January and 29 January of that year. A lesser-quality photograph taken on January 21 confirmed the movement. Upon confirmation, Tombaugh walked into Slipher's office and declared, \"Doctor Slipher, I have found your Planet X.\" The object lay just six degrees from one of two locations for Planet X Lowell had suggested; thus it seemed he had at last been vindicated. After the observatory obtained further confirmatory photographs, news of the discovery was telegraphed to the Harvard College Observatory on March 13, 1930. The new object was later precovered on photographs dating back to 19 March 1915. The decision to name the object \"Pluto\" was intended in part to honour Percival Lowell, as his initials made up the word's first two letters. After discovering Pluto, Tombaugh continued to search the ecliptic for other distant objects. He found hundreds of variable stars and asteroids, as well as two comets, but no further planets.\n\nTo the observatory's disappointment and surprise, Pluto showed no visible disc; it appeared as a point, no different from a star, and, at only 15th magnitude, was six times dimmer than Lowell had predicted, which meant it was either very small, or very dark. Because Lowell astronomers thought Pluto was massive enough to perturb planets, they assumed that its albedo could be no less than 0.07 (meaning that it reflected only 7% of the light that hit it); about as dark as asphalt and similar to that of Mercury, the least reflective planet known. This would give Pluto an estimated mass of no more than 70% that of Earth. Observations also revealed that Pluto's orbit was very elliptical, far more than that of any other planet.\n\nAlmost immediately, some astronomers questioned Pluto's status as a planet. Barely a month after its discovery was announced, on April 14, 1930, in an article in \"The New York Times\", Armin O. Leuschner suggested that Pluto's dimness and high orbital eccentricity made it more similar to an asteroid or comet: \"The Lowell result confirms the possible high eccentricity announced by us on April 5. Among the possibilities are a large asteroid greatly disturbed in its orbit by close approach to a major planet such as Jupiter, or it may be one of many long-period planetary objects yet to be discovered, or a bright cometary object.\" In that same article, Harvard Observatory director Harlow Shapley wrote that Pluto was a \"member of the Solar System not comparable with known asteroids and comets, and perhaps of greater importance to cosmogony than would be another major planet beyond Neptune.\" In 1931, using a mathematical formula, Ernest W. Brown asserted (in agreement with E. C. Bower), that the presumed irregularities in the orbit of Uranus could not be due to the gravitational effect of a more distant planet, and thus that Lowell's supposed prediction was \"purely accidental\".\n\nThroughout the mid-20th century, estimates of Pluto's mass were revised downward. In 1931, Nicholson and Mayall calculated its mass, based on its supposed effect on the giant planets, as roughly that of Earth; a value somewhat in accord with the 0.91 Earth mass calculated in 1942 by Lloyd R. Wylie at the US Naval Observatory, using the same assumptions. In 1949, Gerard Kuiper's measurements of Pluto's diameter with the 200 inch telescope at Mount Palomar Observatory led him to the conclusion that it was midway in size between Mercury and Mars and that its mass was most probably about 0.1 Earth mass.\n\nIn 1973, based on the similarities in the periodicity and amplitude of brightness variation with Triton, Dennis Rawlins conjectured Pluto's mass must be similar to Triton's. In retrospect, the conjecture turns out to have been correct; it had been argued by astronomers Walter Baade and E.C. Bower as early as 1934. However, because Triton's mass was then believed to be roughly 2.5% of the Earth–Moon system (more than ten times its actual value), Rawlins's determination for Pluto's mass was similarly incorrect. It was nonetheless a meagre enough value for him to conclude Pluto was not Planet X. In 1976, Dale Cruikshank, Carl Pilcher, and David Morrison of the University of Hawaii analysed spectra from Pluto's surface and determined that it must contain methane ice, which is highly reflective. This meant that Pluto, far from being dark, was in fact exceptionally bright, and thus was probably no more than  Earth mass.\n\nPluto's size was finally determined conclusively in 1978, when American astronomer James W. Christy discovered its moon Charon. This enabled him, together with Robert Sutton Harrington of the U.S. Naval Observatory, to measure the mass of the Pluto–Charon system directly by observing the moon's orbital motion around Pluto. They determined Pluto's mass to be 1.31×10 kg; roughly one five-hundredth that of Earth or one-sixth that of the Moon, and far too small to account for the observed discrepancies in the orbits of the outer planets. Lowell's \"prediction\" had been a coincidence: If there was a Planet X, it was not Pluto.\n\nAfter 1978, a number of astronomers kept up the search for Lowell's Planet X, convinced that, because Pluto was no longer a viable candidate, an unseen tenth planet must have been perturbing the outer planets.\n\nIn the 1980s and 1990s, Robert Harrington led a search to determine the real cause of the apparent irregularities. He calculated that any Planet X would be at roughly three times the distance of Neptune from the Sun; its orbit would be highly eccentric, and strongly inclined to the ecliptic—the planet's orbit would be at roughly a 32-degree angle from the orbital plane of the other known planets. This hypothesis was met with a mixed reception. Noted Planet X sceptic Brian G. Marsden of the Minor Planet Center pointed out that these discrepancies were a hundredth the size of those noticed by Le Verrier, and could easily be due to observational error.\n\nIn 1972, Joseph Brady of the Lawrence Livermore National Laboratory studied irregularities in the motion of Halley's Comet. Brady claimed that they could have been caused by a Jupiter-sized planet beyond Neptune at 59 AU that is in a retrograde orbit around the Sun. However, both Marsden and Planet X proponent P. Kenneth Seidelmann attacked the hypothesis, showing that Halley's Comet randomly and irregularly ejects jets of material, causing changes to its own orbital trajectory, and that such a massive object as Brady's Planet X would have severely affected the orbits of known outer planets.\n\nAlthough its mission did not involve a search for Planet X, the IRAS space observatory made headlines briefly in 1983 due to an \"unknown object\" that was at first described as \"possibly as large as the giant planet Jupiter and possibly so close to Earth that it would be part of this Solar System\". Further analysis revealed that of several unidentified objects, nine were distant galaxies and the tenth was \"interstellar cirrus\"; none were found to be Solar System bodies.\n\nIn 1988, A. A. Jackson and R. M. Killen studied the stability of Pluto's resonance with Neptune by placing test \"Planet X-es\" with various masses and at various distances from Pluto. Pluto and Neptune's orbits are in a 3:2 resonance, which prevents their collision or even any close approaches, regardless of their separation in the z axis. It was found that the hypothetical object's mass had to exceed 5 Earth masses to break the resonance, and the parameter space is quite large and a large variety of objects could have existed beyond Pluto without disturbing the resonance. Four test orbits of a trans-Plutonian planet have been integrated forward for four million years in order to determine the effects of such a body on the stability of the Neptune–Pluto 3:2 resonance. Planets beyond Pluto with masses of 0.1 and 1.0 Earth masses in orbits at 48.3 and 75.5 AU, respectively, do not disturb the 3:2 resonance. Test planets of 5 Earth masses with semi-major axes of 52.5 and 62.5 AU disrupt the four-million-year libration of Pluto's argument of perihelion.\n\nHarrington died in January 1993, without having found Planet X. Six months before, E. Myles Standish had used data from \"Voyager 2\"&apos;s 1989 flyby of Neptune, which had revised the planet's total mass downward by 0.5%—an amount comparable to the mass of Mars—to recalculate its gravitational effect on Uranus. When Neptune's newly determined mass was used in the Jet Propulsion Laboratory Developmental Ephemeris (JPL DE), the supposed discrepancies in the Uranian orbit, and with them the need for a Planet X, vanished. There are no discrepancies in the trajectories of any space probes such as \"Pioneer 10\", \"Pioneer 11\", \"Voyager 1\", and \"Voyager 2\" that can be attributed to the gravitational pull of a large undiscovered object in the outer Solar System. Today, most astronomers agree that Planet X, as Lowell defined it, does not exist.\n\nAfter the discovery of Pluto and Charon, no more trans-Neptunian objects (TNOs) were found until 15760 Albion in 1992. Since then, thousands of such objects have been discovered. Most are now recognized as part of the Kuiper belt, a swarm of icy bodies left over from the Solar System's formation that orbit near the ecliptic plane just beyond Neptune. Though none were as large as Pluto, some of these distant trans-Neptunian objects, such as Sedna, were initially described in the media as \"new planets\".\n\nIn 2005, astronomer Mike Brown and his team announced the discovery of (later named after the Greek goddess of discord and strife), a trans-Neptunian object then thought to be just barely larger than Pluto. Soon afterwards, a NASA Jet Propulsion Laboratory press release described the object as the \"tenth planet\".\n\nEris was never officially classified as a planet, and the 2006 definition of planet defined both Eris and Pluto not as planets but as dwarf planets because they have not cleared their neighbourhoods. They do not orbit the Sun alone, but as part of a population of similarly sized objects. Pluto itself is now recognized as being a member of the Kuiper belt and the largest dwarf planet, larger than the more-massive Eris.\n\nA number of astronomers, most notably Alan Stern, the head of NASA's New Horizons mission to Pluto, contend that the IAU's definition is flawed, and that Pluto and Eris, and all large trans-Neptunian objects, such as , , , and , should be considered planets in their own right. However, the discovery of Eris did not rehabilitate the Planet X theory because it is far too small to have significant effects on the outer planets' orbits.\n\nAlthough most astronomers accept that Lowell's Planet X does not exist, a number have revived the idea that a large unseen planet could create observable gravitational effects in the outer Solar System. These hypothetical objects are often referred to as \"Planet X\", although the conception of these objects may differ considerably from that proposed by Lowell.\n\nWhen Sedna was discovered, its extreme orbit raised questions about its origin. Its perihelion is so distant (approximately 75 AU) that no currently observed mechanism can explain Sedna's eccentric distant orbit. It is too far from the planets to have been affected by the gravity of Neptune or the other giant planets and too bound to the Sun to be affected by outside forces such as the galactic tides. Hypotheses to explain its orbit include that it was affected by a passing star, that it was captured from another planetary system, or that it was tugged into its current position by a trans-Neptunian planet. The most obvious solution to determining Sedna's peculiar orbit would be to locate a number of objects in a similar region, whose various orbital configurations would provide an indication as to their history. If Sedna had been pulled into its orbit by a trans-Neptunian planet, any other objects found in its region would have a similar perihelion to Sedna (around 80 AU).\n\nIn 2012, Rodney Gomes modelled the orbits of 92 Kuiper belt objects and found that six of those orbits were far more elongated than the model predicted. He concluded that the simplest explanation was the gravitational pull of a distant planetary companion, such as a Neptune-sized object at 1,500 AU or a Mars-sized object at around 53 AU.\n\nIn 2014, astronomers announced the discovery of , a large object with a Sedna-like 4,200-year orbit and a perihelion of roughly 80 AU, which led them to suggest that it offered evidence of a potential trans-Neptunian planet. Trujillo and Sheppard argued that the orbital clustering of arguments of perihelia for VP113 and other extremely distant TNOs suggests the existence of a \"super-Earth\" of between 2 and 15 Earth masses beyond 200 AU and possibly on an inclined orbit at 1500 AU.\n\nIn 2014 astronomers at the Universidad Complutense in Madrid suggested that the available data actually indicate more than one trans-Neptunian planet; subsequent work further suggests that the evidence is robust enough.\n\nOn January 20, 2016, Brown and Konstantin Batygin published an article corroborating Trujillo and Sheppard's initial findings; proposing a super-Earth (dubbed Planet Nine) based on a statistical clustering of the arguments of perihelia (noted before) near zero and also ascending nodes near 113° of six distant trans-Neptunian objects. They estimated it to be ten times the mass of Earth (about 60% the mass of Neptune) with a semimajor axis of approximately 400–1500 AU.\n\nEven without gravitational evidence, Mike Brown, the discoverer of Sedna, has argued that Sedna's 12,000-year orbit means that probability alone suggests that an Earth-sized object exists beyond Neptune. Sedna's orbit is so eccentric that it spends only a small fraction of its orbital period near the Sun, where it can be easily observed. This means that unless its discovery was a freak accident, there is probably a substantial population of objects roughly Sedna's diameter yet to be observed in its orbital region. Mike Brown noted that \"Sedna is about three-quarters the size of Pluto. If there are sixty objects three-quarters the size of Pluto [out there] then there are probably forty objects the size of Pluto ... If there are forty objects the size of Pluto, then there are probably ten that are twice the size of Pluto. There are probably three or four that are three times the size of Pluto, and the biggest of these objects ... is probably the size of Mars or the size of the Earth.\" However, he notes that, should such an object be found, even though it might approach Earth in size, it would still be a dwarf planet by the current definition, because it would not have cleared its neighbourhood sufficiently.\n\nAdditionally, speculation of a possible trans-Neptunian planet has revolved around the so-called \"Kuiper cliff\". The Kuiper belt terminates suddenly at a distance of 48 AU from the Sun. Brunini and Melita have speculated that this sudden drop-off may be attributed to the presence of an object with a mass between those of Mars and Earth located beyond 48 AU. The presence of an object with a mass similar to that of Mars in a circular orbit at 60 AU leads to a trans-Neptunian object population incompatible with observations. For instance, it would severely deplete the plutino population. Astronomers have not excluded the possibility of an object with a mass similar to that of Earth located farther than 100 AU with an eccentric and inclined orbit. Computer simulations by Patryk Lykawka of Kobe University have suggested that an object with a mass between 0.3 and 0.7 Earth masses, ejected outward by Neptune early in the Solar System's formation and currently in an elongated orbit between 101 and 200 AU from the Sun, could explain the Kuiper cliff and the peculiar detached objects such as Sedna and . Although some astronomers, such as Renu Malhotra and David Jewitt, have cautiously supported these claims, others, such as Alessandro Morbidelli, have dismissed them as \"contrived\". In 2017, Malhotra and Kat Volk argued that an unexpected variance in inclination for KBOs farther than the cliff at 50 AU provided evidence of a possible Mars-sized planet residing at the edge of the Solar System.\n\nTyche was a hypothetical gas giant proposed to be located in the Solar System's Oort cloud. It was first proposed in 1999 by astrophysicists John Matese, Patrick Whitman and Daniel Whitmire of the University of Louisiana at Lafayette. They argued that evidence of Tyche's existence could be seen in a supposed bias in the points of origin for long-period comets. In 2013, Matese and Whitmire re-evaluated the comet data and noted that Tyche, if it existed, would be detectable in the archive of data that was collected by NASA's Wide-field Infrared Survey Explorer (WISE) telescope. In 2014, NASA announced that the WISE survey had ruled out any object with Tyche's characteristics, indicating that Tyche as hypothesized by Matese, Whitman, and Whitmire does not exist.\n\nThe oligarch theory of planet formation states that there were hundreds of planet-sized objects, known as oligarchs, in the early stages of the Solar System's evolution. In 2005, astronomer Eugene Chiang speculated that although some of these oligarchs became the planets we know today, most would have been flung outward by gravitational interactions. Some may have escaped the Solar System altogether to become free-floating planets, whereas others would be orbiting in a halo around the Solar System, with orbital periods of millions of years. This halo would lie at between 1,000 and 10,000 AU from the Sun, or between a third and a thirtieth the distance to the Oort cloud.\n\nIn December 2015, astronomers at the Atacama Large Millimeter Array (ALMA) detected a brief series of 350 GHz pulses that they concluded must either be a series of independent sources, or a single, fast moving source. Deciding that the latter was the most likely, they calculated based on its speed that, were it bound to the Sun, the object, which they named \"Gna\" after a fast-moving messenger goddess in Norse mythology, would be about 12–25 AU distant and have a dwarf planet-sized diameter of 220 to 880 km. However, if it were a rogue planet not gravitationally bound to the Sun, and as far away as 4000 AU, it could be much larger. The paper was never formally accepted, and has been withdrawn until the detection is confirmed. Scientists' reactions to the notice were largely sceptical; Mike Brown commented that, \"If it is true that ALMA accidentally discovered a massive outer solar system object in its tiny, tiny, tiny, field of view, that would suggest that there are something like 200,000 Earth-sized planets in the outer solar system ... Even better, I just realized that this many Earth-sized planets existing would destabilize the entire solar system and we would all die.\"\n\nAs of 2016 the following observations severely constrain the mass and distance of any possible additional Solar System planet:\n\n\n\n\n"}
{"id": "42139922", "url": "https://en.wikipedia.org/wiki?curid=42139922", "title": "SchedMD", "text": "SchedMD\n\nSchedMD LLC is an American software company that is the main developer of the Slurm Workload Manager (or Slurm), an open-source workload management system. SchedMD also provides support, training and consulting services around Slurm.\n\nSchedMD was founded in 2010, specifically to develop and provide services around Slurm. Its corporate headquarters are in Lehi, Utah.\n\nSlurm began development as a collaborative effort primarily by Lawrence Livermore National Laboratory, Linux NetworX, Hewlett-Packard and Groupe Bull as a Free Software resource manager in 2001. In 2010 Morris Jette and Danny Auble incorporated SchedMD LLC, to develop and market Slurm.\n\nSchedMD provides services to many national labs, universities, corporations and government agencies, including:\n\nSchedMD partly operates on a professional open-source business model based on open source code, development within a community, professional quality assurance, and subscription-based customer support.\n\nSchedMD sells subscriptions for support, training and integration services. Customers pay a fixed price for unlimited access to services.\n\n"}
{"id": "7955458", "url": "https://en.wikipedia.org/wiki?curid=7955458", "title": "Socioemotional selectivity theory", "text": "Socioemotional selectivity theory\n\nSocioemotional selectivity theory (SST; developed by Stanford psychologist Laura L. Carstensen) is a life-span theory of motivation. The theory maintains that as time horizons shrink, as they typically do with age, people become increasingly selective, investing greater resources in emotionally meaningful goals and activities. According to the theory, motivational shifts also influence cognitive processing. Aging is associated with a relative preference for positive over negative information in attention and memory (called the \"positivity effect\").\n\nBecause they place a high value on emotional satisfaction, older adults often spend more time with familiar individuals with whom they have had rewarding relationships. This selective narrowing of social interaction maximizes positive emotional experiences and minimizes emotional risks as individuals become older. According to this theory, older adults systematically hone their social networks so that available social partners satisfy their emotional needs.\n\nThe theory also focuses on the types of goals that individuals are motivated to achieve. Knowledge-related goals aim at knowledge acquisition, career planning, the development of new social relationships and other endeavors that will pay off in the future. Emotion-related goals are aimed at emotion regulation, the pursuit of emotionally gratifying interactions with social partners and other pursuits whose benefits can be realized in the present.\n\nWhen people perceive their future as open ended, they tend to focus on future-oriented and development- or knowledge-related goals, but when they feel that time is running out and the opportunity to reap rewards from future-oriented goals' realization is dwindling, their focus tends to shift towards present-oriented and emotion- or pleasure-related goals. Research on this theory often compares age groups (\"e.g.\", young adulthood vs. old adulthood), but the shift in goal priorities is a gradual process that begins in early adulthood. Importantly, the theory contends that the cause of these goal shifts is not age itself, \"i.e.\", not the passage of time itself, but rather an age-associated shift in time perspective.\n\nThis justified shift in perspective is the rational equivalent of the psychological perceptual disorder known as \"foreshortened future,\" in which an individual, usually a young and physically healthy individual, unreasonably believes (either consciously or unconsciously) that his/her time horizons are more limited than they actually are, with the effect that the individual undervalues long-term goals and long-run pleasure and instead disproportionately pursues short-term goals and pleasure, thereby diverting resources from investment for the future and often even actively reducing his/her long-term prospects.\n\nResearchers have found that across diverse samples – ranging from Norwegians to Catholic nuns to African-Americans to Chinese Americans to European-Americans – older adults report better control of their emotions and fewer negative emotions than do younger adults. At the same time, culture seems to color how aging-related effects impact one's emotional life: Whereas older Americans were shown to de-emphasize negative experiences more than younger Americans, no such effect has been observed in Japan. Instead, older Japanese were shown to assign a greater value to positive aspects of otherwise negative experiences than younger Japanese, whereas no such effect has been observed in the U.S.\n\nStudies have found that older adults are more likely than younger adults to pay attention to positive than negative stimuli (as assessed by the dot-probe paradigm and eye-tracking methods). However, the effect also differs across cultures. For example, Hong Kong Chinese looked away from happy stimuli and more towards fearful stimuli, and the difference in attention pattern was related to differences in self-construal.\n\nThe term \"positivity effect\" also refers to age differences in emotional attention and memory. As people get older, they experience fewer negative emotions and they tend to look to the past in a positive light. In addition, compared with younger adults' memories, older adults' memories are more likely to consist of positive than negative information and more likely to be distorted in a positive direction. This version of the positivity effect was coined by Laura L. Carstensen's research team. There is a debate about the cross-cultural generalizability of the aging-related positivity effect, with some evidence for different types of emotional processing among Americans as compared to Japanese.\n\nOne theory of the positivity effect in older adults' memories is that it is produced by cognitive control mechanisms that improve and decrease negative information due to older adults' greater focus on emotional regulation. Research shows an age-related reversal in the valence of information processed within the medial prefrontal cortex (MPFC). In younger adults, more MPFC activity was found in the presence of negative stimuli compared to positive stimuli whereas in older adults this was reversed.\n\nHowever, the positivity effect may be different for stimuli processed automatically (pictures) and stimuli processed in a more controlled manner (words). Compared to words, pictures tend to be processed more rapidly and they engage emotion processing centres earlier. Automatic stimuli are processed in the amygdala and dorsal MPFC, whereas controlled stimuli are processed in the temporal pole and ventral MPFC. Compared to younger adults, older adults showed less amygdala activation and more MPFC activation for negative than positive pictures. Increased motivation to regulate emotion leads older adults to actively engage the mPFC differently from younger adults, which in turn yields diverging amygdala activation patterns. The opposite pattern was observed for words. Although older adults showed a positivity effect in memory for words, they did not display one for pictures. Thus, the positivity effect may arise from ageing differences in MPFC use during encoding.\n\n\n\n"}
{"id": "18793433", "url": "https://en.wikipedia.org/wiki?curid=18793433", "title": "Sohan Lal Jain", "text": "Sohan Lal Jain\n\nSohan Lal Jain is an Indian paleontologist, who worked for many years at the Indian Statistical Institute, Kolkata. The large herbivorous sauropod dinosaur genus \"Jainosaurus\", was named in his honour after it was identified as a distinct genus although initially thought to be a species of \"Antarctosaurus\". His other major contributions to paleontology were in the study of sauropod braincases and some fossil turtles.\n"}
{"id": "44036722", "url": "https://en.wikipedia.org/wiki?curid=44036722", "title": "The unsuccessful self-treatment of a case of \"writer's block\"", "text": "The unsuccessful self-treatment of a case of \"writer's block\"\n\nThe unsuccessful self-treatment of a case of \"writer's block\" is a humorous academic article by psychologist Dennis Upper about writer's block. It contains no content outside title and journal formatting elements, including a humorous footnote. Published in 1974 in a peer reviewed journal, \"Journal of Applied Behavior Analysis\", it is recognized as the shortest academic article ever and a classic example of humour in science, or at the very least among behavioral psychologists. It has been cited at least 60 times.\n\nThe article received a humorous positive review which was published alongside the article.\n\nThe article has led to at least three similarly humorous and peer-reviewed, published replication studies, and several similar papers.\n\nMore seriously, the paper is said to be a case reinforcing the image of a writer's block as a \"blank page\", and encouraging brevity in writing. It has been also used as an example that humor can indeed be found in academic publishing.\n"}
{"id": "13439882", "url": "https://en.wikipedia.org/wiki?curid=13439882", "title": "Universal conductance fluctuations", "text": "Universal conductance fluctuations\n\nUniversal conductance fluctuations (UCF) in mesoscopic physics is a phenomenon encountered in electrical transport experiments in mesoscopic species. The measured electrical conductance will vary from sample to sample, mainly due to inhomogeneous scattering sites. Fluctuations originate from coherence effects for electronic wavefunctions and thus the phase-coherence length formula_1 needs be larger than the momentum relaxation length formula_2. UCF is more profound when electrical transport is in weak localization regime. formula_3 where formula_4 , formula_5 is the number of conduction channels and formula_2is the momentum relaxation due to phonon scattering events length or mean free path. For weakly localized samples fluctuation in conductance is equal to fundamental conductance formula_7 regardless of the number of channels.\n\nMany factors will influence the amplitude of UCF. At zero temperature without decoherence, the UCF is influenced by mainly two factors, the symmetry and the shape of the sample. Recently, a third key factor, anisotropy of Fermi surface, is also found to fundamentally influence the amplitude of UCF.\n\n\n"}
{"id": "1437930", "url": "https://en.wikipedia.org/wiki?curid=1437930", "title": "William McBride (doctor)", "text": "William McBride (doctor)\n\nWilliam Griffith McBride CBE AO (25 May 1927 – 27 June 2018) was an Australian obstetrician. He discovered the teratogenicity of thalidomide, which resulted in the reduction of the number of drugs prescribed during pregnancy.\n\nMcBride was born in Sydney, New South Wales, Australia.\n\nDr McBride published a letter in \"The Lancet\", in December 1961, noting a large number of birth defects in children of patients who were prescribed thalidomide, after a midwife first suspected the drug was causing birth defects in the babies of patients under his care at Crown Street Women's Hospital in Sydney. Dr. McBride was awarded a medal and prize money by the prestigious L'Institut de la Vie, a French Institute, in connection with his discovery, in 1971. Using the prize money, he established Foundation 41, a Sydney-based medical research foundation concerned with the causes of birth defects. Working with Dr P H Huang, he proposed that thalidomide caused malformations by interacting with the DNA of the dividing embryonic cells. This finding stimulated their experimentation, which showed that thalidomide may inhibit cell division in rapidly dividing cells of malignant tumors. This work was published in the journal \"Pharmacology and Toxicology\" in 1999 and has been rated in the top ten of the most important Australian medical discoveries. \n\nMcBride's involvement in the Debendox case is less illustrious. In 1981 he published a paper indicating that the drug Debendox (marketed in the US as Bendectin) caused birth defects. His co-authors noted that the published paper contained manipulated data and protested but their voices went unheard. Multiple lawsuits followed by patients and McBride was a willing witness for the claimants. Eventually, the case was investigated and, as a result, McBride was struck off the Australian medical register in 1993 for deliberately falsifying data. An inquiry determined \"we are forced to conclude that Dr. McBride did publish statements which he either knew were untrue or which he did not genuinely believe to be true, and in that respect was guilty of scientific fraud.\" He was reinstated to the medical register in 1998.\n\nThe Bendectin case, and the subsequent removal of the drug from the US market, has had a number of consequences. Firstly, there was an immediate increase in the rates of hospitalization for nausea and vomiting in pregnancy. Secondly, it created a treatment void in terms of having a safe medication that could be used for alleviating morning sickness in US pregnant women, a condition which, in the most severe form, called hyperemesis gravidarum, could be both life-threatening and cause women to terminate their pregnancy.\n\nThe lack of availability of a safe and effective drug for the treatment of nausea and vomiting of pregnancy resulted in the use of other, less studied drugs in pregnancy. Thirdly, it has been claimed that subsequent to the Bendectin experience, drug companies stayed away from developing medications for pregnant patients. As a result, only a few medications were approved by the FDA for obstetrical indications in the past several decades. Lastly, the perception that all medications are teratogenic increased among pregnant women and healthcare professionals. The unfounded fear of using medications during pregnancy has precluded many women from receiving the treatment they require. Leaving medical conditions untreated during pregnancy can result in adverse pregnancy outcomes or significant morbidity for both the mother and baby. Ongoing education of physicians and the general public has resulted in improvements in the perception of medication use in pregnancy; however, further advances are required to overcome the devastating effects of the Bendectin saga.\n\nDr McBride was nominated Man of the Year for 1962, Commander of the Order of the British Empire (1969), Father of the Year (1972) and Officer of the Order of Australia (1977).\n\nMcBride died, aged 91, on 27 June 2018.\n\n"}
