{"id": "418100", "url": "https://en.wikipedia.org/wiki?curid=418100", "title": "A36 road", "text": "A36 road\n\nThe A36 is a trunk road and primary route in southwest England that links the port city of Southampton to the city of Bath. At Bath, the A36 connects with the A4 road to Bristol, thus providing a road link between the major ports of Southampton and Bristol. Originally, the A36 continued to Avonmouth, but this section was renumbered to the A4. On its way south from Bath the A36 passes a number of towns and a city, including Warminster, Wilton and Salisbury in Wiltshire, and Totton in Hampshire, on the western outskirts of Southampton, where it joins the A35.\n\nThe majority of the A36 is built to single carriageway standard, but parts of it have been upgraded to dual carriageway. The A36 is dual carriageway for its 1 mile (1.6 km) bypass of the village of Beckington, 3 miles (5 km) north of Frome, and also dualled for approximately 1 mile (1.6 km) near its grade separated junction with the A303 road, 8 miles (13 km) north-west of Wilton. The A36 in Salisbury acts as the city's ring road, bypassing the city centre to dual carriageway standard. Just south-east of Salisbury the largest dualled section runs for about 4 miles (6 km), bypassing the village of Alderbury. Then, the road is briefly dualled from its roundabout with the A3090 road to the M27 motorway (junction 2) – this part of the road is erroneously thought by some to have been previously known as the A36(M); prior to the opening of the Totton Western Bypass on the other side of the junction, it was named as a spur of the M27, and hence is built to motorway standards. The A36 reverts to single carriageway standard through Totton until it terminates.\n\n"}
{"id": "58081140", "url": "https://en.wikipedia.org/wiki?curid=58081140", "title": "Actino-ugpB RNA motif", "text": "Actino-ugpB RNA motif\n\nThe Actino-ugpB RNA motif is a conserved RNA structure that was discovered by bioinformatics.\nActino-ugpB motifs are found in strains of the species \"Gardnerella vaginalis\", within the phylum Actinobacteria.\n\nIt is ambiguous whether Actino-ugpB RNAs function as cis-regulatory elements or whether they operate in \"trans\". Many of the RNAs are upstream of the gene 'ugpB', which encodes a protein putatively involved in sugar transport. However, several of the RNAs are not located upstream of a protein-coding gene. Structurally, the motif consists of two hairpins with conserved nucleotides located in the stems and outside of the hairpins, but not in their terminal loops.\n"}
{"id": "8590426", "url": "https://en.wikipedia.org/wiki?curid=8590426", "title": "Athermalization", "text": "Athermalization\n\nAthermalization, in optics, is the process of achieving optothermal stability and ensuring that optical properties do not change with variations in temperature.\n\nIdeally, athermalization can be achieved by making the optics and metering structure out of the same material. This is very difficult in practice, as materials that are ideal for use in optics are often brittle or expensive, and are thus not always ideal for use in a metering structure. For this reason, athermalization is often achieved by using switchbacks or in-depth knowledge of the thermal elastic effects that are present in the particular system.\n"}
{"id": "1061913", "url": "https://en.wikipedia.org/wiki?curid=1061913", "title": "Chromotherapy", "text": "Chromotherapy\n\nChromotherapy, sometimes called color therapy, colorology or cromatherapy, is an alternative medicine method, which is considered pseudoscience. Chromotherapists claim to be able to use light in the form of color to balance \"energy\" lacking from a person's body, whether it be on physical, emotional, spiritual, or mental levels.\n\nColor therapy is distinct from other types of light therapy, such as neonatal jaundice treatment and blood irradiation therapy which is a scientifically accepted medical treatment for a number of conditions, and from photobiology, the scientific study of the effects of light on living organisms. The potential risk of retinal damage linked to chromotherapy has been discussed by French skeptic and lighting physicist Sébastien Point. Although Point considers that LED lamps at domestic radiance are safe in normal use for the general population, \nhe also pointed out the risk of overexposure to light from LEDs for practices like chromotherapy, when duration and time exposure are not under control. \n\nAvicenna (980–1037), seeing color as of vital importance both in diagnosis and in treatment, discussed chromotherapy in \"The Canon of Medicine\". He wrote that \"color is an observable symptom of disease\" and also developed a chart that related color to the temperature and physical condition of the body. His view was that red moved the blood, blue or white cooled it, and yellow reduced muscular pain and inflammation.\n\nAmerican Civil War General Augustus Pleasonton (1801–1894) conducted his own experiments and in 1876 published his book \"The Influence Of The Blue Ray Of The Sunlight And Of The Blue Color Of The Sky\" about how the color blue can improve the growth of crops and livestock and can help heal diseases in humans. This led to modern chromotherapy, influencing scientist Dr. Seth Pancoast (1823–1889) and Edwin Dwight Babbitt (1828–1905) to conduct experiments and to publish, respectively, \"Blue and Red Light; or, Light and Its Rays as Medicine\" (1877) and \"The Principles of Light and Color\".\n\nIn 1933, Indian-born American-citizen scientist Dinshah P. Ghadiali (1873–1966), published \"The Spectro Chromemetry Encyclopaedia\", a work on color therapy. Ghadiali claimed to have discovered why and how the different colored rays have various therapeutic effects on organisms. He believed that colors represent chemical potencies in higher octaves of vibration, and for each organism and system of the body there is a particular color that stimulates and another that inhibits the work of that organ or system. Ghadiali also thought that by knowing the action of the different colors upon the different organs and systems of the body, one can apply the correct color that will tend to balance the action of any organ or system that has become abnormal in its functioning or condition. Dinshah P. Ghadiali's son Darius Dinshah continues to provide information about color therapy via his Dinshah Health Society, a nonprofit organization dedicated to advancing non-pharmaceutical home color therapy, and his book \"Let There Be Light\".\n\nScience writer Martin Gardner had described Ghadiali as \"perhaps the greatest quack of them all\". In 1925, Ghadiali was accused of rape and arrested in Seattle and sentenced under the Mann Act for five years at the United States Penitentiary, Atlanta. According to Gardner, photographs of Ghadiali at work in his laboratory are \"indistinguishable from stills of a grade D movie about a mad scientist\".\n\nThroughout the 19th century \"color healers\" claimed colored glass filters could treat many diseases including constipation and meningitis.\n\nPractitioners of ayurvedic medicine believe the body has seven \"chakras\", which some claim are 'spiritual centers', and which are held to be located along the spine. New Age thought associates each of the chakras with a single color of the visible light spectrum, along with a function and organ or bodily system. According to this view, the chakras can become imbalanced and result in physical diseases, but application of the appropriate color can allegedly correct such imbalances. The purported colors and their associations are described as:\n\nChromotherapy is regarded by health experts as quackery.\n\nAccording to a book published by the American Cancer Society, \"available scientific evidence does not support claims that alternative uses of light or color therapy are effective in treating cancer or other illnesses\".\n\nPhotobiology, the term for the scientific study of the effects of light on living tissue, has sometimes been used instead of the term chromotherapy in an effort to distance it from its roots in Victorian mysticism and to strip it of its associations with symbolism and magic. Light therapy is a specific treatment approach using high intensity light to treat specific sleep, skin and mood disorders.\n\n\n"}
{"id": "27918920", "url": "https://en.wikipedia.org/wiki?curid=27918920", "title": "Contextualization (sociolinguistics)", "text": "Contextualization (sociolinguistics)\n\nContextualization in sociolinguistics refers to the use of language and discourse to signal relevant aspects of an interactional or communicative situation. Basil Bernstein (1990 [1971]) uses (re)contextualization when referring to the reformulation of scientific knowledge in pedagogical contexts, for instance in textbooks. John Gumperz (1982a, 1982b) and others in interactional sociolinguistics study subtle \"contextualization cues\", for instance intonation, that allow language users to infer contextually adequate meanings of discourse (see also Eerdmans, Prevignano & Thibault, 2002).\n\nGumperz (1982a) suggests that in the following interaction the linguistic style used by the interviewer signals a context different from that expected by the husband. The interviewer, an African-American graduate student in educational psychology, has been sent to interview a woman at her home in a low-income neighborhood. The interviewer rings the door bell and the woman's husband opens the door.\n\nThe husband addresses the interviewer in an informal style, marking their interaction as friendly. When the interviewer responds in a more formal style, the context becomes more formal. As a result, the interviewer reports that the interview was \"stiff\" (Gumperz 1982a: 133).\n\n"}
{"id": "50741369", "url": "https://en.wikipedia.org/wiki?curid=50741369", "title": "Dead mother complex", "text": "Dead mother complex\n\nThe dead mother complex is a clinical condition described by Andre Green involving an early and destructive identification with the figure of a 'dead' – or rather depressed and emotionally unavailable – mother.\n\nGreen introduced the concept in an essay written in 1980, published in 1983, and translated into English in 1986. He saw the dead mother complex as involving a mother who was initially emotionally engaged with her child, but who then \"switched off\" from emotional resonance to emotional detachment, perhaps under the influence of loss and mourning in her own family of origin. The impact on the child, when it finds itself unable to restore a feeling contact, is the internalisation of a hard unresponsive emotional core, which fosters a destructive form of narcissism, contributes to attachment disorders, and reveals itself as a major resistance to progress in the transference.\n\nLater, writers have argued for differentiating a range of responses within the dead mother complex, reserving the name dead mother syndrome for the most acute form.\n\n\n\n"}
{"id": "36854472", "url": "https://en.wikipedia.org/wiki?curid=36854472", "title": "Double-Blind FROG", "text": "Double-Blind FROG\n\nDouble-Blind FROG is a method for simultaneously measuring two unknown ultrashort laser pulses. Well established ultrafast measurement techniques like Frequency-Resolved Optical Gating (FROG) and its simplified version GRENOUILLE can only measure one unknown ultrashort laser pulse at a time. Another version of FROG, called cross-correlation FROG (XFROG), also measures only one pulse, but it involves two pulses: a known reference pulse and the unknown pulse to be measured.\n\nIn modern optics experiments, ultrashort laser pulses have been used in a great variety of engineering application and scientific research, for example, biomedical engineering, material science, nonlinear spectroscopy, ultrafast chemistry, etc. Often, these experiments involve using two potentially different input laser pulses, for example, Raman spectroscopy, two-color pump-probe experiments, and non-degenerate four-wave mixing. In many situations, an output pulse is generated by a nonlinear optical process, such as harmonic generation, continuum generation, or optical parametric oscillation. In all such cases, measuring more than one pulse simultaneously is required to completely characterize the experiment and understand its results in order to eventually understand the underlying science of the process under study. Thus a measurement device capable of measuring two pulses simultaneously is highly desired.\n\nEarly attempts to solve the two-pulse measurement problem were made by Trebino and Kane and co-workers beginning in 1995. They took advantage of the fact that FROG and its variations involved crossing two replicas of the pulse to be measured in a nonlinear-optical medium (where one gated the other in time) and measuring the spectrum of the product of the two pulse electric fields vs. delay. Thus, instead of using a known reference pulse as the gate, the second unknown pulse was used as the gate. The mathematical form of the measured trace is, formula_1. This particular experimental setup has come to be known as Blind FROG due to its mathematical equivalence to Blind Deconvolution.\nUnfortunately, retrieval of two pulses from the Blind FROG trace proved to be ill-posed; many different pulse pairs yield the same measured trace. Without extra information about the pulses, such as the spectra, non-trivial ambiguities are found by the Blind FROG retrieval algorithm. However, even with the spectra, algorithm convergence was found to be too slow.\n\nThe attosecond-laser-pulse community, however, finds the Blind FROG approach useful due to the specific mathematical form used in the retrieval algorithm in this case. On the other hand, for more common, longer pulses, improvements were required.\nIn 2002, in his book, Trebino proposed “Double Blind FROG” (DB FROG) to solve the two-pulse measurement problem. DB FROG is a slightly modified version of Blind FROG in which two FROG traces are measured. This second FROG trace contains the extra information required to retrieve both pulses essentially uniquely (with only trivial ambiguities, such as the zeroth-order phase and the first-order spectral phase, which corresponds to the pulses’ average arrival time).\n\nTo understand DB FROG, it helps to consider a particular FROG beam geometry. Here we consider the polarization-gate (PG) geometry. The modification required to turn a Blind PG FROG into Double Blind PG FROG is the addition of a pair of crossed polarizers and a spectrometer. The main idea behind DB FROG is that, when one pulse gates the other, the other must also gate the one. In Blind FROG (see Blind FROG schematic), the gated pulse 2 is simply dumped after the nonlinear medium. But in DB PG FROG (see Figure 2), it passes through a crossed polarizer and is spectrally resolved to generate a second FROG trace. In DB PG FROG, trace 1 is proportional to formula_2 where formula_3 and formula_4 are the two light pulse electric fields. This expression is identical to that in PG XFROG. Trace 2 is proportional to formula_5 where the roles of the two pulses are reversed. This term, of course, also has the form of a FROG trace produced by PG XFROG. The DB FROG retrieval algorithm uses information contained in both traces to retrieve both of the unknown pulses.\nA unique DB FROG retrieval algorithm, based on the XFROG algorithm, is used to retrieve the two unknown pulses by making use of the two recorded traces. The retrieval algorithm divides the whole retrieval problem into two XFROG problems. It starts with random initial guesses for both unknown pulses and takes one of the unknown two pulses, say, formula_3 as the unknown and formula_4 as the gate. Even though formula_4 is not the correct gate pulse to begin with, the algorithm treats it as if it is the correct one. The standard XFROG algorithm along with trace 1 is used to retrieve formula_3. The returned formula_3 is not the correct one, but it is an improved version of it, since trace 1 contains information about formula_3. This improved version of formula_3 is then used in the next step of the retrieval algorithm, which now reverses the roles of formula_3 and formula_4 (formula_3 as known and formula_4 as the unknown) and runs trace 2 with standard XFROG algorithm to produce a better version of formula_4. This completes one cycle of the retrieval algorithm and improves both formula_3 and formula_4. The retrieval algorithm then uses the improved results for formula_3 and formula_4 to perform the next cycle. It continues alternating between traces 1 and 2 until the desired agreement between measured and retrieved traces occurs. It typically takes 3-5 cycles to converge depending on the complexity of the pulse pair. Also, complete convergence on early iterations is not necessary.\n\nPulse pairs with Time Bandwidth Products (TBP) ranging from 1 to 6 and also different wavelengths have been measured and retrieved experimentally using DB PG FROG. These measurements demonstrated that the DB FROG retrieval algorithm is capable of ignoring experimental noise and various inevitable non-physical details in the recorded traces and that it returns the correct retrieved pulse.\n\nIn addition to experimental work, numerical simulations have also shown that the DB FROG retrieval algorithm is extremely robust and reliable.\n\nDepending on the gating geometry of the experimental setup, DB FROG inherits both advantages and disadvantages from the particular geometry. In the case of Polarization-Gate geometry, the advantage is the infinite phase-matching bandwidth which makes the system alignment insensitive. On the other hand, a disadvantage of PG geometry is the requirement of high-quality polarizers (calcite polarizers work fine) which could be expensive and introduce non-negligible distortion into the pulse. This distortion could be removed by numerically back propagating the pulse through the polarizer. DB FROG is promising and, although not in widespread use, it is a subject of active current research.\n\n\n"}
{"id": "1596852", "url": "https://en.wikipedia.org/wiki?curid=1596852", "title": "Edward W. Berry", "text": "Edward W. Berry\n\nEdward Wilber Berry (February 10, 1875 – September 20, 1945) was an American paleontologist and botanist; the principal focus of his research was paleobotany. Berry studied North and South American flora and published taxonomic studies with theoretical reconstructions of paleoecology and phytogeography. He started his scientific career as an amateur scientist. At Johns Hopkins University he held various positions including teacher, research scientist, scientific editor and administrator.\n\n\n\n\n\n\n"}
{"id": "4184055", "url": "https://en.wikipedia.org/wiki?curid=4184055", "title": "F-ratio", "text": "F-ratio\n\nIn oceanic biogeochemistry, the f-ratio is the fraction of total primary production fuelled by nitrate (as opposed to that fuelled by other nitrogen compounds such as ammonium). The ratio was originally defined by Richard Eppley and Bruce Peterson in one of the first papers estimating global oceanic production. This fraction was originally believed significant because it appeared to directly relate to the sinking (export) flux of organic marine snow from the surface ocean by the biological pump. However, this interpretation relied on the assumption of a strong depth-partitioning of a parallel process, nitrification, that more recent measurements has questioned.\n\nGravitational sinking of organisms (or the remains of organisms) transfers carbon from the surface waters of the ocean to its deep interior. This process is known as the biological pump, and quantifying it is of interest to scientists because it is an important aspect of the Earth's carbon cycle. Essentially, this is because carbon transported to the deep ocean is isolated from the atmosphere, allowing the ocean to act as a reservoir of carbon. This biological mechanism is accompanied by a physico-chemical mechanism known as the solubility pump which also acts to transfer carbon to the ocean's deep interior.\n\nMeasuring the flux of sinking material (so-called marine snow) is usually done by deploying sediment traps which intercept and store material as it sinks down the water column. However, this is a relatively difficult process, since traps can be awkward to deploy or recover, and they must be left \"in situ\" over a long period to integrate the sinking flux. Furthermore, they are known to experience biases and to integrate horizontal as well as vertical fluxes because of water currents. For this reason, scientists are interested in ocean properties that can be more easily measured, and that act as a proxy for the sinking flux. The f-ratio is one such proxy.\n\nBio-available nitrogen occurs in the ocean in several forms, including simple ionic forms such as nitrate (NO), nitrite (NO) and ammonium (NH), and more complex organic forms such as urea ((NH)CO). These forms are used by autotrophic phytoplankton to synthesise organic molecules such as amino acids (the building blocks of proteins). Grazing of phytoplankton by zooplankton and larger organisms transfers this organic nitrogen up the food chain and throughout the marine food-web.\n\nWhen nitrogenous organic molecules are ultimately metabolised by organisms, they are returned to the water column as ammonium (or more complex molecules that are then metabolised to ammonium). This is known as \"regeneration\", since the ammonium can be used by phytoplankton, and again enter the food-web. Primary production fuelled by ammonium in this way is thus referred to as regenerated production.\n\nHowever, ammonium can also be oxidised to nitrate (via nitrite), by the process of nitrification. This is performed by different bacteria in two stages :\n\nCrucially, this process is believed to only occur in the absence of light (or as some other function of depth). In the ocean, this leads to a vertical separation of nitrification from primary production, and confines it to the aphotic zone. This leads to the situation whereby any nitrate in the water column must be from the aphotic zone, and must have originated from organic material transported there by sinking. Primary production fuelled by nitrate is, therefore, making use of a \"fresh\" nutrient source rather than a regenerated one. Production by nitrate is thus referred to as new production.\n\nThe figure at the head of this section illustrates this. Nitrate and ammonium are taken up by primary producers, processed through the food-web, and then regenerated as ammonium. Some of this return flux is released into the surface ocean (where it is available again for uptake), while some is returned at depth. The ammonium returned at depth is nitrified to nitrate, and ultimately mixed or upwelled into the surface ocean to repeat the cycle.\n\nConsequently, the significance of new production lies in its connection to sinking material. At equilibrium, the export flux of organic material sinking into the aphotic zone is balanced by the upward flux of nitrate. By measuring how much nitrate is consumed by primary production, relative to that of regenerated ammonium, one should be able to estimate the export flux indirectly.\n\nAs an aside, the f-ratio can also reveal important aspects of local ecosystem function. High f-ratio values are typically associated with productive ecosystems dominated by large, eukaryotic phytoplankton (such as diatoms) that are grazed by large zooplankton (and, in turn, by larger organisms such as fish). By contrast, low f-ratio values are generally associated with low biomass, oligotrophic food webs consisting of small, prokaryotic phytoplankton (such as \"Prochlorococcus\") which are kept in check by microzooplankton.\n\nA fundamental assumption in this interpretation of the f-ratio is the spatial separation of primary production and nitrification. Indeed, in their original paper, Eppley & Peterson noted that: \"To relate new production to export requires that nitrification in the euphotic zone be negligible\". However, subsequent observational work on the distribution of nitrification has found that nitrification can occur at shallower depths, and even within the photic zone.\n\nAs the adjacent diagram shows, if ammonium is indeed nitrified to nitrate in the ocean's surface waters it essentially \"short circuits\" the deep pathway of nitrate. In practice, this would lead to an overestimation of new production and a higher f-ratio, since some of the ostensibly new production would actually be fuelled by recently nitrified nitrate that had never left the surface ocean. After including nitrification measurements in its parameterisation, an ecosystem model of the oligotrophic subtropical gyre region (specifically the BATS site) found that, on an annual basis, around 40% of surface nitrate was recently nitrified (rising to almost 90% during summer). A further study synthesising geographically diverse nitrification measurements found high variability but no relationship with depth, and applied this in a global-scale model to estimate that up to a half of surface nitrate is supplied by surface nitrification rather than upwelling.\n\nAlthough measurements of the rate of nitrification are still relatively rare, they do suggest that the f-ratio is not as straightforward a proxy for the biological pump as was once thought. For this reason, some workers have proposed distinguishing between the f-ratio and the ratio of particulate export to primary production, which they term the pe-ratio. While quantitatively different than the f-ratio, the pe-ratio shows similar qualitative variation between high productivity/high biomass/high export regimes and low productivity/low biomass/low export regimes.\n\nIn addition, a further process that potentially complicates the use of the f-ratio to estimate \"new\" and \"regenerated\" production is dissimilatory nitrate reduction to ammonium (DNRA). In low oxygen environments, such as oxygen minimum zones and seafloor sediments, chemoorganoheterotrophic microbes use nitrate as an electron acceptor for respiration, reducing it to nitrite, then to ammonium. Since, like nitrification, DNRA alters the balance in the availability of nitrate and ammonium, it has the potential to introduce inaccuracy to the calculated f-ratio. However, as DNRA's occurrence is limited to anaerobic situations, its importance is less widespread than nitrification, although it can occur in association with primary producers.\n"}
{"id": "91127", "url": "https://en.wikipedia.org/wiki?curid=91127", "title": "Fermat number", "text": "Fermat number\n\nIn mathematics a Fermat number, named after Pierre de Fermat who first studied them, is a positive integer of the form\n\nwhere \"n\" is a nonnegative integer. The first few Fermat numbers are:\n\nIf 2 + 1 is prime, and \"k\" > 0, it can be shown that \"k\" must be a power of two. (If \"k\" = \"ab\" where 1 ≤ \"a\", \"b\" ≤ \"k\" and \"b\" is odd, then 2 + 1 = (2) + 1 ≡ (−1) + 1 = 0 (mod 2 + 1). See below for a complete proof.) In other words, every prime of the form 2 + 1 (other than 2 = 2 + 1) is a Fermat number, and such primes are called Fermat primes. As of 2018, the only known Fermat primes are \"F\", \"F\", \"F\", \"F\", and \"F\" .\n\nThe Fermat numbers satisfy the following recurrence relations:\n\nfor \"n\" ≥ 1,\n\nfor \"n\" ≥ 2. Each of these relations can be proved by mathematical induction. From the last equation, we can deduce Goldbach's theorem (named after Christian Goldbach): no two Fermat numbers share a common integer factor greater than 1. To see this, suppose that 0 ≤ \"i\" < \"j\" and \"F\" and \"F\" have a common factor \"a\" > 1. Then \"a\" divides both\n\nand \"F\"; hence \"a\" divides their difference, 2. Since \"a\" > 1, this forces \"a\" = 2. This is a contradiction, because each Fermat number is clearly odd. As a corollary, we obtain another proof of the infinitude of the prime numbers: for each \"F\", choose a prime factor \"p\"; then the sequence {\"p\"} is an infinite sequence of distinct primes.\n\nFurther properties:\n\n\nFermat numbers and Fermat primes were first studied by Pierre de Fermat, who conjectured (but admitted he could not prove) that all Fermat numbers are prime. Indeed, the first five Fermat numbers \"F\"...,\"F\" are easily shown to be prime. However, the conjecture was refuted by Leonhard Euler in 1732 when he showed that\n\nEuler proved that every factor of \"F\" must have the form \"k\"2 + 1 (later improved to \"k\"2 + 1 by Lucas).\n\nThe fact that 641 is a factor of \"F\" can be easily deduced from the equalities 641 = 2×5+1 and 641 = 2 + 5. It follows from the first equality that 2×5 ≡ −1 (mod 641) and therefore (raising to the fourth power) that 2×5 ≡ 1 (mod 641). On the other hand, the second equality implies that 5 ≡ −2 (mod 641). These congruences imply that −2 ≡ 1 (mod 641).\n\nFermat was probably aware of the form of the factors later proved by Euler, so it seems curious why he failed to follow through on the straightforward calculation to find the factor. One common explanation is that Fermat made a computational mistake.\n\nThere are no other known Fermat primes \"F\" with \"n\" > 4. However, little is known about Fermat numbers with large \"n\". In fact, each of the following is an open problem:\n\n, it is known that \"F\" is composite for , although amongst these, complete factorizations of \"F\" are known only for , and there are no known prime factors for and . The largest Fermat number known to be composite is \"F\", and its prime factor , a megaprime, was discovered by the PrimeGrid collaboration in July 2014.\n\nThere are several probabilistic arguments for estimating the number of Fermat primes. However these arguments give quite different estimates, depending on how much information about Fermat numbers one uses, and some predict no further Fermat primes while others predict infinitely many Fermat primes.\n\nThe following heuristic argument suggests there are only finitely many Fermat primes: according to the prime number theorem, the \"probability\" that a number \"n\" is prime is about 1/ln(\"n\"). Therefore, the total expected number of Fermat primes is at most\n\nThis argument is not a rigorous proof. For one thing, the argument assumes that Fermat numbers behave \"randomly\", yet we have already seen that the factors of Fermat numbers have special properties.\n\nIf (more sophisticatedly) we regard the \"conditional\" probability that \"n\" is prime, given that we know all its prime factors exceed \"B\", as at most \"A\" ln(\"B\") / ln(\"n\"), then using Euler's theorem that the least prime factor of \"F\" exceeds , we would find instead\n\nAlthough such arguments engender the belief that there are only finitely many Fermat primes, one can also produce arguments for the opposite conclusion. Suppose we regard the conditional probability that \"n\" is prime, given that we know all its prime factors are 1 modulo \"M\", as at least \"CM\"/ln(\"n\"). Then using Euler's result that \"M\" = 2 we would find that the expected total number of Fermat primes was at least\n\nand indeed this argument predicts that an asymptotically \"constant fraction\" of Fermat numbers are prime.\n\nLet formula_11 be the \"n\"th Fermat number. Pépin's test states that for \"n\" > 0,\n\nThe expression formula_14 can be evaluated modulo formula_12 by repeated squaring. This makes the test a fast polynomial-time algorithm. However, Fermat numbers grow so rapidly that only a handful of Fermat numbers can be tested in a reasonable amount of time and space.\n\nThere are some tests that can be used to test numbers of the form \"k\"2 + 1, such as factors of Fermat numbers, for primality.\n\nIf \"N\" = \"F\" > 3, then the above Jacobi symbol is always equal to −1 for \"a\" = 3, and this special case of Proth's theorem is known as Pépin's test. Although Pépin's test and Proth's theorem have been implemented on computers to prove the compositeness of some Fermat numbers, neither test gives a specific nontrivial factor. In fact, no specific prime factors are known for \"n\" = 20 and 24.\n\nBecause of the size of Fermat numbers, it is difficult to factorize or even to check primality. Pépin's test gives a necessary and sufficient condition for primality of Fermat numbers, and can be implemented by modern computers. The elliptic curve method is a fast method for finding small prime divisors of numbers. Distributed computing project \"Fermatsearch\" has successfully found some factors of Fermat numbers. Yves Gallot's proth.exe has been used to find factors of large Fermat numbers. Édouard Lucas, improving the above-mentioned result by Euler, proved in 1878 that every factor of Fermat number formula_12, with \"n\" at least 2, is of the form formula_19 (see Proth number), where \"k\" is a positive integer. By itself, this makes it easy to prove the primality of the known Fermat primes.\n\nFactorizations of the first twelve Fermat numbers are:\n\n, only \"F\" to \"F\" have been completely factored. The distributed computing project Fermat Search is searching for new factors of Fermat numbers. The set of all Fermat factors is (or, sorted, ) in OEIS.\n\nIt is possible that the only primes of this form are 3, 5, 17, 257 and 65,537. Indeed, Boklan and Conway published in 2016 a very precise analysis suggesting that the probability of the existence of another Fermat prime is less than one in a billion.\n\nThe following factors of Fermat numbers were known before 1950 (since the '50s, digital computers have helped find more factors):\n\n, 342 prime factors of Fermat numbers are known, and 298 Fermat numbers are known to be composite. Several new Fermat factors are found each year.\n\nLike composite numbers of the form 2 − 1, every composite Fermat number is a strong pseudoprime to base 2. This is because all strong pseudoprimes to base 2 are also Fermat pseudoprimes - i.e.\n\nfor all Fermat numbers.\n\nIn 1904, Cipolla showed that the product of at least two distinct prime or composite Fermat numbers formula_21 will be a Fermat pseudoprime to base 2 if and only if formula_22.\n\n \\right)^{2} \\equiv 2^{1+2^{n-1}} \\pmod p.</math> \n\nSince an odd power of 2 is a quadratic residue modulo \"p\", so is 2 itself.\n\nCarl Friedrich Gauss developed the theory of Gaussian periods in his \"Disquisitiones Arithmeticae\" and formulated a sufficient condition for the constructibility of regular polygons. Gauss stated without proof that this condition was also necessary, but never published his proof. A full proof of necessity was given by Pierre Wantzel in 1837. The result is known as the Gauss–Wantzel theorem:\n\nA positive integer \"n\" is of the above form if and only if its totient φ(\"n\") is a power of 2.\n\nFermat primes are particularly useful in generating pseudo-random sequences of numbers in the range 1 … \"N\", where \"N\" is a power of 2. The most common method used is to take any seed value between 1 and \"P\" − 1, where \"P\" is a Fermat prime. Now multiply this by a number \"A\", which is greater than the square root of \"P\" and is a primitive root modulo \"P\" (i.e., it is not a quadratic residue). Then take the result modulo \"P\". The result is the new value for the RNG.\nThis is useful in computer science since most data structures have members with 2 possible values. For example, a byte has 256 (2) possible values (0–255). Therefore, to fill a byte or bytes with random values a random number generator which produces values 1–256 can be used, the byte taking the output value − 1. Very large Fermat primes are of particular interest in data encryption for this reason. This method produces only pseudorandom values as, after \"P\" − 1 repetitions, the sequence repeats. A poorly chosen multiplier can result in the sequence repeating sooner than \"P\" − 1.\n\nA Fermat number cannot be a perfect number or part of a pair of amicable numbers. \n\nThe series of reciprocals of all prime divisors of Fermat numbers is convergent. \n\nIf \"n\" + 1 is prime, there exists an integer \"m\" such that \"n\" = 2. The equation\n\"n\" + 1 = \"F\"\nholds in that case.\n\nLet the largest prime factor of Fermat number \"F\" be \"P\"(\"F\"). Then,\n\nNumbers of the form formula_25 with \"a\", \"b\" any coprime integers, \"a\" > \"b\" > 0, are called generalized Fermat numbers. An odd prime \"p\" is a generalized Fermat number if and only if \"p\" is congruent to 1 (mod 4). (Here we consider only the case \"n\" > 0, so 3 = formula_26 is not a counterexample.)\n\nAn example of a probable prime of this form is 124 + 57 (found by Serge Batalov).\n\nBy analogy with the ordinary Fermat numbers, it is common to write generalized Fermat numbers of the form formula_27 as \"F\"(\"a\"). In this notation, for instance, the number 100,000,001 would be written as \"F\"(10). In the following we shall restrict ourselves to primes of this form, formula_27, such primes are called \"Fermat primes base \"a\"\". Of course, these primes exist only if \"a\" is even.\n\nIf we require \"n\" > 0, then Landau's fourth problem asks if there are infinitely many generalized Fermat primes \"F\"(\"a\").\n\nBecause of the ease of proving their primality, generalized Fermat primes have become in recent years a topic for research within the field of number theory. Many of the largest known primes today are generalized Fermat primes.\n\nGeneralized Fermat numbers can be prime only for even , because if is odd then every generalized Fermat number will be divisible by 2. The smallest prime number formula_29 with formula_30 is formula_31, or 30+1. Besides, we can define \"half generalized Fermat numbers\" for an odd base, a half generalized Fermat number to base \"a\" (for odd \"a\") is formula_32, and it is also to be expected that there will be only finitely many half generalized Fermat primes for each odd base.\n\nThe smallest base \"b\" such that \"b\" + 1 is prime are\n\nThe smallest \"k\" such that (2\"n\") + 1 is prime are\n\nA more elaborate theory can be used to predict the number of bases for which formula_29 will be prime for fixed formula_36. The number of generalized Fermat primes can be roughly expected to halve as formula_36 is increased by 1.\n\nThe following is a list of the 5 largest known generalized Fermat primes. They are all megaprimes. the whole top-5 was discovered by participants in the PrimeGrid project.\n\nOn the Prime Pages you can perform a search yielding the current top 100 generalized Fermat primes.\n\n\n\n"}
{"id": "2980233", "url": "https://en.wikipedia.org/wiki?curid=2980233", "title": "Focused ion beam", "text": "Focused ion beam\n\nFocused ion beam, also known as FIB, is a technique used particularly in the semiconductor industry, materials science and increasingly in the biological field for site-specific analysis, deposition, and ablation of materials. A FIB setup is a scientific instrument that resembles a scanning electron microscope (SEM). However, while the SEM uses a focused beam of electrons to image the sample in the chamber, a FIB setup uses a focused beam of ions instead. FIB can also be incorporated in a system with both electron and ion beam columns, allowing the same feature to be investigated using either of the beams. FIB should not be confused with using a beam of focused ions for direct write lithography (such as in proton beam writing). These are generally quite different systems where the material is modified by other mechanisms.\n\nMost widespread instruments are using liquid metal ion sources (LMIS), especially gallium ion sources. Ion sources based on elemental gold and iridium are also available. In a gallium LMIS, gallium metal is placed in contact with a tungsten needle, and heated gallium wets the tungsten and flows to the tip of the needle, where the opposing forces of surface tension and electric field form the gallium into a cusp shaped tip called a Taylor cone. The tip radius of this cone is extremely small (~2 nm). The huge electric field at this small tip (greater than 1 x 10 volts per centimeter) causes ionization and field emission of the gallium atoms.\n\nSource ions are then generally accelerated to an energy of 1–50 keV (kiloelectronvolts), and focused onto the sample by electrostatic lenses. LMIS produce high current density ion beams with very small energy spread. A modern FIB can deliver tens of nanoamperes of current to a sample, or can image the sample with a spot size on the order of a few nanometers.\n\nMore recently, instruments using plasma beams of noble gas ions, such as Xenon, have become available more widely. \n\n Focused ion beam (FIB) systems have been produced commercially for approximately twenty years, primarily for large semiconductor manufacturers. FIB systems operate in a similar fashion to a scanning electron microscope (SEM) except, rather than a beam of electrons and as the name implies, FIB systems use a finely focused beam of ions (usually gallium) that can be operated at low beam currents for imaging or at high beam currents for site specific sputtering or milling.\n\nAs the diagram on the right shows, the gallium (Ga+) primary ion beam hits the sample surface and sputters a small amount of material, which leaves the surface as either secondary ions (i+ or i-) or neutral atoms (n). The primary beam also produces secondary electrons (e). As the primary beam rasters on the sample surface, the signal from the sputtered ions or secondary electrons is collected to form an image.\n\nAt low primary beam currents, very little material is sputtered and modern FIB systems can easily achieve 5 nm imaging resolution (imaging resolution with Ga ions is limited to ~5 nm by sputtering and detector efficiency). At higher primary currents, a great deal of material can be removed by sputtering, allowing precision milling of the specimen down to a sub micrometer or even a nano scale.\n\nIf the sample is non-conductive, a low energy electron flood gun can be used to provide charge neutralization. In this manner, by imaging with positive secondary ions using the positive primary ion beam, even highly insulating samples may be imaged and milled without a conducting surface coating, as would be required in an SEM.\n\nUntil recently, the overwhelming usage of FIB has been in the semiconductor industry. Such applications as defect analysis, circuit modification, photomask repair and transmission electron microscope (TEM) sample preparation of site specific locations on integrated circuits have become commonplace procedures. The latest FIB systems have high resolution imaging capability; this capability coupled with in situ sectioning has eliminated the need, in many cases, to examine FIB sectioned specimens in a separate SEM instrument. SEM imaging is still required for the highest resolution imaging and to prevent damage to sensitive samples. However, the combination of SEM and FIB columns onto the same chamber enables the benefits of both to be utilized.\n\nUnlike an electron microscope, FIB is inherently destructive to the specimen. When the high-energy gallium ions strike the sample, they will sputter atoms from the surface. Gallium atoms will also be implanted into the top few nanometers of the surface, and the surface will be made amorphous.\n\nBecause of the sputtering capability, the FIB is used as a micro- and nano-machining tool, to modify or machine materials at the micro- and nanoscale. FIB micro machining has become a broad field of its own, but nano machining with FIB is a field that is still developing. Commonly the smallest beam size for imaging is 2.5–6 nm. The smallest milled features are somewhat larger (10–15 nm) as this is dependent on the total beam size and interactions with the sample being milled.\n\nFIB tools are designed to etch or machine surfaces, an ideal FIB might machine away one atom layer without any disruption of the atoms in the next layer, or any residual disruptions above the surface. Yet currently because of the sputter the machining typically roughens surfaces at the sub-micrometer length scales. \nA FIB can also be used to deposit material via ion beam induced deposition. FIB-assisted chemical vapor deposition occurs when a gas, such as tungsten hexacarbonyl (W(CO)) is introduced to the vacuum chamber and allowed to chemisorb onto the sample. By scanning an area with the beam, the precursor gas will be decomposed into volatile and non-volatile components; the non-volatile component, such as tungsten, remains on the surface as a deposition. This is useful, as the deposited metal can be used as a sacrificial layer, to protect the underlying sample from the destructive sputtering of the beam. From nanometers to hundred of micrometers in length, tungsten metal deposition allows metal lines to be put right where needed. Other materials such as platinum, cobalt, carbon, gold, etc., can also be locally deposited. Gas assisted deposition and FIB etching process are shown below.\nFIB is often used in the semiconductor industry to patch or modify an existing semiconductor device. For example, in an integrated circuit, the gallium beam could be used to cut unwanted electrical connections, and/or to deposit conductive material in order to make a connection. The high level of surface interaction is exploited in patterned doping of semiconductors. FIB is also used for maskless implantation.\n\nThe FIB is also commonly used to prepare samples for the transmission electron microscope. The TEM requires very thin samples, typically ~100 nanometers or less. Other techniques, such as ion milling or electropolishing can be used to prepare such thin samples. However, the nanometer-scale resolution of the FIB allows the exact region of interest to be chosen, such as perhaps a grain boundary or defect in a material. This is vital, for example, in integrated circuit failure analysis. If a particular transistor out of several million on a chip is bad, the only tool capable of preparing an electron microscope sample of that single transistor is the FIB. \nThe same protocol used for preparing samples to transmission electron microscopy can also be used to select a micro area of a sample, extract it and prepare it for analysis using a Secondary ion mass spectrometry (SIMS).\n\nThe drawbacks to FIB sample preparation are the above-mentioned surface damage and implantation, which produce noticeable effects when using techniques such as high-resolution \"lattice imaging\" TEM or electron energy loss spectroscopy. This damaged layer can be minimized by FIB milling with lower beam voltages, or by further milling with a low-voltage argon ion beam after completion of the FIB process.\n\nFIB preparation can be used with cryogenically frozen samples in a suitably equipped instrument, allowing cross sectional analysis of samples containing liquids or fats, such as biological samples, pharmaceuticals, foams, inks, and food products.\n\nFIB is also used for Secondary ion mass spectrometry (SIMS). The ejected secondary ions are collected and analyzed after the surface of the specimen has been sputtered with a primary focused ion beam. \nAt lower beam currents, FIB imaging resolution begins to rival the more familiar scanning electron microscope (SEM) in terms of imaging topography, however the FIB's two imaging modes, using secondary electrons and secondary ions, both produced by the primary ion beam, offer many advantages over SEM. \nFIB secondary electron images show intense grain orientation contrast. As a result, grain morphology can be readily imaged without resorting to chemical etching. Grain boundary contrast can also be enhanced through careful selection of imaging parameters. FIB secondary ion images also reveal chemical differences, and are especially useful in corrosion studies, as secondary ion yields of metals can increase by three orders of magnitude in the presence of oxygen, clearly revealing the presence of corrosion.\n\nAnother advantage of FIB secondary electron imaging is the fact that the ion beam does not alter the signal from fluorescent probes used in the labelling of proteins, thus creating the opportunity to correlate FIB secondary electron images with images obtained by fluorescence microscopes.\n\nHistory of FIB technology\n\nPhysics of LMIS \n\nSome pioneers of LMIS & FIB \n\nAnother ion source seen in commercially available instruments is a helium ion source, which is inherently less damaging to the sample than Ga ions although it will still sputter small amounts of material especially at high magnifications and long scan times. As helium ions can be focused into a small probe size and provide a much smaller sample interaction than high energy (>1 kV) electrons in the SEM, the He ion microscope can generate equal or higher resolution images with good material contrast and a higher depth of focus. Commercial instruments are capable of sub 1 nm resolution.\n\nImaging and milling with Ga ions always result in Ga incorporation near the sample surface. As the sample surface is sputtered away at a rate proportional to the sputtering yield and the ion flux (ions per area per time), the Ga is implanted further into the sample, and a steady-state profile of Ga is reached. This implantation is often a problem in the range of the semiconductor where silicon can be amorphised by the gallium. In order to get an alternative solution to Ga LMI sources, mass-filtered columns have been developed, based on a Wien filter technology. Such sources include Au-Si, Au-Ge and Au-Si-Ge sources providing Si, Cr, Fe, Co, Ni, Ge, In, Sn, Au, Pb and other elements.\nThe principle of a Wien filter is based on the equilibrium of the opposite forces induced by perpendicular electrostatic and a magnetic fields acting on accelerated particles. The proper mass trajectory remains straight and passes through the mass selection aperture while the other masses are stopped.\n\nBesides allowing the use of sources others than gallium, these columns can switch from different species simply by adjusting the properties of the Wien filter. Larger ions can be used to make rapid milling before refining the contours with smaller ones. Users also benefits from the possibility to dope their samples with elements of suitable alloy sources.\n\nThe latter property has found great interests in the investigation of magnetic materials and devices. Khizroev and Litvinov have shown, with the help of magnetic force microscopy (MFM), that there is a critical dose of ions that a magnetic material can be exposed to without experiencing a change in the magnetic properties. Exploiting FIB from such an unconventional perspective is especially favourable today when the future of so many novel technologies depends on the ability to rapidly fabricate prototype nanoscale magnetic devices.\n\n"}
{"id": "1529106", "url": "https://en.wikipedia.org/wiki?curid=1529106", "title": "Full-sky Astrometric Mapping Explorer", "text": "Full-sky Astrometric Mapping Explorer\n\nFull-sky Astrometric Mapping Explorer (or FAME) was a proposed astrometric satellite designed to determine with unprecedented accuracy the positions, distances, and motions of 40 million stars within our galactic neighborhood (distances by stellar parallax possible). This database was to allow astronomers to accurately determine the distance to all of the stars on this side of the Milky Way galaxy, detect large planets and planetary systems around stars within 1,000 light years of the Sun, and measure the amount of dark matter in the galaxy from its influence on stellar motions. It was to be a collaborative effort between the United States Naval Observatory (USNO) and several other institutions. FAME would have measured stellar positions to less than 50 microarcseconds. The NASA MIDEX mission was scheduled for launch in 2004. In January 2002, however, NASA abruptly cancelled this mission, mainly due to concerns about its price tag which grew from $160 million to $220 million. \n\nThis would have been an improvement over the High Precision Parallax Collecting Satellite (Hipparcos) which operated 1989-1993 and produced various star catalogs. Astrometric parallax measurements form part of the cosmic distance ladder, and can also be measured by other Space telescopes such as Hubble (HST) or ground-based telescopes to varying degrees of precision.\n\nCompared to the FAME accuracy of 50 microarcseconds, the Gaia mission is planning 10 microarcseconds accuracy, for mapping stellar parallax up to a distance of tens of thousands of light-years from Earth.\n\n\n"}
{"id": "29686611", "url": "https://en.wikipedia.org/wiki?curid=29686611", "title": "Giorgos Catsadorakis", "text": "Giorgos Catsadorakis\n\nGiorgos Catsadorakis is a Greek biologist. He was awarded the Goldman Environmental Prize in 2001, for his contributions to the protection of the wetlands of Préspa, jointly with fellow biologist Myrsini Malakou. Their efforts resulted in an agreement between Greece, Macedonia and Albania on establishing the Préspa Park as a protected area of the region.\n"}
{"id": "46520615", "url": "https://en.wikipedia.org/wiki?curid=46520615", "title": "Gustaaf Hulstaert", "text": "Gustaaf Hulstaert\n\nGustaaf Hulstaert (1900–1990) was a Belgian entomologist mainly interested in Lepidoptera.\nFrom 1925 Gustaaf Hulstaert was a missionary in the Belgian Congo. Before that year he had studied insects from the Dutch Indies (including Dutch New Guinea) sent to him by other missionaries. His collection is held by Naturalis Biodiversity Center in Leiden.\n\n\n\nMbandaka\n"}
{"id": "2822381", "url": "https://en.wikipedia.org/wiki?curid=2822381", "title": "Harold E. Puthoff", "text": "Harold E. Puthoff\n\nHarold E. Puthoff (born June 20, 1936) is an American engineer and parapsychologist.\n\nIn 1967, Puthoff earned a Ph.D. in electrical engineering from Stanford University. He then worked with, and invented, tunable lasers and electron beam devices, concerning which he holds patents, and he is co-author (with R. Pantell) of \"Fundamentals of Quantum Electronics\" (Wiley, 1969), published in English, French, Russian and Chinese. Puthoff published papers on polarizable vacuum (PV) and stochastic electrodynamics topics, which are examples of alternative approaches to general relativity and quantum mechanics.\n\nPuthoff took an interest in the Church of Scientology in the late 1960s and reached what was then the top OT VII level by 1971. Puthoff wrote up his \"wins\" for a Scientology publication, claiming to have achieved \"remote viewing\" abilities. In 1974, Puthoff also wrote a piece for Scientology's \"Celebrity\" magazine, stating that Scientology had given him \"a feeling of absolute fearlessness\". Puthoff severed all connection with Scientology in the late 1970s.\n\nIn the 1970s and '80s Puthoff directed a CIA/DIA-funded program at SRI International to investigate paranormal abilities, collaborating with Russell Targ in a study of the purported psychic abilities of Uri Geller, Ingo Swann, Pat Price, Joseph McMoneagle and others, as part of the Stargate Project. Both Puthoff and Targ became convinced Geller and Swann had genuine psychic powers. However, Geller employed sleight of hand tricks.\n\nIn 1985, Puthoff founded a for-profit company, EarthTech International in Austin, TX. At about the same time, he founded an academically-oriented scientific research organization, Institute for Advanced Studies at Austin (IASA), also in Austin, TX, where he is Director. Independent of the Institute for Advanced Study in Princeton, NJ, IASA pursues more focused research on topics specifically related to energy generation and space propulsion, with funding from anonymous donors.\n\nPuthoff and EarthTech were granted a US Patent 5,845,220 in 1998 after five years delay. The claims were disputed that information could be transmitted through a distance using a modulated potential with no electric or magnetic field components. The case is used for educational purposes in patent law as an example of a \"valid\" patent where \"The lesson of the Puthoff patent is that in a world where both types of patents are more and more common, even a competent examiner may fail to distinguish innovation from pseudoscience.\"\n\nUri Geller was studied by Russell Targ and Puthoff at the Stanford Research Institute (SRI). Targ and Puthoff declared to have demonstrated that Geller had genuine psychic powers, though it was reported that there were flaws with the controls in the experiments and Geller was caught using sleight of hand on many other occasions. According to Terence Hines:\n\nThe psychologists David Marks and Richard Kammann attempted to replicate Targ and Puthoff’s remote viewing experiments. In a series of thirty-five studies, they were unable to replicate the results so investigated the procedure of the original experiments. Marks and Kammann discovered that the notes given to the judges in Targ and Puthoff's experiments contained clues as to which order they were carried out, such as referring to yesterday's two targets, or they had the date of the session written at the top of the page. They concluded that these clues were the reason for the experiment's high hit rates. Terence Hines has written:\n\nAccording to Marks, when the cues were eliminated the results fell to a chance level. James Randi noted that controlled tests by several other researchers, eliminating several sources of cuing and extraneous evidence present in the original tests, produced negative results. Students were also able to solve Puthoff and Targ's locations from the clues that had inadvertently been included in the transcripts.\n\nMarks and Kamman concluded: \"Until remote viewing can be confirmed in conditions which prevent sensory cueing the conclusions of Targ and Puthoff remain an unsubstantiated hypothesis.\"\n\nMassimo Pigliucci has written Puthoff's research into zero-point energy is considered to be a pseudoscience. According to Martin Gardner, Puthoff (and Targ) \"imagined they could do research in parapsychology but instead dealt with 'psychics' who were cleverer than they were\".\n\n\n\n"}
{"id": "26601024", "url": "https://en.wikipedia.org/wiki?curid=26601024", "title": "History and Anthropology", "text": "History and Anthropology\n\nHistory and Anthropology is a peer-reviewed academic journal covering anthropology published by Routledge. From 1984 until 2013 it was published quarterly. From 2014, the journal began publishing 5 issues per year. The founding editors-in-chief were François Hartog (University of Strasbourg), Lucette Valensi, and Nathan Wachtel (both of Ecole des Hautes Etudes en Sciences Sociales). The current editor is David Henig (University of Kent).\n\nThe journal is abstracted and indexed in Anthropological Index Online, Historical Abstracts, Humanities International Index, Index Islamicus, International Bibliography of the Social Sciences, and Sociological Abstracts.\n"}
{"id": "1802769", "url": "https://en.wikipedia.org/wiki?curid=1802769", "title": "History and philosophy of science", "text": "History and philosophy of science\n\nThe history and philosophy of science (HPS) is an academic discipline that encompasses the philosophy of science and the history of science. Although many scholars in the field are trained primarily as either historians or as philosophers, there are degree-granting departments of HPS at several prominent universities (see below).\n\nThe organization &HPS (Integrated History and Philosophy of Science) has set forth a program for a unified discipline: \"Good history and philosophy of science is not just history of science into which some philosophy of science may enter, or philosophy of science into which some history of science may enter. It is work that is both historical and philosophical at the same time. The founding insight of the modern discipline of HPS is that history and philosophy have a special affinity and one can effectively advance both simultaneously\".\n\nOne origin of the unified discipline is the historical approach to the discipline of the philosophy of science. This hybrid approach is reflected in the career of Thomas Kuhn. His first permanent appointment, at the University of California, Berkeley, was to a position advertised by the philosophy department, but he also taught courses from the history department. When he was promoted to full professor in the history department only, Kuhn was offended at the philosophers' rejection because \"I sure as hell wanted to be there, and it was my philosophy students who were working with me, not on philosophy but on history, were nevertheless my more important students\". This attitude is also reflected in his historicist approach, as outlined in Kuhn's seminal Structure of Scientific Revolutions (1962, 2nd ed. 1970), wherein philosophical questions about scientific theories and, especially, theory change are understood in historical terms, employing concepts such as paradigm shift.\n\nHowever, Kuhn was also critical of attempts fully to unify the \"methods\" of history and philosophy of science: \"Subversion is not, I think, too strong a term for the likely result of an attempt to make the two fields into one. They differ in a number of their central constitutive characteristics, of which the most general and apparent is their goals. The final product of most historical research is a narrative, a story, about particulars of the past. [...] The philosopher, on the other hand, aims principally at explicit generalizations and at those with universal scope. He is no teller of stories, true or false. His goal is to discover and state what is true at all times and places rather than to impart understanding of what occurred at a particular time and place.\" More recent work questions whether these methodological and conceptual divisions are in fact barriers to a unified discipline.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "16569923", "url": "https://en.wikipedia.org/wiki?curid=16569923", "title": "Item tree analysis", "text": "Item tree analysis\n\nItem tree analysis (ITA) is a data analytical method which allows constructing a\nhierarchical structure on the items of a questionnaire or test from observed response\npatterns. Assume that we have a questionnaire with \"m\" items and that subjects can\nanswer positive (1) or negative (0) to each of these items, i.e. the items are\ndichotomous. If \"n\" subjects answer the items this results in a binary data matrix \"D\"\nwith \"m\" columns and \"n\" rows.\nTypical examples of this data format are test items which can be solved (1) or failed\n(0) by subjects. Other typical examples are questionnaires where the items are\nstatements to which subjects can agree (1) or disagree (0).\nDepending on the content of the items it is possible that the response of a subject to an\nitem \"j\" determines her or his responses to other items. It is, for example, possible that\neach subject who agrees to item \"j\" will also agree to item \"i\". In this case we say that\nitem \"j\" implies item \"i\" (short formula_1). The goal of an ITA is to uncover such\ndeterministic implications from the data set \"D\".\n\nITA was originally developed by Van Leeuwe in 1974. The result of his algorithm,\nwhich we refer in the following as \"Classical ITA\", is a logically consistent set of\nimplications formula_1. Logically consistent means that if \"i\" implies \"j\" and \"j\" implies \"k\" then \"i\" implies \"k\" for each triple \"i\", \"j\", \"k\" of items. Thus the outcome of an ITA is a reflexive and transitive relation on the item set, i.e. a quasi-order on the items.\nA different algorithm to perform an ITA was suggested in \"Schrepp (1999)\". This algorithm is called \"Inductive ITA\". \nClassical ITA and inductive ITA both construct a quasi-order on the item set by explorative data analysis. But both methods use a different algorithm to construct this quasi-order. For a given data set the resulting quasi-orders from classical and inductive ITA will usually differ.\nA detailed description of the algorithms used in classical and inductive ITA can be found in \"Schrepp (2003)\" or \"Schrepp (2006)\". In a recent paper (Sargin & Ünlü, 2009) some modifications to the algorithm of inductive ITA are proposed, which improve the ability of this method to detect the correct implications from data (especially in the case of higher random response error rates).\n\nITA belongs to a group of data analysis methods called \"Boolean analysis of questionnaires\".\nBoolean analysis was introduced by Flament in 1976. The goal of a Boolean analysis is to\ndetect deterministic dependencies (formulas from Boolean logic connecting the items, like for example formula_1, formula_4, and formula_5) between the items of a questionnaire or test.\nSince the basic work of \"Flament (1976)\" a number of different methods for boolean analysis\nhave been developed. See, for example, \"Van Buggenhaut and Degreef (1987)\", \"Duquenne (1987)\" or \"Theuns (1994)\".\nThese methods share the goal to derive deterministic dependencies between the items of a\nquestionnaire from data, but differ in the algorithms to reach this goal. A comparison of ITA\nto other methods of boolean data analysis can be found in \"Schrepp (2003)\".\n\nThere are several research papers available, which describe concrete applications of item tree analysis. \n\"Held and Korossy (1998)\" analyzes implications on a set of algebra problems with classical ITA. Item tree analysis is also used in a number of social science studies to get insight into the structure of dichotomous data. In \"Bart and Krus (1973)\", for example, a predecessor of ITA is used to establish a hierarchical order on items that describe socially unaccepted behavior. In \"Janssens (1999)\" a method of Boolean analysis is used to investigate the\nintegration process of minorities into the value system of the dominant culture. Schrepp describes several applications of inductive ITA in the analysis of dependencies between items of social science questionnaires.\n\nTo show the possibilities of an analysis of a data set by ITA we analyse the statements of question 4 of the International Social Science Survey Programme (ISSSP) for the year 1995 by inductive and classical ITA.\nThe ISSSP is a continuing annual program of cross-national collaboration on surveys covering important topics for social science research. The program conducts each year one survey with comparable questions in each of the participating nations. The theme of the 1995 survey was national identity. We analyze the results for question 4 for the data set of Western Germany.\nThe statement for question 4 was:\n\n\"Some people say the following things are important for being truly German. Others say they are not important. How important do you think each of the following is\":\n\"1. to have been born in Germany\"\n\"2. to have German citizenship\"\n\"3. to have lived in Germany for most of one’s life\"\n\"4. to be able to speak German\"\n\"5. to be a Christian\"\n\"6. to respect Germany’s political institutions\"\n\"7. to feel German\"\n\nThe subjects had the response possibilities \"Very important\", \"Important\", \"Not very important\", \"Not important at all\", and \"Can’t choose\" to answer the statements. \nTo apply ITA to this data set we changed the answer categories. \"Very important\" and \"Important\" are coded as 1. \"Not very important\" and \"Not important at all\" are coded as 0. \"Can’t choose\" was handled as missing data. \nThe following figure shows the resulting quasi-orders formula_6 from inductive ITA and formula_7 from classical ITA.\nThe program ITA 2.0 implements both classical and inductive ITA. The program is available on . A short documentation of the program is available in .\n\nItem response theory\n\n"}
{"id": "2851077", "url": "https://en.wikipedia.org/wiki?curid=2851077", "title": "Jiao Bingzhen", "text": "Jiao Bingzhen\n\nJiāo Bǐngzhēn (), 1689–1726) was a native of Jining, Shandong who became a noted painter and astronomer. In painting he is noteworthy as one of the first Qing dynasty painters to be influenced by the West. He is also among the more significant portrait and miniature painters in the early Qing. He was skilled in painting people, landscapes, and buildings.\n\nThe Western influence in his art came from his exposure to the Jesuits at the Directorate of Astronomy. Their influence also exposed him to new ideas on astronomy and religion. At some point Jiao became a Roman Catholic and played a role on the Jesuit side of the Chinese Rites controversy.\n\n\n"}
{"id": "29425499", "url": "https://en.wikipedia.org/wiki?curid=29425499", "title": "Konstantin Valkov", "text": "Konstantin Valkov\n\nKonstantin Anatolyevich Valkov (; November 11, 1971- ) is a Russian former Cosmonaut. He was selected as part of the TsPK-12 Cosmonaut group in 1997.\n\nValkov was born in Kamensk-Uralsky, Sverdlovsk Oblast, Russia on November 11, 1971. In 1994, he graduated from Barnaul Higher Military Air School of Pilots and subsequently became a Colonel in the Russian Air Force.\n\nValkov was selected as part of the TsPK-12 group of cosmonauts to train at the Yuri Gagarin Cosmonaut Training Center in 1997. He then completed basic training in 1999. He retired without flying in space in 2012.\n"}
{"id": "46707327", "url": "https://en.wikipedia.org/wiki?curid=46707327", "title": "Lewis Ryder", "text": "Lewis Ryder\n\nLewis H. Ryder is a British theoretical physicist.\n\nRyder earned a master's degree in physics from Oxford University, a PhD in Mathematical Physics from Edinburgh University under supervision of Peter Higgs, and later an SERC fellowship.\n\nIn 1967 he went to the University of Kent where he is still a senior lecturer.\n\nHis research interests are in geometrical aspects of particle physics and its parallels with general relativity, and the possible existence and detection of torsion and curvature in spacetime. He also does research on the geometric phase and in condensed matter physics.\n\n\n"}
{"id": "50016875", "url": "https://en.wikipedia.org/wiki?curid=50016875", "title": "List of Bangladeshi scientists", "text": "List of Bangladeshi scientists\n\nThis is a list of Bangladeshi scientists.\n\n\n\n\n\n\n\n\n\n"}
{"id": "41692379", "url": "https://en.wikipedia.org/wiki?curid=41692379", "title": "List of intersex people", "text": "List of intersex people\n\nIntersex people are born with sex characteristics, such as genitals, gonads and chromosome patterns that, according to the UN Office of the High Commissioner for Human Rights, \"do not fit the typical definitions for male or female bodies\".\n\nIntersex people have many different sex assignments and gender identities, and so there is no presumption that people on this list have any particular sex assigned at birth, nor any particular gender identity.\n\nThis list consists of well-known intersex people. The individual listings note the subject's main occupation or source of notability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21556534", "url": "https://en.wikipedia.org/wiki?curid=21556534", "title": "List of military inventions", "text": "List of military inventions\n\nA military invention is an invention that was first created by a military. There are many inventions that were originally created by the military and subsequently found civilian uses.\n\n"}
{"id": "39197232", "url": "https://en.wikipedia.org/wiki?curid=39197232", "title": "List of neurochemists", "text": "List of neurochemists\n\nThis is a list of neurochemists.\n\n\n"}
{"id": "42298695", "url": "https://en.wikipedia.org/wiki?curid=42298695", "title": "List of orthopaedic eponyms", "text": "List of orthopaedic eponyms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32528928", "url": "https://en.wikipedia.org/wiki?curid=32528928", "title": "List of place names of Dutch origin", "text": "List of place names of Dutch origin\n\nThe Dutch, aided by their skills in shipping, map making, finance and trade, traveled to every corner of the world and left their language embedded in names of places they visited. A fraction of these are still in use today.\n\nTo be included in this list, the place must have an article on Wikipedia or must have inline references showing the name is or was indeed Dutch. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[Category:Geography-related lists|Toponymy]]\n[[Category:Netherlands-related lists|place]]\n[[Category:Dutch language lists]]\n[[Category:Maritime history of the Dutch Republic]]\n[[Category:Dutch Empire]]"}
{"id": "195653", "url": "https://en.wikipedia.org/wiki?curid=195653", "title": "List of stock exchanges", "text": "List of stock exchanges\n\nThis is a list of major stock exchanges. Those futures exchanges that also offer trading in securities besides trading in futures contracts are listed both here and at the list of futures exchanges.\n\nThere are nineteen stock exchanges in the world that have a market capitalization of over US$1 trillion each. They are sometimes referred to as the \"$1 Trillion Club\". These exchanges accounted for 87% of global market capitalization in 2015. Some exchanges do include companies from outside the country where the exchange is located.\n\nMajor stock exchange groups (top 20 by market capitalization) of issued shares of listed companies, .\n\nSee full article: List of commodities exchanges\n\n\n\n\n\n\n"}
{"id": "10266438", "url": "https://en.wikipedia.org/wiki?curid=10266438", "title": "MERODE", "text": "MERODE\n\nMERODE is an Object Oriented Enterprise Modeling method developed at KU Leuven (Belgium). Its name is the abbreviation of Model driven, Existence dependency Relation, Object oriented DEvelopment. MERODE is a method for creating domain models (also called conceptual models) as basis for building information systems making use of two prominent UML diagramming techniques - class diagram and state diagrams. Starting from a high-level PIM (close to a Computational Independent Model (CIM)) allows removing or hiding details irrelevant for a conceptual modelling view which makes the approach easier to understand. \nThe method is grounded in process algebra, which enables mathematical reasoning on models. Thanks to this, models can be checked for internal consistency and mutual completeness, i.e. inter/intra model consistency and syntactical quality. The automated reasoning (\"consistency by construction\") also caters for autocomplete functionality, which allows creating correct models faster.\n\nA typical MERODE analysis or conceptualisation consists of three views or diagrams: a so-called existence dependency graph (EDG) similar to a UML class diagram, a proprietary concept namely an object event table (OET) and a group of finite state machines.\n\nMERODE fosters a model-driven engineering approach to software development. It targets platform independent domain models that are sufficiently complete for execution, i.e. transformation to platform-specific models and to code. In order to achieve automated transformation of models, MERODE limits the use of UML to a number of well-defined constructs with clear semantics and complements this with the notion of \"existence dependency\" and a proprietary approach to object interaction modelling.\n\nMERODE-models can be created with the opensource case tool JMermaid. The tool also allows checking the models for consistency and readiness for transformation.\n\nA companion code generator allows to generate a fully working prototype. One-click prototype production lowers the required skill-set for its useful application. By embedding the models into the application, the behaviour of the prototype can be traced back to the models, i.e. making it possible to validate the semantic quality of models. MERODE prototypes are augmented with feedback (textual and graphical) that links the test results to their causes in the model.\n\n"}
{"id": "52579862", "url": "https://en.wikipedia.org/wiki?curid=52579862", "title": "Maker education", "text": "Maker education\n\nMaker education (a term coined by Dale Dougherty in 2013) closely associated with STEM learning, is an approach to problem-based and project-based learning that relies upon hands-on, often collaborative, learning experiences as a method for solving authentic problems. People who participate in making often call themselves \"makers\" of the maker movement and develop their projects in makerspaces, or development studios which emphasize prototyping and the repurposing of found objects in service of creating new inventions or innovations. Culturally, makerspaces, both inside and outside of schools, are associated with collaboration and the free flow of ideas. In schools, maker education stresses the importance of learner-driven experience, interdisciplinary learning, peer-to-peer teaching, iteration, and the notion of \"failing forward\", or the idea that mistake-based learning is crucial to the learning process and eventual success of a project.\n\nMaker education is an offshoot of the maker movement, which \"Time\" magazine described as \"the umbrella term for independent innovators, designers and tinkerers. A convergence of computer hackers and traditional artisans, the niche is established enough to have its own magazine, \"Make\", as well as hands-on Maker Faires that are catnip for who used to toil in solitude\". Dale Dougherty, founder of the Maker Faire and \"Make\" magazine, stated in his 2011 TED Talk that \"We are all makers. We are born makers. We don't just live, but we make.\" In the same TED Talk, Dougherty also called for making to be embraced in education, as students are the new generation of makers. Another central contributor to the maker movement, Chris Anderson, who was once the editor-in-chief of \"Wired\" magazine and is now the CEO of 3D Robotics, wrote a manifesto of the maker movement in 2012, called \"Makers\". His third book, \"\" (2012), emphasizes the role that making has to play in the renaissance of American manufacturing. Mark Hatch, who is now the CEO of TechShop, also published \"The Maker Movement Manifesto\". In addition to these contributions, seminal texts include, \"Invent To Learn: Making, Tinkering, and Engineering in the Classroom\" by Sylvia Libow Martinez, and \"The Art of Tinkering,\" by Karen Wilkinson and Mike Petrich, founders of The Tinkering Studio at the Exploratorium.\n\nIn the United States, hands-on learning through making has roots in the nineteenth century, as a result of the influence of educators such as Calvin M. Woodward, who established the Manual Training School of Washington University on June 6, 1879. Unlike later vocational education that would take hold in 1917 through the Smith-Hughes Act that had the aim of reducing the United States reliance on foreign trade, the impetus for the Manual Training School was to provide students with training in making and craftsmanship that had \"no immediate vocational goal\". Today's maker education highlights students' potential to \"change the world\" and \"let their imaginations run wild\" while also emphasizing building students' entrepreneurship skills and ability to earn money by selling their inventions.\n\nThat Arts and Crafts movement of the late nineteenth century is sometimes also referenced in relationship with the maker movement. The Arts and Crafts movement, which originated in Britain before taking hold in Europe and North America, was anti-industrial, critical of machinery and factory production, advocating instead for a return to traditional craftsmanship.\n\nSince 2005, maker education has gained momentum in schools across the United States and around the world. Proponents of the maker movement cite the potential for making to bring more women to STEM fields and close the gender gap. Other potential benefits and goals for making include creating greater educational equity among students in public schools, and the possibility for making to be a driver in educational and societal change. Other educators and innovators have developed offshoot curriculum and technologies related to the intersection of critical thinking and making, called critical making.\n\nIn school models, such as the Lighthouse Community Charter, a charter school in Oakland, California, Aaron Van der Woorf, the robotics teacher leads the students in Maker Ed. At the Park School, in collaboration with Harvard's Project Zero, students hold a mini maker faire in school that also acts as a fundraiser for the school. Some districts have also adopted maker education district-wide, such as the district of Elizabeth Forward, just south of Pittsburgh, which partnered with Carnegie Mellon to provide professional development for teachers through working with students on Maker Ed. Principals in Albemarle County schools cite Superintendent Pam Moran as instrumental in bringing maker education to their school district.\n\nIn addition to bringing maker education to schools, scholars like Paulo Blikstein of Stanford University and Dennis Krannich of the University Bremen, in Germany, state that, \"Digital fabrication and 'making,' and the positive social movement around them, could be an unprecedented opportunity for educators to advance a progressive educational agenda in which project-based, interest-driven, student-centered learning are at the center stage of students' educational experiences.\" Penketh High School Became the first school in the United Kingdom to embedded maker education into the UK education system in 2018.\n\nThe Obama Administration has also strongly supported the growing maker movement as an integral part of STEM education, which it hopes will increase American students ability to compete globally in the areas of science, engineering, and math. At the White House, President Obama hosted the first ever White House Maker Faire in June, 2014, adopting the idea that Americans are a \"Nation of Makers\". On the Nation of Makers webpage, Americans are encouraged to join the movement, asserting that \"empowering students and adults to create, innovate, tinker, and make their ideas and solutions into reality is at the heart of the Maker Movement\". Since the first-ever White House Maker Faire, the Obama Administration has \"continued to support opportunities for students to learn about STEM through making, expand the resources available for maker entrepreneurs, and foster the development of advanced manufacturing in the U.S.\" In summer 2015, the President announced the National Week of Making, June 17–23, to support the Nation of Makers. In 2016, President Obama renewed his commitment to maker education by continuing the National Week of Making. The National Maker Faire will include participation from the Department of Agriculture, the Department of Energy, the Department of the Navy (Navy), the Institute of Museum and Library Services, the National Aeronautics and Space Administration (NASA), the National Endowment for the Arts, and the National Institute of Standards and Technology (NIST). At the time of this announcement, the President also detailed the progress that had been made on the Nation of Makers. He announced, \nIn addition to these developments, on June 17, 2016, the White House issued a press release, detailing the next steps the United States government will take to support the development and expansion of maker education.\n\nThough maker education has been embraced by thousands of schools and school districts across the United States and abroad, there has also emerged criticism of the movement.\n\nAmong the critics is Evgeny Morozov, a Belarusian writer and researcher, whose work focuses on the impact, both social and political, of technology. In his article published in \"The New Yorker\", entitled, \"Making It: Pick up a spot welder and join the revolution\", Morozov criticizes Chris Anderson for \"confusing the history of the Web with the history of capitalism and ends by speculating about the future of the maker movement, which, on closer examination, is actually speculation on the future of capitalism\". He also criticizes companies and organizations that were once committed to open source software for becoming acquired by for-profit companies and embroiled in copyright and trademark lawsuits. Morozov also criticizes the maker movement's major contributors financial relationship with DARPA, which made a $10 million grant to support maker education for high school students, and $3.5 million to TechShop to establish new makerspaces.\n\nWhile Morozov is one of the more vocal critics of maker education, he is not the only one. Debbie Chachra, associate professor at Olin College of Engineering, in her article in January 23, 2015 issue of \"The Atlantic\", entitled, \"Why I Am Not a Maker\", centers her criticism on \"the social history of who makes things—and who doesn't\". Chachra describes the history of the \"makers\" of products as men, rather than those who cared for \"hearth and home\", that is, historically, women. She calls for recognition of \"the work of the educators, those that analyze and characterize and critique, everyone who fixes things, all the other people who do valuable work with and for others—above all, the caregivers—whose work isn't about something you can put in a box and sell\". In \"A more lovingly made world\", by McKenzie Wark of The New School, Wark writes that the problem with maker culture is that makers don't actually make things, they assemble them. While this experience is satisfying and fun (and Wark does acknowledge the way in which his children are not hemmed in by gender expectations while playing at the Maker Faire), it doesn't teach the underlying principles required for the actual making of functional objects. It also does not, though Chris Anderson and Mark Hatch evoke Marx in their Maker manifestos, map accurately onto an understanding of labor, and certainly not the life of the laborer.\n\nFinally, Shirin Vossoughi and Paula K. Hooper of Northwestern University, and Meg Escude of Exploratorium, offer an in-depth look at the ways in which maker education reinforces educational inequality. They begin by offering Haitian writer, Edwidge Danticat's commentary on making: \"If you can't afford clothes, but you can make them--make them. You have to work with what you have, especially if you don't 'have a lot of money. You use creativity, and you use imagination.\" Vossoughi, Hooper, and Escude make the case that true making is often born from difficult environmental circumstances, and that what local communities once did as part of their daily lives (offering an exchange of skills and knowledge, for example, to help others in need in the community) is now being offered to low-income neighborhoods as an intervention program, promising greater educational equality. They argue that the \"American Maker\" highlighted by Dale Dougherty in his 2011 Ted Talk, is symptomatic of the problems associated with American exceptionalism, in that it is \"grounded in gendered, white, middle-class cultural practices\". Not only do Vossoughi, Hooper, and Escude critique maker education as it is currently practiced, they also offer proposed solutions for \"equity-oriented design\" of maker experiences which include, \"critical analyses or educational injustice, historicized approaches to making as cross-cultural activity, explicit attention to pedagogy, and inquiry into the sociopolitical values and purposes of making\".\n\n"}
{"id": "32469641", "url": "https://en.wikipedia.org/wiki?curid=32469641", "title": "Maxwell–Bloch equations", "text": "Maxwell–Bloch equations\n\nThe Maxwell–Bloch equations, also called the optical Bloch equations, were first derived by Tito Arecchi and Rodolfo Bonifacio of Milan, Italy. They describe the dynamics of a two-state quantum system interacting with the electromagnetic mode of an optical resonator. They are analogous to (but not at all equivalent to) the Bloch equations which describe the motion of the nuclear magnetic moment in an electromagnetic field. The equations can be derived either semiclassically or with the field fully quantized when certain approximations are made.\n\nThe derivation of the semi-classical optical Bloch equations is nearly identical to solving the two-state quantum system (see the discussion there). However, usually one casts these equations into a density matrix form. The system we are dealing with can be described by the wave function: \n\nThe density matrix is \n\n(other conventions are possible; this follows the derivation in Metcalf (1999)). One can now solve the Heisenberg equation of motion, or translate the results from solving the Schrödinger equation into density matrix form. One arrives at the following equations, including spontaneous emission: \n\nIn the derivation of these formulae, we define formula_8 and formula_9. It was also explicitly assumed that spontaneous emission is described by an exponential decay of the coefficient formula_10 with decay constant formula_11. formula_12 is the (generalized) Rabi frequency, which is \nwhere formula_14 is the detuning and measures how far the light frequency, formula_15, is from the transition, formula_16. formula_17 where formula_18 is the transition dipole moment for the formula_19 transition and formula_20 is the vector electric field amplitude including the polarization.\n\nBeginning with the Jaynes-Cummings Hamiltonian under coherent drive\n\nwhere formula_22 is the lowering operator for the cavity field, and formula_23 is the atomic lowering operator written as a combination of Pauli matrices. The time dependence can be removed by transforming the wavefunction according to formula_24, leading to a transformed Hamiltonian\n\nwhere formula_26. As it stands now, the Hamiltonian has four terms. The first two are the self energy of the atom (or other two level system) and field. The third term is an energy conserving interaction term allowing the cavity and atom to exchange population and coherence. These three terms alone give rise to the Jaynes-Cummings ladder of dressed states, and the associated anharmonicity in the energy spectrum. The last term models coupling between the cavity mode and a classical field, i.e. a laser. The drive strength formula_27 is given in terms of the power transmitted through the empty two-sided cavity as formula_28, where formula_29 is the cavity linewidth. This brings to light a crucial point concerning the role of dissipation in the operation of a laser or other CQED device; dissipation is the means by which the system (coupled atom/cavity) interacts with its environment. To this end, dissipation is included by framing the problem in terms of the master equation, where the last two terms are in the Lindblad form\n\nThe equations of motion for the expectation values of the operators can be derived from the master equation by the formulas formula_31 and formula_32. The equations of motion for formula_33, formula_34, and formula_35, the cavity field, atomic coherence, and atomic inversion respectively, are\n\nAt this point, we have produced three of an infinite ladder of coupled equations. As can be seen from the third equation, higher order correlations are necessary. The differential equation for the time evolution of formula_39 will contain expectation values of higher order products of operators, thus leading to an infinite set of coupled equations. We heuristically make the approximation that the expectation value of a product of operators is equal to the product of expectation values of the individual operators. This is akin to assuming that the operators are uncorrelated, and is a good approximation in the classical limit. It turns out that the resulting equations give the correct qualitative behavior even in the single excitation regime. Additionally, to simplify the equations we make the following replacements\n\nAnd the Maxwell–Bloch equations can be written in their final form\n\n"}
{"id": "3668787", "url": "https://en.wikipedia.org/wiki?curid=3668787", "title": "Medawar zone", "text": "Medawar zone\n\nThe Medawar Zone is the area of problems which are most likely to produce fruitful results. Problems that are too simple are unlikely to produce novel or significant results. Problems that are too ambitious may not succeed at all or may be rejected by the research community at large. This is illustrated in the figure:\n\nIn an article on creativity in research, Craig Loehle named this zone after Sir Peter Medawar, a Nobel prize-winning medical researcher who was active from the 1940s to the 1960s. In \"The Art of the Soluble\", Medawar suggested that there seems to be a certain time when scientific questions seem especially ripe for answering, whereas other questions remain elusive and out-of-reach from investigation.\n\n\n"}
{"id": "51493917", "url": "https://en.wikipedia.org/wiki?curid=51493917", "title": "NGC 163", "text": "NGC 163\n\nNGC 163 is an elliptical galaxy in the constellation Cetus. It was discovered by William Herschel since 1890. It has been found in a faint object but when seeing using an optical telescope was an elliptical galaxy that ranges up to 13 magnitude.\n"}
{"id": "1583584", "url": "https://en.wikipedia.org/wiki?curid=1583584", "title": "NuSTAR", "text": "NuSTAR\n\nNuSTAR (Nuclear Spectroscopic Telescope Array) is a space-based X-ray telescope that uses a conical approximation to a Wolter telescope to focus high energy X-rays from astrophysical sources, especially for nuclear spectroscopy, and operates in the range of 3 to 79 keV.\n\nNuSTAR is the eleventh mission of NASA's Small Explorer satellite program (SMEX-11) and the first space-based direct-imaging X-ray telescope at energies beyond those of the Chandra X-ray Observatory and XMM-Newton. It was successfully launched on 13 June 2012, having previously been delayed from 21 March due to software issues with the launch vehicle.\n\nThe mission's primary scientific goals are to conduct a deep survey for black holes a billion times more massive than the Sun, to investigate how particles are accelerated to very high energy in active galaxies, and to understand how the elements are created in the explosions of massive stars by imaging the remains, which are called supernova remnants.\n\nAfter a primary mission lasting two years (to 2014) it is now in its fifth year in space.\n\nNuSTAR's predecessor, the High Energy Focusing Telescope (HEFT), was a balloon-borne version that carried telescopes and detectors constructed using similar technologies. In February 2003, NASA issued an Explorer Program Announcement of Opportunity. In response, NuSTAR was submitted to NASA in May, as one of 36 mission proposals vying to be the tenth and eleventh Small Explorer missions.\nIn November, NASA selected NuSTAR and four other proposals for a five-month implementation feasibility study.\n\nIn January 2005, NASA selected NuSTAR for flight pending a one-year feasibility study. The program was cancelled in February 2006 as a result of cuts to science in NASA's 2007 budget. On 21 September 2007 it was announced that the program had been restarted, with an expected launch in August 2011, though this was later delayed to June 2012.\n\nThe principal investigator is Fiona A. Harrison of the California Institute of Technology (Caltech). Other major partners include the Jet Propulsion Laboratory (JPL), University of California at Berkeley, Technical University of Denmark (DTU), Columbia University, Goddard Space Flight Center, Stanford University, University of California, Santa Cruz, Sonoma State University, Lawrence Livermore National Laboratory, and the Italian Space Agency (ASI). NuSTAR's major industrial partners include Orbital Sciences Corporation and ATK Space Components.\n\nNASA contracted with Orbital Sciences Corporation to launch NuSTAR (mass ) on a Pegasus XL rocket for 21 March 2012. It had earlier been planned for 15 August 2011, 3 February 2012, 16 March 2012, and 14 March 2012. After a launch meeting on 15 March 2012, the launch was pushed further back to allow time to review flight software used by the launch vehicle's flight computer. The launch was conducted successfully at 16:00:37 UTC on 13 June 2012 about 117 nautical miles south of Kwajalein Atoll. The Pegasus rocket was dropped from the L-1011 'Stargazer' aircraft.\n\nOn 22 June 2012 it was confirmed that the 10 m mast was fully deployed.\n\nUnlike visible light telescopes – which employ mirrors or lenses working with normal incidence – NuSTAR has to employ grazing incidence optics to be able to focus X-rays. For this two conical approximation Wolter telescope design optics with focal length are held at the end of a long deployable mast. A laser metrology system is used to determine the exact relative positions of the optics and the focal plane at all times, so that each detected photon can be mapped back to the correct point on the sky even if the optics and the focal plane move relative to one another during an exposure.\n\nEach focusing optic consists of 133 concentric shells. One particular innovation enabling NuSTAR is that these shells are coated with depth-graded multilayers (alternating atomically thin layers of a high-density and low-density material); with NuSTAR's choice of Pt/SiC and W/Si multilayers, this enables reflectivity up to 79 keV (the platinum K-edge energy).\n\nThe optics were produced, at Goddard Space Flight Center, by heating thin (210 µm) sheets of flexible glass in an oven so that they slump over precision-polished cylindrical quartz mandrels of the appropriate radius. The coatings were applied by a group at the Danish Technical University.\n\nThe shells were then assembled, at the Nevis Laboratories of Columbia University, using graphite spacers machined to constrain the glass to the conical shape, and held together by epoxy. There are 4680 mirror segments in total (the 65 inner shells each comprise six segments and the 65 outer shells twelve; there are upper and lower segments to each shell, and there are two telescopes); there are five spacers per segment. Since the epoxy takes 24 hours to cure, one shell is assembled per day – it took four months to build up one optic.\n\nThe expected point spread function for the flight mirrors is 43 arc-seconds, giving a spot size of about two millimeters at the focal plane; this is unprecedentedly good resolution for focusing hard X-ray optics, though it is about one hundred times worse than the best resolution achieved at longer wavelengths by the Chandra X-ray Observatory.\n\nEach focusing optic has its own focal plane module, consisting of a solid state cadmium zinc telluride (CdZnTe) pixel detector surrounded by a CsI anti-coincidence shield. One detector unit — or focal plane — comprises four (two-by-two) detectors, manufactured by eV Products. Each detector is a rectangular crystal of dimension 20mm x 20mm and thickness ~2mm that have been gridded into 32 × 32, 0.6 mm pixels (each pixel subtending 12.3 arc seconds) and provides a total of 12 arc minute field of view for each focal plane module.\n\nCZT detectors are state-of-the-art room temperature semiconductors that are very efficient at turning high energy photons into electrons. The electrons are digitally recorded using custom Application Specific Integrated Circuits (ASICs) designed by the NuSTAR Caltech Focal Plane Team. Each pixel has an independent discriminator and individual X-ray interactions trigger the readout process. On-board processors, one for each telescope, identify the row and column with the largest pulse height and read out pulse height information from this pixel as well as its eight neighbors. The event time is recorded to an accuracy of 2 μs relative to the on-board clock. The event location, energy, and depth of interaction in the detector are computed from the nine-pixel signals.\n\nThe focal planes are shielded by cesium iodide (CsI) crystals that surround the detector housings. The crystal shields, grown by Saint-Gobain, register high energy photons and cosmic rays which cross the focal plane from directions other than the along the NuSTAR optical axis. Such events are the primary background for NuSTAR and must be properly identified and subtracted in order to identify high energy photons from cosmic sources. The NuSTAR active shielding ensures that any CZT detector event coincident with an active shield event is ignored.\n\nNuSTAR has proven itself to be a versatile instrument and has opened up new discoveries in wide and varied areas of astrophysical research since it launch.\n\nIn February 2013, NASA revealed that NuSTAR, along with the XMM-Newton space observatory, has measured the spin rate of the supermassive black hole at the centre of the galaxy NGC 1365.\n\nOne of NuSTAR's main goals is to characterize stars' explosions by mapping the radioactive material in a supernova remnant. The NuSTAR map of Cassiopeia A shows the titanium-44 isotope concentrated in clumps at the remnant's center and points to a possible solution to the mystery of how the star exploded. When researchers simulate supernova blasts with computers, as a massive star dies and collapses, the main shock wave often stalls and the star fails to shatter. The latest findings strongly suggest the exploding star literally sloshed around, re-energizing the stalled shock wave and allowing the star to finally blast off its outer layers.\n\nIn January 2017, researchers from Durham University and the University of Southampton, leading a coalition of agencies using NuSTAR data, announced the discovery of supermassive black holes at the center of nearby galaxies NGC 1448 and IC 3639.\n\n\n\n"}
{"id": "35581127", "url": "https://en.wikipedia.org/wiki?curid=35581127", "title": "Observado.org", "text": "Observado.org\n\nObservado.org is a non-profit website which collects sightings in nature. Users can submit sightings regarding any area of fauna and flora, whereby administrators scrutinize these sightings. A forum is present where questions by observers can be posed and commented on by any user.\n\nObservado.org was founded in 2005 as a sister site of Waarneming.nl. The founder was Stichting Natuurinformatie (Foundation nature data) Since its start over 8 million nature sightings have been collected featuring almost 300.000 pictures by alsmost 3.000 users and this number grows exponentially.\n\nThe goal of observado.org is to make available sightings of observers all around the world and help with knowledge on biodiversity. Not only citizen scientist, but also professional researchers share their data on this website. The website also exchanges data with international researchers. For example, sightings have been used by researchers of amongst others the Dutch Butterfly Association (Vlinderstichting) for surveys on dragonflies and butterflies for Europe and the mediterranean. Observers, e.g. birders, collect their own sightings and pictures made during field trips and holidays. Meanwhile, they feed the data that enable them to share their sightings, pictures with other users and ngo's. It is possible to submit sightings from just about any bird, mammal, insect or plant. The sightings and statistics are freely accessible for all users of the website. In order to avoid poaching some sightings have been made invisible for other users.\n\nObservado.org provides an overview of all the biodiversity in the world. Screens like these will provide an insight into this diverse world of wildlife.\n\nObservado.org is available in many languages among which English, German, Danish, Swedish, French, Italian, Spanish, Portuguese, Russian, Slovenian, Tsjech, Polish, Ukrainian, Hebrew, Arabic, Persian, Greek, Thai and West-Frisian. For each country or continent a separate site has been made on which sightings, pictures and statistics.\n\nObservado.org is maintained by volunteers. They add new functions and species and above all enforce quality control with respect to sightings. Missing species or bugs can be reported on the forum.\n\nObservado.org has got special software modules for mobile devices: WnPda, WnSmart (both for Windows Mobile), ObsMapp (Android) and iObs (iOS). These programs enable submitting sighting in the field on a PDA. Sightings can be submitted on-line directly or at the end of a trip to Observado.org. When submitting GPS position and time are registered as well the name of the species, the number and all sorts of features of the sighting. For a correct time the timezone of the device should be correctly filled in.\n\nFor smartphones the website WebObs is available. Webobs also enables submitting sightings in the field. WebObs requires one of the following systems:\n"}
{"id": "221249", "url": "https://en.wikipedia.org/wiki?curid=221249", "title": "Olduvai theory", "text": "Olduvai theory\n\nThe Olduvai theory states that industrial civilization (as defined by \"per capita\" energy production) will have a lifetime of less than or equal to 100 years (1930–2030). The theory provides a quantitative basis of the transient-pulse-theory of modern civilization. The name is a reference to the Olduvai Gorge in Tanzania.\n\nThe Olduvai theory was introduced in 1989 by power system engineer Richard C. Duncan as the \"transient-pulse theory of industrial civilization\" with further details in the 1993 paper \"The life-expectancy of industrial civilization: The decline to global equilibrium\".\n\nIn June, 1996, Duncan introduced a paper titled \"The Olduvai Theory: Sliding Towards a Post-Industrial Stone Age\" where the term \"Olduvai Theory\" replaced \"transient-pulse theory\" used in previous papers. Duncan further updated his theory in \"The Peak of World Oil Production and the Road to the Olduvai Gorge\", at the Summit 2000 Pardee Keynote Symposium of the Geological Society of America, on November 13, 2000. In 2005, Duncan extended his data set to include up to 2003 in \"The Olduvai Theory Energy, Population, and Industrial Civilization\".\n\nIndustrial civilization is defined in Duncan's paper as the time between energy production \"per capita\" rising above 37% of its eventual peak value, and it falling back below 37% of the peak value later. In 1996, he estimated this period to be from 1930 until approximately 2025, with the \"per capita\" peak having occurred in 1977. In 2009, he considered separately the peaks of standard of living in OECD countries, non-OECD countries (specifically China, India and Brazil), and the USA. He found that the standard of living in the US peaked in 1973, that of OECD countries in 2005, while that of the non-OECD countries was still rising in 2007. He predicted a global peak in standard of living in 2010, followed by a decline to the 1930 level by 2030.\n\nThe Olduvai theory claims that exponential growth of energy production ended in 1979, that energy use \"per capita\" will show no growth through 2008, and that after 2008 energy growth will become sharply negative, culminating, after a Malthusian catastrophe, in a world population of 2 billion circa 2050. \n\nThe Olduvai theory divides human history into three phases. The first \"pre-industrial\" phase stretches over most of human history when simple tools and weak machines limited economic growth. The second \"industrial\" phase encompasses modern industrial civilization where machines temporarily lift all limits to growth. The final \"de-industrial\" phase follows where industrial economies decline to a period of equilibrium with renewable resources and the natural environment.\n\nThe decline of the industrial phase is broken into three sections:\n\nAt the time of Duncan's paper, the peak in \"per capita\" energy consumption was 11.15 boe/c/yr (barrels of oil equivalent \"per capita\" per year) and occurred in 1979; however, since then energy use \"per capita\" has increased beyond that level, with 2006 providing a peak value of 12.12 boe/c/yr. This increase directly contradicts Postulate 2 of the most recent version of the theory, namely that \"\"[average \"per capita\" energy] will show no growth from 1979 to circa 2008\"\". Duncan has responded to this criticism in a 2009 essay entitled The Olduvai Theory - Toward Re-Equalizing the World Standard of Living. The response is that certain non-OPEC countries such as China and India have increased their \"per capita\" energy usage whilst efficiency improvements within the USA & OPEC have reduced their requirements for energy without impacting the quality of life. The theory is then re-assessed from the standpoint OPEC, non-OPEC and USA energy consumption. \n\nProponents note that the current trend of increasing \"per capita\" energy consumption may be difficult to sustain in the face of limits on fossil fuel resources such as oil, coal, and natural gas. However, advances in renewable energy have reduced dependence on fossil fuels. For example, in 2015 Sweden produced approximately 8,153 kWh of electricity per capita per year (5.01 boe/c/yr) from renewable resources, with the exclusion of hydropower; Sweden's theoretical maximum electric generation capacity from renewable resources, including hydropower, is estimated to be 24,472 kWh per capita per year (15.03 boe/c/yr) at the current population level.\n\nIn justification of his reference to Olduvai Gorge, Duncan writes:\n…(1) it is famous for the myriad hominid fossils and stone tools discovered there, (2) I've been there, (3) its long hollow sound is eerie and ominous, and (4) it is a good metaphor for the 'Stone Age way of life'.\n\n\n"}
{"id": "37814354", "url": "https://en.wikipedia.org/wiki?curid=37814354", "title": "PLANDET", "text": "PLANDET\n\nPLANDET (Urban Development Planning of Trujillo) is an agency of the Municipality of Trujillo in charge of urban development planning of the city, created by the Municipality of Trujillo with authority granted by the Organic Law of Municipalities, for the government of town. This agency specializes in urban planning support and it has specific functions on the territorial distribution of the city. It is responsible for developing planning documents such as the \"Plan of comprehensive and sustainable development of Trujillo in 2015.\"\n\nPlandet before was called PLANDEMETRU (Metropolitan Development Plan of Trujillo). Currently this agency of the MUnicipality of Trujillo is working continuously in the planning of the urban development of Trujillo city.\n\nAmong the main functions of Plandet are the following: To promote, develop, conduct, monitor and continuously assess and update the management and the execution of the Metropolitan Development Plan Trujillo and Local Plans de Desarrollo complementary, in coordination with relevant municipales.\n\n\n\n\n\n"}
{"id": "35645642", "url": "https://en.wikipedia.org/wiki?curid=35645642", "title": "Paul Géroudet", "text": "Paul Géroudet\n\nPaul Géroudet (1917–2006) was a notable Swiss ornithologist. He was the chief editor of Nos Oiseaux from 1939 to 1994.\n"}
{"id": "16166462", "url": "https://en.wikipedia.org/wiki?curid=16166462", "title": "Plant disease epidemiology", "text": "Plant disease epidemiology\n\nPlant disease epidemiology is the study of disease in plant populations. Much like diseases of humans and other animals, plant diseases occur due to pathogens such as bacteria, viruses, fungi, oomycetes, nematodes, phytoplasmas, protozoa, and parasitic plants. Plant disease epidemiologists strive for an understanding of the cause and effects of disease and develop strategies to intervene in situations where crop losses may occur. Typically successful intervention will lead to a low enough level of disease to be acceptable, depending upon the value of the crop.\n\nPlant disease epidemiology is often looked at from a multi-disciplinary approach, requiring biological, statistical, agronomic and ecological perspectives. Biology is necessary for understanding the pathogen and its life cycle. It is also necessary for understanding the physiology of the crop and how the pathogen is adversely affecting it. Agronomic practices often influence disease incidence for better or for worse. Ecological influences are numerous. Native species of plants may serve as reservoirs for pathogens that cause disease in crops. Statistical models are often applied in order to summarize and describe the complexity of plant disease epidemiology, so that disease processes can be more readily understood. For example, comparisons between patterns of disease progress for different diseases, cultivars, management strategies, or environmental settings can help in determining how plant diseases may best be managed. Policy can be influential in the occurrence of diseases, through actions such as restrictions on imports from sources where a disease occurs.\n\nIn 1963 J. E. van der Plank published \"Plant Diseases: Epidemics and Control\", a seminal work that created a theoretical framework for the study of the epidemiology of plant diseases. This book provides a theoretical framework based on experiments in many different host pathogen systems and moved the study of plant disease epidemiology forward rapidly, especially for fungal foliar pathogens. Using this framework we can now model and determine thresholds for epidemics that take place in a homogeneous environment such as a mono-cultural crop field.\n\nDisease epidemics in plants can cause huge losses in yield of crops as well threatening to wipe out an entire species such as was the case with Dutch Elm Disease and could occur with Sudden Oak Death. An epidemic of potato late blight, caused by \"Phytophthora infestans\", led to the Great Irish Famine and the loss of many lives.\n\nCommonly the elements of an epidemic are referred to as the “disease triangle”: a susceptible host, pathogen, and conducive environment. For a disease to occur all three of these must be present. Below is an illustration of this point. Where all three items meet, there is a disease. The fourth element missing from this illustration for an epidemic to occur is time. As long as all three of these elements are present disease can initiate, an epidemic will only ensue if all three continue to be present. Anyone of the three might be removed from the equation though. The host might out-grow susceptibility as with high-temperature adult-plant resistance, the environment changes and is not conducive for the pathogen to cause disease, or the pathogen is controlled through a fungicide application for instance.\n\nSometimes a fourth factor of time is added as the time at which a particular infection occurs, and the length of time conditions remain viable for that infection, can also play an important role in epidemics. The age of the plant species can also play a role, as certain species change in their levels of disease resistance as they mature; in a process known as ontogenic resistance.\n\nIf all of the criteria are not met, such as a susceptible host and pathogen are present, but the environment is not conducive to the pathogen infecting and causing disease, a disease cannot occur. For example, corn is planted into a field with corn residue that has the fungus \"Cercospora zea-maydis\", the causal agent of Grey leaf spot of corn, but if the weather is too dry, and there is no leaf wetness the spores of the fungus in the residue cannot germinate and initiate infection.\n\nLikewise, it stands to reason if the host is susceptible and the environment favours the development of disease but the pathogen is not present there is no disease. Taking the example above, the corn is planted into a ploughed field where there is no corn residue with the fungus \"Cercospora zea-maydis\", the causal agent of Grey leaf spot of corn, present but the weather means extended periods of leaf wetness, there is no infection initiated.\n\nWhen a pathogen requires a vector to be spread then for an epidemic to occur the vector must be plentiful and active.\n\nPathogens cause monocyclic epidemics with a low birth rate and death rate, meaning they only have one infection cycle per season. They are typical of soil-borne diseases such as Fusarium wilt of flax. Polycyclic epidemics are caused by pathogens capable of several infection cycles a season. They are most often caused by airborne diseases such as powdery mildew. Bimodal polycyclic epidemics can also occur. For example, in brown rot of stone fruits the blossoms and the fruits are infected at different times.\n\nFor some diseases it is important to consider the disease occurrence over several growing seasons, especially if growing the crops in monoculture year after year or growing perennial plants. Such conditions can mean that the inoculum produced in one season can be carried over to the next leading to a build of inoculum over the years. In the tropics there are no clear-cut breaks between growing seasons as there are in temperate regions and this can lead to accumulation of inoculum.\n\nEpidemics that occur under these conditions are referred to as \"polyetic\" epidemics and can be caused by both monocyclic and polycyclic pathogens. Apple powdery mildew is an example of a polyetic epidemic caused by a polycyclic pathogen and Dutch Elm disease a polyetic epidemic caused by a monocyclic pathogen.\n\nPlant disease epidemiologists are typically employed as researchers by universities, or governmental institutions such as the USDA. However, private companies in agricultural fields also employ epidemiologists.\n\n\n"}
{"id": "8026125", "url": "https://en.wikipedia.org/wiki?curid=8026125", "title": "Plowshares Project", "text": "Plowshares Project\n\nThe Plowshares Project is a Peace Studies collaborative between Manchester University, Earlham College and Goshen College in Indiana.\n\nThe Plowshares Project was formed in 2002, and is funded by a four-year grant from Eli Lilly and Company. The project defines its mission and goals by the religious ethos of the Church of the Brethren, the Society of Friends, and Mennonite Church USA.\n\nPlowshares was initiated to enrich the Peace Studies programs at Manchester, Earlham, and Goshen colleges. The program facilitates channels of discussion, and seeks to uphold the peacemaking traditions of the colleges' founding churches.\n\nWith the Lilly grant, the Plowshares Program intends to:\n\nThe Peace House is the main outlet for community involvement for the Plowshares Project. Located in Indianapolis, Indiana, the house serves as a hub for undergraduate students from any US college or university, and in any major, to become directly involved in the fields of peace and justice. \n\nThe Peace House provides semester-long residence for students pursuing internships with Indianapolis companies and organizations who are involved or concerned with peace and justice issues. There is also a nine-week summer intensive available for students. \n\nDuring the regular semester, students at the Peace House take two Peace Studies courses taught by adjunct faculty. From the Plowshares website:\n\nThe internship session mediated by Peace House allows for up to 20 hours of work per week under the supervision of the Peace House director; the work grants up to six credit hours. \n\nThe nine-week summer session operates without the class work, but still facilitates a full-time internship. The student may work up to 40 hours per week with a selected organization, and may still earn up to six credit hours. During the summer, students may also reside at the Peace House without earning college credit.\n\n1: http://www.plowsharesproject.org/php/peacehouse/credits.php\n\n"}
{"id": "12416759", "url": "https://en.wikipedia.org/wiki?curid=12416759", "title": "Recognized Air Picture", "text": "Recognized Air Picture\n\nA Recognized Air Picture or RAP is a (theoretically) complete listing of all aircraft in flight within a particular airspace, with each aircraft being identified as friendly or hostile, and ideally containing additional information such as type of aircraft, flight number, and flight plan. The information may be drawn from a number of different sources, including military radar, civilian air traffic controllers, and allied nations or multi-national organizations such as NATO.\n\nFor the United States and Canada, the Recognized Air Picture is maintained continuously by NORAD at Peterson Air Force Base (with Cheyenne Mountain Complex as a warm standby). In the United Kingdom, it is maintained by the United Kingdom Combined Air Operations Centre at RAF Air Command at RAF High Wycombe. For the rest of Europe, NATO has an Air Command and Control System program underway to unify the various systems in use by NATO members throughout Europe.\n"}
{"id": "28623094", "url": "https://en.wikipedia.org/wiki?curid=28623094", "title": "Taurus–Littrow", "text": "Taurus–Littrow\n\nTaurus–Littrow is a lunar valley located on the near side at the coordinates . It served as the landing site for the American Apollo 17 mission in December 1972, the last manned mission to the Moon to date.\nThe valley is located on the southeastern edge of Mare Serenitatis along a ring of mountains formed between 3.8 and 3.9 billion years ago when a large object impacted the Moon, forming the Serenitatis basin and pushing rock outward and upward. Taurus–Littrow is located in the Taurus mountain range and south of Littrow crater, features after which the valley received its name. The valley's name, coined by the Apollo 17 crew, was eventually approved by the International Astronomical Union in 1973.\n\nData collected on Apollo 17 show that the valley is composed primarily of feldspar-rich breccia in the large massifs surrounding the valley and basalt underlying the valley floor, covered by an unconsolidated layer of regolith, or mixed materials, formed by various geologic events. Taurus–Littrow was selected as the Apollo 17 landing site after the other candidates were eliminated for various reasons. The landing site was chosen with the objectives of sampling highland material and young volcanic material in the same location.\n\nSeveral million years after the formation of the Serenitatis basin, lavas began to upwell from the Moon's interior, filling the basin and forming what is now known as Mare Serenitatis. As a result of these lavas, rock and soil samples from the area that were collected by Apollo 17 astronauts Eugene Cernan and Harrison Schmitt gave insight to the natural history and geologic timeline of the Moon.\nSomewhere between 100 and 200 million years after the Serenitatis basin and Taurus–Littrow formed, the lavas that began to seep through the lunar crust began to flood the low-lying areas. These lava flows were often accompanied by fire fountains that blanketed the surrounding area with tiny glass beads. These beads were sometimes colored orange, explaining the orange soil discovered by the Apollo 17 astronauts at Shorty crater. Most of these beads, however, were darkly colored, resulting in the dark appearance of Mare Serenitatis from Earth.\n\nThe valley itself is elongated along an axis that points toward the center of Mare Serenitatis. Large massifs are located on either side of the valley, called the North and South massifs, respective to their geographic location in relation to each other. The height of these massifs give the valley a depth greater than that of the Grand Canyon in the United States. Along the South Massif lies Bear Mountain, named after a mountain of the same name near Harrison Schmitt's hometown of Silver City, New Mexico. The sculptured hills and East massif make up the east edge of the valley and to the west, a scarp cuts across the valley floor and rises about two kilometres (1.2 miles) above it. The North and South massifs funnel into the main outlet of the valley into Mare Serenitatis, partially blocked by Family mountain.\n\nBased on Apollo 17 observations, the valley floor is generally a gently rolling plane. Boulders of various sizes and other geologic deposits are scattered throughout the valley. At the ALSEP lunar experiment deployment area, located west of the immediate landing site, the boulders average about four meters in size and are higher in concentration than in other areas of the valley.\n\nThe Tycho impact, which occurred between 15–20 and 70–95 million years ago, formed secondary crater clusters in various locations of the Moon. Data from the examination of these clusters suggest that the central crater cluster in the valley formed as a result of said impact. Upon analysis of known secondary impact clusters resulting from the Tycho impact, it has been discovered that the majority of them have a downrange ejecta blanket, or debris layer, with a distinctive 'birdsfoot' pattern. Apollo 17 observation data and comparison between the valley's central crater cluster and known Tycho secondary impacts shows many similarities between the two. The central crater cluster has a birdsfoot ejecta pattern that points in the direction of Tycho and the debris pattern of the light mantle points directly towards the South massif. The latter piece of evidence further supports the hypothesis that the light mantle formed as a result of an avalanche occurring on the aforementioned massif because of secondary Tycho impacts. Large-scale analysis suggests that the crater cluster may be part of a larger secondary cluster of Tycho, including craters on the North massif and other clusters as far north as Littrow crater. If indeed related, the smaller clusters form a large cluster that could be part of a nearby ray of Tycho.\n\nEvidence from the Apollo 17 mission shows that the massifs surrounding the valley are composed primarily of feldspar-rich breccia and that basalt underlies the valley floor, a result of the lava flows during the valley's history. Seismic studies suggest that the basalt below the valley floor is about 1400 metres (4593.2 feet) thick. Above the layer of subfloor basalt lies a deposit of unconsolidated material of various compositions ranging from volcanic material to impact-formed regolith. The valley floor's unusually low albedo, or reflectivity, is a direct result of the volcanic material and glass beads located there. The deeper craters on the valley floor act as 'natural drill holes' and allowed the astronauts to sample the subfloor basalt. These basalt samples are composed primarily of plagioclase, but also contain amounts of clinopyroxene and other minerals.\n\nThe unconsolidated regolith layer on the valley floor has a thickness of about and contains ejecta from many impacts, most notably Tycho. This enabled samples to be retrieved from this impact without having to visit the crater itself. The possibility that some craters in the valley could be secondary impacts of Tycho created further opportunities for sampling ejecta from that impact.\n\nThere are several geologic deposits on the valley floor originating from several different events in the geologic timeline of the Moon. One of these formations, the light mantle, is a deposit of lightly colored material in a series of projections extending about six kilometres (3.7 mi) from the south massif across the floor. Pre-mission analyses suggested that this deposit might be the result of an avalanche originating from the northern slope of the south massif. Post-mission analysis of samples of the material show that it is primarily composed of fine-grained material and scattered rock fragments that were presumably spread across the valley floor from the south massif at some point. Evidence from samples and visual observations taken during Apollo 17 show that the light mantle varies in thickness throughout the valley. Craters located farther away from the south massif penetrate through the light mantle to darker underlying material. Meanwhile, craters close to the south massif as wide as do not appear to penetrate to darker material at all. The age of this formation is estimated to be about the same as the central crater cluster, or about 70–95 million years old.\n\nTroctolite 76535, a coarse-grained troctolite composed primarily of olivine and plagioclase was recovered in the valley as part of a rake sample. The sample has been called the most interesting to be returned from the Moon. This sample has been the subject of thermochronological calculations in order to determine whether the Moon ever generated a core dynamo or formed a metallic core.\n\nRocks sampled in the immediate vicinity of the Lunar Module are mostly vesicular coarse-grained subfloor basalt, with some appearance of fine-grained basalt as well. Much of the valley floor, as displayed in observations of the immediate landing area, is predominately regolith and fragments varying in sizes excavated by several impacts in the Moon's history.\n\nAs Apollo 17 was the final lunar mission of the Apollo program, several different scientific objectives were identified in order to maximize the scientific productivity of the mission. Landing sites considered for previous missions, but were rejected were reconsidered. Taurus–Littrow was one of several potential landing sites considered for Apollo 17 along with Tycho crater, Copernicus crater, Tsiolkovskiy crater on the far side, among others. All but Taurus–Littrow were eventually eliminated for scientific and/or operational reasons. A landing at Tycho was thought to be too dangerous because of the rough terrain found there, a landing on the far side in Tsiolkovskiy would add the expense of communications satellites necessary to maintain contact between the crew and ground control during surface operations, and a landing in Copernicus was regarded as low priority.\n\nTaurus–Littrow was eventually selected with the objectives of sampling ancient highland material and young volcanic material in the same landing site. The Taurus–Littrow site offered both of these in the form of highland material in the Tycho ejecta sampled and the prospect that some of the craters in the area could be volcanic vents.\n<br>\n\n\n"}
{"id": "36675611", "url": "https://en.wikipedia.org/wiki?curid=36675611", "title": "Taxonomy (general)", "text": "Taxonomy (general)\n\nTaxonomy is the practice and science of classification. The word is also used as a count noun: a taxonomy, or taxonomic scheme, is a particular classification. The word finds its roots in the Greek language , \"taxis\" (meaning 'order', 'arrangement') and , \"nomos\" ('law' or 'science'). Originally, \"taxonomy\" referred only to the classification of organisms or a particular classification of organisms. In a wider, more general sense, it may refer to a classification of things or concepts, as well as to the principles underlying such a classification. Taxonomy is different from meronomy which is dealing with the classification of parts of a whole.\n\nMany taxonomies have a hierarchical structure, but this is not a requirement. Taxonomy uses taxonomic units, known as taxa (singular taxon).\n\nWikipedia categories illustrate a taxonomy and a full taxonomy of Wikipedia categories can be extracted by automatic means. Recently, it has been shown that a manually-constructed taxonomy, such as that of computational lexicons like WordNet, can be used to improve and restructure the Wikipedia category taxonomy.\n\nIn a broader sense, taxonomy also applies to relationship schemes other than parent-child hierarchies, such as network structures. Taxonomies may then include single children with multi-parents, for example, \"Car\" might appear with both parents \"Vehicle\" and \"Steel Mechanisms\"; to some however, this merely means that 'car' is a part of several different taxonomies. A taxonomy might also simply be organization of kinds of things into groups, or an alphabetical list; here, however, the term vocabulary is more appropriate. In current usage within knowledge management, taxonomies are considered narrower than ontologies since ontologies apply a larger variety of relation types.\n\nMathematically, a hierarchical taxonomy is a tree structure of classifications for a given set of objects. It is also named containment hierarchy. At the top of this structure is a single classification, the root node, that applies to all objects. Nodes below this root are more specific classifications that apply to subsets of the total set of classified objects. The progress of reasoning proceeds from the general to the more specific.\n\nBy contrast, in the context of legal terminology, an open-ended contextual taxonomy is employed—a taxonomy holding only with respect to a specific context. In scenarios taken from the legal domain, a formal account of the open-texture of legal terms is modeled, which suggests varying notions of the \"core\" and \"penumbra\" of the meanings of a concept. The progress of reasoning proceeds from the specific to the more general.\n\nAnthropologists have observed that taxonomies are generally embedded in local cultural and social systems, and serve various social functions. Perhaps the most well-known and influential study of folk taxonomies is Émile Durkheim's \"The Elementary Forms of Religious Life\". A more recent treatment of folk taxonomies (including the results of several decades of empirical research) and the discussion of their relation to the scientific taxonomy can be found in Scott Atran's \"Cognitive Foundations of Natural History.\" Folk taxonomies of organisms have been found in large part to agree with scientific classification, at least for the larger and more obvious species, which means that it is not the case that folk taxonomies are based purely on utilitarian characteristics.\n\nIn the seventeenth century the German mathematician and philosopher Gottfried Leibniz, following the work of the thirteenth-century Majorcan philosopher Ramon Llull on his \"Ars generalis ultima\", a system for procedurally generating concepts by combining a fixed set of ideas, sought to develop an alphabet of human thought. Leibniz intended his \"characteristica universalis\" to be an \"algebra\" capable of expressing all conceptual thought. The concept of creating such a \"universal language\" was frequently examined in the 17th century, also notably by the English philosopher John Wilkins in his work \"An Essay towards a Real Character and a Philosophical Language\" (1668), from which the classification scheme in Roget's Thesaurus ultimately derives.\n\nVegas et al. make a compelling case to advance the knowledge in the field of software engineering through the use of taxonomies. Similarly, Ore et al. provide a systematic methodology to approach taxonomy building in software engineering related topics.\n\nSeveral taxonomies have been proposed in software testing research to classify techniques, tools, concepts and artifacts. The following are some example taxonomies: \n\nEngström et al. suggest and evaluate the use of a taxonomy to bridge the communication between researchers and practitioners engaged in the area of software testing. They have also developed a web-based tool to facilitate and encourage the use the taxonomy. The tool and its source code are available for public use.\n\nCiting inadequacies with current practices in listing authors of papers in medical research journals, Drummond Rennie and co-authors called in a 1997 article in \"JAMA\", the \"Journal of the American Medical Association\" for \n\n\"a radical conceptual and systematic change, to reﬂect the realities of multiple authorship and to buttress accountability. We propose dropping the outmoded notion of author in favor of the more useful and realistic one of contributor.\" \n\nSince 2012, several major academic and scientific publishing bodies have mounted \"Project CRediT\" to develop a controlled vocabulary of contributor roles. Known as \"CRediT (Contributor Roles Taxonomy)\", this is an example of a flat, non-hierarchical taxonomy; however, it does include an optional, broad classification of the degree of contribution: \"lead\", \"equal\" or \"supporting\". Amy Brand and co-authors summarise their intended outcome as:\n\nIdentifying specific contributions to published research will lead to appropriate credit, fewer author disputes, and fewer disincentives to collaboration and the sharing of data and code. \n\nAs of mid-2018, this taxonomy apparently restricts its scope to \"research outputs\", specifically journal articles; however, it does rather unusually \"hope to … support identification of peer reviewers\". (As such, it has not yet defined terms for such roles as editor or author of a chapter in a \"book\" of research results.) Version 1, established by the first Working Group in the (northern) autumn of 2014, identifies 14 specific contributor roles using the following defined terms:\n\nReception has been mixed, with several major publishers and journals planning to have implemented CRediT by the end of 2018, whilst almost as many aren't persuaded of the need or value of using it. For example,\n\nThe National Academy of Sciences has created a \"TACS (Transparency in Author Contributions in Science)\" webpage to list the journals that commit to setting authorship standards, defining responsibilities for corresponding authors, requiring ORCID iDs, and adopting the CRediT taxonomy.\n\nThe same webpage has a table listing 21 journals (or families of journals), of which:\n\nThe taxonomy is an open standard conforming to the OpenStand principles, and is published under a Creative Commons licence.\n\nWebsites with a good designed taxonomy or hierarchy is easily understood by users, due to their possibility to develop a mental model of the site structure.\n\nTwo of the predominant types of relationships in knowledge-representation systems are predication and the universally quantified conditional. Predication relationships express the notion that an individual entity is an example of a certain type (for example, \"John is a bachelor\"), while universally quantified conditionals express the notion that a type is a subtype of another type (for example, \"A dog is a mammal\", which means the same as \"All dogs are mammals\").\n\n\n"}
{"id": "3043532", "url": "https://en.wikipedia.org/wiki?curid=3043532", "title": "Technological Forecasting and Social Change", "text": "Technological Forecasting and Social Change\n\nTechnological Forecasting and Social Change (formerly \"Technological Forecasting\") is a peer-reviewed academic journal published by Elsevier which discusses futures studies, technology assessment, and technological forecasting. Articles focus on methodology and actual practice, and has been published since 1969.\n\nThe editor-in-chief is Fred Phillips, National Chengchi University, Taiwan. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 2.625 and a 5-Year Impact Factor of 3.226.\n\n\n"}
{"id": "1127090", "url": "https://en.wikipedia.org/wiki?curid=1127090", "title": "The Voyage of the Mimi", "text": "The Voyage of the Mimi\n\nThe Voyage of the Mimi is a thirteen-episode American educational television program depicting the crew of the ship \"Mimi\" exploring the ocean and taking a census of humpback whales. The series aired on PBS (Public Broadcasting Service) and was created by the Bank Street College of Education in 1984 to teach middle-schoolers about science and mathematics in an interesting and interactive way, where every lesson related to real world applications. \n\nThe series was also released on VHS and as a LaserDisc collection. In August 2014, the series was released in digital form via iTunes U.\n\nAfter a segment of a fictional adventure in the first part of each episode, a corresponding \"expedition documentary\" taught viewers something scientific relating to plot events in the previous episode of the show. For example, there is an episode where the plot is about obtaining drinkable water, and over the course of the episode, the viewer is also given lessons about condensation, heat, and the three states of matter. Each lesson has accompanying student and teacher handouts or worksheets. Four software modules are available that covered topics and skills in navigation and map reading, computer literacy and programming, the elements of ecosystems, and the natural environment of whales.\n\n\"The Voyage of the Mimi\" was shot in Marblehead, Massachusetts with some scenes being shot on Dyer Island, Maine. It marked Ben Affleck's television debut role.\n\nA second series was produced in 1988, \"The Second Voyage of the Mimi\", in which the two Granvilles, along with other archaeologists, searched for a lost Mayan city and uncovered a conspiracy along the way. Both series emphasized equal opportunity in math and science with a diverse cast (including race, gender, and disability status) and incorporated an instructional strategy wherein the fictionalized adventure would catch the interest of students for the initial part of the learning process. A third series, which would have been about the Mississippi River, including the river's biology and history, was planned but was not made due to an inability to obtain funding.\n\nBen Affleck played C.T. Granville and Peter G. Marston played his grandfather Captain Granville. Marston was a scientist at Massachusetts Institute of Technology during the production of the program and also the owner of the actual \"Mimi\" at the time.\n\nCast Listing:\n\nFilm Crew Listing:\n\nEach episode consists of two fifteen-minute segments: the fictional story, then an expedition that reveals the fact behind the fiction.\n\nThe first segment of each episode follows a serialized tale of scientists taking a census of humpback whales off the coast of Massachusetts. Captain Clement Tyler Granville, the owner of the sailboat Mimi is hired by scientist Anne Abrams and her colleague Ramon Rojas to make the census. Anne's Graduate Research Assistant is Sally Ruth Cochran. In addition, the two scientists each invite a high school student (Arthur Spencer and Rachel Fairbanks) to take part in the study. Finally, Captain Granville's identically-named grandson comes visiting for the summer in order to give his mother a break during her pregnancy.\n\nEach second segment is a standalone exploration of one of the scientific principles touched on in the serialized tale. In these segments, an actor (Ben Affleck, Mark Graham, or Mary Tanner) portraying a young person comes out of character and interviews a real, in many cases well-known, scientist about his or her work. These scientists include oceanographer Sylvia Earle, geologist Kim Kastens, zoologist Katharine Payne, Greg Watson of the New Alchemy Institute, and physicist Ted Taylor.\n\nIn addition, Judy Pratt, a deaf student at Gallaudet University, and Peter Marston, a scientist at M.I.T., come out of character in interviews with Mary Tanner and Ben Affleck at their respective workplaces.\n\nThe Mimi is a French-built sailboat that is 72 feet (21 meters) in length, originally built in 1934 to function as a deep-hulled cargo barge. She was built in northwest France in the region of Brittany, on the coast of the Mer d'Iroise (the Iroise Sea). Mimi is a type of vessel known as a \"Gabare d'Iroise,\" where \"Gabare\" translates as \"cargo barge\" and \"Iroise\" refers to the region in which she was constructed.\n\nMimi was initially used as a cargo ship in the rough waters of the North Sea, and was thus built to withstand serious maritime conditions. Because Mimi was a \"gabare,\" she was also built with a shallow draft. This combination of strength and ability to operate in shallow waters allowed Mimi to be used both in the open sea and the extensive canal system in Europe at that time.\n\nAfter serving many years in the Northern part of France, Mimi was sold to an owner in the Southern part of France where she was converted to a fishing trawler for tunafish.\n\nThe Mimi was used by German soldiers during the Second World War to transport munitions. In 1942, the Germans seized the Mimi for the purpose of transporting supplies to military outposts in the region of the Brittany coast. When the Allied Forces pushed the retreating Axis forces back eastward through France in August 1944, Nazi protocol was to destroy any military property that might possibly be used against them by the invading forces (i.e. fortresses, ammunition, vehicles, etc.) For reasons unknown, Mimi was not destroyed by retreating Nazi forces, but rather left tied to a tree on a mudflat. \n\nAfter the war the Mimi was sunk, and remained so until the 1960s when a Frenchman and his family bought it and converted it from a trawler to a sailboat.\n\nBy 1984, the Mimi had a new owner, Peter Marston. The boat was kept moored in Gloucester, Massachusetts, throughout the filming of the series and thereafter. In addition to its role in \"The Voyage of the Mimi,\" which began in 1984, the boat was used from the late 1980s through the 1990s to teach schoolchildren using the \"Mimi\" curriculum. Each school year, the Mimi sailed from New England to the Gulf of Mexico and back, stopping at pre-arranged ports of call to meet with students in grades 4 through 7, and their teachers. At each port, \"Mimifests\" were held, which included various activities and presentations about marine life, seamanship, and navigation. These events were attended by approximately 30,000 students each year. In 1988, Peter Marston and other freelance musicians produced a cassette, \"Sea Songs from the Mimi Crew\", of old-time sea songs self-published under the name \"The Barn School\" based in Gloucester, Massachusetts.\nMarston retained ownership of the vessel until 1999, when the boat was sold to new owners Captain George G. Story of Gloucester, Massachusetts, his brother Captain Alan M. Story of Deltona, Florida, and Spiro \"Steve\" Cocotas, also from Gloucester. They operated the vessel as Three Mates Inc. for several years, bringing the boat to as many as 28 cities along the east coast.\n\nAfter three years of ownership under Three Mates Inc., Mimi was repossessed for financial reasons and sold at public auction in Massachusetts. Michael Spurgeon developed a plan to resurrect the Mimi, and the vessel was subsequently purchased with venture capital provided by Spurgeon's employer, Capt. Greg Muzzy, a Boston-area entrepreneur who owns and operates the \"Liberty Fleet of Tall Ships,\". Mimi was sailed from Gloucester to the Mystic River in Boston, where she was kept docked at various marinas in East Boston and Chelsea.\n\nSpurgeon's intention was to rehabilitate the ship and possibly court a Discovery Channel special about Mimi's story. After spending approximately $100,000 on infrastructural investments on the ship, including a complete rebuild of the stern and diesel engine, the ship became too costly to continue work on.\n\nIn 2008, it was discovered that a homeless man had been living aboard the ship while docked at the marina, and he was promptly kicked out. In an act of revenge, the man returned and removed one of several plugs in the belly of the ship, allowing her to rapidly fill with water. Mimi sank while at port, effectively ruining all electronics aboard the ship as well as seriously damaging the recently rebuilt engine. A significant amount of damage occurred above the keel of the ship due to freshwater clams colonizing the wood while she was underwater, rendering restoration nearly impossible.\n\nShe was floated back to the surface by a recovery team two weeks later, and sat disused after that.\n\nIn the summer of 2010, two recent college graduates of the University of Vermont who had been fans of \"The Voyage of the Mimi\" stumbled upon the Mimi at port and mounted an effort to save the ship, which had fallen into a state of disrepair. Their efforts ultimately proved unsuccessful, given among other factors the high cost that would be required to save the ship and the Mimi's limited historical value, so the Mimi was scrapped in July 2011.\n"}
{"id": "33900921", "url": "https://en.wikipedia.org/wiki?curid=33900921", "title": "The Yipping Tiger", "text": "The Yipping Tiger\n\nThe Yipping Tiger and Other Tales from the Neuropsychiatric Clinic is a book by neuropsychiatrist Perminder Sachdev, M.D. consisting of ten case studies which explore the relationship between the brain and the mind. The case studies are based on Sachdev’s experience in the neuropsychiatric clinic at the Neuropsychiatric Institute, Sydney, Australia. Each case study examines a different medical condition, the current research findings related to the condition, and the challenges these conditions pose for the doctor and patient.\n\n\n"}
{"id": "27704981", "url": "https://en.wikipedia.org/wiki?curid=27704981", "title": "University of Maryland College of Behavioral and Social Sciences", "text": "University of Maryland College of Behavioral and Social Sciences\n\nThe University of Maryland College of Behavioral and Social Sciences is one of the 13 schools and colleges at the University of Maryland, College Park. With 10 departments, it is one of the largest colleges at the university, with three in ten University of Maryland undergraduates receiving their degree from the college. 45 research centers also are located in the College. Its social science programs are collectively ranked 10th in the United States by the Faculty Scholarly Productivity Index, and 18th in the world by the Institute of Higher Education at Shanghai Jiao Tong University.\n\nThe College of Behavioral and Social Sciences began as \"The School of Liberal Arts\" in 1919, and was headquartered in Morrill Hall; Frederic E. Lee served as the school's first dean. In the 1920s, it became \"The College of Arts and Sciences,\" with five separate divisions. In 1936, the college moved into the newly completed College of Arts and Sciences Building, which was renamed Francis Scott Key Hall in 1955. In the 1940s, the departments of Economics, Geography and Government & Politics moved into The College of Business and Public Administration.\n\nIn 1972, the College of Arts and Sciences and the College of Business and Public Administration combined to become the new \"Division of Behavioral and Social Sciences\", one of five divisions in the university. In 1986, the five divisions split into fourteen colleges, and The College of Behavioral and Social Sciences was formed. The college has been headquartered in Millard E. Tydings Hall since 1993.\n\nCIVICUS is a two-year living and learning undergraduate program in the College of Behavioral and Social Sciences, which links academic coursework together with participation in internships and community service to provide an experience of civil service engagement for participants (known as CIVICUS Associates). CIVICUS Associates live together in Somerset Hall (located in the North Hill Community), which was renovated in 1999 for the CIVICUS Living and Learning Program. Somerset Hall also houses the program's offices and hosts CIVICUS classes. The hall is designed with several study and social lounges to enhance students' living and learning experience. Somerset is located centrally on campus making the walk to classes, the dining hall, McKeldin Library, and the Stamp Student Union quick and convenient. All the rooms in Somerset are air conditioned and each student has his/her own internet, phone, and cable jack. Most of the rooms are doubles with a few triples and quads scattered throughout the building. There are also some singles. In addition, there is a computer lab and laundry facility located on the ground floor. Each floor also has one or two small kitchenettes with a microwave and a sink.\n\nFor students entering CIVICUS in the fall of their freshman year, the CIVICUS classes students are required to take in their first semester consist of a 1-credit class called CIVICUS Student and the University (BSCV181) and a 3-credit class called Introduction to CIVICUS (BSCV191). In their second semester, students take a 1-credit class called CIVICUS and Service Learning (BSCV182) and a 3 credit class called Introduction to Contemporary Social Problems (SOCY105). In their third semester in the CIVICUS program, students take a 3-credit class called Leadership in a Multicultural Society. In the last semester in the program students take the CIVICUS Capstone 3 credit class (BSCV302) that gives them academic credit for their CIVICUS Internship. A student can take a 1 credit class instead if the student is getting academic credit from another academic program. The creator and current director of the program is Dr. Sue Briggs, who teaches the majority of the required courses, as well as provides assistance and information regarding all aspects of the program.\n\nIn addition to the in-class requirements, CIVICUS associates are responsible for completing four service projects per semester, a long term service project, and an internship. Associates also participate in an additional two community days of service, once per semester. Students are able to participate in wide variety of service activities, ranging from those on campus, to those in the surrounding counties and Washington, D.C. The program is based on the five principles of civil society: citizenship, leadership, community building in a diverse society, scholarship, and community service-learning. CIVICUS is a program for enthusiastic, motivated, and dedicated students who want to get involved with the campus and local community to make a positive difference. CIVICUS comprises a diverse group of student leaders whose distinct personalities, perspectives, and backgrounds enrich class discussions, service projects, and the conversations throughout the halls of Somerset. This camaraderie continues even after CIVICUS students have completed their citation, as many CIVICUS juniors and seniors return regularly to Somerset and remain involved with the program. In addition to being involved with the CIVICUS community, many of the students are actively involved in the university and local communities.\n\nUniversity of Maryland freshmen applicants whose application materials suggest they possess significant levels of leadership, involvement, and motivation are invited to join the program. A total of 130 associates are in the program at a time.\n\nThe University of Maryland Mock Trial Team is a student organization which engages in intercollegiate mock trial competition. Based out of the Department of Government and Politics in the College of Behavioral and Social Sciences, the team first began competing in 1990. The Maryland team has won five national championships (2008, 2000, 1998, 1996, 1992), which ranks the most of any university, and was also the national runner-up in 1992 and 1993.\n\nThere are two endowed chairs within the College of Behavioral and Social Sciences: the \"Anwar Sadat Chair for Peace and Development\", currently held by Shibley Telhami, and the \"Bahá'í Chair for World Peace\", currently held by John Grayzel, are at the Center for International Development and Conflict Management, which is a center within the Department of Government and Politics.\n\nNotable faculty in the College include:\n\nNotable former faculty members include:\n\nOther prominent alumni include:Eric F. Billings, Chairman and Chief Executive Officer of FBR Capital Markets Corporation; John Dryzek, social and political theorist; Robert W. Jordan, former U.S. Ambassador to Saudi Arabia; Kori Schake, former director for Defense Strategy and Requirements on the National Security Council; Charles Schultze, former Chairman of the United States Council of Economic Advisers; and Torrey Smith, an NFL wide receiver currently playing with the Philadelphia Eagles.\n\n\n"}
{"id": "58443485", "url": "https://en.wikipedia.org/wiki?curid=58443485", "title": "V. P. Balagangadharan", "text": "V. P. Balagangadharan\n\nVannadil Puthiyaveettil \"V. P.\" Balagangadharan (born 20 January 1950) is a space scientist, author, and speaker from India, known for his contributions to propellant research, technology transfer, and science communication. He has worked with the Indian Space Research Organisation for over four decades in various roles.\n\nAfter finishing his schooling at AV Smaraka Govt. Higher Secondary School, Karivellur, and Pre-Degree from Payyanur College, Balagangadharan joined Malabar Christian College, Kozhikode in 1967, from where he completed his Bachelor of Science in Chemistry in 1970. While working at Vikram Sarabhai Space Centre, he completed his Master of Science in Chemistry from Pune University in 1989.\n\nBalagangadharan started his career as an Analytical Chemist at Bombay Oil Industries, Angamaly in 1971. In 1972, he joined Vikram Sarabhai Space Centre as a Scientific Assistant in its Propellant Engineering Division, later Analytical & Spectroscopy Division. After almost four decades of service, in 2010, he superannuated as the Group Head, Technology Transfer and Documentation Division at the centre. Post superannuation, he held the emeritus role as Prof. Brahm Prakash Scientist—two stints, separated by a one year role as Consultant and Advisor (Media & Communications)—till 2014\n\nHe was formerly the Chairman of the Patent Advisory Committee of the Government of Kerala.\n\nHe has over 30 research publications in the area of Analytical chemistry. He has delivered over over three hundred talks, and shows on All India Radio and Doordarshan. He has lectured across India on India's space history and Intellectual Property Rights. He has also written many articles in the print media on popular science, space research and IPR.\n\n\nKerala State Science Literature Award, 2015, Kerala State Council for Science, Technology and Environment Fellow, Kerala Academy of Sciences\n"}
