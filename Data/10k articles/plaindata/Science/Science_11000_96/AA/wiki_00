{"id": "8216589", "url": "https://en.wikipedia.org/wiki?curid=8216589", "title": "A1077 road", "text": "A1077 road\n\nThe A1077 road runs through North Lincolnshire, England, between Scunthorpe and South Killingholme.\n\nThe western terminus of the A1077 starts at the M181 motorway \"Frodingham Grange\" roundabout in Gunness, next to the ground of Scunthorpe United F.C., a large Tesco, a Frankie & Benny's, the Gallagher Retail Park, and Travelodge Scunthorpe. It is Scunthorpe's northern ring road, built as the North West Orbital in stages 1, 2A, 2B and 3, funded by the ERDF, and known now as the \"Phoenix Parkway\". There is left turn at the \"Neap House Junction\" for the B1216 for Gunness, in Flixborough, then the \"Skippingdale Roundabout\" for the Skippingdale Industrial Estate to the north and OSI Food Solutions to the south; this factory makes all the burgers for McDonald's in the UK - 3 million a day. There is the \"Foxhills Roundabout\" for the large Foxhills Industrial Estate, home of the national Nisa (retailer), then a roundabout for the B1430 for Normanby Hall and the Normanby Distribution Park, then a smaller roundabout for the Sawcliffe Industrial Estate to the south, and a roundabout with the A1029 (for Scunthorpe) at the Dragonby Vale Enterprise Park. \n\nIt passes through Roxby cum Risby, and Roxby, Lincolnshire itself. At Winterton, the north-south route has a staggered junction with the B1430. The eastwards route is the parish boundary between Winterton and Winteringham, with a right turn for the B1207 for Winterton. The landscape is flat, alongside the southern bank of the Humber. It meets the A15 at the grade-separated \"Barton on Humber Junction\", built in 1978.\n\nThe road passes west-east through the middle of Barton-upon-Humber. The road passes a former Kimberly-Clark factory, to the north, which made Huggies. It passes through Thornton Curtis and the \"Thornton Hunt\", then Wootton, North Lincolnshire and the \"Nags Head\". This section of the road is part of a larger north-west to south-east route that is known as \"Barton Street\" and leads onto the A18. At Ulceby Grange there is a T-junction, and a right turn for the B1211, for Croxton and Brigg. The road passes west-east through Ulceby, North Lincolnshire. \"Barton Street\" is the B1211 to the south, at a right turn, leading to the A18. At Ulceby railway station there is a level crossing with the Barton line, and the road enters South Killingholme, and passes the \"Yarborough Arms\". A few hundred metres after the road is crossed by two 400kV pylon lines, the road has its eastern terminus at the A160 T-junction.\nThe western terminus was originally in Scunthorpe at a roundabout of the A1076 and A1029.\n\n"}
{"id": "1012408", "url": "https://en.wikipedia.org/wiki?curid=1012408", "title": "A Short History of Nearly Everything", "text": "A Short History of Nearly Everything\n\nA Short History of Nearly Everything by American author Bill Bryson is a popular science book that explains some areas of science, using easily accessible language that appeals more so to the general public than many other books dedicated to the subject. It was one of the bestselling popular science books of 2005 in the United Kingdom, selling over 300,000 copies.\n\n\"A Short History\" deviates from Bryson's popular travel book genre, instead describing general sciences such as chemistry, paleontology, astronomy, and particle physics. In it, he explores time from the Big Bang to the discovery of quantum mechanics, via evolution and geology.\n\nBill Bryson wrote this book because he was dissatisfied with his \"scientific knowledge\"—that was, not much at all. He writes that science was a distant, unexplained subject at school. Textbooks and teachers alike did not ignite the passion for knowledge in him, mainly because they never delved in the \"whys\", \"hows\", and \"whens\".\n\nBryson describes graphically and in layperson's terms the size of the universe and that of atoms and subatomic particles. He then explores the history of geology and biology and traces life from its first appearance to today's modern humans, placing emphasis on the development of the modern \"Homo sapiens\". Furthermore, he discusses the possibility of the Earth being struck by a meteorite and reflects on human capabilities of spotting a meteor before it impacts the Earth, and the extensive damage that such an event would cause. He also describes some of the most recent destructive disasters of volcanic origin in the history of our planet, including Krakatoa and Yellowstone National Park.\n\nA large part of the book is devoted to relating humorous stories about the scientists behind the research and discoveries and their sometimes eccentric behaviours. Bryson also speaks about modern scientific views on human effects on the Earth's climate and livelihood of other species, and the magnitude of natural disasters such as earthquakes, volcanoes, tsunamis, hurricanes, and the mass extinctions caused by some of these events.\n\nThe book contains several factual errors and inaccuracies. Some of these have arisen because new discoveries have been made since the book's publication, and some classifications have changed. For example, Pluto has been reclassified as a dwarf planet, and the universe is not going to stop expanding, it is speeding up.\n\nAn illustrated edition of the book was released in November 2005. A few editions in Audiobook form are also available, including an abridged version read by the author, and at least three unabridged versions.\n\nThe book received generally favourable reviews, with reviewers citing the book as informative, well-written, and highly entertaining. \n\nIn 2004, this book won Bryson the prestigious Aventis Prize for best general science book. Bryson later donated the GBP£10,000 prize to the Great Ormond Street Hospital children's charity.\n\nIn 2005, the book won the EU Descartes Prize for science communication.\n\nIt was shortlisted for the Samuel Johnson Prize for the same year.\n\n\n"}
{"id": "8046441", "url": "https://en.wikipedia.org/wiki?curid=8046441", "title": "Ameghinite", "text": "Ameghinite\n\nAmeghinite, Na<nowiki>[</nowiki>HBO] or NaBO(OH), is a mineral found in Argentina. It is a soft mineral with a Mohs hardness of 2-3. Ameghinite has a monoclinic crystal system.\n\nIt was first described in 1967 for an occurrence in the Tincalayu Mine, Salar Del Hombre Muerto, Salta, Argentina. It was named for Argentine geologist brothers, Carlos Ameghino (1865–1936) and Florentino Ameghino (1854–1911).\n"}
{"id": "347167", "url": "https://en.wikipedia.org/wiki?curid=347167", "title": "Aquatic and environmental engineering", "text": "Aquatic and environmental engineering\n\nAquatic and environmental engineering; an engineering topic, used sometimes as a synonym for Civil engineering by some universities in Sweden, since the word 'civil engineer' often refers to an engineering degree.\n\nAquatic engineering is where the engineer studies that of oceanography, and aquatic life in the area of fields."}
{"id": "53155094", "url": "https://en.wikipedia.org/wiki?curid=53155094", "title": "Benjamin August Gimmerthal", "text": "Benjamin August Gimmerthal\n\nBenjamin August Gimmerthal (1779, Zittau- 1848, Riga (then Russian Empire)) was a German entomologist who specialised in Diptera.\nHis collection of Chloropidae is held by the Museum of Systematic Zoology, University of Latvia, Riga. The remaining Diptera and other insects by the Natural History Museum of Latvia (Homepage)\n\npartial list\n\n"}
{"id": "38542005", "url": "https://en.wikipedia.org/wiki?curid=38542005", "title": "Bioinformatics workflow management system", "text": "Bioinformatics workflow management system\n\nA bioinformatics workflow management system is a specialized form of workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, that relate to bioinformatics.\n\nThere are currently many different workflow systems. Some have been developed more generally as scientific workflow systems for use by scientists from many different disciplines like astronomy and earth science. All such systems are based on an abstract representation of how a computation proceeds in the form of a directed graph, where each node represents a task to be executed and edges represent either data flow or execution dependencies between different tasks. Each system typically provides a visual front-end, allowing the user to build and modify complex applications with little or no programming expertise.\n\nIn alphabetical order, some examples of bioinformatics workflow management systems include:\n\nWith a large number of bioinformatics workflow systems to choose from, it becomes difficult to understand and compare the features of the different workflow systems. There has been little work conducted in evaluating and comparing the systems from a bioinformatician's perspective, especially when it comes to comparing the data types they can deal with, the in-built functionalities that are provided to the user or even their performance or usability. Examples of existing comparisons include\n\n\n\n"}
{"id": "10721459", "url": "https://en.wikipedia.org/wiki?curid=10721459", "title": "Byron Lamont", "text": "Byron Lamont\n\nProfessor Byron Barnard Lamont (born 2 January 1945) is a Western Australian botanist. He is currently a senior researcher within the Department of Environmental Biology of Curtin University of Technology. A specialist in ecology of the flora of the South West Botanic Province, he has published hundreds of papers.\n\nBorn in Perth, Western Australia, he attended Applecross and Mount Pleasant Primary Schools, and later Wesley College. From 1963 to 1966 he pursued undergraduate studies at the Instituate of Agriculture, University of Western Australia, graduating with a Bachelor of Agricultural Science with majors in Soils, Agronomy and Microbiology. He then undertook a Master's degree under the supervision of Brian Grieve, focussed on soil-plant relationships of \"Hakea\", especially its proteoid roots. His research into proteoid roots earned him a PhD. in 1974. Thereafter he began studying part time for a Doctor of Science degree, which was awarded in 1993. During this period he held a series of academic positions within Curtin University of Technology.\n\nAmong his many publications are two books, around 30 book chapters and review papers, and over 100 journal papers. He is the author of \"Hakea cygna\", \"H. c.\" subsp. \"needlei\" and \"H. erecta\". He also published the name \"Hakea rubriflora\", but this has since been found to be a synonym of \"H. denticulata\".\n\nLamont was made a Member of the Order of Australia (AM) in the 2010 Australia Day Honours \"For service to conservation and the environment, particularly Australian flora as an educator, researcher and author\".\n\nHe currently lives in the Perth suburb of Bull Creek. He is married with two adult children.\n"}
{"id": "9304783", "url": "https://en.wikipedia.org/wiki?curid=9304783", "title": "Consistent heuristic", "text": "Consistent heuristic\n\nIn the study of path-finding problems in artificial intelligence, a heuristic function is said to be consistent, or monotone, if its estimate is always less than or equal to the estimated distance from any neighboring vertex to the goal, plus the cost of reaching that neighbor.\n\nFormally, for every node \"N\" and each successor \"P\" of \"N\", the estimated cost of reaching the goal from \"N\" is no greater than the step cost of getting to \"P\" plus the estimated cost of reaching the goal from \"P\". That is:\nwhere\n\nA consistent heuristic is also admissible, i.e. it never overestimates the cost of reaching the goal (the converse, however, is not always true). This is proved by induction on formula_3, the length of the best path from node to goal. By assumption, formula_4, where formula_5 denotes the cost of the shortest path from \"n\" to the goal. Therefore,\nmaking it admissible. (formula_7 is any node whose best path to the goal, of length m+1, goes through some immediate child formula_8 whose best path to the goal is of length m.)\n\nHowever, an admissible heuristic formula_9, can be made into a consistent heuristic, formula_10, through the following adjustment:\n\nConsistent heuristics are called monotone because the estimated final cost of a partial solution, formula_12 is monotonically non-decreasing along the best path to the goal, where formula_13 is the cost of the best path from start node formula_14 to formula_15. It's necessary and sufficient for a heuristic to obey the triangle inequality in order to be consistent.\n\nIn the A* search algorithm, using a consistent heuristic means that once a node is expanded, the cost by which it was reached is the lowest possible, under the same conditions that Dijkstra's algorithm requires in solving the shortest path problem (no negative cost cycles). In fact, if the search graph is given cost formula_16 for a consistent formula_9, then A* is equivalent to best-first search on that graph using Dijkstra's algorithm. In the unusual event that an admissible heuristic is not consistent, a node will need repeated expansion every time a new best (so-far) cost is achieved for it.\n\n"}
{"id": "28669607", "url": "https://en.wikipedia.org/wiki?curid=28669607", "title": "Craig telescope", "text": "Craig telescope\n\nThe Craig telescope was a large telescope built in the 1850s, and while much larger than previous refracting telescopes, it had some problems that hampered its use. Its unique design and potential caused a great deal of excitement about it in its day. The telescope was ready in August 1852 and was visited by the William Parsons (he was famous for the Leviathan of Parsonstown, the largest telescope overall of this age). It is known to been used to observe the planet Saturn.\n\nIt was the largest refracting telescope (a telescope with a lens) in the world from 1852 to 1857, erected near London, England. It was a great refractor, a large refracting telescope with an achromatic doublet with an aperture of 61 cm (2 feet (24 inches)) and that was completed in 1852 in Wandsworth Common and dismantled around 1857 (although the brick tower probably survived until 1870). It had a focal length of 76–83 feet. Its namesake, the Rev. John Craig, spent a small fortune to produce a uniquely designed telescope with nearly double the aperture of the next largest refracting telescopes, making it the largest refracting telescope in the World for the better part of a decade. However, it had problem with its lens figuring starting from its first light in the summer of 1852. It soon fell into disuse as that same year Craig lost his only son, then his wife in 1854, and lost his brother and was put in jail for 6 weeks in 1856.\n\nCraig did not have the lens re-figured and the telescope struggled to achieve his modest goals, which included observations of Earth's Moon and Saturn. It was eventually demolished and Craig moved on to other projects, including opening one of the first indoor skating rinks.\n\nThe doublet was made with flint glass by Chance Brothers and a plate glass by Thames Plate Glass Company. The mounting was designed by William Gravatt, and featured a 19.5 meter tall brick tower with a 24.5 m long cigar shaped telescope tube (built by Messrs Rennie) slung from the side.\n\nThe next largest refractors were two 15 inch (38 cm) refractors built by Merz and Mahler of Münich (Joseph Fraunhofer's firm), one at Pulkovo Observatory in Europe and one at Harvard College Observatory in America. The largest telescope at the time was in Ireland, a 6-foot (183 cm) aperture metal mirror by William Parsons, 3rd Earl of Rosse. (see \"Leviathan of Parsonstown\")\n\nOne of the goals for the telescope was to look for a Moon for Venus and to confirm the third (Crepe) ring of saturn Some of the reported issues were with the overall lens quality and troubles in the personal life of Craig.\n\n\n\n"}
{"id": "844619", "url": "https://en.wikipedia.org/wiki?curid=844619", "title": "Crewe (crater)", "text": "Crewe (crater)\n\nCrewe is a crater approximately 3 km in diameter lying situated within the Margaritifer Sinus quadrangle (MC-19) region of the planet Mars, located at 25° South, 10° West.\n\nThe crater was named after the town of Crewe, Cheshire, England.\n\n"}
{"id": "33515811", "url": "https://en.wikipedia.org/wiki?curid=33515811", "title": "Endless Forms Most Beautiful (book)", "text": "Endless Forms Most Beautiful (book)\n\nEndless Forms Most Beautiful: The New Science of Evo Devo and the Making of the Animal Kingdom is a 2005 book by the molecular biologist Sean B. Carroll. It presents a summary of the emerging field of evolutionary developmental biology and the role of toolkit genes. It has won numerous awards for science communication.\n\nThe book's somewhat controversial argument is that evolution in animals (though no doubt similar processes occur in other organisms) proceeds mostly by modifying the way that regulatory genes, which do not code for structural proteins (such as enzymes), control embryonic development. In turn, these regulatory genes turn out to be based on a very old set of highly conserved genes which Carroll nicknames the toolkit. Almost identical sequences can be found across the animal kingdom, meaning that toolkit genes such as \"Hox\" must have evolved before the Cambrian radiation which created most of the animal body plans that exist today. These genes are used and reused, occasionally by duplication but far more often by being applied unchanged to new functions. Thus the same signal may be given at a different time in development, in a different part of the embryo, creating a different effect on the adult body. In Carroll's view, this explains how so many body forms are created with so few structural genes.\n\nThe book has been praised by critics, and called the most important popular science book since Richard Dawkins's \"The Blind Watchmaker\".\n\nSean B. Carroll is a professor of molecular biology and genetics at the University of Wisconsin–Madison. He studies the evolution of cis-regulatory elements (pieces of non-coding DNA) which help to regulate gene expression in developing embryos, using the fruit fly \"Drosophila\" as the model organism. He has won the Shaw Scientist Award and the Stephen Jay Gould Prize for his work.\n\nThe title quotes from the last sentence of Charles Darwin's 1859 \"The Origin of Species\", in which he described the evolution of all living organisms from a common ancestor: \"endless forms most beautiful and most wonderful have been, and are being, evolved.\" Darwin, however, was unable to explain how those body forms actually came into being. The early 20th century modern synthesis of evolution and genetics, too, largely ignored embryonic development to explain the form of organisms, since population genetics appeared to be an adequate explanation of how forms evolved. That task was finally undertaken at the end of the 20th century with the arrival of recombinant DNA technology, when biologists were able to start to explore how development was actually controlled.\n\n\n\nThe book is illustrated with photographs, such as of developing fruit fly embryos dyed to show the effects of toolkit genes, and with line drawings by Jamie W. Carroll, Josh P. Klaiss and Leanne M. Olds.\n\n\nThe evolutionary biologist Lewis Wolpert, writing in \"American Scientist\", called \"Endless Forms Most Beautiful\" \"a beautiful and very important book.\" He summarized the message of the book with the words \"As Darwin's theory made clear, these multitudinous forms developed as a result of small changes in offspring and natural selection of those that were better adapted to their environment. Such variation is brought about by alterations in genes that control how cells in the developing embryo behave. Thus one cannot understand evolution without understanding its fundamental relation to development of the embryo.\" Wolpert noted that Carroll intended to explain evo-devo, and \"has brilliantly achieved what he set out to do.\"\n\nThe evolutionary biologist Jerry Coyne, writing in \"Nature\", described the book as for the interested lay reader, and called it \"a paean to recent advances in developmental genetics, and what they may tell us about the evolutionary process.\" For him, the centrepiece was \"the unexpected discovery that the genes that control the body plans of all bilateral animals, including worms, insects, frogs and humans, are largely identical. These are the 'homeobox' (Hox) genes\". He called Carroll a leader in the field and an \"adept communicator\", but admits to \"feeling uncomfortable\" when Carroll sets out his personal vision of the field \"without admitting that large parts of that vision remain controversial.\" Coyne pointed out that the idea that the \"'regulatory gene' is the locus of evolution\" dates back to Roy Britten and colleagues around 1970, but was still weakly supported by observation or experiment. He granted that chimps and humans are almost 99% identical at DNA level, but points out that \"humans and chimps have different amino-acid sequences in at least 55% of their proteins, a figure that rises to 95% for humans and mice. Thus we can't exclude protein-sequence evolution as an important reason why we lack whiskers and tails.\" He also noted that nearly half of human protein-coding genes do not have homologues in fruit flies, so one could argue the opposite of Carroll's thesis and claim that \"evolution of form is very much a matter of teaching old genes to make new genes.\"\n\nThe review in \"BioScience\" noted that the book serves as a new \"Just So Stories\", explaining the \"spots, stripes, and bumps\" that had attracted Rudyard Kipling's attention in his children's stories. The review praised Carroll for tackling human evolution and covering the key concepts of what Charles Darwin called the grandeur of [the evolutionary view of] life, suggesting that \"Kipling would be riveted.\"\n\nThe science writer Peter Forbes, writing in \"The Guardian\", called it an \"essential book\" and its author \"both a distinguished scientist ... and one of our great science writers.\" The journalist Dick Pountain, writing in \"PC Pro\" magazine, argued that \"Endless Forms Most Beautiful\" was the most important popular science book since Richard Dawkins's \"The Blind Watchmaker\", \"and in effect a sequel [to it].\"\n\nThe paleobiologist Douglas H. Erwin, reviewing the book for \"Artificial Life\", noted that life forms from fruit flies to humans have far fewer genes than many biologists expected – human beings have only some 20,000. \"How could humans, in all our diversity of cell types and complexity of neurons, require essentially the same number of genes as a fly, or worse, a worm (the nematode \"Caenorhabditis elegans\")?\" asks Erwin. He answered his own question about the \"astonishing morphological diversity\" of animals coming from \"such a limited number of genes\", praising Carroll's \"insightful and enthusiastic\" style, writing in a \"witty and engaging\" way, pulling the reader into the complexities of \"Hox\" and \"PAX-6\", as well as celebrating the Cambrian explosion of life forms and much else.\n\n\n"}
{"id": "31170218", "url": "https://en.wikipedia.org/wiki?curid=31170218", "title": "Energy Catalyzer", "text": "Energy Catalyzer\n\nThe Energy Catalyzer (also called E-Cat) is a claimed cold fusion reactor devised by inventor Andrea Rossi with support from the late physicist Sergio Focardi. An Italian patent, which received a formal but not a technical examination, describes the apparatus as a \"process and equipment to obtain exothermal reactions, in particular from nickel and hydrogen\". Rossi and Focardi said the device worked by infusing heated hydrogen into nickel powder, transmuting it into copper and producing excess heat. An international patent application received an unfavorable international preliminary report on patentability in 2011 because it was adjudged to \"offend against the generally accepted laws of physics and established theories\". \n\nThe device has been the subject of demonstrations and tests several times, and commented on by various academics and others, but no independent tests have been made, and no peer-reviewed tests have been published. Steve Featherstone wrote in \"Popular Science\" that by the summer of 2012 Rossi's \"outlandish claims\" for the E-Cat seemed \"thoroughly debunked\".\n\nInvited guests attended several demonstrations in Bologna in 2011. The device has not been independently verified. Of a January demonstration, Discovery Channel analyst Benjamin Radford wrote that \"If this all sounds fishy to you, it should,\" and that \"In many ways cold fusion is similar to perpetual motion machines. The principles defy the laws of physics, but that doesn't stop people from periodically claiming to have invented or discovered one.\" According to Phys.org (11 August 2011), the demonstrations held from January to April 2011 had several flaws that compromised their credibility and Rossi had refused to perform tests that could verify his claims.\n\nUniversity of Bologna researchers have attended some E-Cat demonstrations, but only as observers. On 5 November 2011, the University of Bologna clarified that its researchers had not been involved in the demonstrations and that none of those took place at the university. Rossi had signed a contract with the university, but the contract was terminated and no research was done because Rossi did not make the first payment.\n\nSkeptic Ian Bryce speculated that the E-Cat was misconnected during demonstrations, and that the power attributed to fusion is supplied to the device through the earth wire. Dick Smith offered Rossi one million dollars to demonstrate that the E-Cat system worked as claimed, while the power through the earth wire was also being measured, which Rossi refused. Peter Thieberger, a senior physicist at Brookhaven National Laboratory, said it would be very difficult for this misconnection to happen by accident and that the issue could only be cleared with a fully independent test.\n\nOn 28 October 2011 the unit was \"customer tested\" and was said to release 2,635 kWh during five and a half hours of self-sustained mode, an average power of 479 kilowatts – just under half the promised power of one megawatt. Independent observers were not allowed to watch the measurements or make their own, and the plant remained connected to a power supply during the test allegedly to supply power to the fans and the water pumps.\n\nBecause of his research into cold fusion for over 15 years, Sergio Focardi was contacted by Andrea Rossi in 2007 in order to validate the apparatus at its early stage of development. After four years of work and measurements together with Rossi, Focardi concluded that nuclear fusion reactions happen inside the Energy Catalyzer. Focardi states that the nuclear process is facilitated by a secret additive, known only by Rossi and not by him. According to Focardi, the process would be much less intense without this additive. Rossi and Focardi are then reported to have been unable to find a peer-reviewed scientific journal that would publish their paper describing how they claim the Energy Catalyzer operates. Their paper appears only in Rossi's self-published blog, \"Journal of Nuclear Physics\".\n\nIn May 2013 a non-peer-reviewed paper describing \"results obtained from evaluations of the operation of the E-Cat HT in two test runs\" was submitted to the arXiv digital archive. Although the authors of the paper wrote that they were not in control of all of the aspects of the process, they concluded that, even by the most conservative of measurements, the device produced excess heat with a resulting energy density that was at least one order of magnitude, and possibly several, higher than any other conventional energy source. The test was partly funded by the Swedish energy research consortium, Elforsk. Elforsk stated on their website that the results were very remarkable, but that it was highly questionable to speculate whether nuclear transformation had occurred when no access had been provided to the reactants. In a response to the original manuscript archived on arXiv, commentators criticized the testing as not truly independent, described the report as having \"characteristics more typically found in pseudo‐scientific texts\", and stated that \"The authors seem to jump to conclusions fitting pre‐conceived ideas where alternative explanations are possible.\" Astrophysicist Ethan Siegel commented at ScienceBlogs saying Rossi did not allow the reactants or products to be measured on this occasion. In the previous tests there were not enough and (the only two nickel isotopes which can fuse with hydrogen), at 3.6% and 0.9% respectively, in the reactants to explain the 10% copper output; these isotope levels are typical of natural copper, rather than of fusion by-product. According to Siegel, Rossi also refused to unplug the machine while it was operating despite it being an easy way to surreptitiously power the device. He also added that the supposedly independent testers had to rely on data supplied by Rossi.\n\nIn October 2014 a non-peer-reviewed paper by the same authors as the May 2013 report describes results from evaluations in March 2014 of an upgraded version of the E-Cat which runs at higher temperatures. Unlike previous demonstrations, the test was carried out with monitoring equipment and in a laboratory not supplied by Rossi, and was run over an extended duration (32 days). However, as with the previous report, the authors were not in full control of the process; Rossi intervened during the insertion of the fuel charge, start up of the reactor, shut down of the reactor, and extraction of the spent fuel. Overall, the total excess heat measured was calculated to be well beyond that possible by any conventional, non-nuclear source. In this report, they present analyses of samples of spent fuel, concluding from the isotopes found that \"nuclear reactions are therefore indicated to be present in the run process, which however is hard to reconcile with the fact that no radioactivity was detected outside the reactor during the run.\" Following fuel and ash isotopic analysis, the authors speculate that isotopes of especially nickel and lithium being part of the reaction, in particular transmutation of and to , and from to through some unknown process.\n\nParticle physicist Tommaso Dorigo commented on the 2014 test, called the isotopic measurements \"startling\" but he expressed deep concern about Rossi being involved in collecting the spent fuel, that the testers may have \"overlooked some simple trick\" and that \"given the extraordinary nature of the claim… this constitutes a major flaw, which totally invalidates any conclusions one might otherwise draw.\"\nAstrophysicist Ethan Siegel was highly critical of the test, stating that the testers were not independent, that Rossi could have tampered with the fuel samples, that the 'open calorimeter' set up used was inappropriate, and that \"it’s relatively easy to fake the amount of energy being drawn through a power cord if there is a hookup to an external source.\"\n\nTheoretical astrophysicist Ethan Siegel and nuclear physicist Peter Thieberger argue that the claims for the E-Cat are incompatible with the fundamentals of nuclear physics. In particular, the Coulomb barrier for the claimed fusion reaction is so high that it is insurmountable anywhere in the known universe, including the interior of stars. The reaction also would create gamma radiation that would have penetrated the few inches of shielding apparently provided by the E-Cat, inducing acute radiation syndrome in persons in the vicinity of the purported demonstrations. Given numerous other scientific inconsistencies – such as the ratio of isotopes in the supposed copper \"fusion product\" being identical to that in natural copper – the authors argued that it is now time \"for the E-Cat's proponents to provide the provable, testable, reproducible science that can answer these straightforward physics objections.\"\n\nPeter Ekström, lecturer at the Department of Nuclear Physics at Lund University in Sweden, concluded in May 2011, \"I am convinced that the whole story is one big scam, and that it will be revealed in less than one year.\" He cited the unlikelihood of a chemical reaction being strong enough to overcome the Coulomb barrier, the lack of gamma rays, the lack of explanation for the origin of the extra energy, the lack of the expected radioactivity after fusing a proton with Ni, the unexplained occurrence of 11% iron in the spent fuel, the 10% copper in the spent fuel strangely having the same isotopic ratios as natural copper, and the lack of any unstable copper isotope in the spent fuel as if the reactor only produced stable isotopes. Kjell Aleklett, physics professor at Uppsala University, said the percentage of copper was too high for any known reaction of nickel, and the copper had the same isotopic ratio as natural copper. He also stated, \"Known chemical reactions cannot explain the amount of energy measured. A nuclear reaction can explain the amount of energy, but the knowledge we have today says that this reaction cannot take place.\" Scientific skeptic James Randi, discussing the E-Cat in the context of previous cold fusion claims, suggested that it will eventually be proven to be a fraud.\n\nOther reactions to the device have been mixed. In 2011 Dennis M. Bushnell, Chief Scientist at NASA Langley Research Center, described LENR as a \"promising\" technology and praised the work of Rossi and Focardi.\n\nTheoretical nuclear physicist Yeong E. Kim of Purdue University has proposed a potential theoretical explanation of the reported results of the device, but has stated that, for confirmation of this theory, \"it is very important to carry out Rossi-type experiments independently.\" Kim had previously put forward this theory to explain the results of the now-discredited Fleischman and Pons cold fusion experiment in 1989.\n\nSteve Featherstone wrote in \"Popular Science\" that by the summer of 2012 Rossi's \"outlandish claims\" for the E-Cat seemed \"thoroughly debunked\" and that Rossi \"looked like a con man clinging to his story to the bitter end.\"\n\nAn application in 2008 to patent the device internationally received an unfavorable preliminary report on patentability at the World Intellectual Property Organization from the European Patent Office, noting that the description of the device was based on \"general statements and speculations\" and citing \"numerous deficiencies in both the description and in the evidence provided to support its feasibility\" as well as incompatibilities with \"generally accepted laws of physics and established theories.\" The patent application was published on 15 October 2009.\n\nOn 6 April 2011 an application was approved by the Italian Patent and Trademark Office, which issued a patent for the invention, valid only in Italy. Under then-current Italian law, the examination of the application was more formal and less technical than for the corresponding PCT application.\n\nIn March 2014 the US Patent Office replied to Rossi's US patent application with a provisional decision to reject it, saying \"The specification is objected to as inoperable. Specifically there is no evidence in the corpus of nuclear science to substantiate the claim that nickel will spontaneously ionize hydrogen gas and therefore 'absorb' the resulting proton\".\n\nIn January 2014 a newly formed company, Industrial Heat LLC, announced that it had acquired rights to Rossi's E-Cat technology. In April 2016, Rossi filed a lawsuit in the USA against Industrial Heat, alleging that he was not paid an $89 million licensing fee due after a one-year test period of an E-Cat unit. Industrial Heat's comment on the lawsuit was that after three years of effort they were unable to reproduce Rossi's E-Cat test results.\nOn July 5, 2017 the parties settled; the terms of the settlement were not released.\n\n\n"}
{"id": "50507458", "url": "https://en.wikipedia.org/wiki?curid=50507458", "title": "Evaporative light scattering detector (ELSD)", "text": "Evaporative light scattering detector (ELSD)\n\nAn evaporative light scattering detector (ELSD) is a detector used in conjunction with high-performance liquid chromatography (HPLC), Ultra high-performance liquid chromatography (UHPLC), Purification liquid chromatography such as flash or preparative chromatography, countercurrent or centrifugal partition chromatographies and Supercritical Fluid chromatography (SFC). It is commonly used for analysis of compounds where UV detection might be a restriction and therefore used where compounds do not efficiently absorb UV radiation, such as sugars, antivirals, antibiotics, fatty acids, lipids, oils, phospholipids, polymers, surfactants, terpenoids and triglycerides. ELSDs is related to the charged aerosol detector (CAD) and like the CAD, falls under the category of destructive detectors>\n\nAn evaporative light scattering detector (ELSD) is able to detect all compound which are less volatile than the mobile phase, i.e. non volatile and semi-volatile compounds. \n\nELSDs analyze solvent after elution from HPLC. As the eluent passes from an HPLC, it is mixed with an inert carrier gas and forced through a nebulizer, which separates the liquid into minute aerosolized droplets. These droplets then pass into a heated drift tube, where the mobile phase solvent is evaporated off. As the mobile phase evaporates, the droplets become smaller and smaller until all that is left is minute particles of dried analyte. These particles are pushed through the drift tube by the carrier gas to the detection region. In this region, a beam of light crosses the column of analyte and the scattering of light is measured by a photodiode or photomultiplier tube. The detector's output is non-linear across more than one order of magnitude and proper calibration is required for quantitative analysis.>BDS\n"}
{"id": "3025604", "url": "https://en.wikipedia.org/wiki?curid=3025604", "title": "Exoelectron emission", "text": "Exoelectron emission\n\nIn atomic physics, exoelectron emission (EE) is a weak electron emission, appearing only from pretreated (irradiated, deformed etc.) objects. The pretreatment (\"excitation\") turns the objects into an unequilibrial state. EE accompanies the relaxation of these unequilibria. The relaxation can be stimulated e.g. by slight heating or longwave illumination, not causing emission from untreated samples. Accordingly, thermo- and photostimulated EE (TSEE, PSEE) are distinguished. Thus, EE is an electron emission analogue of such optical phenomena as phosphorescence, thermo- and photostimulated luminescence.\n"}
{"id": "47767659", "url": "https://en.wikipedia.org/wiki?curid=47767659", "title": "Expressive therapies continuum", "text": "Expressive therapies continuum\n\nThe expressive therapies continuum (ETC) is a model of creative functioning used in the field of art therapy that is applicable to creative processes both within and outside of an expressive therapeutic setting. The concept was initially proposed and published in 1978 by art therapists Sandra Kagin and Vija Lusebrink, who based the continuum on existing models of human development and information processing.\n\nThis schematic model serves to describe and assess an individual's level of creative functioning based on aspects such as the artist's purpose for creating a piece, choice of medium, interaction with the chosen medium, and imagery within the piece. Conversely, it also serves to meet the needs of the client by assisting the art therapist in choosing a developmentally or situationally appropriate activity or art medium. By analyzing an individual's art making process and the resulting artwork using the ETC, art therapists can assess strengths, weaknesses, and disconnect in various levels of a client's cognitive functioning - suggesting or substantiating diagnosis of, or recovery from, a mental health condition.\n\nAccording to Lusebrink:\n\nThe first three levels of the ETC reflect three established systems of human information processing: the Kinesthetic/Sensory (K/S level); the Perceptual/Affective (P/A level); and the Cognitive/Symbolic (C/S level) ... The fourth level of the ETC is the Creative level (CR). It is seen as a synthesis of the other three levels of the continuum.A diagram of the ETC, as pictured in the top right of the page, can be read from left to right and from the bottom, upwards. The model flows in a direction that travels from simple information processing and image formation to increasingly complex thought processes and interactions with the media. Individuals can fluctuate from level to level depending on personal and situational factors. They may also display an integration of all of the first three levels of functioning. This integration indicates that the individual is operating on the fourth and final level of functioning, known as the Creative level. The Creative level both transcends and intersects the prior three levels, in which the individual is either equally incorporating all aspects of the ETC \"or\" is able to find a satisfying and meaningful creative experience on one of the three levels alone.\n\nHowever, an individual cannot wholly operate at both ends of a level, as each level is bipolar. For example, if the individual is more focused on the quick and scribbly \"movement\" of a chalk-pastel on paper, then he or she is less focused on the \"sensory\" aspects of the media, such as the sound of the chalk against the paper or the powdery feel of chalk in one's hand.\n\nAs the first level of the ETC, the Kinesthetic/Sensory level is described as a form of preverbal information processing that is \"rhythmic, tactile, and sensual\". This simple type of interaction with various art media stimulates primal areas of the brain and meets basic expressive needs—all while providing sensory and kinesthetic feedback for the artist. If an individual is operating at the kinesthetic end of the spectrum, he or she may find satisfaction in movement—i.e. pounding at a piece of clay or scribbling frantically with a crayon. In contrast, if the individual is gravitating towards the sensory end of the spectrum, he or she might take more pleasure in the \"feel\" of finger-paints or the \"smell\" of scented markers.\n\nThis level is particularly useful for young children but may also be useful for anyone needing to focus on sensorimotor skills. In addition, functioning at this level may allow for better access to preverbal memories or expression of extreme emotions. Individuals may identify operation at the K/S as a personal coping mechanism, in which the experience rather than the product is viewed as therapeutic.\n\nThe second level of the ETC, the Perceptual/Affective level may or may not include verbal thought processes. However, the focus has shifted from the experience alone (with little focus on the outcome, as in the K/S level) to using the media to create an intentionally expressive or self-satisfying final product. The process may be characterized either by an individual's intent to express his or her own literal reality \"or\" be characterized by content that is \"emotional and raw...without regard to form\".\n\nBy working with individuals at the P/A level, art therapists can help clients to perceive images or notions in a new way, strengthening communication and assisting with the formation of meaningful relationships. They can also focus on the identification and healthy expression of one's emotions.\n\nOperation at the third level of the ETC, the cognitive/symbolic level, requires \"complex and sophisticated\" information processing, in which the individual consciously and strategically plans—prior to creating the art piece—for an expressive and self-satisfying final product. At this level, individuals are able to step outside their own sphere of perception and emotional expression and focus on ways that they interact with the world around them. They may begin to use satire and hidden meanings in their pieces to best express their unique response to their surroundings or situation (symbolic) or use art to plan and to problem-solve (cognitive).\n\nThe final level, which either intersects the previous three levels or transcends above them, is the Creative level. This level symbolizes a wholeness, in which the individual achieves a sense of joy, fulfillment, or wellbeing by taking part in the creative process and expressing the self. This may be accomplished through the integration of the three previous levels (where there was inclusion of all expressive operations in the art-making process; a feeling of oneness) or success in fulfilling an individual's need at any given level, which may be healing in and of itself.\n"}
{"id": "43738642", "url": "https://en.wikipedia.org/wiki?curid=43738642", "title": "Fellow of the Academy of Social Sciences", "text": "Fellow of the Academy of Social Sciences\n\nThe Fellowship of the Academy of Social Sciences (FAcSS) is an award granted by the Academy of Social Sciences to leading academics, policy-makers, and practitioners of the social sciences.\n\nFellows were previously known as Academicians and used the post-nominal letter \"AcSS\". This was changed in July 2014 to bring the Academy in line with other British learned societies.\n\nThe first Fellows (then known as Academicians) were elected in 1999. The inaugural fellows include:\n\n\n\n\n\n\n\n\n\nThere were 34 people elected to the fellowship in 2014, including:\n\n\nThere were 33 people elected to the fellowship in 2015, including:\nThere were 84 people elected to the fellowship in 2016, including:\nThere were 69 people elected to the fellowship in 2016, including:\n"}
{"id": "1458219", "url": "https://en.wikipedia.org/wiki?curid=1458219", "title": "Folk taxonomy", "text": "Folk taxonomy\n\nA folk taxonomy is a vernacular naming system, and can be contrasted with scientific taxonomy. Folk biological classification is the way people traditionally describe and organize their natural surroundings/the world around them, typically making generous use of form taxa like \"shrubs\", \"bugs\", \"ducks\", \"ungulates\" and the likes. Astrology involves a folk taxonomy, while astronomy uses a scientific classification system, although both involve observations of the stars and celestial bodies and both terms seem equally scientific, with the former meaning \"the teachings about the stars\" and the latter \"the rules about the stars\".\nFolk taxonomies are generated from social knowledge and are used in everyday speech. They are distinguished from scientific taxonomies that claim to be disembedded from social relations and thus objective and universal.\n\nAnthropologists have observed that taxonomies are generally embedded in local cultural and social systems, and serve various social functions. One of the most well-known and influential studies of folk taxonomies is Émile Durkheim's \"The Elementary Forms of Religious Life\".\n\nFolk taxonomies exist to allow popular identification of classes of objects, and apply to all areas of human activity. All parts of the world have their own systems of naming local plants and animals. These naming systems are a vital aid to survival and include information such as the fruiting patterns of trees and the habits of large mammals. These localised naming systems are folk taxonomies. Theophrastus recorded evidence of a Greek folk taxonomy for plants, but later formalized botanical taxonomies were laid out in the 18th century by Carl Linnaeus.\n\nCritics of the concept of \"race\" in humans argue that race is a folk taxonomy rather than a scientific classification.\n\nScientists generally recognize that folk taxonomies conflict at times with Linnaean taxonomy or current interpretations of evolutionary relationships, and can tend to refer to generalized rather than quantitatively informative traits in an organism.\n\n\n"}
{"id": "43780204", "url": "https://en.wikipedia.org/wiki?curid=43780204", "title": "Gene set enrichment analysis", "text": "Gene set enrichment analysis\n\nGene set enrichment analysis (GSEA) (also functional enrichment analysis) is a method to identify classes of genes or proteins that are over-represented in a large set of genes or proteins, and may have an association with disease phenotypes. The method uses statistical approaches to identify significantly enriched or depleted groups of genes. Transcriptomics technologies and proteomics results often identify thousands of genes which are used for the analysis.\n\nResearchers performing high-throughput experiments that yield sets of genes (for example, genes that are differentially expressed under different conditions) often want to retrieve a functional profile of that gene set, in order to better understand the underlying biological processes. This can be done by comparing the input gene set to each of the bins (terms) in the gene ontology – a statistical test can be performed for each bin to see if it is enriched for the input genes.\n\nWhile the completion of the Human Genome Project gifted researchers with an enormous amount of new information, it also left them with the problem of how to interpret and analyze the incredible amount of data. In order to seek out genes associated with diseases, researchers utilized DNA microarrays, which measure the amount of gene expression in different cells. Researchers would perform these microarrays on thousands of different genes, and compare the results of two different cell categories, e.g. normal cells versus cancerous cells. However, this method of comparison is not sensitive enough to detect the subtle differences between the expression of individual genes, because diseases typically involve entire groups of genes. Multiple genes are linked to a single biological pathway, and so it is the additive change in expression within gene sets that leads to the difference in phenotypic expression. Gene Set Enrichment Analysis was developed to focus on the changes of expression in groups of a priori defined gene sets. By doing so, this method resolves the problem of the undetectable, small changes in the expression of single genes.\n\nGene set enrichment analysis uses \"a priori\" gene sets that have been grouped together by their involvement in the same biological pathway, or by proximal location on a chromosome. A database of these predefined sets can be found at the \"Molecular signatures database\" (MSigDB). In GSEA, DNA microarrays, or now RNA-Seq, are still performed and compared between two cell categories, but instead of focusing on individual genes in a long list, the focus is put on a gene set. Researchers analyze whether the majority of genes in the set fall in the extremes of this list: the top and bottom of the list correspond to the largest differences in expression between the two cell types. If the gene set falls at either the top (over-expressed) or bottom (under-expressed), it is thought to be related to the phenotypic differences.\n\nIn the method that is typically referred to as standard GSEA, there are three steps involved in the analytical process. The general steps are summarized below:\n\n\nWhen GSEA was first proposed in 2003 some immediate concerns were raised regarding its methodology. These criticisms led to the use of the correlation-weighted Kolmogorov–Smirnov test, the normalized ES, and the false discovery rate calculation, all of which are the factors that currently define standard GSEA. However, GSEA has now also been criticized for the fact that its null distribution is superfluous, and too difficult to be worth calculating, as well as the fact that its Kolmogorov–Smirnov-like statistic is not as sensitive as the original. As an alternative, the method known as Singular Enrichment Analysis (SEA), was proposed. This method assumes gene independence and uses a simpler approach to calculate t-test. However, it is thought that these assumptions are in fact too simplifying, and gene correlation cannot be disregarded.\n\nOne other limitation to Gene Set Enrichment Analysis is that the results are very dependent on the algorithm that clusters the genes, and the number of clusters being tested. Spectral Gene Set Enrichment (SGSE) is a proposed, unsupervised test. The method’s founders claim that it is a better way to find associations between MSigDB gene sets and microarray data. The general steps include:\n\n1. Calculating the association between principal components and gene sets.\n\n2. Using the weighted Z-method to calculate association between the gene sets and the spectral structure of the data.\n\nGSEA uses complicated statistics, so it requires a computer program to run the calculations. However, because GSEA has become standard practice in the last decade, there are many websites and downloadable programs that will provide the data sets and run the analysis.\n\nThe gene ontology (GO) annotation for 165 plant species and GO enrichment analysis is available.\n\nThe Molecular Signatures Database hosts an extensive collection of annotated gene sets that can be used with most GSEA Software.\n\nThe Broad Institute website is in cooperation with MSigDB and has a downloadable GSEA software, as well a general tutorial for those new to performing this analytical technique.\n\nEnrichr is a gene set enrichment analysis tool for mammalian gene sets. It contains background libraries for transcription regulation, pathways and protein interactions, ontologies including GO and the human and mouse phenotype ontologies, signatures from cells treated with drugs, and expression of genes in different cells and tissues. Enrichr was developed by the Ma'ayan laboratory at Mount Sinai. The background libraries are from over 70 resources and contain over 200,000 annotated gene sets. The tool can be accessed through API and provides different ways to visualize the results.\n\nGeneSCF is a real-time based functional enrichment tool with support for multiple organisms and is designed to overcome the problems associated with using outdated resources and databases. Advantages of using GeneSCF: real-time analysis, users do not have to depend on enrichment tools to get updated, easy for computational biologists to integrate GeneSCF with their NGS pipeline, it supports multiple organisms, enrichment analysis for multiple gene list using multiple source database in single run, retrieve or download complete GO terms/Pathways/Functions with associated genes as simple table format in a plain text file.\n\nThe Database for Annotation, Visualization and Integrated Discovery (DAVID) is a bioinformatics tool that pools together information from most major bioinformatics sources, with the aim of analyzing large gene lists in a high-throughput manner. DAVID goes beyond standard GSEA with additional functions like switching between gene and protein identifiers on the genome-wide scale, however, it is important to note that the annotations used by DAVID have not been updated since January 2010 which can have a considerable impact on practical interpretation of results. In October 2016, DAVID Knowledgebase version 6.8 was released with a complete rebuild.\n\nThe Gene ontology consortium has also developed their own online GO term enrichment tool,\nallowing species-specific enrichment analysis versus the complete database, coarser-grained GO slims, or custom references.\n\nIn 2010, Gill Bejerano from Stanford University released the \"Genomic region enrichment of annotations tool\" (GREAT), a software which takes advantage of \"regulatory domains\" to better associate gene ontology terms to genes.\n\nThe Functional Enrichment Analysis (FunRich) tool is mainly used for the functional enrichment and network analysis of OMICS data.\n\nInstances of InterMine automatically provide enrichment analysis for uploaded sets of genes and other biological entities.\n\nToppGene is a one-stop portal for gene list enrichment analysis and candidate gene prioritization\nbased on functional annotations and protein interactions network. Developed and maintained by the Division of Biomedical Informatics at Cincinnati Children's Hospital Medical Center.\n\nQuantitative Set Analysis for Gene Expression (QuSAGE) is a computational method for gene set enrichment analysis . QuSAGE improves power by accounting for inter-gene correlations and quantifies gene set activity with a complete probability density function (PDF). From this PDF, P values and confidence intervals can be easily extracted. Preserving the PDF also allows for post-hoc analysis (e.g., pair-wise comparisons of gene set activity) while maintaining statistical traceability. Turner et al. extended the applicability of QuSAGE to longitudinal studies by adding functionality for general linear mixed models. QuSAGE was used by the NIH/NIAID Human Immunology Project Consortium to identify baseline transcriptional signatures that were associated with human influenza vaccination responses. QuSAGE is available as an R/Bioconductor package, and is maintained by the Kleinstein Lab at Yale School of Medicine.\n\nBlast2GO is a bioinformatics platform for functional annotation and analysis of genomic datasets. This tool allows to perform gene set enrichment analysis (GSEA), among other functions.\n\nSingle-nucleotide polymorphisms, or SNPs, are single base mutations that may be associated with diseases. One base change has the potential to affect the protein that results from that gene being expressed; however, it also has the potential to have no effect at all. Genome-wide association studies are comparisons between healthy and disease genotypes to try to find SNPs that are overrepresented in the disease genomes, and might be associationed with that condition. Before GSEA, the accuracy of genome-wide SNP association studies was severely limited by a high number of false positives. The theory that the SNPs contributing to a disease tend to be grouped in a set of genes that are all involved in the same biological pathway, is what the GSEA-SNP method is based on. This application of GSEA does not only aid in the discovery of disease-associated SNPs, but helps illuminate the corresponding pathways and mechanisms of the diseases.\n\nGene Set Enrichment methods led to the discovery of new suspect genes and biological pathways related to the Spontaneous Preterm Birth. Exome sequences from women who had experienced SPTB were compared to those from females from the 1000 Genome Project, using a tool that scored possible disease-causing variants. Genes with higher scores were then run through different programs to group them into gene sets based on pathways and ontology groups. This study found that the variants were significantly clustered in sets related to several pathways, all suspects in SPTB.\n\nGene Set Enrichment Analysis can be used to understand the changes that cells undergo during carcinogenesis and metastasis. In a study, microarrays were performed on renal cell carcinoma metastases, primary renal tumors, and normal kidney tissue, and the data was analyzed using GSEA. This analysis showed significant changes of expression in genes involved in pathways that have not been previously associated with the progression of renal cancer. From this study, GSEA has provided potential new targets for renal cell carcinoma therapy.\n\nGSEA can be used to help understand the molecular mechanisms of complex disorders. Schizophrenia is a largely heritable disorder, but is also very complex, and the onset of the disease involves many genes interacting within multiple pathways, as well the interaction of those genes with environmental factors. For instance, epigenetic changes, like DNA methylation, are affected by the environment, but are also inherently dependent on the DNA itself. DNA methylation is the most well-studied epigenetic change, and was recently analyzed using GSEA in relation to schizophrenia-related intermediate phenotypes. Researchers ranked genes for their correlation between methylation patterns and each of the phenotypes. They then used GSEA to look for an enrichment of genes that are predicted to be targeted by microRNAs in the progression of the disease.\n\nGSEA can help provide molecular evidence for the association of biological pathways with diseases. Previous studies shown that long-term depression symptoms are correlated with changes in immune response and inflammatory pathways. A study by Elovainio et al. was aimed at finding genetic and molecular evidence that supports this association. The researchers took blood samples from the Young Finns Study (participants were depression patients), and used genome-wide expression data, along with GSEA to find expression differences in gene sets related to inflammatory pathways. The study found that patients who rated with the most severe depression symptoms also had significant expression differences in those gene sets, and this result supports the association hypothesis.\n"}
{"id": "24063825", "url": "https://en.wikipedia.org/wiki?curid=24063825", "title": "Houben-Weyl Methods of Organic Chemistry", "text": "Houben-Weyl Methods of Organic Chemistry\n\nHouben-Weyl Methods of Organic Chemistry (Ger. \"Methoden der Organischen Chemie\") established in 1909 by the German chemist Theodor Weyl, is a classic chemistry text. It consisted initially of two volumes and covered literature published as early as 1834. Heinrich J. Houben revised and reissued it in 1913. It is considered one of the most significant resources for chemists.\n\nUp to the 4th edition the work was published in German by Thieme from 1952 to 1987, with supplementary volumes published between 1982 and 1999, some of them (from 1990 on) in English. It consists of 16 volumes, some of which are further divided. Overall, the 4th edition consists of 90 individual books. \n\nA new English-language edition was published by Thieme from 2000 to 2010 as \"Science of Synthesis\" in 48 volumes. It is constantly updated.\n"}
{"id": "39328195", "url": "https://en.wikipedia.org/wiki?curid=39328195", "title": "How It Began", "text": "How It Began\n\nHow It Began: A Time Traveler’s Guide to the Universe is a non-fiction book by the astronomer Chris Impey that discusses the history of the universe, with chapters ranging from the proximate universe to within an iota of the big bang. It was published as a hardcover by W. W. Norton & Company in 2012 and as a paperback in 2013.\n\nHow It Began: A Time Traveler’s Guide to the Universe is a non-fiction book by astronomy professor Chris Impey on the origins of everything from the Moon to the universe itself. The finite speed of light and the vastness of space turn modern large telescopes into time machines and astronomers into armchair time travelers. Looking out in space is looking back in time. Each chapter has vignettes that place the reader in increasingly unfamiliar physical situations that are increasingly unfamiliar. How It Began has an associated web site containing source material on each major topic.\n\nThe first third of the book deals with the proximate universe. The journey starts with the Moon and its formation from an impact with the infant Earth. Next stop is the outer Solar System and the amazing process by particles of dust grew into planets. The nearest star is the place where star formation is considered, followed by the Orion Nebula, where young stars are forming at a furious rate. The last stop in the nearby universe is the center of our galaxy, site of a black hole four million times the mass of the Sun.\n\nThe second third of the book examines the remote universe. Andromeda is seen as it was before humans evolved on the plains of Africa, and galaxies are all seen over time spans that dwarf our existence. The next stop is the massive Coma cluster of galaxies, a swarm of thousands of galaxies bound by invisible dark matter. This section continues with a visit to galaxies that created their stars long before the Earth formed and it finished with the time when stars first congealed out of gas in the expanding universe, 200 million years after the big bang.\n\nThe last third of the book ventures into the alien universe. The microwave background radiation is a relic of time when stable atoms first formed and the “fog lifted” in the infant universe. Reaching back to the time when the universe was as hot as the core of a star, the big bang is manifested in the creation of helium. The last chapters of the book cross into the realm of speculation, with descriptions the tiny asymmetry in the forces of nature that led to a tiny excess of matter over antimatter, the early exponential expansion that flattened space-time, and the idea of the universe as one quantum event among many.\n\nHow It Began received strong reviews. Writing in the Wall Street Journal, Manjit Kumar wrote “In clear, enthusiastic and occasionally lyrical prose, Mr. Impey takes the reader on a mind-blowing tour back through eons, stopping along the way to explain the formation of the solar system, the birth and death of stars, white dwarfs, supernovas, spiral galaxies, cosmic inflation, string theory, black holes and M-theory”. Maclean's magazine notes “With Impey flexing his creative writing muscles, How It Began could almost be a science fiction novel. But when he describes what we really do know about the universe—and questions we’re grappling with—it’s even more incredible than anything a science fiction writer could dream up”. Science fiction author Ben Bova said “Chris Impey has achieved the near-impossible: an accurate, up-to-date account of ‘the state of the universe’ that is told in gripping human terms. A great achievement and a ‘must-read’ book.” Kirkus Reviews concluded its review with “An astute tour of the cosmos by a skillful teacher.”\n\n"}
{"id": "25089370", "url": "https://en.wikipedia.org/wiki?curid=25089370", "title": "HyShot", "text": "HyShot\n\nHyShot is a research project of The University of Queensland, Australia Centre for Hypersonics, to demonstrate the possibility of supersonic combustion under flight conditions using two scramjet engines, one designed by The University of Queensland and one designed by QinetiQ (formerly the MOD's Defence Evaluation & Research Agency).\n\nThe project has involved the successful launch of one engine designed by The University of Queensland, and one launch of the scramjet designed by the British company QinetiQ. Each combustion unit was launched on the nose of a Terrier-Orion Mk70 sounding rocket on a high ballistic trajectory, reaching altitudes of approximately 330 km. The rocket was rotated to face the ground, and the combustion unit ignited for a period of 6–10 seconds while falling between 35 km and 23 km at around Mach 7.6. The system is not designed to produce thrust.\n\nThe carrier rocket for the HyShot experiments was composed of a RIM-2 Terrier first stage (6 second burn, 4000 km/h) and an Orion second stage (26 second burn, 8600 km/h, 56 km altitude). A fairing over the payload was then jettisoned. The package then coasted to an altitude of around 300 km. Cold gas nitrogen attitude control thrusters were used to re-orient the payload for atmospheric reentry. The experiments each lasted for some 5 seconds as the payload descended between approximately 35 and 23 kilometers altitude, when liquid hydrogen fuel was fed to the scramjet. Telemetry reported results to receivers on the ground for later analysis. The payload landed about 400 km down range from the launch site, at which time its temperature was still expected to be about 300 degrees Celsius, which may be enough to cause a small brush fire and thereby make spotting and recovery easier even though a radio beacon was in the payload.\n\nThere has been much analysis of the data obtained in flight and comparison with results from experiments conducted in ground-testing facilities.\n\nThe team continue to work as part of the Australian Hypersonics Initiative, a joint program of The University of Queensland, the Australian National University and the University of New South Wales' Australian Defence Force Academy campus, the governments of Queensland and South Australia and the Australian Defence Department.\n\nThe Hyshot program spawned the HyCAUSE (Hypersonic Collaborative Australian/United States Experiment) program : a collaborative effort between the United States’ Defense Advanced Research Projects Agency (DARPA) and Australia's Defence Science and Technology Organisation (DSTO), also representing the research collaborators in the Australian Hypersonics Initiative (AHI).\n\nAll tests were conducted at the Woomera Test Range in South Australia.\n\nThe Hypersonic International Flight Research Experimentation (HIFiRE) program was created jointly by DSTO and the Air Force Research Laboratory (AFRL). HIFiRE was formed to investigate hypersonic flight technology, the fundamental science and technology required, and its potential for next generation aeronautical systems. This will involve up to ten flights with The University of Queensland involved in at least the first three:\n\n\n"}
{"id": "41265739", "url": "https://en.wikipedia.org/wiki?curid=41265739", "title": "Hydrodynamica", "text": "Hydrodynamica\n\nHydrodynamica (Latin for \"Hydrodynamics\") is a book published by Daniel Bernoulli in 1738. The title of this book eventually christened the field of fluid mechanics as hydrodynamics.\n\nThe book deals with fluid mechanics and is organized around the idea of conservation of energy, as received from Christiaan Huygens's formulation of this principle. The book describes the theory of water flowing through a tube and of water flowing from a hole in a container. In doing so, Bernoulli explained the nature of hydrodynamic pressure and discovered the role of loss of \"vis viva\" in fluid flow, which would later be known as the Bernoulli principle. The book also discusses hydraulic machines and introduces the notion of work and efficiency of a machine. In the tenth chapter, Bernoulli discussed the first model of the kinetic theory of gases. Assuming that heat increases the velocity of the gas particles, he demonstrated that the pressure of air is proportional to kinetic energy of gas particles, thus making the temperature of gas proportional to this kinetic energy as well.\n\nHydrodynamica\n\n"}
{"id": "3793613", "url": "https://en.wikipedia.org/wiki?curid=3793613", "title": "ImageJ", "text": "ImageJ\n\nImageJ is a Java-based image processing program developed at the National Institutes of Health and the Laboratory for Optical and Computational Instrumentation (LOCI, University of Wisconsin). Its first version, ImageJ 1.x, is developed in the public domain, while ImageJ2 and the related projects SciJava, ImgLib2 and SCIFIO are licensed with a permissive BSD-2 license. ImageJ was designed with an open architecture that provides extensibility via Java plugins and recordable macros. Custom acquisition, analysis and processing plugins can be developed using ImageJ's built-in editor and a Java compiler. User-written plugins make it possible to solve many image processing and analysis problems, from three-dimensional live-cell imaging to radiological image processing, multiple imaging system data comparisons to automated hematology systems. ImageJ's plugin architecture and built-in development environment has made it a popular platform for teaching image processing.\n\nImageJ can be run as an online applet, a downloadable application, or on any computer with a Java 5 or later virtual machine. Downloadable distributions are available for Microsoft Windows, the classic Mac OS, macOS, Linux, and the Sharp Zaurus PDA. The source code for ImageJ is freely available.\n\nThe project developer, Wayne Rasband, retired from the Research Services Branch of the National Institute of Mental Health in 2010, but continues to develop the software.\n\nImageJ can display, edit, analyze, process, save, and print 8-bit color and grayscale, 16-bit integer, and 32-bit floating point images. It can read many image file formats, including TIFF, PNG, GIF, JPEG, BMP, DICOM, and FITS, as well as raw formats. ImageJ supports image \"stacks\", a series of images that share a single window, and it is multithreaded, so time-consuming operations can be performed in parallel on multi-CPU hardware. ImageJ can calculate area and pixel value statistics of user-defined selections and intensity-thresholded objects. It can measure distances and angles. It can create density histograms and line profile plots. It supports standard image processing functions such as logical and arithmetical operations between images, contrast manipulation, convolution, Fourier analysis, sharpening, smoothing, edge detection, and median filtering. It does geometric transformations such as scaling, rotation, and flips. The program supports any number of images simultaneously, limited only by available memory.\n\nBefore the release of ImageJ in 1997, a similar freeware image analysis program known as \"NIH Image\" had been developed in Object Pascal for Macintosh computers running pre-OS X operating systems. Further development of this code continues in the form of Image SXM, a variant tailored for physical research of scanning microscope images. A Windows version – ported by Scion Corporation (now defunct), so-called \"Scion Image for Windows\" – was also developed. Both versions are still available but – in contrast to NIH Image – closed-source.\n\n\n"}
{"id": "41256", "url": "https://en.wikipedia.org/wiki?curid=41256", "title": "Index-matching material", "text": "Index-matching material\n\nIn optics, an index-matching material is a substance, usually a liquid, cement (adhesive), or gel, which has an index of refraction that closely approximates that of another object (such as a lens, material, fiber-optic, etc.).\n\nWhen two substances with the same index are in contact, light passes from one to the other with neither reflection nor refraction. As such, they are used for various purposes in science, engineering, and art.\n\nFor example, in a popular home experiment, a glass rod is made almost invisible by immersing it in an index-matched transparent fluid such as mineral spirits.\n\nIn light microscopy, oil immersion is a technique used to increase the resolution of a microscope. This is achieved by immersing both the objective lens and the specimen in a transparent oil of high refractive index, thereby increasing the numerical aperture of the objective lens.\n\nImmersion oils are transparent oils that have specific optical and viscosity characteristics necessary for use in microscopy. Typical oils used have an index of refraction around 1.515. An oil immersion objective is an objective lens specially designed to be used in this way. The index of the oil is typically chosen to match the index of the microscope lens glass, and of the cover slip.\n\nFor more details, see the main article, oil immersion. Some microscopes also use other index-matching materials besides oil; see water immersion objective and solid immersion lens.\n\nIn fiber optics and telecommunications, an index-matching material may be used in conjunction with pairs of mated connectors or with mechanical splices to reduce signal reflected in the guided mode (known as return loss) (see: Optical fiber connector). Without the use of an index-matching material, Fresnel reflections will occur at the smooth end faces of a fiber unless there is no fiber-air interface or other significant mismatch in refractive index. These reflections may be as high as -14 dB (\"i.e.,\" 14 dB below the optical power of the incident signal). When the reflected signal returns to the transmitting end, it may be reflected again and return to the receiving end at a level that is (28 plus twice the fiber loss) dB below the direct signal. The reflected signal will also be delayed by twice the delay time introduced by the fiber. The twice-reflected, delayed signal superimposed on the direct signal may noticeably degrade an analog baseband intensity-modulated video signal. Conversely, for digital transmission, the reflected signal will often have no practical effect on the detected signal seen at the decision point of the digital optical receiver except in marginal cases where bit-error ratio is significant. However, certain digital transmitters such as those employing a Distributed Feedback Laser may be affected by back reflection and then fall outside specifications such as Side Mode Suppression Ratio, potentially degrading system bit error ratio, so networking standards intended for DFB lasers may specify a back-reflection tolerance such as -10 dB for transmitters so that they remain within specification even without index matching. This back-reflection tolerance might be achieved using an optical isolator or by way of reduced coupling efficiency.\n\nFor some applications, instead of standard polished connectors (e.g. FC/PC), angle polished connectors (e.g. FC/APC) may be used, whereby the non-perpendicular polish angle greatly reduces the ratio of reflected signal launched into the guided mode even in the case of a fiber-air interface.\n\nIndex matching is used in liquid-liquid and liquid-solid (Multiphase flow) experimental systems to minimise the distortions that occur in these systems , this is particularly important for systems with many interfaces which become optically inaccessible. Matching the refractive index minimises reflection, refraction, diffraction and rotations that occurs at the interfaces allowing access to regions that would otherwise be inaccessible to optical measurements. This is particularly important for advanced optical measurements like Laser-induced fluorescence, Particle image velocimetry and Particle tracking velocimetry to name a few.\n\nIf a sculpture is broken into several pieces, art conservators may reattach the pieces using an adhesive such as Paraloid B-72 or epoxy. If the sculpture is made of a transparent or semitransparent material (such as glass), the seam where the pieces are attached will usually be much less noticeable if the refractive index of the adhesive matches the refractive index of the surrounding object. Therefore, art conservators may measure the index of objects and then use an index-matched adhesive. Similarly, losses (missing sections) in transparent or semitransparent objects are often filled using an index-matched material.\n\nCertain optical components, such as a Wollaston prism or Nicol prism, are made of multiple transparent pieces that are directly attached to each other. The adhesive is usually index-matched to the pieces. Historically, Canada balsam was used in this application, but it is now more common to use epoxy or other synthetic adhesives.\n"}
{"id": "37620426", "url": "https://en.wikipedia.org/wiki?curid=37620426", "title": "Index of women scientists articles", "text": "Index of women scientists articles\n\n\n\n\n"}
{"id": "743608", "url": "https://en.wikipedia.org/wiki?curid=743608", "title": "John Frederick Miller", "text": "John Frederick Miller\n\nJohn Frederick Miller (1759–1796) was an English illustrator, mainly of botanical subjects.\n\nMiller was the son of the artist Johann Sebastian Müller (1715 – c. 1790). Miller, along with his brother James, produced paintings from the sketches made by Sydney Parkinson on James Cook's first voyage. He accompanied Joseph Banks on his expedition to Iceland in 1772.\n\nMiller published \"Cimelia Physica. Figures of rare and curious quadrupeds, birds, &c. together with several of the most elegant plants\" (1796) with text by George Shaw.\n"}
{"id": "47877849", "url": "https://en.wikipedia.org/wiki?curid=47877849", "title": "List of North Korean flags", "text": "List of North Korean flags\n\nThis is a list of flags used by North Korea.\n\n\n"}
{"id": "3648775", "url": "https://en.wikipedia.org/wiki?curid=3648775", "title": "List of biology journals", "text": "List of biology journals\n\nThis is a list of articles about scientific journals in biology and its various subfields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "298313", "url": "https://en.wikipedia.org/wiki?curid=298313", "title": "List of business theorists", "text": "List of business theorists\n\nThis is an annotated list of important business writers. It is in alphabetical order based on last name. For quick navigation, click on one of the letters:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "53611941", "url": "https://en.wikipedia.org/wiki?curid=53611941", "title": "List of flora and fauna of the Eastern Hills, Bogotá", "text": "List of flora and fauna of the Eastern Hills, Bogotá\n\nThe Eastern Hills of Bogotá is a threatened but rich area of biodiversity. Various species have been registered in the Eastern Hills of the Colombian capital.\n\nIn the Eastern Hills a total of 443 species of flora have been identified, of which 156 species in 111 genera and 64 families of vascular plants.\n\nA study published in 2013 lists as most important and characteristic species:\n\nColombia is the country with the most recorded bird species (1912 as of 2014) in the world. The biodiversity of bird species in the Eastern Hills is higher than in the parks of urban Bogotá. The northern part of the reserve is richer in bird species due to the dense forests and larger space between the urban zones. Birds of 30 families, 92 genera and 119 species have been identified in the Eastern Hills. A study in 2011 provided data on 67 species in an area of . The observation stations were between and in elevation.\n\nMammals of 14 families, 17 genera and 18 species have been identified in the Eastern Hills. Until the first half of the twentieth century, the Eastern Hills were populated by larger species as the puma, spectacled bear and white-tailed deer, but these species have been hunted to extinction.\n\nReptiles of four families, five genera and five species have been identified in the Eastern Hills. Of these species, only the lizards \"Anadia bogotensis\" and \"Proctoporus striatus\" have been found on the Guadalupe Hill. The striped lightbulb lizard is also present on the terrain of the Universidad de los Andes.\n\nAmphibians of four families, six genera and nine species have been identified in the Eastern Hills.\n\nThree species of fish have been identified in the waters of the Eastern Hills. Of \"Trichomycterus venulosus\" only two specimens have been found, and it is thought the species is extinct in the rivers of the Eastern Hills, which may have to do with the introduction of trout.\n\nIn the Eastern Hills two species of butterflies have been identified.\n\n\n"}
{"id": "213473", "url": "https://en.wikipedia.org/wiki?curid=213473", "title": "List of largest optical refracting telescopes", "text": "List of largest optical refracting telescopes\n\nHere is a list of the largest optical refracting telescopes sorted by lens diameter and focal length.\n\nThe largest practical functioning refracting telescope is the Yerkes Observatory 40 inch (102 cm) refractor, used for astronomical and scientific observation for over a century.\n\nMost are classical Great refractors, which used achromatic doublets on an equatorial mount. However, other large refractors include a 21st-century Solar telescope which is not directly comparable because it uses a single element non-achromatic lens, and the short-lived Great Paris Exhibition Telescope of 1900. It used a 78-inch (200 cm) Focault siderostat for aiming light into the Image-forming optical system part of the telescope, which had a 125 cm diameter lens. Using a siderostat incurs a reflective loss. Larger meniscus lenses have been used in later catadioptric telescopes which mix refractors and reflectors in the image-forming part of the telescope. As with reflecting telescopes, there was an ongoing struggle to balance cost with size, quality, and usefulness.\n\n\n"}
{"id": "2377848", "url": "https://en.wikipedia.org/wiki?curid=2377848", "title": "List of mesons", "text": "List of mesons\n\nThis article contains a list of mesons, unstable subatomic particles composed of one quark and one antiquark. They are part of the hadron particle family – particles made of quarks. The other members of the hadron family are the baryons – subatomic particles composed of three quarks. The main difference between mesons and baryons is that mesons have integer spin (thus are bosons) while baryons are fermions (half-integer spin). Because mesons are bosons, the Pauli exclusion principle does not apply to them. Because of this, they can act as force mediating particles on short distances, and thus play a part in processes such as the nuclear interaction.\n\nSince mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. They are classified according to their quark content, total angular momentum, parity, and various other properties such as C-parity and G-parity. While no meson is stable, those of lower mass are nonetheless more stable than the most massive mesons, and are easier to observe and study in particle accelerators or in cosmic ray experiments. They are also typically less massive than baryons, meaning that they are more easily produced in experiments, and will exhibit higher-energy phenomena sooner than baryons would. For example, the charm quark was first seen in the J/Psi meson () in 1974, and the bottom quark in the upsilon meson () in 1977.\n\nEach meson has a corresponding antiparticle (antimeson) where quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion () is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (), is made of one up antiquark and one down quark. Some experiments show the evidence of \"tetraquarks\" – \"exotic\" mesons made of two quarks and two antiquarks, but the particle physics community as a whole does not view their existence as likely, although still possible.\n\nThe symbols encountered in these lists are: I (\"isospin\"), J (\"total angular momentum\"), P (\"parity\"), C (\"C-parity\"), G (\"G-parity\"), u (\"up quark\"), d (\"down quark\"), s (\"strange quark\"), c (\"charm quark\"), b (\"bottom quark\"), Q (\"charge\"), B (\"baryon number\"), S (\"strangeness\"), C (\"charm\"), and B′ (\"bottomness\"), as well as a wide array of subatomic particles (hover for name).\n\nBecause this table was initially derived from published results and many of those results were preliminary, as many as 64 of the mesons in the following table may not exist or have the wrong mass or quantum numbers.\n\nThe following lists details for all known and predicted pseudoscalar (J = 0) and vector (J = 1) mesons.\n\nThe properties and quark content of the particles are tabulated below; for the corresponding antiparticles, simply change quarks into antiquarks (and vice versa) and flip the sign of Q, B, S, C, and B′. Particles with next to their names have been predicted by the standard model but not yet observed. Values in have not been firmly established by experiments, but are predicted by the quark model and are consistent with the measurements.\n\n Makeup inexact due to non-zero quark masses.<br>\n\n PDG reports the resonance width (Γ). Here the conversion τ =  is given instead. <br>\n\nThere are two complications with neutral kaons:\n\n\nNote that these issues also exist in principle for other neutral flavored mesons; however, the weak eigenstates are considered separate particles only for kaons because of their dramatically different lifetimes.\n\n"}
{"id": "3187506", "url": "https://en.wikipedia.org/wiki?curid=3187506", "title": "List of podcatchers", "text": "List of podcatchers\n\nA podcatcher, or podcast client, is a computer program used to download various media via an RSS or XML feed.\n\nWhile podcatchers are most known for downloading podcasts (generally audio files in MP3 format), many are also capable of downloading video, newsfeeds, text, and pictures. Some podcatchers can also automate the transfer of received audio files to a portable media player. Although many include a directory of high-profile podcasts, they generally allow users to manually subscribe directly to a feed by providing the URL.\n\nThe core concepts had been developing since 2000, the first commercial podcast client software was developed in 2001, and podcasting became popular in 2004.\n\nWhen Apple added podcatching to its iTunes software in June 2005, it almost immediately became the most popular client.\n\nThe following is a list of noteworthy podcatchers or software with podcatching capability.\n\n\n"}
{"id": "8700900", "url": "https://en.wikipedia.org/wiki?curid=8700900", "title": "List of pop-up blocking software", "text": "List of pop-up blocking software\n\nThis is a list of software that blocks pop-up ads.\n\nTrident shells\n\nGecko-based browsers\n\nKHTML/WebKit-based browsers\n\nPresto-based browsers\n\nOthers\n\n"}
{"id": "50899791", "url": "https://en.wikipedia.org/wiki?curid=50899791", "title": "Lorenzo Selva", "text": "Lorenzo Selva\n\nLorenzo Selva (1716 – c. 1790) was an Italian scientific instrument maker.\n\nA Venetian optician, Lorenzo Selva worked for 33 years with his father Domenico Selva (?–1758) and continued the business after the latter's death. He made several optical instruments. Remarkably, he signed them with his father's name: \"To show my ever-greater awareness and gratitude, every work of mine, every improvement and invention, will always be stamped, not with my name but with his beloved one.\" He described his father's work and the instruments produced in his optical laboratory in \"Esposizione delle comuni, e nuove spezie di Cannocchiali, Telescopj, Microscopj, ed altri Istrumenti Diottrici, Catottrici, e Catodiottrici Perfezionati ed inventati da Domenico Selva ottico\" [...] (Venice, 1761). Lorenzo is also the author of \"Sei dialoghi ottico teorico-pratici\" [Six theoretical-practical optical dialogues] (Venice, 1787). Some of his instruments are shown in the Museo Galileo in Florence, Italy.\n"}
{"id": "1619885", "url": "https://en.wikipedia.org/wiki?curid=1619885", "title": "Marine Modeling and Analysis Branch", "text": "Marine Modeling and Analysis Branch\n\nThe United States Marine Modeling and Analysis Branch (MMAB) is part of the Environmental Modeling Center, which is responsible for the development of improved numerical weather and marine prediction modeling systems within NCEP/NWS. It provides analysis and real-time forecast guidance (1–16 days) on marine meteorological, oceanographic, and cryospheric parameters over the global oceans and coastal areas of the US.\n\nProducts include:\n\n\n"}
{"id": "9637332", "url": "https://en.wikipedia.org/wiki?curid=9637332", "title": "Mucigel", "text": "Mucigel\n\nMucigel is a slimy substance that covers the root cap of the roots of plants. It is a highly hydrated polysaccharide, most likely a pectin, which is secreted from the outermost (epidermal) cells of the rootcap. Mucigel is formed in the Golgi bodies of such cells, and is secreted through the process of exocytosis. The layer of microorganism-rich soil surrounding the mucigel is called the rhizosphere.\n\nMucigel serves several functions, including:\n\n\n"}
{"id": "30593845", "url": "https://en.wikipedia.org/wiki?curid=30593845", "title": "New Science Projects", "text": "New Science Projects\n\nNew Science Projects is a blues punk band from Denton, Texas. They play a grungy, lo-fi styled punk rock, and are known for their wildly eclectic, often bizarre make-up and on-stage mannerisms. They have been described as an \"anti-house-disco-hipster band,\" by the Dallas Observer in an interview after the band's release of \"Poison Culture\"; however, Jones refutes this, claiming he doesn't \"want to position [himself] to run for political office at this time.\" The band is significant member of both the local Dallas-Fort Worth metroplex music scene as well as the local punk scene. New Science Projects has toured throughout North Texas, including Mount Righteous, Innards, and The Two Knights; they frequently play at 1919 Hemphill in Fort Worth, Texas, where they are also set to release their album Bikini Salute in January, which has been cited as \"...A wild ride... like a Bob Dylan nightmare.\" They were nominated for Best Blues Act in the Dallas Observer's 2010 award series.\n\nNew Science Projects is known for their bizarre make-up, which depicts all of the band members as severely injured and bleeding. The reasoning behind this has never been answered with clarity. Dale Jones speaks on behalf of the rest of the band, and is notorious for his on-stage persona, including his thick, ambiguous, Eastern European accent, though Jones states that his stage persona is no specific character with any particular origin. Jones' mannerisms seem distant and apart from the present; while he is very charismatic, much of his banter is juxtaposed, with multiple conflicting topics being spoken about at any given time, while other speeches very frequently trail off before their conclusion. When asked about his persona, Jones merely replied, \"I feel like it is me. So I don't know how I'd be anything else... Unless I was pretending. Which is not a bad idea.\"\n\n\n"}
{"id": "31239248", "url": "https://en.wikipedia.org/wiki?curid=31239248", "title": "Pension regulation", "text": "Pension regulation\n\nPension regulation is a legal term encompassing the set of laws, rules and authoritative standards governing the pension industry, and the procedures needed to enforce them. \n\nPension regulation varies widely from one jurisdiction to another - notably due to the persistence of discrepancies in the degree of autonomy and breadth of authority and discretionary power that national and regional pension regulators have at their disposal to enforce efficiently existing laws and regulations, in relation with local judicial practices and varying jurisprudential trends.\n\nPension regulation seeks to provide the various norms and standards needed to foster market efficiency, consistency, transparency and accountability across the pension industry: it is a key driver of pension funds’s risk management.\n\nIn Europe, in the wake of the 2008-2009 financial crisis, some pension experts such as Anton van Nunen have argued that excessive or misplaced regulatory activism can sometimes have negative unintended consequences, notably when it comes to the strict enforcement of asset liability matching in times high market volatility and the systematic use of bonds-based risk metrics across all asset classes.\n"}
{"id": "933589", "url": "https://en.wikipedia.org/wiki?curid=933589", "title": "Peter Millman", "text": "Peter Millman\n\nPeter Mackenzie Millman (August 10, 1906 – December 11, 1990) was a Canadian astronomer. He worked at the Dunlap Observatory from 1933 until 1940. In early 1941 he enlisted with the Royal Canadian Air Force. In 1946 he joined the Dominion Observatory in Ottawa. He then transferred to the National Research Council in 1955.\n\nDuring his graduate studies at Harvard University he started a systematic study of meteor spectra at the suggestion of Harlow Shapley in 1929. He continued the work on meteors throughout his active scientific life. He organized one of his most successful observational campaigns in 1946, when on the night of October 9/10 a spectacular shower of the Giacobinids (October Draconids) provided many important photographic spectra.\n\nHe was awarded the J. Lawrence Smith Medal in 1954.\n\nA crater on Mars and the minor planet 2904 Millman were named in his honor.\n\n"}
{"id": "47361951", "url": "https://en.wikipedia.org/wiki?curid=47361951", "title": "Rairakhol State", "text": "Rairakhol State\n\nRairakhol State () was a princely state during the British Raj in what is today India. It was one of the Chota Nagpur States and had its capital at Rairakhol (Redhakhol), located in the present-day Sambalpur district of Odisha. It had an area of and a population of 26,888 in 1901.\n\nMost of the state was covered by forest where wild elephants used to roam. Rairakhol State's inhabitants spoke mostly the Odia language, although there were also large Kol people groups speaking Munda and Oraon language. The Chasa caste was the predominant caste in the state.\n\nRairakhol was a feudatory state to Bamra until the 18th century, when the Garhjat Rajas of Patna freed it from its dependence. According to local tradition the wars between Bamra and Rairakhol used to be constant and during one of these wars the whole of the Rairakhol ruling family was destroyed except for one boy who was hidden by a woman of the Butka Sudh caste —a local caste of agricultural workers. When the soldiers of the King of Bamra Raja came they could not find him and the child was saved. After he became an adult he won back his kingdom and in gratitude the Butka Sudhs are considered by the Rairakhol ruling house as relatives on the mother's side.\nThe rulers of Rairakhol were Rathore Rajputs, related to the dynasties ruling in the states of Seraikela, Kharsawan and Bonai.\n\nDuring the 19th century Raja Bishan Chandra Janamuni's reign lasted 75 years. The state was under the political control of the Commissioner of the Chhattisgarh Division of the Central Provinces until 1905, coming then under the Bengal Presidency. On 1 January 1948 Rairakhol's last ruler, Raja Girish Chandra Deo, signed the instrument of accession to the Indian Union.\n\nRairakhol State's rulers bore the title of \" Maharaja \".\n\n\n"}
{"id": "66774", "url": "https://en.wikipedia.org/wiki?curid=66774", "title": "Regional planning", "text": "Regional planning\n\nRegional planning deals with the efficient placement of land-use activities, infrastructure, and settlement growth across a larger area of land than an individual city or town. Regional planning is a sub-field of urban planning as it relates land use practices on a broader scale. It also includes formulating laws that will guide the efficient planning and management of such said regions.\n\nAlthough the term \"regional planning\" is nearly universal in English-speaking countries the areas covered and specific administrative set ups vary widely. In North America, regional planning may encompass more than one state, such as the Regional Plan Association, or a larger conurbation or network of settlements. North American regional planning is likely to cover a much larger area than the Regional Assemblies of the UK; both, however, are equally \"regional\" in nature.\n\nRegions require various land uses; protection of farmland, cities, industrial space, transportation hubs and infrastructure, military bases, and wilderness. Regional planning is the science of efficient placement of infrastructure and zoning for the sustainable growth of a region. Advocates for regional planning such as new urbanist Peter Calthorpe, promote the approach because it can address region-wide environmental, social, and economic issues which may necessarily require a regional focus.\n\nA ‘region’ in planning terms can be administrative or at least partially functional, and is likely to include a network of settlements and character areas. In most European countries, regional and national plans are ‘spatial’ directing certain levels of development to specific cities and towns in order to support and manage the region depending on specific needs, for example supporting or resisting polycentrism.\n\nSpecific interventions and solutions will depend entirely on the needs of each region in each country, but generally speaking, regional planning at the macro level will seek to:\n\n\n\n"}
{"id": "70401", "url": "https://en.wikipedia.org/wiki?curid=70401", "title": "Satisficing", "text": "Satisficing\n\nSatisficing is a decision-making strategy or cognitive heuristic that entails searching through the available alternatives until an acceptability threshold is met. The term \"satisficing\", a portmanteau of \"satisfy\" and \"suffice\", was introduced by Herbert A. Simon in 1956, although the concept was first posited in his 1947 book \"Administrative Behavior\". Simon used satisficing to explain the behavior of decision makers under circumstances in which an optimal solution cannot be determined. He maintained that many natural problems are characterized by computational intractability or a lack of information, both of which preclude the use of mathematical optimization procedures. He observed in his Nobel Prize in Economics speech that \"decision makers can satisfice either by finding optimum solutions for a simplified world, or by finding satisfactory solutions for a more realistic world. Neither approach, in general, dominates the other, and both have continued to co-exist in the world of management science\".\n\nSimon formulated the concept within a novel approach to rationality, which posits that rational choice theory is an unrealistic description of human decision processes and calls for psychological realism. He referred to this approach as bounded rationality. Some consequentialist theories in moral philosophy use the concept of satisficing in the same sense, though most call for optimization instead.\n\nIn decision making, satisficing refers to the use of aspiration levels when choosing from different paths of action. By this account, decision-makers select the first option that meets a given need or select the option that seems to address most needs rather than the \"optimal\" solution.\n\nA crucial determinant of a satisficing decision strategy concerns the construction of the aspiration level. In many circumstances, the individual may be uncertain about the aspiration level.\n\nAnother key issue concerns an evaluation of satisficing strategies. Although often regarded as an inferior decision strategy, specific satisficing strategies for inference have been shown to be ecologically rational, that is in particular decision environments, they can outperform alternative decision strategies.\n\nSatisficing also occurs in consensus building when the group looks towards a solution everyone can agree on even if it may not be the best.\n\nOne popular method for rationalizing satisficing is optimization when \"all\" costs, including the cost of the optimization calculations themselves and the cost of getting information for use in those calculations, are considered. As a result, the eventual choice is usually sub-optimal in regard to the main goal of the optimization, i.e., different from the optimum in the case that the costs of choosing are not taken into account.\n\nAlternatively, satisficing can be considered to be just constraint satisfaction, the process of finding a solution satisfying a set of constraints, without concern for finding an optimum. Any such satisficing problem can be formulated as an (equivalent) optimization problem using the Indicator function of the satisficing requirements as an objective function. More formally, if denotes the set of all options and denotes the set of \"satisficing\" options, then selecting a satisficing solution (an element of ) is equivalent to the following optimization problem\n\nwhere denotes the Indicator function of , that is\n\nA solution to this optimization problem is optimal if, and only if, it is a satisficing option (an element of ). Thus, from a decision theory point of view, the distinction between \"optimizing\" and \"satisficing\" is essentially a stylistic issue (that can nevertheless be very important in certain applications) rather than a substantive issue. What is important to determine is should be optimized and should be satisficed. The following quote from Jan Odhnoff's 1965 paper is appropriate:\n\nIn economics, satisficing is a behavior which attempts to achieve at least some minimum level of a particular variable, but which does not necessarily maximize its value. The most common application of the concept in economics is in the behavioral theory of the firm, which, unlike traditional accounts, postulates that producers treat profit not as a goal to be maximized, but as a constraint. Under these theories, a critical level of profit must be achieved by firms; thereafter, priority is attached to the attainment of other goals.\n\nMore formally, as before if denotes the set of all options , and we have the payoff function which gives the payoff enjoyed by the agent for each option. Suppose we define the optimum payoff the solution to\n\nwith the optimum actions being the set ' of options such that (i.e. it is the set of all options that yield the maximum payoff). Assume that the set ' has at least one element.\n\nThe idea of the was introduced by Herbert A. Simon and developed in economics by Richard Cyert and James March in their 1963 book \"A Behavioral Theory of the Firm\". The aspiration level is the payoff that the agent aspires to: if the agent achieves at least this level it is satisfied, and if it does not achieve it, the agent is not satisfied. Let us define the aspiration level and assume that . Clearly, whilst it is possible that someone can aspire to something that is better than the optimum, it is in a sense irrational to do so. So, we require the aspiration level to be at or below the optimum payoff.\n\nWe can then define the set of satisficing options ' as all those options that yield at least ': . Clearly since , it follows that . That is, the set of optimum actions is a subset of the set of satisficing options. So, when an agent satisfices, then she will choose from a larger set of actions than the agent who optimizes. One way of looking at this is that the satisficing agent is not putting in the effort to get to the precise optimum or is unable to exclude actions that are below the optimum but still above aspiration.\n\nAn equivalent way of looking at satisficing is (that means you choose your actions so that the payoff is within epsilon of the optimum). If we define the \"gap\" between the optimum and the aspiration as ' where . Then the set of satisficing options ' can be defined as all those options such that .\n\nApart from the behavioral theory of the firm, applications of the idea of satisficing behavior in economics include the Akerlof and Yellen model of menu cost, popular in New Keynesian macroeconomics. Also, in economics and game theory there is the notion of an Epsilon equilibrium, which is a generalization of the standard Nash equilibrium in which each player is within ' of his or her optimal payoff (the standard Nash-equilibrium being the special case where ').).\n\nWhat determines the aspiration level? This can come from past experience (some function of an agent's or firm's previous payoffs), or some organizational or market institutions. For example, if we think of managerial firms, the managers will be expected to earn normal profits by their shareholders. Other institutions may have specific targets imposed externally (for example state-funded universities in the UK have targets for student recruitment).\n\nAn economic example is the Dixon model of an economy consisting of many firms operating in different industries, where each industry is a duopoly. The endogenous aspiration level is the average profit in the economy. This represents the power of the financial markets: in the long-run firms need to earn normal profits or they die (as Armen Alchian once said, \"This is the criterion by which the economic system selects survivors: those who realize positive profits are the survivors; those who suffer losses disappear\"). We can then think what happens over time. If firms are earning profits at or above their aspiration level, then they just stay doing what they are doing (unlike the optimizing firm which would always strive to earn the highest profits possible). However, if the firms are earning below aspiration, then they try something else, until they get into a situation where they attain their aspiration level. It can be shown that in this economy, satisficing leads to collusion amongst firms: competition between firms leads to lower profits for one or both of the firms in a duopoly. This means that competition is unstable: one or both of the firms will fail to achieve their aspirations and hence try something else. The only situation which is stable is one where all firms achieve their aspirations, which can only happen when all firms earn average profits. In general, this will only happen if all firms earn the joint-profit maximizing or collusive profit.\n\nSome research has suggested that satisficing/maximizing and other decision-making strategies, like personality traits, have a strong genetic component and endure over time. This genetic influence on decision-making behaviors has been found through classical twin studies, in which decision-making tendencies are self-reported by pairs of twins and then compared between monozygotic and dizygotic twins. This implies that people can be categorized into \"maximizers\" and \"satisficers\", with some people landing in between.\n\nThe distinction between satisficing and maximizing not only differs in the decision-making process, but also in the post-decision evaluation. Maximizers tend to use a more exhaustive approach to their decision-making process: they seek and evaluate more options than satisficers do to achieve greater satisfaction. However, whereas satisficers tend to be relatively pleased with their decisions, maximizers tend to be less happy with their decision outcomes. This is thought to be due to limited cognitive resources people have when their options are vast, forcing maximizers to not make an optimal choice. Because maximization is unrealistic and usually impossible in everyday life, maximizers often feel regretful in their post-choice evaluation.\n\nAs an example of satisficing, in the field of social cognition, Jon Krosnick proposed a theory of statistical survey satisficing which says that optimal question answering by a survey respondent involves a great deal of cognitive work and that some people would use satisficing to reduce that burden\n. Some people may shortcut their cognitive processes in two ways:\n\nLikelihood to satisfice is linked to respondent ability, respondent motivation and task difficulty.\n\nRegarding survey answers, satisficing manifests in:\n\nThe Effort Satisficing Theory (EST) is a recently posited theory (from a 2015 University of Calgary dissertation) whose core purpose is for understanding, predicting and acceptably influencing effort-satisficing behavior and its impact on phenomena such as operational outcomes, organizational performance and economic performance.\n\nIn EST, satisficing refers to a (for instance an employee or economic agent) not exerting the optimal , the effort required for the best outcomes in all effort requiring activities. Rather than applying the concept of satisficing to the outcome variable (e.g. decision quality), the concept of satisficing is applied to the input variable (the effort) required to generate the outcome variable (e.g. decision quality). In EST the tendency for satisficing is explained with reference to phenomena such as volition and EST self-preservation.\n\nThe fundamental proposition of the effort satisficing theory is that \"Volitional entities tend to satisfice on efforts\". EST postulates that: people generally exhibit volition (choice) and are as such volitional entities (i.e. people exhibit freedom of choice), volitional entities tend to exhibit a self-preservation imperative (i.e. they try to ensure their well-being is not significantly degraded), the exertion of effort results in the incurrence of (which is the outcome of effort exertion for instance, tiredness or financial strain) and volitional entities believe that large amounts of strain signal a threat to their self-preservation imperative. Consequently, when the level of strain incurred by the volitional entity exceeds a threshold called the , volitional entities tend to exhibit satisficing behaviour by satisficing on their effort exertion.\n\nSatisficing in EST is thus the result of an interplay of internal effort (the total effort being exerted by the volitional entity), strain and the satisficing threshold given the prior condition that the axioms defining the theory's universe (namely volition and self-preservation) are satisfied. In EST, factors called the influence the relationship governing the amount of strain incurred per unit effort. Other factors called the influence the value of the satisficing threshold, the level of strain at which satisficing tendency becomes very strong.\n\nIn the effort satisficing theory, there are three modes of influencing satisficing behaviour:\nFrom the EST lens, Keynesian and Hayekian macroeconomic management approaches are not exclusive and contradictory but rather complementary incomplete parts of the much larger EST macroeconomic management approach. For instance, Keynesian macroeconomics attributes economic failure to \"animal spirits\", a satisficing threshold determinant argument whereas Hayekian macroeconomics attributes economic failure to unsustainable interest rates, a strain rate determinant argument. From the EST lens, both can cause economic failure so both Keynes and Hayek are right. From the EST lens, economies will develop faster if we focus on establishing sustainable systems needed to reduce the economic effort exertion of economic agents in their pursuit of economic activities. These include payment systems which reduce the cost of monitoring, systems which reduce grassroots corruption, impactful and sustainable physical infrastructure as well as social enterprises which facilitate the public good. The challenge for 21st-century governments is therefore not necessarily the use or avoidance of stimulus but rather, the establishment of sustainable systems which permanently reduce the economic effort exertion of economic agents in their pursuit of economic activity to make the world a better place.\n\nThe propositions of the Effort Satisficing Theory have been used by countries such as Nigeria to actually raise an extra $600 million in annual tax revenue without increasing tax rates.\n\n\n"}
{"id": "2514850", "url": "https://en.wikipedia.org/wiki?curid=2514850", "title": "Small Is Beautiful", "text": "Small Is Beautiful\n\nSmall Is Beautiful: A Study of Economics As If People Mattered is a collection of essays by German born British economist E. F. Schumacher. The phrase \"Small Is Beautiful\" came from a phrase by his teacher Leopold Kohr. It is often used to champion small, appropriate technologies that are believed to empower people more, in contrast with phrases such as \"bigger is better\".\n\nFirst published in 1973, \"Small Is Beautiful\" brought Schumacher's critiques of Western economics to a wider audience during the 1973 energy crisis and emergence of globalization. \"The Times Literary Supplement\" ranked \"Small Is Beautiful\" among the 100 most influential books published since World War II. A further edition with commentaries was published in 1999.\n\nThe book is divided into four parts: \"The Modern World\", \"Resources\", \"The Third World\", and \"Organization and Ownership\".\nPart I summarizes the Economic world of the early 1970s, from Schumacher's perspective. In the first chapter, \"The Problem of Production\", Schumacher argues that the modern economy is unsustainable. Natural resources (like fossil fuels), are treated as expendable income, when in fact they should be treated as capital, since they are not renewable, and thus subject to eventual depletion. He further argues that nature's resistance to pollution is limited as well. He concludes that government effort must be concentrated on sustainable development, because relatively minor improvements, for example, technology transfer to Third World countries, will not solve the underlying problem of an unsustainable economy. Schumacher's philosophy is one of \"enoughness\", appreciating both human needs and limitations, and appropriate use of technology. It grew out of his study of village-based economics, which he later termed Buddhist economics, which is the subject of the book's fourth chapter.\n\nPart II casts Education as the greatest resource, and discusses Land, Industry, Nuclear Energy and the human impact of Technology.\n\nPart III discusses the gap between the center of the World System and the developing world as it existed then, with a focus on village culture and unemployment in India.\n\nPart IV presents a sketch of a Theory of Large Scale Organization, refutes and exposes some commonplace and false platitudes about Capitalism as a social order and discusses alternatives. Chapter 3 of this part concludes with advice to Socialists (who presumably are at the Commanding Heights):\n\n\"Socialists should insist on using the nationalised industries not simply to out-capitalise the capitalists -- an attempt in which they may or may not succeed -- but to evolve a more democratic and dignified system of industrial administration, a more humane employment of machinery, and a more intelligent utilization of the fruits of human ingenuity and effort. If they can do this, they have the future in their hands. If they cannot, they have nothing to offer that is worthy of the sweat of free-born men.\"\n\n"}
{"id": "31835742", "url": "https://en.wikipedia.org/wiki?curid=31835742", "title": "Social Studies of Science", "text": "Social Studies of Science\n\nSocial Studies of Science is a bimonthly peer-reviewed academic journal that publishes papers relating to the history and philosophy of science. The journal's editor-in-chief is Sergio Sismondo (Queen's University). The journal was established in 1971 under the name \"Science Studies\" and assumed its present title in 1975. It is currently published by SAGE Publications.\n\nIn the inaugural issue, the editors announced that the journal \"will devote itself to original research, whether empirical or theoretical, which brings fresh light to bear on the\nconcepts, processes and consequences of modern science. It will be interdisciplinary in the sense that it will encourage appropriate contributions from political science, sociology, economics, history, philosophy, social anthropology, and the legal and educational\ndisciplines. It will welcome studies of fundamental research, applied research and development; of university science, industrial science and science in government.\"\n\n\"Social Studies of Science\" is abstracted and indexed in Scopus and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2014 impact factor is 2.351, ranking it 1st out of 59 journals in the category \"History of Philosophy and Science\".\n"}
{"id": "8260206", "url": "https://en.wikipedia.org/wiki?curid=8260206", "title": "The Sociological Imagination", "text": "The Sociological Imagination\n\nThe Sociological Imagination is a 1959 book by American sociologist C. Wright Mills published by Oxford University Press. In it, he develops the idea of sociological imagination, the means by which the relation between self and society can be understood.\n\nMills felt that the central task for sociology and sociologists was to find (and articulate) the connections between the particular social environments of individuals (also known as \"milieu\") and the wider social and historical forces in which they are enmeshed. The approach challenges a structural functionalist approach to sociology, as it opens new positions for the individual to inhabit with regard to the larger social structure. Individual function that reproduces larger social structure is only one of many possible roles and is not necessarily the most important. Mills also wrote of the danger of malaise (apathy), which he saw as inextricably embedded in the creation and maintenance of modern societies. This led him to question whether individuals exist in modern societies in the sense that \"individual\" is commonly understood (Mills, 1959, 7–12).\n\nIn writing \"The Sociological Imagination\", Mills tried to reconcile two varying, abstract conceptions of social reality, the \"individual\" and the \"society\", and thereby challenged the dominant sociological discourse to define some of its most basic terms and be forthright about the premises behind its definitions. He began the project of reconciliation and challenge with critiques of \"grand theory\" and \"abstracted empiricism\", outlining and criticizing their use in the current sociology of the day.\n\nIn 1998 the International Sociological Association listed the work as the second most important sociological book of the 20th century.\n\nIn chapter two, Mills seems to be criticizing Parsonian Sociology. In this he directly addresses \"The Social System\", written by Talcott Parsons.\n\nIn \"The Social System\", Parsons describes the nature of the structure of society and the creation and maintenance of a culture through the socialization of individuals. Mills criticizes this tendency in sociology on several grounds. He argues for a more heterogeneous form of society in that he challenges the extent to which a single uniformity of society is indeed possible (Mills, 1959, 26-30).\n\nMills criticizes the Parsonian formulation of social order, particularly the idea that social order can indeed be seen as a whole. He writes that every individual cannot simply be fully integrated into society and internalize all its cultural forms. Furthermore, such domination may be seen as a further extension of power and social stratification.\n\nBrewer (2004) sees \"The Sociological Imagination\" as an extension of Mills's other works on power and social stratification, i.e. \"The Power Elite\" and \"\".\nAccording to Mills, what grand theorists call \"value orientation\" could in actuality be a form of domination and thereby may simply be a form of legitimation (Mills, 1959, 33-36).\n\nHe further criticizes Parsonian Sociology on its ability to theorize as a form of pure abstraction that society can be understood irrespective of its historical and contextual nature without observation.\n\nHe argues that society and its cultural symbols cannot be seen as self-determining and cannot be derived without reference to individuals and their consciousness. All power according to Parsons is based on a system of beliefs enforced by society, writes Mills. In this he criticizes Parsons for his view in terms of historical and social change and diversity (Mills, 1959, 40-46).\n\nHe thereby criticizes the means by which a social order can be derived without observation (Mills, 1959, 46-48).\n\nIn the third chapter Mills criticizes the empirical methods of social research which he saw as evident at the time in the conception of data and the handling of methodological tools.\n\nThis can be seen as a reaction to the plethora of social research being developed from about the time of World War II. This can thereby be seen as much a criticism by Brewer that Mills may have been critical of the research being conducted and sponsored by the American government.\n\nAs such Mills criticizes the methodological inhibition which he saw as characteristic of what he called abstract empiricism. In this he can be seen criticizing the work of Paul F. Lazarsfeld who conceives of sociology not as a discipline but as a methodological tool (Mills, 1959, 55-59).\n\nHe argues that the problem of such social research is that there may be a tendency towards \"psychologism\", which explains human behavior on the individual level without reference to the social context. This, he argues, may lead to the separation of research from theory.\nHe then writes of the construction of milieu in relation to social research and how both theory and research are related (Mills, 1959, 65-68).\n\nIn chapter seven Mills sets out what is thought to be his vision of Sociology. He writes of the need to integrate the social, biographical, and historical versions of reality in which individuals construct their social milieus with reference to the wider society (Mills, 1959, 132-134).\n\nHe argues that the nature of society is continuous with historical reality. In doing so, Mills writes of the importance of the empirical adequacy of theoretical frameworks. He also writes of the notion of a unified social sciences. This he believes is not a conscious effort but is a result of the historical problem-based discourses out of which the disciplines developed, in which the divisions between the disciplines become increasingly fluid (Mills, 1959, 136-140). Thus, Mills sets out what he believed to be a problem-based approach to his conception of social sciences (140-142).\n\nMills opens \"On Reason and Freedom\" with the two facets of the sociological imagination (history and biography) in relationship to the social scientist. Mills asserts that it is time for social scientists to address the troubles of the individual and the issues of society to better understand the state of freedom specific to this historical moment. According to Mills, understanding personal troubles in relationship to social structure is the task of the social scientist.\n\nMills goes on to situate the reader in the historically specific moment that he wrote the book, or what Mills refers to as the Fourth Epoch. Mills explains that \"nowadays men everywhere seek to know where they stand, where they may be going, and what—if anything—they can do about the present as history and the future as responsibility\" (165). To have a better understanding of the self and society, it is necessary to develop new ways of making sense of reality as old methods for understanding associated with liberalism and socialism are inadequate in this new epoch. Enlightenment promises associated with the previous epoch have failed; increased rationality moves society further away from freedom rather than closer to it.\n\nMills explains that highly rationalized organizations, such as bureaucracies, have increased in society; however, reason as used by the individual has not because the individual does not have the time or means to exercise reason. Mills differentiates reason and rationality. Reason, or that which is associated with critical and reflexive thought, can move individuals closer to freedom. On the other hand, rationality, which is associated with organization and efficiency, results in a lack of reason and the destruction of freedom. Despite this difference, rationality is often conflated with freedom.\n\nGreater rationality in society, as understood by Mills, results in the rationalization of every facet of life for the individual until there is the loss \"of his capacity and will to reason; it also affects his chances and his capacity to act as a free man\" (170). This does not mean that individuals in society are unintelligent or hopeless. Mills is not suggesting determinism. Under Mills' conception, freedom is not totally absent as the \"average\" individual in society has \"a real potential for freedom\". Individuals have adapted to the rationalization of society. Mills believed in the individual's autonomy and potential to alter societal structures.\n\nThe individual who does not exercise reason and passively accepts their social position is referred to by Mills as \"The Cheerful Robot\" in which the individual is alienated from the self and society totally. Mills asks if, at some point and time in the future, individuals will accept this state of total rationality and alienation willingly and happily. This is a pressing concern as the Cheerful Robot is the \"antithesis\" of democratic society; the Cheerful Robot is the \"ultimate problem of freedom\" (175) as a threat to society's values. According to Mills, social scientists must study social structure, using the sociological imagination, to understand the state of freedom in this epoch. Mills concludes this section of \"The Sociological Imagination\" with a call to social scientists: it is the promise of the social sciences to analyze the individual's troubles and society's issues in order to not only evaluate freedom in society but to foster it.\n\nMills's work was widely read in its time, and \"The Sociological Imagination\" is still one of the most widely read tracts of sociology and a staple of undergraduate sociology courses. His work was not well received at the time, which can be seen as a result of Mills's professional and personal reputation (Brewer, 2004, 317).\n\nThis is somewhat appropriate given that the nature of Mills's work patterned around the biography of individuals, their historical actions and the relation to the wider society in terms of structure, in as much as Mills's own life has been seen by others as illustrative of his conception of Sociology. He hoped to reconcile the issues of individuals with the problems facing society, thereby framing individuals' problems in social, political, and historical reality (Brewer, 2004, 320).\n\nThus, he can be seen as trying to create a three-dimensional view of society and, according to Brewer (2004), attempted to break down the divide between the public and the private realms of society, something characteristic of Sociology at the time. In this, he was viewing society as simultaneously macroscopic and microscopic in nature whilst trying to merge both historical and contemporary social realities (Brewer, 2004, 320-321).\n\nHis work was widely criticized due to what were perceived critical attacks on the discipline. This can be seen in his writings where he criticizes both the \"methodological inhibition\" of what he refers to as abstract empiricism (i.e. the work of Paul F. Lazarsfield) and what he refers to as the \"fetishisation of concepts\" in the works of those such as Talcott Parsons. Mills criticized the \"grand theory\" and the positivism of structural functionalism in Parsons' work (Brewer, 2004, 322-324).\n\nThis exacerbated what were seen as professional disagreements which were then ongoing with other professionals in the discipline. In particular his criticism of abstracted empiricism was seen in conjunction to his criticisms of both state sponsored research and the political policies of the Cold War American government (Brewer, 2004, 326-328).\n\nAs such, his work was not well received. Both in Britain and in America he came under criticism. In Britain his work was criticized for the extent to which he was seen to attack empirical Sociology which was then common in Britain at the time. In America, his criticism of \"structural functionalism\" and of its accompanying critiques of power and stratification made him somewhat subject to severe criticism (Brewer, 2004, 328-330).\n\nThe reception of C. Wright Mills can now be seen as somewhat illustrative of Mills's personality. In his work, we can see the \"space of selfhood\" which Mills argued individuals connect individuals with society as a whole. Thus, of personalized experiences being used to link public discourses he can thereby be seen to mark a biographical turn in post-structuralist Sociology (Brewer, 2005, 661-663).\n\nHis work can also be seen as reaction to cold war America and the radicalism and disengagement with establishment sociology. It can also, however, be seen as return by those such as Brewer to a tradition of \"social reformism\" as well as a response to the professionalization of the discipline (Brewer, 2005, 663-665).\n\nHis conception of the specialization of the discipline can be seen in the works of Georg Simmel, in his idea of social space and social configurations of space. Thus, Brewer (2005) seems to see him returning the discipline to the configuration of biography and self in the configuration of social space. This can also be seen in the social constructionism and the importance of space and time in the work of Anthony Giddens. This is most reminiscent of \"the templates of the self\" as seen as the understanding of the self in relation to social space as written by Erving Goffman and his conception of \"frontstage\" and \"backstage\". Thus the work of Mills can be seen as an illustrative example in terms of his biography of the conception of social space and the importance of narrative (Brewer, 2005, 665-667).\n\nHis life is therefore seen as having an impact on his construction of self. This can be seen as a reflection therefore of his background and the importance he placed on independence, self-reliance, and individualism in the creation of autonomy and what others would refer to as the \"[o]ccupational role of the loner\". This \"outsider mentality\", as referred to by Brewer (2005), can be seen as form of personal survival whereby Mills could thereby distance himself from personal and professional criticism. Thus, the Sociological Imagination is seen by many as a connection between Mills' life and work (Brewer, 2005, 668-671).\n\nThe work of C. Wright Mills can be seen as extended in the work of Michael Burawoy and his conception of \"public sociology\". In his speech to the American Sociological Association he speaks of the importance of public discourse and the importance of Sociology as an agent of historical change (Burawoy, 2005, 259-261).\n\nThis can also be seen in his work \"Ethnography Unbound\", in which he refers to his Extended case method of ethnography and relates C. Wright Mills work in his idea of theory construction as the relation of \"the personal troubles of the milieu\" to \"the public issues of the social structure\" (C. Wright Mills, in Burawoy, 1991, 6).\n\n\n\n"}
{"id": "18152904", "url": "https://en.wikipedia.org/wiki?curid=18152904", "title": "Visual analytics", "text": "Visual analytics\n\nVisual analytics is an outgrowth of the fields of information visualization and scientific visualization that focuses on analytical reasoning facilitated by interactive visual interfaces.\n\nVisual analytics is \"the science of analytical reasoning facilitated by interactive visual interfaces.\" It can attack certain problems whose size, complexity, and need for closely coupled human and machine analysis may make them otherwise intractable. Visual analytics advances science and technology developments in analytical reasoning, interaction, data transformations and representations for computation and visualization, analytic reporting, and technology transition. As a research agenda, visual analytics brings together several scientific and technical communities from computer science, information visualization, cognitive and perceptual sciences, interactive design, graphic design, and social sciences.\n\nVisual analytics integrates new computational and theory-based tools with innovative interactive techniques and visual representations to enable human-information discourse. The design of the tools and techniques is based on cognitive, design, and perceptual principles. This science of analytical reasoning provides the reasoning framework upon which one can build both strategic and tactical visual analytics technologies for threat analysis, prevention, and response. Analytical reasoning is central to the analyst’s task of applying human judgments to reach conclusions from a combination of evidence and assumptions.\n\nVisual analytics has some overlapping goals and techniques with information visualization and scientific visualization. There is currently no clear consensus on the boundaries between these fields, but broadly speaking the three areas can be distinguished as follows:\n\n\nVisual analytics seeks to marry techniques from information visualization with techniques from computational transformation and analysis of data. Information visualization forms part of the direct interface between user and machine, amplifying human cognitive capabilities in six basic ways:\n\n\nThese capabilities of information visualization, combined with computational data analysis, can be applied to analytic reasoning to support the sense-making process.\n\nVisual analytics is a multidisciplinary field that includes the following focus areas:\n\n\nAnalytical reasoning techniques are the method by which users obtain deep insights that directly support situation assessment, planning, and decision making. Visual analytics must facilitate high-quality human judgment with a limited investment of the analysts’ time. Visual analytics tools must enable diverse analytical tasks such as:\n\n\nThese tasks will be conducted through a combination of individual and collaborative analysis, often under extreme time pressure. Visual analytics must enable hypothesis-based and scenario-based analytical techniques, providing support for the analyst to reason based on the available evidence.\n\nData representations are structured forms suitable for computer-based transformations. These structures must exist in the original data or be derivable from the data themselves. They must retain the information and knowledge content and the related context within the original data to the greatest degree possible. The structures of underlying data representations are generally neither accessible nor intuitive to the user of the visual analytics tool. They are frequently more complex in nature than the original data and are not necessarily smaller in size than the original data. The structures of the data representations may contain hundreds or thousands of dimensions and be unintelligible to a person, but they must be transformable into lower-dimensional representations for visualization and analysis.\n\nTheories of visualization include:\n\nVisual representations translate data into a visible form that highlights important features, including commonalities and anomalies. These visual representations make it easy for users to perceive salient aspects of their data quickly. Augmenting the cognitive reasoning process with perceptual reasoning through visual representations permits the analytical reasoning process to become faster and more focused.\n\nThe input for the data sets used in the visual analytics process are heterogeneous data sources (i.e., the internet, newspapers, books, scientific experiments, expert systems). From these rich sources, the data sets \"S = S, ..., S\" are chosen, whereas each \"S , i ∈ (1, ..., m)\" consists of attributes A, ..., A. The goal or output of the process is insight \"I\". Insight is either directly obtained from the set of created visualizations \"V\" or through confirmation of hypotheses \"H\" as the results of automated analysis methods. This formalization of the visual analytics process is illustrated in the following figure. Arrows represent the transitions from one set to another one.\n\nMore formally the visual analytics process is a transformation \"F: S → I\", whereas \"F\" is a concatenation of functions \"f ∈ {D, V, H, U}\" defined as follows:\n\n\"D\" describes the basic data pre-processing functionality with \"D : S → S and W ∈ {T, C, SL, I}\" including data transformation functions \"D\", data cleaning functions \"D\", data selection functions \"D\" and data integration functions \"D\" that are needed to make analysis functions applicable to the data set.\n\n\"V, W ∈ {S, H}\" symbolizes the visualization functions, which are either functions visualizing data \"V : S → V\" or functions visualizing hypotheses \"V : H → V\".\n\n\"H, Y ∈ {S, V}\" represents the hypotheses generation process. We distinguish between functions that generate hyphotheses from data \"H : S → H\" and functions that generate hypotheses from visualizations \"H : V → H\".\n\nMoreover, user interactions \"U, Z ∈ {V, H, CV, CH}\" are an integral part of the visual analytics process. User interactions can either effect only visualizations \"U : V → V\" (i.e., selecting or zooming), or can effect only hypotheses \"U : H → H\" by generating a new hypotheses from given ones. Furthermore, insight can be concluded from visualizations \"U : V → I\" or from hypotheses \"U : H → I\".\n\nThe typical data pre-processing applying data cleaning, data integration and data transformation functions is defined as \"D = D(D(D(S, ..., S)))\". After the pre-processing step either automated analysis methods \"H = {f, ..., f}\" (i.e., statistics, data mining, etc.) or visualization methods \"V : S → V, V = {f, ..., f}\" are applied to the data, in order to reveal patterns as shown in the figure above.\n\nIn general the following paradigm is used to process the data:\n\n\"Analyse First – Show the Important – Zoom, Filter and Analyse Further – Details on Demand\"\n\n"}
{"id": "571226", "url": "https://en.wikipedia.org/wiki?curid=571226", "title": "Walla Walla River", "text": "Walla Walla River\n\nThe Walla Walla River is a tributary of the Columbia River, joining the Columbia just above Wallula Gap in southeastern Washington in the United States. The river flows through Umatilla County, Oregon, and Walla Walla County, Washington. Its drainage basin is in area.\n\nThe headwaters of the Walla Walla River lie in the Blue Mountains of northeastern Oregon. The river originates as the North and South Forks of the Walla Walla River. The surrounding forested land holds a network of hiking and mountain-biking trails.\n\nThe confluence of the North and South Forks lies east of Milton–Freewater, Oregon. The river flows eastward to reach Milton–Freewater, which is built along its banks, and then flows northward through Milton–Freewater. Irrigation water is drawn from the river here and at numerous locations along the river.\n\nThe Walla Walla River flows southwest of the city of Walla Walla in the Walla Walla valley. Mill Creek, which flows through the city of Walla Walla, joins the Walla Walla River at the Whitman Mission west of the city of Walla Walla.\n\nThe Touchet River joins the Walla Walla at the town of Touchet, Washington. The annual mean discharge of the Walla Walla River just below the Touchet River confluence is . The maximum recorded discharge was in 1964.\n\nThe river enters the Columbia a mile south of the town of Wallula just north of Wallula Gap. The section of the Columbia River is called Wallula Lake, the reservoir impoundment created by McNary Dam.\n\nThe Walla Walla tribe occupied the region around the Walla Walla river prior to white settlers entry to the region.\n\nThe Lewis and Clark Expedition (1804–1806) was the first United States overland expedition to the Pacific coast and back—the return expedition stopped at the mouth of the Walla Walla and stayed with the Walla Walla tribe for a portion of the journey, proceeding from there overland to the Snake River. British explorer David Thompson was the first European to navigate the entire length of the Columbia River, to the Pacific Ocean, in 1811.\n\nFort Nez Percés (later known as Fort Walla Walla) was a fortified fur trading post on the Columbia River on the territory of modern-day Wallula, Washington. It was in operation from 1818 until 1857 on the eastern shore of the Columbia River, immediately north of the mouth of the Walla Walla River. The Oregon Treaty ended joint U.S.A. - British occupation that had been effect since the Treaty of 1818. The fort was abandoned in 1857 when the Hudson's Bay Company gave up its Columbia District business in the Oregon Territory.\n\nThe Whitman Mission was established in 1836. It lies near the banks of the river to the west of the modern city of Walla Walla.\n\nThe Walla Walla River supports populations of spring Chinook salmon, summer steelhead, and bull trout among other species. There is a sport fishery for steelhead in the river.\nIt also holds channel catfish and smallmouth bass in the summer.\n\nThere have been a wide range of studies on the Walla Walla River and the overall catchment area. Work done by both Oregon and Washington State government, federal and state environmental agencies and local watershed councils and groups have produced a wide range of studies. The WWBWC (www.wwbwc.org) has both electronic and paper copies of many of these reports that date back to the 1930s. A considerable amount of work has gone into the assessment of water quantity and quality for the purpose of salmon recovery and sustainable irrigation supply. A highly interconnected alluvial groundwater system and its over abstraction through over allocated irrigation usage has also acted to influence flows and quality in the Walla Walla River. \n\nThe promise of the Walla Walla river lies inside its integrated water management strategy using Managed Aquifer Recharge (Or Shallow Aquifer Recharge) to utilise available non-irrigation season water to replenish groundwater supplies and the over allocation of groundwater resources by both state governments. The Walla Walla partnership along with the WWBWC are setting a national example with this innovative and low cost alternative to surface storage using dams.\n\n\n"}
