{"id": "42787879", "url": "https://en.wikipedia.org/wiki?curid=42787879", "title": "Acciptrid herpesvirus 1", "text": "Acciptrid herpesvirus 1\n\nThe acciptrid herpesvirus 1 (AcHV-1) is an unassigned virus in the family Herpesviridae, isolated from a nesting bald eagle, Haliaeetus leucocephalus.\n"}
{"id": "28700263", "url": "https://en.wikipedia.org/wiki?curid=28700263", "title": "American Association for the Promotion of Social Science", "text": "American Association for the Promotion of Social Science\n\nThe American Association for the Promotion of Social Science (est.1865) was founded in Boston, Massachusetts, by several high-profile academics. Officers in the first years of the society included William B. Rogers, Thomas Hill, George S. Boutwell, Francis Lieber, Erastus O. Haven, Mary Eliot Parkman, David A. Wells, Emory Washburn, Caroline Healey Dall, Samuel Eliot, F. B. Sanborn, Joseph White, George Walker, Theodore W. Dwight, and James J. Higginson.\n\nIn 1865 the group intended \"to aid the development of social science, and to guide the public mind to the best practical means of promoting the amendment of laws, the advancement of education, the prevention and repression of crime, the reformation of criminals, and the progress of public morality, the adoption of sanitary regulations, and the diffusion of sound principles on questions of economy, trade, and finance. It will give attention to pauperism, and the topics related thereto ; including the responsibility of the well-endowed and successful, the wise and educated, the honest and respectable, for the failures of others. It will aim to bring together the various societies and individuals now interested in these objects, for the purpose of obtaining by discussion the real elements of truth; by which doubts are removed, conflicting opinions harmonized, and a common ground afforded for treating wisely the great social problems of the day.\"\n\nThe society divided itself into departments of inquiry (education; health; jurisprudence; economy, trade and finance) and laid out research questions to guide collection of the most pertinent \"data required.\" The questions proposed for research reflected key issues of the time in American society: national debt and a national currency; taxation and revenue; labor and capital; hasty and excessive legislation; crime and punishment; the province of law in regard to education, public health, and social morals; education of neglected and vicious children; relative value of classical and scientific instruction in schools and colleges; fine arts in education and industry; half-time system of instruction; quarantine considered in its relation to cholera; the tenement house; inspection of food and drugs; pork as an article of food; sewerage of great cities; and management of hospitals and insane asylums.\"\n\nMeetings took place in Boston at the State House (Oct. 1865) and the Lowell Institute (Dec. 1865); and in New York at the YMCA on 5th Ave. (Nov. 1867). In 1866 the group joined with the \"Boston Social Science Association\" to form a joint committee called the \"American Social Science Association\" (ASSA); the committee met in Boston's city hall to discuss school reform and other matters. \"The first annual meeting of the ASSA ... was ... held [in] 1868.\"\n\n\n"}
{"id": "48440127", "url": "https://en.wikipedia.org/wiki?curid=48440127", "title": "Applied Biomathematics", "text": "Applied Biomathematics\n\nApplied Biomathematics is a private research and software firm in\nEast Setauket, New York, that conducts scientific research and\ndevelops scientific and statistical software for research and education.\nThe corporate offices are located in a historical district on Long Island, in the oldest settlement in Brookhaven Town, about\none mile from Stony Brook University.\n\nApplied Biomathematics translates theoretical concepts from biology and the\nphysical sciences into mathematical\nand statistical methods to quantitatively solve practical environmental, health,\nand engineering problems. AB offers software products for use in\nconservation biology, resource management, healthcare,\necology, and various engineering disciplines such as risk analysis,\nuncertainty quantification, reliability assessments,\nviability analysis, and\nsurvival analysis.\nThe methods and RAMAS brand software products\ndeveloped by AB are used by hundreds of academic institutions around\nthe world, government agencies, and industrial and private\nlabs.\n\nApplied Biomathematics is funded primarily by research grants and contracts from the U.S. government and private industry associations.\nAB has received several grants from the Small Business Innovation Research\nprogram, including awards from the National Institutes of Health, United States Department of Agriculture, NASA, National Science Foundation, and the Nuclear Regulatory Commission.\nOther project funding has come from the Electric Power Research Institute and individual utility companies, healthcare, pharmaceutical and seed companies such as Pfizer, DuPont and Dow. The company has provided risk analysis operations to the U.S. Army Corps of Engineers.\nAbout 10% of its revenues come from software sales and licensing.\n\nApplied Biomathematics is a registered service mark and RAMAS is a registered trademark of Applied Biomathematics, Inc.\n\n"}
{"id": "159191", "url": "https://en.wikipedia.org/wiki?curid=159191", "title": "Arecibo Observatory", "text": "Arecibo Observatory\n\nThe Arecibo Observatory is a radio telescope in the municipality of Arecibo, Puerto Rico. This observatory is operated by University of Central Florida, Yang Enterprises and UMET, under cooperative agreement with the National Science Foundation (NSF). The observatory is the sole facility of the National Astronomy and Ionosphere Center (NAIC), which is the formal name of the observatory. From its construction in the 1960s until 2011, the observatory was managed by Cornell University.\n\nFor more than 50 years, the observatory's radio telescope was the world's largest single-aperture telescope, from its completion in 1963 until July 2016 when the Five hundred meter Aperture Spherical Telescope (FAST) in China was completed. It is used in three major areas of research: radio astronomy, atmospheric science, and radar astronomy. Scientists who want to use the observatory submit proposals that are evaluated by an independent scientific board.\n\nThe observatory has appeared in film, gaming and television productions, gaining more recognition in 1999 when it began to collect data for the SETI@home project. It has been listed on the American National Register of Historic Places starting in 2008. It was the featured listing in the National Park Service's weekly list of October 3, 2008. The center was named an IEEE Milestone in 2001. It has a visitor center that is open part-time.\n\nOn September 21, 2017, high winds associated with Hurricane Maria caused the 430 MHz line feed to break and fall onto the primary dish, damaging about 30 out of 38,000 aluminum panels. Most Arecibo observations do not use the line feed but instead rely on the feeds and receivers located in the dome. Overall, the damage inflicted by Maria was minimal.\n\nThe main collecting dish is in diameter, constructed inside the depression left by a karst sinkhole. The dish surface is made of 38,778 perforated aluminum panels, each about , supported by a mesh of steel cables. The ground beneath is accessible and supports shade-tolerant vegetation.\n\nThe observatory has four radar transmitters, with effective isotropic radiated powers of 20 TW (continuous) at 2380 MHz, 2.5 TW (pulse peak) at 430 MHz, 300 MW at 47 MHz, and 6 MW at 8 MHz.\n\nThe reflector is a spherical reflector, not a parabolic reflector. To aim the device, the receiver is moved to intercept signals reflected from different directions by the spherical dish surface of radius. A parabolic mirror would have varying astigmatism when the receiver is off the focal point, but the error of a spherical mirror is uniform in every direction.\n\nThe receiver is on a 900-ton platform suspended above the dish by 18 cables running from three reinforced concrete towers, one high and the other two high, placing their tops at the same elevation. The platform has a rotating, bow-shaped track long, called the azimuth arm, carrying the receiving antennas and secondary and tertiary reflectors. This allows the telescope to observe any region of the sky in a forty-degree cone of visibility about the local zenith (between −1 and 38 degrees of declination). Puerto Rico's location near the Northern Tropic allows Arecibo to view the planets in the Solar System over the Northern half of their orbit. The round trip light time to objects beyond Saturn is longer than the 2.6 hour time that the telescope can track a celestial position, preventing radar observations of more distant objects.\n\nThe origins of the observatory trace to late 1950s efforts to develop anti-ballistic missile (ABM) defences as part of the newly formed ARPA's ABM umbrella-effort, Project Defender. Even at this early stage it was clear that the use of radar decoys would be a serious problem at the long ranges needed to successfully attack a warhead, ranges on the order of .\n\nAmong the many Defender projects were several studies based on the concept that a re-entering nuclear warhead would cause unique physical signatures while still in the upper atmosphere. It was known that hot, high-speed objects caused ionization of the atmosphere that reflects radar waves, and it appeared that a warhead's signature would be different enough from decoys that a detector could pick out the warhead directly, or alternately, provide added information that would allow operators to focus a conventional tracking radar on the single return from the warhead.\n\nAlthough the concept appeared to offer a solution to the tracking problem, there was almost no information on either the physics of re-entry or a strong understanding of the normal composition of the upper layers of the ionosphere. ARPA began to address both simultaneously. To better understand the radar returns from a warhead, several radars were built on Kwajalein Atoll, while Arecibo started with the dual purpose of understanding the ionosphere's F-layer while also producing a general-purpose scientific radio observatory.\n\nThe observatory was built between mid-1960 and November 1963. William E. Gordon of Cornell University oversaw its design, who intended to use it to study the Earth's ionosphere. He was attracted to the sinkholes in the karst regions of Puerto Rico that offered perfect cavities for a very large dish. Originally, a fixed parabolic reflector was envisioned, pointing in a fixed direction with a tower to hold equipment at the focus. This design would have limited its use in other research areas, such as radar astronomy, radio astronomy and atmospheric science, which require the ability to point at different positions in the sky and track those positions for an extended time as Earth rotates. Ward Low of the Advanced Research Projects Agency (ARPA) pointed out this flaw and put Gordon in touch with the Air Force Cambridge Research Laboratory (AFCRL) in Boston, Massachusetts, where one group headed by Phil Blacksmith was working on spherical reflectors and another group was studying the propagation of radio waves in and through the upper atmosphere. Cornell University proposed the project to ARPA in mid-1958 and a contract was signed between the AFCRL and the University in November 1959. Cornell University and Zachary Sears published a request for proposals (RFP) asking for a design to support a feed moving along a spherical surface above the stationary reflector. The RFP suggested a tripod or a tower in the center to support the feed. On the day the project for the design and construction of the antenna was announced at Cornell University, Gordon had also envisioned a tower centered in the reflector to support the feed.\n\nGeorge Doundoulakis, who directed research at General Bronze Corporation in Garden City, New York, along with Zachary Sears, who directed Internal Design at Digital B & E Corporation, New York, received the RFP from Cornell University for the antenna design and studied the idea of suspending the feed with his brother, Helias Doundoulakis, a civil engineer. George Doundoulakis identified the problem that a tower or tripod would have presented around the center, (the most important area of the reflector), and devised a better design by suspending the feed. He presented his proposal to Cornell University for a doughnut or torus-type truss suspended by four cables from four towers above the reflector, having along its edge a rail track for the azimuthal truss positioning. This second truss, in the form of an arc, or arch, was to be suspended below, which would rotate on the rails through 360 degrees. The arc also had rails on which the unit supporting the feed would move for the feed's elevational positioning. A counterweight would move symmetrically opposite to the feed for stability and, if a hurricane struck, the whole feed could be raised and lowered. Helias Doundoulakis designed the cable suspension system which was finally adopted. Although the present configuration is substantially the same as the original drawings by George and Helias Doundoulakis, (although with three towers, instead of the original four as drawn in the original patent), the U.S. Patent office granted Helias Doundoulakis a patent, for the brothers' innovative idea. Two other assignees on the patent were friends William J. Casey, who later became director of the Central Intelligence Agency under President Ronald Reagan, and Constantine Michalos, an attorney.\n\nThe idea of a spherical reflecting mirror with a steerable secondary has since been used in optical telescopes, in particular, the Hobby–Eberly Telescope and the Southern African Large Telescope.\n\nConstruction began in mid-1960, with the official opening on November 1, 1963. As the primary dish is spherical, its focus is along a line rather than at one point, as would be the case for a parabolic reflector. As a result, complex line feeds were implemented to carry out observations. Each line feed covered a narrow frequency band: 2–5% of the center frequency of the band. A limited number of line feeds could be used at any one time, limiting the telescope's flexibility.\n\nSince then, the telescope has been upgraded several times. Initially, when the maximum expected operating frequency was about 500 MHz, the surface consisted of half-inch galvanized wire mesh laid directly on the support cables. In 1974, a high-precision surface consisting of 40,000 individually adjustable aluminum panels replaced the old wire mesh, and the highest usable frequency rose to about 5000 MHz. A Gregorian reflector system was installed in 1997, incorporating secondary and tertiary reflectors to focus radio waves at one point. This allowed installing a suite of receivers, covering the full 1–10 GHz range, that could be easily moved to the focal point, giving Arecibo more flexibility. A metal mesh screen was also installed around the perimeter to block the ground's thermal radiation from reaching the feed antennas. Finally, a more powerful 2400 MHz transmitter was added.\n\nMany scientific discoveries have been made with the observatory. On April 7, 1964, soon after it began operating, Gordon Pettengill's team used it to determine that the rotation period of Mercury was not 88 days, as formerly thought, but only 59 days. In 1968, the discovery of the periodicity of the Crab Pulsar (33 milliseconds) by Lovelace and others provided the first solid evidence that neutron stars exist. In 1974, Hulse and Taylor discovered the first binary pulsar PSR B1913+16, an accomplishment for which they later received the Nobel Prize in Physics. In 1982, the first millisecond pulsar, PSR B1937+21, was discovered by Donald C. Backer, Shrinivas Kulkarni, Carl Heiles, Michael Davis, and Miller Goss. This object spins 642 times per second and, until the discovery of PSR J1748-2446ad in 2005, was identified as the fastest-spinning pulsar.\n\nIn August 1989, the observatory directly imaged an asteroid for the first time in history: 4769 Castalia. The following year, Polish astronomer Aleksander Wolszczan made the discovery of pulsar PSR B1257+12, which later led him to discover its three orbiting planets. These were the first extrasolar planets discovered. In 1994, John Harmon used the Arecibo Radio Telescope to map the distribution of ice in the polar regions of Mercury.\n\nIn January 2008, detection of prebiotic molecules methanimine and hydrogen cyanide were reported from the observatory's radio spectroscopy measurements of the distant starburst galaxy Arp 220.\n\nFrom January 2010 to February 2011, American astronomers Matthew Route and Aleksander Wolszczan detected bursts of radio emission from the T6.5 brown dwarf 2MASS J10475385+2124234. This was the first time that radio emission had been detected from a T dwarf, which has methane absorption lines in its atmosphere. It is also the coolest brown dwarf (at a temperature of ~900K) from which radio emission has been observed. The highly polarized and highly energetic radio bursts indicated that the object has a >1.7 kG-strength magnetic field and magnetic activity similar to both the planet Jupiter and the Sun.\n\nIn 1974, the Arecibo Message, an attempt to communicate with potential extraterrestrial life, was transmitted from the radio telescope toward the globular cluster Messier 13, about 25,000 light-years away. The 1,679 bit pattern of 1s and 0s defined a 23 by 73 pixel bitmap image that included numbers, stick figures, chemical formulas and a crude image of the telescope.\n\nSearch for Extra-Terrestrial Intelligence (SETI) is the search for extraterrestrial life or advanced technologies. SETI aims to answer the question \"Are we alone in the Universe?\" by scanning the skies for transmissions from intelligent civilizations elsewhere in our galaxy.\nIn comparison, METI (Messaging to Extra-Terrestrial Intelligence) refers to the active search by transmitting messages.\n\nArecibo is the source of data for the SETI@home and Astropulse distributed computing projects put forward by the Space Sciences Laboratory at the University of California, Berkeley and was used for the SETI Institute's Project Phoenix observations. The Einstein@Home distributed computing project has found more than 20 pulsars in Arecibo data.\n\nTerrestrial aeronomy experiments at Arecibo have included the Coqui 2 experiment, supported by NASA. The telescope also has military intelligence uses, some of which include locating Soviet radar installations by detecting their signals bouncing off the Moon.\n\nLimited amateur radio operations have occurred, using \"moon bounce\" or Earth–Moon–Earth communication, in which radio signals aimed at the Moon are reflected back to Earth. The first of these operations was on June 13–14, 1964, using the call KP4BPZ. A dozen or so two-way contacts were made on 144 and 432 MHz. On July 3 and July 24, 1965, KP4BPZ was again activated on 432 MHz, making approximately 30 contacts on 432 MHz during the limited time slots available. For these tests, a very wide-band instrumentation recorder captured a large segment of the receiving bandwidth, enabling later verification of other amateur station callsigns. These were not two-way contacts. From April 16–18, 2010, again, the Arecibo Amateur Radio Club KP4AO conducted moon-bounce activity using the antenna.\nOn November 10, 2013, the KP4AO Arecibo Amateur Radio Club conducted a Fifty-Year Commemoration Activation, lasting 7 hours on 14.250 MHz SSB, without using the main dish antenna.\n\nSince the early 1970s, the Arecibo Observatory has been supported by the NSF (National Science Foundation divisions of Astronomical Sciences and of Atmospheric Sciences) with incremental support by NASA, for operating the planetary radar. Between 2001 and 2006, NASA decreased, then eliminated, its support of the planetary radar,\nbut restored and increased the funding in FY-2010.\n\nA report by the NSF division of Astronomical Sciences, made public on November 3, 2006, recommended substantially decreased astronomy funding for the Arecibo Observatory, from $10.5 million in 2007 to $4.0 million in 2011. If other sources of money could not be obtained, the observatory would be forced to close. The report also advised that 80 percent of observing time be allocated to the surveys already in progress, reducing available time for smaller programs.\n\nAcademics and researchers responded by organizing to protect and advocate for the observatory. They established the Arecibo Science Advocacy Partnership (ASAP), to advance the scientific excellence of Arecibo Observatory research and to publicize its accomplishments in astronomy, aeronomy and planetary radar. ASAP's goals included mobilizing the existing broad base of support for Arecibo science within the fields it serves directly, the broad scientific community; provide a forum for the Arecibo research community and enhance communication within it; promote the potential of Arecibo for groundbreaking science; suggest paths that will maximize it into the foreseeable future, and showcase the broad impact and far-reaching implications of the science currently carried out with this unique instrument.\n\nContributions by the government of Puerto Rico may be one way to help fill the funding gap, but remain controversial and uncertain. At town hall meetings about the potential closure, Puerto Rican Senate President Kenneth McClintock announced an initial local appropriation of $3.0 million during fiscal year 2008 to fund a major maintenance project to restore the three pillars that support the antenna platform to their original condition, pending inclusion in the next bond issue. The bond authorization, with a $3.0 million appropriation, was approved by the Senate of Puerto Rico on November 14, 2007, on the first day of a special session called by Aníbal Acevedo Vilá. The Puerto Rico House of Representatives repeated this action on June 30, 2008. Puerto Rico's governor signed the measure into law in August 2008. These funds were made available during the second half of 2009.\n\nIn a letter published on September 19, 2007, José Enrique Serrano, a member of the U.S. House of Representatives Appropriations Committee, asked the National Science Foundation to keep Arecibo operating.\n\nLanguage similar to that of the letter of September 19 was included in the FY-2008 omnibus spending bill. In October 2007, Puerto Rico's then-Resident Commissioner, Luis Fortuño, along with Dana Rohrabacher, filed legislation to assure the continued operation of the famed observatory. A similar bill was filed in the U.S. Senate in April 2008 by the Junior Senator from New York, Hillary Clinton.\n\nIn September 2007, in an open letter to researchers, the NSF clarified the status of the budget for NAIC, stating the present plan could hit the targeted budgetary revision. No mention of private funding was made. However, in the event that its budget target is not reached, it must be noted that the NSF is undertaking studies to mothball or demolish the observatory to return it to its natural setting.\n\nIn November 2007, The Planetary Society urged the U.S. Congress to prevent the Arecibo Observatory from closing because of insufficient funding, since its radar contributes greatly to the accuracy of predictions of asteroid impacts on the Earth. The Planetary Society believes that continued operation of the observatory will reduce the cost of mitigation (that is, deflection of a near-Earth asteroid on collision to Earth), should that be necessary.\n\nAlso in November of that year \"The New York Times\" described the consequences of the budget cuts at the site. In July 2008, the British newspaper \"The Daily Telegraph\" reported that the funding crisis, due to federal budget cuts, was still very much alive.\n\nThe SETI@home program is using the telescope as a primary source for ET research. The program urges people to send a letter to their political representatives in support of full federal funding of the observatory.\n\nThe NAIC received $3.1 million from the American Recovery and Reinvestment Act of 2009. This was used for basic maintenance and for a second, much smaller, antenna to be used for very long baseline interferometry, new Klystron amplifiers for the planetary radar system and student training. This allotment was an increase of about 30 percent over the FY-2009 budget. However, the FY-2010 funding request by NSF was cut by $1.2 million (−12.5%) over the FY-2009 budget, in light of their continued plans to reduce funding.\n\nThe 2011 NSF budget was reduced by a further $1.6 million, −15% compared to 2010, with a further $1.0 million reduction projected by FY-2014. Starting in FY-2010, NASA restored its historical support by contributing $2.0 million per year for planetary science, particularly the study of near-Earth objects, at Arecibo. NASA implemented this funding through its Near Earth Object Observations program. NASA increased its support to $3.5 million per year in 2012.\n\nFurthermore, in 2010 the NSF issued a call for new proposals for the management of NAIC starting in FY-2012. On May 12, 2011, the agency informed Cornell University that, as of October 1, 2011, it would no longer be the operator of the NAIC and the Arecibo Observatory. At that time, Cornell transferred its operations to SRI International, along with two other managing partners, Universities Space Research Association and Universidad Metropolitana de Puerto Rico, with a number of other collaborators. Upon the award of the new cooperative agreement for NAIC management and operation, NSF also decertified NAIC as a Federally Funded Research and Development Center (FFRDC), with the stated goal of providing the NAIC with greater freedom to establish broader scientific partnerships and pursue funding opportunities for activities beyond the scope of those supported by NSF.\n\nIn October 2015, the NSF released a \"Dear Colleague Letter\" reiterating its desire for a \"substantially reduced funding commitment from NSF\".\n\nOn September 30, 2016, the NSF released a followup to the October 2015 \"Dear Colleague Letter\" announcing a solicitation for future operation of the Observatory stating \"The subject Solicitation will request the submission of formal proposals involving the continued operation of Arecibo Observatory under conditions of a substantially reduced funding commitment from NSF.\"\n\nThe damage sustained from Hurricane Maria in September 2017 further clouded the observatory's future. Although the damage was minimal, restoring all the previous capabilities required more than the observatory's already-threatened operating budget, and users feared the decision would be made to decommission it instead. However, it was announced in February 2018 that a consortium led by the University of Central Florida will allow the NSF to reduce its contribution towards Arecibo's operating costs from $8 million to $2 million from the fiscal year 2022–2023, with the shortfall made up by the consortium partners, thus securing the observatory's future.\n\nOpened in 1997, the Ángel Ramos Foundation Visitor Center features interactive exhibits and displays about the operations of the radio telescope, astronomy and atmospheric sciences. The center is named after the financial foundation that honors Ángel Ramos, owner of the \"El Mundo\" newspaper and founder of Telemundo. The Foundation provided half of the funds to build the Visitor Center, with the remainder received from private donations and Cornell University.\n\nThe center, in collaboration with the Caribbean Astronomical Society, host a series of Astronomical Nights throughout the year, which feature diverse discussions regarding exoplanets, and astronomical phenomena and discoveries (such as Comet ISON). The main purpose of the center is to increase public interest in astronomy, the observatory's research successes, and space endeavors.\n\n\nDue to its unique shape and concept the observatory is featured in many movies, video games and novels.\n\n\n\n"}
{"id": "40563740", "url": "https://en.wikipedia.org/wiki?curid=40563740", "title": "Ballas", "text": "Ballas\n\nBallas or shot bort is a term used in the diamond industry to refer to shards of non-gem-grade/quality diamonds. It comprises small diamond crystals that are concentrically arranged in rough spherical stones with a fibrous texture. Ballas is hard, tough, and difficult to cleave. It is mostly found in Brazil and South Africa.\n"}
{"id": "24960500", "url": "https://en.wikipedia.org/wiki?curid=24960500", "title": "Bibliomining", "text": "Bibliomining\n\nBibliomining is the use of a combination of data mining, data warehousing, and bibliometrics for the purpose of analyzing library services. The term was created in 2003 by Scott Nicholson, Assistant Professor, Syracuse University School of Information Studies, in order to distinguish data mining in a library setting from other types of data mining.\n\nFirst a data warehouse must be created. This is done by compiling information on the resources, such as titles and authors, subject headings, and descriptions of the collections. Then the demographic surrogate information is organized. Finally the library information (such as the librarian, whether or not the information came from the reference desk or circulation desk, and the location of the library) is obtained. \n\nOnce this is organized, the data can be processed and analyzed. This can be done via a few methods, such as online analytical processing (OLAP), using a data mining program, or through data visualization.\n\nBibliomining is used to discover patterns in what people are reading and researching and allows librarians to target their community better. Bibliomining can also help library directors focus their budgets on resources that will be utilized. Another use is to determine when people use the library more often, so staffing needs can be adequately met. Combining bibliomining with other research techniques such as focus groups, surveys and cost-benefit analysis, will help librarians to get a better picture of their patrons and their needs.\n\nThere is some concern that data mining violates patron privacy. But by extracting the data, all personally identifiable information is deleted, and the data warehouse is clean. The original patron data can then be totally deleted and there will be no way to link the new data to a particular patron. This can be done in a few ways. One, used with information regarding database access, is to track the IP address, but then replace it with a similar code, that will allow identification without violating privacy. Another is to keep track of an item returned to the library and create a \"demographic surrogate\" of the patron. The demographic surrogate would not give any identifiable information such as names, library card numbers or addresses.\n\nThe other concern in bibliomining is that it only provides data in a very detached manner. Information is given as to how a patron uses library resources, but there is no way to track if the resources met the user's needs completely. Someone could take out a book on a topic, but not find the information they were seeking. Bibliomining only helps identify which books are used, not how useful they actually were. Bibliomining cannot provide information on how well a collection serves a patron. In order to counteract this, bibliomining must be used in accordance with other research techniques.\n\n1. Nicholson, S. (2006). The Basis for Bibliomining: Frameworks for Bringing Together Usage-Based Data Mining and Bibliometrics through Data Warehousing in Digital Library Services. \"Information Processing and Management\" 42(3), 785-804.\n\n2. Nicholson, S. (2003). The Bibliomining Process: Data Warehousing and Data Mining for Library Decision-Making \"Information Technology and Libraries\" 22 (4), 146-151.\n\n3. Jiann-Cherng, S. (2009). \"The Integration System for Librarians' Bibliomining\", Asia-Pacific Conference on Library & Information Education & Practice.\n\n4. Gunther, K. (2000). Applying data mining principles to a library data collection — Data mining can help you make decisions and serve patrons better. \"Computers in Libraries\" 20(4), 60-63.\n"}
{"id": "4472724", "url": "https://en.wikipedia.org/wiki?curid=4472724", "title": "Broca's Brain", "text": "Broca's Brain\n\nBroca's Brain: Reflections on the Romance of Science is a 1979 book by astrophysicist Carl Sagan. Its chapters were originally articles published between 1974 and 1979 in various magazines, including \"The Atlantic Monthly\", \"The New Republic\", \"Physics Today\", \"Playboy\" and \"Scientific American\". In the introduction, Sagan wrote:\n\nThe title essay is named in honor of the French physician, anatomist and anthropologist, Paul Broca (1824–1880). He is best known for his discovery that different functions are assigned to different parts of the brain. He believed that by studying the brains of cadavers and correlating the known experiences of the former owners of the organs, human behavior could eventually be discovered and understood. To that end, he saved hundreds of human brains in jars of formalin; among the collection is his own neural organ. When Sagan finds it in Musée de l'Homme, he poses questions that challenge some core ideas of human existence such as \"How much of that man known as Paul Broca can still be found in this jar?\"—a question that evokes both religious and scientific argument.\n\nA major part of the book is devoted to debunking \"paradoxers\" who either live at the edge of science or are outright charlatans. An example of this is the controversy surrounding Immanuel Velikovsky's ideas presented in the book \"Worlds in Collision\". Another large part of the book discusses naming conventions for the members of our solar system and their physical features. Sagan also discusses Science fiction at some length. Here, he mentions Robert A. Heinlein as being one of his favorite science fiction authors in his childhood. Near death experiences and their cultural ambiguity is another topic of the essays. Sagan also criticizes ideas developed in Robert K. G. Temple's book \"The Sirius Mystery\", published three years earlier in 1975.\n\nIn the final section of the book, \"Ultimate Questions.\" Sagan writes:\n\n"}
{"id": "58400340", "url": "https://en.wikipedia.org/wiki?curid=58400340", "title": "CNS (chemical weapon)", "text": "CNS (chemical weapon)\n\nCNS is a mixture of chloroacetophenone, chloropicrin and chloroform that is used as a chemical warfare agent. CNS has the lachrymatory effects of chloroacetophenone and choking effects of chloropicrin. It has a flypaper-like odor.\n\nCNS was used as a riot control agent, but it's no longer used now.\n"}
{"id": "25021648", "url": "https://en.wikipedia.org/wiki?curid=25021648", "title": "Charles M. Wheatley", "text": "Charles M. Wheatley\n\nCharles Moore Wheatley (16 March 1822 Ongar, England – 6 May 1882 Phoenixville, Pennsylvania) was a noted English-American miner and palaeontologist of the 19th century. He is noted for identifying several new fossilized species, some of which bear his name, and for his connection to the Port Kennedy Bone Cave, which contained one of the most important middle Pleistocene (Irvingtonian, approximately 750,000 years ago) fossil deposits in North America.\n\nHe also managed successful mines in Connecticut and Pennsylvania, including a lead mine in Phoenixville, Pennsylvania.\n\n\n"}
{"id": "7463", "url": "https://en.wikipedia.org/wiki?curid=7463", "title": "Cold fusion", "text": "Cold fusion\n\nCold fusion is a hypothesized type of nuclear reaction that would occur at, or near, room temperature. This is compared with the \"hot\" fusion which takes place naturally within stars, under immense pressure and at temperatures of millions of degrees, and distinguished from muon-catalyzed fusion. There is currently no accepted theoretical model that would allow cold fusion to occur.\n\nIn 1989 Martin Fleischmann (then one of the world's leading electrochemists) and Stanley Pons reported that their apparatus had produced anomalous heat (\"excess heat\") of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode. The reported results received wide media attention, and raised hopes of a cheap and abundant source of energy.\n\nMany scientists tried to replicate the experiment with the few details available. Hopes faded due to the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. In 1989 the United States Department of Energy (DOE) concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004, which looked at new research, reached similar conclusions and did not result in DOE funding of cold fusion.\n\nA small community of researchers continues to investigate cold fusion, now often preferring the designation low-energy nuclear reactions (LENR) or condensed matter nuclear science (CMNS). Since cold fusion articles are rarely published in peer-reviewed mainstream scientific journals, they do not attract the level of scrutiny expected for mainstream scientific publications.\nNuclear fusion is normally understood to occur at temperatures in the tens of millions of degrees. Since the 1920s, there has been speculation that nuclear fusion might be possible at much lower temperatures by catalytically fusing hydrogen absorbed in a metal catalyst. In 1989, a claim by Stanley Pons and Martin Fleischmann (then one of the world's leading electrochemists) that such cold fusion had been observed caused a brief media sensation before the majority of scientists criticized their claim as incorrect after many found they could not replicate the excess heat. Since the initial announcement, cold fusion research has continued by a small community of researchers who believe that such reactions happen and hope to gain wider recognition for their experimental evidence.\n\nThe ability of palladium to absorb hydrogen was recognized as early as the nineteenth century by Thomas Graham. In the late 1920s, two Austrian born scientists, Friedrich Paneth and Kurt Peters, originally reported the transformation of hydrogen into helium by nuclear catalysis when hydrogen was absorbed by finely divided palladium at room temperature. However, the authors later retracted that report, saying that the helium they measured was due to background from the air.\n\nIn 1927 Swedish scientist John Tandberg reported that he had fused hydrogen into helium in an electrolytic cell with palladium electrodes. On the basis of his work, he applied for a Swedish patent for \"a method to produce helium and useful reaction energy\". Due to Paneth and Peters's retraction and his inability to explain the physical process, his patent application was denied. After deuterium was discovered in 1932, Tandberg continued his experiments with heavy water. The final experiments made by Tandberg with heavy water were similar to the original experiment by Fleischmann and Pons. Fleischmann and Pons were not aware of Tandberg's work.\n\nThe term \"cold fusion\" was used as early as 1956 in a \"New York Times\" article about Luis Alvarez's work on muon-catalyzed fusion. Paul Palmer and then Steven Jones of Brigham Young University used the term \"cold fusion\" in 1986 in an investigation of \"geo-fusion\", the possible existence of fusion involving hydrogen isotopes in a planetary core. In his original paper on this subject with Clinton Van Siclen, submitted in 1985, Jones had coined the term \"piezonuclear fusion\".\n\nThe most famous cold fusion claims were made by Stanley Pons and Martin Fleischmann in 1989. After a brief period of interest by the wider scientific community, their reports were called into question by nuclear physicists. Pons and Fleischmann never retracted their claims, but moved their research program to France after the controversy erupted.\n\nMartin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.\n\nIn 1988 Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled \"Cold nuclear fusion\" that had been published in \"Scientific American\" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable \"excess energy\", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to simultaneously publish their results, though their accounts of their 6 March meeting differ.\n\nIn mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on 24 March to send their papers to \"Nature\" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, submitting their paper to the \"Journal of Electroanalytical Chemistry\" on 11 March, and disclosing their work via a press release and press conference on 23 March. Jones, upset, faxed in his paper to \"Nature\" after the press conference.\n\nFleischmann and Pons' announcement drew wide media attention. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established theories. Many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.\n\nThe announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Chase N. Peterson, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems, and would provide a limitless inexhaustible source of clean energy, using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: \"What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics.\"\n\nAlthough the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to \"Nature\" reproducing excess heat, although it passed peer-review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal \"Fusion Technology\". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that \"essentially all\" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On 10 April 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. On 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement on 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as \"cold fusion\" or \"fusion confusion\" in the news.\n\nIn April 1989, Fleischmann and Pons published a \"preliminary note\" in the \"Journal of Electroanalytical Chemistry\". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.\n\nNevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.\n\nOn 30 April 1989 cold fusion was declared dead by the \"New York Times\". The \"Times\" called it a circus the same day, and the \"Boston Herald\" attacked cold fusion the following day.\n\nOn 1 May 1989 the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of \"the incompetence and delusion of Pons and Fleischmann,\" which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.\n\nOn 4 May, due to all this new criticism, the meetings with various representatives from Washington were cancelled.\n\nFrom 8 May only the A&M tritium results kept cold fusion afloat.\n\nIn July and November 1989, \"Nature\" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including \"Science\", \"Physical Review Letters\", and \"Physical Review C\" (nuclear physics).\n\nIn August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.\n\nThe United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of \"focused experiments within the general funding system.\" Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 different countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of \"pathological science\".\n\nIn March 1990 Michael H. Salamon, a physicist from the University of Utah, and nine co-authors reported negative results. University faculty were then \"stunned\" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.\n\nIn early May 1990 one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in \"Science\" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.\n\nOn 30 June 1991 the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.\n\nOn 1 January 1991 Pons left the University of Utah and went to Europe. In 1992, Pons and Fleischman resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million. Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.\n\nMostly in the 1990s, several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years, several books have appeared that defended them. Around 1998, the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997, Japan cut off research and closed its own lab after spending $20 million.\n\nA 1991 review by a cold fusion proponent had calculated \"about 600 scientists\" were still conducting research. After 1991, cold fusion research only continued in relative obscurity, conducted by groups that had increasing difficulty securing public funding and keeping programs open. These small but committed groups of cold fusion researchers have continued to conduct experiments using Fleischmann and Pons electrolysis set-ups in spite of the rejection by the mainstream community. \"The Boston Globe\" estimated in 2004 that there were only 100 to 200 researchers working in the field, most suffering damage to their reputation and career. Since the main controversy over Pons and Fleischmann had ended, cold fusion research has been funded by private and small governmental scientific investment funds in the United States, Italy, Japan, and India.\n\nCold fusion research continues today in a few specific venues, but the wider scientific community has generally marginalized the research being done and researchers have had difficulty publishing in mainstream journals. The remaining researchers often term their field Low Energy Nuclear Reactions (LENR), Chemically Assisted Nuclear Reactions (CANR), Lattice Assisted Nuclear Reactions (LANR), Condensed Matter Nuclear Science (CMNS) or Lattice Enabled Nuclear Reactions; one of the reasons being to avoid the negative connotations associated with \"cold fusion\". The new names avoid making bold implications, like implying that fusion is actually occurring.\n\nThe researchers who continue acknowledge that the flaws in the original announcement are the main cause of the subject's marginalization, and they complain of a chronic lack of funding and no possibilities of getting their work published in the highest impact journals. University researchers are often unwilling to investigate cold fusion because they would be ridiculed by their colleagues and their professional careers would be at risk. In 1994, David Goodstein, a professor of physics at Caltech, advocated for increased attention from mainstream researchers and described cold fusion as:\n\nUnited States Navy researchers at the Space and Naval Warfare Systems Center (SPAWAR) in San Diego have been studying cold fusion since 1989. In 2002 they released a two-volume report, \"Thermal and nuclear aspects of the Pd/DO system,\" with a plea for funding. This and other published papers prompted a 2004 Department of Energy (DOE) review.\nIn August 2003, the U.S. Secretary of Energy, Spencer Abraham, ordered the DOE to organize a second review of the field. This was thanks to an April 2003 letter sent by MIT's Peter L. Hagelstein, and the publication of many new papers, including the Italian ENEA and other researchers in the 2003 International Cold Fusion Conference, and a two-volume book by U.S. SPAWAR in 2002. Cold fusion researchers were asked to present a review document of all the evidence since the 1989 review. The report was released in 2004. The reviewers were \"split approximately evenly\" on whether the experiments had produced energy in the form of heat, but \"most reviewers, even those who accepted the evidence for excess power production, 'stated that the effects are not repeatable, the magnitude of the effect has not increased in over a decade of work, and that many of the reported experiments were not well documented.'\" In summary, reviewers found that cold fusion evidence was still not convincing 15 years later, and they didn't recommend a federal research program. They only recommended that agencies consider funding individual well-thought studies in specific areas where research \"could be helpful in resolving some of the controversies in the field\". They summarized its conclusions thus:\n\nCold fusion researchers placed a \"rosier spin\" on the report, noting that they were finally being treated like normal scientists, and that the report had increased interest in the field and caused \"a huge upswing in interest in funding cold fusion research.\" However, in a 2009 BBC article on an American Chemical Society's meeting on cold fusion, particle physicist Frank Close was quoted stating that the problems that plagued the original cold fusion announcement were still happening: results from studies are still not being independently verified and inexplicable phenomena encountered are being labelled as \"cold fusion\" even if they are not, in order to attract the attention of journalists.\n\nIn February 2012, millionaire Sidney Kimmel, convinced that cold fusion was worth investing in by a 19 April 2009 interview with physicist Robert Duncan on the US news-show \"60 Minutes\", made a grant of $5.5 million to the University of Missouri to establish the Sidney Kimmel Institute for Nuclear Renaissance (SKINR). The grant was intended to support research into the interactions of hydrogen with palladium, nickel or platinum under extreme conditions. In March 2013 Graham K. Hubler, a nuclear physicist who worked for the Naval Research Laboratory for 40 years, was named director. One of the SKINR projects is to replicate a 1991 experiment in which a professor associated with the project, Mark Prelas says bursts of millions of neutrons a second were recorded, which was stopped because \"his research account had been frozen\". He claims that the new experiment has already seen \"neutron emissions at similar levels to the 1991 observation\".\n\nIn May 2016, the United States House Committee on Armed Services, in its report on the 2017 National Defense Authorization Act, directed the Secretary of Defense to \"provide a briefing on the military utility of recent U.S. industrial base LENR advancements to the House Committee on Armed Services by September 22, 2016.\"\n\nSince the Fleischmann and Pons announcement, the Italian National agency for new technologies, energy and sustainable economic development (ENEA) has funded Franco Scaramuzzi's research into whether excess heat can be measured from metals loaded with deuterium gas. Such research is distributed across ENEA departments, CNR laboratories, INFN, universities and industrial laboratories in Italy, where the group continues to try to achieve reliable reproducibility (i.e. getting the phenomenon to happen in every cell, and inside a certain frame of time). In 2006–2007, the ENEA started a research program which claimed to have found excess power of up to 500 percent, and in 2009, ENEA hosted the 15th cold fusion conference.\n\nBetween 1992 and 1997, Japan's Ministry of International Trade and Industry sponsored a \"New Hydrogen Energy (NHE)\" program of US$20 million to research cold fusion. Announcing the end of the program in 1997, the director and one-time proponent of cold fusion research Hideo Ikegami stated \"We couldn't achieve what was first claimed in terms of cold fusion. (...) We can't find any reason to propose more money for the coming year or for the future.\" In 1999 the Japan C-F Research Society was established to promote the independent research into cold fusion that continued in Japan. The society holds annual meetings. Perhaps the most famous Japanese cold fusion researcher is Yoshiaki Arata, from Osaka University, who claimed in a demonstration to produce excess heat when deuterium gas was introduced into a cell containing a mixture of palladium and zirconium oxide, a claim supported by fellow Japanese researcher Akira Kitamura of Kobe University and McKubre at SRI.\n\nIn the 1990s India stopped its research in cold fusion at the Bhabha Atomic Research Centre because of the lack of consensus among mainstream scientists and the US denunciation of the research. Yet, in 2008, the National Institute of Advanced Studies recommended that the Indian government revive this research. Projects were commenced at the Chennai's Indian Institute of Technology, the Bhabha Atomic Research Centre and the Indira Gandhi Centre for Atomic Research. However, there is still skepticism among scientists and, for all practical purposes, research has stalled since the 1990s. A special section in the Indian multidisciplinary journal \"Current Science\" published 33 cold fusion papers in 2015 by major cold fusion researchers including several Indian researchers.\n\nA cold fusion experiment usually includes:\n\nElectrolysis cells can be either open cell or closed cell. In open cell systems, the electrolysis products, which are gaseous, are allowed to leave the cell. In closed cell experiments, the products are captured, for example by catalytically recombining the products in a separate part of the experimental system. These experiments generally strive for a steady state condition, with the electrolyte being replaced periodically. There are also \"heat-after-death\" experiments, where the evolution of heat is monitored after the electric current is turned off.\n\nThe most basic setup of a cold fusion cell consists of two electrodes submerged in a solution containing palladium and heavy water. The electrodes are then connected to a power source to transmit electricity from one electrode to the other through the solution. Even when anomalous heat is reported, it can take weeks for it to begin to appear—this is known as the \"loading time,\" the time required to saturate the palladium electrode with hydrogen (see \"Loading ratio\" section).\n\nThe Fleischmann and Pons early findings regarding helium, neutron radiation and tritium were never replicated satisfactorily, and its levels were too low for the claimed heat production and inconsistent with each other. Neutron radiation has been reported in cold fusion experiments at very low levels using different kinds of detectors, but levels were too low, close to background, and found too infrequently to provide useful information about possible nuclear processes.\n\nAn excess heat observation is based on an energy balance. Various sources of energy input and output are continuously measured. Under normal conditions, the energy input can be matched to the energy output to within experimental error. In experiments such as those run by Fleischmann and Pons, an electrolysis cell operating steadily at one temperature transitions to operating at a higher temperature with no increase in applied current. If the higher temperatures were real, and not an experimental artifact, the energy balance would show an unaccounted term. In the Fleischmann and Pons experiments, the rate of inferred excess heat generation was in the range of 10–20% of total input, though this could not be reliably replicated by most researchers. Researcher Nathan Lewis discovered that the excess heat in Fleischmann and Pons's original paper was not measured, but estimated from measurements that didn't have any excess heat.\n\nUnable to produce excess heat or neutrons, and with positive experiments being plagued by errors and giving disparate results, most researchers declared that heat production was not a real effect and ceased working on the experiments. In 1993, after their original report, Fleischmann reported \"heat-after-death\" experiments—where excess heat was measured after the electric current supplied to the electrolytic cell was turned off. This type of report has also become part of subsequent cold fusion claims.\n\nKnown instances of nuclear reactions, aside from producing energy, also produce nucleons and particles on readily observable ballistic trajectories. In support of their claim that nuclear reactions took place in their electrolytic cells, Fleischmann and Pons reported a neutron flux of 4,000 neutrons per second, as well as detection of tritium. The classical branching ratio for previously known fusion reactions that produce tritium would predict, with 1 watt of power, the production of 10 neutrons per second, levels that would have been fatal to the researchers. In 2009, Mosier-Boss et al. reported what they called the first scientific report of highly energetic neutrons, using CR-39 plastic radiation detectors, but the claims cannot be validated without a quantitative analysis of neutrons.\n\nSeveral medium and heavy elements like calcium, titanium, chromium, manganese, iron, cobalt, copper and zinc have been reported as detected by several researchers, like Tadahiko Mizuno or George Miley. The report presented to the United States Department of Energy (DOE) in 2004 indicated that deuterium-loaded foils could be used to detect fusion reaction products and, although the reviewers found the evidence presented to them as inconclusive, they indicated that those experiments did not use state-of-the-art techniques.\n\nIn response to doubts about the lack of nuclear products, cold fusion researchers have tried to capture and measure nuclear products correlated with excess heat. Considerable attention has been given to measuring He production. However, the reported levels are very near to background, so contamination by trace amounts of helium normally present in the air cannot be ruled out. In the report presented to the DOE in 2004, the reviewers' opinion was divided on the evidence for He; with the most negative reviews concluding that although the amounts detected were above background levels, they were very close to them and therefore could be caused by contamination from air.\n\nOne of the main criticisms of cold fusion was that deuteron-deuteron fusion into helium was expected to result in the production of gamma rays—which were not observed and were not observed in subsequent cold fusion experiments. Cold fusion researchers have since claimed to find X-rays, helium, neutrons and nuclear transmutations. Some researchers also claim to have found them using only light water and nickel cathodes. The 2004 DOE panel expressed concerns about the poor quality of the theoretical framework cold fusion proponents presented to account for the lack of gamma rays.\n\nResearchers in the field do not agree on a theory for cold fusion. One proposal considers that hydrogen and its isotopes can be absorbed in certain solids, including palladium hydride, at high densities. This creates a high partial pressure, reducing the average separation of hydrogen isotopes. However, the reduction in separation is not enough by a factor of ten to create the fusion rates claimed in the original experiment. It was also proposed that a higher density of hydrogen inside the palladium and a lower potential barrier could raise the possibility of fusion at lower temperatures than expected from a simple application of Coulomb's law. Electron screening of the positive hydrogen nuclei by the negative electrons in the palladium lattice was suggested to the 2004 DOE commission, but the panel found the theoretical explanations not convincing and inconsistent with current physics theories.\n\nCriticism of cold fusion claims generally take one of two forms: either pointing out the theoretical implausibility that fusion reactions have occurred in electrolysis set-ups or criticizing the excess heat measurements as being spurious, erroneous, or due to poor methodology or controls. There are a couple of reasons why known fusion reactions are an unlikely explanation for the excess heat and associated cold fusion claims.\n\nBecause nuclei are all positively charged, they strongly repel one another. Normally, in the absence of a catalyst such as a muon, very high kinetic energies are required to overcome this charged repulsion. Extrapolating from known fusion rates, the rate for uncatalyzed fusion at room-temperature energy would be 50 orders of magnitude lower than needed to account for the reported excess heat. In muon-catalyzed fusion there are more fusions because the presence of the muon causes deuterium nuclei to be 207 times closer than in ordinary deuterium gas. But deuterium nuclei inside a palladium lattice are further apart than in deuterium gas, and there should be fewer fusion reactions, not more.\n\nPaneth and Peters in the 1920s already knew that palladium can absorb up to 900 times its own volume of hydrogen gas, storing it at several thousands of times the atmospheric pressure. This led them to believe that they could increase the nuclear fusion rate by simply loading palladium rods with hydrogen gas. Tandberg then tried the same experiment but used electrolysis to make palladium absorb more deuterium and force the deuterium further together inside the rods, thus anticipating the main elements of Fleischmann and Pons' experiment. They all hoped that pairs of hydrogen nuclei would fuse together to form helium, which at the time was needed in Germany to fill zeppelins, but no evidence of helium or of increased fusion rate was ever found.\n\nThis was also the belief of geologist Palmer, who convinced Steven Jones that the helium-3 occurring naturally in Earth perhaps came from fusion involving hydrogen isotopes inside catalysts like nickel and palladium. This led their team in 1986 to independently make the same experimental setup as Fleischmann and Pons (a palladium cathode submerged in heavy water, absorbing deuterium via electrolysis). Fleischmann and Pons had much the same belief, but they calculated the pressure to be of 10 atmospheres, when cold fusion experiments only achieve a loading ratio of one to one, which only has between 10,000 and 20,000 atmospheres. John R. Huizenga says they had misinterpreted the Nernst equation, leading them to believe that there was enough pressure to bring deuterons so close to each other that there would be spontaneous fusions.\n\nConventional deuteron fusion is a two-step process, in which an unstable high energy intermediary is formed:\nExperiments have observed only three decay pathways for this excited-state nucleus, with the branching ratio showing the probability that any given intermediate follows a particular pathway. The products formed via these decay pathways are:\nOnly about one in one million of the intermediaries decay along the third pathway, making its products comparatively rare when compared to the other paths. This result is consistent with the predictions of the Bohr model. If one watt (1 eV = 1.602 x 10 joule) of nuclear power were produced from deuteron fusion consistent with known branching ratios, the resulting neutron and tritium (H) production would be easily measured. Some researchers reported detecting He but without the expected neutron or tritium production; such a result would require branching ratios strongly favouring the third pathway, with the actual rates of the first two pathways lower by at least five orders of magnitude than observations from other experiments, directly contradicting both theoretically predicted and observed branching probabilities. Those reports of He production did not include detection of gamma rays, which would require the third pathway to have been changed somehow so that gamma rays are no longer emitted.\n\nThe known rate of the decay process together with the inter-atomic spacing in a metallic crystal makes heat transfer of the 24 MeV excess energy into the host metal lattice prior to the intermediary's decay inexplicable in terms of conventional understandings of momentum and energy transfer, and even then there would be measurable levels of radiation. Also, experiments indicate that the ratios of deuterium fusion remain constant at different energies. In general, pressure and chemical environment only cause small changes to fusion ratios. An early explanation invoked the Oppenheimer–Phillips process at low energies, but its magnitude was too small to explain the altered ratios.\n\nCold fusion setups utilize an input power source (to ostensibly provide activation energy), a platinum group electrode, a deuterium or hydrogen source, a calorimeter, and, at times, detectors to look for byproducts such as helium or neutrons. Critics have variously taken issue with each of these aspects and have asserted that there has not yet been a consistent reproduction of claimed cold fusion results in either energy output or byproducts. Some cold fusion researchers who claim that they can consistently measure an excess heat effect have argued that the apparent lack of reproducibility might be attributable to a lack of quality control in the electrode metal or the amount of hydrogen or deuterium loaded in the system. Critics have further taken issue with what they describe as mistakes or errors of interpretation that cold fusion researchers have made in calorimetry analyses and energy budgets.\n\nIn 1989, after Fleischmann and Pons had made their claims, many research groups tried to reproduce the Fleischmann-Pons experiment, without success. A few other research groups, however, reported successful reproductions of cold fusion during this time. In July 1989, an Indian group from the Bhabha Atomic Research Centre (P. K. Iyengar and M. Srinivasan) and in October 1989, John Bockris' group from Texas A&M University reported on the creation of tritium. In December 1990, professor Richard Oriani of the University of Minnesota reported excess heat.\n\nGroups that did report successes found that some of their cells were producing the effect, while other cells that were built exactly the same and used the same materials were not producing the effect. Researchers that continued to work on the topic have claimed that over the years many successful replications have been made, but still have problems getting reliable replications. Reproducibility is one of the main principles of the scientific method, and its lack led most physicists to believe that the few positive reports could be attributed to experimental error. The DOE 2004 report said among its conclusions and recommendations:\n\nCold fusion researchers (McKubre since 1994, ENEA in 2011) have speculated that a cell that is loaded with a deuterium/palladium ratio lower than 100% (or 1:1) will not produce excess heat. Since most of the negative replications from 1989–1990 did not report their ratios, this has been proposed as an explanation for failed replications. This loading ratio is hard to obtain, and some batches of palladium never reach it because the pressure causes cracks in the palladium, allowing the deuterium to escape. Fleischmann and Pons never disclosed the deuterium/palladium ratio achieved in their cells, there are no longer any batches of the palladium used by Fleischmann and Pons (because the supplier uses now a different manufacturing process), and researchers still have problems finding batches of palladium that achieve heat production reliably.\n\nSome research groups initially reported that they had replicated the Fleischmann and Pons results but later retracted their reports and offered an alternative explanation for their original positive results. A group at Georgia Tech found problems with their neutron detector, and Texas A&M discovered bad wiring in their thermometers. These retractions, combined with negative results from some famous laboratories, led most scientists to conclude, as early as 1989, that no positive result should be attributed to cold fusion.\n\nThe calculation of excess heat in electrochemical cells involves certain assumptions. Errors in these assumptions have been offered as non-nuclear explanations for excess heat.\n\nOne assumption made by Fleischmann and Pons is that the efficiency of electrolysis is nearly 100%, meaning nearly all the electricity applied to the cell resulted in electrolysis of water, with negligible resistive heating and substantially all the electrolysis product leaving the cell unchanged. This assumption gives the amount of energy expended converting liquid DO into gaseous D and O. The efficiency of electrolysis is less than one if hydrogen and oxygen recombine to a significant extent within the calorimeter. Several researchers have described potential mechanisms by which this process could occur and thereby account for excess heat in electrolysis experiments.\n\nAnother assumption is that heat loss from the calorimeter maintains the same relationship with measured temperature as found when calibrating the calorimeter. This assumption ceases to be accurate if the temperature distribution within the cell becomes significantly altered from the condition under which calibration measurements were made. This can happen, for example, if fluid circulation within the cell becomes significantly altered. Recombination of hydrogen and oxygen within the calorimeter would also alter the heat distribution and invalidate the calibration.\n\nThe ISI identified cold fusion as the scientific topic with the largest number of published papers in 1989, of all scientific disciplines. The Nobel Laureate Julian Schwinger declared himself a supporter of cold fusion in the fall of 1989, after much of the response to the initial reports had turned negative. He tried to publish his theoretical paper \"Cold Fusion: A Hypothesis\" in \"Physical Review Letters\", but the peer reviewers rejected it so harshly that he felt deeply insulted, and he resigned from the American Physical Society (publisher of \"PRL\") in protest.\n\nThe number of papers sharply declined after 1990 because of two simultaneous phenomena: first, scientists abandoned the field; second, journal editors declined to review new papers. Consequently cold fusion fell off the ISI charts. Researchers who got negative results turned their backs on the field; those who continued to publish were simply ignored. A 1993 paper in \"Physics Letters A\" was the last paper published by Fleischmann, and \"one of the last reports [by Fleischmann] to be formally challenged on technical grounds by a cold fusion skeptic.\"\n\nThe \"Journal of Fusion Technology\" (FT) established a permanent feature in 1990 for cold fusion papers, publishing over a dozen papers per year and giving a mainstream outlet for cold fusion researchers. When editor-in-chief George H. Miley retired in 2001, the journal stopped accepting new cold fusion papers. This has been cited as an example of the importance of sympathetic influential individuals to the publication of cold fusion papers in certain journals.\n\nThe decline of publications in cold fusion has been described as a \"failed information epidemic\". The sudden surge of supporters until roughly 50% of scientists support the theory, followed by a decline until there is only a very small number of supporters, has been described as a characteristic of pathological science. The lack of a shared set of unifying concepts and techniques has prevented the creation of a dense network of collaboration in the field; researchers perform efforts in their own and in disparate directions, making the transition to \"normal\" science more difficult.\n\nCold fusion reports continued to be published in a small cluster of specialized journals like \"Journal of Electroanalytical Chemistry\" and \"Il Nuovo Cimento\". Some papers also appeared in \"Journal of Physical Chemistry\", \"Physics Letters A\", \"International Journal of Hydrogen Energy\", and a number of Japanese and Russian journals of physics, chemistry, and engineering. Since 2005, \"Naturwissenschaften\" has published cold fusion papers; in 2009, the journal named a cold fusion researcher to its editorial board. In 2015 the Indian multidisciplinary journal \"Current Science\" published a special section devoted entirely to cold fusion related papers.\n\nIn the 1990s, the groups that continued to research cold fusion and their supporters established (non-peer-reviewed) periodicals such as \"Fusion Facts\", \"Cold Fusion Magazine\", \"Infinite Energy Magazine\" and \"New Energy Times\" to cover developments in cold fusion and other fringe claims in energy production that were ignored in other venues. The internet has also become a major means of communication and self-publication for CF researchers.\n\nCold fusion researchers were for many years unable to get papers accepted at scientific meetings, prompting the creation of their own conferences. The first International Conference on Cold Fusion (ICCF) was held in 1990, and has met every 12 to 18 months since. Attendees at some of the early conferences were described as offering no criticism to papers and presentations for fear of giving ammunition to external critics, thus allowing the proliferation of crackpots and hampering the conduct of serious science. Critics and skeptics stopped attending these conferences, with the notable exception of Douglas Morrison, who died in 2001. With the founding in 2004 of the International Society for Condensed Matter Nuclear Science (ISCMNS), the conference was renamed the International Conference on Condensed Matter Nuclear Science — for reasons that are detailed in the subsequent research section below — but reverted to the old name in 2008. Cold fusion research is often referenced by proponents as \"low-energy nuclear reactions\", or LENR, but according to sociologist Bart Simon the \"cold fusion\" label continues to serve a social function in creating a collective identity for the field.\n\nSince 2006, the American Physical Society (APS) has included cold fusion sessions at their semiannual meetings, clarifying that this does not imply a softening of skepticism. Since 2007, the American Chemical Society (ACS) meetings also include \"invited symposium(s)\" on cold fusion. An ACS program chair said that without a proper forum the matter would never be discussed and, \"with the world facing an energy crisis, it is worth exploring all possibilities.\"\n\nOn 22–25 March 2009, the American Chemical Society meeting included a four-day symposium in conjunction with the 20th anniversary of the announcement of cold fusion. Researchers working at the U.S. Navy's Space and Naval Warfare Systems Center (SPAWAR) reported detection of energetic neutrons using a heavy water electrolysis set-up and a CR-39 detector, a result previously published in \"Naturwissenschaften\". The authors claim that these neutrons are indicative of nuclear reactions; without quantitative analysis of the number, energy, and timing of the neutrons and exclusion of other potential sources, this interpretation is unlikely to find acceptance by the wider scientific community.\n\nAlthough details have not surfaced, it appears that the University of Utah forced the 23 March 1989 Fleischmann and Pons announcement to establish priority over the discovery and its patents before the joint publication with Jones. The Massachusetts Institute of Technology (MIT) announced on 12 April 1989 that it had applied for its own patents based on theoretical work of one of its researchers, Peter L. Hagelstein, who had been sending papers to journals from the 5 to 12 April. On 2 December 1993 the University of Utah licensed all its cold fusion patents to ENECO, a new company created to profit from cold fusion discoveries, and in March 1998 it said that it would no longer defend its patents.\n\nThe U.S. Patent and Trademark Office (USPTO) now rejects patents claiming cold fusion. Esther Kepplinger, the deputy commissioner of patents in 2004, said that this was done using the same argument as with perpetual motion machines: that they do not work. Patent applications are required to show that the invention is \"useful\", and this utility is dependent on the invention's ability to function. In general USPTO rejections on the sole grounds of the invention's being \"inoperative\" are rare, since such rejections need to demonstrate \"proof of total incapacity\", and cases where those rejections are upheld in a Federal Court are even rarer: nevertheless, in 2000, a rejection of a cold fusion patent was appealed in a Federal Court and it was upheld, in part on the grounds that the inventor was unable to establish the utility of the invention.\n\nA U.S. patent might still be granted when given a different name to disassociate it from cold fusion, though this strategy has had little success in the US: the same claims that need to be patented can identify it with cold fusion, and most of these patents cannot avoid mentioning Fleischmann and Pons' research due to legal constraints, thus alerting the patent reviewer that it is a cold-fusion-related patent. David Voss said in 1999 that some patents that closely resemble cold fusion processes, and that use materials used in cold fusion, have been granted by the USPTO. The inventor of three such patents had his applications initially rejected when they were reviewed by experts in nuclear science; but then he rewrote the patents to focus more in the electrochemical parts so they would be reviewed instead by experts in electrochemistry, who approved them. When asked about the resemblance to cold fusion, the patent holder said that it used nuclear processes involving \"new nuclear physics\" unrelated to cold fusion. Melvin Miles was granted in 2004 a patent for a cold fusion device, and in 2007 he described his efforts to remove all instances of \"cold fusion\" from the patent description to avoid having it rejected outright.\n\nAt least one patent related to cold fusion has been granted by the European Patent Office.\n\nA patent only legally prevents others from using or benefiting from one's invention. However, the general public perceives a patent as a stamp of approval, and a holder of three cold fusion patents said the patents were very valuable and had helped in getting investments.\n\nA 1990 Michael Winner film \"Bullseye!\", starring Michael Caine and Roger Moore, referenced the Fleischmann and Pons experiment. The film — a comedy — concerned conmen trying to steal scientists' purported findings. However, the film had a poor reception, described as \"appallingly unfunny\".\n\nIn \"Undead Science\", sociologist Bart Simon gives some examples of cold fusion in popular culture, saying that some scientists use cold fusion as a synonym for outrageous claims made with no supporting proof, and courses of ethics in science give it as an example of pathological science. It has appeared as a joke in \"Murphy Brown\" and \"The Simpsons\". It was adopted as a software product name Adobe ColdFusion and a brand of protein bars (Cold Fusion Foods). It has also appeared in advertising as a synonym for impossible science, for example a 1995 advertisement for Pepsi Max.\n\nThe plot of \"The Saint\", a 1997 action-adventure film, parallels the story of Fleischmann and Pons, although with a different ending. The film might have affected the public perception of cold fusion, pushing it further into the science fiction realm.\n\n\"Final Exam\", the 16th episode of season 4 of \"The Outer Limits\", depicts a student named Todtman who has invented a cold fusion weapon, and attempts to use it as a tool for revenge on people who have wronged him over the years. Despite the secret being lost with his death at the end of the episode, it is implied that another student elsewhere is on a similar track, and may well repeat Todtman's efforts.\n\nIn the \"DC's Legends of Tomorrow\" episode \"No Country for Old Dads,\" Ray Palmer theorizes that cold fusion could repair the shattered Fire Totem, if it wasn't only theoretical. Damien Dahrk reveals that he assassinated a scientist in 1962 East Germany that developed a formula for cold fusion. Ray and Dahrk's daughter Nora time travel from 2018 to 1962 in an attempt to rescue the scientist from the younger version of Dahrk and/or retrieve the formula.\n\n"}
{"id": "3182362", "url": "https://en.wikipedia.org/wiki?curid=3182362", "title": "Continuum concept", "text": "Continuum concept\n\nThe continuum concept is an idea, coined by Jean Liedloff in her 1975 book The Continuum Concept, that human beings have an innate set of expectations (which Liedloff calls the continuum) that our evolution as a species has designed us to meet in order to achieve optimal physical, mental, and emotional development and adaptability. According to Liedloff, in order to achieve this level of development, young humans—especially babies—require the kind of experience to which our species adapted during the long process of our evolution by natural selection. For infants, these include, for example, that they experience:\n\n\nLiedloff suggests that when certain evolutionary expectations are not met as infants and toddlers, compensation for these needs will be sought, by alternate means, throughout life—resulting in many forms of mental and social disorders. She also argues that these expectations are largely distorted, neglected, and/or not properly met in civilized cultures which have removed themselves from the natural evolutionary process, resulting in the aforementioned abnormal psychological and social conditions. Liedloff's recommendations fit in more generally with evolutionary psychology, attachment theory, and the philosophy known as the Paleolithic lifestyle: optimizing well-being by living more like our hunter-gatherer ancestors, who Liedloff refers to as \"evolved\" humans, since their lifeways developed through natural selection by living in the wild.\n\n\n\n"}
{"id": "44819733", "url": "https://en.wikipedia.org/wiki?curid=44819733", "title": "Coordination good", "text": "Coordination good\n\nIn economics, coordination goods are a form of good created by the coordination of people within civil society.\n\nCoordination goods are non-rivalrous, but may be partially excludable through the means of withholding cooperation from a non-cooperative state. \n\n"}
{"id": "11483011", "url": "https://en.wikipedia.org/wiki?curid=11483011", "title": "Coster–Kronig transition", "text": "Coster–Kronig transition\n\nThe Coster–Kronig transition is a special case of the Auger process in which the vacancy is filled by an electron from a higher subshell of the same shell. If, in addition, the electron emitted (the \"Auger electron\") also belongs to the same shell, one calls this a super Coster–Kronig transition.\n\nThe Coster–Kronig process is named after the physicists Dirk Coster and Ralph Kronig.\n"}
{"id": "46972715", "url": "https://en.wikipedia.org/wiki?curid=46972715", "title": "Cynthia Mulrow", "text": "Cynthia Mulrow\n\nCynthia Mulrow (born May 23, 1953) is an American physician and scholar from Edinburg, Texas. She has regularly contributed academic research on many topics to the medical community. Her academic work mainly focuses on systematic reviews and evidence reports, research methodology, and chronic medical conditions.\n\nMulrow graduated from high school in, Alice, Texas. She received her MD degree from the Baylor College of Medicine in 1978, completed a fellowship in general medicine at Duke University School of Medicine in 1983, and a Masters in epidemiology at the London School of Hygiene & Tropical Medicine in 1984.\n\nMulrow is the Senior Deputy Editor of the academic journal, Annals of Internal Medicine, and an adjunct Clinical Professor of Medicine at the University of Texas Health Science Center at San Antonio. Past positions that she has held were: Program Director of the Robert Wood Johnson Foundation Generalist Physician Faculty Scholars Program (2000-2008) and Director of the San Antonio Cochrane Collaboration Center (1994-2000) and the San Antonio Evidence-based Practice Center (1997-2000). She was elected to the American Society for Clinical Investigation in 1997, honored as a Master of the American College of Physicians in 2005, and elected to the Institute of Medicine in 2008. She has authored numerous papers, including a seminal article on the medical review article in 1987, and has served on guideline panels including the United States Preventive Services Task Force. She currently contributes to groups who set standards for reporting research including PRISMA (systematic reviews and meta-analyses), and STROBE (observational studies).\n\nMulrow has two children, Emily and Benjamin. Outside of contributing to the medical community, she enjoys traveling, sightseeing, visiting art museums, decorating, gardening, swimming and running. \n\n"}
{"id": "32126830", "url": "https://en.wikipedia.org/wiki?curid=32126830", "title": "Danish bicycle VIN-system", "text": "Danish bicycle VIN-system\n\nThe Danish bicycle VIN-system is a system introduced in 1942 by the Danish government, providing all bicycles in Denmark with a unique code. The code is a combination of letters and digits embedded into the bicycle frame and consists of a manufacturer code, a serial number, and construction year code.\n\nSince 1948, it has been illegal to sell bicycle frames in Denmark without an embedded VIN. Because of this, insurance companies in Denmark will not pay indemnities for stolen bicycles without a VIN.\n\nBy default, the VIN is to be engraved into the seat tube or the down tube, but if these are made of such a material that hinders this, it may alternately be put on the bottom bracket shell. In very special instances, and only with the approval of the Danish National Police Commissioner’s Office, it may be applied by other means and on other locations.\n\nThe bicycle VIN is constructed of three elements: a letter-block, a digit-block and a letter-block:\n\nThe manufacturers-code, is a 1, 2, 3 or 4-letter block. If the code starts with a W, it is an imported bicycle frame. The second block is the frame serial number from that manufacturer. This shall have at least one digit, but can otherwise have any number of digits. The third block is a single letter code that identifies the production year. Certain limits regarding letters are instated for the VIN:\n\nIn the examples above, the first bicycle is a bicycle imported by FDB, with serial number \"1234\" and made in either 1963, 1984 or 2005. The second bike is a SCO bicycle, with serial number \"57\", made in 1942, 1964, 1985 or 2006.\n\nThis table shows examples of manufacturer codes, but is in no way to be considered a complete listing.\n\n"}
{"id": "45159528", "url": "https://en.wikipedia.org/wiki?curid=45159528", "title": "DeLano Award for Computational Biosciences", "text": "DeLano Award for Computational Biosciences\n\nThe DeLano Award for Computational Biosciences is a prize in the field of computational biology. It is awarded annually for \"the most accessible and innovative development or application of computer technology to enhance research in the life sciences at the molecular level\".\n\nThe prize was established by the American Society for Biochemistry and Molecular Biology (ASBMB) in memory of Warren Lyford DeLano, an American bioinformatician. DeLano developed the PyMOL open source molecular viewer software and was an advocate for the increased adoption of open source practices in the sciences. DeLano died unexpectedly in 2009.\n\nLaureates include the Nobel Prize winner Michael Levitt, who was awarded the prize in 2014 for his work in computational bioscience, including the Rosetta protein-modelling software.\n\n\n"}
{"id": "14595828", "url": "https://en.wikipedia.org/wiki?curid=14595828", "title": "Debussy quadrangle", "text": "Debussy quadrangle\n\nThe Debussy quadrangle (H-14) is one of fifteen quadrangles on Mercury. It runs from 270 to 360° longitude and from -20 to -70° latitude. Named after the Debussy crater, it was mapped in detail for the first time after \"MESSENGER\" entered orbit around Mercury in 2011. It had not been mapped prior to that point because it was one of the six quadrangles that was not illuminated when \"Mariner 10\" made its flybys in 1974 and 1975. These six quadrangles continued to be known by their albedo feature names, with this one known as the Cyllene quadrangle.\n"}
{"id": "16427261", "url": "https://en.wikipedia.org/wiki?curid=16427261", "title": "Demand shaping", "text": "Demand shaping\n\nDemand shaping is the influencing of demand to match planned supply. For example, in a manufacturing business, dynamic pricing can be used to manage demand.\n\nDell Inc., is one of the best example of companies that practice Demand Shaping and dynamic pricing. From its currently available supplies, Dell posts special sales weeks that influences the demand.\n"}
{"id": "3165654", "url": "https://en.wikipedia.org/wiki?curid=3165654", "title": "European Amalfi Prize for Sociology and Social Sciences", "text": "European Amalfi Prize for Sociology and Social Sciences\n\nThe European Amalfi Prize for Sociology and Social Sciences \"(Premio Europeo Amalfi per la Sociologia e le Scienze Sociali)\" is a prestigious Italian award in the social sciences. Established in 1987 on the initiative of the Section for Sociological Theories and Social Transformations of the Italian Association of Sociology, it is conferred annually in Amalfi to the author of a book or an article which was published within the previous two years and has made an important contribution to sociology.\n\nThe award ceremony is accompanied by an international sociological conference.\nThe awardees are selected by a jury currently consisting of Margaret Archer, Allessandro Cavalli, Salvador Giner, Joachim Israel, Michel Maffesoli, Carlo Mongardini (chair), Birgitta Nedelmann, Helga Nowotny, and Piotr Sztompka. Previous members include Anthony Giddens, Peter Gerlich and Friedrich H. Tenbruck.\n\nThe award was suspended in 1993 and 1996, 1992 and 2011-2015 the jury decided not to assign the award.\n\nIn 1999, a biennial special award for an outstanding first book was established and named after Norbert Elias, the first recipient of the Amalfi Award. There are also other auxiliary awards in various categories; the main award can also be split.\n\n1989 Elected to US Sociological\n\n"}
{"id": "23345729", "url": "https://en.wikipedia.org/wiki?curid=23345729", "title": "Frost boil", "text": "Frost boil\n\nA frost boil, also known as mud boils, a stony earth circles, frost scars, or mud circles, are small circular mounds of fresh soil material formed by frost action and cryoturbation. They are found typically found in periglacial or alpine environments where permafrost is present, and may damage roads and other man-made structures. They are typically 1 to 3 metres in diameter.\n\nFrost boils are amongst the most common features of patterned ground, the pervasive process shaping the topology of soils in periglacial regions. They generally form regular patterns of polygons. Frost boils are a type of nonsorted circle, and are characterized from other circles by barren centres of mineral soil and intercircle regions filled with vegetation and peat. It is named after skin boils due to similarities in their formation processes, although subsequent research has shown other methods of formation.\n\nFrost boils have been observed on Mars, indicating the presence of periglacial processes similar to those on Earth.\n\nThe most accepted theory involves cryoturbation caused by differences in moisture conditions and ground temperature. Other recent research posits that frost boils are formed by several interacting mechanisms, including differential frost heaving, load casting, convection, frost cracking, mass displacement, and soil sorting. The traditional model of injection, however, may still apply for some frost boils. Models generally presume soil is predominately silt or clay, for the reasons listed under the injection subsection.\n\nFrost boils occur in soils of poorly-sorted sediments with significant silt and/or clay content. These soils include perennially frozen till, marine clay, colluvium, and other muds. These soils have low liquid limits, low plasticity limits, and high natural moisture contents. These soils liquefy and flow readily in response to slight changes to either internal or external stress, or a change in water content. Localized stresses are often the result of moisture being confined in the active layer by the underlying permafrost and a semi-rigid carapace of dried surface mud, created by desiccation during the late summer. The moisture content of soils may increase during summer due to rain. Other stresses include the volumetric change of water during the freezing and thawing, and the flow of groundwater. \n\nThe subsequent increase of hydrostatic, artesian, and/or pore water pressure pressures on slopes. When internal stresses cannot be contained, the semi-rigid surface layer ruptures. The saturated mud bursts above the surface, creating a mud boil. \n\nThis process is analogous to the formation of sand boils. Where soils are badly drained, soil temperatures are more sensitive to changes in the atmospheric temperature. Soil aggregates are less stable near the surface as freezing occurs more rapidly. Deeper soils experience longer periods of stability due to freeze drying, or cryodesiccation. Deeper soils also experience greater stresses due to the secondary refreezing of soil in late autumn. As a result, the introduction of additional water due to thaw or groundwater flows is likely to cause deeper soil to liquefy and deform like plastic. The high viscosity of water close to 0°C promotes aggregate explosion and particle dispersion. \n\nThis process is commonplace in alpine regions where soil temperatures rarely drop below -10°C.\n\nFrost heaving is greater at the center of frost boils when compared to the margins of frost boils due to the ice-rich conditions at the center and vegetative cover at the margins. Due to the higher moisture content, ice predominantly form segregated ice lenses in shallow soils near the center of the frost boil. Moisture content at the margins, however, are predominantly in the form of pore ice. Ground subsistence at the center of frost boils during thawing season is correspondingly more rapid and of a greater magnitude when compared to the margins. Subsidence at the margins advances slowly in the earlier thawing period but increases to rates comparable to center by mid-summer. Measurements conducted on frost boils in Adventdalen, Svalbard has found that the ground subsistence rates at center of frost boils of averaged 8 mm per day during late May but decreased to less than 1 mm per day in mid-July. The same found that heaving was considerably greater at centers (c. 9.5 mm per day) than margins (c. 1.6 mm per day). Correspondingly, ice core analyses conducted on frost boils has found that samples extracted from the center of Frost Boils have higher concentration of ice lenses in shallow soils, when compared to cores extracted from marginal and intercircle regions. Most ice lenses have a diameter smaller than 3 mm. \n\nFrost boils often occur in groups, and may form terraces if a series of them occur on a slope. On slopes, frost boils are sometimes protected from erosion by a thin layer of mosses and lichens which retains moisture through surface tension as sediments flow downslope to form a lobe. These landforms eventually settle like a caterpillar track.\n\nCommon characteristics of landforms created by frost boils include a bowl-shaped boil, an elevated center, a formation of an organic layer on the outer edge, and resistance of the soil surface to vegetation colonization.\n\nDrainage on frost boils differs as a result of micro relief across the frost boil surface. In warm seasons (summer), the elevated center of the frost boil is moderately well drained compared to the depressed inter boil. The permafrost table surface is also affected by differing activity across the boil. The inner boil is more active and generally has more than twice the active depth than the inter boil, which causes the permafrost table surface to be in a nearly perfect bowl shape.\n\nFrost boils may be the predominant form of topology and patterned ground in tundras. Three elements of frost boils may repeat over large areas: patches (the center of frost boils), rims, and troughs. The density of these elements are higher in the high arctic when compared to southern tundras. Each element of frost boils is a distinct microecosystem. Although vegetation is rare on patches, it may host many species of small mosses, crustose lichens, and solitary small vascular plants. Well-developed moss covers the surface of most rims and troughs. Rims and troughs are also home to a large number of herbs and small or stunted shrubs.\n\nArctic soils acidify over time due to the presence of aerobic bacteria which breaks down water-soluble salts within soil moisture, reducing the fertility of most periglacial regions. Cryoturbation within active frost boils may allow water containing basic salts to permeate from depth to the surface, neutralizing soil acidity and replenishing the supply of nutrients. Nutrients in plant matter, particularly carbon and nitrogen, are deposited and concentrated in troughs. These nutrients are intensely recycled in each stage of ecological succession. Troughs thus have an overall higher net ecosystem production and carbon accumulation rate than patches. Other reasons contributing to the greater carbon accumulation in troughs include a higher soil moisture content that makes troughs unfavorable for decomposition. Troughs may also have a higher carbon content due to it being older and having experienced a longer period of soil formation. \n\nThe presence of plants affect the development of frost boils. In the high arctic where plants are rare, physical processes of heave and soil formation are dominant. In warmer temperate regions, dense vegetation insulates inter-boil areas, lowering soil temperatures and decreasing the potential for heave. The strong contrast between vegetated inter-boil regions and center patches lead to maximum differential heave, resulting in frost boils being better developed. \n\n"}
{"id": "4674539", "url": "https://en.wikipedia.org/wiki?curid=4674539", "title": "Guilbert (crater)", "text": "Guilbert (crater)\n\nGuilbert is an impact crater on Venus.\n"}
{"id": "56270111", "url": "https://en.wikipedia.org/wiki?curid=56270111", "title": "HT-29", "text": "HT-29\n\nHT-29 is a human colon cancer cell line used extensively in biological and cancer research.\n\nInitially derived in 1964 by Jorden Fogh from a 44-year-old Caucasian female, HT-29 cells form a tight monolayer while exhibiting similarity to enterocytes from the small intestine. HT-29 cells overproduce the p53 tumor antigen, but have a mutation in the p53 gene at position 273, resulting in a histidine replacing an arginine. The cells proliferate rapidly in media containing suramin, with corresponding high expression of the \"c-myc\" oncogene. However, \"c-myc\" is deregulated, but may have a relation with the growth factor requirements of HT-29 cells.\n\nIn preclinical research, HT-29 cells have been studied for their ability to differentiate and thus simulate real colon tissue \"in vitro\", a characteristic that has made HT-29 useful for epithelial cell research. The cells can also be tested \"in vivo\" via xenografts with rodents. HT-29 cells terminally differentiate into enterocytes with the replacement of glucose by galactose in cell culture, and with the addition of butyrate or acids, the differentiation pathways can be closely studied along with their dependence on surrounding conditions. Accordingly, studies of HT-29 cells have shown induced differentation as a result of forskolin, Colchicine, nocodazole, and taxol, with galactose-mediated differentiation also causing the strengthening of adherens junctions.\n\nThough HT-29 cells can proliferate in cell culture lacking growth factors with a doubling time of around 4 days, the doubling time can be reduced to one day with added fetal bovine serum. The cells have high glucose consumption, and in standard medium containing 25 mM glucose and 10% serum, remain undifferentiated.\n\n"}
{"id": "13500312", "url": "https://en.wikipedia.org/wiki?curid=13500312", "title": "Ideotype", "text": "Ideotype\n\nIn systematics, an ideotype is a specimen identified as belonging to a specific taxon by the author of that taxon, but collected from somewhere other than the type locality.\n\nThe concept of ideotype in plant breeding was introduced by Donald in 1968 to describe the idealized appearance of a plant variety. It literally means 'a form denoting an idea'. According to Donald, ideotype is a biological model which is expected to perform or behave in a particular manner within a defined environment: \"a crop ideotype is a plant model, which is expected to yield a greater quantity or quality of grain, oil or other useful product when developed as a cultivar.\" Donald and Hamblin (1976) proposed the concepts of isolation, competition and crop ideotypes. Market ideotype, climatic ideotype, edaphic ideotype, stress ideotype and disease/pest ideotypes are its other concepts. The term ideotype has the following synonyms: model plant type, ideal model plant type and ideal plan type.\n\nThe term is also used in cognitive science and cognitive psychology, where Ronaldo Vigo (2011, 2013, 2014) introduced it to refer to a type of concept metarepresentation that is a compound memory trace consisting of the structural information detected by humans in categorical stimuli.\n"}
{"id": "13206504", "url": "https://en.wikipedia.org/wiki?curid=13206504", "title": "Juan Enríquez", "text": "Juan Enríquez\n\nJuan Enríquez Cabot (born 1959) is a Mexican-American academic, businessman, author, and speaker. He is currently the Managing Director of Excel Venture Management.\n\nEnríquez is the son of Antonio Enríquez Savignac and Marjorie Cabot Lewis of the Boston Cabot family. He is a graduate of Phillips Academy Andover (1977) and Harvard, where he earned a B.A. (1981) and an MBA (1986), with honors.\n\nThe Founding Director of the Life Sciences Project at Harvard Business School (HBS), Enríquez is also a fellow of Harvard's Center for International Affairs. His work has been published in the \"Harvard Business Review\", \"Foreign Policy\", \"Science\", and \"The New York Times\". He is the author of many books, including \"Evolving Ourselves: How Unnatural Selection and Nonrandom Mutation are Changing Life on Earth\" (Current-Penguin Group, 2015), \"Homo Evolutis: Please Meet the Next Human Species\" (TED, 2012), \"As the Future Catches You: How Genomics & Other Forces are Changing Your Life, Work, Health & Wealth\" (Crown Business, 2005), and \"The United States of America: Polarization, Fracturing, and Our Future\" (Random House, 2005). He works in business, science, and domestic/international politics.\n\nEnríquez is recognized as a leading authority on the economic and political impacts of the life sciences. He is currently chairman and CEO of Biotechonomy LLC, a life sciences research and investment firm.\n\nHe has written several articles, including \"Transforming Life, Transforming Business: the Life Science Revolution,\" which was co-authored with Ray Goldberg and which received a McKinsey Prize in 2000 (2nd place). Enríquez also co-authored the first map of global nucleotide data flow, as well as HBS's working papers on \"Life Sciences in Arabic Speaking Countries,\" \"Global Life Science Data Flows and the IT industry,\" \"SARS, Smallpox, and Business Unusual,\" and \"Technology, Gene Research and National Competitiveness.\" \"Harvard Business School Interactive\" picked Enríquez as one of the best teachers at the school, and showcased his work in its first set of faculty products.\n\nThe \"Harvard Business Review\" showcased his ideas as one of the breakthrough concepts in its first HBR List. \"Fortune\" profiled him as \"Mr. Gene.\" The Van Heyst Group asked him to co-organize the life sciences summit commemorating the fiftieth anniversary of the discovery of DNA. The summit \"The Future of Life\" was sponsored by \"Time\". \"Seed\" picked his ideas as one of fifty that \"shaped our identity, our culture, and the world as we know it\".\n\nEnríquez serves on a variety of boards including: Cabot Corporation, Zipongo, ShapeUp, Synthetic Genomics, the Harvard Medical School Genetics Advisory Council, the Chairman's International Council of the Americas Society, the Visiting Committee of Harvard's David Rockefeller Center, Tufts University's EPIIC, TED's Brain Trust, Harvard Business School's PAPSAC, WGBH, and the Museum of Science (Boston).\n\nEnríquez joined a multi-stage world sailing discovery voyage led by J. Craig Venter, who sequenced the human genome. The expedition voyage sampled microbial genomes throughout the world's oceans. This expedition involved a number of institutions and top scholars including The Institute for Genomic Research, Woods Hole Oceanographic Institution, The Explorers Club, and Prof. E. O. Wilson. It also led to the discovery of an unprecedented number of new species.\n\nEnríquez served as CEO of Mexico City's Urban Development Corporation, coordinator general of economic policy and chief of staff for Mexico's secretary of state, and as a member of the peace commission that negotiated the cease-fire in Chiapas' Zapatista rebellion.\n\n"}
{"id": "1978851", "url": "https://en.wikipedia.org/wiki?curid=1978851", "title": "Juno (spacecraft)", "text": "Juno (spacecraft)\n\nJuno is a NASA space probe orbiting the planet Jupiter. It was built by Lockheed Martin and is operated by NASA Jet Propulsion Laboratory. The spacecraft was launched from Cape Canaveral Air Force Station on August 5, 2011 (UTC), as part of the New Frontiers program, and entered a polar orbit of Jupiter on July 5, 2016 (UTC), to begin a scientific investigation of the planet. After completing its mission, \"Juno\" will be intentionally deorbited into Jupiter's atmosphere.\n\n\"Juno\" mission is to measure Jupiter's composition, gravity field, magnetic field, and polar magnetosphere. It will also search for clues about how the planet formed, including whether it has a rocky core, the amount of water present within the deep atmosphere, mass distribution, and its deep winds, which can reach speeds up to .\n\n\"Juno\" is the second spacecraft to orbit Jupiter, after the nuclear powered \"Galileo\" orbiter, which orbited from 1995 to 2003. Unlike all earlier spacecraft sent to the outer planets, \"Juno\" is powered by solar arrays, commonly used by satellites orbiting Earth and working in the inner Solar System, whereas radioisotope thermoelectric generators are commonly used for missions to the outer Solar System and beyond. For \"Juno\", however, the three largest solar array wings ever deployed on a planetary probe play an integral role in stabilizing the spacecraft as well as generating power.\n\nThe mission had previously been referred to by the backronym \"Jupiter Near-polar Orbiter\". \"Juno\" is sometimes called \"New Frontiers 2\" as the second mission in the New Frontiers program, but is not to be confused with New Horizons 2, a proposed but unselected New Frontiers mission.\n\n\"Juno\" was selected in 2005 as the next New Frontiers mission after \"New Horizons\". The desire for a Jupiter probe was strong in the years prior to this, but there had not been any approved missions. The Discovery Program had passed over the somewhat similar but more limited Interior Structure and Internal Dynamical Evolution of Jupiter (INSIDE Jupiter) proposal, and the turn-of-the-century era Europa Orbiter was cancelled in 2002. The flagship-level Europa Jupiter System Mission was in the works in the early 2000s, but funding issues resulted in it evolving into ESA's Jupiter Icy Moons Explorer.\n\n\"Juno\" completed a five-year cruise to Jupiter, arriving on July 5, 2016. The spacecraft traveled a total distance of roughly to reach Jupiter. The spacecraft was designed to orbit Jupiter 37 times over the course of its mission. This was originally planned to take 20 months. \"Juno\" trajectory used a gravity assist speed boost from Earth, accomplished by an Earth flyby in October 2013, two years after its launch on August 5, 2011. The spacecraft performed an orbit insertion burn to slow it enough to allow capture. It was expected to make three 53-day orbits before performing another burn on December 11, 2016 that would bring it into a 14-day polar orbit called the Science Orbit. Because of a suspected problem in Juno's main engine, the burn of December 11 was canceled, and Juno will remain in its 53-day orbit for its remaining orbits of Jupiter.\n\nDuring the science mission, infrared and microwave instruments will measure the thermal radiation emanating from deep within Jupiter's atmosphere. These observations will complement previous studies of its composition by assessing the abundance and distribution of water, and therefore oxygen. This data will provide insight into Jupiter's origins. \"Juno\" will also investigate the convection that drives natural circulation patterns in Jupiter's atmosphere. Other instruments aboard \"Juno\" will gather data about its gravitational field and polar magnetosphere. The \"Juno\" mission was planned to conclude in February 2018, after completing 37 orbits of Jupiter. The probe was then intended to be de-orbited and burn up in Jupiter's outer atmosphere, to avoid any possibility of impact and biological contamination of one of its moons.\n\n\"Juno\" was launched atop the Atlas V at Cape Canaveral Air Force Station, Florida. The Atlas V (AV-029) used a Russian-built RD-180 main engine, powered by kerosene and liquid oxygen. At ignition it underwent checkout 3.8 seconds prior to the ignition of five strap-on solid rocket boosters (SRBs). Following the SRB burnout, about 93 seconds into the flight, two of the spent boosters fell away from the vehicle, followed 1.5 seconds later by the remaining three. When heating levels had dropped below predetermined limits, the payload fairing that protected \"Juno\" during launch and transit through the thickest part of the atmosphere separated, about 3 minutes 24 seconds into the flight. The Atlas V main engine cut off 4 minutes 26 seconds after liftoff. Sixteen seconds later, the Centaur second stage ignited, and it burned for about 6 minutes, putting the satellite into an initial parking orbit. The vehicle coasted for about 30 minutes, and then the Centaur was reignited for a second firing of 9 minutes, placing the spacecraft on an Earth escape trajectory in a heliocentric orbit.\n\nPrior to separation, the Centaur stage used onboard reaction engines to spin \"Juno\" up to 1.4 r.p.m.. About 54 minutes after launch, the spacecraft separated from the Centaur and began to extend its solar panels. Following the full deployment and locking of the solar panels, \"Juno\" batteries began to recharge. Deployment of the solar panels reduced \"Juno\" spin rate by two-thirds. The probe is spun to ensure stability during the voyage and so that all instruments on the probe are able to observe Jupiter.\n\nThe voyage to Jupiter took five years, and included a flyby of the Earth on October 10, 2013. When it reached the Jovian system, \"Juno\" had traveled approximately 19 AU, almost two billion miles.\n\nAfter traveling for two years in an elliptical heliocentric orbit, \"Juno\" returned to pass by the Earth in October 2013. It used Earth's gravity to help slingshot itself toward the Jovian system in a maneuver called a gravity assist. The spacecraft received a boost in speed of more than , and it was set on a course to Jupiter. The flyby was also used as a rehearsal for the \"Juno\" science team to test some instruments and practice certain procedures before the arrival at Jupiter.\n\nJupiter's gravity accelerated the approaching spacecraft to around . On July 5, 2016, between 03:18 and 03:53 UTC Earth-received time, an insertion burn lasting 2,102 seconds decelerated Juno by and changed its trajectory from a hyperbolic flyby to an elliptical, polar orbit with a period of about 53.5 days. The spacecraft successfully entered Jupiter orbit on July 5 at 03:53 UTC.\n\n\"Juno\" highly elliptical initial polar orbit takes it within of the planet and out to , far beyond Callisto's orbit. An eccentricity-reducing burn, called the Period Reduction Maneuver, was planned that would drop the probe into a much shorter 14 day science orbit. Originally, \"Juno\" was expected to complete 37 orbits over 20 months before the end of its mission. Due to problems with helium valves that are important during main engine burns, mission managers announced on February 17, 2017, that \"Juno\" would remain in its original 53-day orbit, since the chance of an engine misfire putting the spacecraft into a bad orbit was too high. \"Juno\" will now complete only 12 science orbits before the end of its budgeted mission plan, ending July 2018.\n\nThe orbits were carefully planned in order to minimize contact with Jupiter's dense radiation belts, which can damage spacecraft electronics and solar panels, by exploiting a gap in the radiation envelope near the planet, passing through a region of minimal radiation. The \"Juno Radiation Vault\", with 1-centimeter-thick titanium walls, also aids in protecting Juno's electronics. Despite the intense radiation, JunoCam and the Jovian Infrared Auroral Mapper (JIRAM) are expected to endure at least eight orbits, while the Microwave Radiometer (MWR) should endure at least eleven orbits. \"Juno\" will receive much lower levels of radiation in its polar orbit than the \"Galileo\" orbiter received in its equatorial orbit. \"Galileo\" subsystems were damaged by radiation during its mission, including an LED in its data recording system.\n\nThe spacecraft completed its first flyby of Jupiter (perijove 1) on August 27, 2016, and captured the first images of the planet's north pole.\n\nOn October 14, 2016, days prior to perijove 2 and the planned Period Reduction Maneuver, telemetry showed that some of \"Juno\" helium valves were not opening properly. On October 18, 2016, some 13 hours before its second close approach to Jupiter, \"Juno\" entered into safe mode, an operational mode engaged when its onboard computer encounters unexpected conditions. The spacecraft powered down all non-critical systems and reoriented itself to face the Sun to gather the most power. Due to this, no science operations were conducted during perijove 2.\n\nOn December 11, 2016, the spacecraft completed perijove 3, with all but one instrument operating and returning data. One instrument, JIRAM, was off pending a flight software update. Perijove 4 occurred on February 2, with all instruments operating. Perijove 5 occurred on March 27, 2017. Perijove 6 took place on May 19, 2017.\n\nAlthough the mission's lifetime is inherently limited by radiation exposure, almost all of this dose was planned to be acquired during the perijoves. , the 53.4 day orbit was planned to be maintained through July 2018 for a total of twelve science-gathering perijoves. At the end of this prime mission, the project was planned to go through a science review process by NASA's Planetary Science Division to determine if it will receive funding for an extended mission.\n\nIn June 2018, NASA extended the mission operations plan to July 2021. When \"Juno\" reaches the end of the mission, it will perform a controlled deorbit and disintegrate into Jupiter's atmosphere. During the mission, the spacecraft will be exposed to high levels of radiation from Jupiter's magnetosphere, which may cause future failure of certain instruments and risk collision with Jupiter's moons. \n\nNASA plans to eventually deorbit the spacecraft into the atmosphere of Jupiter after 2021. The controlled deorbit is intended to eliminate space debris and risks of contamination in accordance with NASA's Planetary Protection Guidelines.\n\nScott Bolton of the Southwest Research Institute in San Antonio, Texas is the principal investigator and is responsible for all aspects of the mission. The Jet Propulsion Laboratory in California manages the mission and the Lockheed Martin Corporation was responsible for the spacecraft development and construction. The mission is being carried out with the participation of several institutional partners. Coinvestigators include Toby Owen of the University of Hawaii, Andrew Ingersoll of California Institute of Technology, Frances Bagenal of the University of Colorado at Boulder, and Candy Hansen of the Planetary Science Institute. Jack Connerney of the Goddard Space Flight Center served as instrument lead.\n\n\"Juno\" was originally proposed at a cost of approximately (fiscal year 2003) for a launch in June 2009. NASA budgetary restrictions resulted in postponement until August 2011, and a launch on board an Atlas V rocket in the 551 configuration. , the mission was projected to cost over its life.\n\nThe \"Juno\" spacecraft's suite of science instruments will:\n\nAmong early results, Juno gathered information about Jovian lightning that revised earlier theories.\n\nThe \"Juno\" mission's scientific objectives will be achieved with a payload of nine instruments on board the spacecraft:\n\n\"Juno\" is the first mission to Jupiter to use solar panels instead of the radioisotope thermoelectric generators (RTG) used by \"Pioneer 10\", \"Pioneer 11\", the Voyager program, \"Ulysses\", \"Cassini–Huygens\", \"New Horizons\", and the \"Galileo\" orbiter. It is also the farthest solar-powered trip in the history of space exploration. Once in orbit around Jupiter, \"Juno\" receives only 4% as much sunlight as it would on Earth, but the global shortage of plutonium-238, as well as advances made in solar cell technology over the past several decades, makes it economically preferable to use solar panels of practical size to provide power at a distance of 5 AU from the Sun.\n\nThe \"Juno\" spacecraft uses three solar panels symmetrically arranged around the spacecraft. Shortly after it cleared Earth's atmosphere, the panels were deployed. Two of the panels have four hinged segments each, and the third panel has three segments and a magnetometer. Each panel is long, the biggest on any NASA deep-space probe.\n\nThe combined mass of the three panels is nearly . If the panels were optimized to operate at Earth, they would produce 12 to 14 kilowatts of power. Only about 486 W was generated when \"Juno\" arrived at Jupiter, projected to decline to near 420 W as radiation degrades the cells. The solar panels will remain in sunlight continuously from launch through the end of the mission, except for short periods during the operation of the main engine and eclipses by Jupiter. A central power distribution and drive unit monitors the power that is generated by the solar panels and distributes it to instruments, heaters, and experiment sensors, as well as to batteries that are charged when excess power is available. Two 55-amp-hour lithium-ion batteries that are able to withstand the radiation environment of Jupiter provide power when \"Juno\" passes through eclipse.\n\n\"Juno\" uses in-band signaling (\"tones\") for several critical operations as well as status reporting during cruise mode, but it is expected to be used infrequently. Communications are via the and antennas of the NASA Deep Space Network (DSN) utilizing an X band direct link. The command and data processing of the \"Juno\" spacecraft includes a flight computer capable of providing about 50 Mbit/s of instrument throughput. Gravity science subsystems use the X-band and K-band Doppler tracking and autoranging.\n\nDue to telecommunications constraints, \"Juno\" will only be able to return about 40 megabytes of JunoCam data during each 11-day orbital period, limiting the number of images that are captured and transmitted during each orbit to somewhere between 10 and 100 depending on the compression level used. The overall amount of data downlinked on each orbit is significantly higher and used for the mission's scientific instruments; JunoCam is intended for public outreach and is thus secondary to the science data. This is comparable to the previous \"Galileo\" mission that orbited Jupiter, which captured thousands of images despite its slow data rate of 1000 bit/s (at maximum compression level) due to the failure of its high-gain antenna.\n\nThe communication system is also used as part of the Gravity Science experiment.\n\n\"Juno\" uses a LEROS 1b main engine with hypergolic propellant, manufactured by Moog Inc in Westcott, Buckinghamshire, England. It uses hydrazine and nitrogen tetroxide for propulsion and provides a thrust of 645 newtons. The engine bell is enclosed in a debris shield fixed to the spacecraft body, and is used for major burns. For control of the vehicle's orientation (attitude control) and to perform trajectory correction maneuvers, \"Juno\" utilizes a monopropellant reaction control system (RCS) consisting of twelve small thrusters that are mounted on four engine modules.\n\n\"Juno\" carries a plaque to Jupiter, dedicated to Galileo Galilei. The plaque was provided by the Italian Space Agency and measures . It is made of flight-grade aluminum and weighs . The plaque depicts a portrait of Galileo and a text in Galileo's own handwriting, penned in January 1610, while observing what would later be known to be the Galilean moons. The text translates as:\n\nThe spacecraft also carries three Lego minifigures representing Galileo, the Roman god Jupiter, and his sister and wife, the goddess Juno. In Roman mythology, Jupiter drew a veil of clouds around himself to hide his mischief. Juno was able to peer through the clouds and reveal Jupiter's true nature. The Juno minifigure holds a magnifying glass as a sign of searching for the truth, and Jupiter holds a lightning bolt. The third Lego crew member, Galileo Galilei, has his telescope with him on the journey. The figurines were produced in partnership between NASA and Lego as part of an outreach program to inspire children's interest in science, technology, engineering, and mathematics (STEM). Although most Lego toys are made of plastic, Lego specially made these minifigures of aluminum to endure the extreme conditions of space flight.\n\n\n"}
{"id": "37573969", "url": "https://en.wikipedia.org/wiki?curid=37573969", "title": "Ladeana Hillier", "text": "Ladeana Hillier\n\nLadeana Hillier is a biomedical engineer and computational biologist. She was one of the earliest scientists involved in the Human Genome Project and is noted for her work in various branches of DNA sequencing, as well as for having co-developed Phred, a widely used DNA trace analyzer.\n\n"}
{"id": "13566669", "url": "https://en.wikipedia.org/wiki?curid=13566669", "title": "Leo Cluster", "text": "Leo Cluster\n\nThe Leo Cluster (Abell 1367) is a galaxy cluster about 330 million light-years distant (z = 0.022) in the constellation Leo, with at least 70 major galaxies. The galaxy known as NGC 3842 is the brightest member of this cluster. Along with the Coma Cluster, it is one of the two major clusters comprising the Coma Supercluster, which in turn is part of the CfA2 Great Wall, which is hundreds of millions light years long and is one of the largest known structures in the universe. \n\nA team of scientists decided to observe the Leo Cluster with the intention of creating a catalog of extended ionized gas clouds, or EIGs. This data also led to the discovery of many star-forming parents (galaxies) within the cluster. These star-forming galaxies turned out to be very similar to those found in the neighboring Coma cluster. The EIGs in the Leo cluster, however, turned out to be longer in the Leo cluster compared to the Coma cluster. This likely means that the Leo cluster and its stars are probably younger than most comparable clusters in the universe and evolve at a different pace. \n\nMost dense galaxy clusters are composed mostly of elliptical galaxies. The Leo Cluster, however, mostly contains spiral galaxies, suggesting that it is much younger than other comparable clusters, such as the Coma Cluster. It is also home to one of the universe's largest known black holes, which lies in the center of NGC 3842. The black hole is 9.7 billion times more massive than our sun. \n\nIt can be very difficult for stars to form within the Leo Cluster. This is because infalling galaxies have a tendency to strip gas away from other stars that are attempting to form. This has led to the creation of a \"hot zone\" where stars are unable to maintain their gas long enough to properly form.\n\nThere appears to be a number of subpopulations within the Leo Cluster. The first consists of elliptical galaxies that seem to be roughly as old as the universe. The second subpopulation contains red-sequence lenticular (lens shaped) galaxies whose ages are directly tied to their mass. The third and final subpopulation is of galaxies where star formation is still taking place,and are morphologically distributed. \n\n\n"}
{"id": "46827997", "url": "https://en.wikipedia.org/wiki?curid=46827997", "title": "Lighthouse Labs", "text": "Lighthouse Labs\n\nLighthouse Labs is a coding bootcamp for web and mobile software development in multiple cities across Canada. The establishment organizes an annual free learn-to-code event, The HTML500, in partnership with Telus. More recently, the bootcamp became the first organization to offer a remote-hybrid web development program in Whitehorse through a partnership with the Government of Yukon.\n\nOpened in 2013, Lighthouse Labs' first immersive bootcamp facility is based out of Launch Academy, an incubator in the heart of the city's Gastown neighborhood. Students learn front and back-end development through the bootcamp's Web and iOS programs. The iOS program, launched in July 2014, is arguably Canada's first immersive iOS immersive program.\n\nAs part of a national partnership with Highline, a seed stage investment platform, Lighthouse Labs announced the launch of its Toronto operations during The HTML500 in February 2015. Located in Highline's office in downtown Toronto, the facility offers enrollment in any of the bootcamp's full-time or part-time programs. It joins existing Toronto-based bootcamps such as Bitmaker Labs, BrainStation, HackerYou, and Red Academy as one of several developer bootcamps that offer in-person learning opportunities to individuals in the Greater Toronto Area.\n\nIn April 2015, Lighthouse Labs launched a 'pop-up' program in Calgary through its national partnership with Highline, a co-venture platform that helps early stage digital startups. Lighthouse Labs in Calgary currently offers students the opportunity to enroll in a 6-week part-time program focused primarily on learning the fundamentals of web development.\n\nIn partnership with the Government of Yukon, Lighthouse Labs launched the territory's first web development bootcamp via a remote-hybrid program. Students remotely attend lectures from the bootcamp's Vancouver location and have access to on-site assistance through Mentors in the facility \n\nThe 'Lighthouse Labs Prize' is an annual scholarship open to students of Simon Fraser University's Faculty of Communication, Art & Technology. Worth over $7000, the scholarship awards one student a spot in the bootcamp's web-development program in Vancouver and is currently in its second year of institution.\n\n\n"}
{"id": "39217282", "url": "https://en.wikipedia.org/wiki?curid=39217282", "title": "List of Mars analogs", "text": "List of Mars analogs\n\nThis is list of Mars analogs, which simulate aspects of the conditions human beings could experience during a future mission to Mars, or different aspects of Mars such as its materials or conditions. This is often used for testing aspects of spacecraft missions to that planet\n\n\nSome examples of analog tests with people include NASA conducting a 120-day study in Hawaii to test a space food diet (HI-SEAS), and equipment tests inside Austrian mountain caves in 2012. A future Mars base has been compared to the Amundsen-Scott South Pole Station in Antarctica, because relatively small groups must survive in extreme conditions there.\n\nMars analogs are sometimes chosen for their location, for example, Devon Island is at 75°N latitude which provides solar radiance similar to the Martian Equator. Similarly, high altitudes can provide an equivalent to the low pressure of the Mars atmosphere\n\nMars regolith has been attempted to be replicated by Mars regolith simulant\n\nAmong these are:\n\n\nAt about 28 miles (45 km, 150 thousand feet ) Earth altitude the pressure starts to be equivalent to Mars surface pressure. However, the major component of Mars air, CO gas, is denser than Earth air for a given pressure. Perhaps more significantly there is no land at this altitude on earth. The highest point on earth is the summit of Mount Everest at about 5.5 miles (8.8 km, 29 thousand feet), where the pressure is about fifty times greater than on the surface of Mars. The correct atmospheric pressure can be created by a vacuum chamber. NASA's Space Power Facility was used to test the airbag landing systems for the Mars Pathfinder and the Mars Exploration Rovers, Spirit and Opportunity, under simulated Mars atmospheric conditions.\n\nThe gravity of Mars is about 38% of Earth's gravity at the surface, about 3.7 meters per second. This can be simulated for short time by an aircraft following a flight profile that causes this type of acceleration. This technique (using a variation on free-fall) has allowed the gait of people in Mars gravity to be studied.\n\nThe Russians conducted the BIOS-3 study in the 60s and 70s which had 315 cubic meters of space, and a later confinement study was Mars 500.\n\n"}
{"id": "11486250", "url": "https://en.wikipedia.org/wiki?curid=11486250", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: X", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: X\n\n\n"}
{"id": "1355700", "url": "https://en.wikipedia.org/wiki?curid=1355700", "title": "Louis Bolk", "text": "Louis Bolk\n\nLodewijk 'Louis' Bolk (December 10, 1866, Overschie – June 17, 1930, Amsterdam) was a Dutch anatomist who created the fetalization theory about the human body. It states that when a human being is born, it is still a fetus, as can be seen if one pays attention to its (proportionally) big head, to its uncoordinated motility or to its absolute helplessness, for instance. Furthermore, this \"prematuration\" is specifically human.\n\nGavin de Beer and Stephen Jay Gould wrote about him and further developed this theory, which is sometimes called neoteny. \n\nAlso Jacques Lacan took Bolk's fetalization theory into account in order to introduce his own thesis on the mirror stage.\n"}
{"id": "12538295", "url": "https://en.wikipedia.org/wiki?curid=12538295", "title": "Madagascar sucker-footed bat", "text": "Madagascar sucker-footed bat\n\nThe Madagascar sucker-footed bat, Old World sucker-footed bat, or simply sucker-footed bat (\"Myzopoda aurita\") is a species of bat in the family Myzopodidae endemic to Madagascar, especially in the eastern part of the forests. The genus was thought to be monospecific until a second species, \"Myzopoda schliemanni\", was discovered in the central western lowlands. It was classified as Vulnerable in the 1996 IUCN Red List of Threatened Species but is now known to be more abundant and was reclassified in 2008 as of \"Least Concern\".\n\nThe bat is named for the presence of small cups on its wrists and ankles. They roost inside the rolled leaves of the traveller's tree, using their suckers to attach themselves to the smooth surface. Despite the name, it is now known that the bats do not use suction to attach themselves to roost sites, but instead use a form of wet adhesion by secreting a body fluid at their pads.\n\nBecause of its unique habitat, there are no ectoparasites on sucker-footed bats because the surface of the leaves are too smooth for arthropods. The majority of sucker-footed bats caught in eastern Madagascar were within or close to stands of traveller's trees, and according to research, the maximum distance they can travel while foraging is about . Sucker-footed bats feed largely on beetles and small moths.\n\n"}
{"id": "27140683", "url": "https://en.wikipedia.org/wiki?curid=27140683", "title": "Maurice Cossmann", "text": "Maurice Cossmann\n\nMaurice Cossmann, full name Alexandre Édouard Maurice Cossmann (18 September 1850 – 17 May 1924) was a French paleontologist and malacologist.\n\nMaurice Cossmann's father was an artist draughtsman and a talented lithographer. His early education was at Condorcet College in Paris and he later gained the Diploma of the Central School of Arts and Manufacturing in the same city. He was then employed by the Compagnie des chemins de fer du Nord. Cossmann made all his career there, finally as Chief of the Engineering services (Ingénieur Chef des Services Techniques). Married and without a child, he loved to tell how the arrival of a small collection of fossils had led to his future studies of paleontology. Cossman specialised in the fossils of the Paleogene and Neogene periods. Certain of his works are still a major reference especially:\n\nCossman published, sometimes collaboratively, 186 works (scientific papers and monographs) between 1879 and 1926 and he was the Editor of \"Revue critique de paléozoologie et de paléophytologie\" Paris 1897–1919. His work is described in two obituaries written by G.F.Dollfus in 1925 and A.R.Kabat in 1989.\n"}
{"id": "29388477", "url": "https://en.wikipedia.org/wiki?curid=29388477", "title": "Multiple-prism grating laser oscillator", "text": "Multiple-prism grating laser oscillator\n\nMultiple-prism grating laser oscillators, or MPG laser oscillators, use multiple-prism beam expansion to illuminate a diffraction grating mounted either in Littrow configuration or grazing-incidence configuration. Originally, these narrow-linewidth tunable dispersive oscillators were introduced as multiple-prism Littrow (MPL) grating oscillators, or hybrid multiple-prism near-grazing-incidence (HMPGI) grating cavities, in organic dye lasers. However, these designs were quickly adopted for other types of lasers such as gas lasers, diode lasers, and more recently fiber lasers. \n\nMultiple-prism grating laser oscillators can be excited either electrically, as in the case of gas lasers and semiconductor lasers, or optically, as in the case of crystalline lasers and organic dye lasers. In the case of optical excitation it is often necessary to match the polarization of the excitation laser to the polarization preference of the multiple-prism grating oscillator. This can be done using a polarization rotator thus improving the laser conversion efficiency.\n\nThe multiple-prism dispersion theory is applied to design these beam expanders either in additive configuration, thus adding or subtracting their dispersion to the dispersion of the grating, or in compensating configuration (yielding zero dispersion at a design wavelength) thus allowing the diffraction grating to control the tuning characteristics of the laser cavity. Under those conditions, that is, zero dispersion from the multiple-prism beam expander, the single-pass laser linewidth is given by\n\nwhere formula_2 is the beam divergence and \"M\" is the beam magnification provided by the beam expander that multiplies the angular dispersion provided by the diffraction grating. In the case of multiple-prism beam expanders this factor can be as high as 100-200.\n\nWhen the dispersion of the multiple-prism expander is not equal to zero, then the single-pass linewidth is given by\n\nwhere the first differential refers to the angular dispersion from the grating and the second differential refers to the overall dispersion from the multiple-prism beam expander.\n\nOptimized solid-state multiple-prism grating laser oscillators have been shown, by Duarte, to generate pulsed single-longitudinal-mode emission limited only by Heisenberg's uncertainty principle. The laser linewidth in these experiments is reported as formula_4 ≈ 350 MHz (or formula_5 ≈ 0.0004 nm at 590 nm) in pulses ~ 3 ns wide, at power levels in the kW regime.\n\nApplications of these tunable narrow-linewidth lasers include:\n\n\n\n"}
{"id": "884094", "url": "https://en.wikipedia.org/wiki?curid=884094", "title": "Near Infrared Camera and Multi-Object Spectrometer", "text": "Near Infrared Camera and Multi-Object Spectrometer\n\nThe Near Infrared Camera and Multi-Object Spectrometer (NICMOS) is a scientific instrument for infrared astronomy, installed on the Hubble Space Telescope (HST), operating from 1997 to 1999, and from 2002 to 2008. Images produced by NICMOS contain data from the near-infrared part of the light spectrum.\n\nNICMOS was conceived and designed by the NICMOS Instrument Definition Team centered at Steward Observatory, University of Arizona, USA. NICMOS is an imager and spectrometer built by Ball Aerospace & Technologies Corp. that allows the HST to observe infrared light, with wavelengths between 0.8 and 2.4 micrometers, providing imaging and slitless spectrophotometric capabilities. NICMOS contains three near-infrared detectors in three optical channels providing high (~ 0.1 arcsecond) resolution, coronagraphic and polarimetric imaging, and slitless spectroscopy in 11-, 19-, and 52-arcsecond square fields of view. Each optical channel contains a 256×256 pixel photodiode array of mercury cadmium telluride infrared detectors bonded to a sapphire substrate, read out in four independent 128×128 quadrants.\n\nNICMOS last worked in 2008, and has been largely replaced by the infrared channel of Wide Field Camera 3 after its installation in 2009.\n\nThe infrared performance of the Hubble has limitations since it was not designed with infrared performance as an objective. For example, the mirror is kept at a stable and relatively high temperature (15 °C) by heaters.\n\nHST is a warm telescope. The IR background flux collected by cooled focal plane IR instruments like NICMOS or WFC3 is dominated, at rather short wavelengths, by telescope thermal emission rather than by zodiacal scattering. NICMOS data show that the telescope background exceeds the zodiacal background at wavelengths longer than λ ≈ 1.6μm, the exact value depending on the pointing on the sky and on the position of the Earth on its orbit.\n\nDespite this, the combination of Hubble's mirror and NICMOS offered never-before seen levels of quality in near-infrared performance at that time. Dedicated infrared telescopes like the Infrared Space Observatory were ground-breaking in their own way, but had a smaller primary mirror, and were also out of service at the time of NICMOS installation because they ran out of coolant. NICMOS later overcame this problem by using a machine chiller like a refrigerator, which allowed it operate for years until it went offline in 2008\n\nNICMOS was installed on Hubble during its second servicing mission in 1997 (STS-82) along with the Space Telescope Imaging Spectrograph, replacing two earlier instruments. NICMOS in turn has been largely superseded by the Wide Field Camera 3, which has a much larger field of view (135 by 127 arcsec, or 2.3 by 2.1 arcminutes), and reaches almost as far into the infrared. \n\nWhen conducting infrared measurements, it is necessary to keep the infrared detectors cooled to avoid having infrared interference from the instrument's own thermal emissions. NICMOS contains a cryogenic dewar, that cooled its detectors to about 61 K, and optical filters to ~ 105 K, with a block of solid nitrogen ice. When NICMOS was installed in 1997, the dewar flask contained a 230-pound (104 kg) block of nitrogen ice. Due to a thermal short that arose on March 4, 1997, during the instrument commissioning, the dewar ran out of nitrogen coolant sooner than expected in January 1999.\n\nDuring Hubble Service Mission 3B in 2002,(STS-109) a replacement cooling system comprising a cryocooler, cryogenic circulator, and external radiator was installed on the Hubble that now cools NICMOS through a cryogenic neon loop. NICMOS was returned to service soon after SM 3B.\n\nA new software upload in September 2008 necessitated a brief shutdown of the NICMOS cooling system. Several attempts to restart the cooling system were unsuccessful due to issues with the cryogenic circulator. After waiting more than six weeks for parts of the instrument to warm up, and theorized ice particles to sublimate from the neon circulating loop, the cooler once again failed to restart. An Anomaly Review Board (ARB) was then convened by NASA. The ARB concluded that ice or other solid particle migrated from the dewar to the circulator during the September 2008 restart attempt and that the circulator may be damaged, and determined an alternative set of startup parameters. A successful restart at 13:30 EST on 16 December 2008 led to four days of cooler operations followed by another shutdown. On 1 August 2009, the cooler was restarted again; NICMOS was expected to resume operations in mid-February 2010 and operated through October 22, 2009, at which point a lock-up of Hubble's data handling system caused the telescope to shut down. The circulation flow rate to NICMOS was greatly reduced during this operating period confirming blockage in the circulation loop. Continued operation at reduced flow rates would limit NICMOS science so plans for purging and refilling the circulation system with clean neon gas were developed by NASA. The circulation loop is equipped with an extra neon tank and remotely operated solenoid valves for on-orbit purge-fill operations. As of 2013, these purge-fill operations have not yet been performed.\n\nOn June 18, 2010, it was announced NICMOS would not be available for science during the latest proposal Cycle 18. As of 2013, a decision as to whether the purge-fill operations will be performed and whether NICMOS will be available for science in the future has not been made.\n\nNICMOS is also the name of the devices's 256×256-pixel imaging sensor built by Rockwell International Electro-Optical Center (now DRS Technologies).\n\nNICMOS was noted for its performance in Near-infrared space astronomy, in particular its ability to see objects through dust. It was used for about 23 months after it was installed, its life limited by set amount of cryo-coolant, and then later it was used for several years when a new cyro-cooler was installed in 2002. NICMOS combined near infrared performance with a large mirror.\n\nNICMOS allowed investigation of high redshift galaxies and QSO with high spatial resolution, which was especially useful when analyzed in conjunction with other instruments such as the STIS, and it also allowed deeper investigation of stellar populations. In planetary science, NICMOS was used to discover an impact basin on the bottom asteroid 4 Vesta. (4 Vesta was later visited by Dawn (spacecraft) in the 2010s which investigated it more closely by orbiting it)\n\nIn 2009, an old NICMOS image was processed to show a predicted exoplanet around the star HR 8799. The system is thought to be about 130 light-years from Earth.\n\nIn 2011, around that same star, four exoplanets were rendered viewable in a NICMOS image taken in 1998, using advanced data processing. The exoplanets were originally discovered with the Keck telescopes and the Gemini North telescope between 2007 and 2010. The image allows the orbits of the exoplanets to be analyzed more closely, since they take many decades, even hundreds of Earth years, to orbit their host star.\n\nNICMOS observed the exoplanet XO-2b at star XO-2, and a spectroscopy result was obtained for this exoplanet in 2012. This uses the spectroscopic abilities of the instrument, and in astronomy spectroscopy during a planetary transit (an exoplanet passes in front of star from the perspective of Earth) is a way to study that exoplanet's possible atmosphere.\n\nIn 2014, researchers recovered planetary discs in old NICMOS data using new image processing techniques.\n\n\n\n"}
{"id": "8143703", "url": "https://en.wikipedia.org/wiki?curid=8143703", "title": "On Human Nature", "text": "On Human Nature\n\nOn Human Nature (1978; second edition 2004) is a book by Harvard biologist E. O. Wilson, in which the author attempts to explain human nature and society through sociobiology. Wilson argues that evolution has left its traces on characteristics such as generosity, self-sacrifice, worship and the use of sex for pleasure, and proposes a sociobiological explanation of homosexuality. He attempts to complete the Darwinian revolution by bringing biological thought into social sciences and humanities. Wilson describes \"On Human Nature\" as a sequel to his earlier books \"The Insect Societies\" (1971) and \"\" (1975).\n\nThe book won the Pulitzer Prize in 1979.\n\nWilson writes that \"On Human Nature\" is the third of a trilogy, the previous volumes of which were \"The Insect Societies\" (1971) and \"Sociobiology: The New Synthesis\" (1975), and that its thesis is that general sociobiology, \"the extension of population biology and evolutionary theory to social organization\", is the appropriate means of closing \"the famous gap between the two cultures\". He proposes that homosexuality may be \"a distinctive beneficent behavior that evolved as an important element of early human social organization\", describing it as \"above all a form of bonding\", possibly based on a genetic predisposition.\n\nThe biologist Jerry Coyne, writing in \"The New Republic\", accused Wilson of trying to use evolutionary psychology to control social science and social policy, arguing that \"On Human Nature\" was similar in this respect to Wilson's subsequent book \"Consilience\" (1998) and to the biologist Randy Thornhill and the anthropologist Craig Palmer's \"A Natural History of Rape\" (2000).\n\nBryan Walsh, writing in \"Time\" magazine in 2011, named \"On Human Nature\" as one of the \"100 best and most influential\" books written in English since 1923. He considered Wilson's \"real achievement\" to be to \"show how a sociobiological view of humanity could still have grandeur.\" The computer scientist Paul Brown, writing in \"Skeptical Inquirer\" in 2018, stated that \"On Human Nature\" is \"still brimful with ideas and insights about who we are, how we got here, and how to get wherever we want to go.\"\n\n\"On Human Nature\" was discussed by Gregory Hanlon in the \"Journal of Interdisciplinary History\". Hanlon credited Wilson with helping to establish that human behavior could not be understood solely in terms of \"learned cultural values\", that the behavioral sciences could help to explain \"the interpersonal actions in past societies.\" He compared the book to the ethnologist Irenäus Eibl-Eibesfeldt's \"Human Ethology\" (1989) and the historian Daniel Lord Smail's \"Deep History and the Brain\" (2008).\n\nThe anthropologist Sarah Blaffer Hrdy, writing in \"The Woman That Never Evolved\" (1981), argued that a reading of the book refutes the accusation that Wilson aims to use sociobiology to reinforce traditional sex roles. The philosopher Roger Scruton, writing in \"Sexual Desire\" (1986), criticized Wilson's sociobiological explanations of human social behavior, arguing that because of Wilson's \"polemical purpose\" he was forced to oversimplify the facts. However, he granted that sociobiological explanations of the sort favored by Wilson might possibly be correct. The anthropologist Donald E. Brown, writing in \"Human Universals\" (1991), commented that he at first failed to read Wilson's book because his views were still conditioned by the \"sociocultural perspectives\" in which he had been trained. However, Brown concluded that \"sociobiologists might be more convincing if they confined their explanations to universals rather than attempting to show that virtually everything that humans do somehow maximizes their reproductive success.\"\n\nScience writers John Gribbin and Mary Gribbin, writing in \"Being Human\" (1993), described \"On Human Nature\" as an \"accessible account of the application of sociobiology to people\". The sociologist Ullica Segerstråle, writing in \"Defenders of the Truth: The Battle for Science in the Sociobiology Debate and Beyond\" (2000), considered \"On Human Nature\" essentially a development of Wilson's earlier ideas. Segerstråle commented that, unlike opponents of sociobiology, Wilson saw it as having liberal political implications, and tried to develop these suggestions in \"On Human Nature\". The psychologist David P. Barash, writing with Ilona A. Barash in \"The Mammal in the Mirror: Understanding Our Place in the Natural World\" (2001), called \"On Human Nature\", \"A wide-ranging, thoughtful, and controversial classic of human sociobiology\".\n\n\"On Human Nature\" won a 1979 Pulitzer Prize.\n\n\n\n\n\n"}
{"id": "8320597", "url": "https://en.wikipedia.org/wiki?curid=8320597", "title": "Operation Moonwatch", "text": "Operation Moonwatch\n\nOperation Moonwatch (also known as \"Project Moonwatch\" and, more simply, as \"Moonwatch\") was an amateur science program formally initiated by the Smithsonian Astrophysical Observatory (SAO) in 1956. The SAO organized Moonwatch as part of the International Geophysical Year (IGY) which was probably the largest single scientific undertaking in history. Its initial goal was to enlist the aid of amateur astronomers and other citizens who would help professional scientists spot the first artificial satellites. However, until professionally manned optical tracking stations came on-line in 1958, this network of amateur scientists and other interested citizens played a critical role in providing crucial information regarding the world’s first satellites.\n\nMoonwatch’s origins can be traced to two sources. In the United States, there was a thriving culture of amateur scientists including thousands of citizens who did astronomy for an avocation. During the Cold War, the United States also encouraged thousands of citizens to take part in the Ground Observer Corps, a nationwide program to spot Soviet bombers. Moonwatch brought together these two activities and attitudes, melding curiosity and vigilance into a thriving activity for citizens. Moonwatch, in other words, was an expression of 1950s popular culture and fixed properly within the context of the Cold War.\n\nMoonwatch was the brainchild of Harvard astronomer Fred L. Whipple. In 1955, as the recently appointed director of the Smithsonian Astrophysical Observatory in Cambridge, MA, Whipple proposed that amateurs could play a vital role in efforts to track the first satellites. He overcame the objections of colleagues who doubted ordinary citizens could do the job or who wanted the task for their own institutions. Eventually, Whipple carved out a place for amateurs in the IGY.\n\nIn the late 1950s, thousands of teenagers, housewives, amateur astronomers, school teachers, and other citizens served on Moonwatch teams around the globe. Initially conceived as a way for citizens to participate in science and as a supplement to professionally manned optical and radio tracking stations, Moonwatchers around the world found themselves an essential component of the professional scientists’ research program. Using specially designed telescopes, hand-built or purchased from vendors like Radio Shack, scores of Moonwatchers nightly monitored the skies. Their prompt response was aided by the extensive training they had done by spotting pebbles tossed in the air, registering the flight of moths, and participating in national alerts organized by the Civil Air Patrol.\n\nOnce professional scientists had accepted the idea that ordinary citizens could spot satellites and contribute to legitimate scientific research, Whipple and his colleagues organized amateurs around the world. Citizens formed Operation Moonwatch teams in towns and cities all around the globe, built their own equipment, and courted sponsors. In many cases, Moonwatch was not just a fad but an expression of real interest in science. By October 1957, Operation Moonwatch had some 200 teams ready to go into action, including observers in Hawaii and Australia \n\nWhipple envisioned a global network of specially designed instruments that could track and photograph satellites. This network, aided by a corps of volunteer satellite spotters and a computer at the MIT Computation Center, would establish ephemerides – predictions of where a satellite will be at particular times. The instruments at these stations were eventually designed by Dr. James G. Baker and Joseph Nunn and hence known as Baker-Nunn cameras. Based on a series of super-Schmidt wide-angle telescopes and strategically placed around the globe at 12 locations, the innovative cameras could track rapidly moving targets while simultaneously viewing large swaths of the sky. \n\nFrom the start, Whipple planned that the professionally manned Baker-Nunn stations would be complemented by teams of dedicated amateurs. Amateur satellite spotters would inform the Baker-Nunn stations as to where to look, an important task given that scientists working on the Vanguard program likened finding a satellite in the sky to finding a golf ball tossed out of a jet plane. Amateur teams would relay the information back to the SAO in Cambridge where professional scientists would use it to generate accurate satellite orbits. At this point, professionals at the Baker-Nunn stations would take over the full-time task of photographing them.\n\n\"Sputnik 1's\" sudden launch was followed less than a month later with the Soviets orbiting Sputnik 2 and the dog Laika. Moonwatch teams networked around the world who provided tracking information needed by scientists in Western nations. For the opening months of the Space Age, members of Moonwatch were the only organized worldwide network that was prepared to spot and help track satellites. The information they provided was complemented by the radio tracking program called Minitrack the United States Navy operated as well as some information from amateur radio buffs.\n\nIn many cases, Moonwatch teams also had the responsibility of communicating news of Sputnik and the first American satellites to the public. The public responded, in turn, with infectious enthusiasm as local radio stations aired times to spot satellites and local and national newspapers ran hundreds of articles that described the nighttime activities of Moonwatchers.\n\nMoonwatch caught the attention of those citizens interested in science or the Space Race during the late 1950s and much of the general public as well. Newspapers and popular magazines featured stories about Moonwatch regularly; dozens of articles appeared in the Los Angeles Times, The New Yorker, and The New York Times alone. Meanwhile, in the U.S. local businesses sponsored teams with monikers like Spacehounds and The Order of Lunartiks. Meanwhile, Moonwatch teams in Peru, Japan, Australia, and even the Arctic regularly sent their observations to the Smithsonian.\n\nMoonwatch complemented the professional system of satellite tracking stations that Fred Whipple organized around the globe. These two networks – one composed of amateurs and the other of seasoned professionals – helped further Whipple’s personal goals of expanding his own astronomical empire. Operation Moonwatch was the most successful amateur activity of the IGY and it became the public face of a satellite tracking network that expanded the Smithsonian’s global reach. Whipple used satellite tracking as a gateway for his observatory to participate in new research opportunities that appeared in the early years of space exploration.\n\nIn February 1958, President Dwight D. Eisenhower publicly thanked the SAO, Fred Whipple, and the global corps of satellite spotters that comprised Moonwatch for their efforts in tracking the first Soviet and American satellites.\n\nEven after the IGY ended, the Smithsonian maintained Operation Moonwatch. Hundreds of dedicated amateur scientists continued to help NASA and other agencies track satellites. Their observations often rivaled those of professional tracking stations, blurring the boundary between professional and amateur. Moonwatch members and the Smithsonian were important contributors to US Department of Defense satellite tracking research and development efforts, 1957–1961; see Project Space Track.\n\nMoonwatch continued long after the IGY ended in 1958. In fact, the Smithsonian operated Moonwatch until 1975 making it one of the longest running amateur science activities ever. As the fad of satellite spotting passed, the Smithsonian refashioned Operation Moonwatch to perform new functions. It encouraged teams of dedicated amateurs to contribute increasingly precise data for satellite tracking. Moonwatchers adapted to the needs of the Smithsonian through the activities of “hard core” groups in places like Walnut Creek, California. Throughout the 1960s, the Smithsonian gave them ever more challenging assignments such as locating extremely faint satellites and tracking satellites as they re-entered the earth’s atmosphere. At times, the precise observations and calculations of dedicated Moonwatchers surpassed the work of professionals.\n\nOne of the most notable activities of Moonwatchers after the IGY was the observance of Sputnik 4 when it reentered the atmosphere in September 1962. Moonwatchers and other amateur scientists near Milwaukee, WI observed the flaming re-entry and their observations eventually led to the recovery and analysis of several fragments from the Soviet satellite.\n\nMoonwatch affected the lives of participants long after they stopped looking for satellites. When the Smithsonian discontinued the program in 1975, one long-time Moonwatcher compared his participation to “winning the Medal of Honor.” Moonwatch inspired some future scientists, for example, James A. Westphal, a Moonwatcher from Oklahoma, who eventually helped design instruments for the Hubble Space Telescope at Caltech. The program boosted science programs at many schools throughout the country and helped revitalize amateur science in the United States.\n\nThe United States Space Surveillance Network and other modern tracking systems are professional and automated, but amateurs remain active in satellite watching.\n\n\n"}
{"id": "40528847", "url": "https://en.wikipedia.org/wiki?curid=40528847", "title": "Outcome primacy", "text": "Outcome primacy\n\nOutcome primacy is a psychological phenomenon that describes lasting effects on a subject's behavior based on the outcome of first experiences with a given task or decision. It was found that this outcome primacy can account for much of the underweighting of rare events in experience based decisions, where participants apparently underestimate small probabilities (in contrast to prospect theory where people tend to overestimate low probabilities, when lotteries are described).\n\nBehaviour in this task can be modelled using a standard, model-free reinforcement learning algorithm. In this model, the values of the different actions are learned over time and are used to determine the next action according to a predefined action-selection rule. It was shown that a substantial effect of first experience on behaviour is consistent with the reinforcement learning model if one assumes that the outcome of first experience resets the values of the experienced actions, but not if symmetric initial conditions are assumed.\n\nMoreover, the predictive power of the resetting model outperforms previously published models regarding the aggregate choice behaviour.\n\nThese findings suggest that first experience has a disproportionately large effect on subsequent actions, similar to primacy effects in other fields of cognitive psychology, such as in the application of the serial position effect. The mechanism of resetting of the initial conditions that underlies outcome primacy may thus also account for other forms of primacy.\n"}
{"id": "55602882", "url": "https://en.wikipedia.org/wiki?curid=55602882", "title": "Patricia Whitelock", "text": "Patricia Whitelock\n\nProfessor Patricia Ann Whitelock is a South African astrophysicist and a member of the Academy of Science of South Africa.\n\nShe was acting director of the South African Astronomical Observatory from June 2002 to November 2003 and was the director in 2012. Whitelock has published over 100 peer-reviewed scholarly articles. Her research includes that on stellar evolution and Local Group galaxies.\n"}
{"id": "32122183", "url": "https://en.wikipedia.org/wiki?curid=32122183", "title": "Seed germinator", "text": "Seed germinator\n\nA seed germinator is a device for germinating seeds. Typically, these create an environment in which light, humidity and temperature are controlled to provide optimum conditions for the germination of seeds.\n\nOne type of germinator is the Copenhagen or Jacobsen tank. The seeds rest upon blotting paper which is kept moist by wicks which draw from a bath of water whose temperature is regulated. The humidity around each seed is kept high by means of glass funnels and a lid covering the tank.\n"}
{"id": "29802605", "url": "https://en.wikipedia.org/wiki?curid=29802605", "title": "Sphingobacteria (phylum)", "text": "Sphingobacteria (phylum)\n\nSphingobacteria is a division (phylum), created by Cavalier-Smith, which contains the classes Chlorobea, Fibrobacteres, Bacteroidetes and Flavobacteria.\n\nIt is however not followed by the larger scientific community. The group is commonly referred to as the \"FCB group\" with the rank of superphylum and the subdivisions are of the rank phylum and are referred to as:\n\nAn analogous situation is seen with the PVC group/Planctobacteria.\n"}
{"id": "224636", "url": "https://en.wikipedia.org/wiki?curid=224636", "title": "Supersymmetry", "text": "Supersymmetry\n\nIn particle physics, supersymmetry (SUSY) is a principle that proposes a relationship between two basic classes of elementary particles: bosons, which have an integer-valued spin, and fermions, which have a half-integer spin. A type of spacetime symmetry, supersymmetry is a possible candidate for undiscovered particle physics, and seen as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to be incomplete. A supersymmetrical extension to the Standard Model would resolve major hierarchy problems within gauge theory, by guaranteeing that quadratic divergences of all orders will cancel out in perturbation theory.\n\nIn supersymmetry, each particle from one group would have an associated particle in the other, which is known as its superpartner, the spin of which differs by a half-integer. These superpartners would be new and undiscovered particles. For example, there would be a particle called a \"selectron\" (superpartner electron), a bosonic partner of the electron. In the simplest supersymmetry theories, with perfectly \"unbroken\" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. Since we expect to find these \"superpartners\" using present-day equipment, if supersymmetry exists then it consists of a spontaneously broken symmetry allowing superpartners to differ in mass.\n\nThere is no evidence at this time to show whether or not supersymmetry is correct, or what other extensions to current models might be more accurate. In part this is because it is only since around 2010 that particle accelerators specifically designed to study physics beyond the Standard Model have become operational, and because it is not yet known where exactly to look nor the energies required for a successful search. \n\nThe main reasons for supersymmetry being supported by physicists is that the current theories are known to be incomplete and their limitations are well established, and supersymmetry would be an attractive solution to some of the major concerns. Direct confirmation would entail production of superpartners in collider experiments, such as the Large Hadron Collider (LHC). The first runs of the LHC found no previously-unknown particles other than the Higgs boson which was already suspected to exist as part of the Standard Model, and therefore no evidence for supersymmetry. Indirect methods include the search for a permanent electric dipole moment (EDM) in the known Standard Model particles, which can arise when the Standard Model particle interacts with the supersymmetric particles. The current best constraint on the electron electric dipole moment put it to be smaller than 10 e·cm, equivalent to a sensitivity to new physics at the TeV scale and matching that of the current best particle colliders. A permanent EDM in any fundamental particle points towards time-reversal violating physics, and therefore also CP-symmetry violation via the CPT theorem. Such EDM experiments are also much more scalable than conventional particle accelerators and offer a practical alternative to detecting physics beyond the standard model as accelerator experiments become increasingly costly and complicated to maintain.\n\nThese findings disappointed many physicists, who believed that supersymmetry (and other theories relying upon it) were by far the most promising theories for \"new\" physics, and had hoped for signs of unexpected results from these runs. Former enthusiastic supporter Mikhail Shifman went as far as urging the theoretical community to search for new ideas and accept that supersymmetry was a failed theory. However it has also been argued that this \"naturalness\" crisis was premature, because various calculations were too optimistic about the limits of masses which would allow a supersymmetry based solution.\n\nThere are numerous phenomenological motivations for supersymmetry close to the electroweak scale, as well as technical motivations for supersymmetry at any scale.\n\nSupersymmetry close to the electroweak scale ameliorates the hierarchy problem that afflicts the Standard Model. In the Standard Model, the electroweak scale receives enormous Planck-scale quantum corrections. The observed hierarchy between the electroweak scale and the Planck scale must be achieved with extraordinary fine tuning. In a supersymmetric theory, on the other hand, Planck-scale quantum corrections cancel between partners and superpartners (owing to a minus sign associated with fermionic loops). The hierarchy between the electroweak scale and the Planck scale is achieved in a natural manner, without miraculous fine-tuning.\n\nThe idea that the gauge symmetry groups unify at high-energy is called Grand unification theory. In the Standard Model, however, the weak, strong and electromagnetic couplings fail to unify at high energy. In a supersymmetry theory, the running of the gauge couplings are modified, and precise high-energy unification of the gauge couplings is achieved. The modified running also provides a natural mechanism for radiative electroweak symmetry breaking.\n\nTeV-scale supersymmetry (augmented with a discrete symmetry) typically provides a candidate dark matter particle at a mass scale consistent with thermal relic abundance calculations.\n\nSupersymmetry is also motivated by solutions to several theoretical problems, for generally providing many desirable mathematical properties, and for ensuring sensible behavior at high energies. Supersymmetric quantum field theory is often much easier to analyze, as many more problems become mathematically tractable. When supersymmetry is imposed as a \"local\" symmetry, Einstein's theory of general relativity is included automatically, and the result is said to be a theory of supergravity. It is also a necessary feature of the most popular candidate for a theory of everything, superstring theory, and a SUSY theory could explain the issue of cosmological inflation.\n\nAnother theoretically appealing property of supersymmetry is that it offers the only \"loophole\" to the Coleman–Mandula theorem, which prohibits spacetime and internal symmetries from being combined in any nontrivial way, for quantum field theories like the Standard Model with very general assumptions. The Haag–Łopuszański–Sohnius theorem demonstrates that supersymmetry is the only way spacetime and internal symmetries can be combined consistently.\n\nA supersymmetry relating mesons and baryons was first proposed, in the context of hadronic physics, by Hironari Miyazawa in 1966. This supersymmetry did not involve spacetime, that is, it concerned internal symmetry, and was broken badly. Miyazawa's work was largely ignored at the time.\n\nJ. L. Gervais and B. Sakita (in 1971), Yu. A. Golfand and E. P. Likhtman (also in 1971), and D. V. Volkov and V. P. Akulov (1972), independently rediscovered supersymmetry in the context of quantum field theory, a radically new type of symmetry of spacetime and fundamental fields, which establishes a relationship between elementary particles of different quantum nature, bosons and fermions, and unifies spacetime and internal symmetries of microscopic phenomena. Supersymmetry with a consistent Lie-algebraic graded structure on which the Gervais−Sakita rediscovery was based directly first arose in 1971 in the context of an early version of string theory by Pierre Ramond, John H. Schwarz and André Neveu.\n\nFinally, Julius Wess and Bruno Zumino (in 1974) identified the characteristic renormalization features of four-dimensional supersymmetric field theories, which identified them as remarkable QFTs, and they and Abdus Salam and their fellow researchers introduced early particle physics applications. The mathematical structure of supersymmetry (graded Lie superalgebras) has subsequently been applied successfully to other topics of physics, ranging from nuclear physics, critical phenomena, quantum mechanics to statistical physics. It remains a vital part of many proposed theories of physics.\n\nThe first realistic supersymmetric version of the Standard Model was proposed in 1977 by Pierre Fayet and is known as the Minimal Supersymmetric Standard Model or MSSM for short. It was proposed to solve, amongst other things, the hierarchy problem.\n\nOne reason that physicists explored supersymmetry is because it offers an extension to the more familiar symmetries of quantum field theory. These symmetries are grouped into the Poincaré group and internal symmetries and the Coleman–Mandula theorem showed that under certain assumptions, the symmetries of the S-matrix must be a direct product of the Poincaré group with a compact internal symmetry group or if there is not any mass gap, the conformal group with a compact internal symmetry group. In 1971 Golfand and Likhtman were the first to show that the Poincaré algebra can be extended through introduction of four anticommuting spinor generators (in four dimensions), which later became known as supercharges.\nin 1975 the Haag-Lopuszanski-Sohnius theorem analyzed all possible superalgebras in the general form, including those with an extended number of the supergenerators and central charges. This extended super-Poincaré algebra paved the way for obtaining a very large and important class of supersymmetric field theories.\n\nTraditional symmetries of physics are generated by objects that transform by the tensor representations of the Poincaré group and internal symmetries. Supersymmetries, however, are generated by objects that transform by the spin representations. According to the spin-statistics theorem, bosonic fields commute while fermionic fields anticommute. Combining the two kinds of fields into a single algebra requires the introduction of a Z-grading under which the bosons are the even elements and the fermions are the odd elements. Such an algebra is called a Lie superalgebra.\n\nThe simplest supersymmetric extension of the Poincaré algebra is the Super-Poincaré algebra. Expressed in terms of two Weyl spinors, has the following anti-commutation relation:\nand all other anti-commutation relations between the \"Q\"s and commutation relations between the \"Q\"s and \"P\"s vanish. In the above expression formula_2 are the generators of translation and formula_3 are the Pauli matrices.\n\nThere are representations of a Lie superalgebra that are analogous to representations of a Lie algebra. Each Lie algebra has an associated Lie group and a Lie superalgebra can sometimes be extended into representations of a Lie supergroup.\n\nIncorporating supersymmetry into the Standard Model requires doubling the number of particles since there is no way that any of the particles in the Standard Model can be superpartners of each other. With the addition of new particles, there are many possible new interactions. The simplest possible supersymmetric model consistent with the Standard Model is the Minimal Supersymmetric Standard Model (MSSM) which can include the necessary additional new particles that are able to be superpartners of those in the Standard Model.\nOne of the main motivations for SUSY comes from the quadratically divergent contributions to the Higgs mass squared. The quantum mechanical interactions of the Higgs boson causes a large renormalization of the Higgs mass and unless there is an accidental cancellation, the natural size of the Higgs mass is the greatest scale possible. This problem is known as the hierarchy problem. Supersymmetry reduces the size of the quantum corrections by having automatic cancellations between fermionic and bosonic Higgs interactions. If supersymmetry is restored at the weak scale, then the Higgs mass is related to supersymmetry breaking which can be induced from small non-perturbative effects explaining the vastly different scales in the weak interactions and gravitational interactions.\n\nIn many supersymmetric Standard Models there is a heavy stable particle (such as neutralino) which could serve as a weakly interacting massive particle (WIMP) dark matter candidate. The existence of a supersymmetric dark matter candidate is related closely to R-parity.\n\nThe standard paradigm for incorporating supersymmetry into a realistic theory is to have the underlying dynamics of the theory be supersymmetric, but the ground state of the theory does not respect the symmetry and supersymmetry is broken spontaneously. The supersymmetry break can not be done permanently by the particles of the MSSM as they currently appear. This means that there is a new sector of the theory that is responsible for the breaking. The only constraint on this new sector is that it must break supersymmetry permanently and must give superparticles TeV scale masses. There are many models that can do this and most of their details do not matter. In order to parameterize the relevant features of supersymmetry breaking, arbitrary soft SUSY breaking terms are added to the theory which temporarily break SUSY explicitly but could never arise from a complete theory of supersymmetry breaking.\n\nOne piece of evidence for supersymmetry existing is gauge coupling unification.\nThe renormalization group evolution of the three gauge coupling constants of the Standard Model is somewhat sensitive to the present particle content of the theory. These coupling constants do not quite meet together at a common energy scale if we run the renormalization group using the Standard Model. With the addition of minimal SUSY joint convergence of the coupling constants is projected at approximately 10 GeV.\n\n\"Supersymmetric quantum mechanics\" adds the SUSY superalgebra to quantum mechanics as opposed to quantum field theory. Supersymmetric quantum mechanics often becomes relevant when studying the dynamics of supersymmetric solitons, and due to the simplified nature of having fields which are only functions of time (rather than space-time), a great deal of progress has been made in this subject and it is now studied in its own right.\n\nSUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called \"partner Hamiltonians\". (The potential energy terms which occur in the Hamiltonians are then known as \"partner potentials\".) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy. This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a \"bosonic Hamiltonian\", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be \"fermionic\", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy.\n\nSUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker-Planck equation being an example of a non-quantum theory. The 'supersymmetry' in all these systems arises from the fact that one is modelling one particle and as such the 'statistics' don't matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called 'problem of the denominator' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see the book\n\nIntegrated optics was recently found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics\n\nAll stochastic (partial) differential equations, the models for all types of continuous time dynamical systems, possess topological supersymmetry. In the operator representation of stochastic evolution, the topological supersymmetry is the exterior derivative which is commutative with the stochastic evolution operator defined as the stochastically averaged pullback induced on differential forms by SDE-defined diffeomorphisms of the phase space. The topological sector of the so-emerging supersymmetric theory of stochastic dynamics can be recognized as the Witten-type topological field theory.\n\nThe meaning of the topological supersymmetry in dynamical systems is the preservation of the phase space continuity—infinitely close points will remain close during continuous time evolution even in the presence of noise. When the topological supersymmetry is broken spontaneously, this property is violated in the limit of the infinitely long temporal evolution and the model can be said to exhibit (the stochastic generalization of) the butterfly effect. From a more general perspective, spontaneous breakdown of the topological supersymmetry is the theoretical essence of the ubiquitous dynamical phenomenon variously known as chaos, turbulence, self-organized criticality etc. The Goldstone theorem explains the associated emergence of the long-range dynamical behavior that manifests itself as 1/f noise, butterfly effect, and the scale-free statistics of sudden (instantonic) processes, e.g., earthquakes, neuroavalanches, solar flares etc., known as the Zipf's law and the Richter scale.\n\nSUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful \"toy models\" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.\n\nThe proof of the Atiyah-Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.\n\nSupersymmetry is part of superstring theory, a string theory of quantum gravity, although it could in theory be a component of other quantum gravity theories as well, such as loop quantum gravity. For superstring theory to be consistent, supersymmetry seems to be required at some level (although it may be a strongly broken symmetry). If experimental evidence confirms supersymmetry in the form of supersymmetric particles such as the neutralino that is often believed to be the lightest superpartner, some people believe this would be a major boost to superstring theory. Since supersymmetry is a required component of superstring theory, any discovered supersymmetry would be consistent with superstring theory. If the Large Hadron Collider and other major particle physics experiments fail to detect supersymmetric partners, many versions of superstring theory which had predicted certain low mass superpartners to existing particles may need to be significantly revised.\n\nSupersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.\n\nIt is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2, i.e. 1, 2, 4, 8. In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.\n\nThe maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This is due to the Weinberg-Witten theorem. This corresponds to an \"N\" = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.\n\nFor four dimensions there are the following theories, with the corresponding multiplets (CPT adds a copy, whenever they are not invariant under such symmetry)\nChiral multiplet:\nVector multiplet:\nGravitino multiplet:\nGraviton multiplet:\nhypermultiplet:\nvector multiplet:\nsupergravity multiplet:\nVector multiplet:\nSupergravity multiplet:\nSupergravity multiplet:\n\nIt is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In \"d\" dimensions, the size of spinors is approximately 2 or 2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.\n\nFractional supersymmetry is a generalization of the notion of supersymmetry in which the minimal positive amount of spin does not have to be formula_4 but can be an arbitrary formula_5 for integer value of \"N\". Such a generalization is possible in two or less spacetime dimensions.\n\nSupersymmetric models are constrained by a variety of experiments, including measurements of low-energy observables – for example, the anomalous magnetic moment of the muon at Fermilab; the WMAP dark matter density measurement and direct detection experiments – for example, XENON-100 and LUX; and by particle collider experiments, including B-physics, Higgs phenomenology and direct searches for superpartners (sparticles), at the Large Electron–Positron Collider, Tevatron and the LHC.\n\nHistorically, the tightest limits were from direct production at colliders. The first mass limits for squarks and gluinos were made at CERN by the UA1 experiment and the UA2 experiment at the Super Proton Synchrotron. LEP later set very strong limits, which in 2006 were extended by the D0 experiment at the Tevatron. From 2003-2015, WMAP's and Planck's dark matter density measurements have strongly constrained supersymmetry models, which, if they explain dark matter, have to be tuned to invoke a particular mechanism to sufficiently reduce the neutralino density.\n\nPrior to the beginning of the LHC, in 2009 fits of available data to CMSSM and NUHM1 indicated that squarks and gluinos were most likely to have masses in the 500 to 800 GeV range, though values as high as 2.5 TeV were allowed with low probabilities. Neutralinos and sleptons were expected to be quite light, with the lightest neutralino and the lightest stau most likely to be found between 100 and 150 GeV.\n\nThe first run of the LHC found no evidence for supersymmetry, and, as a result, surpassed existing experimental limits from the Large Electron–Positron Collider and Tevatron and partially excluded the aforementioned expected ranges.\n\nIn 2011–12, the LHC discovered a Higgs boson with a mass of about 125 GeV, and with couplings to fermions and bosons which are consistent with the Standard Model. The MSSM predicts that the mass of the lightest Higgs boson should not be much higher than the mass of the Z boson, and, in the absence of fine tuning (with the supersymmetry breaking scale on the order of 1 TeV), should not exceed 135 GeV.\n\nThe LHC result seemed problematic for the minimal supersymmetric model, as the value of 125 GeV is relatively large for the model and can only be achieved with large radiative loop corrections from top squarks, which many theorists had considered to be \"unnatural\" (see naturalness (physics) and fine tuning).\n\n\n\n"}
{"id": "9651630", "url": "https://en.wikipedia.org/wiki?curid=9651630", "title": "The Measure of Our Days", "text": "The Measure of Our Days\n\nThe Measure of Our Days: A Spiritual Exploration of Illness (alternately \"New Beginnings at Life's End\") is a book of case studies of patients by Dr. Jerome Groopman, published by Penguin Books in October 1997. It was later serialized in \"The New Yorker\" and in \"The Boston Globe Sunday Magazine\". In 2000, it became the inspiration for the TV show \"Gideon's Crossing\", which was nominated for a Golden Globe.\n"}
{"id": "14522051", "url": "https://en.wikipedia.org/wiki?curid=14522051", "title": "The Natural History and Antiquities of Selborne", "text": "The Natural History and Antiquities of Selborne\n\nThe Natural History and Antiquities of Selborne, or just The Natural History of Selborne is a book by English naturalist and ornithologist Gilbert White. It was first published in 1789 by his brother Benjamin. It has been continuously in print since then, with nearly 300 editions up to 2007.\n\nThe book was published late in White's life, compiled from a mixture of his letters to other naturalists — Thomas Pennant and Daines Barrington; a 'Naturalist's Calendar' (in the second edition) comparing phenology observations made by White and William Markwick of the first appearances in the year of different animals and plants; and observations of natural history organized more or less systematically by species and group. A second volume, less often reprinted, covered the antiquities of Selborne. Some of the letters were never posted, and were written for the book.\n\nWhite's \"Natural History\" was at once well received by contemporary critics and the public, and continued to be admired by a diverse range of nineteenth and twentieth century literary figures including Samuel Taylor Coleridge, Thomas Carlyle, Charles Darwin, John Ruskin, Virginia Woolf, and W. H. Auden. His work has been seen as an early contribution to ecology and in particular to phenology. The book has been enjoyed for its charm and apparent simplicity, and the way that it creates a vision of pre-industrial England.\n\nThe original manuscript has been preserved and is displayed in the Gilbert White museum at The Wakes, Selborne.\n\nThe main part of the book, the \"Natural History\", is presented as a compilation of 44 letters nominally to Thomas Pennant, a leading British zoologist of the day, and 66 letters to Daines Barrington, an English barrister and Fellow of the Royal Society. In these letters, White details the natural history of the area around his family home at the vicarage of Selborne in Hampshire.\n\nMany of the 'letters' were never posted, and were written especially for the book. Patrick Armstrong, in his book \"The English Parson-Naturalist\", notes that in particular, \"an obvious example is the first, nominally to Thomas Pennant, but which is clearly contrived, as it introduces the parish, briefly summarizing its position, geography and principal physical features.\" White's biographer, Richard Mabey, estimates that up to 46 out of 66 'letters to Daines Barrington' \"were probably never sent through the post\"; Mabey explains that it is hard to be more precise, because of White's extensive editing. Some letters are dated although never sent. Some dates have been altered. Some letters have been cut down, split into shorter 'letters', merged, or distributed in small parts into other letters. A section about insect-eating birds in a letter sent to Barrington in 1770 appears in the book as letter 41 to Pennant. Personal remarks have been removed throughout. Thus, while the book is genuinely based on letters to Pennant and Barrington, the structure of the book is a literary device.\n\nAs a compilation of letters and other materials, the book as a whole has an uneven structure. The first part is a diary-like sequence of 'letters', with the breaks and wanderings that naturally follow. The second is a calendar, organized by phenological event around the year. The third is a collection of observations, organised by animal or plant group and species, with a section on meteorology. The apparently rambling structure of the book is in fact bracketed by opening and closing sections, arranged like the rest as letters, which \"give form and scale and even a semblance of narrative structure to what would otherwise have been a shapeless anthology.\" The unposted Letter 1 begins\n\n\"No novelist could have opened better\", wrote Virginia Woolf; \"Selborne is set solidly in the foreground.\"\n\nThe first edition was illustrated with paintings by the Swiss artist Samuel Hieronymus Grimm, engraved by W. Angus and aquatinted. Grimm had lived in England since 1768, and was quite a famous artist, costing 2½ guineas per week. In the event, he stayed in Selborne for 28 days, and White recorded that he worked very hard on 24 of them. White also described Grimm's method, which was to sketch the landscape in lead pencil, then to put in the shading, and finally to add a light wash of watercolour. The illustrations were engraved (signed at lower right) by a variety of engravers including William Angus and Peter Mazell.\n\nThere are 44 letters to White's friend Thomas Pennant (1726–1798), of which the first nine were never posted and are thus undated. Of those that were posted, the first, Letter 10 giving an overview of Selborne, is dated 4 August 1767; the last, Letter 44 on wood pigeons, is dated 30 November 1780. It is not known how the men became friends, or even if they ever met; White writes repeatedly that he would like to meet \"to have a little conversation face to face after we have corresponded so freely for several years\" so it is certain they did not meet for long periods, and possible they never met at all. The letters are edited from the form in which they were actually posted; for example, Letter 10 as posted had a cringing introductory paragraph of thanks to Pennant which White edited out of the published version.\n\nThere are 66 letters to the lawyer Daines Barrington (1727–1800), occupying half the book. Letter 1, on summer birds of passage, is dated 30 June 1769; Letter 66, on thunderstorms, is dated 25 June 1787. The Barrington letters therefore largely overlap the time frame of those to Pennant, but began and ended somewhat later. It was Barrington who suggested to White that he should write a book from his observations; although Pennant had been corresponding with White for a while, he was relying on White for natural history information for his own books, and, suggests White's biographer Richard Mabey, must have wanted White as a continuing source of information, not as a rival author. Barrington, on the other hand, liked to theorize about the natural world, but had little interest in making observations himself, and tended to accept claimed facts uncritically.\n\nA character in some of the letters is a tortoise:\n\nLetter 65 describes the summer of 1783 as \"an amazing and portentous one, and full of horrible phenomena; for, besides the alarming meteors and tremendous thunderstorms that affrighted and distressed the different counties of this kingdom, the peculiar haze, or smoky fog, that prevailed for many weeks in this island, and in every part of Europe, and even beyond its limits, was a most extraordinary appearance, unlike anything known within the memory of man ... The sun, at noon, looked as blank as a clouded moon, and shed a rust-coloured ferruginous light on the ground, and floors of rooms; but was particularly lurid and blood-coloured at rising and setting. All this time the heat was so intense that butcher's meat could hardly be eaten on the day after it was killed ...\" This was caused by the eruption of the Laki volcano in Iceland between 8 June 1783 and February 1784, killing up to a quarter of the people of Iceland and spreading a haze as far as Egypt.\n\nThis section, often omitted from later editions, consists like the \"Natural History\" of 26 \"Letters\", none of them posted, and without even the fiction of being addressed to Pennant or Barrington. Letter 1 begins \"It is reasonable to suppose that in remote ages this woody and mountainous district was inhabited only by bears and wolves.\" Letter 2 discusses Selborne in Saxon times; Selborne was according to White a royal manor, belonging to Editha, queen to Edward the Confessor. Letter 3 describes the village's church, which \"has no pretensions to antiquity, and is, as I suppose, of no earlier date than the beginning of the reign of Henry VII.\" Letter 5 describes the ancient Yew tree in the churchyard. Letter 7 describes the (ruined) priory. Letter 11 discusses the properties of the Knights Templar in and near the village.\n\nLetter 14 describes the visit of bishop William of Wykeham in 1373, to correct the scandalous \"particular abuses\" in the religious houses in the parish. He orders the canons of Selborne priory (Item 5th) \"to take care that the doors of their church and priory be so attended to that no suspected and disorderly females, \"suspectae at aliae inhonestae\", pass through their choir and cloiser in the dark\"; (Item 10th) to cease \"living dissolutely after the flesh, and not after the spirit\" as it has been proven that some of the canons \"sleep naked in their beds without their breeches and shirts\"; (Item 11th) to stop \"keeping hounds, and publicly attending hunting-matches\" and \"noisy tumultuous huntings\"; (Item 17th) to properly maintain their houses and the convent itself, since they have allowed \"through neglect, notorious dilapidations to take place\"; (Item 29th) to stop wearing \"foppish ornaments, and the affectation of appearing like beaux with garments edged with costly furs, with fringed gloves, and silken girdles trimmed with gold and silver.\" Richard Mabey describes White's reaction to the \"Priory saga\" as \"grave disapproval of the monks' sensuality and ... general delinquency\".\n\nA sequence of Letters then relate the history of the priors of Selborne, until Letter 24 which relates the takeover of the priory by Magdalen College, Oxford under bishop William Waynflete in 1459. White describes this as a disastrous fall: \"Thus fell the considerable and well-endowed priory of Selborne after it had subsisted about two hundred and fifty-four years; about seventy-four years after the suppression of priories alien by Henry V., and about fifty years before the general dissolution of the monasteries by Henry VIII.\" The final letter records that \"No sooner did the priory .. become an appendage to the college, but it must at once have tended to swift decay.\" White notes that since then, even \"the very foundations have been torn up for the repair of the highways\" so that nothing is left but a rough pasture \"full of hillocks and pits, choaked with nettles, and dwarf-elder, and trampled by the feet of the ox and the heifer\". White had reason to be bitter about the takeover by Magdalen College, as it had made them Lords of the Manor of Selborne, which in turn gave them the right to appoint the parish priest. White's biographer Richard Mabey casts doubt on the \"frequent assumption\" that White's \"deepest regret was that he could never be vicar of Selborne\", but it was true that he was ineligible, as only fellows of Magdalen could be granted the living.\n\nThis section, compiled posthumously, contains a list of some 500 phenological observations in Selborne from White's manuscripts, organised by William Markwick (1739–1812), and supplemented by Markwick's own observations from Catsfield, near Battle, Sussex. The observations depend on the latitude of these places and on the (global) climate, forming a baseline for comparison with modern observations. For example, \"Cuckoo \"(Cuculus canorus)\" heard\" is recorded by White for 7—26 April, and by Markwick for 15 April and 3 May (presumably only once at the earlier date) and \"last heard\" by Markwick on 28 June. The table begins as follows:\n\nThis is the longest section of the observations, with comments in each instance by Markwick.\n\nThese are a few entries on sheep, rabbits, cats and squirrels, horse and hounds.\n\nThe 'Vermes' cover glow-worms, earthworms, snails and slugs, and a \"snake's slough\", a cast skin.\n\nThe observations relate to trees, seeds, \"beans sown by birds\", \"cucumbers set by bees\", and a few fungi (truffles, \"Tremella nostoc\", and fairy rings).\n\nThese are a few curiosities such as frozen sleet and the \"black spring\" of 1771. He also recorded the effects on the weather of the 1783 volcanic eruption of the Icelandic crater Laki.\n\nWhite's lifelong friend John Mulso wrote to him in 1776, correctly predicting that \"Your work, upon the whole, will immortalize your Place of Abode as well as Yourself.\"\n\nThomas White wrote \"a long, appreciative, but.. properly restrained review\" of his brother's book in \"The Gentleman's Magazine\" of January 1789, commenting that \"Sagacity of observation runs through the work\".\n\nAn anonymous reviewer in \"The Topographer\" of April 1789 wrote that \"A more delightful, or more original work than Mr. White's History of Selborne has seldom been published ... Natural History has evidently been the author's principal study, and, of that, ornithology is evidently the favourite. The book is not a compilation from former publications, but the result of many years' attentive observations to nature itself, which are told not only with the precision of a philosopher, but with that happy selection of circumstances, which mark the \"poet\".\"\n\nIn 1830, an anonymous critic, in what critic Tobias Menely called a description of Selborne \"as a place that lingers beyond the spatio-temporal horizon of modern life\", wrote having visited the village that\n\nThe 1907–1921 Cambridge History of English and American Literature begins its essay on White's \"Selborne\" with the words\n\nWhite is sometimes treated as a pioneer of ecology. The British ornithologist James Fisher gives a more balanced view, writing in 1941: \n\nThe medical historian Richard Barnett writes that\n\nBarnett notes, too, that\n\nYale nonfiction tutor Fred Strebeigh, writing in \"Audubon magazine\" in 1988, compared White with Henry Thoreau's Walden:\n\nTobias Menely of Indiana University notes that the book \"has garnered praise from Coleridge, Carlyle, Darwin, Ruskin, Woolf, and Auden\" and that\n\nThe naturalist Richard Mabey writes in his biography of White that\n\nThe manuscript for the book stayed in the White family until 1895, when it was auctioned at Sotheby's. The purchaser was Stuart M. Samuel, who mounted the letters and bound the book in green Morocco leather. His library was sold in 1907. The manuscript was bought by the dealer A.S.W. Rosenbach in 1923, and passed into the collection of Arthur A. Houghton. The Houghton collection was auctioned by Christie's in 1980, where the manuscript was purchased by and for Gilbert White's museum at The Wakes, Selborne, where it is displayed.\n\nThomas Bewick, in the first volume (\"Land Birds\") of his \"A History of British Birds\" (1797), presents a phenological list of 19 birds which are \"chiefly selected from Mr. White's Natural History of Selborne, and are arranged nearly in the order of their appearing\". The list begins with the wryneck (\"Middle of March\"), places the cuckoo in the middle of April, and ends with the flycatcher in the middle of May.\n\nCharles Darwin read the \"Natural History\" as a young man, inspiring him to take \"much pleasure in watching the habits of birds\" and to wonder \"why every gentleman did not become an ornithologist\". Sara Losh, too, read the \"Natural History\" as part of her \"wonderful, varied and advanced [home] education for a young girl\".\n\nWhite's \"Natural History\" has been continuously in print since its first publication. A paperback edition of \"The Illustrated Natural History of Selborne\" was reprinted by Thames & Hudson in 2007. It was long held (\"apocryphally\", according to White's biographer, Richard Mabey) to be the fourth-most published book in the English language after the Bible, the works of Shakespeare, and John Bunyan's \"The Pilgrim's Progress\".\n\nWhite's frequent accounts in \"The Natural History and Antiquities of Selborne\" of his tortoise Timothy, inherited from his aunt, form the basis for a variety of literary mentions. Verlyn Klinkenborg's book, \"Timothy; or, Notes of an Abject Reptile\" (2006) is based wholly on that reptile, as is Sylvia Townsend Warner's \"The Portrait of a Tortoise\" (1946). The tortoise also finds its way into science, as its species, \"Testudo whitei\" (Bennett 1836), long thought to be a synonym of \"Testudo graeca\", has been rediscovered in Algeria.\n\nVarious writers have commented on the book. The poet Samuel Taylor Coleridge called it \"This sweet delightful book\". The novelist Virginia Woolf observed that \"By some apparently unconscious device .. a door [is] left open, through which we hear distant sounds.\" Among poets, Edward Thomas wrote that \"In this present year, 1915, at least, it is hard to find a flaw in the life he led\" while W. H. Auden stated that \"Selfishly, I, too, would have plumbed to know you: I could have learned so much.\" The naturalist and broadcaster David Attenborough called White \"A man in total harmony with his world.\" The novelist Roald Dahl has the main character in his short story \"The Visitor\" read the book. The writer and zookeeper Gerald Durrell commented in \"The Amateur Naturalist\" that White \"simply observed nature with a sharp eye and wrote about it lovingly.\"\n\n\n\n"}
{"id": "21842531", "url": "https://en.wikipedia.org/wiki?curid=21842531", "title": "The OpenMS Proteomics Pipeline", "text": "The OpenMS Proteomics Pipeline\n\nThe OpenMS Proteomics Pipeline (TOPP) is a set of computational tools that can be chained together to tailor problem-specific analysis pipelines for HPLC-MS data. It transforms most of the OpenMS functionality into small command line tools that are the building blocks for more complex analysis pipelines. The functionality of the tools ranges from data preprocessing (file format conversion, baseline reduction, noise reduction, peak picking, map alignment...) over quantitation (isotope-labeled and label-free) to identification (wrapper tools for Mascot, Sequest, InsPecT and OMSSA).\n\nTOPP is developed in the groups of Prof. Knut Reinert at the Free University of Berlin and in the group of Prof. Kohlbacher at the University of Tübingen.\n\nFor more detailed information about the TOPP tools, see the TOPP documentation of the latest release and the TOPP publication in the references.\n\nThe OpenMS Proteomics Pipeline is free software released under the 3-clause BSD license.\n\n"}
{"id": "53614338", "url": "https://en.wikipedia.org/wiki?curid=53614338", "title": "Unihalt", "text": "Unihalt\n\nUnihalt is a Visakhapatnam, India, based data analytics startup. It is first financially sustainable startup mentored by Startup Village. Unihalt is one of the IBM SmartCamp Vizag winners. They are picked up under IBM GEP for support.\n\nDanfeng Li, Director and the Chief Data Officer at Alibaba Group joined Unihalt as an advisor on its board.\n"}
{"id": "38043761", "url": "https://en.wikipedia.org/wiki?curid=38043761", "title": "University Physics", "text": "University Physics\n\nUniversity Physics is the name of a two-volume physics textbook written by Hugh Young and Roger Freedman. The first edition of \"University Physics\" was published by Mark Zemansky and Francis Sears in 1949. Hugh Young became a coauthor with Sears and Zemansky in 1973. Now in its 14th edition, \"University Physics\" is among the most widely used introductory textbooks in the world.\n\nUniversity Physics by Pearson is not to be confused with a free textbook by the same name, available from OpenStax\n\n"}
{"id": "32782", "url": "https://en.wikipedia.org/wiki?curid=32782", "title": "Voyager 2", "text": "Voyager 2\n\nVoyager 2 is a space probe launched by NASA on August 20, 1977, to study the outer planets. Part of the Voyager program, it was launched 16 days before its twin, \"Voyager 1\", on a trajectory that took longer to reach Jupiter and Saturn but enabled further encounters with Uranus and Neptune. It is the only spacecraft to have visited either of the ice giants.\n\nIts primary mission ended with the exploration of the Neptunian system on October 2, 1989, after having visited the Uranian system in 1986, the Saturnian system in 1981, and the Jovian system in 1979. \"Voyager 2\" is now in its extended mission to study the outer reaches of the Solar System and has been operating for as of . It remains in contact through the Deep Space Network.\n\nAt a distance of from the Sun as of November 02, 2018, \"Voyager 2\" is the fourth of five spacecraft to achieve the escape velocity that will allow them to leave the Solar System. The probe was moving at a velocity of relative to the Sun as of November 2018 and is traveling through the heliosheath. Upon reaching interstellar space, \"Voyager 2\" is expected to provide the first direct measurements of the density and temperature of the interstellar plasma.\n\nIn the early space age, it was realized that a coincidental alignment of the outer planets would occur in the late 1970s and enable a single probe to visit Jupiter, Saturn, Uranus, and Neptune by taking advantage of the then-new technique of gravity assists. NASA began work on a Grand Tour, which evolved into a massive project involving two groups of two probes each, with one group visiting Jupiter, Saturn, and Pluto and the other Jupiter, Uranus, and Neptune. The spacecraft would be designed with redundant systems to ensure survival through the entire tour. By 1972 the mission was scaled back and replaced with two Mariner-derived spacecraft, the Mariner Jupiter-Saturn probes. To keep apparent lifetime program costs low, the mission would include only flybys of Jupiter and Saturn, but keep the Grand Tour option open. As the program progressed, the name was changed to Voyager.\n\nThe primary mission of \"Voyager 1\" was to explore Jupiter, Saturn, and Saturn's moon, Titan. \"Voyager 2\" was also to explore Jupiter and Saturn, but on a trajectory that would have option of continuing on to Uranus and Neptune, or being redirected to Titan as a backup for \"Voyager 1\". Upon successful completion of \"Voyager 1\"<nowiki>'</nowiki>s objectives, \"Voyager 2\" would get a mission extension to send the probe on towards Uranus and Neptune.\n\nConstructed by the Jet Propulsion Laboratory (JPL), \"Voyager 2\" included 16 hydrazine thrusters, three-axis stabilization, gyroscopes and celestial referencing instruments (Sun sensor/Canopus Star Tracker) to maintain pointing of the high-gain antenna toward Earth. Collectively these instruments are part of the Attitude and Articulation Control Subsystem (AACS) along with redundant units of most instruments and 8 backup thrusters. The spacecraft also included 11 scientific instruments to study celestial objects as it traveled through space.\n\nBuilt with the intent for eventual interstellar travel, \"Voyager 2\" included a large, parabolic, high-gain antenna () to transceive data via the Deep Space Network on the Earth. Communications are conducted over the S-band (about 13 cm wavelength) and X-band (about 3.6 cm wavelength) providing data rates as high as 115.2 kilobits per second at the distance of Jupiter, and then ever-decreasing as the distance increased, because of the inverse-square law. When the spacecraft is unable to communicate with Earth, the Digital Tape Recorder (DTR) can record about 64 kilobytes of data for transmission at another time.\n\nThe spacecraft was equipped with 3 Multihundred-Watt radioisotope thermoelectric generators (MHW RTG). Each RTG includes 24 pressed plutonium oxide spheres, and provided enough heat to generate approximately 157 W of electrical power at launch. Collectively, the RTGs supplied the spacecraft with 470 watts at launch (halving every 87.7 years), and will allow operations to continue until at least 2020.\n\n\"For more details on the Voyager space probes' identical instrument packages, see the separate article on the overall Voyager Program.\"\n\nThe \"Voyager 2\" probe was launched on August 20, 1977, by NASA from Space Launch Complex 41 at Cape Canaveral, Florida, aboard a Titan IIIE/Centaur launch vehicle. Two weeks later, the twin \"Voyager 1\" probe would be launched on September 5, 1977. However, \"Voyager 1\" would reach both Jupiter and Saturn sooner, as \"Voyager 2\" had been launched into a longer, more circular trajectory.\n\nIn April 1978, a complication arose when no commands were transmitted to \"Voyager 2\" for a period of time, causing the spacecraft to switch from its primary radio receiver to its backup receiver. Sometime afterwards, the primary receiver failed altogether. The backup receiver was functional, but a failed capacitor in the receiver meant that it could only receive transmissions that were sent at a precise frequency, and this frequency would be affected by the Earth's rotation (due to the Doppler effect) and the onboard receiver's temperature, among other things. For each subsequent transmission to \"Voyager 2\", it was necessary for engineers to calculate the specific frequency for the signal so that it could be received by the spacecraft.\n\"Voyager 2\"s closest approach to Jupiter occurred on July 9, 1979. It came within of the planet's cloud tops. It discovered a few rings around Jupiter, as well as volcanic activity on the moon Io.\n\nThe Great Red Spot was revealed as a complex storm moving in a counterclockwise direction. An array of other smaller storms and eddies were found throughout the banded clouds.\n\nDiscovery of active volcanism on Io was easily the greatest unexpected discovery at Jupiter. It was the first time active volcanoes had been seen on another body in the Solar System. Together, the Voyagers observed the eruption of nine volcanoes on Io, and there is evidence that other eruptions occurred between the two Voyager fly-bys.\n\nThe moon Europa displayed a large number of intersecting linear features in the low-resolution photos from \"Voyager 1\". At first, scientists believed the features might be deep cracks, caused by crustal rifting or tectonic processes. The closer high-resolution photos from \"Voyager 2\", however, left scientists puzzled: The features were so lacking in topographic relief that as one scientist described them, they \"might have been painted on with a felt marker.\" Europa is internally active due to tidal heating at a level about one-tenth that of Io. Europa is thought to have a thin crust (less than thick) of water ice, possibly floating on a 50-kilometer-deep (30 mile) ocean.\n\nTwo new, small satellites, Adrastea and Metis, were found orbiting just outside the ring. A third new satellite, Thebe, was discovered between the orbits of Amalthea and Io.\n\nThe closest approach to Saturn occurred on August 26, 1981.\n\nWhile passing behind Saturn (as viewed from Earth), \"Voyager 2\" probed Saturn's upper atmosphere with its radio link to gather information on atmospheric temperature and density profiles. \"Voyager 2\" found that at the uppermost pressure levels (seven kilopascals of pressure), Saturn's temperature was 70 kelvins (−203 °C), while at the deepest levels measured (120 kilopascals) the temperature increased to 143 K (−130 °C). The north pole was found to be 10 kelvins cooler, although this may be seasonal (\"see also Saturn Oppositions\").\n\nAfter the fly-by of Saturn, the camera platform of \"Voyager 2\" locked up briefly, putting plans to officially extend the mission to Uranus and Neptune in jeopardy. The mission's engineers were able to fix the problem (caused by an overuse that temporarily depleted its lubricant), and the \"Voyager 2\" probe was given the go-ahead to explore the Uranian system.\n\nThe closest approach to Uranus occurred on January 24, 1986, when \"Voyager 2\" came within of the planet's cloud tops. \"Voyager 2\" also discovered the moons Cordelia, Ophelia, Bianca, Cressida, Desdemona, Juliet, Portia, Rosalind, Belinda, Perdita and Puck; studied the planet's unique atmosphere, caused by its axial tilt of 97.8°; and examined the Uranian ring system.\n\nUranus is the third largest (Neptune has a larger mass, but a smaller volume) planet in the Solar System. It orbits the Sun at a distance of about 2.8 billion kilometers (1.7 billion miles), and it completes one orbit every 84 Earth years. The length of a day on Uranus as measured by \"Voyager 2\" is 17 hours, 14 minutes. Uranus is unique among the planets in that its axial tilt is about 90°, meaning that its axis is roughly parallel with, instead of roughly perpendicular to, the plane of the ecliptic. This extremely large tilt of its axis is thought to be the result of a collision between the accumulating planet Uranus with another planet-sized body early in the history of the Solar System. Given the unusual orientation of its axis, with the polar regions of Uranus exposed for periods of many years to either continuous sunlight or darkness, planetary scientists were not at all sure what to expect when observing Uranus.\n\n\"Voyager 2\" found that one of the most striking effects of the sideways orientation of Uranus is the effect on the tail of the planetary magnetic field. This is itself tilted about 60° from the Uranian axis of rotation. The planet's magneto tail was shown to be twisted by the rotation of Uranus into a long corkscrew shape following the planet. The presence of a significant magnetic field for Uranus was not at all known until \"Voyager 2\"s arrival.\n\nThe radiation belts of Uranus were found to be of an intensity similar to those of Saturn. The intensity of radiation within the Uranian belts is such that irradiation would \"quickly\" darken — within 100,000 years — any methane that is trapped in the icy surfaces of the inner moons and ring particles. This kind of darkening might have contributed to the darkened surfaces of the moons and the ring particles, which are almost uniformly dark gray in color.\n\nA high layer of haze was detected around the sunlit pole of Uranus. This area was also found to radiate large amounts of ultraviolet light, a phenomenon that is called \"dayglow.\" The average atmospheric temperature is about 60 K (−350°F/−213°C). Surprisingly, the illuminated and dark poles, and most of the planet, exhibit nearly the same temperatures at the cloud tops.\n\nThe Uranian moon Miranda, the innermost of the five large moons, was discovered to be one of the strangest bodies yet seen in the Solar System. Detailed images from \"Voyager 2\"s flyby of Miranda showed huge canyons made from geological faults as deep as , terraced layers, and a mixture of old and young surfaces. One hypothesis suggests that Miranda might consist of a reaggregation of material following an earlier event when Miranda was shattered into pieces by a violent impact.\n\nAll nine of the previously known Uranian rings were studied by the instruments of \"Voyager 2\". These measurements showed that the Uranian rings are distinctly different from those at Jupiter and Saturn. The Uranian ring system might be relatively young, and it did not form at the same time that Uranus did. The particles that make up the rings might be the remnants of a moon that was broken up by either a high-velocity impact or torn up by tidal effects.\n\nFollowing a mid-course correction in 1987, \"Voyager 2\"s closest approach to Neptune occurred on August 25, 1989. Because this was the last planet of the Solar System that \"Voyager 2\" could visit, the Chief Project Scientist, his staff members, and the flight controllers decided to also perform a close fly-by of Triton, the larger of Neptune's two originally known moons, so as to gather as much information on Neptune and Triton as possible, regardless of Voyager 2's departure angle from the planet. This was just like the case of \"Voyager 1's\" encounters with Saturn and its massive moon Titan.\n\nThrough repeated computerized test simulations of trajectories through the Neptunian system conducted in advance, flight controllers determined the best way to route \"Voyager 2\" through the Neptune-Triton system. Since the plane of the orbit of Triton is tilted significantly with respect to the plane of the ecliptic, through mid-course corrections, \"Voyager 2\" was directed into a path about three thousand miles above the north pole of Neptune. At that time, Triton was behind and below (south of) Neptune (at an angle of about 25 degrees below the ecliptic), close to the apoapsis of its elliptical orbit. The gravitational pull of Neptune bent the trajectory of \"Voyager 2\" down in the direction of Triton. In less than 24 hours, \"Voyager 2\" traversed the distance between Neptune and Triton, and then observed Triton's northern hemisphere as it passed over its north pole.\n\nThe net and final effect on \"Voyager 2\" was to bend its trajectory south below the plane of the ecliptic by about 30 degrees. \"Voyager 2\" is on this path permanently, and hence, it is exploring space south of the plane of the ecliptic, measuring magnetic fields, charged particles, etc., there, and sending the measurements back to the Earth via telemetry.\n\nWhile in the neighborhood of Neptune, \"Voyager 2\" discovered the \"Great Dark Spot\", which has since disappeared, according to observations by the Hubble Space Telescope. Originally thought to be a large cloud itself, the \"Great Dark Spot\" was later hypothesized to be a hole in the visible cloud deck of Neptune.\n\nWith the decision of the International Astronomical Union to reclassify Pluto as a \"dwarf planet\" in 2006, the flyby of Neptune by \"Voyager 2\" in 1989 became the point when every known planet in the Solar System had been visited at least once by a space probe.\n\nOnce its planetary mission was over, \"Voyager 2\" was described as working on an interstellar mission, which NASA is using to find out what the Solar System is like beyond the heliosphere. \"Voyager 2\" is currently transmitting scientific data at about 160 bits per second. Information about continuing telemetry exchanges with \"Voyager 2\" is available from Voyager Weekly Reports.\nIn 1992 \"Voyager 2\" observed the nova V1974 Cygni in the far-ultraviolet. \n\nIn July 1994 an attempt was made to observe the impacts fragments of the comet Comet Shoemaker–Levy 9 with Jupiter. The craft's position meant it had a direct line of sight to the impacts and observations were made in the ultraviolet and radio spectrum. Voyager 2 failed to detect anything with calculations showing that the fireballs were just below the craft's limit of detection.\n\nOn November 29, 2006, a telemetered command to \"Voyager 2\" was incorrectly decoded by its on-board computer—in a random error—as a command to turn on the electrical heaters of the spacecraft's magnetometer. These heaters remained turned on until December 4, 2006, and during that time, there was a resulting high temperature above , significantly higher than the magnetometers were designed to endure, and a sensor rotated away from the correct orientation. As of this date it had not been possible to fully diagnose and correct for the damage caused to \"Voyager 2's\" magnetometer, although efforts to do so were proceeding.\n\nOn August 30, 2007, \"Voyager 2\" passed the termination shock and then entered into the heliosheath, approximately 1 billion miles (1.6 billion km) closer to the Sun than \"Voyager 1\" did. This is due to the interstellar magnetic field of deep space. The southern hemisphere of the Solar System's heliosphere is being pushed in.\n\nOn April 22, 2010, \"Voyager 2\" encountered scientific data format problems. On May 17, 2010, JPL engineers revealed that a flipped bit in an on-board computer had caused the issue, and scheduled a bit reset for May 19. On May 23, 2010, \"Voyager 2\" resumed sending science data from deep space after engineers fixed the flipped bit. Currently research is being made into marking the area of memory with the flipped bit off limits or disallowing its use. The Low-Energy Charged Particle Instrument is currently operational, and data from this instrument concerning charged particles is being transmitted to Earth. This data permits measurements of the heliosheath and termination shock. There has also been a modification to the on-board flight software to delay turning off the AP Branch 2 backup heater for one year. It was scheduled to go off February 2, 2011 (DOY 033, 2011–033).\n\nOn July 25, 2012, \"Voyager 2\" was traveling at 15.447 km/s relative to the Sun at about from the Sun, at −55.29° declination and 19.888 h right ascension, and also at an ecliptic latitude of −34.0 degrees, placing it in the constellation Telescopium as observed from Earth. This location places it deep in the scattered disc, and traveling outward at roughly 3.264 AU per year. It is more than twice as far from the Sun as Pluto, and far beyond the perihelion of 90377 Sedna, but not yet beyond the outer limits of the orbit of the dwarf planet Eris.\n\nOn September 9, 2012, \"Voyager 2\" was from the Earth and from the Sun; and traveling at (relative to the Sun) and traveling outward at about 3.256 AU per year. Sunlight takes 13.73 hours to get to \"Voyager 2\". The brightness of the Sun from the spacecraft is magnitude -16.7. \"Voyager 2\" is heading in the direction of the constellation Telescopium. (To compare, Proxima Centauri, the closest star to the Sun, is about 4.2 light-years (or ) distant. \"Voyager 2's\" current relative velocity to the Sun is . This calculates as 3.254 AU per year, about 10% slower than \"Voyager 1\". At this velocity, 81,438 years would pass before \"Voyager 2\" reaches the nearest star, Proxima Centauri, were the spacecraft traveling in the direction of that star. (\"Voyager 2\" will need about 19,390 years at its current velocity to travel a complete light year)\n\nOn November 7, 2012, \"Voyager 2\" reached 100 AU from the sun, making it the third human-made object to reach 100 AU. \"Voyager 1\" was 122 AU from the Sun, and \"Pioneer 10\" is presumed to be at 107 AU. While Pioneer has ceased communications, both the \"Voyager\" spacecraft are performing well and are still communicating.\nIn 2013 \"Voyager 1\" was escaping the solar system at a speed of about 3.6 AU per year, while \"Voyager 2\" was only escaping at 3.3 AU per year. (Each year \"Voyager 1\" increases its lead over \"Voyager 2\")\n\nBy November 02, 2018, \"Voyager 2\" was at a distance of from the Sun. There is a variation in distance from Earth caused by the Earth's revolution around the Sun relative to \"Voyager 2\".\n\nIt was originally thought that \"Voyager 2\" would enter interstellar space in early 2016, with its plasma spectrometer providing the first direct measurements of the density and temperature of the interstellar plasma.\n\nHowever, the spacecraft may instead reach interstellar space sometime in 2019. \"Voyager 2\" is not headed toward any particular star, although in roughly 40,000 years it should pass 1.7 light-years from the star Ross 248. And if undisturbed for 296,000 years, \"Voyager 2\" should pass by the star Sirius at a distance of 4.3 light-years. \"Voyager 2\" is expected to keep transmitting weak radio messages until at least 2025, over 48 years after it was launched.\n\nEach Voyager space probe carries a gold-plated audio-visual disc in the event that either spacecraft is ever found by intelligent life-forms from other planetary systems. The discs carry photos of the Earth and its lifeforms, a range of scientific information, spoken greetings from the people (e.g. the Secretary-General of the United Nations and the President of the United States, and the children of the Planet Earth) and a medley, \"Sounds of Earth\", that includes the sounds of whales, a baby crying, waves breaking on a shore, and a collection of music, including works by Mozart, Blind Willie Johnson, Chuck Berry's \"Johnny B. Goode\", Valya Balkanska and other Eastern and Western classics and ethnic performers. (see also Music in space)\n\n\n\n"}
