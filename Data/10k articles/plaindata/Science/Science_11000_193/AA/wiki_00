{"id": "805180", "url": "https://en.wikipedia.org/wiki?curid=805180", "title": "A-002", "text": "A-002\n\nA-002 was the third abort test of the Apollo spacecraft.\n\nMission A-002 was the third in the series of abort tests to demonstrate that the launch system would perform satisfactorily under selected critical abort conditions. The main objective of this mission was to demonstrate the abort capability of the launch escape vehicle in the maximum dynamic pressure region of the Saturn trajectory with conditions approximating the altitude limit at which the Saturn emergency detection system would signal an abort.\n\nThe launch vehicle was the third in the Little Joe II series. This vehicle differed from the previous two in that flight controls and instrumentation were incorporated, and the vehicle was powered by two Algol and four Recruit rocket motors. The launch escape system was also changed from previous configurations in that canards (forward control surfaces used to orient and stabilize the escape vehicle in the entry attitude) and a command module boost protective cover were incorporated. The Apollo spacecraft was simulated by a boilerplate command and service module (BP-23). The Earth landing system was modified from the previous configuration by the installation of modified dual-drogue parachutes instead of a single-drogue parachute.\n\nThe A-002 vehicle was launched on December 8, 1964, at 08:00:00 a.m. M.S.T. (15:00:00 UTC) by igniting all launch vehicle motors simultaneously. Conditions at abort initiation were selected from Saturn boost trajectories, and a nominal test point was used for the maximum dynamic pressure region. A pitch up maneuver and the abort were initiated by using a real-time plot of the dynamic pressure versus Mach number. However, an improper constant was used in the meteorological data input to the real-time data system, resulting in the pitch up maneuver being initiated 2.4 seconds early. Although the planned test point was not achieved, the early pitch up caused a higher maximum dynamic pressure than the design value.\n\nCanard deployment took place as expected 11.1 seconds after abort initiation. The launch escape vehicle tumbled four times before stabilizing with the aft heat shield forward. During the first turnaround, the soft portion of the boost protective cover was torn away from the command module. Maximum altitude attained by the launch escape vehicle was 50,360 feet (15,350 m) above mean sea level.\n\nBaro-switches initiated the Earth landing sequence at an altitude of approximately 23,500 feet (7,163 m) above mean sea level. All parachutes deployed properly and the command module, supported by the three main parachutes, descended at the planned rate of about 24 ft/s (7 m/s) to an Earth landing 32,800 feet (10 km) down range.\n\nThe abort conditions obtained were more than adequate in verifying the abort capability in the maximum dynamic pressure region. Only one test objective was not achieved: the boost protective cover was structurally inadequate for the environment experienced during the mission.\n\nBP-23 was refurbished as BP-23A and used for Launch Pad Abort Test 2. BP-23A is on display as part of the SA-500D Saturn V exhibit at the US Space & Rocket Center, Huntsville, Alabama.\n\n"}
{"id": "6095415", "url": "https://en.wikipedia.org/wiki?curid=6095415", "title": "A1068 road", "text": "A1068 road\n\nThe A1068 is a road in northern England that runs from Seaton Burn in North Tyneside to Alnwick in Northumberland. The section between Ellington and Alnmouth is signposted as part of the \"Northumberland Coastal Route\".\nThe A1068 begins at a roundabout with the A19 road at Seaton Burn. It has a brief dual carriageway section before crossing from the county of Tyne and Wear into Northumberland. It is dual carriageway standard again past Nelson Village, and joins the route of the A192 road for about . It continues through the town of Bedlington where it meets the A193 road and beyond the town it has a junction with the A196 road (to Morpeth). It joins the route of the A197 road for about close to the town of Ashington. It re-emerges and heads in a north-easterly direction until it reaches the roundabout with the A189 road. From its junction with the A189 the A1068 gains primary status and heads roughly north. It passes the coastal town of Amble and the villages of Warkworth and Alnmouth, crossing the River Aln at Lesbury before reaching the town of Alnwick. The road terminates at its junction with the A1.\n"}
{"id": "2696696", "url": "https://en.wikipedia.org/wiki?curid=2696696", "title": "A512 road", "text": "A512 road\n\nThe A512 is an A road entirely in Leicestershire, UK. It links the primary destination of Loughborough with the M1, A42 road, and the town of Ashby de la Zouch.\n\nThe road begins just outside Loughborough Town Centre, near to The Rushes It heads out of town, crossing the A6004 Epinal Way and passing Loughborough University. On leaving the town, there is a short dual carriageway section, leading to the junction with Snells Nook Lane to Nanpanton and Woodhouse.\n\nAfter this junction, the road returns to single carriageway for about , to Junction 23 of the M1. \n\nAt this junction, much of the traffic from Loughborough turns off, and it is a much quieter A512 that enters Shepshed. After Shepshed, the road passes through Northern Charnwood Forest, near the villages of Peggs Green, Thringstone, Griffydam, Belton and Osgathorpe. Shortly before Ashby it passes Coleorton Hall on the right – a house which has played host to Scott, Wordsworth, Coleridge and others – now converted to apartments.\n\nThe road ends at the junction with the A42 and A511 (formerly the A50), on the edge of Ashby \n\nThe A512 is long, with the M1 junction from the southern end.\n"}
{"id": "4520906", "url": "https://en.wikipedia.org/wiki?curid=4520906", "title": "Aadhiya system", "text": "Aadhiya system\n\nThe Aadhiya system, also sometimes spelled as \"adhiya\", is a system, most prevalent in north-eastern, northern Bengali speaking parts of India(As the word is Bengali), where a sex worker is rented a room or apartment by a \"mashi\" or brothel keeper, usually an older retired sex worker, who charges the worker rent for the room based on her total earnings rather than at a fixed rate, so that the mashi gets a share of the worker's earnings.\n\n"}
{"id": "42787988", "url": "https://en.wikipedia.org/wiki?curid=42787988", "title": "Alajuela virus", "text": "Alajuela virus\n\nThe Alajuela virus (ALJV) is a species in the genus Orthobunyavirus in the Gamboa serogroup. It is isolated from mosquitoes, Aedeomyia squamipennis. It has not been reported to cause disease in humans.\n"}
{"id": "36281866", "url": "https://en.wikipedia.org/wiki?curid=36281866", "title": "Altmetrics", "text": "Altmetrics\n\nIn scholarly and scientific publishing, altmetrics are non-traditional bibliometrics proposed as an alternative or complement to more traditional citation impact metrics, such as impact factor and \"h\"-index. The term altmetrics was proposed in 2010, as a generalization of article level metrics, and has its roots in the #altmetrics hashtag. Although altmetrics are often thought of as metrics about articles, they can be applied to people, journals, books, data sets, presentations, videos, source code repositories, web pages, etc. Altmetrics use public APIs across platforms to gather data with open scripts and algorithms. Altmetrics did not originally cover citation counts, but calculate scholar impact based on diverse online research output, such as social media, online news media, online reference managers and so on. It demonstrates both the impact and the detailed composition of the impact. Altmetrics could be applied to research filter, promotion and tenure dossiers, grant applications and for ranking newly-published articles in academic search engines.\n\nThe development of web 2.0 has changed the research publication seeking and sharing within or outside the academy, but also provides new innovative constructs to measure the broad scientific impact of scholar work. Although the traditional metrics are useful, they might be insufficient to measure immediate and uncited impacts, especially outside the peer-review realm.\n\nProjects such as ImpactStory, and various companies, including Altmetric, and Plum Analytics are calculating altmetrics. Several publishers have started providing such information to readers, including BioMed Central, Public Library of Science (PLOS), Frontiers, Nature Publishing Group, and Elsevier.\n\nIn 2008, the Journal of Medical Internet Research started to systematically collect tweets about its articles. Starting in March 2009, the Public Library of Science also introduced article-level metrics for all articles. Funders have started showing interest in alternative metrics, including the UK Medical Research Council. Altmetrics have been used in applications for promotion review by researchers. Furthermore, several universities, including the University of Pittsburgh are experimenting with altmetrics at an institute level.\n\nHowever, it is also observed that an article needs little attention to jump to the upper quartile of ranked papers, suggesting that not enough sources of altmetrics are currently available to give a balanced picture of impact for the majority of papers.\n\nImportant in determining the relative impact of a paper, a service that calculates altmetrics statistics needs a considerably sized knowledge base. The following table shows the number of papers covered by services:\n\nAltmetrics are a very broad group of metrics, capturing various parts of impact a paper or work can have. A classification of altmetrics was proposed by ImpactStory in September 2012, and a very similar classification is used by the Public Library of Science:\n\nOne of the first alternative metrics to be used was the number of views of a paper. Traditionally, an author would wish to publish in a journal with a high subscription rate, so many people would have access to the research. With the introduction of web technologies it became possible to actually count how often a single paper was looked at. Typically, publishers count the number of HTML views and PDF views. As early as 2004, the \"BMJ\" published the number of views for its articles, which was found to be somewhat correlated to citations.\n\nThe discussion of a paper can be seen as a metric that captures the potential impact of a paper. Typical sources of data to calculate this metric include Facebook, Google+, Twitter, Science Blogs, and Wikipedia pages. Some researchers regard the mentions on social media as citations. For example, citations on a social media platform could be divided into two categories: internal and external. For instance, the former includes retweets, the latter refers to tweets containing links to outside documents. The correlation between the mentions and likes and citation by primary scientific literature has been studied, and a slight correlation at best was found, e.g. for articles in PubMed. In 2008 the \"Journal of Medical Internet Research\" began publishing views and tweets. These \"tweetations\" proved to be a good indicator of highly cited articles, leading the author to propose a \"Twimpact factor\", which is the number of Tweets it receives in the first seven days of publication, as well as a Twindex, which is the rank percentile of an article's Twimpact factor. However, if implementing use of the Twimpact factor, research shows scores to be highly subject specific, and as a result, comparisons of Twimpact factors should be made between papers of the same subject area. It is necessary to note that while past research in the literature has demonstrated a correlation between tweetations and citations, it is not a causative relationship. At this point in time, it is unclear whether higher citations occur as a result of greater media attention via twitter and other platforms, or is simply reflective of the quality of the article itself.\n\nRecent research conducted at the individual level, rather than the article level, supports the use of twitter and social media platforms as a mechanism for increasing impact value. Results indicate that researchers whose work is mentioned on twitter have significantly higher h-indices than those of researchers whose work was not mentioned on twitter. The study highlights the role of using discussion based platforms, such as twitter, in order to increase the value of traditional impact metrics.\n\nBesides Twitter and other streams, blogging has shown to be a powerful platform to discuss literature. Various platforms exist that keep track of which papers are being blogged about. Altmetric.com uses this information for calculating metrics, while other tools just report where discussion is happening, such as ResearchBlogging and Chemical blogspace.\n\nPlatforms may even provide a formal way of ranking papers or recommending papers otherwise, such as Faculty of 1000.\n\nIt is also informative to quantify the number of times a page has been saved, or bookmarked. It is thought that individuals typically choose to bookmark pages that have a high relevance to their own work, and as a result, bookmarks may be an additional indicator of impact for a specific study. Providers of such information include science specific social bookmarking services such as CiteULike and Mendeley.\n\nThe cited category is a narrowed definition, different from the discussion. Besides the traditional metrics based on citations in scientific literature, such as those obtained from Google Scholar, CrossRef, PubMed Central, and Scopus, altmetrics also adopt citations in secondary knowledge sources. For example, ImpactStory counts the number of times a paper has been referenced by Wikipedia. Plum Analytics also provides metrics for various academic publications, seeking to track research productivity. PLOS is also a tool that may be used to utilize information on engagement.\n\nWhile there is less consensus on the validity and consistency of altmetrics, the interpretation of altmetrics in particular is discussed. Proponents of altmetrics make clear that many of the metrics show attention or engagement, rather than the quality of impacts on the progress of science. It should be noted that even citation-based metrics do not indicate if a high score implies a positive impact on science; that is, papers are also cited in papers that disagree with the cited paper, an issue for example addressed by the Citation Typing Ontology project.\n\nAltmetrics could be more appropriately interpreted by providing detailed context and qualitative data. For example, in order to evaluate the scientific contribution of a scholar work to policy making by altmetrics, qualitative data, such as who’s citing online and to what extent the online citation is relevant to the policymaking, should be provided as evidence.\n\nRegarding the relatively low correlation between traditional metrics and altmetrics, altmetrics might measure complementary perspectives of the scholar impact. It is reasonable to combine and compare the two types of metrics in interpreting the societal and scientific impacts. Researchers built a 2*2 framework based on the interactions between altmetrics and traditional citations. Further explanations should be provided for the two groups with high altmetrics/low citations and low altmetrics/high citations. Thus, altmetrics provide convenient approaches for researchers and institutions to monitor the impact of their work and avoid inappropriate interpretations.\n\nThe usefulness of metrics for estimating scientific impact is controversial. Research has found that online buzz could amplify the effect of other forms of outreach on researchers’ scientific impact. For the nano-scientists that are mentioned on Twitter, their interactions with reporters and non-scientists positively and significantly predicted higher h-index, whereas the non-mentioned group failed. Altmetrics expands the measurement of scholar impact for containing a rapid uptake, a broader range of audiences and diverse research outputs. In addition, the community shows a clear need: funders demand measurables on the impact of their spending, such as public engagement.\n\nHowever, there are limitations that affect the usefulness due to technique problems and systematic bias of construct, such as data quality, heterogeneity and particular dependencies. In terms of technique problems, the data might be incomplete, because it is difficult to collect those online research outputs without direct links to their mentions (i.e. videos) and identify different versions of one research work. Additionally, whether the API leads to any missing data is unsolved.\n\nAs for systematic bias, like other metrics, altmetrics are prone to self-citation, gaming, and other mechanisms to boost one's apparent impact. Altmetrics can be gamed: for example, likes and mentions can be bought. Altmetrics can be more difficult to standardize than citations. One example is the number of tweets linking to a paper where the number can vary widely depending on how the tweets are collected. Besides, online popularity may not equal to scientific values. Some popular online citations might be far from the value of generating further research discoveries, while some theoretical-driven or minority-targeted research of great science-related importance might be marginalized online. For example, the top tweeted articles in biomedicine in 2011 were relevant to curious or funny content, potential health applications, and catastrophe.\n\nAltmetrics for more recent articles may be higher because of the increasing uptake of the social web and because articles may be mentioned mainly when they are published. As a result, it might not be fair to compare the altmetrics scores of articles unless they have been published at a similar time. Researchers has developed a sign test to avoid the usage uptake bias by comparing the metrics of an article with the two articles published immediately before and after it.\n\nIt should be kept in mind that the metrics are only one of the outcomes of tracking how research is disseminated and used. Altmetrics should be carefully interpreted to overcome the bias. Even more informative than knowing how often a paper is cited, is which papers are citing it. That information allows researchers to see how their work is impacting the field (or not). Providers of metrics also typically provide access to the information from which the metrics were calculated. For example, Web of Science shows which are the citing papers, ImpactStory shows which Wikipedia pages are referencing the paper, and CitedIn shows which databases extracted data from the paper.\n\nAnother concern of altmetrics, or any metrics, is how universities or institutions are using metrics to rank their employees make promotion or funding decisions, and the aim should be limited to measure engagement.\n\nThe overall online research output is very little and varied among different disciplines. The phenomenon might be consistent with the social media use among scientists. Surveys has shown that nearly half of their respondents held ambivalent attitudes of social media’s influence on academic impact and never announced their research work on social media. With the changing shift in open science and social media use, the consistent altmetrics across disciplines and institutions will more likely be adopted.\n\nThe specific use cases and characteristics is an active research field in bibliometrics, providing much needed data to measure the impact of altmetrics itself. Public Library of Science has an Altmetrics Collection and both the \"Information Standards Quarterly\" and the \"Aslib Journal of Information Management\" recently published special issues on altmetrics. A series of articles that extensively reviews altmetrics was published in late 2015.\n\nThere is other research examining the validity of one altmetrics or make comparisons across different platforms. Researchers examine the correlation between altmetrics and traditional citations as the validity test. They assume that the positive and significant correlation reveals the accuracy of altmetrics to measure scientific impact as citations. The low correlation (less than 0.30) leads to the conclusion that altmetrics serves a complementary role in scholar impact measurement. However, it remains unsolved that what altmetrics are most valuable and what degree of correlation between two metrics generates a stronger impact on the measurement. Additionally, the validity test itself faces some technical problems as well. For example, replication of the data collection is impossible because of the instant changing algorithms of data providers.\n\n\n"}
{"id": "2035308", "url": "https://en.wikipedia.org/wiki?curid=2035308", "title": "Animal science", "text": "Animal science\n\nAnimal Science (also Animal Bioscience) is described as \"studying the biology of animals that are under the control of humankind.\" It can also be described as the production and management of farm animals. Historically, the degree was called animal husbandry and the animals studied were livestock species, like cattle, sheep, pigs, poultry, and horses. Today, courses available now look at a far broader area to include companion animals like dogs and cats, and many exotic species. Degrees in Animal Science are offered at a number of colleges and universities. Typically, the Animal Science curriculum not only provides a strong science background, but also hands-on experience working with animals on campus-based farms. \n\nProfessional education in animal science prepares students for career opportunities in areas such as animal breeding, food and fiber production, nutrition, animal agribusiness, animal behavior and welfare. Courses in a typical Animal Science program may include genetics, microbiology, animal behavior, nutrition, physiology, and reproduction. Courses in support areas, such as genetics, soils, agricultural economics and marketing, legal aspects, and the environment also are offered. All of these courses are essential to entering an animal science profession.\n\nAt many universities, a Bachelor of Science (BS) degree in Animal Science allows emphasis in certain areas. Typical areas are species-specific or career-specific. Species-specific areas of emphasis prepare students for a career in dairy management, beef management, swine management, sheep or small ruminant management, poultry production, or the horse industry. Other career-specific areas of study include pre-veterinary medicine studies, livestock business and marketing, animal welfare and behavior, animal nutrition science, animal reproduction science, or genetics. Youth programs are also an important part of animal science programs.\n\nMany schools that offer a degree option in Animal Science also offer a pre-veterinary emphasis such as Iowa State University, the University of Nebraska–Lincoln and the University of Minnesota, for example. This option provides an in-depth knowledge base of the biological and physical sciences including nutrition, reproduction, physiology, and genetics. This can prepare students for graduate studies in animal science, veterinary school, and pharmaceutical or animal science industries.\n\nIn a Master of Science degree option, students take required courses in areas that support their main interest. These courses are above courses normally required for a Bachelor of Science degree in the Animal Science major. For example, in a Ph.D. degree program students take courses related to their major that are more in-depth than those for the Master of Science degree, with an emphasis on research or teaching.\n\nGraduate studies in animal sciences are considered preparation for upper-level positions in production, management, education, research, or agriservices. Professional study in veterinary medicine, law, and business administration are among the most commonly chosen programs by graduates. Other areas of study include growth biology, physiology, nutrition, and production systems.\n\nThis program is offered in Bangladesh at the Bangladesh Agricultural University.\n\nIn Denmark this program is offered at University of Copenhagen\n\nIn Ireland this program is offered at University College Dublin as part of the agricultural science degree.\n\nPhilippines\n\nIn the Philippines, the program is offered in a number of schools and universities which includes, University of the Philippines Los Baños, Benguet State University, Don Mariano Marcos Memorial State University, Mindanao State University, Central Luzon State University, Cavite State University, etc. \n\nIn Tanzania the program is offered at Sokoine University of Agriculture.\n\nIn the United States, the universities offering such a program consist of Land Grant and non-Land Grant institutions such as University of Illinois at Urbana–Champaign, University of Nebraska–Lincoln, Cornell University, University of California Davis, Michigan State University, Purdue University, The Ohio State University, Pennsylvania State University, Iowa State University, University of Minnesota, University of Vermont, etc.\n\n\n"}
{"id": "28913449", "url": "https://en.wikipedia.org/wiki?curid=28913449", "title": "Apollo Glacier", "text": "Apollo Glacier\n\nApollo Glacier () is a glacier, long, flowing northeast and joining the lower part of Aphrodite Glacier from the east coast of the Antarctic Peninsula. The lower part of this glacier was first plotted by W.L.G. Joerg, from aerial photographs taken by Sir Hubert Wilkins in December 1928 and by Lincoln Ellsworth in November 1935. The glacier was subsequently photographed by the Ronne Antarctic Research Expedition in December 1947 (trimetrogon air photography) and roughly surveyed by the Falkland Islands Dependencies Survey in November 1960. It was named by the UK Antarctic Place-Names Committee after Apollo, the god of manly youth and beauty in Greek mythology.\n\n"}
{"id": "49769215", "url": "https://en.wikipedia.org/wiki?curid=49769215", "title": "Astronomy Centre", "text": "Astronomy Centre\n\nThe Astronomy Centre, also known as the Amateur Astronomy Centre, is an astronomical observatory located in northern England which is run by experienced amateur astronomers and is open to the public at certain times.\n\nFounded in 1982 by Peter Drew, Linda Simonian and Rob Miller on the site of a disused factory, high in the Pennines, the Centre provides opportunities for its members, schools, local community groups and the general public to observe and photograph astronomical phenomena at a range of wavelengths during daylight and night hours.\n\nAt the time the Centre was conceived, access to equipment and expertise was unavailable for many amateur astronomers in the UK and a national centre would have provided an invaluable focal point Developments in optical fabrication, photography and communications now permit many visitors and members to complement their home astronomical facilities, skills and experience with those of the Astronomy Centre.\n\nIn keeping with the Centres original ethos, besides welcoming visitors to the facility, current members engage off-site with schools, youth organisations and community groups and also provide contributions to national, regional, local print and broadcast media.\n\nThe first permanent housing on the site was built to shelter a aperture Newtonian telescope. Further construction took the total number of separate telescope mountings to 14 by the end of 2015.\n\nThe main observatory tower is a three level round building topped by an aluminium dome with twin sliding doors. Its construction by the members was completed in April 2000.\n\nThe largest optical telescope currently in regular use is a open truss Newtonian on a mount inspired by the designs of John Dobson. A mirror blank is available for a future enhancement of the facilities but construction of the UK's largest reflector since the destruction of the original Isaac Newton Telescope at Herstmonceux Castle is currently on hold. \n\nIn addition to the permanently mounted 30\", 20\", 17\", 16\", 12\" and 8\" instruments there are fixed locations to allow a number of smaller portable items to be quickly set up if visitor numbers increase on a clear night.\n\nMany of the optical instruments were constructed by telescope maker Peter Drew, who has also provided many other societies and individuals (such as Hoober Observatory) with their equipment. This includes a number of cameras obscura similar to, but smaller than, the one installed on the main Observatory building.\n\nObservational astronomy takes place on weekly open nights, for special events and by special arrangement with keyholders. As well as the basics of visual astronomy visitors can undertake a wide range of more advanced observations guided by volunteers with long experience in safe solar astronomy, infra-red astronomy and radio astronomy.\n"}
{"id": "35924364", "url": "https://en.wikipedia.org/wiki?curid=35924364", "title": "Available space theory", "text": "Available space theory\n\nIn botany, Available space theory, also known as \"first available space theory\", is a theory used to explain why most plants have an alternating leaf pattern on their stems. The theory states that the location of a new leaf on a stem is determined by the physical space between existing leaves. In other words, the location of a new leaf on a growing stem is directly related to the amount of space between the previous two leaves. Building on ideas first put forth by Hoffmeister in 1868, Snow and Snow hypothesized in 1947 that leaves sprouted in the first available space on the stem.\n\n"}
{"id": "22162864", "url": "https://en.wikipedia.org/wiki?curid=22162864", "title": "Boeddicker (crater)", "text": "Boeddicker (crater)\n\nBoeddicker is a crater in the Aeolis quadrangle of Mars, located at 15° south latitude and 197.7° west longitude. It is 109 km in diameter and was named after Otto Boeddicker, a German astronomer (1853–1937).\n\nBoeddicker Crater was discussed as a landing site for the 2003 Mars Exploration Rovers. It was one of 25 from a list of 185 after the FirstLanding Site Workshop for the 2003 Mars Exploration Rovers, January 24–25, 2001, at NASA Ames Research Center.\n\n"}
{"id": "2030226", "url": "https://en.wikipedia.org/wiki?curid=2030226", "title": "Boppard line", "text": "Boppard line\n\nIn German linguistics, the Boppard Line is an isogloss separating the dialects to the north, which have a /v/ in words such as \"Korv\" (or \"Korf\"), \"basket\", and \"leven\", \"to live\", from the dialects to the south (including standard German), which have a /b/: \"Korb\", \"leben\". The line runs from east to west and crosses the river Rhine at the town of Boppard.\n\n"}
{"id": "19452016", "url": "https://en.wikipedia.org/wiki?curid=19452016", "title": "CLEAN (algorithm)", "text": "CLEAN (algorithm)\n\nThe CLEAN algorithm is a computational algorithm to perform a deconvolution on images created in radio astronomy. It was published by in 1974 and several variations have been proposed since then.\n\nThe algorithm assumes that the image consists of a number of point sources. It will iteratively find the highest value in the image and subtract a small gain of this point source convolved with the point spread function (\"dirty beam\") of the observation, until the highest value is smaller than some threshold.\n"}
{"id": "45524498", "url": "https://en.wikipedia.org/wiki?curid=45524498", "title": "Catalogue of spectroscopic binary orbits", "text": "Catalogue of spectroscopic binary orbits\n\nThe catalogue of spectroscopic binary orbits (SB) is a compilation of orbital data for spectroscopic binary stars which have been produced since 1969 by Alan Henry Batten of the Dominion Astrophysics Observatory and various collaborators. \n\nAt the 24th International Astronomical Union general assembly, in 2000, a working group was established to take responsibility for maintenance of the catalogue, and to take it from a paper based system to an online database. \n\nAs of 7 August 2009, the catalogue database contained information on over 2940 binary systems, and was estimated for completion by 2015.\n\nThe catalogue is used for a variety of purposes:\n"}
{"id": "24053541", "url": "https://en.wikipedia.org/wiki?curid=24053541", "title": "Center on Organizational Innovation", "text": "Center on Organizational Innovation\n\nThe Center on Organizational Innovation (COI) is a research center at Columbia University's Institute for Social and Economic Research and Policy. The center, established in 2000 and directed by sociologist David Stark, promotes research in the areas of organizational studies, science and technology studies and economic sociology, with an emphasis on innovation and reflexivity.\n\n"}
{"id": "1161073", "url": "https://en.wikipedia.org/wiki?curid=1161073", "title": "Community of interest", "text": "Community of interest\n\nA community of interest, or interest-based community, is a community of people who share a common interest or passion. These people exchange ideas and thoughts about the given passion, but may know (or care) little about each other outside this area. Participation in a community of interest can be compelling, entertaining and create a community where people return frequently and remain for extended periods. Frequently, they cannot be easily defined by a particular geographical area.\n\nIn other words, \"a community of interest is a gathering of people assembled around a topic of common interest. Its members take part in the community to exchange information, to obtain answers to personal questions or problems, to improve their understanding of a subject, to share common passions or to play.\" In contrast to a spatial community, \"a 'community of interest' is defined not by space, but by some common bond (e.g. feeling of attachment) or entity (e.g. farming, church group).\"\n\n\"Online communities\" connect to communities of interest in that often times, they develop out of interests in a particular topic. A benefit of online communities over place-based communities is that of non-physical access to group involvement.\n\n"}
{"id": "43564795", "url": "https://en.wikipedia.org/wiki?curid=43564795", "title": "Crowdfunded satellites", "text": "Crowdfunded satellites\n\nSeveral crowdfunded satellites have been launched. These include SkyCube, KickSat, and ArduSat, each of which resulted from successful Kickstarter campaigns. Crowdfunded satellites are an example of public participation to research.\n"}
{"id": "2149045", "url": "https://en.wikipedia.org/wiki?curid=2149045", "title": "Development geography", "text": "Development geography\n\nDevelopment geography is a branch of geography which refers to the standard of living and its quality of life of its human inhabitants. In this context, development is a process of change that affects people's lives. It may involve an improvement in the quality of life as perceived by the people undergoing change. However, development is not always a positive process. Gunder Frank commented on the global economic forces that lead to the development of underdevelopment. This is covered in his dependency theory.\n\nIn development geography, geographers study spatial patterns in development. They try to find by what characteristics they can measure development by looking at economic, political and social factors. They seek to understand both the geographical \"causes\" and \"consequences\" of varying development. Studies compare More Economically Developed Countries (MEDCs) with Less Economically Developed Countries (LEDCs). Additionally variations within countries are looked at such as the differences between northern and southern Italy, the Mezzogiorno.\n\nQuantitative indicators are numerical indications of development.\n\nQualitative indicators include descriptions of living conditions and people's quality of life. They are useful in analyzing features that are not easily calculated or measured in numbers such as freedom, corruption, or security, which are largely non-material benefits.\n\nThere is a considerable spatial variation in development rates.\n\nThe most famous pattern in development is the North-South divide. The North-South divide separates the rich North or the developed world, from the poor South. This line of division is not as straightforward as it sounds and splits the globe into two main parts. It is also known as the Brandt Line.\n\nThe \"North\" in this divide is regarded as being North America, Europe, Russia, South Korea, Japan, Australia, New Zealand and the like. The countries within this area are generally the more economically developed. The \"South\" therefore encompasses the remainder of the Southern Hemisphere, mostly consisting of KFCs. Another possible dividing line is the Tropic of Cancer with the exceptions of Australia and New Zealand. It is critical to understand that the status of countries is far from static and the pattern is likely to become distorted with the fast development of certain southern countries, many of them NICs (Newly Industrialised Countries) including India, Thailand, Brazil, Malaysia, Mexico and others. These countries are experiencing sustained fast development on the back of growing manufacturing industries and exports.\n\nMost countries are experiencing significant increases in wealth and standard of living. However, there are unfortunate exceptions to this rule. Noticeably some of the former Soviet Union countries has experienced major disruption of industry in the transition to a market economy. Many African nations have recently experienced reduced GNPs due to wars and the AIDS epidemic, including Angola, Congo, Sierra Leone and others. Arab oil producers rely very heavily on oil exports to support their GDPs so any reduction in oil's market price can lead to rapid decreases in GNP. Countries which rely on only a few exports for much of their income are very vulnerable to changes in the market value of those commodities and are often derogatively called banana republics. Many developing countries do rely on exports of a few primary goods for a large amount of their income (coffee and timber for example), and this can create havoc when the value of these commodities drops, leaving these countries with no way to pay off their debts.\n\nWithin countries the pattern is that wealth is more concentrated around urban areas than rural areas. Wealth also tends towards areas with natural resources or in areas that are involved in tertiary (service) industries and trade. This leads to a gathering of wealth around mines and monetary centres such as New York, London and Tokyo.\n\nGeographers along with other social scientists have recognized that certain factors present in a given society may impede the social and economic development of that society. Factors, which have been identified as obstructing the economic and social welfare of developing societies, include:\n\n\nEffective governments may address many barriers to economic and social development, however in many instances this is challenging due to the path dependency societies develop regarding many of these issues. Some barriers to development may be impossible to address, such as climatic barriers to development. In these cases societies must evaluate whether such climatic barriers to development dictate that society must relocate a given settlement in order to enjoy greater economic development.\n\nMany scholars agree that foreign aid provided to developing nations is ineffective and in many instances counter productive. This is due to the manner in which foreign aid changes the incentives for productivity in a given developing society, and the manner in which foreign aid has the tendency to corrupt the governments responsible for its allocation and distribution.\n\nCultural barriers to development such as discrimination based on gender, race, religion, or sexual orientation are challenging to address in certain oppressive societies, though recent progress has been significant in some societies.\n\nWhile the aforementioned barriers to economic growth and development are most prevalent in the less developed economies of the world, even the most developed economies are plagued by select barriers to development such as drug prohibition and income inequality.\n\nMEDCs (More Economically Developed Countries) can give aid to LEDCs (Less Economically Developed Countries). There are several types of aid:\n\nAid can be given in several ways. Through money, materials, or skilled and learned people (e.g. teachers).\n\nAid has advantages. Mostly short-term or emergency aid help people in LEDCs to survive a natural (earthquake, tsunami, volcano eruption etc.) or human (civil war etc.) disaster. Aid helps make the recipient country (the country that receives aid) get more developed.\n\nHowever, aid also has disadvantages. Often aid does not even reach the poorest people. Often money gained from aid is used up to make infrastructures (bridges, roads etc.), which only the rich can use. Also, the recipient country becomes more dependent on aid from a donor country (the country giving aid).\n\nWhilst the above conception of aid has been the most pervasive within development geography work, it is important to remember that the aid landscape is far more complex than one directional flows from 'developed' to 'developing' countries. Development geographers have been at the forefront of research that aims to understand both the material exchanges and discourse surrounding 'South-South' development cooperation. 'Non-traditional' foreign aid from Southern, Middle Eastern and post-Socialist states (those outside the Development Assistance Committee (DAC) of the OECD) provide alternative development discourses and approaches to that of the mainstream Western model. Development geographers seek to examine the geopolitical drivers behind the aid donor programmes of \"LEDCs\", as well as the discursive symbolic repertoires of non-DAC donor states. Two illustrative examples of the complex aid landscape are that of China, which has been active as an aid donor throughout the latter half of the twentieth century but published its first report on foreign aid policy as recently as 2011 and India, an often cited aid recipient, but which has had donor programmes to Nepal and Bhutan since the 1950s.\n\n"}
{"id": "1272984", "url": "https://en.wikipedia.org/wiki?curid=1272984", "title": "Distal radius fracture", "text": "Distal radius fracture\n\nA distal radius fracture, also known as wrist fracture, is a break of the part of the radius bone which is close to the wrist. Symptoms include pain, bruising, and rapid-onset swelling. The wrist may be deformed. The ulna bone may also be broken.\nIn younger people, these fractures typically occur during sports or a motor vehicle collision. In older people, the most common cause is falling on an outstretched hand. Specific types include Colles, Smith, Barton, and Chauffeur's fractures. The diagnosis is generally suspected based on symptoms and confirmed with X-rays.\nTreatment is with casting for six weeks or surgery. Surgery is generally indicated if the joint surface is broken and does not line up, the radius is overly short, or the joint surface of the radius is tilted more than 10% backwards. Among those who are casted, repeated X-rays are recommended within three weeks to verify that a good position is maintained.\nDistal radius fractures are common. They represent between 25% and 50% of all broken bones. They occur most commonly in young males and older females. A year or two may be required for healing to occur.\nPeople usually present with a history of falling on an outstretched hand and complaint of pain and swelling around the wrist, sometimes with deformity around the wrist. Any numbness should be asked to exclude median and ulnar nerve injuries. Any pain in the limb of the same side should also be investigated to exclude associated injuries to the same limb.\n\nSwelling, deformity, tenderness, and loss of wrist motion are normal features on examination of a person with a distal radius fracture. \"Dinner fork\" deformity of the wrist is caused by dorsal displacement of the carpal bones (Colle's fracture). Reverse deformity is seen in volar angulation (Smith's fracture). The wrist may be radially deviated due to shortening of the radius bone. Examination should also rule out a skin wound which might suggest an open fracture, usually at the side. Tenderness at an area with no obvious deformity may still point to underlying fractures. Decreased sensation over the thenar eminence can be due to median nerve injury. Swelling and displacement can cause compression on the median nerve which results in acute carpal tunnel syndrome and requires prompt treatment. Very rarely, pressure on the muscle components of the hand or forearm is sufficient to create a compartment syndrome.\n\nThe most common cause of this type of fracture is a fall on an outstretched hand from standing height, although some fractures will be due to high-energy injury. People who fall on the outstretched hand are usually fitter and have better reflexes when compared to those with elbow or humerus fractures. The characteristics of distal radius fractures are influenced by the position of the hand at the time of impact, the type of surface at point of contact, the speed of the impact, and the strength of the bone. Distal radius fractures typically occur with the wrist bent back from 60 to 90 degrees. Radial styloid fracture would occur if the wrist is ulnar deviated and vice versa. If the wrist is bent back less, then proximal forearm fracture would occur, but if the bending back is more, then the carpal bones fracture would occur. With increased bending back, more force is required to produce a fracture. More force is required to produce a fracture in males than females. Risk of injury increases in those with osteoporosis.\n\nCommon injuries associated with distal radius fractures are interosseous intercarpal ligaments injuries, especially scapholunate (4.7% to 46% of cases) and lunotriquetral ligaments (12% to 34% of cases) injuries. There is an increased risk of interosseous intercarpal injury if the ulnar variance (the difference in height between the distal end of the ulna and the distal end of the radius) is more than 2mm and there is fracture into the wrist joint. Triangular fibrocartilage complex (TFCC) injury occurs in 39% to 82% of cases. Ulnar styloid process fracture increases the risk of TFCC injury by a factor of 5:1. However, it is unclear whether intercarpal ligaments and triangular fibrocartilage injuries are associated with long term pain and disability for those who are affected.\n\nDiagnosis may be evident clinically when the distal radius is deformed, but should be confirmed by X-ray.\nThe differential diagnosis includes scaphoid fractures and wrist dislocations, which can also co-exist with a distal radius fracture. Occasionally, fractures may not be seen on X-rays immediately after the injury. Delayed X-rays, X-ray computed tomography (CT scan), or Magnetic resonance imaging (MRI) can confirm the diagnosis.\n\nX-ray of the affected wrist is required if a fracture is suspected. Posteroanterior, lateral, and oblique views can be used together to describe the fracture. X-ray of the uninjured wrist should also be taken to determine if any normal anatomic variations exist before surgery. \n\nA CT scan is often performed to further investigate the articular anatomy of the fracture, especially for fracture and displacement within the distal radio-ulnar joint.\n\nVarious kinds of information can be obtained from X-rays of the wrist:\n\nLateral view\n\nPosteroanterior view\n\nOblique view\n\nThere are many classification systems for distal radius fracture. AO/OTA classification is adopted by Orthopaedic Trauma Association and is the most commonly used classification system. There are three major groups: A—extra-articular, B—partial articular, and C—complete articular which can further subdivided into nine main groups and 27 subgroups depending on the degree of communication and direction of displacement. However, none of the classification systems demonstrate good liability. A qualification modifier (Q) is used for associated ulnar fracture.\n\nCorrection should be undertaken if the wrist radiology falls outside the acceptable limits:\n\nTreatment options for distal radius fractures include nonoperative management, external fixation, and internal fixation. Indications for each depend on a variety of factors such as the patient's age, initial fracture displacement, and metaphyseal and articular alignment, with the ultimate goal to maximize strength and function in the affected upper extremity. Surgeons use these factors combined with radiologic imaging to predict fracture instability, and functional outcome to help decide which approach would be most appropriate. Treatment is often directed to restore normal anatomy to avoid the possibility of malunion, which may cause decreased strength in the hand and wrist. The decision to pursue a specific type of management varies greatly by geography, physician specialty (hand surgeons vs. orthopedic surgeons), and advancements in new technology such as the volar locking plating system.\n\nDistal radius fractures are often associated with distal radial ulnar joint (DRUJ) injuries, and the American Academy of Orthopaedic Surgeons recommends that postreduction lateral wrist X-rays should be obtained in all patients with distal radius fractures in order to preclude DRUJ injuries or dislocations.\n\nThe majority of distal radius fractures are treated with conservative nonoperative management, which involves immobilization through application of plaster or splint with or without closed reduction. The prevalence of nonoperative approach to distal radius fractures is around 70%. Nonoperative management is indicated for fractures that are undisplaced, or for displaced fractures that are stable following reduction. Variations in immobilization techniques involve the type of cast, position of immobilization, and the length of time required in the cast.\n\nFor those with low demand, cast and splint can be applied for two weeks. In those who are young and active, if the fracture is not displaced, the patient can be followed up in one week. If the fracture is still undisplaced, cast and splint can be applied for three weeks. If the fracture is displaced, then manipulative reduction or surgical stabilisation is required. Shorter immobilization is associated with better recovery when compared to prolonged immobilization. 10% of the minimally displaced fractures will become unstable in the first two weeks and cause malunion. Therefore, follow up within the first week of fracture is important. 22% of the minimally displaced fractures will malunite after two weeks. Subsequent follow ups at two to three weeks are therefore also important.\n\nWhere the fracture is undisplaced and stable, nonoperative treatment involves immobilization. Initially, a backslab or a sugar tong splint is applied to allow swelling to expand and subsequently a cast is applied. Depending on the nature of the fracture, the cast may be placed above the elbow to control forearm rotation. However, an above-elbow cast may cause long-term rotational contracture. For torus fractures, a splint may be sufficient and casting may be avoided. The position of the wrist in cast is usually slight flexion and ulnar deviation. However, neutral and dorsiflex position may not affect the stability of the fracture.\n\nIn displaced distal radius fracture, in those with low demands, the hand can be casted until the person feels comfortable. If the fracture affects the median nerve, only then is a reduction indicated. If the instability risk is less than 70%, the hand can be manipulated under regional block or general anaesthesia to achieve reduction. If the post reduction radiology of the wrist is acceptable, then the person can come for follow up at one, two, or three weeks to look for any displacement of fractures during this period. If the reduction is maintained, then the cast should continue for 4 to 6 weeks. If the fracture is displaced, surgical management is the proper treatment. If the instability risk of the wrist is more than 70%, then surgical management is required. 43% of displaced fractures will be unstable within the first two weeks and 47% of the remaining unstable fractures will become unstable after two weeks. Therefore, periodic reviews are important to prevent malunion of the displaced fractures.\n\nClosed reduction of a distal radius fracture involves first anesthetizing the affected area with a hematoma block, intravenous regional anesthesia (Bier's block), sedation or a general anesthesia. Manipulation generally includes first placing the arm under traction and unlocking the fragments. The deformity is then reduced with appropriate closed manipulative (depending on the type of deformity) reduction, after which a splint or cast is placed and an X-ray is taken to ensure that the reduction was successful. The cast is usually maintained for about 6 weeks.\n\nFailure of nonoperative treatment leading to functional impairment and anatomic deformity is the largest risk associated with conservative management. Prior studies have shown that the fracture often redisplaces to its original position even in a cast. Only 27-32% of fractures are in acceptable alignment 5 weeks after closed reduction. For those less than 60 years in age, there will be a dorsal angulation of 13 degrees, while for those older than 60, the dorsal angulation can reach as high as 18 degrees. In people over 60, functional impairment can last for more than 10 years.\n\nDespite these risks with nonoperative treatment, more recent systematic reviews suggest that when indicated, nonsurgical management in the elderly population may lead to similar functional outcomes as surgical approaches. In these studies, no significant differences in pain scores, grip strength, and range of motion in patients' wrists occurred when comparing conservative nonsurgical approaches with surgical management. Although the nonsurgical group exhibited greater anatomic misalignment such as radial deviation, and ulnar variance, these changes did not seem to have significant impact on overall pain and quality of life.\n\nThe techniques of surgical management include open reduction internal fixation (ORIF), external fixation, percutaneous pinning, or some combination of the above. The choice of operative treatment is often determined by the type of fracture, which can be categorized broadly into three groups: partial articular fractures, displaced articular fractures, and metaphyseal unstable extra- or minimal articular fractures.\n\nSignificant advances have been made in ORIF treatments. Two newer treatment are fragment-specific fixation and fixed-angle volar plating. These attempt fixation rigid enough to allow almost immediate mobility, in an effort to minimize stiffness and improve ultimate function; no improved final outcome from early mobilization (prior to 6 weeks after surgical fixation) has been shown. Although restoration of radiocarpal alignment is thought to be of obvious importance, the exact amount of angulation, shortening, intra-articular gap/step which impact final function are not exactly known. The alignment of the DRUJ is also important, as this can be a source of a pain and loss of rotation after final healing and maximum recovery.\n\nAn arthroscope can be used at the time of fixation to evaluate for soft-tissue injury. Structures at risk include the triangular fibrocartilage complex and the scapholunate ligament. Scapholunate injuries in radial styloid fractures where the fracture line exits distally at the scapholunate interval should be considered. TFCC injuries causing obvious DRUJ instability can be addressed at the time of fixation.\n\nPrognosis varies depending on dozens of variables. If the anatomy (bony alignment) is not properly restored, function may remain poor even after healing. Restoration of bony alignment is not a guarantee of success, as soft tissue contributes significantly to the healing process.\n\nThese fractures are the most common of the three groups mentioned above that require surgical management. A minimal articular fracture involves the joint, but does not require reduction of the joint. Manipulative reduction and immobilization were thought to be appropriate for metaphyseal unstable fractures. However, several studies suggest this approach is largely ineffective in patients with high functional demand, and in this case, more stable fixation techniques should be used.\n\nSurgical options have been shown to be successful in patients with unstable extra-articular or minimal articular distal radius fractures. These options include percutaneous pinning, external fixation, and ORIF using plating. Patients with low functional demand of their wrists can be treated successfully with nonsurgical management; however, in more active and fit patients with fractures that are reducible by closed means, nonbridging external fixation is preferred, as it has less serious complications when compared to other surgical options. The most common complication associated with nonbridging external fixation is pin tract infection, which can be managed with antibiotics and frequent dressing changes, and rarely results in reoperation. The external fixator is placed for 5 to 6 weeks and can be removed in an outpatient setting.\n\nIf the fractures are unlikely to be reduced by closed means, open reduction with internal plate fixation is preferred. Although major complications (i.e. tendon injury, fracture collapse, or malunion) result in higher reoperation rates (36.5%) compared to external fixation (6%), ORIF is preferred, as this provides better stability and restoration of the volar tilt. Following the operation, a removable splint is placed for 2 weeks, during which time patients should mobilize the wrist as tolerated.\n\nThese fractures, although less common, often require surgery in active, healthy patients to address displacement of both the joint and the metaphysis. The two mainstays of treatment are bridging external fixation or ORIF. If reduction can be achieved by closed/percutaneous reduction, then open reduction can generally be avoided. Percutaneous pinning is preferred to plating due to similar clinical and radiological outcomes, as well as lower costs, when compared to plating, despite increased risk of superficial infections. Level of joint restoration, as opposed to surgical technique, has been found to be a better indicator of functional outcomes.\n\nWorld Health Organization (WHO) divides outcomes into three categories: impairment, disabilities, and handicaps. Impairment is the abnormal physical function, such as lack of forearm rotation. It is measured clinically. Disability is the lack of ability to perform physical daily activities. It is measured by Patient Reported Outome Measures (PROMs). Examples of scoring system based on clinical assessment are: Mayo Wrist Score (for perilunate fracture dislocation), Green and O’Brien Score (carpal dislocation and pain), and Gartland and Werley Score (evaluating distal radius fractures). These scores includes assessment of range of motion, grip strength, ability to perform activities of daily living, and radiological picture. However, none of the three scoring system demonstrated good reliability.\n\nThere are also two scoring systems for Patient Reported Outome Measures (PROMs): the Disabilities of Hand, Arm and Shoulder (DASH) Score and the Patient-Related Wrist Evaluation (PRWE) Score. These scoring systems measures the ability of a person to perform a task, pain score, presence of tingling and numbness, the effect on activities of daily living, and self-image. Both scoring systems show good reliability and validity.\n\nNonunion is rare; almost all of these fractures heal. Malunion, however, is not uncommon, and can lead to residual pain, grip weakness, reduced range of motion (especially rotation), and persistent deformity. Symptomatic malunion may require additional surgery. If the joint surface is damaged and heals with more than 1–2 mm of unevenness, the wrist joint will be prone to post-traumatic osteoarthritis. Half of nonosteoporotic patients will develop post-traumatic arthritis, specifically limited radial deviation and wrist flexion. This arthritis can worsen over time. Displaced fractures of the ulnar styloid base associated with a distal radius fracture result in instability of the DRUJ and resulting loss of forearm rotation.\n\nNerve injury, especially of the median nerve and presenting as carpal tunnel syndrome, is commonly reported following distal radius fractures. Tendon injury can occur in people treated both nonoperatively and operatively, most commonly to the extensor pollicis longus tendon. This can be due to the tendon coming in contact with protruding bone or with hardware placed following surgical procedures.\n\nComplex regional pain syndrome is also associated with distal radius fractures, and can present with pain, swelling, changes in color and temperature, and/or joint contracture. The cause for this condition is unknown.\n\nIn children, the outcome of distal radius fracture treatment in casts is usually very successful with healing and return to normal function expected. Some residual deformity is common, but this often remodels as the child grows. \n\nIn young patients, the injury requires greater force and results in more displacement, particularly to the articular surface. Unless an accurate reduction of the joint surface is obtained, these patients are very likely to have long-term symptoms of pain, arthritis, and stiffness.\n\nIn the elderly, distal radius fractures heal and may result in adequate function following nonoperative treatment. A large proportion of these fractures occur in elderly people who may have less requirement for strenuous use of their wrists. Some of these patients tolerate severe deformities and minor loss of wrist motion very well, even without reduction of the fracture. There is no difference in functional outcomes between operative and non-operative management in the elderly age group, despite better anatomical results in the operative group.\n\nDistal radius fractures are the most common fractures seen in adults, accounting for 17.5% of all adult fractures with an approximate rate of 23.6 to 25.8 per 100,000 per year. Incidences in females outnumber incidences in males by a factor of three to two. Average age of occurrence is between 57 and 66 years. Men who sustain distal radius fractures are usually younger, generally in their 40s (vs. 60s in females). Low energy injury (usually fall from standing height) is the usual cause of distal end radius fracture (66 to 77% of cases). High energy injuries accounts for 10% of wrist fractures. About 57% to 66% of the fractures are extra-articular fractures, 9% to 16% are partial-articular fractures, and 25% to 35% are complete articular fractures. Unstable metaphyseal fractures are ten times more common than severe articular fractures. Older people with osteoporosis who are still active are at an increased risk of getting distal radius fractures.\n\nBefore the 18th century, distal radius fracture was believed to be due to dislocation of the carpal bones or the displacement of the distal radioulnar articulation. In the 18th century, Petit first suggested that these types of injuries might be due to fractures rather than dislocations. Another author, Pouteau, suggested the common mechanism of injury which leads to this type of fractures - injury to the wrist when a person falls on an outstretched hand with dorsal displacement of the wrist. However, he also suggested that volar displacement of the wrist was due to the ulnar fracture. His work was met with skepticism from colleagues and little recognition, since the article was published after he died. In 1814, Abraham Colles described the characteristics of distal end radius fracture. In 1841, Guilaume Dupuytren acknowledged the contributions by Petit and Pouteau, agreeing that the distal end radius fracture is indeed a fracture, not a dislocation. In 1847, Malgaigne described the mechanism of injury for distal end radius fractures which can be caused by falling on the outstretched hand or on the back of the hand, and also the consequences if the hand fracture is not treated adequately. After that, Robert William Smith, professor of surgery in Dublin, Ireland, first described the characteristics of volar displacement of distal radius fractures. In 1895, with the advent of X-rays, the visualisation of the distal radius fracture become more apparent. Lucas-Champonnière first described the management of fractures using massage and early mobilization techniques. Anaesthesia, aseptic technique, immbolization, and external fixation all have contributed to the management of fixation of distal radius fracture. Ombredanne, a Parisian surgeon in 1929, first reported the use of nonbridging external fixation in the management of distal radius fractures. Bridging external fixation was first introduced by Roger Anderson and Gordon O’Neill from Seattle in 1944 due to poor results in conservative management (using orthopaedic cast) of distal end radius fractures. Raoul Hoffman of Geneva designed orthopaedic clamps which allow adjustments of the external fixator to reduce the fractures by closed reduction. In 1907, percutaneous pinning was first used. This was followed by the use of plating in 1965.\n\n"}
{"id": "26749292", "url": "https://en.wikipedia.org/wiki?curid=26749292", "title": "Dust lane", "text": "Dust lane\n\nA dust lane is a relatively dense obscuring band of interstellar dust, observed as a dark swath against the background of a brighter object, especially a galaxy. These dust lanes can usually be seen in spiral galaxies (e.g., the Milky Way) when viewed from the edge. Due to the dense and relatively thick nature of this dust, light from the galaxy is reduced by several magnitudes. In the Milky Way, this reduction of light makes it impossible to see the light from the central bulge of the galaxy from Earth. This dust, as well as the gasses also found within these lanes, mix together and combine to form stars and planets.\n\n"}
{"id": "3030305", "url": "https://en.wikipedia.org/wiki?curid=3030305", "title": "Economic oppression", "text": "Economic oppression\n\nEconomic oppression may take several forms, including the practice of bonded labour in some parts of India; serfdom; forced labour; low wages; denial of equal opportunity; practicing employment discrimination; and economic discrimination based on sex, nationality, race, and religion.. \n\nThe term economic oppression is sometimes misunderstood in the sense of economic sanction, embargo or economic boycott which each have different significances. The contextual application and significance of the term economic oppression has been changing over a period of time.\n\n"}
{"id": "3562081", "url": "https://en.wikipedia.org/wiki?curid=3562081", "title": "Emergent (software)", "text": "Emergent (software)\n\nEmergent (formerly PDP++) is neural simulation software that is primarily intended for creating models of the brain and cognitive processes. Development initially began in 1995 at Carnegie Mellon University, and , continues at the University of Colorado at Boulder. The 3.x release of the software, which was known as PDP++, is featured in the textbook \"Computational Explorations in Cognitive Neuroscience\".\n\nEmergent features a modular design, based on the principles of object-oriented programming. It runs on Microsoft Windows, Darwin / macOS and Linux. C-Super-Script (variously, CSS and C^C), a built-in C++-like interpreted scripting language, allows access to virtually all simulator objects and can initiate all the same actions as the GUI, and more. Version 4 and upward features a full 3D environment for visualizations, based on Qt and Open Inventor. Robotics simulations are made possible by integration with the Open Dynamics Engine. A plugin system allows for expanding the software in many ways. Version 5 introduced parallel threading support, numerous speed improvements, a help browser featuring an interface to the project's Wiki and auto-generated documentation, undo and redo using diffs and a definable undo depth. In addition, 5.0.2 introduced a built-in plugin source code editor, and plugins can now be compiled from the main interface, enabling full development of plugins within Emergent.\n\nEmergent also provides an implementation of Leabra which was developed by Randall C. O'Reilly in his PhD thesis.\n\n\n"}
{"id": "886300", "url": "https://en.wikipedia.org/wiki?curid=886300", "title": "Explorer 35", "text": "Explorer 35\n\nExplorer 35 (IMP-E, AIMP 2, Anchored IMP 2, Interplanetary Monitoring Platform-E) was a spin-stabilized spacecraft instrumented for interplanetary studies, at lunar distances, of the interplanetary plasma, magnetic field, energetic particles, and solar X rays. It was launched into an elliptical lunar orbit. The spin axis direction was nearly perpendicular to the ecliptic plane, and the spin rate was 25.6 rpm. Mission objectives were achieved. After successful operation for 6 years, the spacecraft was turned off on June 24, 1973.\n\nThe Ames magnetometer experiment consisted of a boom-mounted triaxial fluxgate magnetometer and an electronics package. The sensors were orthogonally mounted, with one sensor oriented along the spin axis of the spacecraft. A motor interchanged a sensor in the spin plane with the sensor along the spin axis every 24 hours, allowing inflight calibration. The instrument package included a circuit for demodulating the outputs from the sensors in the spin plane. The noise threshold was about 0.2 nT . The instrument had three ranges covering plus or minus 20, 60, and 200 nT full scale for each vector component. The digitization accuracy for each range was 1% of the entire range covered. The magnetic field vector was measured instantaneously, and the instrument range was changed after each measurement. A period of 2.05 seconds elapsed between adjacent measurements and a period of 6.14 s elapsed between measurements using the same range. The instrument performance was normal.\n\nThe experiment consisted of a boom-mounted triaxial fluxgate magnetometer. Each sensor had dual ranges of minus to plus 24 nT and 64 nT, with digitization resolutions of minus to plus 0.094 nT and 0.25 nT, respectively. Zero level drift was checked by periodic reorientation of the sensors until May 20, 1969, when the flipper mechanism failed. Past this point, data analysis was more difficult as the zero level drift of the sensor parallel to the spacecraft spin axis was not readily determined. Spacecraft interference was less than 0.125 nT. One vector measurement was obtained each 5.12 s. The bandpass of the magnetometer was 0 to 5 Hz, with a 20-dB per decade decrease for higher frequencies. Except for the flipper failure, the experiment functioned normally from launch to spacecraft turnoff (June 24, 1973).\n\nThe purpose of this experiment was to study the electromagnetic reflective properties of the lunar surface. The 136.10-MHz (2.2 m) telemetry transmissions from the spacecraft were scattered from the lunar surface and then recorded by use of the 150-ft Stanford dish antenna. The reflected signal intensity depended upon the lunar reflectivity, the spacecraft altitude above the lunar surface, and the mean curvature of the Moon. The returned signal bandwidth was proportional to RMS lunar surface slopes. Occultation phenomena permitted a determination of the scattering properties of the lunar limb. The dielectric constant of the lunar subsurface in the scattering region below a depth of about 25 cm was then determined from a profile of reflectivity values vs the angle of incidence on the Moon. The mean lunar slope over each area from which signals were reflected has also been inferred. The observations were located within about 10 degrees of the lunar equator. Experiment operation was normal as of March 1971.\n\nThis experiment consisted of a 12-cm Neher-type ionization chamber and two Lionel type 205 HT Geiger-Müller (GM) tubes. The ion chamber responded omnidirectionally to electrons above 0.7 MeV and protons above 12 MeV. Both GM tubes were mounted parallel to the spacecraft spin axis. GM tube 1 detected electrons above 45 keV that were scattered off a gold foil. The acceptance cone for these electrons had a 70-deg full-angle and an axis of symmetry that was 20 deg off the spacecraft spin axis. GM tube 2 responded to electrons and protons above 22 and 300 keV, respectively, in an acceptance cone of 70-deg full-angle centered at the spacecraft spin axis. Both GM tubes responded omnidirectionally to electrons and protons of energies above 2.5 and 50 MeV, respectively. Pulses from the ion chamber and counts from each GM tube were accumulated for 39.72 s and read out every 40.96 s. In addition, the time between the first ion chamber pulses in an accumulation period was also telemetered. This experiment performed well initially.\n\nThis experiment was designed to measure the ionization, momentum, speed, and direction of micrometeorites, using thin film charged detectors, induction devices, and microphones.\n\nA multigrid, split-collector Faraday cup mounted on the equator of the spacecraft was used to study the directional intensity of solar wind positive ions and electrons with particular emphasis on the interaction of the solar wind with the Moon. Twenty-seven integral current samples (requiring about 4.3 s) were taken in an energy-per-charge window from 80 to 2850 eV. Then the current was sampled in eight differential energy-per-charge windows between 50 and 5400 eV at the azimuth where the peak current appeared in the previous series of integral measurements. These measurements (integral and differential) took about 25 s. Both the sum and difference of collector currents were obtained for positive ions. Only the sum was obtained for electrons. A complete set of measurements (two collector plate sums and one difference for protons, and one collector plate sum for electrons) required 328 s. The experiment worked well from launch until its failure in July 1968.\n\n\n"}
{"id": "1703917", "url": "https://en.wikipedia.org/wiki?curid=1703917", "title": "Gender expression", "text": "Gender expression\n\nA gender expression is a person's behavior, mannerisms, interests, and appearance that are associated with gender in a particular cultural context, specifically with the categories of femininity or masculinity. This also includes gender roles. These categories rely on stereotypes about gender.\n\nGender expression typically reflects a person's gender identity (their internal sense of their own gender), but this is not always the case. Gender expression is separate and independent both from sexual orientation and gender assigned at birth. A type of gender expression that is considered atypical for a person's externally perceived gender may be described as gender non-conforming.\n\nIn men and boys, typical gender expression is often described as \"manly\", while atypical expression is known as effeminate. In girls, atypical expression is called tomboyish. In (especially queer) women, atypical and typical expression are known as butch and femme respectively. A mixture of typical and atypical expression may be described as androgynous. A type of expression that is perceived as neither typically feminine or masculine can be described as gender-neutral or undifferentiated.\n\nThe term \"gender expression\" is used in the Yogyakarta Principles, which concern the application of international human rights law in relation to sexual orientation, gender identity, gender expression and sex characteristics.\n\nThe Bem Sex-Role Inventory was designed to evaluate gender expression objectively (within a White American cultural context).\n\n\n"}
{"id": "21341051", "url": "https://en.wikipedia.org/wiki?curid=21341051", "title": "Hamlet (crater)", "text": "Hamlet (crater)\n\nHamlet is the largest crater on the known part of the surface of Uranus' moon Oberon. It has diameter of about 206 km and is named after the title character of the play Hamlet, by William Shakespeare. The crater has a dark floor and is surrounded by a system of bright rays, which are ice ejecta deposited during the impact event. The nature of the dark material on the floor is unknown, but it may have erupted from the depth through cryovolcanism. The crater was first imaged by the Voyager 2 spacecraft in January 1986.\n"}
{"id": "49650527", "url": "https://en.wikipedia.org/wiki?curid=49650527", "title": "Howard Turner Barnes", "text": "Howard Turner Barnes\n\nHoward Turner Barnes (21 July 1873, Woburn, Massachusetts – 4 October 1950, Burlington, Vermont) was an American-Canadian physicist who specialized in calorimetry, electrolytes, ice formation and ice engineering.\n\nIn 1879, Howard T. Barnes moved with his family from Massachusetts to Montreal. where his father was appointed minister of Montreal's Unitarian church. After attending secondary school in Montreal, he entered in 1889 McGill University, where he received in 1893 his bachelor's degree in physics and, after working there as a demonstrator in chemistry, an M.S. in Applied Science in 1896. He became at McGill a demonstrator in physics and worked under Hugh L. Callendar. In 1898, Ernest Rutherford succeeded to Callendar's professorial chair and supervised Barnes, among others. In 1899 Barnes went to the U.K. on a scholarship from the Royal Society; he returned to McGill in 1900 as a lecturer in physics. In 1900 he received a D.Sc. from McGill, where he became an assistant professor in 1901 and associate professor in 1906. In 1907 he succeeded Ernest Rutherford as Macdonald Professor of Physician, but resigned his chair in 1919. In the early 1920s he again became a professor at McGill, where he remained until his retirement as professor emeritus in 1933.\n\nBarnes worked with Callendar on extremely precise measurements in constant-flow calorimetry, in which a given amount of electrical energy is added to a given mass of flowing liquid whose consequent increase in temperature is precisely measured. Barnes pioneered the constant-flow calorimeter which is used by contemporary physical chemists. He also studied turbulence, electrolytes, and the heat effects of radium. In the 1920s, he became a world-class expert on anchor ice, frazil ice, and ice engineering.\n\nBarnes was elected a Fellow of the Royal Society of Canada in 1908 and a Fellow of the Royal Society of London in 1911. He was honoured as the Tyndall Lecturer for 1912 at the Royal Institution in London.\n\n\n\n\n"}
{"id": "16586757", "url": "https://en.wikipedia.org/wiki?curid=16586757", "title": "Irregular warfare", "text": "Irregular warfare\n\nIrregular warfare is defined in United States joint doctrine as “A violent struggle among state and non-state actors for legitimacy and influence over the relevant populations.”\n\nIrregular warfare favors indirect and asymmetric warfare approaches, though it may employ the full range of military and other capabilities, in order to erode the adversary’s power, influence, and will. It is inherently a protracted struggle that will test the resolve of a state and its strategic partners. Concepts associated with irregular warfare are older than the term itself.\n\nThe distinction between regular and irregular forces is unrelated to the term \"irregular warfare.\" The term, irregular warfare, was settled upon in distinction from \"traditional warfare\" and \"unconventional warfare\", and to differentiate it as such.\n\nOne of the earliest known uses of the term \"irregular warfare\" is in the 1986 English edition of \"Modern Irregular Warfare in Defense Policy and as a Military Phenomenon\" by Friedrich August Freiherr von der Heydte. The original 1972 German edition of the book is titled \"Der Moderne Kleinkrieg als Wehrpolitisches und Militarisches Phänomen\". The German word \"Kleinkrieg\" is literally translated as \"Small War\". The word \"Irregular\", used in the title of the English translation of the book, seems to be a reference non \"regular armed forces\" as per the aforementioned Third Geneva Convention.\n\nOne of the earliest known uses of the term IW is in a 1996 Central Intelligence Agency document by Jeffrey B. White. Major military doctrine developments related to IW were done between 2004 and 2007 as a result of the September 11 attacks on the United States. A key proponent of IW within US DoD is Michael G. Vickers, a former paramilitary officer in the CIA.\n\nThe CIA's Special Activities Division (SAD) is the premiere unit for unconventional warfare, both for creating and for combating irregular warfare units. For example, SAD paramilitary officers created and led successful irregular units from the Hmong tribe during the war in Laos in the 1960s from the Northern Alliance against the Taliban during the war in Afghanistan in 2001 and from the Kurdish Peshmerga against Ansar al-Islam and the forces of Saddam Hussein during the war in Iraq in 2003.\n\n\nActivities and types of conflict included in IW are:\n\nAccording to the DoD, there are five core activities of IW:\n\nNearly all modern wars include at least some element of irregular warfare. Since the time of Napoleon, approximately 80% of conflict has been irregular in nature.\nHowever, the following conflicts may be considered to have exemplified by irregular warfare:\n\n\nThere have been several military wargames and military exercises associated with IW, including:\n\nAs a result of DoD Directive 3000.07, United States armed forces are studying irregular warfare concepts using modeling and simulation.\n\n\n\nIndividuals:\n\n\n"}
{"id": "12936910", "url": "https://en.wikipedia.org/wiki?curid=12936910", "title": "Journal of Regional Science", "text": "Journal of Regional Science\n\nThe Journal of Regional Science is a peer-reviewed academic journal published by Wiley-Blackwell. Established in 1958, it was the first journal in the field of Regional science. The current editors-in-chief are Marlon G. Boarnet (University of Southern California), Steven Brakman (University of Groningen), Mark D. Partridge (The Ohio State University) and Gianmarco I.P. Ottaviano (The London School of Economics and Political Science). Contributors hold positions in a variety of academic disciplines: economics, geography, agricultural economics, rural sociology, urban and regional planning, and civil engineering. Articles are usually empirical, occasionally methodological or theoretical, but always quantitative. \n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.743, ranking it 19th out of 55 journals in the category \"Planning & Development\".\n"}
{"id": "56646524", "url": "https://en.wikipedia.org/wiki?curid=56646524", "title": "Lead magnesium niobate", "text": "Lead magnesium niobate\n\nLead magnesium niobate is a relaxor ferroelectric. It has been used to make piezoelectric microcantilever sensors.\n"}
{"id": "33904406", "url": "https://en.wikipedia.org/wiki?curid=33904406", "title": "Length measurement", "text": "Length measurement\n\nLength measurement is implemented in practice in many ways. The most commonly used approaches are the transit-time methods and the interferometer methods based upon the speed of light. For objects such as crystals and diffraction gratings, diffraction is used with X-rays and electron beams. Measurement techniques for three-dimensional structures very small in every dimension use specialized instruments such as ion microscopy coupled with intensive computer modeling.\n\nFor a discussion of astronomical methods for determining cosmological distances, see the article Cosmic distance ladder.\n\nThe ruler the simplest kind of length measurement tool: lengths are defined by printed marks or engravings on a stick. The meter was initially defined using a ruler before more accurate methods became available.\n\nGauge blocks are a common method for precise measurement or calibration of measurement tools.\n\nFor small or microscopic objects, microphotography where the length is calibrated using a graticule can be used. A graticule is a piece that has lines for precise lengths etched into it. Graticules may be fitted into the eyepiece or they may be used on the measurement plane.\n\nThe basic idea behind a transit-time measurement of length is to send a signal from one end of the length to be measured to the other, and back again. The time for the round trip is the transit time Δt, and the length ℓ is then 2ℓ = Δt*\"v\",with \"v\" the speed of propagation of the signal, assuming that is the same in both directions. If light is used for the signal, its speed depends upon the medium in which it propagates; in SI units the speed is a defined value \"c\" in the reference medium of classical vacuum. Thus, when light is used in a transit-time approach, length measurements are not subject to knowledge of the source frequency (apart from possible frequency dependence of the correction to relate the medium to classical vacuum), but are subject to the error in measuring transit times, in particular, errors introduced by the response times of the pulse emission and detection instrumentation. An additional uncertainty is the \"refractive index correction\" relating the medium used to the reference vacuum, taken in SI units to be the classical vacuum. A refractive index of the medium larger than one slows the light.\n\nTransit-time measurement underlies most radio navigation systems for boats and aircraft, for example, radar and the nearly obsolete Long Range Aid to Navigation LORAN-C. For example, in one radar system, pulses of electromagnetic radiation are sent out by the vehicle (interrogating pulses) and trigger a response from a \"responder beacon\". The time interval between the sending and the receiving of a pulse is monitored and used to determine a distance. In the global positioning system a code of ones and zeros is emitted at a known time from multiple satellites, and their times of arrival are noted at a receiver along with the time they were sent (encoded in the messages). Assuming the receiver clock can be related to the synchronized clocks on the satellites, the \"transit time\" can be found and used to provide the distance to each satellite. Receiver clock error is corrected by combining the data from four satellites.\n\nSuch techniques vary in accuracy according to the distances over which they are intended for use. For example, LORAN-C is accurate to about GPS about enhanced GPS, in which a correction signal is transmitted from terrestrial stations (that is, differential GPS (DGPS)) or via satellites (that is, Wide Area Augmentation System (WAAS)) can bring accuracy to a few meters or or, in specific applications, tens of centimeters. Time-of-flight systems for robotics (for example, Laser Detection and Ranging LADAR and Light Detection and Ranging LIDAR) aim at lengths of and have an accuracy of about \n\nIn many practical circumstances, and for precision work, measurement of dimension using transit-time measurements is used only as an initial indicator of length and is refined using an interferometer. Generally, transit time measurements are preferred for longer lengths, and interferometers for shorter lengths.\n\nThe figure shows schematically how length is determined using a Michelson interferometer: the two panels show a laser source emitting a light beam split by a \"beam splitter\" (BS) to travel two paths. The light is recombined by bouncing the two components off a pair of \"corner cubes\" (CC) that return the two components to the beam splitter again to be reassembled. The corner cube serves to displace the incident from the reflected beam, which avoids some complications caused by superposing the two beams. The distance between the left-hand corner cube and the beam splitter is compared to that separation on the fixed leg as the left-hand spacing is adjusted to compare the length of the object to be measured.\n\nIn the top panel the path is such that the two beams reinforce each other after reassembly, leading to a strong light pattern (sun). The bottom panel shows a path that is made a half wavelength longer by moving the left-hand mirror a quarter wavelength further away, increasing the path difference by a half wavelength. The result is the two beams are in opposition to each other at reassembly, and the recombined light intensity drops to zero (clouds). Thus, as the spacing between the mirrors is adjusted, the observed light intensity cycles between reinforcement and cancellation as the number of wavelengths of path difference changes, and the observed intensity alternately peaks (bright sun) and dims (dark clouds). This behavior is called interference and the machine is called an interferometer. By \"counting fringes\" it is found how many wavelengths long the measured path is compared to the fixed leg. In this way, measurements are made in units of wavelengths \"λ\" corresponding to a particular atomic transition. The length in wavelengths can be converted to a length in units of metres if the selected transition has a known frequency \"f\". The length as a certain number of wavelengths \"λ\" is related to the metre using \"λ\" = . With \"c\" a defined value of 299,792,458 m/s, the error in a measured length in wavelengths is increased by this conversion to metres by the error in measuring the frequency of the light source.\n\nBy using sources of several wavelengths to generate sum and difference , absolute distance measurements become possible.\n\nThis methodology for length determination requires a careful specification of the wavelength of the light used, and is one reason for employing a laser source where the wavelength can be held stable. Regardless of stability, however, the precise frequency of any source has linewidth limitations. Other significant errors are introduced by the interferometer itself; in particular: errors in light beam alignment, collimation and fractional fringe determination. Corrections also are made to account for departures of the medium (for example, air) from the reference medium of classical vacuum. Resolution using wavelengths is in the range of ΔL/L ≈ depending upon the length measured, the wavelength and the type of interferometer used.\n\nThe measurement also requires careful specification of the medium in which the light propagates. A \"refractive index correction\" is made to relate the medium used to the reference vacuum, taken in SI units to be the classical vacuum. These refractive index corrections can be found more accurately by adding frequencies, for example, frequencies at which propagation is sensitive to the presence of water vapor. This way non-ideal contributions to the refractive index can be measured and corrected for at another frequency using established theoretical models.\n\nIt may be noted again, by way of contrast, that the transit-time measurement of length is independent of any knowledge of the source frequency, except for a possible dependence of the correction relating the measurement medium to the reference medium of classical vacuum, which may indeed depend on the frequency of the source. Where a pulse train or some other wave-shaping is used, a range of frequencies may be involved.\n\nFor small objects, different methods are used that also depend upon determining size in units of wavelengths. For instance, in the case of a crystal, atomic spacings can be determined using X-ray diffraction. The present best value for the lattice parameter of silicon, denoted \"a\", is:\n\ncorresponding to a resolution of ΔL/L ≈ Similar techniques can provide the dimensions of small structures repeated in large periodic arrays like a diffraction grating.\n\nSuch measurements allow the calibration of electron microscopes, extending measurement capabilities. For non-relativistic electrons in an electron microscope, the de Broglie wavelength is:\n\nwith \"V\" the electrical voltage drop traversed by the electron, \"m\" the electron mass, \"e\" the elementary charge, and \"h\" the Planck constant. This wavelength can be measured in terms of inter-atomic spacing using a crystal diffraction pattern, and related to the metre through an optical measurement of the lattice spacing on the same crystal. This process of extending calibration is called \"metrological traceability\". The use of metrological traceability to connect different regimes of measurement is similar to the idea behind the cosmic distance ladder for different ranges of astronomical length. Both calibrate different methods for length measurement using overlapping ranges of applicability.\n\nMeasuring dimensions of localized structures (as opposed to large arrays of atoms like a crystal), as in modern integrated circuits, is done using the scanning electron microscope. This instrument bounces electrons off the object to be measured in a high vacuum enclosure, and the reflected electrons are collected as a photodetector image that is interpreted by a computer. These are not transit-time measurements, but are based upon comparison of Fourier transforms of images with theoretical results from computer modeling. Such elaborate methods are required because the image depends on the three-dimensional geometry of the measured feature, for example, the contour of an edge, and not just upon one- or two-dimensional properties. The underlying limitations are the beam width and the wavelength of the electron beam (determining diffraction), determined, as already discussed, by the electron beam energy. \nThe calibration of these scanning electron microscope measurements is tricky, as results depend upon the material measured and its geometry. A typical wavelength is and a typical resolution is about \n\nOther small dimension techniques are the atomic force microscope, the focused ion beam and the helium ion microscope. Calibration is attempted using standard samples measured by transmission electron microscope (TEM).\n\nNuclear Overhauser effect spectroscopy (NOESY) is a specialized type of nuclear magnetic resonance spectroscopy where distances between atoms can be measured. It is based on the effect where nuclear spin cross-relaxation after excitation by a radio pulse depends on the distance between the nuclei. Unlike spin-spin coupling, NOE propagates through space and does not require that the atoms are connected by bonds, so it is a true distance measurement instead of a chemical measurement. Unlike diffraction measurements, NOESY does not require a crystalline sample, but is done in solution state and can be applied to substances that are difficult to crystallize.\n\nIn some systems of units, unlike the current SI system, lengths are fundamental units (for example, \"wavelengths\" in the older SI units and \"bohrs\" in atomic units) and are not defined by times of transit. Even in such units, however, the \"comparison\" of two lengths can be made by comparing the two transit times of light along the lengths. Such time-of-flight methodology may or may not be more accurate than the determination of a length as a multiple of the fundamental length unit.\n\n"}
{"id": "8424642", "url": "https://en.wikipedia.org/wiki?curid=8424642", "title": "Linguistic insecurity", "text": "Linguistic insecurity\n\nLinguistic insecurity comprises feelings of anxiety, self-consciousness, or lack of confidence in the mind of a speaker surrounding the use of their own language. Often, this anxiety comes from speakers' belief that their use of language does not conform to the perceived standard and/or the style of language expected by the speakers' interlocutor(s). Linguistic insecurity is situationally induced and is often based on a feeling of inadequacy regarding personal performance in certain contexts, rather than a fixed attribute of an individual. This insecurity can lead to stylistic, and phonetic shifts away from an affected speaker's default speech variety; these shifts may be performed consciously on the part of the speaker, or may be reflective of an unconscious effort to conform to a more prestigious or context-appropriate style of speech. Linguistic insecurity is linked to the perception of speech styles in any community, and so may vary based on socioeconomic class and gender. It is also especially pertinent in multilingual societies.\n\nLinguistic insecurity is the negative self-image a speaker has regarding his or her own speech variety or language as a whole, especially in the perceived difference between phonetic and syntactic characteristics of one's own speech and those characteristics of what is perceived to be the \"correct\" form of the spoken language. Linguistic insecurity arises based on the perception of a lack of correctness regarding one's own speech, rather than any objective deficiencies in a particular speech variety.\n\nIn one of its earliest usages, the term linguistic insecurity was employed by linguist William Labov in his 1972 paper on the social stratification of the pronunciation of /r/ to describe the attitude that employees, at three different retail stores in New York, have towards their own speech patterns, in comparison to the Standard English form. Labov theorized that those employees who had the most extreme shift in style from their own speech variety (a casual style) to the standard form (a more emphatic style) were more insecure in a linguistic sense. The term has since been used to describe any situation in which a speaker is led to hypercorrect, or shift one's patterns of speech, due to a negative attitude or lack of confidence regarding one's normal speech. This lack of confidence need not be consciously acknowledged by a speaker in order for him/her to be affected by linguistic insecurity, and the changes in pronunciation and stylistic shifts indicative of linguistic insecurity can emerge absent of speaker intent. Linguistic insecurity may also be a characteristic of an entire speech community, especially in how it relates to other speech communities of the same language that employ a more standardized form.\n\nAs linguistic insecurity is related to the perception of how one speaks in comparison to a certain form, the notion of standard and prestige forms of languages is important. The standard form of a language is regarded as the codified form of language used in public discourse, while the prestige form is the one perceived to receive the most respect accorded to any variety of the language. Variables that differentiate standard and prestige forms include phonetic realization, vocabulary, syntax, among other features of speech. The status of these forms is related to the concept of language ideology, which explains how varieties of language are correlated with certain moral, social or political values. Many societies value the belief that language homogeneity is beneficial to society; in fact, the existence of a \"common language\" is an intrinsic part of an imagined community, which defines a nation.\nHowever, the concept of a language norm is highly flexible. Nations often codify a standard language that may be different from regional norms. For example, Standard English in the United Kingdom is based on the south-eastern dialect and accent centered around London. In other parts of the UK, various dialects are spoken, such as Scots and Geordie; even in London, there exist Cockney and Estuary accents. Studies of young people in Glasgow show that they self-report linguistic insecurity, describing their own speech as 'slang' in comparison to the 'standard form' and attempting to incline their own speech to the standard.\n\nPrestige forms may also demonstrate linguistic insecurity. Again, in the UK, Received Pronunciation (RP), a prestige accent, has been affected by other varieties of speech. Though the standard form historically aimed towards RP, it is not a perfect imitation. The result is that RP speakers now demonstrate changes in phonetic realization in the direction of the standard.\n\nDespite these shifts, a person using an RP accent would tend to give the impression that he or she is well-educated and part of a higher socioeconomic class. This is because these traits are often associated with RP speakers; they index specific concepts that are presupposed by the community. Similarly, in general, forms of speech gain their status by their association with certain class characteristics. This indexicality does not need to be passive: in Beijing, young urban professionals actively adopt usages considered typical of prestigious Hong Kong and Taiwan speech in an effort to index themselves as cosmopolitan. It also does not need to be positive: speech forms may also index negative characteristics. In his study of attitudes towards varieties of United States English, Preston demonstrates that people often associate the Southern accent with a lack of sophistication, indexing speakers with such an accent as being backwards and conservative; and that Southern speakers themselves perceive their language to be inferior, exhibiting linguistic insecurity.\n\nSpeakers experiencing linguistic insecurity exhibit alterations of their normal speech which are reflective of their insecurity, and often are a result of the speaker attempting to compensate for the perceived deficiencies in their own speech variety. These effects of linguistic insecurity can come in the form of changes in pronunciation, as in the case of the retail store employees in William Labov's example, or even syntactic deviations from the speaker's normal speech variant.\n\nOne documented linguistic effect of linguistic insecurity is hypercorrection. Hypercorrection is the over-application of a perceived rule of grammar in order to appear more formal or to appear to belong to a more prestigious speech community. A common instance of hypercorrection in English is the use of the personal pronouns \"you and I\" as a correction of \"me and you\" in situations in which the accusative personal pronoun \"me\" is more appropriate. Because the use of \"you and I\" is internalized as the more grammatically sound form in the mind of many English speakers, that rule becomes over-applied in a situation when a speaker wants to compensate for perceived linguistic deficiencies. A speaker may try to avoid feelings of linguistic insecurity and perceived stigmatization by projecting a more educated or formal identity and emulating what is perceived as a more prestigious speech variety. Inadvertently, hypercorrection may index a speaker as belonging to the very social class or societal group that led to the linguistic insecurity. For example, linguist Donald Winford found after studying Trinidadian English that there was a knowledge that there was a stigmatization associated with less prestigious phonological variants, creating a situation in which individuals belonging to a \"lower\" social class would attempt to replicate phonological aspects of the more prestigious forms of English, but did not do so successfully, thus engaging in hypercorrection.\n\nSpeakers experiencing linguistic insecurity may also undergo, either consciously or unconsciously, a change in register from their default language variety. Linguistic register refers to a variety of speech in a given language that corresponds to a specific situational purpose or social setting. An example of the phonological impact of register in English is when speaking in a formal setting, it is customary to pronounce words ending in \"-ing\" with a velar nasal rather than substituting it for the [n] sound that is typical of \"-ing\" endings in informal speech. A register shift cannot always be accounted for by documenting the individual phonological differences in speech from one's default speech variety to the newly registered speech variety, but instead may include a difference in the overall \"tenor\" of speech and in the way a speaker gives deference to his/her interlocutors who are more experienced in interacting in that register. Having to navigate in a linguistic register markedly different from one's own speech variety can be a catalyst for hypercorrection and other behavioral effects of linguistic insecurity that can further contribute to a sense of communicative inadequacy if the speaker feels he is not convincingly interacting in that linguistic register.\n\nFindings show that the members of the lower middle class have the greatest tendency toward linguistic insecurity. Labov notes that evidence of their insecurity can be found in their wide range of stylistic variation, fluctuation in given stylistic contexts, conscious striving for correctness, and negative attitude towards their native speech pattern.\nAfter conducting a linguistic survey in 1960s New York City, Labov found evidence that the usage of /r/ by speakers was predictable except in a specific case involving the lower middle class. At the time, the pronunciation of /r/ at the end of words and before consonants became a prestige marker and the degree to which it was realized in casual speech correlated with the socioeconomic status of the respondents. However, members of the lower middle class showed a dramatic increase of r-pronunciation when a more formal style of speech was elicited, even surpassing the usage by the classes above. Labov interpreted this tendency to hypercorrect by adopting the prestigious form of the high ranking class as a sign of the linguistic insecurity of the lower middle class.\nExplanations for why the lower middle class exhibits this tendency have yet to be fully explored. A study conducted by Owens and Baker (1984) shows that the lower middle class of Winnipeg, Manitoba, Canada had highest scores for the CILI (Canadian Index of Linguistic Insecurity), which was adopted from Labov's original test – the ILI (Index of Linguistic Insecurity). In their paper, they hypothesize that this effect can be explained by an interaction between behavior and attitudes about social status. Members the lower middle class are caught between the linguistic behavior of the classes below them and the attitudes of the upper class. Members of the lower middle class accept the idea of correct speech from those above them, but changes in their usage lag behind changes in attitude. They identify the upper class usage as correct and admit that their behavior is different, leading to a disparity that manifests itself as linguistic insecurity. Though Owens and Baker admit that a measure of the mobility aspirations of the respondents is needed to test their explanation, others agree that the effect can be best interpreted as a function of upward social mobility rather than of social class distinctions themselves. In his later work, Labov highlights that it is often the second highest status groups that display the steepest slope of style shifting, most hypercorrection, highest levels on linguistic insecurity tests, and the strongest tendency to stigmatize the speech of others in subjective evaluation tests for that variable. In many cases of socioeconomic stratification, this group is equated with the lower middle class.\n\nIn the Owens and Baker study mentioned above, the authors used the CILI and ILI test to conclude that women are more linguistically insecure than men. Out of a sampling data of 80 participants, 42 of which were female, women scored higher on the ILI and the CILI, which indicates high manifest linguistic insecurity. On the CILI, the mean score was 3.23 for females and 2.10 for males. On the ILI, the means scores were 2.23 for females and 1.40 for males. Though the t-tests for the differences were only significant at .07 and .06 levels, the authors feel that this was due to a small sample size and that the uniformity of the results was enough to confirm their hypothesis. Additionally, these findings are consistent with Labov's original New York study and lead to the conclusion by Owens and Baker that women display more linguistic insecurity than men.\n\nLinguistic insecurity can be heightened in speech communities in which multiple dialects exist beyond the standard language. Insecure speakers suffer from a negative attitude toward the speech of their dialect group and often feel pressured to mask their dialectal versatility since the norm of communication is to use the standard form. Bidialectal speakers, who speak both the standard and their own dialect, are most vulnerable to this problem because they are more aware of linguistic norms and the contexts to which they must adapt their speech to these norms. For monodialectal speakers, conversations can be difficult or stressful because they are locked into their nonstandard dialect and have a harder time explaining themselves in the standard dialect.\n\nAfrican American Vernacular English (AAVE) is a dialect of American English that is associated with the African American ethnic group. Speakers of AAVE (as well as speakers of other dialects found in the United States) have encountered a variety of sociolinguistic problems in many important institutions since Standard American English (SAE) is the predominant form of English used.\n\nOne of these important institutions is school. Concerns about the academic achievement of African American children have motivated researchers to study the role AAVE plays though there are various explanations for how it might affect achievement. Dialectal differences could lead to inappropriate testing procedures or prejudice of educators (having lowered expectations and assuming the child is inarticulate and hesitant). In this environment, AAVE-speaking students may develop linguistic insecurity, leading to a rejection of the standards as \"posh\" or reluctance to speak at all to hide their \"inability\" to use language. AAVE-speaking students have also been shown to hypercorrect in attempts to speak or write in Standard English. Insecurity about what \"sounds right\" may result in the avoidance of the invariant \"be\" by deleting it from an instance in which it would be proper to use it (e.g. \"They said they were told if they didn't follow orders they \"would courtmarshled\" or shot as deserters\").\n\nSpeakers of AAVE may also encounter problems in seeking treatment for mental health problems, where professionals predominantly use Standard American English. Linguistic insecurity can be a cause of miscommunication for AAVE patients. For example, mental health care providers may attribute speaker's behavior to cognitive or emotional deficits, even to a psychopathological extent. In a study of a psychiatric ward, Bucci and Baxter collected data on the impact of linguistic problems of the patients, which included several monodialectal speakers and bidialectal speakers of AAVE. In the case of \"Jimmy\", his background led his therapist to believe that his \"muteness\" resulted from emotional or neurophysiological problems. However, Bucci and Baxter found evidence indicating his position as a monodialectal AAVE speaker made him unwilling to speak. His linguistic insecurity in the clinical setting with a norm of SAE made him reluctant to speak, but he was fluent and expressive in his own speech community and with his descriptions of his experiences outside the ward. Moreover, standard therapeutic techniques may have a negative and opposite effect for linguistically insecure patient. In the case of the bidialectal \"Arlene\", the patient thought that her speech was an obstacle to communication because her therapist often asked her what she meant. The intervention of eliciting answers was meant to encourage Arlene to speak more freely, but her linguistic insecurity led her to focus her attention on the perceived inadequacy of her language style and she responded by saying less rather than more.\n\nOne example of linguistic insecurity arising from dialectal differences can be found in work done by Canut and Keita (1994). They conducted a study of an area in the Mandingo zone of Mali that exhibited a linguistic continuum between two different forms: Bambara and Malinke. The study included two villages (Bendugu and Sagabari), a middle-sized town (Kita), and the capital of Mali (Bamako). Bamako is on the Bambara extreme of the continuum, Sagabari on the Malinke extreme, and Bendugu and Kita in between. The linguistic features important for understanding the differences between the dialects are mainly phonological.\n\nThe area encompassing these four places has relatively high social mobility and those who gain status often move towards Bamako, the capital. The dialects follow this pattern, as those closer to the capital are perceived as more prestigious; the most peripheral form in Sagabari can even prompt mockery of the individual using it. Thus, those speaking a dialect different from Bambara are likely to be affected by linguistic insecurity, particularly those closer to the Malinke end of the continuum.\nSince migration is common, there are many examples of young migrants to the area who display linguistic insecurity. Most migrants who speak Malinke try to hide their origins and assimilate to the higher status society by changing the way that they speak. In their attempts to escape their geosocial status, however, they tend to hypercorrect to the point where they create non-existent terms in Bambara. One example is replacing every /h/ in Malinke with the /f/ used in Bamako, leading one to say 'young boy' /foron/ (which does not exist in Bamako) for 'noble' /horon/.\n\nLinguistic insecurity in relation to creoles has to do with the underlying assumption and classification of these languages as inferior forms of the parent languages from which they are derived. Typical of most non-official languages, creoles are regarded as mere degenerate variants and rudimentary dialects that are subsumed under the main \"standard\" languages for that particular community. With this popular view, creoles are thought to be impoverished, primitive outputs that are far from their European target languages. The negative nonlinguistic implications lead to claims of creole use as being a \"handicap\" for their speakers. This has caused speakers of these creole languages to experience insecurity and lack of confidence in the use of their form of language, which has undermined the prevalence of creoles spoken in societies.\n\nOne explanation concerning the different attitudes of speakers is that some populations are more insistent of the use of their particular form of language, as it is commonly claimed to be more \"pure.\" This assumption places this form as a more prestigious standard, creating a tense environment that promotes feelings of insecurity to those who do not follow this standard (and speak \"impure\" variations).\n\nAn instance of linguistic insecurity can be found in relation to Haitian Creole, which developed from a combination of French and other languages. Although the vast majority in this country grows up hearing and speaking exclusively this creole, it continues to be seen as an inferior, primitive tongue as well as a malformed version of French. This disfavor against the creole, which exists throughout the society, is present even among those who can speak only in that variation. The cause of the view has been attributed to the association of French with prestige, as most of the island's land-owning, well-educated elite speaks this language. These judgments contribute to the widespread belief that success is linked to French and that one must speak French to become part of the middle class with a financially stable job, a notion that places Haitian Creole on a lower status. Though it is the majority of people who cannot participate in the French-driven areas of society, the \"ideology of disrespect and degradation\" surrounding creoles leads to great linguistic insecurity. As Arthur Spears put it, an \"internalized oppression\" is present in these members who relate important figures in society (and their success) to speaking French, devaluing their own language of Haitian Creole.\n\nLinguistic insecurity can arise in multilingual environments in speakers of the non-dominant language or of a non-standard dialect. Issues caused by the linguistic variation range from \"total communication breakdowns involving foreign language speakers to subtle difficulties involving bilingual and bidialectal speakers\". Divergence from the standard variety by minority languages causes \"a range of attitudinal issues surrounding the status of minority languages as a standard linguistic variety\".\n\nAn example of mother-tongue-based linguistic insecurity in a multilingual environment is Quebec French. Due to a general perception of Quebec French as lacking in quality and diverging from the norm, French speaking Quebeckers have suffered from a sense of linguistic insecurity. Though French is widely spoken in Quebec, the French in France is considered by many to be the standard and prestigious form. This comparison and the fact that Quebec French diverges from the standard form of France have caused linguistic insecurity among Quebec speakers.\n\nDue to the separation from France after the Treaty of Paris in 1763 and the multilingual environment, Quebec French become more anglicized through English pronunciations and borrowings. Though French Canadian speakers were aware of the differences between Quebec French and French, the foreign perception of Quebec French as \"non-standard\" was not an issue until the mid 19th century. The opinions of the French elite that Quebec French was \"far removed from the prestigious variety spoken in Paris\" had spread through the general public by the end of the 19th century, causing a deep sense of linguistic insecurity in French speaking Quebec. The insecurity was twofold since Quebeckers spoke neither the dominant English language nor, as they were being told, Standard French.\n\n"}
{"id": "33703023", "url": "https://en.wikipedia.org/wiki?curid=33703023", "title": "List of largest infrared telescopes", "text": "List of largest infrared telescopes\n\nList of largest infrared telescopes, by diameter of entrance aperture, oriented towards large observatories dedicated to infrared astronomy.\n\n For comparison\n\n"}
{"id": "1557623", "url": "https://en.wikipedia.org/wiki?curid=1557623", "title": "Longitudinal study", "text": "Longitudinal study\n\nA longitudinal study (or longitudinal survey, or panel study) is a research design that involves repeated observations of the same variables (e.g., people) over short or long periods of time (i.e., uses longitudinal data). It is often a type of observational study, although they can also be structured as longitudinal randomized experiments.\n\nLongitudinal studies are often used in social-personality and clinical psychology, to study rapid fluctuations in behaviors, thoughts, and emotions from moment to moment or day to day; in developmental psychology, to study developmental trends across the life span; and in sociology, to study life events throughout lifetimes or generations. The reason for this is that, unlike cross-sectional studies, in which different individuals with the same characteristics are compared, longitudinal studies track the same people, and so the differences observed in those people are less likely to be the result of cultural differences across generations. Longitudinal studies thus make observing changes more accurate and are applied in various other fields. In medicine, the design is used to uncover predictors of certain diseases. In advertising, the design is used to identify the changes that advertising has produced in the attitudes and behaviors of those within the target audience who have seen the advertising campaign. Longitudinal studies allow social scientists to distinguish short from long-term phenomena, such as poverty. If the poverty rate is 10% at a point in time, this may mean that 10% of the population are always poor or that the whole population experiences poverty for 10% of the time. It is impossible to conclude which of these possibilities is the case by using one-off cross-sectional studies.\n\nWhen longitudinal studies are observational, in the sense that they observe the state of the world without manipulating it, it has been argued that they may have less power to detect causal relationships than experiments. However, because of the repeated observation at the individual level, they have more power than cross-sectional observational studies, by virtue of being able to exclude time-invariant unobserved individual differences and also of observing the temporal order of events. Some of the disadvantages of longitudinal study are that they take a lot of time and are very expensive. Therefore, they are not very convenient.\n\nLongitudinal studies can be retrospective (looking back in time, thus using existing data such as medical records or claims database) or prospective (requiring the collection of new data).\n\nCohort studies are one type of longitudinal study which sample a cohort (a group of people who share a defining characteristic, typically who experienced a common event in a selected period, such as birth or graduation) and perform cross-section observations at intervals through time. However, not all longitudinal studies are cohort studies, as longitudinal studies can instead include a group of people who do not share a common event.\n\n\n"}
{"id": "583444", "url": "https://en.wikipedia.org/wiki?curid=583444", "title": "Medical intuitive", "text": "Medical intuitive\n\nA medical intuitive is an alternative medicine practitioner who claims to use their self-described intuitive abilities to find the cause of a physical or emotional condition through the use of insight rather than modern medicine. Other terms for such a person include medical clairvoyant, medical psychic, intuitive counselor or fraud.\n\nThe practice of claiming to use intuition or clairvoyance for medical information dates back to Phineas Parkhurst Quimby (1802–1866), whose intuitive healing practice began in 1854. Edgar Cayce (1877–1945) was known as one of the most well known medical clairvoyants. William M. Branham, the father of the Pentecostal Latter Rain Movement, was said by his followers to be able to discern the health condition of people that attended his services, and in many cases heal them of their affliction.\n\nMaking a formal medical diagnosis is not a practice for many medical intuitives, but some medical intuitives also work with M.D.s or N.D.s including some general practitioners who have called on medical intuitives for second opinions. In a few cases medical intuitives have been hired by hospitals, clinics and medical offices, particularly in California. Many medical professionals and psychologists attribute perceived anecdotal successes by medical intuitives to a combination of wishful thinking, confirmation bias, the placebo effect, and regression fallacy associated with self-limiting conditions.\n\nA few educational institutions offer graduate degrees that include \"research-based training\" and certifications for medical intuitives. Other medical intuitives may be licensed medical professionals and their ability to accurately diagnose diseases and heal may not be supported by scientific evidence.\n\nA published study on medical intuitives said that \"patients relying solely on psychic diagnosis as the basis for therapy are at risk of serious medical problems going undetected\".\n\nIn 2009 Steven Novella, writing on Science Based Medicine, calls medical intuitive diagnosis as \"purely magical thinking\" and refers to a Huffington Post article about it as \"a promotion of a dubious pseudoscientific medical claim\".\n\n\n"}
{"id": "56652006", "url": "https://en.wikipedia.org/wiki?curid=56652006", "title": "Morantel", "text": "Morantel\n\nMorantel is an anthelmintic drug used for the removal of parasitic worms in livestock. It affects the nervous system of worms given the drug is an inhibitor of acetylcholinesterase.\n"}
{"id": "27417865", "url": "https://en.wikipedia.org/wiki?curid=27417865", "title": "Muringa vila", "text": "Muringa vila\n\nMuringa vila (Malayalam: The place of the muringa tree) is an international development project in Kovalam, Kerala, India, for sustainable building and income structures for the local participants. Structured after the concept of creative participation \"Muringa Vila\" integrates vital interests of its cultural creative participants as well as environmental and spiritual aspects, providing housing and income through self-created business as well as pursuing a sustainable local infrastructure.\n\nThe project was founded by a local fisherman and a German ethnologist in 2005 with the goal of finding practical solutions for problems of the indigenous society of this particular place which are created by a changing global environment, i.e. depleted oceans (as a source of income for the fishermen), gentrification of the village, emigration, educational deficits, immediate environmental threads, the take over of the beaches by tourism.\n\n\n"}
{"id": "20422283", "url": "https://en.wikipedia.org/wiki?curid=20422283", "title": "NGC 15", "text": "NGC 15\n\nNGC 15 is a spiral galaxy located in the Pegasus constellation. It was discovered by Albert Marth on October 30, 1864.\n"}
{"id": "3009810", "url": "https://en.wikipedia.org/wiki?curid=3009810", "title": "Nitratine", "text": "Nitratine\n\nNitratine or nitratite, also known as \"cubic niter\" (UK: \"nitre\"), \"soda niter\" or \"Chile saltpeter\" (UK: \"Chile saltpetre\"), is a mineral, the naturally occurring form of sodium nitrate, NaNO. Chemically it is the sodium analogue of saltpeter. Nitratine crystallizes in the trigonal system, but rarely occurs as well formed crystals. It is isostructural with calcite. It is quite soft and light with a Mohs hardness of 1.5 to 2 and a specific gravity of 2.24 to 2.29. Its refractive indices are nω=1.587 and nε=1.336.\n\nThe typical form is as coatings of white, grey to yellowish brown masses. The rare crystals when found typically have the scalenohedral form of the calcite structure. It is found only as an efflorescence in very dry environments. It is very soluble in water such that it is deliquescent and will absorb water out of the air and turn into a \"puddle\" of sodium nitrate solution when exposed to humid air. \n\nNitratine was once an important source of nitrates for fertilizer and other chemical uses including fireworks. It has been known since 1845 from occurrences in the Confidence Hills, Southern Death Valley, California and the Atacama Desert, Chile. It is still used in organic farming (where Haber-Bosch ammonia is forbidden) in the US, but prohibited in international organic agriculture.\n\n\n"}
{"id": "376476", "url": "https://en.wikipedia.org/wiki?curid=376476", "title": "Ocean current", "text": "Ocean current\n\nAn ocean current is a continuous, directed movement of sea water generated by a number forces acting upon the water, including wind, the Coriolis effect, breaking waves, cabbeling, and temperature and salinity differences. Depth contours, shoreline configurations, and interactions with other currents influence a current's direction and strength. Ocean currents are primarily horizontal water movements.\n\nOcean currents flow for great distances, and together, create the global conveyor belt which plays a dominant role in determining the climate of many of the Earth’s regions. More specifically, ocean currents influence the temperature of the regions through which they travel. For example, warm currents traveling along more temperate coasts increase the temperature of the area by warming the sea breezes that blow over them. Perhaps the most striking example is the Gulf Stream, which makes northwest Europe much more temperate than any other region at the same latitude. Another example is Lima, Peru, where the climate is cooler, being sub-tropical, than the tropical latitudes in which the area is located, due to the effect of the Humboldt Current. \n\nSurface oceanic currents are sometimes wind driven and develop their typical clockwise spirals in the northern hemisphere and counter-clockwise rotation in the southern hemisphere due to imposed wind stresses. In these wind-driven currents, the Ekman spiral effect results in the currents flowing at an angle to the driving winds. In addition, the areas of surface ocean currents move somewhat with the seasons; this is most notable in equatorial currents.\n\nDeep ocean basins generally have a non-symmetric surface current, in that the eastern equatorward-flowing branch is broad and diffuse whereas the western poleward flowing branch is very narrow. These western boundary currents (of which the Gulf Stream is an example) are a consequence of the rotation of the Earth.\n\nDeep ocean currents are driven by density and temperature gradients. Thermohaline circulation is also known as the ocean's conveyor belt (which refers to deep ocean density-driven ocean basin currents). These currents, called submarine rivers, flow under the surface of the ocean and are hidden from immediate detection. Where significant vertical movement of ocean currents is observed, this is known as upwelling and downwelling. Deep ocean currents are currently being researched using a fleet of underwater robots called Argo.\n\nSurface currents make up only 8% of all water in the ocean, are generally restricted to the upper of ocean water, and are separated from lower regions by varying temperatures and salinity which affect the density of the water, which in turn, defines each oceanic region. Because the movement of deep water in ocean basins is caused by density-driven forces and gravity, deep waters sink into deep ocean basins at high latitudes where the temperatures are cold enough to cause the density to increase.\n\nOcean currents are measured in sverdrup (sv), where 1 sv is equivalent to a volume flow rate of per second.\n\nSurface currents are found on the surface of an ocean, and are driven by large scale wind currents. They are directly affected by the windthe Coriolis effect plays a role in their behaviours.\n\nThe thermohaline circulation is a part of the large-scale ocean circulation that is driven by global density gradients created by surface heat and freshwater fluxes. The adjective \"thermohaline\" derives from \"thermo-\" referring to temperature and \"\" referring to salt content, factors which together determine the density of sea water. Wind-driven surface currents (such as the Gulf Stream) travel polewards from the equatorial Atlantic Ocean, cooling en route, and eventually sinking at high latitudes (forming North Atlantic Deep Water). This dense water then flows into the ocean basins. While the bulk of it upwells in the Southern Ocean, the oldest waters (with a transit time of around 1000 years) upwell in the North Pacific. Extensive mixing therefore takes place between the ocean basins, reducing differences between them and making the Earth's oceans a global system. On their journey, the water masses transport both energy (in the form of heat) and matter (solids, dissolved substances and gases) around the globe. As such, the state of the circulation has a large impact on the climate of the Earth. The thermohaline circulation is sometimes called the ocean conveyor belt, the great ocean conveyor, or the global conveyor belt. On occasion, it is imprecisely used to refer to the meridional overturning circulation, \"MOC\".\n\nKnowledge of surface ocean currents is essential in reducing costs of shipping, since traveling with them reduces fuel costs. In the wind powered sailing-ship era, knowledge of wind patterns and ocean currents was even more essential. A good example of this is the Agulhas Current (down along eastern Africa), which long prevented Portuguese sailors from reaching India. In recent times, around-the-world sailing competitors make good use of surface currents to build and maintain speed. Ocean currents are also very important in the dispersal of many life forms. An example is the life-cycle of the European Eel.\n\nOcean currents are important in the study of marine debris, and vice versa. These currents also affect temperatures throughout the world. For example, the ocean current that brings warm water up the north Atlantic to northwest Europe also cumulatively and slowly blocks ice from forming along the seashores, which would also block ships from entering and exiting inland waterways and seaports, hence ocean currents play a decisive role in influencing the climates of regions through which they flow. Cold ocean water currents flowing from polar and sub-polar regions bring in a lot of plankton that are crucial to the continued survival of several key sea creature species in marine ecosystems. Since plankton are the food of fish, abundant fish populations often live where these currents prevail.\n\nOcean currents can also be used for marine power generation, with areas off of Japan, Florida and Hawaii being considered for test projects.\n\nThe OSCAR Near-realtime global ocean surface currents website describes the project and links to data validation and data downloads.\n\nOSCAR data is used extensively in climate studies. maps and descriptions or annotations of climatic anomalies have been published in the monthly Climate Diagnostic Bulletin since 2001 and are routinely used to monitor ENSO and to test weather prediction models. OSCAR currents are routinely used to evaluate the surface currents in Global Circulation Models (GCMs), for example in NCEP Global Ocean Data Assimilation System (GODAS) and European Centre for Medium-Range Weather Forecasts (ECMWF).\n\n\n\n"}
{"id": "9527259", "url": "https://en.wikipedia.org/wiki?curid=9527259", "title": "Reginald Hooley", "text": "Reginald Hooley\n\nReginald Walter Hooley (5 September 1865 – 5 May 1923) was a businessman and amateur paleontologist, collecting on the Isle of Wight.\n\nRegular visits to the island resulted in the find of hundreds of fossils among which several major specimens on which he, from 1900, published fourteen scientific papers. He described remains of many turtles, and named the dinosaur \"Iguanodon atherfieldensis\" and the pterosaur \"Ornithodesmus latidens\". Hooley was a member of the Hampshire Field Club & Archaeological Society at Winchester from 1890. He was one of the founders of the Isle of Wight Natural History and Archaeological Society. He was an honorary curator of the Winchester Museum between 1918 and 1923.\n\nReginald was born in Southampton, the son of William Hooley, a wealthy gentleman. In 1889 R.W. Hooley began to work for \"Godrich & Petman\", wine merchants, and later in life became managing director of that firm. Living in Portswood, in 1912 he married E.E. Holden and moved to Winchester. In 1913 he was elected a member of the Winchester city council. Hooley made his most famous finds in 1889 and 1914 when two iguanodontid skeletons were exposed by erosion at the cliffs. In 1904 the remains of \"Ornithodesmus\" were uncovered by a cliff fall. After Hooley's death, the paper naming \"Iguanodon atherfieldensis\" was posthumously published and most of the \"Hooley Collection\", over 1330 specimens, was, in 1924. acquired by the British Museum of Natural History which displays the iguanodontid skeletons in the Dinosaur Hall. In 1926 the extinct plant \"Hooleya\" was named after him.\n"}
{"id": "52616828", "url": "https://en.wikipedia.org/wiki?curid=52616828", "title": "Roland Andrews", "text": "Roland Andrews\n\nRoland Stuart Andrews CMG DSc FAA, (20 September 1897 (Granville, Sydney) – 14 October 1961 (Glen Iris, Melbourne)), was an industrial chemist and administrator.\n"}
{"id": "29427548", "url": "https://en.wikipedia.org/wiki?curid=29427548", "title": "Scanning joule expansion microscopy", "text": "Scanning joule expansion microscopy\n\nScanning Joule Expansion Microscopy is a form of scanning probe microscopy heavily based on atomic force microscopy that maps the temperature distribution along a surface. Resolutions down to 10 nm have been achieved and 1 nm resolution is theoretically possible. Thermal measurements at the nanometer scale are of both academic and industrial interest, particularly in regards to nanomaterials and modern integrated circuits.\n\nScanning Joule Expansion Microscopy (SJEM) is based on the contact operation model of Atomic Force Microscopy (AFM). During the operation, the tip on the cantilever is brought into contact with the surface of the sample. AC or pulsed electrical signal is applied to the sample creating Joule heating and resulting in periodic thermal expansion. At the same time, the laser, which is focused on the top surface of the cantilever and the photodiode of the equipment, detects the displacement of the cantilever. The detecting photodiode is composed of two segments, which normalizes the incoming signal deflected from the cantilever. This differential signal is proportional to the cantilever deflection.\n\nThe deflection signals are caused not only by sample topography, but also by the thermal expansion caused by Joule heating. Since AFM has feedback controller with a bandwidth, for example 20 kHz (different AFM may have different bandwidths), the signal below 20 kHz is captured and processed by the feedback controller which then adjusts the z-piezo to image surface topography. Joule heating frequency is kept well above 20 kHz to avoid feedback response and to separate topological and thermal effects. The upper limit of the frequency is limited by the decrease of thermoelastic expansion with the inverse power of the modulation frequency and the frequency characteristics of the cantilever arrangement. A lock-in amplifier is specially tuned to the Joule heating frequency for detecting only the expansion signal and provides the information to an auxiliary Atomic Force Microscopy channel to create the thermal expansion image. Usually expansion signals approximately 0.1 Angstroms start to be detected, although the resolution of SJEM highly depends on the whole system (cantilever, sample surface, etc.).\n\nBy comparison, Scanning Thermal Microscopy (SThM) has coaxial thermocouple at the end of sharp metal tip. The spatial resolution of SThM critically depends on the thermocouple sensor size. Much effort has been dedicated to reducing sensor size to sub-micrometre scales. The quality and resolution of the images are very dependent on the nature of the thermal contact between tip and the sample; hence it is quite difficult to control in a reproducible way. The fabrication also becomes very challenging particularly for thermocouple sensor size below 500 nm. With optimization on the design and the fabrication, it was possible to achieve resolution around 25 nm. Scanning Joule Expansion Microscopy, however, has the potential of achieving similar to AFM resolution of 1~10 nm. In practice, however, the spatial resolution is limited to the size of the liquid film bridge between the tip and the sample, which is typically about 20 nm. The microfabricated thermocouples used for Scanning Thermal Microscopy are rather expensive and more importantly very fragile. Scanning Joule Expansion Microscopy has been used to measure the local heat dissipation of an in-plane gate (IPG) transistor to study hot spots in semiconductor devices, and thin-film alloy like cobalt-nickel silicide.\n\nSignal obtained by the AFM (and captured by lock-in amplifier) are actually representations of the cantilever deflection at a specific frequency. However, besides thermal expansion, several other sources may also result in cantilever deflection.\n\nThis is usually due to the mismatch in thermal expansion of two cantilever materials, for instance, silicon cantilever coated with a thin layer of metal (to increase the deflection). When heated, materials with higher expansion coefficient will expand more than the material with lower expansion coefficient. In this case, two materials, one in tensile strain, the other in compression strain, will induce substantial bending. However, this mechanism can be excluded for two reasons; first, cantilever coatings have been stripped experimentally and no change in signal was observed; second, the calculated thermal diffusion length in SiNx and Si cantilevers at the SJEM working frequency (typically 10 kHz~100 kHz) is small, much smaller than the length of the cantilever(typically 100 um).\n\nWhen the sample heats and contracts due to rapid Joule heating from an applied AC power source, pressure waves may be radiated from the sample. This wave may interact with the cantilever, causing additional deflection. However, this possibility is unlikely. For sinusoidal heating, the wavelength of the acoustic wave in air with speed of 340 m/s is about several millimeters, which is much larger than the length of cantilever. Furthermore, experiments have been carried out under vacuum, in which case there are no air pressure waves. In the experiment, it was observed that when the cantilever was out of contact with sample surface, no deflection signal was detected.\n\nIn piezoelectric materials, mechanical expansion occurs due to applied bias. Therefore, if the sample is such a material, an additional piezoelectric effect must be considered when analyzing the signal. Typically, piezoelectric expansion is linearly dependent on applied voltage and a simple subtraction can be used to correct for this effect.\n\nWhen a bias is applied to the sample for Joule heating, there is also an electrostatic force interaction between the tip and the sample. The tip-sample electrostatic force can be represented as formula_1, in which C is the tip sample capacitance, and V is the voltage, Z is the tip and sample distance. This force also depends on formula_2, the same as the expansion signal. Usually electrostatic force is small because the sample has been covered with a polymer layer. However, when applied voltage is large, this force needs to be considered. Electrostatic force does not depend on the frequency of the applied AC signal, therefore allowing for a simple method to differentiate and account for this contribution.\n\nThis is primary mode of signal and the main goal of SJEM. The substrate expands when Joule heated, resulting in change in the measured profile by the cantilever, resulting in a change in signal. However, thermal expansion coefficients can vary significantly. For example, the thermal expansion coefficients of metal are typically one order of magnitude higher than those of dielectric and amorphous materials; while the expansion coefficient of polymer is one order higher than those of metals. So by coating the sample surface with a layer of polymer, the expansion signal could be enhanced. More importantly, after coating, the signal only depends on the temperature, independent of the expansion coefficient of different materials, allowing for SJEM to be used for a wide array of samples. Expansion signal increases linearly with temperature and thus quadratically with voltage. In addition, expansion signal increases monotonically with the thickness of coating polymer, while the resolution will decrease due to greater thermal diffusion. Lastly, expansion signal decreases as the frequency increases.\n\nBy using the expansion signal, the temperature can be extracted as follows: the signal that captured by the lock-in amplifier is converted into the bending of cantilever. Using formula_3,and applying the known expansion coefficient, formula_4 and polymer thickness, L(which could be measured by AFM or ellipsometer), the expansion signal is obtained. The smallest expansion that can be resolved is about 10pm. \nIn order to extract accurate temperatures, additional modeling taking into account thermal expansion and cantilever bending is necessary. Moreover, calibration using a reference system, such as metallic films, is required.\n\nWhen the sample is large enough, edge effects can be ignored. Therefore, a simple one-dimensional finite element model can be a good approximation.\n\nThe basic thermal equation is:\n\nformula_5\n\nHere, ρCp is the heat capacitance;K is the thermal conductivity and Q is the input power.\n\nRearrange the equation in a discrete form according to each element:\n\nformula_6\n\nHere, formula_7 represents the specific temperature of position element n at time element t. Using software could solve the equations and obtained the temperature T.\nThe expansion magnitude could be obtained by:\n\nformula_3\n\nformula_4\nis the thermal expansion coefficient of the polymer and L is its thickness.\n\nCommercialized software can be used for 2D/3D finite element modeling. In such software, the appropriate differential equations for electrical, thermal and mechanical expansion are chosen and proper boundary conditions are set. In addition, electrical-thermal coupling exists in the sample because the resistance is a function of temperature. This is additionally accounted for by typical FEM software packages.\n\nMiniaturization of modern integrated circuits has led to hugely increased current densities and therefore, self-heating. In particular, vias, or vertical interconnects, experience extreme local temperature fluctuations, which can strongly influence the electrical performance of multi-level interconnect structures. In addition, these large, highly localized temperature fluctuations cause repeated stress gradients on the vias, ultimately leading to device failure. Traditional thermometry techniques use electrical characterization to determine resistivity and estimate the average temperature along an interconnect. However, this method is not able to characterize local temperature rises which may be significantly higher near vias due to their extremely high aspect ratios. Optical methods are diffraction limited to resolutions greater than 1 um, far larger than most modern vias feature sizes. SJEM has been used to do in situ thermal mapping of these devices with lateral resolution in the sub-0.1 um range.\n\nIn addition, size effects also play an important role in modern interconnects. As dimensions of the metal decrease, thermal conductivity begins to decrease from that of the bulk material, further creating cause for concern. SJEM has been used to extract thermal conductivities of constrictions in different thicknesses of thin metallic films. The extracted values show agreement with those predicted by the Wiedemann-Franz law.\n\nUnderstanding thermal properties of transistors are vital for the semiconductor industry as well. Similar to interconnects, repeated thermal stresses can eventually lead to device failure. However, more importantly, electrical behavior, and therefore device parameters change significantly with temperature. SJEM has been used to map local hotspots in thin film transistors. By determining the location of these hotspots, they can be better understood and reduced or eliminated. One disadvantage to this method is that, like AFM, only the surface can be mapped. Consequently, additional processing steps would be required in order to map buried features, such as most features in modern IC transistors.\n\nNanoscale materials are becoming widely investigated for their many advantages in commercial electronics. In particular, these materials are known for excellent mobility as well as ability to carry high current densities. In addition, new applications have been realized for these materials including thermoelectrics, solar cells, fuel cells, etc. However, significant decrease in size scale in conjunction with increases in current density and device density leads to extreme temperature rises in these devices. These temperature fluctuations can influence electrical behavior and lead to device failure. Therefore, these thermal effects must be studied carefully, in situ, to realize nanoscale electronics. SJEM can be used for this purpose, allowing for in situ high-resolution thermal mapping.\n\nPossible materials and devices for thermal mapping include high electron mobility transistors, nanotubes, nanowires, graphene sheets, nanomeshes, and nanoribbons, and other molecular electronic materials. In particular, SJEM can be directly used for characterization of band gap distributions in nanotube transistors, nanowires, and graphene nanomeshes and nanoribbons. It can also be used to locate hotspots and defects in these materials. Another example of a simple, direct application is thermal mapping of rough nanowires for thermolelectric applications.\n\nAlthough SJEM is a very powerful technique for temperature detection, significant questions still remain regarding its performance.\n\nThis technique is far more complex than traditional AFM. Unlike AFM, SJEM needs to consider the type of polymer, the thickness of polymer used to coat the sample and the frequency to drive the device. This additional processing often can degrade or compromise the integrity of the sample. For micro/nano devices, wire-bonding is usually necessary to apply voltage, further increasing processing and decreasing throughput. During scanning, the magnitude of the voltage, frequency, and scanning speeds need to be considered. Calibration must also be done using a reference system in order to ensure accuracy. Finally, a complex model must be used to account for all these factors and parameters.\n\nSecond, there may be artifact effects near the edges (or steps). Near the edges where large height differences or material mismatches exist, artifact expansion signals are usually detected. The exact cause has not been found. It is widely believed that the tip sample interaction near the edges can account for these artifacts. At the edges, forces are present not only in the vertical direction but possibly also in the lateral direction, disrupting the cantilever motion. In addition, at a large step, loss of contact between the tip and the sample could result in an artifact in the image. Another concern is that the polymer coating near the step may not be uniform, or possibly not continuous. Further investigations near edges and junctions need to be carried out. \nFinally, interactions between the tip and electric field can occur when large gate biases are applied to the substrate. Fringing effects and other geometric concerns can lead to electric field concentrations, leading to large deviations from the normal baseline tip interaction which cannot be easily subtracted. This is especially problematic where the polymer expansion is small, leading to artifacts from this effect dominating. The contribution from these artifacts can be reduced by applying thicker polymer coatings or operating at a lower gate bias to decrease electric field. However, this occurs at the expense of resolution due to increased thermal diffusion in the thicker polymer layer as well as increased noise. In addition, devices may not be fully modulated at lower gate biases.\n"}
{"id": "181891", "url": "https://en.wikipedia.org/wiki?curid=181891", "title": "School science technician", "text": "School science technician\n\nIn schools, the science technician is the person who prepares the practical equipment and makes up the solutions used in school science labs. The role also includes instructing and assisting teachers with practical skills, including class demonstrations, for advanced techniques across all disciplines. Many are very well qualified and have degrees, such as a Bachelor's degree (B.A. or B.Sc.) or Master's degree (M.Sc.) and/or other professional qualifications such as the HNC, HND and NVQ. \nTheir main duties include:\n\nIn December 2002 CLEAPSS commissioned a survey into the Specific Job roles of Science Technicians. The pdf Document G228 - Technicians and their jobs which can be freely downloaded was released and later updated in 2009. The guide was written to help promote a professional technician service in schools and colleges\n\n\n"}
{"id": "1139242", "url": "https://en.wikipedia.org/wiki?curid=1139242", "title": "Social balance theory", "text": "Social balance theory\n\nSocial balance theory is a class of theories about balance or imbalance of sentiment relation in dyadic or triadic relations with social network theory. Sentiments can result in the emergence of two groups. Disliking exists between the two subgroups within liking agents.\n\nThis theory evolved over time to produce models more closely resembling real-world social networks. It uses a balance index to measure the effect of local balance on that of a global level and also on a more intimate level, like in interpersonal relationships. Dorwin Cartwright and Frank Harary introduced \"clustering\" to account for multiple social cliques. Davis introduced \"hierarchical clustering\" to account for asymmetric relations.\n\n"}
{"id": "37461", "url": "https://en.wikipedia.org/wiki?curid=37461", "title": "State of matter", "text": "State of matter\n\nIn physics, a state of matter is one of the distinct forms in which matter can exist. Four states of matter are observable in everyday life: solid, liquid, gas, and plasma. Many other states are known to exist, such as glass or liquid crystal, and some only exist under extreme conditions, such as Bose–Einstein condensates, neutron-degenerate matter, and quark-gluon plasma, which only occur, respectively, in situations of extreme cold, extreme density, and extremely high-energy. Some other states are believed to be possible but remain theoretical for now. For a complete list of all exotic states of matter, see the list of states of matter.\n\nHistorically, the distinction is made based on qualitative differences in properties. Matter in the solid state maintains a fixed volume and shape, with component particles (atoms, molecules or ions) close together and fixed into place. Matter in the liquid state maintains a fixed volume, but has a variable shape that adapts to fit its container. Its particles are still close together but move freely. Matter in the gaseous state has both variable volume and shape, adapting both to fit its container. Its particles are neither close together nor fixed in place. Matter in the plasma state has variable volume and shape, but as well as neutral atoms, it contains a significant number of ions and electrons, both of which can move around freely.\n\nThe term phase is sometimes used as a synonym for state of matter, but a system can contain several immiscible phases of the same state of matter.\n\nIn a solid, constituent particles (ions, atoms, or molecules) are closely packed together. The forces between particles are so strong that the particles cannot move freely but can only vibrate. As a result, a solid has a stable, definite shape, and a definite volume. Solids can only change their shape by force, as when broken or cut.\n\nIn crystalline solids, the particles (atoms, molecules, or ions) are packed in a regularly ordered, repeating pattern. There are various different crystal structures, and the same substance can have more than one structure (or solid phase). For example, iron has a body-centred cubic structure at temperatures below 912 °C, and a face-centred cubic structure between 912 and 1394 °C. Ice has fifteen known crystal structures, or fifteen solid phases, which exist at various temperatures and pressures.\n\nGlasses and other non-crystalline, amorphous solids without long-range order are not thermal equilibrium ground states; therefore they are described below as nonclassical states of matter.\n\nSolids can be transformed into liquids by melting, and liquids can be transformed into solids by freezing. Solids can also change directly into gases through the process of sublimation, and gases can likewise change directly into solids through deposition.\n\nA liquid is a nearly incompressible fluid that conforms to the shape of its container but retains a (nearly) constant volume independent of pressure. The volume is definite if the temperature and pressure are constant. When a solid is heated above its melting point, it becomes liquid, given that the pressure is higher than the triple point of the substance. Intermolecular (or interatomic or interionic) forces are still important, but the molecules have enough energy to move relative to each other and the structure is mobile. This means that the shape of a liquid is not definite but is determined by its container. The volume is usually greater than that of the corresponding solid, the best known exception being water, HO. The highest temperature at which a given liquid can exist is its critical temperature.\n\nA gas is a compressible fluid. Not only will a gas conform to the shape of its container but it will also expand to fill the container.\n\nIn a gas, the molecules have enough kinetic energy so that the effect of intermolecular forces is small (or zero for an ideal gas), and the typical distance between neighboring molecules is much greater than the molecular size. A gas has no definite shape or volume, but occupies the entire container in which it is confined. A liquid may be converted to a gas by heating at constant pressure to the boiling point, or else by reducing the pressure at constant temperature.\n\nAt temperatures below its critical temperature, a gas is also called a vapor, and can be liquefied by compression alone without cooling. A vapor can exist in equilibrium with a liquid (or solid), in which case the gas pressure equals the vapor pressure of the liquid (or solid).\n\nA supercritical fluid (SCF) is a gas whose temperature and pressure are above the critical temperature and critical pressure respectively. In this state, the distinction between liquid and gas disappears. A supercritical fluid has the physical properties of a gas, but its high density confers solvent properties in some cases, which leads to useful applications. For example, supercritical carbon dioxide is used to extract caffeine in the manufacture of decaffeinated coffee.\n\nLike a gas, plasma does not have definite shape or volume. Unlike gases, plasmas are electrically conductive, produce magnetic fields and electric currents, and respond strongly to electromagnetic forces. Positively charged nuclei swim in a \"sea\" of freely-moving disassociated electrons, similar to the way such charges exist in conductive metal, where this electron \"sea\" allows matter in the plasma state to conduct electricity.\n\nA gas is usually converted to a plasma in one of two ways. e.g. Either from a huge voltage difference between two points, or by exposing it to extremely high temperatures. Heating matter to high temperatures causes electrons to leave the atoms, resulting in the presence of free electrons. This creates a so-called partially ionised plasma. At very high temperatures, such as those present in stars, it is assumed that essentially all electrons are \"free\", and that a very high-energy plasma is essentially bare nuclei swimming in a sea of electrons. This forms the so-called fully ionised plasma.\n\nThe plasma state is often misunderstood, and although not freely existing under normal conditions on Earth, it is quite commonly generated by either lightning, electric sparks, fluorescent lights, neon lights or in plasma televisions. Also plasma appears in some types of flame, the Sun's corona, and stars are all examples of illuminated matter in the plasma state.\n\nA state of matter is also characterized by phase transitions. A phase transition indicates a change in structure and can be recognized by an abrupt change in properties. A distinct state of matter can be defined as any set of states distinguished from any other set of states by a phase transition. Water can be said to have several distinct solid states. The appearance of superconductivity is associated with a phase transition, so there are superconductive states. Likewise, ferromagnetic states are demarcated by phase transitions and have distinctive properties.\nWhen the change of state occurs in stages the intermediate steps are called mesophases. Such phases have been exploited by the introduction of liquid crystal technology.\n\nThe state or \"phase\" of a given set of matter can change depending on pressure and temperature conditions, transitioning to other phases as these conditions change to favor their existence; for example, solid transitions to liquid with an increase in temperature. Near absolute zero, a substance exists as a solid. As heat is added to this substance it melts into a liquid at its melting point, boils into a gas at its boiling point, and if heated high enough would enter a plasma state in which the electrons are so energized that they leave their parent atoms.\n\nForms of matter that are not composed of molecules and are organized by different forces can also be considered different states of matter. Superfluids (like Fermionic condensate) and the quark–gluon plasma are examples.\n\nIn a chemical equation, the state of matter of the chemicals may be shown as (s) for solid, (l) for liquid, and (g) for gas. An aqueous solution is denoted (aq). Matter in the plasma state is seldom used (if at all) in chemical equations, so there is no standard symbol to denote it. In the rare equations that plasma is used in plasma is symbolized as (p).\n\nGlass is a non-crystalline or amorphous solid material that exhibits a glass transition when heated towards the liquid state. Glasses can be made of quite different classes of materials: inorganic networks (such as window glass, made of silicate plus additives), metallic alloys, ionic melts, aqueous solutions, molecular liquids, and polymers.\nThermodynamically, a glass is in a metastable state with respect to its crystalline counterpart. The conversion rate, however, is practically zero.\n\nA plastic crystal is a molecular solid with long-range positional order but with constituent molecules retaining rotational freedom; in an orientational glass this degree of freedom is frozen in a quenched disordered state.\n\nSimilarly, in a spin glass magnetic disorder is frozen.\n\nLiquid crystal states have properties intermediate between mobile liquids and ordered solids. Generally, they are able to flow like a liquid, but exhibiting long-range order. For example, the nematic phase consists of long rod-like molecules such as para-azoxyanisole, which is nematic in the temperature range 118–136 °C. In this state the molecules flow as in a liquid, but they all point in the same direction (within each domain) and cannot rotate freely. Like a crystalline solid, but unlike a liquid, liquid crystals react to polarized light.\n\nOther types of liquid crystals are described in the main article on these states. Several types have technological importance, for example, in liquid crystal displays.\n\nTransition metal atoms often have magnetic moments due to the net spin of electrons that remain unpaired and do not form chemical bonds. In some solids the magnetic moments on different atoms are ordered and can form a ferromagnet, an antiferromagnet or a ferrimagnet.\n\nIn a ferromagnet—for instance, solid iron—the magnetic moment on each atom is aligned in the same direction (within a magnetic domain). If the domains are also aligned, the solid is a permanent magnet, which is magnetic even in the absence of an external magnetic field. The magnetization disappears when the magnet is heated to the Curie point, which for iron is 768 °C.\n\nAn antiferromagnet has two networks of equal and opposite magnetic moments, which cancel each other out so that the net magnetization is zero. For example, in nickel(II) oxide (NiO), half the nickel atoms have moments aligned in one direction and half in the opposite direction.\n\nIn a ferrimagnet, the two networks of magnetic moments are opposite but unequal, so that cancellation is incomplete and there is a non-zero net magnetization. An example is magnetite (FeO), which contains Fe and Fe ions with different magnetic moments.\n\nA quantum spin liquid (QSL) is a disordered state in a system of interacting quantum spins which preserves its disorder to very low temperatures, unlike other disordered states. It is not a liquid in physical sense, but a solid whose magnetic order is inherently disordered. The name \"liquid\" is due to an analogy with the molecular disorder in a conventional liquid. A QSL is neither a ferromagnet, where magnetic domains are parallel, nor an antiferromagnet, where the magnetic domains are antiparallel; instead, the magnetic domains are randomly oriented. This can be realized e.g. by geometrically frustrated magnetic moments that cannot point uniformly parallel or antiparallel. When cooling down and settling to a state, the domain must \"choose\" an orientation, but if the possible states are similar in energy, one will be chosen randomly. Consequently, despite strong short-range order, there is no long-range magnetic order.\n\n Copolymers can undergo microphase separation to form a diverse array of periodic nanostructures, as shown in the example of the styrene-butadiene-styrene block copolymer shown at right. Microphase separation can be understood by analogy to the phase separation between oil and water. Due to chemical incompatibility between the blocks, block copolymers undergo a similar phase separation. However, because the blocks are covalently bonded to each other, they cannot demix macroscopically as water and oil can, and so instead the blocks form nanometer-sized structures. Depending on the relative lengths of each block and the overall block topology of the polymer, many morphologies can be obtained, each its own phase of matter.\n\nIonic liquids also display microphase separation. The anion and cation are not necessarily compatible and would demix otherwise, but electric charge attraction prevents them from separating. Their anions and cations appear to diffuse within compartmentalized layers or micelles instead of freely as in a uniform liquid.\n\nClose to absolute zero, some liquids form a second liquid state described as superfluid because it has zero viscosity (or infinite fluidity; i.e., flowing without friction). This was discovered in 1937 for helium, which forms a superfluid below the lambda temperature of 2.17 K. In this state it will attempt to \"climb\" out of its container. It also has infinite thermal conductivity so that no temperature gradient can form in a superfluid. Placing a superfluid in a spinning container will result in quantized vortices.\n\nThese properties are explained by the theory that the common isotope helium-4 forms a Bose–Einstein condensate (see next section) in the superfluid state. More recently, Fermionic condensate superfluids have been formed at even lower temperatures by the rare isotope helium-3 and by lithium-6.\n\nIn 1924, Albert Einstein and Satyendra Nath Bose predicted the \"Bose–Einstein condensate\" (BEC), sometimes referred to as the fifth state of matter. In a BEC, matter stops behaving as independent particles, and collapses into a single quantum state that can be described with a single, uniform wavefunction.\n\nIn the gas phase, the Bose–Einstein condensate remained an unverified theoretical prediction for many years. In 1995, the research groups of Eric Cornell and Carl Wieman, of JILA at the University of Colorado at Boulder, produced the first such condensate experimentally. A Bose–Einstein condensate is \"colder\" than a solid. It may occur when atoms have very similar (or the same) quantum levels, at temperatures very close to absolute zero (−273.15 °C).\n\nA \"fermionic condensate\" is similar to the Bose–Einstein condensate but composed of fermions. The Pauli exclusion principle prevents fermions from entering the same quantum state, but a pair of fermions can behave as a boson, and multiple such pairs can then enter the same quantum state without restriction.\n\nOne of the metastable states of strongly non-ideal plasma is Rydberg matter, which forms upon condensation of excited atoms. These atoms can also turn into ions and electrons if they reach a certain temperature. In April 2009, \"Nature\" reported the creation of Rydberg molecules from a Rydberg atom and a ground state atom, confirming that such a state of matter could exist. The experiment was performed using ultracold rubidium atoms.\n\nA \"quantum Hall state\" gives rise to quantized Hall voltage measured in the direction perpendicular to the current flow. A \"quantum spin Hall state\" is a theoretical phase that may pave the way for the development of electronic devices that dissipate less energy and generate less heat. This is a derivation of the Quantum Hall state of matter.\n\nPhotonic matter is a phenomenon where photons interacting with a gas develop apparent mass, and can interact with each other, even forming photonic \"molecules\". The source of mass is the gas, which is massive. This is in contrast to photons moving in empty space, which have no rest mass, and cannot interact.\n\nA \"quantum fog\" of electrons and holes that flow around each other and even ripple like a liquid, rather than existing as discrete pairs.\n\nUnder extremely high pressure, as in the cores of dead stars, ordinary matter undergoes a transition to a series of exotic states of matter collectively known as degenerate matter, which are supported mainly by quantum mechanical effects. In physics, \"degenerate\" refers to two states that have the same energy and are thus interchangeable. Degenerate matter is supported by the Pauli exclusion principle, which prevents two fermionic particles from occupying the same quantum state. Unlike regular plasma, degenerate plasma expands little when heated, because there are simply no momentum states left. Consequently, degenerate stars collapse into very high densities. More massive degenerate stars are smaller, because the gravitational force increases, but pressure does not increase proportionally.\n\nElectron-degenerate matter is found inside white dwarf stars. Electrons remain bound to atoms but are able to transfer to adjacent atoms. Neutron-degenerate matter is found in neutron stars. Vast gravitational pressure compresses atoms so strongly that the electrons are forced to combine with protons via inverse beta-decay, resulting in a superdense conglomeration of neutrons. Normally free neutrons outside an atomic nucleus will decay with a half life of just under 15 minutes, but in a neutron star, the decay is overtaken by inverse decay. Cold degenerate matter is also present in planets such as Jupiter and in the even more massive brown dwarfs, which are expected to have a core with metallic hydrogen. Because of the degeneracy, more massive brown dwarfs are not significantly larger. In metals, the electrons can be modeled as a degenerate gas moving in a lattice of non-degenerate positive ions.\n\nIn regular cold matter, quarks, fundamental particles of nuclear matter, are confined by the strong force into hadrons that consist of 2–4 quarks, such as protons and neutrons. Quark matter or quantum chromodynanamical (QCD) matter is a group of phases where the strong force is overcome and quarks are deconfined and free to move. Quark matter phases occur at extremely high densities or temperatures, and there are no known ways to produce them in equilibrium in the laboratory; in ordinary conditions, any quark matter formed immediately undergoes radioactive decay.\n\nStrange matter is a type of quark matter that is suspected to exist inside some neutron stars close to the Tolman–Oppenheimer–Volkoff limit (approximately 2–3 solar masses), although there is no direct evidence of its existence. In strange matter, part of the energy available manifests as strange quarks, a heavier analogue of the common down quark. It may be stable at lower energy states once formed, although this is not known.\n\nQuark–gluon plasma is a very high-temperature phase in which quarks become free and able to move independently, rather than being perpetually bound into particles, in a sea of gluons, subatomic particles that transmit the strong force that binds quarks together. This is analogous to the liberation of electrons from atoms in a plasma. This state is briefly attainable in extremely high-energy heavy ion collisions in particle accelerators, and allows scientists to observe the properties of individual quarks, and not just theorize. Quark–gluon plasma was discovered at CERN in 2000. Unlike plasma, which flows like a gas, interactions within QGP are strong and it flows like a liquid.\n\nAt high densities but relatively low temperatures, quarks are theorized to form a quark liquid whose nature is presently unknown. It forms a distinct color-flavor locked (CFL) phase at even higher densities. This phase is superconductive for color charge. These phases may occur in neutron stars but they are presently theoretical.\n\nColor-glass condensate is a type of matter theorized to exist in atomic nuclei traveling near the speed of light. According to Einstein's theory of relativity, a high-energy nucleus appears length contracted, or compressed, along its direction of motion. As a result, the gluons inside the nucleus appear to a stationary observer as a \"gluonic wall\" traveling near the speed of light. At very high energies, the density of the gluons in this wall is seen to increase greatly. Unlike the quark–gluon plasma produced in the collision of such walls, the color-glass condensate describes the walls themselves, and is an intrinsic property of the particles that can only be observed under high-energy conditions such as those at RHIC and possibly at the Large Hadron Collider as well.\n\nVarious theories predict new states of matter at very high energies. An unknown state has created the baryon asymmetry in the universe, but little is known about it. In string theory, a Hagedorn temperature is predicted for superstrings at about 10 K, where superstrings are copiously produced. At Planck temperature (10 K), gravity becomes a significant force between individual particles. No current theory can describe these states and they cannot be produced with any foreseeable experiment. However, these states are important in cosmology because the universe may have passed through these states in the Big Bang.\n\nThe gravitational singularity predicted by general relativity to exist at the center of a black hole is \"not\" a phase of matter; it is not a material object at all (although the mass-energy of matter contributed to its creation) but rather a property of spacetime at a location. It could be argued, of course, that all particles are properties of spacetime at a location, leaving a half-note of controversy on the subject.\n\nA supersolid is a spatially ordered material (that is, a solid or crystal) with superfluid properties. Similar to a superfluid, a supersolid is able to move without friction but retains a rigid shape. Although a supersolid is a solid, it exhibits so many characteristic properties different from other solids that many argue it is another state of matter.\n\nIn a string-net liquid, atoms have apparently unstable arrangement, like a liquid, but are still consistent in overall pattern, like a solid. When in a normal solid state, the atoms of matter align themselves in a grid pattern, so that the spin of any electron is the opposite of the spin of all electrons touching it. But in a string-net liquid, atoms are arranged in some pattern that requires some electrons to have neighbors with the same spin. This gives rise to curious properties, as well as supporting some unusual proposals about the fundamental conditions of the universe itself.\n\nA superglass is a phase of matter characterized, at the same time, by superfluidity and a frozen amorphous structure.\n\nWhile dark matter is estimated to comprise 83% of the mass of matter in the universe, most of its properties remain a mystery due to the fact that it neither absorbs nor emits electromagnetic radiation, and there are many competing theories regarding what dark matter is actually made of. Thus, while it is hypothesized to exist and comprise the vast majority of matter in the universe, almost all of its properties are unknown and a matter of speculation, because it has only been observed through its gravitational effects.\n\n"}
{"id": "28158932", "url": "https://en.wikipedia.org/wiki?curid=28158932", "title": "Tag management", "text": "Tag management\n\nTag management is the ability to manage user-generated tags (also known as categories, taxonomies or folksonomies) within collaborative software. Tag management features and processes are put in place to encourage cross-user consistency, navigation efficiency and compliance with an existing taxonomy.\n\nThe tags that users will be able to use can be controlled up-hill (before they have entered the data set) by\n\nTags can be gardened down-hill (after they have entered the data set) by\n\n\n"}
{"id": "9523495", "url": "https://en.wikipedia.org/wiki?curid=9523495", "title": "Three Flags Day", "text": "Three Flags Day\n\nThree Flags Day commemorates March 9 and 10, 1804, when Spain officially completed turning over the Louisiana (New Spain) colonial territory to France, who then officially turned over the same lands to the United States, in order to finalize the 1803 Louisiana Purchase.\n\nThe ceremony in St. Louis cleared the way for Lewis and Clark to begin their exploration.\n\nFrance had ruled Louisiana from its founding until the Treaty of Paris (1763) which ended the Seven Years' War (whose North American phase was the French and Indian War), in which treaty Spain received the French land west of the Mississippi River (the \"right bank\" going downstream) plus New Orleans, and Great Britain received the French lands east of the River (the \"left bank\") -- which included what had previously been called the Illinois Country or \"Upper Louisiana\".\n\nSpain officially took control of its territory in 1769, when it suppressed the Rebellion of 1768 by area residents who had resisted Spain's assumption of colonial authority in the formerly French domain.\n\nThe United States extended its western boundaries to the Mississippi River during the American Revolutionary War, when General George Rogers Clark took possession of the lands east of the Mississippi River which had for some years belonged to Great Britain. American control of the territory which became today's Midwestern states (the former Illinois Country and Ohio Country) was not secure until both the Treaty of Paris (1783) and the Jay Treaty (1794) had been formalized.\n\nOn October 1, 1800, Napoleon Bonaparte concluded France's re-acquisition of \"La Louisiane\" (Spanish: \"Luisiana\") from Spain, in the Third Treaty of San Ildefonso. However, the treaty was kept secret and Spain continued to administer the territory.\n\nThe U.S. and France agreed on April 30, 1803, to the American purchase of Louisiana (which was announced publicly in the United States on July 4). However, the U.S. did not immediately take possession of these lands on the west side of the Mississippi, and Spain continued to administer the territory because it had not yet formally turned it over to France.\n\nAfter the United States' purchase, Thomas Jefferson announced plans for an exploration of the new territory. Spain, however, prohibited any foreign exploration of its territory. Lewis and Clark were to spend the winter of 1803-04 at Camp Dubois in what was then the Indiana Territory, opposite the confluence of the Missouri and Mississippi Rivers. until the lands had been formally turned over to the United States.\n\nOn November 30, 1803, Spain formally transferred the territory in a ceremony at the Cabildo and Plaza de Armas in New Orleans attended by Spanish Governors Juan Manuel de Salcedo and Sebastián Calvo de la Puerta y O'Farrill and new French Governor Pierre Clement de Laussat. \n\nOn December 20, 1803, New Orleans and the rest of Louisiana were transferred to the United States in a ceremony with Laussat and incoming United States governor William C.C. Claiborne, with Gen. James Wilkinson in attendance. However, with navigation on the Mississippi halted because of winter, the news was not conveyed to St. Louis.\n\nOn March 9, 1804, Amos Stoddard, the new U.S. lieutenant governor for District of Louisiana, and Meriwether Lewis arrived in St. Louis by boat and were met by the Spanish lieutenant for Upper Louisiana, Carlos de Hault de Lassus. Hault de Lassus said:\n\nThe Spanish flag was lowered on March 9, and the French flag was hoisted to fly over the city of St. Louis for 24 hours. The French flag, initially supposed to have been lowered at sunset, remained under guard all night.\n\nThe next morning, March 10, 1804, the American flag was hoisted. This event is sometimes referred to as the \"Three Flag Ceremony\" or the \"Ceremony of Three Flags.\"\n\n"}
{"id": "971656", "url": "https://en.wikipedia.org/wiki?curid=971656", "title": "Tpoint", "text": "Tpoint\n\nTPoint is computer software that implements a mathematical model of conditions leading to errors in telescope pointing and tracking. The model can then be used in a telescope control system to correct the pointing and tracking. Such errors are typically caused by mechanical or structural defects. For example, TPoint can analyze and compensate for systematic errors such as polar misalignment, mechanical and optical non-orthogonality, lack of roundness in telescope mounting drive gears, as well as for flexure of the mounting caused by gravity.\n\nTPoint is in use on the majority of professional telescopes worldwide, including the Anglo-Australian Observatory, Keck Observatory, Gemini Observatory, and many others. It has significantly improved the performance and efficiency of telescope operation and has had an especially strong impact on the development of automated and robotic telescopes. \n\nTPoint is also widely used by amateur astronomers. Software Bisque distributes TPoint for Mac OS and Windows as an add-on to TheSkyX Serious Astronomer Edition and TheSkyX Professional; this version is used to improve the pointing on amateur telescopes.\nTPoint was invented and developed by Patrick Wallace. It grew out of work he and John Straede performed\nat the Anglo-Australian Telescope (AAT) between 1974 and 1980\nusing Interdata 70 computers. In the early 1980s, it was ported to the Digital Equipment Corporation VAX running under\nthe VMS operating system and between 1990 and 1992 was also ported to run on the PC/MS-DOS platform as well\nas various UNIX platforms. A TPoint add-on is available for TheSkyX Serious Astronomer Edition and TheSkyX Professional Edition from Software Bisque, and it runs under both Macintosh OS and Microsoft Windows. \n\n"}
{"id": "208772", "url": "https://en.wikipedia.org/wiki?curid=208772", "title": "World Values Survey", "text": "World Values Survey\n\nThe World Values Survey (WVS), a global research project, explores people's values and beliefs, how they change over time and what social and political impact they have. Since 1981 a worldwide network of social scientists have conducted representative national surveys as part of WVS in almost 100 countries.\n\nThe WVS measures, monitors and analyzes: support for democracy, tolerance of foreigners and ethnic minorities, support for gender equality, the role of religion and changing levels of religiosity, the impact of globalization, attitudes toward the environment, work, family, politics, national identity, culture, diversity, insecurity, and subjective well-being.\n\nThe findings provide information for policy makers seeking to build civil society and democratic institutions in developing countries. The work is also frequently used by governments around the world, scholars, students, journalists and international organizations and institutions such as the World Bank and the United Nations (UNDP and UN-Habitat). Data from the World Values Survey have (for example) been used to better understand the motivations behind events such as the Arab Spring, the 2005 French civil unrest, the Rwandan genocide in 1994 and the Yugoslav wars and political upheaval in the 1990s. \n\nRomano Prodi, former Prime Minister of Italy and the tenth President of the European Commission, said about WVS work: \n\nThe growing globalization of the world makes it increasingly important to understand ... diversity. People with varying beliefs and values can live together and work together productively, but for this to happen it is crucial to understand and appreciate their distinctive worldviews.\nThe WVS has over the years demonstrated that people's beliefs play a key role in economic development, the emergence and flourishing of democratic institutions, the rise of gender equality, and the extent to which societies have effective government.\n\nAnalysis of WVS data made by political scientists Ronald Inglehart and Christian Welzel asserts that there are two major dimensions of cross cultural variation in the world: \nThe global cultural map (below) shows how scores of societies are located on these two dimensions. Moving upward on this map reflects the shift from Traditional values to Secular-rational and moving rightward reflects the shift from Survival values to Self–expression values.\n\n\"Traditional values\" emphasize the importance of religion, parent-child ties, deference to authority and traditional family values. People who embrace these values also reject divorce, abortion, euthanasia and suicide. These societies have high levels of national pride and a nationalistic outlook.\n\n\"Secular-rational values\" have the opposite preferences to the traditional values. These societies place less emphasis on religion, traditional family values and authority. Divorce, abortion, euthanasia and suicide are seen as relatively acceptable.\n\n\"Survival values\" place emphasis on economic and physical security. It is linked with a relatively ethnocentric outlook and low levels of trust and tolerance.\n\n\"Self-expression values\" give high priority to environmental protection, growing tolerance of foreigners, gays and lesbians and gender equality, and rising demands for participation in decision-making in economic and political life.\n\nA somewhat simplified analysis is that following an increase in standards of living, and a transit from development country via industrialization to post-industrial knowledge society, a country tends to move diagonally in the direction from lower-left corner (poor) to upper-right corner (rich), indicating a transit in both dimensions.\n\nHowever, the attitudes among the population are also highly correlated with the philosophical, political and religious ideas that have been dominating in the country. Secular-rational values and materialism were formulated by philosophers and the left-wing politics side in the French revolution, and can consequently be observed especially in countries with a long history of social democratic or socialistic policy, and in countries where a large portion of the population have studied philosophy and science at universities. Survival values are characteristic for eastern-world countries and self-expression values for western-world countries. In a liberal post-industrial economy, an increasing share of the population has grown up taking survival and freedom of thought for granted, resulting in that self-expression is highly valued.\n\n\nFindings from the WVS indicate that support for gender equality is not just a consequence of democratization. It is part of a broader cultural change that is transforming industrialized societies with mass demands for increasingly democratic institutions. Although a majority of the world's population still believes that men make better political leaders than women, this view is fading in advanced industrialized societies, and also among young people in less prosperous countries.\n\nThe data from the World Values Survey cover several important aspects of people's religious orientation. One of them tracks how involved people are in religious services and how much importance they attach to their religious beliefs. In the data from 2000, 98% of the public in Indonesia said that religion was very important in their lives while in China only three percent considered religion very important. Another aspect concerns people's attitudes towards the relation between religion and politics and whether they approve of religious spokesmen who try to influence government decisions and people's voting preferences.\n\nIn a factor analysis of the latest wave (6) of World Values Survey data, Arno Tausch (Corvinus University Budapest) found that family values in the tradition of Joseph Alois Schumpeter and religious values in the research tradition of Robert Barro can be an important positive asset for society. Negative phenomena, like the distrust in the state of law; the shadow economy; the distance from altruistic values; a growing fatigue of democracy; and the lack of entrepreneurial spirit are all correlated with the loss of religiosity. Tausch based his results on a factor analysis with promax rotation of 78 variables from 45 countries with complete data, and also calculated performance indices for the 45 countries with complete data and the nine main global religious denominations. On this account, Judaism and also Protestantism emerge as most closely combining religion and the traditions of the Enlightenment.\n\nThe WVS has shown that from 1981 to 2007 happiness rose in 45 of the 52 countries for which long-term data are available. Since 1981, economic development, democratization, and rising social tolerance have increased the extent to which people perceive that they have free choice, which in turn has led to higher levels of happiness around the world, which supports the human development theory.\n\nSome of the survey's basic findings are:\n\nThe World Values Surveys were designed to test the hypothesis that economic and technological changes are transforming the basic values and motivations of the publics of industrialized societies. The surveys build on the European Values Study (EVS) first carried out in 1981. The EVS was conducted under the aegis of Jan Kerkhofs and Ruud de Moor and continues to be based in the Netherlands at the Tilburg University. The 1981 study was largely limited to developed societies, but interest in this project spread so widely that surveys were carried out in more than twenty countries, located on all six inhabited continents. Ronald Inglehart of the University of Michigan played a leading role in extending these surveys to be carried out in countries around the world. Today the network includes hundreds of social scientist from more than 100 countries.\n\nFindings from the first wave of surveys pointed to the conclusion that intergenerational changes were taking place in basic values relating to politics, economic life, religion, gender roles, family norms and sexual norms. The values of younger generations differed consistently from those prevailing among older generations, particularly in societies that had experienced rapid economic growth. To examine whether changes were actually taking place in these values and to analyze the underlying causes, a second wave of WVS surveys was carried out in 1990–91. Because these changes seem to be linked with economic and technological development, it was important to include societies across the entire range of development, from low income societies to rich societies.\n\nA third wave of surveys was carried out in 1995–97, this time in 55 societies and with increased attention being given to analysing the cultural conditions for democracy. A fourth wave of surveys was carried out in 1999–2001 in 65 societies. A key goal was to obtain better coverage of African and Islamic societies, which had been under-represented in previous surveys. A fifth wave was carried out in 2005–07 and a sixth wave was carried out during 2011–12.\n\nDue to the European origin of the project, the early waves of the WVS were eurocentric in emphasis, with little representation in Africa and South-East Asia. To expand, the WVS adopted a decentralised structure, in which social scientists from countries throughout the world participated in the design, execution and analysis of the data, and in publication of findings. In return for providing the data from a survey in their own society, each group obtained immediate access to the data from all participating societies enabling them to analyse social change in a broader perspective.\n\nThe WVS network has produced over 300 publications in 20 languages and secondary users have produced several thousand additional publications. The database of the WVS has been published on the internet with free access.\n\nThe official archive of the World Values Survey is located in [ASEP/JDS] Madrid, Spain.\n\nThe World Values Survey uses the sample survey as its mode of data collection, a systematic and standardized approach to collect information through interviewing representative national samples of individuals. The basic stages of a sample survey are Questionnaire design; Sampling; Data collection and Analysis.\n\nFor each wave, suggestions for questions are solicited by social scientists from all over the world and a final master questionnaire is developed in English. Since the start in 1981 each successive wave has covered a broader range of societies than the previous one. Analysis of the data from each wave has indicated that certain questions tapped interesting and important concepts while others were of little value. This has led to the more useful questions or themes being replicated in future waves while the less useful ones have been dropped making room for new questions.\n\nThe questionnaire is translated into the various national languages and in many cases independently translated back to English to check the accuracy of the translation. In most countries, the translated questionnaire is pre-tested to help identify questions for which the translation is problematic. In some cases certain problematic questions are omitted from the national questionnaire.\n\nSamples are drawn from the entire population of 18 years and older. The minimum sample is 1000. In most countries, no upper age limit is imposed and some form of stratified random sampling is used to obtain representative national samples. In the first stages, a random selection of sampling points is made based on the given society statistical regions, districts, census units, election sections, electoral registers or polling place and central population registers. In most countries the population size and/or degree of urbanization of these Primary Sampling Units are taken into account. In some countries, individuals are drawn from national registers.\n\nFollowing the sampling, each country is left with a representative national sample of its public. These persons are then interviewed during a limited time frame decided by the Executive Committee of the World Values Survey using the uniformly structured questionnaires. The survey is carried out by professional organizations using face-to-face interviews or phone interviews for remote areas. Each country has a Principal Investigator (social scientists working in academic institutions) who is responsible for conducting the survey in accordance with the fixed rules and procedures. During the field work, the agency has to report in writing according to a specific check-list. Internal consistency checks are made between the sampling design and the outcome and rigorous data cleaning procedures are followed at the WVS data archive. No country is included in a wave before full documentation has been delivered. This means a data set with the completed methodological questionnaire. and a report of country-specific information (for example important political events during the fieldwork, problems particular to the country). Once all the surveys are completed, the Principal Investigator has access to all surveys and data.\n\nThe World Values Survey group works with leading social scientists, recruited from each society studied. They represent a wide range of cultures and perspectives which makes it possible to draw on the insights of well-informed insiders in interpreting the findings. It also helps disseminate social science techniques to new countries.\n\nEach research team, that has contributed to the survey, analyses the findings according to its hypotheses. Because all researchers obtain data from all of the participating societies, they are also able to compare the values and beliefs of the people of their own society with those from scores of other societies and to test alternative hypotheses. In addition, the participants are invited to international meetings at which they can compare findings and interpretations with other members of the WVS network. The findings are then disseminated through international conferences and joint publications.\n\nThe World Values Survey data has been downloaded by over 100,000 researchers, journalists, policy-makers and others. The data is available on the WVS website which contains tools developed for online analysis.\n\nThe World Values Survey is organised as a network of social scientists coordinated by a central body - the World Values Survey Association. It is established as a non-profit organization seated in Stockholm, Sweden, with a constitution and mission statement. The project is guided by an Executive Committee representing all regions of the world. The Committee is also supported by a Scientific Advisory Committee, a Secretariat and an Archive. The WVS Executive Committee provides leadership and strategic planning for the association. It is responsible for the recruitment of new members, the organization of meetings and workshops, data processing and distribution, capacity building and the promotion of publications and dissemination of results. The WVS Executive Committee also raises funds for central functions and assists member groups in their fundraising.\n\nEach national team is responsible for its own expenses and most surveys are financed by local sources. However, central funding has been obtained in cases where local funding is not possible. Presently, the activities of the WVS Secretariat and WVS Executive Committee are funded by the Bank of Sweden Tercentenary Foundation. Other funding has been obtained from the U.S. National Science Foundation, the Swedish International Development Cooperation Agency (SIDA), the Volkswagen Foundation, the German Science Foundation (DFG) and the Dutch Ministry of Education, Culture and Science.\n\nThe World Values Survey data has been used in a large number of scholarly publications and the findings have been reported in media such as BBC News, Bloomberg Businessweek, China Daily, Chinadialogue.net, CNN, Der Spiegel, Der Standard, Discover, Rzeczpospolita, Gazeta Wyborcza, Le Monde, Neue Zürcher Zeitung, Newsweek, Russia Today, Süddeutsche Zeitung, Time, The Economist, The Guardian, The New Yorker, The New York Times, The Sydney Morning Herald, The Washington Post, and the World Development Report.\n\nWorld Values Research (WVR), registered as , is the official online paper series of the World Values Survey Association. The series is edited by the Executive Committee of the Association. WVR publishes research papers of high scientific standards based on evidence from World Values Surveys data. Papers in WVR follow good academic practice and abide to ethical norms in line with the mission of the World Values Survey Association. Publication of submitted papers is pending on an internal review by the Executive Committee of the World Values Survey Association. WVR papers present original research based on data from the World Values Surveys, providing new evidence and novel insights of theoretical relevance to the theme of human values. An archive of published WVR papers is available on the project's website.\n\nSocial science surveys\n\nIndices\n\nOther\n\n\n"}
