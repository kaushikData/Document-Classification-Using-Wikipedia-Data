{"id": "37965697", "url": "https://en.wikipedia.org/wiki?curid=37965697", "title": "AURIGA", "text": "AURIGA\n\nAURIGA (\"Antenna Ultracriogenica Risonante per l'Indagine Gravitazionale Astronomica\") is an ultracryogenic resonant bar gravitational wave detector in Italy. It is at the Laboratori Nazionali di Legnaro of the Istituto Nazionale di Fisica Nucleare, near Padova. It is being used for research into gravitational waves and quantum gravity.\n\nWhen the oscillator gets hit with a burst of gravitational waves, it will excite the oscillator and it will vibrate for a time span longer than the duration of the gravitational wave burst. This allows for the extraction of the signal from the detector.\n\n"}
{"id": "5003360", "url": "https://en.wikipedia.org/wiki?curid=5003360", "title": "Analogical models", "text": "Analogical models\n\nAnalogical models are a method of representing a phenomenon of the world, often called the \"target system\" by another, more understandable or analysable system. They are also called dynamical analogies.\n\nTwo open systems have \"analog\" representations (see illustration) if they are black box isomorphic systems.\n\nAnalogizing is the process of representing information about a particular subject (the analogue or source system) by another particular subject (the target system). A simple type of analogy is one that is based on shared properties (Stanford Encyclopedia of Philosophy). Analogical models, also called \"analog\" or \"analogue\" models, therefore seek the analog systems that share properties with the target system as a means of representing the world. It is often practicable to construct source systems that are smaller and/or faster than the target system so that one can deduce \"a priori\" knowledge of target system behaviour. Analog devices are therefore those in which may differ in substance or structure but share properties of dynamic behaviour (Truit and Rogers, p. 1-3).\n\nFor example, in analog electronic circuits, one can use voltage to represent an arithmetic quantity; operational amplifiers might then represent the arithmetic operations (addition, subtraction, multiplication, and division).Through the process of calibration these smaller/bigger, slower/faster systems are scaled up or down so that they match the functioning of the target system, and are therefore called analogs of the target system. Once the calibration has taken place, modellers speak of a \"one-to-one correspondence in behaviour\" between the primary system and its analog. Thus the behaviour of two systems can be determined by experimenting with one.\n\nMany different instruments and systems can be used to create an analogical model. A mechanical device can be used to represent mathematical calculations. For instance, the Phillips Hydraulic Computer MONIAC used the flow of water to model economic systems (the target system); electronic circuits can be used to represent both physiological and ecological systems. When a model is run on either an analog or digital computer this is known as the process of simulation.\n\nAny number of systems could be used for mapping electrical phenomena to mechanical phenomena, but two principle systems are commonly used: the impedance analogy and the mobility analogy. The impedance analogy maps force to voltage whereas the mobility analogy maps force to current.\n\nThe impedance analogy preserves the analogy between electrical impedance and mechanical impedance but does not preserve the network topology. The mobility analogy preserves the network topology but does not preserve the analogy between impedances. Both preserve the correct energy and power relationships by making power conjugate pairs of variables analogous.\n\n\n\n\nDynamical analogies establish analogies between systems in different energy domains by means of comparison of the system dynamic equations. There are many ways such analogies can be built, but one of the most useful methods is to form analogies between pairs of power conjugate variables. That is, a pair of variables whose product is power. Doing so preserves the correct energy flow between domains, a useful feature when modelling a system as an integrated whole. Examples of systems that require unified modelling are mechatronics and audio electronics.\n\nThe earliest such analogy is due to James Clerk Maxwell who, in 1873, associated mechanical force with electrical voltage. This analogy became so widespread that sources of voltage are still today referred to as electromotive force. The power conjugate of voltage is electric current which, in the Maxwell analogy, maps to mechanical velocity. Electrical impedance is the ratio of voltage and current, so by analogy, mechanical impedance is the ratio of force and velocity. The concept of impedance can be extended to other domains, for instance in acoustics and fluid flow it is the ratio of pressure to rate of flow. In general, impedance is the ratio of an \"effort\" variable and the \"flow\" variable that results. For this reason, the Maxwell analogy is often referred to as the impedance analogy, although the concept of impedance was not conceived until 1886 by Oliver Heaviside, some time after Maxwell's death.\n\nSpecifying power conjugate variables still does not result in a unique analogy, there are multiple ways the conjugates and analogies can be specified. A new analogy was proposed by Floyd A. Firestone in 1933 now known as the mobility analogy. In this analogy electrical impedance is made analogous to mechanical mobility (the inverse of mechanical impedance). Firestone's idea was to make analogous variables that are measured across an element, and make analogous variables that flow through an element. For instance, the \"across\" variable voltage is the analogy of velocity, and the \"through\" variable current is the analogy of force. Firestone's analogy has the advantage of preserving the topology of element connections when converting between domains. A modified form of the through and across analogy was proposed in 1955 by Horace M. Trent and is the modern understanding of \"through and across\".\n\nThe Hamiltonian variables, also called the energy variables, are those variables which when time differentiated are equal to the power conjugate variables. The Hamiltonian variables are so called because they are the variables which usually appear in Hamiltonian mechanics. The Hamiltonian variables in the electrical domain are charge (\"q\") and flux linkage (λ) because,\n\nIn the translational mechanical domain the Hamiltonian variables are distance displacement (\"x\") and momentum (\"p\") because,\n\nThere is a corresponding relationship for other analogies and sets of variables. The Hamiltonian variables are also called the energy variables. The integrand of a power conjugate variable with respect to a Hamiltonian variable is a measure of energy. For instance,\nare both expressions of energy.\n\nMaxwell's analogy was initially used merely to help explain electrical phenomena in more familiar mechanical terms. The work of Firestone, Trent and others moved the field well beyond this, looking to represent systems of multiple energy domains as a single system. In particular, designers started converting the mechanical parts of an electromechanical system to the electrical domain so that the whole system could be analyzed as an electrical circuit. Vannevar Bush was a pioneer of this kind of modelling in his development of analogue computers, and a coherent presentation of this method was presented in a 1925 paper by Clifford A. Nickle.\n\nFrom the 1950s onward, manufacturers of mechanical filters, notably Collins Radio, widely used these analogies in order to take the well -developed theory of filter design in electrical engineering and apply it to mechanical systems. The quality of filters required for radio applications could not be achieved with electrical components. Much better quality resonators (higher Q factor) could be made with mechanical parts but there was no equivalent filter theory in mechanical engineering. It was also necessary to have the mechanical parts, the transducers, and the electrical components of the circuit analyzed as a complete system in order to predict the overall response of the filter.\n\nHarry F. Olson helped popularise the use of dynamical analogies in the audio electronics field with his book \"dynamical analogies\" first published in 1943.\n\nA common analogy of magnetic circuits maps magnetomotive force (mmf) to voltage and magnetic flux (φ) to electrical current. However, mmf and φ are not power conjugate variables. The product of these is not in units of power and the ratio, known as magnetic reluctance, does not measure the rate of dissipation of energy so is not a true impedance. Where a compatible analogy is required, mmf can be used as the effort variable and \"dφ/dt\" (rate of change of magnetic flux) will then be the flow variable. This is known as the gyrator-capacitor model.\n\nA widely used analogy in the thermal domain maps temperature difference as the effort variable and thermal power as the flow variable. Again, these are not power conjugate variables, and the ratio, known as thermal resistance, is not really an analogy of either impedance or electrical resistance as far as energy flows are concerned. A compatible analogy could take temperature difference as the effort variable and entropy flow rate as the flow variable.\n\nMany applications of dynamical models convert all energy domains in the system into an electrical circuit and then proceed to analyse the complete system in the electrical domain. There are, however, more generalised methods of representation. One such representation is through the use of bond graphs, introduced by Henry M. Paynter in 1960. It is usual to use the force-voltage analogy (impedance analogy) with bond graphs, but it is not a requirement to do so. Likewise Trent used a different representation (linear graphs) and his representation has become associated with the force-current analogy (mobility analogy), but again this is not mandatory.\n\nSome authors discourage the use of domain specific terminology for the sake of generalisation. For instance, because much of the theory of dynamical analogies arose from electrical theory the power conjugate variables are sometimes called \"V-type\" and \"I-type\" according to whether they are analogs of voltage or current respectively in the electrical domain. Likewise, the Hamiltonian variables are sometimes called \"generalised momentum\" and \"generalised displacement\" according to whether they are analogs of momentum or displacement in the mechanical domain.\n\nA fluid or hydraulic analogy of an electric circuit attempts to explain circuitry intuitively in terms of plumbing, where water is analogous to the mobile sea of charge within metals, pressure difference is analogous to voltage, and water's flow rate is analogous to electric current.\n\nElectronic circuits were used to model and simulate engineering systems such as aeroplanes and nuclear power plants before digital computers became widely available with fast enough turn over times to be practically useful. Electronic circuit instruments called analog computers were used to speed up circuit construction time. However analog computers like the Norden bombsight could also consist of gears and pulleys in calculation.\n\nExamples are Vogel and Ewel who published 'An Electrical Analog of a Trophic Pyramid' (1972, Chpt 11, pp. 105–121), Elmore and Sands (1949) who published circuits devised for research in nuclear physics and the study of fast electrical transients done under the Manhattan Project (however no circuits having application to weapon technology were included for security reasons), and Howard T. Odum (1994) who published circuits devised to analogically model ecological-economic systems at many scales of the geobiosphere.\n\nThe process of analogical modelling has philosophical difficulties. As noted in the Stanford Encyclopedia of Philosophy, there is the question of how the physical/biological laws of the target system relate to the analogical models created by humans to represent the target system. We seem to assume that the process of constructing analogical models gives us access to the fundamental laws governing the target system. However strictly speaking we only have empirical knowledge of the laws that hold true for the analogical system, and if the time constant for the target system is larger than the life cycle of human being (as in the case of the geobiosphere) it is therefore very difficult for any single human to empirically verify the validity of the extension of the laws of their model to the target system in their lifetime.\n\n\n\n"}
{"id": "328736", "url": "https://en.wikipedia.org/wiki?curid=328736", "title": "Archaeogenetics", "text": "Archaeogenetics\n\nArchaeogenetics is the study of ancient DNA using various molecular genetic methods and DNA resources. This form of genetic analysis can be applied to human, animal, and plant specimens. Ancient DNA can be extracted from various fossilized specimens including bones, eggshells, and artificially preserved tissues in human and animal specimens. In plants, Ancient DNA can be extracted from seeds, tissue, and in some cases, feces. Archaeogenetics provides us with genetic evidence of ancient population group migrations, domestication events, and plant and animal evolution. The ancient DNA cross referenced with the DNA of relative modern genetic populations allows researchers to run comparison studies that provide a more complete analysis when ancient DNA is compromised.\n\nArchaeogenetics receives its name from the Greek word \"arkhaios\", meaning \"ancient\", and the term \"genetics\", meaning \"the study of heredity\". The term archaeogenetics was conceived by archaeologist Colin Renfrew.\n\nLudwik Hirszfeld was a Polish microbiologist and serologist who was the President of the Blood Group Section of the Second International Congress of Blood Transfusion. He founded blood group inheritance with Erich von Dungern in 1910, and contributed to it greatly throughout his life. He studied ABO blood groups. In one of his studies in 1919, Hirszfeld documented the ABO blood groups and hair color of people at the Macedonian front, leading to his discovery that the hair color and blood type had no correlation. In addition to that he observed that there was a decrease of blood group A from western Europe to India and the opposite for blood group B. He hypothesized that the east-to-west blood group ratio stemmed from two blood groups consisting of mainly A or B mutating from blood group O, and mixing through migration or intermingling. A majority of his work was researching the links of blood types to sex, disease, climate, age, social class, and race. His work led him to discover that peptic ulcer was more dominant in blood group O, and that AB blood type mothers had a high male-to-female birth ratio.\n\nArthur Mourant was a British hematologist and chemist. He received many awards, most notably Fellowship of the Royal Society. His work included organizing the existing data on blood group gene frequencies, and largely contributing to the genetic map of the world through his investigation of blood groups in many populations. Mourant discovered the new blood group antigens of the Lewis, Henshaw, Kell, and Rhesus systems, and analyzed the association of blood groups and various other diseases. He also focused on the biological significance of polymorphisms. His work provided the foundation for archaeogenetics because it facilitated the separation of genetic evidence for biological relationships between people. This genetic evidence was previously used for that purpose. It also provided material that could be used to appraise the theories of population genetics.\n\nWilliam Boyd was an American immunochemist and biochemist who became famous for his research on the genetics of race in the 1950s. During the 1940s, Boyd and Karl O. Renkonen independently discovered that lectins react differently to various blood types, after finding that the crude extracts of the lima bean and tufted vetch agglutinated the red blood cells from blood type A but not blood types B or O. This ultimately led to the disclosure of thousands of plants that contained these proteins. In order to examine racial differences and the distribution and migration patterns of various racial groups, Boyd systematically collected and classified blood samples from around the world, leading to his discovery that blood groups are not influenced by the environment, and are inherited. In his book \"Genetics and the Races of Man\" (1950), Boyd categorized the world population into 13 distinct races, based on their different blood type profiles and his idea that human races are populations with differing alleles. One of the most abundant information sources regarding inheritable traits linked to race remains the study of blood groups.\n\nFossil retrieval starts with selecting an excavation site. Potential excavation sites are usually identified with the mineralogy of the location and visual detection of bones in the area. However, there are more ways to discover excavation zones using technology such as field portable x-ray fluorescence and Dense Stereo Reconstruction. Tools used include knives, brushes, and pointed trowels which assist in the removal of fossils from the earth.\n\nTo avoid contaminating the ancient DNA, specimens are handled with gloves and stored in -20 °C immediately after being unearthed. Ensuring that the fossil sample is analyzed in a lab that has not been used for other DNA analysis could prevent contamination as well. Bones are milled to a powder and treated with a solution before the polymerase chain reaction (PCR) process. Samples for DNA amplification may not necessarily be fossil bones. Preserved skin, salt- preserved or air-dried, can also be used in certain situations.\n\nDNA preservation is difficult because the bone fossilisation degrades and DNA is chemically modified, usually by bacteria and fungi in the soil. The best time to extract DNA from a fossil is when it is freshly out of the ground as it contains six times the DNA when compared to stored bones. The temperature of extraction site also affects the amount of obtainable DNA, evident by a decrease in success rate for DNA amplification if the fossil is found in warmer regions. A drastic change of a fossil's environment also affects DNA preservation. Since excavation causes an abrupt change in the fossil's environment, it may lead to physiochemical change in the DNA molecule. Moreover, DNA preservation is also affected by other factors such as the treatment of the unearthed fossil like (e.g. washing, brushing and sun dring), pH, irradiation, the chemical composition of bone and soil, and hydrology. There are three perseveration diagenetic phases. The first phase is bacterial putrefaction, which is estimated to cause a 15-fold degradation of DNA. Phase 2 is when bone chemically degrades, mostly by depurination. The third diagenetic phase occurs after the fossil is excavated and stored, in which bone DNA degradation occurs most rapidly.\n\nOnce a specimen is collected from an archaeological site, DNA can be extracted through a series of processes. One of the more common methods utilizes silica and takes advantage of polymerase chain reactions in order to collect ancient DNA from bone samples.\n\nThere are several challenges that add to the difficulty when attempting to extract ancient DNA from fossils and prepare it for analysis. DNA is continuously being split up. While the organism is alive these splits are repaired; however, once an organism has died, the DNA will begin to deteriorate without repair. This results in samples having strands of DNA measuring around 100 base pairs in length. Contamination is another significant challenge at multiple steps throughout the process. Often other DNA, such as bacterial DNA, will be present in the original sample. To avoid contamination it is necessary to take many precautions such as separate ventilation systems and workspaces for ancient DNA extraction work. The best samples to use are fresh fossils as uncareful washing can lead to mold growth. DNA coming from fossils also occasionally contains a compound that inhibits DNA replication. Coming to a consensus on which methods are best at mitigating challenges is also difficult due to the lack of repeatability caused by the uniqueness of specimens.\n\nSilica-based DNA extraction is a method used as a purification step to extract DNA from archaeological bone artifacts and yield DNA that can be amplified using polymerase chain reaction (PCR) techniques. This process works by using silica as a means to bind DNA and separate it from other components of the fossil process that inhibit PCR amplification. However, silica itself is also a strong PCR inhibitor, so careful measures must be taken to ensure that silica is removed from the DNA after extraction. The general process for extracting DNA using the silica-based method is outlined by the following:\nOne of the main advantages of silica-based DNA extraction is that it is relatively quick and efficient, requiring only a basic laboratory setup and chemicals. It is also independent of sample size, as the process can be scaled to accommodate larger or smaller quantities. Another benefit is that the process can be executed at room temperature. However, this method does contain some drawbacks. Mainly, silica-based DNA extraction can only be applied to bone and teeth samples; they cannot be used on soft tissue. While they work well with a variety of different fossils, they may be less effective in fossils that are not fresh (e.g. treated fossils for museums). Also, contamination poses a risk for all DNA replication in general, and this method may result in misleading results if applied to contaminated material.\n\nPolymerase chain reaction is a process that can amplify segments of DNA and is often used on extracted ancient DNA. It has three main steps: denaturation, annealing, and extension. Denaturation splits the DNA into two single strands at high temperatures. Annealing involves attaching primer strands of DNA to the single strands that allow Taq polymerase to attach to the DNA. Extension occurs when Taq polymerase is added to the sample and matches base pairs to turn the two single strands into two complete double strands. This process is repeated many times, and is usually repeated a higher number of times when used with ancient DNA. Some issues with PCR is that it requires overlapping primer pairs for ancient DNA due to the short sequences. There can also be “jumping PCR” which causes recombination during the PCR process which can make analyzing the DNA more difficult in inhomogeneous samples.\n\nDNA extracted from fossil remains is primarily sequenced using Massive parallel sequencing, which allows simultaneous amplification and sequencing of all DNA segments in a sample, even when it is highly fragmented and of low concentration. It involves attaching a generic sequence to every single strand that generic primers can bond to, and thus all of the DNA present is amplified. This is generally more costly and time intensive than PCR but due to the difficulties involved in ancient DNA amplification it is cheaper and more efficient. One method of massive parallel sequencing, developed by Margulies et al., employs bead-based emulsion PCR and pyrosequencing, and was found to be powerful in analyses of aDNA because it avoids potential loss of sample, substrate competition for templates, and error propagation in replication.\n\nThe most common way to analyze aDNA sequence is to compare it with a known sequence from other sources, and this could be done in different ways for different purposes.\n\nThe identity of the fossil remain can be uncovered by comparing its DNA sequence with those of known species using software such as BLASTN. This archaeogenetic approach is especially helpful when the morphology of the fossil is ambiguous. Apart from that, species identification can also be done by finding specific genetic markers in an aDNA sequence. For example, the American indigenous population is characterized by specific mitochondrial RFLPs and deletions defined by Wallace et al.\n\naDNA comparison study can also reveal the evolutionary relationship between two species. The number of base differences between DNA of an ancient species and that of a closely related extant species can be used to estimate the divergence time of those two species from their last common ancestor. The phylogeny of some extinct species, such as Australian marsupial wolves and American ground sloths, has been constructed by this method. Mitochondrial DNA in animals and chloroplast DNA in plants are usually used for this purpose because they have hundreds of copies per cell and thus are more easily accessible in ancient fossils.\n\nAnother method to investigate relationship between two species is through DNA hybridization. Single-stranded DNA segments of both species are allowed to form complementary pair bonding with each other. More closely related species have a more similar genetic makeup, and thus a stronger hybridization signal. Scholz et al. conducted southern blot hybridization on Neanderthal aDNA (extracted from fossil remain W-NW and Krapina). The results showed weak ancient human-Neanderthal hybridization and strong ancient human-modern human hybridization. The human-chimpanzee and neanderthal-chimpanzee hybridization are of similarly weak strength. This suggests that humans and neanderthals are not as closely related as two individuals of the same species are, but they are more related to each other than to chimpanzees.\n\nThere have also been some attempts to decipher aDNA to provide valuable phenotypic information of ancient species. This is always done by mapping aDNA sequence onto the karyotype of a well-studied closely related species, which share a lot of similar phenotypic traits. For example, Green et al. compared the aDNA sequence from Neanderthal Vi-80 fossil with modern human X and Y chromosome sequence, and they found a similarity in 2.18 and 1.62 bases per 10,000 respectively, suggesting Vi-80 sample was from a male individual. Other similar studies include finding of a mutation associated with dwarfism in \"Arabidopsis\" in ancient Nubian cotton, and investigation on the bitter taste perception locus in Neanderthals.\n\nModern humans arose in Africa approximately 200 kya (thousand years ago). Examination of mitochondrial DNA (mtDNA), Y-chromosome DNA, and X-chromosome DNA indicate that the earliest population to leave Africa consisted of approximately 1500 males and females. It has been suggested by various studies that populations were geographically “structured” to some degree prior to the expansion out of Africa; this is suggested by the antiquity of shared mtDNA lineages. One study of 121 populations from various places throughout the continent found 14 genetic and linguistic “clusters,” suggesting an ancient geographic structure to African populations. In general, genotypic and phenotypic analysis have shown “large and subdivided throughout much of their evolutionary history.”\n\nGenetic analysis has supported archaeological hypotheses of a large-scale migrations of Bantu speakers into Southern Africa approximately 5 kya. Microsatellite DNA, single nucleotide polymorphisms (SNPs), and insertion/deletion polymorphisms (INDELS) have shown that Nilo-Saharan speaking populations originate from Sudan. Furthermore, there is genetic evidence that Chad-speaking descendents of Nilo-Saharan speakers migrated from Sudan to Lake Chad about 8 kya. Genetic evidence has also indicated that non-African populations made significant contributions to the African gene pool. For example, the Saharan African Beja people have high levels of Middle-Eastern as well as East African Cushitic DNA.\n\nAnalysis of mtDNA shows that Eurasia was occupied in a single migratory event between 60 and 70 kya. Genetic evidence shows that occupation of the Near East and Europe happened no earlier than 50 kya. Studying haplogroup U has shown separate dispersals from the Near East both into Europe and into North Africa.\n\nMuch of the work done in archaeogenetics focuses on the Neolithic transition in Europe. Cavalli-Svorza’s analysis of genetic-geographic patterns led him to conclude that there was a massive influx of Near Eastern populations into Europe at the start of the Neolithic. This view led him “to strongly emphasize the expanding early farmers at the expense of the indigenous Mesolithic foraging populations.” mtDNA analysis in the 1990s, however, contradicted this view. M.B. Richards estimated that merely 10-22% of extant European mtDNA’s had come from Near Eastern populations during the Neolithic. Most mtDNA’s were “already established” among existing Mesolithic and Paleolithic groups. Most “control-region lineages” of modern European mtDNA are traced to a founder event of reoccupying northern Europe towards the end of the Last Glacial Maximum (LGM). One study of extant European mtDNA’s suggest this reoccupation occurred after the end of the LGM, although another suggests it occurred before. Analysis of haplogroups V, H, and U5 support a “pioneer colonization” model of European occupation, with incorporation of foraging populations into arriving Neolithic populations. Furthermore, analysis of ancient DNA, not just extant DNA, is shedding light on some issues. For instance, comparison of neolithic and mesolithic DNA has indicated that the development of dairying preceded widespread lactose tolerance.\n\nSouth Asia has served as the major early corridor for geographical dispersal of modern humans from out-of-Africa. Studies of mtDNA line M suggest that the first occupants of India were Austro-Asiatic speakers who entered about 45-60 kya. The Indian gene pool has contributions from earliest settlers, as well as West Asian and Central Asian populations from migrations no earlier than 8 kya. The lack of variation in mtDNA lineages compared to the Y-chromosome lineages indicate that primarily males partook in these migrations. The discovery of two subbranches U2i and U2e of the U mtDNA lineage, which arose in Central Asia has “modulated” views of a large migration from Central Asia into India, as the two branches diverged 50 kya. Furthermore, U2e is found in large percentages in Europe but not India, and vice versa for U2i, implying U2i is native to India.\n\nAnalysis of mtDNA and NRY (non-recombining region of Y chromosome) sequences have indicated that the first major dispersal out of Africa went through Saudi Arabia and the Indian coast 50-100 kya, and a second major dispersal occurred 15-50 kya north of the Himalayas.\n\nMuch work has been done to discover the extent of north-to-south and south-to-north migrations within Eastern Asia. Comparing the genetic diversity of northeastern groups with southeastern groups has allowed archaeologists to conclude many of the northeast Asian groups came from the southeast. The Pan-Asian SNP (single nucleotide polymorphism) study found “a strong and highly significant correlation between haplotype diversity and latitude,” which, when coupled with demographic analysis, supports the case for a primarily south-to-north occupation of East Asia. Archaeogenetics has also been used to study hunter-gatherer populations in the region, such as the Ainu from Japan and Negrito groups in the Philippines. For example, the Pan-Asian SNP study found that Negrito populations in the Philippines and the Negrito populations in the Philippines were more closely related to non-Negrito local populations than to each other, suggesting Negrito and non-Negrito populations are linked by one entry event into East Asia.\n\nArchaeogenetics has been used to better understand the populating of the Americas from Asia. Native American mtDNA haplogroups have been estimated to be between 15 and 20 kya, although there is some variation in these estimates. Genetic data has been used to propose various theories regarding how the Americas were colonized. Although the most widely held theory suggests “three waves” of migration after the LGM through the Bering Strait, genetic data have given rise to alternative hypotheses. For example, one hypothesis proposes a migration from Siberia to South America 20-15 kya and a second migration that occurred after glacial recession. Y-chromosome data has led some to hold that there was a single migration starting from the Aldai Mountains of Siberia between 17.2- 10.1 kya, after the LGM. Analysis of both mtDNA and Y-chromosome DNA reveals evidence of “small, founding populations.” Studying haplogroups has led some scientists to conclude that a southern migration into the Americas from one small population was impossible, although separate analysis has found that such a model is feasible if such a migration happened along the coasts.\n\nFinally, archaeogenetics has been used to study the occupation of Australia and New Guinea. The aborigines of Australia and New Guinea are phenotypically very similar, but mtDNA has shown that this is due to convergence from living in similar conditions. Non-coding regions of mt-DNA have shown “no similarities” between the aboriginal populations of Australia and New Guinea. Furthermore, no major NRY lineages are shared between the two populations. The high frequency of a single NRY lineage unique to Australia coupled with “low diversity of lineage-associated Y-chromosomal short tandem repeat (Y-STR) haplotypes” provide evidence for a “recent founder or bottleneck” event in Australia. But there is relatively large variation in mtDNA, which would imply that the bottleneck effect impacted males primarily. Together, NRY and mtDNA studies show that the splitting event between the two groups was over 50kya, casting doubt on recent common ancestry between the two.\n\nArchaeogentics has been used to understand the development of domestication of plants and animals.\n\nThe combination of genetics and archeological findings have been used to trace the earliest signs of plant domestication around the world. However, since the nuclear, mitochondrial, and chloroplast genomes used to trace domestication’s moment of origin have evolved at different rates, its use to trace genealogy have been somewhat problematic. Nuclear DNA in specific is used over mitochondrial and chloroplast DNA because of its faster mutation rate as well as its intraspecific variation due to a higher consistency of polymorphism genetic markers. Findings in crop ‘domestication genes’ (traits that were specifically selected for or against) include\nThrough the study of archaeogenetics in plant domestication, signs of the first global economy can also be uncovered. The geographical distribution of new crops highly selected in one region found in another where it would have not originally been introduced serve as evidence of a trading network for the production and consumption of readily available resources.\n\nArchaeogenetics has been used to study the domestication of animals. By analyzing genetic diversity in domesticated animal populations researchers can search for genetic markers in DNA to give valuable insight about possible traits of progenitor species. These traits are then used to help distinguish archaeological remains between wild and domesticated specimens. The genetic studies can also lead to the identification of ancestors for domesticated animals. The information gained from genetics studies on current populations helps guide the Archaeologist’s search for documenting these ancestors.\n\nArchaeogenetics has been used to trace the domestication of pigs throughout the old world. These studies also reveal evidence about the details of early farmers. Methods of Archaeogenetics have also been used to further understand the development of domestication of dogs. Genetic studies have shown that all dogs are descendants from the gray wolf, however, it is currently unknown when, where, and how many times dogs were domesticated. Some genetic studies have indicated multiple domestications while others have not. Archaeological findings help better understand this complicated past by providing solid evidence about the progression of the domestication of dogs. As early humans domesticated dogs the archaeological remains of buried dogs became increasingly more abundant. Not only does this provide more opportunities for archaeologists to study the remains, it also provides clues about early human culture.\n\n\n\n\n"}
{"id": "6902178", "url": "https://en.wikipedia.org/wiki?curid=6902178", "title": "Arcturus moving group", "text": "Arcturus moving group\n\nIn astronomy, the Arcturus moving group or Arcturus stream is a moving group or stellar stream which includes the nearby bright star Arcturus. It comprises many stars which share similar proper motion and so appear to be physically associated.\n\nThis group of stars is not in the plane of the Milky Way galaxy and has been proposed as a remnant of an ancient dwarf satellite galaxy, long since disrupted and assimilated into the Milky Way. It consists of old stars deficient in heavy elements. However, Bensby and colleagues in analysing chemical composition of F and G dwarf stars in the solar neighbourhood found there was no difference in chemical makeup of stars from the stream, suggesting an intragalactic rather than extragalactic origin. One possibility is that the stream appeared in a manner similar to the Hercules group, which is hypothesized to have formed due to Outer Lindblad Resonance with the Galactic bar. However, it is unclear how this could produce an overdensity of stars in the thick disk.\n\nResearch from the Radial Velocity Experiment at the Australian Astronomical Observatory headed by Quentin Parker was first to quantify the nature of the group, though astronomers had known of its existence for some time. It was first discovered in 1971.\n\nOther members include the red giant κ Gruis and the M-class stars 27 Cancri, Alpha Vulpeculae and RT Hydrae.\n"}
{"id": "42445", "url": "https://en.wikipedia.org/wiki?curid=42445", "title": "Atomic mass unit", "text": "Atomic mass unit\n\nThe unified atomic mass unit or dalton (symbol: u, or Da) is a standard unit of mass that quantifies mass on an atomic or molecular scale (atomic mass). One unified atomic mass unit is approximately the mass of one nucleon (either a single proton or neutron) and is numerically equivalent to 1g/mol. It is defined as one twelfth of the mass of an unbound neutral atom of carbon-12 in its nuclear and electronic ground state and at rest, and has a value of , or approximately 1.66 yoctograms. The CIPM has categorised it as a non-SI unit accepted for use with the SI, and whose value in SI units must be obtained experimentally.\n\nThe atomic mass unit (amu) without the \"unified\" prefix is technically an obsolete unit based on oxygen, which was replaced in 1961. However, many sources still use the term \"amu\" but now define it in the same way as u (i.e., based on carbon-12). In this sense, most uses of the terms \"atomic mass units\" and \"amu\", today, actually refer to unified atomic mass unit. For standardization, a specific atomic nucleus (carbon-12 vs. oxygen-16) had to be chosen because the average mass of a nucleon depends on the count of the nucleons in the atomic nucleus due to mass defect. This is also why the mass of a proton or neutron by itself is more than (and not equal to) 1 u.\n\nThe atomic mass unit is not the unit of mass in the atomic units system, which is rather the electron rest mass (\"m\").\n\nThe standard atomic weight (or atomic weight) scale has traditionally been a relative value, that is without a unit, with the first relative atomic mass basis suggested by John Dalton in 1803 as H. Despite the initial mass of H being used as the natural unit for relative atomic mass, it was suggested by Wilhelm Ostwald that relative atomic mass would be best expressed in terms of units of mass of oxygen (). This evaluation was made prior to the discovery of the existence of elemental isotopes, which occurred in 1912.\n\nThe discovery of isotopic oxygen in 1929 led to a divergence in relative atomic mass representation, with isotopically weighted oxygen (i.e., naturally occurring oxygen relative atomic mass) given a value of exactly 16 atomic mass units (amu) in chemistry, while pure O (oxygen-16) was given the mass value of exactly 16 amu in physics.\n\nThe divergence of these values could result in errors in computations, and was unwieldy. The chemistry amu, based on the relative atomic mass (atomic weight) of natural oxygen (including the heavy naturally-occurring isotopes O and O), was about as massive as the physics amu, based on pure isotopic O.\n\nFor these and other reasons, the reference standard for both physics and chemistry was changed to carbon-12 in 1961. The choice of carbon-12 was made to minimise further divergence with prior literature. The new and current unit was referred to as the \"unified atomic mass unit\", u. and given a new symbol, \"u\", which replaced the now deprecated \"amu\" that had been connected to the old oxygen-based system. The dalton (Da) is another name for the unified atomic mass unit.\n\nDespite this change, modern sources often still use the old term \"amu\" but define it as u ( of the mass of a carbon-12 atom), as mentioned in the article's introduction. Therefore, in general, \"amu\" likely does not refer to the old oxygen standard unit, unless the source material originates from the 1960s or before.\n\nThe unified atomic mass unit and the dalton are different names for the same unit of measure. As with other unit names such as watt and newton, \"dalton\" is not capitalized in English, but its symbol, Da, is capitalized. With the introduction of the name \"dalton\", there has been a gradual change towards using that name in preference to the name, \"unified atomic mass unit\":\n\nThe former definition of the mole, an SI base unit, was accepted by the CGPM in 1971 as:\nHowever, the first part of this definition was changed in 2018 to:\n\nOne consequence of this change is that the current defined relationship between the mass of the C atom, the dalton, the kilogram, and the Avogadro number will no longer be valid. One of the following must change:\nThe wording of the ninth SI Brochure implies that the first statement remains valid, which means that the second is no longer true. The molar mass constant, while still with great accuracy remaining 1 g/mol, is no longer exactly equal to that.\nGiven that the unified atomic mass unit is one twelfth the mass of one atom of carbon-12, meaning the mass of such an atom is 12 u, it follows that there are only approximately \"N\" atoms of carbon-12 in 0.012 kg of carbon-12. This can be expressed mathematically as\n\nMolecular masses of proteins are often expressed in kilodaltons (kDa or kD). For example, a molecule of a protein with molar mass has a mass of 64 kDa.\n\nIn research and commerce, the degree of polymerization of synthetic polymers is conventionally expressed in daltons.\n\nThe US Supreme Court based a major precedent of appellate law on a disputed case of counting daltons for a molecular distribution.\n\n\n\n"}
{"id": "23853172", "url": "https://en.wikipedia.org/wiki?curid=23853172", "title": "Bike Arc", "text": "Bike Arc\n\nBike Arc LLC, located in downtown Palo Alto, California, is a Silicon Valley startup that designs secure bicycle parking racks and systems. It was founded by Joseph Bellomo and Jeff Selzer in 2008. Jeff Selzer sits on the Board of Directors of the Silicon Valley Bicycle Coalition and is the General Manager of Palo Alto Bicycles. Joseph Bellomo, a California-licensed architect, is the founder and owner of Joseph Bellomo Architects, Inc. in Palo Alto, which he founded in 1986. In addition to collaborating on Bike Arc, Mr. Bellomo and Mr. Selzer also worked together on the Palo Alto Bikestation at the Caltrain depot.\n\nIn 2009, the American Institute of Architects, California Council, gave Bike Arc the Honor Award for Small Projects.\n\nMr. Bellomo and Mr. Selzer—both being bicycle enthusiasts—set out to design bicycle-parking systems that don't touch and potentially damage bicycles, a solution that prevents bicycles from contacting each other, which can cause wear-and-tear.\n\nBike Arc is patented in both the United States and in Europe (Office for Harmonization In the Internal Market), and the company offers multiple iterations of the same fundamental concept: a modular structure of steel arcs. The Bike Arc family includes the Rack Arc, the Half Arc, the Umbrella Arc, the Tube Arc, the Car Arc, the Bus Arc, the House Arc, and the Ad Arc.\n\nAll Bike Arc products are manufactured in the United States.\n\nThe City of Palo Alto has purchased and installed multiple Bike Arc products, including numerous Rack Arcs, Half Arcs and Umbrella Arcs, and Bike Arcs are also in public spaces in Boston, Las Vegas, Redwood City, California, and Norfolk, Virginia. They have also been installed at the University of Buffalo and the University of Nebraska, as well as—among others—at Juniper Networks, Inc., Varian Medical Systems, Inc., and the Seattle Repertory Theater.\n\n\n\n"}
{"id": "56853150", "url": "https://en.wikipedia.org/wiki?curid=56853150", "title": "Bonding molecular orbital", "text": "Bonding molecular orbital\n\nThe bonding orbital is used in molecular orbital (MO) theory to describe the attractive interactions between the atomic orbitals of two or more atoms in a molecule. In MO theory, electrons are portrayed to move in waves. When more than one of these waves come close together, the in-phase combination of these waves produces an interaction that leads to a species that is greatly stabilized. The result of the waves’ constructive interference causes the density of the electrons to be found within the binding region, creating a stable bond between the two species.\n\nIn the classic example of the H MO, the two separate H atoms have identical atomic orbitals. When creating the molecule dihydrogen, the individual valence orbitals, 1\"s\", either: merge in phase to get bonding orbitals, where the electron density is in between the nuclei of the atoms; or, merge out of phase to get antibonding orbitals, where the electron density is everywhere around the atom except for the space between the nuclei of the two atoms. Bonding orbitals lead to a more stable species than when the two hydrogens are monatomic. Antibonding orbitals are less stable because, with very little to no electron density in the middle, the two nuclei (holding the same charge) repulse each other. Therefore, it would require more energy to hold the two atoms together through the antibonding orbital. Each electron in the valence 1\"s\" shell of hydrogen come together to fill in the stabilizing bonding orbital. So, hydrogen prefers to exist as a diatomic, and not monatomic, molecule.\nWhen looking at helium, the atom holds two electrons in each valence 1\"s\" shell. When the two atomic orbitals come together, they first fill in the bonding orbital with two electrons, but unlike hydrogen, it has two electrons left, which must then go to the antibonding orbital. The instability of the antibonding orbital cancels out the stabilizing effect provided by the bonding orbital; therefore, dihelium's bond order is 0. This is why helium would prefer to be monatomic over diatomic.\n\nPi bonds are created by the “side-on” interactions of the orbitals. Once again, in molecular orbitals, bonding pi (π) electrons occur when the interaction of the two π atomic orbitals are in-phase. In this case, the electron density of the π orbitals need to be symmetric along the mirror plane in order to create the bonding interaction. Asymmetry along the mirror plane will lead to a node in that plane and is described in the antibonding orbital, π*.\n\nAn example of a MO of a simple conjugated π system is butadiene. To create the MO for butadiene, the resulting π and π* orbitals of the previously described system will interact with each other. This mixing will result in the creation of 4 group orbitals (which can also be used to describe the π MO of any diene): π contains no vertical nodes, π contains one and both are considered bonding orbitals; π contains 2 vertical nodes, π contains 3 and are both considered antibonding orbitals.\n\nThe spherical 3D shape of \"s\" orbitals have no directionality in space and \"p\", \"p\", and \"p\" orbitals are all 90 with respect to each other. Therefore, in order to capture the actual geometry of molecules in space, hybridization is key. Hybridizing molecular orbitals is a way of combining the theory of orbitals and the experimental evidence of chemical bonds. Edmiston and Ruedenberg best explain local hybridization and the history of molecular orbitals that lead to this method. The hybridized \"sp\" character of the carbon atom in the molecule of methane can be seen in its MO. The four electrons from the 1\"s\" orbitals of the hydrogen atoms and the valence electrons from the carbon atom (2 in \"s\" and 2 in \"p\") occupy the bonding orbitals, σ and π, in CH. In this MO example, the bonding orbital of π is less than that of the antibonding orbital of σ, or σ*. The \"sp\" notation was derived from the \"s\" orbital being filled in σ and three of the \"p\" orbitals being filled in the π.\n\nMolecular orbitals and, more specifically, the bonding orbital is a theory that is taught in all different areas of chemistry, from organic to physical and even analytical, because it is widely applicable. Organic chemists use molecular orbital theory in their thought rationale for reactions; analytical chemists use it in different spectroscopy methods; physical chemists use it in calculations; it is even seen in materials chemistry through band theory—an extension of molecular orbital theory.\n"}
{"id": "7346789", "url": "https://en.wikipedia.org/wiki?curid=7346789", "title": "Ceres Connection", "text": "Ceres Connection\n\nThe Ceres Connection is a cooperative program between MIT's Lincoln Laboratory and the Society for Science and the Public dedicated to promoting science education. It names asteroids discovered under the LINEAR project after teachers and contesting students who performed outstandingly in the following Society for Science and the Public competitions: the Discovery Channel Young Scientist Challenge, the Intel Science Talent Search, the Intel International Science and Engineering Fair.\n\nSince 2002, over 200 asteroids are named each year through this program.\n\n\n"}
{"id": "27992054", "url": "https://en.wikipedia.org/wiki?curid=27992054", "title": "Condenser (optics)", "text": "Condenser (optics)\n\nA condenser is an optical lens which renders a divergent beam from a point source into a parallel or converging beam to illuminate an object. \n\nCondensers are an essential part of any imaging device, such as microscopes, enlargers, slide projectors, and telescopes. The concept is applicable to all kinds of radiation undergoing optical transformation, such as electrons in electron microscopy, neutron radiation and synchrotron radiation optics.\n\nCondensers are located above the light source and under the sample in an upright microscope, and above the stage and below the light source in an inverted microscope. They act to gather light from the microscope's light source and concentrate it into a cone of light that illuminates the specimen. The aperture and angle of the light cone must be adjusted (via the size of the diaphragm) for each different objective lens with different numerical apertures.\n\nCondensers typically consist of a variable-aperture diaphragm and one or more lenses. Light from the illumination source of the microscope passes through the diaphragm and is focused by the lens(es) onto the specimen. After passing through the specimen the light diverges into an inverted cone to fill the front lens of the objective.\n\nThe first simple condensers were introduced on pre-achromatic microscopes in the 17th century. Robert Hooke used a combination of a salt water filled globe and a plano-convex lens, and shows in the 'Micrographia' that he understands the reasons for its efficiency. Makers in the 18th century such as Benjamin Martin, Adams and Jones understood the advantage of condensing the area of the light source to that of the area of the object on the stage. This was a simple plano-convex or bi-convex lens, or sometimes a combination of lenses. With the development of the modern achromatic objective in 1829, by Joseph Jackson Lister, the need for better condensers became increasingly apparent. By 1837, the use of the achromatic condenser was introduced in France, by Felix Dujardin, and Chevalier. English makers early took up this improvement, due to the obsession with resolving test objects such as diatoms and Nobert ruled gratings. By the late 1840s, English makers such as Ross, Powell and Smith; all could supply highly corrected condensers on their best stands, with proper centring and focus. It is erroneously stated that these developments were purely empirical - no-one can design a good achromatic, spherically corrected condenser relying only on empirics. \nOn the Continent, in Germany, the corrected condenser was not considered either useful or essential, mainly due to a misunderstanding of the basic optical principles involved. Thus the leading German company, Carl Zeiss in Jena, offered nothing more than a very poor chromatic condenser into the late 1870s. French makers, such as Nachet, provided excellent achromatic condensers on their stands. When the leading German bacteriologist, Robert Koch, complained to Ernst Abbe, that he was forced to buy a Seibert achromatic condenser for his Zeiss microscope, in order to make satisfactory photographs of bacteria, Abbe produced a very good achromatic design in 1878.\n\nThere are three types of condenser:\n\nThe Abbe condenser is named for its inventor Ernst Abbe, who developed it in 1870. The Abbe condenser, which was originally designed for Zeiss, is mounted below the stage of the microscope. The condenser concentrates and controls the light that passes through the specimen prior to entering the objective. It has two controls, one which moves the Abbe condenser closer to or further from the stage, and another, the iris diaphragm, which controls the diameter of the beam of light. The controls can be used to optimize brightness, evenness of illumination, and contrast. Abbe condensers are difficult to use for magnifications of above 400X, as the aplanatic cone is only representative of a numerical aperture (NA) of 0.6.\n\nThis condenser is composed of two lenses, a plano-convex lens somewhat larger than a hemisphere and a large bi-convex lens serving as a collecting lens to the first. The focus of the first lens is traditionally about 2mm away from the plane face coinciding with the sample plane. A pinhole cap can be used to align the optical axis of the condenser with that of the microscope. The Abbe condenser is still the basis for most modern light microscope condenser designs, even though its optical performance is poor.\n\nAn aplanatic condenser corrects for spherical aberration in the concentrated light path, while an achromatic compound condenser corrects for both spherical and chromatic aberration.\n\nDark field and phase contrast setups are based on an Abbe, aplanatic, or achromatic condenser, but to the light path add a dark field stop or various size phase rings. These additional elements are housed in various ways. In most modern microscope (ca. 1990s–), such elements are housed in sliders that fit into a slot between the illuminator and the condenser lens. Many older microscopes house these elements in a turret-type condenser, these elements are housed in a turret below the condenser lens and rotated into place.\n\nSpecialised condensers are also used as part of Differential Interference Contrast and Hoffman Modulation Contrast systems, which aim to improve contrast and visibility of transparent specimens.\n\nIn epifluorescence microscopy, the objective lens acts not only as a magnifier for the light emitted by the fluorescing object, but also as a condenser for the incident light.\n\nThe Arlow-Abbe condenser is a modified Abbe condenser that replaces the iris diaphragm, filter holder, lamp and lamp optics with a small OLED or LCD digital display unit. The display unit allows for digitally synthesised filters for dark-field, Rheinberg, oblique and dynamic (constantly changing) illumination under direct computer control. The device was first described by Dr. Jim Arlow in Microbe Hunter magazine, issue 38.\n\nLike objective lenses, condensers vary in their numerical aperture (NA). It is NA that determines optical resolution, in combination with the NA of the objective. Different condensers vary in their maximum and minimum numerical aperture, and the numerical aperture of a single condenser varies depending on the diameter setting of the condenser aperture. In order for the maximum numerical aperture (and therefore resolution) of an objective lens to be realized, the numerical aperture of the condenser must be matched to the numerical aperture of the used objective. The technique most commonly used in microscopy to optimize the light pathway between the condenser (and other illumination components of the microscope) and the objective lens is known as Köhler illumination.\n\nThe maximum NA is limited by the refractive index of the medium between the lens and the sample. As with objective lenses, a condenser lens with a maximum numerical aperture of greater than 0.95 is designed to be used under oil immersion (or, more rarely, under water immersion), with a layer of immersion oil placed in contact with both the slide/coverslip and the lens of the condenser. An oil immersion condenser may typically have NA of up to 1.25. Without this oil layer, not only is maximum numerical aperture not realized, but the condenser may not be able to precisely focus light on the object. Condensers with a numerical aperture of 0.95 or less are designed to be used without oil or other fluid on the top lens and are termed dry condensers. Dual dry/immersion condensers are basically oil immersion condensers that can nonetheless focus light with the same degree of precision even without oil between the top lens and the slide.\n\nMicroscopy Resource Center\", 2006.\n\n"}
{"id": "8076881", "url": "https://en.wikipedia.org/wiki?curid=8076881", "title": "Elso Sterrenberg Barghoorn", "text": "Elso Sterrenberg Barghoorn\n\nElso Sterrenberg Barghoorn (June 15, 1915 – January 22, 1984) was an American paleobotanist, called by his student Andrew Knoll, the present Fisher Professor of Natural History at Harvard, \"the father of Pre-Cambrian palaeontology.\"\n\nBarghoorn is best known for discovering in South African rocks fossil evidence of life that is at least 3.4 billion years old. These fossils show that life was present on Earth comparatively soon after the Late Heavy Bombardment (about 3.8 billion years ago).\n\nBarghoorn was born in New York City. After graduating from Miami University with a BSc and an MSc in biology, Barghoorn obtained his Ph.D. in paleobotany from the Harvard University, faculty of Biological Sciences, in 1941. After teaching for five years at Amherst College, he joined the Harvard faculty, becoming Fisher Professor of Natural History and Curator of the University's plant fossils collections. He was elected a Fellow of the American Academy of Arts and Sciences in 1950. In 1972 Barghoorn was awarded the Charles Doolittle Walcott Medal from the National Academy of Sciences.\n\nBarghoorn married Margaret Alden MaCleod in 1941, Teresa Joan LaCroix, and Dorothy Dellmer Osgood in 1964. The first two marriages ended in divorce.\n\n"}
{"id": "21887623", "url": "https://en.wikipedia.org/wiki?curid=21887623", "title": "Euroluna", "text": "Euroluna\n\nThe European Lunar Exploration Association (Euroluna) is a Danish-Italian-Swiss team led by Palle Haastrup, which is participating in the Google Lunar X Prize Challenge.\n\nThe members of the Euroluna team are:\n\n, EuroLuna was designing a lightweight solar-powered four-wheeled Luna Rover called ROMIT, which will be pared down to just 110 pounds by minimising redundant systems.\n\n\n"}
{"id": "6599910", "url": "https://en.wikipedia.org/wiki?curid=6599910", "title": "Gene delivery", "text": "Gene delivery\n\nGene delivery is the process of introducing foreign genetic material, such as DNA or RNA, into host cells. Genetic material must reach the nucleus of the host cell to induce gene expression. Successful gene delivery requires the foreign genetic material to remain stable within the host cell and can either integrate into the genome or replicate independently of it. This requires foreign DNA to be synthesized as part of a vector, which is designed to enter the desired host cell and deliver the transgene to that cell's genome. Vectors utilized as the method for gene delivery can be divided into two categories, recombinant viruses and synthetic vectors (viral and non-viral).\n\nIn complex multicellular eukaryotes (more specifically Weissmanists), if the transgene is incorporated into the host's germline cells, the resulting host cell can pass the transgene to its progeny. If the transgene is incorporated into somatic cells, the transgene will stay with the somatic cell line, and thus its host organism.\n\nGene delivery is a necessary step in gene therapy for the introduction or silencing of a gene to promote a therapeutic outcome in patients and also has applications in the genetic modification of crops. There are many different methods of gene delivery for various types of cells and tissues.\n\nViral based vectors emerged in the 1980s as a tool for transgene expression. In 1983, Siegel described the use of viral vectors in plant transgene expression although viral manipulation via cDNA cloning was not yet available. The first virus to be used as a vaccine vector was the vaccinia virus in 1984 as a way to protect chimpanzees against hepatitis B. Non-viral gene delivery was first reported on in 1943 by Avery et al. who showed cellular phenotype change via exogenous DNA exposure.\n\nThere are a variety of methods available to deliver genes to host cells. When genes are delivered to bacteria or plants the process is called transformation and when it is used to deliver genes to animals it is called transfection. This is because transformation has a different meaning in relation to animals, indicating progression to a cancerous state. For some bacteria no external methods are need to introduce genes as they are naturally able to take up foreign DNA. Most cells require some sort of intervention to make the cell membrane permeable to DNA and allow the DNA to be stably inserted into the hosts genome.\n\nChemical based methods of gene delivery can uses natural or synthetic compounds to form particles that facilitate the transfer of genes into cells. These synthetic vectors have the ability to electrostatically bind DNA or RNA and compact the genetic information to accommodate larger genetic transfers. Chemical vectors usually enter cells by endocytosis and can protect genetic material from degradation.\n\nOne of the simplest method involves altering the environment of the cell and then stressing it by giving it a heat shock. Typically the cells are incubated in a solution containing divalent cations (often calcium chloride) under cold conditions, before being exposed to a heat pulse. Calcium chloride partially disrupts the cell membrane, which allows the recombinant DNA to enter the host cell. It is suggested that exposing the cells to divalent cations in cold condition may change or weaken the cell surface structure, making it more permeable to DNA. The heat-pulse is thought to create a thermal imbalance across the cell membrane, which forces the DNA to enter the cells through either cell pores or the damaged cell wall.\n\nAnother simple methods involves using calcium phosphate to bind the DNA and then exposing it to cultured cells. The solution, along with the DNA, is encaspulated by the cells and a small amount of DNA can be integrated into the genome.\n\nLiposomes and polymers can be used as vectors to deliver DNA into cells. Positively charged liposomes bind with the negatively charged DNA, while polymers can be designed that interact with DNA. They form lipoplexes and polyplexes respectively, which are then up-taken by the cells. The two systems can also be combined. Polymer-based non-viral vectors uses polymers to interact with DNA and form polyplexes.\n\nThe use of engineered inorganic and organic nanoparticles is another non-viral approach for gene delivery.\n\nArtificial gene delivery can be mediated by physical methods which uses force to introduce genetic material through the cell membrane.\n\nElectroporation is a method of promoting competence. Cells are briefly shocked with an electric field of 10-20 kV/cm, which is thought to create holes in the cell membrane through which the plasmid DNA may enter. After the electric shock, the holes are rapidly closed by the cell's membrane-repair mechanisms.\n\nAnother method used to transform plant cells is biolistics, where particles of gold or tungsten are coated with DNA and then shot into young plant cells or plant embryos. Some genetic material enters the cells and transforms them. This method can be used on plants that are not susceptible to \"Agrobacterium\" infection and also allows transformation of plant plastids. Plants cells can also be transformed using electroporation, which uses an electric shock to make the cell membrane permeable to plasmid DNA. Due to the damage caused to the cells and DNA the transformation efficiency of biolistics and electroporation is lower than agrobacterial transformation.\n\nMicroinjection is where DNA is injected through the cell's nuclear envelope directly into the nucleus.\n\nSonoporation uses sound waves create pores in a cell membrane to allow entry of genetic material\n\nPhotoporation is when laser pulses are used to create pores in a cell membrane to allow entry of genetic material\n\nMagnetofection uses magnetic particles complexed with DNA and an external magnetic field concentrate nucleic acid particles into target cells.\n\nA hydrodynamic capillary effect can be used to manipulate cell permeability\n\nIn plants the DNA is often inserted using \"Agrobacterium\"-mediated recombination, taking advantage of the \"Agrobacterium\"s T-DNA sequence that allows natural insertion of genetic material into plant cells. Plant tissue are cut into small pieces and soaked in a fluid containing suspended \"Agrobacterium\". The bacteria will attach to many of the plant cells exposed by the cuts. The bacteria uses conjugation to transfer a DNA segment called T-DNA from its plasmid into the plant. The transferred DNA is piloted to the plant cell nucleus and integrated into the host plants genomic DNA.The plasmid T-DNA is integrated semi-randomly into the genome of the host cell.\n\nBy modifying the plasmid to express the gene of interest, researchers can insert their chosen gene stably into the plants genome. The only essential parts of the T-DNA are its two small (25 base pair) border repeats, at least one of which is needed for plant transformation. The genes to be introduced into the plant are cloned into a plant transformation vector that contains the T-DNA region of the plasmid. An alternative method is agroinfiltration.\n\nVirus mediated gene delivery utilizes the ability of a virus to inject its DNA inside a host cell and takes advantage of the virus' own ability to replicate and implement their own genetic material. Viral methods of gene delivery are more likely to induce an immune response, but they have high efficiency. Transduction is the process that describes virus-mediated insertion of DNA into the host cell. Viruses are a particularly effective form of gene delivery because the structure of the virus prevents degradation via lysosomes of the DNA it is delivering to the nucleus of the host cell. In gene therapy a gene that is intended for delivery is packaged into a replication-deficient viral particle to form a viral vector. Viruses used for gene therapy to date include retrovirus, adenovirus, adeno-associated virus and herpes simplex virus. However, there are drawbacks to using viruses to deliver genes into cells. Viruses can only deliver very small pieces of DNA into the cells, it is labor-intensive and there are risks of random insertion sites, cytophathic effects and mutagenesis.\n\nViral vector based gene delivery uses a viral vector to deliver genetic material to the host cell. This is done by using a virus that contains the desired gene and removing the part of the viruses genome that is infectious. Viruses are efficient at delivering genetic material to the host cell's nucleus, which is vital for replication. \n\nRNA-based viruses were developed because of the ability to transcribe directly from infectious RNA transcripts. RNA vectors are quickly expressed and expressed in the targeted form since no processing is required. Gene integration leads to long-term transgene expression but RNA-based delivery is usually transient and not permanent. Some retroviral vectors include: Oncoretroviral vectors, Lentiviral vector in gene therapy, Human foamy virus.\n\nDNA-based viral vectors are usually longer lasting with the possibility of integrating into the genome. Some DNA-based viral vectors include: Adenoviridae, Adeno-associated virus, Herpes simplex virus.\n\nSeveral of the methods used to facilitate gene delivery have applications for therapeutic purposes. Gene therapy utilizes gene delivery to deliver genetic material with the goal of treating a disease or condition in the cell. Gene delivery in therapeutic settings utilizes non-immunogenic vectors capable of cell specificity that can deliver an adequate amount of transgene expression to cause the desired effect.\n\nAdvances in genomics have enabled a variety of new methods and gene targets to be identified for possible applications. DNA microarrays used in a variety of next-gen sequencing can identify thousands of genes simultaneously, with analytical software looking at gene expression patterns, and orthologous genes in model species to identify function. This has allowed a variety of possible vectors to be identified for use in gene therapy. As a method for creating a new class of vaccine, gene delivery has been utilized to generate a hybrid biosynthetic vector to deliver a possible vaccine. This vector overcomes traditional barriers to gene delivery by combining \"E. coli\" with a synthetic polymer to create a vector that maintains plasmid DNA while having an increased ability to avoid degradation by target cell lysosomes.\n\n\n"}
{"id": "31948970", "url": "https://en.wikipedia.org/wiki?curid=31948970", "title": "Gode Venkata Juggarow", "text": "Gode Venkata Juggarow\n\nGode Venkata Juggarow (1817–1856) was an Indian astronomer and instrument maker. He was one of the few Indians who started and operated an astronomical observatory in British India at Visakhapatnam. He started his own observatory in Visakhapatnam at his residence in Daba Gardens.\n"}
{"id": "31671361", "url": "https://en.wikipedia.org/wiki?curid=31671361", "title": "Hiroshima: BBC History of World War II", "text": "Hiroshima: BBC History of World War II\n\nHiroshima is a BBC docudrama that premiered as a television special on 5 August 2005, marking the eve of the 60th anniversary of the atomic bombing of Hiroshima. The program was aired on the Discovery Channel and BBC America in the United States. The documentary features historical reenactments using firsthand eyewitness accounts and computer-generated imagery of the explosion. The film won an Emmy and three BAFTA awards in 2006.\n\nThe documentary recounts the world's first nuclear attack and examines the repercussions. Covering a three-week period from the Trinity test to the atomic bombing of Hiroshima, the program chronicles America's political gamble and the planning for the momentous event. Archival film, dramatizations, and special effects depict what occurred aboard the \"Enola Gay\" and inside the nuclear blast.\n\nFive Japanese survivors are interviewed: Kinuko Laskey (a nurse in a communications hospital), Morio Ozaki (an army cadet), Toruko Fujii (16-year-old tram driver), Thomas Takashi Tanemori (an eight-year-old schoolboy), Dr. Shuntaro Hida (a doctor at a military hospital), and Akiko Takakura (a 17-year-old city bank clerk).\n\nFrom the United States the interviewees are Paul Tibbets (the commanding officer and pilot of the \"Enola Gay\"), Theodore Van Kirk (the navigator of the aircraft), Morris R. Jeppson (the weapon test officer), and Russell Gackenbach (the navigator of the accompanying photographic aircraft \"Necessary Evil\"). White House Map Room Duty Officer George Elsey is interviewed as an eyewitness to the Potsdam Conference.\n\n\n"}
{"id": "221773", "url": "https://en.wikipedia.org/wiki?curid=221773", "title": "Human migration", "text": "Human migration\n\nHuman migration is the movement by people from one place to another with the intentions of settling, permanently or temporarily in a new location. The movement is often over long distances and from one country to another, but internal migration is also possible; indeed, this is the dominant form globally. People may migrate as individuals, in family units or in large groups. A person who moves from their home to another place because of natural disaster or civil disturbance may be described as a refugee or, especially within the same country, a displaced person. A person seeking refuge from political, religious, or other forms of persecution is usually described as an asylum seeker.\n\nNomadic movements are normally not regarded as migrations as there is no intention to settle in the new place and because the movement is generally seasonal. Only a few nomadic people have retained this form of lifestyle in modern times. Also, the temporary movement of people for the purpose of travel, tourism, pilgrimages, or the commute is not regarded as migration, in the absence of an intention to live and settle in the visited places.\nMany estimates of statistics in worldwide migration patterns exist.\n\nThe World Bank has published its \"Migration and Remittances Factbook\" annually since 2008. The International Organisation for Migration (IOM) has published a yearly \"World Migration Report\" since 1999. The United Nations Statistics Division also keeps a database on worldwide migration. Recent advances in research on migration via the Internet promise better understanding of migration patterns and migration motives.\n\nSubstantial internal migration can also take place within a country, either seasonal human migration (mainly related to agriculture and to tourism to urban places), or shifts of population into cities (urbanisation) or out of cities (suburbanisation). Studies of worldwide migration patterns, however, tend to limit their scope to international migration.\n\nThe World Bank's \"Migration and Remittances Factbook\" of 2011 lists the following estimates for the year 2010: total number of immigrants: 215.8 million or 3.2% of world population. In 2013, the percentage of international migrants worldwide increased by 33% with 59% of migrants targeting developed regions. Almost half of these migrants are women, which is one of the most significant migrant-pattern changes in the last half century. Women migrate alone or with their family members and community. Even though female migration is largely viewed as associations rather than independent migration, emerging studies argue complex and manifold reasons for this.\n\nOften a distinction is made between voluntary and involuntary migration, or between refugees fleeing political conflict or natural disaster vs. economic or labor migration, but these distinctions are difficult to make and partially subjective, as the motivators for migration are often correlated. The World Bank's report estimates that, as of 2010, 16.3 million or 7.6% of migrants qualified as refugees. At the end of 2012, approximately 15.4 million people were refugees and persons in refugee-like situations - 87% of them found asylum in developing countries.\n\nStructurally, there is substantial South-South and North-North migration, i.e., most emigrants from high-income O.E.C.D. countries migrate to other high-income countries, and a substantial part (estimated at 43%) of emigrants from developing countries migrate to other developing countries. The United Nations Population Fund says that \"[while the North has experienced a higher absolute increase in the migrant stock since 2000 (32 million) compared to the South (25 million), the South recorded a higher growth rate. Between 2000 and 2013 the average annual rate of change of the migrant population in the developing regions (2.3%) slightly exceeded that of the developed regions (2.1%).\nThe top immigration countries are:\n\nThe top countries of origin are:\n\nThe top migration corridors worldwide are:<br>1. Libya–European Union <br>2. Mexico–United States<br>3. Morocco-European Union<br>4. Russia–Ukraine<br>5. Ukraine–Russia<br>6. Bangladesh–India<br>7. Nepal-India<br>8. Turkey–Germany<br>9. Kazakhstan–Russia<br>10. Russia–Kazakhstan<br>11. Cuba-United States<br>12. China–Northern America<br>13. Algeria-France<br>14. India-Northen America<br>15. Philippines-Northern America<br>16. South Korea-Northern America<br>17. Vietnam-Northern America<br>18. China mainland–Hong Kong<br>19. Vietnam-Australia<br>20. Hong Kong-Canada\n\nRemittances, i.e., funds transferred by migrant workers to their home country, form a substantial part of the economy of some countries. The top ten remittance recipients in 2017.\nThe Global Commission on International Migration (GCIM), launched in 2003, published a report in 2005. International migration challenges at the global level are addressed through the Global Forum on Migration and Development and the Global Migration Group, both established in 2006.\n\nThe United Nations reported that 2014 had the highest level of forced migration on record: 59.5 million individuals, caused by \"persecution, conflict, generalized violence, or human rights violations\", as compared with 51.2 million in 2013 (an increase of 8.3 million) and with 37.5 million a decade prior. one of every 122 humans is a refugee, internally displaced, or seeking asylum. National Geographic has published 5 maps showing human migrations in progress in 2015 based on the UN report.\n\nNumerous causes impel migrants to move to another country. For instance, globalization has increased the demand for workers in order to sustain national economies. Thus one category of economic migrants - generally from impoverished developing countries - migrates to obtain sufficient income for survival.\nSuch migrants often send some of their income home to family members in the form of economic remittances, which have become an economic staple in a number of developing countries. People may also move or are forced to move as a result of conflict, of human-rights violations, of violence, or to escape persecution. In 2013 it was estimated that around 51.2 million people fell into this category. Other reasons people may move include to gain access to opportunities and services or to escape extreme weather. This type of movement, usually from rural to urban areas, may class as internal migration. Socio-cultural and geo-historical factors also play a major role. In North Africa, for example, emigrating Europe counts as a sign of social prestige. Moreover, many countries were former colonies. This means that many have relatives who live legally in the (former) colonial metropole, and who often provide important help for immigrants arriving in that metropole.\nRelatives may help with job research and with accommodation. The geographical proximity of Africa to Europe and the long historical ties between Northern and Southern Mediterranean countries also prompt many to migrate.\n\nA number of theories attempt to explain the international flow of capital and people from one country to another.\n\nThis theory of migration states that the main reason for labor migration is wage difference between two geographic locations. These wage differences are usually linked to geographic labor demand and supply. It can be said that areas with a shortage of labor but an excess of capital have a high relative wage while areas with a high labor supply and a dearth of capital have a low relative wage. Labor tends to flow from low-wage areas to high-wage areas. Often, with this flow of labor comes changes in the sending as well as the receiving country. Neoclassical economic theory is best used to describe transnational migration, because it is not confined by international immigration laws and similar governmental regulations.\n\nDual labor market theory states that migration is mainly caused by pull factors in more developed countries. This theory assumes that the labor markets in these developed countries consist of two segments: the primary market, which requires high-skilled labor, and the secondary market, which is very labor-intensive requiring low-skilled workers. This theory assumes that migration from less developed countries into more developed countries is a result of a pull created by a need for labor in the developed countries in their secondary market. Migrant workers are needed to fill the lowest rung of the labor market because the native laborers do not want to do these jobs as they present a lack of mobility. This creates a need for migrant workers. Furthermore, the initial dearth in available labor pushes wages up, making migration even more enticing.\n\nThis theory states that migration flows and patterns can't be explained solely at the level of individual workers and their economic incentives, but that wider social entities must be considered as well. One such social entity is the household. Migration can be viewed as a result of risk aversion on the part of a household that has insufficient income. The household, in this case, is in need of extra capital that can be achieved through remittances sent back by family members who participate in migrant labor abroad. These remittances can also have a broader effect on the economy of the sending country as a whole as they bring in capital. Recent research has examined a decline in U.S. interstate migration from 1991 to 2011, theorizing that the reduced interstate migration is due to a decline in the geographic specificity of occupations and an increase in workers’ ability to learn about other locations before moving there, through both information technology and inexpensive travel. Other researchers find that the location-specific nature of housing is more important than moving costs in determining labour reallocation.\n\nRelative deprivation theory states that awareness of the income difference between neighbors or other households in the migrant-sending community is an important factor in migration. The incentive to migrate is a lot higher in areas that have a high level of economic inequality. In the short run, remittances may increase inequality, but in the long run, they may actually decrease it. There are two stages of migration for a worker: first, they invest in human capital formation, and then they try to capitalize on their investments. In this way, successful migrants may use their new capital to provide for better schooling for their children and better homes for their families. Successful high-skilled emigrants may serve as an example for neighbors and potential migrants who hope to achieve that level of success.\n\nWorld-systems theory looks at migration from a global perspective. It explains that interaction between different societies can be an important factor in social change within societies. Trade with one country, which causes economic decline in another, may create incentive to migrate to a country with a more vibrant economy. It can be argued that even after decolonization, the economic dependence of former colonies still remains on mother countries. This view of international trade is controversial, however, and some argue that free trade can actually reduce migration between developing and developed countries. It can be argued that the developed countries import labor-intensive goods, which causes an increase in employment of unskilled workers in the less developed countries, decreasing the outflow of migrant workers. The export of capital-intensive goods from rich countries to poor countries also equalizes income and employment conditions, thus also slowing migration. In either direction, this theory can be used to explain migration between countries that are geographically far apart.\n\nOld migration theories are generally embedded in geography, sociology or economics. They explain migration in specific periods and spaces. In fact, Osmosis theory explains the whole phenomenon of human migration. Based on the history of human migration, Djelti (2017a) studies the evolution of its natural determinants. According to him, human migration is divided into two main types: the simple migration and the complicated one. The simple migration is divided, in its turn, into diffusion, stabilisation and concentration periods. During these periods, water availability, adequate climate, security and population density represent the natural determinants of human migration. For the complicated migration, it is characterised by the speedy evolution and the emergence of new sub-determinants notably earning, unemployment, networks and migration policies. Osmosis theory (Djelti, 2017b) explains analogically human migration by the biophysical phenomenon of osmosis. In this respect, the countries are represented by animal cells, the borders by the semipermeable membranes and the humans by ions of water. As to osmosis phenomenon, according to the theory, humans migrate from countries with less migration pressure to countries with high migration pressure. In order to measure the latter, the natural determinants of human migration replace the variables of the second principle of thermodynamics used to measure the osmotic pressure.\n\nA number of social scientists have examined immigration from a sociological perspective, paying particular attention to how immigration affects, and is affected by, matters of race and ethnicity, as well as social structure. They have produced three main sociological perspectives: symbolic interactionism, which aims to understand migration via face-to-face interactions on a micro-level; social conflict theory examines migration through the prism of competition for power and resources; structural functionalism, based on the ideas of Émile Durkheim, examines the role of migration in fulfilling certain functions within each society, such as the decrease of despair and aimlessness and the consolidation of social networks.\n\nMore recently, as attention shifted away from countries of destination, sociologists have attempted to understand how transnationalism allows us to understand the interplay between migrants, their countries of destination, and their countries of origins. In this framework, work on social remittances by Peggy Levitt and others has led to a stronger conceptualisation of how migrants affect socio-political processes in their countries of origin.\n\nPolitical scientists have put forth a number of theoretical frameworks on migration, offering different perspectives on processes of security, citizenship, and international relations. The political importance of diasporas has also become a growing field of interest, as scholars examine questions of diaspora activism, state-diaspora relations, out-of-country voting processes, and states' soft power strategies. In this field, the majority of work has focused on immigration politics, viewing migration from the perspective of the country of destination. With regard to emigration processes, political scientists have expanded on Albert Hirschman's framework on 'voice' vs. 'exit' to discuss how emigration affects the politics within the countries of origin.\n\nCertain laws of social science have been proposed to describe human migration. The following was a standard list after Ravenstein's (1834–1913) proposal in the 1880s. The laws are as follows:\n\n\nLee's laws divide factors causing migrations into two groups of factors: push and pull factors. Push factors are things that are unfavourable about the area that one lives in, and pull factors are things that attract one to another area.\n\nPush factors\nPull factors\n\nSee also article by Gürkan Çelik, in Turkish Review: Turkey Pulls, The Netherlands Pushes? An increasing number of Turks, the Netherlands’ largest ethnic minority, are beginning to return to Turkey, taking with them the education and skills they have acquired abroad, as the Netherlands faces challenges from economic difficulties, social tension and increasingly powerful far-right parties. At the same time Turkey’s political, social and economic conditions have been improving, making returning home all the more appealing for Turks at large. (pp. 94–99)\n\nThe modern field of climate history suggests that the successive waves of Eurasian nomadic movement throughout history have had their origins in climatic cycles, which have expanded or contracted pastureland in Central Asia, especially Mongolia and to its west the Altai. People were displaced from their home ground by other tribes trying to find land that could be grazed by essential flocks, each group pushing the next further to the south and west, into the highlands of Anatolia, the Pannonian Plain, into Mesopotamia, or southwards, into the rich pastures of China. Bogumil Terminski uses the term \"migratory domino effect\" to describe this process in the context of Sea People invasion.\n\n\n\n\n\n\n"}
{"id": "8381118", "url": "https://en.wikipedia.org/wiki?curid=8381118", "title": "Inspirator", "text": "Inspirator\n\nAn inspirator is a device, similar to a venturi tube and an orifice plate, which mixes a fuel gas with atmospheric air in a precise ratio to regulate burn characteristics. Only the pressure of the fuel gas is used to draw in and mix the air. They are the most simple and common type of mixing device for gas stoves and furnaces. Burners using an inspirator are considered to be naturally aspirated.\n\nIn an inspirator there are two tubes. The first is a fuel gas pipe with an orifice at the end where the gas comes out. Then in front of this there is another section of tubing with a larger diameter that the gas blows into. Usually (but not always) this second piece of tubing is tapered so that it starts getting narrower downstream from the orifice. Then, at a certain point, it stops getting narrower and either straightens out or starts getting larger again. This gives the fuel and air time to mix. The fuel/air ratio is determined by the ratio of the diameter of the orifice to the diameter of the mixing tube.\n"}
{"id": "25122627", "url": "https://en.wikipedia.org/wiki?curid=25122627", "title": "International Centre for Black Sea Studies", "text": "International Centre for Black Sea Studies\n\nThe International Centre for Black Sea Studies (Greek: Διεθνές Κέντρο Μελετών Ευξείνου Πόντου, English Acronym: ICBSS, Greek Acronym: ΔΙΚΕΜΕΠ) is a think-tank based in Athens, Greece, committed to promoting multilateral cooperation between the countries of the Black Sea region and with their international partners.\n\nThe ICBSS was established in 1998 as a private non-profit organisation under Greek law (art. 50 paragraph 2 of L. 2594/1998, Government Gazette 62-A’, as modified by Presidential Decrees No 137 & 138, Government Gazette No.119, 1st issue, April 2000). The Centre is a de facto related body of the intergovernmental Organization of the Black Sea Economic Cooperation (BSEC) and forms part of the so-called “BSEC family” of institutions.\n\nThe ICBSS was created originally to contribute through policy-oriented research and advocacy work to the realisation of the goals of the BSEC and continues to act as the BSEC’s acknowledged think-tank. Aside from the mandates received from the BSEC the Centre also carries out a range of projects and activities on its own initiative, aiming to foster multilateral cooperation in and with the wider Black Sea region. A special focus lies on relations with the European Union.\n\nThrough its activities, the ICBSS aims to foster multilateral cooperation among the BSEC member states as well as with their international partners.\n\nAs an independent research and training centre the ICBSS strives “to pursue applied, policy-oriented research, build capacity and promote knowledge on the Black Sea region both within and outside its boundaries”.\n\nAs a related body of the BSEC its goal is \"to fulfil in the best possible way its institutional role and the assignments received by carrying out studies, offering policy advice and coordinating activities”.\n\nIn 2008, undergoing a review of its image on the occasion of its 10th anniversary, the Centre coined the motto: “Promoting Synergies Across Regions”.\n\nTo fulfil its mission the Centre carries out applied, policy-oriented research either independently or in its capacity as a related body of the BSEC. While it is not restricted in its thematic scope the ICBSS has so far focused on the following thematic areas:\n\n\nResearch supporting on these topics is pursued in-house, through external experts and/or in collaboration with other institutions. It feeds into the Centre's projects, publications, events, and advocacy activities.\n\n\nAside from its ongoing programmes, the Centre is also undertaking a range of one-off projects of limited duration often carried out with the collaboration of international partners and usually receiving external funding. The Centre has been in particular involved in a number of large-scale EU co-funded projects aiming to foster cooperation and policy coordination in the fields of Science and Technology:\n\n\nOther projects include:\n\nThe ICBSS produces the following publication series:\n\nIn addition to its series, the Centre occasionally produces books and other publications, while ICBSS staff members contribute to edited volumes, journals and a range of online and offline media. The Centre’s publications are available free of charge from its website and can also be consulted in its in-house library as part of a collection of publications on Black Sea related topics.\n\nAs a think-tank the ICBSS produces policy-oriented research in the areas of its expertise designed to provide decision-makers and other stakeholders with information and analysis for evidence-based decision-making. In its capacity as a related body of the BSEC the Centre participates in the deliberations of the Organisation’s decision-making and subsidiary organs and related bodies, mainly in a consultative role. Upon specific mandates, it drafts policy documents offering policy advice and is engaged in various working groups.\n\nThe ICBSS is overseen by a Board of Directors comprised by representatives from all twelve member states of the BSEC as well as the Secretary General of the BSEC Permanent International Secretariat, the Director General and Alternate Director General of the ICBSS, and three floating chairs for persons of high international standing.\n\nManaged by the ICBSS’ Director General with the assistance of the Alternate Director General, the ICBSS team consists of 10-15 staff working on the premises and a pool of external experts and collaborators appointed on a project basis.\n\nThe Centre has established a network of international partners including the following “special partners”:\n\nThe Government of the Hellenic Republic supports the Centre’s operation and development. Additional funding is obtained through research contracts, grants and voluntary contributions of BSEC member states and other donors.\n\n"}
{"id": "35764931", "url": "https://en.wikipedia.org/wiki?curid=35764931", "title": "Janez Matjašič", "text": "Janez Matjašič\n\nJanez Matjašič (14 May 1921 – 9 August 1996) was a Slovene zoologist.\n\nMatjašič was an associate member of the Slovenian Academy of Sciences and Arts from 1974 and a full member from 1989.\n\nApart from scientific contributions he also wrote two popular science books \"Nevidno življenje\" (Invisible Life) and \"Iz življenja najmanjših\" (From the Lives of the Smallest). For the latter he won the Levstik Award in 1956.\n"}
{"id": "11121146", "url": "https://en.wikipedia.org/wiki?curid=11121146", "title": "Jet noise", "text": "Jet noise\n\nIn aeroacoustics, jet noise is the field that focuses on the noise generation caused by high-velocity jets and the turbulent eddies generated by shearing flow. Such noise is known as broadband noise and extends well beyond the range of human hearing (100 kHz and higher). Jet noise is also responsible for some of the loudest sounds ever produced by mankind.\n\nThe primary sources of jet noise for a high-speed air jet (meaning when the exhaust velocity exceeds about 100 m/s) are \"jet mixing noise\" and, for supersonic flow, shock associated noise. Also, acoustic sources within the \"jet pipe\" also contribute to the noise, mainly at lower speeds, which include combustion noise and sounds produced by interactions of a turbulent stream with fans, compressors, and turbine systems.\n\nThe jet mixing sound is created by the turbulent mixing of a jet with the ambient fluid, in most cases, air. The mixing initially occurs in an annular shear layer, which grows with the length of the nozzle. The mixing region generally fills the entire jet at four or five diameters from the nozzle. The high-frequency components of the sound are mainly stationed close to the nozzle, where the dimensions of the turbulence eddies are small. Further down the jet, where the eddy size is similar to the jet diameter, is where lower frequency begins.\n\nIn Supersonic, or \"choked\" jets there are cells through which the flow continuously expands and contracts. Several of these \"shock cells\" can be seen extending up to ten jet diameters from the nozzle and are responsible for two additional components of jet noise, \"screech tones\" and broadband \"shock associated noises\". Screech is produced by a feedback mechanism in which a disturbance convecting in the shear layer generates sound as it traverses the standing system of shock waves in the jet. Even though screech is a side effect of the jet's flight, it can be suppressed by an appropriate design for a nozzle.\n\nAircraft noise is also sometimes called \"jet noise\" when emanating from jet aircraft, regardless of the mechanism of noise production.\n\n\nWorks cited\n"}
{"id": "18010608", "url": "https://en.wikipedia.org/wiki?curid=18010608", "title": "John Wilbanks", "text": "John Wilbanks\n\nJohn Wilbanks is the chief commons officer at Sage Bionetworks and a senior fellow at the Ewing Marion Kauffman Foundation and at FasterCures. He runs the Consent to Research Project.\n\nWilbanks grew up in Knoxville, Tennessee, US. He attended Tulane University and received a Bachelor of Arts in philosophy in 1994. He also studied modern letters at the Sorbonne in Paris.\n\nFrom 1994 to 1997, he worked in Washington, DC as a legislative aide to Congressman Fortney \"Pete\" Stark. During this time Wilbanks was also a grassroots coordinator and fundraiser for the American Physical Therapy Association. Wilbanks was the Berkman Center for Internet & Society's first assistant director from the fall of 1998 to the summer of 2000. There he led efforts in software development and Internet-mediated learning, and was involved in the Berkman Center's work on ICANN.\n\nWhile at the Berkman Center, Wilbanks founded Incellico, Inc., a bioinformatics company that built semantic graph networks for use in pharmaceutical research and development. He served as President and CEO, and led to the company's acquisition in the summer of 2003. He has also served as a Fellow at the World Wide Web Consortium on Semantic Web for Life Sciences, was a Visiting Scientist in the Project on Mathematics and Computation at MIT, and was a member of the National Advisory Committee for PubMed Central. He is a member of the Board of Directors for Sage Bionetworks and on the advisory boards of Genomera, Genomic Arts, and Boundless Learning. He is an original author of the Panton Principles for sharing data. \n\nWilbanks led a We the People petition supporting the free access of taxpayer-funded research data, which gained over 65,000 signatures. In February 2013, the White House responded, detailing a plan to freely publicize taxpayer-funded research data.\n\nConsent to Research (CtR) is a project that provides a platform for people to donate their health data for the purposes of scientific research and the advancement of medicine. Since health data is restricted and expensive, this project provides people the opportunity to freely donate information that can only positively benefit medicine and patients at large. Consent to Research is connected to the Access2Research project, which aims to free access over the Internet to scientific journal articles that are already taxpayer-funded. Wilbanks founded the project in 2011 and gave a TED Global talk about the project in 2012.\n\nWilbanks worked at Science Commons and Creative Commons from October 2004 to September 2011. As vice president of science he ran the Science Commons project for its five-year lifetime and continued to work on science after he joined the core Creative Commons organization. He has been interviewed by Popular Science magazine, KRUU Radio, and BioMed Central to discuss Science Commons.\n\n\"Scientific American\" featured Wilbanks in \"The Machine That Would Predict The Future\" in 2011. Seed magazine named Wilbanks among their Revolutionary Minds of 2008, as a \"Game Changer\" and the Utne Reader named him in 2009 as one of \"50 visionaries who are changing your world\". He frequently campaigns for wider adoption of open access publishing in science and the increased sharing of data by scientists.\n\n"}
{"id": "11193275", "url": "https://en.wikipedia.org/wiki?curid=11193275", "title": "Julius Wilhelm Gintl", "text": "Julius Wilhelm Gintl\n\nJulius Wilhelm Gintl was an Austrian physicist. He was notable as the developer of an early form of duplex electrical telegraph, which allowed two messages to be transmitted on a single wire, in opposite directions. This \"duplex\" communication was an early specific case of the general practice of multiplexing.\n\nGintl's method would be developed to economic viability by J. B. Stearns, and the refined method used in Edison's implementation of a quadruplex telegraph.\n"}
{"id": "4486201", "url": "https://en.wikipedia.org/wiki?curid=4486201", "title": "Kalevi Kull", "text": "Kalevi Kull\n\nKalevi Kull (born 12 August 1952, Tartu) is a biosemiotics professor at the University of Tartu, Estonia.\n\nHe graduated from the University of Tartu in 1975. His earlier work dealt with ethology and field ecology. He has studied the mechanisms of species coexistence in species-rich communities and developed mathematical modelling in ecophysiology. Since 1975, he has been the main organiser of annual meetings of theoretical biology in Estonia. In 1992, he became a Professor of Ecophysiology in the University of Tartu. In 1997, he joined the Department of Semiotics, and became a Professor in Biosemiotics. From 2006 to 2018, he was the Head of the Department of Semiotics in the University of Tartu, Estonia. His field of interests include biosemiotics, ecosemiotics, general semiotics, theoretical biology, theory of evolution, history and philosophy of semiotics and life science.\n\nHe was the president of the Estonian Naturalists' Society in 1991–1994. He is the president of the International Society for Biosemiotic Studies since 2015.\n\nEcologist Olevi Kull was his younger brother.\n\n\n"}
{"id": "81818", "url": "https://en.wikipedia.org/wiki?curid=81818", "title": "Lagoon", "text": "Lagoon\n\nA lagoon is a shallow body of water separated from a larger body of water by barrier islands or reefs. Lagoons are commonly divided into coastal lagoons and atoll lagoons. They have also been identified as occurring on mixed-sand and gravel coastlines. There is an overlap between bodies of water classified as coastal lagoons and bodies of water classified as estuaries. Lagoons are common coastal features around many parts of the world.\n\nLagoons are shallow, often elongated bodies of water separated from a larger body of water by a shallow or exposed shoal, coral reef, or similar feature. Some authorities include fresh water bodies in the definition of \"lagoon\", while others explicitly restrict \"lagoon\" to bodies of water with some degree of salinity. The distinction between \"lagoon\" and \"estuary\" also varies between authorities. Richard A. Davis Jr. restricts \"lagoon\" to bodies of water with little or no fresh water inflow, and little or no tidal flow, and calls any bay that receives a regular flow of fresh water an \"estuary\". Davis does state that the terms \"lagoon\" and \"estuary\" are \"often loosely applied, even in scientific literature.\" Timothy M. Kusky characterizes lagoons as normally being elongated parallel to the coast, while estuaries are usually drowned river valleys, elongated perpendicular to the coast. When used within the context of a distinctive portion of coral reef ecosystems, the term \"lagoon\" is synonymous with the term \"back reef\" or \"backreef\", which is more commonly used by coral reef scientists to refer to the same area. Coastal lagoons are classified as inland bodies of water.\n\nMany lagoons do not include \"lagoon\" in their common names. Albemarle and Pamlico sounds in North Carolina, Great South Bay between Long Island and the barrier beaches of Fire Island in New York, Isle of Wight Bay, which separates Ocean City, Maryland from the rest of Worcester County, Maryland, Banana River in Florida, Lake Illawarra in New South Wales, Montrose Basin in Scotland, and Broad Water in Wales have all been classified as lagoons, despite their names. In England, The Fleet at Chesil Beach has also been described as a lagoon.\n\nIn Latin America, the term \"laguna\" in Spanish, which lagoon translates to, may be used for a small fresh water lake in a similar way a creek is considered a small river. However, sometimes it is popularly used to describe a full-sized lake, such as Laguna Catemaco in Mexico, which is actually the third largest lake by area in the country. The brackish water lagoon may be thus explicitly identified as a \"coastal lagoon\" (\"laguna costera\"). In Portuguese the same usage is found: \"lagoa\" may be a body of shallow sea water, or a small freshwater lake not linked to the sea.\n\nLagoon is derived from the Italian \"laguna\", which refers to the waters around Venice, the Lagoon of Venice. \"Laguna\" is attested in English by at least 1612, and had been Anglicized to \"lagune\" by 1673. In 1697 William Dampier referred to a \"Lagune or Lake of Salt water\" on the coast of Mexico. Captain James Cook described an island \"of Oval form with a Lagoon in the middle\" in 1769.\n\nAtoll lagoons form as coral reefs grow upwards while the islands that the reefs surround subside, until eventually only the reefs remain above sea level. Unlike the lagoons that form shoreward of fringing reefs, atoll lagoons often contain some deep (>20m) portions.\n\nCoastal lagoons form along gently sloping coasts where barrier islands or reefs can develop off-shore, and the sea-level is rising relative to the land along the shore (either because of an intrinsic rise in sea-level, or subsidence of the land along the coast). Coastal lagoons do not form along steep or rocky coasts, or if the range of tides is more than . Due to the gentle slope of the coast, coastal lagoons are shallow. They are sensitive to changes in sea level due to global warming. A relative drop in sea level may leave a lagoon largely dry, while a rise in sea level may let the sea breach or destroy barrier islands, and leave reefs too deep under water to protect the lagoon. Coastal lagoons are young and dynamic, and may be short-lived in geological terms. Coastal lagoons are common, occurring along nearly 15 percent of the world's shorelines. In the United States, lagoons are found along more than 75 percent of the Eastern and Gulf coasts.\n\nCoastal lagoons are usually connected to the open ocean by inlets between barrier islands. The number and size of the inlets, precipitation, evaporation, and inflow of fresh water all affect the nature of the lagoon. Lagoons with little or no interchange with the open ocean, little or no inflow of fresh water, and high evaporation rates, such as Lake St. Lucia, in South Africa, may become highly saline. Lagoons with no connection to the open ocean and significant inflow of fresh water, such as the Lake Worth Lagoon in Florida in the middle of the 19th century, may be entirely fresh. On the other hand, lagoons with many wide inlets, such as the Wadden Sea, have strong tidal currents and mixing. Coastal lagoons tend to accumulate sediments from inflowing rivers, from runoff from the shores of the lagoon, and from sediment carried into the lagoon through inlets by the tide. Large quantities of sediment may be occasionally be deposited in a lagoon when storm waves overwash barrier islands. Mangroves and marsh plants can facilitate the accumulation of sediment in a lagoon. Benthic organisms may stabilize or destabilize sediments.\n\nRiver-mouth lagoons on mixed sand and gravel (MSG) beaches form at the river-coast interface where a typically braided, although sometimes meandering, river interacts with a coastal environment that is significantly affected by longshore drift. The lagoons which form on the MSG coastlines are common on the east coast of the South Island of New Zealand and have long been referred to as hapua by the Māori. This classification differentiates hapua from similar lagoons located on the New Zealand coast termed waituna. Hapua are often located on paraglacial coastal areas where there is a low level of coastal development and minimal population density. Hapua form as the river carves out an elongated coast-parallel area, blocked from the sea by a MSG barrier which constantly alters its shape and volume due to longshore drift. Longshore drift continually extends the barrier behind which the hapua forms by transporting sediment along the coast. Hapua are defined as a narrow shore-parallel extensions of the coastal riverbed. They discharge the majority of stored water to the ocean via an ephemeral and highly mobile drainage channel or outlet. The remainder percolates through the MSG barrier due to its high levels of permeability. Hapua systems are driven by a wide range of dynamic processes that are generally classified as fluvial or marine; changes in the balance between these processes as well as the antecedent barrier conditions can cause shifts in the morphology of the hapua, in particular the barrier. New Zealand examples include the Rakaia, Ashburton and Hurunui river-mouths.\n\nHapua have been identified as establishing in the Canterbury Bight coastal region on the east coast of the South Island. They are often found in areas of coarse-grained sediment where contributing rivers have moderately steep bed gradients. MSG beaches in the Canterbury Bight region contain a wide range of sediment sizes from sand to boulders and are exposed to the high energy waves that make up an east coast swell environment. MSG beaches are reflective rather than dissipative energy zones due to their morphological characteristics. They have a steep foreshore which is known as the ‘engine room’ of the beach profile. In this zone, swash and backwash are dominating processes alongside longshore transport. MSG beaches do not have a surf zone; instead a single line of breakers is visible in all sea conditions. Hapua are associated with MSG beaches as the variation in sediment size allows for the barrier to be permeable.\n\nThe east coast of the South Island has been identified as being in a period of chronic erosion of approximately 0.5 metres per year. This erosion trend is a result of a number of factors. According to the classification scheme of Zenkovich, the rivers on the east coast can be described as ‘small’; this classification is not related to their flow rate but to the insufficient amount of sediment that they transport to the coast to nourish it. The sediment provided is not adequate to nourish the coast against its typical high energy waves and strong longshore drift. These two processes constantly remove sediment depositing it either offshore or further up drift. As the coastline becomes eroded the hapua have been 'rolling back' by eroding the backshore to move landwards.\n\nHapua or river-mouth lagoons form in micro-tidal environments. A micro-tidal environment is where the tidal range (distance between low tide and high tide) is less than two metres. Tidal currents in a micro-tidal zone are less than those found on meso-tidal (two – four metres) and macro-tidal (greater than four metres) coastlines. Hapua form in this type of tidal environment as the tidal currents are unable to compete with the powerful freshwater flows of the rivers therefore there is no negligible tidal penetration to the lagoon. A fourth element of the environment in which hapua form is the strong longshore drift component. Longshore or littoral drift is the transportation of sediments along the coast at an angle to the shoreline. In the Canterbury Bight coastal area; the dominant swell direction is northwards from the Southern Ocean. Therefore, the principal movement of sediment via longshore drift is north towards Banks Peninsula. Hapua are located in areas dominated by longshore drift; because it aids the formation of the barrier behind which the hapua is sited.\n\nA hapua also requires sediment to form the lagoon barrier. Sediment which nourishes the east coast of New Zealand can be sourced from three different areas. Material from the highly erodible Southern Alps is removed via weathering; then carried across the Canterbury Plains by various braided rivers to the east coast beaches. The second source of sediment is the high cliffs which are located in the hinterland of lagoons. These can be eroded during the occurrence of high river flow or sea storm events. Beaches further south provide nourishment to the northern coast via longshore transport.\n\nHapua have a number of characteristics which includes shifts between a variety of morphodynamic states due to changes in the balance between marine and fluvial processes as well as the antecedent barrier conditions. The MSG barrier constantly changes size and shape as a result of the longshore drift. Water stored in the hapua drains to the coast predominately though an outlet; although it can also seep through the barrier depending on the permeability of the material.\n\nChanges in the level of the lagoon water do not occur as a result of saltwater or tidal intrusion. Water in a hapua is predominately freshwater originating from the associated river. Hapua are non-estuarine, there is no tidal inflow however the tide does have an effect on the level of water in the lagoon. As the tide reaches its peak, the lagoon water has a much smaller amount of barrier to permeate through so the lagoon level rises. This is related to a physics theory known as hydraulic head. The lagoon level has a similar sinusoidal wave shape as the tide but reaches its peak slightly later. In general, any saltwater intrusion into the hapua will only occur during a storm via wave overtopping or sea spray.\n\nHapua can act as both a source and sink of sediment. The majority of sediment in the hapua is fluvial sourced. During medium to low river flows, coarser sediment generally collects in the hapua; while some of the finer sediment can be transported through the outlet to the coast. During flood events the hapua is 'flushed out' with larger amounts of sediment transferred through the outlet. This sediment can be deposited offshore or downdrift of the hapua replenishing the undernourished beach. If a large amount of material is released to the coast at one time it can be identified as a 'slug'. These can often be visible from aerial photographs.\n\nAntecedent barrier conditions combined with changes in the balance between marine and fluvial processes results in shifts between a variety of morphological states in a hapua or river-mouth lagoon on a MSG beach. Marine processes includes the direction of wave approach, wave height and the coincidence of storm waves with high tides. Marine processes tend to dominate the majority of morphodynamic conditions until there is a large enough flood event in the associated river to breach the barrier. The level and frequency of base or flood flows are attributed to fluvial processes. Antecedent barrier conditions are the permeability, volume and height of the barrier as well as the width and presence of previous outlet channels. During low to medium river flows, the outlet from the lagoon to the sea becomes offset in the direction of longshore drift. Outlet efficiency tends to decrease the further away from the main river-mouth the outlet is. A decrease in efficiency can cause the outlet to become choked with sediment and the hapua to close temporarily. The potential for closure varies between different hapua depending on whether marine or fluvial processes are the bigger driver in the event. A high flow event; such as a fresh or flood can breach the barrier directly opposite the main river channel. This causes an immediate decrease in the water level of the hapua; as well as transporting previously deposited sediments into the ocean. Flood events are important for eroding lagoon back shores; this is a behaviour which allows hapua to retreat landward and thus remain coastal landforms even with coastal transgression and sea level rise. During high flow events there is also the possibility for secondary breaches of the barrier or lagoon truncation to occur.\n\nStorm events also have the ability to close hapua outlets as waves overtop the barrier depositing sediment and choking the scoured channel. The resultant swift increase in lagoon water level causes a new outlet to be breached rapidly due to the large hydraulic head that forms between the lagoon and sea water levels. Storm breaching is believed to be an important but unpredictable control on the duration of closures at low to moderate river flow levels in smaller hapua.\n\nHapua are extremely important for a number of reasons. They provide a link between the river and sea for migrating fish as well as a corridor for migratory birds. To lose this link via closure of the hapua outlet could result in losing entire generations of specific species as they may need to migrate to the ocean or the river as a vital part of their lifecycle. River-mouth lagoons such as hapua were also used a source for \"mahinga kai\" (food gathering) by the Māori people. However, this is no longer the case due to catchment degradation which has resulted in lagoon deterioration. River-mouth lagoons on MSG beaches are not well explained in international literature.\n\nThe hapua located at the mouth of the Rakaia River stretches approximately three kilometres north from where the river-mouth reaches the coast. The average width of the hapua between 1952 and 2004 was approximately 50 metres; whilst the surface area has stabilised at approximately 600,000 square metres since 1966. The coastal hinterland is composed of erodible cliffs and a low-lying area commonly known as the Rakaia Huts. This area has changed notably since European Settlement; with the drainage of ecologically significant wetlands and development of the small bach community.\n\nThe Rakaia River begins in the Southern Alps, providing approximately 4.2 Mt per year of sediment to the east coast. It is a braided river with a catchment area of 3105 kilometres squared and a mean flow of 221 cubic metres per second. The mouth of the Rakaia River reaches the coast south of Banks Peninsula. As the river reaches the coast it diverges into two channels; with the main channel flowing to the south of the island. As the hapua is located in the Canterbury Bight it is in a state of constant morphological change due to the prevailing southerly sea swells and resultant northwards longshore drift.\n"}
{"id": "49917609", "url": "https://en.wikipedia.org/wiki?curid=49917609", "title": "Lazzaro Mongiardini", "text": "Lazzaro Mongiardini\n\nLazzaro Mongiardini (... – 18th century) was an Italian mathematician.\n"}
{"id": "796647", "url": "https://en.wikipedia.org/wiki?curid=796647", "title": "List of Slovenian physicists", "text": "List of Slovenian physicists\n\nThis is a list of notable Slovenian physicists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also: Physicist, List of Slovenians.\n"}
{"id": "23538370", "url": "https://en.wikipedia.org/wiki?curid=23538370", "title": "List of Superfund sites in Minnesota", "text": "List of Superfund sites in Minnesota\n\nThis is a list of Superfund sites in Minnesota designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of May 4, 2010, there were 25 Superfund sites on the National Priorities List in Minnesota. 21 others have been cleaned up and removed from the list; none are currently proposed for addition.\n\n\n"}
{"id": "3212877", "url": "https://en.wikipedia.org/wiki?curid=3212877", "title": "List of ecologists", "text": "List of ecologists\n\nThis is a list of ecologists who have pages on Wikipedia, in alphabetical order by surname.\n\n"}
{"id": "22759867", "url": "https://en.wikipedia.org/wiki?curid=22759867", "title": "List of flags of the Royal Thai Armed Forces", "text": "List of flags of the Royal Thai Armed Forces\n\nFlags of the Royal Thai Armed Forces (). Most of the flags used by the Thai military today were stipulated in the Flag Act of 1979 (พระราชบัญญัติ ธง พ.ศ. ๒๕๒๒).\n\n\n"}
{"id": "36943246", "url": "https://en.wikipedia.org/wiki?curid=36943246", "title": "List of inclusion bodies that aid in diagnosis of cutaneous conditions", "text": "List of inclusion bodies that aid in diagnosis of cutaneous conditions\n\nMany cutaneous conditions require a skin biopsy for confirmation of the diagnosis. With several of these conditions there are features within the cells contained in the skin biopsy specimen that have elements in their cytoplasm or nucleus that have a characteristic appearance unique to the condition. These elements are termed inclusion bodies.\n\n\n"}
{"id": "15357987", "url": "https://en.wikipedia.org/wiki?curid=15357987", "title": "List of mergers and acquisitions by Apple", "text": "List of mergers and acquisitions by Apple\n\nApple Inc. is an American multinational corporation that designs and manufactures consumer electronics and software products. It was established in Los Altos, California, on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne, and was incorporated on January 3, 1977. The company's hardware products include the Mac line of personal computers, the iPod line of portable media players, the iPad line of tablets, the iPhone line of smartphones, the Apple TV line of digital media players, and the Apple Watch line of smartwatches. Apple's software products include the macOS, iOS, tvOS and watchOS operating systems, the iTunes media player, the Safari web browser, and the iLife suite of multimedia and creativity software. As of , Apple is publicly known to have acquired 72 companies. The actual number of acquisitions is possibly larger as Apple does not reveal the majority of its acquisitions unless discovered by journals. Apple has also purchased stakes in three companies, as well as made three divestments. Apple has not released the financial details for the majority of its mergers and acquisitions.\n\nApple's business philosophy is to acquire small companies that can be easily integrated into existing company projects. For instance, Apple acquired Emagic and its professional music software, Logic Pro, in 2002. The acquisition was incorporated in the creation of the digital audio workstation software GarageBand, now part of the iLife software suite.\n\nThe company made its first acquisition on March 2, 1988, with its purchase of Network Innovations. In its two divestments, both of which were during the 1990s, parts of the company were sold to other companies. In 2013, Apple acquired thirteen companies—more than it has in any other year so far. Apple's largest acquisition was that of Beats Electronics in August 2014 for US$3 billion. Of the companies Apple has acquired, 67 were based in the United States.\n\n\n\n"}
{"id": "21098058", "url": "https://en.wikipedia.org/wiki?curid=21098058", "title": "List of software under the GNU AGPL", "text": "List of software under the GNU AGPL\n\nThis is an incomplete list of software that are licensed under the GNU Affero General Public License, in alphabetical order.\n"}
{"id": "36403736", "url": "https://en.wikipedia.org/wiki?curid=36403736", "title": "List of things named after Enrico Fermi", "text": "List of things named after Enrico Fermi\n\nEnrico Fermi (1901–1954), an Italian-born, naturalized American physicist, is the eponym of the topics listed below.\n\n\n\n\n"}
{"id": "37919559", "url": "https://en.wikipedia.org/wiki?curid=37919559", "title": "List of things named after Isaac Newton", "text": "List of things named after Isaac Newton\n\nThis is a list of things named after Isaac Newton.\n\n\n\n\n\n\n"}
{"id": "40951356", "url": "https://en.wikipedia.org/wiki?curid=40951356", "title": "Mouyassue virus", "text": "Mouyassue virus\n\nMouyassue virus is a novel, single-stranded, enveloped, negative-sense RNA orthohantavirus.\n\nThe banana pipistrelle (\"Neoromicia nanus\") found in the Côte d'Ivoire is the natural reservoir of Mouyassue virus. It shares a common lineage with the Magboi virus (MGBV) found in the hairy split-faced bat (\"Nycteris hispida\") in Sierra Leone \n"}
{"id": "21838255", "url": "https://en.wikipedia.org/wiki?curid=21838255", "title": "Multiple-prism dispersion theory", "text": "Multiple-prism dispersion theory\n\nThe first description of multiple-prism arrays, and multiple-prism dispersion, was given by Newton in his book \"Opticks\". Prism pair expanders were introduced by Brewster in 1813. A modern mathematical description of the single-prism dispersion was given by Born and Wolf in 1959. The generalized multiple-prism dispersion theory was introduced by Duarte and Piper in 1982.\n\nThe generalized mathematical description of multiple-prism dispersion, as a function of the angle of incidence, prism geometry, prism refractive index, and number of prisms, was introduced as a design tool for multiple-prism grating laser oscillators by Duarte and Piper, and is given by\n\nwhich can also be written as\n\nusing \n\nAlso,\n\nHere, formula_8 is the angle of incidence, at the \"m\"th prism, and formula_9 its corresponding angle of refraction. Similarly, formula_10 is the exit angle and formula_11 its corresponding angle of refraction. The two main equations give the first order dispersion for an array of \"m\" prisms at the exit surface of the \"m\"th prism. The plus sign in the second term in parentheses refers to a positive dispersive configuration while the minus sign refers to a compensating configuration. The \"k\" factors are the corresponding beam expansions, and the \"H\" factors are additional geometrical quantities. It can also be seen that the dispersion of the \"m\"th prism depends on the dispersion of the previous prism (\"m\" - 1).\n\nThese equations can also be used to quantify the angular dispersion in prism arrays, as described in Isaac Newton's book \"Opticks\", and as deployed in dispersive instrumentation such as multiple-prism spectrometers. A comprehensive review on practical multiple-prism beam expanders and multiple-prism angular dispersion theory, including explicit and ready to apply equations (engineering style), is given by Duarte.\n\nMore recently, the generalized multiple-prism dispersion theory has been extended to include positive and negative refraction. Also, higher order phase derivatives have been derived using a Newtonian iterative approach. This extension of the theory enables the evaluation of the Nth higher derivative via an elegant mathematical framework. Applications include further refinements in the design of prism pulse compressors and nonlinear optics.\n\nFor a single generalized prism (\"m\" = 1), the generalized multiple-prism dispersion equation simplifies to\n\nIf the single prism is a right-angled prism with the beam exiting normal to the output face, that is formula_10 equal to zero, this equation reduces to\n\nThe first application of this theory was to evaluate the laser linewidth in multiple-prism grating laser oscillators. The total intracavity angular dispersion plays an important role in the linewidth narrowing of pulsed tunable lasers through the equation\n\nwhere formula_16 is the beam divergence and the \"overall intracavity angular dispersion\" is the quantity in parentheses (elevated to –1). Although originally classical in origin, in 1992 it was shown that this laser cavity linewidth equation can also be derived from interferometric quantum principles.\n\nFor the special case of zero dispersion from the multiple-prism beam expander, the single-pass laser linewidth is given by\n\nwhere \"M\" is the beam magnification provided by the beam expander that multiplies the angular dispersion provided by the diffraction grating. In practice, \"M\" can be as high as 100-200.\n\nWhen the dispersion of the multiple-prism expander is not equal to zero, then the single-pass linewidth is given by\n\nwhere the first differential refers to the angular dispersion from the grating and the second differential refers to the overall dispersion from the multiple-prism beam expander (given in the section above).\n\nIn 1987 the multiple-prism angular dispersion theory was extended to provide explicit second order equations directly applicable to the design of prismatic pulse compressors.\nThe generalized multiple-prism dispersion theory is applicable to:\n\n\n\n"}
{"id": "31866393", "url": "https://en.wikipedia.org/wiki?curid=31866393", "title": "Pal Lahara State", "text": "Pal Lahara State\n\nPal Lahara (, was a princely state in what is today India during the British Raj. It had its capital at Pal Lahara. \n\nThe state had an area of and a population of 34,130 in 1892. In 1947 it was merged into independent India, becoming Dhenkanal District of Orissa State (now Odisha) in 1948.\n\nAccording to tradition, the first ruler was a Suryavanshi Rajput of Dharanagar named Santosh Pal. While returning from a pilgrimage to Puri he crossed this territory and made peace between two local tribal groups. As a result, Santosh Pal was elected as ruler by the Sabara people, one of the tribes involved in the conflict.\n\nThe rulers of the Pal Lahara princely state claimed to be descendants of Raja Bhoj of Dhar, from the ancient Rajput Paramara dynasty.\n\n\n"}
{"id": "25146654", "url": "https://en.wikipedia.org/wiki?curid=25146654", "title": "Peter von Glehn", "text": "Peter von Glehn\n\nPeter von Glehn (, \"Pyotr Petrovich Glen\"; in Jälgimäe, Governorate of Estonia, Russian Empire – in Saint Petersburg, Russian Empire) was a Russian botanist.\n\nPeter von Glehn was born to a Baltic German landowner, member of the Glehn family, Peter von Glehn (1796–1843) (he bought the Jälgimäe manor () in 1821) and Auguste Caroline Marie Burchart von Bellavary, member of the Burchardt family who took care of the Town Hall Pharmacy in Reval. He had 2 younger brothers: Nikolai (1841–1923), the founder of Nõmme, and Len von Glehn, (1844-1920), and 3 sisters: Marie Elisabeth (1840–????), Julie Wilhelmine (1842–1867) and Marie (1843–1884).\n\nHe graduated the University of Tartu with gold medal.\n\nGlehn's Spruce (\"Picea glehnii\"), a species of conifer in the Pinaceae family, is named after Peter von Glehn.\n\n"}
{"id": "25011", "url": "https://en.wikipedia.org/wiki?curid=25011", "title": "Plankton", "text": "Plankton\n\nPlankton are the diverse collection of organisms that live in large bodies of water and are unable to swim against a current. The individual organisms constituting plankton are called plankters. They provide a crucial source of food to many large aquatic organisms, such as fish and whales.\n\nThese organisms include bacteria, archaea, algae, protozoa and drifting or floating animals that inhabit—for example—the pelagic zone of oceans, seas, or bodies of fresh water. Essentially, plankton are defined by their ecological niche rather than any phylogenetic or taxonomic classification.\n\nThough many planktonic species are microscopic in size, \"plankton\" includes organisms over a wide range of sizes, including large organisms such as jellyfish.\nTechnically the term does not include organisms on the surface of the water, which are called \"pleuston\"—or those that swim actively in the water, which are called \"nekton\".\n\nThe name \"plankton\" is derived from the Greek adjective πλαγκτός (\"planktos\"), meaning \"errant\", and by extension, \"wanderer\" or \"drifter\", and was coined by Victor Hensen in 1887. While some forms are capable of independent movement and can swim hundreds of meters vertically in a single day (a behavior called diel vertical migration), their horizontal position is primarily determined by the surrounding water movement, and plankton typically flow with ocean currents. This is in contrast to nekton organisms, such as fish, squid and marine mammals, which can swim against the ambient flow and control their position in the environment.\n\nWithin the plankton, holoplankton spend their entire life cycle as plankton (e.g. most algae, copepods, salps, and some jellyfish). By contrast, meroplankton are only planktic for part of their lives (usually the larval stage), and then graduate to either a nektic (swimming) or benthic (sea floor) existence. Examples of meroplankton include the larvae of sea urchins, starfish, crustaceans, marine worms, and most fish.\n\nThe amount and distribution of plankton depends on available nutrients, the state of water and a large amount of other plankton.\n\nThe study of plankton is termed planktology and a planktonic individual is referred to as a plankter. The adjective \"planktonic\" is widely used in both the scientific and popular literature, and is a generally accepted term. However, from the standpoint of prescriptive grammar, the less-commonly used \"planktic\" is more strictly the correct adjective. When deriving English words from their Greek or Latin roots, the gender-specific ending (in this case, \"-on\" which indicates the word is neuter) is normally dropped, using only the root of the word in the derivation.\n\nPlankton are primarily divided into broad functional (or trophic level) groups: \nThis scheme divides the plankton community into broad producer, consumer and recycler groups. However, determining the trophic level of many plankton is not always straightforward. For example, although most dinoflagellates are either photosynthetic producers or heterotrophic consumers, many species perform both roles. In this mixed trophic strategy — known as mixotrophy — organisms act as both producers and consumers, either at the same time or switching between modes of nutrition in response to ambient conditions. For instance, relying on photosynthesis for growth when nutrients and light are abundant, but switching to predation when growing conditions are poor. Recognition of the importance of mixotrophy as an ecological strategy is increasing, as well as the wider role this may play in marine biogeochemistry.\n\nPlankton are also often described in terms of size. Usually the following divisions are used:\nHowever, some of these terms may be used with very different boundaries, especially on the larger end. The existence and importance of nano- and even smaller plankton was only discovered during the 1980s, but they are thought to make up the largest proportion of all plankton in number and diversity.\n\nThe microplankton and smaller groups are microorganisms and operate at low Reynolds numbers, where the viscosity of water is much more important than its mass or inertia.\n\nPlankton inhabit oceans, seas, lakes, ponds. Local abundance varies horizontally, vertically and seasonally. The primary cause of this variability is the availability of light. All plankton ecosystems are driven by the input of solar energy (but see chemosynthesis), confining primary production to surface waters, and to geographical regions and seasons having abundant light.\n\nA secondary variable is nutrient availability. Although large areas of the tropical and sub-tropical oceans have abundant light, they experience relatively low primary production because they offer limited nutrients such as nitrate, phosphate and silicate. This results from large-scale ocean circulation and water column stratification. In such regions, primary production usually occurs at greater depth, although at a reduced level (because of reduced light).\n\nDespite significant macronutrient concentrations, some ocean regions are unproductive (so-called HNLC regions). The micronutrient iron is deficient in these regions, and adding it can lead to the formation of phytoplankton blooms. Iron primarily reaches the ocean through the deposition of dust on the sea surface. Paradoxically, oceanic areas adjacent to unproductive, arid land thus typically have abundant phytoplankton (e.g., the eastern Atlantic Ocean, where trade winds bring dust from the Sahara Desert in north Africa).\n\nWhile plankton are most abundant in surface waters, they live throughout the water column. At depths where no primary production occurs, zooplankton and bacterioplankton instead consume organic material sinking from more productive surface waters above. This flux of sinking material, so-called marine snow, can be especially high following the termination of spring blooms.\n\nAside from representing the bottom few levels of a food chain that supports commercially important fisheries, plankton ecosystems play a role in the biogeochemical cycles of many important chemical elements, including the ocean's carbon cycle.\n\nPrimarily by grazing on phytoplankton, zooplankton provide carbon to the planktic foodweb, either respiring it to provide metabolic energy, or upon death as biomass or detritus. Organic material tends to be denser than seawater, so it sinks into open ocean ecosystems away from the coastlines, transporting carbon along with it. This process, called the \"biological pump\", is one reason that oceans constitute the largest carbon sink on Earth. However, it has been shown to be influenced by increments of temperature.\n\nIt might be possible to increase the ocean's uptake of carbon dioxide () generated through human activities by increasing plankton production through \"seeding\", primarily with the micronutrient iron. However, this technique may not be practical at a large scale. Ocean oxygen depletion and resultant methane production (caused by the excess production remineralising at depth) is one potential drawback.\n\nPhytoplankton absorb energy from the Sun and nutrients from the water to produce their own nourishment or energy. In the process of photosynthesis, phytoplankton release molecular oxygen () into the water as a waste biproduct. It is estimated that about 50% of the world's oxygen is produced via phytoplankton photosynthesis. The rest is produced via photosynthesis on land by plants. Furthermore, phytoplankton photosynthesis has controlled the atmospheric / balance since the early Precambrian Eon.\n\nThe growth of phytoplankton populations is dependent on light levels and nutrient availability. The chief factor limiting growth varies from region to region in the world's oceans. On a broad scale, growth of phytoplankton in the oligotrophic tropical and subtropical gyres is generally limited by nutrient supply, while light often limits phytoplankton growth in subarctic gyres. Environmental variability at multiple scales influences the nutrient and light available for phytoplankton, and as these organisms form the base of the marine food web, this variability in phytoplankton growth influences higher trophic levels. For example, at interannual scales phytoplankton levels temporarily plummet during El Niño periods, influencing populations of zooplankton, fishes, sea birds, and marine mammals.\n\nThe effects of anthropogenic warming on the global population of phytoplankton is an area of active research. Changes in the vertical stratification of the water column, the rate of temperature-dependent biological reactions, and the atmospheric supply of nutrients are expected to have important impacts on future phytoplankton productivity. Additionally, changes in the mortality of phytoplankton due to rates of zooplankton grazing may be significant.\n\nFreshly hatched fish larvae are also plankton for a few days, as long as it takes before they can swim against currents.\n\nZooplankton are the initial prey item for almost all fish larvae as they switch from their yolk sacs to external feeding. Fish rely on the density and distribution of zooplankton to match that of new larvae, which can otherwise starve. Natural factors (e.g., current variations) and man-made factors (e.g. river dams) can strongly affect zooplankton, which can in turn strongly affect larval survival, and therefore breeding success.\n\nThe importance of both phytoplankton and zooplankton is also well-recognized in extensive and semi-intensive pond fish farming. Plankton population based pond management strategies for fish rearing have been practised by traditional fish farmers for decades, illustrating the importance of plankton even in man-made environments.\n\n\n"}
{"id": "286230", "url": "https://en.wikipedia.org/wiki?curid=286230", "title": "Policy Analysis Market", "text": "Policy Analysis Market\n\nThe Policy Analysis Market (PAM), part of the FutureMAP project, was a proposed futures exchange developed by the United States' Defense Advanced Research Projects Agency (DARPA) and based on an idea first proposed by Net Exchange, a San Diego research firm specializing in the development of online prediction markets.\n\nPAM was to be \"a market in the future of the Middle East\", and would have allowed trading of futures contracts based on possible political developments in several Middle Eastern countries. The theory behind such a market is that the monetary value of a futures contract on an event reflects the probability that that event will actually occur, since a market's actors rationally bid a contract either up or down based on reliable information. One of the models for PAM was a political futures market run by the University of Iowa, which had predicted U.S. election outcomes more accurately than either opinion polls or political pundits. PAM was also inspired by the work of George Mason University economist Robin Hanson.\n\nAt a July 28, 2003, press conference, Senators Byron L. Dorgan (D-ND) and Ron Wyden (D-OR) claimed that PAM would allow trading in such events as \"coups d'état\", assassinations, and terrorist attacks, due to such events appearing on interface pictures on the project website.\n\nThey denounced the idea, with Wyden stating, \"The idea of a federal betting parlor on atrocities and terrorism is ridiculous and it's grotesque,\" while Dorgan called it \"useless, offensive and unbelievably stupid\". Other critics offered similar outrage. Within less than a day, the Pentagon announced the cancellation of PAM, and by the end of the week John Poindexter, head of the DARPA unit responsible for developing it, but better known for his role in the Iran–Contra affair, had offered his resignation; the PAM had first been proposed and funded in 2001, and Poindexter joined DARPA in December 2002. Robin Hanson claimed that Poindexter \"actually had little involvement with PAM\".\n\nCNN reported the program would be relaunched by the private firm, Net Exchange, which helped create it, but that the newer version \"will not include any securities based on forecasts of violent events such as assassinations or terror attacks\".\n\nOn June 11, 2007, Popular Science launched a similar program, known as the Popsci Predictions Exchange. Another project was the 'American Action Market' announced by Tad Hirsh of the MIT Media Lab in 2003, which would permit for-profit betting on major events.\n\nThere are now commercial policy analysis markets that perform this function. One such market, Intrade, had previously offered futures on events such as the capture of Osama bin Laden, the U.S. Presidential Election, and the bombing of Iran. As of March 10, 2013, all trading had been suspended on Intrade's website due to undisclosed financial irregularities.\n\n\n\"Was http://www.policyanalysis.org, no longer present.\"\n\n\n"}
{"id": "50900343", "url": "https://en.wikipedia.org/wiki?curid=50900343", "title": "Raymond L. Ethington", "text": "Raymond L. Ethington\n\nRaymond (Ray) Lindsay Ethington (born in 1929) is an American paleontologist. He works in the Geology department at the University of Missouri.\n\nHe was one of the Chief Panderers of the Pander Society, an informal organisation founded in 1967 for the promotion of the study of conodont palaeontology.\n\nIn 1983, with John E. Repetski, he described the conodont genus \"Rossodus\"\n\nIn 2007, he received the Raymond C. Moore Medal awarded by the Society for Sedimentary Geology to persons who have made significant contributions in the field which have promoted the science of stratigraphy by research in paleontology and evolution and the use of fossils for interpretations of paleoecology.\n\n"}
{"id": "28692", "url": "https://en.wikipedia.org/wiki?curid=28692", "title": "Sabermetrics", "text": "Sabermetrics\n\nSabermetrics is the empirical analysis of baseball, especially baseball statistics that measure in-game activity.\n\nSabermetricians collect and summarize the relevant data from this in-game activity to answer specific questions. The term is derived from the acronym SABR, which stands for the Society for American Baseball Research, founded in 1971. The term sabermetrics was coined by Bill James, who is one of its pioneers and is often considered its most prominent advocate and public face.\n\nHenry Chadwick, a sportswriter in New York, developed the box score in 1858. This was the first way statisticians were able to describe the sport of baseball. The creation of the box score has given baseball statisticians a summary of the individual and team performances for a given game. David Smith founded Retrosheet in 1989, with the objective of computerizing the box score of every major league baseball game ever played, in order to more accurately collect and compare the statistics of the game.\n\nSabermetrics research began in the middle of the 20th century. Earnshaw Cook was one of the earliest researchers who contributed to this idea. Cook gathered the majority of his research into his 1964 book, \"Percentage Baseball\". The book was the first of its kind to gain national media attention, although it was widely criticized and not accepted by most baseball organizations. The idea of advanced baseball statistics did not become prominent in the baseball community until Bill James began writing his annual \"Baseball Abstracts\" in 1977.\n\nBill James believed that people misunderstood how the game of baseball was played, claiming that it is actually defined by the conditions under which the sport is played. Sabermetricians, sometimes considered baseball statisticians, began trying to replace the longtime favorite statistic known as the batting average. It has been claimed that team batting average provides a relatively poor fit for team runs scored. Sabermetric reasoning would say that runs win ballgames, and that a good measure of a player's worth is his ability to help his team score more runs than the opposing team.\n\nBefore Bill James made the concept of sabermetrics known, Davey Johnson used an IBM System/360 at team owner Jerold Hoffberger's brewery to write a FORTRAN baseball computer simulation while playing for the Baltimore Orioles in the early 1970s. He used his results in an unsuccessful attempt to promote the idea that he should bat second in the lineup to his manager Earl Weaver. He wrote IBM BASIC programs to help him manage the Tidewater Tides, and after becoming manager of the New York Mets in 1984, he arranged for a team employee to write a dBASE II application to compile and store advanced metrics on team statistics. Craig R. Wright was another employee in Major League Baseball, working with the Texas Rangers in the early 1980s. During his time with the Rangers, he became known as the first front office employee in MLB history to work under the title Sabermetrician.\n\nThe Oakland Athletics began to use a more quantitative approach to baseball by focusing on sabermetric principles in the 1990s. This initially began with Sandy Alderson as the former general manager of the team when he used the principles toward obtaining relatively undervalued players. His ideas were continued when Billy Beane took over as general manager in 1997, a job he held until 2015, and hired his assistant Paul DePodesta. Through the statistical analysis done by Beane and DePodesta in the 2002 season, the Oakland A's went on to win 20 games in a row. This was a historic moment for the franchise, in which the 20th game was played at the Alameda County Coliseum. His approaches to baseball soon gained national recognition when Michael Lewis published \"\" in 2003 to detail Beane's use of Sabermetrics. In 2011, a film based on Lewis' book also called \"Moneyball\" was released to further provide insight into the techniques used in the Oakland Athletics' front office.\n\nSabermetrics was created in an attempt for baseball fans to learn about the sport through objective evidence. This is performed by evaluating players in every aspect of the game, specifically batting, pitching, and fielding. These evaluation measures are usually phrased in terms of either runs or team wins as older statistics were deemed ineffective.\n\nThe traditional measure of batting performance is considered to be the batting average. To calculate the batting average, the number of base hits was divided by the total number of at-bats. Bill James, along with other fathers of sabermetrics, proved this measure to be flawed as it ignores any other way a batter can reach base besides a hit. This led to the creation of the On-base percentage, which takes walks and hit-by-pitches into consideration. To calculate the On-Base percentage, the total number of hits + bases on balls + hit by pitch are divided by plate appearances.\n\nAnother flaw with the traditional measure of the batting average is that it will not take doubles, triples, and home runs into consideration and will give each hit the same value. Thus, a measure that will distinguish between these different hit outcomes, the slugging percentage, was created. To calculate the slugging percentage, the total number of bases of all hits is divided by the total numbers of time at bat. Stephen Jay Gould proposed that the disappearance of .400 batting average is actually a sign of general improvement in batting. This is because, in the modern era, players are becoming more focused on hitting for power than for average. Therefore, it has become more valuable to compare players using the slugging percentage and on-base percentage over the batting average.\n\nThese two improved sabermetric measures are important skills to measure in a batter and have been combined to create the modern statistic OPS. On-base plus slugging is the sum of the on-base percentage and the slugging percentage. This modern statistic has become useful in comparing players and is a powerful method of predicting runs scored from a certain player.\n\nSome of the other statistics that sabermetricians use to evaluate batting performance are weighted on-base average, secondary average, runs created, and equivalent average.\n\nThe traditional measure of pitching performance is considered to be the earned run average. It is calculated by dividing the number of earned runs allowed by the number of innings pitched and multiplying by nine because of the nine innings. This statistic provides the number of runs that a pitcher allows per game. It has proven to be flawed as it does not separate the ability of the pitcher from the abilities of the fielders that he plays with. Another classic measure for pitching is a pitcher's winning percentage. Winning percentage is calculated by dividing wins by the number of decisions (wins plus losses). This statistic can also be flawed as it is dependent on the pitcher's teammates' performances at the plate and in the field.\n\nSabermetricians have attempted to find different measures of pitching performance that does not include the performances of the fielders involved. This led to the creation of defense independent pitching statistics (DIPS) system. Voros McCracken has been credited with the development of this system in 1999. Through his research, McCracken was able to show that there is little to no difference between pitchers in the amount of hits they allow, regardless of their skill level. Some examples of these statistics are defense-independent ERA, fielding independent pitching, and defense-independent component ERA. Other sabermetricians have furthered the work in DIPS, such as Tom Tango who runs the \"Tango on Baseball\" sabermetrics website.\n\n\"Baseball Prospectus\" created another statistics called the peripheral ERA. This measure of a pitcher's performance takes hits, walks, home runs allowed, and strikeouts while adjusting for ballpark factors. Each ballpark has different dimensions when it comes to the outfield wall so a pitcher should not be measured the same for each of these parks.\n\nBatting average on balls in play (BABIP) is another useful measurement for determining pitcher's performance. When a pitcher has a high BABIP, they will often show improvements in the following season, while a pitcher with low BABIP will often show a decline in the following season. This is based on the statistical concept of regression to the mean. Others have created various means of attempting to quantify individual pitches based on characteristics of the pitch, as opposed to runs earned or balls hit.\n\nValue over replacement player (VORP) is considered a popular sabermetric statistic. This statistic demonstrates how much a player contributes to his team in comparison to a fake replacement player that performs below average. This measurement was founded by Keith Woolner, a former writer for the sabermetric group/website \"Baseball Prospectus\".\n\nWins above replacement (WAR) is another popular sabermetric statistic that will evaluate a player's contributions to his team. Similar to VORP, WAR compares a certain player to a replacement-level player in order to determine the number of additional wins the player has provided to his team. WAR values vary with hitting positions and are largely determined by a player's successful performance and their amount of playing time.\n\nMany traditional and modern statistics, such as ERA and Wins Shared, don't give a full understanding of what is taking place on the field. Simple ratios are not sufficient to understand the statistical data of baseball. Structured quantitative analysis is capable of explaining many aspects of the game, for example, to examine how often a team should attempt to steal.\n\nRelated rates can be used in baseball to give exact calculations of different plays in a game. For example, if a runner is being sent home from third, related rates can be used to show if a throw from the outfield would have been on time or if it was correctly cut off before the plate. Related rates also can aid in determining how fast a player can get around the bases after a batted ball, information that helps in the development of scouting reports and individual player development.\n\nMomentum and force is a similar application of calculus in baseball. Particularly, the average force on a bat while hitting a ball can be calculated by combining different concepts within applied calculus. First, the change in the ball's Momentum by the external force F(t) must be calculated. The momentum can be found by multiplying the mass and velocity. The external force F(t) is a continuous function of time\n\nSabermetrics can be used for multiple purposes, but the most common are evaluating past performance and predicting future performance to determine a player's contributions to his team. These may be useful when determining who should win end-of-the-season awards such as MVP and when determining the value of making a certain trade.\n\nMost baseball players tend to play a few years in the minor leagues before they are called up to the major league. The competitive differences coupled with ballpark effects make the exact comparison of a player's statistics a problem. Sabermetricians have been able to clear this problem by adjusting the player's minor league statistics, also known as the Minor-League Equivalency (MLE). Through these adjustments, teams are able to look at a player's performance in both AA and AAA to determine if he is fit to be called up to the majors.\n\nSabermetrics methods are generally used for three purposes:\n\n1. To compare key performances among certain specific players under realistic data conditions. The evaluation of past performance of a player enables an analytic overview. The comparison of this data between players can help one understand key points such as their market values. In that way, the role and the salary that should be given to that player can be defined.\n\n2. To provide prediction of future performance of a given player or a team. When past data is available about the performance of a team or a specific player, Sabermetrics can be used to predict the average future performances for the next season. Thus, a prediction can be made with a certain probability about the number of wins and loses.\n\n3. To provide a useful function of the player's contributions to his team. When analyzing data, one is able to understand the contributions a player makes to the success/failure of his team. Given that correlation, we can sign or release players with certain characteristics.\n\nA machine learning model can be built using data sets available at sources such as baseball-reference. This model will give probability estimates for the outcome of specific games or the performance of particular players. These estimates are increasingly accurate when applied to a large number of events over a long term. The game outcome (win/lose) is treated as having a binomial distribution. Predictions can be made using a logistic regression model with explanatory variables including: \n\nMany sabermetricians are still working hard to contribute to the field through creating new measures and asking new questions. Bill James' two \"Historical Baseball Abstract\" editions and \"Win Shares\" book have continued to advance the field of sabermetrics, 25 years after he helped start the movement. His former assistant Rob Neyer, who is now a senior writer at ESPN.com and national baseball editor of SBNation, also worked on popularizing sabermetrics since the mid-1980s.\n\nNate Silver, a former writer and managing partner of \"Baseball Prospectus\", invented PECOTA. This acronym stands for \"Player Empirical Comparison and Optimization Test Algorithm\", and is a sabermetric system for forecasting Major League Baseball player performance. This system has been owned by \"Baseball Prospectus\" since 2003 and helps the website's authors invent or improve widely relied upon sabermetric measures and techniques.\n\nBeginning in the 2007 baseball season, the MLB started looking at technology to record detailed information regarding each pitch that is thrown in a game. This became known as the PITCHf/x system which is able to record the speed of the pitch, at its release point and as it crossed the plate, as well as the location and angle of the break of certain pitches through video cameras. FanGraphs is a website that favors this system as well as the analysis of play-by-play data. The website also specializes in publishing advanced baseball statistics as well as graphics that evaluate and track the performance of players and teams.\n\n\n\n"}
{"id": "3050160", "url": "https://en.wikipedia.org/wiki?curid=3050160", "title": "Static universe", "text": "Static universe\n\nA static universe, also referred to as a \"stationary\" or \"infinite\" or \"static infinite\" universe, is a cosmological model in which the universe is both spatially infinite and temporally infinite, and space is neither expanding nor contracting. Such a universe does not have so-called spatial curvature; that is to say that it is 'flat' or Euclidean. A static infinite universe was first proposed by Thomas Digges (1546 .. 1595) .\n\nIn contrast to this model, Albert Einstein proposed a temporally infinite but spatially finite model as his preferred cosmology during 1917, in his paper \"Cosmological Considerations in the General Theory of Relativity\".\n\nAfter the discovery of the redshift–distance relationship (deduced by the inverse correlation of galactic brightness to redshift) by Vesto Slipher and Edwin Hubble, the astrophysicist and Roman Catholic priest Georges Lemaître interpreted the redshift as proof of universal expansion and thus a Big Bang, whereas Fritz Zwicky proposed that the redshift was caused by the photons losing energy as they passed through the matter and/or forces in intergalactic space. Zwicky's proposal would come to be termed 'tired light'- a term invented by the major Big Bang proponent Richard Tolman.\n\nDuring 1917, Albert Einstein added a positive cosmological constant to his equations of general relativity to counteract the attractive effects of gravity on ordinary matter, which would otherwise cause a static, spatially finite universe to either collapse or expand forever. \nThis model of the universe became known as the Einstein World or Einstein's static universe. \n\nThis motivation ended after the proposal by the astrophysicist and Roman Catholic priest Georges Lemaître that the universe seems to be not static, but expanding. Edwin Hubble had researched data from the observations made by astronomer Vesto Slipher to confirm a relationship between redshift and distance, which forms the basis for the modern expansion paradigm that was introduced by Lemaître. According to George Gamow this caused Einstein to declare this cosmological model, and especially the introduction of the cosmological constant, his \"biggest blunder\".\n\nEinstein's static universe is closed (i.e. has hyperspherical topology and positive spatial curvature), and contains uniform dust and a positive cosmological constant with value precisely formula_1, where formula_2 is Newtonian gravitational constant, formula_3 is the energy density of the matter in the universe and formula_4 is the speed of light. The radius of curvature of space of the Einstein universe is equal to\n\nThe Einstein universe is one of Friedmann's solutions to Einstein's field equation for dust with density formula_3, cosmological constant formula_7, and radius of curvature formula_8. It is the only non-trivial static solution to Friedmann's equations.\n\nBecause the Einstein universe soon was recognized to be inherently unstable, it was presently abandoned as a viable model for the universe. It is unstable in the sense that any slight change in either the value of the cosmological constant, the matter density, or the spatial curvature will result in a universe that either expands and accelerates forever or re-collapses to a singularity.\n\nAfter Einstein renounced his cosmological constant, and embraced the Friedmann-LeMaitre model of an expanding universe, most physicists of the twentieth century assumed that the cosmological constant is zero. If so (absent some other form of dark energy), the expansion of the universe would be decelerating. However, after Saul Perlmutter, Brian P. Schmidt, and Adam G. Riess introduced the theory of an accelerating universe during 1998, a positive cosmological constant has been revived as a simple explanation for dark energy.\n\nIn order for a static infinite universe model to be viable, it must explain three things:\n\nFirst, it must explain the intergalactic redshift. Second, it must explain the cosmic microwave background radiation. Third, it must have a mechanism to re-create matter (particularly hydrogen atoms) from radiation or other sources in order to avoid a gradual 'running down' of the universe due to the conversion of matter into energy in stellar processes. With the absence of such a mechanism, the universe would consist of dead objects such as black holes and black dwarfs.\n\n\n"}
{"id": "3180746", "url": "https://en.wikipedia.org/wiki?curid=3180746", "title": "Szilárd petition", "text": "Szilárd petition\n\nThe Szilárd petition, drafted by scientist Leo Szilard, was signed by 70 scientists working on the Manhattan Project in Oak Ridge, Tennessee, and the Metallurgical Laboratory in Chicago, Illinois. It was circulated in July 1945 and asked President Harry S. Truman to inform Japan of the terms of surrender demanded by the allies, and allow Japan to either accept or refuse these terms, before America used atomic weapons. However, the petition never made it through the chain of command to President Truman. It also was not declassified and made public until 1961.\n\nLater, in 1946, Szilard jointly with Albert Einstein, created the Emergency Committee of Atomic Scientists that counted among its board, Linus Pauling (Nobel Peace Prize in 1962).\n\nIn the spring of 1945, Szilard took the petition to the man who was soon to be named Secretary of State, James F. Byrnes, hoping to find someone who would pass on to President Truman the message from scientists that the bomb should not be used on a civilian population in Japan, and that after the war it should be put under international control in order to avoid a post-war arms race. Byrnes was not sympathetic to the idea at all. Szilard regretted that such a man was so influential in politics, and he appeared to also be despondent at having become a physicist, because in his career he had contributed to the creation of the bomb. After the meeting with Byrnes, he is quoted as having said, \"How much better off the world might be had I been born in America and become influential in American politics, and had Byrnes been born in Hungary and studied physics.\" In reaction to the petition, General Leslie Groves, the director of the Manhattan Project, sought evidence of unlawful behavior against Szilard. Most of the signers lost their jobs in weapons work.\n\nThe 70 signers at the Manhattan Project's Metallurgical Laboratory in Chicago, in alphabetical order, with their positions, were:\n\n\n"}
{"id": "21548766", "url": "https://en.wikipedia.org/wiki?curid=21548766", "title": "The Culture (series)", "text": "The Culture (series)\n\nThe \"Culture\" series is a science fiction series written by Scottish author Iain M. Banks. The stories centre on the Culture, a utopian, post-scarcity space society of humanoids, aliens, and very advanced artificial intelligences living in socialist habitats spread across the Milky Way galaxy. The main theme of the novels is the dilemmas that an idealistic hyperpower faces in dealing with civilizations that do not share its ideals, and whose behavior it sometimes finds repulsive. In some of the stories, action takes place mainly in non-Culture environments, and the leading characters are often on the fringes of (or non-members of) the Culture, sometimes acting as agents of Culture (knowing and unknowing) in its plans to civilize the galaxy.\n\nThe Culture is a society formed by various humanoid races and artificial intelligences about 9,000 years before the events of novels in the series. Since the majority of its biological population can have virtually anything they want without the need to work, there is little need for laws or enforcement, and the culture is described by Banks as space socialism. It features a post-scarcity economy where technology is advanced to such a degree that all production is automated. Its members live mainly in spaceships and other off-planet constructs, because its founders wished to avoid the centralised political and corporate power-structures that planet-based economies foster. Most of the planning and administration is done by Minds, very advanced AIs.\n\nAlthough the Culture has more advanced technology and a more powerful economy than the vast majority of known civilizations, it is just one of the \"Involved\" civilizations that take an active part in galactic affairs. The much older Homomda are slightly more advanced at the time of \"Consider Phlebas\" (this is, however, set several centuries before the other books, and Culture technology and martial power continues to advance in the interim); the Morthanveld have a much larger population and economy, but are hampered by a more restrictive attitude to the role of AI in their society. The capabilities of all such societies are vastly exceeded by those of the Elder civilisations (semi-retired from Galactic politics but who remain supremely potent) and the Sublimed, entities which have abandoned their material form for a non-corporeal, multi-dimensional existence, but these generally refrain from intervention in the material world.\n\nSome other civilizations hold less favorable views of the Culture. At the time of their war with the Culture, the Idirans and some of their allies regarded the control that the Minds exercised over the Culture as a form of idolatry. The Homomda regard the Culture as idealistic and hyper-active. Some members of the Culture have seceded to form related civilizations, known collectively as the Ulterior. These include the Peace Faction, the AhForgetIt Tendency and the Zetetic Elench. Others simply drop out temporarily or permanently.\n\n<onlyinclude>\n\n</onlyinclude>\n\nSince the Culture's biological population commonly live as long as 400 years and have no need to work, they face the difficulty of giving meaning to their lives when the Minds and other intelligent machines can do almost anything better than the biological population can. Many try—few successfully—to join Contact, the Culture's combined diplomatic / military / government service, and fewer still are invited to the even more elite Special Circumstances (SC), Contact's secret service and special operations division. Normal Culture citizens vicariously derive meaning from their existence via the works of Contact and SC. Banks described the Culture as \"some incredibly rich lady of leisure who does good, charitable works... Contact does that on a large scale.\" The same need to find a purpose for existence led the Culture as a whole to embark voluntarily on its only full-scale war, to stop the expansion of the theocratic and militaristic Idirans—otherwise the Culture's economic and technological advancement would have been a pointless exercise in hedonism.\n\nAll of the stories feature the tension between the Culture's humane, anarcho-communist ideals and its need to intervene in the affairs of less enlightened and often less advanced civilisations. The first Culture novel, \"Consider Phlebas\", describes an episode in the Idiran War, which the Culture's Minds foresaw would cause billions of deaths on both sides, but which their utilitarian calculations predicted would be the best course in the long term. The Idiran War serves as a recurring reference point in most of the subsequent novels, influencing the Culture's development for centuries and dividing its residents—both humanoids and AI Minds—along the pacifist and interventionist ideals.\n\nIn subsequent novels, the Culture—particularly SC and, to a lesser degree, Contact—continue to employ subterfuge, espionage, and even direct action (collectively called \"dirty tricks\") in order to protect itself and spread the Culture's \"good works\" and ideals. These dirty tricks include blackmailing persons, employing mercenaries, recruiting double agents, attempting to effect regime change, and even engaging in false flag operations against the Culture itself (potentially resulting in the death of billions). Though each of these individual actions would horrify the average Culture citizen, the Culture's Minds tend to justify these actions in terms of lives saved in the long-term, perhaps over the course of several hundred years. The Culture is willing to use not only preemptive, but also retaliatory actions in order to deter future hostile actions against itself. Banks commented that in order to prevent atrocities, \"even the Culture throws away its usual moral rule-book.\" Andrew M. Butler noted that, \"Having established the peaceful, utopian, game-playing tendencies of the Culture, ... in later volumes the Culture’s dirty tricks are more exposed.\"\n\nThe Culture stories have been described as \"eerily prescient\". \"Consider Phlebas\" explicitly presents a clash of civilizations, although this phrase was coined by Samuel P. Huntington in 1992. This is highlighted by the novel's description of the Idirans' expansion as a \"jihad\" and by its epigraphic verse from the Koran, \"Idolatry is worse than carnage\". However, it was as much a \"holy war\" from the Culture's point of view. Throughout the series, Contact and Special Circumstances show themselves willing to intervene, sometimes forcefully, in other civilizations to make them more Culture-like. \n\nMuch of \"Look to Windward\" is a commentary on the Idiran-Culture war, from a viewpoint 800 years later, mainly reflecting grief over both personal and large-scale losses and guilt over actions taken in the war. It combines these with similar reflections on the catastrophic miscarriage of the Culture's attempt to dissolve the Chelgrians' oppressive caste system. In neither case, however, does distress over the consequences of Culture policy lead its representatives to reject that policy. The book illustrates the limitations of power, and also points out that Minds and other AIs are as vulnerable as biological persons to grief, guilt and regrets.\n\nWhen the first Culture stories appeared, science fiction was dominated by cyberpunk, a pessimistic subgenre that worried about, but offered no solutions for, the offshoring of jobs to countries with lower costs or less strict regulations, the increasing power of corporations and the threats to privacy posed by computer networks. The Culture stories are space opera, with certain elements that are free from scientific realism, and Banks uses this freedom extravagantly in order to focus on the human and political aspects of his universe; he rejects the dystopian direction of present day capitalism, which both cyberpunk and earlier space operas assume, in creating a communist society as the primary civilization of focus. Space opera had peaked in the 1930s, but started to decline as magazine editors such as John W. Campbell demanded more realistic approaches. By the 1960s many space operas were satires on earlier styles, such as Harry Harrison's Stainless Steel Rat and Bill, the Galactic Hero stories, while televised and film space operas such as \"Star Trek\" and \"Star Wars\" were thought to have dumbed down the subgenre. The Culture stories did much to revive space opera.\n\nBanks has been described as \"an incorrigible player of games\" with both style and structure – and with the reader. In both the Culture stories and his work outside science fiction, there are two sides to Banks, the \"merry chatterer\" who brings scenes to life and \"the altogether less amiable character\" who \"engineers the often savage structure of his stories\". Banks uses a wide range of styles. \"The Player of Games\" opens in a leisurely manner as it presents the main character's sense of boredom and inertia, and adopts for the main storyline a \"spare, functional\" style that contrasts with the \"linguistic fireworks\" of later stories. Sometimes the styles used in \"Excession\" relate to the function and focal character of the scene: slow-paced and detailed for Dajeil, who is still mourning over traumatic events that happened decades earlier; a parody of huntin', shootin', and fishin' country gentlemen, sometimes reminiscent of P. G. Wodehouse, when describing the viewpoint of the Affront; the ship \"Serious Callers Only\", afraid of becoming involved in the conflict between factions of Minds, speaks in cryptic verse, while the \"Sleeper Service\", acting as a freelance detective, adopts a hardboiled style. On the other hand, Banks often wrong-foots readers by using prosaic descriptions for the grandest scenery, self-deprecation and humour for the most heroic actions, and a poetic style in describing one of the Affront's killings.\n\nHe delights in building up expectations and then surprising the reader. Even in \"The Player of Games\", which has the simplest style and structure of the series, the last line of the epilogue reveals who was really pulling the strings all along. In all the Culture stories, Banks subverts many clichés of space opera. The Minds are not plotting to take over the universe, and no-one is following a grand plan. The darkly comic double-act of Ferbin and Holse in \"Matter\" is not something most writers would place in \"the normally po-faced context of space opera\". Even the names of Culture spaceships are jokes – for example \"Lightly Seared on the Reality Grill\", \"Experiencing a Significant Gravitas Shortfall\" (part of a running gag in the series) and \"Liveware Problem\" (see liveware).\n\nBanks often uses \"outsiders\" as viewpoint characters, and said that using an enemy of the Culture as the main character of \"Consider Phlebas\", the first story in the series, enabled him to present a more rounded view of the Culture. However, this character realises that his attempts to plan for anything that might conceivably happen on a mission are very similar to the way in which the Culture makes all its decisions, and by the end suspects he has chosen the wrong side.\n\nThe focal character of \"The Player of Games\" is bored with the lack of real challenges in his life, is blackmailed into becoming a Culture agent, admires the vibrancy of the Azad Empire but is then disgusted by its brutality, and wins the final of the tournament by playing in a style that reflects the Culture's values.\n\n\"Use of Weapons\" features a non-Culture mercenary who accepts the benefits of association with the Culture, including immortality as the fee for his first assignment, and completes several dangerous missions as a Culture agent, but complains that he is kept in the dark about the aims of his missions and that in some of the wars he has fought maybe the Culture was backing both sides, with good reason.\n\n\"Look to Windward\" uses three commentators on the Culture, a near-immortal Behemothaur, a member of the race plunged into civil war by a Culture intervention that went wrong, and the ambassador of a race at similar technological level to the Culture's.\n\nThe action scenes of the Culture stories are comparable to those of blockbuster films. In an interview, Banks said he would like \"Consider Phlebas\" to be filmed \"with a very, very, very big budget indeed\" and would not mind if the story were given a happy ending, provided the biggest action scenes were kept. On the other hand, \"The Player of Games\" relies mainly on the psychological tension of the games by which the ruler of the Azad Empire is selected.\n\nBanks is unspecific about many of the background details in the stories, such as the rules of the game that is the centrepiece of \"The Player of Games\", and cheerfully makes no attempt at scientific credibility.\n\nBanks says he conceived the Culture in the 1960s, and that it is a combination of wish fulfilment and a reaction against the predominantly right-wing science fiction produced in the United States. In his opinion, the Culture might be a \"great place to live\", with no exploitation of people or AIs, and whose people could create beings greater than themselves.\n\nBefore his first published novel, \"The Wasp Factory\" (1984; not science fiction), was accepted in 1983, Banks wrote five books that were rejected, of which three were science fiction. In Banks' first draft of \"Use of Weapons\" in 1974, his third attempt at a novel, the Culture was just a backdrop intended to show that the mercenary agent was working for the \"good guys\" and was responsible for his own misdeeds. At the time he persuaded his friend Ken MacLeod to read it and MacLeod tried to suggest improvements, but the book had too much purple prose and a very convoluted structure. In 1984, shortly after \"The Wasp Factory\" was published, MacLeod was asked to read \"Use of Weapons\" again, and said there was \"a good novel in there struggling to get out\", and suggested the interleaved forwards and backwards narratives that appeared in the published version in 1990. The novella \"The State of the Art\", which provides the title of the 1991 collection, dates from 1979, the first draft of \"The Player of Games\" from 1980 and that of \"Consider Phlebas\" from 1982.\n\n\"Inversions\" won the 2004 Italia Science Fiction Award for the Best International Novel.\n\nThe American edition of \"Look to Windward\" was listed by the editors of SF Site as one of the \"Best SF and Fantasy Books of 2001\" after the UK edition had missed out by just one place the previous year.\n\n\"Use of Weapons\" was listed in Damien Broderick's book \"Science Fiction: The 101 Best Novels 1985-2010\".\n\nAs a posthumous tribute to Iain Banks, aerospace manufacturer SpaceX named two of its autonomous spaceport drone ships after sentient star ships Just Read the Instructions and Of Course I Still Love You which first appeared in the novel The Player of Games.\n\n"}
{"id": "15269528", "url": "https://en.wikipedia.org/wiki?curid=15269528", "title": "Thomas Carr (paleontologist)", "text": "Thomas Carr (paleontologist)\n\nThomas D. Carr is a vertebrate paleontologist who received his Ph.D. from the University of Toronto in 2005. He is now a member of the biology faculty at Carthage College in Kenosha, Wisconsin. Much of his work centers on tyrannosauroid dinosaurs. Carr published the first quantitative analysis of tyrannosaurid ontogeny in 1999, establishing that several previously-recognized genera and species of tyrannosaurids were in fact juveniles of other recognized taxa. Carr shared the Lanzendorf Prize for scientific illustration at the 2000 Society of Vertebrate Paleontology conference for the artwork in this article. In 2005, he and two colleagues described and named \"Appalachiosaurus\", a late-surviving basal tyrannosauroid found in Alabama. He is also scientific advisor to the Dinosaur Discovery Museum in Kenosha, Wisconsin.\n\n"}
{"id": "37991406", "url": "https://en.wikipedia.org/wiki?curid=37991406", "title": "Ulrike Felt", "text": "Ulrike Felt\n\nUlrike Felt (born 1957) is an Austrian social scientist, active in the field of Science and Technology Studies. Currently, she holds the chair for Social Studies of Science and is Dean of the Faculty of Social Sciences at the University of Vienna. From 2002 to 2007, she has been editor-in-chief of the journal “Science, Technology, & Human Values”.\n\nTrained as a physicist, she acquired her PhD in Physics at the University of Vienna in 1983. From 1983 until 1988, she was part of a research team investigating the history of the European High Energy Physics Lab (CERN) in Genève. Subsequently, she was part of the Department for the Philosophy and Social Studies of Science at the University of Vienna, which had been newly founded under the lead of Helga Nowotny, becoming an assistant professor in 1989. Since 1999, she is full Professor of Social Studies of Science. From 2004 to 2014, she was Head of the newly founded Department of Science and Technology Studies. She has held guest professorships at the Université du Québec à Montréal, the Université Louis Pasteur, Strasbourg, the ETH Zurich and visiting scholar at the STS group at Harvard. She has been part of numerous international professional committees and held many scientific advisory posts, among them being a member of the expert advisory group “Science and Society” for the European Unions 6th Framework Program, and has been co-director of the EC DG Research expert group on “Science and Governance”, from 2005 to 2007. She was the leading founder of the interdisciplinary Master program \"Science - Technology - Society\", which has been set up at the University of Vienna in 2009. She has been editor of the leading STS journal \"Science, Technology, & Human Values\" (SAGE) from 2002-2007 and has been the leading editor of the new Handbook of Science and Technology Studies (MIT Press, 2017). Since 2014 she is Dean of the Faculty of Social Sciences at the University of Vienna.\n\nUlrike Felt has published widely in different areas of Science, Technology, and Society. Throughout her work, questions of the public engagement with science and of science policy have been a major concern for Felt. Her work on public perception of different technologies, the organisation and reflection of different participatory events as well as on the complex relations of science and democracy has contributed in many innovative ways to the debates in STS and beyond.\n\nAn important line of her work has focused on changing modes of knowledge production within the sciences, and on how this impacts ways of working and living within research cultures. She has introduced the concept of “epistemic living spaces” in order to describe how the social and epistemic are co-produced within scientific work spaces:\n\n\"By epistemic living space, we mean researchers’ individual or collective perceptions and narrative re-constructions of the structures, contexts, rationales, actors and values which mould, guide and delimit their potential actions, both in what they aim to know as well as in how they act in social contexts in science and beyond.\" (Felt/Fochler 2010: 4f)\n\nUsing this concept, she has pointed to the potential implications of recent changes within career structures and the organization of the sciences for the knowledge produced within contemporary societies, focusing recently on changing temporal orders of research practices and policy.\n\nAnother line of her work has focused on how science and technology are embedded within local and national contexts. By introducing the notion of “technopolitical cultures” (Felt et al. 2010), Felt has pointed to the nationally distinct ways of how technoscience is entangled with cultural norms and values. Further, she is interested in how novel technologies like nano or genetic testing become imagined and integrated within specific local contexts. These questions are closely tied to her methodological interests. Felt and the Department of Social Studies of Science have engaged not only in the development of novel qualitative social science methods, but also in reflecting the performativity and politics of both participatory engagements and traditional socio-scientific methods.\n\nMore recently, and lying across her different research interests, she has been engaging with the role of changing temporal structures and the growing importance of future in shaping the interface of science, technology and society.\n\nFinally, since late 2015 she is leading a new interfaculty research platform at the University of Vienna \"Responsible Research and Innovation in Academic Practice\".\n\n\n"}
{"id": "25025505", "url": "https://en.wikipedia.org/wiki?curid=25025505", "title": "What Mad Pursuit", "text": "What Mad Pursuit\n\nWhat Mad Pursuit: A Personal View of Scientific Discovery is a book published in 1988 and written by Francis Crick, the English co-discoverer in 1953 of the structure of DNA. In this book, Crick gives important insights into his work on the DNA structure, along with the Central Dogma of molecular biology and the genetic code, and his later work on neuroscience.\n\nThe main purpose of Crick's book is to describe some of his experiences before and during the \"classical period\" of molecular biology from the 1953 discovery of the DNA double helix to the 1966 elucidation of the genetic code. There is a prologue outlining Crick's upbringing, education, and war work on magnetic and acoustic mines, and following World War II his decision on what branch of science to study, using the \"gossip test\". (Your interests are revealed by your gossip.) There is also an epilogue that outlines Crick's work after 1966, his move to the Salk Institute with his career transition to neuroscience concentrating on visual consciousness in primates, and some of his conclusions regarding research in theoretical biology, especially with regard to the brain sciences.\n\nCrick comments on various aspects of the DNA double helix discovery and gives a qualified endorsement to the 1987 television movie \"Life Story\" with Jeff Goldblum as Jim Watson and Tim Pigott-Smith as Francis Crick. There is a clear presentation of the basic ideas of molecular biology with appendices \"A Brief Outline of Classical Molecular Biology\" and \"The Genetic Code.\" Crick gives some anecdotes and explains some important ideas and insights without too much technical jargon.\n\nAccording to the Nobel prize-winning physicist Philip W. Anderson, the basic goal of experimental science is \"learning the truth about the world around us. Crick's words are as good a guide to that end as I have seen.\"\n\n\"This is a book to be read more than once; the beauty of its style masks much hard science and subtle thought. In spite of having heard it many times from others, the story of DNA as told by Crick still makes a marvelous read. A sense of clarity of thought combined with an equally strong sense of commitment and overlaid with the deep power of his thinking runs through the book. One sees that Crick possesses that all-important but dismayingly elusive knack of distinguishing what is significant from what is not. His confidence in the power of structural chemistry to unravel the functioning of biological molecule is unflagging. At the same time, warning signals sound constantly to keep possible evolutionary arbitrariness in mind\".\n"}
{"id": "4875266", "url": "https://en.wikipedia.org/wiki?curid=4875266", "title": "William Henry Harvey", "text": "William Henry Harvey\n\nWilliam Henry Harvey, FRS FLS (5 February 1811 – 15 May 1866) was an Irish botanist and phycologist who specialised in algae.\n\nHarvey was born at Summerville near Limerick, Ireland, in 1811, the youngest of 11 children. His father Joseph Massey Harvey, was a Quaker and prominent merchant. William started his education at Ballitore School in County Kildare and by the age of 15 had already established algae as his over-riding interest. After leaving school he joined the family business.\n\nHarvey was an authority on algae and bryophytes (mosses), and author of \"A Manual of the British Algae\" (1841), \"Phycologia Britannica\" (4 vols., 1846–51), \"Nereis Boreali-Americana.\" (3 parts 1852–85) and \"Phycologia Australica\" (5 vol., 1858–63). He spent several years in South Africa, and was the author, with German botanist Otto Wilhelm Sonder, of the \"Flora Capensis\" (7 vol. in 11, 1859 – 1933). Harvey's main algal herbarium is located at Trinity College, Dublin.\n\nHarvey's discovery in 1831 of the moss \"Hookeria laetevirens\" at Killarney, new to Ireland, led to a lifelong friendship with Sir William Jackson Hooker, who was then Regius Professor of Botany at Glasgow University. Hooker recognised the talent of the young man and lent him books and specimens. Soon afterwards Hooker invited him to contribute the section on algae to his \"British Flora\" (1833) as well as the section on algae for \"The Botany of Captain Beechy's Voyage\".\nIn 1835 Harvey went to South Africa aboard the vessel \"Carnatic\", with his brother Joseph who had been mistakenly nominated as Colonial Treasurer by Thomas Spring Rice instead of William. When Joseph's health failed in the following year, William took over his duties. They left for Britain together on 14 April 1836 and Joseph died on the voyage.\n\nBack in Cape Town, and now officially Treasurer-General, William took up residence at Bishop's Court, rising before dawn every day, collecting in the mountains or sea-shore, and working on the plants at night. In March 1837 he wrote: 'I have taken so many excursions lately that I almost fear I shall earn the sobriquet of Her Majesty's pleasurer general'. In the same year he enlisted the services of botanical collector Karl Zeyher, who was in Uitenhage, to collect specimens. He developed a close friendship with Baron von Ludwig who had started his famous gardens in Cape Town, and dedicated his \"Genera of South African Plants\" to him. Under the patronage of Sir George Grey and with the assistance of a team of collectors and of Otto Wilhelm Sonder, he set about writing a \"Flora Capensis\" in English – he lived long enough to see the first three volumes completed and published in Dublin, the third in 1865. He came home in 1842, having resigned his position due to illness.\n\nIn 1844 Harvey became curator of the Trinity College Herbarium (TCD) and in 1848 Professor of Botany of the Royal Dublin Society.\n\nIn 1853 he made a three-year voyage, visiting South Africa, Ceylon, Australia, New Zealand, Tonga, Fiji, and Chile. On his return he published further important books dealing with the botany of North America and South Africa and in 1858 was appointed Professor of Botany at Trinity College, Dublin.\n\nHe died from tuberculosis on 15 May 1866 at Torquay and was buried there.\n\nAs a result of the publication of his 1858 book, \"The genera of South African plants\", in which he asked South African readers to send him specimens so that he could begin documenting the flora of the Cape, he began a correspondence with Mary Elizabeth Barber, an amateur naturalist who lived in Cape Colony. Their ongoing correspondence took place during a time when it was not generally accepted for women to engage in scientific discussion; indeed, in the beginning Barber did not disclose the fact that she was a woman. Barber became one of Harvey's main suppliers of plants from South Africa and also assisted him in the naming and classification of numerous species. Over a nearly 30-year correspondence, she sent Harvey approximately 1,000 species with notes on each one.\n\nHarvey described over 750 species and in excess of 75 genera of algae.\n\nHis \"Phycologia Britannica\" was published in 1846–1851 and his publication of \"Nereis Australis Or Algae of the Southern Ocean\" (1847–49) along with other publications established his reputation. His \"Phycologia Australica\" represents one of the most important books on phycology in the 19th century. Published in five volumes between 1858 and 1863 it is the result of his extensive collecting on the Australian shores.\n\nBy the time Harvey set foot in Western Australia he had already established himself as a leading phycologist having published several large works. He earned the title: \"father of Australian Phycology\". He was elected as a Fellow of the Royal Society in 1858.\n\nAbout 600 specimens from Ireland, Ceylon, Friendly Islands, Australia and Tasmania collected by Harvey are in store in the Ulster Museum Herbarium (BEL), almost 90 of which are in the 5th volume of the William Thompson collection in the Ulster Museum, catalogue numbers: F8848–F8937. However his primary collection is still in the TCD Herbarium attached to Botany School building of Trinity College. There are also collections of Harvey's specimens in: The former Botany Department of University College, Cork, Ireland; West Chester, Pennsylvania, USA; National Herbarium of Victoria (MEL), Melbourne, Australia; National Herbarium of New South Wales (NSW), Sydney, Australia and the Herbarium of St. Andrews University (STA).\n\nIn Harvey's era naturalists often relied upon the exchanging of specimens with other scientists and contributions by amateur collectors. His 1841 \"Manual of the British Algae\" was dedicated to British beachcomber, Amelia Griffiths. In his \"Phycologia Britannica\" Harvey often notes the \"distribution\" of each species giving the name of the collector who reported the record. In the \"Preface\" of Vol. 1 he lists 19 people to whom he is indebted. These include: Rev. Mr. Pollexfen and Dr. McBain for Orkney algae, the others are: Rev. Mr.Hore, Dr.Cocks, Mr. Rohloff, Mr. Boswarva, Miss White, Miss Magdalene Turner, Miss Warren, Miss (Anne) Ball, Miss (Isabella) Gifford (1823?–1891)(4), Miss Cutler (1), Mrs Gatty (1809–1873), Mrs Gulson (?–1871)(5), Mrs Hayden, Rev. Dr. Landsborough, Dr. Dickie (2), Mr. Ralfs and Mr. Cresswell. Others noted in volume 1 include: Mr. Winch, Mr. McCalla (c.1814–1849)(3), Mr. Wigg, Mr. Borrer, Miss Hutchins, Mr. John Templeton, Mr. T.N.Cole, Rev. Mr. Clouston, Rev. H. Davies (Mr.) Stackhouse, Mrs. Ovens, Mr. W. Backhouse, James Dr. P. Neill and others. Harvey recognised Magdalene Turner's help named \"Cladophora magdalenae\" Harv. in her honour. Harvey also honoured Susan Fereday's contribution to his work by naming the species \"Dasya feredayae\" and \"Nemastoma feredayae\" after her.\n\nThis botanist is denoted by the author abbreviation Harv. when citing a botanical name.\n\nSpecimens of some of these collectors are to be found in the Ulster Museum Herbarium (BEL):\n\nGeorge Clifton (1823–1913) Mr G. Clifton is mentioned in Harvey's Memoirs, as the Superintendent of the Water Police in Perth, West Australia whose boat Harvey used when collecting in Fremantle (Blackler, 1977). Some of his specimens are in the Ulster Museum Herbarium: BEL: F2195; F2196 from \"W.Australia.\"\n\nRonald Campbell Gunn (1808–1881) Harvey's specimens in the Ulster Museum are from George Town. The handwriting has been determined by Dr H. B. S. Womersley (1980): F2256; F2242; F2083; F2081 and others.\n\nHarvey was an honorary M.D. of Dublin University (1844) and F.R.S. (1858). His portrait is in the National Gallery of Ireland, Dublin.\n\n\n\n\n"}
{"id": "1705511", "url": "https://en.wikipedia.org/wiki?curid=1705511", "title": "World Reference Base for Soil Resources", "text": "World Reference Base for Soil Resources\n\nThe World Reference Base for Soil Resources (WRB) is an international soil classification system for naming soils and creating legends for soil maps. The currently valid version is the Update 2015 of the third edition 2014. It is edited by a working group of the International Union of Soil Sciences (IUSS).\n\nSince the 19 century, several countries developed national soil classification systems. During the 20 century, the need for an international soil classification system became more and more obvious. \n\nFrom 1971 to 1981, the Food and Agriculture Organization (FAO) and UNESCO published the Soil Map of the World, 10 volumes, scale 1 : 5 M. The Legend for this map, published in 1974 under the leadership of Rudi Dudal, became the FAO soil classification. Many ideas from national soil classification systems were brought together in this worldwide-applicable system, among them the idea of diagnostic horizons as established in the ‘7 approximation to the USDA soil taxonomy’ from 1960. The next step was the Revised Legend of the Soil Map of the World, published in 1988.\n\nIn 1982, the International Soil Science Society (ISSS; now: International Union of Soil Sciences, IUSS) established a working group named International Reference Base for Soil Classification (IRB). Chair of this working group was Ernst Schlichting. Its mandate was to develop an international soil classification system that should better consider soil-forming processes than the FAO soil classification. Drafts were presented in 1982 and 1990.\n\nIn 1992, the IRB working group decided to develop a new system named World Reference Base for Soil Resources (WRB) that should further develop the Revised Legend of the FAO soil classification and include some ideas of the more systematic IRB approach. Otto Spaargaren (International Soil Reference and Information Centre) and Freddy Nachtergaele (FAO) were nominated to prepare a draft. This draft was presented at the 15 World Congress of Soil Science in Acapulco in 1994. At the same congress, the WRB was established as an ISSS working group replacing the IRB. At the 16 World Congress of Soil Science in Montpellier in 1998, the first edition of the WRB was published. At the same congress, the ISSS endorsed the WRB as its correlation system for soil classification. (In 2014, the USDA soil taxonomy also received the status of a correlation system.) At the 18 World Congress of Soil Science in Philadelphia in 2006, the second edition of the WRB was presented, and at the 20 World Congress of Soil Science in Jeju in 2014, the third edition. An update of the third edition was issued in 2015. Whereas the second edition was only suitable for naming soils, the third edition can additionally be used for creating map legends.\n\nThe WRB has only two hierarchical levels (see below) and has in that sense a similar approach as the French référencial pédologique (1992, 1995, 2008). Contrary to that, the USDA soil taxonomy is strongly hierarchical and has six levels. The classification in WRB is based mainly on soil morphology (field and laboratory data) as an expression of pedogenesis. Another difference with USDA soil taxonomy is that soil climate is regarded only as a soil-forming factor and not as a soil characteristic. The WRB is not meant to replace national soil classification systems, which, for their area, may be more detailed than the WRB.\n\nThe WRB is edited by a working group of the International Union of Soil Sciences (IUSS). The current chair of the working group is Peter Schad (Technical University of Munich, Germany, since 2010). The current vice-chair is Stephan Mantel (International Soil Reference and Information Centre, The Netherlands, since 2018). \n\nChairs of the WRB working group and responsible first authors of the WRB editions are: Seppe Deckers (Belgium, 1 edition 1998), Erika Michéli (Hungary, 2 edition 2006) and Peter Schad (Germany, 3 edition 2014). \n\nThe WRB working group has a homepage that is currently hosted by the Chair of Soil Science of the Technical University of Munich. It provides the following: \n\n\nThe classification is based on diagnostic horizons, diagnostic properties and diagnostic materials, altogether called diagnostics. Diagnostic materials are materials that significantly influence soil-forming processes (pedogenesis). They are either inherited from the parent material or the result of soil-forming processes. Diagnostic properties are typical results of soil-forming processes or reflect specific conditions of soil formation. Diagnostic horizons are typical results of soil-forming processes that show a minimum thickness and therefore a horizontal appearance. \n\nThe diagnostics have names (e. g. argic horizon, stagnic properties, fluvic material), The WRB does not use horizons symbols (A horizons, B horizons). Therefore, horizons that are not diagnostic, do not have names. Instead, WRB recommends using the horizon symbols provided by the FAO Guidelines for Soil Description (2006). \n\nThe classification comprises two levels: \n\nThe first level has 32 Reference Soil Groups (RSGs).\n\nAt the second level, for further differentiation a set of qualifiers is added to the name of the RSG. There are 185 qualifiers in total. For every RSG, there is a list of available qualifiers, which are subdivided into two types: \n\n\nThe names of the RSGs and the qualifiers start with capital letters. They must be given in English and must not be translated into any other language in order to guarantee that a certain soil has the same name all over the world. \n\nA key is used for allocating a soil to a certain RSG. In a defined sequence, the key asks for the presence or absence of certain diagnostics in a certain depth range. In addition, the key asks for single characteristics, e. g., a certain clay content or a certain base saturation. The soil belongs to the first RSG, for which it fulfils the set of criteria.\n\nThe qualifiers available for use with a particular RSG are listed in the key, along with the RSG. Their number is from 35 to 68. All applying qualifiers must be added to the soil name. The principal qualifiers are added before the name of the RSG. The sequence is from right to left, i.e. the uppermost qualifier in the list is placed closest to the name of the RSG. The supplementary qualifiers are added in brackets after the name of the RSG and are separated from each other by commas. The sequence is from left to right, i.e. the first qualifier according to the alphabet is placed closest to the name of the RSG. If no other principal qualifier applies, the Haplic qualifier is used. If two or more qualifiers in the list are separated by a slash (/) only one of them can be used. The slash signifies that these qualifiers are either mutually exclusive (e.g. Dystric and Eutric) or one of them is redundant with the redundant qualifier(s) listed after the slash(es). In the soil name, supplementary qualifiers are always placed in the order of the alphabet, even if their position in the list differs from alphabetical sequence due to the use of the slash. It is a general rule that qualifiers conveying redundant information are not used. Example: If a soil has the Calcaric qualifier (carbonates present) the Eutric qualifier (high base saturation) is not used.\n\nQualifiers may be combined with specifiers (e.g. Epi-, Proto-) to form subqualifiers (e.g. Epiarenic, Protocalcic). The depth-related specifiers are of special importance, although their use is optional:\n\n\nThe number of qualifiers used in a map legend depends on the scale. The WRB distinguishes four map scale levels: \n\n\nCorrelating the map scale levels with concrete scales (e.g. fourth map scale level from 1 : 250 000 to 1 : 1 000 000) is difficult because selecting a map scale level depends very much from the homogeneity/heterogeneity of the landscape.\n\nThe principal qualifiers are added before the name of the RSG following the rules explained for naming a soil. Depending on the purpose of the map or according to national traditions, at any scale level, further qualifiers may be added optionally. They may be additional principal qualifiers from further down the list and not already used in the soil name, or they may be supplementary qualifiers. They are placed using the above-mentioned rules for supplementary qualifiers; principal qualifiers first, then supplementary qualifiers.\n\nThe WRB recommends that on a map unit not just one soil is indicated but an association of soils. For this purpose, WRB uses the following nomenclature: \n\n\nFor codominant and associated soils, it is allowed to use less principal qualifiers than would correspondent to the used map scale level. The use of specifiers is not recommended due to the generalization that is required when making maps. In map legends, the names of the RSGs are given in plural; in all other cases they are given in singular. \n\nThe WRB Manual comprises five chapters and four annexes. \n\nChapter 1 reports on background and basics. It includes tables of the diagnostic horizons and of the RSGs. The latter is given below. Chapter 2 provides the rules for classifying soils and creating map legends. It is highly recommended to read this short chapter before using the WRB. Chapter 3 presents the diagnostic horizons, properties and materials, each with a general description, the diagnostic criteria and some additional information. For the decision, whether a diagnostic is present or absent in a soil, only the diagnostic criteria are relevant. Chapter 4 provides the key to the RSGs and for every RSG a list with the available principal and supplementary qualifiers. Chapter 5 gives the definitions of the qualifiers. These five chapters are concluded with a list of references. \n\nThey are followed by four annexes. Annex 1 briefly describes the 32 RSGs. Annex 2 lists the laboratory methods. This is only a list; it is not a laboratory manual. Annex 3 gives the codes for the RSGs, the qualifiers and the specifiers and the rules for the sequence of the codes for naming soils and creating map legends. Annex 4 provides a texture triangle, in which the ranges of the texture-related qualifiers are marked with different grey shades. \n\nThis is the list of the 32 Reference Soil Groups in the sequence of the key (Chapter 4 of the WRB Manual), including the codes (Annex 3 of the WRB Manual). This list is mainly taken from Table 2 (Chapter 1) of the WRB Manual.\n\nSoils with thick organic layers \n\n\nSoils with strong human influence \n\n\nSoils with limitations to root growth \n\n\nSoils distinguished by Fe/Al chemistry \n\n\nPronounced accumulation of organic matter in the mineral topsoil \n\n\nAccumulation of moderately soluble salts or non-saline substances \n\n\nSoils with clay-enriched subsoil \n\n\nSoils with little or no profile differentiation \n\n\nOur example soil has the following characteristics: \n\nField characteristics: A soil developed from loess shows a marked clay increase in around 60 cm depth and clay coatings in the clay-richer horizon. According to the landscape setting, we presume that high-activity clays dominate. In the field, a pH value of 6 is measured in the subsoil. The lower part of the clay-poorer topsoil is bleached. In the clay-richer horizon, we observe a mottling; the oximorphic and the reductimorphic colours sum up to 30% of the exposed area, the intensive colours in the interiors of the aggregates. In spring time, reducing conditions occur. The soil is ploughed regularly. Organic matter concentrations in the topsoil are small.\n\nLaboratory characteristics: The laboratory analyses confirm the high cation exchange capacity per kg clay in the clay-richer horizon and the high base saturation in the subsoil. In the topsoil, we find 20% clay, 10% sand and 70% silt, in the subsoil 35% clay, 10% sand and 55% silt. \n\nThe naming of the soil consists of four steps. \n\nQuestion 1: Does the soil have diagnostic horizons, properties and materials? \n\nThe soil has the following diagnostics: \n\n\nQuestion 2: To which RSG does the soil belong? \n\nWe have to go through the key RSG for RSG. This soil is not a Histosol, not an Anthrosol, not a Technosol etc. Finally, we end up with the Luvisols. This is the first RSG in the key, the criteria of which our soil completely fulfils. \n\nQuestion 3: Which qualifiers apply? \n\nFrom the list of the principal qualifiers, Stagnic (stagnic properties und reducing conditions) and Albic (light colours) apply. Stagnic is found further up in the list. Therefore, the soil has to be named up till now Albic Stagnic Luvisol. From the list of the supplementary qualifiers, Siltic (silty from 0 to 60 cm), Loamic (loamy from 60 cm downwards), Aric (ploughed), Cutanic (clay coatings) und Ochric (small concentrations of organic carbon) apply. Bringing the supplementary qualifiers into the alphabetical order, the soil is an Albic Stagnic Luvisol (Aric, Cutanic, Loamic, Ochric, Siltic). \n\nQuestion 4: Which specifiers can be used to form subqualifiers? \n\nThe soil is Siltic from 0 to 60 cm and Loamic from 60 cm downwards. We can use the depth-related specifiers Ano- and Endo- to construct the subqualifiers Anosiltic and Endoloamic. The stagnic properties occur only in the subsoil and the albic material only around 50 cm. This means that we can use the subqualifiers Endostagnic and Amphialbic. \n\nNow, the soil name is: Amphialbic Endostagnic Luvisol (Aric, Cutanic, Endoloamic, Ochric, Anosiltic). \n\nUsing the codes of Annex 3 of the WRB Manual gives us the following short name: LV-stn.abm-ai.ct.lon.oh.sia. \n\nLet’s say that our example soil Amphialbic Endostagnic Luvisol (Aric, Cutanic, Endoloamic, Ochric, Anosiltic) covers 60% of the area of a map unit. The other 40% are covered by a Eutric Endoluvic Amphialbic Stagnosol (Humic, Endoloamic, Anosiltic). The map unit will be named as follows: \n\nFirst map scale level: \n\n\nSecond map scale level: \n\n\nThird map scale level: \n\n\nFourth map scale level: \n\n\nRemarks: The use of the depth-related specifiers is not recommended in map legends, where generalization is required. The fourth scale level would allow three principal qualifiers, but the dominant soil in our example has only two. \n\nAt every scale level, optional qualifiers may be added. If one wants to give information about organic carbon, one can do that even at the first map scale level and write: \n\n\nIf somebody wants to give additional information on soil genesis, this can also be done on the first map scale level: \n\n\nBoth in combination would read, e. g., at the second map scale level: \n\n\n\n\n"}
