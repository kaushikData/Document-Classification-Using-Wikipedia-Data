{"id": "42787857", "url": "https://en.wikipedia.org/wiki?curid=42787857", "title": "Acatinga virus", "text": "Acatinga virus\n\nΤhe Acatinga virus (ACTV) is a probable species in the genus Orbivirus, isolated from phlebotomine sandflies in the Amazon region of Brazil. Antigenically related to Changuinola virus. This virus have not reported to cause disease in humans.\n"}
{"id": "38882666", "url": "https://en.wikipedia.org/wiki?curid=38882666", "title": "Adelmar Faria Coimbra-Filho", "text": "Adelmar Faria Coimbra-Filho\n\nAdelmar Faria Coimbra-Filho (June 4, 1924 – June 27, 2016) was a Brazilian biologist and primatologist. He is a pioneer in studies of and conservation of lion tamarins. He is founder and Former Director of the Rio de Janeiro Primate Centre. Coimbra Filho's titi is named after him.\n\nCoimbra-Filho was born in Fortaleza. He began his career in 1947. He rediscovered the black lion tamarin, and assisted in conservation of the golden lion tamarin through a zoo-based breeding program in collaboration with Devra G. Kleiman.\n\nHis awards and honors include the Augusto Ruschi Award from the Brazilian Academy of Sciences.\n\n"}
{"id": "10969318", "url": "https://en.wikipedia.org/wiki?curid=10969318", "title": "Adjacency pairs", "text": "Adjacency pairs\n\nIn linguistics, an adjacency pair is an example of conversational turn-taking. An adjacency pair is composed of two utterances by two speakers, one after the other. The speaking of the first utterance (the first-pair part, or the first turn) provokes a responding utterance (the second-pair part, or the second turn).\n\nFor example, a question such as \"\"What's your name?\" requires the addressee to provide an answer in the following turn, thus completing the adjacency pair. A satisfactory response could be \"Jennifer\". To provide an irrelevant response, or to fail to complete the pair, is noticed as a breach of conversational maxim. A reply like \"I'm allergic to shellfish\" would not satisfy the adjacency pair, as it violates Grice's conversational maxim of relevance.\n\nMany actions in conversation are accomplished through established adjacency pairs, examples of which include:\n\n\n"}
{"id": "17640895", "url": "https://en.wikipedia.org/wiki?curid=17640895", "title": "Alate", "text": "Alate\n\nAlate is an adjective that refers to wings or winglike structures. In entomology it usually refers to the winged form of a social insect, especially ants or termites, though can also be applied to aphids and some thrips. Alate females are typically those destined to become gynes (queens), whereas alate males are occasionally referred to as \"drones\" (or \"kings\", in the case of termites). However, the existence of reproductives that do \"not\" have wings (e.g., ergatoid queens and gamergates) necessitates a term to distinguish the winged from the wingless reproductive forms. This is an example of polymorphism associated with eusociality. A \"dealate\" is an adult insect that shed or lost its wings (\"dealation\").\nIn botany alate refers to winglike structures on some seeds that use wind dispersal or it may be used to describe flattened ridges which run longtitudianally on stems.\n"}
{"id": "28752081", "url": "https://en.wikipedia.org/wiki?curid=28752081", "title": "Analytical sociology", "text": "Analytical sociology\n\nAnalytical sociology is a strategy for understanding the social world. It is concerned with explaining important macro-level facts such as the diffusion of various social practices, patterns of segregation, network structures, typical beliefs, and common ways of acting. It explains such facts not merely by relating them to other macro-level facts, but by detailing in clear and precise ways the mechanisms through which they were brought about. This is accomplished by a detailed focus on individuals’ actions and interactions, and the use of state-of-the-art simulation techniques to derive the macro-level outcomes that such actions and interactions are likely to bring about. Analytical sociology can be seen as contemporary incarnation of Robert K. Merton's well-known notion of middle-range theory.\n\nThe analytical approach is founded on the premise that proper explanations detail the \"cogs and wheels\" through which social outcomes are brought about, and it is driven by a commitment to realism. Empirically false assumptions about human motivation, cognitive processes, access to information, or social relations cannot bear the explanatory burden in a mechanistic explanation no matter how well they predict the outcome to be explained.\n\nWith its focus on the macro-level outcomes that individuals in interaction with one another bring about, analytical sociology is part of the \"complexity turn\" within sociology. Until very recently sociologists did not have the tools needed for analyzing the dynamics of complex systems, but powerful computers and simulation software have changed the picture considerably. So-called agent-based computer simulations are transforming important parts of sociology (as well as many other parts of the social and natural sciences) because they allow for rigorous theoretical analyses of large complex systems. The basic idea behind such analyses is to perform virtual experiments reflecting the analyst’s theoretical ideas and empirically-based knowledge about the social mechanisms influencing the action and interaction of the individuals. The key is to identify the core mechanisms at work, assemble them into a simulation model, and establish the macro-level outcomes the individuals bring about when acting and interacting in accordance with these mechanisms.\n\nContemporary scholars working in this tradition include Peter Bearman, Peter Hedström, Michael Macy, and Gianluca Manzo. The work of James Coleman, Jon Elster, Robert Merton, Thomas Schelling and Raymond Boudon was of pivotal importance for the development of the analytical approach.\n\n\n"}
{"id": "14450419", "url": "https://en.wikipedia.org/wiki?curid=14450419", "title": "Anthropogenic biome", "text": "Anthropogenic biome\n\nAnthropogenic biomes, also known as anthromes or human biomes, describe the terrestrial biosphere in its contemporary, human-altered form using global ecosystem units defined by global patterns of sustained direct human interaction with ecosystems. Anthromes were first named and mapped by Erle Ellis and Navin Ramankutty in their 2008 paper, \"Putting People in the Map: Anthropogenic Biomes of the World\". Anthrome maps now appear in numerous textbooks and in the National Geographic World Atlas\n\nFor more than a century, the biosphere has been described in terms of global ecosystem units called biomes, which are vegetation types like tropical rainforests and grasslands that are identified in relation to global climate patterns. Considering that human populations and their use of land have fundamentally altered global patterns of ecosystem form, process, and biodiversity, anthropogenic biomes provide a framework for integrating human systems with the biosphere in the Anthropocene.\n\nHumans have been altering ecosystems since we have evolved. Evidence suggests that our ancestors were burning land to clear it at one million years ago. 600,000 years ago, humans were using spears to kill horses and other large animals in Great Britain and China. For the past tens of thousands of years, humans have greatly changed the plant and animal life around the globe, from what type of wildlife and plant life dominated to what type of ecosystems dominate. Examples include Native Americans; they altered the forest, burnt land to clear it, settled in cities, disrupting forests and other ecosystems, and built monuments that required moving large amounts of earth, such as the Cahokia Monuments. More examples are the civilizations of the ancient world; they mined large amounts of material, made roads, and especially for the Romans, when mining lead, released large amounts of Mercury and lead into the air. They also moved large numbers of animals for entertainment, disrupting wildlife patterns.\n\nHumans have been altering ecosystems since before agriculture first developed, and as the human population has grown and become more technologically advanced over time, the land use for agricultural purposes has increased significantly. The anthropogenic biome in the 1700s, before the industrial revolution, was made up of mostly wild, untouched land, with no human settlement disturbing the natural state. In this time period, most of the Earth's ice-free land consisted of wildlands and natural anthromes, and it wasn't until after the industrial revolution in the 19th century that land use for agriculture and human settlements started to increase. With technology advancing and manufacturing processes becoming more efficient, the human population was beginning to thrive, and was subsequently requiring and using more natural resources. By the year 2000, over half of the Earth's ice free land was transformed into rangelands, croplands, villages and dense settlements, which left less than half of the Earth's land untouched. Anthropogenic changes between 1700 and 1800 were far smaller than those of the following centuries, and as such the rate of change has increased over time. As a result, the 20th century had the fastest rate of anthropogenic ecosystem transformation of the past 300 years.\n\nAs the human population steadily increased in numbers throughout history, the use of natural resources and land began to increase, and the distribution of land used for various agricultural and settlement purposes began to change. The use of land around the world was transformed from its natural state to land used for agriculture, settlements and pastures to sustain the population and its growing needs. The distribution of land among anthromes underwent a shift away from natural anthromes and wildlands towards human-altered anthromes we are familiar with today. Now, the most populated anthromes (dense settlements and villages) account for only a small fraction of the global ice-free land. From the year 1700-2000, lands used for agriculture and urban settlements increased significantly, however the area occupied by rangelands increased even more rapidly, so that it became the dominant anthrome in the 20th century. As a result, the biggest global land-use change as a result of the industrial revolution, was the expansion of pastures.\n\nFollowing the industrial revolution, the human population experienced a rapid increase. The human population density in certain anthromes began to change, shifting away from rural environments to urban settlements, where the population density was much higher. These changes in population density between areas shifted global patterns of anthrome emergence, and also had wide-spread effects on various ecosystems. Half of the Earth's population now lives in cities, and most people reside in urban anthromes, with some populations dwelling in smaller cities and towns. Currently, human populations are expected to grow until at least midcentury, and the transformation of the Earth's anthromes are expected to follow this growth.\n\nThe present state of the terrestrial biosphere is predominantly anthropogenic. More than half of the terrestrial biosphere remains unused directly for agriculture or urban settlements, and of these unused lands still remaining, less than half are wildlands. Most of Earth's unused lands are now within the agricultural and settled landscapes of semi-natural, rangeland, cropland and village anthromes.\n\nAnthromes include dense settlements (urban and mixed settlements), villages, croplands, rangelands and semi-natural lands and have been mapped globally using two different classification systems, viewable on Google Maps and Google Earth. There are currently 18 anthropogenic biomes, the most prominent of which are listed below.\n\nDense Settlements are the second most densely populated regions in the world. They are defined as areas with a high population density, though the density can be variable. The Population density, however, never falls below 100 persons/km, even in the non-urban parts of the dense settlements, and it has been suggested that these areas consist of both the edges of major cities in underdeveloped nations, and the long standing small towns throughout western Europe and Asia. Most often we think of dense settlements as cities, but dense settlements can also be suburbs, towns and rural settlements with high but fragmented populations.\n\nCroplands are another major anthrome throughout the world. Croplands include most of the cultivated lands of the world, and also about a quarter of global tree cover. Croplands which are locally irrigated have the highest human population density, likely due to the fact that it provides crops with a constant supply on water. This makes harvest time and crop survival more predictable. Croplands that are sustained mainly from the local rainfall are the most extensive of the populated anthromes, with annual precipitation near 1000 mm in certain areas of the globe. In these areas, there is sufficient water supplied by the climate to support all aspects of life without hardly any irrigation. However, in dryer areas, this method of agriculture would not be as productive.\n\nRangelands are a very broad anthropogenic biome group that has been described according to three levels of population density: residential, populated and remote. The Residential rangeland anthrome has two key features: its population density is never below 10 persons per square kilometre, and a substantial portion of its area is used for pasture. Pastures in rangelands are the most dominant land cover. Bare earth is significant in this anthrome, covering nearly one third of the land for every one square kilometer. Rangeland anthromes are less altered than croplands, but their alteration tends to increase with population. Domesticated grazing livestock are typically adapted to grasslands and savannas, so the alteration of these biomes tends to be less noticeable.\n\nForested anthromes are dominated by tree cover, and they have high precipitation and minimal human populations, where the population density is usually less than 3 persons/km². Most populated forests act as carbon sinks because of the lack of human activity. Without harmful emissions being released in the forests due to human activity, the vegetation is able to utilize carbon dioxide in the atmosphere, and act as a sink. Remote forests are a little different than populated forests because the majority of the vegetation in these forests have been clear-cut for human consumption. Forests are generally cleared to sustain substantial populations of domestic livestock, and to utilize the lumber.\n\nVery few biologists have studied the evolutionary processes at work in indoor environments. Estimates of the extent of residential and commercial buildings range between 1.3% and 6% of global ice-free land area. This area is just as extensive as other small biomes such as flooded grass-lands and tropical coniferous forests. The indoor biome is rapidly expanding, while forest anthromes are shrinking. The indoor biome of Manhattan is almost three times as large, in terms of its floor space, as is the geographical area of the island itself, due to the buildings rising up instead of spreading out. Thousands of species live in the indoor biome, many of them preferentially or even obligatorily. The only action that humans take to alter the evolution of the indoor biome is with cleaning practices. The field of indoor biomes will continue to change as long as our culture will change.\n\nHumans have fundamentally altered global patterns of biodiversity and ecosystem processes. It is no longer possible to explain or predict ecological patterns or processes across the Earth without considering the human role. Human societies began transforming terrestrial ecology more than 50 000 years ago, and evolutionary evidence has been presented demonstrating that the ultimate causes of human transformation of the biosphere are social and cultural, not biological, chemical, or physical. Anthropogenic biomes offer a new way forward by acknowledging human influence on global ecosystems and moving us toward models and investigations of the terrestrial biosphere that integrate human and ecological systems.\n\nOver the past century, anthrome extent and land use intensity increased rapidly together with growing human populations, leaving wildlands without human population or land use in less than one quarter of the terrestrial biosphere. This massive transformation of Earth's ecosystems for human use has occurred with enhanced rates of species extinctions. Humans are directly causing species extinctions, especially of megafauna, by reducing, fragmenting and transforming native habitats and by overexploiting individual species. Current rates of extinctions vary greatly by taxa, with mammals, reptiles and amphibians especially threatened; however there is growing evidence that viable populations of many, if not most native taxa, especially plants, may be sustainable within anthromes. With the exception of especially vulnerable taxa, the majority of native species may be capable of maintaining viable populations in anthromes.\n\nAnthromes present an alternative view of the terrestrial biosphere by characterizing the diversity of global ecological land cover patterns created and sustained by human population densities and land use while also incorporating their relationships with biotic communities. Biomes and ecoregions are limited in that they reduce human influences, and an increasing number of conservation biologists have argued that biodiversity conservation must be extended to habitats directly shaped by humans. Within anthromes, including densely populated anthromes, humans rarely use all available land. As a result, anthromes are generally mosaics of heavily used lands and less intensively used lands. Protected areas and biodiversity hotspots are not distributed equally across anthromes. Less populated anthromes contain a greater proportion of protected areas. While 23.4% of remote woodland anthrome is protected, only 2.3% of irrigated village anthrome is protected. There is increasing evidence that suggests that biodiversity conservation can be effective in both densely and sparsely settled anthromes. Land sharing and land sparing are increasingly seen as conservation strategies.\n\n\n"}
{"id": "43518148", "url": "https://en.wikipedia.org/wiki?curid=43518148", "title": "Butterfly count", "text": "Butterfly count\n\nA butterfly count is an event that is organized for the purpose of identifying and counting butterflies in a specific geographical area. Butterfly counts occur regularly in North America and Europe.\n\nThe counts are conducted by interested, mostly non-professional, residents of the area who maintain an interest in determining the numbers and species of butterflies in their locale. A butterfly count usually occurs at a specific time during the year and is sometimes coordinated to occur with other counts which may include a park, county, entire state or country. The results of the counts are usually shared with other interested parties including professional lepidopterists and researchers. The data gathered during a count can indicate population changes and health within a species.\n\nProfessional, universities, clubs, elementary and secondary schools, other educational providers, nature preserves, parks and amateur organizations can organize a count. The participants often receive training to help them identify the butterfly species. The North American Butterfly Association has organized over 400 counts in 2014.\n\nThere are several methods for counting butterflies currently in use, with the notable division being between restricted and open searches. Most counts are designed to count all butterflies observed in a locality.\n\nCounts may be targeted at single species and, in some cases, butterflies are observed and counted as they move from one area to another. A heavily researched example of butterfly migration is the annual migration of monarch butterflies in North America. Some programs will tag butterflies to trace their migration routes, but these are migratory programs and not butterfly counts. Butterfly counts are sometimes done where there is a concentration (a roost) of a species of butterflies in an area. One example of this is the winter count of western monarch butterflies as they roost together at sites in California, northern Mexico and Arizona.\n\nFrequently referred to as \"Pollard Transects\" or \"Pollard Walks\" in North America, a transect is a protocol designed to standardize the recording of butterfly observations, the initial format was outlined by Ernie Pollard in 1977. The transect protocol involves one observer walking a fixed path at a constant pace, multiple times in a season. Butterflies are counted when they are seen within a prescribed distance from the path, often 2.5 meters on either side of the path, and only when the butterflies are seen in front of, or above, the observer (i.e., no backtracking). A second person may work with the observer to identify and/or photograph insects spotted by the observer. Transects should not change from year to year and ideally should sample a variety of habitats.\n\nExamples of long-running restricted searches are Art Shapiro's Butterfly Project in the US (started in 1972), and the UK Butterfly Monitoring Scheme (started in 1976).\n\nOpen searches, also sometimes referred to as \"checklist searches\", are intended to focus on the presence and abundance of butterflies in a given area. They can be single events such as the North American Butterfly Association's July 1 and July 4 counts in Canada and the U.S. respectively, or they can be regular or ad hoc counts conducted by individuals or groups. The lack of formal structure makes them suitable for many citizen science programs.\n\nIn terms of the relative outcomes or the efficacy of open vs. restricted searches, studies have shown that open searches are more likely to find a greater number of species in a given area. Royer, et al. note that one reason for this is that during an open search, the \"observer is free to search out places where butterflies typically would breed or congregate\" rather than follow the fixed path of a transect.\n\nTo promote the broad public participation in butterfly monitoring, researcher from Austria propose to combine a simplified assessment scheme on group level executed by laypeople, with detailed assessments from butterfly experts. To evaluate their approach they compared data collected by pupils with independent assessments of professional butterfly experts. Beside some identification uncertainties data collected by trained and supervised pupils were successfully used to predict the general habitat quality for butterflies. \n\nOpportunistic or incidental sightings are a butterfly sightings that are not part of a formal count. Observers may note signal butterflies or multiple species. An example of an opportunistic sighting is observing a butterfly in garden and reporting it.\n\nDescribed as a \"special type of open search\", atlas projects are generally targeted at a specific geographical area such as a province or state. The goal is to assess the presence or absence of species, usually over a multi-year period. Each atlas program will design its own data requirements but as they are measuring abundance and presence, they tend to accept data from transects, counts and opportunistic sightings to build a database. The longest running atlas program in North America is the Ontario Butterfly Atlas Online, which is supported by the Toronto Entomologists' Association and began collecting data in 1969.\nTransects and open searches are not as comprehensive in tropical locations due to issues such as density of flora and the heigh of the forest canopy. A count system using bait stations with fermenting fruit has been used to assess specific populations.\n\nParticipants are encouraged to employ a number of techniques to quantify large aggregations by making estimates of butterflies:\n\n\nThe number of butterflies can be estimated by the area size they inhabit, for example, in the overwintering population present in Mexico the population expressed in hectares.\nButterflies can be counted in their egg, larvae and instar number.\n\nButterflies are sometimes captured, tagged and recovered. The number of tags recovered in a specific area is used to determine population size and direction of flight.\n\n\n"}
{"id": "7132760", "url": "https://en.wikipedia.org/wiki?curid=7132760", "title": "Camill Heller", "text": "Camill Heller\n\nCamill Heller (26 September 1823 – 25 February 1917) was a zoologist and anatomist.\n\nHeller was born in Sobochleben (Soběchleby) near Teplitz in Bohemia (now Teplice, part of the Czech Republic). He received a doctorate in medical studies in Vienna in 1849. Heller was the 'Professor of Zoology and Comparative Anatomy' at the University of Krakow in Poland from 1858 to 1863 and from 1863 until his retirement in 1894 he taught at the University of Innsbruck in Austria.\n\nHeller primarily specialised in crustaceans but also worked on bryozoans, echinoderms, pycnogonids, and tunicates.\n"}
{"id": "27477352", "url": "https://en.wikipedia.org/wiki?curid=27477352", "title": "Common Weakness Enumeration", "text": "Common Weakness Enumeration\n\nThe Common Weakness Enumeration (CWE) is a category system for software weaknesses and vulnerabilities. It is sustained by a community project with the goals of understanding flaws in software and creating automated tools that can be used to identify, fix, and prevent those flaws. The project is sponsored by the National Cybersecurity FFRDC, which is owned by The MITRE Corporation, with support from US-CERT and the National Cyber Security Division of the U.S. Department of Homeland Security.\n\nVersion 3.0 of the CWE standard was released in November 2017.\n\nCWE has over 600 categories, including classes for buffer overflows, path/directory tree traversal errors, race conditions, cross-site scripting, hard-coded passwords, and insecure random numbers.\n\n\nCommon Weakness Enumeration (CWE) Compatibility program allows a service or a product to be reviewed and registered as officially \"CWE-Compatible\" and \"CWE-Effective\". The program assists organizations in selecting the right software tools and learning about possible weaknesses and their possible impact.\n\nIn order to obtain CWE Compatible status a product or a service must meet 4 out of 6 requirements, shown below:\n\nThere are 48 organizations as of January 2018 that develop and maintain products and services that achieved CWE Compatible status.\n\nSome researchers think that ambiguities in CWE can be avoided or reduced.\n\n\n"}
{"id": "40222474", "url": "https://en.wikipedia.org/wiki?curid=40222474", "title": "DENIS-P J1058.7-1548", "text": "DENIS-P J1058.7-1548\n\nDENIS-P J1058.7-1548 is a brown dwarf of spectral type L3, located in constellation Crater at approximately 17.3 parsecs or 56.5 light-years from Earth. With a surface temperature of between 1700 and 2000 K, it is cool enough for clouds to form. Variations in its brightness in visible and infrared spectra suggest it has some form of atmospheric cloud cover.\n\n\n"}
{"id": "13027942", "url": "https://en.wikipedia.org/wiki?curid=13027942", "title": "Differentiation (sociology)", "text": "Differentiation (sociology)\n\n\"Differentiation\" is a term in system theory. From the viewpoint of this theory, the principal feature of modern society is the increased process of system differentiation as a way of dealing with the complexity of its environment. This is accomplished through the creation of subsystems in an effort to copy within a system the difference between it and the environment. The differentiation process is a means of increasing the complexity of a system, since each subsystem can make different connections with other subsystems. It allows for more variation within the system in order to respond to variation in the environment. Increased variation facilitated by differentiation not only allows for better responses to the environment, but also allows for faster evolution (or perhaps sociocultural evolution), which is defined sociologically as a process of selection from variation; the more differentiation (and thus variation) that is available, the better the selection.\n\nTalcott Parsons was the first major theorist to develop a theory of society consisting of functionally defined sub-system, which emerges from an evolutionary point of view through a cybernetic process of differentiation. Niklas Luhmann, who studied under Talcott Parsons, took the latter's model and changed it in significant ways. Parsons regarded society as the combined activities of its subsystems within the logic of a cybernetic hierarchy. For Parsons, although each subsystem (e.g. his classical quadripartite AGIL scheme or AGIL paradigm) would tend to have self-referential tendencies and follow a related path of structural differentiation, it would occur in a constant interpenetrative communication with the other subsystems and the historical equilibrium between the interpenetrative balance between various subsystem would termine the relative degree in which the structural differentiation between subsystem would occur or not. In contrast to Luhmann, Parsons would highlight that although each subsystem had self-referential capacities and had an internal logic of this own (ultimately located in the pattern maintenance of each system) in historical reality, the actual interaction, communication and mutual enable-ness between the subsystems was crucial not only for each subsystem but for the overall development of the social system (and/or \"society\"). In actual history, Parsons maintained that the relative historical strength of various subsystems (including the interpenetrative equilibrium of each subsystem's subsystems) could either block or promote the forces of system-differentiation. Generally, Parsons was of the opinion that the main \"gatekeeper\" blocking-promoting question was to be found in the historical codification of the cultural system, including \"cultural traditions\" (which Parsons in general regarded as a part of the so-called \"fiduciary system\" (which facilitated the normatively defining epicenter of the communication and historical mode of institutionalization between cultural and social system). (For example, the various way Islam has been transferred as a cultural pattern into various social systems (Egypt, Iran, Tunisia, Yemen, Pakistan, Indonesia etc.) depend on the particular way in which the core Islamic value-symbols has been codified within each particular fiduciary system (which again depend on a serie of various societal and history-related factors)). Within the realm of the cultural traditions Parsons focused particular on the influence of the major world-religions yet he also maintain that in the course of the general rationalization process of the world and the related secularization process, the value-scheme structure of the religious and \"magic\" systems would stepwise be \"transformed\" into political ideologies, market doctrines, folklore systems, social lifestyles and aesthetic movements (and so on). This transformation Parsons maintain was not so much the destruction of the religious value-schemes (although such a process could also occur) but was generally the way in which \"religious\" (and in a broader sense \"constitutive\") values would tend to move from a religious-magic and primordial \"representation\" to one which was more secularized and more \"modern\" in its institutionalized and symbolistic expression; this again would coincide with the increasing relative independence of systems of expressive symbolization vis-a-vis cognitive and evaluative lines of differentiation (for example, the flower-power movement in the 60s and early 70s would be a particular moment in this increased impact on factors of expressive symbolization on the overall interpenetrative mode of the social system. The breakthrough of rock music in the 1950s and the sensual expressiveness of Elvis would be another example, for the way in which expressive symbolization would tend to increase its impact vis-a-vis other factors of system-differentiation, which again according to Parsons was a part of the deeper evolutionary logic, which in part was related to the increased impact of the goal-attachment function of the cultural system and at the same time related the increased factor of institutionalized individualism, which have become a fundamental feature for historical modernity). Luhmann tend to claim that each subsystem has autopoeitic \"drives\" of their own. Instead of reducing society as a whole to one of its subsystems, i.e.; Karl Marx and Economics, or Hans Kelsen and Law, Luhmann bases his analysis on the idea that society is a self differentiating system that will, in order to attain mastery over an environment that is always more complex than it, increase its own complexity through a proliferating of subsystems. Although Luhmann claims that society cannot be reduced to any one of its subsystems, his critics maintain that his autopoeitic assumptions make it impossible to \"constitute\" a society at all and that Luhmann's theory is inherently self-contradictory. \"Religion\" is more extensive than the church, \"politics\" transcends the governmental apparatus, and \"economics\" encompasses more than the sum total of organizations of production.\n\nThere are four types of differentiation: segmentation, stratification, center-periphery, and functional.\n\nNiklas Luhmann (December 8, 1927 – November 6, 1998) was a German sociologist and \"social systems theorist\", as well as one of the most prominent modern day thinkers in the sociological systems theory. Luhmann was born in Lüneburg, Germany, studied law at the University of Freiburg from 1946 to 1949, in 1961 he went to Harvard, where he met and studied under Talcott Parsons, then the world's most influential social systems theorist. In later years, Luhmann dismissed Parsons' theory, developing a rival approach of his own. His magnum opus, \"Die Gesellschaft der Gesellschaft\" (\"The Society of Society\"), appeared in 1997 and has been subject to much review and critique since.\n\n\"Segmentary differentiation\" divides parts of the system on the basis of the need to fulfil identical functions over and over. For instance, a car manufacturer may have functionally similar factories for the production of cars at many different locations. Every location is organized in much the same way; each has the same structure and fulfils the same function – producing cars.\n\n\"Stratificatory differentiation\" or \"social stratification\" is a vertical differentiation according to rank or status in a system conceived as a hierarchy. Every rank fulfills a particular and distinct function in the system, for instance the manufacturing company president, the plant manager, trickling down to the assembly line worker. In segmentary differentiation inequality is an accidental variance and serves no essential function, however, inequality is systemic in the function of stratified systems. A stratified system is more concerned with the higher ranks (president, manager) than it is with the lower ranks (assembly worker) with regard to \"influential communication.\" However, the ranks are dependent on each other and the social system will collapse unless all ranks realize their functions. This type of system tends to necessitate the lower ranks to initiate conflict in order to shift the influential communication to their level.\n\nCenter-periphery differentiation is a link between Segmentary and Stratificatory, an example is again, automobile firms, may have built factories in other countries, nevertheless the headquarters for the company remains the center ruling, and to whatever extent controlling, the peripheral factories.\n\nFunctional differentiation is the form that dominates modern society and is also the most complex form of differentiation. All functions within a system become ascribed to a particular unit or site. Again, citing the automobile firm as an example, it may be \"functionally differentiated\" departmentally, having a production department, administration, accounting, planning, personnel, etc. Functional Differentiation tends to be more flexible than Stratifactory, but just as a stratified system is dependent on all rank, in a Functional system if one part fails to fulfill its task, the whole system will have great difficulty surviving. However, as long as each unit is able to fulfill its separate function, the differentiated units become largely independent; functionally differentiated systems are a complex mixture of interdependence and independence. E.g., the planning division may be dependent on the accounting division for economic data, but so long as the data is accurately compiled the planning division can be ignorant of the methodology involved to collect the data, interdependence yet independence.\n\n\"Code\" is a way to distinguish elements within a system from those elements not belonging to that system. It is the basic language of a functional system. Examples are truth for the science system, payment for the economic system, legality for the legal system; its purpose is to limit the kinds of permissible communication. According to Luhmann a system will only understand and use its own code, and will not understand nor use the code of another system; there is no way to import the code of one system into another because the systems are closed and can only react to things within their environment.\n\nIt is exemplified that in Segmentary differentiation if a segment fails to fulfill its function it does not affect or threaten the larger system. If an auto plant in Michigan stops production this does not threaten the overall system, or the plants in other locations. However, as complexity increases so does the risk of system breakdown. If a rank structure in a Stratified system fails, it threatens the system; a Center-Periphery system might be threatened if the control measure, or the Center/Headquarters failed; and in a Functionally differentiated system, due to the existence of interdependence despite independence the failure of one unit will cause a problem for the social system, possibly leading to its breakdown. The growth of complexity increases the abilities of a system to deal with its environment, but complexity increases the risk of system breakdown. It is important to note that more complex systems do not necessarily exclude less complex systems, in some instances the more complex system may require the existence of the less complex system to function.\n\nLuhmann uses the operative distinction between system and environment to determine that society is a complex system which replicates the system/environment distinction to form internal subsystems. Science is among these internally differentiated social systems, and within this system is the sub-system sociology. Here, in the system sociology, Luhmann finds himself again, an observer observing society. His knowledge of society as an internally differentiated system is a contingent observation made from within one of the specialized function-systems he observes. He concludes, therefore, that any social theory claiming universal status must take this contingency into account. Once one uses the basic system/environment distinction, then none of the traditional philosophical or sociological distinctions – transcendental and empirical, subject and object, ideology and science – can eliminate the contingency of enforced selectivity. Thus, Luhmann’s theory of social systems breaks not only with all forms of transcendentalism, but with the philosophy of history as well.\n\nLuhmann is criticized as being self-referential and repetitive, this is because a system is forced to observe society from within society. Systems theory, for its part, unfolds this paradox with the notion that the observer observes society from within a subsystem (in this case: sociology) of a subsystem (science) of the social system. Its descriptions are thus \"society of society\".\n\nLuhmann felt that the society that thematized itself as political society misunderstood itself. It was simply a social system in which a newly differentiated political subsystem had functional primacy. Luhmann analyzes the Marxist approach to an economy based society: In this theory, the concept of economic society is understood to denote a new type of society in which production, and beyond that \"a metabolically founded system of needs\" replaces politics as the central social process. From another perspective also characteristic of Marxist thought, the term \"bourgeois society\" is meant to signify that a politically defined ruling segment is now replaced as the dominant stratum by the owners of property. Luhmann’s reservations concerning not only Marxist, but also bourgeois theories of economic society parallel his criticisms of Aristotelian political philosophy as a theory of political society. Both theories make the understandable error of \"pars pro toto\", of taking the part for the whole, which in this context means identifying a social subsystem with the whole of society. The error can be traced to the dramatic nature of the emergence of each subsystem and its functional primacy (for a time) in relation to the other spheres of society. Nevertheless, the functional primacy claimed for the economy should not have led to asserting an economic permeation of all spheres of life. The notion of the economy possessing functional primacy is compatible with the well-known circumstance that the political subsystem not only grew increasingly differentiated (from religion, morals, and customs if not from the economy) but also continued to increase in size and internal complexity over the course of the entire capitalist epoch. For functional primacy need only imply that the internal complexity of a given subsystem is the greatest, and that the new developmental stage of society is characterized by tasks and problems originating primarily in this sphere.\n\n"}
{"id": "17509328", "url": "https://en.wikipedia.org/wiki?curid=17509328", "title": "Discrete event dynamic system", "text": "Discrete event dynamic system\n\nIn control engineering, a discrete event dynamic system (DEDS) is a discrete-state, event-driven system of which the state evolution depends entirely on the occurrence of asynchronous discrete events over time. Although similar to continuous-variable dynamic systems (CVDS), DEDS consists solely of discrete state spaces and event-driven state transition mechanisms.\n\nTopics in DEDS include:\n\n\n"}
{"id": "8195175", "url": "https://en.wikipedia.org/wiki?curid=8195175", "title": "Drifting ice station", "text": "Drifting ice station\n\nSoviet and Russian manned drifting ice stations are research stations built on the ice of the high latitudes of the Arctic Ocean. They are important contributors to exploration of the Arctic. The stations are named North Pole (NP; , ), followed by an ordinal number: North Pole-1, etc.\n\n\"NP\" drift stations carry out the program of complex year-round research in the fields of oceanology, ice studies, meteorology, aerology, geophysics, hydrochemistry, hydrophysics, and marine biology. On average, an \"NP\" station is the host for 600 to 650 ocean depth measurements, 3500 to 3900 complex meteorology measurements, 1200 to 1300 temperature measurements and sea water probes for chemical analysis, and 600 to 650 research balloon launches. Magnetic, ionosphere, ice and other observations are also carried out there. Regular measurements of the ice floe coordinates provide the data on the direction and speed of its drift.\n\nThe modern \"NP\" drifting ice station resembles a small settlement with housing for polar explorers and special buildings for the scientific equipment. Usually an \"NP\" station begins operations in April and continues for two or three years until the ice floe reaches the Greenland Sea. Polar explorers are substituted yearly. Since 1937 some 800 people were drifting at \"NP\" stations.\n\nThere are two groups of \"NP\" stations:\n\nAll \"NP\" stations are organized by the Russian (former Soviet) Arctic and Antarctic Research Institute (AARI).\n\nAn idea to use the drift ice for the exploration of nature in the high latitudes of the Arctic Ocean belongs to Fridtjof Nansen, who fulfilled it on \"Fram\" between 1893 and 1896. The first stations to use drift ice as means of scientific exploration of the Arctic originated in the Soviet Union in 1937, when the first such station in the world, North Pole-1, started operations.\n\nNorth Pole-1 was established on May 21, 1937 some 20 km from the North Pole by the expedition into the high latitudes. Sever-1, led by Otto Schmidt. \"NP-1\" operated for 9 months, during which the ice floe travelled 2,850 kilometres. On February 19, 1938, Soviet ice breakers \"Taimyr\" and \"Murman\" took off four polar explorers from the station, who immediately became famous in the USSR and were awarded titles Hero of the Soviet Union: hydrobiologist Pyotr Shirshov, geophysicist Yevgeny Fyodorov, radioman Ernst Krenkel and their leader Ivan Papanin.\n\nSince 1954 Soviet \"NP\" stations worked continuously, with one to three such stations operating simultaneously each year. The total distance drifted between 1937 and 1973 was over 80,000 kilometres. North Pole-22 is particularly notable for its record drift, lasting nine years. On June 28, 1972 the ice floe with North Pole-19 passed over the North Pole for the first time ever.\n\nDuring such long-term observations by \"NP\" stations, a lot of important discoveries in physical geography were made, valuable conclusions on regularities and the connection between processes in the polar region of the Earth's hydrosphere and atmosphere were obtained. Some of the most important discoveries were finding the deep-water Lomonosov Ridge, which crosses the Arctic Ocean, other large features of the ocean bottom's relief, the discovery of two systems of the drift (circular and \"wash-out\"), the fact of cyclones' active penetration into the Central Arctic.\n\nThe last Soviet \"NP\" station, North Pole-31, was closed in July 1991.\n\nIn the post-Soviet era, Russian exploration of the Arctic by drifting ice stations was suspended for twelve years. The year 2003 was notable for Russia's return into the Arctic. , three \"NP\" stations had carried out scientific measurements and research since then: \"NP-32\" through \"NP-34\". The latter was closed on May 25, 2006.\n\n\"NP-35\" started operations on September 21, 2007 at the point , when flags of Russia and Saint Petersburg were raised there. 22 scientists, led by A.A.Visnevsky are working on the ice floe. Establishment of the station was the third stage of the Arktika 2007 expedition. An appropriate ice floe was searched for from \"Akademik Fedorov\" research vessel, accompanied by nuclear icebreaker \"Russia\", using MI-8 helicopters, for a week, until an ice floe with an area of 16 square kilometres was found. The ice has since shrunk significantly, however, and the station is now being abandoned ahead of schedule.\n\n\n\n"}
{"id": "28929736", "url": "https://en.wikipedia.org/wiki?curid=28929736", "title": "Early Palaeozoic Icehouse", "text": "Early Palaeozoic Icehouse\n\nThe Early Palaeozoic Icehouse was a cool period that interrupted the greenhouse temperatures of the Ordovician and Silurian periods, culminating in the Hirnantian glaciation and the Ordovician extinction event. The icehouse was formerly thought only to consist of the Hirnantian glaciation itself, but has now been recognized as a longer, more gradual event.\n\nOver an interval of 30 million years, seven glacial maxima were recorded in the sedimentary record:\n"}
{"id": "41878399", "url": "https://en.wikipedia.org/wiki?curid=41878399", "title": "History of the portable gas stove", "text": "History of the portable gas stove\n\nThe portable gas stove is a combination of portability and functionality; combining the light weight of a small gas canister with the heat output needed to cook a meal. Portable stoves in modern times can be divided into several broad categories based on the type of fuel used and the design of the aluminium stoving frame. Unpressurised stoves use solid/liquid fuel placed in the burner before ignition. Combustible stove hangers use a form of volatile liquid fuel in a pressurized burner i.e. bottled gas stoves. They originate from the gravity-fed 1932 \"spirit\" stoves or \"réchaud de gaz de dirigeant\".\n\nThe production of the \"réchaud de gaz de dirigeant\" (Portable gas stove) was commissioned and begun in the workshop of the industrial designer Jue Lafare (born April 7, 1896) in 1932 after he placed his plan for a portable gas cooker before the French Army two and a half years earlier.\n\nBy 1934 Lafare's portable cooker was in officer ranks and within the next 16 months would come to be a staple of every officer in the French army. Due to fear of common misuse by soldiers of lower ranks obtaining the cookers without consent, the army requested that all instructional writing be placed upon the cookers in English as only the officers were taught fluency in English.\n\nUnfortunately Lafares design, however brilliant, was not enough to stop the German ranks marching over his homeland in 1940 after only 6 weeks. The majority of \"réchaud de gaz de dirigeant\" were melted down into what can only be assumed as scrap metal for the German war machine. However, from Jeu Lafare's stove the idea was adapted in various forms around the world during World War II and afterwards. Examples can be seen in many military originations around the world today with the American 'Bunga' or Japanese '携帯用ガス炊飯器'. Today there are estimated to be less than 80 of Lafare's original Full Blue French made with English writing \"réchaud de gaz de dirigeant\". Although the stove was a wonderful invention, the Germans were incredible in their precision at melting them down in such a systematic way.\n\nAs mentioned above, examples of Lafare's original full blue French made \"réchaud de gaz de dirigeant\" are extremely rare. The War Memorial Museum in Boulogne, France, paid 390EU for the piece pictured above in their collection some 20 years ago. However, it should be pointed out that the gas canister is a later 1940s Deutsch army model.\n\n"}
{"id": "17472023", "url": "https://en.wikipedia.org/wiki?curid=17472023", "title": "Jon Blundy", "text": "Jon Blundy\n\nJonathan David Blundy FRS (born 7 August 1961) is Professor of Petrology in the Department of Earth Sciences at the University of Bristol.\n\nHe is a graduate of University College, Oxford (B.A., 1980) and Trinity Hall, Cambridge, (PhD, 1989) and a former Kennedy Scholar at the Massachusetts Institute of Technology (1985). He was educated at St Paul's School, Brazil, Giggleswick School and Leeds Grammar School, where petrologists Keith Cox and Lawrence Wager also studied.\n\nBlundy is most noted for advancing the understanding of how magmas are generated in the Earth's crust and mantle and of the processes that occur in volcanoes before they erupt. He undertook his PhD research at the University of Cambridge under the supervision of Professor Robert Stephen John Sparks on the granites of Adamello-Presanella in the Italian Alps. In series of seminal papers with Professor Bernard John Wood in the 1990s Blundy developed a theory of elastic strain to describe the uptake of trace elements into the crystal lattices of igneous minerals. The theory was based on high temperature and pressure experiments on molten rocks, and is now widely used to predict crystal-melt partition coefficients for use in modelling magmatic processes.\n\nBlundy subsequently collaborated with Katharine Cashman at the University of Oregon on Mount St. Helens volcano in the Cascade Range of northwestern USA. Blundy and Cashman demonstrated the importance of degassing in driving the crystallisation of volatile-bearing magmas, a process that can occur without any attendant cooling. In fact, because of the release of latent heat of fusion, magmas that crystallise by decompression can actually get hotter in the process.\n\nBlundy is a recipient of the F.W. Clarke Medal of the Geochemical Society (1997), and Murchison Fund (1998) and the Bigsby Medal of the Geological Society of London (2005). He was a Fulbright Scholar at University of Oregon in 1998, Guest Professor at Nagoya University in 2007 and elected as a Fellow of the Royal Society (FRS) in 2008. His nomination reads: Blundy was also awarded the Royal Society Wolfson Research Merit Award in 2011.\n\nIn 1992, Blundy married Katharine Fawcett (rn1963), from whom he has been separated since 2004. He has a daughter, Lilian (born 1997), a son, Stanley (born 1994) and a step-daughter, Jennifer (born 1989).\n"}
{"id": "797707", "url": "https://en.wikipedia.org/wiki?curid=797707", "title": "Joseph Leidy", "text": "Joseph Leidy\n\nJoseph Mellick Leidy (September 9, 1823 – April 30, 1891) was an American paleontologist, parasitologist, and anatomist.\n\nLeidy was professor of anatomy at the University of Pennsylvania, and later was a professor of natural history at Swarthmore College. His book \"Extinct Fauna of Dakota and Nebraska\" (1869) contained many species not previously described and many previously unknown on the North American continent. At the time, scientific investigation was largely the province of wealthy amateurs.\n\nJoseph Leidy was born on September 9, 1823, to an established Philadelphia family of German extraction. His father, Philip, was a hatter; his mother, Catharine, died when he was young, during childbirth, whereupon his father remarried to his wife's first cousin, Christiana Mellick. Leidy also had a brother named Thomas Leidy. With the support of his stepmother, and after overcoming the opposition of his father (who wanted him to be a sign painter), Leidy studied medicine at the University of Pennsylvania. He graduated with his medical degree in 1844.\n\nHe married Anna Harden, a woman who took a serious interest in his work and helped him with it on occasion. Their marriage was childless, and they eventually adopted an orphaned seven-year-old girl.\n\nLeidy named the holotype specimen of \"Hadrosaurus foulkii\", which was recovered from the marl pits of Haddonfield, New Jersey. It was notable for being the first nearly-complete fossilized skeleton of a dinosaur ever recovered. The specimen was originally discovered by William Parker Foulke. Leidy concluded, contrary to the view prevailing at the time, that this dinosaur could adopt a bipedal posture. He also described the holotype specimens of \"Arctodus\" (\"A. simus\"), the dire wolf (\"Canis dirus\"), and the American lion (\"Panthera leo atrox\"), among many others.\n\nThe noted American fossil collector and paleontologist E. D. Cope was a student of Leidy's, but the enmity and ruthless competition that developed between him and rival paleontologist O. C. Marsh eventually drove Leidy out of western American vertebrate paleontology, a field that Leidy had helped to found. Marsh claimed Leidy contributed to the falling out of the two by showing Cope in the presence of Marsh that Cope had mistakenly placed the head of a fossil \"Elasmosaurus\" on the tail, rather than on the neck, and then publishing a correction.\n\nLeidy was an early American supporter of Darwin's theory of evolution, and lobbied successfully for Darwin's election to membership in the Academy of Natural Sciences of Philadelphia.\n\nIn 1852 Leidy referred \"Bison antiquus\", the North American fossil bison, to the genus \"Bison\", the first to do so. Sometimes called the \"ancient bison\", it was the most common large herbivore of the North American continent for over ten thousand years, and is a direct ancestor of the living American bison.\n\nLeidy was also a renowned parasitologist, and determined as early as 1846 that trichinosis was caused by a parasite in undercooked meat. He was also a pioneering protozoologist, publishing \"Fresh-water Rhizopods of North America\" in 1879 – a work that is still referenced today.\n\nLeidy collected gems as well as fossils, and donated his important collection of the former to the Smithsonian before he died. At Swarthmore, he also taught a class on mineralogy and geology.\n\nIn 1846, Leidy became the first person ever to use a microscope to solve a murder mystery. A man accused of killing a Philadelphia farmer had blood on his clothes and hatchet. The suspect claimed the blood was from chickens he had been slaughtering. Using his microscope, Leidy found no nuclei in these erythrocytes (human erythrocytes are anucleate). Moreover, he found that if he let chick erythrocytes remain outside the body for hours, they did not lose their nuclei. Thus, he concluded that the blood stains could not have been chicken blood. The suspect subsequently confessed.\n\nHis bibliography includes 553 works.\n\n"}
{"id": "34442159", "url": "https://en.wikipedia.org/wiki?curid=34442159", "title": "Joseph Ward (astronomer)", "text": "Joseph Ward (astronomer)\n\nJoseph Thomas Ward (25 January 1862 – 4 January 1927) was a New Zealand farmworker, bookseller and stationer, astronomer. He was born in London, England in 1862. He was the driving force behind the establishment of the Ward Observatory in Whanganui. He died in Wairoa while visiting his daughter.\n"}
{"id": "57806752", "url": "https://en.wikipedia.org/wiki?curid=57806752", "title": "Kate Marvel", "text": "Kate Marvel\n\nKate Marvel is a climate scientist and science writer based in New York City. She is an Associate Research Scientist at NASA Goddard Institute for Space Studies and Columbia Engineering's Department of Applied Physics and Mathematics, and writes regularly for Scientific American in her column \"Hot Planet.\" \n\nMarvel attended the University of California at Berkeley, where she received her Bachelor of Arts degree in physics and astronomy in 2003. She received her PhD in 2008 in theoretical physics from University of Cambridge as a Gates Scholar and member of Trinity College. Following her PhD, she shifted her focus to climate science and energy as a Postdoctoral Science Fellow at the Center for International Security and Cooperation at Stanford University and at the Carnegie Institution for Science in the Department of Global Ecology. She continued that trajectory as a postdoctoral fellow at the Lawrence Livermore National Laboratory before joining the research faulty at NASA Goddard Institute for Space Studies and Columbia University. \n\nMarvel's current research centers on climate modeling to better predict how much the Earth's temperature will rise rise in the future. This work led Marvel to investigate the effects of cloud cover on modeling rising temperatures, which has proved an important variable in climate models. Clouds can play a double-edged role in mitigating or amplifying the rate of global warming. On one hand, clouds reflect solar energy back into space, serving to cool the planet; on the other, clouds can trap the planet's heat and radiate back onto Earth's surface. While computer models have difficulty simulating the changing patterns of cloud cover, improved satellite data can begin to fill in the gaps.\n\nMarvel has also studied practical limitations in renewable energy as a Postdoctoral Scholar at the Carnegie Institution for Science. At the 2017 TED conference, following computer theorist Danny Hillis's talk proposing geoengineering strategies to mitigate global warming, Marvel was brought on stage to share why she believes geoengineering may cause more harm than good in the long run.\n\nMarvel is a science communicator whose efforts center on communicating about the impacts of climate change. She has been a guest on popular science shows like Star Talk Radio and BRIC Arts Media TV, speaking about her expertise in climate change and the need to act on climate. She has also spoken about her path to becoming a scientist as a story for the science-inspired storytelling series, The Story Collider. Marvel has also appeared on the TED Main Stage, giving a talk at the 2017 TED conference about the double-edged effect clouds can have on global warming.\n\nMarvel's writing has been featured in \"On Being\" and \"Nautilus\", and she's a regular contributor to \"Scientific American\" with her \"Hot Planet\". The column launched in June 2018 and focuses on climate change, covering the science behind global warming, policies, and human efforts in advocacy. \n"}
{"id": "351394", "url": "https://en.wikipedia.org/wiki?curid=351394", "title": "Lev Dyomin", "text": "Lev Dyomin\n\nLev Stepanovich Dyomin (; January 11, 1926, in Moscow – December 18, 1998, in Zvyozdny Gorodok) was a Soviet cosmonaut who flew on the Soyuz 15 spaceflight in 1974. This spaceflight was intended to dock with the space station Salyut 3, but the docking failed.\n\nDyomin gained a doctoral degree in engineering from the Soviet Air Force Engineering Academy and the rank of Colonel in the Soviet Air Force.\n\nAged 48 at the time of his flight on Soyuz 15, he was the oldest cosmonaut up to that point as well as the first grandfather to go into space. He remained in the program until leaving in 1982 to pursue deep-sea research. Dyomin died of cancer in Zvyozdny Gorodok in 1998.\n\nHe was awarded:\n"}
{"id": "46192330", "url": "https://en.wikipedia.org/wiki?curid=46192330", "title": "Limostatin", "text": "Limostatin\n\nLimostatin (from Limos, the Greek goddess of starvation) is a peptide hormone found in Drosophila melanogaster that suppresses the production and release of Insulin. The hormone is important in adaptation to starvation conditions, and represents a mechanism by which insulin is negatively regulated.\n\n"}
{"id": "11150437", "url": "https://en.wikipedia.org/wiki?curid=11150437", "title": "List of HDL simulators", "text": "List of HDL simulators\n\nHDL simulators are software packages that compile and simulate expressions written in one of the hardware description languages.\n\nHDL simulation software has come a long way since its early origin as a single proprietary product offered by one company. Today, simulators are available from many vendors at various prices, including free ones. For desktop/personal use, Aldec, Mentor, LogicSim, SynaptiCAD,TarangEDA and others offer tool-suites under US$5000 for the Windows 2000/XP platform. The suites bundle the simulator engine with a complete development environment: text editor, waveform viewer, and RTL-level browser. Additionally, limited-functionality editions of the Aldec and ModelSim simulator are downloadable free of charge, from their respective OEM partners (Microsemi, Altera, Lattice Semiconductor, Xilinx, etc.) For those desiring open-source software, there is Icarus Verilog, GHDL among others.\n\nBeyond the desktop level, enterprise-level simulators offer faster simulation runtime, more robust support for mixed-language (VHDL and Verilog) simulation, and most importantly, are validated for timing-accurate (SDF-annotated) gate-level simulation. The last point is critical for the ASIC tapeout process, when a design database is released to manufacturing. (semiconductor foundries stipulate the usage of tools chosen from an approved list, in order for the customer's design to receive signoff status. Although the customer is not required to perform any signoff checking, the tremendous cost of a wafer order has generally ensured thorough design validation on the part of the customer.) The three major signoff-grade simulators include Cadence Incisive Enterprise Simulator, Mentor ModelSim/SE, and Synopsys VCS. Pricing is not openly published, but all three vendors charge $25,000-$100,000 USD per seat, 1-year time-based license.\n\nFPGA vendors do not require expensive enterprise simulators for their design flow. In fact, most vendors include an OEM version of a third-party HDL simulator in their design suite. The bundled simulator is taken from an entry-level or low-capacity edition, and bundled with the FPGA vendor's device libraries. For designs targeting high-capacity FPGA, a standalone simulator is recommended, as the OEM-version may lack the capacity or speed to effectively handle large designs.\n\nBelow is a list of various HDL simulators.\n\nSome non-free commercial simulators (such as ModelSim) are available in student, or evaluation/demo editions. These editions generally have many features disabled, arbitrary limits on simulation design size, but are offered free of charge.\n\n"}
{"id": "43028476", "url": "https://en.wikipedia.org/wiki?curid=43028476", "title": "List of botanists by author abbreviation (H)", "text": "List of botanists by author abbreviation (H)\n\nTo find entries for A–G, use the table of contents above.\n\n\nTo find entries for I–Z, use the table of contents above.\n"}
{"id": "38486257", "url": "https://en.wikipedia.org/wiki?curid=38486257", "title": "List of camouflage methods", "text": "List of camouflage methods\n\nCamouflage is the concealment of animals or objects of military interest by any combination of methods that helps them to remain unnoticed. This includes the use of high-contrast disruptive patterns as used on military uniforms, but anything that delays recognition can be used as camouflage. Camouflage involves deception, whether by looking like the background or by resembling something else, which may be plainly visible to observers. This article lists methods used by animals and the military to escape notice.\n\nDifferent camouflage methods employed by terrestrial, aerial, and aquatic animals, and in military usage, are compared in the table. Several methods are often combined, so for example the Bushbuck is both countershaded over its whole body, and disruptively coloured with small pale spots. Until the discovery of countershading in the 1890s, protective coloration was considered to be mainly a matter of colour matching, but while this is certainly important, a variety of other methods are used to provide effective camouflage.\n\nWhen an entry is marked Dominant, that method is used widely in that environment, in most cases. For example, countershading is very common among land animals, but not for military camouflage. The dominant camouflage methods on land are countershading and disruptive coloration, supported by less frequent usage of many other methods. The dominant camouflage methods in the open ocean are transparency, reflection, and counterillumination. Transparency and reflectivity are dominant in the top of the ocean; counterillumination is dominant from down to . Most animals of the open sea use one or more of these methods. Military camouflage relies predominantly on disruptive patterns, though methods such as outline disruption are also used, and others have been prototyped.\n\nIn 1890 the English zoologist Edward Bagnall Poulton categorised animal colours by their uses, which cover both camouflage and mimicry. Poulton's categories were largely followed by Hugh Cott in 1940. Relevant Poulton categories are listed in the table. Where Poulton's definition covers a method but does not name it explicitly, the category is named in parentheses.\n\n"}
{"id": "7244703", "url": "https://en.wikipedia.org/wiki?curid=7244703", "title": "List of countries by date of transition to republican system of government", "text": "List of countries by date of transition to republican system of government\n\nThis is a list of countries by date of their last transition from a monarchy to a democratic republic form of government. There were two periods in recent history when many such transitions took place:\nSome of the countries on this list were part of larger, now extinct, states (such as the Russian Empire or Yugoslavia) when the transition to a republic took place. Countries that have always had non-republican forms of government (such as absolute monarchy, theocracy, etc.) are not included in this list. Some were also independent states that shared their head of state with other countries (such as Denmark or the United Kingdom) before abolishing the link with the shared monarchy. Countries marked in yellow have since ceased to be republics in favour of another form of government.\n\n\n"}
{"id": "236339", "url": "https://en.wikipedia.org/wiki?curid=236339", "title": "List of diseases (F)", "text": "List of diseases (F)\n\nThis is a list of diseases starting with the letter \"F\".\n\n\n\n\n\nFamilial a – Familial i\nFamilial m – Familial w\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "256043", "url": "https://en.wikipedia.org/wiki?curid=256043", "title": "List of science magazines", "text": "List of science magazines\n\nA science magazine is a periodical publication with news, opinions and reports about science, generally written for a non-expert audience. In contrast, a periodical publication, usually including primary research and/or reviews, that is written by scientific experts is called a \"scientific journal\". Science magazines are read by non-scientists and scientists who want accessible information on fields outside their specialization.\n\nArticles in science magazines are sometimes republished or summarized by the general press.\n\n\n\n\n"}
{"id": "1071314", "url": "https://en.wikipedia.org/wiki?curid=1071314", "title": "List of straight-chain alkanes", "text": "List of straight-chain alkanes\n\nThe following is a list of straight-chain and branched alkanes and their common names, sorted by number of carbon atoms. \n\n"}
{"id": "51139131", "url": "https://en.wikipedia.org/wiki?curid=51139131", "title": "Marc Straus", "text": "Marc Straus\n\nMarc Straus is an art gallery owner, poet and a retired oncologist. In the late 1970s, he was implicated in a high-profile case of scientific misconduct.\n\nStraus is the son of Samuel Straus (1914–1985) an Ashkenazi Jewish Polish immigrant from Sambir (now in the Ukraine), who came to New York impoverished at age 15. Samuel's mother had died when he was three months old. Marc's mother, Dora Straus (1918–2008) was a daughter of immigrants from Austro-Hungary. Her father worked as a tailor in Brooklyn, NY. In 1943 Samuel opened, Roman Cotton Goods, a wholesale textile store, in Manhattan's Lower Eastside.\n\nIn 1946, the family moved to West Hempstead, Long Island where he attended public school. At age 10 he a began commuting to Yeshiva Central Queens, an orthodox Jewish school, and at the age of 14 to Yeshiva of Flatbush in Brooklyn. In his first day of High School, he met Livia Selmanowitz, granddaughter of a scholar rabbi, who headed a Ger Hassidic group in Brooklyn. In their senior year they began dating and were married in 1964.\n\nStraus earned a BA in history in 1964 from Franklin & Marshall College in Lancaster, PA, and a MD in 1968 from SUNY Downstate Medical Center. He was a medical intern (1968–69) at Kings County Hospital in Brooklyn, NY and 1969 – 71), was a Research Fellow in Drug Research and Development at the National Cancer Institute, working on cancer drug studies in mice which established optimal ways of combining different drugs, with a number of publications in cancer research journals.\n\nFrom 1971–72 he was a medical resident at Barnes Hospital, Washington University, St. Louis. Here he devised a combination chemotherapy trial for patients with Small Cell lung cancer, an approach that became the basis for many other trials. In 1972–74, he was a clinical oncology fellow at the National Cancer Institute and Head of its Laboratory of Cell Kinetics from which he published numerous papers on the growth and treatment of human cancers. In 1971, he edited the first of three textbooks on lung cancer which for many years were the standard texts.\n\nIn 1973 he was recruited as Chief of Oncology and Associate Professor of Medicine at Boston University Medical Center where his clinical and research work focused primarily on breast and lung cancers. From 1978 to 1982 he was Professor of Medicine and Chief of Oncology at New York Medical College and Westchester Medical Center where he continued his clinical and laboratory research work. In all, he published some 100 scientific papers in journals.\n\nFrom 1982 – 2008 he headed Access Medical Group, a multi-specialty group in the Hudson Valley, NY, which ultimately had 36 doctors, and headed the oncology program at a number of suburban hospitals in the areaIn 1997 he founded a medical management company which managed some 300 doctors in New York state.\n\nHe is a Fellow of the American College of Physicians.\n\nIn June 1978, two nurses and other researchers who worked with him at the Boston University School of Medicine (BUSM) alleged that Straus, head of the oncology division, had ordered them to falsify patients' records, had failed (in violation of law) to get proper consent from patients, and had improperly administered drugs. According to one narrative, the allegations had included changing a patient's birth date to make them eligible for inclusion in a study, reporting treatments and laboratory tests that had never been carried out, and in one instance, Dr Judith P. Swazey alleged the existence of a tumor in a patient who never had one but she left Boston University because she said school officials were reluctant to investigate the incident and even suggested that support for her department's work might be reduced.\nThe work in question had been carried out as part of a collaborative study with the Eastern Cooperative Oncology Group (ECOG) within a project funded by the National Cancer Institute (NCI). Boston University's (BU's) investigation showed that almost 15% of the results were flawed. ECOG erased from their records all data supplied by BUSM, and expelled that research unit from the study.\nStraus has denied personal involvement in wrongdoing asserting his innocence and maintains that he has been framed by disgruntled subordinates. Dr Straus said that he had settled with the Government, acknowledging in an agreement to ECOG that false reports were submitted, only after becoming convinced that this legal outcome was unavoidable since the Government's regulations impose these penalties on a principal investigator even when he did not know of any wrongdoing.\nOn June 30, 1978, Straus resigned from his posts at Boston University Medical Center Hospital (since 1996 part of Boston Medical Center) and BUSM. He subsequently took up posts at the Westchester County Medical Center as Chief of Neoplastic Diseases and the New York Medical College as Professor of Medicine.\n\nOn September 15, 1978, Straus requested that BUMC establish a peer review by experts in the area of cooperative cancer data, but BUMC declined. Straus at his request met with the Executive Committee of ECOG April 23, 1979 and requested a full peer review investigation of the records. ECOG rejected the request.\n\nIn 1980, NCI awarded Straus a three-year grant of some $910,000 to conduct research into cancer. Hatch Senate Committee, Vincent T. DeVita Jr., Director of the NCI testified on June 2, 1981, \"his agencies award last year of a $910,000 research grant to a scientist (Straus) …on the ground that the charges …had not yet been proven.\n\nIn 1981, Straus filed a $33 million lawsuit today against five of his former co-workers, charging they had conspired to discredit his work. Straus spoke publicly for the first time before the President's Commission for the Study of Ethical Problems in Medicine and strongly denied he had engaged in any wrongdoing. Straus alleged forgeries of his signature. Ruth Moran, Straus’ lab chief from 1972 testified, \"Having worked side by side with Dr. Straus for 8 years … the allegations to be totally inconsistent with what I know his scientific standards to be… The (BUMC) committee refused to see me. Dr. Straus’ chief data manager (Mary Jane Rimmer, R.N.) reported to me … that she had always been instructed by Dr. Straus to record data accurately.\"\n\n\"Scientists Supporting the Rights of Marc J. Straus, M.D.\" comprised 78 cancer doctors and researchers in 1981, chaired by Mendel Krim, M.D. and Ruth Moran, Ph.D. They petitioned DeVita, Director NCI and Arthur Hayes, Commissioner, FDA, for an unitary blue ribbon review with full disclosure of relevant documents.\n\nIn 1982, two years after the grant was awarded the propriety of that award was called into question by two committees of the U.S. Congress: the Investigations and Oversight Subcommittee of the House Science and Technology Committee (chaired by Representative Al Gore (D-Tennessee)), and the Senate Committee on Labor and Human Resources (chaired by Senator Orrin Hatch (R-Utah)). The United States Department of Health and Human Services terminated the grant under Federal regulations passed in 1980, the first time that power had been used. At no time did the committees review the records in question.\n\nIn 1982 Straus obtained medical privileges at St. Agnes Hospital in White Plains, NY. As a result of the preceding allegations the Board empaneled a review committee chaired by Terence W. Murphy, M.D., J.D. to make a recommendation regarding privileges. Strauss was appointed Chief of Oncology at Yonkers General Hospital and head of a 22 patient cancer unit. In 1985 he was appointed head of oncology at Putnam Hospital Center and head of a cancer unit.\n\nIn October 1982, Straus published a paper in \"Cancer Treatment Reports\", a journal funded by the National Cancer Institute. The propriety of that publication was questioned in the journal \"Science\". Dr. Straus continued to publish scientific clinical studies through 1998.\n\nIn June 2005 Dr. Straus was invited as a Visiting Lecturer at the National Institutes of Health.\n\nIn 1982, Straus started CarePlus, a public company, a home nutrition companies in the U.S. In 1983 he founded Medical Registry Services, a software company providing cancer registry software support for hospitals. . In 2008 he and his son, Ari, founded MDINR, a software company which supported doctors treating patients with the anti-coagulant, Coumadin.\n\nMarc and Livia Straus began collecting contemporary art by 1966. Their collection has been featured in \"The NY Times\", Forbes, Harper's Bazaar, Contemporanea, Art & Antiques, and ArtNews. The collection has been listed in Art & Antiques as one of the Top 100 collections in the US.\n\nStraus has written some 35 articles in leading publications on art collecting and art criticism.\n\nIn 2002 the couple founded the Hudson Valley Center of Contemporary Art, in Peekskill, NY. Its focus is on emerging artists from around the world and supporting an economically challenged area with education programs. In 2011 Marc Straus opened MARC STRAUS, a contemporary art gallery on Grand Street, Manhattan, NYC across the street from where his father's store had been. The gallery represents 18 artists from 11 different counties. In 2014 Flashart listed MARC STRAUS as among the top 100 galleries in the world. \n\nFrom 2004 to the present he is Chairman of the Board of HVCCA. From 1994 – 2004 he was a Trustee at Franklin & Marshall College, Lancaster, PA.\n\nIn 1991 Straus took a workshop at the NYC 92nd Street Y with poet Thomas Lux, and the next year received a Yaddo Fellowship in poetry at Yaddo in Saratoga Springs, NY. By now his poems have published in 100 journals including Kenyon Review, Ploughshares, and TriQuarterly. He has three poetry collections published by TriQuarterly Press – Northwestern University Press: ONE WORD (1994), SYMMETRY (2000), NOT GOD (2006), a play in verse . which has been produced on stage at eight different venues including a run at LUNA Stage, NJ in 2009. Marc's poetry has been anthologized many times and he is the recipient of numerous writing prizes including the Robert Penn Warren Award lecture in the Humanities form Yale.\n\nHis poetry, which often deals with doctor patient communication and many poems are in the voices of patients or doctors. has been used in the teaching curricula of many medical schools and training programs. \n\n\n"}
{"id": "11921351", "url": "https://en.wikipedia.org/wiki?curid=11921351", "title": "Michigan Science Center", "text": "Michigan Science Center\n\nThe Michigan Science Center is a science museum in Detroit, Michigan. It is located at the site of the former Detroit Science Center which closed in 2011. The Michigan Science Center was formed as a new non-profit organization and purchased the assets of the Detroit Science Center. The Michigan Science Center (MiSci) opened December 26, 2012.\nThe Michigan Science Center has Michigan’s only Chrysler IMAX Dome Theatre; the Dassault Systèmes Planetarium; the DTE Energy Sparks Theater; the Chrysler Science Stage; a Science Hall for traveling exhibits; hands-on exhibit galleries focusing on space, life and physical science; the United States Steel Fun Factory; an exhibit gallery just for pint-size scientists; and more. \n\nDexter Ferry is credited for the vision and dedication that led to the creation of the Detroit Science Center; the Detroit-area businessman and philanthropist founded the center in 1970. In 1978, the DSC moved to its Midtown, Detroit, facility designed by Master Architect William Kessler of Detroit-based William Kessler Associates in the midtown cultural center adjacent to the Detroit Institute of Arts and Charles H. Wright Museum of African American History. \n\nThe center was closed briefly in the early 1990s after losing funding from the State of Michigan, but re-opened in 1991. The Detroit Science Center operated until 1999 when it closed for construction on a $30 million renovation and expansion - tripling the available exhibit space and adding new theater and performance areas. Neumann/Smith Architecture completed the \n. addition and . renovation. The Detroit Science Center had a grand re-opening celebration in July 2001 and continued to expand, adding a new Digital Dome Planetarium in December 2001 and a 4D Toyota Engineering Theater in 2008. \n\nIn partnership with the Thompson Educational Foundation, the Detroit Science Center embarked in April 2008 on another expansion of . to create a new college-prep charter school named University Prep Science & Math Middle School on its downtown campus. The school facility has classrooms, a gymnasium with locker rooms, food service, and offices, and shares conference space and lobby with the Detroit Science Center. Two Detroit companies completed the new addition, GunnLevine Architects (Architect of Record) and DeMaria Building Company.\n\nOn September 26, 2011, the Detroit Science Center closed due to monetary issues. Several planned events, programs, and trips were either postponed or rescheduled to take place at the Detroit Children's Museum. (Ironically, the Detroit Children's Museum itself was closed in December 2011 due to financial difficulties, but reopened in February 2012 solely to Detroit Public School students and lacking Detroit Science Center affiliation.) \n\nOn September 7, 2012, local Detroit news reports indicated that a new organization, the Michigan Science Center, would open and operate the facility. A spokesperson stated funding over the past year had been \"significant\" but did not disclose numbers. A board of directors for the Michigan Science Center first met on September 10, 2012. The Michigan Science Center began operations on December 26, 2012.\n\n"}
{"id": "86586", "url": "https://en.wikipedia.org/wiki?curid=86586", "title": "Military science", "text": "Military science\n\nMilitary science is the study of military processes, institutions, and behavior, along with the study of warfare, and the theory and application of organized coercive force. It is mainly focused on theory, method, and practice of producing military capability in a manner consistent with national defense policy. Military science serves to identify the strategic, political, economic, psychological, social, operational, technological, and tactical elements necessary to sustain relative advantage of military force; and to increase the likelihood and favorable outcomes of victory in peace or during a war. Military scientists include theorists, researchers, experimental scientists, applied scientists, designers, engineers, test technicians, and other military personnel.\n\nMilitary personnel obtain weapons, equipment, and training to achieve specific strategic goals. Military science is also used to establish enemy capability as part of technical intelligence.\n\nIn military history, military science had been used during the period of Industrial Revolution as a general term to refer to all matters of military theory and technology application as a single academic discipline, including that of the deployment and employment of troops in peacetime or in battle.\n\nIn military education, military science is often the name of the department in the education institution that administers officer candidate education. However, this education usually focuses on the officer leadership training and basic information about employment of military theories, concepts, methods and systems, and graduates are not military scientists on completion of studies, but rather junior military officers.\n\nEven until the Second World War, military science was written in English starting with capital letters, and was thought of as an academic discipline alongside Physics, Philosophy and the Medical Science. In part this was due to the general mystique that accompanied education in a World where as late as the 1880s 75% of the European population was illiterate. The ability by the officers to make complex calculations required for the equally complex \"evolutions\" of the troop movements in linear warfare that increasingly dominated the Renaissance and later history, and the introduction of the gunpowder weapons into the equation of warfare only added to the veritable arcana of building fortifications as it seemed to the average individual.\n\nUntil the early 19th century, one observer, a British veteran of the Napoleonic Wars, Major John Mitchell thought that it seemed nothing much had changed from the application of force on a battlefield since the days of the Greeks. He suggested that this was primarily so because as Clausewitz suggested, \"unlike in any other science or art, in war the object reacts\".\n\nUntil this time, and even after the Franco-Prussian War, military science continued to be divided between the formal thinking of officers brought up in the \"shadow\" of Napoleonic Wars and younger officers like Ardant du Picq who tended to view fighting performance as rooted in the individual's and group psychology and suggested detailed analysis of this. This set in motion the eventual fascination of the military organisations with application of quantitative and qualitative research to their theories of combat; the attempt to translate military thinking as philosophic concepts into concrete methods of combat.\n\nMilitary implements, the supply of an army, its organization, tactics, and discipline, have constituted the elements of military science in all ages; but improvement in weapons and accoutrements appears to lead and control all the rest.\n\nThe breakthrough of sorts made by Clausewitz in suggesting eight principles on which such methods can be based, in Europe, for the first time presented an opportunity to largely remove the element of chance and error from command decision making process. At this time emphasis was made on the Topography (including Trigonometry), Military art (Military science), Military history, Organisation of the Army in the field, Artillery and Science of Projectiles, Field fortifications and Permanent fortifications, Military legislation, Military administration and Manoeuvres.\n\nThe military science on which the model of German combat operations was built for the First World War remained largely unaltered from the Napoleonic model, but took into the consideration the vast improvements in the firepower and the ability to conduct \"great battles of annihilation\" through rapid concentration of force, strategic mobility, and the maintenance of the strategic offensive better known as the Cult of the offensive. The key to this, and other modes of thinking about war remained analysis of military history and attempts to derive tangible lessons that could be replicated again with equal success on another battlefield as a sort of bloody laboratory of military science. Few were bloodier than the fields of the Western Front between 1914 and 1918. Fascinatingly the man who probably understood Clausewitz better than most, Marshal Foch would initially participate in events that nearly destroyed the French Army.\n\nIt is not however true to say that military theorists and commanders were suffering from some collective case of stupidity; quite the opposite is true. Their analysis of military history convinced them that decisive and aggressive strategic offensive was the only doctrine of victory, and feared that overemphasis of firepower, and the resultant dependence on entrenchment would make this all but impossible, and leading to the battlefield stagnant in advantages of the defensive position, destroying troop morale and willingness to fight. Because only the offensive could bring victory, lack of it, and not the firepower, was blamed for the defeat of the Imperial Russian Army in the Russo-Japanese War. Foch thought that \"In strategy as well as in tactics one attacks\".\n\nIn many ways military science was born as a result of the experiences of the Great War. \"Military implements\" had changed armies beyond recognition with cavalry to virtually disappear in the next 20 years. The \"supply of an army\" would become a science of logistics in the wake of massive armies, operations and troops that could fire ammunition faster than it could be produced, for the first time using vehicles that used the combustion engine, a watershed of change. Military \"organisation\" would no longer be that of the linear warfare, but assault teams, and battalions that were becoming multi-skilled with introduction of machine gun and mortar, and for the first time forcing military commanders to think not only in terms of rank and file, but force structure.\n\nTactics changed too, with infantry for the first time segregated from the horse-mounted troops, and required to cooperate with tanks, aircraft and new artillery tactics. Perception of military discipline too had changed. Morale, despite strict disciplinarian attitudes, had cracked in all armies during the war, but best performing troops were found to be those where emphasis on discipline had been replaced with display of personal initiative and group cohesiveness such as that found in the Australian Corps during the Hundred Days Offensive. The military sciences' analysis of military history that had failed European commanders was about to give way to a new military science, less conspicuous in appearance, but more aligned to the processes of science of testing and experimentation, the scientific method, and forever \"wed\" to the idea of the superiority of technology on the battlefield.\n\nCurrently military science still means many things to different organisations. In the United Kingdom and much of the European Union the approach is to relate it closely to the civilian application and understanding. The Defence Scientific Advisory Council sees this in terms of the fields of science, engineering, technology and analysis (SETA) that includes broad strategic issues, priorities and policies related to developing military capabilities. In Europe, for example Belgium's Royal Military Academy, military science remains an academic discipline, and is studied alongside Social Sciences, including such subjects as Humanitarian law. The United States Department of Defense defines military science in terms of specific systems and operational requirements, and include among other areas civil defense and force structure.\n\nIn the first instance military science is concerned with who will participate in military operations, and what sets of skills and knowledge they will require to do so effectively and somewhat ingeniously.\n\nDevelops optimal methods for the administration and organization of military units, as well as the military as a whole. In addition, this area studies other associated aspects as mobilization/demobilization, and military government for areas recently conquered (or liberated) from enemy control.\n\nForce structuring is the method by which personnel and the weapons and equipment they use are organized and trained for military operations, including combat. Development of force structure in any country is based on strategic, operational, and tactical needs of the national defense policy, the identified threats to the country, and the technological capabilities of the threats and the armed forces.\n\nForce structure development is guided by doctrinal considerations of strategic, operational and tactical deployment and employment of formations and units to territories, areas and zones where they are expected to perform their missions and tasks. Force structuring applies to all Armed Services, but not to their supporting organisations such as those used for defense science research activities.\n\nIn the United States force structure is guided by the table of organization and equipment (TOE or TO&E). The TOE is a document published by the U.S. Department of Defense which prescribes the organization, manning, and equipage of units from divisional size and down, but also including the headquarters of Corps and Armies.\n\nForce structuring also provides information on the mission and capabilities of specific units, as well as the unit's current status in terms of posture and readiness. A general TOE is applicable to a type of unit (for instance, infantry) rather than a specific unit (the 3rd Infantry Division). In this way, all units of the same branch (such as Infantry) follow the same structural guidelines which allows for more efficient financing, training, and employment of like units operationally.\n\nStudies the methodology and practices involved in training soldiers, NCOs (non-commissioned officers, i.e. sergeants and corporals), and officers. It also extends this to training small and large units, both individually and in concert with one another for both the regular and reserve organizations. Military training, especially for officers, also concerns itself with general education and political indoctrination of the armed forces.\n\nMuch of capability development depends on the concepts which guide use of the armed forces and their weapons and equipment, and the methods employed in any given theatre of war or combat environment.\n\nMilitary activity has been a constant process over thousands of years, and the essential tactics, strategy, and goals of military operations have been unchanging throughout history. As an example, one notable maneuver is the double envelopment, considered to be the consummate military maneuver, first executed by Hannibal at the Battle of Cannae in 216 BCE, and later by Khalid ibn al-Walid at the Battle of Walaja in 633 CE.\n\nVia the study of history, the military seeks to avoid past mistakes, and improve upon its current performance by instilling an ability in commanders to perceive historical parallels during battle, so as to capitalize on the lessons learned. The main areas military history includes are the history of wars, battles, and combats, history of the military art, and history of each specific military service.\n\nMilitary strategy is in many ways the centerpiece of military science. It studies the specifics of planning for, and engaging in combat, and attempts to reduce the many factors to a set of principles that govern all interactions of the field of battle. In Europe these principles were first defined by Clausewitz in his Principles of War. As such, it directs the planning and execution of battles, operations, and wars as a whole. Two major systems prevail on the planet today. Broadly speaking, these may be described as the \"Western\" system, and the \"Russian\" system. Each system reflects and supports strengths and weakness in the underlying society.\n\nModern Western military art is composed primarily of an amalgam of French, German, British, and American systems. The Russian system borrows from these systems as well, either through study, or personal observation in the form of invasion (Napoleon's War of 1812, and The Great Patriotic War), and form a unique product suited for the conditions practitioners of this system will encounter. The system that is produced by the analysis provided by Military Art is known as doctrine.\n\nWestern military doctrine relies heavily on technology, the use of a well-trained and empowered NCO cadre, and superior information processing and dissemination to provide a level of battlefield awareness that opponents cannot match. Its advantages are extreme flexibility, extreme lethality, and a focus on removing an opponent's C3I (command, communications, control, and intelligence) to paralyze and incapacitate rather than destroying their combat power directly (hopefully saving lives in the process). Its drawbacks are high expense, a reliance on difficult-to-replace personnel, an enormous logistic train, and a difficulty in operating without high technology assets if depleted or destroyed.\n\nSoviet military doctrine (and its descendants, in CIS countries) relies heavily on masses of machinery and troops, a highly educated (albeit very small) officer corps, and pre-planned missions. Its advantages are that it does not require well educated troops, does not require a large logistic train, is under tight central control, and does not rely on a sophisticated C3I system after the initiation of a course of action. Its disadvantages are inflexibility, a reliance on the shock effect of mass (with a resulting high cost in lives and material), and overall inability to exploit unexpected success or respond to unexpected loss.\n\nChinese military doctrine is currently in a state of flux as the People's Liberation Army is evaluating military trends of relevance to China. Chinese military doctrine is influenced by a number of sources including an indigenous classical military tradition characterized by strategists such as Sun Tzu, Western and Soviet influences, as well as indigenous modern strategists such as Mao Zedong. One distinctive characteristic of Chinese military science is that it places emphasis on the relationship between the military and society as well as viewing military force as merely one part of an overarching grand strategy.\n\nEach system trains its officer corps in its philosophy regarding military art. The differences in content and emphasis are illustrative. The United States Army principles of war are defined in the U.S. Army Field Manual FM 100–5. The Canadian Forces principles of war/military science are defined by Land Forces Doctrine and Training System (LFDTS) to focus on \"principles of command\", \"principles of war\", \"operational art and campaign planning\", and \"scientific principles\".\n\nRussian Federation armed forces derive their principles of war predominantly from those developed during the existence of the Soviet Union. These, although based significantly on the Second World War experience in conventional war fighting, have been substantially modified since the introduction of the nuclear arms into strategic considerations. The Soviet–Afghan War and the First and Second Chechen Wars further modified the principles that Soviet theorists had divided into the operational art and tactics. The very scientific approach to military science thinking in the Soviet union had been perceived as overly rigid at the tactical level, and had affected the training in the Russian Federation's much reduced forces to instil greater professionalism and initiative in the forces.\n\nThe military principles of war of the People's Liberation Army were loosely based on those of the Soviet Union until the 1980s when a significant shift begun to be seen in a more regionally-aware, and geographically-specific strategic, operational and tactical thinking in all services. The PLA is currently influenced by three doctrinal schools which both conflict and complement each other: the People's war, the Regional war, and the Revolution in military affairs that led to substantial increase in the defense spending and rate of technological modernisation of the forces.\n\nThe differences in the specifics of Military art notwithstanding, Military science strives to provide an integrated picture of the chaos of battle, and illuminate basic insights that apply to all combatants, not just those who agree with your formulation of the principles.\n\nMilitary geography encompasses much more than simple protestations to take the high ground. Military geography studies the obvious, the geography of theatres of war, but also the additional characteristics of politics, economics, and other natural features of locations of likely conflict (the political \"landscape\", for example). As an example, the Soviet–Afghan War was predicated on the ability of the Soviet Union to not only successfully invade Afghanistan, but also to militarily and politically flank the Islamic Republic of Iran simultaneously.\n\nHow effectively and efficiently militaries accomplish their operations, missions and tasks is closely related not only to the methods they use, but the equipment and weapons they use.\n\nMilitary intelligence supports the combat commanders' decision making process by providing intelligence analysis of available data from a wide range of sources. To provide that informed analysis the commanders information requirements are identified and input to a process of gathering, analysis, protection, and dissemination of information about the operational environment, hostile, friendly and neutral forces and the civilian population in an area of combat operations, and broader area of interest. Intelligence activities are conducted at all levels from tactical to strategic, in peacetime, the period of transition to war, and during the war.\n\nMost militaries maintain a military intelligence capability to provide analytical and information collection personnel in both specialist units and from other arms and services. Personnel selected for intelligence duties, whether specialist intelligence officers and enlisted soldiers or non-specialist assigned to intelligence may be selected for their analytical abilities and intelligence before receiving formal training.\n\nMilitary intelligence serves to identify the threat, and provide information on understanding best methods and weapons to use in deterring or defeating it.\n\nThe art and science of planning and carrying out the movement and maintenance of military forces. In its most comprehensive sense, it is those aspects or military operations that deal with the design, development, acquisition, storage, distribution, maintenance, evacuation, and disposition of material; the movement, evacuation, and hospitalization of personnel; the acquisition or construction, maintenance, operation, and disposition of facilities; and the acquisition or furnishing of services.\n\nMilitary technology is not just the study of various technologies and applicable physical sciences used to increase military power. It may also extend to the study of production methods of military equipment, and ways to improve performance and reduce material and/or technological requirements for its production. An example is the effort expended by Nazi Germany to produce artificial rubbers and fuels to reduce or eliminate their dependence on imported POL (petroleum, oil, and lubricants) and rubber supplies.\n\nMilitary technology is unique only in its application, not in its use of basic scientific and technological achievements. Because of the uniqueness of use, military technological studies strive to incorporate evolutionary, as well as the rare revolutionary technologies, into their proper place of military application.\n\nThe following are notable journals in the field:\n\nUniversities (or colleges) around the world also offer a degree(s) in military science:\n\n\n\n\nUS Military/Government Texts\n"}
{"id": "51317367", "url": "https://en.wikipedia.org/wiki?curid=51317367", "title": "Nastulus", "text": "Nastulus\n\nMuḥammad ibn ʿAbd Allāh Nasṭūlus (or Basṭūlus) was a notable 10th-century astronomer and astrolabist. He is known for making the oldest surviving astrolabe, dated 927/928 AD. Another partially preserved astrolabe that bears his signature, \"Made by Nasṭūlus in the year 315\" of hijra (925 AD), contains the earliest known geographical list on an instrument.\n\nVery little is known about his life. His full name, based on a testimony given by a contemporary astronomer Abu Sa'id al-Sijzi, indicates that he was a Muslim. But some modern historians have suggested that his foreign last name may indicate that he was Greek or Nestorian.\n\n"}
{"id": "21842199", "url": "https://en.wikipedia.org/wiki?curid=21842199", "title": "OpenMS", "text": "OpenMS\n\nOpenMS is an open-source project for data analysis and processing in protein mass spectrometry and is released under the 3-clause BSD licence. It supports most common operating systems including Microsoft Windows, OS X and Linux.\n\nOpenMS has tools for many common data analysis pipelines used in proteomics, providing algorithms for signal processing, feature finding (including de-isotoping), visualization in 1D (spectra or chromatogram level), 2D and 3D, map mapping and peptide identification. It supports label-free and isotopic-label based quantification (such as iTRAQ and TMT and SILAC). Furthermore, it also supports metabolomics workflows and targeted analysis of DIA/SWATH data.\n\nTo achieve a wide variety of tasks in proteomics, OpenMS provides The OpenMS Proteomics Pipeline (TOPP) which is a set of computational tools that can be chained together to tailor problem-specific analysis pipelines for HPLC-MS data. It transforms most of the OpenMS functionality into small command line tools that are the building blocks for more complex analysis pipelines.\n\nOpenMS was originally released in 2007 in version 1.0 and was described in two articles published in Bioinformatics in 2007 and 2008 and has since seen continuous releases.\nIn 2009, the visualization tool TOPPView was published and in 2012, the workflow manager and editor TOPPAS was described in a scientific article. In 2013, a complete high-throughput label-free analysis pipeline using OpenMS 1.8 was described in the literature and compared with similar, proprietary software (such as MaxQuant and Progenesis). The authors conclude that \"[...] all three software solutions produce adequate and largely comparable quantification results; all have some weaknesses, and none can outperform the other two in every aspect that we examined. However, the performance of OpenMS is on par with that of its two tested competitors [...]\".\n\nThe OpenMS 1.10 release contained several new analysis tools, including OpenSWATH (a tool for targeted DIA data analysis), a metabolomics feature finder and a TMT analysis tool. Furthermore, full support for TraML 1.0.0 and the search engine MyriMatch were added. The OpenMS 1.11 release was the first release to contain fully integrated bindings to the Python programming language (termed pyOpenMS). In addition, several new tools were added such as tools relating to QcML (for quality control) and for metabolomics accurate mass analysis. Multiple tools were significantly improved with regard to memory and CPU performance.\n\nWith OpenMS 2.0, released in April 2015, the project provides a new version that has been completely cleared of GPL code and uses git (in combination with GitHub) for its version control and ticketing system. Other changes include support for mzIdentML, mzQuantML and mzTab while multiple improvements in the kernel allowed for faster access to data stored in mzML and provided a novel API for accessing mass spectrometric data. In 2016, the new features of OpenMS 2.0 were described in an article in Nature Methods.\n\nSince the inception of the project, a yearly OpenMS user meeting has been held at several universities where developers and users of the framework had the chance to present new features of OpenMS and direct, biological applications of OpenMS. The 3rd OpenMS user meeting took place in March 2010 in Dortmund, with the next meetings taking place in Berlin (4th meeting in September 2011), Salzburg (5th meeting in October 2012), Zurich (6th meeting in September 2013), Berlin (7th meeting in September 2014), Bochum (8th user meeting in September 2015) and Tübingen (9th meeting in September 2016).\n\nOpenMS is currently developed in the groups of Knut Reinert at the Free University of Berlin, in the group of Oliver Kohlbacher at the University of Tübingen and in the group of Ruedi Aebersold at ETH Zurich.\n\nOpenMS provides several features to users and developers, foremost providing a set of over 100 different executable tools than can be chained together into pipelines for proteomics data analysis (the TOPP Tools). It also provides visualization tools for spectra and chromatograms (1D), mass spectrometric heat maps (2D \"m/z\" vs \"RT\") as well as a three-dimensional visualization of a mass spectrometry experiment. Finally, OpenMS also provides a C++ library (with bindings to Python available since 1.11) for LC/MS data management and analyses accessible to developers to create new tools and implement their own algorithms using the OpenMS library. OpenMS is free software available under the 3-clause BSD licence (previously under the LGPL).\n\nAmong others, it provides algorithms for signal processing, feature finding (including de-isotoping), visualization, map mapping and peptide identification. It supports label-free and isotopic-label based quantification (such as iTRAQ and TMT and SILAC).\n\nTOPPView is a viewer software that allows visualization of mass spectrometric data on MS1 and MS2 level as well as in 3D; additionally it also displays chromatographic data from SRM experiments (in version 1.10). TOPPAS is a graphic integrated workflow manager that allows chaining the TOPP tools into a reusable and reproducible workflow.\n\nOpenMS is compatible with the current and the upcoming Proteomics Standard Initiative (PSI) formats for mass spectrometric data.\n\n\n\n"}
{"id": "28196464", "url": "https://en.wikipedia.org/wiki?curid=28196464", "title": "Pavlovsk Experimental Station", "text": "Pavlovsk Experimental Station\n\nPavlovsk Experimental Station () is an agricultural experiment station and gene bank that is part of the Institute of Plant Industry and situated in Pavlovsk near St. Petersburg, Russia.\n\nIt was started in 1926 by agricultural scientist Nikolai Vavilov and contains an extensive collection of more than 5,000 varieties of fruits and berries.\n\nThe Pavlovsk station's collection contains more than 100 varieties each of gooseberries, raspberries, and cherries. It also contains more than 1,000 varieties of strawberries. More than 90% of the collection is found in no other research collection or genebank.\n\nThe collection is a field genebank, meaning that the varieties are stored as plants in the ground. Most of the species concerned do not breed true from seeds, and so the varieties cannot be stored as seeds.\n\nThe Pavlovsk station itself fell into German hands during the Siege of Leningrad in 1941–1944, but prior to the arrival of German troops, scientists from the Institute of Plant Industry were able to move much of the station's tuber collection to a location within the city. Twelve of these scientists died of starvation while protecting the Institute's edible collection of tubers and seeds.\n\nIn 2010 the experimental station faced an uncertain future, because the land it sits on is being sold to a developer who plans to build private homes on the site. If this planned development had gone forward, much of the collection would have been lost. Due to technical issues and quarantine regulations, it would not have been feasible to move the collection before demolition of the station was slated to have begun. Russian President Dmitry Medvedev recently announced via Twitter that the issue will be \"scrutinised\". Prime Minister Vladimir Putin had not yet responded to public calls to save the experimental station and its collection as of the end of 2010. However, in April 2012 the Russian government took formal action to preserve this important genetic repository and stop the land from being conveyed to private interests for development. http://www.vir.nw.ru/news/14.05.2012_en.html\n\nThe novel \"Hunger\", by American writer Elise Blackwell, is a fictionalized retelling of the plight of the scientists who starved to death while protecting the gene bank's edible seed and tuber collection during the Siege of Leningrad. The song \"When the War Came,\" by the band The Decemberists, also tells the story of these scientists, with one verse saying \"We made our oath to Vavilov / We'd not betray the Solanum / The acres of asteraceae / To our own pangs of starvation.\"\n"}
{"id": "3198869", "url": "https://en.wikipedia.org/wiki?curid=3198869", "title": "Pedersen index", "text": "Pedersen index\n\nThe Pedersen index is a measure of electoral volatility in party systems. It was described by Mogens Pedersen in a paper published in 1979 entitled \"The Dynamics of European Party Systems: Changing Patterns of Electoral Volatility\".\n\n\"The net change within the electoral party system resulting from individual vote transfers\"\n\nTo calculate the index, the percentage gains of the winning parties must be determined. The resulting index will be between 0 (no parties gained, and thus no parties lost either) and 100 (all the parties from the last election were reduced to zero votes), because for every gain there is an equal (in terms of percentage of votes) loss. In other words, the index is equal to the net percentage of voters who changed their votes. (\"Net percentage,\" because if the only change is a Party A voter switching to Party B, and a Party B voter switching to Party A, there is no net volatility.)\nThe index can also be constructed by summing the absolute values of all gains and all losses, and dividing this total by two.\n\nAssume that in the first election the Blue Party won 65%, the Orange Party won 25%, and the Fuchsia Party won 10%. Furthermore, assume that in the second election the Blue Party won 65%, the Orange Party won 15%, and the Fuchsia Party won 20%.\n\nThe index would be equal to Blue gains (none) plus Orange's loss (10% since we do not consider sign differences) plus Fuchsia gains (10%). We then multiply it by 1/2 or divide by 2 for a total volatility of 10%.\n\nIf all three parties had disappeared in the next election, and been replaced by the Red Party (75%) and the Black Party (25%), the volatility would have been 100%: The first three lose all (100%) + the Red Party gaining 75% and the Black Party 25% since the previous election (when they both received no votes.) 100+100 = 200 -> divide by 2 = 100\n"}
{"id": "2470183", "url": "https://en.wikipedia.org/wiki?curid=2470183", "title": "Philosophical Magazine", "text": "Philosophical Magazine\n\nThe Philosophical Magazine is one of the oldest scientific journals published in English. It was established by Alexander Tilloch in 1798; in 1822 Richard Taylor became joint editor and it has been published continuously by Taylor & Francis ever since.\n\nThe name of the journal dates from a period when \"natural philosophy\" embraced all aspects of science. The very first paper published in the journal carried the title \"Account of Mr Cartwright's Patent Steam Engine\". Other articles in the first volume include \"Methods of discovering whether Wine has been adulterated with any Metals prejudicial to Health\" and \"Description of the Apparatus used by Lavoisier to produce Water from its component Parts, Oxygen and Hydrogen\".\n\nEarly in the nineteenth century, classic papers by Humphry Davy, Michael Faraday and James Prescott Joule appeared in the journal and in the 1860s James Clerk Maxwell contributed several long articles, culminating in a paper containing the deduction that light is an electromagnetic wave or, as he put it himself, \"We can scarcely avoid the inference that light consists in transverse undulations of the same medium which is the cause of electric and magnetic phenomena\". The famous experimental paper of Albert A. Michelson and Edward Morley was published in 1887 and this was followed ten years later by J. J. Thomson with article \"Cathode Rays\" – essentially the discovery of the electron.\n\nIn 1814, the \"Philosophical Magazine\" merged with the \"Journal of Natural Philosophy, Chemistry, and the Arts\", otherwise known as \"Nicholson's Journal\" (published by William Nicholson), to form \"The Philosophical Magazine and Journal\". Further mergers with the \"Annals of Philosophy\" and \"The Edinburgh Journal of Science\" led to the retitling of the journal in 1840, as \"The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science\". In 1949, the title reverted to \"The Philosophical Magazine\".\n\nIn the early part of the 20th century, Ernest Rutherford was a frequent contributor. He once told a friend to \"watch out for the next issue of \"Philosophical Magazine\"; it is highly radioactive!\" Aside from his work on understanding radioactivity, Rutherford proposed the experiments of Hans Geiger and Ernest Marsden that verified his nuclear model of the atom and led to Niels Bohr's famous paper on planetary electrons, which was published in the journal in 1913. Another classic contribution from Rutherford was entitled \"Collision of α Particles with Light Atoms. IV. An Anomalous Effect in Nitrogen\" – an article describing no less than the first artificial transmutation of an element.\n\nIn 1978 the journal was divided into two independent parts, \"Philosophical Magazine A\" and \"Philosophical Magazine B\". Part A published papers on structure, defects and mechanical properties while Part B focussed on statistical mechanics, electronic, optical and magnetic properties.\n\nSince the middle of the 20th century, the journal has focused on condensed matter physics and published significant papers on dislocations, mechanical properties of solids, amorphous semiconductors and glasses. As subject area evolved and it became more difficult to classify research into distinct areas, it was no longer considered necessary to publish the journal in two parts, so in 2003 parts A and B were re-merged. In its current form, 36 issues of the \"Philosophical Magazine\" are published each year, supplemented by 12 issues of Philosophical Magazine Letters.\n\nPrevious editors of the \"Philosophical Magazine\" have been John Tyndall, J.J. Thomson, Sir Nevill Mott, and William Lawrence Bragg. The journal is currently edited by Edward A. Davis.\n\nIn 1987, the sister journal Philosophical Magazine Letters was established with the aim of rapidly publishing short communications on all aspects of condensed matter physics. It is edited by Edward A. Davis and Peter Riseborough. This monthly journal had a 2014 impact factor of 1.087.\n\nOver its 200-year history, \"Philosophical Magazine\" has occasionally restarted its volume numbers at 1, designating a new 'series\" each time. The journal's series are as follows:\nIf the renumbering had not occurred, the 2015 volume (series 8, volume 95) would have been volume 407.\n\n"}
{"id": "1976946", "url": "https://en.wikipedia.org/wiki?curid=1976946", "title": "Rapoport's rule", "text": "Rapoport's rule\n\nRapoport's rule is an ecogeographical rule that states that latitudinal ranges of plants and animals are generally smaller at lower latitudes than at higher latitudes.\n\nStevens (1989) named the rule after Eduardo H. Rapoport, who had earlier provided evidence for the phenomenon for subspecies of mammals (Rapoport 1975, 1982). Stevens used the rule to \"explain\" greater species diversity in the tropics in the sense that latitudinal gradients in species diversity and the rule have identical exceptional data and so must have the same underlying cause. Narrower ranges in the tropics would facilitate more species to coexist. He later extended the rule to altitudinal gradients, claiming that altitudinal ranges are greatest at greater altitudes (Stevens 1992), and to depth gradients in the oceans (Stevens 1996). The rule has been the focus of intense discussion and given much impetus to exploring distributional patterns of plants and animals. Stevens' original paper has been cited about 330 times in the scientific literature.\n\nSupport for the generality of the rule is at best equivocal. For example, marine teleost fishes have the greatest latitudinal ranges at low latitudes. In contrast, freshwater fishes do show the trend, although only above a latitude of about 40 degrees North. Some subsequent papers have found support for the rule, others, probably even more numerous, have found exceptions to it. For most groups that have been shown to follow the rule, it is restricted to or at least most distinct above latitudes of about 40–50 degrees. Rohde therefore concluded that the rule describes a local phenomenon. Computer simulations using the Chowdhury Ecosystem Model did not find support for the rule.\n\nRohde (1996) explained the fact that the rule is restricted to very high latitudes by effects of glaciations which have wiped out species with narrow ranges, a view also expressed by Brown (1995). Another explanation of Rapoport's rule is the \"climatic variability\" or \"seasonal variability hypothesis\". According to this hypothesis, seasonal variability selects for greater climatic tolerances and therefore wider latitudinal ranges (see also Fernandez and Vrba 2005).\n\nThe methods used to demonstrate the rule have been subject to some controversy. Most commonly, authors plot means of latitudinal ranges in a particular 5° latitudinal band against latitude, although modal or median ranges have been used by some. In the original paper by Stevens, all species occurring in each band were counted, i.e., a species with a range of 50 degrees occurs in 10 or 11 bands. However, this may lead to an artificial inflation of latitudinal ranges of species occurring at high latitudes, because even a few tropical species with wide ranges will affect the means of ranges at high latitudes, whereas the opposite effect due to high latitude species extending into the tropics is negligible: species diversity is much smaller at high than low latitudes. As an alternative method the \"midpoint method\" has been proposed, which avoids this problem. It counts only those species with the midpoint of their ranges in a particular latitudinal band. An additional complication in assessing Rapoport's rule for data based on field sampling is the possibility of a spurious pattern driven by a sample-size artifact. Equal sampling effort at species-rich and species-poor localities tends to underestimate range size at the richer localities relative to the poorer, when in fact range sizes might not differ among localities.\n\nMarine benthic invertebrates and some parasites have been shown to have smaller dispersal abilities in cold seas (Thorson's rule), which would counteract Rapoport's rule. The tropics have far more uniform temperatures over a far wider latitudinal range (about 45 degrees) than high latitude species. As temperature is one of the most important (if not the most important) factor determining geographical distribution, wider latitudinal ranges in the tropics might therefore be expected.\n\nThe inconsistent results concerning Rapoport's rule suggest that certain characteristics of species may be responsible for their different latitudinal ranges. These characteristics may include, for example, their evolutionary age: species that have evolved recently in the tropics may have small latitudinal ranges because they have not had the time to spread far from their origin, whereas older species have extended their ranges.\n\n\n"}
{"id": "8286402", "url": "https://en.wikipedia.org/wiki?curid=8286402", "title": "Ribot's law", "text": "Ribot's law\n\nRibot's law of retrograde amnesia was hypothesized in 1881 by Théodule Ribot. It states that there is a time gradient in retrograde amnesia, so that recent memories are more likely to be lost than the more remote memories. Not all patients suffering from retrograde amnesia report the symptoms of Ribot's law.\n\nRibot's law was first postulated by the French psychologist Théodule Ribot (1839–1916), who is recognized as one of the pioneer 19th century advocates for psychology as an objective and biologically based empirical field. Ribot's split from the mainstream \"Eclectic\" psychology of the era was associated with a transition from philosophical to evolutionary explanations of human psychology and behavior. As Ribot was not a true experimentalist himself, this increased focus on the natural science basis of human mentality was manifested in an interest for case studies and diseases of dysfunction which helped to shape theories of psychological function. Ribot's law actually was first defined in terms of a broad generalization of functional decline in psychopathology: the observation that functions acquired most recently are the first to degenerate. However, in the current context of neuroscience research, Ribot's law is used almost exclusively to describe the perceived effect of older memories being less prone to disruption.\n\nIn his 1882 book, \"Diseases of Memory: An Essay in the Positive Psychology\", Ribot explained the retroactive phenomena of trauma or event-induced memory loss. Patients who incurred amnesia from a specific event such as an accident often also lost memory of the events leading up to the incident as well. In the case of some, this retrograde loss included several years leading up to the precipitating event of injury or trauma had occurred – yet left much older memories intact – suggesting that the effect was not just due to interference with consolidation of memories immediately before brain damage.\n\nOther historical accounts supporting the greater strength of older memories include some studies of aphasia starting as early as the late 1700s, in which bilingual patients recovered different languages with differential progress. In some cases, aphasics recover or preferentially improve only the first-acquired language, although this only seems to be the case mostly in people who were never truly fluent in their secondary language. \nCurrently, Ribot’s law is not universally accepted as a supporting example for memory consolidation and storage. As a component of the standard model memory of systems consolidation, it is challenged by the multiple trace theory which states that the hippocampus is always activated in the storage and retrieval of episodic memory regardless of memory age.\n\nA large body of research supports the predictions of Ribot's law. The theory concerns the relative strength of memories over time, which is not directly testable. Instead, scientists investigate the processes of forgetting (amnesia), and recollection. Ribot's law states that following a disruptive event, patients will show a temporally graded retrograde amnesia that preferentially spares more distant memories.\nExperimental evidence largely confirms these predictions. In a study of electroconvulsive shock therapy patients, memories formed at least four years prior to treatment were unaffected, while more recent ones were impaired. An experiment with rats showed similar results. Rats were conditioned to fear stimuli in two different contexts: one 50 days before receiving hippocampal brain lesions, and the other 1 day before lesioning. Subsequently, they only showed fear memory in the 50-day-old context.\nMany neurological disorders, including Alzheimer's disease, are also associated with a temporally graded retrograde amnesia, indicating that older memories are somehow strengthened against degeneration while newer memories are not. Although the mechanism for this strengthening is unclear, some models exist to explain the effects.\n\nInitially proposed in 1984 by Larry Squire, Neal Cohen, and Lynn Nadel, the standard model of systems consolidation is a contemporary theory used to explain the cognitive processes behind Ribot's law. In the model, interaction between the medial temporal hippocampus (MTH) and multiple areas in the neocortex lead to the formation of a cortical trace which represents a single memory. While this MTH-neocortex interaction is initially required to maintain the memory trace, the model predicts that over time the importance of the MTH becomes diminished and eventually is unnecessary for the storage of the memory trace. The medial temporal hippocampus mediates memory formation by maintaining the connections between various neocortical regions that make up each memory trace. At first the associations between neocortical areas that make up a newly formed memory trace are weak, however repeated activation of these areas in succession lead to \"consolidation\" of the trace within the neocortex. Once consolidation is sufficiently complete, the memory trace becomes mediated through neocortical activity alone and the MTH is no longer necessary for re-activation.\n\nFigure 1 provides a visual explanation of the standard model. Initially, the memory trace (features of the experience represented by red circles) is weak in the neocortex and is reliant on its connections to the medial temporal hippocampal system (MTH) for retrieval. Over time, an intrinsic process results in the strengthening of the connections between memory trace representations in the neocortex. Since the connections are consolidated, the memory can now be retrieved without the hippocampus.\n\nWhile never explicitly described by Squire and colleagues, the timescale of MTH-dependence in memory formation and maintenance is believed to vary by species as well as by the extent of hippocampal damage. For example, hippocampal lesion experiments with mouse models have shown retrograde amnesia for approximately one week prior to surgery, while case studies of human subjects with similar hippocampal damage have had retrograde amnesia limited to around two to three years prior to the accident).\n\nThe standard model of systems consolidation largely applies to the formation of declarative memories, which include semantic, factual memories and episodic, autobiographical memories. This has been supported by case studies of human patients with MTH lesions who exhibit difficulties in remembering experiences and fact learned post-surgery, however are able to retain motor and skill memories such as how to ride a bike or perform mirror tracing tasks.\n"}
{"id": "7614316", "url": "https://en.wikipedia.org/wiki?curid=7614316", "title": "SUSTAIN (military)", "text": "SUSTAIN (military)\n\nSmall Unit Space Transport and INsertion or SUSTAIN is a concept first proposed in 2002 by the United States Air Force and United States Marine Corps to deploy Marines via spaceflight to any location on Earth.\n\nProject Hot Eagle, launched by the Defense Advanced Research Projects Agency and the Air Force Research Laboratory, is an investigation into the development and use of suborbital spacecraft to fulfill this vision. Hot Eagle would use a craft based on a design similar to Space Ship One, which could launch a squad on a suborbital trajectory in two stages and deliver them anywhere on two hours' notice.\n\nDelivery of soldiers by rocket has been proposed before, including by General John B. Medaris, head of the Army Ballistic Missile Agency in the 1950s. The lander itself is designed to hold a 13-man squad and land in almost any terrain at any time, avoiding diplomatic concern for airspace rights.\n\n\n\n"}
{"id": "12908866", "url": "https://en.wikipedia.org/wiki?curid=12908866", "title": "Sackett self-selection circus", "text": "Sackett self-selection circus\n\nThe Sackett self-selection circus is an apparatus used in experimental psychology with non-human primates. It is a space divided into compartments containing objects; the time an animal spends with each object is measured, indicating any amount of fear of or anxiety from those objects. \n"}
{"id": "44126726", "url": "https://en.wikipedia.org/wiki?curid=44126726", "title": "School of Chemistry, University of Manchester", "text": "School of Chemistry, University of Manchester\n\nThe School of Chemistry at the University of Manchester is one of the largest Schools of Chemistry in the United Kingdom, with over 600 undergraduate and more than 200 postgraduate research students.\n\nThe School has comprehensive academic coverage across the chemical sciences and in all the core sub-disciplines of chemistry, with over 120 postdoctoral researchers.\n\n The School employs 34 full-time Professors and 11 Emeritus Professors including:\n\nThe School is also home to a number of Emeritus Professors, pursuing their research interests after their formal retirement including:\n\nManchester has a long and distinguished history of Chemistry. John Dalton founded modern Chemistry in 1803 with his atomic theory. William Henry (1774 – 1836) was a Manchester chemist who developed what is known today as Henry's Law. James Joule pioneered the science of thermodynamics in the 1840s while working in Manchester. In the basement of the Royal Manchester Institution a laboratory was installed by Lyon Playfair who worked there briefly as Professor of Chemistry after he left Thomson's of Clitheroe. He was succeeded by Frederick Crace Calvert who made phenol which was used by Joseph Lister as an antiseptic.\n\nThe teaching of chemistry in Owens College began in 1851 in a house in St John Street and was later transferred to the main college building in Quay Street. When the college removed to the present university site in 1873 the chemical laboratory was designed by Henry Roscoe. To this was added in 1895 the Schorlemmer laboratory for organic chemistry and in 1904 three more laboratories were added; these were the Dalton and Perkin laboratories and the Schunck laboratory which was brought from Kersal and rebuilt. The Morley laboratories (1909) provided further accommodation for organic chemistry. After the 2nd World War three more laboratories were built further down Burlington Street; these were the Dixon Laboratory (1946), the Robinson Laboratory (1950) and the Lapworth Laboratory (1950); all three were vacated in the 1960s when the present building in Brunswick Street was available. The architect for the present chemistry building was H. S. Fairhurst & Son.\n\nProfessors at Owens College and the Victoria University of Manchester:\n\nOther distinguished alumni and former staff from the school of Chemistry include:\n\n\nSee also Notable chemists (and biologists) at the University of Manchester\n\n"}
{"id": "12930232", "url": "https://en.wikipedia.org/wiki?curid=12930232", "title": "Scientific equipment optician", "text": "Scientific equipment optician\n\nA Scientific equipment optician is an individual who makes and adjusts other optical aids, including telescope optics and microscope lenses. See also Optician for individuals who make and adjust eyeglasses.\n\n\nSee also Timeline of telescope technology and List of astronomical instrument makers\n\n\nSee also Timeline of microscope technology\n"}
{"id": "9517150", "url": "https://en.wikipedia.org/wiki?curid=9517150", "title": "Shogun (toolbox)", "text": "Shogun (toolbox)\n\nShogun is a free, open-source machine learning software library written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.\n\nIt is licensed under the terms of the GNU General Public License version 3 or later.\n\nThe focus of \"Shogun\" is on kernel machines such as support vector machines for regression and classification problems. \"Shogun\" also offers a full implementation of Hidden Markov models.\nThe core of \"Shogun\" is written in C++ and offers interfaces for MATLAB, Octave, Python, R, Java, Lua, Ruby and C#.\n\"Shogun\" has been under active development since 1999. Today there is a vibrant user community all over the world using \"Shogun\" as a base for research and education, and contributing to the core package.\nCurrently \"Shogun\" supports the following algorithms:\n\nMany different kernels are implemented, ranging from kernels for numerical data (such as gaussian or linear kernels) to kernels on special data (such as strings over certain alphabets). The currently implemented kernels for numeric data include:\n\nThe supported kernels for special data include:\n\nThe latter group of kernels allows processing of arbitrary sequences over fixed alphabets such as DNA sequences as well as whole e-mail texts.\n\nAs \"Shogun\" was developed with bioinformatics applications in mind it is capable of processing huge datasets consisting of up to 10 million samples. \n\"Shogun\" supports the use of pre-calculated kernels. It is also possible to use a combined kernel i.e. a kernel consisting of a linear combination of arbitrary kernels over different domains. The coefficients or weights of the linear combination can be learned as well. For this purpose \"Shogun\" offers a \"multiple kernel learning\" functionality.\n\n\n"}
{"id": "3006497", "url": "https://en.wikipedia.org/wiki?curid=3006497", "title": "Social rejection", "text": "Social rejection\n\nSocial rejection occurs when an individual is deliberately excluded from a social relationship or social interaction. The topic includes \"interpersonal rejection\" (or peer rejection), \"romantic rejection\" and \"familial estrangement\". A person can be rejected by individuals or an entire group of people. Furthermore, rejection can be either \"active\", by bullying, teasing, or ridiculing, or \"passive\", by ignoring a person, or giving the \"silent treatment\". The experience of being rejected is subjective for the recipient, and it can be perceived when it is not actually present. The word ostracism is often used for the process (in Ancient Greece ostracism was voting into temporary exile).\n\nAlthough humans are social beings, some level of rejection is an inevitable part of life. Nevertheless, rejection can become a problem when it is prolonged or consistent, when the relationship is important, or when the individual is highly sensitive to rejection. Rejection by an entire group of people can have especially negative effects, particularly when it results in social isolation.\n\nThe experience of rejection can lead to a number of adverse psychological consequences such as loneliness, low self-esteem, aggression, and depression. It can also lead to feelings of insecurity and a heightened sensitivity to future rejection.\n\nRejection may be emotionally painful because of the social nature of human beings and the need of social interaction between other humans is essential. Abraham Maslow and other theorists have suggested that the need for love and belongingness is a fundamental human motivation. According to Maslow, all humans, even introverts, need to be able to give and receive affection to be psychologically healthy.\n\nPsychologists believe that simple contact or social interaction with others is not enough to fulfill this need. Instead, people have a strong motivational drive to form and maintain caring interpersonal relationships. People need both stable relationships and satisfying interactions with the people in those relationships. If either of these two ingredients is missing, people will begin to feel lonely and unhappy. Thus, rejection is a significant threat. In fact, the majority of human anxieties appear to reflect concerns over social exclusion.\n\nBeing a member of a group is also important for social identity, which is a key component of the self-concept. Mark Leary of Duke University has suggested that the main purpose of self-esteem is to monitor social relations and detect social rejection. In this view, self-esteem is a sociometer which activates negative emotions when signs of exclusion appear.\n\nSocial psychological research confirms the motivational basis of the need for acceptance. Specifically, fear of rejection leads to conformity to peer pressure (sometimes called normative influence), and compliance to the demands of others. Our need for affiliation and social interaction appears to be particularly strong when we are under stress.\n\nPeer rejection has been measured using sociometry and other rating methods. Studies typically show that some children are popular, receiving generally high ratings, many children are in the middle, with moderate ratings, and a minority of children are rejected, showing generally low ratings. One measure of rejection asks children to list peers they like and dislike. Rejected children receive few \"like\" nominations and many \"dislike\" nominations. Children classified as \"neglected\" receive few nominations of either type.\n\nAccording to Karen Bierman of Pennsylvania State University, most children who are rejected by their peers display one or more of the following behavior patterns:\n\nBierman states that well-liked children show social savvy and know when and how to join play groups. Children who are at risk for rejection are more likely to barge in disruptively, or hang back without joining at all. Aggressive children who are athletic or have good social skills are likely to be accepted by peers, and they may become ringleaders in the harassment of less skilled children. Minority children, children with disabilities, or children who have unusual characteristics or behavior may face greater risks of rejection. Depending on the norms of the peer group, sometimes even minor differences among children lead to rejection or neglect. Children who are less outgoing or simply prefer solitary play are less likely to be rejected than children who are socially inhibited and show signs of insecurity or anxiety.\n\nPeer rejection, once established, tends to be stable over time, and thus difficult for a child to overcome. Researchers have found that active rejection is more stable, more harmful, and more likely to persist after a child transfers to another school, than simple neglect. One reason for this is that peer groups establish reputational biases that act as stereotypes and influence subsequent social interaction. Thus, even when rejected and popular children show similar behavior and accomplishments, popular children are treated much more favorably.\n\nRejected children are likely to have lower self-esteem, and to be at greater risk for \"internalizing\" problems like depression. Some rejected children display \"externalizing\" behavior and show aggression rather than depression. The research is largely correlational, but there is evidence of reciprocal effects. This means that children with problems are more likely to be rejected, and this rejection then leads to even greater problems for them. Chronic peer rejection may lead to a negative developmental cycle that worsens with time.\n\nRejected children are more likely to be bullied and to have fewer friends than popular children, but these conditions are not always present. For example, some popular children do not have close friends, whereas some rejected children do. Peer rejection is believed to be less damaging for children with at least one close friend.\n\nAn analysis of 15 school shootings between 1995 and 2001 found that peer rejection was present in all but two of the cases (87%). The documented rejection experiences included both acute and chronic rejection and frequently took the form of ostracism, bullying, and romantic rejection. The authors stated that although it is likely that the rejection experiences contributed to the school shootings, other factors were also present, such as depression, poor impulse control, and other psychopathology.\n\nThere are programs available for helping children who suffer from social rejection. One large scale review of 79 controlled studies found that social skills training is very effective (\"r\" = .40 effect size), with a 70% success rate, compared to 30% success in control groups. There was a decline in effectiveness over time, however, with follow-up studies showing a somewhat smaller effect size (\"r\" = .35).\n\nLaboratory research has found that even short-term rejection from strangers can have powerful (if temporary) effects on an individual. In several social psychology experiments, people chosen at random to receive messages of social exclusion become more aggressive, more willing to cheat, less willing to help others, and more likely to pursue short-term over long-term goals. Rejection appears to lead very rapidly to self-defeating and antisocial behavior.\n\nResearchers have also investigated how the brain responds to social rejection. One study found that the dorsal anterior cingulate cortex is active when people are experiencing both physical pain and \"social pain,\" in response to social rejection. A subsequent experiment, also using fMRI neuroimaging, found that three regions become active when people are exposed to images depicting rejection themes. These areas are the posterior cingulate, the parahippocampal gyrus, and the dorsal anterior cingulate cortex. Furthermore, individuals who are high in rejection sensitivity (see below) show less activity in the left prefrontal cortex and the right dorsal superior frontal gyrus, which may indicate less ability to regulate emotional responses to rejection.\n\nA recent experiment at the University of California at Berkeley found that individuals with a combination of low self-esteem and low attentional control are more likely to exhibit eye-blink startle responses while viewing rejection themed images. These findings indicate that people who feel bad about themselves are especially vulnerable to rejection, but that people can also control and regulate their emotional reactions.\n\nA study at Miami University indicated that individuals who recently experienced social rejection were better than both accepted and control participants in their ability to discriminate between real and fake smiles. Though both accepted and control participants were better than chance (they did not differ from each other), rejected participants were much better at this task, nearing 80% accuracy. This study is noteworthy in that it is one of the few cases of a positive or adaptive consequence of social rejection.\n\nA common experimental technique is the \"ball toss\" paradigm, which was developed by Kip Williams and his colleagues at Purdue University. This procedure involves a group of three people tossing a ball back and forth. Unbeknownst to the actual participant, two members of the group are working for the experimenter and following a pre-arranged script. In a typical experiment, half of the subjects will be excluded from the activity after a few tosses and never get the ball again. Only a few minutes of this treatment are sufficient to produce negative emotions in the target, including anger and sadness. This effect occurs regardless of self-esteem and other personality differences.\n\nGender differences have been found in these experiments. In one study, women showed greater nonverbal engagement whereas men disengaged faster and showed face-saving techniques, such as pretending to be uninterested. The researchers concluded that women seek to regain a sense of belonging whereas men are more interested in regaining self-esteem.\n\nA computerized version of the task known as \"cyberball\" has also been developed and leads to similar results. Cyberball is a virtual ball toss game where the participant is led to believe they are playing with two other participants sitting at computers elsewhere who can toss the ball to either player. The participant is included in the game for the first few minutes, but then excluded by the other players for the remaining three minutes. This simple and short time period of ostracism has been found to produce significant increases to self-reported levels of anger and sadness, as well as lowering levels of the four needs. These effects have been found even when the participant is ostracised by out-group members, when the out-group member is identified as a despised person such as someone in the Ku Klux Klan, when they know the source of the ostracism is just a computer, and even when being ostracised means they will be financially rewarded and being included would incur a financial cost.\n\nPeople feel rejected even when they know they are playing only against the computer. A recent set of experiments using cyberball demonstrated that rejection impairs will power or self-regulation. Specifically, people who are rejected are more likely to eat cookies and less likely to drink an unpleasant tasting beverage that they are told is good for them. These experiments also showed that the negative effects of rejection last longer in individuals who are high in social anxiety.\n\nMost of the research on the psychology of ostracism has been conducted by the social psychologist Kip Williams. He and his colleagues have devised a model of ostracism which provides a framework to show the complexity in the varieties of ostracism and the processes of its effects. There he theorises that ostracism can potentially be so harmful that we have evolved an efficient warning system to immediately detect and respond to it.\n\nIn the animal kingdom as well as in primitive human societies, ostracism can lead to death due to the lack of protection benefits and access to sufficient food resources from the group. Living apart from the whole of society also means not having a mate, so being able to detect ostracism would be a highly adaptive response to ensure survival and continuation of the genetic line.\n\nIt is proposed that ostracism uniquely poses a threat to four fundamental human needs; the need to belong, the need for control in social situations, the need to maintain high levels of self-esteem, and the need to have a sense of a meaningful existence. A threat to these needs produces psychological distress and pain. Thus, people are motivated to remove this pain with behaviours aimed at reducing the likelihood of others ostracising them any further and increasing their inclusionary status.\n\nThere has been recent research into the function of popularity on development, specifically how a transition from ostracization to popularity can potentially reverse the deleterious effects of being socially ostracized. While various theories have been put forth regarding what skills or attributes confer an advantage at obtaining popularity, it appears that individuals who were once popular and subsequently experienced a transient ostracization are often able to employ the same skills that led to their initial popularity to bring about a popularity resurgence.\n\nIn contrast to the study of childhood rejection, which primarily examines rejection by a group of peers, some researchers focus on the phenomenon of a single individual rejecting another in the context of a romantic relationship. In both teenagers and adults, \"romantic rejection\" occurs when a person refuses the romantic advances of another, ignores/avoids or is repulsed by someone who is romantically interested in them, or unilaterally ends an existing relationship. The state of unrequited love is a common experience in youth, but mutual love becomes more typical as people get older.\n\nRomantic rejection is a painful, emotional experience that appears to trigger a response in the caudate nucleus of the brain, and associated dopamine and cortisol activity. Subjectively, rejected individuals experience a range of negative emotions, including frustration, intense anger, jealousy, hate, and eventually, resignation, despair, and possible long-term depression.\n\nKaren Horney was the first theorist to discuss the phenomenon of \"rejection sensitivity\". She suggested that it is a component of the neurotic personality, and that it is a tendency to feel deep anxiety and humiliation at the slightest rebuff. Simply being made to wait, for example, could be viewed as a rejection and met with extreme anger and hostility.\n\nAlbert Mehrabian developed an early questionnaire measure of rejection sensitivity. Mehrabian suggested that sensitive individuals are reluctant to express opinions, tend to avoid arguments or controversial discussions, are reluctant to make requests or impose on others, are easily hurt by negative feedback from others, and tend to rely too much on familiar others and situations so as to avoid rejection.\n\nA more recent (1996) definition of rejection sensitivity is the tendency to \"anxiously expect, readily perceive, and overreact\" to social rejection. People differ in their readiness to perceive and react to rejection. The causes of individual differences in rejection sensitivity are not well understood. Because of the association between rejection sensitivity and neuroticism, there is a likely genetic predisposition. Others posit that rejection sensitivity stems from early attachment relationships and parental rejection; also peer rejection is thought to play a role. Bullying, an extreme form of peer rejection, is likely connected to later rejection sensitivity. However, there is no conclusive evidence for any of these theories.\n\nSocial rejection has a large effect on a person's health. Baumeister and Leary originally suggested that an unsatisfied need to belong would inevitably lead to problems in behavior as well as mental and physical health. Corroboration of these assumptions about behavior deficits were seen by John Bowlby in his research. Numerous studies have found that being socially rejected leads to an increase in levels of anxiety. Additionally, the level of depression a person feels as well as the amount they care about their social relationships is directly proportional to the level of rejection they perceive. Rejection affects the emotional health and well being of a person as well. Overall, experiments show that those who have been rejected will suffer from more negative emotions and have fewer positive emotions than those who have been accepted or those who were in neutral or control conditions.\n\nIn addition to the emotional response to rejection, there is a large effect on physical health as well. Having poor relationships and being more frequently rejected is predictive of mortality. Also, as long as a decade after the marriage ends, divorced women have higher rates of illness than their non-married or currently married counterparts. In the case of a family estrangement, a core part of the mother's identity may be betrayed by the rejection of an adult child. The chance for reconciliation, however slight, results in an inability to attain closure. The resulting emotional state and societal stigma from the estrangement may harm psychological and physical health of the parent through end of life.\n\nThe immune system tends to be harmed when a person experiences social rejection. This can cause severe problems for those with diseases such as HIV. One study by Cole, Kemeny, and Taylor investigated the differences in the disease progression of HIV positive gay men who were sensitive to rejection compared to those who were not considered rejection sensitive. The study, which took place over nine years, indicated significantly faster rate of low T helper cells, therefore leading to an earlier AIDS diagnosis. They also found that those patients who were more sensitive to rejection died from the disease an average of 2 years earlier than their non-rejection sensitive counterparts.\n\nOther aspects of health are also affected by rejection. Both systolic and diastolic blood pressure increase upon imagining a rejection scenario. Those who are socially rejected have an increased likelihood of suffering from tuberculosis, as well as dying by suicide. Rejection and isolation were found to affect levels of pain following an operation as well as other physical forms of pain. Social rejection may cause a reduction in intelligence. MacDonald and Leary theorize that rejection and exclusion cause physical pain because that pain is a warning sign to help us survive. As we developed into social creatures, social interactions and relationships became necessary to our survival, and the physical pain systems already existed within our bodies.\n\nArtistic depictions of rejection occur in a variety of art forms. One example of rejection in art is Gian Lorenzo Bernini's sculpture of the Greek deity, Apollo. In this work, the mythical nymph, Daphne, depicts the act of rejection. Apollo had been filled with passion for Daphne, but she repeatedly rejected his advances. The sculpture portrays the moment when Daphne cries out for safety from Apollo and as a result is transformed into a laurel tree. Apollo has been defeated and turns away, rejected.\n\nDepictions of rejection also occur in film. One genre of film that most frequently depicts rejection is romantic comedies. In the film \"He's Just Not That Into You\", the main characters deal with the challenges of reading and misreading human behavior. This presents a fear of rejection in romantic relationships as reflected in this quote by the character Mary, \"And now you have to go around checking all these different portals just to get rejected by seven different technologies. It's exhausting.\"\n\nSocial rejection is also depicted in theatrical plays and musicals. For example, the film \"Hairspray\" shares the story of Tracy Turnblad, an overweight 15-year-old dancer set in the 1960s. Tracy and her mother are faced with overcoming society's expectations regarding weight and physical appearances.\n\n\n"}
{"id": "319941", "url": "https://en.wikipedia.org/wiki?curid=319941", "title": "Thaumatin", "text": "Thaumatin\n\nThaumatin (also known as talin) is a low-calorie sweetener and flavour modifier. The protein is often used primarily for its flavour-modifying properties and not exclusively as a sweetener.\n\nThe thaumatins were first found as a mixture of proteins isolated from the katemfe fruit (\"Thaumatococcus daniellii\" Bennett) of west Africa. Some proteins in the thaumatin family of sweeteners are roughly 2000 times more potent than sugar. Although very sweet, thaumatin's taste is markedly different from sugar's. The sweetness of thaumatin builds very slowly. Perception lasts a long time, leaving a liquorice-like aftertaste at high usage levels. Thaumatin is highly water-soluble, stable to heating, and stable under acidic conditions.\n\nThaumatin production is induced in katemfe in response to an attack upon the plant by viroid pathogens. Several members of the thaumatin protein family display significant \"in vitro\" inhibition of hyphal growth and sporulation by various fungi. The thaumatin protein is considered a prototype for a pathogen-response protein domain. This thaumatin domain has been found in species as diverse as rice and \"Caenorhabditis elegans\".\nThaumatins are pathogenesis-related (PR) proteins, which are induced by various agents ranging from ethylene to pathogens, and are structurally diverse and ubiquitous in plants: They include thaumatin, osmotin, tobacco major and minor PR proteins, alpha-amylase/trypsin inhibitor, and P21 and PWIR2 soybean and wheat leaf proteins. The proteins are involved in systematically\nacquired resistance and stress response in plants, although their precise role is unknown. Thaumatin is an intensely sweet-tasting protein (on a molar basis about 100,000 times as sweet as sucrose) found in the West African shrub \"Thaumatococcus daniellii\": it is induced by attack by viroids, which are single-stranded unencapsulated RNA molecules that do not code for protein. The thaumatin protein I consists of a single polypeptide chain of 207 residues.\n\nLike other PR proteins, thaumatin is predicted to have a mainly beta structure, with a high content of beta-turns and little helix. Tobacco cells exposed to gradually increased salt concentrations develop a greatly increased tolerance to salt, due to the expression of osmotin, a member of the PR protein family. Wheat plants attacked by barley powdery mildew express a PR protein (PWIR2), which results in resistance against that infection. The similarity between this PR protein and other PR proteins to the maize alpha-amylase/trypsin inhibitor has suggested PR proteins may act as some form of inhibitor.\n\nThe thaumatin-like proteins isolated from kiwi fruit or apple appear to have their allergenic properties minimally reduced by gastroduodenal digestive processes, but not by heating.\n\nWithin West Africa, the katemfe fruit has been locally cultivated and used to flavour foods and beverages for some time. The fruit's seeds are encased in a membranous sac, or aril, that is the source of thaumatin. In the 1970s, Tate and Lyle began extracting thaumatin from the fruit. In 1990, researchers at Unilever reported the isolation and sequencing of the two principal proteins found in thaumatin, which they dubbed thaumatin I and thaumatin II. These researchers were also able to express thaumatin in genetically engineered bacteria.\n\nThaumatin has been approved as a sweetener in the European Union (E957), Israel, and Japan. In the United States, it is generally recognized as safe as a flavouring agent (FEMA GRAS 3732) but not as a sweetener.\n\nSince thaumatin crystallizes rapidly and easily in the presence of tartrate ions, thaumatin-tartrate mixtures are frequently used as model systems to study protein crystallization. The solubility of thaumatin, its crystal habit, and mechanism of crystal formation are dependent upon the chirality of precipitant used. When crystallized with L- tartrate, thaumatin forms bipyramidal crystals and displays a solubility that increases with temperature; with D- and meso-tartrate, it forms stubby and prismatic crystals and displays a solubility that decreases with temperature. This suggests control of precipitant chirality may be an important factor in protein crystallization in general.\n\n"}
{"id": "42959804", "url": "https://en.wikipedia.org/wiki?curid=42959804", "title": "Two layer hypothesis", "text": "Two layer hypothesis\n\nThe 'Two Layer' Hypothesis, or immigration hypothesis, is an archaeological theory that suggests the human occupation of mainland Southeast Asia occurred over two distinct periods by two separate racial groups, hence the term 'layer'. According to the Two Layer Hypothesis, early indigenous Australo-Melanesian peoples comprised the first population of Southeast Asia before their genetic integration with a second wave of inhabitants from East Asia, including Southern China, during the agricultural expansion of the Neolithic.\nThe majority of evidence for the Two Layer Hypothesis consists of dental and morphometric analyses from archaeological sites throughout Southeast Asia, most prominently Thailand and Vietnam. The credibility of the Two Layer Hypothesis has been criticized due mainly to similarities between Southeast Asian and Chinese cranial and dental characteristics, excluding Australo-Melanesians.\n\nThe first fossilized skeletal remains and indication of early 'Proto-Australian' Southeast Asian inhabitants surfaced in 1920 during an excavation by Dubois on the island of Java. Despite this, a formal connection to mainland Southeast Asia and the suggestion of an initial population of Australomelanesoids was not suggested until 1952 by Koenigswald in his response to Hooijer, who sharply criticized the attribution of 'big toothed' dental remains to early Australo-Melanesians.\nThe immigration hypothesis proposed by Koenigswald was formally termed the 'Two Layer' model by Jacob Teuku. In 1967, Teuku analyzed the cranial and dental proportions of 152 adult skeletal samples recovered from prehistoric sites in Malaysia and Indonesia, the majority displaying robust jaws and teeth, prominent glabellae, and slender, elongated limbs. Teuku argued these characteristics correspond to the Australo-Melanesian population proposed by Koenigswald that predated the East Asian immigrants of the Neolithic; also suggesting the initial inhabitants were likely forced south of Southeast Asia's mainland by the second wave of migrants, due to resource competition or conflict.\n\nExcavations at Moh Khiew Cave resulted in the discovery of a Late Pleistocene female human skeleton, an AMS radiocarbon date on a charcoal sample that was gathered from the burial grave gave an age of 25,800 +/- 600 BP. Measurements such as the bimaxillary breadth, upper facial height, bicondylar breadth, mandibular length, nasal breadth and height, palatal height, and mandibular angle were compared from the cranial and dental area to other female human specimens from East Asia and the Southwest Pacific regions. Twelve measurements were used in total for the statistical comparison in addition to 14 buccolingual crown diameters of the maxillary and mandibular teeth. After a comparison was completed the closest sample to the Moh Khiew Cave sample is the Late Pleistocene Coobool Creek from Australia. The next closest sample is the modern Australian Aborigines. However, those from Flores, Vietnam, and Thailand are distant samples in comparison. This discovery suggests that the Moh Khiew Cave skeleton may have shared a common ancestor with Australian Aborigines and Melanesians and also supports the ‘Two Layer’ hypothesis.\n\nA study conducted in 2005, examined and compared permanent teeth from 4,002 individuals found in 42 prehistoric and historic samples throughout East Asia, Southeast Asia, Australia, and Melanesia. Metric dental traits were denoted by mesiodistal and buccolingual crown diameters. Measurements were taken as maximum diameters and male individuals were primarily measured because males generally have larger teeth and therefore show more differences. One culture that was prevalent during this time was the Đa Bút culture which occurred during 6000-5000 BP. The metric and nonmetric dental analyses of the Đa Bút resulted in one culture that are highly differentiated from the East Asians. A cluster analysis of the transformed Q-mode correlation coefficients of the dental crown measurements, showed a major cluster with Australo-Melanesians. Da But also has similarities with Australo-Melanesians in tooth size proportions and nonmetric traits. These close traits indicates a likeness with Australo-Melanesians.\n\nThe main controversy concerning the 'Two Layer' hypothesis is whether or not the evolutionary process truly involved the Australo-Melanesians. Archaeologists such as Matsumura suggest Southern Chinese people comprised the initial population of Southeast Asia, rather than Australo-Melanesians while researchers such as Turner argue that prehistoric Southeast Asians did not mix with either racial group.\nThough the early prehistoric Vietnamese and Malaysians both resembled the Australo-Melanesian samples the most, the Mán Bạc people had a greater resemblance to the Đông Sơn samples dating back to the Iron Age. Analyzing cranial and dental remains, Matsumura concluded based on chronological differences that the Mán Bạc people were immigrants affiliated with peoples near the Yangtze River region in Southern China. Molecular anthropologists have used classical genetic markers and mtDNA to analyze the similarities between early Chinese and Southeast Asians. Such genetic markers suggest the genetic layout of Southern Chinese peoples is quite similar to that of Southeast Asians.\n\nOther controversies completely reject the 'Two Layer' hypothesis. Using dental evidence, Turner’s Sundadont/Sinodont hypothesis suggests the “Sundadont” trait seen in present-day Southeast Asians is a result of long-standing continuity. Turner created a cluster analysis of MMD values in order to test existing hypotheses of origins, concluding that all Southeast Asians, Micronesians, Polynesians, and Jomonese form their own branch and descend from a common ancestor. The Australians and Melanesians, however, are scattered over the African and European branch along with a side branch of Tasmanians and Solomon Islanders. Howell analyzed crania of major racial branches worldwide, and linked Australian and Melanesian cranial morphology most closely with African cranials. Howell discovered, however, that the size and features of present-day Asian cranial morphology differed significantly from that of Australians, Melanesians, and Africans.\n\n\n\n"}
{"id": "873996", "url": "https://en.wikipedia.org/wiki?curid=873996", "title": "Valery Rozhdestvensky", "text": "Valery Rozhdestvensky\n\nValery Ilyich Rozhdestvensky (Russian: Валерий Ильич Рождественский, 13 February 1939 – 31 August 2011) was a Soviet cosmonaut.\n\nRozhdestvensky was born in Leningrad and graduated from the Higher Military Engineering School of Soviet Navy in Pushkin in engineering. From 1961 to 1965 he was commander of a deepsea diving unit in the Baltic Sea War Fleet.\n\nRozhdestvensky was selected as a cosmonaut on 23 October 1965 and flew as Flight Engineer on Soyuz 23. After his space flight he continued to work with the space program at the Yuri Gagarin Cosmonaut Training Center. He retired on 24 June 1986 and worked with Metropolis Industries. He was married with one child. He died on 31 August 2011 at the age of 72.\n\nHe was awarded:\n"}
{"id": "5024539", "url": "https://en.wikipedia.org/wiki?curid=5024539", "title": "Wildlife Wars: My Fight to Save Africa's Natural Treasures", "text": "Wildlife Wars: My Fight to Save Africa's Natural Treasures\n\nWildlife Wars: My Fight to Save Africa's Natural Treasures is a book written by Richard Leakey and Virginia Morell. Book was published in 2001 by St. Martin's Press.\n\nIt tells of how Leakey had been director of National Museum when appointed in 1989, President Daniel arap Moi appointed him to run the Kenya Wildlife Service. This was an entirely new experience to Leakey, because he had been accustomed to studying hominids, not managing wildlife. Elephant poaching had been a major problem in the Kenyan National Parks, and the book tells of his efforts to stop it, sometimes with a danger to his life. \n\nThe book was reviewed in the journal \"Endangered Species\", the ALA magazine \"Booklist\", \"Publishers Weekly\", \"Books in Canada\", \"African Business\", the Royal Geographical Society's \"Geographical\" magazine.\n\n"}
