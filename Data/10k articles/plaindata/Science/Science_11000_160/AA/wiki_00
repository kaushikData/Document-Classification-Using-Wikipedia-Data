{"id": "5259071", "url": "https://en.wikipedia.org/wiki?curid=5259071", "title": "A Briefer History of Time (Hawking and Mlodinow book)", "text": "A Briefer History of Time (Hawking and Mlodinow book)\n\nA Briefer History of Time is a 2005 popular-science book by the English physicist Stephen Hawking and the American physicist Leonard Mlodinow.\n\nThe book is an update and rewrite of Hawking's 1988 \"A Brief History of Time\". In this book Hawking and Mlodinow present quantum mechanics, string theory, the big bang theory, and other topics in a more accessible fashion to the general public. The book is updated with newly discovered topics, and informs of recurring subjects throughout the book in greater detail.\n"}
{"id": "52514550", "url": "https://en.wikipedia.org/wiki?curid=52514550", "title": "Acidicapsa", "text": "Acidicapsa\n\nAcidicapsa is a bacterial genus from the family of Acidobacteriaceae.\n"}
{"id": "51421991", "url": "https://en.wikipedia.org/wiki?curid=51421991", "title": "Alison Van Eenennaam", "text": "Alison Van Eenennaam\n\nAlison L. Van Eenennaam is a Cooperative Extension Specialist in the Department of Animal Science at the University of California, Davis and runs the Animal Genomics and Biotechnology Laboratory. She has served on national committees such as the USDA National Advisory Committee on Biotechnology in the 21st Century (AC21) and was awarded the 2014 Borlaug CAST Communication Award. Van Eenennaam writes the Biobeef Blog.\n\nVan Eenennaam began her university career at the University of Melbourne, Victoria, Australia in 1987, receiving a BS (Honors) degree in Agricultural Science. She received a Master of Science degree in Animal Science in 1990, and a Ph.D in Genetics in 1997, both from University of California, Davis.\n\nVan Eenennaam began her work in animal science as an intern at Genetic Resources Inc.'s Bovine Reproduction Facility in San Marcos, Texas in 1984. From 1991 to 1993 she worked as a livestock and dairy farm advisor for the UC Cooperative Extension in the San Joaquin and Sacramento Counties of California. From 1998 to 2002, following the completion of her Ph.D degree, Van Eenennaam worked for Calgene (purchased by Monsanto Corporation in 1997) in Davis, California, first as a research scientist, and then as a project leader. Since 2002, Van Eenennaam has been a Cooperative Extension Specialist in the field of Animal Genomics and Biotechnology in the Department of Animal Science at University of California, Davis.\n\nShe has served on several national committees including the USDA National Advisory Committee on Biotechnology and 21st Century Agriculture, (2005–2009), and was a temporary voting member of the 2010 FDA Veterinary Medicine Advisory Committee meeting on the AquAdvantage salmon, the first genetically engineered animal to be evaluated for entry into the food supply.\n\nThe mission of Van Eenennaam's animal biotechnology lab is to \"provide research and education on the use of animal genomics and biotechnology in livestock production systems\", with a focus on beef cattle production.\n\nVan Eenennaam's biotechnology lab at UC Davis is working on a collaborative project focused on the production of hornless dairy cattle through gene editing on a USDA National Institute of Food and Agriculture grant. This project involves using a gene sequence from Angus cattle in the genome of dairy cattle to prevent horns from growing. Van Eenennaam stated that the use of genetics rather than chemicals to solve problems can address some of the animal welfare concerns and environmental impacts of animal production. In October, 2016, this project was featured on Science Friday. Funding sources for this research and extension program are found on Van Eenennaam's public website.\n\nVan Eenennaam was appointed to the Food and Drug Administration Veterinary Medicine Advisory Committee evaluating the AquAdvantage salmon, a genetically engineered fish. A paper, authored in 2006 by Van Eenennaam with Paul Olin of University of California Cooperative Extension Sea Grant, discussed transgenic fish. The paper cites a number of benefits of genetically engineered fish, including a larger number of eggs laid per female, a low probability of carrying human pathogens, strong markets for aquaculture, and increased feed-conversion efficiency. This paper also describes the risk factor that these fish could escape breeding locations and mix with wild fish populations. \n\nIn 2014 Van Eenennaam co-authored a review article on the use of genetically modified feed for cattle. The data represented more than 100 billion animals in 29 studies and found \"GMO feed is safe and nutritionally equivalent to non-GMO feed\".\n\nVan Eenennaam won two awards from American Society of Animal Science. One was for the 2013 video \"Gene Shop\", a five-minute parody of Macklemore’s “Thrift Shop” in which Dr. Van Eenennaam and UC Davis students engagingly emphasize the importance of funding for agricultural research. The second award was for the 2012 video \"Were Those the Days My friend?\", a take on a ballad from the 1960s that highlights the importance of genetic advances for improved production efficiency and food security. This competition was designed to further the \"goal of sharing the importance of animal science with the public\".\n\nAdditional YouTube videos on biotechnology topics are linked to the BioBeef Blog written by Van Eenennaam in order \"to try to interject scientific nuance into these controversial and often politicized scientific topics\". Van Eenennaam participated in the 2014 Intelligence Squared debate on the topic of genetically modify food.\n\nIn 2014, Van Eenennaam was awarded the Borlaug CAST Communication Award by the Council for Agricultural Science and Technology (CAST), named after agricultural biologist and 1970 Nobel Peace Prize winner Norman Borlaug. CAST indicated that Van Eenennaam is known for her communication skills and praised for her understanding of biotechnology, her enthusiasm for agricultural education, and her abilities to use novel ideas to get important messages to policymakers and the public alike.\n\nVan Eenennaam appeared in the 2016 documentary production, \"Food Evolution\", written and produced by Trace Sheehan and Scott Hamilton Kennedy. The film, narrated by Neil deGrasse Tyson, features scientific experts in the areas of genetics, biology, biotechnology, and nutrition, as well as farmers and activists discussing the problems of feeding a growing global population.\n\n\nVan Eenennaam has authored or co-authored more than 80 academic articles. The following are selected articles in which Van Eenennaam is listed as the lead author.\n"}
{"id": "56375", "url": "https://en.wikipedia.org/wiki?curid=56375", "title": "Aniara", "text": "Aniara\n\nAniara () is a poem of science fiction written by Swedish Nobel laureate Harry Martinson in 1956. It was published on 13October 1956. The title comes from ancient Greek ἀνιαρός, \"sad, despairing\", plus special resonances that the sound \"a\" had for Martinson.\n\n\"Aniara\" is an effort to \"[mediate] between science and poetry, between the wish to understand and the difficulty to comprehend.\" Martinson translates scientific imagery into the poem: for example, the \"curved space\" from Einstein's general theory of relativity is likely an inspiration for Martinson's description of the cosmos as \"a bowl of glass.\" Martinson also said he was influenced by Paul Dirac.\n\nThe poem consists of 103 cantos and relates the tragedy of a spacecraft ( long and wide) which, originally bound for Mars with a cargo of colonists from the ravaged Earth, after an accident is ejected from the Solar System and into an existential struggle. The style is symbolic, sweeping and innovative for its time, with creative use of neologisms to suggest the science fictional setting:\n\nThe first 29 cantos of \"Aniara\" had previously been published in Martinson's collection \"Cikada\" (1953), under the title \"Sången om Doris och Mima\" (\"The Song of Doris and Mima\"), relating the departure from Earth, the accidental near-collision with an asteroid (incidentally named Hondo, another name for the main Japanese isle where Hiroshima is situated) and ejection from the solar system, the first few years of increasing despair and distractions of the passengers, until news is received of the destruction of their home port (and perhaps of Earth). According to Martinson, he dictated the initial cycle as in a fever after a troubling dream, affected by the Cold War and the Soviet suppression of the 1956 Hungarian revolution; in another version, the first 29 cantos were said to be inspired by an astronomic observation of Andromeda Galaxy.\n\nOne of the major themes explored is the nature and necessity of art, symbolised by the semi-mystical machinery of the \"Mima\", who relieves the ennui of crew and passengers with scenes of far-off times and places, and whose operator is also the sometimes naïve main narrator. The rooms of Mima, according to Martinson, represent different kinds of life styles or forms of consciousness. The accumulated destruction the Mima witnesses impels her to destroy herself in despair, to which she, the machine, is finally moved by the \"white tears of the granite\" melted by the \"phototurb\" which annihilates their home port, the great city of Dorisburg. Without the succour of the Mima, the erstwhile colonists seek distraction in sensual orgies, memories of their own and earlier lives, low comedy, religious cults, observations of strange astronomical phenomena, empty entertainments, science, routine tasks, brutal totalitarianism, and in all kinds of human endeavour, but ultimately cannot face the emptiness outside and inside.\n\nIn form, the poems are metrical and mostly rhymed, using both traditional and individual forms, several alluding to a wide range of Swedish and Nordic poetry, such as the Finnish Kalevala.\n\nAn opera by Karl-Birger Blomdahl also called \"Aniara\" premiered in 1959 with a libretto by Erik Lindegren based on Martinson's poem; it was staged in Hamburg, Brussels and Darmstadt.\n\nSwedish musician Kleerup released an album based on \"Aniara\" in 2012.\n\nA melding of \"Aniara\" and Beethoven's opera \"Fidelio\" was staged by the Opéra de Lyon under the direction of American artist Gary Hill in 2013.\n\nThe fourth album from the Swedish progressive metal band Seventh Wonder called \"The Great Escape\" is based on \"Aniara\".\n\nA feature film starring Emelie Jonsson was released in 2018, premiering at the Toronto International Film Festival.\n\n\"Aniara\" was translated into English as \"Aniara, A Review of Man in Time and Space\" by Hugh MacDiarmid and E. Harley Schubert in 1956. It was translated again into English by Stephen Klass and Leif Sjöberg for a 1999 edition. The book is not currently in print.\n\nTheodore Sturgeon, reviewing a 1964 American edition for a genre audience, declared that \"Martinson's achievement here is an inexpressible, immeasurable sadness. [It] transcends panic and terror and even despair [and] leaves you in the quiet immensities, with the feeling that you have spent time, and have been permanently tinted, by and with an impersonal larger-than-God force.\"\n\nThe poem was referenced in Vernor Vinge's hard science fiction novel \"A Fire Upon the Deep\".\nIt was also an influence for Poul Anderson's hard science fiction novel \"Tau Zero\".\n\nThe song \"On aika soittaa sinfonia\" (\"It's time to play a symphony\") on the Finnish rock musician Tuomari Nurmio's critically acclaimed 1982 album \"Punainen Planeetta\" (\"The Red Planet\") is inspired by the poem.\n\nThe Swedish progressive metal band Seventh Wonder's album \"The Great Escape\" (2010) contains a 30-minute track of the same name which is based on the \"Aniara\" saga.\n"}
{"id": "1068478", "url": "https://en.wikipedia.org/wiki?curid=1068478", "title": "Apparent weight", "text": "Apparent weight\n\nIn physics, apparent weight is a property of objects that corresponds to how heavy an object is. The apparent weight of an object will differ from the weight of an object whenever the force of gravity acting on the object is not balanced by an equal but opposite normal force. By definition, the weight of an object is equal to the magnitude of the force of gravity acting on it. This means that even a \"weightless\" astronaut in low Earth orbit, with an apparent weight of zero, has almost the same weight as he would have while standing on the ground; this is due to the force of gravity in low Earth orbit and on the ground being almost the same.\n\nAn object that rests on the ground is subject to a normal force exerted by the ground. The normal force acts only on the boundary of the object that is in contact with the ground. This force is transferred into the body; the force of gravity on every part of the body is balanced by stress forces acting on that part. A \"weightless\" astronaut feels weightless due to the absence of these stress forces.\nBy defining the apparent weight of an object in terms of normal forces, one can capture this effect of the stress forces. A common definition is \"the force the body exerts on whatever it rests on.\" \n\nThe apparent weight can also differ from weight when an object is \"partially or completely immersed in a fluid\", where there is an \"upthrust\" from the liquid that is working against the force of gravity. Another example is the weight of an object or person riding in an elevator. When the elevator begins rising, the object begins exerting a force in the downward direction. If a scale was used, it would be seen that the weight of the object is becoming heavier because of the downward force, changing the apparent weight.\n\nThe role of apparent weight is also important in fluidization, when dealing with a number of particles, as it is the amount of force that the \"upward drag force\" needs to overcome in order for the particles to rise and for fluidization to occur.\n"}
{"id": "45511672", "url": "https://en.wikipedia.org/wiki?curid=45511672", "title": "Boffins (TV series)", "text": "Boffins (TV series)\n\nBoffins (also known as The Boffins) is an Australian children's television programme produced by Film Australia in 1993. It was created by John Patterson and Ian Munro, who also co-created \"Johnson and Friends\". 13 episodes were produced, and the program was purchased for broadcast by ABC in 1994, but was never aired for unknown reasons. However, it was later released on home video by the ABC in 1995.\n\nBoffins follows the adventures of four tiny furry alien-like creatures known as Boffins, who spend their days in kitchen cupboards and surrounding areas, trying to discover the science behind how the world works, Madame Curie and Aristotle are very close to some of the answers, but Newton and Echo, the young boffins from the house next door are only interested in having fun!\n\nThe series was designed to introduce primary school children to some of the basic laws of science. Each episode explores a different concept; the transmission of sound, levers, pulleys, magnetism, gravity, siphons, evaporation, reflection and dispersion of light, friction, and requirements for growth.\n\n\nThe costumes were built and designed by Studio Kite.\n\nDespite being relatively obscure and never airing in Australia, Boffins was broadcast in various countries around the world - including Singapore, Malaysia, Israel, Brunei, Pakistan, Sri Lanka, Canada, The Middle East, Africa and possibly elsewhere.\n\nThe series was also released in the United States and Canada as a set of educational videos, which were only available to schools and universities.\n\nThree VHS tapes were released by ABC Video in 1995.\n\n"}
{"id": "1033516", "url": "https://en.wikipedia.org/wiki?curid=1033516", "title": "Breeding bird survey", "text": "Breeding bird survey\n\nA breeding bird survey monitors the status and trends of bird populations. Data from the survey are an important source for the range maps found in field guides. The North American Breeding Bird Survey is a joint project of the United States Geological Survey (USGS) and the Canadian Wildlife Service. The UK Breeding Bird Survey is administered by the British Trust for Ornithology, the Joint Nature Conservation Committee, and the Royal Society for the Protection of Birds.\n\nThe results of the BBS are valuable in evaluating the increasing and decreasing range of bird population which can be a key point to bird conservation. The BBS was designed to provide a continent-wide perspective of population change.\n\nThe North American Breeding Bird Survey was launched in 1966 after the concept of a continental monitoring program for all breeding birds had been developed by Chandler Robbins and his associates from the Migratory Bird Population Station. The program was developed in Laurel, Maryland. In the first year of its existence there were nearly 600 surveys conducted in the east part of the Mississippi River. One year later, in 1967, the survey spread to the Great Plains states and by 1968 almost 2000 routes had been established across southern Canada and 48 American states. As more birders were finding out about this program, the activity of BBS kept on increasing. In the 1980s, Breeding Bird Survey included areas such as Yukon, Northwest Territories of Canada and Alaska. Moreover, the number of routes placed in a number of states has had increased. Nowadays, BBS counts approximately 3700 active routes in the United States and Canada. From all the BBS routes, approximately 2900 are surveyed on a regular basis, each year. The density of the routes varies greatly across the continent and the largest number of routes can be found in New England and Mid-Atlantic states, in which there are more skilled birders to study the behavior of birds. Many bird watchers participate in these surveys as they find the experience rewarding. Currently, the BBS is planned to be expanded to parts of central and western North America as well as northern Mexico.\n\nThe surveys conducted by BBS take place during the peak of the nesting season, June, or May in countries with warmer temperatures. The BBS routes are 24.5 miles long and there are 50 stops at every 0.5 mile along the route. Routes are randomly located in order to sample habitats that are representative of the entire region. \n\nBBS data is quite difficult to analyze given that the survey does not produce a complete counting of the breeding bird populations but more like a relative abundance index. And yet, these surveys have proved to be of great value in studying the bird population trends.\n\nBBS data can also be used to produce continental-scale relative abundance maps. When analyzed at larger scales, the relative abundance maps can offer a clear indication of the relative abundances of bird species that are observed by the BBS. However, maybe the most effective use of these surveys is the opportunity to analyze population change, even though they do not provide information on the factors that cause these changes in the population trends.\n\nThe BTO/JNCC/RSPB Breeding Bird Survey (BBS) is a national project aimed at keeping track of changes in the breeding populations of widespread bird species in the UK. In the UK, there are over 3200 active routes and more than 3000 individuals involved in monitoring the population trends of more than 100 bird species.\n\nThe program started in 1992, and after being tested for two years, it was officially launched. As of 1994 the BBS data has been successfully used by Governments and different non-Governmental organizations for bird conservation purposes.\n\n\n"}
{"id": "44438966", "url": "https://en.wikipedia.org/wiki?curid=44438966", "title": "British Association for Slavonic &amp; East European Studies", "text": "British Association for Slavonic &amp; East European Studies\n\nThe British Association for Slavonic and East European Studies, is a learned society in the United Kingdom dedicated to promoting study of Russia, Eastern Europe and the former Soviet Union. It is a member of the Academy of Social Sciences.\n"}
{"id": "12318264", "url": "https://en.wikipedia.org/wiki?curid=12318264", "title": "Bulliform cell", "text": "Bulliform cell\n\nBulliform cells are large, bubble-shaped epidermal cells that occur in groups on the upper surface of the leaves of many monocots.These cells are present on the adaxial or the upper surface of the leaf. They are generally present near the mid vein. These cells are large, empty and colourless.\n\nThe mechanism of working of bulliform cells can be explained as:\n\nDuring drought, the loss of water through vacuoles induces the reduced bulliform cells to allow the leaves of many grass species to close as the two edges of the grass blade fold up toward each other. Once adequate water is available, these cells enlarge and the leaves are forced open again.\n\nFolded leaves offer less exposure to sunlight, so they are heated less thus reducing evaporation and conserving the remaining water in the plant. Bulliform cells occur on the leaves of a wide variety of monocotyledon families but are probably best known in grasses. They are thought to play a role in the unfolding of developing leaves and in the rolling and unrolling of mature leaves in response to alternating wet and dry periods.\n"}
{"id": "26837931", "url": "https://en.wikipedia.org/wiki?curid=26837931", "title": "Camp Chopunnish", "text": "Camp Chopunnish\n\nCamp Chopunnish was the first major camp on the Lewis and Clark Expedition's return voyage. It is located in Idaho County, Idaho, along the north bank of the Clearwater River, it is now part of the Nez Perce National Historical Park. It was named after Lewis' name for the Nez Perces tribe native there. In the expedition journals it was also called Long Camp (because of the duration of the stay) and Camp Kamiah (for its location).\n\nThe expedition departed Fort Clatsop, near present-day Astoria, Oregon in March 1806, after a dismal winter near the Pacific coast. By May 3, they had arrived back among the Nez Perce tribe, but determined that because of snow it was too early to cross the mountains on the Lolo Trail over the Bitterroot Mountains. From mid-May to mid-June the expedition stopped at Camp Chopunnish, stocking food for the mountain crossing.\n\nOn June 10 the group moved to a camp on Weippe Prairie in preparation for crossing the Lolo Trail, which, after an initial failure, was crossed in the last week in June.\n\nIn 1902, when historian Olin D. Wheeler visited the site, he could still see the sunken circular ring where Lewis and Clark had established their camp. Since that time, however, the integrity of the site has been destroyed. A large sawmill now covers it, and numerous other buildings are located in the vicinity. The area may be viewed from an unmarked turnout on U.S. 12 along the opposite, or southside of the river.\n\n"}
{"id": "8021451", "url": "https://en.wikipedia.org/wiki?curid=8021451", "title": "Clifford H. Stockwell", "text": "Clifford H. Stockwell\n\nClifford Howard Stockwell (September 26, 1897 – April 26, 1987) was a Canadian geologist, who published many scientific papers, reports and memoirs in the fields of Mineralogy, Structural Geology, Petrology, and Stratigraphy. He earned his PhD in geology at McGill University in Montreal in 1926.\n\nHe completed his earliest publication, \"Galena Hill, Mayo District, Yukon\", in 1925 as a graduate student. In 1927, Stockwell's paper on \"The X-ray Study of the Garnet Group\", earned him recognition, as it was an important step in the understanding of the crystal structure of these minerals. In 1933, he received acclaim for his work on \"The Genesis of Pegmatites of Southwest Manitoba\". Some of his other noteworthy papers include; \"The Chromite Deposits of the Eastern Townships, Quebec,\" \"The Gold Deposits of Herb Lake Area\", and \"The Rice Lake-Gold Lake Area\", in Manitoba.\n\nDr. Stockwell was also an explorer. In July/August 1932, he canoed through the unexplored region north of Great Slave Lake. This work delineated the basic features and problems of the geology of this large part of the Precambrian Shield, and laid the foundation for the many studies that followed.\n\nHe devoted much of his time working for the Geological Survey of Canada.\n\nIn the 1950s he concentrated on Structural Geology- the structural controls of mineral deposits.\n\nDuring World War 1, he trained to be a pilot with the Royal Flying Corps but by the time he qualified as a pilot, hostilities were over.\n\nHe was married to A. Elizabeth Johnston (1909–1964), a talented amateur oilpainter. He is buried with his wife in Pinecrest Cemetery, Ottawa.\n\n\n"}
{"id": "8490325", "url": "https://en.wikipedia.org/wiki?curid=8490325", "title": "Ctirad Uher", "text": "Ctirad Uher\n\nProfessor Ctirad Uher is the C. Wilbur Peters Collegiate Professor at the University of Michigan in Ann Arbor. Born in Prague, Czech Republic, he graduated from the University of New South Wales, Australia in 1972 and earned his Ph.D. from there in 1979. \n\nHe currently teaches in the physics department at the University of Michigan and does research in the field of condensed matter physics. Ctirad Uher was Associate Dean for two years before becoming Chair of the Physics Department in 1994. In 2004, he stepped down from that position. During Uher's administration the Physics Department faculty expanded enormously, from 60 faculty members to 80. Of these 20 new faculty, 19 eventually received tenure. It was also during this time that Martinus Veltman received his Nobel Prize. Before Uher's administration, there were no distinguished professorships in the physics department; afterward, there were six.\n\nIt was during Uher's administration that FOCUS (Frontiers in Optical Coherent and Ultrafast Science) was started, a program that continued until 2010. It was funded at first by an NSF grant and later through the MURI (Multidisciplinary University Research Initiative). Thanks in part to FOCUS, the University of Michigan was top-ranked in atomic/molecular/optical physics in the early 2000s.\n\nIt was also during Uher's term that the Michigan Center for Theoretical Physics (MCTP) was established, designed to create an intellectual atmosphere conductive to theoretical physicists, a platform for theoretical research. One of the first faculty members hired for the program was Michael Duff, a leading string theorist. The MCTP held several week-long sessions dedicated to specialized topics, for which experts were brought in from other Universities. Gordon Kane was the director of the MCTP for several years.\n\nIn 1998 the Program in Nonlinear Studies began, which in 1999 became the Center for the Study of Complex Systems, an interdisciplinary program studying complex systems that reaches out to many other fields including mathematics, biology, and economics.\n\nUher is a Fellow of the American Physical Society. In 2008, he was chosen for the American Physical Society's Outstanding Referees Program for excellence in peer review.\n\nIn 2011, Uher was awarded the Friendship Award by the People's Republic of China.\n\nDiluted Magnetic Semiconductors Based on Sb2-xVxTe3, (0.01 £ x £0.03), (J. S. Dyck, P. Hajek, P. Lostak, and C. Uher), Phys. Rev. B 65, 115212-1—115212-7 (2002).\n\nSkutterudites: Prospective Novel Thermoelectrics in Semiconductors and Semimetals, (C. Uher, ed. T.M. Tritt), Vol. 69, Academic Press, San Diego, 139-253 (2001).\n\nAnomalous Barium Filling Fraction and n-type Thermoelectric Performance of BayCo4Sb12, (L. D. Chen, T. Kawahara, X. F. Tang, T. Goto, T. Hirai, J. S. Dyck, W. Chen, and C. Uher), J. Appl. Phys. 90, 1864-1868 (2001).\n\nCsBi4Te6: A High-Performance Thermoelectric Material for Low-Temperature Applications (D.Y. Chung, T. Hogan, P. Brazis, M. Rocci-Lane, C. Kannawurf, M. Bastea, C. Uher, and M. Kanatzidis), Science 287, 1024-1027 (2000).\n\nMaterials with Open Crystal Structure as Prospective Novel Thermoelectrics, (C. Uher, J. Yang, S. Hu), Mater. Res. Soc. Symp. Proc., 545, 247-258 (1999).\n\nStructure and Lattice Thermal Conductivity of Fractionally Filled Skutterudites: Solid Solutions of Fully Filled and Unfilled Endmembers, (G.P. Meisner, D.T. Morelli, S. Hu, J. Yang, and C. Uher),Phys. Rev. Lett. 80, 3551-3554 (1998).\n\nCerium Filling and Doping of Cobalt Triantimonide, (D.T. Morelli, G.P. Meisner, B. Chen, S. Hu, and C. Uher), Phys. Rev. B 56, 7376 (1997).\n\nHeat Conduction of (111) Co/Cu Superlattices, (F. Tsui, B. Chen, J. Wellman, R. Clarke, and C. Uher), J. Appl. Phys. 81, 4586-4588 (1997).\n\nThermal Conductivity of High-Temperature Superconductors, (C. Uher, edited by D.M. Ginsberg), in Physical Properties of High Temperature Superconductors, World Scientific, Vol. III, 159-284 (1992).\n\n\"Thermal conductivity 25 : thermal expansion 13 : joint conferences, June 13–16, 1999, Ann Arbor, Michigan, USA\" / Ctirad Uher, editor of proceedings ; Donald Morelli, co-editor of proceedings ; conference host, University of Michigan, Ann Arbor, Michigan, USA. Lancaster, Pa. : Technomic Pub. Co., [c2000] xiv, 391 p. : ill. ; 24 cm. \n"}
{"id": "13144407", "url": "https://en.wikipedia.org/wiki?curid=13144407", "title": "Cultural practice", "text": "Cultural practice\n\nCultural practice generally refers to the manifestation of a culture or sub-culture, especially in regard to the traditional and customary practices of a particular ethnic or other cultural group. In the broadest sense, this term can apply to any person manifesting any aspect of any culture at any time. However, in practical usage it often refers to the traditional practices developed within specific ethnic cultures, especially those aspects of culture that have been practiced since ancient times.\n\nThe term is gaining in importance due to the increased controversy over \"rights of cultural practice\", which are protected in many jurisdictions for indigenous peoples and sometimes ethnic minorities. It is also a major component of the field of cultural studies, and is a primary focus of international works such as the United Nations declaration of the rights of indigenous Peoples.\n\nCultural practice is also a subject of discussion in questions of cultural survival. If an ethnic group retains its formal ethnic identity but loses its core cultural practices or the knowledge, resources, or ability to continue them, questions arise as to whether the culture is able to actually survive at all. International bodies such as the United Nations Permanent Forum on Indigenous Issues continually work on these issues, which are increasingly at the forefront of globalization questions.\n\n\nThe real question of what qualifies as a legitimate cultural practice is the subject of much legal and ethnic community debate. The question arises in controversial subject areas such as genital mutilation, indigenous hunting and gathering practices, and the question of licensing of traditional medical practitioners.\n\nMany traditional cultures acknowledge members outside of their ethnicity as cultural practitioners, but only under special circumstances. Generally, the knowledge or title must be passed in a traditional way, such as family knowledge shared through adoption, or through a master of that practice choosing a particular student who shows qualities desired for that practice, and teaching that student in a hands-on manner, in which they are able to absorb the core values and belief systems of the culture. The degree to which these non-ethnic practitioners are able to exercise \"customary and traditional\" rights, and the degree to which their practice is acknowledged as valid, is often a subject of considerable debate among indigenous and other ethnic communities, and sometimes with the legal systems under which these communities function. The difference between bona fide non-native cultural practitioners and cultural piracy, or cultural appropriation, is a major issue within the study of globalization and modernization.\n\nThe evolution of traditional cultures is a subject of much discussion in legal, scholarly, and community forums. It is generally accepted that all cultures are to some degree in a continual state of sociocultural evolution. However, major questions surround the legitimacy of newly evolved cultural expressions, especially when these are influenced by modernization or by the influence of other cultures. Also, there is significant debate surrounding the source of evolution: for example, an indigenous community may accept the use of store-bought materials in the creation of traditional arts, but may reject requirements to apply for a permit for certain gathering purposes; the central difference being that one is an \"internal\" cultural evolution, while the other is \"externally\" driven by the society or legal body that surrounds the culture.\n"}
{"id": "2780651", "url": "https://en.wikipedia.org/wiki?curid=2780651", "title": "Discovery science", "text": "Discovery science\n\n\"Discovery science\" (also known as discovery-based science) is a scientific methodology which emphasizes analysis of large volumes of experimental data with the goal of finding new patterns or correlations, leading to hypothesis formation and other scientific methodologies.\n\nDiscovery-based methodologies are often viewed in contrast to traditional scientific practice, where hypotheses are formed before close examination of experimental data. However, from a philosophical perspective where all or most of the observable \"low hanging fruit\" has already been plucked, examining the phenomenological world more closely than the senses alone (even augmented senses, e.g. via microscopes, telescopes, bifocals etc.) opens a new source of knowledge for hypothesis formation. This process is also known as inductive reasoning or the use of specific observations to make generalizations.\n\nData mining is the most common tool used in discovery science, and is applied to data from diverse fields of study such as DNA analysis, climate modeling, nuclear reaction modeling, and others.\n\nThe use of data mining in discovery science follows a general trend of increasing use of computers and computational theory in all fields of science. Further following this trend, the cutting edge of data mining employs specialized machine learning algorithms for automated hypothesis forming and automated theorem proving.\n"}
{"id": "53771690", "url": "https://en.wikipedia.org/wiki?curid=53771690", "title": "Earth forest", "text": "Earth forest\n\nAn earth forest is a form of geomorphology caused by earth movement and erosion. It takes the form of pillars which from a distance give the impression of a forest.\n\nThere are multiple examples of earth forest in Yuanmou County in the Yunnan Province of China, in an area of about 50 square kilometers. The tallest formation has a height of . The features are one to two million years old. The area was opened for tourism as the Yuanmou Earth Forest (\"Tulin\" in Chinese) in 1985.\n"}
{"id": "7259047", "url": "https://en.wikipedia.org/wiki?curid=7259047", "title": "Ekman water bottle", "text": "Ekman water bottle\n\nThe Ekman water bottle is a sea water temperature sample device. The cylinder is dropped at the desired depth, the trap door below is opened to let the water enter and then closed tightly. This can be repeated at different depths as each sample goes to a different chamber of the insulated bottle.\n\nIt was used for greater depths during the research cruises of 1903 and 1904. The first instruments made, however, were too delicate; after being used for some time, the brass rods which press the lids towards both ends of the cylinder and close the water-bottle, became bent and therefore did not work sufficiently well. For this reason the instruments had to be frequently tested and repaired. As they are now made, they work very well and are very easily handled.\n\n\n"}
{"id": "11071609", "url": "https://en.wikipedia.org/wiki?curid=11071609", "title": "Essentials of Programming Languages", "text": "Essentials of Programming Languages\n\nEssentials of Programming Languages (EOPL) is a textbook on programming languages by Daniel P. Friedman, Mitchell Wand, and Christopher T. Haynes.\n\nEOPL surveys the principles of programming languages from an operational perspective. It starts with an interpreter in Scheme for a simple functional core language similar to the lambda calculus and then systematically adds constructs. For each addition, for example, variable assignment or thread-like control, the book illustrates an increase in expressive power of the programming language and a demand for new constructs for the formulation of a direct interpreter. The book also demonstrates that systematic transformations, say, store-passing style or continuation-passing style, can eliminate certain constructs from the language in which the interpreter is formulated.\n\nThe second part of the book is dedicated to a systematic translation of the interpreter(s) into register machines. The transformations show how to eliminate higher-order closures; continuation objects; recursive function calls; and more. At the end, the reader is left with an \"interpreter\" that uses nothing but tail-recursive function calls and assignment statements plus conditionals. It becomes trivial to translate this code into a C program or even an assembly program. As a bonus, the book shows how to pre-compute certain pieces of \"meaning\" and how to generate a representation of these pre-computations. Since this is the essence of compilation, the book also prepares the reader for a course on the principles of compilation and language translation, a related but distinct topic.\n\nLike SICP, EOPL represents a significant departure from the prevailing textbook approach in the 1980s. At the time, a book on the principles of programming languages presented four to six (or even more) programming languages and discussed their programming idioms and their implementation at a high level. The most successful books typically covered ALGOL 60 (and the so-called Algol family of programming languages), SNOBOL, Lisp, and Prolog. Even today a fair number of textbooks on programming languages are just such surveys, though their scope has narrowed.\n\nEOPL was started in 1983 when Indiana was one of the leading departments in programming languages research. Eugene Kohlbecker, one of Friedman's PhD students, transcribed and collected his \"311 lectures\". Other faculty members, including Mitch Wand and Christopher Haynes, started contributing and turned \"The Hitchhiker's Guide to the Meta-Universe\"—as Kohlbecker had called it—into the systematic, interpreter and transformation-based survey that it is now. Over the 25 years of its existence, the book has become a near-classic; it is now in its third edition, including additional topics such as types and modules. Its first part now incorporates ideas on programming from HtDP, another unconventional textbook, which uses Scheme to teach the principles of program design. The authors as well as Matthew Flatt have recently provided DrRacket plug-ins and language levels for teaching with EOPL.\n\nEOPL has spawned at least two other, related texts: Queinnec's \"Lisp in Small Pieces\" and Krishnamurthi's \".\n\n\n"}
{"id": "38617311", "url": "https://en.wikipedia.org/wiki?curid=38617311", "title": "Fair Access to Science and Technology Research Act", "text": "Fair Access to Science and Technology Research Act\n\nThe Fair Access to Science and Technology Research Act (FASTR) is a bill in the United States that would mandate earlier public release of taxpayer-funded research. The bill has been introduced in 2013, 2015, and 2017. Sen. Ron Wyden (D-Ore.) and Sen. John Cornyn (R-Texas) introduced the Senate version, while the bill was introduced to the House by Reps. Zoe Lofgren (D-Calif.), Mike Doyle (D-Penn.) and Kevin Yoder (R-Kans.). The bill is a successor to the Federal Research Public Access Act (FRPAA), which had been introduced in 2006, 2010, and 2012.\n\nSen. Wyden explained the bill in a press release: \n\"Breakthroughs in technology, science, medicine and dozens of other disciplines are made every year due to the billions in research funding provided by the American people. Making those findings available to all Americans is the best way to lead the next generation of discovery and innovation or create the next game-changing business. The FASTR act provides that access because taxpayer funded research should never be hidden behind a paywall.\"\n\nFASTR has been described as \"The Other Aaron's Law\", named for open-access activist Aaron Swartz who died in January 2013.\n\nThe Senate Committee on Homeland Security and Governmental Affairs unanimously approved the bill on July 29, 2015. It was the first time that the bill or any of its predecessors had gained committee approval and been forwarded to a full house of Congress.\n\nThe bill is often compared to and discussed in conjunction with the Public Access to Public Science (PAPS) Act, also introduced in 2013.\n\nDays after FASTR was introduced in 2013, the Executive Branch's Office of Science and Technology Policy (OSTP) issued a memorandum that \"hereby directs each Federal agency with over $100 million in annual conduct of research and development expenditures to develop a plan to support increased public access to the results of research funded by the Federal Government.\" The change was in part prompted by an online Whitehouse petition to \"Require free access over the Internet to scientific journal articles arising from taxpayer-funded research.\" \n\n"}
{"id": "40365", "url": "https://en.wikipedia.org/wiki?curid=40365", "title": "Galvanometer", "text": "Galvanometer\n\nA galvanometer is an electromechanical instrument used for detecting and indicating an electric current. A galvanometer works as an actuator, by producing a rotary deflection (of a \"pointer\"), in response to electric current flowing through a coil in a constant magnetic field. Early galvanometers were not calibrated, but their later developments were used as measuring instruments, called ammeters, to measure the current flowing through an electric circuit. \n\nGalvanometers developed from the observation that the needle of a magnetic compass is deflected near a wire that has electric current flowing through it, first described by Hans Christian Ørsted in 1820. They were the first instruments used to detect and measure small amounts of electric currents. André-Marie Ampère, who gave mathematical expression to Ørsted's discovery and named the instrument after the Italian electricity researcher Luigi Galvani, who in 1791 discovered the principle of the frog galvanoscope – that electric current would make the legs of a dead frog jerk.\n\nSensitive galvanometers have been essential for the development of science and technology in many fields. For example, they enabled long range communication through submarine cables, such as the earliest Transatlantic telegraph cables, and were essential to discovering the electrical activity of the heart and brain, by their fine measurements of current.\n\nGalvanometers also had widespread use as the visualising part in other kinds of analog meters, for example in light meters, VU meters, etc., where they were used to measure and display the output of other sensors. Today the main type of galvanometer mechanism, still in use, is the moving coil, D'Arsonval/Weston type.\n\nModern galvanometers, of the D'Arsonval/Weston type, are constructed with a small pivoting coil of wire, called a spindle, in the field of a permanent magnet. The coil is attached to a thin pointer that traverses a calibrated scale. A tiny torsion spring pulls the coil and pointer to the zero position.\n\nWhen a direct current (DC) flows through the coil, the coil generates a magnetic field. This field acts against the permanent magnet. The coil twists, pushing against the spring, and moves the pointer. The hand points at a scale indicating the electric current. Careful design of the pole pieces ensures that the magnetic field is uniform, so that the angular deflection of the pointer is proportional to the current. A useful meter generally contains provision for damping the mechanical resonance of the moving coil and pointer, so that the pointer settles quickly to its position without oscillation.\n\nThe basic sensitivity of a meter might be, for instance, 100 microamperes full scale (with a voltage drop of, say, 50 millivolts at full current). Such meters are often calibrated to read some other quantity that can be converted to a current of that magnitude. The use of current dividers, often called shunts, allows a meter to be calibrated to measure larger currents. A meter can be calibrated as a DC voltmeter if the resistance of the coil is known by calculating the voltage required to generate a full scale current. A meter can be configured to read other voltages by putting it in a voltage divider circuit. This is generally done by placing a resistor in series with the meter coil. A meter can be used to read resistance by placing it in series with a known voltage (a battery) and an adjustable resistor. In a preparatory step, the circuit is completed and the resistor adjusted to produce full scale deflection. When an unknown resistor is placed in series in the circuit the current will be less than full scale and an appropriately calibrated scale can display the value of the previously unknown resistor.\n\nThese capabilities to translate different kinds of electric quantities, in to pointer movements, make the galvanometer ideal for turning output of other sensors that outputs electricity (in some form or another), into something that can be read by a human.\n\nBecause the pointer of the meter is usually a small distance above the scale of the meter, parallax error can occur when the operator attempts to read the scale line that \"lines up\" with the pointer. To counter this, some meters include a mirror along the markings of the principal scale. The accuracy of the reading from a mirrored scale is improved by positioning one's head while reading the scale so that the pointer and the reflection of the pointer are aligned; at this point, the operator's eye must be directly above the pointer and any parallax error has been minimized.\n\nProbably the largest use of galvanometers was of the D'Arsonval/Weston type used in analog meters in electronic equipment. Since the 1980s, galvanometer-type analog meter movements have been displaced by analog to digital converters (ADCs) for many uses. A digital panel meter (DPM) contains an analog to digital converter and numeric display. The advantages of a digital instrument are higher precision and accuracy, but factors such as power consumption or cost may still favour application of analog meter movements.\n\nMost modern uses for the galvanometer mechanism are in positioning and control systems. \nGalvanometer mechanisms are divided into moving magnet and moving coil galvanometers; in addition, they are divided into \"closed-loop\" and \"open-loop\" - or \"resonant\" - types.\n\n\"Mirror\" galvanometer systems are used as beam positioning or beam steering elements in laser scanning systems. For example, for material processing with high-power lasers, closed loop mirror galvanometer mechanisms are used with servo control systems. These are typically high power galvanometers and the newest galvanometers designed for beam steering applications can have frequency responses over 10 kHz with appropriate servo technology. Closed-loop mirror galvanometers are also used in similar ways in stereolithography, laser sintering, laser engraving, laser beam welding, laser TVs, laser displays and in imaging applications such as retinal scanning with Optical Coherence Tomography (OCT). Almost all of these galvanometers are of the moving magnet type. The closed loop is obtained measuring the position of the rotating axis with an infrared emitter and 2 photodiodes. This feedback is an analog signal.\n\nOpen loop, or resonant mirror galvanometers, are mainly used in some types of laser-based bar-code scanners, printing machines, imaging applications, military applications and space systems. Their non-lubricated bearings are especially of interest in applications that require functioning in a high vacuum.\n\nMoving coil type galvanometer mechanisms (called 'voice coils' by hard disk manufacturers) are used for controlling the \"head positioning\" servos in hard disk drives and CD/DVD players, in order to keep mass (and thus access times), as low as possible.\n\nA major early use for galvanometers was for finding faults in telecommunications cables. They were superseded in this application late in the 20th century by time-domain reflectometers.\n\nGalvanometer mechanisms were also used to get readings from photoresistors in the metering mechanisms of film cameras (as seen in the adjacent image).\n\nIn analog strip chart recorders such as used in electrocardiographs, electroencephalographs and polygraphs, galvanometer mechanisms were used to position the \"pen\". Strip chart recorders with galvanometer driven pens may have a full scale frequency response of 100 Hz and several centimeters of deflection.\n\nThe deflection of a magnetic compass needle by current in a wire was first described by Hans Oersted in 1820. The phenomenon was studied both for its own sake and as a means of measuring electric current. \n\nThe earliest galvanometer was reported by Johann Schweigger at the University of Halle on 16 September 1820. André-Marie Ampère also contributed to its development. Early designs increased the effect of the magnetic field generated by the current by using multiple turns of wire. The instruments were at first called \"multipliers\" due to this common design feature. The term \"galvanometer,\" in common use by 1836, was derived from the surname of Italian electricity researcher Luigi Galvani, who in 1791 discovered that electric current would make a dead frog's leg jerk.\n\nOriginally, the instruments relied on the Earth's magnetic field to provide the restoring force for the compass needle. These were called \"tangent\" galvanometers and had to be oriented before use. Later instruments of the \"astatic\" type used opposing magnets to become independent of the Earth's field and would operate in any orientation. The most sensitive form, the Thomson or mirror galvanometer, was patented in 1858 by William Thomson (Lord Kelvin) as an improvement of an earlier design invented in 1826 by Johann Christian Poggendorff. Thomson's design was able to detect very rapid current changes by using small magnets attached to a lightweight mirror, suspended by a thread, instead of a compass needle. The deflection of a light beam on the mirror greatly magnified the deflection induced by small currents. Alternatively, the deflection of the suspended magnets could be observed directly through a microscope.\n\nThe ability to measure quantitatively voltage and current allowed Georg Ohm, in 1827, to formulate Ohm's Law – that the voltage across a conductor is directly proportional to the current through it.\n\nThe early moving-magnet form of galvanometer had the disadvantage that it was affected by any magnets or iron masses near it, and its deflection was not linearly proportional to the current. In 1882 Jacques-Arsène d'Arsonval and Marcel Deprez developed a form with a stationary permanent magnet and a moving coil of wire, suspended by fine wires which provided both an electrical connection to the coil and the restoring torque to return to the zero position. An iron tube between the magnet's pole pieces defined a circular gap through which the coil rotated. This gap produced a consistent, radial magnetic field across the coil, giving a linear response throughout the instrument's range. A mirror attached to the coil deflected a beam of light to indicate the coil position. The concentrated magnetic field and delicate suspension made these instruments sensitive; d'Arsonval's initial instrument could detect ten microamperes.\n\nEdward Weston extensively improved the design. He replaced the fine wire suspension with a pivot, and provided restoring torque and electrical connections through spiral springs rather like those of a wristwatch balance wheel hairspring. He developed a method of stabilizing the magnetic field of the permanent magnet, so the instrument would have consistent accuracy over time. He replaced the light beam and mirror with a knife-edge pointer that could be read directly. A mirror under the pointer, in the same plane as the scale, eliminated parallax observation error. To maintain the field strength, Weston's design used a very narrow circumferential slot through which the coil moved, with a minimal air-gap. This improved linearity of pointer deflection with respect to coil current. Finally, the coil was wound on a light-weight form made of conductive metal, which acted as a damper. By 1888, Edward Weston had patented and brought out a commercial form of this instrument, which became a standard electrical equipment component. It was known as a \"portable\" instrument because it was affected very little by mounting position or by transporting it from place to place. This design is almost universally used in moving-coil meters today.\n\nInitially laboratory instruments relying on the Earth's own magnetic field to provide restoring force for the pointer, galvanometers were developed into compact, rugged, sensitive portable instruments essential to the development of electro-technology.\n\nThe taut-band movement is a modern development of the D'Arsonval-Weston movement. The jewel pivots and hairsprings are replaced by tiny strips of metal under tension. Such a meter is more rugged for field use.\n\nSome galvanometers use a solid pointer on a scale to show measurements; other very sensitive types use a miniature mirror and a beam of light to provide mechanical amplification of low-level signals.\n\nA tangent galvanometer is an early measuring instrument used for the measurement of electric current. It works by using a compass needle to compare a magnetic field generated by the unknown current to the magnetic field of the Earth. It gets its name from its operating principle, the tangent law of magnetism, which states that the tangent of the angle a compass needle makes is proportional to the ratio of the strengths of the two perpendicular magnetic fields. It was first described by Claude Pouillet in 1837.\n\nA tangent galvanometer consists of a coil of insulated copper wire wound on a circular non-magnetic frame. The frame is mounted vertically on a horizontal base provided with levelling screws. The coil can be rotated on a vertical axis passing through its centre. A compass box is mounted horizontally at the centre of a circular scale. It consists of a tiny, powerful magnetic needle pivoted at the centre of the coil. The magnetic needle is free to rotate in the horizontal plane. The circular scale is divided into four quadrants. Each quadrant is graduated from 0° to 90°. A long thin aluminium pointer is attached to the needle at its centre and at right angle to it. To avoid errors due to parallax, a plane mirror is mounted below the compass needle.\n\nIn operation, the instrument is first rotated until the magnetic field of the Earth, indicated by the compass needle, is parallel with the plane of the coil. Then the unknown current is applied to the coil. This creates a second magnetic field on the axis of the coil, perpendicular to the Earth's magnetic field. The compass needle responds to the vector sum of the two fields, and deflects to an angle equal to the tangent of the ratio of the two fields. From the angle read from the compass's scale, the current could be found from a table. The current supply wires have to be wound in a small helix, like a pig's tail, otherwise the field due to the wire will affect the compass needle and an incorrect reading will be obtained.\n\nThe galvanometer is oriented so that the plane of the coil is vertical and aligned along parallel to the horizontal component of the Earth's magnetic field (i.e. parallel to the local \"magnetic meridian\"). When an electric current flows through the galvanometer coil, a second magnetic field is created. At the center of the coil, where the compass needle is located, the coil's field is perpendicular to the plane of the coil. The magnitude of the coil's field is:\nwhere is the current in amperes, is the number of turns of the coil and is the radius of the coil. These two perpendicular magnetic fields add vectorially, and the compass needle points along the direction of their resultant . The current in the coil causes the compass needle to rotate by an angle :\nFrom tangent law, , i.e.\nor\nor , where is called the Reduction Factor of the tangent galvanometer.\n\nOne problem with the tangent galvanometer is that its resolution degrades at both high currents and low currents. The maximum resolution is obtained when the value of is 45°. When the value of is close to 0° or 90°, a large percentage change in the current will only move the needle a few degrees. \n\nA tangent galvanometer can also be used to measure the magnitude of the horizontal component of the geomagnetic field. When used in this way, a low-voltage power source, such as a battery, is connected in series with a rheostat, the galvanometer, and an ammeter. The galvanometer is first aligned so that the coil is parallel to the geomagnetic field, whose direction is indicated by the compass when there is no current through the coils. The battery is then connected and the rheostat is adjusted until the compass needle deflects 45 degrees from the geomagnetic field, indicating that the magnitude of the magnetic field at the center of the coil is the same as that of the horizontal component of the geomagnetic field. This field strength can be calculated from the current as measured by the ammeter, the number of turns of the coil, and the radius of the coils.\n\nUnlike the tangent galvanometer, the astatic galvanometer does not use the Earth's magnetic field for measurement, so it does not need to be oriented with respect to the Earth's field, making it easier to use. It has two magnetized needles parallel to each other, but with the magnetic poles reversed. suspended by a silk thread. The lower needle is inside a vertical current sensing coil of wire and is deflected by the magnetic field created by the passing current, as in the tangent galvanometer above. The purpose of the second needle is to cancel the dipole moment of the first needle, so the suspended armature has no net magnetic dipole moment, and thus is not affected by the earth's magnetic field. The needle's rotation is opposed by the torsional elasticity of the suspension thread, which is proportional to the angle.\n\nThe \"astatic galvanometer\" was developed by Leopoldo Nobili in 1825.\n\nTo achieve higher sensitivity to detect extremely small currents, the mirror galvanometer substituted a lightweight mirror for the pointer. It consisted of horizontal magnets suspended from a fine fiber, inside a vertical coil of wire, with a mirror attached to the magnets. A beam of light reflected from the mirror fell on a graduated scale across the room, acting as a long mass-less pointer. The mirror galvanometer was used as the receiver in the first trans-Atlantic submarine telegraph cables in the 1850s, to detect the extremely faint pulses of current after their thousand mile journey under the Atlantic. In a device called an oscillograph, the moving beam of light was used, to produce graphs of current versus time, by recording measurements on photographic film. The string galvanometer was a type of mirror galvanometer so sensitive that it was used to make the first electrocardiogram of the electrical activity of the human heart.\n\nA ballistic galvanometer is a type of sensitive galvanometer for measuring the quantity of charge discharged through it. In reality it is an integrator, unlike a current-measuring galvanometer, the moving part has a large moment of inertia that gives it a long oscillation period. It can be either of the moving coil or moving magnet type; commonly it is a mirror galvanometer.\n\n\n"}
{"id": "351422", "url": "https://en.wikipedia.org/wiki?curid=351422", "title": "Georgy Grechko", "text": "Georgy Grechko\n\nGeorgy Mikhaylovich Grechko (; 25 May 1931 – 8 April 2017) was a Soviet cosmonaut who flew on several space flights including Soyuz 17, Soyuz 26, and Soyuz T-14.\n\nGrechko graduated from the Leningrad Institute of Mechanics with a doctorate in mathematics. He was a member of Communist Party of the Soviet Union. He went on to work at Sergei Korolev's design bureau and from there was selected for cosmonaut training for the Soviet moon programme. When that program was cancelled, he went on to work on the Salyut space stations.\n\nGrechko made the first spacewalk in an Orlan space suit on 20 December 1977 during the Salyut 6 EO-1 mission.\n\nHe was twice awarded the medal of Hero of the Soviet Union.\n\nHe resigned from the space programme in 1992 to lecture on atmospheric physics at the Russian Academy of Sciences. Grechko has written his memoirs as \"Космонавт No. 34: От лучины до пришельцев,\" (Cosmonaut No. 34 From Splinter to Aliens) Olma Media Grupp, Moscow, 2013.\n\nA minor planet 3148 Grechko discovered by Soviet astronomer Nikolai Stepanovich Chernykh in 1979 is named after him.\n\nGrechko had a brief cameo role in Richard Viktorov's 1981 film \"Per Aspera Ad Astra\", and as a result attained pop culture status in his home city of Leningrad. He also appeared in the 1979 film \"Under the Constellation Gemini\".\n\nGrechko, along with Alexey Leonov, Vitaly Sevastyanov and Rusty Schweickart established the Association of Space Explorers in 1984. Membership is open to all people who have flown in outer space.\n\nGrechko died aged 85 as a result of several chronic illnesses. He was survived by wife Lyudmila and daughter Olga.\n\n\n"}
{"id": "6219644", "url": "https://en.wikipedia.org/wiki?curid=6219644", "title": "High Luminosity Large Hadron Collider", "text": "High Luminosity Large Hadron Collider\n\nThe High Luminosity Large Hadron Collider (HL-LHC; formerly SLHC, Super Large Hadron Collider) is an upgrade to the Large Hadron Collider started in June 2018 that will boost the accelerator's potential for new discoveries in physics, starting in 2026. The upgrade aims at increasing the luminosity of the machine by a factor of 10, up to 10 cms, providing a better chance to see rare processes and improving statistically marginal measurements.\n\nMany different paths exist for upgrading the collider. A collection of different designs of the high luminosity interaction regions is being maintained by the European Organization for Nuclear Research (CERN).\nA workshop was held in 2006 to establish which are the most promising options.\n\nIncreasing LHC luminosity involves reduction of beam size at the collision point and either reduction of bunch length and spacing, or significant increase in bunch length and population. The maximum integrated luminosity increase of the existing options is about a factor of 4 higher than the LHC ultimate performance, unfortunately far below the LHC upgrade project's initial ambition of a factor of 10. However, at the latest LUMI'06 workshop, several suggestions were proposed that would boost the LHC peak luminosity by a factor of 10 beyond nominal towards 10 cms.\n\nThe resultant higher event rate poses important challenges for the particle detectors located in the collision areas.\n\nAs part of the Phase 2 Super LHC, significant changes will be made to the proton injector.\n\nSuperconducting Proton Linac (SPL): Accelerating protons with superconducting radio frequency cavities to an energy of 5 GeV.\n\nProton Synchrotron 2 (PS2): Accelerating the beam from 5 GeV at injection to 50 GeV at extraction.\n\nSuper Proton Synchrotron (SPS) Upgraded: The present SPS will be substantially upgraded to handle an increased beam intensity from PS2.\n\n"}
{"id": "2866348", "url": "https://en.wikipedia.org/wiki?curid=2866348", "title": "Incubous", "text": "Incubous\n\nThe term incubous describes the way in which the leaves of a liverwort are attached to the stem. If one were to look down from above (dorsal side) on a plant where the leaf attachment is \"incubous\", the upper edge of each leaf would overlap the next higher leaf along the stem. Because of this, the upper edge of each leaf is visible from above, but the lower edge of each leaf is obscured by its neighboring leaf. The opposite of \"incubous\" is succubous.\n"}
{"id": "947674", "url": "https://en.wikipedia.org/wiki?curid=947674", "title": "Ivy Mike", "text": "Ivy Mike\n\nIvy Mike was the codename given to the first test of a full-scale thermonuclear device, in which part of the explosive yield comes from nuclear fusion. It was detonated on November 1, 1952 by the United States on the island of Elugelab in Enewetak Atoll, in the Pacific Ocean, as part of Operation Ivy. It was the first full test of the Teller–Ulam design, a staged fusion device.\n\nDue to its physical size and fusion fuel type (cryogenic liquid deuterium), the Mike device was not suitable for use as a deliverable weapon; it was intended as an extremely conservative proof of concept experiment to validate the concepts used for multi-megaton detonations. A simplified and lightened bomb version (the EC-16) was prepared and scheduled to be tested in operation Castle Yankee, as a backup in case the non-cryogenic \"Shrimp\" fusion device (tested in Castle Bravo) failed to work; that test was cancelled when the Bravo device was tested successfully, making the cryogenic designs obsolete.\n\nThe 82-ton \"Mike\" device was essentially a building that resembled a factory rather than a weapon. It has been reported that Soviet engineers derisively referred to Mike as a \"thermonuclear installation\". At its center, a very large cylindrical thermos flask or cryostat held the cryogenic deuterium fusion fuel. A regular fission bomb (the \"primary\") at one end was used to create the conditions needed to initiate the fusion reaction.\n\nThe device was designed by Richard Garwin, a student of Enrico Fermi, on the suggestion of Edward Teller. It had been decided that nothing other than a full-scale test would validate the idea of the Teller-Ulam design, and Garwin was instructed to use very conservative estimates when designing the test, and that it need not be small and light enough to be deployed by air.\n\nThe primary stage was a TX-5 fission bomb (it was not boosted) nested inside the radiation case at the upper section of the device, and it was not in contact with the \"secondary\" fusion stage (the TX-5 primary was also not refrigerated). The \"secondary\" fusion stage used liquid deuterium despite the difficulty of handling this material, because this fuel simplified the experiment, and made the results easier to analyze. Running down the center of the flask which held it, was a cylindrical rod of plutonium (the \"sparkplug\") to ignite the fusion reaction. Surrounding this assembly was a five-ton (4.5 tonne) natural uranium \"tamper\". The exterior of the tamper was lined with sheets of lead and polyethylene, which formed a radiation channel to conduct X-rays from the primary to secondary. The function of the X-rays was to compress the secondary; with tamper/pusher ablation, foam plasma pressure and radiation pressure. This process increases the density and temperature of the deuterium to the level needed to sustain a thermonuclear reaction, and compress the sparkplug to a supercritical mass - Inducing the sparkplug to undergo nuclear fission, to start a fusion reaction in the surrounding deuterium fuel. The outermost layer was a steel casing 10–12 inches (25–30 cm) thick. The entire assembly, nicknamed \"Sausage\", measured 80 inches (2.03 m) in diameter and 244 inches (6.19 m) in height and weighed about 54 tons.\n\nThe entire Mike device (including cryogenic equipment) weighed 82 short tons (73.8 metric tonnes), and was housed in a large corrugated-aluminium building called a \"shot cab\" which was set up on the Pacific island of Elugelab, part of the Enewetak atoll.\n\nA 9,000-foot (2.7 km) artificial causeway connected the islands of Elugelab, Teiter, Bogairikk, and Bogon. Atop this causeway was an aluminium-sheathed plywood tube (named a \"Krause-Ogle box\") filled with helium ballonets. This allowed gamma and neutron radiation to pass uninhibited to an unmanned detection station housed in a bunker on Bogon.\n\nIn total, 9,350 military and 2,300 civilian personnel were involved in the Mike shot. A large cryogenics plant was installed on Parry Island, at the South end of the Enewetak atoll, to produce the liquid hydrogen (used for cooling the device) and deuterium needed for the test.\n\nThe test was carried out on 1 November 1952 at 07:15 local time (19:15 on 31 October, Greenwich Mean Time). It produced a yield of 10.4 megatons of TNT. However, 77% of the final yield came from fast fission of the uranium tamper, which produced large amounts of radioactive fallout.\n\nThe fireball created by the explosion had a maximum radius of . This maximum is reached a number of seconds after the detonation and during this time the hot fireball invariably rises due to buoyancy. While still relatively close to the ground, the fireball had yet to reach its maximum dimensions and was thus approximately wide. The mushroom cloud rose to an altitude of in less than 90 seconds. One minute later it had reached , before stabilizing at with the top eventually spreading out to a diameter of with a stem wide.\n\nThe blast created a crater in diameter and deep where Elugelab had once been; the blast and water waves from the explosion (some waves up to high) stripped the test islands clean of vegetation, as observed by a helicopter survey within 60 minutes after the test, by which time the mushroom cloud and steam were blown away. Radioactive coral debris fell upon ships positioned away, and the immediate area around the atoll was heavily contaminated for some time. Two new elements, einsteinium and fermium, were produced by intensely concentrated neutron flux about the detonation site. One USAF pilot was lost when his F-86 Sabre crashed during a sampling mission.\n\nClose to the fireball, lightning discharges were rapidly triggered.\n\nThe entire shot was documented by the filmmakers of Lookout Mountain studios. A post production explosion sound was overdubbed over what was a completely silent detonation from the vantage point of the camera, with the blast wave sound only arriving a number of seconds later, as akin to thunder, with the exact time depending on its distance. The film was also accompanied by powerful, Wagner-esque music featured on many test films of that period and was hosted by actor Reed Hadley. A private screening was given to President Dwight D. Eisenhower who had succeeded President Harry S. Truman in January 1953. In 1954, the film was released to the public after censoring, and was shown on commercial television channels.\n\nEdward Teller, perhaps the most ardent supporter of the development of the hydrogen bomb, was in Berkeley, California at the time of the shot. He was able to receive first notice that the test was successful by observing a seismometer, which picked up the shock wave that traveled through the earth from the Pacific Proving Grounds. In his memoirs, Teller wrote that he immediately sent an unclassified telegram to Dr. Elizabeth \"Diz\" Graves, the head of the rump project remaining at Los Alamos during the shot. The unclassified telegram contained only the words \"It's a boy.\", which came hours earlier than any other word from Enewetak.\n\n\n\n"}
{"id": "39775772", "url": "https://en.wikipedia.org/wiki?curid=39775772", "title": "Journal for Early Modern Cultural Studies", "text": "Journal for Early Modern Cultural Studies\n\nThe Journal for Early Modern Cultural Studies is a quarterly peer-reviewed academic journal and the official publication of the Group for Early Modern Cultural Studies. It covers the cultural history of the period from the late fifteenth to the late nineteenth centuries. The journal was established in 2001 and has been published by the University of Pennsylvania Press since 2011. The journal was published biannually until 2012, when it became a quarterly publication. The editor-in-chief is Daniel Vitkus.\n\nThe journal is abstracted and indexed in the MLA International Bibliography.\n"}
{"id": "49050655", "url": "https://en.wikipedia.org/wiki?curid=49050655", "title": "Kalahari Debate", "text": "Kalahari Debate\n\nThe Kalahari Debate is a series of back and forth arguments that began in the 1980s amongst anthropologists, archaeologists, and historians about how the San people and hunter-gatherer societies in southern Africa have lived in the past. On one side of the debate were scholars led by Richard Borshay Lee and Irven DeVore, considered traditionalists or \"isolationists.\" On the other side of the debate were scholars led by Edward Wilmsen and James Denbow, considered revisionists or \"integrationists.\"\nLee conducted early and extensive ethnographic research among a San community, the !Kung San. He and other traditionalists consider the San to have been, historically, isolated and independent hunter/gatherers separate from nearby societies. Wilmsen, Denbow and the revisionists oppose these views. They believe that the San have not always been an isolated community, but rather have played important economic roles in surrounding communities. They claim that over time the San have become a dispossessed and marginalized people.\n\nBoth sides use both anthropological and archaeological evidence to fuel their arguments. They interpret cave paintings in Tsodilo Hills, and they also use artifacts such as faunal remains of cattle or sheep found at San sites. They even find Early Stone Age and Early Iron Age technologies at San sites, which both sides use to back their arguments.\n\nThe San are a relatively small group of people whose communities are scattered throughout the Kalahari Desert in southern Africa. They are well known for practicing a hunter/gatherer subsistence strategy (also known as a \"foraging\" mode of production). Traditionalists, including Richard Lee and other anthropologists, view the San as maintaining this old but adaptable way of life, even in the face of changing external circumstances. These anthropologists view the San as isolates who are not, and have never been, part of a greater Kalahari economy. The traditionalists believe that the San have adapted over time but without help from other societies. Emphasis is thereby placed on the cultural continuity and the cultural integrity of the San peoples.\n\nIn Lee’s 1979 book \"The !Kung San: Men, Women, and Work in a Foraging Society,\" his main goal was to be fully immersed in the !Kung San culture so that he could fully understand their way of life. He was puzzled as to how these people seemed to be living such an easy and happy life that relied heavily on hard work and the availability of food. Most of his studies of the San took place in the Dobe area, near the Tsodilo Hills. He was adopted into a kinship and given the name /Tontah which meant “White-Man.” He claims that the San were an isolated hunter-gatherer society that changed to farming and foraging at the end of the 1970s. Most of Lee’s historical data comes from oral stories told by the !Kung San because they did not have anything written down. According to Lee the San were originally afraid of contact with outsiders.\n\nLee reports that the men did the hunting and hard labor while the women did housework. He later found out that the San weren’t just hunter-gatherers, but also herders, foragers, and farmers. In his book he states, “I learned that most of the men had had experience herding cattle at some point in their lives and that many men had owned cattle and goats in the past.” He claims that they have learned all of this on their own. The San wanted wage pay for farming and taking care of cattle, goats, and sheep. This was their new way of life.\n\nEdwin Wilmsen's 1989 book \"Land Filled With Flies\" kicked off the Kalahari Debate. Wilmsen made several remarks attacking anthropologists’ view of the San people. Most of his attacks were at Richard Lee and his work. Wilmsen made claims about the San such as, “Their appearance as foragers is a function of their relegations to an underclass in the playing out of historical processes that began before the current millennium and culminated in the early decades of this century.” This statement upsets the traditionalists because it says that the San are not isolates but have been an underclass in a society throughout history. Wilmsen makes another statement against the traditionalists when he says, “The isolation in which they are said to have been found is a creation of our own view of them, not of their history as they lived it.” He is beginning to say that anthropologists’ judgment is clouded because they already have a predisposed view of the San and hunter-gatherer societies as being isolates. Wilmsen states that the terms “Bushmen,” “Forager,” and “Hunter-Gatherer” contribute to the ideology of them being isolates. He says this is because these terms are commonly associated with isolated groups but his main claim is that for the San this is not the case. Wilmsen also goes on to claim that Lee approaches the San as a people without a history, that they have been doing the same thing forever. He states, “they are permitted antiquity while denied history” Wilmsen continues the argument that anthropologists’ goal is to study hunter-gatherer groups who have lived on their own for centuries, which builds a stereotype for hunter-gatherers. He believes this is why Richard Lee’s views are flawed, and also why he is saying that the San are incorporated in a wider political economy in southern Africa.\n\nThe revisionists believe the !Kung were associated with Bantu-speaking overlords throughout history, and involved with merchant capital. They believe the San in the Kalahari are a classless society because they are actually the lower class of a greater Kalahari society. The revisionists believe the !Kung San were heavily involved in trade. They believe the San were transformed by centuries of contact with Iron Age, Bantu-speaking agro-pastoralists. This argues against the idea that they were a well-adapted hunter-gatherer culture, but instead advanced only through trade and help from nearby economies.\n\nWhen it comes to archaeological evidence, much work still has yet to be done. However, artifacts and ecofacts have been found at southern African sites that could help prove the revisionist view of the San people.\n\nTheir strongest supporting site is in the Tsodilo Hills, where rock art displays San looking over Bantu cattle. In the hills, there are 160 cattle pictures, 10 of which display stick figures near them.\n\nOther evidence revisionists point to includes Early Iron Age products found in Later Stone Age sites. This includes metal and pottery found in the Dobe, Xia, and Botswana regions. Cow bones have also been found in northern Botswana, at Lotshitshi. These products are believed to be payment to the San for labor of caring for or possibly herding Bantu cattle.\n\nThe fuel of this debate is the constant back and forth critiquing by various scholars of each other’s work. Wilmsen would say Lee is blinded by a pre-destined view of the San as isolates. Lee would counter-argue every point that Wilmsen would make, saying either that he made mistakes in research or presents conclusions with little evidence to support them.\n\nOne specific instance is where Lee called out Wilmsen for mistaking the word “oxen” for “onins”, which meant “onions” in an old map of the Kalahari region. This discovery would make the San herders before the arrival of the anthropologists in the 50’s and 60’s and not after the 70’s, as Lee believes. This instance gave rise to Lee’s article \"Oxen or Onions.\" In the article, Lee points out other flaws he believes he has found in Wilmsen’s argument. Critiques of Wilmsen’s work say that the cattle paintings could represent San stealing cattle rather than herding them. Another attack on Wilmsen’s work was that the amounts of pottery and iron found in Dobe and Botswana regions were so small they could fit in one hand. The small numbers of these artifacts make some scholars believe they are insufficient to be able to make such a claim. The same is true of the cattle bones found in Botswana. The small numbers of cattle bone fragments found on San archaeological sites have made scholars question Wilmsen’s argument.\n"}
{"id": "31559837", "url": "https://en.wikipedia.org/wiki?curid=31559837", "title": "Kirchhoff's diffraction formula", "text": "Kirchhoff's diffraction formula\n\nKirchhoff's diffraction formula (also Fresnel–Kirchhoff diffraction formula)\ncan be used to model the propagation of light in a wide range of configurations, either analytically or using numerical modelling. It gives an expression for the wave disturbance when a monochromatic spherical wave passes through an opening in an opaque screen. The equation is derived by making several approximations to the Kirchhoff integral theorem which uses Green's theorem to derive the solution to the homogeneous wave equation.\n\nKirchhoff's integral theorem, sometimes referred to as the Fresnel–Kirchhoff integral theorem, uses Green's identities to derive the solution to the homogeneous wave equation at an arbitrary point P in terms of the values of the solution of the wave equation and its first order derivative at all points on an arbitrary surface which encloses P.\n\nThe solution provided by the integral theorem for a monochromatic source is:\n\nwhere \"U\" is the complex amplitude of the disturbance at the surface, \"k\" is the wavenumber, and \"s\" is the distance from P to the surface.\n\nThe assumptions made are:\n\n Consider a monochromatic point source at P, which illuminates an aperture in a screen. The energy of the wave emitted by a point source falls off as the inverse square of the distance traveled, so the amplitude falls off as the inverse of the distance. The complex amplitude of the disturbance at a distance \"r\" is given by\n\nwhere \"a\" represents the magnitude of the disturbance at the point source.\n\nThe disturbance at a point P can be found by applying the integral theorem to the closed surface formed by the intersection of a sphere of radius R with the screen. The integration is performed over the areas \"A\", \"A\" and \"A\", giving\n\nTo solve the equation, it is assumed that the values of \"U\" and ∂\"U\"/∂\"n\" in the area \"A\" are the same as when the screen is not present, giving at Q:\n\nwhere \"r\" is the length P\"\"Q, and (\"n\", \"r\") is the angle between PQ and the normal to the aperture.\n\nKirchhoff assumes that the values of \"U\" and ∂\"U\"/∂\"n\" in \"A\" are zero. This implies that \"U\" and ∂\"U\"/∂\"n\" are discontinuous at the edge of the aperture. This is not the case, and this is one of the approximations used in deriving the equation. These assumptions are sometimes referred to as Kirchhoff's boundary conditions.\n\nThe contribution from \"A\" to the integral is also assumed to be zero. This can be justified by making the assumption that the source starts to radiate at a particular time, and then by making \"R\" large enough, so that when the disturbance at P is being considered, no contributions from \"A\" will have arrived there. Such a wave is no longer monochromatic, since a monochromatic wave must exist at all times, but that assumption is not necessary, and a more formal argument avoiding its use has been derived.\n\nWe have\n\nwhere (\"n\", \"s\") is the angle between the normal to the aperture and PQ.\n\nFinally, the terms 1/\"r\" and 1/\"s\" are assumed to be negligible compared with \"k\", since \"r \" and \"s\" are generally much greater than 2π/\"k\", which is equal to the wavelength. Thus, the integral above, which represents the complex amplitude at P, becomes\n\nThis is the Kirchhoff or Fresnel–Kirchhoff diffraction formula.\n\nThe Huygens–Fresnel principle can be derived by integrating over a different closed surface. The area \"A\" above is replaced by a wavefront from \"P\", which almost fills the aperture, and a portion of a cone with a vertex at \"P\", which is labeled \"A\" in the diagram. If the radius of curvature of the wave is large enough, the contribution from \"A\" can be neglected. We also have\n\nwhere χ is as defined in Huygens–Fresnel principle, and cos(\"n\", \"r\") = 1. The complex amplitude of the wavefront at \"r\" is given by\n\nThe diffraction formula becomes\n\nThis is the Kirchhoff's diffraction formula, which contains parameters that had to be arbitrarily assigned in the derivation of the Huygens–Fresnel equation.\n\nAssume that the aperture is illuminated by an extended source wave. The complex amplitude at the aperture is given by \"U\"(\"r\").\n\nIt is assumed, as before, that the values of \"U\" and ∂\"U\"/∂\"n\" in the area \"A\" are the same as when the screen is not present, that the values of \"U\" and ∂\"U\"/\"∂n\" in \"A\" are zero (Kirchhoff's boundary conditions) and that the contribution from \"A\" to the integral are also zero. It is also assumed that 1/\"s\" is negligible compared with \"k\". We then have\n\nThis is the most general form of the Kirchhoff diffraction formula. To solve this equation for an extended source, an additional integration would be required to sum the contributions made by the individual points in the source. If, however, we assume that the light from the source at each point in the aperture has a well-defined direction, which is the case if the distance between the source and the aperture is significantly greater than the wavelength, then we can write\n\nwhere \"a\"(\"r\") is the magnitude of the disturbance at the point \"r\" in the aperture. We then have\n\nand thus\n\nIn spite of the various approximations that were made in arriving at the formula, it is adequate to describe the majority of problems in instrumental optics. This is mainly because the wavelength of light is much smaller than the dimensions of any obstacles encountered. Analytical solutions are not possible for most configurations, but the Fresnel diffraction equation and Fraunhofer diffraction equation, which are approximations of Kirchhoff's formula for the near field and far field, can be applied to a very wide range of optical systems.\n\nOne of the important assumptions made in arriving at the Kirchhoff diffraction formula is that \"r\" and \"s\" are significantly greater than λ. A further approximation can be made, which significantly simplifies the equation further: this is that the distances PQ and QP are much greater than the dimensions of the aperture. This allows one to make two further approximations:\n\nWe can express \"r\" and \"s\" as follows:\n\nThese can be expanded as power series:\n\nThe complex amplitude at P can now be expressed as\n\nwhere \"f\"(\"x\", \"y\") includes all the terms in the expressions above for \"s\" and \"r\" apart from the first term in each expression and can be written in the form\n\nwhere the \"c\" are constants.\n\nIf all the terms in \"f\"(\"x\", \"y\") can be neglected except for the terms in \"x\" and \"y\", we have the Fraunhofer diffraction equation. If the direction cosines of PQ and PQ are\n\nThe Fraunhofer diffraction equation is then\n\nwhere \"C\" is a constant. This can also be written in the form\n\nwhere k and k are the wave vectors of the waves traveling from P to the aperture and from the aperture to P respectively, and r is a point in the aperture.\n\nIf the point source is replaced by an extended source whose complex amplitude at the aperture is given by \"U\"(r' ), then the Fraunhofer diffraction equation is:\n\nwhere \"a\"(r') is, as before, the magnitude of the disturbance at the aperture.\n\nIn addition to the approximations made in deriving the Kirchhoff equation, it is assumed that\n\nWhen the quadratic terms cannot be neglected but all higher order terms can, the equation becomes the Fresnel diffraction equation. The approximations for the Kirchhoff equation are used, and additional assumptions are:\n\n"}
{"id": "901062", "url": "https://en.wikipedia.org/wiki?curid=901062", "title": "Large Helical Device", "text": "Large Helical Device\n\nThe (LHD) is a fusion research device in Toki, Gifu, Japan, belonging to the National Institute for Fusion Science.\nIt is the second largest superconducting stellarator in the world, after the Wendelstein 7-X.\nThe LHD employs a heliotron magnetic field originally developed in Japan.\n\nThe objective of the project is to conduct fusion plasma confinement research in a steady state in order to elucidate possible solutions to physics and engineering problems in helical plasma reactors. The LHD uses neutral beam injection, ion cyclotron radio frequency (ICRF), and electron cyclotron resonance heating (ECRH) to heat the plasma, much like conventional tokamaks.\n\n\n"}
{"id": "43028143", "url": "https://en.wikipedia.org/wiki?curid=43028143", "title": "List of botanists by author abbreviation (W–Z)", "text": "List of botanists by author abbreviation (W–Z)\n\nTo find entries for A–V, use the table of contents above.\n\n\n\n\n"}
{"id": "50327796", "url": "https://en.wikipedia.org/wiki?curid=50327796", "title": "List of corporate titles", "text": "List of corporate titles\n\nCorporate titles or business titles are given to company and organization officials to show what duties and responsibilities they have in the organization. \n\n"}
{"id": "14263966", "url": "https://en.wikipedia.org/wiki?curid=14263966", "title": "List of medical colleges in India", "text": "List of medical colleges in India\n\n, there are 460 medical colleges in India where qualifications are recognised by the Medical Council of India; these medical schools have a combined capacity to provide medical education for 63,985 students. The Medical Council of India's motto is to provide quality medical care to all Indians through promotion and maintenance of excellence in medical education. Its website maintains an up-to-date list.\n\nIndia's medical schools are usually called medical colleges. Medical school quality is controlled by the central regulatory authority, the Medical Council of India, which inspects the institutes from time to time and recognises institutes for specific courses. Most of the medical school were set up by the central and state governments in the 1950s and 60s. But in the 1980s, several private medical institutes were founded in several states, particularly in Karnataka. Andhra Pradesh state allowed the founding of several private institutions in the new millennium. Medical education in a private institute is very expensive.\n\nThe basic medical qualification obtained in Indian medical schools is MBBS. The MBBS course is four-and-a-half years, followed by one year of Compulsory Rotating Residential Internship (CRRI). The MBBS course is followed by MS, a post-graduation course in surgical specialities, MS or MD and DNB (Highly qualified P.G. and Super specialization), which are postgraduate courses in medical specialities usually of three years duration, or by diploma postgraduate courses of two years duration. Super or sub-specialities can be pursued and only a MS or MD holder is eligible. A qualification in a super- or sub-speciality is called DM or M.Ch.\n\nIn most Indian states, entry to medical education is based on entrance examinations. Some prestigious institutes like the All India Institute of Medical Sciences (AIIMS), Christian Medical College & Hospital (CMCH) Vellore, Kasturba Medical College (Manipal and Mangalore), Jawaharlal Institute of Postgraduate Medical Education and Research (JIPMER), Armed Forces Medical College (AFMC), St. John's Medical College (Bangalore) and National Institute of Mental Health and Neurosciences (NIMHANS)conduct entrance tests at the national level and attract candidates from all over India.\n\nThough India has many medical schools and produces thousands of medical graduates every year, there is a great shortage of doctors in rural areas. Most graduates do not wish to practice in rural areas due to understaffed hospitals and inadequate facilities.\n\nIndia is one of only a few countries where graduates from local medical schools end up working in other countries all over the world, but particularly in the Middle East, the UK and the USA.\n\nIndian states with the most medical colleges include Karnataka, Maharashtra, Tamil Nadu, and Andhra Pradesh. States with the fewest include Manipur, Tripura, Chandigarh, Goa, and Sikkim. Following is an incomplete list of medical colleges in India.\n\n\n\n\n\n\n\n\nTotal: 600 seats\n\nTotal: 1400 seats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "39341946", "url": "https://en.wikipedia.org/wiki?curid=39341946", "title": "List of nursing colleges in India", "text": "List of nursing colleges in India\n\nThis is list of notable Nursing colleges in India.\n\n\n\n\n\n\n\n\nQuality health care school and college of nursing\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "27473409", "url": "https://en.wikipedia.org/wiki?curid=27473409", "title": "List of places with columnar jointed volcanics", "text": "List of places with columnar jointed volcanics\n\nColumnar jointed volcanic rocks exist in many places on Earth. Perhaps the most famous basalt lava flow in the world is the Giant's Causeway in Northern Ireland, in which the vertical joints form polygonal columns and give the impression of having been artificially constructed.\n\nThis may have formed from contractional cooling of basltic lavas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral exposures of columnar jointing have been discovered on the planet Mars by the High Resolution Imaging Science Experiment (HiRISE) camera, which is carried by the Mars Reconnaissance Orbiter (MRO).\n\nA now-ruined thirteenth-century religious complex called Nan Madol was built using columnar basalt quarried from various locations on the island of Pohnpei.\n"}
{"id": "53482266", "url": "https://en.wikipedia.org/wiki?curid=53482266", "title": "List of reportedly haunted locations in South Africa", "text": "List of reportedly haunted locations in South Africa\n\nThe following is a list of reportedly haunted locations in South Africa.\n\n"}
{"id": "40750190", "url": "https://en.wikipedia.org/wiki?curid=40750190", "title": "Meta-cold dark matter", "text": "Meta-cold dark matter\n\nMeta\"-cold dark matter, also known as m\"CDM, is a form of cold dark matter proposed to solve the cuspy halo problem. It consists of particles \"that emerge relatively late in cosmic time (z ≲ 1000) and are born non-relativistic from the decays\nof cold particles\".\n"}
{"id": "38462339", "url": "https://en.wikipedia.org/wiki?curid=38462339", "title": "Neighbourhoods Green", "text": "Neighbourhoods Green\n\nNeighbourhoods Green is an English partnership initiative which works with social landlords and housing associations to highlight the importance of open and green space for residents and raise the overall quality of design and management with these groups.\n\nThe partnership was established in 2003 when Peabody Trust and Notting Hill Housing Group held a conference which identified the need to raise the profile of the green and open spaces owned and managed by social landlords. The scheme attracted praise from the then Minister for parks and green spaces Yvette Cooper\n\nSince 2003 the partnership has expanded to include National Housing Federation, Groundwork, The Wildlife Trusts, Landscape Institute, Green Flag Award, Royal Horticultural Society, Natural England and CABE. It is overseen by a steering group which includes representatives from Circle Housing Group, Great Places Housing Group, Helena Homes, London Borough of Hammersmith & Fulham, Medina Housing, New Charter Housing Trust, Notting Hill Housing, Peabody Trust, Places for People, Regenda Group and Wakefield & District Housing.\n\nNeighbourhoods Green has three main areas of emphasis. It produces best practice guidance, highlighting the contribution parks, gardens and play areas make to the quality of life for residents – including the mitigation of climate change, promotion of biodiversity and aesthetic qualities. It also generates a number of case studies from housing associations and community groups and offers training for landlords, residents and partners on areas such as playspace, green infrastructure and growing food.\n\nIn 2011, working in conjunction with University of Sheffield and the National Housing Federation, Neighbourhoods Green produced \"Greener Neighbourhoods\" a best practice guide to managing green space for social housing. Its ten principles for housing green space were:\n\nDuring 2013/14 Neighbourhoods Green will be working with Keep Britain Tidy to support the expansion of the Green Flag Award into the social housing sector.\n\n"}
{"id": "18952422", "url": "https://en.wikipedia.org/wiki?curid=18952422", "title": "North American DC-3", "text": "North American DC-3\n\nThe DC-3 was a proposed spaceplane designed by Maxime Faget at the Manned Spacecraft Center (MSC) in Houston. The design was nominally developed by North American Aviation (NAA), although it was a purely NASA-internal design.\n\nUnlike the eventual Space Shuttle design that emerged, the DC-3 was a fully reusable launch vehicle two-stage-to-orbit design with a smaller payload capacity of about 12,000 lbs and limited maneuverability. Its inherent strengths were good low-speed handling during landing, and a low-risk development that was relatively immune to changes in weight and balance.\n\nWork on the DC-3 program ended when the US Air Force joined the Shuttle program and demanded a much greater \"cross-range\" maneuverability than the DC-3 could deliver. They also expressed serious concerns about its stability during re-entry. NAA eventually won the Shuttle Orbiter contract, based on a very different design from another team at MSC.\n\nIn the mid-1960s the US Air Force conducted a series of classified studies on next-generation space transportation systems. Among their many goals, the new launchers were intended to support a continued manned military presence in space, and so needed to dramatically lower the cost of launches and increase launch rates. Selecting from a series of proposals, the Air Force concluded that semi-reusable designs were the best choice from an overall cost basis, and the Lockheed Star Clipper design was one of the most-studied examples. They proposed a development program with an immediate start on a \"Class I\" vehicle based on expendable boosters, followed by a slower development of a \"Class II\" semi-reusable design, and perhaps a \"Class III\" fully reusable design in the further future. Although is it estimated that the Air Force spent up to $1 billion on the associated studies, only the Class I program that proceeded to development, as the X-20 Dyna-Soar, which was later cancelled.\n\nNot long after the Air Force studies, NASA started studying the post-Project Apollo era. A wide variety of projects were examined, many based on re-using Apollo hardware (Apollo X, Apollo Applications Program, etc.) Flush with the success of the moon landings, a series of ever-more ambitious projects gained currency, a process that was considerably expanded under the new NASA director, Thomas O. Paine. By about 1970 these had settled on the near-term launching of a 12-man space station in 1975, expanding this to a 50-man \"space base\" by 1980, a smaller lunar-orbiting station, and then eventually a manned mission to Mars in the 1980s. NASA awarded $2.9-million study contracts for the space stations to North American and McDonnell Douglas in July 1969.\n\nAlmost as an afterthought the idea of a small and inexpensive \"logistics vehicle\" for supporting these missions developed in the late 1960s. George Mueller was handed the task of developing plans for such a system, and held a one-day symposium at NASA headquarters in December 1967 to study various options. Eighty people attended and presented a wide variety of potential designs, many from the earlier Air Force work, from small Dyna-Soar like vehicles primarily carrying crew and launched on existing expendable boosters, to much larger fully reusable designs.\n\nOn 30 October 1968 NASA officially began work on what was then known as the \"Integrated Launch and Re-entry Vehicle\" (ILRV), a name they borrowed from the earlier Air Force studies. The development program was to take place in four phases; Phase A: Advanced Studies; Phase B: Project Definition; Phase C: Vehicle Design; and Phase D: Production and Operations. Four teams were to participate in Phase A; two in Phase B; and then a single prime contractor for Phases C and D. A separate Space Shuttle Main Engine (SSME) competition was to run in parallel.\n\nNASA Houston and Huntsville jointly issued the Request for Proposal (RFP) for eight-month Phase A ILRV studies. The requirements were for 5,000 to 50,000 lb of payload to be delivered into a 500 km altitude orbit. The re-entry vehicle should have a cross range of at least 450 miles, meaning that it could fly to the left or right of its normal orbital path. General Dynamics, Lockheed, McDonnell-Douglas, Martin Marietta, and (the newly named) North American Rockwell were invited to bid. In February 1969, following study of the RFPs, Martin Marietta's entry was dropped, although they continued work on their own. The other entries were all given additional Phase A funding.\n\nSupported by Paine's ambitious plans, in August 1969 the ILRV program was re-defined to be a \"maximum effort\" design, and only fully reusable designs would be accepted. This led to a second series of Phase A studies. The designs that were returned varied widely, meeting the huge payload range specified in the original RFP. Two basic fuselage designs seemed to be the most common; lifting body designs that offered high cross-range but limited maneuverability after re-entry, and delta-winged designs that reversed these criteria.\n\nFaget felt that all of the proposed designs incorporated an unacceptable amount of development risk. Unlike a conventional aircraft, with separate fuselage and wings, the ILRV designs had blended wing-body layouts. This meant that changes in weight and balance, which are almost unavoidable during development, would require changes to the entire orbiter structure to compensate. He also felt that the poor low-speed handling of any of these layouts presented a real danger during landing. Upset by what he felt was a project that seemed to guarantee failure, he started work on his own design, and presented it as the DC-3.\n\nUnlike the other entries, DC-3 was much more conventional in layout, with an almost cylindrical fuselage and low-mounted slightly swept wings. The design looked more like a cargo aircraft than a spacecraft. Re-entry was accomplished in a 60 degree nose-high attitude that presented the lower surface of the spacecraft to the airflow, using a ballistic blunt-body approach that was similar to the one Faget had successfully pioneered on the Mercury capsule. During re-entry, the wings provided little or no aerodynamic lift. After re-entry, when the spacecraft entered the lower atmosphere, it would pitch over into a conventional flying attitude, ducts would open, and jet engines would start up for landing.\n\nThe upside of this design approach was that changes in the weight and balance could be addressed simply by moving the wing or re-shaping it, a common solution that had been used for decades in aircraft design—including the original Douglas DC-3 whose wings were swept rearward for just this reason. The downside was that the spacecraft would have little hypersonic lift, so its ability to maneuver while re-entering would be limited and its cross-range would be about 300 miles. It could make up for some of this with its improved low-speed flying ability, but would still not be able to match the mandated 450 miles.\n\nAlthough the DC-3 had never been part of the original ILRV plans, Faget's name was so well respected that others at NASA MSC in Houston quickly rallied around him. Other NASA departments all selected their own favorite designs, including recoverable versions of Saturn boosters developed at the Marshall Space Flight Center in Huntsville, lifting-bodies based on the HL-10 that were favored by the Langley Research Center and Dryden Flight Research Center (Edwards), and even a single-stage-to-orbit Aerospaceplane were also proposed. From then on, the entire program was beset with in-fighting between the various teams. On 1 June 1969, a report was published that attacked the DC-3 design, followed by several others over the remainder of the year. In spite of this, North American quickly took up the DC-3 design, having learned over the years that the best way to win a NASA contract was to make whatever design Faget favored. They won contract NAS9-9205 to develop the DC-3 in December 1969.\n\nIn order to clear the logjam developing between the departments, on 23 January 1970 a meeting was held in Houston to study all of the in-house concepts. Over the next year a number of proposed designs would be dropped, including the entire series of lifting-body-derived vehicles as it proved too difficult to fit cylindrical tanks into the airframe. This left two basic approaches, delta wings and Faget's DC-3 series. Development of the DC-3 continued, with a drop test of a 1/10-scale model starting on 4 May.\n\nOn 12 February 1969 Richard Nixon formed the Space Task Group under the direction of Vice President Spiro Agnew, giving them the task of selecting missions for a post-Apollo NASA. Agnew quickly became a proponent of NASA's ambitious plans that would culminate in a Mars attempt. The Task Group's final report, delivered on 11 September 1969, outlined three broad plans; the first required funding at $8 to $10 billion a year and would fulfill all of NASA's goals, the second would reduce this to $8 billion or less if the manned lunar orbiting station was dropped, and finally the third would require only $5 billion a year and would develop only the space stations and shuttle.\n\nAt first Nixon did not comment on the plans. Later he demanded that the program be greatly reduced even from the smallest of the Task Group's proposals, forcing them to select either the space base \"or\" the shuttle. Discussing the problem, NASA engineers concluded that the development of a shuttle would lower the cost of launching portions of the space station, so it seemed that proceeding with the shuttle might make the future development of the station more likely. However, NASA's estimates of the shuttle development costs were met with great skepticism by the Office of Management and Budget (OMB). Studies by RAND in 1970 showed that there was no benefit to developing a reusable spacecraft when development costs were taken into account. The report concluded that a manned station would be more cheaply supported with expendable boosters.\n\nBy this time Paine had left NASA to return to General Electric, and had been replaced by the more pragmatic James Fletcher. Fletcher ordered independent reviews of the shuttle concept; Lockheed was to prepare a report on how the shuttle could reduce payload costs, Aerospace Corporation was to make an independent report on development and operational costs, and Mathematica would later combine these two into a final definitive report. Mathematica's report was extremely positive; it showed that development of a fully reusable design would lower the per-launch cost, thereby reducing payload costs and driving up demand. However, the report was based on a greatly increased rate of launch; inherent in the math was the fact that lower launch rates would completely upset any advantage. Nevertheless, the report was extremely influential, and made the shuttle program an ongoing topic of discussion in Washington.\n\nLooking to shore up support for the program, Fletcher directed NASA to develop the shuttle to be able to support the Air Force's requirements as well, as initially developed in their \"Class III\" fully reusable vehicles. If the shuttle became vital to the Air Force as well as NASA, it would be effectively unkillable. The Air Force's requirements were based about a projected series of large spy satellites then under development, which were 60 feet long and weighed 40,000 lbs. They needed to be launched into polar orbits, corresponding to a normal launch from Kennedy Space Center (KSC) of 65,000 lbs (launches to the east receive a free boost from the Earth's natural rotation).\n\nThe Air Force also demanded a cross-range capability of 1,500 miles, meaning that the spacecraft would have to be able to land at a point to either side of its orbital path when it started re-entry. This was due to the desire to be able to land again after one orbit, the so-called \"orbit-once-around\".\n\nThe new Air Force cross-range requirements doomed the DC-3 design.\n\nSpacecraft orbit around the center of the Earth, not the surface. If a spacecraft is launched due East from the equator into a 90-minute low-Earth orbit, it will circle the Earth and return to the spot where it was launched 90 minutes later. During this time, however, the launch site will have moved due to the Earth's rotation. Over the 90 minute period, the Earth would rotate about to the west, towards the spacecraft. Given a spacecraft speed of about , simply starting the re-entry a few minutes earlier than the complete 90 minute orbit would make up this difference.\n\nAt KSC's ~30 degree north latitude the picture is similar. Over the same 90 minute orbit KSC will rotate about . Unlike the equatorial orbit case, however, letting the spacecraft stay in the inclined orbit a little longer will start taking it south of the launch site, its closest point of approach being about to the southwest. A spacecraft wishing to return to its launch site will need about 300 miles of cross-range maneuverability during re-entry, and the NASA shuttle designs demanded about 450 miles in order to have some working room.\n\nPolar orbits from the Air Force's Vandenberg Air Force Base are another matter entirely. Located slightly north of KSC, the distance it would move over a single orbit would be similar, but critically, the shuttle would be traveling south, not east. This meant that it was not flying toward the launch point as it traveled in its orbit, and when it completed one orbit it would have to make up the entire 1,350 miles during re-entry. These missions required a dramatically improved cross-range capability, set at 1,500 miles to give it a slight excess capability. The ballistic re-entry profile of the DC-3 series simply could not come close to matching this requirement.\n\nOn 1 May 1971 the OMB finally released a budget plan, limiting NASA to $3.2 billion per year for the next five years. Given existing project budgets, this limited any spending on the shuttle to about $1 billion a year, far less than required to develop any of the completely reusable designs. Based on these constraints, NASA returned to a Class II-like vehicle with external tankage, which led to the MSC-020 design. Later that year all straight-wing designs were officially abandoned, although Faget's team continued to work on them for some time in spite of this.\n\nThe DC-3 was a two-stage vehicle with a large booster and smaller shuttle of overall similar design. Both were similar to \"jumbo jets\" in layout in general terms, with their large cylindrical fuselage containing fuel tanks instead of passengers or cargo. The bottom of the fuselage was flattened for re-entry aerodynamics, with a slight upward curve as you approached the nose in early models. The wings were low-mounted, in-line with the bottom of the fuselage, with a 14 degree rearward sweep on the front and no sweep on the back. The general layout of the wing planform was similar to the original DC-3. The empennage was a conventional three-surface unit, although in the original MSC-001 design the delta-shaped horizontal stabilizer was located at the bottom of the fuselage and served double-duty in protecting the rear-mounted engines during re-entry. Later versions did not generally include this feature, and used more conventional surfaces mid-mounted on the fuselage.\n\nThe orbiter carried a crew of two, and had accommodations for up to ten passengers. A cargo area was mounted in the middle of the craft between the liquid hydrogen (LH2) tank behind it, and a combined LH2/liquid oxygen tank in front of it. This arrangement was used in order to center the cargo over the wing, with the heavier oxygen and crew compartment balancing the weight of the engines. The lighter weight hydrogen then filled out the rest of the internal space. The booster had no cargo area, so it used a simpler arrangement of tankage with a single LH2 tank at the rear. The booster normally flew unmanned, but included a two-man cockpit area that was used during ferry flights.\n\nThe orbiter was powered by two modified XLR-129 engines with the thrust increased from 250,000 to 300,000 lbf, two 15,000 lbf RL-10 orbital manoeuvring engines, and six Rolls-Royce RB162 jet engines for landing. The booster used eleven of the same XLR-129 engines, and four Pratt & Whitney JT8D for landing. XLR-129s on both the shuttle and booster were fired for take-off. The orbiter was mounted relatively far forward for launch, its tail in-line with the booster's wings. The combined weight at launch would be about 2,030 tons.\n\nThe orbiter would re-enter nose-high at an angle of about 60 degrees above horizontal, decelerating at a peak of 2G until it reached low subsonic speeds at 40,000 ft. At this point the forward speed of the craft would be very low, so the nose was pitched down and the orbiter dove to pick up airspeed over the wings and transition to level flight. Expected re-entry heating rates on the orbiter were 1650 deg C on the leading edge, and 790 deg C over 80% of the lower surface.\n\nIn order to maximize overall performance, the booster released the orbiter at Mach 10 and 45 miles altitude. This required the booster to carry a complete thermal protection system in order to re-enter for landing. Both the orbiter and booster were to be protected with the LI-1500 silica tiles similar to those eventually used on the Space Shuttle, a design that had recently been introduced by Lockheed and quickly became a baseline design for all of the shuttle contenders. As a result, both airframes were able to be built out of aluminum, greatly reducing airframe cost.\n\nBoth craft carried just enough JP-4 for landing go-around. Both could also carry increased loads of JP-4 for test flights or ferrying. After dispatching the orbiter the booster would be too far down-range to easily turn around and return to Kennedy, so the normal mission profile had it coast across the ocean, land automatically, refuel and pick up a crew, and then be flown back to Kennedy on its JT8D engines.\n\nLockheed estimated that development and initial production would cost $5.912 billion over a period from 1970 to 1975. A fleet of six orbiters and four boosters would have supported a launch rate of 50 flights per year.\n\n"}
{"id": "3262761", "url": "https://en.wikipedia.org/wiki?curid=3262761", "title": "Outline of archaeology", "text": "Outline of archaeology\n\nThe following outline is provided as an overview of and topical guide to archaeology:\n\nArchaeology – study of human cultures through the recovery, documentation, and analysis of material remains and environmental data, including architecture, artifacts, biofacts, human remains, and landscapes.\n\nArcheology can be described as all of the following:\n\n\n\nArchaeological science\n\n\n\n\n\nHistory of archaeology\n\n\nArchaeological theory\n\nList of archaeological periods\n\n\nArchaeological site\n\n\n\nList of archaeologists\n\n\n"}
{"id": "41710955", "url": "https://en.wikipedia.org/wiki?curid=41710955", "title": "Perennation", "text": "Perennation\n\nIn botany, perennation is the ability of organisms, particularly plants, to survive from one germinating season to another, especially under unfavourable conditions such as drought or winter. It typically involves development of a perennating organ, which stores enough nutrients to sustain the organism during the unfavourable season, and develops into one or more new plants the following year. Common forms of perennating organs are storage organs (e.g. tubers and rhizomes), and buds. Perennation is closely related with vegetative reproduction, as the organisms commonly use the same organs for both survival and reproduction.\n\n"}
{"id": "45473681", "url": "https://en.wikipedia.org/wiki?curid=45473681", "title": "Post Instrument", "text": "Post Instrument\n\nThe Post Plotting Instrument, or simply Post Instrument, was the standard optical sighting system used by the UK's Royal Observer Corps (ROC) to determine the location of aircraft. It was used during the period from the mid-1930s into the early 1950s, and was one of the main sources of daytime tracking information during World War II.\n\nThere were two versions of the Post Instrument, a pre-war model using a pantograph, and a wartime version of somewhat more sophistication. Both required the operator to estimate the altitude of the aircraft and enter that into the device, then point a mechanical indicator, or sight, at the aircraft. The motion of the sight moved an indicator on a small Ordnance Survey National Grid map. The grid location indicated by the pointer was then telephoned to central control rooms, where several such reports were combined to produce a more accurate location estimate.\n\nLater models added the Micklethwait Height Corrector, which allowed the posts to measure altitude with some accuracy and thus improve the quality of the measurements. The ROC also developed a methodology that allowed the Post Instrument to be used to produce measurements purely by sound, but it is unclear how often this was used.\n\nPrior to the introduction of radar, optical tracking systems of widely varying complexity were commonly used to spot and report aircraft positions. The Post Instrument was intended to be at the simple end of the scale, an inexpensive and easy to use instrument to make rough but rapid measurements of the locations of aircraft.\n\nPost Instruments were installed at hundreds of observation posts across the UK, typically in small groups of three or four posts about apart. This spacing allowed the operators to cross check each other's altitude measurements. Each post was normally manned by two or three operators, one operating the Post Instrument, another using the telephone to report the locations to a plotting center, and the third, if present, operating as a lookout and helper.\n\nThe original Post Instrument was supported by a metal rod extending vertically from the center of a circular table. A small section of map showing the surrounding area was attached to the tabletop.\n\nThe instrument itself consisted of an open rectangle of metal bars, with the long axis horizontal. Hinges at the connection points between the bars allowed the bars to be rotated to form various parallelograms. Similar hinges were located at the midpoints of the long horizontal bars of the rectangle. These midpoint pivots connected to the vertical bar on the table. The result was a pantograph that allowed the long horizontal bars to be rotated into the vertical to point upward at an aircraft, sighting along the upper bar.\n\nA final piece was a separate vertical bar connected to the two horizontals and pivoted in the same way so that it remained pointing vertically as the horizontal bars were rotated. This bar was able to be moved along the horizontal bars, fore and aft, which was used to adjust the estimated altitude.\n\nTo use the system, the operator would first estimate the altitude of the target aircraft and then move the smaller pointer as measured against a scale on the upper horizontal arm. They would then rotate the apparatus around the vertical shaft so the target aircraft lay along the line of the upper bar, and then rotate the bar vertically until it pointed at the aircraft. The vertical pointer now pointed to a grid location on the map, which could be read off to the reporting center.\n\nThe original model worked, but was somewhat difficult and time consuming to use. Just prior to the war a new version was introduced that was easier to use. Officially known as the \"Observer Instrument, Mark 2\", the first examples were built by R.B. Pullin & Co., starting in 1934.\n\nThe vertical rod of the original version was replaced by a horizontal framework, roughly T-shaped, that was suspended above the table on three wheels running on a metal track around the rim of the map. This provided a much more robust framework for holding the sighting system, and rotated much more smoothly. A pointer behind the front wheel made it easy to read off the bearing, when required. Travelling along the framework horizontally fore and aft was a sliding mechanism that held the sights. This formed the altitude adjustment that would be set prior to sighting. The map pointer was connected to the bottom of the slider.\n\nThe sights, in the form of an open-framework tube containing a crosshairs, was mounted to the horizontal slider on a vertical square tube. Sightings could be taken either through the crosshairs or along open sights on the top of the tube. A geared rack running down the back of the tube held the sights at a selected angle, and the angle was adjusted by rotating a geared knob on the right side of the sights. As the sights were rotated upwards, they forced the horizontal slider to the rear, moving the pointer over the map.\n\nWartime models were modified in 1940 with the Micklethwait Height Corrector, named for its inventor, Eric Walter Eustace Micklethwait. Micklethwait was an observer at the Gower Street Post on the roof of a building at University of London, near Euston Square tube station. Formerly a patent clerk, he devised the Corrector and quickly patented it.\n\nThe Corrector consisted of a second map pointer on a second horizontal slider, with a crank that moved the horizontal slider fore and aft. A second arm suspended from the main sight tube was pushed up and down as the horizontal portion slid. This arm was measured against a short vertical bar marked with altitude corrections. The system indicated only corrections, not the actual altitude. Two or more posts had to work in concert to use the system, using two measured angles and simple trigonometry to solve the altitude.\n\nThe simplest measurement took place when an aircraft flew directly over one of the posts. Other posts that could see the same aircraft would continue to track the target as normal, set to whatever altitude they had initially estimated. When the first post called that the aircraft was directly overhead, the other posts would crank the Corrector until its pointer lay over a marking for the other post printed on the map.\n\nFor instance, if the original estimate was and the aircraft was actually at , the operator at a second post would set his instrument to 10,000 and continue to measure as normal until they heard the first post call \"aircraft overhead\". At this point they would stop measuring the aircraft and instead crank the Corrector until its pointer lay over the marking on the map indicating the location of the first post. By comparing the position of the bar suspended from the sighting tube to the Corrector's vertical scale, they would see it indicate +1000. This correction was called to the ROC center who then forwarded it to all the posts in the area to update their altitude settings to 11,000 feet.\n\nWhen the target did not pass directly over a post the calculation was somewhat more complex. In this case two posts would measure the location of the aircraft at the same time, and then one would call the aircraft's measured grid reference to the other. The second would then place a ruler on the map lying along the line from their measured location on the map to the one called in from the other post. They would then crank the Corrector until its pointer lay directly above the nearest point on the ruler, and the correction could then be read off as normal.\n\nAn alternate procedure involved the use of an operator at the observer center. Gathering indicated grid locations from several sites set to the same arbitrary altitude, they would triangulate the grid location of the target and pass this information back to the posts. The operators at the posts would then crank the Corrector until its pointer lay over the calculated grid location, at which point the correction could be read.\n\nThe Post Instrument was introduced in an era when sound locating was still common, and some techniques for measuring the angle by sound were developed. This basically consisted of moving the horizontal slider until the indicator pointer was over the \"sound line\", a circle on the map representing a distance around the post. The operator would then rotate the sights horizontally and vertically to try to point the sights in the direction they estimated the sound to be coming from. Instead of using the map, the operator instead called the horizontal and vertical angles to their operations room. The horizontal angle could be read off a scale around the outer edge of the map, but the vertical angle was instead measured by dropping the last three zeros of the altitude measurement, so if the sights were over the 14,000 foot marker, they would call in \"angle 14\".\n\nIn the operations room, a plotter trained in sound measurements would take the angle measurements from multiple stations and determine the location by plotting the angles on a map and looking for the intersections. They would then calculate the distance from one or more of the posts and calculate the altitude using the formula . For instance, if they found that the aircraft was from a particular post that indicated the angle was 14, then the altitude would be 14 x 4 / 5 = 56/5 = about 11,000 feet. Plotters were equipped with pre-calculated tables to make these calculations quickly.\n\nThe Observer Corp was an expansion of a system originally set up in World War I to coordinate the reports from observers in the London area, part of the London Air Defence Area (LADA). In this system, originally set up by Edward Ashmore, observers telephoned reports of aircraft to a plotting center in the Horse Guards building in London. Information from the map would then be forwarded to the searchlights and anti-aircraft guns in the LADA area.\n\nIn the post-World War I era the system was taken over by the Air Defence of Great Britain organization, formally part of the Royal Air Force but containing British Army and Royal Engineer units as well. It was re-organized and expanded, covering not only the London area but adding similar reporting organizations in The Midlands. They also introduced new techniques to deal with faster aircraft.\n\nIn the new systems, \"plotters\" would take the reports from the observers and place a colored marker on a large map inside the indicated grid location. The marker held information about the number and altitude of the aircraft. The marker colors changed every five minutes, based on a sector clock, and when the marker was moved to a new location, a smaller marker was left behind in its former location. This produced a trail of colored markers on the map that allowed observers to easily see the \"track\" of the aircraft, as well as estimate how quickly it was moving.\n\nThe Dowding system was built on top of this reporting system. It added a central \"filter room\" that acted as a plotting station for all of the Chain Home radar stations. Reports from the filter room were then forwarded to Group plotting rooms where they were combined with information from the Observer Corps. The same basic system using colored markers indicating the time, altitude and number of aircraft was used throughout the system. Just prior to the war, two additional Groups were added to cover Scotland and the north, and the southwest.\n\nStarting in 1942, additional charts were installed at the Group plotting centers that allowed information from neighbouring Groups to be recorded. This was useful for handing over tracks that were moving across Group boundaries.\n\n"}
{"id": "34402185", "url": "https://en.wikipedia.org/wiki?curid=34402185", "title": "Strozzi Institute", "text": "Strozzi Institute\n\nStrozzi Institute is an organization located in Oakland, California that offers coaching services and trainings in leadership, organizational development, and personal mastery. It uses a somatic approach to learning. Programs are offered primarily at the institute's Sonoma training center.\nStrozzi Institute was founded in 1985 by Richard Strozzi-Heckler, Ph.D. as an application of his research into a \"somatic philosophy of learning\". \n\nIn the 1970s Strozzi-Heckler and Robert K. Hall, M.D. developed The Lomi School of body oriented psychotherapy, influenced by the work of Fritz Perls, Ida Rolf, Randolph Stone, and Charan Singh. In addition to the usual psychiatric and psychoanalytic methods, this program includes touch, group process, breathwork, attention training, and movement to its approach, to provide its framework for working with the mind unified with the physical body. \n\nIn 1985 Strozzi-Heckler contributed to a pilot program for the U.S. Army Special Forces to evaluate mind-body approaches to military training. In addition to many of the measured outcomes related to increased endurance, alertness, capacity for stress management, and team cohesion, participants unexpectedly demonstrated significant increases in leadership characteristics. Reflecting on this, Strozzi-Heckler began exploring ways to adapt the program for leaders in business and government.In collaboration with Fernando Flores during the 1980s and 1990s, Flores's Ontology of Language research into speech acts was integrated and applied to the somatic leadership domain. \n\nIn 2000, Strozzi Institute introduced a training program for coaches interested in their somatic approach.\n\nThe Strozzi Institute methodology, known as Strozzi Somatics, is used one-on-one and in groups of varying size.\n\nThe Strozzi Somatics methodology makes a distinction between \"soma\" - the living body in its entirety, and the mechanistic view of the physical body as an assemblage of anatomical parts. Using this first definition the body is regarded as the primary domain of feeling, action, language, and meaning. From this perspective, coaches observe the ways people hold their bodies and how they respond to stress situations, such as verbal or physical surprises. The body’s overall organization in this way is referred to as a somatic shape. Each person’s unique somatic shape is formed by responses to past experiences, positive and negative, which are established as deep, mostly unconscious patterns of muscular activity in the body. Over time these patterns produce conditioned tendencies of reaction to people, situations and environments.\n\nStrozzi-Heckler has described the process in which, when an individual is exposed to a stressful stimulus, they revert to this conditioned tendency, limiting their available choices for action. Withdrawal, fear, attempting to dominate, rigidity, and over-accommodation are examples of different conditioned tendency shapes. Because this is a somatic event rather than an exclusively cognitive one, new information or theoretical insight will not shift the response. As an illustration, he describes a team leader who has extensive education in management principles but, especially under stress, comports himself in a way that produces mistrust and resentment from the people he manages, eventually creating significant breakdowns within his team.\n\nStrozzi Institute has stressed that leadership characteristics often considered innate are teachable, and can be improved with practice. These traits include: high self awareness and awareness of the environment, being open to possibilities rather than limited to past options, being motivated by a connection to what one cares about,\nthe ability to deal directly with matters that need attention,\ndirecting attention outward in a way that enables listening and connection with others,\nand the ability to coordinate with others and empathize with their concerns.\nMany of the practices taught are adapted from aikido and different forms of meditation.\nAikido movements are presented in a non-martial context and principles of the art such as:\n\"centering\" oneself, \"facing\" an attack, \"extending\" outward into the environment, \"entering\" into shared space, and \"blending\" with the momentum of an incursion, are used as physical metaphors to guide the practice of embodying leadership characteristics.\nAn aikido dojo, \"Two Rock Aikido\", is located on the Strozzi Institute site in Sonoma.\n\nWorking from the premise that the body and the self are indistinguishable from one another, Strozzi Institute offers training in a style of bodywork developed by Strozzi-Heckler and Hall to produce change in a person's core historical limitations. Strozzi Bodywork involves addressing deeply held muscular contractions (also known as armoring) maintained in the soma using touch, breath, and directed attention. Practitioners train to develop an empathetic, compassionate presence that can build trust and enable them to work with others through a variety of emotional states. Some somatic coaches use Strozzi Bodywork in their coaching sessions.\n\nThe Strozzi Somatics methodology has been applied to a broad range of organizations and with diverse populations. These include Fortune 50 companies, U.S. Navy SEALs, U.S. Marines, law enforcement agencies, social justice groups, professional sports teams, as well as urban gang members, prisoners, Olympic athletes, and survivors of sexual trauma.\n\nStrozzi Institute has contributed to U.S. Military counter-insurgency training, integrating somatic practices to enhance soldiers' abilities to connect with others across cultures rather than rely predominantly on force. Strozzi-Heckler has said, \"Working with the body gives you a way to do that because it transcends words and language. It takes us to that common core of being human.\"\n\n\n"}
{"id": "36966166", "url": "https://en.wikipedia.org/wiki?curid=36966166", "title": "The Case of the Missing Moon Rocks", "text": "The Case of the Missing Moon Rocks\n\nIt begins by telling the story of Operation Lunar Eclipse, the first successful sting operation to recover a piece of the moon brought back by American astronauts, a sting operation the professor led and went undercover in, while still an agent. The sting operation successfully recovered the Honduras Apollo 17 Goodwill Moon Rock that was in the possession of Florida businessman Alan H. Rosen. This operation was funded in part with the financial assistance of H. Ross Perot, billionaire and former Presidential candidate.\n\nOn October 1, 2012, Gutheinz gave a major speech on Operation Lunar Eclipse and the Moon Rock Project before the Engineering Colloquium at NASA's Goddard Space Flight Center. The speech was entitled \"Finding the Missing Moon Rocks\", and is preserved on a video at the Space Flight Center Library. In that speech Joe Gutheinz was critical of NASA and the U.S. Government for not turning over the Cyprus Apollo 17 Goodwill Moon Rock to Cyprus, which was also a major topic in Joe Kloc’s novel. On May 16, 2013 news reports first broke that bowing from pressure from Cyprus the United States Government would give Cyprus its Apollo 17 Goodwill Moon Rock.\n\n"}
{"id": "30531695", "url": "https://en.wikipedia.org/wiki?curid=30531695", "title": "The Hidden Reality", "text": "The Hidden Reality\n\nThe Hidden Reality: Parallel Universes and the Deep Laws of the Cosmos is a book by Brian Greene published in 2011 which explores the concept of the multiverse and the possibility of parallel universes. It has been nominated for the Royal Society Winton Prize for Science Books for 2012.\n\nIn his book, Greene discussed nine types of parallel universes:\n\n\nThe book and its author were featured on the television series \"The Big Bang Theory\" in episode 20 of season 4, \"The Herb Garden Germination\".\n\n\n"}
{"id": "22460884", "url": "https://en.wikipedia.org/wiki?curid=22460884", "title": "The Marshall Illustrated Encyclopedia of Dinosaurs and Prehistoric Animals", "text": "The Marshall Illustrated Encyclopedia of Dinosaurs and Prehistoric Animals\n\nThe Marshall Illustrated Encyclopedia of Dinosaurs and Prehistoric Animals is an encyclopedia of prehistoric and extinct animals which had a vertebrate. The \"Encyclopedia\" is published by Chartwell Books.\n\n\nEarly Tetrapods\n\nLepospondyls\n\nEarly Reptiles\n\nTurtles, Tortoises, and Terrapins\n\nSemi-Aquatic and Marine Reptiles\n\nMarine Reptiles\n\nEarly Diapsids\n\nSnakes and Lizards\n\nEarly Ruling Reptiles\n\nCrocodiles\n\nFlying Reptiles\n\nCarnivorous Dinosaurs\n\nEarly Herbivorous Dinosaurs\n\nLong-necked Browsing Dinosaurs\n\nFabrosaurs, Heterodontosaurs, and Pachycephalosaurs\n\nHypsilophodonts\n\nIguanodonts\n\nDuckbilled Dinosaurs\n\nArmored Dinosaurs\n\nHorned Dinosaurs\n\nEarly and Flightless Birds\n\nWater and Land Birds\n\nPelycosaurs\n\nTheraspids\n\nPrimitive Mammals\n\nMarsupials\n\nGlyptodonts, Sloths, Armadillos, and Anteaters\n\nInsectovores and Creodonts\n\nMustelids and Bears\n\nDogs and Hyenas\n\nCats and Mongooses\n\nSeals, Sealions, and Walruses\n\nWhales, Dolphins, and Porpoises\n\nEarly Rooters and Browsers\n\nEarly Elephants\n\nMastodonts, Mammoths, and Modern Elephants\n\nSouth American Hoofed Mammals\n\nHorses\n\nTapirs and Brontotheres\n\nRhinoceroses\n\nSwine and Hippopotamuses\n\nOreodonts and Early Horned Browsers\n\nCamels\n\nGiraffes, Deer, and Cattle\n\nRodents, Rabbits, and Hares\n\nLemurs and Monkeys\n\nApes\n\nHumans\n\n"}
{"id": "54898290", "url": "https://en.wikipedia.org/wiki?curid=54898290", "title": "The problem of the speckled hen", "text": "The problem of the speckled hen\n\nIn the theory of empirical knowledge, the problem of the speckled hen is whether a single immediate observation of a speckled hen provides a certain knowledge of the number of speckles observed. Clearly, this is not an isolated example, and therefore it is of fundamental nature. Philosophically, this problem probes the limits of knowledge by acquaintance: one is unable to know with certainty the existence of determinate things in one's experience merely by the virtue of the experience.\n\nRoderick Chisholm attributes it to Gilbert Ryle suggesting to A. J. Ayer. It is viewed as a critisicm of the view expressed by C. I. Lewis that there can never be \"positive bafflement in the presence of the immediate, because there is here no question which fails to find an answer\".\n\nJoseph Heath remarks that this problem is one of the \"descendants of Descartes's 'chiliagon' argument in the sixth of his \"Meditations\"\". \n\nA. J. Ayer suggested that if we are unable to enumerate speckles accurately, then it is incorrect to suggest that the \"sense-data\" provides a definite number of speckles despite the fact that the hen does have a definite number of them, clearly outlined. In Ayers' words, speckles are enumerable only if in fact they have been enumerated. \n\nA number of philosophers analyzed the merits of this proposition. Chisholm concludes that the problem of the speckled hen emphacises the fact that there are basic propositions (synthetic propositions which do not refer beyond the content of the immediate experience) that are necessarily imprecise. \n"}
{"id": "24787", "url": "https://en.wikipedia.org/wiki?curid=24787", "title": "UGM-27 Polaris", "text": "UGM-27 Polaris\n\nThe UGM-27 Polaris missile was a two-stage solid-fueled nuclear-armed submarine-launched ballistic missile. The United States Navy's first SLBM, it served from 1961 to 1996.\n\nThe Polaris project was created to replace the solid-fueled Jupiter S project, which had been approved in 1956 to replace the liquid-fueled SM-78 and PGM-19 Jupiter missiles. In December 1956, the United States Navy awarded Polaris development contracts to Lockheed Corporation and Aerojet Rocketdyne.\n\nThe Polaris missile was designed to be used for second strike countervalue (since the CEP was not good enough for first strike counterforce) as part of the Navy's contribution to the United States arsenal of nuclear weapons, replacing the Regulus cruise missile. Known as a Fleet Ballistic Missile (FBM), the Polaris was first launched from the Cape Canaveral, Florida, missile test base on January 7, 1960.\n\nFollowing the Polaris Sales Agreement in 1963, Polaris missiles were also carried on British Royal Navy submarines between 1968 and the mid-1990s.\n\nPlans to equip the Italian Navy with the missile ended in the mid-60s, after several successful test launches carried out on board the . Despite the successful launching tests, the plan was abandoned due to the completion of initial SSBN vessels. Nonetheless, the Italian government set out to develop an indigenous missile, called Alfa. The program was successful, but was halted by Italy's ratification of the Nuclear Non-Proliferation Treaty and the failure of the NATO Multilateral Force.\n\nThe Polaris missile was gradually replaced on 31 of the 41 original SSBNs in the U.S. Navy by the MIRV-capable Poseidon missile beginning in 1972. During the 1980s, these missiles were replaced on 12 of these submarines by the Trident I missile. The 10 - and SSBNs retained Polaris A-3 until 1980 because their missile tubes were not large enough to accommodate Poseidon. With beginning sea trials in 1980, these submarines were disarmed and redesignated as attack submarines to avoid exceeding the SALT II strategic arms treaty limits.\n\nThe Polaris missile program's complexity led to the development of new project management techniques, including the Program Evaluation and Review Technique (PERT) to replace the simpler Gantt chart methodology.\n\nAt the start of the second World War, nearly every major world military force that was involved in the war had at least developed rough ideas of a rocket program. It is important to note that at this time the distinction between rockets and missiles was simply this: rockets traveled over a fixed trajectory and missiles could be guided to their destination. Rockets of all shapes and sizes were being implemented in war-zones around the globe. The Soviets deployed rockets such as the Stalin Organ, which were fired from a mobile launcher in waves of up to nearly 50 small, unguided rockets, and the Japanese were implementing rockets that would be used on the front lines. Rockets such as the Stalin Organ could fire at targets within three miles, while the first Japanese rockets were only valuable for targets less than five-hundred feet away. The initial version of the Japanese Kamikaze planes were made of wood and powered by rockets. These wooden suicide planes did not provide the Japanese forces with a reliable weapon, and by 1945 the Kamikaze gliders were being used in combat, no matter how ineffective they may have been. British forces, too, had begun developments on anti-aircraft rockets of their own, which proved effective as early as 1941. Soon after the attacks on Pearl Harbor, the United States also joined arms in the race for rockets borrowing much of its initial products from the British armed forces. The United States rocket program began regularly testing both rockets and missiles, and by 1945, the Army was investing roughly $150 million a year, while the Navy was spending $1.2 billion. Despite these efforts from the major contributing forces in the war, German scientists excelled quickly at mastering the largest and most advanced weapons. One of which, the German V-2 rocket, would become the blueprint for all of the serious global missile programs to come.\n\nAs the United States Army continued to make steady advancements in its rocket and missile programs it quickly became apparent that if the program wished to keep up with its own rapid growth, as well as with the rest of the world, it would certainly need more space than what was available. On October 28, 1949, Huntsville, Alabama, was chosen based on its promising location and easy access to resources to be the new home to the American program. By the end of 1950, the Redstone Arsenal was operational and took on the new designation as the Ordnance Guided Missile Center.\n\nThe Polaris missile replaced an earlier plan to create a submarine-based missile force based on a derivative of the U.S. Army Jupiter Intermediate-range ballistic missile. Chief of Naval Operations Admiral Arleigh Burke appointed Rear Admiral W. F. \"Red\" Raborn as head of a Special Project Office to develop Jupiter for the Navy in late 1955. The Jupiter missile's large diameter was a product of the need to keep the length short enough to fit in a reasonably-sized submarine. At the seminal Project Nobska conference in 1956, with Admiral Burke present, nuclear physicist Edward Teller stated that a physically small one-megaton warhead could be produced for Polaris within a few years, and this prompted Burke to leave the Jupiter program and concentrate on Polaris in December of that year. Polaris was spearheaded by the Special Project Office's Missile Branch under Rear Admiral Roderick Osgood Middleton, and is still under the Special Project Office. Admiral Burke later was instrumental in determining the size of the Polaris submarine force, suggesting that 40-45 submarines with 16 missiles each would be sufficient. Eventually, the number of Polaris submarines was fixed at 41.\n\nThe was the first submarine that was capable of deploying a submarine-launched ballistic missiles (SLBM) the US developed. The responsibility of the development of SLBMs was given to the Navy and the Army. The Air Force was charged with developing a land-based intermediate range ballistic missile (IRBM), while an IRBM which could be launched by land or by sea was tasked to the Navy and Army. The Navy Special Projects (SP) office was at the head of the project. It was led by Rear Admiral William Raborn.\n\nOn September 13, 1955, James R. Killian, head of a special committee organized by President Eisenhower, recommended that both the Army and Navy come together under a program aimed at developing an Intermediate Range Ballistic Missile (IRBM). The missile, later known as Jupiter, would be developed under the Joint Army-Navy Ballistic Missile Committee approved by Secretary of Defense Charles E. Wilson in early November of that year. The first IRBM boasted a liquid-fueled design. Liquid fuel is compatible with aircraft; it is less compatible with submarines. Solid fuels, on the other hand, make logistics and storage simpler and are safer. Not only was the Jupiter a liquid fuel design, it was also very large; even after it was designed for solid fuel, it was still a whopping 160,000 pounds. A smaller, new design would weigh much less, estimated at 30,000 pounds. The Navy would rather develop a smaller, more easily manipulated design. Edward Teller was one of the scientists encouraging the progress of smaller rockets. He argued that the technology needed to be discovered, rather than apply technology that is already created. Raborn was also convinced he could develop smaller rockets. He sent officers to make independent estimates of size to determine the plausibility of a small missile; while none of the officers could agree on a size, their findings were encouraging nonetheless.\n\nThe US Navy began work on nuclear-powered submarines in 1946. They launched the first one, the in 1955. Nuclear powered submarines were the least vulnerable to a first strike from the Soviet Union.The next question that led to further development was what kind of arms the nuclear-powered submarines should be equipped with. In the summer of 1956, the navy sponsored a study by the National Academy of Sciences on anti-submarine warfare at Nobska Point in Woods Hole, Massachusetts, known as Project NOBSKA. The navy’s intention was to have a new missile developed that would be lighter than existing missiles and cover a range up to fifteen hundred miles. A problem that needed to be solved was that this design would not be able to carry the desired one-megaton thermonuclear warhead.\n\nThis study brought Edward Teller from the recently formed nuclear weapons laboratory at Livermore and J. Carson Mark, representing the Los Alamos nuclear weapons laboratory. Teller was already known as a nuclear salesman, but this became the first instance where there was a big betting battle where he outbid his Los Alamos counterpart. The two knew each other well: Mark was named head of the theoretical division of Los Alamos in 1947, a job that was originally offered for Teller. Mark was a cautious physicist and no match for Teller in a bidding war.\n\nAt the NOBSKA summer study, Edward Teller made his famous contribution to the FBM program. Teller offered to develop a lightweight warhead of one-megaton strength within a prescribed five years. He suggested that nuclear-armed torpedoes could be substituted for conventional ones to provide a new anti-submarine weapon. Livermore received the project. When Teller returned to Livermore, people were astonished by the boldness of Teller’s promise. It seemed inconceivable with the current size of nuclear warheads, and Teller was challenged to support his assertion. He pointed out the trend in warhead technology, which indicated reduced weight to yield ratios in each succeeding generation. When Teller was questioned about the application of this to the FBM program, he asked, ‘Why use a 1958 warhead in a 1965 weapon system?’\n\nMark disagreed with Teller’s prediction that the desired one-megaton warhead could be made to fit the missile envelope within the timescale envisioned. Instead, Mark suggested that half a megaton would be more realistic and he quoted a higher price and a longer deadline. This simply confirmed the validity of Teller’s prediction in the Navy’s eyes. Whether the warhead was half or one megaton mattered little so long as it fitted the missile and would be ready by the deadline. Almost four decades later, Teller said, referring to Mark’s performance, that it was “an occasion when I was happy about the other person being bashful.”\nWhen the Atomic Energy Commission backed up Teller’s estimate in early September, Admiral Burke and the Navy Secretariat decided to support SPO in heavily pushing for the new missile, now named Polaris by Admiral Raborn.\n\nThere is a contention that the Navy's \"Jupiter\" missile program was unrelated to the Army program. The Navy also expressed an interest in Jupiter as an SLBM, but left the collaboration to work on their Polaris. At first, the newly assembled SPO team had the problem of making the large, liquid-fuel Jupiter IRBM to work properly. Jupiter retained the short, squat shape intended to fit in naval submarines. Its sheer size and volatility of its fuel made it very unsuited to submarine launching and was only slightly more attractive for deployment on ships. The missile continued to be developed by the Army’s German team in collaboration with their main contractor, Chrysler Corporation. SPO’s responsibility was to develop a sea-launching platform with necessary fire control and stabilization systems for that very purpose. The original schedule was to have a ship-based IRBM system ready for operation evaluation by January 1, 1960, and a submarine-based one by January 1, 1965.\nHowever, the Navy was deeply dissatisfied with the liquid fuel IRBM. The first concern was that the cryogenic liquid fuel was not only extremely dangerous to handle, but launch-preparations were also very time-consuming. Second, an argument was made that liquid-fueled rockets gave relatively low initial acceleration, which is disadvantageous in launching a missile from a moving platform in certain sea states. By mid-July 1956, the Secretary of Defense’s Scientific Advisory Committee had recommended that a solid-propellant missile program be fully instigated but not using the unsuitable Jupiter payload and guidance system.\nBy October 1956, a study group comprising key figures from Navy, industry and academic organizations considered various design parameters of the Polaris system and trade-offs between different sub-sections. The estimate that a 30,000-pound missile could deliver a suitable warhead over 1500 nautical miles was endorsed. With this optimistic assessment, the Navy now decided to scrap the Jupiter program altogether and sought out the Department of Defense to back a separate Navy missile.\nA huge surfaced submarine would carry four \"Jupiter\" missiles, which would be carried and launched horizontally. This was probably the never-built SSM-N-2 Triton program. However, a history of the Army's Jupiter program states that the Navy was involved in the Army program, but withdrew at an early stage.\n\nOriginally, the Navy favored cruise missile systems in a strategic role, such as the Regulus missile deployed on the earlier and a few other submarines, but a major drawback of these early cruise missile launch systems (and the Jupiter proposals) was the need to surface, and remain surfaced for some time, to launch. Submarines were very vulnerable to attack during launch, and a fully or partially fueled missile on deck was a serious hazard. The difficulty of preparing a launch in rough weather was another major drawback for these designs, but rough sea conditions did not unduly affect Polaris' submerged launches.\n\nIt quickly became apparent that solid-fueled ballistic missiles had advantages over cruise missiles in range and accuracy, and could be launched from a submerged submarine, improving submarine survivability.\n\nThe prime contractor for all three versions of Polaris was Lockheed Missiles and Space Company, now Lockheed Martin.\n\nThe Polaris program started development in 1956. , the first US missile submarine, successfully launched the first Polaris missile from a submerged submarine on July 20, 1960. The A-2 version of the Polaris missile was essentially an upgraded A-1, and it entered service in late 1961. It was fitted on a total of 13 submarines and served until June 1974.(1). Ongoing problems with the W-47 warhead, especially with its mechanical arming and safing equipment, led to large numbers of the missiles being recalled for modifications, and the U.S. Navy sought a replacement with either a larger yield or equivalent destructive power. The result was the W-58 warhead used in a \"cluster\" of three warheads for the Polaris A-3, the final model of the Polaris missile.\n\nOne of the initial problems the Navy faced in creating an SLBM was that the sea moves, while a launch platform on land does not. Waves and swells rocking the boat or submarine, as well as possible flexing of the ship’s hull, had to be taken into account to properly aim the missile.\n\nThe Polaris development was kept on a tight schedule and the only influence that changed this was the USSR’s launching of SPUTNIK on October 4, 1957. This caused many working on the project to want to accelerate development. The launch of a second Russian satellite and pressing public and government opinions caused Secretary Wilson to move the project along more quickly.\n\nThe Navy favored an underwater launch of an IRBM, although the project began with an above-water launch goal. They decided to continue the development of an underwater launch, and developed two ideas for this launch: wet and dry. Dry launch meant encasing the missile in a shell that would peel away when the missile reached the water’s surface. Wet launch meant shooting the missile through the water without a casing. While they Navy was in favor of a wet launch, they developed both methods as a failsafe. They did this with the development of gas and air propulsion of the missile out of the submerged tube as well.\n\nThe first Polaris missile tests were given the names “AX-#” and later renamed “A1X-#”. Testing of the missiles occurred:\n\nSept 24, 1958: AX-1, at Cape Canaveral from a launch pad; the missile was destroyed, after it failed to turn into the correct trajectory following a programming-error.\n\nOctober 1958: AX-2, at Cape Canaveral from a launch pad; exploded on the launch pad.\n\nDecember 30, 1958: AX-3, at Cape Canaveral from a launch pad; launched correctly, but was destroyed because of the fuel overheating.\n\nJanuary 19, 1959: AX-4, at Cape Canaveral from launch pad: launched correctly but began to behave erratically and was destroyed.\n\nFebruary 27, 1959: AX-5, at Cape Canaveral from launch pad: launched correctly but began to behave erratically and was destroyed.\n\nApril 20, 1959: AX-6, at Cape Canaveral from launch pad: this test was a success. The missile launched, separated, and splashed into the Atlantic 300 miles off shore.\n\nIt was in between these two tests that the inertial guidance system was developed and implemented for testing.\n\nJuly 1, 1959: AX-11 at Cape Canaveral from a launch pad: this launch was successful, but pieces of the missile detached causing failure. It did show that the new guidance systems worked.\n\nAt the time that the Polaris project went live, submarine navigation systems were and at this time that standard was sufficient enough to sustain effective military efforts given the existing weapons systems in use by the Army, Air Force and Navy. Initially, developers of Polaris were set to utilize the existing 'Stable Platform' configuration of the inertial guidance system. Created at the MIT Instrumentation Laboratory, this Ships Inertial Navigation System (SINS) was supplied to the Navy in 1954. The developers of Polaris encountered many issues from the birth of the project, however, perhaps the most unsettling for them was the outdated technology of the gyroscopes they would be implementing.\nThis 'Stable Platform' configuration did not account for the change in gravitational fields that the submarine would experience while it was in motion, nor did it account for the ever-altering position of the Earth. This problem raised many concerns, as this would make it nearly impossible for navigational read outs to remain accurate and reliable. A submarine hauling several hundred-thousand pounds of Fleet Ballistic Missiles was of little to no use if operators had no way to direct them. Polaris was thus forced to seek elsewhere and quickly found hope in a guidance system that had been abandoned by the US Air Force. A company by the name of Autonetics Division of North American Aviation had previously been faced with the task of developing a guidance system for the US Air Force Navaho known as the XN6 Autonavigator. The XN6 was a system designed for air-breathing Cruise missiles, but by 1958 had proved useful for installment on submarines.\n\nA predecessor to the GPS satellite navigation system, the Transit system (later called NAVSAT), was developed because the submarines needed to know their position at launch in order for the missiles to hit their targets. Two American physicists, William Guier and George Weiffenbach, at Johns Hopkins's Applied Physics Laboratory (APL), began this work in 1958. A computer small enough to fit through a submarine hatch was developed in 1958, the AN/UYK-1. It was used to interpret the Transit satellite data and send guidance information to the Polaris, which had its own guidance computer made with ultra miniaturized electronics, very advanced for its time, because there wasn't much room in a Polaris—there were 16 on each submarine. The Ship's Inertial Navigation System (SINS) was developed earlier to provide a continuous dead reckoning update of the submarine's position between position fixes via other methods, such as LORAN. This was especially important in the first few years of Polaris, because Transit was not operational until 1964. By 1965 microchips similar to the Texas Instruments units made for the Minuteman II were being purchased by the Navy for the Polaris. The Minuteman guidance systems each required 2000 of these, so the Polaris guidance system may have used a similar number. To keep the price under control, the design was standardized and shared with Westinghouse Electric Company and RCA. In 1962, the price for each Minuteman chip was $50, the price dropped to $2 in 1968.\n\nThis missile replaced the earlier A-1 and A-2 models in the US Navy, and also equipped the British Polaris force. The A-3 had a range extended to and a new weapon bay housing three Mk 2 re-entry vehicles (ReB or Re-Entry Body in US Navy and British usage); and the new W-58 warhead of 200 kt yield. This arrangement was originally described as a \"cluster warhead\" but was replaced with the term Multiple Re-Entry Vehicle (MRV). The three warheads were spread about a common target and were not independently targeted (such as a MIRV missile is). The three warheads were stated to be equivalent in destructive power to a single one-megaton warhead. Later the Polaris A-3 missiles (but not the ReBs) were also given limited hardening to protect the missile electronics against nuclear electromagnetic pulse effects while in the boost phase. This was known as the A-3T (\"Topsy\") and was the final production model.\n\nThe initial test model of the Polaris was referred to as the AX series and made its maiden flight from Cape Canaveral on September 24, 1958. The missile failed to perform its pitch and roll maneuver and instead just flew straight up, however the flight was considered a partial success (at that time, \"partial success\" was used for any missile test that returned usable data). The next flight on October 15 failed spectacularly when the second stage ignited on the pad and took off by itself. Range Safety blew up the errant rocket while the first stage sat on the pad and burned. The third and fourth tests (December 30 and January 9) had problems due to overheating in the boattail section. This necessitated adding extra shielding and insulation to wiring and other components. When the final AX flight was conducted a year after the program began, 17 Polaris missiles had been flown of which five met all of their test objectives.\n\nThe first operational version, the Polaris A-1, had a range of and a single Mk 1 re-entry vehicle, carrying a single W-47-Y1 600 kt nuclear warhead, with an inertial guidance system which provided a circular error probable (CEP) of . The two-stage solid propellant missile had a length of , a body diameter of , and a launch weight of .\n\nWork on its W47 nuclear warhead began in 1957 at the facility that is now called the Lawrence Livermore National Laboratory by a team headed by John Foster and Harold Brown. The Navy accepted delivery of the first 16 warheads in July 1960. On May 6, 1962, a Polaris A-2 missile with a live W47 warhead was tested in the \"Frigate Bird\" test of Operation Dominic by in the central Pacific Ocean, the only American test of a live strategic nuclear missile.\n\nThe two stages were both steered by thrust vectoring. Inertial navigation guided the missile to about a 900 m (3,000-foot) CEP, insufficient for use against hardened targets. They were mostly useful for attacking dispersed military surface targets (airfields or radar sites), clearing a pathway for heavy bombers, although in the general public perception Polaris was a strategic second-strike retaliatory weapon.\n\nThe Polaris A-1 missile was developed to complement the limited number of medium-range systems deployed throughout Europe. As those systems lacked the range to attack major Soviet targets, Polaris was developed to increase the level of nuclear deterrence. At this time there was little threat of counterforce strikes, as few systems had the accuracy to destroy missile systems. The primary advantages of ballistic missile submarines was their ability to launch submerged, which offered improved survivability for the submarine while also (like their Regulus predecessors) keeping shorter ranged systems within range.\n\nThe USN had forward-basing arrangements for its Atlantic-based Polaris fleet with both the United Kingdom and Spain, permitting the use of bases at the Holy Loch in Scotland (established in 1961) and at Naval Station Rota (Polaris base established 1964) in the Bay of Cadiz. The forward deployment bases were much closer to patrol areas than U.S. East Coast bases, avoiding the necessity for lengthy transit times. In the Pacific, a Polaris base was also established at Guam in 1964. The Regulus missile program was deactivated with the advent of Polaris in the Pacific. The forward-basing arrangement was continued when Poseidon replaced Polaris, starting in 1972, in what by then were the 31 Atlantic Fleet SSBNs. The 10 older SSBNs that could not use Poseidon were assigned to the Pacific Fleet in the 1970s. Polaris was not accurate enough to destroy hardened targets, but would have been effective against dispersed surface targets, such as airfields, radar and SAM sites, as well as military and industrial centers of strategic importance. The military authorities, however, regarded Polaris as but one part of a nuclear triad including ICBMs and bombers, each with its own function. The task allotted to Polaris of 'taking out' peripheral defenses was well-suited to its characteristics and limitations.\n\nThe forward deployment strategy required some infrastructure. To allow quick establishment of bases and to minimize the impact on the host country, each base was centered around a submarine tender and a floating drydock, with minimal facilities on shore, mostly family support for the tender's crew. The first Polaris submarine tender was , a World War II tender that was refitted in 1959–60 with the insertion of a midships missile storage compartment and handling crane. \"Proteus\" established each of the three forward deployment bases. Four additional Polaris tenders (, , , and ) were commissioned 1962–65.\n\nA two-crew concept was established for SSBNs, combined with forward deployment to maximize the time each submarine would spend on patrol. The crews were named Blue and Gold after the US Naval Academy colors. The crews were deployed for 105 days and at their home bases for 95 days, with a 3-day turnover period on each end of the deployed period. Crews were flown from their home bases to and from the forward deployment bases. After taking over the boat, the crew would perform a 30-day refit assisted by the tender, followed by a 70-day deterrent patrol. Sometimes a port visit would be arranged in the middle of the patrol. The home bases for Atlantic Fleet crews were Groton, Connecticut and Charleston, South Carolina. Pacific Fleet crews were based at Pearl Harbor, Hawaii.\n\nTwo Polaris missile depots were established in the United States, Polaris Missile Facility Atlantic (POMFLANT) at Charleston, South Carolina in 1960 and later Strategic Weapons Facility Pacific (SWFPAC) at Bangor, Washington. To transport missiles and other supplies from the missile depots to the forward deployment bases, several cargo ships were converted to carry missiles and were designated as T-AKs, operated by the Military Sealift Command with a mostly-civilian crew.\n\nThe advent of the Trident I missile, refitted to 12 Atlantic Fleet SSBNs starting in 1979 and with a much greater range than Polaris or Poseidon, meant that SSBNs could be based in the United States. The 18 s, slated to replace the 41 older SSBNs, also started commissioning in 1981, initially carrying 24 Trident I missiles but later refitted with the much larger and more capable Trident II missile. In the late 1970s it was decided that Pacific Fleet \"Ohio\"-class SSBNs would be based at Bangor, WA, collocated with SWFPAC, and that the refitted Trident I SSBNs and additional \"Ohio\"-class SSBNs would be based at a new facility in King's Bay, Georgia. Also, a new missile depot, Strategic Weapons Facility Atlantic (SWFLANT), was constructed at King's Bay to replace POMFLANT. The SSBN facility at Rota was closed in 1979 as King's Bay began refitting submarines. As commenced sea trials in 1980, the 10 remaining Polaris submarines in the Pacific Fleet were disarmed and reclassified as SSNs to avoid exceeding SALT II treaty limits. The SSBN base at Guam was closed at this time. By 1992, the Soviet Union had collapsed, 12 \"Ohio\"-class SSBNs had been commissioned, and the START I treaty had gone into effect, so Holy Loch was closed and the remaining 31 original SSBNs disarmed. Most of these were decommissioned and later scrapped in the Ship-Submarine Recycling Program, but a few were converted to other roles. Two remain in service but decommissioned as nuclear power training vessels attached to Naval Nuclear Power School at Charleston, SC, and .\n\nTo meet the need for greater accuracy over the longer ranges the Lockheed designers included a reentry vehicle concept, improved guidance, fire control, and navigation systems to achieve their goals. To obtain the major gains in performance of the Polaris A3 in comparison to early models, there were many improvements, including propellants and material used in the construction of the burn chambers. The later versions (the A-2, A-3, and B-3) were larger, weighed more, and had longer ranges than the A-1. The range increase was most important: The A-2 range was , the A-3 , and the B-3 . The A-3 featured multiple re-entry vehicles (MRVs) which spread the warheads about a common target, and the B-3 was to have penetration aids to counter Soviet Anti-Ballistic Missile defenses.\n\nThe US Navy began to replace Polaris with Poseidon in 1972. The B-3 missile evolved into the C-3 Poseidon missile, which abandoned the decoy concept in favor of using the C3's greater throw-weight for larger numbers (10–14) of new hardened high-re-entry-speed reentry vehicles that could overwhelm Soviet defenses by sheer weight of numbers, and its high speed after re-entry. This turned out to be a less than reliable system and soon after both systems were replaced by the Trident. A proposed Undersea Long-Range Missile System (ULMS) program outlined a long-term plan which proposed the development of a longer-range missile designated as ULMS II, which was to achieve twice the range of the existing Poseidon (ULMS I) missile. In addition to a longer-range missile, a larger submarine (Ohio-class) was proposed to replace the submarines currently being used with Poseidon. The ULMS II missile system was designed to be retrofitted to the existing SSBNs, while also being fitted to the proposed Ohio-class submarine.\n\nIn May 1972, the term ULMS II was replaced with Trident. The Trident was to be a larger, higher-performance missile with a range capacity greater than 6000 miles. Under the agreement, the United Kingdom paid an additional 5% of their total procurement cost of 2.5 billion dollars to the US government as a research and development contribution.\nIn 2002, the United States Navy announced plans to extend the life of the submarines and the D5 missiles to the year 2040. This requires a D5 Life Extension Program (D5LEP), which is currently underway. The main aim is to replace obsolete components at minimal cost by using commercial off the shelf (COTS) hardware; all the while maintaining the demonstrated performance of the existing Trident II missiles.\n\nSTARS, a strategic targeting system, is a BMDO program managed by the U. S. Army Space and Strategic Defense Command (SSDC). It began in 1985 in response to concerns that the supply of surplus Minuteman I boosters used to launch targets and other experiments on intercontinental ballistic missile flight trajectories in support of the Strategic Defense Initiative would be depleted by 1988. SSDC tasked Sandia National Laboratories, a Department of Energy laboratory, to develop an alternative launch vehicle using surplus Polaris boosters. The Sandia National Laboratories developed two STARS booster configurations: STARS I and STARS II.\n\nSTARS I consisted of refurbished Polaris first and second stages and a\ncommercially procured Orbis I third stage. It can deploy single or multiple payloads, but the multiple payloads cannot be deployed in a manner that simulates the operation of a post-boost vehicle. To meet this specific need, Sandia developed an Operations and Deployment\nExperiments Simulator (ODES), which functions as a PBV. When ODES was added to STARS I, the configuration is became known as STARS II. The development phase of the STARS program was completed in year 1994, and BMDO provided about $192.1 million for this effort. The operational phase began in year 1995. The first STARS I flight, a hardware check-out flight, was launched in February 1993, and the second flight, a STARS I reentry vehicle experiment, was launched in August 1993.\n\nThe third flight, a STARS II development mission, was launched in July 1994, with all three flights considered to be successful by BMDO. The Secretary of Defense conducted a comprehensive review in 1993 of the nation’s defense strategy, which drastically reduced the number of STARS launches required to support National Missile Defense (NMD)2 and BMDO funding. Due to the launch and budget reductions, the STARS office developed a draft long-range plan for the STARS program. The study examined three options:\nWhen the STARS program was started in 1985 it was perceived that there would be four launches per year. Because of the large number of anticipated launches and an unknown defect rate for surplus Polaris motors, the STARS office acquired 117 first-stage and 102 second-stage surplus motors. As of December 1994, seven first-stage and five second-stage refurbished motors were available for future launches. BMDO is currently evaluating STARS as a potential long-range system for launching targets for development tests of future Theater Missile Defense 3 systems. STARS I was first launched in 1993, and from 2004 onwards has served as the standard booster for trials of the Ground-Based Interceptor.\n\nFrom the early days of the Polaris program, American senators and naval officers suggested that the United Kingdom might use Polaris. In 1957 Chief of Naval Operations Arleigh Burke and First Sea Lord Louis Mountbatten began corresponding on the project. After the cancellations of the Blue Streak and Skybolt missiles in the 1960s, under the 1962 Nassau Agreement that emerged from meetings between Harold Macmillan and John F. Kennedy, the United States would supply Britain with Polaris missiles, launch tubes, ReBs, and the fire-control systems. Britain would make its own warheads and initially proposed to build five ballistic missile submarines, later reduced to four by the incoming Labour government of Harold Wilson, with 16 missiles to be carried on each boat. The Nassau Agreement also featured very specific wording. The intention of wording the agreement in this manner was to make it intentionally opaque. The sale of the Polaris was malleable in how an individual country could interpret it due to the diction choices taken in the Nassau Agreement. For the United States of America, the wording allowed for the sale to fall under the scope of NATO's deterrence powers. On the other hand, for the British, the sale could be viewed as a solely British deterrent. The Polaris Sales Agreement was signed on April 6, 1963.\n\nIn return, the British agreed to assign control over their Polaris missile targeting to the SACEUR (Supreme Allied Commander, Europe), with the provision that in a national emergency when unsupported by the NATO allies, the targeting, permission to fire, and firing of those Polaris missiles would reside with the British national authorities. Nevertheless, the consent of the British Prime Minister is and has been always required for the use of British nuclear weapons, including SLBMs.\n\nThe operational control of the Polaris submarines was assigned to another NATO Supreme Commander, the SACLANT (Supreme Allied Commander, Atlantic), who is based near Norfolk, Virginia, although the SACLANT routinely delegated control of the missiles to his deputy commander in the Eastern Atlantic area, COMEASTLANT, who was always a British admiral.\n\nPolaris was the largest project in the Royal Navy's peacetime history. Although in 1964 the new Labour government considered cancelling Polaris and turning the submarines into conventionally armed hunter-killers, it continued the program as Polaris gave Britain a global nuclear capacity—perhaps east of Suez—at a cost £150 million less than that of the V bomber force. By adopting many established, American, methodologies and components Polaris was finished on time and within budget. On 15 February 1968, , the lead ship of her class, became the first British vessel to fire a Polaris. All Royal Navy SSBNs have been based at Faslane, only a few miles from Holy Loch. Although one submarine of the four was always in a shipyard undergoing a refit, recent declassifications of archived files disclose that the Royal Navy deployed four boatloads of reentry vehicles and warheads, plus spare warheads for the Polaris A3T, retaining a limited ability to re-arm and put to sea the submarine that was in refit. When replaced by the Chevaline warhead, the sum total of deployed RVs and warheads was reduced to three boatloads.\n\nThe original U.S. Navy Polaris had not been designed to penetrate anti-ballistic missile (ABM) defenses, but the Royal Navy had to ensure that its small Polaris force operating alone, and often with only one submarine on deterrent patrol, could penetrate the ABM screen around Moscow. Britain’s submarines featured the Polaris A3T missiles, a modification to the model of the Polaris used by the U.S. from 1968 to 1972. Similar concerns were present in the U.S. as well, resulting in a new American defense program.\n\nThe program became known as Antelope, and its purpose was to alter the Polaris. Various aspects of the Polaris, such as increasing deployment efficiency and creating ways to improve the penetrative power were specific items considered in the tests conducted during the Antelope program. The British's uncertainty with their missiles led to the examination of the Antelope program. The assessments of Antelope occurred at Aldermaston. Evidence from the evaluation of Antelope led to the British decision to undertake their program following that of the United States.\n\nThe result was a programme called \"Chevaline\" that added multiple decoys, chaff, and other defensive countermeasures. Its existence was only revealed in 1980, partly because of the cost overruns of the project, which had almost quadrupled the original estimate given when the project was finally approved in January 1975.The program also ran into trouble when dealing with the British Labour Party. Their Chief Scientific Adviser, Solly Zuckerman, believed that Britain no longer needed new designs for nuclear weapons and no more nuclear warhead tests would be necessary. Though the Labour party provided a clear platform on nuclear weapons, the Chevaline program found supporters. One such individual who supported modification to the Polaris was the Secretary of state for Defense, Denis Healey.\n\nDespite the approval of the program, the expenses caused hurdles that augmented the time it took for the system to come to fruition. The cost of the project led to Britain’s revisit of disbanding the program in 1977. The system became operational in mid-1982 on , and the last British SSBN submarine was equipped with it in mid-1987. Chevaline was withdrawn from service in 1996.\n\nThough Britain adopted the Antelope program methods, no input on the design came from the United States. Aldermaston was solely responsible for the Chevaline warheads.\n\nThe British did not ask to extend the Polaris Sales Agreement to cover the Polaris successor Poseidon due to its cost. The Ministry of Defence upgraded its nuclear missiles to the longer-ranged Trident after much political wrangling within the Callaghan Labour Party government over its cost and whether it was necessary. The outgoing Prime Minister James Callaghan made his government's papers on Trident available to Margaret Thatcher's new incoming Conservative Party government, which took the decision to acquire the Trident C4 missile.\n\nA subsequent decision to upgrade the missile purchase to the even larger, longer-ranged Trident D5 missile was possibly taken to ensure that there was missile commonality between the U.S. Navy and the Royal Navy, which was considerably important when the Royal Navy Trident submarines were also to use the Naval Submarine Base Kings Bay.\n\nEven though the U.S. Navy initially deployed the Trident C4 missile in the original set of its \"Ohio\"-class submarines, it was always planned to upgrade all of these submarines to the larger and longer-ranged Trident D5 missile—and that eventually, all of the C4 missiles would be eliminated from the U.S. Navy. This change-over has been completely carried out, and no Trident C4 missiles remain in service.\n\nThe Polaris missile remained in Royal Navy service long after it had been completely retired and scrapped by the U.S. Navy in 1980–1981. Consequently, many spare parts and repair facilities for the Polaris that were located in the U.S. ceased to be available (such as at Lockheed, which had moved on first to the Poseidon and then to the Trident missile).\n\nDuring its reconstruction program in 1957–1961, the was fitted with four Polaris missile launchers located in the aft part of the ship. The Italian usage of Polaris missiles was partially the result of the Kennedy administration. Prior to 1961, the Italian and Turkish fleets were outfitted with Jupiter missiles. Three factors were instrumental in the movement away from the Jupiter project in Italy and Turkey: the president’s view of the project, new understanding about weapons systems and the diminished necessity of the Jupiter missile. The Joint Congressional Committee report on Atomic Energy accentuated the three previous factors in Italy's decision to switch to the Polaris missiles.Successful tests held in 1961–1962 induced the United States to study a NATO Multilateral Nuclear Force (MLF), consisting of 25 international surface vessels from the US, United Kingdom, France, Italy, and West Germany, equipped with 200 Polaris nuclear missiles, enabling European allies to participate in the management of the NATO nuclear deterrent.\n\nThe report advocated a change from the outdated Jupiter missiles, already housed by the Italians, to the newer missile, Polaris. The report resulted in Secretary of State Dean Rusk and Assistant Secretary of Defense Paul Nitze discussing the possibility of changing the warheads in the Mediterranean. The Italians were not swayed by the American’s interest in modernizing their warheads. However, after the Cuban Missile Crisis, Kennedy met the Italian leader Amitore Fanfani in Washington. Fanfani conceded and went along with Kennedy’s Polaris plan, despite the Italians hoping to stick with the Jupiter missile.\n\nThe MLF plan, as well as the Italian Polaris Program, were abandoned, both for political reasons (in consequence of the Cuban Missile Crisis) and the initial operational availability of the first SSBN , which was capable of launching SLBMs while submerged, a solution preferable to surface-launched missiles.\n\nItaly developed a new domestic version of the missile, the SLBM-designated Alfa. That program was cancelled in 1975 after Italy ratified the Nuclear Non-Proliferation Treaty, with the final launch of the third prototype in 1976.\n\nTwo Italian Navy cruisers, commissioned in 1963–1964, were \"fitted for but not with\" two Polaris missile launchers per ship. All four launchers were built but never installed, and were stored at the La Spezia naval facility.\n\nThe , launched in 1969, was also \"fitted for but not with\" four Polaris missile launchers. During refit periods in 1980–1983, these facilities were removed and used for other weapons and systems.\n\n\nNotes\nBibliography\n\n\n"}
{"id": "13240212", "url": "https://en.wikipedia.org/wiki?curid=13240212", "title": "Under the Sea Wind", "text": "Under the Sea Wind\n\nUnder the Sea Wind: A Naturalist's Picture of Ocean Life (1941) is the first book written by the American marine biologist Rachel Carson. It was published by Simon & Schuster in 1941, when it received very good reviews but sold poorly. After the great success of a sequel \"The Sea Around Us\" (Oxford, 1951), it was reissued by Oxford University Press; that edition was an alternate Book-of-the-Month Club selection and became another bestseller. It is recognised today as one of the \"definitive works of American nature writing,\" and is in print as one of the Penguin Nature Classics.\n\n\"Under the Sea Wind\" describes the behaviour of fish and seabirds accurately, but in story form, often using the scientific names of species as character names. Carson's stated goal in doing so was \"to make the sea and its life as vivid a reality for those who may read the book as it has become for me during the past decade.\" The first of her characters is introduced this way:\n\nWith the dusk a strange bird came to the island from its nesting grounds on the outer banks. Its wings were pure black, and from tip to tip their spread was more than the length of a man's arm. It flew steadily and without haste across the sound, its progress as measured and as meaningful as that of the shadows which little by little were dulling the bright water path. The bird was called Rynchops, the black skimmer.\"\nThe middle section of the book follows the life-story of Scomber, the mackerel, while the last part describes pond creatures such as eels and ducks. A glossary at the end of the book provides additional detail.\n\nThe style of Carson's writing makes the book suitable for children as well as adults, and the appeal is enhanced with illustrations, originally by Howard Frech, and replaced in 1991 with illustrations by Robert W. Hines. Carson acknowledged the influence of nature-novelist Henry Williamson on her writing style, but uses her scientific expertise to ground \"Under the Sea Wind\" in scientifically accurate detail on each animal's appearance, diet and behaviour.\n"}
{"id": "57876418", "url": "https://en.wikipedia.org/wiki?curid=57876418", "title": "Valerie Saena Tuia", "text": "Valerie Saena Tuia\n\nValerie Saena Tuia is a plant scientist from Samoa. She served as Officer in Charge of the Genetic Resources at the Secretariat of the Pacific Community Centre for Pacific Crops and Trees for over 15 years, retiring in 2017.\n\nTuia holds a Bachelor of Agriculture, a postgraduate Diploma in Agriculture and a Masters in Agriculture. In Samoa she held the positions of senior agricultural officer and principal research office in the Samoan civil service. In these roles she was responsible for working with farmers to assist them to diversify and improve their crops. She later moved to Fiji to work at the Secretariat of the Pacific Community Centre for Pacific Crops and Trees as officer in change of genetic resources.\n\nDuring her tenure, Tuia carried out scientific research into the root crop taro, established the world’s largest collection of taro and managed the region’s plant genebank. The centre's collection of taro has been used to develop new varieties of taro tolerant to leaf blight disease. Tuia also worked on a system for propagating breadfruit which produced more vigorous and taller plants that those grown using conventional methods.\n\nFollowing her retirement, Tuia returned to Samoa to run a family business.\n"}
{"id": "15452231", "url": "https://en.wikipedia.org/wiki?curid=15452231", "title": "Wind stress", "text": "Wind stress\n\nIn physical oceanography and fluid dynamics, the wind stress is the shear stress exerted by the wind on the surface of large bodies of water – such as oceans, seas, estuaries and lakes. It is the force component parallel to the surface, per unit area, as applied by the wind on the water surface. The wind stress is affected by the wind speed, the shape of the wind waves and the atmospheric stratification. It is one of the components of the air–sea interaction, with others being the atmospheric pressure on the water surface, as well as the exchange of heat and mass between the water and the atmosphere.\n\nThe air blowing parallel to a water body imparts motion to the surface water by shear action.\nThis is a downward transfer of momentum from the air to the water that ultimately generates a drift current underneath.\nThe water surface also deforms under the action of wind and becomes wavy for increasing wind speeds, which modifies the grip that the wind has on the surface itself.\nThe mechanics of the interaction between wind and water becomes thus increasingly complex.\nThe Beaufort scale, for example, shows the correspondence between wind speed and sea states.\n\nThe magnitude of this shear force per unit contact area (\"τ\", shear stress) is estimated through wind-shear or wind-drag formulas.\nThese formulas parametrize the shear stress as a function of the wind speed at a certain height above the surface (\"U\") in the form\n\nwhere formula_2 is the density of the air; \"C\" is a dimensionless quantity wind-drag coefficient and is a repository function for all remaining dependencies.\nThe height at which the wind speed is referred to in wind-drag formulas is usually 10 meters above the water surface.\n\nThe expression of \"C\" contains, in first place, the correction to dependency on the second power of \"U\" for different ranges of \"U\".\nThe functional form of \"C\" is determined by an empirical formula that is determined from experiments in the laboratory and/or in the field.\nDifferent formulas have been established by various authors for different wind-speed ranges and taking into account the mechanics of the wind to varying degrees of detail.\n"}
{"id": "23284520", "url": "https://en.wikipedia.org/wiki?curid=23284520", "title": "Zeev Reiss", "text": "Zeev Reiss\n\nZeev Reiss, (April 2, 1917 - July 11, 1996) was an Israeli micropaleontologist and geologist, whose career included positions in government service and academia. He studied biology and medical sciences at the University of Cernăuţi, Bukovina, which was then part of Romania. He could not finish his studies at the university due to Nazi occupation and imprisonment. After World War II, he administered a health department for Holocaust survivors in Displaced Person camps under the American Forces in Munich.\n\nReiss immigrated to Israel in 1949. He took up studies toward a degree in Geology and Paleontology at Hebrew University. After receiving his Ph.D., Reiss was given the responsibility to establish a micropaleontology and stratigraphy laboratory in the Israel Geological Survey, where he became chief micropaleontologist and Director of the Paleontology Division.\n"}
