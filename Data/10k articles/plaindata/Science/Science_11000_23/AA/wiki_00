{"id": "12342942", "url": "https://en.wikipedia.org/wiki?curid=12342942", "title": "Anderson orthogonality theorem", "text": "Anderson orthogonality theorem\n\nThe Anderson orthogonality theorem is a theorem in physics by the physicist P. W. Anderson.\n\nIt relates to the introduction of a magnetic impurity in a metal. When a magnetic impurity is introduced into a metal, the conduction electrons will tend to screen the potential formula_1 that the impurity creates. The N-electron ground state for the system when formula_2, which corresponds to the absence of the impurity and formula_3, which corresponds to the introduction of the impurity are orthogonal in the thermodynamic limit formula_4.\n\n"}
{"id": "28913873", "url": "https://en.wikipedia.org/wiki?curid=28913873", "title": "Asimut Glacier", "text": "Asimut Glacier\n\nAsimutbreen Glacier () is a small, steep tributary glacier to Vangengeym Glacier, descending southeast and then northeast between Solhogdene Heights and Skuggekammen Ridge, in the eastern Gruber Mountains of the Wohlthat Mountains, Queen Maud Land. It was discovered and plotted from air photos by the Third German Antarctic Expedition, 1938–39, replotted from air photos and from surveys by the Sixth Norwegian Antarctic Expedition, 1956–60, and named Asimutbreen (the azimuth glacier).\n\n"}
{"id": "28914043", "url": "https://en.wikipedia.org/wiki?curid=28914043", "title": "Balish Glacier", "text": "Balish Glacier\n\nBalish Glacier () is a glacier, long, flowing north from the Soholt Peaks to enter Splettstoesser Glacier just northeast of Springer Peak, in the Heritage Range, Ellsworth Mountains. It was mapped by the United States Geological Survey from surveys and from U.S. Navy air photos, 1961–66, and was named by the Advisory Committee on Antarctic Names for Commander Daniel Balish, Executive Officer of U.S. Navy Squadron VX-6 during Operation Deep Freeze 1965, and Commanding Officer in 1967.\n\n"}
{"id": "40943939", "url": "https://en.wikipedia.org/wiki?curid=40943939", "title": "Bat SARS-like coronavirus WIV1", "text": "Bat SARS-like coronavirus WIV1\n\nBat SARS-like coronavirus WIV1, (Bat SL-CoV-WIV1) also sometimes called SARS-like coronavirus WIV1, is a newly identified CoV isolated from Chinese rufous horseshoe bats. The discovery confirms that bats are the natural reservoir of the SARS virus. Phylogenetic analysis shows the possibility of direct transmission of SARS from bats to humans without the intermediary Chinese civets, as previously believed. It is a single-stranded, enveloped, positive-sense RNA betacoronavirus.\n\n"}
{"id": "54097327", "url": "https://en.wikipedia.org/wiki?curid=54097327", "title": "Bay de Verde Formation", "text": "Bay de Verde Formation\n\nThe Bay de Verde Formation is a formation cropping out in Newfoundland.\n"}
{"id": "35019188", "url": "https://en.wikipedia.org/wiki?curid=35019188", "title": "Bethesda Statement on Open Access Publishing", "text": "Bethesda Statement on Open Access Publishing\n\nThe Bethesda Statement on Open Access Publishing is a 2003 statement which defines the concept of open access and then supports that concept.\n\nOn 11 April 2003, the Howard Hughes Medical Institute held a meeting for 24 people to discuss better access to scholarly literature. The group made a definition of an open access journal as one which grants a \"free, irrevocable, worldwide, perpetual right of access to, and a license to copy, use, distribute, transmit, and display the work publicly and to make and distribute derivative works, in any digital medium for any responsible purpose, subject to proper attribution of authorship\" and from which every article is \"deposited immediately upon initial publication in at least one online repository\".\n\nAlong with the Budapest Open Access Initiative (BOAI) and the Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities, the Bethesda Statement established \"open access\" as the term to describe initiatives to make information more widely and easily available.\n\nThe Bethesda Statement builds on the BOAI by saying how users will enact open access. Specifically, open access practitioners will put content online with a license granting rights for reuse including the right to make derivative works. The BOAI does not mention derivative works.\n\n"}
{"id": "1191936", "url": "https://en.wikipedia.org/wiki?curid=1191936", "title": "Bongard problem", "text": "Bongard problem\n\nA Bongard problem is a kind of puzzle invented by the Russian computer scientist Mikhail Moiseevich Bongard (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his 1967 book on pattern recognition. Bongard, in the introduction of the book (which deals with a number of topics including perceptrons) credits the ideas in it to a group including M. N. Vaintsvaig, V. V. Maksimov, and M. S. Smirnov.\n\nThe idea of a Bongard problem is to present two sets of relatively simple diagrams, say \"A\" and \"B\". All the diagrams from set \"A\" have a common factor or attribute, which is lacking in all the diagrams of set \"B\". The problem is to find, or to formulate, convincingly, the common factor. The problems were popularised by their occurrence in the 1979 book \"Gödel, Escher, Bach\" by Douglas Hofstadter, himself a composer of Bongard problems. Bongard problems are also at the heart of the game Zendo.\n\nMany computational architectures have been devised to solve Bongard problems, the most extensive of which being Phaeaco, by Harry Foundalis, who left the field in 2008 due to ethical concerns regarding machines that can pass as human.\n\n\n"}
{"id": "53466077", "url": "https://en.wikipedia.org/wiki?curid=53466077", "title": "Brain Electrical Oscillation Signature Profiling", "text": "Brain Electrical Oscillation Signature Profiling\n\nBrain Electrical Oscillation Signature Profiling (BEOSP or BEOS) is a technique by which a suspect's participation in a crime is detected by eliciting electrophysiological impulses.\n\nIt is a non-invasive, scientific technique with a great degree of sensitivity and a neuro-psychological method of interrogation which is sometimes also referred to as ‘brain fingerprinting’.\n\nThe methodology was developed by Champadi Raman Mukundan (C. R. Mukundan), a Neuroscientist, former Professor & Head of Clinical Psychology at the National Institute of Mental Health and Neurosciences (Bangalore, India), while he worked as a Research Consultant to TIFAC-DFS Project on ‘Normative Data for Brain Electrical Activation Profiling’.\n\nHis works are based on research that was also formerly pursued by other scientists at American universities including J. Peter Rosenfeld, Lawrence Farwell & Emanuel Donchin.\n\nThe human brain receives millions of arrays of signals in different modalities, all through the waking periods. These signals are classified and stored in terms of their relationship perceived as function of experience and available knowledge base of an individual, as well as new relationship produced through sequential processing. The process of encoding is primarily when the individual is directly participating in an activity or experiencing it.\n\nIt is considered secondary, when the information is obtained from a secondary source viz. books, conversations, hearsay etc. in which there is no primary experiential component and the brain deals mainly with conceptual aspects.\n\nPrimary encoding is deep seated and has specific source memory in terms of time and space of occurrence of experience, as individual himself/herself has shared or participated in the experience/act/event at certain time in his/her life at a certain place.\n\nIt is found that when the brain of an individual is activated by a piece of information of an event in which he/she has taken part, the brain of the individual will respond differently from that of a person who has received the same information from secondary sources (non-experiential).\n\nBEOSP is based on this principle, thereby intending to demonstrate that the suspect who have primary encoded information of those who have participated in the suspected events will show responses indicating firsthand (personally acquired) knowledge of the event.\n\n\nIdeally, no questions are to be asked while conducting the test; rather, the subject is simply provided with the probable events/scenarios in the aftermath of which, the results are analyzed to verify if the brain produces any experiential knowledge, which is essentially the recognition of events disclosed. This way, all fundamental rights are protected, as neither there are no questions that are being asked or any answers reciprocated.\n\nUniversity of Pennsylvania conducted a research along with the Brigham & Women's Hospital (Boston, Massachusetts), Children's Hospital Boston & the University Hospital of Freiburg, Germany which determined that Gamma Oscillations in the brain could help distinguish false memories from the real ones. Their analysis concluded that in the retrieval of truthful memories, as compared to false, human brain creates an extremely distinct pattern of gamma oscillations, indicating a recognition of context based information associated with a prior experience.\n\n\n"}
{"id": "4150226", "url": "https://en.wikipedia.org/wiki?curid=4150226", "title": "Centre de Sociologie de l'Innovation", "text": "Centre de Sociologie de l'Innovation\n\nThe Centre de Sociologie de l'Innovation (CSI; \"Center for the Sociology of Innovation\") is a research center at the Mines Paristech, France.\n\nThe CSI was created in 1967 and is known for its members' contributions to the field of science and technology studies and to actor–network theory. Prominent past and current members include academics such as Bruno Latour and Michel Callon.\n\n"}
{"id": "2290139", "url": "https://en.wikipedia.org/wiki?curid=2290139", "title": "Chen Quan", "text": "Chen Quan\n\nChen Quan () is a Chinese pilot selected as part of the Shenzhou program.\n\nChen was born in Suining, Sichuan, China.\n\nHe joined the People's Liberation Army Air Force and became a fighter interceptor pilot and later as a regiment commander in the PLAAF.\n\nChen was selected to be an astronaut in 1998 and served as commander of the backup crew for Shenzhou 7 which flew in September 2008.\n\n\n"}
{"id": "1235972", "url": "https://en.wikipedia.org/wiki?curid=1235972", "title": "Citation analysis", "text": "Citation analysis\n\nCitation analysis is the examination of the frequency, patterns, and graphs of citations in documents. It uses the pattern of citations, links from one document to another document, to reveal properties of the documents. A typical aim would be to identify the most important documents in a collection. A classic example is that of the citations between academic articles and books. The judgements produced by judges of law to support their decisions refer back to judgements made in earlier cases so citation analysis in a legal context is important. Another example is provided by patents which contain prior art, citation of earlier patents relevant to the current claim.\n\nDocuments can be associated with many other features in addition to citations, such as authors, publishers, journals as well as their actual texts. The general analysis of collections of documents is known as bibliometrics and citation analysis is a key part of that field. For example, bibliographic coupling and co-citation are association measures based on citation analysis (shared citations or shared references). The citations in a collection of documents can also be represented in forms such as a citation graph, as pointed out by Derek J. de Solla Price in his 1965 article \"Networks of Scientific Papers\". This means that citation analysis draws on aspects of social network analysis and network science.\n\nAn early example of automated citation indexing was CiteSeer, which was used for citations between academic papers, while Google Scholar is an example of a modern system which includes more than just academic books and articles reflecting a wider range of information sources. Today, automated citation indexing has changed the nature of citation analysis research, allowing millions of citations to be analyzed for large-scale patterns and knowledge discovery. Citation analysis tools can be used to compute various impact measures for scholars based on data from citation indices. These have various applications, from the identification of expert referees to review papers and grant proposals, to providing transparent data in support of academic merit review, tenure, and promotion decisions. This competition for limited resources may lead to ethical questionable behavior to increase citations.\n\nA great deal of criticism has been made of the practice of naively using citation analyses to compare the impact of different scholarly articles without taking into account other factors which may affect citation patterns. Among these criticisms, a recurrent one focuses on “field-dependent factors”, which refers to the fact that citation practices vary from one area of science to another, and even between fields of research within a discipline.\n\nWhile citation indexes were originally designed for information retrieval, they are increasingly used for bibliometrics and other studies involving research evaluation. Citation data is also the basis of the popular journal impact factor.\n\nThere is a large body of literature on citation analysis, sometimes called scientometrics, a term invented by Vasily Nalimov, or more specifically bibliometrics. The field blossomed with the advent of the Science Citation Index, which now covers source literature from 1900 on. The leading journals of the field are \"Scientometrics,\" \"Informetrics,\" and the \"Journal of the Association for Information Science and Technology\". ASIST also hosts an electronic mailing list called SIGMETRICS at ASIST. This method is undergoing a resurgence based on the wide dissemination of the Web of Science and Scopus subscription databases in many universities, and the universally available free citation tools such as CiteBase, CiteSeerX, Google Scholar, and the former Windows Live Academic (now available with extra features as Microsoft Academic). Methods of citation analysis research include qualitative, quantitative and computational approaches. The main foci of such scientometric studies have included productivity comparisons, institutional research rankings, journal rankings establishing faculty productivity and tenure standards, assessing the influence of top scholarly articles, tracing the development trajectory of a science or technology field, and developing profiles of top authors and institutions in terms of research performance.\n\nLegal citation analysis is a citation analysis technique for analyzing legal documents to facilitate the understanding of the inter-related regulatory compliance documents by the exploration the citations that connect provisions to other provisions within the same document or between different documents. Legal citation analysis uses a citation graph extracted from a regulatory document, which could supplement E-discovery - a process that leverages on technological innovations in big data analytics.\n\nIn a 1965 paper, Derek J. de Solla Price described the inherent linking characteristic of the SCI as \"Networks of Scientific Papers\". The links between citing and cited papers became dynamic when the SCI began to be published online. The Social Sciences Citation Index became one of the first databases to be mounted on the Dialog system in 1972. With the advent of the CD-ROM edition, linking became even easier and enabled the use of bibliographic coupling for finding related records. In 1973, Henry Small published his classic work on Co-Citation analysis which became a self-organizing classification system that led to document clustering experiments and eventually an \"Atlas of Science\" later called \"Research Reviews\".\n\nThe inherent topological and graphical nature of the worldwide citation network which is an inherent property of the scientific literature was described by Ralph Garner (Drexel University) in 1965.\n\nThe use of citation counts to rank journals was a technique used in the early part of the nineteenth century but the systematic ongoing measurement of these counts for scientific journals was initiated by Eugene Garfield at the Institute for Scientific Information who also pioneered the use of these counts to rank authors and papers. In a landmark paper of 1965 he and Irving Sher showed the correlation between citation frequency and eminence in demonstrating that Nobel Prize winners published five times the average number of papers while their work was cited 30 to 50 times the average. In a long series of essays on the Nobel and other prizes Garfield reported this phenomenon. The usual summary measure is known as impact factor, the number of citations to a journal for the previous two years, divided by the number of articles published in those years. It is widely used, both for appropriate and inappropriate purposes—in particular, the use of this measure alone for ranking authors and papers is therefore quite controversial.\n\nIn an early study in 1964 of the use of Citation Analysis in writing the history of DNA, Garfield and Sher demonstrated the potential for generating historiographs, topological maps of the most important steps in the history of scientific topics. This work was later automated by E. Garfield, A. I. Pudovkin of the Institute of Marine Biology, Russian Academy of Sciences and V. S. Istomin of Center for Teaching, Learning, and Technology, Washington State University and led to the creation of the HistCite software around 2002.\n\nAutomatic citation indexing was introduced in 1998 by Lee Giles, Steve Lawrence and Kurt Bollacker and enabled automatic algorithmic extraction and grouping of citations for any digital academic and scientific document. Where previous citation extraction was a manual process, citation measures could now scale up and be computed for any scholarly and scientific field and document venue, not just those selected by organizations such as ISI. This led to the creation of new systems for public and automated citation indexing, the first being CiteSeer (now CiteSeerX, soon followed by Cora, which focused primarily on the field of computer science and information science. These were later followed by large scale academic domain citation systems such as the Google Scholar and Microsoft Academic. Such autonomous citation indexing is not yet perfect in citation extraction or citation clustering with an error rate estimated by some at 10% though a careful statistical sampling has yet to be done. This has resulted in such authors as Ann Arbor, Milton Keynes, and Walton Hall being credited with extensive academic output. SCI claims to create automatic citation indexing through purely programmatic methods. Even the older records have a similar magnitude of error.\n\nCitation analysis for legal documents is an approach to facilitate the understanding and analysis of inter-related regulatory compliance documents by exploration of the citations that connect provisions to other provisions within the same document or between different documents. Citation analysis uses a citation graph extracted from a regulatory document, which could supplement E-discovery - a process that leverages on technological innovations in big data analytics.\n\nE-publishing. Due to the unprecedented growth of electronic resource (e-resource) availability, one of the questions currently being explored is, \"how often are e-resources being cited in my field?\" For instance, there are claims that on-line access to computer science literature leads to higher citation rates, however, humanities articles may suffer if not in print.\n\nSelf-citations. It has been criticized that authors game the system by accumulating citations by citing themselves excessively.<ref name=\"10.1007/s11192-017-2330-1\"></ref> For instance, it has been found that men tend to cite themselves more often than women.\n\n"}
{"id": "52918756", "url": "https://en.wikipedia.org/wiki?curid=52918756", "title": "Clark's Tree", "text": "Clark's Tree\n\nClark's Tree is a bronze memorial sculpture in Long Beach, Washington commemorating Lewis and Clark's journey across North America. It sits on a dune above the Pacific Ocean beach at Breakers near where Clark carved a message on a living tree to establish United States precedence of discovery and occupation in what was then the Oregon Country. The memorial was created by Stanley Wanlass, a sculptor educated at Brigham Young University. The sculpture marks the westernmost and northernmost point of Lewis and Clark's journey on the Pacific coast. \n\nThe sculpture was built in Clarkston, then barged down the Columbia River in 2003 with stops for public viewing in Richland, Hood River, Portland and Vancouver, then into the Pacific Ocean to reach Long Beach. In the process the sculpture was nearly lost at sea, according to Wanlass.\n\nAnother marker with the same name was constructed in 1932 at 3rd and Pacific in Long Beach's downtown area.\n\n"}
{"id": "24320602", "url": "https://en.wikipedia.org/wiki?curid=24320602", "title": "Colorpuncture", "text": "Colorpuncture\n\nColorpuncture, or color light acupuncture, is a pseudoscientific alternative medicine practice based on \"mystical or supernatural\" beliefs which asserts that colored lights can be used to stimulate acupuncture points to promote healing and better health. It is a form of chromotherapy or color therapy. There is no known anatomical or histological basis for the existence of acupuncture points or meridians. Scientific support for the efficacy of colorpuncture is lacking.\n\nColorpuncture was developed in the 1980s by German naturopath and acupuncturist Peter Mandel, who named it \"esogetic colorpuncture\". \"Esogetic\" is a term coined by Mandel to refer to the \"merger of esoteric wisdom of life with the energetic principles of life's processes\".\n\nMandel cited Fritz-Albert Popp, who claimed that the body's cells communicate with each other through a steady stream of photons. This is not a scientifically recognized method of cell communication. Using Kirlian photography, Mandel concluded that the acupuncture meridians absorb and disseminate colored light within the body.\n\nColorpuncture is based on the idea that illness and pain occur when an individual has strayed off his or her \"life path\". For example, a treatment might be intended to release an emotional blockage to heal a nervous system condition, allowing patients to devote themselves to their individual spiritual purpose. Mandel's model is a holographic representation of how vital energy is produced in the body. Three of the six factors (called \"molecules\") represent the subtle energies: the \"chakras\", the \"formative field\", and the \"converter model\". The other three factors describe the physical reality: the \"body systems\", the \"coordination system\", and the \"transmitter relays\".\n\nColorpuncture employs seven basic colors. In general, the warm colors - red, orange, and yellow - are believed to add energy, while the cool colors - green, blue, and violet - decrease energy. Mandel also claims that warm and cool colors, when used together, balance yin and yang energy flows.\n\nA small handheld instrument resembling a torch (flashlight) with a colored quartz rod is used. The tip is placed directly onto acupoints or held a short distance above. Unlike acupuncture, the skin is not broken. Colorpuncture sessions last 10 to 90 minutes. Colorpuncturists claim to diagnose through the use of Kirlian photography.\n\nJack Raso writing in the \"Skeptical Inquirer\" included colorpuncture in a list of \"mystical or supernaturalistic\" therapies. Doctor Harriet Hall points out there is no supporting research for colorpuncture and explains how color can be used for diagnosis rather than treatment. A review of research studies conducted in Europe to evaluate the efficacy of colorpuncture concluded that the approach lacked a research base to be considered anything but a pilot or preliminary research stage. Quackwatch lists it as a questionable treatment, and research on colorpuncture has failed to demonstrate a consistent effect.\n\n"}
{"id": "44158", "url": "https://en.wikipedia.org/wiki?curid=44158", "title": "Conservative force", "text": "Conservative force\n\nA conservative force is a force with the property that the total work done in moving a particle between two points is independent of the taken path. Equivalently, if a particle travels in a closed loop, the net work done (the sum of the force acting along the path multiplied by the displacement) by a conservative force is zero.\n\nA conservative force is dependent only on the position of the object. If a force is conservative, it is possible to assign a numerical value for the potential at any point. When an object moves from one location to another, the force changes the potential energy of the object by an amount that does not depend on the path taken. If the force is not conservative, then defining a scalar potential is not possible, because taking different paths would lead to conflicting potential differences between the start and end points.\n\nGravitational force is an example of a conservative force, while frictional force is an example of a non-conservative force.\n\nOther examples of conservative forces are: force in elastic spring, electrostatic force between two electric charges, and magnetic force between two magnetic poles. The last two forces are called central forces as they act along the line joining the centres of two charged/magnetized bodies. Thus, all central forces are conservative forces.\n\nInformally, a conservative force can be thought of as a force that \"conserves\" mechanical energy. Suppose a particle starts at point A, and there is a force \"F\" acting on it. Then the particle is moved around by other forces, and eventually ends up at A again. Though the particle may still be moving, at that instant when it passes point A again, it has traveled a closed path. If the net work done by \"F\" at this point is 0, then \"F\" passes the closed path test. Any force that passes the closed path test for all possible closed paths is classified as a conservative force.\n\nThe gravitational force, spring force, magnetic force (according to some definitions, see below) and electric force (at least in a time-independent magnetic field, see Faraday's law of induction for details) are examples of conservative forces, while friction and air drag are classical examples of non-conservative forces.\n\nFor non-conservative forces, the mechanical energy that is lost (not conserved) has to go somewhere else, by conservation of energy. Usually the energy is turned into heat, for example the heat generated by friction. In addition to heat, friction also often produces some sound energy. The water drag on a moving boat converts the boat's mechanical energy into not only heat and sound energy, but also wave energy at the edges of its wake. These and other energy losses are irreversible because of the second law of thermodynamics.\n\nA direct consequence of the closed path test is that the work done by a conservative force on a particle moving between any two points does not depend on the path taken by the particle.\n\nThis is illustrated in the figure to the right: The work done by the gravitational force on an object depends only on its change in height because the gravitational force is conservative. The work done by a conservative force is equal to the negative of change in potential energy during that process. For a proof, imagine two paths 1 and 2, both going from point A to point B. The variation of energy for the particle, taking path 1 from A to B and then path 2 backwards from B to A, is 0; thus, the work is the same in path 1 and 2, i.e., the work is independent of the path followed, as long as it goes from A to B.\n\nFor example, if a child slides up a frictionless slide, the work done by the gravitational force on the child from the down of the slide to the up will be the same no matter what the shape of the slide; it can be straight or it can be a spiral or conical. The amount of work done only depends on the vertical displacement of the child.\n\nA force field \"F\", defined everywhere in space (or within a simply-connected volume of space), is called a \"conservative force\" or \"conservative vector field\" if it meets any of these three \"equivalent\" conditions:\n\nThe term \"conservative force\" comes from the fact that when a conservative force exists, it conserves mechanical energy. The most familiar conservative forces are gravity, the electric force (in a time-independent magnetic field, see Faraday's law), and spring force.\n\nMany forces (particularly those that depend on velocity) are not force \"fields\". In these cases, the above three conditions are not mathematically equivalent. For example, the magnetic force satisfies condition 2 (since the work done by a magnetic field on a charged particle is always zero), but does not satisfy condition 3, and condition 1 is not even defined (the force is not a vector field, so one cannot evaluate its curl). Accordingly, some authors classify the magnetic force as conservative, while others do not. The magnetic force is an unusual case; most velocity-dependent forces, such as friction, do not satisfy any of the three conditions, and therefore are unambiguously nonconservative.\n\nNon-conservative forces can arise in classical physics due to neglected degrees of freedom or from time-dependent potentials. For instance, friction may be treated without resorting to the use of nonconservative forces by considering the motion of individual molecules; however that means every molecule's motion must be considered rather than handling it through statistical methods. For macroscopic systems the nonconservative approximation is far easier to deal with than millions of degrees of freedom. Examples of nonconservative forces are friction and non-elastic material stress.\n\nHowever, general relativity is non-conservative, as seen in the anomalous precession of Mercury's orbit. However, general relativity can be shown to conserve a stress–energy–momentum pseudotensor.\n\n"}
{"id": "24483792", "url": "https://en.wikipedia.org/wiki?curid=24483792", "title": "Current (fluid)", "text": "Current (fluid)\n\nA current in a fluid is the magnitude and direction of flow within that fluid. An air current presents the same properties specifically for a gaseous medium.\n\nTypes of fluid currents include\n\n"}
{"id": "19107240", "url": "https://en.wikipedia.org/wiki?curid=19107240", "title": "Cyprinid herpesvirus 3", "text": "Cyprinid herpesvirus 3\n\nCyprinid herpesvirus 3 (also CyHV-3, koi herpes virus or KHV) is a species of virus causing a viral disease that is very contagious to the common carp \"Cyprinus carpio\". It is most commonly found in ornamental koi, which are often used in outdoor ponds or as feeder stock. The first case of KHV was reported in 1998, but not confirmed until later in 1999.\n\nKHV is a DNA-based virus. After discovery, it was identified as a strain of herpesvirus. Like other strains, KHV stays with the infected fish for the duration of their lives, making the recovered and exposed fish potential carriers of the virus. Koi fish infected with KHV may die within the first 24–48 hours of exposure. The virus is found in 33 countries.\n\nKHV is listed as a nonexotic disease of the EU, so is watched closely by the European Community Reference Laboratory for Fish Diseases.\n\nSymptoms of KHV include:\n\nChanges in the specimen's behaviour may also indicate the presence of KHV. Behavioural symptoms may include disorientation, hyperactivity and potentially isolation, in which the specimen detaches themselves from the shoal.\n\nIn 2016 the Australian Government announced plans to release the virus into the Murray-Darling basin in an attempt to control carp populations in the water system.\n\n"}
{"id": "5409095", "url": "https://en.wikipedia.org/wiki?curid=5409095", "title": "Dead-beat control", "text": "Dead-beat control\n\nIn discrete-time control theory, the dead-beat control problem consists of finding what input signal must be applied to a system in order to bring the output to the steady state in the smallest number of time steps.\n\nFor an \"N\"th-order linear system it can be shown that this minimum number of steps will be at most \"N\" (depending on the initial condition), provided that the system is null controllable (that it can be brought to state zero by \"some\" input). The solution is to apply feedback such that all poles of the closed-loop transfer function are at the origin of the \"z\"-plane. (For more information about transfer functions and the \"z\"-plane see z-transform). Therefore the linear case is easy to solve. By extension, a closed loop transfer function which has all poles of the transfer function at the origin is sometimes called a dead beat transfer function.\n\nFor nonlinear systems, dead beat control is an open research problem. (See Nesic reference below).\n\nDead beat controllers are often used in process control due to their good dynamic properties. They are a classical feedback controller where the control gains are set using a table based on the plant system order and normalized natural frequency.\n\nThe deadbeat response has the following characteristics:\n\n"}
{"id": "47231050", "url": "https://en.wikipedia.org/wiki?curid=47231050", "title": "Detunatele", "text": "Detunatele\n\nDetunatele is a site of columnar jointing in Transylvania, Alba County, Romania. \"Detunatele\" means lightning strike. The columns are hexagonal shaped basalt and 1,258 meters tall on two peaks: Detunata Goala and Detunata Flocoasa (Barren Detunata and Shaggy Detunata). They are located in the Metaliferi Mountains and are a tourist attraction. They are 1.5 km apart. They formations are shaped like humps and there are stories and legends associated with them.\n\n"}
{"id": "286106", "url": "https://en.wikipedia.org/wiki?curid=286106", "title": "Force field (fiction)", "text": "Force field (fiction)\n\nIn speculative fiction, a force field, sometimes known as an energy shield, force shield, defence shield or deflector shield, is a barrier made of energy, plasma, or particles. It protects a person, area, or object from attacks or intrusions. This fictional technology is created as a field of energy without mass that acts as a wall, so that objects affected by the particular force relating to the field are unable to pass through the field and reach the other side. This concept has become a staple of many science-fiction works, so much that authors frequently do not even bother to explain or justify them to their readers treating them almost as established fact and attributing whatever capabilities the plot requires.\n\nThere is ongoing scientific research into real force fields, primarily to protect against radiation.\n\nThe concept of a force field goes back at least as far as the 1920s, in the works of E.E. 'Doc' Smith and others; in William Hope Hodgson's \"The Night Land\" (1912) the Last Redoubt, the remnants of an humanity shelter is generated by something very like a force field.\n\nIn Isaac Asimov's \"Foundation\" universe, personal shields have been developed by scientists specializing in the miniaturization of planet-based shields. As they are primarily used by Foundation Traders, most other inhabitants of the Galactic Empire do not know about this technology. In an unrelated short story \"Breeds There a Man...?\" by Asimov, scientists are working on a force field (\"energy so channelled as to create a wall of matter-less inertia\"), capable of protecting the population in case of a nuclear war. The force field demonstrated in the end is a solid hemisphere, apparently completely opaque and reflective from both sides.\n\nThe concept of force fields as a defensive measure from enemy attack or as a form of attack can be regularly found in modern video games as well as in movies, such as in \"The War of the Worlds\" (1953, George Pál) and \"Independence Day\".\n\nThe ability to create a force field has been a common superpower in comic books and associated media. While only a few characters have the explicit ability to create force fields (for example, the Invisible Woman of the Fantastic Four and Violet Parr from \"The Incredibles\"), it has been emulated via other powers, such as Green Lantern's energy constructs, Jean Grey's telekinesis, and Magneto's manipulation of electromagnetic fields. Apart from this, its importance is also highlighted in Dr. Michio Kaku's books (such as \"Physics of the Impossible\").\n\nScience fiction and fantasy avenues suggest a number of potential uses of force fields\n\n\nThe capabilities and functionality of force fields vary; in some works of fiction (such as in the \"Star Trek\" universe), energy shields can nullify or mitigate the effects of both energy and particle (e.g., phasers) and conventional weapons, as well as supernatural forces. In many fictional scenarios, the shields function primarily as a defensive measure against weapons fired from other spacecraft. Force fields in these stories also generally prevent transporting. There are generally two kinds of force fields postulated: one in which energy is projected as a flat plane from emitters around the edges of a spacecraft and another where energy surrounds a ship like a bubble.\n\nA University of Washington group in Seattle has been experimenting with using a bubble of charged plasma, contained by a fine mesh of superconducting wire, to surround a spacecraft. This would protect the spacecraft from interstellar radiation and some particles without needing physical shielding.\n\nLikewise, Rutherford Appleton Laboratory is attempting to design an actual test satellite, which would orbit Earth with a charged plasma field around it.\n\nIn 2008, \"Cosmos Magazine\" reported on research into creating an artificial replica of Earth’s magnetic field around a spacecraft to protect astronauts from dangerous cosmic rays. British and Portuguese scientists used a mathematical simulation to prove that it would be possible to create a \"mini-magnetosphere\" bubble several hundred meters wide, possibly generated by a small unmanned vessel that could accompany a future manned mission to Mars.\n\nIn 2015, Boeing was granted a patent on a force field designed to protect against shock waves generated by explosions. It is not intended to protect against projectiles, radiation, or energy weapons such as lasers. The field purportedly works by creating a field of (ionised) superheated air-plasma which disrupts, or at least attenuates, the shock wave. As of March 2016, no working models are known to have been demonstrated.\n\n\n"}
{"id": "4281425", "url": "https://en.wikipedia.org/wiki?curid=4281425", "title": "Gaze heuristic", "text": "Gaze heuristic\n\nThe gaze heuristic is a heuristic used in directing correct motion to achieve a goal using one main variable. An example of the gaze heuristic is catching a ball. The gaze heuristic is one example of psychologist Gerd Gigerenzer's one good reason heuristic, where humans and animals are able to process large amounts of information quickly and react, regardless of whether the information is consciously processed.\n\nThe gaze heuristic is a critical element in animal behavior, being used in predation heavily. At the most basic level, the gaze heuristic ignores all casual relevant variables to make quick gut reactions. \n\nA catcher using the gaze heuristic observes the initial angle of the ball and runs towards it in such a way as to keep this angle constant. Experimental studies have shown that if people ignore the fact they were solving a system of differential equations to catch the ball, and simply follow the gaze heuristic they will consistently arrive in the exact spot the ball is predicted to hit the ground. The gaze heuristic does not require knowledge of any of the variables required by the optimizing approach, nor does it require the catcher to integrate information, yet it allows the catcher to successfully catch the ball. The gaze heuristic may therefore be described at ecologically rational at least in the simple case of catching a ball in the air. A criticism is that application of the gaze heuristic is limited by simple situations far from real complexity of environment.\n"}
{"id": "16182302", "url": "https://en.wikipedia.org/wiki?curid=16182302", "title": "Henry Hurd Swinnerton", "text": "Henry Hurd Swinnerton\n\nHenry Hurd Swinnerton (1875–1966) was a British geologist (Not to be confused with American geologist A.C. Swinnerton). He was professor of geology at University College Nottingham from 1910 to 1946.\n\nSwinnerton was educated at the Royal College of Science, and earned a doctorate in zoology (D.Sc) from the University of London in July 1902.\n\nIn the 1930s Swinnerton was a member of the Fenland Research Committee, contributing valuable knowledge of the geomorphology of the Lincolnshire coast. In 1942 he was awarded the Murchison Medal of the Geological Society of London.\n\n\n"}
{"id": "98770", "url": "https://en.wikipedia.org/wiki?curid=98770", "title": "Hidden Markov model", "text": "Hidden Markov model\n\nHidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. \"hidden\") states.\n\nThe hidden Markov model can be represented as the simplest dynamic Bayesian network. The mathematics behind the HMM were developed by L. E. Baum and coworkers.\n\nIn simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, the state is not directly visible, but the output (in the form of data or \"token\" in the following), dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states; this is also known as pattern theory, a topic of grammar induction.\n\nThe adjective \"hidden\" refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a hidden Markov model even if these parameters are known exactly.\n\nHidden Markov models are especially known for their application in reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.\n\nA hidden Markov model can be considered a generalization of a mixture model where the hidden variables (or latent variables), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. Recently, hidden Markov models have been generalized to pairwise Markov models and triplet Markov models which allow consideration of more complex data structures and the modeling of nonstationary data.\n\nIn its discrete form, a hidden Markov process can be visualized as a generalization of the Urn problem with replacement (where each item from the urn is returned to the original urn before the next step). Consider this example: in a room that is not visible to an observer there is a genie. The room contains urns X1, X2, X3, … each of which contains a known mix of balls, each ball labeled y1, y2, y3, … . The genie chooses an urn in that room and randomly draws a ball from that urn. It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. The genie has some procedure to choose urns; the choice of the urn for the \"n\"-th ball depends only upon a random number and the choice of the urn for the (\"n\" − 1)-th ball. The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a Markov process. It can be described by the upper part of Figure 1.\n\nThe Markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a \"hidden Markov process\". This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state. Even if the observer knows the composition of the urns and has just observed a sequence of three balls, \"e.g.\" y1, y2 and y3 on the conveyor belt, the observer still cannot be \"sure\" which urn (\"i.e.\", at which state) the genie has drawn the third ball from. However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.\n\nThe diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt any of a number of values. The random variable \"x\"(\"t\") is the hidden state at time (with the model from the above diagram, \"x\"(\"t\") ∈ { \"x\", \"x\", \"x\" }). The random variable \"y\"(\"t\") is the observation at time (with \"y\"(\"t\") ∈ { \"y\", \"y\", \"y\", \"y\" }). The arrows in the diagram (often called a trellis diagram) denote conditional dependencies.\n\nFrom the diagram, it is clear that the conditional probability distribution of the hidden variable \"x\"(\"t\") at time , given the values of the hidden variable at all times, depends \"only\" on the value of the hidden variable \"x\"(\"t\" − 1); the values at time \"t\" − 2 and before have no influence. This is called the Markov property. Similarly, the value of the observed variable \"y\"(\"t\") only depends on the value of the hidden variable \"x\"(\"t\") (both at time ).\n\nIn the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). The parameters of a hidden Markov model are of two types, \"transition probabilities\" and \"emission probabilities\" (also known as \"output probabilities\"). The transition probabilities control the way the hidden state at time is chosen given the hidden state at time formula_1.\n\nThe hidden state space is assumed to consist of one of possible values, modeled as a categorical distribution. (See the section below on extensions for other possibilities.) This means that for each of the possible states that a hidden variable at time can be in, there is a transition probability from this state to each of the possible states of the hidden variable at time formula_2, for a total of formula_3 transition probabilities. Note that the set of transition probabilities for transitions from any given state must sum to 1. Thus, the formula_4 matrix of transition probabilities is a Markov matrix. Because any one transition probability can be determined once the others are known, there are a total of formula_5 transition parameters.\n\nIn addition, for each of the possible states, there is a set of emission probabilities governing the distribution of the observed variable at a particular time given the state of the hidden variable at that time. The size of this set depends on the nature of the observed variable. For example, if the observed variable is discrete with possible values, governed by a categorical distribution, there will be formula_6 separate parameters, for a total of formula_7 emission parameters over all hidden states. On the other hand, if the observed variable is an -dimensional vector distributed according to an arbitrary multivariate Gaussian distribution, there will be parameters controlling the means and formula_8 parameters controlling the covariance matrix, for a total of formula_9 emission parameters. (In such a case, unless the value of is small, it may be more practical to restrict the nature of the covariances between individual elements of the observation vector, e.g. by assuming that the elements are independent of each other, or less restrictively, are independent of all but a fixed number of adjacent elements.)\n\nSeveral inference problems are associated with hidden Markov models, as outlined below.\n\nThe task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence. This requires summation over all possible state sequences:\n\nThe probability of observing a sequence\nof length \"L\" is given by\nwhere the sum runs over all possible hidden-node sequences\n\nApplying the principle of dynamic programming, this problem, too, can be handled efficiently using the forward algorithm.\n\nA number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations formula_13\n\nThe task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e. to compute formula_14. This task is normally used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points of time, with corresponding observations at each point in time. Then, it is natural to ask about the state of the process at the end.\n\nThis problem can be handled efficiently using the forward algorithm.\n\nThis is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e. to compute formula_15 for some formula_16. From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time \"k\" in the past, relative to time \"t\".\n\nThe forward-backward algorithm is an efficient method for computing the smoothed values for all hidden state variables.\n\nThe task, unlike the previous two, asks about the joint probability of the \"entire\" sequence of hidden states that generated a particular sequence of observations (see illustration on the right). This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable. An example is part-of-speech tagging, where the hidden states represent the underlying parts of speech corresponding to an observed sequence of words. In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.\n\nThis task requires finding a maximum over all possible state sequences, and can be solved efficiently by the Viterbi algorithm.\n\nFor some of the above problems, it may also be interesting to ask about statistical significance. What is the probability that a sequence drawn from some null distribution will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence? When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the false positive rate associated with failing to reject the hypothesis for the output sequence.\n\n\"A similar example is further elaborated in the Viterbi algorithm page.\"\n\nThe parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum–Welch algorithm or the Baldi–Chauvin algorithm. The Baum–Welch algorithm is a special case of the expectation-maximization algorithm. If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability. Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g. Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.\n\nA basic hidden Markov model can be described as follows:\n\nNote that, in the above model (and also the one below), the prior distribution of the initial state formula_17 is not specified. Typical learning models correspond to assuming a discrete uniform distribution over possible states (i.e. no particular prior distribution is assumed).\n\nIn a Bayesian setting, all parameters are associated with random variables, as follows:\n\nThese characterizations use formula_18 and formula_19 to describe arbitrary distributions over observations and parameters, respectively. Typically formula_19 will be the conjugate prior of formula_18. The two most common choices of formula_18 are Gaussian and categorical; see below.\n\nAs mentioned above, the distribution of each observation in a hidden Markov model is a mixture density, with the states of the corresponding to mixture components. It is useful to compare the above characterizations for an HMM with the corresponding characterizations, of a mixture model, using the same notation.\n\nA non-Bayesian mixture model:\n\nA Bayesian mixture model:\n\nThe following mathematical descriptions are fully written out and explained, for ease of implementation.\n\nA typical non-Bayesian HMM with Gaussian observations looks like this:\n\nA typical Bayesian HMM with Gaussian observations looks like this:\n\nA typical non-Bayesian HMM with categorical observations looks like this:\n\nA typical Bayesian HMM with categorical observations looks like this:\n\nNote that in the above Bayesian characterizations, formula_23 (a concentration parameter) controls the density of the transition matrix. That is, with a high value of formula_23 (significantly above 1), the probabilities controlling the transition out of a particular state will all be similar, meaning there will be a significant probability of transitioning to any of the other states. In other words, the path followed by the Markov chain of hidden states will be highly random. With a low value of formula_23 (significantly below 1), only a small number of the possible transitions out of a given state will have significant probability, meaning that the path followed by the hidden states will be somewhat predictable.\n\nAn alternative for the above two Bayesian examples would be to add another level of prior parameters for the transition matrix. That is, replace the lines\n\nwith the following:\n\nWhat this means is the following:\n\nImagine that the value of formula_23 is significantly above 1. Then the different formula_32 vectors will be dense, i.e. the probability mass will be spread out fairly evenly over all states. However, to the extent that this mass is unevenly spread, formula_26 controls which states are likely to get more mass than others.\n\nNow, imagine instead that formula_23 is significantly below 1. This will make the formula_32 vectors sparse, i.e. almost all the probability mass is distributed over a small number of states, and for the rest, a transition to that state will be very unlikely. Notice that there are different formula_32 vectors for each starting state, and so even if all the vectors are sparse, different vectors may distribute the mass to different ending states. However, for all of the vectors, formula_26 controls which ending states are likely to get mass assigned to them. For example, if formula_23 is 0.1, then each formula_32 will be sparse and, for any given starting state \"i\", the set of states formula_40 to which transitions are likely to occur will be very small, typically having only one or two members. Now, if the probabilities in formula_26 are all the same (or equivalently, one of the above models without formula_26 is used), then for different \"i\", there will be different states in the corresponding formula_40, so that all states are equally likely to occur in any given formula_40. On the other hand, if the values in formula_26 are unbalanced, so that one state has a much higher probability than others, almost all formula_40 will contain this state; hence, regardless of the starting state, transitions will nearly always occur to this given state.\n\nHence, a two-level model such as just described allows independent control over (1) the overall density of the transition matrix, and (2) the density of states to which transitions are likely (i.e. the density of the prior distribution of states in any particular hidden variable formula_47). In both cases this is done while still assuming ignorance over which particular states are more likely than others. If it is desired to inject this information into the model, the probability vector formula_26 can be directly specified; or, if there is less certainty about these relative probabilities, a non-symmetric Dirichlet distribution can be used as the prior distribution over formula_26. That is, instead of using a symmetric Dirichlet distribution with the single parameter formula_27 (or equivalently, a general Dirichlet with a vector all of whose values are equal to formula_27), use a general Dirichlet with values that are variously greater or less than formula_27, according to which state is more or less preferred.\n\n\"Poisson hidden Markov models (PHMM)\" are special cases of hidden Markov models where a Poisson process has a rate which varies in association with changes between the different states of a Markov model. PHMMs are not necessarily Markovian processes themselves because the underlying Markov chain or Markov process cannot be observed and only the Poisson signal is observed.\n\nHMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are). Applications include:\n\nThe forward and backward recursions used in HMM as well as computations of marginal smoothing probabilities were first described by Ruslan L. Stratonovich in 1960 (pages 160—162) and in the late 1950s in his papers in Russian.\nThe Hidden Markov Models were later described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s. One of the first applications of HMMs was speech recognition, starting in the mid-1970s.\n\nIn the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences, in particular DNA. Since then, they have become ubiquitous in the field of bioinformatics.\n\nHidden Markov models can model complex Markov processes where the states emit the observations according to some probability distribution. One such example is the Gaussian distribution; in such a Hidden Markov Model the states output are represented by a Gaussian distribution.\n\nMoreover, it could represent even more complex behavior when the output of the states is represented as mixture of two or more Gaussians, in which case the probability of generating an observation is the product of the probability of first selecting one of the Gaussians and the probability of generating that observation from that Gaussian. In cases of modeled data exhibiting artifacts such as outliers and skewness, one may resort to finite mixtures of heavier-tailed elliptical distributions, such as the multivariate Student's-t distribution, or appropriate non-elliptical distributions, such as the multivariate Normal Inverse-Gaussian.\n\nIn the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). Hidden Markov models can also be generalized to allow continuous state spaces. Examples of such models are those where the Markov process over hidden variables is a linear dynamical system, with a linear relationship among related variables and where all hidden and observed variables follow a Gaussian distribution. In simple cases, such as the linear dynamical system just mentioned, exact inference is tractable (in this case, using the Kalman filter); however, in general, exact inference in HMMs with continuous latent variables is infeasible, and approximate methods must be used, such as the extended Kalman filter or the particle filter.\n\nHidden Markov models are generative models, in which the joint distribution of observations and hidden states, or equivalently both the prior distribution of hidden states (the \"transition probabilities\") and conditional distribution of observations given states (the \"emission probabilities\"), is modeled. The above algorithms implicitly assume a uniform prior distribution over the transition probabilities. However, it is also possible to create hidden Markov models with other types of prior distributions. An obvious candidate, given the categorical distribution of the transition probabilities, is the Dirichlet distribution, which is the conjugate prior distribution of the categorical distribution. Typically, a symmetric Dirichlet distribution is chosen, reflecting ignorance about which states are inherently more likely than others. The single parameter of this distribution (termed the \"concentration parameter\") controls the relative density or sparseness of the resulting transition matrix. A choice of 1 yields a uniform distribution. Values greater than 1 produce a dense matrix, in which the transition probabilities between pairs of states are likely to be nearly equal. Values less than 1 result in a sparse matrix in which, for each given source state, only a small number of destination states have non-negligible transition probabilities. It is also possible to use a two-level prior Dirichlet distribution, in which one Dirichlet distribution (the upper distribution) governs the parameters of another Dirichlet distribution (the lower distribution), which in turn governs the transition probabilities. The upper distribution governs the overall distribution of states, determining how likely each state is to occur; its concentration parameter determines the density or sparseness of states. Such a two-level prior distribution, where both concentration parameters are set to produce sparse distributions, might be useful for example in unsupervised part-of-speech tagging, where some parts of speech occur much more commonly than others; learning algorithms that assume a uniform prior distribution generally perform poorly on this task. The parameters of models of this sort, with non-uniform prior distributions, can be learned using Gibbs sampling or extended versions of the expectation-maximization algorithm.\n\nAn extension of the previously described hidden Markov models with Dirichlet priors uses a Dirichlet process in place of a Dirichlet distribution. This type of model allows for an unknown and potentially infinite number of states. It is common to use a two-level Dirichlet process, similar to the previously described model with two levels of Dirichlet distributions. Such a model is called a \"hierarchical Dirichlet process hidden Markov model\", or \"HDP-HMM\" for short. It was originally described under the name \"Infinite Hidden Markov Model\" and was further formalized in.\n\nA different type of extension uses a discriminative model in place of the generative model of standard HMMs. This type of model directly models the conditional distribution of the hidden states given the observations, rather than modeling the joint distribution. An example of this model is the so-called \"maximum entropy Markov model\" (MEMM), which models the conditional distribution of the states using logistic regression (also known as a \"maximum entropy model\"). The advantage of this type of model is that arbitrary features (i.e. functions) of the observations can be modeled, allowing domain-specific knowledge of the problem at hand to be injected into the model. Models of this sort are not limited to modeling direct dependencies between a hidden state and its associated observation; rather, features of nearby observations, of combinations of the associated observation and nearby observations, or in fact of arbitrary observations at any distance from a given hidden state can be included in the process used to determine the value of a hidden state. Furthermore, there is no need for these features to be statistically independent of each other, as would be the case if such features were used in a generative model. Finally, arbitrary features over pairs of adjacent hidden states can be used rather than simple transition probabilities. The disadvantages of such models are: (1) The types of prior distributions that can be placed on hidden states are severely limited; (2) It is not possible to predict the probability of seeing an arbitrary observation. This second limitation is often not an issue in practice, since many common usages of HMM's do not require such predictive probabilities.\n\nA variant of the previously described discriminative model is the linear-chain conditional random field. This uses an undirected graphical model (aka Markov random field) rather than the directed graphical models of MEMM's and similar models. The advantage of this type of model is that it does not suffer from the so-called \"label bias\" problem of MEMM's, and thus may make more accurate predictions. The disadvantage is that training can be slower than for MEMM's.\n\nYet another variant is the \"factorial hidden Markov model\", which allows for a single observation to be conditioned on the corresponding hidden variables of a set of formula_53 independent Markov chains, rather than a single Markov chain. It is equivalent to a single HMM, with formula_54 states (assuming there are formula_55 states for each chain), and therefore, learning in such a model is difficult: for a sequence of length formula_56, a straightforward Viterbi algorithm has complexity formula_57. To find an exact solution, a junction tree algorithm could be used, but it results in an formula_58 complexity. In practice, approximate techniques, such as variational approaches, could be used.\n\nAll of the above models can be extended to allow for more distant dependencies among hidden states, e.g. allowing for a given state to be dependent on the previous two or three states rather than a single previous state; i.e. the transition probabilities are extended to encompass sets of three or four adjacent states (or in general formula_53 adjacent states). The disadvantage of such models is that dynamic-programming algorithms for training them have an formula_60 running time, for formula_53 adjacent states and formula_56 total observations (i.e. a length-formula_56 Markov chain).\n\nAnother recent extension is the \"triplet Markov model\", in which an auxiliary underlying process is added to model some data specificities. Many variants of this model have been proposed. One should also mention the interesting link that has been established between the \"theory of evidence\" and the \"triplet Markov models\" and which allows to fuse data in Markovian context and to model nonstationary data. Note that alternative multi-stream data fusion strategies have also been proposed in the recent literature, e.g.\n\nFinally, a different rationale towards addressing the problem of modeling nonstationary data by means of hidden Markov models was suggested in 2012. It consists in employing a small recurrent neural network (RNN), specifically a reservoir network, to capture the evolution of the temporal dynamics in the observed data. This information, encoded in the form of a high-dimensional vector, is used as a conditioning variable of the HMM state transition probabilities. Under such a setup, we eventually obtain a nonstationary HMM the transition probabilities of which evolve over time in a manner that is inferred from the data itself, as opposed to some unrealistic ad-hoc model of temporal evolution.\n\nThe model suitable in the context of longitudinal data is named latent Markov model. The basic version of this model has been extended to include individual covariates, random effects and to model more complex data structures such as multilevel data. A complete overview of the latent Markov models, with special attention to the model assumptions and to their practical use is provided in\n\n\n"}
{"id": "35843160", "url": "https://en.wikipedia.org/wiki?curid=35843160", "title": "Index of biophysics articles", "text": "Index of biophysics articles\n\nThis is a list of articles on biophysics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "493259", "url": "https://en.wikipedia.org/wiki?curid=493259", "title": "Index of logarithm articles", "text": "Index of logarithm articles\n\nThis is a list of logarithm topics, by Wikipedia page. See also the list of exponential topics.\n"}
{"id": "1075046", "url": "https://en.wikipedia.org/wiki?curid=1075046", "title": "Interactome", "text": "Interactome\n\nIn molecular biology, an interactome is the whole set of molecular interactions in a particular cell. The term specifically refers to physical interactions among molecules (such as those among proteins, also known as protein–protein interactions, PPIs) but can also describe sets of indirect interactions among genes (genetic interactions). The interactomes based on PPIs should be associated to the proteome of the corresponding species in order to provide a global view (\"omic\") of all the possible molecular interactions that a protein can present.\nThe word \"interactome\" was originally coined in 1999 by a group of French scientists headed by Bernard Jacq. Mathematically, interactomes are generally displayed as graphs. Though interactomes may be described as biological networks, they should not be confused with other networks such as neural networks or food webs.\n\nMolecular interactions can occur between molecules belonging to different biochemical families (proteins, nucleic acids, lipids, carbohydrates, etc.) and also within a given family. Whenever such molecules are connected by physical interactions, they form molecular interaction networks that are generally classified by the nature of the compounds involved. Most commonly, \"interactome\" refers to \"protein–protein interaction\" (PPI) network (PIN) or subsets thereof. For instance, the Sirt-1 protein interactome and Sirt family second order interactome is the network involving Sirt-1 and its directly interacting proteins where as second order interactome illustrates interactions up to second order of neighbors (Neighbors of neighbors). Another extensively studied type of interactome is the protein–DNA interactome, also called a \"gene-regulatory network\", a network formed by transcription factors, chromatin regulatory proteins, and their target genes. Even \"metabolic networks\" can be considered as molecular interaction networks: metabolites, i.e. chemical compounds in a cell, are converted into each other by enzymes, which have to bind their substrates physically.\n\nIn fact, all interactome types are interconnected. For instance, protein interactomes contain many enzymes which in turn form biochemical networks. Similarly, gene regulatory networks overlap substantially with protein interaction networks and signaling networks.\n\nIt has been suggested that the size of an organism's interactome correlates better than genome size with the biological complexity of the organism. Although protein–protein interaction maps containing several thousand binary interactions are now available for several species, none of them is presently complete and the size of interactomes is still a matter of debate.\n\nThe yeast interactome, i.e. all protein–protein interactions among proteins of \"Saccharomyces cerevisiae\", has been estimated to contain between 10,000 and 30,000 interactions. A reasonable estimate may be on the order of 20,000 interactions. Larger estimates often include indirect or predicted interactions, often from affinity purification/mass spectrometry (AP/MS) studies.\n\nGenes interact in the sense that they affect each other's function. For instance, a mutation may be harmless, but when it is combined with another mutation, the combination may turn out to be lethal. Such genes are said to \"interact genetically\". Genes that are connected in such a way form \"genetic interaction networks\". Some of the goals of these networks are: develop a functional map of a cell's processes, drug target identification, and to predict the function of uncharacterized genes.\n\nIn 2010, the most \"complete\" gene interactome produced to date was compiled from about 5.4 million two-gene comparisons to describe \"the interaction profiles for ~75% of all genes in the budding yeast\", with ~170,000 gene interactions. The genes were grouped based on similar function so as to build a functional map of the cell's processes. Using this method the study was able to predict known gene functions better than any other genome-scale data set as well as adding functional information for genes that hadn't been previously described. From this model genetic interactions can be observed at multiple scales which will assist in the study of concepts such as gene conservation. Some of the observations made from this study are that there were twice as many negative as positive interactions, negative interactions were more informative than positive interactions, and genes with more connections were more likely to result in lethality when disrupted.\n\nInteractomics is a discipline at the intersection of bioinformatics and biology that deals with studying both the interactions and the consequences of those interactions between and among proteins, and other molecules within a cell. Interactomics thus aims to compare such networks of interactions (i.e., interactomes) between and within species in order to find how the traits of such networks are either preserved or varied.\n\nInteractomics is an example of \"top-down\" systems biology, which takes an overhead, as well as overall, view of a biosystem or organism. Large sets of genome-wide and proteomic data are collected, and correlations between different molecules are inferred. From the data new hypotheses are formulated about feedbacks between these molecules. These hypotheses can then be tested by new experiments.\n\nThe study of interactomes is called interactomics. The basic unit of a protein network is the protein–protein interaction (PPI). While there are numerous methods to study PPIs, there are relatively few that have been used on a large scale to map whole interactomes.\n\nThe yeast two hybrid system (Y2H) is suited to explore the binary interactions among two proteins at a time. Affinity purification and subsequent mass spectrometry is suited to identify a protein complex. Both methods can be used in a high-throughput (HTP) fashion. Yeast two hybrid screens allow false positive interactions between proteins that are never expressed in the same time and place; affinity capture mass spectrometry does not have this drawback, and is the current gold standard. Yeast two-hybrid data better indicates non-specific tendencies towards sticky interactions rather while affinity capture mass spectrometry better indicates functional in vivo protein–protein interactions.<ref name=\"10.3389/fnmol.2014.00058\"></ref>\n\nOnce an interactome has been created, there are numerous ways to analyze its properties. However, there are two important goals of such analyses. First, scientists try to elucidate the systems properties of interactomes, e.g. the topology of its interactions. Second, studies may focus on individual proteins and their role in the network. Such analyses are mainly carried out using bioinformatics methods and include the following, among many others:\n\nFirst, the coverage and quality of an interactome has to be evaluated. Interactomes are never complete, given the limitations of experimental methods. For instance, it has been estimated that typical Y2H screens detect only 25% or so of all interactions in an interactome. The coverage of an interactome can be assessed by comparing it to benchmarks of well-known interactions that have been found and validated by independent assays. Other methods filter out false positives calculating the similarity of known annotations of the proteins involved or define a likelihood of interaction using the subcellular localization of these proteins.\n\nUsing experimental data as a starting point, \"homology transfer\" is one way to predict interactomes. Here, PPIs from one organism are used to predict interactions among homologous proteins in another organism (\"interologs\"). However, this approach has certain limitations, primarily because the source data may not be reliable (e.g. contain false positives and false negatives). In addition, proteins and their interactions change during evolution and thus may have been lost or gained. Nevertheless, numerous interactomes have been predicted, e.g. that of \"Bacillus licheniformis\".\n\nSome algorithms use experimental evidence on structural complexes, the atomic details of binding interfaces and produce detailed atomic models of protein–protein complexes as well as other protein–molecule interactions. Other algorithms use only sequence information, thereby creating unbiased complete networks of interaction with many mistakes.\n\nSome methods use machine learning to distinguish how interacting protein pairs differ from non-interacting protein pairs in terms of pairwise features such as cellular colocalization, gene co-expression, how closely located on a DNA are the genes that encode the two proteins, and so on. Random Forest has been found to be most-effective machine learning method for protein interaction prediction. Such methods have been applied for discovering protein interactions on human interactome, specifically the interactome of Membrane proteins and the interactome of Schizophrenia-associated proteins.\n\nSome efforts have been made to extract systematically interaction networks directly from the scientific literature. Such approaches range in terms of complexity from simple co-occurrence statistics of entities that are mentioned together in the same context (e.g. sentence) to sophisticated natural language processing and machine learning methods for detecting interaction relationships.\n\nProtein interaction networks have been used to predict the function of proteins of unknown functions. This is usually based on the assumption that uncharacterized proteins have similar functions as their interacting proteins (\"guilt by association\"). For example, YbeB, a protein of unknown function was found to interact with ribosomal proteins and later shown to be involved in translation. Although such predictions may be based on single interactions, usually several interactions are found. Thus, the whole network of interactions can be used to predict protein functions, given that certain functions are usually enriched among the interactors.\n\nThe \"topology\" of an interactome makes certain predictions how a network reacts to the perturbation (e.g. removal) of nodes (proteins) or edges (interactions). Such perturbations can be caused by mutations of genes, and thus their proteins, and a network reaction can manifest as a disease. A network analysis can identify drug targets and biomarkers of diseases.\n\nInteraction networks can be analyzed using the tools of graph theory. Network properties include the degree distribution, clustering coefficients, betweenness centrality, and many others. The distribution of properties among the proteins of an interactome has revealed that the interactome networks often have scale-free topology where functional modules within a network indicate specialized subnetworks. Such modules can be functional, as in a signaling pathway, or structural, as in a protein complex. In fact, it is a formidable task to identify protein complexes in an interactome, given that a network on its own does not directly reveal the presence of a stable complex.\n\nViral protein interactomes consist of interactions among viral or phage proteins. They were among the first interactome projects as their genomes are small and all proteins can be analyzed with limited resources. Viral interactomes are connected to their host interactomes, forming virus-host interaction networks. Some published virus interactomes include\n\nBacteriophage\n\nThe lambda and VZV interactomes are not only relevant for the biology of these viruses but also for technical reasons: they were the first interactomes that were mapped with multiple Y2H vectors, proving an improved strategy to investigate interactomes more completely than previous attempts have shown.\n\nHuman (mammalian) viruses\n\nRelatively few bacteria have been comprehensively studied for their protein–protein interactions. However, none of these interactomes are complete in the sense that they captured all interactions. In fact, it has been estimated that none of them covers more than 20% or 30% of all interactions, primarily because most of these studies have only employed a single method, all of which discover only a subset of interactions. Among the published bacterial interactomes (including partial ones) are\n\nThe \"E. coli\" and \"Mycoplasma\" interactomes have been analyzed using large-scale protein complex affinity purification and mass spectrometry (AP/MS), hence it is not easily possible to infer direct interactions. The others have used extensive yeast two-hybrid (Y2H) screens. The \"Mycobacterium tuberculosis\" interactome has been analyzed using a bacterial two-hybrid screen (B2H).\n\nNote that numerous additional interactomes have been predicted using computational methods (see section above).\n\nThere have been several efforts to map eukaryotic interactomes through HTP methods. While no biological interactomes have been fully characterized, over 90% of proteins in \"Saccharomyces cerevisiae\" have been screened and their interactions characterized, making it the best-characterized interactome. Species whose interactomes have been studied in some detail include \nRecently, the pathogen-host interactomes of Hepatitis C Virus/Human (2008), Epstein Barr virus/Human (2008), Influenza virus/Human (2009) were delineated through HTP to identify essential molecular components for pathogens and for their host's immune system.\n\nAs described above, PPIs and thus whole interactomes can be predicted. While the reliability of these predictions is debatable, they are providing hypotheses that can be tested experimentally. Interactomes have been predicted for a number of species, e.g.\n\nProtein interaction networks can be analyzed with the same tool as other networks. In fact, they share many properties with biological or social networks. Some of the main characteristics are as follows.\n\nThe degree distribution describes the number of proteins that have a certain number of connections. Most protein interaction networks show a scale-free (power law) degree distribution where the connectivity distribution P(k) ~ k with k being the degree. This relationship can also be seen as a straight line on a log-log plot since, the above equation is equal to log(P(k)) ~ —y•log(k). One characteristic of such distributions is that there are many proteins with few interactions and few proteins that have many interactions, the latter being called \"hubs\".\n\nHighly connected nodes (proteins) are called hubs. Han et al. have coined the term \"party hub\" for hubs whose expression is correlated with its interaction partners. Party hubs also connect proteins within functional modules such as protein complexes. In contrast, \"date hubs\" do not exhibit such a correlation and appear to connect different functional modules. Party hubs are found predominantly in AP/MS data sets, whereas date hubs are found predominantly in binary interactome network maps. Note that the validity of the date hub/party hub distinction was disputed. Party hubs generally consist of multi-interface proteins whereas date hubs are more frequently single-interaction interface proteins. Consistent with a role for date-hubs in connecting different processes, in yeast the number of binary interactions of a given protein is correlated to the number of phenotypes observed for the corresponding mutant gene in different physiological conditions.\n\nNodes involved in the same biochemical process are highly interconnected.\n\nThe evolution of interactome complexity is delineated in a study published in \"Nature\". In this study it is first noted that the boundaries between prokaryotes, unicellular eukaryotes and multicellular eukaryotes are accompanied by orders-of-magnitude reductions in effective population size, with concurrent amplifications of the effects of random genetic drift. The resultant decline in the efficiency of selection seems to be sufficient to influence a wide range of attributes at the genomic level in a nonadaptive manner. The Nature study shows that the variation in the power of random genetic drift is also capable of influencing phylogenetic diversity at the subcellular and cellular levels. Thus, population size would have to be considered as a potential determinant of the mechanistic pathways underlying long-term phenotypic evolution. In the study it is further shown that a phylogenetically broad inverse relation exists between the power of drift and the structural integrity of protein subunits. Thus, the accumulation of mildly deleterious mutations in populations of small size induces secondary selection for protein–protein interactions that stabilize key gene functions, mitigating the structural degradation promoted by inefficient selection. By this means, the complex protein architectures and interactions essential to the genesis of phenotypic diversity may initially emerge by non-adaptive mechanisms.\n\nKiemer and Cesareni raise the following concerns with the state (circa 2007) of the field especially with the comparative interactomic: The experimental procedures associated with the field are error prone leading to \"noisy results\". This leads to 30% of all reported interactions being artifacts. In fact, two groups using the same techniques on the same organism found less than 30% interactions in common. However, some authors have argued that such non-reproducibility results from the extraordinary sensitivity of various methods to small experimental variation. For instance, identical conditions in Y2H assays result in very different interactions when different Y2H vectors are used.\n\nTechniques may be biased, i.e. the technique determines which interactions are found. In fact, any method has built in biases, especially protein methods. Because every protein is different no method can capture the properties of each protein. For instance, most analytical methods that work fine with soluble proteins deal poorly with membrane proteins. This is also true for Y2H and AP/MS technologies.\n\nInteractomes are not nearly complete with perhaps the exception of \"S. cerevisiae.\" This is not really a criticism as any scientific area is \"incomplete\" initially until the methodologies have been improved. Interactomics in 2015 is where genome sequencing was in the late 1990s, given that only a few interactome datasets are available (see table above).\n\nWhile genomes are stable, interactomes may vary between tissues, cell types, and developmental stages. Again, this is not a criticism, but rather a description of the challenges in the field.\n\nIt is difficult to match evolutionarily related proteins in distantly related species. While homologous DNA sequences can be found relatively easily, it is much more difficult to predict homologous interactions (\"interologs\") because the homologs of two interacting proteins do not need to interact. For instance, even within a proteome two proteins may interact but their paralogs may not.\n\nEach protein–protein interactome may represent only a partial sample of potential interactions, even when a supposedly definitive version is published in a scientific journal. Additional factors may have roles in protein interactions that have yet to be incorporated in interactomes. The binding strength of the various protein interactors, microenvironmental factors, sensitivity to various procedures, and the physiological state of the cell all impact protein–protein interactions, yet are usually not accounted for in interactome studies.\n\n\n\n\n\n"}
{"id": "737905", "url": "https://en.wikipedia.org/wiki?curid=737905", "title": "Johann Gottlieb Georgi", "text": "Johann Gottlieb Georgi\n\nJohann Gottlieb Georgi (31 December 1729 – 27 October 1802) was a German botanist, naturalist and geographer.\n\nA native of Pomerania, Georgi accompanied both Johan Peter Falk and Peter Simon Pallas on their respective journeys through Siberia. During 1770-1774 he travelled on its behalf to Astrakhan, the Urals, Bashkir, the Barabinsk steppe, the Kolyvanskoe silver mines (to assess the ore content), Altai, Tomsk, Irkutsk, Baikal, and Dauren. In 1783 he became an academician of the Russian Academy of Sciences in St Petersburg.\n\nGeorgi was particularly interested in the Baikal region. Based on collections from far eastern Russia, in his 1775 publication \"Bemerkungen einer Reise im Russischen Reich im Jahre 1772\", Georgi provided the first botanical descriptions of many of the region's flowering plants, among them the \"Baikal Scullcap\" (S. baicalensis.) Many of these plants and herbs were later collected by European botanists in China, and thereafter became rare specimens in European botanical gardens.\n\nAfter his fellow botanist and travelling companion Falk took his own life in 1774, Georgi edited his notes which were published in Germany in 1785 as \"Beyträge zur topgraphischen Kentniss des russischen Reichs I.III\".\n\nIn 1790, Georgi's German work of the description and urban plans of the city of St. Petersburg was published. It appeared in a second edition in Riga in 1793, and was finally translated into Russian a year later. His \"Geographisch-physikalische und Naturhistor. Beschreibung des Russ. Reiches\", a nine volume edition of the geography and natural history of the Russian Empire, was published in Königsberg, Germany during 1797 - 1802.\n\n"}
{"id": "8648100", "url": "https://en.wikipedia.org/wiki?curid=8648100", "title": "Juan José Giambiagi", "text": "Juan José Giambiagi\n\nJuan José Giambiagi (18 June 1924 – 8 January 1996) was an Argentinian physicist and co-discoverer of the dimensional regularization.\n\n"}
{"id": "3697442", "url": "https://en.wikipedia.org/wiki?curid=3697442", "title": "Karl Grobben", "text": "Karl Grobben\n\nKarl Grobben (August 27, 1854, Brno – April 13, 1945, Salzburg) was an Austrian biologist. He graduated from, and later worked at, the University of Vienna, chiefly on molluscs and crustaceans. He was also the editor of a new edition of Carl Friedrich Wilhelm Claus' \"Lehrbuch der Zoologie\", and the coiner of the terms \"protostome\" and \"deuterostome\".\n\nTaxa named by Grobben include:\n\nTaxa named in Grobben's honour include:\n"}
{"id": "14450665", "url": "https://en.wikipedia.org/wiki?curid=14450665", "title": "Lajos Abafi", "text": "Lajos Abafi\n\nLajos Abafi or Ludwig Abafi-Aigner (11 February 1840 in Nagyjécsa, Kingdom of Hungary, Austrian Empire – 19 June 1909 in Budapest, Austria-Hungary) was a Hungarian editor, a librarian and entomologist.\n\nHis family, of German origin, moved to Pozsony (today Bratislava) in 1858. There, he learned the Hungarian language. His family then moved to Pest in 1863. Ludwig part completed his studies in Cologne and part in Stuttgart. He was especially interested in Lepidoptera and he founded a popular library. In 1870, he became a freemason. He worked for twelve years on a history of freemasonry. He changed his first name Ludwig to its Hungarian form, Lajos, and he assumed his \"nom de plume\" Abafi. His enterprise declined during the years 1880 to 1890, when he closed it. From 1890, he was entirely devoted to entomology. He published his observations in the revue of the Budapest museum \"Természetrajzi Füzetek\" and participated as editor and author in the \"Fauna Regni Hungariae\". His book, \"Magyarország lepkéi\" (butterflies of Hungary) of 1907, was extremely popular and influenced many generations of entomologists of his country.\n\n"}
{"id": "32617710", "url": "https://en.wikipedia.org/wiki?curid=32617710", "title": "Lev Rukhin", "text": "Lev Rukhin\n\nLev Rukhin ( 16 October 1912 in Moscow – 8 September 1959 in Leningrad) was a Russian geologist.\n\nHe was born in Moscow. He graduated from Leningrad State University (1933).\nMajor works are devoted to the lithology and paleogeography. He was one of the first to use statistical methods in studying sedimentary rocks.\nHe was awarded the \"Badge of Honor\" and medals.\n\n\n"}
{"id": "52583141", "url": "https://en.wikipedia.org/wiki?curid=52583141", "title": "List of GRC Software", "text": "List of GRC Software\n\nList of GRC Software\n\nThis is a list of Governance, risk management, and compliance (GRC) software. GRC software is used across a wide variety of mid to large enterprise corporations. Professionals responsible for the acquisition, development and maintenance of GRC software are generally the CRO (Chief Risk Officer), CIO and risk officers.\n\n"}
{"id": "2366759", "url": "https://en.wikipedia.org/wiki?curid=2366759", "title": "List of Game Boy colors and styles", "text": "List of Game Boy colors and styles\n\nThis is a list of case colors and styles that have been produced for the Game Boy line of handheld systems since 1989.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome others: Cup of Noodles, Fujifilm, the Hanshin Tigers baseball team, Parco, Shiseido and a movie called \"Custom Made\"\n\n\n"}
{"id": "42017954", "url": "https://en.wikipedia.org/wiki?curid=42017954", "title": "List of cosmological computation software", "text": "List of cosmological computation software\n\nThe cosmic microwave background (CMB) is the thermal radiation assumed to be left over from the \"Big Bang\" of cosmology. The CMB is a snapshot of the oldest light in our universe, imprinted on the sky when the universe was just 380,000 years old. It shows tiny temperature fluctuations that correspond to regions of slightly different densities, representing the seeds of all future structure: the stars and galaxies of today. Therefore, analysis of the small anisotropies in the CMB helps us to understand the origin and the fate of our universe. In past few decades, there has been a lot of improvement in the observations and several experiments, performed to understand the basic structure of the universe. For analyzing data of different cosmological experiments and for understanding the theoretical nature of the universe many advanced methods and computing software are developed in and used by Cosmologists for years. These software are widely used by the cosmologists across the globe.\n\nThe computational software, used in cosmology can be classified into the following major classes.\n\n\nHEALPix (sometimes written as Healpix), an acronym for Hierarchical Equal Area isoLatitude Pixelisation of a 2-sphere, can refer to either an algorithm for pixelization of the 2-sphere, an associated software package, or an associated class of map projections. Healpix is widely used for cosmological random map generation. The original motivation for devising HEALPix was one of necessity. NASA's WMAP and the European Space Agency’s mission Planck - produce multi-frequency data sets sufficient for the construction of full-sky maps of the microwave sky at an angular resolution of a few arc minutes. The principal requirements in the development of HEALPix were to create a mathematical structure that supports a suitable discretization of functions on a sphere at sufficiently high resolution, and to facilitate fast and accurate statistical and astrophysical analysis of massive full-sky data sets. The HEALPix maps are used in almost all the data processing research in cosmology.\n\nCMBFAST is a computer code, developed by Uroš Seljak and Matias Zaldarriaga (based on a Boltzmann code written by Edmund Bertschinger, Chung-Pei Ma and Paul Bode) for computing the power spectrum of the cosmic microwave background anisotropy. It is the first efficient program to do so, reducing the time taken to compute the anisotropy from several days to a few minutes by using a novel semi-analytic line-of-sight approach.\n\nCode for Anisotropies in the Microwave Background by Antony Lewis and Anthony Challinor. The code was originally based on CMBFAST. Later several developments are made to make it a faster and more accurate and compatible with the present research. The code is written in an object oriented manner to make it more user friendly.\n\nCMBEASY is a software package written by Michael Doran, Georg Robbers and Christian M. Müller. The code is based on the CMBFAST package. CMBEASY is fully object oriented C++. This considerably simplifies manipulations and extensions of the CMBFAST code. In addition, a powerful Spline class can be used to easily store and visualize data. Many features of the CMBEASY package are also accessible via a graphical user interface. This may be helpful for gaining intuition, as well as for instruction purposes.\n\nCLASS is a new Boltzmann code developed in this line. The purpose of CLASS is to simulate the evolution of linear perturbations in the universe and to compute CMB and large scale structure observables. Its name also comes from the fact that it is written in object-oriented style mimicking the notion of class. Classes are a programming feature available, e.g., in C++ and python, but these languages are known to be less vectorizable/parallelizable than plain C (or Fortran), and hence potentially slower. CLASS is written in plain C for high performances, while organizing the code in a few modules that reproduce the architecture and philosophy of C++ classes, for optimal readability and modularity.\n\nAnalizeThis is a parameter estimation package used by cosmologists. It comes with the CMBEASY package. The code is written in C++ and uses the global metropolis Algorithm for estimation of cosmological parameters. The code was developed by Michael Doran, for parameter estimation using WMAP-5 likelihood. However, the code was not updated after 2008 for the new CMB experiments. Hence this package is currently not in use by the CMB research community. The package comes up with a nice GUI.\n\nCosmoMC is a Fortran 2003 Markov chain Monte Carlo (MCMC) engine for exploring cosmological parameter space. The code does brute force (but accurate) theoretical matter power spectrum and Cl calculations using CAMB. CosmoMC uses a simple local Metropolis algorithm along with an optimized fast-slow sampling method. This fast-slow sampling method provides faster convergence for the cases with many nuisance parameters like Planck. CosmoMC package also provides subroutines for post processing and plotting of the data.\n\nCosmoMC was written by Antony Lewis in 2002 and later several versions are developed to keep the code up-to date with different cosmological experiments. It is presently the most used cosmological parameter estimation code.\n\nSCoPE/Slick Cosmological Parameter Estimator is a newly developed cosmological MCMC package written by Santanu Das in C language. Apart of standard global metropolis algorithm the code uses three unique technique named as 'delayed rejection' that increases the acceptance rate of a chain, 'pre-fetching' that helps an individual chain to run on parallel CPUs and 'inter-chain covariance update' that prevents clustering of the chains allowing faster and better mixing of the chains. The code is capable of faster computation of cosmological parameters from WMAP and Planck data.\n\n\nDifferent cosmology experiments, in particular the CMB experiments like WMAP and Planck measures the temperature fluctuations in the CMB sky and then measure the CMB power spectrum from the observed skymap. But for parameter estimation the χ² is required. Therefore, all these CMB experiments comes up with its own likelihood software.\n\n"}
{"id": "40789532", "url": "https://en.wikipedia.org/wiki?curid=40789532", "title": "List of photochemists", "text": "List of photochemists\n\nThis is a list of notable photochemists. Photochemistry, a sub-discipline of chemistry, is the study of chemical reactions that proceed with the absorption of light by atoms or molecules.\n\n\n"}
{"id": "8599665", "url": "https://en.wikipedia.org/wiki?curid=8599665", "title": "List of therapeutic monoclonal antibodies", "text": "List of therapeutic monoclonal antibodies\n\nThis is a list of therapeutic, diagnostic and preventive monoclonal antibodies, antibodies that are clones of a single parent cell. When used as drugs, the International Nonproprietary Names (INNs) end in -mab. The remaining syllables of the INNs, as well as the column \"Source\", are explained in \"Nomenclature of monoclonal antibodies\".\nThe abbreviations in the column \"Type\" are as follows:\n\nThis list of over 500 monoclonal antibodies includes approved and investigational drugs as well as drugs that have been withdrawn from market; consequently, the column \"Use\" does not necessarily indicate clinical usage. See the list of FDA approved therapeutic monoclonal antibodies in the Monoclonal antibody therapy page.\n"}
{"id": "46402668", "url": "https://en.wikipedia.org/wiki?curid=46402668", "title": "McGill Arctic Research Station", "text": "McGill Arctic Research Station\n\nThe director of the research station is Wayne Pollard (from McGill University). He has many experiences with drilling and geophysical tools. His goals are to identify niche environments in permafrost capable of harbouring microbial life at or near the limit of its habitability.\n\nMcGill Arctic Research Station is located near the terminus of three glaciers: Baby Glacier, White Glacier and Thompson Glacier. It is also beside the Expedition River and Colour Lake. The glaciers have been monitored from 1960 to 1999 with gaps in between them. The closest community is Grise Fiord, away. However, all flights to and from the station all go through Resolute Bay Airport, which is more than away. The area surrounding it are rolling hills and tall mountains.\n\nThe average temperature at the research station is . Because of the cold temperatures, they can test how microbes can survive on the other colder planets in our Solar System. The station receives polar night from the first week of November until the beginning of February, and the midnight sun from the beginning of April until the second week of September.\n\n\n"}
{"id": "20324423", "url": "https://en.wikipedia.org/wiki?curid=20324423", "title": "Model solid approximation", "text": "Model solid approximation\n\nThe model solid approximation is a method used for determining the extrema of energy bands in semiconductors. The method was first proposed for silicon-germanium alloys by Chris G. Van de Walle and Richard M. Martin in 1986 and extended to several other semiconductor materials by Van de Walle in 1989. It has been used extensively for modelling semiconductor heterostructure devices such as quantum cascade lasers.\n\nAlthough the electrostatic potential in a semiconductor crystal fluctuates on an atomic scale, the model solid approximation averages these fluctuations out to obtain a constant energy level for each material.\n"}
{"id": "22320247", "url": "https://en.wikipedia.org/wiki?curid=22320247", "title": "Morphological Catalogue of Galaxies", "text": "Morphological Catalogue of Galaxies\n\nThe Morphological Catalogue of Galaxies (MCG) or Morfologiceskij Katalog Galaktik, is a Russian catalogue of 30,642 galaxies compiled by Boris Vorontsov-Velyaminov and V. P. Arkhipova. It is based on scrutiny of prints of the Palomar Sky Survey plates, and putatively complete to a photographic magnitude of 15. Including galaxies to magnitude 16 would have resulted in an unmanageably large dataset. The catalog was published in five parts (chapters) between 1962 and 1974, the final chapter including a certain number of galaxies with a photographic magnitude above 15.\n"}
{"id": "227214", "url": "https://en.wikipedia.org/wiki?curid=227214", "title": "Nekton", "text": "Nekton\n\nNekton or necton refers to the aggregate of actively swimming aquatic organisms in a body of water. The term was proposed by German biologist Ernst Haeckel to differentiate between the active swimmers in a body of water, and the passive organisms that were carried along by the current, the plankton. As a guideline, nektonic organisms have a high Reynolds number (greater than 1000) and planktonic organisms a low one (less than 10). However, some organisms can begin life as plankton and transition to nekton later on in life, sometimes making distinction difficult when attempting to classify certain plankton-to-nekton species as one or the other. For this reason, some biologists choose not to use this term. \n\nThe term was first proposed and used by the German biologist Ernst Haeckel in 1891 in his article \"Plankton-Studien\" where he contrasted it with plankton, the aggregate of passively floating, drifting, or somewhat motile organisms present in a body of water, primarily tiny algae and bacteria, small eggs and larvae of marine organisms, and protozoa and other minute consumers. Today it is sometimes considered an obsolete term because it often does not allow for the meaningful quantifiable distinction between these two groups. Some biologists no longer use it.\n\nAs a guideline, nekton are larger and tend to swim largely at biologically high Reynolds numbers (>10³ and up beyond 10⁹), where inertial flows are the rule, and eddies (vortices) are easily shed. Plankton, on the other hand, are small and, if they swim at all, do so at biologically low Reynolds numbers (0.001 to 10), where the viscous behavior of water dominates, and reversible flows are the rule. Organisms such as jellyfish and others are considered plankton when they are very small and swim at low Reynolds numbers, and considered nekton as they grow large enough to swim at high Reynolds numbers. Many animals considered classic examples of nekton (e.g., \"Mola mola\", squid, marlin) start out life as tiny members of the plankton and then, it was argued, gradually transition to nekton as they grow.\n\nOceanic nekton comprises animals largely from three clades:\n\nThere are organisms whose initial life stage is identified as being planktonic but when they grow and increase in body size they become nektonic. A typical example is the medusa of the jellyfish.\n\n\n"}
{"id": "17749634", "url": "https://en.wikipedia.org/wiki?curid=17749634", "title": "Numerical sign problem", "text": "Numerical sign problem\n\nIn applied mathematics, the numerical sign problem is the problem of numerically evaluating the integral of a highly oscillatory function of a large number of variables. Numerical methods fail because of the near-cancellation of the positive and negative contributions to the integral. Each has to be integrated to very high precision in order for their difference to be obtained with useful accuracy.\n\nThe sign problem is one of the major unsolved problems in the physics of many-particle systems. It often arises in calculations of the properties of a quantum mechanical system with large number of strongly interacting fermions, or in field theories involving a non-zero density of strongly interacting fermions.\n\nIn physics the sign problem is typically (but not exclusively) encountered in calculations of the properties of a quantum mechanical system with large number of strongly interacting fermions, or in field theories involving a non-zero density of strongly interacting fermions. Because the particles are strongly interacting, perturbation theory is inapplicable, and one is forced to use brute-force numerical methods. Because the particles are fermions, their wavefunction changes sign when any two fermions are interchanged (due to the anti-symmetry of the wave function, see Pauli principle). So unless there are cancellations arising from some symmetry of the system, the quantum-mechanical sum over all multi-particle states involves an integral over a function that is highly oscillatory, and hence hard to evaluate numerically, particularly in high dimension. Since the dimension of the integral is given by the number of particles, the sign problem becomes severe in the thermodynamic limit. The field-theoretic manifestation of the sign problem is discussed below.\n\nThe sign problem is one of the major unsolved problems in the physics of many-particle systems, impeding progress in many areas:\n\nSources:\n\nIn a field theory approach to multi-particle systems, the fermion density is controlled by the value of the fermion chemical potential formula_1. One evaluates the partition function formula_2 by summing over all classical field configurations, weighted by formula_3 where formula_4 is the action of the configuration. The sum over fermion fields can be performed analytically, and one is left with a sum over the bosonic fields formula_5 (which may have been originally part of the theory, or have been produced by a Hubbard–Stratonovich transformation to make the fermion action quadratic)\n\nwhere formula_7 represents the measure for the sum over all configurations formula_8 of the bosonic fields, weighted by\n\nwhere formula_4 is now the action of the bosonic fields, and formula_11 is a matrix that encodes how the fermions were coupled to the bosons. The expectation value of an observable formula_12 is therefore an average over all configurations weighted by formula_13\n\nIf formula_13 is positive, then it can be interpreted as a probability measure, and formula_16 can be calculated by performing the sum over field configurations numerically, using standard techniques such as Monte Carlo importance sampling.\n\nThe sign problem arises when formula_13 is non-positive. This typically occurs in theories of fermions when the fermion chemical potential formula_1 is nonzero, i.e. when there is a nonzero background density of fermions. If formula_19 there is no particle-antiparticle symmetry, and formula_20, and hence the weight formula_21, is in general a complex number, so Monte Carlo importance sampling cannot be used to evaluate the integral.\n\nA field theory with a non-positive weight can be transformed to one with a positive weight, by incorporating the non-positive part (sign or complex phase) of the weight into the observable. For example, one could decompose the weighting function into its modulus and phase,\nwhere formula_23 is real and positive, so\n\nNote that the desired expectation value is now a ratio where the numerator and denominator are expectation values that both use a positive weighting function, formula_23. However, the phase formula_26 is a highly oscillatory function in the configuration space, so if one uses Monte Carlo methods to evaluate the numerator and denominator, each of them will evaluate to a very small number, whose exact value is swamped by the noise inherent in the Monte Carlo sampling process. The \"badness\" of the sign problem is measured by the smallness of the denominator formula_27: if it is much less than 1 then the sign problem is severe.\nIt can be shown (e.g.) that\nwhere formula_29 is the volume of the system, formula_30 is the temperature, and formula_31 is an energy density. The number of Monte Carlo sampling points needed to obtain an accurate result therefore rises exponentially as the volume of the system becomes large, and as the temperature goes to zero.\n\nThe decomposition of the weighting function into modulus and phase is just one example (although it has been advocated as the optimal choice since it minimizes the variance of the denominator ). In general one could write\nwhere formula_23 can be any positive weighting function (for example, the weighting function of the formula_34 theory.) The badness of the sign problem is then measured by\nwhich again goes to zero exponentially in the large-volume limit.\n\nThe sign problem is NP-hard, implying that a full and generic solution of the sign problem would also solve all problems in the complexity class NP in polynomial time. If (as is generally suspected) there are no polynomial-time solutions to NP problems (see P versus NP problem), then there is no \"generic\" solution to the sign problem. This leaves open the possibility that there may be solutions that work in specific cases, where the oscillations of the integrand have a structure that can be exploited to reduce the numerical errors.\n\nIn systems with a moderate sign problem, such as field theories at a sufficiently high temperature or in a sufficiently small volume, the sign problem is not too severe and useful results can be obtained by various methods, such as more carefully tuned reweighting, analytic continuation from imaginary formula_1 to real formula_1, or Taylor expansion in powers of formula_1.\n\nThere are various proposals for solving systems with a severe sign problem:\n\n\n"}
{"id": "36379523", "url": "https://en.wikipedia.org/wiki?curid=36379523", "title": "Planet Hunters", "text": "Planet Hunters\n\nPlanet Hunters is a citizen science project to find planets using human eyes. It does this by having users analyze data from the NASA Kepler Space Mission. It was launched by a team led by Debra Fischer at Yale University, as part of the Zooniverse project.\n\nThe Planet Hunters project exploits the fact that humans are better at recognising visual patterns than computers. The website displays an image of data collected by the NASA Kepler Space Mission and asks human users (referred to as \"Citizen Scientists\") to look at the data and see how the brightness of a star changes over time. This brightness data is represented as a graph and referred to as a star's \"light curve\". Such curves are helpful in discovering extrasolar planets due to the brightness of a star decreasing when a planet passes in front of it, as seen from Earth. Periods of reduced brightness can thus provide evidence of planetary transits, but may also be caused by errors in recording, projection, or other phenomena.\n\nUsers are asked to sort the light curves based on the patterns. The patterns available are \"quiet\" and \"variable\". When the light curve is quiet, the graph mostly follows a straight path. When the light curve is variable, there are a number of things that it can do. It can be \"regular\", \"pulsating\", or \"irregular\". A regular light curve means it goes up once and down once, reaching an apex and descending. A pulsating light curve means it goes up and down along the y-axis on a rather steep basis. An irregular light curve has points that go up and down on the y-axis in an unpredictable pattern.\n\nFrom time to time, the project will observe eclipsing binary stars. Essentially these are stars that orbit each other. Much as a planet can interrupt the brightness of a star, another star can too. There is a noticeable difference on the light curves. It will appear as a large transit (a large dip) and a smaller transit (a smaller dip).\n\nAs of December 2017, there are a total of 621 multiplanetary stars, or stars that contains at least two confirmed planets. In a multiplanet system plot, there are many different patterns of transit. Due to the different sizes of planets, the transits dip down to different points.\n\nStellar flares are observed when there is an explosion on the surface of a star. This will cause the star's brightness to shoot up considerably, with a steep drop off.\n\nSo far, over 12 million observations have been analyzed. Out of those, 34 candidate planets had been found as of July 2012. In October 2012 it was announced that two volunteers from the Planet Hunters initiative had discovered a novel Neptune-like planet which is part of a four star double binary system, orbiting one of the pairs of stars while the other pair of stars orbits at a distance of around 1000 AU. This is the first planet discovered to have a stable orbit in such a complex stellar environment. The system is located just under 5000 light years away, and the new planet has been designated PH1, short for Planet Hunters 1.\n\n\"Yellow rows indicate a circumbinary planet. Light green rows indicate planet orbiting around one star in a multiple star system.\"\nFurthermore, the unusual light curve of KIC 8462852 (also known as Tabby's Star) has engendered speculation that an alien civilization's Dyson sphere is responsible.\n\nZooniverse projects:\n"}
{"id": "37798147", "url": "https://en.wikipedia.org/wiki?curid=37798147", "title": "Research Moored Array for African-Asian-Australian Monsoon Analysis and Prediction", "text": "Research Moored Array for African-Asian-Australian Monsoon Analysis and Prediction\n\nThe Research Moored Array for African-Asian-Australian Monsoon Analysis and Prediction, also known as RAMA, is a system of moored observation buoys in the Indian Ocean that collects meteorological and oceanographic data. The data collected by RAMA will greatly enhance the ability of scientists to understand climatic events and predict monsoon events. Climatic and oceanic events in the Indian Ocean affect weather and climate throughout the rest of the world (such as El Niño, hurricanes, and United States weather), so RAMA will support weather forecasting and climate research worldwide. Although widely supported internationally, the system has only been partially implemented () due to pirate activity off the coast of Somalia.\n\nAlthough the data coverage for the Indian Ocean has been poor, it has not been non-existent. Satellites have been taking measurements, but those measurements require validation in situ. Some nations, like India and Australia, operate national ocean observing programs. Researchers have mounted observing equipment on ships of opportunity to take measurements. Also, the Argo float system has taken data in the Indian Ocean. What RAMA will contribute is large scale, long term data with high temporal resolution. High temporal resolution will allow rapid changes to be captured. With the installation of RAMA, the Indian Ocean will have a basin-wide observing system similar to TAO/TRITON in the Pacific Ocean and PIRATA in the tropical Atlantic. RAMA will complete the worldwide network of tropical ocean observing buoys, which will help with modeling and forecasting.The data RAMA collects will facilitate the study of \"ocean-atmosphere interactions, mixed layer dynamics, and ocean circulation related to the monsoon on intraseasonal to decadal time scales.\"\n\nWhen complete RAMA will have 46 moored buoys, each of which is designed to be serviced annually. 38 are to be surface buoys and eight are subsurface ones. \nThere are four types of moored buoys: \n\nData obtained from the buoys is beamed to the Global Telecommunications System using the Service Argos Satellite Relay System. From there, it is distributed to agencies that require that information, such as weather centers. All of the data is free to access.\n\nBy the end of 2008, 22 of the 46 buoys were in position, and the system was expected to be fully implemented by 2012. However, pirate activity off the coast of Somalia has jeopardized the completion of the project. Shipping insurers require special insurance to enter the piracy regions, which cover a large portion of the Indian Ocean. As of July 2011, 30 of the 46 moored buoys were established, but \"13 of the remaining 16 are in the insurer's exclusion zone\". Some buoys have even been shot at. While satellite data can still give enough information to make monsoon predictions, climate system studies will be affected by the gap in information.\n\nDespite the system being incomplete, it has already begun providing valuable data. Farmers in Australia were able to use the data provided by RAMA to prepare for a bad growing season in 2008. It has also helped correct improper satellite measurements of surface heat flux.\n"}
{"id": "42642303", "url": "https://en.wikipedia.org/wiki?curid=42642303", "title": "SLAC bag model", "text": "SLAC bag model\n\nThe SLAC bag model is a simple theoretical model for a possible structure for hadrons. The MIT bag model is another similar model. The \"SLAC\" in the name stands for Stanford Linear Accelerator Center.\n"}
{"id": "40593828", "url": "https://en.wikipedia.org/wiki?curid=40593828", "title": "Science, Numbers, and I", "text": "Science, Numbers, and I\n\nScience, Numbers, and I is a collection of seventeen scientific essays by Isaac Asimov. It was the sixth of a series of books collecting essays from \"The Magazine of Fantasy and Science Fiction\". It was first published by Doubleday & Company in 1968.\n\n\n"}
{"id": "3846723", "url": "https://en.wikipedia.org/wiki?curid=3846723", "title": "Taylor's salamander", "text": "Taylor's salamander\n\nTaylor's salamander, \"Ambystoma taylori,\" is a species of salamander found only in Laguna Alchichica, a high-altitude ( above sea level) crater lake to the southwest of Perote, Puebla, Mexico. It was first described in 1982 but had been known to science prior to that. It is a neotenic salamander, breeding while still in the larval state and not undergoing metamorphosis. The lake in which it lives is becoming increasingly saline and less suitable for the salamander, which is declining in numbers. The International Union for Conservation of Nature (IUCN) has rated it as being \"critically endangered\".\n\nIt was described in 1982 by Brandon, Maruska, and Rumph, and named for Edward Harrison Taylor (1889–1978), an American herpetologist. However, the species had been known to science long before then. Taylor himself attempted to describe the species as \"Ambystoma subsalsum\" in 1943, but mistakenly used a Mexican or plateau tiger salamander as the holotype. This rendered the name invalid, and made it into a synonym for the tiger salamander. \n\nThis salamander is moderately sized, with most individuals measuring being mature, while the largest one being in snout–vent length. It is a neotenic species, which means it retains its caudal fin and external gills into adulthood, never undergoing complete metamorphosis. It is entirely aquatic, breeding and laying its eggs in the same lake where it lives. Taylor's salamanders are pale yellowish in color, with dark spots along their dorsal sides. They have relatively short, thick external gill stalks. Their heads are quite large, and their limbs are underdeveloped, as in most \"Ambystoma\" neotenes. They feed by buccal suction, and are basically omnivores.\n\nThe \"A. taylori\" habitat in Lake Alchichica is brackish, with a salinity of . It is also very alkaline, with a pH of 8.5–10. The lake's water has a temperature range of 18–21 °C. The salamanders typically hide below the water line, under overhangs in the crater's edge, and into the deep water.\n\nLake Alchichica is becoming more saline as water is extracted for irrigation and drinking. The level of the lake has fallen and if this deterioration in water quality continues, this salamander is likely to become extinct. The International Union for Conservation of Nature has assessed the salamander's conservation status as being \"critically endangered\" and has proposed that a captive breeding programme be established.\n"}
{"id": "19228597", "url": "https://en.wikipedia.org/wiki?curid=19228597", "title": "The Four Faces of Nuclear Terrorism", "text": "The Four Faces of Nuclear Terrorism\n\nThe Four Faces of Nuclear Terrorism is a 2004 book by Charles D. Ferguson and William C. Potter (with Amy Sands, Leonard S. Spector and Fred L. Wehling) which explores the motivations and capabilities of terrorist organizations to carry out significant attacks using stolen nuclear weapons, to construct and detonate crude nuclear weapons, to release radiation by attacking or sabotaging nuclear facilities, and to build and use radiological weapons or \"dirty bombs.\" The authors argue that these \"four faces\" of nuclear terrorism are real threats which U.S. policy has failed to take into account. The book is the result of a two-year study by the Monterey Institute's Center for Nonproliferation Studies.\n\n"}
{"id": "13478782", "url": "https://en.wikipedia.org/wiki?curid=13478782", "title": "The Mathematical Experience", "text": "The Mathematical Experience\n\nThe Mathematical Experience (1981) is a book by Philip J. Davis and Reuben Hersh that discusses the practice of modern mathematics from a historical and philosophical perspective. The book discusses the psychology of mathematicians. It gives examples of famous proofs and outstanding problems, such as the Riemann hypothesis. It goes on to speculate about what a proof really means, in relationship to actual truth. Other topics include mathematics in education and some computer mathematics.\n\nThe first paperback edition won a U.S. National Book Award in Science. It is cited by some mathematicians as influential in their decision to continue their studies in graduate school; and has been hailed as a classic of mathematical literature.\nOn the other hand, Martin Gardner disagreed with some of the authors' philosophical opinions.\n\nA new edition, published in 1995, includes exercises and problems, making the book more suitable for classrooms. There is also \"The Companion Guide to The Mathematical Experience, Study Edition\". Both were co-authored with Elena Marchisotto. Davis and Hersh wrote a follow-up book, \"Descartes' Dream: The World According to Mathematics\" (Harcourt, 1986), and each has written other books with related themes, such as \"Mathematics And Common Sense: A Case of Creative Tension\" by Davis and \"What is Mathematics, Really?\" by Hersh.\n\n"}
{"id": "48738380", "url": "https://en.wikipedia.org/wiki?curid=48738380", "title": "The Oprah Effect", "text": "The Oprah Effect\n\nThe Oprah Effect is the phenomenon in which consuming soft news causes the politically unaware to vote more consistently with their own views, articulated by Matthew A. Baum and Angela S. Jamison in their 2006 study, \"The Oprah Effect: How Soft News Helps Inattentive Citizens Vote Consistently\". The findings and conclusions of the study centered around the distinction between hard and soft news as written about by Baum, and the complex interactions both types of news have on people with low and high political awareness. The term was pioneered by Oprah Winfrey.\n\nThe increase of soft news content available has led some scholars to question its value in informing the public, and therefore question whether the media is properly fulfilling its duty to inform the citizens. The Oprah Effect was demonstrated by testing a dependent variable of voting consistently versus an independent variable which measured how often the voter watched daytime talk shows. Voting consistency measures the consistency between a voter's preferences and their vote. For the context of this experiment, the measure of voting consistently was created by applying Lau and Redlawsk's model of \"correct\" voting to data from the 2000 National Election Studies. Increased consumption of soft news caused low information voters to vote more consistently, but had no significant effect on more aware voters. Exposure to hard news modestly increased the voting consistency of politically aware individuals, but had no significant effect on those with low political awareness.\n\nWhile Baum and Jamison agree that soft news contains lower quality political information, they conclude from their analysis that soft news fulfills a valuable duty by providing information to apolitical individuals. This is because soft news still exposes those individuals to the political information that they need to cast a vote for the candidate which best reflects their personal interests, even if it doesn't contain highly complex political information. They theorize that exposure to soft news has this effect on low information individuals because most people with little political knowledge have no interest in harder news, making their decision to consume soft news one between consuming soft news or consuming no news. Even though soft news may not be highly informational, it's still beneficial compared to consuming no news. They also go even further, contending that even if an individual with little political knowledge were to consume hard news, the more complex political information would be difficult for them to comprehend, and they would get relatively little out of it. As a result, they conclude their results should give some pause to critics of the increase soft news content in the media, because it increases the voting consistency of those with little political knowledge.\n"}
{"id": "1034511", "url": "https://en.wikipedia.org/wiki?curid=1034511", "title": "Tuberculosis classification", "text": "Tuberculosis classification\n\nThe current clinical classification system for tuberculosis (TB) is based on the pathogenesis of the disease.\n\nHealth care providers should comply with local laws and regulations requiring the reporting of TB. All persons with class 3 or class 5 TB should be reported promptly to the local health department. See list of notifiable diseases.\n\nThe U.S. Citizenship and Immigration Services has an additional TB classification for immigrants and refugees developed by the Centers for Disease Control and Prevention (CDC). The \"B notification program\" is an important screening strategy to identify new arrivals who have a high risk for TB.\n"}
{"id": "30046055", "url": "https://en.wikipedia.org/wiki?curid=30046055", "title": "Walter Jacobi", "text": "Walter Jacobi\n\nWalter Jacobi (January 13, 1918 – August 19, 2009) was a rocket scientist and member of the \"von Braun rocket group\", at Peenemünde (1939–1945) working on the V-2 rockets in World War II. He was among the scientists to surrender and travel to the United States to provide rocketry expertise via Operation Paperclip. He came to the United States on the first boat, November 16, 1945. with Operation Paperclip and Fort Bliss, Texas (1945–1949). He continued his work with the team when they moved to Redstone Arsenal, and he joined Marshall Space Flight Center to work for NASA. Jacobi worked on rocket \"structure and components.\"\n\nHe continued to support the space program and appear at public events until his death.\n"}
